{
  "owner": {
    "id": "GhostScientist",
    "display_name": "Dakota Kim",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/13916079?u=f0fafc4aef35fecd31d2ea99ac0626f4826e005d&v=4",
    "url": "https://github.com/GhostScientist",
    "bio": "Applied AI Engineer building interfaces for intelligent software, tools, and sometimes robots",
    "stats": {
      "total_repos": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 55,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "GhostScientist/skills",
      "url": "https://github.com/GhostScientist/skills",
      "description": "Public Skills for Claude Code by Dakota Kim (GhostScientist)",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-10T22:28:06Z",
        "created_at": "2025-12-08T15:22:00Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1802
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 10763
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2494
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/create-watchos-version",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/create-watchos-version/SKILL.md",
          "type": "blob",
          "size": 5394
        },
        {
          "path": "skills/create-watchos-version/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/create-watchos-version/references/api-compatibility.md",
          "type": "blob",
          "size": 4591
        },
        {
          "path": "skills/create-watchos-version/references/plan-template.md",
          "type": "blob",
          "size": 8325
        },
        {
          "path": "skills/create-watchos-version/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/create-watchos-version/scripts/analyze_project.py",
          "type": "blob",
          "size": 10340
        },
        {
          "path": "skills/experiment-design-checklist",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/experiment-design-checklist/SKILL.md",
          "type": "blob",
          "size": 6727
        },
        {
          "path": "skills/hugging-face-space-deployer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hugging-face-space-deployer/SKILL.md",
          "type": "blob",
          "size": 17580
        },
        {
          "path": "skills/hugging-face-space-deployer/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hugging-face-space-deployer/scripts/create_space.py",
          "type": "blob",
          "size": 7654
        },
        {
          "path": "skills/hugging-face-space-deployer/scripts/deploy_model.py",
          "type": "blob",
          "size": 27290
        },
        {
          "path": "skills/hugging-face-space-deployer/scripts/manage_space.py",
          "type": "blob",
          "size": 6090
        },
        {
          "path": "skills/hugging-face-space-deployer/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/README_template.md",
          "type": "blob",
          "size": 509
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/README_zerogpu.md",
          "type": "blob",
          "size": 696
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/gradio_chat.py",
          "type": "blob",
          "size": 2720
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/gradio_image_gen.py",
          "type": "blob",
          "size": 3170
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/gradio_lora_chat.py",
          "type": "blob",
          "size": 5046
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/gradio_zerogpu_chat.py",
          "type": "blob",
          "size": 4103
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/requirements_inference_api.txt",
          "type": "blob",
          "size": 137
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/requirements_lora.txt",
          "type": "blob",
          "size": 143
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/requirements_zerogpu.txt",
          "type": "blob",
          "size": 150
        },
        {
          "path": "skills/hugging-face-space-deployer/templates/streamlit_app.py",
          "type": "blob",
          "size": 3119
        },
        {
          "path": "skills/implement-paper-from-scratch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/implement-paper-from-scratch/SKILL.md",
          "type": "blob",
          "size": 5783
        },
        {
          "path": "skills/ios-app-icon-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ios-app-icon-generator/SKILL.md",
          "type": "blob",
          "size": 4159
        },
        {
          "path": "skills/paper-to-intuition",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/paper-to-intuition/SKILL.md",
          "type": "blob",
          "size": 4168
        },
        {
          "path": "skills/research-question-refiner",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-question-refiner/SKILL.md",
          "type": "blob",
          "size": 5905
        },
        {
          "path": "skills/research-taste-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-taste-developer/SKILL.md",
          "type": "blob",
          "size": 7076
        },
        {
          "path": "skills/reviewer-2-simulator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/reviewer-2-simulator/SKILL.md",
          "type": "blob",
          "size": 6191
        },
        {
          "path": "skills/ted-mosby",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ted-mosby/SKILL.md",
          "type": "blob",
          "size": 9469
        },
        {
          "path": "skills/turn-this-feature-into-a-blog-post",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
          "type": "blob",
          "size": 2836
        },
        {
          "path": "template",
          "type": "tree",
          "size": null
        },
        {
          "path": "template/SKILL.md",
          "type": "blob",
          "size": 509
        }
      ],
      "marketplace": {
        "name": "GhostScientist-skills",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "GhostScientist"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "writing-skills",
            "description": "Skills for technical writing and content creation",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add GhostScientist/skills",
              "/plugin install writing-skills@GhostScientist-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-10T22:28:06Z",
              "created_at": "2025-12-08T15:22:00Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "create-watchos-version",
                "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation.",
                "path": "skills/create-watchos-version/SKILL.md",
                "frontmatter": {
                  "name": "create-watchos-version",
                  "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation."
                },
                "content": "# Create watchOS Version\n\nAnalyzes existing Apple platform projects and creates detailed, phased implementation plans for watchOS apps that are elegant, top-tier experiences‚Äînot naive skins of the parent app.\n\n## Workflow\n\n1. **Project Discovery** - Analyze project structure, patterns, architecture\n2. **Feature Mapping** - Identify watchOS-suitable features and priorities\n3. **API Compatibility** - Search web for current watchOS API availability\n4. **Architecture Planning** - Design watchOS-specific architecture\n5. **Plan Generation** - Create phased plan with warnings and alternatives\n6. **User Review** - Present plan for approval before implementation\n\n## Phase 1: Project Discovery\n\nScan project root for:\n\n```\n‚îú‚îÄ‚îÄ App Architecture (SwiftUI, UIKit, AppKit, hybrid)\n‚îú‚îÄ‚îÄ Data Layer (Core Data, SwiftData, Realm, custom)\n‚îú‚îÄ‚îÄ Networking (URLSession, Alamofire, custom)\n‚îú‚îÄ‚îÄ State Management (ObservableObject, TCA, Redux-like)\n‚îú‚îÄ‚îÄ Navigation (NavigationStack, Coordinator)\n‚îú‚îÄ‚îÄ Shared Frameworks (SPM packages, shared targets)\n‚îú‚îÄ‚îÄ Assets (colors, images, SF Symbols)\n‚îú‚îÄ‚îÄ Existing Watch Target (if any)\n‚îî‚îÄ‚îÄ Minimum iOS Version (affects watchOS targeting)\n```\n\nKey files: `*.xcodeproj`, `Package.swift`, `Info.plist`, App entry points, ViewModels, Models.\n\n## Phase 2: Feature Mapping\n\n**Glanceable (High Priority)**: Status displays, counters, progress, recent items, quick stats\n\n**Quick Actions (High Priority)**: Single-tap toggles, shortcuts, haptic confirmations\n\n**Complications/Widgets (Critical)**: Map data to WidgetKit families‚ÄîaccessoryCircular, accessoryRectangular, accessoryInline, accessoryCorner. Consider Smart Stack relevance.\n\n**Background**: HealthKit integration, background refresh, Watch Connectivity sync\n\n**Defer/Exclude**: Complex data entry, long-form content, sustained screen time features\n\n## Phase 3: API Compatibility\n\n**CRITICAL**: Always search web for current watchOS docs before finalizing. APIs change frequently.\n\nSearch: `[FrameworkName] watchOS availability site:developer.apple.com`\n\n### Quick Reference\n\n**Available**: SwiftUI, SwiftData (10+), WidgetKit (9+), HealthKit, WorkoutKit, CoreLocation (limited), WatchConnectivity, CloudKit, CoreMotion, AVFoundation (audio), CoreBluetooth, Combine, Swift Concurrency\n\n**Unavailable/Limited**: UIKit, WebKit, MapKit (limited), CoreImage (limited), ARKit, RealityKit, StoreKit (limited), Background URLSession (limited)\n\nSee `references/api-compatibility.md` for detailed compatibility matrix.\n\n## Phase 4: Architecture\n\n### Version Targeting\n\n```\niOS 16+ ‚Üí watchOS 9+  (WidgetKit complications)\niOS 17+ ‚Üí watchOS 10+ (SwiftData, Smart Stack)\niOS 18+ ‚Üí watchOS 11+ (Live Activities on Watch)\n```\n\n### Structure\n\n```\nShared/\n‚îú‚îÄ‚îÄ Models/           # Pure Swift, shared via target membership\n‚îú‚îÄ‚îÄ Services/         # Platform-agnostic logic\n‚îî‚îÄ‚îÄ Utilities/\n\nWatchApp/\n‚îú‚îÄ‚îÄ App.swift\n‚îú‚îÄ‚îÄ Views/\n‚îú‚îÄ‚îÄ ViewModels/\n‚îú‚îÄ‚îÄ Complications/\n‚îî‚îÄ‚îÄ WatchConnectivity/\n```\n\n### Design Principles\n\n1. **Glanceability** - Visible within 2 seconds\n2. **Minimal Interaction** - 1-3 taps max\n3. **Context Awareness** - Time, location, activity\n4. **Battery Conscious** - Efficient refresh, TimelineSchedule\n5. **Haptic Feedback** - Confirm actions appropriately\n\n### SwiftUI Gotchas\n\n- Avoid nested TabViews (memory leaks)\n- Use TimelineSchedule for efficient metric updates\n- Check `isLuminanceReduced` to reduce work when dimmed\n- Don't use data-driven high-frequency UI refreshes\n\n## Phase 5: Plan Generation\n\nUse template in `references/plan-template.md` to generate:\n\n1. Executive Summary\n2. ‚ö†Ô∏è API Compatibility Warnings table\n3. Phased implementation tasks\n4. Testing checklist\n\n## Phase 6: User Review\n\nPresent plan and ask for approval before implementing:\n\n> \"I've analyzed your project and created a watchOS plan. Before proceeding:\n> 1. **API Warnings**: [N] APIs unavailable‚Äîalternatives documented.\n> 2. **Recommended Features**: [list] prioritized for Watch.\n> 3. **Scope**: [N] phases.\n> \n> Proceed with implementation, or adjust the plan?\"\n\n**Do not implement until user approves.**\n\n## Best Practices Reference\n\n### Watch Connectivity\n\n```swift\nguard WCSession.default.activationState == .activated else { return }\n// sendMessage: immediate, requires reachability\n// transferUserInfo: queued, guaranteed\n// transferCurrentComplicationUserInfo: complication priority\n```\n\n### Complications\n\n```swift\n// Use appropriate reload policy\nTimeline(entries: entries, policy: .after(nextUpdateDate))\n// Use .never for static complications\n```\n\n### Battery Efficiency\n\n- Timeline-based over active refresh\n- Check `isLuminanceReduced`\n- Batch Watch Connectivity transfers\n- Significant location change vs continuous updates"
              },
              {
                "name": "experiment-design-checklist",
                "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates.",
                "path": "skills/experiment-design-checklist/SKILL.md",
                "frontmatter": {
                  "name": "experiment-design-checklist",
                  "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates."
                },
                "content": "# Experiment Design Checklist\n\nPrevent the \"I ran experiments for 3 months and they're meaningless\" disaster through rigorous upfront design.\n\n## The Core Principle\n\nBefore running ANY experiment, you should be able to answer:\n1. What specific claim will this experiment support or refute?\n2. What would convince a skeptical reviewer?\n3. What could go wrong that would invalidate the results?\n\n## Process\n\n### Step 1: State the Hypothesis Precisely\n\nConvert your research question into falsifiable predictions:\n\n**Template:**\n```\nIf [intervention/method], then [measurable outcome], because [mechanism].\n```\n\n**Examples:**\n- \"If we add auxiliary contrastive loss, then downstream task accuracy increases by >2%, because representations become more separable.\"\n- \"If we use learned positional encodings, then performance on sequences >4096 tokens improves, because the model can extrapolate beyond training length.\"\n\n**Null hypothesis:** What does \"no effect\" look like? This is what you're trying to reject.\n\n### Step 2: Identify Variables\n\n**Independent Variables (what you manipulate):**\n| Variable | Levels | Rationale |\n|----------|--------|-----------|\n| [Var 1] | [Level A, B, C] | [Why these levels] |\n\n**Dependent Variables (what you measure):**\n| Metric | How Measured | Why This Metric |\n|--------|--------------|-----------------|\n| [Metric 1] | [Procedure] | [Justification] |\n\n**Control Variables (what you hold constant):**\n| Variable | Fixed Value | Why Fixed |\n|----------|-------------|-----------|\n| [Var 1] | [Value] | [Prevents confound X] |\n\n### Step 3: Choose Baselines\n\nEvery experiment needs comparisons. No result is meaningful in isolation.\n\n**Baseline Hierarchy:**\n\n1. **Random/Trivial Baseline**\n   - What does random chance achieve?\n   - Sanity check that the task isn't trivial\n\n2. **Simple Baseline**\n   - Simplest reasonable approach\n   - Often embarrassingly effective\n\n3. **Standard Baseline**\n   - Well-known method from literature\n   - Apples-to-apples comparison\n\n4. **State-of-the-Art Baseline**\n   - Current best published result\n   - Only if you're claiming SOTA\n\n5. **Ablated Self**\n   - Your method minus key components\n   - Shows each component contributes\n\n**For each baseline, document:**\n- Source (paper, implementation)\n- Hyperparameters used\n- Whether you re-ran or used reported numbers\n- Any modifications made\n\n### Step 4: Design Ablations\n\nAblations answer: \"Is each component necessary?\"\n\n**Ablation Template:**\n| Variant | What's Removed/Changed | Expected Effect | If No Effect... |\n|---------|----------------------|-----------------|-----------------|\n| Full Model | Nothing | Best performance | - |\n| w/o Component A | Remove A | Performance drops X% | A isn't helping |\n| w/o Component B | Remove B | Performance drops Y% | B isn't helping |\n| Component A only | Only A, no B | Shows A's isolated contribution | - |\n\n**Good ablations are:**\n- Surgical (one change at a time)\n- Interpretable (clear what was changed)\n- Informative (result tells you something)\n\n### Step 5: Address Confounds\n\nThings that could explain your results OTHER than your hypothesis:\n\n**Common Confounds:**\n\n| Confound | How to Check | How to Control |\n|----------|--------------|----------------|\n| Hyperparameter tuning advantage | Same tuning budget for all | Report tuning procedure |\n| Compute advantage | Matched FLOPs/params | Report compute used |\n| Data leakage | Check train/test overlap | Strict separation |\n| Random seed luck | Multiple seeds | Report variance |\n| Implementation bugs (baseline) | Verify baseline numbers | Use official implementations |\n| Cherry-picked examples | Random or systematic selection | Pre-register selection criteria |\n\n### Step 6: Statistical Rigor\n\n**Sample Size:**\n- How many random seeds? (Minimum: 3, better: 5+)\n- How many data splits? (If applicable)\n- Power analysis: Can you detect expected effect size?\n\n**What to Report:**\n- Mean ¬± standard deviation (or standard error)\n- Confidence intervals where appropriate\n- Statistical significance tests if claiming \"better\"\n\n**Appropriate Tests:**\n| Comparison | Test | Assumptions |\n|------------|------|-------------|\n| Two methods, normal data | t-test | Normality, equal variance |\n| Two methods, unknown dist | Mann-Whitney U | Ordinal data |\n| Multiple methods | ANOVA + post-hoc | Normality |\n| Multiple methods, unknown | Kruskal-Wallis | Ordinal data |\n| Paired comparisons | Wilcoxon signed-rank | Same test instances |\n\n**Avoid:**\n- p-hacking (running until significant)\n- Multiple comparison problems (Bonferroni correct)\n- Reporting only favorable metrics\n\n### Step 7: Compute Budget\n\nBefore running, estimate:\n\n| Component | Estimate | Notes |\n|-----------|----------|-------|\n| Single training run | X GPU-hours | [Details] |\n| Hyperparameter search | Y runs √ó X hours | [Search strategy] |\n| Baselines | Z runs √ó W hours | [Which baselines] |\n| Ablations | N variants √ó X hours | [Which ablations] |\n| Seeds | M seeds √ó above | [How many seeds] |\n| **Total** | **T GPU-hours** | Buffer: 1.5-2x |\n\n**Go/No-Go Decision:** Is this feasible with available resources?\n\n### Step 8: Pre-Registration (Optional but Recommended)\n\nWrite down BEFORE running:\n- Exact hypotheses\n- Primary metrics (not chosen post-hoc)\n- Analysis plan\n- What would constitute \"success\"\n\nThis prevents unconscious goal-post moving.\n\n## Output: Experiment Design Document\n\n```markdown\n# Experiment Design: [Title]\n\n## Hypothesis\n[Precise statement]\n\n## Variables\n### Independent\n[Table]\n\n### Dependent\n[Table]\n\n### Controls\n[Table]\n\n## Baselines\n1. [Baseline 1]: [Source, details]\n2. [Baseline 2]: [Source, details]\n\n## Ablations\n[Table]\n\n## Confound Mitigation\n[Table]\n\n## Statistical Plan\n- Seeds: [N]\n- Tests: [Which tests for which comparisons]\n- Significance threshold: [Œ± level]\n\n## Compute Budget\n[Table with total estimate]\n\n## Success Criteria\n- Primary: [What must be true]\n- Secondary: [Nice to have]\n\n## Timeline\n- Phase 1: [What, when]\n- Phase 2: [What, when]\n\n## Known Risks\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n```\n\n## Red Flags in Experiment Design\n\nüö© \"We'll figure out the metrics later\"\nüö© \"One run should be enough\"\nüö© \"We don't need baselines, it's obviously better\"\nüö© \"Let's just see what happens\"\nüö© \"We can always run more if it's not significant\"\nüö© No compute estimate before starting\nüö© Vague success criteria"
              },
              {
                "name": "hugging-face-space-deployer",
                "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons.",
                "path": "skills/hugging-face-space-deployer/SKILL.md",
                "frontmatter": {
                  "name": "hugging-face-space-deployer",
                  "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons."
                },
                "content": "# Hugging Face Space Deployer\n\nA skill for AI engineers to create, configure, and deploy interactive ML demos on Hugging Face Spaces.\n\n## CRITICAL: Pre-Deployment Checklist\n\n**Before writing ANY code, gather this information about the model:**\n\n### 1. Check Model Type (LoRA Adapter vs Full Model)\n\n**Use the HF MCP tool to inspect the model files:**\n```\nhf-skills - Hub Repo Details (repo_ids: [\"username/model\"], repo_type: \"model\")\n```\n\n**Look for these indicators:**\n\n| Files Present | Model Type | Action Required |\n|---------------|------------|-----------------|\n| `model.safetensors` or `pytorch_model.bin` | Full model | Load directly with `AutoModelForCausalLM` |\n| `adapter_model.safetensors` + `adapter_config.json` | LoRA/PEFT adapter | Must load base model first, then apply adapter with `peft` |\n| Only config files, no weights | Broken/incomplete | Ask user to verify |\n\n**If adapter_config.json exists, check for `base_model_name_or_path` to identify the base model.**\n\n### 2. Check Inference API Availability\n\nVisit the model page on HF Hub and look for \"Inference Providers\" widget on the right side.\n\n**Indicators that model HAS Inference API:**\n- Inference widget visible on model page\n- Model from known provider: `meta-llama`, `mistralai`, `HuggingFaceH4`, `google`, `stabilityai`, `Qwen`\n- High download count (>10,000) with standard architecture\n\n**Indicators that model DOES NOT have Inference API:**\n- Personal namespace (e.g., `GhostScientist/my-model`)\n- LoRA/PEFT adapter (adapters never have direct Inference API)\n- Missing `pipeline_tag` in model metadata\n- No inference widget on model page\n\n### 3. Check Model Metadata\n\n- Ensure `pipeline_tag` is set (e.g., `text-generation`)\n- Add `conversational` tag for chat models\n\n### 4. Determine Hardware Needs\n\n| Model Size | Recommended Hardware |\n|------------|---------------------|\n| < 3B parameters | ZeroGPU (free) or CPU |\n| 3B - 7B parameters | ZeroGPU or T4 |\n| > 7B parameters | A10G or A100 |\n\n### 5. Ask User If Unclear\n\n**If you cannot determine the model type, ASK THE USER:**\n\n> \"I'm analyzing your model to determine the best deployment strategy. I found:\n> - [what you found about files]\n> - [what you found about inference API]\n>\n> Is this model:\n> 1. A full model you trained/uploaded?\n> 2. A LoRA/PEFT adapter on top of another model?\n> 3. Something else?\n>\n> Also, would you prefer:\n> A. Free deployment with ZeroGPU (may have queue times)\n> B. Paid GPU for faster response (~$0.60/hr)\"\n\n## Hardware Options\n\n| Hardware | Use Case | Cost |\n|----------|----------|------|\n| `cpu-basic` | Simple demos, Inference API apps | Free |\n| `cpu-upgrade` | Faster CPU inference | ~$0.03/hr |\n| **`zero-a10g`** | **Models needing GPU on-demand (recommended for most)** | **Free (with quota)** |\n| `t4-small` | Small GPU models (<7B) | ~$0.60/hr |\n| `t4-medium` | Medium GPU models | ~$0.90/hr |\n| `a10g-small` | Large models (7B-13B) | ~$1.50/hr |\n| `a10g-large` | Very large models (30B+) | ~$3.15/hr |\n| `a100-large` | Largest models | ~$4.50/hr |\n\n**ZeroGPU Note:** ZeroGPU (`zero-a10g`) provides free GPU access on-demand. The Space runs on CPU, and when a user triggers inference, a GPU is allocated temporarily (~60-120 seconds). **After deployment, you must manually set the runtime to \"ZeroGPU\" in Space Settings > Hardware.**\n\n## Deployment Decision Tree\n\n```\nAnalyze Model\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have adapter_config.json?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a LoRA adapter\n‚îÇ       ‚îú‚îÄ‚îÄ Find base_model_name_or_path in adapter_config.json\n‚îÇ       ‚îî‚îÄ‚îÄ Use Template 3 (LoRA + ZeroGPU)\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have model.safetensors or pytorch_model.bin?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a full model\n‚îÇ       ‚îú‚îÄ‚îÄ Is it from a major provider with inference widget?\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Use Inference API (Template 1)\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Use ZeroGPU (Template 2)\n‚îÇ\n‚îî‚îÄ‚îÄ Neither found?\n    ‚îî‚îÄ‚îÄ ASK USER - model may be incomplete\n```\n\n## Dependencies\n\n**For Inference API (cpu-basic, free):**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**For ZeroGPU full models (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**For ZeroGPU LoRA adapters (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n## CLI Commands (CORRECT Syntax)\n\n```bash\n# Create Space\nhf repo create my-space-name --repo-type space --space-sdk gradio\n\n# Upload files\nhf upload username/space-name ./local-folder --repo-type space\n\n# Download model files to inspect\nhf download username/model-name --local-dir ./model-check --dry-run\n\n# Check what files exist in a model\nhf download username/model-name --local-dir /tmp/check --dry-run 2>&1 | grep -E '\\.(safetensors|bin|json)'\n```\n\n## Template 1: Inference API (For Supported Models)\n\n**Use when:** Model has inference widget, is from major provider, or explicitly supports serverless API.\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nMODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"  # Must support Inference API!\nclient = InferenceClient(MODEL_ID)\n\ndef respond(message, history, system_message, max_tokens, temperature, top_p):\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    response = \"\"\n    for token in client.chat_completion(\n        messages,\n        max_tokens=max_tokens,\n        stream=True,\n        temperature=temperature,\n        top_p=top_p,\n    ):\n        delta = token.choices[0].delta.content or \"\"\n        response += delta\n        yield response\n\ndemo = gr.ChatInterface(\n    respond,\n    title=\"Chat Assistant\",\n    description=\"Powered by Hugging Face Inference API\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\"),\n        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Write a Python function to sort a list\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Chat App\nemoji: üí¨\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n```\n\n## Template 2: ZeroGPU Full Model (For Models Without Inference API)\n\n**Use when:** Full model (has model.safetensors) but no Inference API support.\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"username/my-full-model\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Global model - loaded lazily on first GPU call for faster Space startup\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Model\",\n    description=\"Powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me write some code\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Model\nemoji: ü§ñ\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Template 3: ZeroGPU LoRA Adapter (CRITICAL FOR FINE-TUNED MODELS)\n\n**Use when:** Model has `adapter_config.json` and `adapter_model.safetensors` (NOT `model.safetensors`)\n\n**You MUST identify the base model from `adapter_config.json` field `base_model_name_or_path`**\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Your LoRA adapter\nADAPTER_ID = \"username/my-lora-adapter\"\n# Base model (from adapter_config.json -> base_model_name_or_path)\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Global model - loaded lazily on first GPU call\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            BASE_MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n        model = model.merge_and_unload()  # Merge for faster inference\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for item in history:\n        if isinstance(item, (list, tuple)) and len(item) == 2:\n            user_msg, assistant_msg = item\n            if user_msg:\n                messages.append({\"role\": \"user\", \"content\": user_msg})\n            if assistant_msg:\n                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Fine-Tuned Model\",\n    description=\"LoRA fine-tuned model powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me with a coding task\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt (MUST include peft):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Fine-Tuned Model\nemoji: üîß\ncolorFrom: green\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Post-Deployment Steps\n\n**After uploading your Space files:**\n\n### 1. Set the Runtime Hardware (REQUIRED for GPU models)\n\n- Go to: `https://huggingface.co/spaces/USERNAME/SPACE_NAME/settings`\n- Under \"Space Hardware\", select the appropriate option:\n  - **ZeroGPU** for free on-demand GPU (recommended)\n  - Or a dedicated GPU tier if needed\n\n### 2. Verify the Space is Running\n\n- Check the Space URL for any build errors\n- Review container logs in Settings if issues occur\n\n### 3. Common Post-Deploy Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| \"No API found\" error | Hardware mismatch | Set runtime to ZeroGPU in Settings |\n| Model not loading | LoRA vs full model confusion | Check if it's an adapter, use correct template |\n| Inference API errors | Model not on serverless | Load directly with transformers instead |\n\n## Detecting Model Type - Quick Reference\n\n### Full Model\nFiles include: `model.safetensors`, `pytorch_model.bin`, or sharded versions\n```python\n# Can load directly\nmodel = AutoModelForCausalLM.from_pretrained(\"username/model\")\n```\n\n### LoRA/PEFT Adapter\nFiles include: `adapter_config.json`, `adapter_model.safetensors`\n```python\n# Must load base model first, then apply adapter\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model-id\")\nmodel = PeftModel.from_pretrained(base_model, \"username/adapter\")\nmodel = model.merge_and_unload()  # Optional: merge for faster inference\n```\n\n### Inference API Available\nModel page shows \"Inference Providers\" widget on the right side\n```python\n# Can use InferenceClient (simplest approach)\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\"username/model\")\n```\n\n## Fixing Missing pipeline_tag (To Enable Inference API)\n\nIf a model doesn't have an inference widget but should, it may be missing metadata:\n\n```bash\n# Download the README\nhf download username/model-name README.md --local-dir /tmp/fix\n\n# Edit to add pipeline_tag in YAML frontmatter:\n# ---\n# pipeline_tag: text-generation\n# tags:\n# - conversational\n# ---\n\n# Upload the fix\nhf upload username/model-name /tmp/fix/README.md README.md\n```\n\n**Note:** Even with correct tags, custom models may not get Inference API - it depends on HF's infrastructure decisions.\n\n## CRITICAL: Gradio 5.x Requirements\n\n### Examples Format (MUST be nested lists)\n```python\n# CORRECT:\nexamples=[\n    [\"Example 1\"],\n    [\"Example 2\"],\n]\n\n# WRONG (causes ValueError):\nexamples=[\n    \"Example 1\",\n    \"Example 2\",\n]\n```\n\n### Version Requirements\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\nDo NOT use `gradio==4.44.0` - causes `ImportError: cannot import name 'HfFolder'`\n\n## Troubleshooting\n\n### \"No API found\" Error\n**Cause:** Gradio app isn't exposing API correctly, often due to hardware mismatch\n**Fix:** Go to Space Settings and set runtime to \"ZeroGPU\" or appropriate GPU tier\n\n### \"OSError: does not appear to have a file named pytorch_model.bin, model.safetensors\"\n**Cause:** Trying to load a LoRA adapter as a full model\n**Fix:** Check for `adapter_config.json` - if present, use PEFT to load:\n```python\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\nmodel = PeftModel.from_pretrained(base_model, \"adapter-id\")\n```\n\n### Inference API Not Available\n**Cause:** Model doesn't have pipeline_tag or isn't deployed to serverless\n**Fix:** Either:\n  a. Add `pipeline_tag: text-generation` to model's README.md\n  b. Or load model directly with transformers instead of InferenceClient\n\n### `ImportError: cannot import name 'HfFolder'`\n**Cause:** gradio/huggingface_hub version mismatch\n**Fix:** Use `gradio>=5.0.0` and `huggingface_hub>=0.26.0`\n\n### `ValueError: examples must be nested list`\n**Cause:** Gradio 5.x format change\n**Fix:** Use `[[\"ex1\"], [\"ex2\"]]` not `[\"ex1\", \"ex2\"]`\n\n### Space builds but model doesn't load\n**Cause:** Missing `peft` for adapters, or wrong base model\n**Fix:** Check adapter_config.json for correct base_model_name_or_path\n\n## Workflow Summary\n\n1. **Analyze model** (check for adapter_config.json, model files, inference widget)\n2. **Determine strategy** (Inference API vs ZeroGPU, full model vs LoRA)\n3. **Ask user if unclear** about model type or cost preferences\n4. **Generate correct template** based on analysis\n5. **Create Space** with correct requirements and README\n6. **Upload files** using `hf upload`\n7. **Set hardware** in Space Settings (ZeroGPU for free GPU access)\n8. **Monitor build logs** for any issues"
              },
              {
                "name": "implement-paper-from-scratch",
                "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions.",
                "path": "skills/implement-paper-from-scratch/SKILL.md",
                "frontmatter": {
                  "name": "implement-paper-from-scratch",
                  "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions."
                },
                "content": "# Implement Paper From Scratch\n\nThe best way to truly understand a paper is to implement it. This skill guides you through that process methodically.\n\n## Philosophy\n\n- **No copy-pasting from reference implementations** - We build understanding, not just working code\n- **Checkpoint questions verify understanding** - You should be able to answer \"why\" at each step\n- **Minimal dependencies** - Use NumPy/PyTorch fundamentals, not high-level wrappers\n- **Deliberate debugging** - Bugs are learning opportunities, not obstacles\n\n## Process\n\n### Phase 1: Pre-Implementation Analysis\n\nBefore writing any code:\n\n1. **Identify the core algorithm** - Strip away ablations, extensions, bells and whistles. What's the minimal version?\n\n2. **List the components** - Break into modules:\n   - Data pipeline\n   - Model architecture\n   - Loss function(s)\n   - Training loop\n   - Evaluation metrics\n\n3. **Find the tricky parts** - What's non-obvious?\n   - Custom layers or operations\n   - Numerical stability concerns\n   - Hyperparameter sensitivity\n   - Implementation details buried in appendices\n\n4. **Gather reference numbers** - What should we expect?\n   - Training loss trajectory\n   - Validation metrics at convergence\n   - Compute requirements (if stated)\n\n### Phase 2: Scaffolded Implementation\n\nBuild up the implementation in this order:\n\n#### Step 1: Data\n```python\n# Start with synthetic/toy data\n# Verify shapes and types before touching real data\n```\n\n**Checkpoint:** Can you describe what each tensor represents and its expected shape?\n\n#### Step 2: Model Architecture\n```python\n# Build layer by layer\n# Print shapes at each stage\n# Verify parameter counts match paper\n```\n\n**Checkpoint:** If you randomly initialize and do a forward pass, do the output shapes match what the paper describes?\n\n#### Step 3: Loss Function\n```python\n# Implement exactly as described\n# Test with known inputs/outputs\n# Check gradient flow\n```\n\n**Checkpoint:** Can you explain each term in the loss and why it's there?\n\n#### Step 4: Training Loop\n```python\n# Minimal loop first (no logging, checkpointing, etc.)\n# Verify loss decreases on tiny overfit test\n# Then add bells and whistles\n```\n\n**Checkpoint:** Can you overfit a single batch? If not, something is broken.\n\n#### Step 5: Evaluation\n```python\n# Implement paper's exact metrics\n# Compare against reported numbers\n```\n\n**Checkpoint:** On the same data split, how close are you to paper's numbers?\n\n### Phase 3: The Debugging Gauntlet\n\nWhen it doesn't work (and it won't at first):\n\n1. **The Overfit Test**\n   - Can you memorize 1 example? 10? 100?\n   - If not, architecture or gradient bug\n\n2. **The Gradient Check**\n   - Are gradients flowing to all parameters?\n   - Any NaN or exploding gradients?\n\n3. **The Initialization Check**\n   - Match paper's initialization exactly\n   - This matters more than people think\n\n4. **The Learning Rate Sweep**\n   - Log scale: 1e-5 to 1e-1\n   - Loss should decrease for some range\n\n5. **The Ablation Debug**\n   - Remove components until it works\n   - Add back one at a time\n\n### Phase 4: Checkpoint Questions\n\nAt each stage, you should be able to answer:\n\n**Understanding:**\n- Why does this component exist?\n- What would happen without it?\n- What alternatives were considered?\n\n**Implementation:**\n- Why this specific implementation choice?\n- Where could numerical issues arise?\n- What's the computational complexity?\n\n**Debugging:**\n- What would it look like if this was broken?\n- How would you test this in isolation?\n- What are the most likely bugs?\n\n## Output Format\n\nFor each implementation session, provide:\n\n```markdown\n## Today's Implementation Goal\n[Specific component we're building]\n\n## Prerequisites Check\n- [ ] Previous components working\n- [ ] Understand what we're building\n- [ ] Know expected behavior\n\n## Implementation\n\n### Code\n[Code blocks with extensive comments]\n\n### Checkpoint Questions\n1. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n2. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n### Verification Steps\n- [ ] Test 1: [What to check]\n- [ ] Test 2: [What to check]\n\n### Common Bugs at This Stage\n1. [Bug pattern]: [How to identify and fix]\n\n## What's Next\n[Preview of next component and how it connects]\n```\n\n## Tips for Specific Paper Types\n\n### Transformer-based\n- Attention mask shapes are the #1 bug source\n- Verify positional encoding is applied correctly\n- Check layer norm placement (pre vs post)\n\n### RL/Policy Gradient\n- Sign errors in policy gradient are silent killers\n- Advantage normalization matters\n- Verify discount factor handling\n\n### Generative Models\n- KL term balancing is finicky\n- Check latent space distribution\n- Verify reconstruction looks reasonable before training\n\n### Computer Vision\n- Normalization (ImageNet stats, batch norm) is crucial\n- Data augmentation can make or break results\n- Verify input preprocessing matches paper exactly\n\n## Success Criteria\n\nYou're done when:\n\n1. **Numbers match** - Within reasonable variance of paper's results\n2. **Understanding is deep** - You can explain every line of code\n3. **You found the gotchas** - You know what breaks and why\n4. **You could modify it** - Confident to try your own variations\n\n## Anti-Patterns to Avoid\n\n- ‚ùå Copying code you don't understand\n- ‚ùå Skipping checkpoint questions\n- ‚ùå Using pre-built components for core algorithm\n- ‚ùå Ignoring discrepancies with paper\n- ‚ùå Moving on before current step works"
              },
              {
                "name": "ios-app-icon-generator",
                "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons.",
                "path": "skills/ios-app-icon-generator/SKILL.md",
                "frontmatter": {
                  "name": "ios-app-icon-generator",
                  "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons."
                },
                "content": "# iOS App Icon Generator\n\nCreate beautiful, production-ready iOS app icons through a two-phase creative process.\n\n## Phase 1: Visual Philosophy\n\nBefore drawing anything, develop a 2-3 paragraph **Icon Philosophy** that articulates:\n\n- **Core concept**: What single idea or feeling should the icon convey?\n- **Visual metaphor**: What shape, object, or abstraction represents the app's purpose?\n- **Color psychology**: What palette evokes the right emotional response?\n- **Silhouette test**: Will it be recognizable as a tiny black shape?\n\nWrite this philosophy out. It guides every design decision.\n\n### Design Principles\n\nIcons that work follow these rules:\n\n- **Simplicity**: One focal element. No more than 2-3 colors. No text (illegible at small sizes).\n- **Distinctiveness**: Must stand out in a grid of 30 other icons. Avoid generic symbols (gears, checkmarks, clouds).\n- **Scalability**: The 16x16 notification icon must read as clearly as the 1024x1024 App Store version.\n- **No photography**: Apple's guidelines discourage photos. Use illustration, geometry, or abstract forms.\n- **Optical balance**: Center of visual weight, not geometric center. Curves feel heavier than straight lines.\n\n## Phase 2: Icon Generation\n\nGenerate the icon as a **self-contained HTML file** with embedded SVG that:\n\n1. Renders the icon design at 1024x1024 (the master size)\n2. Includes iOS-style rounded corners (superellipse, not CSS border-radius)\n3. Shows a preview grid of all sizes to verify readability\n4. Provides a download mechanism for each size\n\n### Required Sizes\n\nGenerate all iOS app icon sizes:\n\n| Size | Purpose |\n|------|---------|\n| 1024x1024 | App Store |\n| 180x180 | iPhone (@3x) |\n| 167x167 | iPad Pro (@2x) |\n| 152x152 | iPad (@2x) |\n| 120x120 | iPhone (@2x) |\n| 87x87 | Spotlight (@3x) |\n| 80x80 | Spotlight (@2x) |\n| 76x76 | iPad (@1x) |\n| 60x60 | iPhone (@1x) |\n| 58x58 | Settings (@2x) |\n| 40x40 | Spotlight (@1x) |\n| 29x29 | Settings (@1x) |\n| 20x20 | Notification (@1x) |\n\n### HTML Artifact Structure\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>App Icon: [Name]</title>\n  <style>\n    /* Dark interface, icon grid layout, download buttons */\n  </style>\n</head>\n<body>\n  <!-- Philosophy statement -->\n  <!-- Master SVG at 1024x1024 -->\n  <!-- Preview grid showing all sizes -->\n  <!-- Download buttons (use canvas to convert SVG ‚Üí PNG) -->\n  <script>\n    // SVG ‚Üí Canvas ‚Üí PNG download logic\n  </script>\n</body>\n</html>\n```\n\n### SVG Guidelines\n\n- Use `viewBox=\"0 0 1024 1024\"` for the master\n- Apply the iOS squircle mask (superellipse with n‚âà5)\n- Use gradients sparingly but effectively\n- Ensure googd stroke widths scale proportionally\n- Test: zoom browser to 25% - is the icon still clear?\n\n### iOS Squircle Mask\n\nThe iOS icon shape is NOT a rounded rectangle. Use this superellipse path or approximate with:\n\n```svg\n<clipPath id=\"ios-squircle\">\n  <path d=\"M512,1024 C252,1024 0,772 0,512 C0,252 252,0 512,0 C772,0 1024,252 1024,512 C1024,772 772,1024 512,1024 Z\" />\n</clipPath>\n```\n\nOr generate programmatically with the superellipse formula: `|x/a|^n + |y/b|^n = 1` where n ‚âà 5.\n\n## Process\n\n1. Ask about the app's purpose, name, and any existing brand colors\n2. Write the Icon Philosophy\n3. Describe 2-3 concept directions with rationale\n4. Get user approval on a direction\n5. Generate the HTML artifact with full icon set\n6. Iterate based on feedback\n\n## Quality Bar\n\nThe output should look like it belongs on a top-10 App Store chart. Every icon in that grid was crafted by a professional designer - yours should be indistinguishable from theirs.\n\nAvoid:\n- Glossy/skeuomorphic styles (outdated since iOS 7)\n- Thin hairline details (disappear at small sizes)\n- Overly complex illustrations\n- Generic clip-art aesthetics\n- Centered-circle-on-gradient laziness"
              },
              {
                "name": "paper-to-intuition",
                "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams.",
                "path": "skills/paper-to-intuition/SKILL.md",
                "frontmatter": {
                  "name": "paper-to-intuition",
                  "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams."
                },
                "content": "# Paper to Intuition\n\nTransform dense academic papers into genuine understanding through layered explanation and visual intuition.\n\n## Process\n\n1. **Get the paper** - Ask for the arXiv link, PDF, or paper title\n2. **Extract the core** - Identify the single key insight (one sentence)\n3. **Build the ladder** - Create explanations at 4 levels\n4. **Visualize intuition** - Generate interactive diagrams\n5. **Stress test understanding** - \"What breaks if we remove X?\"\n\n## The Explanation Ladder\n\nGenerate explanations at each level, with each building on the last:\n\n### Level 1: ELI5 (1 paragraph)\n- No jargon, no equations\n- Use familiar analogies from everyday life\n- A curious 10-year-old should roughly get it\n\n### Level 2: Undergraduate (2-3 paragraphs)\n- Assume calculus, basic linear algebra, intro ML\n- Introduce key terms with definitions\n- Connect to textbook concepts they'd know\n\n### Level 3: Graduate (3-4 paragraphs)\n- Assume ML fundamentals, optimization, probability\n- Discuss relationship to prior work\n- Explain why naive approaches don't work\n- Cover the key equations with plain-English annotations\n\n### Level 4: Researcher (2-3 paragraphs)\n- Assume field expertise\n- Subtle technical contributions\n- Limitations and open questions\n- How this changes what's possible\n\n## Key Equations Breakdown\n\nFor each important equation:\n\n```\n[Equation in LaTeX]\n\nIn words: [Plain English translation]\n\nEach term:\n- [symbol]: [what it represents] [why it's there]\n\nIntuition: [Why this mathematical form? What would change if we used a different form?]\n```\n\n## Visual Intuition Artifact\n\nGenerate a self-contained HTML file with:\n\n- **Architecture diagram** - Boxes and arrows showing information flow\n- **Interactive sliders** - Manipulate key parameters, see effects\n- **Before/after comparisons** - What the method improves over baselines\n- **Failure case visualization** - When and why it breaks down\n\nUse SVG for diagrams, vanilla JavaScript for interactivity. Dark theme, clean typography.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>[Paper Name] - Visual Intuition</title>\n  <style>\n    :root { --bg: #1a1a2e; --text: #eee; --accent: #4f8cff; }\n    /* Clean, research-aesthetic styling */\n  </style>\n</head>\n<body>\n  <h1>[Paper Title]</h1>\n  <p class=\"tldr\">[One-sentence insight]</p>\n\n  <section id=\"architecture\">\n    <svg><!-- Information flow diagram --></svg>\n  </section>\n\n  <section id=\"interactive\">\n    <!-- Parameter sliders with live updates -->\n  </section>\n\n  <section id=\"comparisons\">\n    <!-- Before/after, ablations -->\n  </section>\n</body>\n</html>\n```\n\n## The \"What Breaks?\" Analysis\n\nFor each major component, explain:\n\n1. **What it does** - The role this component plays\n2. **What breaks without it** - Concrete failure mode\n3. **Why this solution** - Alternatives considered, why this won\n4. **The tradeoff** - What we pay for this choice (compute, complexity, assumptions)\n\n## Output Structure\n\nDeliver as a structured document:\n\n```markdown\n# [Paper Title]\n\n**TL;DR:** [One sentence]\n\n**Why it matters:** [One paragraph on significance]\n\n## The Explanation Ladder\n\n### ELI5\n[...]\n\n### Undergraduate Level\n[...]\n\n### Graduate Level\n[...]\n\n### Researcher Level\n[...]\n\n## Key Equations\n\n### Equation 1: [Name]\n[Breakdown as specified above]\n\n## What Breaks If We Remove...\n\n### [Component 1]\n[Analysis]\n\n### [Component 2]\n[Analysis]\n\n## Visual Intuition\n\n[Link to or embed HTML artifact]\n\n## Further Reading\n\n- [Prerequisite paper 1]\n- [Follow-up work 1]\n```\n\n## Quality Standards\n\n- Every analogy must be accurate, not just catchy\n- Equations must be explained, not just translated\n- Visuals must reveal structure, not just decorate\n- The researcher-level section should contain insight, not just summary\n- Admit when something is genuinely confusing or poorly explained in the original paper"
              },
              {
                "name": "research-question-refiner",
                "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis.",
                "path": "skills/research-question-refiner/SKILL.md",
                "frontmatter": {
                  "name": "research-question-refiner",
                  "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis."
                },
                "content": "# Research Question Refiner\n\nTransform \"I'm interested in X\" into \"I will investigate whether Y under conditions Z, measuring W.\"\n\n## The Problem\n\nMost research ideas fail not because they're bad, but because they're:\n- Too vague to act on\n- Too ambitious to complete\n- Too incremental to matter\n- Missing a clear success criterion\n\nThis skill fixes that.\n\n## Process\n\n### Stage 1: Excavate the Interest\n\nStart by understanding what's actually pulling at you:\n\n**Questions to ask:**\n1. What sparked this interest? (Paper, conversation, problem you encountered?)\n2. What's the version that excites you most?\n3. What would be cool if it worked?\n4. Who would care about the answer?\n\n**Output:** A paragraph capturing the raw interest, unfiltered.\n\n### Stage 2: Map the Territory\n\nBefore scoping, understand the landscape:\n\n**What's Known:**\n- What's the current state-of-the-art?\n- What are the established approaches?\n- What have people tried that didn't work?\n\n**What's Unknown:**\n- What are the acknowledged open problems?\n- What assumptions does current work make?\n- Where do methods fail?\n\n**What's Controversial:**\n- Where do researchers disagree?\n- What's claimed but not convincingly shown?\n- What's believed but not rigorously tested?\n\n**Output:** A structured map with citations/references for each area.\n\n### Stage 3: Find the Gap\n\nA good research question lives in a gap that is:\n\n| Property | Too Little | Just Right | Too Much |\n|----------|-----------|------------|----------|\n| **Novelty** | Redoing existing work | New angle or combination | No foundation to build on |\n| **Difficulty** | Trivial to answer | Challenging but doable | Requires breakthroughs |\n| **Impact** | No one cares | Community would update beliefs | Nobel prize (unrealistic) |\n| **Scope** | One experiment | Thesis chapter / paper | Multiple PhDs |\n\n**Gap-finding questions:**\n- What would change if we relaxed assumption X?\n- What if we applied method A to domain B?\n- What's between approach X and approach Y?\n- What fails in setting Z that works elsewhere?\n\n**Output:** 3-5 candidate gaps, each as one sentence.\n\n### Stage 4: Refine to Concrete Question\n\nFor each candidate gap, sharpen into a question:\n\n**The Formula:**\n```\n[Action verb] + [specific phenomenon] + [under conditions] + [measurable outcome]\n```\n\n**Examples of refinement:**\n\n‚ùå Vague: \"How can we make transformers more efficient?\"\n‚úÖ Concrete: \"Does structured sparsity in attention patterns preserve performance on long-context tasks while reducing compute by >50%?\"\n\n‚ùå Vague: \"Can robots learn from humans better?\"\n‚úÖ Concrete: \"Does incorporating gaze direction in demonstrations improve sample efficiency for manipulation tasks compared to kinesthetic teaching alone?\"\n\n‚ùå Vague: \"What makes language models hallucinate?\"\n‚úÖ Concrete: \"Do retrieval-augmented models hallucinate less on factual questions when retrieval confidence is used to modulate generation temperature?\"\n\n### Stage 5: Feasibility Check\n\nFor each refined question, assess:\n\n**Resources Required:**\n- Compute: GPU-hours estimate\n- Data: Available or needs collection?\n- Time: Weeks/months realistically\n- Expertise: What skills are needed?\n\n**Risk Assessment:**\n- What's the probability this works at all?\n- What if the hypothesis is wrong? (Is negative result publishable?)\n- What could go wrong technically?\n- What could invalidate the whole direction?\n\n**Dependencies:**\n- Does this require other work to finish first?\n- Are there rate-limiting steps?\n- What can be parallelized?\n\n### Stage 6: The Litmus Tests\n\nA good research question passes all of these:\n\n**The Advisor Test:**\n> \"If I pitched this in 2 minutes, would a busy professor say 'yes, go do that' rather than 'hmm, let's talk more'?\"\n\n**The Paper Test:**\n> \"Can I envision the title, abstract, and figure 1 of the resulting paper?\"\n\n**The Null Result Test:**\n> \"If my hypothesis is wrong, would that still be interesting to report?\"\n\n**The Motivation Test:**\n> \"Am I actually excited to work on this for 6+ months?\"\n\n**The Explanation Test:**\n> \"Can I explain why this matters to a smart non-expert in 60 seconds?\"\n\n## Output Format\n\nDeliver a Research Question Brief:\n\n```markdown\n# Research Question Brief\n\n## The Interest (Raw)\n[Original unfiltered interest]\n\n## Territory Map\n\n### What's Known\n- [Point 1] ([citation])\n- [Point 2] ([citation])\n\n### What's Unknown\n- [Open question 1]\n- [Open question 2]\n\n### What's Controversial\n- [Debate 1]\n\n## Candidate Gaps\n1. [Gap 1]\n2. [Gap 2]\n3. [Gap 3]\n\n## Refined Questions\n\n### Question 1: [Title]\n**Statement:** [Precise question]\n**Hypothesis:** [What you expect to find]\n**Feasibility:** [Brief assessment]\n**If it works:** [Impact]\n**If it doesn't:** [What we still learn]\n\n### Question 2: [Title]\n[Same structure]\n\n## Recommendation\n[Which question to pursue and why]\n\n## Immediate Next Steps\n1. [Concrete action 1]\n2. [Concrete action 2]\n3. [Concrete action 3]\n```\n\n## Common Failure Modes\n\n**The Kitchen Sink:** Trying to answer too many questions at once\n‚Üí Fix: Ruthlessly cut until there's ONE core question\n\n**The Solution in Search of a Problem:** Starting with a method, not a question\n‚Üí Fix: Ask \"Who has this problem? Why hasn't it been solved?\"\n\n**The Incremental Trap:** Small delta on existing work\n‚Üí Fix: Ask \"Would this change how people think?\"\n\n**The Impossible Dream:** Beautiful question, can't be answered\n‚Üí Fix: Ask \"What's the minimal version that's still interesting?\"\n\n**The Boring Sure Thing:** Will definitely work, nobody cares\n‚Üí Fix: Add ambition until there's meaningful risk"
              },
              {
                "name": "research-taste-developer",
                "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently.",
                "path": "skills/research-taste-developer/SKILL.md",
                "frontmatter": {
                  "name": "research-taste-developer",
                  "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently."
                },
                "content": "# Research Taste Developer\n\nResearch taste is the ability to distinguish work that matters from work that doesn't - before the community tells you. This skill helps you develop that instinct.\n\n## What is Research Taste?\n\nIt's the intuition that lets experienced researchers:\n- Pick problems that turn out to be important\n- Know when an idea is \"close\" vs. \"far\" from working\n- Recognize a good result even with imperfect execution\n- Predict which papers will be remembered in 5 years\n\nTaste isn't magic - it's pattern recognition from deep exposure. This skill accelerates that exposure.\n\n## Process\n\n### Phase 1: Analyze the Field\n\nPick a specific subfield. We'll study what \"good\" looks like there.\n\n**Questions to investigate:**\n1. What are the 10 most-cited papers of the last 5 years?\n2. What are the 5 papers experts say \"changed how we think\"?\n3. What are the best papers from top venues (NeurIPS, ICML, CVPR, etc.)?\n4. What got awards? What got invited talks?\n\n**For each landmark paper, analyze:**\n- What was the state before this paper?\n- What's the single core insight?\n- What specifically made people cite it?\n- Was it obvious in hindsight?\n\n### Phase 2: Pattern Recognition\n\nLook for what the great papers have in common:\n\n**The Patterns of Impact:**\n\n#### 1. The New Primitive\nPapers that introduce a building block others build on.\n- Examples: Attention mechanism, ResNet skip connections, Dropout\n- Pattern: Simple idea, surprisingly general applicability\n- Why it works: Reduces friction for future work\n\n#### 2. The Surprising Connection\nPapers that link two previously separate areas.\n- Examples: VAE (variational inference + neural nets), NeRF (neural nets + ray marching)\n- Pattern: \"X, but for Y\" where the combination is non-obvious\n- Why it works: Cross-pollinates communities\n\n#### 3. The Scaling Insight\nPapers showing that scale changes qualitative behavior.\n- Examples: GPT-3, Chinchilla\n- Pattern: What everyone \"knew\" was wrong at sufficient scale\n- Why it works: Forces field to update beliefs\n\n#### 4. The Rigorous Foundation\nPapers that formalize what was previously folklore.\n- Examples: Theoretical convergence proofs, generalization bounds\n- Pattern: Makes hand-wavy intuitions precise\n- Why it works: Enables confident building\n\n#### 5. The Elegant Solution\nPapers that solve a problem far more simply than expected.\n- Examples: Simple baseline papers, \"X is all you need\"\n- Pattern: Previous solutions were overcomplicated\n- Why it works: Shifts field's complexity assumptions\n\n### Phase 3: Anti-Patterns\n\nLearn to recognize work that won't age well:\n\n**The Incremental Treadmill:**\n- Pattern: +0.5% on benchmark with architectural tweak\n- Why it fails: No one remembers or uses it\n- Exception: When it reveals something fundamental\n\n**The Method Mashing:**\n- Pattern: \"We combine A, B, C, and D\"\n- Why it fails: No insight about why the combination works\n- Exception: When combination reveals unexpected interaction\n\n**The Benchmark Overfitter:**\n- Pattern: Method that works only on specific benchmarks\n- Why it fails: Doesn't transfer, forgotten when benchmarks change\n- Exception: When it exposes benchmark weaknesses\n\n**The Complexity Monster:**\n- Pattern: Works but requires 47 hyperparameters and 3 loss terms\n- Why it fails: No one can reproduce or build on it\n- Exception: Rarely\n\n**The Solution Without a Problem:**\n- Pattern: Novel method without compelling use case\n- Why it fails: \"Interesting but why?\"\n- Exception: When use case emerges later (rare)\n\n### Phase 4: Develop Your Own Taste\n\n**Exercise 1: Prediction Game**\nBefore reading a paper, predict based on title/abstract:\n- Will this paper be cited >100 times in 5 years?\n- Write down your prediction and reasoning\n- Track your accuracy over time\n- Analyze where your predictions went wrong\n\n**Exercise 2: Explain the Gap**\nFor any two papers in citation count:\n- Paper A: 2000 citations\n- Paper B: 50 citations (same venue, same year)\n- What explains the difference?\n- Write a paragraph explanation\n\n**Exercise 3: The Time Machine**\nPick a highly-cited paper. Go back to when it was published:\n- What was the state of the field?\n- Would you have recognized its importance?\n- What signals would you have looked for?\n\n**Exercise 4: Design a Hit**\nGiven current state of a field:\n- What's the most important open problem?\n- What would a \"great paper\" on this look like?\n- What would make people cite it?\n\n### Phase 5: Meta-Principles\n\nWhat top researchers seem to do differently:\n\n**Problem Selection:**\n- Work on problems that are \"ready\" (pieces exist, no one assembled them)\n- Avoid problems that are stuck for fundamental reasons\n- Pick problems where you have unfair advantages\n\n**Execution Taste:**\n- Know when to stop polishing (diminishing returns)\n- Know when result is \"strong enough\" to share\n- Prefer simple-that-works over complex-that-works-slightly-better\n\n**Communication Taste:**\n- Lead with the insight, not the method\n- Make contribution obvious in first 2 minutes\n- Anticipate and address likely objections\n\n**Portfolio Taste:**\n- Mix safe and risky projects\n- Build a coherent research identity\n- Create compound interest (each paper enables the next)\n\n## Output: Taste Development Report\n\n```markdown\n# Research Taste Analysis: [Field/Subfield]\n\n## Landmark Paper Analysis\n\n### [Paper 1 Title] ([Year])\n- **Pre-existing state:** [What was true before]\n- **Core insight:** [One sentence]\n- **Why it's cited:** [Specific reason]\n- **Pattern type:** [New Primitive / Connection / etc.]\n\n### [Paper 2 Title]\n[Same structure]\n\n## Pattern Distribution\nIn this subfield, highly-cited papers tend to be:\n- [X]% New Primitives\n- [Y]% Surprising Connections\n- [Z]% Other\n\n## Anti-Pattern Warnings\nThe following patterns are common but don't lead to impact:\n1. [Anti-pattern common in this field]\n2. [Another one]\n\n## Taste Heuristics for [Field]\nWhen evaluating a paper in this field, ask:\n1. [Field-specific question that distinguishes good from meh]\n2. [Another one]\n3. [Another one]\n\n## Current Opportunities\nBased on this analysis, promising directions seem to be:\n1. [Direction 1]: [Why it's ripe]\n2. [Direction 2]: [Why it's ripe]\n\n## Your Taste Development Exercises\n1. [Specific exercise for this field]\n2. [Another one]\n```\n\n## The Ultimate Test\n\nYou have good taste when:\n- You're bored by work others find impressive (correctly predicting it won't matter)\n- You're excited by work others overlook (correctly predicting it will matter)\n- Your intuitions about importance are calibrated with reality\n- You can articulate *why* something is good, not just that it is\n\nThis takes years. But deliberate practice - not just reading, but *analyzing* - accelerates it dramatically."
              },
              {
                "name": "reviewer-2-simulator",
                "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims.",
                "path": "skills/reviewer-2-simulator/SKILL.md",
                "frontmatter": {
                  "name": "reviewer-2-simulator",
                  "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims."
                },
                "content": "# Reviewer 2 Simulator\n\nChannel the energy of the harshest (but fair) reviewer to find weaknesses before your actual reviewers do.\n\n## The Mindset\n\nReviewer 2 is:\n- Skeptical but not hostile\n- Technically rigorous\n- Short on time (will skim, not read carefully)\n- Looking for reasons to reject (high-volume venues)\n- But wants to champion good work\n\nReviewer 2 is NOT:\n- Trying to be mean\n- Unfamiliar with the field (usually)\n- Unable to be convinced by good arguments\n\n## Process\n\n### Phase 1: First Pass (5-minute skim)\n\nRead like a busy reviewer would:\n- Title and abstract\n- Figures and captions\n- Section headers\n- Conclusion\n\n**First-pass questions:**\n1. Can I understand the contribution from abstract alone?\n2. Do the figures tell the story?\n3. Is this obviously incremental or obviously interesting?\n4. Any immediate red flags?\n\n### Phase 2: Deep Read Critique\n\nGo section by section:\n\n#### Abstract\n- [ ] Clear problem statement?\n- [ ] Specific contribution (not vague \"we propose...\")?\n- [ ] Key result with number?\n- [ ] Any overclaims?\n\n**Common issues:**\n- \"We achieve state-of-the-art\" without specifying where/what\n- \"Novel\" without explaining what's actually new\n- Claims not supported in the paper\n\n#### Introduction\n- [ ] Motivation compelling?\n- [ ] Gap in prior work clearly identified?\n- [ ] Contribution stated precisely?\n- [ ] Paper organization clear?\n\n**Common issues:**\n- Straw-man characterization of prior work\n- Gap is manufactured, not real\n- Contribution buried in paragraph 4\n\n#### Related Work\n- [ ] Comprehensive coverage?\n- [ ] Fair characterization of prior work?\n- [ ] Clear differentiation from closest work?\n- [ ] Missing obvious citations?\n\n**Common issues:**\n- Missing direct competitors\n- Misrepresenting prior work to look better\n- No clear statement of difference from closest work\n\n#### Method\n- [ ] Technically sound?\n- [ ] Reproducible from description?\n- [ ] Assumptions stated explicitly?\n- [ ] Notation consistent?\n\n**Common issues:**\n- Hand-wavy justification\n- Critical details in appendix (or missing entirely)\n- Unstated assumptions\n- Notation changes mid-paper\n\n#### Experiments\n- [ ] Baselines appropriate and strong?\n- [ ] Metrics justified?\n- [ ] Ablations support claims?\n- [ ] Statistical significance addressed?\n- [ ] Error bars / variance reported?\n\n**Common issues:**\n- Weak or outdated baselines\n- Metric chosen to favor method\n- Missing ablations for key components\n- Single seed results\n- Cherry-picked examples\n\n#### Results/Analysis\n- [ ] Claims supported by evidence?\n- [ ] Alternative explanations considered?\n- [ ] Limitations acknowledged?\n- [ ] Failure cases shown?\n\n**Common issues:**\n- Overclaiming from marginal improvements\n- Ignoring results that don't fit narrative\n- No discussion of when method fails\n\n#### Conclusion\n- [ ] Restates contribution accurately?\n- [ ] Future work is genuine (not hand-wavy)?\n- [ ] Doesn't introduce new claims?\n\n### Phase 3: The Killer Questions\n\nThese are the questions that sink papers:\n\n**Novelty:**\n- \"How is this different from [X]?\" (where X is obvious prior work)\n- \"Why couldn't you just do [simpler thing]?\"\n- \"What's the actual technical contribution?\"\n\n**Significance:**\n- \"Why should anyone care about this?\"\n- \"What changes if this paper exists vs. doesn't?\"\n- \"Is this solving a real problem or a made-up one?\"\n\n**Soundness:**\n- \"How do you know [claim]?\"\n- \"What if [assumption] is violated?\"\n- \"Did you try [obvious baseline]?\"\n\n**Clarity:**\n- \"What exactly do you mean by [term]?\"\n- \"How would someone reproduce this?\"\n- \"Why is [unexplained design choice] the right choice?\"\n\n### Phase 4: Scoring\n\nRate on standard conference criteria:\n\n| Criterion | Score (1-5) | Justification |\n|-----------|-------------|---------------|\n| **Novelty** | | How new is this? |\n| **Significance** | | How much does it matter? |\n| **Soundness** | | Is it technically correct? |\n| **Clarity** | | Is it well-written? |\n| **Reproducibility** | | Could I implement this? |\n\n**Overall Recommendation:**\n- Strong Accept: Top 5%, must be in conference\n- Weak Accept: Above threshold, would be OK to accept\n- Borderline: Could go either way\n- Weak Reject: Below threshold, but not fatally flawed\n- Strong Reject: Fundamental issues\n\n## Output Format\n\n```markdown\n# Reviewer 2 Report: [Paper Title]\n\n## Summary (2-3 sentences)\n[What the paper does and claims]\n\n## Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n## Weaknesses\n\n### Major Issues (any one is grounds for rejection)\n1. **[Issue Title]**\n   - What's wrong: [Description]\n   - Why it matters: [Impact on claims]\n   - How to fix: [Concrete suggestion]\n\n### Minor Issues (should be fixed but not fatal)\n1. **[Issue Title]**\n   - [Description and suggestion]\n\n### Nitpicks (take or leave)\n- [Small thing 1]\n- [Small thing 2]\n\n## Questions for Authors\n1. [Question that must be answered]\n2. [Question that would strengthen paper]\n\n## Missing References\n- [Paper 1]: [Why it should be cited]\n- [Paper 2]: [Why it should be cited]\n\n## Scores\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Novelty | X/5 | |\n| Significance | X/5 | |\n| Soundness | X/5 | |\n| Clarity | X/5 | |\n\n## Overall Assessment\n**Recommendation:** [Accept/Reject with confidence]\n\n**In one sentence:** [The core issue or strength]\n\n## Author Rebuttal Priorities\nIf I were the author, I would address these in order:\n1. [Most important thing to address]\n2. [Second most important]\n3. [Third]\n```\n\n## Calibration Notes\n\n**Reviewer 2 is harsh but fair:**\n- Points out real issues, not imagined ones\n- Suggests fixes, not just complaints\n- Acknowledges strengths genuinely\n- Would update opinion if given good rebuttal\n\n**Reviewer 2 is NOT:**\n- Dismissive without reason\n- Demanding impossible experiments\n- Rejecting due to missing tangential work\n- Penalizing for honest limitations"
              },
              {
                "name": "ted-mosby",
                "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured.",
                "path": "skills/ted-mosby/SKILL.md",
                "frontmatter": {
                  "name": "ted-mosby",
                  "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured."
                },
                "content": "# Ted Mosby - Architecture Wiki Generator\n\nGenerate comprehensive architectural documentation for any codebase with source code traceability (file:line references).\n\n## Overview\n\nTed Mosby creates architectural wikis that help developers understand codebases. Every concept links directly to source code, so you can navigate from documentation to implementation.\n\n**Output includes:**\n- Architecture overview with Mermaid diagrams\n- Module documentation with source traceability\n- Data flow documentation\n- Getting started guides\n- Interactive static site with search, keyboard nav, dark mode\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Document a codebase or project architecture\n- Generate a wiki or documentation site\n- Create architecture diagrams with source references\n- Understand and document how a project is structured\n- Produce navigable documentation with file:line traceability\n\n**Trigger phrases:**\n- \"Generate docs for this project\"\n- \"Create architecture documentation\"\n- \"Document this codebase\"\n- \"Make a wiki for this repo\"\n- \"Help me understand this project's structure\"\n\n## Prerequisites\n\n### Required\n- Node.js >= 18.0.0\n- Anthropic API key (`ANTHROPIC_API_KEY` environment variable)\n\n### Check Prerequisites\n```bash\n# Verify Node.js version\nnode --version  # Should be >= 18.0.0\n\n# Verify API key is set\necho $ANTHROPIC_API_KEY  # Should show your key\n```\n\n### Install Ted Mosby\n```bash\nnpm install -g ted-mosby\n```\n\n## Quick Start Commands\n\n### Basic Wiki Generation\n```bash\n# Generate wiki for current directory\nted-mosby generate -r .\n\n# Generate wiki for a specific project\nted-mosby generate -r ./my-project\n\n# Generate wiki for a GitHub repository\nted-mosby generate -r https://github.com/user/repo\n```\n\n### With Interactive Site\n```bash\n# Generate wiki + interactive static site\nted-mosby generate -r ./my-project --site\n\n# Custom title and theme\nted-mosby generate -r ./my-project --site --site-title \"My Project Docs\" --theme dark\n\n# Generate site only (if wiki already exists)\nted-mosby generate -r ./my-project --site-only\n```\n\n### Other Useful Options\n```bash\n# Focus on specific subdirectory\nted-mosby generate -r ./my-project -p src/core\n\n# Custom output directory\nted-mosby generate -r ./my-project -o ./docs/architecture\n\n# Verbose output (see agent progress)\nted-mosby generate -r ./my-project -v\n\n# Estimate time/cost before running (dry run)\nted-mosby generate -r ./my-project -e\n```\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nBefore running Ted Mosby, clarify with the user:\n\n1. **Target path** - What directory or repo to document?\n2. **Output location** - Where should the wiki go? (default: `./wiki`)\n3. **Site generation** - Do they want an interactive static site?\n4. **Focus area** - Any specific subdirectory to focus on?\n5. **Theme preference** - Light, dark, or auto?\n\n### Step 2: Pre-flight Checks\n\nVerify the environment is ready:\n\n```bash\n# Check Node.js version\nnode --version\n\n# Verify ted-mosby is installed\nwhich ted-mosby || echo \"Run: npm install -g ted-mosby\"\n\n# Check API key\n[ -z \"$ANTHROPIC_API_KEY\" ] && echo \"Set ANTHROPIC_API_KEY environment variable\"\n```\n\n### Step 3: Run Generation\n\nChoose the appropriate command based on user needs:\n\n| User Wants | Command |\n|------------|---------|\n| Basic wiki only | `ted-mosby generate -r ./project` |\n| Wiki + interactive site | `ted-mosby generate -r ./project --site` |\n| Site with custom title | `ted-mosby generate -r ./project --site --site-title \"Docs\"` |\n| Dark theme site | `ted-mosby generate -r ./project --site --theme dark` |\n| Focus on subdirectory | `ted-mosby generate -r ./project -p src/core` |\n| Large codebase | `ted-mosby generate -r ./project --max-chunks 5000` |\n| Quick iteration | `ted-mosby generate -r ./project --skip-index` |\n\n### Step 4: Review Output\n\nAfter generation completes:\n\n1. **Wiki location:** `./wiki/README.md` (or custom output dir)\n2. **Site location:** `./wiki/site/index.html` (if `--site` used)\n3. **Open site:** Open `index.html` in browser\n\n### Step 5: Fix Issues (if needed)\n\nIf there are broken links or missing pages:\n\n```bash\n# Check for and generate missing pages\nted-mosby continue -r ./my-project -o ./wiki\n\n# Verify only (don't generate)\nted-mosby continue -r ./my-project -o ./wiki --verify-only\n```\n\n## Output Structure\n\n```\nwiki/\n‚îú‚îÄ‚îÄ README.md                    # Navigation entry point\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ overview.md              # System architecture + Mermaid diagrams\n‚îÇ   ‚îî‚îÄ‚îÄ data-flow.md             # Data flow documentation\n‚îú‚îÄ‚îÄ components/\n‚îÇ   ‚îî‚îÄ‚îÄ {module}/\n‚îÇ       ‚îî‚îÄ‚îÄ index.md             # Per-module documentation\n‚îú‚îÄ‚îÄ guides/\n‚îÇ   ‚îî‚îÄ‚îÄ getting-started.md       # Quick start guide\n‚îú‚îÄ‚îÄ glossary.md                  # Concept index\n‚îî‚îÄ‚îÄ site/                        # (with --site flag)\n    ‚îú‚îÄ‚îÄ index.html               # Interactive site entry\n    ‚îú‚îÄ‚îÄ styles.css\n    ‚îî‚îÄ‚îÄ scripts.js\n```\n\n## Source Traceability\n\nEvery architectural concept includes clickable source references:\n\n```markdown\n## Authentication Flow\n\nThe authentication system uses JWT tokens for stateless auth.\n\n**Source:** [`src/auth/jwt-provider.ts:23-67`](../../../src/auth/jwt-provider.ts#L23-L67)\n```\n\nThis allows developers to navigate directly from documentation to implementation.\n\n## Interactive Site Features\n\nWhen `--site` is used, the generated site includes:\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Instant search across all pages (Cmd/Ctrl+K) |\n| Keyboard navigation | Arrow keys, vim-style (j/k/h/l) |\n| Dark/light mode | Respects system preference or manual toggle |\n| Table of contents | Auto-generated from headings |\n| Mobile responsive | Works on all devices |\n| Offline capable | No server required |\n| Mermaid diagrams | Rendered automatically |\n\n## Command Reference\n\n### `generate` - Create wiki documentation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path/url>` | Repository path or GitHub URL (required) | - |\n| `-o, --output <dir>` | Output directory for wiki | `./wiki` |\n| `-p, --path <path>` | Focus on specific directory | - |\n| `-s, --site` | Generate interactive static site | - |\n| `--site-only` | Generate site only (skip wiki) | - |\n| `--site-title <title>` | Custom site title | Project name |\n| `--theme <theme>` | Site theme: light, dark, auto | `auto` |\n| `-v, --verbose` | Show detailed progress | - |\n| `-e, --estimate` | Estimate time/cost (dry run) | - |\n| `--max-chunks <n>` | Limit indexed chunks (for large repos) | unlimited |\n| `--skip-index` | Use cached embeddings index | - |\n| `--direct-api` | Use Anthropic API directly | - |\n| `-m, --model <model>` | Claude model to use | `claude-sonnet-4-20250514` |\n| `--max-turns <n>` | Limit agent iterations | 200 |\n\n### `continue` - Resume/fix wiki generation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path>` | Repository path (required) | - |\n| `-o, --output <dir>` | Wiki output directory | `./wiki` |\n| `--verify-only` | Only check, don't generate | - |\n| `--skip-index` | Use cached embeddings index | - |\n| `-v, --verbose` | Show detailed progress | - |\n\n## Large Codebase Options\n\nFor repositories with 10,000+ files:\n\n```bash\n# Limit indexed chunks (reduces memory usage)\nted-mosby generate -r ./large-project --max-chunks 5000\n\n# Reduce search results per query\nted-mosby generate -r ./large-project --max-results 5\n\n# Batched processing (for very large repos)\nted-mosby generate -r ./large-project --batch-size 3000\n```\n\n## Typical Runtime\n\n| Codebase Size | Approximate Time |\n|---------------|------------------|\n| Small (<50 files) | 1-2 minutes |\n| Medium (50-200 files) | 2-5 minutes |\n| Large (200+ files) | 5-10 minutes |\n\nUse `--estimate` to get a cost/time estimate before running.\n\n## Troubleshooting\n\n### \"Credit balance is too low\" error\nUse direct API mode:\n```bash\nted-mosby generate -r ./my-project --direct-api\n```\n\n### Out of memory on large repos\nLimit indexed chunks:\n```bash\nted-mosby generate -r ./large-project --max-chunks 5000 --batch-size 3000\n```\n\n### Slow re-runs during development\nSkip re-indexing:\n```bash\nted-mosby generate -r ./my-project --skip-index\n```\n\n### Missing pages / broken links\nUse the continue command:\n```bash\nted-mosby continue -r ./my-project -o ./wiki\n```\n\n## Example Conversation\n\n**User:** \"Can you document this project's architecture?\"\n\n**Assistant:** I'll use Ted Mosby to generate architectural documentation for your project.\n\nFirst, let me verify the prerequisites are in place, then generate the wiki with an interactive site:\n\n```bash\n# Generate wiki with interactive site\nted-mosby generate -r . --site --site-title \"Project Architecture\"\n```\n\nThis will create:\n- `wiki/README.md` - Main navigation\n- `wiki/architecture/overview.md` - Architecture diagrams\n- `wiki/site/index.html` - Interactive documentation site\n\n## Resources\n\n- [Ted Mosby GitHub](https://github.com/your-username/ted-mosby)\n- [Build an Agent Workshop](https://buildanagentworkshop.com)"
              },
              {
                "name": "turn-this-feature-into-a-blog-post",
                "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\".",
                "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
                "frontmatter": {
                  "name": "turn-this-feature-into-a-blog-post",
                  "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\"."
                },
                "content": "# Turn This Feature Into a Blog Post\n\nGenerate a Markdown blog post that explains a code implementation in an engaging, educational way.\n\n## Process\n\n1. **Analyze the implementation** - Read and understand all relevant code files, tracing the feature from entry point to completion\n2. **Identify the narrative** - Find the core problem being solved and why it matters\n3. **Structure the post** - Organize as What ‚Üí Why ‚Üí How (from first principles)\n4. **Write accessibly** - Use friendly, conversational language while maintaining technical authority\n5. **Output Markdown** - Create a complete `.md` file ready for publishing\n\n## Blog Post Structure\n\n### Title\n- Clear, specific, and searchable\n- Format: \"How We Built [Feature]\" or \"Building [Feature]: A Deep Dive\"\n\n### Introduction (2-3 paragraphs)\n- Hook the reader with the problem or outcome\n- Briefly explain what the feature does\n- Preview what readers will learn\n\n### The What (1-2 sections)\n- Describe the feature from the user's perspective\n- Include screenshots or diagrams if applicable\n- Keep technical jargon minimal\n\n### The Why (1-2 sections)\n- Explain the problem this solves\n- Discuss alternatives considered and why this approach won\n- Connect to broader engineering principles\n\n### The How (2-4 sections)\n- Walk through the implementation from first principles\n- Include relevant code snippets with explanations\n- Explain non-obvious decisions\n- Build up complexity gradually\n\n### Conclusion\n- Summarize key takeaways\n- Mention potential future improvements\n- Invite engagement (questions, feedback)\n\n## Writing Style\n\n- **Friendly but authoritative** - Write like a knowledgeable colleague explaining over coffee\n- **First-person plural** - Use \"we\" to create shared ownership\n- **Active voice** - \"We built\" not \"It was built\"\n- **Show, don't just tell** - Use code examples liberally\n- **Explain the \"why\"** - Every code block should have context\n- **Avoid jargon walls** - Define terms on first use\n\n## Code Snippets\n\n- Include only relevant portions, not entire files\n- Add comments for non-obvious lines\n- Use syntax highlighting with language tags\n- Provide context before each snippet\n\n## Output\n\nSave the blog post as a Markdown file with:\n- Kebab-case filename matching the title\n- Frontmatter with title, date, author, and tags (if appropriate for the target platform)\n- Properly formatted headers, code blocks, and lists"
              }
            ]
          },
          {
            "name": "design-skills",
            "description": "Skills for visual design and creative asset generation",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add GhostScientist/skills",
              "/plugin install design-skills@GhostScientist-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-10T22:28:06Z",
              "created_at": "2025-12-08T15:22:00Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "create-watchos-version",
                "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation.",
                "path": "skills/create-watchos-version/SKILL.md",
                "frontmatter": {
                  "name": "create-watchos-version",
                  "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation."
                },
                "content": "# Create watchOS Version\n\nAnalyzes existing Apple platform projects and creates detailed, phased implementation plans for watchOS apps that are elegant, top-tier experiences‚Äînot naive skins of the parent app.\n\n## Workflow\n\n1. **Project Discovery** - Analyze project structure, patterns, architecture\n2. **Feature Mapping** - Identify watchOS-suitable features and priorities\n3. **API Compatibility** - Search web for current watchOS API availability\n4. **Architecture Planning** - Design watchOS-specific architecture\n5. **Plan Generation** - Create phased plan with warnings and alternatives\n6. **User Review** - Present plan for approval before implementation\n\n## Phase 1: Project Discovery\n\nScan project root for:\n\n```\n‚îú‚îÄ‚îÄ App Architecture (SwiftUI, UIKit, AppKit, hybrid)\n‚îú‚îÄ‚îÄ Data Layer (Core Data, SwiftData, Realm, custom)\n‚îú‚îÄ‚îÄ Networking (URLSession, Alamofire, custom)\n‚îú‚îÄ‚îÄ State Management (ObservableObject, TCA, Redux-like)\n‚îú‚îÄ‚îÄ Navigation (NavigationStack, Coordinator)\n‚îú‚îÄ‚îÄ Shared Frameworks (SPM packages, shared targets)\n‚îú‚îÄ‚îÄ Assets (colors, images, SF Symbols)\n‚îú‚îÄ‚îÄ Existing Watch Target (if any)\n‚îî‚îÄ‚îÄ Minimum iOS Version (affects watchOS targeting)\n```\n\nKey files: `*.xcodeproj`, `Package.swift`, `Info.plist`, App entry points, ViewModels, Models.\n\n## Phase 2: Feature Mapping\n\n**Glanceable (High Priority)**: Status displays, counters, progress, recent items, quick stats\n\n**Quick Actions (High Priority)**: Single-tap toggles, shortcuts, haptic confirmations\n\n**Complications/Widgets (Critical)**: Map data to WidgetKit families‚ÄîaccessoryCircular, accessoryRectangular, accessoryInline, accessoryCorner. Consider Smart Stack relevance.\n\n**Background**: HealthKit integration, background refresh, Watch Connectivity sync\n\n**Defer/Exclude**: Complex data entry, long-form content, sustained screen time features\n\n## Phase 3: API Compatibility\n\n**CRITICAL**: Always search web for current watchOS docs before finalizing. APIs change frequently.\n\nSearch: `[FrameworkName] watchOS availability site:developer.apple.com`\n\n### Quick Reference\n\n**Available**: SwiftUI, SwiftData (10+), WidgetKit (9+), HealthKit, WorkoutKit, CoreLocation (limited), WatchConnectivity, CloudKit, CoreMotion, AVFoundation (audio), CoreBluetooth, Combine, Swift Concurrency\n\n**Unavailable/Limited**: UIKit, WebKit, MapKit (limited), CoreImage (limited), ARKit, RealityKit, StoreKit (limited), Background URLSession (limited)\n\nSee `references/api-compatibility.md` for detailed compatibility matrix.\n\n## Phase 4: Architecture\n\n### Version Targeting\n\n```\niOS 16+ ‚Üí watchOS 9+  (WidgetKit complications)\niOS 17+ ‚Üí watchOS 10+ (SwiftData, Smart Stack)\niOS 18+ ‚Üí watchOS 11+ (Live Activities on Watch)\n```\n\n### Structure\n\n```\nShared/\n‚îú‚îÄ‚îÄ Models/           # Pure Swift, shared via target membership\n‚îú‚îÄ‚îÄ Services/         # Platform-agnostic logic\n‚îî‚îÄ‚îÄ Utilities/\n\nWatchApp/\n‚îú‚îÄ‚îÄ App.swift\n‚îú‚îÄ‚îÄ Views/\n‚îú‚îÄ‚îÄ ViewModels/\n‚îú‚îÄ‚îÄ Complications/\n‚îî‚îÄ‚îÄ WatchConnectivity/\n```\n\n### Design Principles\n\n1. **Glanceability** - Visible within 2 seconds\n2. **Minimal Interaction** - 1-3 taps max\n3. **Context Awareness** - Time, location, activity\n4. **Battery Conscious** - Efficient refresh, TimelineSchedule\n5. **Haptic Feedback** - Confirm actions appropriately\n\n### SwiftUI Gotchas\n\n- Avoid nested TabViews (memory leaks)\n- Use TimelineSchedule for efficient metric updates\n- Check `isLuminanceReduced` to reduce work when dimmed\n- Don't use data-driven high-frequency UI refreshes\n\n## Phase 5: Plan Generation\n\nUse template in `references/plan-template.md` to generate:\n\n1. Executive Summary\n2. ‚ö†Ô∏è API Compatibility Warnings table\n3. Phased implementation tasks\n4. Testing checklist\n\n## Phase 6: User Review\n\nPresent plan and ask for approval before implementing:\n\n> \"I've analyzed your project and created a watchOS plan. Before proceeding:\n> 1. **API Warnings**: [N] APIs unavailable‚Äîalternatives documented.\n> 2. **Recommended Features**: [list] prioritized for Watch.\n> 3. **Scope**: [N] phases.\n> \n> Proceed with implementation, or adjust the plan?\"\n\n**Do not implement until user approves.**\n\n## Best Practices Reference\n\n### Watch Connectivity\n\n```swift\nguard WCSession.default.activationState == .activated else { return }\n// sendMessage: immediate, requires reachability\n// transferUserInfo: queued, guaranteed\n// transferCurrentComplicationUserInfo: complication priority\n```\n\n### Complications\n\n```swift\n// Use appropriate reload policy\nTimeline(entries: entries, policy: .after(nextUpdateDate))\n// Use .never for static complications\n```\n\n### Battery Efficiency\n\n- Timeline-based over active refresh\n- Check `isLuminanceReduced`\n- Batch Watch Connectivity transfers\n- Significant location change vs continuous updates"
              },
              {
                "name": "experiment-design-checklist",
                "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates.",
                "path": "skills/experiment-design-checklist/SKILL.md",
                "frontmatter": {
                  "name": "experiment-design-checklist",
                  "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates."
                },
                "content": "# Experiment Design Checklist\n\nPrevent the \"I ran experiments for 3 months and they're meaningless\" disaster through rigorous upfront design.\n\n## The Core Principle\n\nBefore running ANY experiment, you should be able to answer:\n1. What specific claim will this experiment support or refute?\n2. What would convince a skeptical reviewer?\n3. What could go wrong that would invalidate the results?\n\n## Process\n\n### Step 1: State the Hypothesis Precisely\n\nConvert your research question into falsifiable predictions:\n\n**Template:**\n```\nIf [intervention/method], then [measurable outcome], because [mechanism].\n```\n\n**Examples:**\n- \"If we add auxiliary contrastive loss, then downstream task accuracy increases by >2%, because representations become more separable.\"\n- \"If we use learned positional encodings, then performance on sequences >4096 tokens improves, because the model can extrapolate beyond training length.\"\n\n**Null hypothesis:** What does \"no effect\" look like? This is what you're trying to reject.\n\n### Step 2: Identify Variables\n\n**Independent Variables (what you manipulate):**\n| Variable | Levels | Rationale |\n|----------|--------|-----------|\n| [Var 1] | [Level A, B, C] | [Why these levels] |\n\n**Dependent Variables (what you measure):**\n| Metric | How Measured | Why This Metric |\n|--------|--------------|-----------------|\n| [Metric 1] | [Procedure] | [Justification] |\n\n**Control Variables (what you hold constant):**\n| Variable | Fixed Value | Why Fixed |\n|----------|-------------|-----------|\n| [Var 1] | [Value] | [Prevents confound X] |\n\n### Step 3: Choose Baselines\n\nEvery experiment needs comparisons. No result is meaningful in isolation.\n\n**Baseline Hierarchy:**\n\n1. **Random/Trivial Baseline**\n   - What does random chance achieve?\n   - Sanity check that the task isn't trivial\n\n2. **Simple Baseline**\n   - Simplest reasonable approach\n   - Often embarrassingly effective\n\n3. **Standard Baseline**\n   - Well-known method from literature\n   - Apples-to-apples comparison\n\n4. **State-of-the-Art Baseline**\n   - Current best published result\n   - Only if you're claiming SOTA\n\n5. **Ablated Self**\n   - Your method minus key components\n   - Shows each component contributes\n\n**For each baseline, document:**\n- Source (paper, implementation)\n- Hyperparameters used\n- Whether you re-ran or used reported numbers\n- Any modifications made\n\n### Step 4: Design Ablations\n\nAblations answer: \"Is each component necessary?\"\n\n**Ablation Template:**\n| Variant | What's Removed/Changed | Expected Effect | If No Effect... |\n|---------|----------------------|-----------------|-----------------|\n| Full Model | Nothing | Best performance | - |\n| w/o Component A | Remove A | Performance drops X% | A isn't helping |\n| w/o Component B | Remove B | Performance drops Y% | B isn't helping |\n| Component A only | Only A, no B | Shows A's isolated contribution | - |\n\n**Good ablations are:**\n- Surgical (one change at a time)\n- Interpretable (clear what was changed)\n- Informative (result tells you something)\n\n### Step 5: Address Confounds\n\nThings that could explain your results OTHER than your hypothesis:\n\n**Common Confounds:**\n\n| Confound | How to Check | How to Control |\n|----------|--------------|----------------|\n| Hyperparameter tuning advantage | Same tuning budget for all | Report tuning procedure |\n| Compute advantage | Matched FLOPs/params | Report compute used |\n| Data leakage | Check train/test overlap | Strict separation |\n| Random seed luck | Multiple seeds | Report variance |\n| Implementation bugs (baseline) | Verify baseline numbers | Use official implementations |\n| Cherry-picked examples | Random or systematic selection | Pre-register selection criteria |\n\n### Step 6: Statistical Rigor\n\n**Sample Size:**\n- How many random seeds? (Minimum: 3, better: 5+)\n- How many data splits? (If applicable)\n- Power analysis: Can you detect expected effect size?\n\n**What to Report:**\n- Mean ¬± standard deviation (or standard error)\n- Confidence intervals where appropriate\n- Statistical significance tests if claiming \"better\"\n\n**Appropriate Tests:**\n| Comparison | Test | Assumptions |\n|------------|------|-------------|\n| Two methods, normal data | t-test | Normality, equal variance |\n| Two methods, unknown dist | Mann-Whitney U | Ordinal data |\n| Multiple methods | ANOVA + post-hoc | Normality |\n| Multiple methods, unknown | Kruskal-Wallis | Ordinal data |\n| Paired comparisons | Wilcoxon signed-rank | Same test instances |\n\n**Avoid:**\n- p-hacking (running until significant)\n- Multiple comparison problems (Bonferroni correct)\n- Reporting only favorable metrics\n\n### Step 7: Compute Budget\n\nBefore running, estimate:\n\n| Component | Estimate | Notes |\n|-----------|----------|-------|\n| Single training run | X GPU-hours | [Details] |\n| Hyperparameter search | Y runs √ó X hours | [Search strategy] |\n| Baselines | Z runs √ó W hours | [Which baselines] |\n| Ablations | N variants √ó X hours | [Which ablations] |\n| Seeds | M seeds √ó above | [How many seeds] |\n| **Total** | **T GPU-hours** | Buffer: 1.5-2x |\n\n**Go/No-Go Decision:** Is this feasible with available resources?\n\n### Step 8: Pre-Registration (Optional but Recommended)\n\nWrite down BEFORE running:\n- Exact hypotheses\n- Primary metrics (not chosen post-hoc)\n- Analysis plan\n- What would constitute \"success\"\n\nThis prevents unconscious goal-post moving.\n\n## Output: Experiment Design Document\n\n```markdown\n# Experiment Design: [Title]\n\n## Hypothesis\n[Precise statement]\n\n## Variables\n### Independent\n[Table]\n\n### Dependent\n[Table]\n\n### Controls\n[Table]\n\n## Baselines\n1. [Baseline 1]: [Source, details]\n2. [Baseline 2]: [Source, details]\n\n## Ablations\n[Table]\n\n## Confound Mitigation\n[Table]\n\n## Statistical Plan\n- Seeds: [N]\n- Tests: [Which tests for which comparisons]\n- Significance threshold: [Œ± level]\n\n## Compute Budget\n[Table with total estimate]\n\n## Success Criteria\n- Primary: [What must be true]\n- Secondary: [Nice to have]\n\n## Timeline\n- Phase 1: [What, when]\n- Phase 2: [What, when]\n\n## Known Risks\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n```\n\n## Red Flags in Experiment Design\n\nüö© \"We'll figure out the metrics later\"\nüö© \"One run should be enough\"\nüö© \"We don't need baselines, it's obviously better\"\nüö© \"Let's just see what happens\"\nüö© \"We can always run more if it's not significant\"\nüö© No compute estimate before starting\nüö© Vague success criteria"
              },
              {
                "name": "hugging-face-space-deployer",
                "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons.",
                "path": "skills/hugging-face-space-deployer/SKILL.md",
                "frontmatter": {
                  "name": "hugging-face-space-deployer",
                  "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons."
                },
                "content": "# Hugging Face Space Deployer\n\nA skill for AI engineers to create, configure, and deploy interactive ML demos on Hugging Face Spaces.\n\n## CRITICAL: Pre-Deployment Checklist\n\n**Before writing ANY code, gather this information about the model:**\n\n### 1. Check Model Type (LoRA Adapter vs Full Model)\n\n**Use the HF MCP tool to inspect the model files:**\n```\nhf-skills - Hub Repo Details (repo_ids: [\"username/model\"], repo_type: \"model\")\n```\n\n**Look for these indicators:**\n\n| Files Present | Model Type | Action Required |\n|---------------|------------|-----------------|\n| `model.safetensors` or `pytorch_model.bin` | Full model | Load directly with `AutoModelForCausalLM` |\n| `adapter_model.safetensors` + `adapter_config.json` | LoRA/PEFT adapter | Must load base model first, then apply adapter with `peft` |\n| Only config files, no weights | Broken/incomplete | Ask user to verify |\n\n**If adapter_config.json exists, check for `base_model_name_or_path` to identify the base model.**\n\n### 2. Check Inference API Availability\n\nVisit the model page on HF Hub and look for \"Inference Providers\" widget on the right side.\n\n**Indicators that model HAS Inference API:**\n- Inference widget visible on model page\n- Model from known provider: `meta-llama`, `mistralai`, `HuggingFaceH4`, `google`, `stabilityai`, `Qwen`\n- High download count (>10,000) with standard architecture\n\n**Indicators that model DOES NOT have Inference API:**\n- Personal namespace (e.g., `GhostScientist/my-model`)\n- LoRA/PEFT adapter (adapters never have direct Inference API)\n- Missing `pipeline_tag` in model metadata\n- No inference widget on model page\n\n### 3. Check Model Metadata\n\n- Ensure `pipeline_tag` is set (e.g., `text-generation`)\n- Add `conversational` tag for chat models\n\n### 4. Determine Hardware Needs\n\n| Model Size | Recommended Hardware |\n|------------|---------------------|\n| < 3B parameters | ZeroGPU (free) or CPU |\n| 3B - 7B parameters | ZeroGPU or T4 |\n| > 7B parameters | A10G or A100 |\n\n### 5. Ask User If Unclear\n\n**If you cannot determine the model type, ASK THE USER:**\n\n> \"I'm analyzing your model to determine the best deployment strategy. I found:\n> - [what you found about files]\n> - [what you found about inference API]\n>\n> Is this model:\n> 1. A full model you trained/uploaded?\n> 2. A LoRA/PEFT adapter on top of another model?\n> 3. Something else?\n>\n> Also, would you prefer:\n> A. Free deployment with ZeroGPU (may have queue times)\n> B. Paid GPU for faster response (~$0.60/hr)\"\n\n## Hardware Options\n\n| Hardware | Use Case | Cost |\n|----------|----------|------|\n| `cpu-basic` | Simple demos, Inference API apps | Free |\n| `cpu-upgrade` | Faster CPU inference | ~$0.03/hr |\n| **`zero-a10g`** | **Models needing GPU on-demand (recommended for most)** | **Free (with quota)** |\n| `t4-small` | Small GPU models (<7B) | ~$0.60/hr |\n| `t4-medium` | Medium GPU models | ~$0.90/hr |\n| `a10g-small` | Large models (7B-13B) | ~$1.50/hr |\n| `a10g-large` | Very large models (30B+) | ~$3.15/hr |\n| `a100-large` | Largest models | ~$4.50/hr |\n\n**ZeroGPU Note:** ZeroGPU (`zero-a10g`) provides free GPU access on-demand. The Space runs on CPU, and when a user triggers inference, a GPU is allocated temporarily (~60-120 seconds). **After deployment, you must manually set the runtime to \"ZeroGPU\" in Space Settings > Hardware.**\n\n## Deployment Decision Tree\n\n```\nAnalyze Model\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have adapter_config.json?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a LoRA adapter\n‚îÇ       ‚îú‚îÄ‚îÄ Find base_model_name_or_path in adapter_config.json\n‚îÇ       ‚îî‚îÄ‚îÄ Use Template 3 (LoRA + ZeroGPU)\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have model.safetensors or pytorch_model.bin?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a full model\n‚îÇ       ‚îú‚îÄ‚îÄ Is it from a major provider with inference widget?\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Use Inference API (Template 1)\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Use ZeroGPU (Template 2)\n‚îÇ\n‚îî‚îÄ‚îÄ Neither found?\n    ‚îî‚îÄ‚îÄ ASK USER - model may be incomplete\n```\n\n## Dependencies\n\n**For Inference API (cpu-basic, free):**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**For ZeroGPU full models (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**For ZeroGPU LoRA adapters (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n## CLI Commands (CORRECT Syntax)\n\n```bash\n# Create Space\nhf repo create my-space-name --repo-type space --space-sdk gradio\n\n# Upload files\nhf upload username/space-name ./local-folder --repo-type space\n\n# Download model files to inspect\nhf download username/model-name --local-dir ./model-check --dry-run\n\n# Check what files exist in a model\nhf download username/model-name --local-dir /tmp/check --dry-run 2>&1 | grep -E '\\.(safetensors|bin|json)'\n```\n\n## Template 1: Inference API (For Supported Models)\n\n**Use when:** Model has inference widget, is from major provider, or explicitly supports serverless API.\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nMODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"  # Must support Inference API!\nclient = InferenceClient(MODEL_ID)\n\ndef respond(message, history, system_message, max_tokens, temperature, top_p):\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    response = \"\"\n    for token in client.chat_completion(\n        messages,\n        max_tokens=max_tokens,\n        stream=True,\n        temperature=temperature,\n        top_p=top_p,\n    ):\n        delta = token.choices[0].delta.content or \"\"\n        response += delta\n        yield response\n\ndemo = gr.ChatInterface(\n    respond,\n    title=\"Chat Assistant\",\n    description=\"Powered by Hugging Face Inference API\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\"),\n        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Write a Python function to sort a list\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Chat App\nemoji: üí¨\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n```\n\n## Template 2: ZeroGPU Full Model (For Models Without Inference API)\n\n**Use when:** Full model (has model.safetensors) but no Inference API support.\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"username/my-full-model\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Global model - loaded lazily on first GPU call for faster Space startup\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Model\",\n    description=\"Powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me write some code\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Model\nemoji: ü§ñ\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Template 3: ZeroGPU LoRA Adapter (CRITICAL FOR FINE-TUNED MODELS)\n\n**Use when:** Model has `adapter_config.json` and `adapter_model.safetensors` (NOT `model.safetensors`)\n\n**You MUST identify the base model from `adapter_config.json` field `base_model_name_or_path`**\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Your LoRA adapter\nADAPTER_ID = \"username/my-lora-adapter\"\n# Base model (from adapter_config.json -> base_model_name_or_path)\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Global model - loaded lazily on first GPU call\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            BASE_MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n        model = model.merge_and_unload()  # Merge for faster inference\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for item in history:\n        if isinstance(item, (list, tuple)) and len(item) == 2:\n            user_msg, assistant_msg = item\n            if user_msg:\n                messages.append({\"role\": \"user\", \"content\": user_msg})\n            if assistant_msg:\n                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Fine-Tuned Model\",\n    description=\"LoRA fine-tuned model powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me with a coding task\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt (MUST include peft):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Fine-Tuned Model\nemoji: üîß\ncolorFrom: green\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Post-Deployment Steps\n\n**After uploading your Space files:**\n\n### 1. Set the Runtime Hardware (REQUIRED for GPU models)\n\n- Go to: `https://huggingface.co/spaces/USERNAME/SPACE_NAME/settings`\n- Under \"Space Hardware\", select the appropriate option:\n  - **ZeroGPU** for free on-demand GPU (recommended)\n  - Or a dedicated GPU tier if needed\n\n### 2. Verify the Space is Running\n\n- Check the Space URL for any build errors\n- Review container logs in Settings if issues occur\n\n### 3. Common Post-Deploy Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| \"No API found\" error | Hardware mismatch | Set runtime to ZeroGPU in Settings |\n| Model not loading | LoRA vs full model confusion | Check if it's an adapter, use correct template |\n| Inference API errors | Model not on serverless | Load directly with transformers instead |\n\n## Detecting Model Type - Quick Reference\n\n### Full Model\nFiles include: `model.safetensors`, `pytorch_model.bin`, or sharded versions\n```python\n# Can load directly\nmodel = AutoModelForCausalLM.from_pretrained(\"username/model\")\n```\n\n### LoRA/PEFT Adapter\nFiles include: `adapter_config.json`, `adapter_model.safetensors`\n```python\n# Must load base model first, then apply adapter\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model-id\")\nmodel = PeftModel.from_pretrained(base_model, \"username/adapter\")\nmodel = model.merge_and_unload()  # Optional: merge for faster inference\n```\n\n### Inference API Available\nModel page shows \"Inference Providers\" widget on the right side\n```python\n# Can use InferenceClient (simplest approach)\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\"username/model\")\n```\n\n## Fixing Missing pipeline_tag (To Enable Inference API)\n\nIf a model doesn't have an inference widget but should, it may be missing metadata:\n\n```bash\n# Download the README\nhf download username/model-name README.md --local-dir /tmp/fix\n\n# Edit to add pipeline_tag in YAML frontmatter:\n# ---\n# pipeline_tag: text-generation\n# tags:\n# - conversational\n# ---\n\n# Upload the fix\nhf upload username/model-name /tmp/fix/README.md README.md\n```\n\n**Note:** Even with correct tags, custom models may not get Inference API - it depends on HF's infrastructure decisions.\n\n## CRITICAL: Gradio 5.x Requirements\n\n### Examples Format (MUST be nested lists)\n```python\n# CORRECT:\nexamples=[\n    [\"Example 1\"],\n    [\"Example 2\"],\n]\n\n# WRONG (causes ValueError):\nexamples=[\n    \"Example 1\",\n    \"Example 2\",\n]\n```\n\n### Version Requirements\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\nDo NOT use `gradio==4.44.0` - causes `ImportError: cannot import name 'HfFolder'`\n\n## Troubleshooting\n\n### \"No API found\" Error\n**Cause:** Gradio app isn't exposing API correctly, often due to hardware mismatch\n**Fix:** Go to Space Settings and set runtime to \"ZeroGPU\" or appropriate GPU tier\n\n### \"OSError: does not appear to have a file named pytorch_model.bin, model.safetensors\"\n**Cause:** Trying to load a LoRA adapter as a full model\n**Fix:** Check for `adapter_config.json` - if present, use PEFT to load:\n```python\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\nmodel = PeftModel.from_pretrained(base_model, \"adapter-id\")\n```\n\n### Inference API Not Available\n**Cause:** Model doesn't have pipeline_tag or isn't deployed to serverless\n**Fix:** Either:\n  a. Add `pipeline_tag: text-generation` to model's README.md\n  b. Or load model directly with transformers instead of InferenceClient\n\n### `ImportError: cannot import name 'HfFolder'`\n**Cause:** gradio/huggingface_hub version mismatch\n**Fix:** Use `gradio>=5.0.0` and `huggingface_hub>=0.26.0`\n\n### `ValueError: examples must be nested list`\n**Cause:** Gradio 5.x format change\n**Fix:** Use `[[\"ex1\"], [\"ex2\"]]` not `[\"ex1\", \"ex2\"]`\n\n### Space builds but model doesn't load\n**Cause:** Missing `peft` for adapters, or wrong base model\n**Fix:** Check adapter_config.json for correct base_model_name_or_path\n\n## Workflow Summary\n\n1. **Analyze model** (check for adapter_config.json, model files, inference widget)\n2. **Determine strategy** (Inference API vs ZeroGPU, full model vs LoRA)\n3. **Ask user if unclear** about model type or cost preferences\n4. **Generate correct template** based on analysis\n5. **Create Space** with correct requirements and README\n6. **Upload files** using `hf upload`\n7. **Set hardware** in Space Settings (ZeroGPU for free GPU access)\n8. **Monitor build logs** for any issues"
              },
              {
                "name": "implement-paper-from-scratch",
                "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions.",
                "path": "skills/implement-paper-from-scratch/SKILL.md",
                "frontmatter": {
                  "name": "implement-paper-from-scratch",
                  "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions."
                },
                "content": "# Implement Paper From Scratch\n\nThe best way to truly understand a paper is to implement it. This skill guides you through that process methodically.\n\n## Philosophy\n\n- **No copy-pasting from reference implementations** - We build understanding, not just working code\n- **Checkpoint questions verify understanding** - You should be able to answer \"why\" at each step\n- **Minimal dependencies** - Use NumPy/PyTorch fundamentals, not high-level wrappers\n- **Deliberate debugging** - Bugs are learning opportunities, not obstacles\n\n## Process\n\n### Phase 1: Pre-Implementation Analysis\n\nBefore writing any code:\n\n1. **Identify the core algorithm** - Strip away ablations, extensions, bells and whistles. What's the minimal version?\n\n2. **List the components** - Break into modules:\n   - Data pipeline\n   - Model architecture\n   - Loss function(s)\n   - Training loop\n   - Evaluation metrics\n\n3. **Find the tricky parts** - What's non-obvious?\n   - Custom layers or operations\n   - Numerical stability concerns\n   - Hyperparameter sensitivity\n   - Implementation details buried in appendices\n\n4. **Gather reference numbers** - What should we expect?\n   - Training loss trajectory\n   - Validation metrics at convergence\n   - Compute requirements (if stated)\n\n### Phase 2: Scaffolded Implementation\n\nBuild up the implementation in this order:\n\n#### Step 1: Data\n```python\n# Start with synthetic/toy data\n# Verify shapes and types before touching real data\n```\n\n**Checkpoint:** Can you describe what each tensor represents and its expected shape?\n\n#### Step 2: Model Architecture\n```python\n# Build layer by layer\n# Print shapes at each stage\n# Verify parameter counts match paper\n```\n\n**Checkpoint:** If you randomly initialize and do a forward pass, do the output shapes match what the paper describes?\n\n#### Step 3: Loss Function\n```python\n# Implement exactly as described\n# Test with known inputs/outputs\n# Check gradient flow\n```\n\n**Checkpoint:** Can you explain each term in the loss and why it's there?\n\n#### Step 4: Training Loop\n```python\n# Minimal loop first (no logging, checkpointing, etc.)\n# Verify loss decreases on tiny overfit test\n# Then add bells and whistles\n```\n\n**Checkpoint:** Can you overfit a single batch? If not, something is broken.\n\n#### Step 5: Evaluation\n```python\n# Implement paper's exact metrics\n# Compare against reported numbers\n```\n\n**Checkpoint:** On the same data split, how close are you to paper's numbers?\n\n### Phase 3: The Debugging Gauntlet\n\nWhen it doesn't work (and it won't at first):\n\n1. **The Overfit Test**\n   - Can you memorize 1 example? 10? 100?\n   - If not, architecture or gradient bug\n\n2. **The Gradient Check**\n   - Are gradients flowing to all parameters?\n   - Any NaN or exploding gradients?\n\n3. **The Initialization Check**\n   - Match paper's initialization exactly\n   - This matters more than people think\n\n4. **The Learning Rate Sweep**\n   - Log scale: 1e-5 to 1e-1\n   - Loss should decrease for some range\n\n5. **The Ablation Debug**\n   - Remove components until it works\n   - Add back one at a time\n\n### Phase 4: Checkpoint Questions\n\nAt each stage, you should be able to answer:\n\n**Understanding:**\n- Why does this component exist?\n- What would happen without it?\n- What alternatives were considered?\n\n**Implementation:**\n- Why this specific implementation choice?\n- Where could numerical issues arise?\n- What's the computational complexity?\n\n**Debugging:**\n- What would it look like if this was broken?\n- How would you test this in isolation?\n- What are the most likely bugs?\n\n## Output Format\n\nFor each implementation session, provide:\n\n```markdown\n## Today's Implementation Goal\n[Specific component we're building]\n\n## Prerequisites Check\n- [ ] Previous components working\n- [ ] Understand what we're building\n- [ ] Know expected behavior\n\n## Implementation\n\n### Code\n[Code blocks with extensive comments]\n\n### Checkpoint Questions\n1. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n2. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n### Verification Steps\n- [ ] Test 1: [What to check]\n- [ ] Test 2: [What to check]\n\n### Common Bugs at This Stage\n1. [Bug pattern]: [How to identify and fix]\n\n## What's Next\n[Preview of next component and how it connects]\n```\n\n## Tips for Specific Paper Types\n\n### Transformer-based\n- Attention mask shapes are the #1 bug source\n- Verify positional encoding is applied correctly\n- Check layer norm placement (pre vs post)\n\n### RL/Policy Gradient\n- Sign errors in policy gradient are silent killers\n- Advantage normalization matters\n- Verify discount factor handling\n\n### Generative Models\n- KL term balancing is finicky\n- Check latent space distribution\n- Verify reconstruction looks reasonable before training\n\n### Computer Vision\n- Normalization (ImageNet stats, batch norm) is crucial\n- Data augmentation can make or break results\n- Verify input preprocessing matches paper exactly\n\n## Success Criteria\n\nYou're done when:\n\n1. **Numbers match** - Within reasonable variance of paper's results\n2. **Understanding is deep** - You can explain every line of code\n3. **You found the gotchas** - You know what breaks and why\n4. **You could modify it** - Confident to try your own variations\n\n## Anti-Patterns to Avoid\n\n- ‚ùå Copying code you don't understand\n- ‚ùå Skipping checkpoint questions\n- ‚ùå Using pre-built components for core algorithm\n- ‚ùå Ignoring discrepancies with paper\n- ‚ùå Moving on before current step works"
              },
              {
                "name": "ios-app-icon-generator",
                "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons.",
                "path": "skills/ios-app-icon-generator/SKILL.md",
                "frontmatter": {
                  "name": "ios-app-icon-generator",
                  "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons."
                },
                "content": "# iOS App Icon Generator\n\nCreate beautiful, production-ready iOS app icons through a two-phase creative process.\n\n## Phase 1: Visual Philosophy\n\nBefore drawing anything, develop a 2-3 paragraph **Icon Philosophy** that articulates:\n\n- **Core concept**: What single idea or feeling should the icon convey?\n- **Visual metaphor**: What shape, object, or abstraction represents the app's purpose?\n- **Color psychology**: What palette evokes the right emotional response?\n- **Silhouette test**: Will it be recognizable as a tiny black shape?\n\nWrite this philosophy out. It guides every design decision.\n\n### Design Principles\n\nIcons that work follow these rules:\n\n- **Simplicity**: One focal element. No more than 2-3 colors. No text (illegible at small sizes).\n- **Distinctiveness**: Must stand out in a grid of 30 other icons. Avoid generic symbols (gears, checkmarks, clouds).\n- **Scalability**: The 16x16 notification icon must read as clearly as the 1024x1024 App Store version.\n- **No photography**: Apple's guidelines discourage photos. Use illustration, geometry, or abstract forms.\n- **Optical balance**: Center of visual weight, not geometric center. Curves feel heavier than straight lines.\n\n## Phase 2: Icon Generation\n\nGenerate the icon as a **self-contained HTML file** with embedded SVG that:\n\n1. Renders the icon design at 1024x1024 (the master size)\n2. Includes iOS-style rounded corners (superellipse, not CSS border-radius)\n3. Shows a preview grid of all sizes to verify readability\n4. Provides a download mechanism for each size\n\n### Required Sizes\n\nGenerate all iOS app icon sizes:\n\n| Size | Purpose |\n|------|---------|\n| 1024x1024 | App Store |\n| 180x180 | iPhone (@3x) |\n| 167x167 | iPad Pro (@2x) |\n| 152x152 | iPad (@2x) |\n| 120x120 | iPhone (@2x) |\n| 87x87 | Spotlight (@3x) |\n| 80x80 | Spotlight (@2x) |\n| 76x76 | iPad (@1x) |\n| 60x60 | iPhone (@1x) |\n| 58x58 | Settings (@2x) |\n| 40x40 | Spotlight (@1x) |\n| 29x29 | Settings (@1x) |\n| 20x20 | Notification (@1x) |\n\n### HTML Artifact Structure\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>App Icon: [Name]</title>\n  <style>\n    /* Dark interface, icon grid layout, download buttons */\n  </style>\n</head>\n<body>\n  <!-- Philosophy statement -->\n  <!-- Master SVG at 1024x1024 -->\n  <!-- Preview grid showing all sizes -->\n  <!-- Download buttons (use canvas to convert SVG ‚Üí PNG) -->\n  <script>\n    // SVG ‚Üí Canvas ‚Üí PNG download logic\n  </script>\n</body>\n</html>\n```\n\n### SVG Guidelines\n\n- Use `viewBox=\"0 0 1024 1024\"` for the master\n- Apply the iOS squircle mask (superellipse with n‚âà5)\n- Use gradients sparingly but effectively\n- Ensure googd stroke widths scale proportionally\n- Test: zoom browser to 25% - is the icon still clear?\n\n### iOS Squircle Mask\n\nThe iOS icon shape is NOT a rounded rectangle. Use this superellipse path or approximate with:\n\n```svg\n<clipPath id=\"ios-squircle\">\n  <path d=\"M512,1024 C252,1024 0,772 0,512 C0,252 252,0 512,0 C772,0 1024,252 1024,512 C1024,772 772,1024 512,1024 Z\" />\n</clipPath>\n```\n\nOr generate programmatically with the superellipse formula: `|x/a|^n + |y/b|^n = 1` where n ‚âà 5.\n\n## Process\n\n1. Ask about the app's purpose, name, and any existing brand colors\n2. Write the Icon Philosophy\n3. Describe 2-3 concept directions with rationale\n4. Get user approval on a direction\n5. Generate the HTML artifact with full icon set\n6. Iterate based on feedback\n\n## Quality Bar\n\nThe output should look like it belongs on a top-10 App Store chart. Every icon in that grid was crafted by a professional designer - yours should be indistinguishable from theirs.\n\nAvoid:\n- Glossy/skeuomorphic styles (outdated since iOS 7)\n- Thin hairline details (disappear at small sizes)\n- Overly complex illustrations\n- Generic clip-art aesthetics\n- Centered-circle-on-gradient laziness"
              },
              {
                "name": "paper-to-intuition",
                "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams.",
                "path": "skills/paper-to-intuition/SKILL.md",
                "frontmatter": {
                  "name": "paper-to-intuition",
                  "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams."
                },
                "content": "# Paper to Intuition\n\nTransform dense academic papers into genuine understanding through layered explanation and visual intuition.\n\n## Process\n\n1. **Get the paper** - Ask for the arXiv link, PDF, or paper title\n2. **Extract the core** - Identify the single key insight (one sentence)\n3. **Build the ladder** - Create explanations at 4 levels\n4. **Visualize intuition** - Generate interactive diagrams\n5. **Stress test understanding** - \"What breaks if we remove X?\"\n\n## The Explanation Ladder\n\nGenerate explanations at each level, with each building on the last:\n\n### Level 1: ELI5 (1 paragraph)\n- No jargon, no equations\n- Use familiar analogies from everyday life\n- A curious 10-year-old should roughly get it\n\n### Level 2: Undergraduate (2-3 paragraphs)\n- Assume calculus, basic linear algebra, intro ML\n- Introduce key terms with definitions\n- Connect to textbook concepts they'd know\n\n### Level 3: Graduate (3-4 paragraphs)\n- Assume ML fundamentals, optimization, probability\n- Discuss relationship to prior work\n- Explain why naive approaches don't work\n- Cover the key equations with plain-English annotations\n\n### Level 4: Researcher (2-3 paragraphs)\n- Assume field expertise\n- Subtle technical contributions\n- Limitations and open questions\n- How this changes what's possible\n\n## Key Equations Breakdown\n\nFor each important equation:\n\n```\n[Equation in LaTeX]\n\nIn words: [Plain English translation]\n\nEach term:\n- [symbol]: [what it represents] [why it's there]\n\nIntuition: [Why this mathematical form? What would change if we used a different form?]\n```\n\n## Visual Intuition Artifact\n\nGenerate a self-contained HTML file with:\n\n- **Architecture diagram** - Boxes and arrows showing information flow\n- **Interactive sliders** - Manipulate key parameters, see effects\n- **Before/after comparisons** - What the method improves over baselines\n- **Failure case visualization** - When and why it breaks down\n\nUse SVG for diagrams, vanilla JavaScript for interactivity. Dark theme, clean typography.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>[Paper Name] - Visual Intuition</title>\n  <style>\n    :root { --bg: #1a1a2e; --text: #eee; --accent: #4f8cff; }\n    /* Clean, research-aesthetic styling */\n  </style>\n</head>\n<body>\n  <h1>[Paper Title]</h1>\n  <p class=\"tldr\">[One-sentence insight]</p>\n\n  <section id=\"architecture\">\n    <svg><!-- Information flow diagram --></svg>\n  </section>\n\n  <section id=\"interactive\">\n    <!-- Parameter sliders with live updates -->\n  </section>\n\n  <section id=\"comparisons\">\n    <!-- Before/after, ablations -->\n  </section>\n</body>\n</html>\n```\n\n## The \"What Breaks?\" Analysis\n\nFor each major component, explain:\n\n1. **What it does** - The role this component plays\n2. **What breaks without it** - Concrete failure mode\n3. **Why this solution** - Alternatives considered, why this won\n4. **The tradeoff** - What we pay for this choice (compute, complexity, assumptions)\n\n## Output Structure\n\nDeliver as a structured document:\n\n```markdown\n# [Paper Title]\n\n**TL;DR:** [One sentence]\n\n**Why it matters:** [One paragraph on significance]\n\n## The Explanation Ladder\n\n### ELI5\n[...]\n\n### Undergraduate Level\n[...]\n\n### Graduate Level\n[...]\n\n### Researcher Level\n[...]\n\n## Key Equations\n\n### Equation 1: [Name]\n[Breakdown as specified above]\n\n## What Breaks If We Remove...\n\n### [Component 1]\n[Analysis]\n\n### [Component 2]\n[Analysis]\n\n## Visual Intuition\n\n[Link to or embed HTML artifact]\n\n## Further Reading\n\n- [Prerequisite paper 1]\n- [Follow-up work 1]\n```\n\n## Quality Standards\n\n- Every analogy must be accurate, not just catchy\n- Equations must be explained, not just translated\n- Visuals must reveal structure, not just decorate\n- The researcher-level section should contain insight, not just summary\n- Admit when something is genuinely confusing or poorly explained in the original paper"
              },
              {
                "name": "research-question-refiner",
                "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis.",
                "path": "skills/research-question-refiner/SKILL.md",
                "frontmatter": {
                  "name": "research-question-refiner",
                  "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis."
                },
                "content": "# Research Question Refiner\n\nTransform \"I'm interested in X\" into \"I will investigate whether Y under conditions Z, measuring W.\"\n\n## The Problem\n\nMost research ideas fail not because they're bad, but because they're:\n- Too vague to act on\n- Too ambitious to complete\n- Too incremental to matter\n- Missing a clear success criterion\n\nThis skill fixes that.\n\n## Process\n\n### Stage 1: Excavate the Interest\n\nStart by understanding what's actually pulling at you:\n\n**Questions to ask:**\n1. What sparked this interest? (Paper, conversation, problem you encountered?)\n2. What's the version that excites you most?\n3. What would be cool if it worked?\n4. Who would care about the answer?\n\n**Output:** A paragraph capturing the raw interest, unfiltered.\n\n### Stage 2: Map the Territory\n\nBefore scoping, understand the landscape:\n\n**What's Known:**\n- What's the current state-of-the-art?\n- What are the established approaches?\n- What have people tried that didn't work?\n\n**What's Unknown:**\n- What are the acknowledged open problems?\n- What assumptions does current work make?\n- Where do methods fail?\n\n**What's Controversial:**\n- Where do researchers disagree?\n- What's claimed but not convincingly shown?\n- What's believed but not rigorously tested?\n\n**Output:** A structured map with citations/references for each area.\n\n### Stage 3: Find the Gap\n\nA good research question lives in a gap that is:\n\n| Property | Too Little | Just Right | Too Much |\n|----------|-----------|------------|----------|\n| **Novelty** | Redoing existing work | New angle or combination | No foundation to build on |\n| **Difficulty** | Trivial to answer | Challenging but doable | Requires breakthroughs |\n| **Impact** | No one cares | Community would update beliefs | Nobel prize (unrealistic) |\n| **Scope** | One experiment | Thesis chapter / paper | Multiple PhDs |\n\n**Gap-finding questions:**\n- What would change if we relaxed assumption X?\n- What if we applied method A to domain B?\n- What's between approach X and approach Y?\n- What fails in setting Z that works elsewhere?\n\n**Output:** 3-5 candidate gaps, each as one sentence.\n\n### Stage 4: Refine to Concrete Question\n\nFor each candidate gap, sharpen into a question:\n\n**The Formula:**\n```\n[Action verb] + [specific phenomenon] + [under conditions] + [measurable outcome]\n```\n\n**Examples of refinement:**\n\n‚ùå Vague: \"How can we make transformers more efficient?\"\n‚úÖ Concrete: \"Does structured sparsity in attention patterns preserve performance on long-context tasks while reducing compute by >50%?\"\n\n‚ùå Vague: \"Can robots learn from humans better?\"\n‚úÖ Concrete: \"Does incorporating gaze direction in demonstrations improve sample efficiency for manipulation tasks compared to kinesthetic teaching alone?\"\n\n‚ùå Vague: \"What makes language models hallucinate?\"\n‚úÖ Concrete: \"Do retrieval-augmented models hallucinate less on factual questions when retrieval confidence is used to modulate generation temperature?\"\n\n### Stage 5: Feasibility Check\n\nFor each refined question, assess:\n\n**Resources Required:**\n- Compute: GPU-hours estimate\n- Data: Available or needs collection?\n- Time: Weeks/months realistically\n- Expertise: What skills are needed?\n\n**Risk Assessment:**\n- What's the probability this works at all?\n- What if the hypothesis is wrong? (Is negative result publishable?)\n- What could go wrong technically?\n- What could invalidate the whole direction?\n\n**Dependencies:**\n- Does this require other work to finish first?\n- Are there rate-limiting steps?\n- What can be parallelized?\n\n### Stage 6: The Litmus Tests\n\nA good research question passes all of these:\n\n**The Advisor Test:**\n> \"If I pitched this in 2 minutes, would a busy professor say 'yes, go do that' rather than 'hmm, let's talk more'?\"\n\n**The Paper Test:**\n> \"Can I envision the title, abstract, and figure 1 of the resulting paper?\"\n\n**The Null Result Test:**\n> \"If my hypothesis is wrong, would that still be interesting to report?\"\n\n**The Motivation Test:**\n> \"Am I actually excited to work on this for 6+ months?\"\n\n**The Explanation Test:**\n> \"Can I explain why this matters to a smart non-expert in 60 seconds?\"\n\n## Output Format\n\nDeliver a Research Question Brief:\n\n```markdown\n# Research Question Brief\n\n## The Interest (Raw)\n[Original unfiltered interest]\n\n## Territory Map\n\n### What's Known\n- [Point 1] ([citation])\n- [Point 2] ([citation])\n\n### What's Unknown\n- [Open question 1]\n- [Open question 2]\n\n### What's Controversial\n- [Debate 1]\n\n## Candidate Gaps\n1. [Gap 1]\n2. [Gap 2]\n3. [Gap 3]\n\n## Refined Questions\n\n### Question 1: [Title]\n**Statement:** [Precise question]\n**Hypothesis:** [What you expect to find]\n**Feasibility:** [Brief assessment]\n**If it works:** [Impact]\n**If it doesn't:** [What we still learn]\n\n### Question 2: [Title]\n[Same structure]\n\n## Recommendation\n[Which question to pursue and why]\n\n## Immediate Next Steps\n1. [Concrete action 1]\n2. [Concrete action 2]\n3. [Concrete action 3]\n```\n\n## Common Failure Modes\n\n**The Kitchen Sink:** Trying to answer too many questions at once\n‚Üí Fix: Ruthlessly cut until there's ONE core question\n\n**The Solution in Search of a Problem:** Starting with a method, not a question\n‚Üí Fix: Ask \"Who has this problem? Why hasn't it been solved?\"\n\n**The Incremental Trap:** Small delta on existing work\n‚Üí Fix: Ask \"Would this change how people think?\"\n\n**The Impossible Dream:** Beautiful question, can't be answered\n‚Üí Fix: Ask \"What's the minimal version that's still interesting?\"\n\n**The Boring Sure Thing:** Will definitely work, nobody cares\n‚Üí Fix: Add ambition until there's meaningful risk"
              },
              {
                "name": "research-taste-developer",
                "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently.",
                "path": "skills/research-taste-developer/SKILL.md",
                "frontmatter": {
                  "name": "research-taste-developer",
                  "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently."
                },
                "content": "# Research Taste Developer\n\nResearch taste is the ability to distinguish work that matters from work that doesn't - before the community tells you. This skill helps you develop that instinct.\n\n## What is Research Taste?\n\nIt's the intuition that lets experienced researchers:\n- Pick problems that turn out to be important\n- Know when an idea is \"close\" vs. \"far\" from working\n- Recognize a good result even with imperfect execution\n- Predict which papers will be remembered in 5 years\n\nTaste isn't magic - it's pattern recognition from deep exposure. This skill accelerates that exposure.\n\n## Process\n\n### Phase 1: Analyze the Field\n\nPick a specific subfield. We'll study what \"good\" looks like there.\n\n**Questions to investigate:**\n1. What are the 10 most-cited papers of the last 5 years?\n2. What are the 5 papers experts say \"changed how we think\"?\n3. What are the best papers from top venues (NeurIPS, ICML, CVPR, etc.)?\n4. What got awards? What got invited talks?\n\n**For each landmark paper, analyze:**\n- What was the state before this paper?\n- What's the single core insight?\n- What specifically made people cite it?\n- Was it obvious in hindsight?\n\n### Phase 2: Pattern Recognition\n\nLook for what the great papers have in common:\n\n**The Patterns of Impact:**\n\n#### 1. The New Primitive\nPapers that introduce a building block others build on.\n- Examples: Attention mechanism, ResNet skip connections, Dropout\n- Pattern: Simple idea, surprisingly general applicability\n- Why it works: Reduces friction for future work\n\n#### 2. The Surprising Connection\nPapers that link two previously separate areas.\n- Examples: VAE (variational inference + neural nets), NeRF (neural nets + ray marching)\n- Pattern: \"X, but for Y\" where the combination is non-obvious\n- Why it works: Cross-pollinates communities\n\n#### 3. The Scaling Insight\nPapers showing that scale changes qualitative behavior.\n- Examples: GPT-3, Chinchilla\n- Pattern: What everyone \"knew\" was wrong at sufficient scale\n- Why it works: Forces field to update beliefs\n\n#### 4. The Rigorous Foundation\nPapers that formalize what was previously folklore.\n- Examples: Theoretical convergence proofs, generalization bounds\n- Pattern: Makes hand-wavy intuitions precise\n- Why it works: Enables confident building\n\n#### 5. The Elegant Solution\nPapers that solve a problem far more simply than expected.\n- Examples: Simple baseline papers, \"X is all you need\"\n- Pattern: Previous solutions were overcomplicated\n- Why it works: Shifts field's complexity assumptions\n\n### Phase 3: Anti-Patterns\n\nLearn to recognize work that won't age well:\n\n**The Incremental Treadmill:**\n- Pattern: +0.5% on benchmark with architectural tweak\n- Why it fails: No one remembers or uses it\n- Exception: When it reveals something fundamental\n\n**The Method Mashing:**\n- Pattern: \"We combine A, B, C, and D\"\n- Why it fails: No insight about why the combination works\n- Exception: When combination reveals unexpected interaction\n\n**The Benchmark Overfitter:**\n- Pattern: Method that works only on specific benchmarks\n- Why it fails: Doesn't transfer, forgotten when benchmarks change\n- Exception: When it exposes benchmark weaknesses\n\n**The Complexity Monster:**\n- Pattern: Works but requires 47 hyperparameters and 3 loss terms\n- Why it fails: No one can reproduce or build on it\n- Exception: Rarely\n\n**The Solution Without a Problem:**\n- Pattern: Novel method without compelling use case\n- Why it fails: \"Interesting but why?\"\n- Exception: When use case emerges later (rare)\n\n### Phase 4: Develop Your Own Taste\n\n**Exercise 1: Prediction Game**\nBefore reading a paper, predict based on title/abstract:\n- Will this paper be cited >100 times in 5 years?\n- Write down your prediction and reasoning\n- Track your accuracy over time\n- Analyze where your predictions went wrong\n\n**Exercise 2: Explain the Gap**\nFor any two papers in citation count:\n- Paper A: 2000 citations\n- Paper B: 50 citations (same venue, same year)\n- What explains the difference?\n- Write a paragraph explanation\n\n**Exercise 3: The Time Machine**\nPick a highly-cited paper. Go back to when it was published:\n- What was the state of the field?\n- Would you have recognized its importance?\n- What signals would you have looked for?\n\n**Exercise 4: Design a Hit**\nGiven current state of a field:\n- What's the most important open problem?\n- What would a \"great paper\" on this look like?\n- What would make people cite it?\n\n### Phase 5: Meta-Principles\n\nWhat top researchers seem to do differently:\n\n**Problem Selection:**\n- Work on problems that are \"ready\" (pieces exist, no one assembled them)\n- Avoid problems that are stuck for fundamental reasons\n- Pick problems where you have unfair advantages\n\n**Execution Taste:**\n- Know when to stop polishing (diminishing returns)\n- Know when result is \"strong enough\" to share\n- Prefer simple-that-works over complex-that-works-slightly-better\n\n**Communication Taste:**\n- Lead with the insight, not the method\n- Make contribution obvious in first 2 minutes\n- Anticipate and address likely objections\n\n**Portfolio Taste:**\n- Mix safe and risky projects\n- Build a coherent research identity\n- Create compound interest (each paper enables the next)\n\n## Output: Taste Development Report\n\n```markdown\n# Research Taste Analysis: [Field/Subfield]\n\n## Landmark Paper Analysis\n\n### [Paper 1 Title] ([Year])\n- **Pre-existing state:** [What was true before]\n- **Core insight:** [One sentence]\n- **Why it's cited:** [Specific reason]\n- **Pattern type:** [New Primitive / Connection / etc.]\n\n### [Paper 2 Title]\n[Same structure]\n\n## Pattern Distribution\nIn this subfield, highly-cited papers tend to be:\n- [X]% New Primitives\n- [Y]% Surprising Connections\n- [Z]% Other\n\n## Anti-Pattern Warnings\nThe following patterns are common but don't lead to impact:\n1. [Anti-pattern common in this field]\n2. [Another one]\n\n## Taste Heuristics for [Field]\nWhen evaluating a paper in this field, ask:\n1. [Field-specific question that distinguishes good from meh]\n2. [Another one]\n3. [Another one]\n\n## Current Opportunities\nBased on this analysis, promising directions seem to be:\n1. [Direction 1]: [Why it's ripe]\n2. [Direction 2]: [Why it's ripe]\n\n## Your Taste Development Exercises\n1. [Specific exercise for this field]\n2. [Another one]\n```\n\n## The Ultimate Test\n\nYou have good taste when:\n- You're bored by work others find impressive (correctly predicting it won't matter)\n- You're excited by work others overlook (correctly predicting it will matter)\n- Your intuitions about importance are calibrated with reality\n- You can articulate *why* something is good, not just that it is\n\nThis takes years. But deliberate practice - not just reading, but *analyzing* - accelerates it dramatically."
              },
              {
                "name": "reviewer-2-simulator",
                "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims.",
                "path": "skills/reviewer-2-simulator/SKILL.md",
                "frontmatter": {
                  "name": "reviewer-2-simulator",
                  "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims."
                },
                "content": "# Reviewer 2 Simulator\n\nChannel the energy of the harshest (but fair) reviewer to find weaknesses before your actual reviewers do.\n\n## The Mindset\n\nReviewer 2 is:\n- Skeptical but not hostile\n- Technically rigorous\n- Short on time (will skim, not read carefully)\n- Looking for reasons to reject (high-volume venues)\n- But wants to champion good work\n\nReviewer 2 is NOT:\n- Trying to be mean\n- Unfamiliar with the field (usually)\n- Unable to be convinced by good arguments\n\n## Process\n\n### Phase 1: First Pass (5-minute skim)\n\nRead like a busy reviewer would:\n- Title and abstract\n- Figures and captions\n- Section headers\n- Conclusion\n\n**First-pass questions:**\n1. Can I understand the contribution from abstract alone?\n2. Do the figures tell the story?\n3. Is this obviously incremental or obviously interesting?\n4. Any immediate red flags?\n\n### Phase 2: Deep Read Critique\n\nGo section by section:\n\n#### Abstract\n- [ ] Clear problem statement?\n- [ ] Specific contribution (not vague \"we propose...\")?\n- [ ] Key result with number?\n- [ ] Any overclaims?\n\n**Common issues:**\n- \"We achieve state-of-the-art\" without specifying where/what\n- \"Novel\" without explaining what's actually new\n- Claims not supported in the paper\n\n#### Introduction\n- [ ] Motivation compelling?\n- [ ] Gap in prior work clearly identified?\n- [ ] Contribution stated precisely?\n- [ ] Paper organization clear?\n\n**Common issues:**\n- Straw-man characterization of prior work\n- Gap is manufactured, not real\n- Contribution buried in paragraph 4\n\n#### Related Work\n- [ ] Comprehensive coverage?\n- [ ] Fair characterization of prior work?\n- [ ] Clear differentiation from closest work?\n- [ ] Missing obvious citations?\n\n**Common issues:**\n- Missing direct competitors\n- Misrepresenting prior work to look better\n- No clear statement of difference from closest work\n\n#### Method\n- [ ] Technically sound?\n- [ ] Reproducible from description?\n- [ ] Assumptions stated explicitly?\n- [ ] Notation consistent?\n\n**Common issues:**\n- Hand-wavy justification\n- Critical details in appendix (or missing entirely)\n- Unstated assumptions\n- Notation changes mid-paper\n\n#### Experiments\n- [ ] Baselines appropriate and strong?\n- [ ] Metrics justified?\n- [ ] Ablations support claims?\n- [ ] Statistical significance addressed?\n- [ ] Error bars / variance reported?\n\n**Common issues:**\n- Weak or outdated baselines\n- Metric chosen to favor method\n- Missing ablations for key components\n- Single seed results\n- Cherry-picked examples\n\n#### Results/Analysis\n- [ ] Claims supported by evidence?\n- [ ] Alternative explanations considered?\n- [ ] Limitations acknowledged?\n- [ ] Failure cases shown?\n\n**Common issues:**\n- Overclaiming from marginal improvements\n- Ignoring results that don't fit narrative\n- No discussion of when method fails\n\n#### Conclusion\n- [ ] Restates contribution accurately?\n- [ ] Future work is genuine (not hand-wavy)?\n- [ ] Doesn't introduce new claims?\n\n### Phase 3: The Killer Questions\n\nThese are the questions that sink papers:\n\n**Novelty:**\n- \"How is this different from [X]?\" (where X is obvious prior work)\n- \"Why couldn't you just do [simpler thing]?\"\n- \"What's the actual technical contribution?\"\n\n**Significance:**\n- \"Why should anyone care about this?\"\n- \"What changes if this paper exists vs. doesn't?\"\n- \"Is this solving a real problem or a made-up one?\"\n\n**Soundness:**\n- \"How do you know [claim]?\"\n- \"What if [assumption] is violated?\"\n- \"Did you try [obvious baseline]?\"\n\n**Clarity:**\n- \"What exactly do you mean by [term]?\"\n- \"How would someone reproduce this?\"\n- \"Why is [unexplained design choice] the right choice?\"\n\n### Phase 4: Scoring\n\nRate on standard conference criteria:\n\n| Criterion | Score (1-5) | Justification |\n|-----------|-------------|---------------|\n| **Novelty** | | How new is this? |\n| **Significance** | | How much does it matter? |\n| **Soundness** | | Is it technically correct? |\n| **Clarity** | | Is it well-written? |\n| **Reproducibility** | | Could I implement this? |\n\n**Overall Recommendation:**\n- Strong Accept: Top 5%, must be in conference\n- Weak Accept: Above threshold, would be OK to accept\n- Borderline: Could go either way\n- Weak Reject: Below threshold, but not fatally flawed\n- Strong Reject: Fundamental issues\n\n## Output Format\n\n```markdown\n# Reviewer 2 Report: [Paper Title]\n\n## Summary (2-3 sentences)\n[What the paper does and claims]\n\n## Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n## Weaknesses\n\n### Major Issues (any one is grounds for rejection)\n1. **[Issue Title]**\n   - What's wrong: [Description]\n   - Why it matters: [Impact on claims]\n   - How to fix: [Concrete suggestion]\n\n### Minor Issues (should be fixed but not fatal)\n1. **[Issue Title]**\n   - [Description and suggestion]\n\n### Nitpicks (take or leave)\n- [Small thing 1]\n- [Small thing 2]\n\n## Questions for Authors\n1. [Question that must be answered]\n2. [Question that would strengthen paper]\n\n## Missing References\n- [Paper 1]: [Why it should be cited]\n- [Paper 2]: [Why it should be cited]\n\n## Scores\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Novelty | X/5 | |\n| Significance | X/5 | |\n| Soundness | X/5 | |\n| Clarity | X/5 | |\n\n## Overall Assessment\n**Recommendation:** [Accept/Reject with confidence]\n\n**In one sentence:** [The core issue or strength]\n\n## Author Rebuttal Priorities\nIf I were the author, I would address these in order:\n1. [Most important thing to address]\n2. [Second most important]\n3. [Third]\n```\n\n## Calibration Notes\n\n**Reviewer 2 is harsh but fair:**\n- Points out real issues, not imagined ones\n- Suggests fixes, not just complaints\n- Acknowledges strengths genuinely\n- Would update opinion if given good rebuttal\n\n**Reviewer 2 is NOT:**\n- Dismissive without reason\n- Demanding impossible experiments\n- Rejecting due to missing tangential work\n- Penalizing for honest limitations"
              },
              {
                "name": "ted-mosby",
                "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured.",
                "path": "skills/ted-mosby/SKILL.md",
                "frontmatter": {
                  "name": "ted-mosby",
                  "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured."
                },
                "content": "# Ted Mosby - Architecture Wiki Generator\n\nGenerate comprehensive architectural documentation for any codebase with source code traceability (file:line references).\n\n## Overview\n\nTed Mosby creates architectural wikis that help developers understand codebases. Every concept links directly to source code, so you can navigate from documentation to implementation.\n\n**Output includes:**\n- Architecture overview with Mermaid diagrams\n- Module documentation with source traceability\n- Data flow documentation\n- Getting started guides\n- Interactive static site with search, keyboard nav, dark mode\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Document a codebase or project architecture\n- Generate a wiki or documentation site\n- Create architecture diagrams with source references\n- Understand and document how a project is structured\n- Produce navigable documentation with file:line traceability\n\n**Trigger phrases:**\n- \"Generate docs for this project\"\n- \"Create architecture documentation\"\n- \"Document this codebase\"\n- \"Make a wiki for this repo\"\n- \"Help me understand this project's structure\"\n\n## Prerequisites\n\n### Required\n- Node.js >= 18.0.0\n- Anthropic API key (`ANTHROPIC_API_KEY` environment variable)\n\n### Check Prerequisites\n```bash\n# Verify Node.js version\nnode --version  # Should be >= 18.0.0\n\n# Verify API key is set\necho $ANTHROPIC_API_KEY  # Should show your key\n```\n\n### Install Ted Mosby\n```bash\nnpm install -g ted-mosby\n```\n\n## Quick Start Commands\n\n### Basic Wiki Generation\n```bash\n# Generate wiki for current directory\nted-mosby generate -r .\n\n# Generate wiki for a specific project\nted-mosby generate -r ./my-project\n\n# Generate wiki for a GitHub repository\nted-mosby generate -r https://github.com/user/repo\n```\n\n### With Interactive Site\n```bash\n# Generate wiki + interactive static site\nted-mosby generate -r ./my-project --site\n\n# Custom title and theme\nted-mosby generate -r ./my-project --site --site-title \"My Project Docs\" --theme dark\n\n# Generate site only (if wiki already exists)\nted-mosby generate -r ./my-project --site-only\n```\n\n### Other Useful Options\n```bash\n# Focus on specific subdirectory\nted-mosby generate -r ./my-project -p src/core\n\n# Custom output directory\nted-mosby generate -r ./my-project -o ./docs/architecture\n\n# Verbose output (see agent progress)\nted-mosby generate -r ./my-project -v\n\n# Estimate time/cost before running (dry run)\nted-mosby generate -r ./my-project -e\n```\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nBefore running Ted Mosby, clarify with the user:\n\n1. **Target path** - What directory or repo to document?\n2. **Output location** - Where should the wiki go? (default: `./wiki`)\n3. **Site generation** - Do they want an interactive static site?\n4. **Focus area** - Any specific subdirectory to focus on?\n5. **Theme preference** - Light, dark, or auto?\n\n### Step 2: Pre-flight Checks\n\nVerify the environment is ready:\n\n```bash\n# Check Node.js version\nnode --version\n\n# Verify ted-mosby is installed\nwhich ted-mosby || echo \"Run: npm install -g ted-mosby\"\n\n# Check API key\n[ -z \"$ANTHROPIC_API_KEY\" ] && echo \"Set ANTHROPIC_API_KEY environment variable\"\n```\n\n### Step 3: Run Generation\n\nChoose the appropriate command based on user needs:\n\n| User Wants | Command |\n|------------|---------|\n| Basic wiki only | `ted-mosby generate -r ./project` |\n| Wiki + interactive site | `ted-mosby generate -r ./project --site` |\n| Site with custom title | `ted-mosby generate -r ./project --site --site-title \"Docs\"` |\n| Dark theme site | `ted-mosby generate -r ./project --site --theme dark` |\n| Focus on subdirectory | `ted-mosby generate -r ./project -p src/core` |\n| Large codebase | `ted-mosby generate -r ./project --max-chunks 5000` |\n| Quick iteration | `ted-mosby generate -r ./project --skip-index` |\n\n### Step 4: Review Output\n\nAfter generation completes:\n\n1. **Wiki location:** `./wiki/README.md` (or custom output dir)\n2. **Site location:** `./wiki/site/index.html` (if `--site` used)\n3. **Open site:** Open `index.html` in browser\n\n### Step 5: Fix Issues (if needed)\n\nIf there are broken links or missing pages:\n\n```bash\n# Check for and generate missing pages\nted-mosby continue -r ./my-project -o ./wiki\n\n# Verify only (don't generate)\nted-mosby continue -r ./my-project -o ./wiki --verify-only\n```\n\n## Output Structure\n\n```\nwiki/\n‚îú‚îÄ‚îÄ README.md                    # Navigation entry point\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ overview.md              # System architecture + Mermaid diagrams\n‚îÇ   ‚îî‚îÄ‚îÄ data-flow.md             # Data flow documentation\n‚îú‚îÄ‚îÄ components/\n‚îÇ   ‚îî‚îÄ‚îÄ {module}/\n‚îÇ       ‚îî‚îÄ‚îÄ index.md             # Per-module documentation\n‚îú‚îÄ‚îÄ guides/\n‚îÇ   ‚îî‚îÄ‚îÄ getting-started.md       # Quick start guide\n‚îú‚îÄ‚îÄ glossary.md                  # Concept index\n‚îî‚îÄ‚îÄ site/                        # (with --site flag)\n    ‚îú‚îÄ‚îÄ index.html               # Interactive site entry\n    ‚îú‚îÄ‚îÄ styles.css\n    ‚îî‚îÄ‚îÄ scripts.js\n```\n\n## Source Traceability\n\nEvery architectural concept includes clickable source references:\n\n```markdown\n## Authentication Flow\n\nThe authentication system uses JWT tokens for stateless auth.\n\n**Source:** [`src/auth/jwt-provider.ts:23-67`](../../../src/auth/jwt-provider.ts#L23-L67)\n```\n\nThis allows developers to navigate directly from documentation to implementation.\n\n## Interactive Site Features\n\nWhen `--site` is used, the generated site includes:\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Instant search across all pages (Cmd/Ctrl+K) |\n| Keyboard navigation | Arrow keys, vim-style (j/k/h/l) |\n| Dark/light mode | Respects system preference or manual toggle |\n| Table of contents | Auto-generated from headings |\n| Mobile responsive | Works on all devices |\n| Offline capable | No server required |\n| Mermaid diagrams | Rendered automatically |\n\n## Command Reference\n\n### `generate` - Create wiki documentation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path/url>` | Repository path or GitHub URL (required) | - |\n| `-o, --output <dir>` | Output directory for wiki | `./wiki` |\n| `-p, --path <path>` | Focus on specific directory | - |\n| `-s, --site` | Generate interactive static site | - |\n| `--site-only` | Generate site only (skip wiki) | - |\n| `--site-title <title>` | Custom site title | Project name |\n| `--theme <theme>` | Site theme: light, dark, auto | `auto` |\n| `-v, --verbose` | Show detailed progress | - |\n| `-e, --estimate` | Estimate time/cost (dry run) | - |\n| `--max-chunks <n>` | Limit indexed chunks (for large repos) | unlimited |\n| `--skip-index` | Use cached embeddings index | - |\n| `--direct-api` | Use Anthropic API directly | - |\n| `-m, --model <model>` | Claude model to use | `claude-sonnet-4-20250514` |\n| `--max-turns <n>` | Limit agent iterations | 200 |\n\n### `continue` - Resume/fix wiki generation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path>` | Repository path (required) | - |\n| `-o, --output <dir>` | Wiki output directory | `./wiki` |\n| `--verify-only` | Only check, don't generate | - |\n| `--skip-index` | Use cached embeddings index | - |\n| `-v, --verbose` | Show detailed progress | - |\n\n## Large Codebase Options\n\nFor repositories with 10,000+ files:\n\n```bash\n# Limit indexed chunks (reduces memory usage)\nted-mosby generate -r ./large-project --max-chunks 5000\n\n# Reduce search results per query\nted-mosby generate -r ./large-project --max-results 5\n\n# Batched processing (for very large repos)\nted-mosby generate -r ./large-project --batch-size 3000\n```\n\n## Typical Runtime\n\n| Codebase Size | Approximate Time |\n|---------------|------------------|\n| Small (<50 files) | 1-2 minutes |\n| Medium (50-200 files) | 2-5 minutes |\n| Large (200+ files) | 5-10 minutes |\n\nUse `--estimate` to get a cost/time estimate before running.\n\n## Troubleshooting\n\n### \"Credit balance is too low\" error\nUse direct API mode:\n```bash\nted-mosby generate -r ./my-project --direct-api\n```\n\n### Out of memory on large repos\nLimit indexed chunks:\n```bash\nted-mosby generate -r ./large-project --max-chunks 5000 --batch-size 3000\n```\n\n### Slow re-runs during development\nSkip re-indexing:\n```bash\nted-mosby generate -r ./my-project --skip-index\n```\n\n### Missing pages / broken links\nUse the continue command:\n```bash\nted-mosby continue -r ./my-project -o ./wiki\n```\n\n## Example Conversation\n\n**User:** \"Can you document this project's architecture?\"\n\n**Assistant:** I'll use Ted Mosby to generate architectural documentation for your project.\n\nFirst, let me verify the prerequisites are in place, then generate the wiki with an interactive site:\n\n```bash\n# Generate wiki with interactive site\nted-mosby generate -r . --site --site-title \"Project Architecture\"\n```\n\nThis will create:\n- `wiki/README.md` - Main navigation\n- `wiki/architecture/overview.md` - Architecture diagrams\n- `wiki/site/index.html` - Interactive documentation site\n\n## Resources\n\n- [Ted Mosby GitHub](https://github.com/your-username/ted-mosby)\n- [Build an Agent Workshop](https://buildanagentworkshop.com)"
              },
              {
                "name": "turn-this-feature-into-a-blog-post",
                "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\".",
                "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
                "frontmatter": {
                  "name": "turn-this-feature-into-a-blog-post",
                  "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\"."
                },
                "content": "# Turn This Feature Into a Blog Post\n\nGenerate a Markdown blog post that explains a code implementation in an engaging, educational way.\n\n## Process\n\n1. **Analyze the implementation** - Read and understand all relevant code files, tracing the feature from entry point to completion\n2. **Identify the narrative** - Find the core problem being solved and why it matters\n3. **Structure the post** - Organize as What ‚Üí Why ‚Üí How (from first principles)\n4. **Write accessibly** - Use friendly, conversational language while maintaining technical authority\n5. **Output Markdown** - Create a complete `.md` file ready for publishing\n\n## Blog Post Structure\n\n### Title\n- Clear, specific, and searchable\n- Format: \"How We Built [Feature]\" or \"Building [Feature]: A Deep Dive\"\n\n### Introduction (2-3 paragraphs)\n- Hook the reader with the problem or outcome\n- Briefly explain what the feature does\n- Preview what readers will learn\n\n### The What (1-2 sections)\n- Describe the feature from the user's perspective\n- Include screenshots or diagrams if applicable\n- Keep technical jargon minimal\n\n### The Why (1-2 sections)\n- Explain the problem this solves\n- Discuss alternatives considered and why this approach won\n- Connect to broader engineering principles\n\n### The How (2-4 sections)\n- Walk through the implementation from first principles\n- Include relevant code snippets with explanations\n- Explain non-obvious decisions\n- Build up complexity gradually\n\n### Conclusion\n- Summarize key takeaways\n- Mention potential future improvements\n- Invite engagement (questions, feedback)\n\n## Writing Style\n\n- **Friendly but authoritative** - Write like a knowledgeable colleague explaining over coffee\n- **First-person plural** - Use \"we\" to create shared ownership\n- **Active voice** - \"We built\" not \"It was built\"\n- **Show, don't just tell** - Use code examples liberally\n- **Explain the \"why\"** - Every code block should have context\n- **Avoid jargon walls** - Define terms on first use\n\n## Code Snippets\n\n- Include only relevant portions, not entire files\n- Add comments for non-obvious lines\n- Use syntax highlighting with language tags\n- Provide context before each snippet\n\n## Output\n\nSave the blog post as a Markdown file with:\n- Kebab-case filename matching the title\n- Frontmatter with title, date, author, and tags (if appropriate for the target platform)\n- Properly formatted headers, code blocks, and lists"
              }
            ]
          },
          {
            "name": "research-skills",
            "description": "Skills for AI/ML research: understanding papers, designing experiments, refining research questions, and developing research taste",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add GhostScientist/skills",
              "/plugin install research-skills@GhostScientist-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-10T22:28:06Z",
              "created_at": "2025-12-08T15:22:00Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "create-watchos-version",
                "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation.",
                "path": "skills/create-watchos-version/SKILL.md",
                "frontmatter": {
                  "name": "create-watchos-version",
                  "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation."
                },
                "content": "# Create watchOS Version\n\nAnalyzes existing Apple platform projects and creates detailed, phased implementation plans for watchOS apps that are elegant, top-tier experiences‚Äînot naive skins of the parent app.\n\n## Workflow\n\n1. **Project Discovery** - Analyze project structure, patterns, architecture\n2. **Feature Mapping** - Identify watchOS-suitable features and priorities\n3. **API Compatibility** - Search web for current watchOS API availability\n4. **Architecture Planning** - Design watchOS-specific architecture\n5. **Plan Generation** - Create phased plan with warnings and alternatives\n6. **User Review** - Present plan for approval before implementation\n\n## Phase 1: Project Discovery\n\nScan project root for:\n\n```\n‚îú‚îÄ‚îÄ App Architecture (SwiftUI, UIKit, AppKit, hybrid)\n‚îú‚îÄ‚îÄ Data Layer (Core Data, SwiftData, Realm, custom)\n‚îú‚îÄ‚îÄ Networking (URLSession, Alamofire, custom)\n‚îú‚îÄ‚îÄ State Management (ObservableObject, TCA, Redux-like)\n‚îú‚îÄ‚îÄ Navigation (NavigationStack, Coordinator)\n‚îú‚îÄ‚îÄ Shared Frameworks (SPM packages, shared targets)\n‚îú‚îÄ‚îÄ Assets (colors, images, SF Symbols)\n‚îú‚îÄ‚îÄ Existing Watch Target (if any)\n‚îî‚îÄ‚îÄ Minimum iOS Version (affects watchOS targeting)\n```\n\nKey files: `*.xcodeproj`, `Package.swift`, `Info.plist`, App entry points, ViewModels, Models.\n\n## Phase 2: Feature Mapping\n\n**Glanceable (High Priority)**: Status displays, counters, progress, recent items, quick stats\n\n**Quick Actions (High Priority)**: Single-tap toggles, shortcuts, haptic confirmations\n\n**Complications/Widgets (Critical)**: Map data to WidgetKit families‚ÄîaccessoryCircular, accessoryRectangular, accessoryInline, accessoryCorner. Consider Smart Stack relevance.\n\n**Background**: HealthKit integration, background refresh, Watch Connectivity sync\n\n**Defer/Exclude**: Complex data entry, long-form content, sustained screen time features\n\n## Phase 3: API Compatibility\n\n**CRITICAL**: Always search web for current watchOS docs before finalizing. APIs change frequently.\n\nSearch: `[FrameworkName] watchOS availability site:developer.apple.com`\n\n### Quick Reference\n\n**Available**: SwiftUI, SwiftData (10+), WidgetKit (9+), HealthKit, WorkoutKit, CoreLocation (limited), WatchConnectivity, CloudKit, CoreMotion, AVFoundation (audio), CoreBluetooth, Combine, Swift Concurrency\n\n**Unavailable/Limited**: UIKit, WebKit, MapKit (limited), CoreImage (limited), ARKit, RealityKit, StoreKit (limited), Background URLSession (limited)\n\nSee `references/api-compatibility.md` for detailed compatibility matrix.\n\n## Phase 4: Architecture\n\n### Version Targeting\n\n```\niOS 16+ ‚Üí watchOS 9+  (WidgetKit complications)\niOS 17+ ‚Üí watchOS 10+ (SwiftData, Smart Stack)\niOS 18+ ‚Üí watchOS 11+ (Live Activities on Watch)\n```\n\n### Structure\n\n```\nShared/\n‚îú‚îÄ‚îÄ Models/           # Pure Swift, shared via target membership\n‚îú‚îÄ‚îÄ Services/         # Platform-agnostic logic\n‚îî‚îÄ‚îÄ Utilities/\n\nWatchApp/\n‚îú‚îÄ‚îÄ App.swift\n‚îú‚îÄ‚îÄ Views/\n‚îú‚îÄ‚îÄ ViewModels/\n‚îú‚îÄ‚îÄ Complications/\n‚îî‚îÄ‚îÄ WatchConnectivity/\n```\n\n### Design Principles\n\n1. **Glanceability** - Visible within 2 seconds\n2. **Minimal Interaction** - 1-3 taps max\n3. **Context Awareness** - Time, location, activity\n4. **Battery Conscious** - Efficient refresh, TimelineSchedule\n5. **Haptic Feedback** - Confirm actions appropriately\n\n### SwiftUI Gotchas\n\n- Avoid nested TabViews (memory leaks)\n- Use TimelineSchedule for efficient metric updates\n- Check `isLuminanceReduced` to reduce work when dimmed\n- Don't use data-driven high-frequency UI refreshes\n\n## Phase 5: Plan Generation\n\nUse template in `references/plan-template.md` to generate:\n\n1. Executive Summary\n2. ‚ö†Ô∏è API Compatibility Warnings table\n3. Phased implementation tasks\n4. Testing checklist\n\n## Phase 6: User Review\n\nPresent plan and ask for approval before implementing:\n\n> \"I've analyzed your project and created a watchOS plan. Before proceeding:\n> 1. **API Warnings**: [N] APIs unavailable‚Äîalternatives documented.\n> 2. **Recommended Features**: [list] prioritized for Watch.\n> 3. **Scope**: [N] phases.\n> \n> Proceed with implementation, or adjust the plan?\"\n\n**Do not implement until user approves.**\n\n## Best Practices Reference\n\n### Watch Connectivity\n\n```swift\nguard WCSession.default.activationState == .activated else { return }\n// sendMessage: immediate, requires reachability\n// transferUserInfo: queued, guaranteed\n// transferCurrentComplicationUserInfo: complication priority\n```\n\n### Complications\n\n```swift\n// Use appropriate reload policy\nTimeline(entries: entries, policy: .after(nextUpdateDate))\n// Use .never for static complications\n```\n\n### Battery Efficiency\n\n- Timeline-based over active refresh\n- Check `isLuminanceReduced`\n- Batch Watch Connectivity transfers\n- Significant location change vs continuous updates"
              },
              {
                "name": "experiment-design-checklist",
                "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates.",
                "path": "skills/experiment-design-checklist/SKILL.md",
                "frontmatter": {
                  "name": "experiment-design-checklist",
                  "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates."
                },
                "content": "# Experiment Design Checklist\n\nPrevent the \"I ran experiments for 3 months and they're meaningless\" disaster through rigorous upfront design.\n\n## The Core Principle\n\nBefore running ANY experiment, you should be able to answer:\n1. What specific claim will this experiment support or refute?\n2. What would convince a skeptical reviewer?\n3. What could go wrong that would invalidate the results?\n\n## Process\n\n### Step 1: State the Hypothesis Precisely\n\nConvert your research question into falsifiable predictions:\n\n**Template:**\n```\nIf [intervention/method], then [measurable outcome], because [mechanism].\n```\n\n**Examples:**\n- \"If we add auxiliary contrastive loss, then downstream task accuracy increases by >2%, because representations become more separable.\"\n- \"If we use learned positional encodings, then performance on sequences >4096 tokens improves, because the model can extrapolate beyond training length.\"\n\n**Null hypothesis:** What does \"no effect\" look like? This is what you're trying to reject.\n\n### Step 2: Identify Variables\n\n**Independent Variables (what you manipulate):**\n| Variable | Levels | Rationale |\n|----------|--------|-----------|\n| [Var 1] | [Level A, B, C] | [Why these levels] |\n\n**Dependent Variables (what you measure):**\n| Metric | How Measured | Why This Metric |\n|--------|--------------|-----------------|\n| [Metric 1] | [Procedure] | [Justification] |\n\n**Control Variables (what you hold constant):**\n| Variable | Fixed Value | Why Fixed |\n|----------|-------------|-----------|\n| [Var 1] | [Value] | [Prevents confound X] |\n\n### Step 3: Choose Baselines\n\nEvery experiment needs comparisons. No result is meaningful in isolation.\n\n**Baseline Hierarchy:**\n\n1. **Random/Trivial Baseline**\n   - What does random chance achieve?\n   - Sanity check that the task isn't trivial\n\n2. **Simple Baseline**\n   - Simplest reasonable approach\n   - Often embarrassingly effective\n\n3. **Standard Baseline**\n   - Well-known method from literature\n   - Apples-to-apples comparison\n\n4. **State-of-the-Art Baseline**\n   - Current best published result\n   - Only if you're claiming SOTA\n\n5. **Ablated Self**\n   - Your method minus key components\n   - Shows each component contributes\n\n**For each baseline, document:**\n- Source (paper, implementation)\n- Hyperparameters used\n- Whether you re-ran or used reported numbers\n- Any modifications made\n\n### Step 4: Design Ablations\n\nAblations answer: \"Is each component necessary?\"\n\n**Ablation Template:**\n| Variant | What's Removed/Changed | Expected Effect | If No Effect... |\n|---------|----------------------|-----------------|-----------------|\n| Full Model | Nothing | Best performance | - |\n| w/o Component A | Remove A | Performance drops X% | A isn't helping |\n| w/o Component B | Remove B | Performance drops Y% | B isn't helping |\n| Component A only | Only A, no B | Shows A's isolated contribution | - |\n\n**Good ablations are:**\n- Surgical (one change at a time)\n- Interpretable (clear what was changed)\n- Informative (result tells you something)\n\n### Step 5: Address Confounds\n\nThings that could explain your results OTHER than your hypothesis:\n\n**Common Confounds:**\n\n| Confound | How to Check | How to Control |\n|----------|--------------|----------------|\n| Hyperparameter tuning advantage | Same tuning budget for all | Report tuning procedure |\n| Compute advantage | Matched FLOPs/params | Report compute used |\n| Data leakage | Check train/test overlap | Strict separation |\n| Random seed luck | Multiple seeds | Report variance |\n| Implementation bugs (baseline) | Verify baseline numbers | Use official implementations |\n| Cherry-picked examples | Random or systematic selection | Pre-register selection criteria |\n\n### Step 6: Statistical Rigor\n\n**Sample Size:**\n- How many random seeds? (Minimum: 3, better: 5+)\n- How many data splits? (If applicable)\n- Power analysis: Can you detect expected effect size?\n\n**What to Report:**\n- Mean ¬± standard deviation (or standard error)\n- Confidence intervals where appropriate\n- Statistical significance tests if claiming \"better\"\n\n**Appropriate Tests:**\n| Comparison | Test | Assumptions |\n|------------|------|-------------|\n| Two methods, normal data | t-test | Normality, equal variance |\n| Two methods, unknown dist | Mann-Whitney U | Ordinal data |\n| Multiple methods | ANOVA + post-hoc | Normality |\n| Multiple methods, unknown | Kruskal-Wallis | Ordinal data |\n| Paired comparisons | Wilcoxon signed-rank | Same test instances |\n\n**Avoid:**\n- p-hacking (running until significant)\n- Multiple comparison problems (Bonferroni correct)\n- Reporting only favorable metrics\n\n### Step 7: Compute Budget\n\nBefore running, estimate:\n\n| Component | Estimate | Notes |\n|-----------|----------|-------|\n| Single training run | X GPU-hours | [Details] |\n| Hyperparameter search | Y runs √ó X hours | [Search strategy] |\n| Baselines | Z runs √ó W hours | [Which baselines] |\n| Ablations | N variants √ó X hours | [Which ablations] |\n| Seeds | M seeds √ó above | [How many seeds] |\n| **Total** | **T GPU-hours** | Buffer: 1.5-2x |\n\n**Go/No-Go Decision:** Is this feasible with available resources?\n\n### Step 8: Pre-Registration (Optional but Recommended)\n\nWrite down BEFORE running:\n- Exact hypotheses\n- Primary metrics (not chosen post-hoc)\n- Analysis plan\n- What would constitute \"success\"\n\nThis prevents unconscious goal-post moving.\n\n## Output: Experiment Design Document\n\n```markdown\n# Experiment Design: [Title]\n\n## Hypothesis\n[Precise statement]\n\n## Variables\n### Independent\n[Table]\n\n### Dependent\n[Table]\n\n### Controls\n[Table]\n\n## Baselines\n1. [Baseline 1]: [Source, details]\n2. [Baseline 2]: [Source, details]\n\n## Ablations\n[Table]\n\n## Confound Mitigation\n[Table]\n\n## Statistical Plan\n- Seeds: [N]\n- Tests: [Which tests for which comparisons]\n- Significance threshold: [Œ± level]\n\n## Compute Budget\n[Table with total estimate]\n\n## Success Criteria\n- Primary: [What must be true]\n- Secondary: [Nice to have]\n\n## Timeline\n- Phase 1: [What, when]\n- Phase 2: [What, when]\n\n## Known Risks\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n```\n\n## Red Flags in Experiment Design\n\nüö© \"We'll figure out the metrics later\"\nüö© \"One run should be enough\"\nüö© \"We don't need baselines, it's obviously better\"\nüö© \"Let's just see what happens\"\nüö© \"We can always run more if it's not significant\"\nüö© No compute estimate before starting\nüö© Vague success criteria"
              },
              {
                "name": "hugging-face-space-deployer",
                "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons.",
                "path": "skills/hugging-face-space-deployer/SKILL.md",
                "frontmatter": {
                  "name": "hugging-face-space-deployer",
                  "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons."
                },
                "content": "# Hugging Face Space Deployer\n\nA skill for AI engineers to create, configure, and deploy interactive ML demos on Hugging Face Spaces.\n\n## CRITICAL: Pre-Deployment Checklist\n\n**Before writing ANY code, gather this information about the model:**\n\n### 1. Check Model Type (LoRA Adapter vs Full Model)\n\n**Use the HF MCP tool to inspect the model files:**\n```\nhf-skills - Hub Repo Details (repo_ids: [\"username/model\"], repo_type: \"model\")\n```\n\n**Look for these indicators:**\n\n| Files Present | Model Type | Action Required |\n|---------------|------------|-----------------|\n| `model.safetensors` or `pytorch_model.bin` | Full model | Load directly with `AutoModelForCausalLM` |\n| `adapter_model.safetensors` + `adapter_config.json` | LoRA/PEFT adapter | Must load base model first, then apply adapter with `peft` |\n| Only config files, no weights | Broken/incomplete | Ask user to verify |\n\n**If adapter_config.json exists, check for `base_model_name_or_path` to identify the base model.**\n\n### 2. Check Inference API Availability\n\nVisit the model page on HF Hub and look for \"Inference Providers\" widget on the right side.\n\n**Indicators that model HAS Inference API:**\n- Inference widget visible on model page\n- Model from known provider: `meta-llama`, `mistralai`, `HuggingFaceH4`, `google`, `stabilityai`, `Qwen`\n- High download count (>10,000) with standard architecture\n\n**Indicators that model DOES NOT have Inference API:**\n- Personal namespace (e.g., `GhostScientist/my-model`)\n- LoRA/PEFT adapter (adapters never have direct Inference API)\n- Missing `pipeline_tag` in model metadata\n- No inference widget on model page\n\n### 3. Check Model Metadata\n\n- Ensure `pipeline_tag` is set (e.g., `text-generation`)\n- Add `conversational` tag for chat models\n\n### 4. Determine Hardware Needs\n\n| Model Size | Recommended Hardware |\n|------------|---------------------|\n| < 3B parameters | ZeroGPU (free) or CPU |\n| 3B - 7B parameters | ZeroGPU or T4 |\n| > 7B parameters | A10G or A100 |\n\n### 5. Ask User If Unclear\n\n**If you cannot determine the model type, ASK THE USER:**\n\n> \"I'm analyzing your model to determine the best deployment strategy. I found:\n> - [what you found about files]\n> - [what you found about inference API]\n>\n> Is this model:\n> 1. A full model you trained/uploaded?\n> 2. A LoRA/PEFT adapter on top of another model?\n> 3. Something else?\n>\n> Also, would you prefer:\n> A. Free deployment with ZeroGPU (may have queue times)\n> B. Paid GPU for faster response (~$0.60/hr)\"\n\n## Hardware Options\n\n| Hardware | Use Case | Cost |\n|----------|----------|------|\n| `cpu-basic` | Simple demos, Inference API apps | Free |\n| `cpu-upgrade` | Faster CPU inference | ~$0.03/hr |\n| **`zero-a10g`** | **Models needing GPU on-demand (recommended for most)** | **Free (with quota)** |\n| `t4-small` | Small GPU models (<7B) | ~$0.60/hr |\n| `t4-medium` | Medium GPU models | ~$0.90/hr |\n| `a10g-small` | Large models (7B-13B) | ~$1.50/hr |\n| `a10g-large` | Very large models (30B+) | ~$3.15/hr |\n| `a100-large` | Largest models | ~$4.50/hr |\n\n**ZeroGPU Note:** ZeroGPU (`zero-a10g`) provides free GPU access on-demand. The Space runs on CPU, and when a user triggers inference, a GPU is allocated temporarily (~60-120 seconds). **After deployment, you must manually set the runtime to \"ZeroGPU\" in Space Settings > Hardware.**\n\n## Deployment Decision Tree\n\n```\nAnalyze Model\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have adapter_config.json?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a LoRA adapter\n‚îÇ       ‚îú‚îÄ‚îÄ Find base_model_name_or_path in adapter_config.json\n‚îÇ       ‚îî‚îÄ‚îÄ Use Template 3 (LoRA + ZeroGPU)\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have model.safetensors or pytorch_model.bin?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a full model\n‚îÇ       ‚îú‚îÄ‚îÄ Is it from a major provider with inference widget?\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Use Inference API (Template 1)\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Use ZeroGPU (Template 2)\n‚îÇ\n‚îî‚îÄ‚îÄ Neither found?\n    ‚îî‚îÄ‚îÄ ASK USER - model may be incomplete\n```\n\n## Dependencies\n\n**For Inference API (cpu-basic, free):**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**For ZeroGPU full models (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**For ZeroGPU LoRA adapters (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n## CLI Commands (CORRECT Syntax)\n\n```bash\n# Create Space\nhf repo create my-space-name --repo-type space --space-sdk gradio\n\n# Upload files\nhf upload username/space-name ./local-folder --repo-type space\n\n# Download model files to inspect\nhf download username/model-name --local-dir ./model-check --dry-run\n\n# Check what files exist in a model\nhf download username/model-name --local-dir /tmp/check --dry-run 2>&1 | grep -E '\\.(safetensors|bin|json)'\n```\n\n## Template 1: Inference API (For Supported Models)\n\n**Use when:** Model has inference widget, is from major provider, or explicitly supports serverless API.\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nMODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"  # Must support Inference API!\nclient = InferenceClient(MODEL_ID)\n\ndef respond(message, history, system_message, max_tokens, temperature, top_p):\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    response = \"\"\n    for token in client.chat_completion(\n        messages,\n        max_tokens=max_tokens,\n        stream=True,\n        temperature=temperature,\n        top_p=top_p,\n    ):\n        delta = token.choices[0].delta.content or \"\"\n        response += delta\n        yield response\n\ndemo = gr.ChatInterface(\n    respond,\n    title=\"Chat Assistant\",\n    description=\"Powered by Hugging Face Inference API\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\"),\n        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Write a Python function to sort a list\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Chat App\nemoji: üí¨\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n```\n\n## Template 2: ZeroGPU Full Model (For Models Without Inference API)\n\n**Use when:** Full model (has model.safetensors) but no Inference API support.\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"username/my-full-model\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Global model - loaded lazily on first GPU call for faster Space startup\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Model\",\n    description=\"Powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me write some code\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Model\nemoji: ü§ñ\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Template 3: ZeroGPU LoRA Adapter (CRITICAL FOR FINE-TUNED MODELS)\n\n**Use when:** Model has `adapter_config.json` and `adapter_model.safetensors` (NOT `model.safetensors`)\n\n**You MUST identify the base model from `adapter_config.json` field `base_model_name_or_path`**\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Your LoRA adapter\nADAPTER_ID = \"username/my-lora-adapter\"\n# Base model (from adapter_config.json -> base_model_name_or_path)\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Global model - loaded lazily on first GPU call\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            BASE_MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n        model = model.merge_and_unload()  # Merge for faster inference\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for item in history:\n        if isinstance(item, (list, tuple)) and len(item) == 2:\n            user_msg, assistant_msg = item\n            if user_msg:\n                messages.append({\"role\": \"user\", \"content\": user_msg})\n            if assistant_msg:\n                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Fine-Tuned Model\",\n    description=\"LoRA fine-tuned model powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me with a coding task\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt (MUST include peft):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Fine-Tuned Model\nemoji: üîß\ncolorFrom: green\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Post-Deployment Steps\n\n**After uploading your Space files:**\n\n### 1. Set the Runtime Hardware (REQUIRED for GPU models)\n\n- Go to: `https://huggingface.co/spaces/USERNAME/SPACE_NAME/settings`\n- Under \"Space Hardware\", select the appropriate option:\n  - **ZeroGPU** for free on-demand GPU (recommended)\n  - Or a dedicated GPU tier if needed\n\n### 2. Verify the Space is Running\n\n- Check the Space URL for any build errors\n- Review container logs in Settings if issues occur\n\n### 3. Common Post-Deploy Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| \"No API found\" error | Hardware mismatch | Set runtime to ZeroGPU in Settings |\n| Model not loading | LoRA vs full model confusion | Check if it's an adapter, use correct template |\n| Inference API errors | Model not on serverless | Load directly with transformers instead |\n\n## Detecting Model Type - Quick Reference\n\n### Full Model\nFiles include: `model.safetensors`, `pytorch_model.bin`, or sharded versions\n```python\n# Can load directly\nmodel = AutoModelForCausalLM.from_pretrained(\"username/model\")\n```\n\n### LoRA/PEFT Adapter\nFiles include: `adapter_config.json`, `adapter_model.safetensors`\n```python\n# Must load base model first, then apply adapter\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model-id\")\nmodel = PeftModel.from_pretrained(base_model, \"username/adapter\")\nmodel = model.merge_and_unload()  # Optional: merge for faster inference\n```\n\n### Inference API Available\nModel page shows \"Inference Providers\" widget on the right side\n```python\n# Can use InferenceClient (simplest approach)\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\"username/model\")\n```\n\n## Fixing Missing pipeline_tag (To Enable Inference API)\n\nIf a model doesn't have an inference widget but should, it may be missing metadata:\n\n```bash\n# Download the README\nhf download username/model-name README.md --local-dir /tmp/fix\n\n# Edit to add pipeline_tag in YAML frontmatter:\n# ---\n# pipeline_tag: text-generation\n# tags:\n# - conversational\n# ---\n\n# Upload the fix\nhf upload username/model-name /tmp/fix/README.md README.md\n```\n\n**Note:** Even with correct tags, custom models may not get Inference API - it depends on HF's infrastructure decisions.\n\n## CRITICAL: Gradio 5.x Requirements\n\n### Examples Format (MUST be nested lists)\n```python\n# CORRECT:\nexamples=[\n    [\"Example 1\"],\n    [\"Example 2\"],\n]\n\n# WRONG (causes ValueError):\nexamples=[\n    \"Example 1\",\n    \"Example 2\",\n]\n```\n\n### Version Requirements\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\nDo NOT use `gradio==4.44.0` - causes `ImportError: cannot import name 'HfFolder'`\n\n## Troubleshooting\n\n### \"No API found\" Error\n**Cause:** Gradio app isn't exposing API correctly, often due to hardware mismatch\n**Fix:** Go to Space Settings and set runtime to \"ZeroGPU\" or appropriate GPU tier\n\n### \"OSError: does not appear to have a file named pytorch_model.bin, model.safetensors\"\n**Cause:** Trying to load a LoRA adapter as a full model\n**Fix:** Check for `adapter_config.json` - if present, use PEFT to load:\n```python\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\nmodel = PeftModel.from_pretrained(base_model, \"adapter-id\")\n```\n\n### Inference API Not Available\n**Cause:** Model doesn't have pipeline_tag or isn't deployed to serverless\n**Fix:** Either:\n  a. Add `pipeline_tag: text-generation` to model's README.md\n  b. Or load model directly with transformers instead of InferenceClient\n\n### `ImportError: cannot import name 'HfFolder'`\n**Cause:** gradio/huggingface_hub version mismatch\n**Fix:** Use `gradio>=5.0.0` and `huggingface_hub>=0.26.0`\n\n### `ValueError: examples must be nested list`\n**Cause:** Gradio 5.x format change\n**Fix:** Use `[[\"ex1\"], [\"ex2\"]]` not `[\"ex1\", \"ex2\"]`\n\n### Space builds but model doesn't load\n**Cause:** Missing `peft` for adapters, or wrong base model\n**Fix:** Check adapter_config.json for correct base_model_name_or_path\n\n## Workflow Summary\n\n1. **Analyze model** (check for adapter_config.json, model files, inference widget)\n2. **Determine strategy** (Inference API vs ZeroGPU, full model vs LoRA)\n3. **Ask user if unclear** about model type or cost preferences\n4. **Generate correct template** based on analysis\n5. **Create Space** with correct requirements and README\n6. **Upload files** using `hf upload`\n7. **Set hardware** in Space Settings (ZeroGPU for free GPU access)\n8. **Monitor build logs** for any issues"
              },
              {
                "name": "implement-paper-from-scratch",
                "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions.",
                "path": "skills/implement-paper-from-scratch/SKILL.md",
                "frontmatter": {
                  "name": "implement-paper-from-scratch",
                  "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions."
                },
                "content": "# Implement Paper From Scratch\n\nThe best way to truly understand a paper is to implement it. This skill guides you through that process methodically.\n\n## Philosophy\n\n- **No copy-pasting from reference implementations** - We build understanding, not just working code\n- **Checkpoint questions verify understanding** - You should be able to answer \"why\" at each step\n- **Minimal dependencies** - Use NumPy/PyTorch fundamentals, not high-level wrappers\n- **Deliberate debugging** - Bugs are learning opportunities, not obstacles\n\n## Process\n\n### Phase 1: Pre-Implementation Analysis\n\nBefore writing any code:\n\n1. **Identify the core algorithm** - Strip away ablations, extensions, bells and whistles. What's the minimal version?\n\n2. **List the components** - Break into modules:\n   - Data pipeline\n   - Model architecture\n   - Loss function(s)\n   - Training loop\n   - Evaluation metrics\n\n3. **Find the tricky parts** - What's non-obvious?\n   - Custom layers or operations\n   - Numerical stability concerns\n   - Hyperparameter sensitivity\n   - Implementation details buried in appendices\n\n4. **Gather reference numbers** - What should we expect?\n   - Training loss trajectory\n   - Validation metrics at convergence\n   - Compute requirements (if stated)\n\n### Phase 2: Scaffolded Implementation\n\nBuild up the implementation in this order:\n\n#### Step 1: Data\n```python\n# Start with synthetic/toy data\n# Verify shapes and types before touching real data\n```\n\n**Checkpoint:** Can you describe what each tensor represents and its expected shape?\n\n#### Step 2: Model Architecture\n```python\n# Build layer by layer\n# Print shapes at each stage\n# Verify parameter counts match paper\n```\n\n**Checkpoint:** If you randomly initialize and do a forward pass, do the output shapes match what the paper describes?\n\n#### Step 3: Loss Function\n```python\n# Implement exactly as described\n# Test with known inputs/outputs\n# Check gradient flow\n```\n\n**Checkpoint:** Can you explain each term in the loss and why it's there?\n\n#### Step 4: Training Loop\n```python\n# Minimal loop first (no logging, checkpointing, etc.)\n# Verify loss decreases on tiny overfit test\n# Then add bells and whistles\n```\n\n**Checkpoint:** Can you overfit a single batch? If not, something is broken.\n\n#### Step 5: Evaluation\n```python\n# Implement paper's exact metrics\n# Compare against reported numbers\n```\n\n**Checkpoint:** On the same data split, how close are you to paper's numbers?\n\n### Phase 3: The Debugging Gauntlet\n\nWhen it doesn't work (and it won't at first):\n\n1. **The Overfit Test**\n   - Can you memorize 1 example? 10? 100?\n   - If not, architecture or gradient bug\n\n2. **The Gradient Check**\n   - Are gradients flowing to all parameters?\n   - Any NaN or exploding gradients?\n\n3. **The Initialization Check**\n   - Match paper's initialization exactly\n   - This matters more than people think\n\n4. **The Learning Rate Sweep**\n   - Log scale: 1e-5 to 1e-1\n   - Loss should decrease for some range\n\n5. **The Ablation Debug**\n   - Remove components until it works\n   - Add back one at a time\n\n### Phase 4: Checkpoint Questions\n\nAt each stage, you should be able to answer:\n\n**Understanding:**\n- Why does this component exist?\n- What would happen without it?\n- What alternatives were considered?\n\n**Implementation:**\n- Why this specific implementation choice?\n- Where could numerical issues arise?\n- What's the computational complexity?\n\n**Debugging:**\n- What would it look like if this was broken?\n- How would you test this in isolation?\n- What are the most likely bugs?\n\n## Output Format\n\nFor each implementation session, provide:\n\n```markdown\n## Today's Implementation Goal\n[Specific component we're building]\n\n## Prerequisites Check\n- [ ] Previous components working\n- [ ] Understand what we're building\n- [ ] Know expected behavior\n\n## Implementation\n\n### Code\n[Code blocks with extensive comments]\n\n### Checkpoint Questions\n1. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n2. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n### Verification Steps\n- [ ] Test 1: [What to check]\n- [ ] Test 2: [What to check]\n\n### Common Bugs at This Stage\n1. [Bug pattern]: [How to identify and fix]\n\n## What's Next\n[Preview of next component and how it connects]\n```\n\n## Tips for Specific Paper Types\n\n### Transformer-based\n- Attention mask shapes are the #1 bug source\n- Verify positional encoding is applied correctly\n- Check layer norm placement (pre vs post)\n\n### RL/Policy Gradient\n- Sign errors in policy gradient are silent killers\n- Advantage normalization matters\n- Verify discount factor handling\n\n### Generative Models\n- KL term balancing is finicky\n- Check latent space distribution\n- Verify reconstruction looks reasonable before training\n\n### Computer Vision\n- Normalization (ImageNet stats, batch norm) is crucial\n- Data augmentation can make or break results\n- Verify input preprocessing matches paper exactly\n\n## Success Criteria\n\nYou're done when:\n\n1. **Numbers match** - Within reasonable variance of paper's results\n2. **Understanding is deep** - You can explain every line of code\n3. **You found the gotchas** - You know what breaks and why\n4. **You could modify it** - Confident to try your own variations\n\n## Anti-Patterns to Avoid\n\n- ‚ùå Copying code you don't understand\n- ‚ùå Skipping checkpoint questions\n- ‚ùå Using pre-built components for core algorithm\n- ‚ùå Ignoring discrepancies with paper\n- ‚ùå Moving on before current step works"
              },
              {
                "name": "ios-app-icon-generator",
                "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons.",
                "path": "skills/ios-app-icon-generator/SKILL.md",
                "frontmatter": {
                  "name": "ios-app-icon-generator",
                  "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons."
                },
                "content": "# iOS App Icon Generator\n\nCreate beautiful, production-ready iOS app icons through a two-phase creative process.\n\n## Phase 1: Visual Philosophy\n\nBefore drawing anything, develop a 2-3 paragraph **Icon Philosophy** that articulates:\n\n- **Core concept**: What single idea or feeling should the icon convey?\n- **Visual metaphor**: What shape, object, or abstraction represents the app's purpose?\n- **Color psychology**: What palette evokes the right emotional response?\n- **Silhouette test**: Will it be recognizable as a tiny black shape?\n\nWrite this philosophy out. It guides every design decision.\n\n### Design Principles\n\nIcons that work follow these rules:\n\n- **Simplicity**: One focal element. No more than 2-3 colors. No text (illegible at small sizes).\n- **Distinctiveness**: Must stand out in a grid of 30 other icons. Avoid generic symbols (gears, checkmarks, clouds).\n- **Scalability**: The 16x16 notification icon must read as clearly as the 1024x1024 App Store version.\n- **No photography**: Apple's guidelines discourage photos. Use illustration, geometry, or abstract forms.\n- **Optical balance**: Center of visual weight, not geometric center. Curves feel heavier than straight lines.\n\n## Phase 2: Icon Generation\n\nGenerate the icon as a **self-contained HTML file** with embedded SVG that:\n\n1. Renders the icon design at 1024x1024 (the master size)\n2. Includes iOS-style rounded corners (superellipse, not CSS border-radius)\n3. Shows a preview grid of all sizes to verify readability\n4. Provides a download mechanism for each size\n\n### Required Sizes\n\nGenerate all iOS app icon sizes:\n\n| Size | Purpose |\n|------|---------|\n| 1024x1024 | App Store |\n| 180x180 | iPhone (@3x) |\n| 167x167 | iPad Pro (@2x) |\n| 152x152 | iPad (@2x) |\n| 120x120 | iPhone (@2x) |\n| 87x87 | Spotlight (@3x) |\n| 80x80 | Spotlight (@2x) |\n| 76x76 | iPad (@1x) |\n| 60x60 | iPhone (@1x) |\n| 58x58 | Settings (@2x) |\n| 40x40 | Spotlight (@1x) |\n| 29x29 | Settings (@1x) |\n| 20x20 | Notification (@1x) |\n\n### HTML Artifact Structure\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>App Icon: [Name]</title>\n  <style>\n    /* Dark interface, icon grid layout, download buttons */\n  </style>\n</head>\n<body>\n  <!-- Philosophy statement -->\n  <!-- Master SVG at 1024x1024 -->\n  <!-- Preview grid showing all sizes -->\n  <!-- Download buttons (use canvas to convert SVG ‚Üí PNG) -->\n  <script>\n    // SVG ‚Üí Canvas ‚Üí PNG download logic\n  </script>\n</body>\n</html>\n```\n\n### SVG Guidelines\n\n- Use `viewBox=\"0 0 1024 1024\"` for the master\n- Apply the iOS squircle mask (superellipse with n‚âà5)\n- Use gradients sparingly but effectively\n- Ensure googd stroke widths scale proportionally\n- Test: zoom browser to 25% - is the icon still clear?\n\n### iOS Squircle Mask\n\nThe iOS icon shape is NOT a rounded rectangle. Use this superellipse path or approximate with:\n\n```svg\n<clipPath id=\"ios-squircle\">\n  <path d=\"M512,1024 C252,1024 0,772 0,512 C0,252 252,0 512,0 C772,0 1024,252 1024,512 C1024,772 772,1024 512,1024 Z\" />\n</clipPath>\n```\n\nOr generate programmatically with the superellipse formula: `|x/a|^n + |y/b|^n = 1` where n ‚âà 5.\n\n## Process\n\n1. Ask about the app's purpose, name, and any existing brand colors\n2. Write the Icon Philosophy\n3. Describe 2-3 concept directions with rationale\n4. Get user approval on a direction\n5. Generate the HTML artifact with full icon set\n6. Iterate based on feedback\n\n## Quality Bar\n\nThe output should look like it belongs on a top-10 App Store chart. Every icon in that grid was crafted by a professional designer - yours should be indistinguishable from theirs.\n\nAvoid:\n- Glossy/skeuomorphic styles (outdated since iOS 7)\n- Thin hairline details (disappear at small sizes)\n- Overly complex illustrations\n- Generic clip-art aesthetics\n- Centered-circle-on-gradient laziness"
              },
              {
                "name": "paper-to-intuition",
                "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams.",
                "path": "skills/paper-to-intuition/SKILL.md",
                "frontmatter": {
                  "name": "paper-to-intuition",
                  "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams."
                },
                "content": "# Paper to Intuition\n\nTransform dense academic papers into genuine understanding through layered explanation and visual intuition.\n\n## Process\n\n1. **Get the paper** - Ask for the arXiv link, PDF, or paper title\n2. **Extract the core** - Identify the single key insight (one sentence)\n3. **Build the ladder** - Create explanations at 4 levels\n4. **Visualize intuition** - Generate interactive diagrams\n5. **Stress test understanding** - \"What breaks if we remove X?\"\n\n## The Explanation Ladder\n\nGenerate explanations at each level, with each building on the last:\n\n### Level 1: ELI5 (1 paragraph)\n- No jargon, no equations\n- Use familiar analogies from everyday life\n- A curious 10-year-old should roughly get it\n\n### Level 2: Undergraduate (2-3 paragraphs)\n- Assume calculus, basic linear algebra, intro ML\n- Introduce key terms with definitions\n- Connect to textbook concepts they'd know\n\n### Level 3: Graduate (3-4 paragraphs)\n- Assume ML fundamentals, optimization, probability\n- Discuss relationship to prior work\n- Explain why naive approaches don't work\n- Cover the key equations with plain-English annotations\n\n### Level 4: Researcher (2-3 paragraphs)\n- Assume field expertise\n- Subtle technical contributions\n- Limitations and open questions\n- How this changes what's possible\n\n## Key Equations Breakdown\n\nFor each important equation:\n\n```\n[Equation in LaTeX]\n\nIn words: [Plain English translation]\n\nEach term:\n- [symbol]: [what it represents] [why it's there]\n\nIntuition: [Why this mathematical form? What would change if we used a different form?]\n```\n\n## Visual Intuition Artifact\n\nGenerate a self-contained HTML file with:\n\n- **Architecture diagram** - Boxes and arrows showing information flow\n- **Interactive sliders** - Manipulate key parameters, see effects\n- **Before/after comparisons** - What the method improves over baselines\n- **Failure case visualization** - When and why it breaks down\n\nUse SVG for diagrams, vanilla JavaScript for interactivity. Dark theme, clean typography.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>[Paper Name] - Visual Intuition</title>\n  <style>\n    :root { --bg: #1a1a2e; --text: #eee; --accent: #4f8cff; }\n    /* Clean, research-aesthetic styling */\n  </style>\n</head>\n<body>\n  <h1>[Paper Title]</h1>\n  <p class=\"tldr\">[One-sentence insight]</p>\n\n  <section id=\"architecture\">\n    <svg><!-- Information flow diagram --></svg>\n  </section>\n\n  <section id=\"interactive\">\n    <!-- Parameter sliders with live updates -->\n  </section>\n\n  <section id=\"comparisons\">\n    <!-- Before/after, ablations -->\n  </section>\n</body>\n</html>\n```\n\n## The \"What Breaks?\" Analysis\n\nFor each major component, explain:\n\n1. **What it does** - The role this component plays\n2. **What breaks without it** - Concrete failure mode\n3. **Why this solution** - Alternatives considered, why this won\n4. **The tradeoff** - What we pay for this choice (compute, complexity, assumptions)\n\n## Output Structure\n\nDeliver as a structured document:\n\n```markdown\n# [Paper Title]\n\n**TL;DR:** [One sentence]\n\n**Why it matters:** [One paragraph on significance]\n\n## The Explanation Ladder\n\n### ELI5\n[...]\n\n### Undergraduate Level\n[...]\n\n### Graduate Level\n[...]\n\n### Researcher Level\n[...]\n\n## Key Equations\n\n### Equation 1: [Name]\n[Breakdown as specified above]\n\n## What Breaks If We Remove...\n\n### [Component 1]\n[Analysis]\n\n### [Component 2]\n[Analysis]\n\n## Visual Intuition\n\n[Link to or embed HTML artifact]\n\n## Further Reading\n\n- [Prerequisite paper 1]\n- [Follow-up work 1]\n```\n\n## Quality Standards\n\n- Every analogy must be accurate, not just catchy\n- Equations must be explained, not just translated\n- Visuals must reveal structure, not just decorate\n- The researcher-level section should contain insight, not just summary\n- Admit when something is genuinely confusing or poorly explained in the original paper"
              },
              {
                "name": "research-question-refiner",
                "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis.",
                "path": "skills/research-question-refiner/SKILL.md",
                "frontmatter": {
                  "name": "research-question-refiner",
                  "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis."
                },
                "content": "# Research Question Refiner\n\nTransform \"I'm interested in X\" into \"I will investigate whether Y under conditions Z, measuring W.\"\n\n## The Problem\n\nMost research ideas fail not because they're bad, but because they're:\n- Too vague to act on\n- Too ambitious to complete\n- Too incremental to matter\n- Missing a clear success criterion\n\nThis skill fixes that.\n\n## Process\n\n### Stage 1: Excavate the Interest\n\nStart by understanding what's actually pulling at you:\n\n**Questions to ask:**\n1. What sparked this interest? (Paper, conversation, problem you encountered?)\n2. What's the version that excites you most?\n3. What would be cool if it worked?\n4. Who would care about the answer?\n\n**Output:** A paragraph capturing the raw interest, unfiltered.\n\n### Stage 2: Map the Territory\n\nBefore scoping, understand the landscape:\n\n**What's Known:**\n- What's the current state-of-the-art?\n- What are the established approaches?\n- What have people tried that didn't work?\n\n**What's Unknown:**\n- What are the acknowledged open problems?\n- What assumptions does current work make?\n- Where do methods fail?\n\n**What's Controversial:**\n- Where do researchers disagree?\n- What's claimed but not convincingly shown?\n- What's believed but not rigorously tested?\n\n**Output:** A structured map with citations/references for each area.\n\n### Stage 3: Find the Gap\n\nA good research question lives in a gap that is:\n\n| Property | Too Little | Just Right | Too Much |\n|----------|-----------|------------|----------|\n| **Novelty** | Redoing existing work | New angle or combination | No foundation to build on |\n| **Difficulty** | Trivial to answer | Challenging but doable | Requires breakthroughs |\n| **Impact** | No one cares | Community would update beliefs | Nobel prize (unrealistic) |\n| **Scope** | One experiment | Thesis chapter / paper | Multiple PhDs |\n\n**Gap-finding questions:**\n- What would change if we relaxed assumption X?\n- What if we applied method A to domain B?\n- What's between approach X and approach Y?\n- What fails in setting Z that works elsewhere?\n\n**Output:** 3-5 candidate gaps, each as one sentence.\n\n### Stage 4: Refine to Concrete Question\n\nFor each candidate gap, sharpen into a question:\n\n**The Formula:**\n```\n[Action verb] + [specific phenomenon] + [under conditions] + [measurable outcome]\n```\n\n**Examples of refinement:**\n\n‚ùå Vague: \"How can we make transformers more efficient?\"\n‚úÖ Concrete: \"Does structured sparsity in attention patterns preserve performance on long-context tasks while reducing compute by >50%?\"\n\n‚ùå Vague: \"Can robots learn from humans better?\"\n‚úÖ Concrete: \"Does incorporating gaze direction in demonstrations improve sample efficiency for manipulation tasks compared to kinesthetic teaching alone?\"\n\n‚ùå Vague: \"What makes language models hallucinate?\"\n‚úÖ Concrete: \"Do retrieval-augmented models hallucinate less on factual questions when retrieval confidence is used to modulate generation temperature?\"\n\n### Stage 5: Feasibility Check\n\nFor each refined question, assess:\n\n**Resources Required:**\n- Compute: GPU-hours estimate\n- Data: Available or needs collection?\n- Time: Weeks/months realistically\n- Expertise: What skills are needed?\n\n**Risk Assessment:**\n- What's the probability this works at all?\n- What if the hypothesis is wrong? (Is negative result publishable?)\n- What could go wrong technically?\n- What could invalidate the whole direction?\n\n**Dependencies:**\n- Does this require other work to finish first?\n- Are there rate-limiting steps?\n- What can be parallelized?\n\n### Stage 6: The Litmus Tests\n\nA good research question passes all of these:\n\n**The Advisor Test:**\n> \"If I pitched this in 2 minutes, would a busy professor say 'yes, go do that' rather than 'hmm, let's talk more'?\"\n\n**The Paper Test:**\n> \"Can I envision the title, abstract, and figure 1 of the resulting paper?\"\n\n**The Null Result Test:**\n> \"If my hypothesis is wrong, would that still be interesting to report?\"\n\n**The Motivation Test:**\n> \"Am I actually excited to work on this for 6+ months?\"\n\n**The Explanation Test:**\n> \"Can I explain why this matters to a smart non-expert in 60 seconds?\"\n\n## Output Format\n\nDeliver a Research Question Brief:\n\n```markdown\n# Research Question Brief\n\n## The Interest (Raw)\n[Original unfiltered interest]\n\n## Territory Map\n\n### What's Known\n- [Point 1] ([citation])\n- [Point 2] ([citation])\n\n### What's Unknown\n- [Open question 1]\n- [Open question 2]\n\n### What's Controversial\n- [Debate 1]\n\n## Candidate Gaps\n1. [Gap 1]\n2. [Gap 2]\n3. [Gap 3]\n\n## Refined Questions\n\n### Question 1: [Title]\n**Statement:** [Precise question]\n**Hypothesis:** [What you expect to find]\n**Feasibility:** [Brief assessment]\n**If it works:** [Impact]\n**If it doesn't:** [What we still learn]\n\n### Question 2: [Title]\n[Same structure]\n\n## Recommendation\n[Which question to pursue and why]\n\n## Immediate Next Steps\n1. [Concrete action 1]\n2. [Concrete action 2]\n3. [Concrete action 3]\n```\n\n## Common Failure Modes\n\n**The Kitchen Sink:** Trying to answer too many questions at once\n‚Üí Fix: Ruthlessly cut until there's ONE core question\n\n**The Solution in Search of a Problem:** Starting with a method, not a question\n‚Üí Fix: Ask \"Who has this problem? Why hasn't it been solved?\"\n\n**The Incremental Trap:** Small delta on existing work\n‚Üí Fix: Ask \"Would this change how people think?\"\n\n**The Impossible Dream:** Beautiful question, can't be answered\n‚Üí Fix: Ask \"What's the minimal version that's still interesting?\"\n\n**The Boring Sure Thing:** Will definitely work, nobody cares\n‚Üí Fix: Add ambition until there's meaningful risk"
              },
              {
                "name": "research-taste-developer",
                "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently.",
                "path": "skills/research-taste-developer/SKILL.md",
                "frontmatter": {
                  "name": "research-taste-developer",
                  "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently."
                },
                "content": "# Research Taste Developer\n\nResearch taste is the ability to distinguish work that matters from work that doesn't - before the community tells you. This skill helps you develop that instinct.\n\n## What is Research Taste?\n\nIt's the intuition that lets experienced researchers:\n- Pick problems that turn out to be important\n- Know when an idea is \"close\" vs. \"far\" from working\n- Recognize a good result even with imperfect execution\n- Predict which papers will be remembered in 5 years\n\nTaste isn't magic - it's pattern recognition from deep exposure. This skill accelerates that exposure.\n\n## Process\n\n### Phase 1: Analyze the Field\n\nPick a specific subfield. We'll study what \"good\" looks like there.\n\n**Questions to investigate:**\n1. What are the 10 most-cited papers of the last 5 years?\n2. What are the 5 papers experts say \"changed how we think\"?\n3. What are the best papers from top venues (NeurIPS, ICML, CVPR, etc.)?\n4. What got awards? What got invited talks?\n\n**For each landmark paper, analyze:**\n- What was the state before this paper?\n- What's the single core insight?\n- What specifically made people cite it?\n- Was it obvious in hindsight?\n\n### Phase 2: Pattern Recognition\n\nLook for what the great papers have in common:\n\n**The Patterns of Impact:**\n\n#### 1. The New Primitive\nPapers that introduce a building block others build on.\n- Examples: Attention mechanism, ResNet skip connections, Dropout\n- Pattern: Simple idea, surprisingly general applicability\n- Why it works: Reduces friction for future work\n\n#### 2. The Surprising Connection\nPapers that link two previously separate areas.\n- Examples: VAE (variational inference + neural nets), NeRF (neural nets + ray marching)\n- Pattern: \"X, but for Y\" where the combination is non-obvious\n- Why it works: Cross-pollinates communities\n\n#### 3. The Scaling Insight\nPapers showing that scale changes qualitative behavior.\n- Examples: GPT-3, Chinchilla\n- Pattern: What everyone \"knew\" was wrong at sufficient scale\n- Why it works: Forces field to update beliefs\n\n#### 4. The Rigorous Foundation\nPapers that formalize what was previously folklore.\n- Examples: Theoretical convergence proofs, generalization bounds\n- Pattern: Makes hand-wavy intuitions precise\n- Why it works: Enables confident building\n\n#### 5. The Elegant Solution\nPapers that solve a problem far more simply than expected.\n- Examples: Simple baseline papers, \"X is all you need\"\n- Pattern: Previous solutions were overcomplicated\n- Why it works: Shifts field's complexity assumptions\n\n### Phase 3: Anti-Patterns\n\nLearn to recognize work that won't age well:\n\n**The Incremental Treadmill:**\n- Pattern: +0.5% on benchmark with architectural tweak\n- Why it fails: No one remembers or uses it\n- Exception: When it reveals something fundamental\n\n**The Method Mashing:**\n- Pattern: \"We combine A, B, C, and D\"\n- Why it fails: No insight about why the combination works\n- Exception: When combination reveals unexpected interaction\n\n**The Benchmark Overfitter:**\n- Pattern: Method that works only on specific benchmarks\n- Why it fails: Doesn't transfer, forgotten when benchmarks change\n- Exception: When it exposes benchmark weaknesses\n\n**The Complexity Monster:**\n- Pattern: Works but requires 47 hyperparameters and 3 loss terms\n- Why it fails: No one can reproduce or build on it\n- Exception: Rarely\n\n**The Solution Without a Problem:**\n- Pattern: Novel method without compelling use case\n- Why it fails: \"Interesting but why?\"\n- Exception: When use case emerges later (rare)\n\n### Phase 4: Develop Your Own Taste\n\n**Exercise 1: Prediction Game**\nBefore reading a paper, predict based on title/abstract:\n- Will this paper be cited >100 times in 5 years?\n- Write down your prediction and reasoning\n- Track your accuracy over time\n- Analyze where your predictions went wrong\n\n**Exercise 2: Explain the Gap**\nFor any two papers in citation count:\n- Paper A: 2000 citations\n- Paper B: 50 citations (same venue, same year)\n- What explains the difference?\n- Write a paragraph explanation\n\n**Exercise 3: The Time Machine**\nPick a highly-cited paper. Go back to when it was published:\n- What was the state of the field?\n- Would you have recognized its importance?\n- What signals would you have looked for?\n\n**Exercise 4: Design a Hit**\nGiven current state of a field:\n- What's the most important open problem?\n- What would a \"great paper\" on this look like?\n- What would make people cite it?\n\n### Phase 5: Meta-Principles\n\nWhat top researchers seem to do differently:\n\n**Problem Selection:**\n- Work on problems that are \"ready\" (pieces exist, no one assembled them)\n- Avoid problems that are stuck for fundamental reasons\n- Pick problems where you have unfair advantages\n\n**Execution Taste:**\n- Know when to stop polishing (diminishing returns)\n- Know when result is \"strong enough\" to share\n- Prefer simple-that-works over complex-that-works-slightly-better\n\n**Communication Taste:**\n- Lead with the insight, not the method\n- Make contribution obvious in first 2 minutes\n- Anticipate and address likely objections\n\n**Portfolio Taste:**\n- Mix safe and risky projects\n- Build a coherent research identity\n- Create compound interest (each paper enables the next)\n\n## Output: Taste Development Report\n\n```markdown\n# Research Taste Analysis: [Field/Subfield]\n\n## Landmark Paper Analysis\n\n### [Paper 1 Title] ([Year])\n- **Pre-existing state:** [What was true before]\n- **Core insight:** [One sentence]\n- **Why it's cited:** [Specific reason]\n- **Pattern type:** [New Primitive / Connection / etc.]\n\n### [Paper 2 Title]\n[Same structure]\n\n## Pattern Distribution\nIn this subfield, highly-cited papers tend to be:\n- [X]% New Primitives\n- [Y]% Surprising Connections\n- [Z]% Other\n\n## Anti-Pattern Warnings\nThe following patterns are common but don't lead to impact:\n1. [Anti-pattern common in this field]\n2. [Another one]\n\n## Taste Heuristics for [Field]\nWhen evaluating a paper in this field, ask:\n1. [Field-specific question that distinguishes good from meh]\n2. [Another one]\n3. [Another one]\n\n## Current Opportunities\nBased on this analysis, promising directions seem to be:\n1. [Direction 1]: [Why it's ripe]\n2. [Direction 2]: [Why it's ripe]\n\n## Your Taste Development Exercises\n1. [Specific exercise for this field]\n2. [Another one]\n```\n\n## The Ultimate Test\n\nYou have good taste when:\n- You're bored by work others find impressive (correctly predicting it won't matter)\n- You're excited by work others overlook (correctly predicting it will matter)\n- Your intuitions about importance are calibrated with reality\n- You can articulate *why* something is good, not just that it is\n\nThis takes years. But deliberate practice - not just reading, but *analyzing* - accelerates it dramatically."
              },
              {
                "name": "reviewer-2-simulator",
                "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims.",
                "path": "skills/reviewer-2-simulator/SKILL.md",
                "frontmatter": {
                  "name": "reviewer-2-simulator",
                  "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims."
                },
                "content": "# Reviewer 2 Simulator\n\nChannel the energy of the harshest (but fair) reviewer to find weaknesses before your actual reviewers do.\n\n## The Mindset\n\nReviewer 2 is:\n- Skeptical but not hostile\n- Technically rigorous\n- Short on time (will skim, not read carefully)\n- Looking for reasons to reject (high-volume venues)\n- But wants to champion good work\n\nReviewer 2 is NOT:\n- Trying to be mean\n- Unfamiliar with the field (usually)\n- Unable to be convinced by good arguments\n\n## Process\n\n### Phase 1: First Pass (5-minute skim)\n\nRead like a busy reviewer would:\n- Title and abstract\n- Figures and captions\n- Section headers\n- Conclusion\n\n**First-pass questions:**\n1. Can I understand the contribution from abstract alone?\n2. Do the figures tell the story?\n3. Is this obviously incremental or obviously interesting?\n4. Any immediate red flags?\n\n### Phase 2: Deep Read Critique\n\nGo section by section:\n\n#### Abstract\n- [ ] Clear problem statement?\n- [ ] Specific contribution (not vague \"we propose...\")?\n- [ ] Key result with number?\n- [ ] Any overclaims?\n\n**Common issues:**\n- \"We achieve state-of-the-art\" without specifying where/what\n- \"Novel\" without explaining what's actually new\n- Claims not supported in the paper\n\n#### Introduction\n- [ ] Motivation compelling?\n- [ ] Gap in prior work clearly identified?\n- [ ] Contribution stated precisely?\n- [ ] Paper organization clear?\n\n**Common issues:**\n- Straw-man characterization of prior work\n- Gap is manufactured, not real\n- Contribution buried in paragraph 4\n\n#### Related Work\n- [ ] Comprehensive coverage?\n- [ ] Fair characterization of prior work?\n- [ ] Clear differentiation from closest work?\n- [ ] Missing obvious citations?\n\n**Common issues:**\n- Missing direct competitors\n- Misrepresenting prior work to look better\n- No clear statement of difference from closest work\n\n#### Method\n- [ ] Technically sound?\n- [ ] Reproducible from description?\n- [ ] Assumptions stated explicitly?\n- [ ] Notation consistent?\n\n**Common issues:**\n- Hand-wavy justification\n- Critical details in appendix (or missing entirely)\n- Unstated assumptions\n- Notation changes mid-paper\n\n#### Experiments\n- [ ] Baselines appropriate and strong?\n- [ ] Metrics justified?\n- [ ] Ablations support claims?\n- [ ] Statistical significance addressed?\n- [ ] Error bars / variance reported?\n\n**Common issues:**\n- Weak or outdated baselines\n- Metric chosen to favor method\n- Missing ablations for key components\n- Single seed results\n- Cherry-picked examples\n\n#### Results/Analysis\n- [ ] Claims supported by evidence?\n- [ ] Alternative explanations considered?\n- [ ] Limitations acknowledged?\n- [ ] Failure cases shown?\n\n**Common issues:**\n- Overclaiming from marginal improvements\n- Ignoring results that don't fit narrative\n- No discussion of when method fails\n\n#### Conclusion\n- [ ] Restates contribution accurately?\n- [ ] Future work is genuine (not hand-wavy)?\n- [ ] Doesn't introduce new claims?\n\n### Phase 3: The Killer Questions\n\nThese are the questions that sink papers:\n\n**Novelty:**\n- \"How is this different from [X]?\" (where X is obvious prior work)\n- \"Why couldn't you just do [simpler thing]?\"\n- \"What's the actual technical contribution?\"\n\n**Significance:**\n- \"Why should anyone care about this?\"\n- \"What changes if this paper exists vs. doesn't?\"\n- \"Is this solving a real problem or a made-up one?\"\n\n**Soundness:**\n- \"How do you know [claim]?\"\n- \"What if [assumption] is violated?\"\n- \"Did you try [obvious baseline]?\"\n\n**Clarity:**\n- \"What exactly do you mean by [term]?\"\n- \"How would someone reproduce this?\"\n- \"Why is [unexplained design choice] the right choice?\"\n\n### Phase 4: Scoring\n\nRate on standard conference criteria:\n\n| Criterion | Score (1-5) | Justification |\n|-----------|-------------|---------------|\n| **Novelty** | | How new is this? |\n| **Significance** | | How much does it matter? |\n| **Soundness** | | Is it technically correct? |\n| **Clarity** | | Is it well-written? |\n| **Reproducibility** | | Could I implement this? |\n\n**Overall Recommendation:**\n- Strong Accept: Top 5%, must be in conference\n- Weak Accept: Above threshold, would be OK to accept\n- Borderline: Could go either way\n- Weak Reject: Below threshold, but not fatally flawed\n- Strong Reject: Fundamental issues\n\n## Output Format\n\n```markdown\n# Reviewer 2 Report: [Paper Title]\n\n## Summary (2-3 sentences)\n[What the paper does and claims]\n\n## Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n## Weaknesses\n\n### Major Issues (any one is grounds for rejection)\n1. **[Issue Title]**\n   - What's wrong: [Description]\n   - Why it matters: [Impact on claims]\n   - How to fix: [Concrete suggestion]\n\n### Minor Issues (should be fixed but not fatal)\n1. **[Issue Title]**\n   - [Description and suggestion]\n\n### Nitpicks (take or leave)\n- [Small thing 1]\n- [Small thing 2]\n\n## Questions for Authors\n1. [Question that must be answered]\n2. [Question that would strengthen paper]\n\n## Missing References\n- [Paper 1]: [Why it should be cited]\n- [Paper 2]: [Why it should be cited]\n\n## Scores\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Novelty | X/5 | |\n| Significance | X/5 | |\n| Soundness | X/5 | |\n| Clarity | X/5 | |\n\n## Overall Assessment\n**Recommendation:** [Accept/Reject with confidence]\n\n**In one sentence:** [The core issue or strength]\n\n## Author Rebuttal Priorities\nIf I were the author, I would address these in order:\n1. [Most important thing to address]\n2. [Second most important]\n3. [Third]\n```\n\n## Calibration Notes\n\n**Reviewer 2 is harsh but fair:**\n- Points out real issues, not imagined ones\n- Suggests fixes, not just complaints\n- Acknowledges strengths genuinely\n- Would update opinion if given good rebuttal\n\n**Reviewer 2 is NOT:**\n- Dismissive without reason\n- Demanding impossible experiments\n- Rejecting due to missing tangential work\n- Penalizing for honest limitations"
              },
              {
                "name": "ted-mosby",
                "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured.",
                "path": "skills/ted-mosby/SKILL.md",
                "frontmatter": {
                  "name": "ted-mosby",
                  "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured."
                },
                "content": "# Ted Mosby - Architecture Wiki Generator\n\nGenerate comprehensive architectural documentation for any codebase with source code traceability (file:line references).\n\n## Overview\n\nTed Mosby creates architectural wikis that help developers understand codebases. Every concept links directly to source code, so you can navigate from documentation to implementation.\n\n**Output includes:**\n- Architecture overview with Mermaid diagrams\n- Module documentation with source traceability\n- Data flow documentation\n- Getting started guides\n- Interactive static site with search, keyboard nav, dark mode\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Document a codebase or project architecture\n- Generate a wiki or documentation site\n- Create architecture diagrams with source references\n- Understand and document how a project is structured\n- Produce navigable documentation with file:line traceability\n\n**Trigger phrases:**\n- \"Generate docs for this project\"\n- \"Create architecture documentation\"\n- \"Document this codebase\"\n- \"Make a wiki for this repo\"\n- \"Help me understand this project's structure\"\n\n## Prerequisites\n\n### Required\n- Node.js >= 18.0.0\n- Anthropic API key (`ANTHROPIC_API_KEY` environment variable)\n\n### Check Prerequisites\n```bash\n# Verify Node.js version\nnode --version  # Should be >= 18.0.0\n\n# Verify API key is set\necho $ANTHROPIC_API_KEY  # Should show your key\n```\n\n### Install Ted Mosby\n```bash\nnpm install -g ted-mosby\n```\n\n## Quick Start Commands\n\n### Basic Wiki Generation\n```bash\n# Generate wiki for current directory\nted-mosby generate -r .\n\n# Generate wiki for a specific project\nted-mosby generate -r ./my-project\n\n# Generate wiki for a GitHub repository\nted-mosby generate -r https://github.com/user/repo\n```\n\n### With Interactive Site\n```bash\n# Generate wiki + interactive static site\nted-mosby generate -r ./my-project --site\n\n# Custom title and theme\nted-mosby generate -r ./my-project --site --site-title \"My Project Docs\" --theme dark\n\n# Generate site only (if wiki already exists)\nted-mosby generate -r ./my-project --site-only\n```\n\n### Other Useful Options\n```bash\n# Focus on specific subdirectory\nted-mosby generate -r ./my-project -p src/core\n\n# Custom output directory\nted-mosby generate -r ./my-project -o ./docs/architecture\n\n# Verbose output (see agent progress)\nted-mosby generate -r ./my-project -v\n\n# Estimate time/cost before running (dry run)\nted-mosby generate -r ./my-project -e\n```\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nBefore running Ted Mosby, clarify with the user:\n\n1. **Target path** - What directory or repo to document?\n2. **Output location** - Where should the wiki go? (default: `./wiki`)\n3. **Site generation** - Do they want an interactive static site?\n4. **Focus area** - Any specific subdirectory to focus on?\n5. **Theme preference** - Light, dark, or auto?\n\n### Step 2: Pre-flight Checks\n\nVerify the environment is ready:\n\n```bash\n# Check Node.js version\nnode --version\n\n# Verify ted-mosby is installed\nwhich ted-mosby || echo \"Run: npm install -g ted-mosby\"\n\n# Check API key\n[ -z \"$ANTHROPIC_API_KEY\" ] && echo \"Set ANTHROPIC_API_KEY environment variable\"\n```\n\n### Step 3: Run Generation\n\nChoose the appropriate command based on user needs:\n\n| User Wants | Command |\n|------------|---------|\n| Basic wiki only | `ted-mosby generate -r ./project` |\n| Wiki + interactive site | `ted-mosby generate -r ./project --site` |\n| Site with custom title | `ted-mosby generate -r ./project --site --site-title \"Docs\"` |\n| Dark theme site | `ted-mosby generate -r ./project --site --theme dark` |\n| Focus on subdirectory | `ted-mosby generate -r ./project -p src/core` |\n| Large codebase | `ted-mosby generate -r ./project --max-chunks 5000` |\n| Quick iteration | `ted-mosby generate -r ./project --skip-index` |\n\n### Step 4: Review Output\n\nAfter generation completes:\n\n1. **Wiki location:** `./wiki/README.md` (or custom output dir)\n2. **Site location:** `./wiki/site/index.html` (if `--site` used)\n3. **Open site:** Open `index.html` in browser\n\n### Step 5: Fix Issues (if needed)\n\nIf there are broken links or missing pages:\n\n```bash\n# Check for and generate missing pages\nted-mosby continue -r ./my-project -o ./wiki\n\n# Verify only (don't generate)\nted-mosby continue -r ./my-project -o ./wiki --verify-only\n```\n\n## Output Structure\n\n```\nwiki/\n‚îú‚îÄ‚îÄ README.md                    # Navigation entry point\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ overview.md              # System architecture + Mermaid diagrams\n‚îÇ   ‚îî‚îÄ‚îÄ data-flow.md             # Data flow documentation\n‚îú‚îÄ‚îÄ components/\n‚îÇ   ‚îî‚îÄ‚îÄ {module}/\n‚îÇ       ‚îî‚îÄ‚îÄ index.md             # Per-module documentation\n‚îú‚îÄ‚îÄ guides/\n‚îÇ   ‚îî‚îÄ‚îÄ getting-started.md       # Quick start guide\n‚îú‚îÄ‚îÄ glossary.md                  # Concept index\n‚îî‚îÄ‚îÄ site/                        # (with --site flag)\n    ‚îú‚îÄ‚îÄ index.html               # Interactive site entry\n    ‚îú‚îÄ‚îÄ styles.css\n    ‚îî‚îÄ‚îÄ scripts.js\n```\n\n## Source Traceability\n\nEvery architectural concept includes clickable source references:\n\n```markdown\n## Authentication Flow\n\nThe authentication system uses JWT tokens for stateless auth.\n\n**Source:** [`src/auth/jwt-provider.ts:23-67`](../../../src/auth/jwt-provider.ts#L23-L67)\n```\n\nThis allows developers to navigate directly from documentation to implementation.\n\n## Interactive Site Features\n\nWhen `--site` is used, the generated site includes:\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Instant search across all pages (Cmd/Ctrl+K) |\n| Keyboard navigation | Arrow keys, vim-style (j/k/h/l) |\n| Dark/light mode | Respects system preference or manual toggle |\n| Table of contents | Auto-generated from headings |\n| Mobile responsive | Works on all devices |\n| Offline capable | No server required |\n| Mermaid diagrams | Rendered automatically |\n\n## Command Reference\n\n### `generate` - Create wiki documentation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path/url>` | Repository path or GitHub URL (required) | - |\n| `-o, --output <dir>` | Output directory for wiki | `./wiki` |\n| `-p, --path <path>` | Focus on specific directory | - |\n| `-s, --site` | Generate interactive static site | - |\n| `--site-only` | Generate site only (skip wiki) | - |\n| `--site-title <title>` | Custom site title | Project name |\n| `--theme <theme>` | Site theme: light, dark, auto | `auto` |\n| `-v, --verbose` | Show detailed progress | - |\n| `-e, --estimate` | Estimate time/cost (dry run) | - |\n| `--max-chunks <n>` | Limit indexed chunks (for large repos) | unlimited |\n| `--skip-index` | Use cached embeddings index | - |\n| `--direct-api` | Use Anthropic API directly | - |\n| `-m, --model <model>` | Claude model to use | `claude-sonnet-4-20250514` |\n| `--max-turns <n>` | Limit agent iterations | 200 |\n\n### `continue` - Resume/fix wiki generation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path>` | Repository path (required) | - |\n| `-o, --output <dir>` | Wiki output directory | `./wiki` |\n| `--verify-only` | Only check, don't generate | - |\n| `--skip-index` | Use cached embeddings index | - |\n| `-v, --verbose` | Show detailed progress | - |\n\n## Large Codebase Options\n\nFor repositories with 10,000+ files:\n\n```bash\n# Limit indexed chunks (reduces memory usage)\nted-mosby generate -r ./large-project --max-chunks 5000\n\n# Reduce search results per query\nted-mosby generate -r ./large-project --max-results 5\n\n# Batched processing (for very large repos)\nted-mosby generate -r ./large-project --batch-size 3000\n```\n\n## Typical Runtime\n\n| Codebase Size | Approximate Time |\n|---------------|------------------|\n| Small (<50 files) | 1-2 minutes |\n| Medium (50-200 files) | 2-5 minutes |\n| Large (200+ files) | 5-10 minutes |\n\nUse `--estimate` to get a cost/time estimate before running.\n\n## Troubleshooting\n\n### \"Credit balance is too low\" error\nUse direct API mode:\n```bash\nted-mosby generate -r ./my-project --direct-api\n```\n\n### Out of memory on large repos\nLimit indexed chunks:\n```bash\nted-mosby generate -r ./large-project --max-chunks 5000 --batch-size 3000\n```\n\n### Slow re-runs during development\nSkip re-indexing:\n```bash\nted-mosby generate -r ./my-project --skip-index\n```\n\n### Missing pages / broken links\nUse the continue command:\n```bash\nted-mosby continue -r ./my-project -o ./wiki\n```\n\n## Example Conversation\n\n**User:** \"Can you document this project's architecture?\"\n\n**Assistant:** I'll use Ted Mosby to generate architectural documentation for your project.\n\nFirst, let me verify the prerequisites are in place, then generate the wiki with an interactive site:\n\n```bash\n# Generate wiki with interactive site\nted-mosby generate -r . --site --site-title \"Project Architecture\"\n```\n\nThis will create:\n- `wiki/README.md` - Main navigation\n- `wiki/architecture/overview.md` - Architecture diagrams\n- `wiki/site/index.html` - Interactive documentation site\n\n## Resources\n\n- [Ted Mosby GitHub](https://github.com/your-username/ted-mosby)\n- [Build an Agent Workshop](https://buildanagentworkshop.com)"
              },
              {
                "name": "turn-this-feature-into-a-blog-post",
                "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\".",
                "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
                "frontmatter": {
                  "name": "turn-this-feature-into-a-blog-post",
                  "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\"."
                },
                "content": "# Turn This Feature Into a Blog Post\n\nGenerate a Markdown blog post that explains a code implementation in an engaging, educational way.\n\n## Process\n\n1. **Analyze the implementation** - Read and understand all relevant code files, tracing the feature from entry point to completion\n2. **Identify the narrative** - Find the core problem being solved and why it matters\n3. **Structure the post** - Organize as What ‚Üí Why ‚Üí How (from first principles)\n4. **Write accessibly** - Use friendly, conversational language while maintaining technical authority\n5. **Output Markdown** - Create a complete `.md` file ready for publishing\n\n## Blog Post Structure\n\n### Title\n- Clear, specific, and searchable\n- Format: \"How We Built [Feature]\" or \"Building [Feature]: A Deep Dive\"\n\n### Introduction (2-3 paragraphs)\n- Hook the reader with the problem or outcome\n- Briefly explain what the feature does\n- Preview what readers will learn\n\n### The What (1-2 sections)\n- Describe the feature from the user's perspective\n- Include screenshots or diagrams if applicable\n- Keep technical jargon minimal\n\n### The Why (1-2 sections)\n- Explain the problem this solves\n- Discuss alternatives considered and why this approach won\n- Connect to broader engineering principles\n\n### The How (2-4 sections)\n- Walk through the implementation from first principles\n- Include relevant code snippets with explanations\n- Explain non-obvious decisions\n- Build up complexity gradually\n\n### Conclusion\n- Summarize key takeaways\n- Mention potential future improvements\n- Invite engagement (questions, feedback)\n\n## Writing Style\n\n- **Friendly but authoritative** - Write like a knowledgeable colleague explaining over coffee\n- **First-person plural** - Use \"we\" to create shared ownership\n- **Active voice** - \"We built\" not \"It was built\"\n- **Show, don't just tell** - Use code examples liberally\n- **Explain the \"why\"** - Every code block should have context\n- **Avoid jargon walls** - Define terms on first use\n\n## Code Snippets\n\n- Include only relevant portions, not entire files\n- Add comments for non-obvious lines\n- Use syntax highlighting with language tags\n- Provide context before each snippet\n\n## Output\n\nSave the blog post as a Markdown file with:\n- Kebab-case filename matching the title\n- Frontmatter with title, date, author, and tags (if appropriate for the target platform)\n- Properly formatted headers, code blocks, and lists"
              }
            ]
          },
          {
            "name": "deployment-skills",
            "description": "Skills for deploying ML models and applications to cloud platforms",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add GhostScientist/skills",
              "/plugin install deployment-skills@GhostScientist-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-10T22:28:06Z",
              "created_at": "2025-12-08T15:22:00Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "create-watchos-version",
                "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation.",
                "path": "skills/create-watchos-version/SKILL.md",
                "frontmatter": {
                  "name": "create-watchos-version",
                  "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation."
                },
                "content": "# Create watchOS Version\n\nAnalyzes existing Apple platform projects and creates detailed, phased implementation plans for watchOS apps that are elegant, top-tier experiences‚Äînot naive skins of the parent app.\n\n## Workflow\n\n1. **Project Discovery** - Analyze project structure, patterns, architecture\n2. **Feature Mapping** - Identify watchOS-suitable features and priorities\n3. **API Compatibility** - Search web for current watchOS API availability\n4. **Architecture Planning** - Design watchOS-specific architecture\n5. **Plan Generation** - Create phased plan with warnings and alternatives\n6. **User Review** - Present plan for approval before implementation\n\n## Phase 1: Project Discovery\n\nScan project root for:\n\n```\n‚îú‚îÄ‚îÄ App Architecture (SwiftUI, UIKit, AppKit, hybrid)\n‚îú‚îÄ‚îÄ Data Layer (Core Data, SwiftData, Realm, custom)\n‚îú‚îÄ‚îÄ Networking (URLSession, Alamofire, custom)\n‚îú‚îÄ‚îÄ State Management (ObservableObject, TCA, Redux-like)\n‚îú‚îÄ‚îÄ Navigation (NavigationStack, Coordinator)\n‚îú‚îÄ‚îÄ Shared Frameworks (SPM packages, shared targets)\n‚îú‚îÄ‚îÄ Assets (colors, images, SF Symbols)\n‚îú‚îÄ‚îÄ Existing Watch Target (if any)\n‚îî‚îÄ‚îÄ Minimum iOS Version (affects watchOS targeting)\n```\n\nKey files: `*.xcodeproj`, `Package.swift`, `Info.plist`, App entry points, ViewModels, Models.\n\n## Phase 2: Feature Mapping\n\n**Glanceable (High Priority)**: Status displays, counters, progress, recent items, quick stats\n\n**Quick Actions (High Priority)**: Single-tap toggles, shortcuts, haptic confirmations\n\n**Complications/Widgets (Critical)**: Map data to WidgetKit families‚ÄîaccessoryCircular, accessoryRectangular, accessoryInline, accessoryCorner. Consider Smart Stack relevance.\n\n**Background**: HealthKit integration, background refresh, Watch Connectivity sync\n\n**Defer/Exclude**: Complex data entry, long-form content, sustained screen time features\n\n## Phase 3: API Compatibility\n\n**CRITICAL**: Always search web for current watchOS docs before finalizing. APIs change frequently.\n\nSearch: `[FrameworkName] watchOS availability site:developer.apple.com`\n\n### Quick Reference\n\n**Available**: SwiftUI, SwiftData (10+), WidgetKit (9+), HealthKit, WorkoutKit, CoreLocation (limited), WatchConnectivity, CloudKit, CoreMotion, AVFoundation (audio), CoreBluetooth, Combine, Swift Concurrency\n\n**Unavailable/Limited**: UIKit, WebKit, MapKit (limited), CoreImage (limited), ARKit, RealityKit, StoreKit (limited), Background URLSession (limited)\n\nSee `references/api-compatibility.md` for detailed compatibility matrix.\n\n## Phase 4: Architecture\n\n### Version Targeting\n\n```\niOS 16+ ‚Üí watchOS 9+  (WidgetKit complications)\niOS 17+ ‚Üí watchOS 10+ (SwiftData, Smart Stack)\niOS 18+ ‚Üí watchOS 11+ (Live Activities on Watch)\n```\n\n### Structure\n\n```\nShared/\n‚îú‚îÄ‚îÄ Models/           # Pure Swift, shared via target membership\n‚îú‚îÄ‚îÄ Services/         # Platform-agnostic logic\n‚îî‚îÄ‚îÄ Utilities/\n\nWatchApp/\n‚îú‚îÄ‚îÄ App.swift\n‚îú‚îÄ‚îÄ Views/\n‚îú‚îÄ‚îÄ ViewModels/\n‚îú‚îÄ‚îÄ Complications/\n‚îî‚îÄ‚îÄ WatchConnectivity/\n```\n\n### Design Principles\n\n1. **Glanceability** - Visible within 2 seconds\n2. **Minimal Interaction** - 1-3 taps max\n3. **Context Awareness** - Time, location, activity\n4. **Battery Conscious** - Efficient refresh, TimelineSchedule\n5. **Haptic Feedback** - Confirm actions appropriately\n\n### SwiftUI Gotchas\n\n- Avoid nested TabViews (memory leaks)\n- Use TimelineSchedule for efficient metric updates\n- Check `isLuminanceReduced` to reduce work when dimmed\n- Don't use data-driven high-frequency UI refreshes\n\n## Phase 5: Plan Generation\n\nUse template in `references/plan-template.md` to generate:\n\n1. Executive Summary\n2. ‚ö†Ô∏è API Compatibility Warnings table\n3. Phased implementation tasks\n4. Testing checklist\n\n## Phase 6: User Review\n\nPresent plan and ask for approval before implementing:\n\n> \"I've analyzed your project and created a watchOS plan. Before proceeding:\n> 1. **API Warnings**: [N] APIs unavailable‚Äîalternatives documented.\n> 2. **Recommended Features**: [list] prioritized for Watch.\n> 3. **Scope**: [N] phases.\n> \n> Proceed with implementation, or adjust the plan?\"\n\n**Do not implement until user approves.**\n\n## Best Practices Reference\n\n### Watch Connectivity\n\n```swift\nguard WCSession.default.activationState == .activated else { return }\n// sendMessage: immediate, requires reachability\n// transferUserInfo: queued, guaranteed\n// transferCurrentComplicationUserInfo: complication priority\n```\n\n### Complications\n\n```swift\n// Use appropriate reload policy\nTimeline(entries: entries, policy: .after(nextUpdateDate))\n// Use .never for static complications\n```\n\n### Battery Efficiency\n\n- Timeline-based over active refresh\n- Check `isLuminanceReduced`\n- Batch Watch Connectivity transfers\n- Significant location change vs continuous updates"
              },
              {
                "name": "experiment-design-checklist",
                "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates.",
                "path": "skills/experiment-design-checklist/SKILL.md",
                "frontmatter": {
                  "name": "experiment-design-checklist",
                  "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates."
                },
                "content": "# Experiment Design Checklist\n\nPrevent the \"I ran experiments for 3 months and they're meaningless\" disaster through rigorous upfront design.\n\n## The Core Principle\n\nBefore running ANY experiment, you should be able to answer:\n1. What specific claim will this experiment support or refute?\n2. What would convince a skeptical reviewer?\n3. What could go wrong that would invalidate the results?\n\n## Process\n\n### Step 1: State the Hypothesis Precisely\n\nConvert your research question into falsifiable predictions:\n\n**Template:**\n```\nIf [intervention/method], then [measurable outcome], because [mechanism].\n```\n\n**Examples:**\n- \"If we add auxiliary contrastive loss, then downstream task accuracy increases by >2%, because representations become more separable.\"\n- \"If we use learned positional encodings, then performance on sequences >4096 tokens improves, because the model can extrapolate beyond training length.\"\n\n**Null hypothesis:** What does \"no effect\" look like? This is what you're trying to reject.\n\n### Step 2: Identify Variables\n\n**Independent Variables (what you manipulate):**\n| Variable | Levels | Rationale |\n|----------|--------|-----------|\n| [Var 1] | [Level A, B, C] | [Why these levels] |\n\n**Dependent Variables (what you measure):**\n| Metric | How Measured | Why This Metric |\n|--------|--------------|-----------------|\n| [Metric 1] | [Procedure] | [Justification] |\n\n**Control Variables (what you hold constant):**\n| Variable | Fixed Value | Why Fixed |\n|----------|-------------|-----------|\n| [Var 1] | [Value] | [Prevents confound X] |\n\n### Step 3: Choose Baselines\n\nEvery experiment needs comparisons. No result is meaningful in isolation.\n\n**Baseline Hierarchy:**\n\n1. **Random/Trivial Baseline**\n   - What does random chance achieve?\n   - Sanity check that the task isn't trivial\n\n2. **Simple Baseline**\n   - Simplest reasonable approach\n   - Often embarrassingly effective\n\n3. **Standard Baseline**\n   - Well-known method from literature\n   - Apples-to-apples comparison\n\n4. **State-of-the-Art Baseline**\n   - Current best published result\n   - Only if you're claiming SOTA\n\n5. **Ablated Self**\n   - Your method minus key components\n   - Shows each component contributes\n\n**For each baseline, document:**\n- Source (paper, implementation)\n- Hyperparameters used\n- Whether you re-ran or used reported numbers\n- Any modifications made\n\n### Step 4: Design Ablations\n\nAblations answer: \"Is each component necessary?\"\n\n**Ablation Template:**\n| Variant | What's Removed/Changed | Expected Effect | If No Effect... |\n|---------|----------------------|-----------------|-----------------|\n| Full Model | Nothing | Best performance | - |\n| w/o Component A | Remove A | Performance drops X% | A isn't helping |\n| w/o Component B | Remove B | Performance drops Y% | B isn't helping |\n| Component A only | Only A, no B | Shows A's isolated contribution | - |\n\n**Good ablations are:**\n- Surgical (one change at a time)\n- Interpretable (clear what was changed)\n- Informative (result tells you something)\n\n### Step 5: Address Confounds\n\nThings that could explain your results OTHER than your hypothesis:\n\n**Common Confounds:**\n\n| Confound | How to Check | How to Control |\n|----------|--------------|----------------|\n| Hyperparameter tuning advantage | Same tuning budget for all | Report tuning procedure |\n| Compute advantage | Matched FLOPs/params | Report compute used |\n| Data leakage | Check train/test overlap | Strict separation |\n| Random seed luck | Multiple seeds | Report variance |\n| Implementation bugs (baseline) | Verify baseline numbers | Use official implementations |\n| Cherry-picked examples | Random or systematic selection | Pre-register selection criteria |\n\n### Step 6: Statistical Rigor\n\n**Sample Size:**\n- How many random seeds? (Minimum: 3, better: 5+)\n- How many data splits? (If applicable)\n- Power analysis: Can you detect expected effect size?\n\n**What to Report:**\n- Mean ¬± standard deviation (or standard error)\n- Confidence intervals where appropriate\n- Statistical significance tests if claiming \"better\"\n\n**Appropriate Tests:**\n| Comparison | Test | Assumptions |\n|------------|------|-------------|\n| Two methods, normal data | t-test | Normality, equal variance |\n| Two methods, unknown dist | Mann-Whitney U | Ordinal data |\n| Multiple methods | ANOVA + post-hoc | Normality |\n| Multiple methods, unknown | Kruskal-Wallis | Ordinal data |\n| Paired comparisons | Wilcoxon signed-rank | Same test instances |\n\n**Avoid:**\n- p-hacking (running until significant)\n- Multiple comparison problems (Bonferroni correct)\n- Reporting only favorable metrics\n\n### Step 7: Compute Budget\n\nBefore running, estimate:\n\n| Component | Estimate | Notes |\n|-----------|----------|-------|\n| Single training run | X GPU-hours | [Details] |\n| Hyperparameter search | Y runs √ó X hours | [Search strategy] |\n| Baselines | Z runs √ó W hours | [Which baselines] |\n| Ablations | N variants √ó X hours | [Which ablations] |\n| Seeds | M seeds √ó above | [How many seeds] |\n| **Total** | **T GPU-hours** | Buffer: 1.5-2x |\n\n**Go/No-Go Decision:** Is this feasible with available resources?\n\n### Step 8: Pre-Registration (Optional but Recommended)\n\nWrite down BEFORE running:\n- Exact hypotheses\n- Primary metrics (not chosen post-hoc)\n- Analysis plan\n- What would constitute \"success\"\n\nThis prevents unconscious goal-post moving.\n\n## Output: Experiment Design Document\n\n```markdown\n# Experiment Design: [Title]\n\n## Hypothesis\n[Precise statement]\n\n## Variables\n### Independent\n[Table]\n\n### Dependent\n[Table]\n\n### Controls\n[Table]\n\n## Baselines\n1. [Baseline 1]: [Source, details]\n2. [Baseline 2]: [Source, details]\n\n## Ablations\n[Table]\n\n## Confound Mitigation\n[Table]\n\n## Statistical Plan\n- Seeds: [N]\n- Tests: [Which tests for which comparisons]\n- Significance threshold: [Œ± level]\n\n## Compute Budget\n[Table with total estimate]\n\n## Success Criteria\n- Primary: [What must be true]\n- Secondary: [Nice to have]\n\n## Timeline\n- Phase 1: [What, when]\n- Phase 2: [What, when]\n\n## Known Risks\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n```\n\n## Red Flags in Experiment Design\n\nüö© \"We'll figure out the metrics later\"\nüö© \"One run should be enough\"\nüö© \"We don't need baselines, it's obviously better\"\nüö© \"Let's just see what happens\"\nüö© \"We can always run more if it's not significant\"\nüö© No compute estimate before starting\nüö© Vague success criteria"
              },
              {
                "name": "hugging-face-space-deployer",
                "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons.",
                "path": "skills/hugging-face-space-deployer/SKILL.md",
                "frontmatter": {
                  "name": "hugging-face-space-deployer",
                  "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons."
                },
                "content": "# Hugging Face Space Deployer\n\nA skill for AI engineers to create, configure, and deploy interactive ML demos on Hugging Face Spaces.\n\n## CRITICAL: Pre-Deployment Checklist\n\n**Before writing ANY code, gather this information about the model:**\n\n### 1. Check Model Type (LoRA Adapter vs Full Model)\n\n**Use the HF MCP tool to inspect the model files:**\n```\nhf-skills - Hub Repo Details (repo_ids: [\"username/model\"], repo_type: \"model\")\n```\n\n**Look for these indicators:**\n\n| Files Present | Model Type | Action Required |\n|---------------|------------|-----------------|\n| `model.safetensors` or `pytorch_model.bin` | Full model | Load directly with `AutoModelForCausalLM` |\n| `adapter_model.safetensors` + `adapter_config.json` | LoRA/PEFT adapter | Must load base model first, then apply adapter with `peft` |\n| Only config files, no weights | Broken/incomplete | Ask user to verify |\n\n**If adapter_config.json exists, check for `base_model_name_or_path` to identify the base model.**\n\n### 2. Check Inference API Availability\n\nVisit the model page on HF Hub and look for \"Inference Providers\" widget on the right side.\n\n**Indicators that model HAS Inference API:**\n- Inference widget visible on model page\n- Model from known provider: `meta-llama`, `mistralai`, `HuggingFaceH4`, `google`, `stabilityai`, `Qwen`\n- High download count (>10,000) with standard architecture\n\n**Indicators that model DOES NOT have Inference API:**\n- Personal namespace (e.g., `GhostScientist/my-model`)\n- LoRA/PEFT adapter (adapters never have direct Inference API)\n- Missing `pipeline_tag` in model metadata\n- No inference widget on model page\n\n### 3. Check Model Metadata\n\n- Ensure `pipeline_tag` is set (e.g., `text-generation`)\n- Add `conversational` tag for chat models\n\n### 4. Determine Hardware Needs\n\n| Model Size | Recommended Hardware |\n|------------|---------------------|\n| < 3B parameters | ZeroGPU (free) or CPU |\n| 3B - 7B parameters | ZeroGPU or T4 |\n| > 7B parameters | A10G or A100 |\n\n### 5. Ask User If Unclear\n\n**If you cannot determine the model type, ASK THE USER:**\n\n> \"I'm analyzing your model to determine the best deployment strategy. I found:\n> - [what you found about files]\n> - [what you found about inference API]\n>\n> Is this model:\n> 1. A full model you trained/uploaded?\n> 2. A LoRA/PEFT adapter on top of another model?\n> 3. Something else?\n>\n> Also, would you prefer:\n> A. Free deployment with ZeroGPU (may have queue times)\n> B. Paid GPU for faster response (~$0.60/hr)\"\n\n## Hardware Options\n\n| Hardware | Use Case | Cost |\n|----------|----------|------|\n| `cpu-basic` | Simple demos, Inference API apps | Free |\n| `cpu-upgrade` | Faster CPU inference | ~$0.03/hr |\n| **`zero-a10g`** | **Models needing GPU on-demand (recommended for most)** | **Free (with quota)** |\n| `t4-small` | Small GPU models (<7B) | ~$0.60/hr |\n| `t4-medium` | Medium GPU models | ~$0.90/hr |\n| `a10g-small` | Large models (7B-13B) | ~$1.50/hr |\n| `a10g-large` | Very large models (30B+) | ~$3.15/hr |\n| `a100-large` | Largest models | ~$4.50/hr |\n\n**ZeroGPU Note:** ZeroGPU (`zero-a10g`) provides free GPU access on-demand. The Space runs on CPU, and when a user triggers inference, a GPU is allocated temporarily (~60-120 seconds). **After deployment, you must manually set the runtime to \"ZeroGPU\" in Space Settings > Hardware.**\n\n## Deployment Decision Tree\n\n```\nAnalyze Model\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have adapter_config.json?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a LoRA adapter\n‚îÇ       ‚îú‚îÄ‚îÄ Find base_model_name_or_path in adapter_config.json\n‚îÇ       ‚îî‚îÄ‚îÄ Use Template 3 (LoRA + ZeroGPU)\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have model.safetensors or pytorch_model.bin?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a full model\n‚îÇ       ‚îú‚îÄ‚îÄ Is it from a major provider with inference widget?\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Use Inference API (Template 1)\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Use ZeroGPU (Template 2)\n‚îÇ\n‚îî‚îÄ‚îÄ Neither found?\n    ‚îî‚îÄ‚îÄ ASK USER - model may be incomplete\n```\n\n## Dependencies\n\n**For Inference API (cpu-basic, free):**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**For ZeroGPU full models (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**For ZeroGPU LoRA adapters (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n## CLI Commands (CORRECT Syntax)\n\n```bash\n# Create Space\nhf repo create my-space-name --repo-type space --space-sdk gradio\n\n# Upload files\nhf upload username/space-name ./local-folder --repo-type space\n\n# Download model files to inspect\nhf download username/model-name --local-dir ./model-check --dry-run\n\n# Check what files exist in a model\nhf download username/model-name --local-dir /tmp/check --dry-run 2>&1 | grep -E '\\.(safetensors|bin|json)'\n```\n\n## Template 1: Inference API (For Supported Models)\n\n**Use when:** Model has inference widget, is from major provider, or explicitly supports serverless API.\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nMODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"  # Must support Inference API!\nclient = InferenceClient(MODEL_ID)\n\ndef respond(message, history, system_message, max_tokens, temperature, top_p):\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    response = \"\"\n    for token in client.chat_completion(\n        messages,\n        max_tokens=max_tokens,\n        stream=True,\n        temperature=temperature,\n        top_p=top_p,\n    ):\n        delta = token.choices[0].delta.content or \"\"\n        response += delta\n        yield response\n\ndemo = gr.ChatInterface(\n    respond,\n    title=\"Chat Assistant\",\n    description=\"Powered by Hugging Face Inference API\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\"),\n        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Write a Python function to sort a list\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Chat App\nemoji: üí¨\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n```\n\n## Template 2: ZeroGPU Full Model (For Models Without Inference API)\n\n**Use when:** Full model (has model.safetensors) but no Inference API support.\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"username/my-full-model\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Global model - loaded lazily on first GPU call for faster Space startup\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Model\",\n    description=\"Powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me write some code\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Model\nemoji: ü§ñ\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Template 3: ZeroGPU LoRA Adapter (CRITICAL FOR FINE-TUNED MODELS)\n\n**Use when:** Model has `adapter_config.json` and `adapter_model.safetensors` (NOT `model.safetensors`)\n\n**You MUST identify the base model from `adapter_config.json` field `base_model_name_or_path`**\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Your LoRA adapter\nADAPTER_ID = \"username/my-lora-adapter\"\n# Base model (from adapter_config.json -> base_model_name_or_path)\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Global model - loaded lazily on first GPU call\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            BASE_MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n        model = model.merge_and_unload()  # Merge for faster inference\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for item in history:\n        if isinstance(item, (list, tuple)) and len(item) == 2:\n            user_msg, assistant_msg = item\n            if user_msg:\n                messages.append({\"role\": \"user\", \"content\": user_msg})\n            if assistant_msg:\n                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Fine-Tuned Model\",\n    description=\"LoRA fine-tuned model powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me with a coding task\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt (MUST include peft):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Fine-Tuned Model\nemoji: üîß\ncolorFrom: green\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Post-Deployment Steps\n\n**After uploading your Space files:**\n\n### 1. Set the Runtime Hardware (REQUIRED for GPU models)\n\n- Go to: `https://huggingface.co/spaces/USERNAME/SPACE_NAME/settings`\n- Under \"Space Hardware\", select the appropriate option:\n  - **ZeroGPU** for free on-demand GPU (recommended)\n  - Or a dedicated GPU tier if needed\n\n### 2. Verify the Space is Running\n\n- Check the Space URL for any build errors\n- Review container logs in Settings if issues occur\n\n### 3. Common Post-Deploy Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| \"No API found\" error | Hardware mismatch | Set runtime to ZeroGPU in Settings |\n| Model not loading | LoRA vs full model confusion | Check if it's an adapter, use correct template |\n| Inference API errors | Model not on serverless | Load directly with transformers instead |\n\n## Detecting Model Type - Quick Reference\n\n### Full Model\nFiles include: `model.safetensors`, `pytorch_model.bin`, or sharded versions\n```python\n# Can load directly\nmodel = AutoModelForCausalLM.from_pretrained(\"username/model\")\n```\n\n### LoRA/PEFT Adapter\nFiles include: `adapter_config.json`, `adapter_model.safetensors`\n```python\n# Must load base model first, then apply adapter\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model-id\")\nmodel = PeftModel.from_pretrained(base_model, \"username/adapter\")\nmodel = model.merge_and_unload()  # Optional: merge for faster inference\n```\n\n### Inference API Available\nModel page shows \"Inference Providers\" widget on the right side\n```python\n# Can use InferenceClient (simplest approach)\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\"username/model\")\n```\n\n## Fixing Missing pipeline_tag (To Enable Inference API)\n\nIf a model doesn't have an inference widget but should, it may be missing metadata:\n\n```bash\n# Download the README\nhf download username/model-name README.md --local-dir /tmp/fix\n\n# Edit to add pipeline_tag in YAML frontmatter:\n# ---\n# pipeline_tag: text-generation\n# tags:\n# - conversational\n# ---\n\n# Upload the fix\nhf upload username/model-name /tmp/fix/README.md README.md\n```\n\n**Note:** Even with correct tags, custom models may not get Inference API - it depends on HF's infrastructure decisions.\n\n## CRITICAL: Gradio 5.x Requirements\n\n### Examples Format (MUST be nested lists)\n```python\n# CORRECT:\nexamples=[\n    [\"Example 1\"],\n    [\"Example 2\"],\n]\n\n# WRONG (causes ValueError):\nexamples=[\n    \"Example 1\",\n    \"Example 2\",\n]\n```\n\n### Version Requirements\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\nDo NOT use `gradio==4.44.0` - causes `ImportError: cannot import name 'HfFolder'`\n\n## Troubleshooting\n\n### \"No API found\" Error\n**Cause:** Gradio app isn't exposing API correctly, often due to hardware mismatch\n**Fix:** Go to Space Settings and set runtime to \"ZeroGPU\" or appropriate GPU tier\n\n### \"OSError: does not appear to have a file named pytorch_model.bin, model.safetensors\"\n**Cause:** Trying to load a LoRA adapter as a full model\n**Fix:** Check for `adapter_config.json` - if present, use PEFT to load:\n```python\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\nmodel = PeftModel.from_pretrained(base_model, \"adapter-id\")\n```\n\n### Inference API Not Available\n**Cause:** Model doesn't have pipeline_tag or isn't deployed to serverless\n**Fix:** Either:\n  a. Add `pipeline_tag: text-generation` to model's README.md\n  b. Or load model directly with transformers instead of InferenceClient\n\n### `ImportError: cannot import name 'HfFolder'`\n**Cause:** gradio/huggingface_hub version mismatch\n**Fix:** Use `gradio>=5.0.0` and `huggingface_hub>=0.26.0`\n\n### `ValueError: examples must be nested list`\n**Cause:** Gradio 5.x format change\n**Fix:** Use `[[\"ex1\"], [\"ex2\"]]` not `[\"ex1\", \"ex2\"]`\n\n### Space builds but model doesn't load\n**Cause:** Missing `peft` for adapters, or wrong base model\n**Fix:** Check adapter_config.json for correct base_model_name_or_path\n\n## Workflow Summary\n\n1. **Analyze model** (check for adapter_config.json, model files, inference widget)\n2. **Determine strategy** (Inference API vs ZeroGPU, full model vs LoRA)\n3. **Ask user if unclear** about model type or cost preferences\n4. **Generate correct template** based on analysis\n5. **Create Space** with correct requirements and README\n6. **Upload files** using `hf upload`\n7. **Set hardware** in Space Settings (ZeroGPU for free GPU access)\n8. **Monitor build logs** for any issues"
              },
              {
                "name": "implement-paper-from-scratch",
                "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions.",
                "path": "skills/implement-paper-from-scratch/SKILL.md",
                "frontmatter": {
                  "name": "implement-paper-from-scratch",
                  "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions."
                },
                "content": "# Implement Paper From Scratch\n\nThe best way to truly understand a paper is to implement it. This skill guides you through that process methodically.\n\n## Philosophy\n\n- **No copy-pasting from reference implementations** - We build understanding, not just working code\n- **Checkpoint questions verify understanding** - You should be able to answer \"why\" at each step\n- **Minimal dependencies** - Use NumPy/PyTorch fundamentals, not high-level wrappers\n- **Deliberate debugging** - Bugs are learning opportunities, not obstacles\n\n## Process\n\n### Phase 1: Pre-Implementation Analysis\n\nBefore writing any code:\n\n1. **Identify the core algorithm** - Strip away ablations, extensions, bells and whistles. What's the minimal version?\n\n2. **List the components** - Break into modules:\n   - Data pipeline\n   - Model architecture\n   - Loss function(s)\n   - Training loop\n   - Evaluation metrics\n\n3. **Find the tricky parts** - What's non-obvious?\n   - Custom layers or operations\n   - Numerical stability concerns\n   - Hyperparameter sensitivity\n   - Implementation details buried in appendices\n\n4. **Gather reference numbers** - What should we expect?\n   - Training loss trajectory\n   - Validation metrics at convergence\n   - Compute requirements (if stated)\n\n### Phase 2: Scaffolded Implementation\n\nBuild up the implementation in this order:\n\n#### Step 1: Data\n```python\n# Start with synthetic/toy data\n# Verify shapes and types before touching real data\n```\n\n**Checkpoint:** Can you describe what each tensor represents and its expected shape?\n\n#### Step 2: Model Architecture\n```python\n# Build layer by layer\n# Print shapes at each stage\n# Verify parameter counts match paper\n```\n\n**Checkpoint:** If you randomly initialize and do a forward pass, do the output shapes match what the paper describes?\n\n#### Step 3: Loss Function\n```python\n# Implement exactly as described\n# Test with known inputs/outputs\n# Check gradient flow\n```\n\n**Checkpoint:** Can you explain each term in the loss and why it's there?\n\n#### Step 4: Training Loop\n```python\n# Minimal loop first (no logging, checkpointing, etc.)\n# Verify loss decreases on tiny overfit test\n# Then add bells and whistles\n```\n\n**Checkpoint:** Can you overfit a single batch? If not, something is broken.\n\n#### Step 5: Evaluation\n```python\n# Implement paper's exact metrics\n# Compare against reported numbers\n```\n\n**Checkpoint:** On the same data split, how close are you to paper's numbers?\n\n### Phase 3: The Debugging Gauntlet\n\nWhen it doesn't work (and it won't at first):\n\n1. **The Overfit Test**\n   - Can you memorize 1 example? 10? 100?\n   - If not, architecture or gradient bug\n\n2. **The Gradient Check**\n   - Are gradients flowing to all parameters?\n   - Any NaN or exploding gradients?\n\n3. **The Initialization Check**\n   - Match paper's initialization exactly\n   - This matters more than people think\n\n4. **The Learning Rate Sweep**\n   - Log scale: 1e-5 to 1e-1\n   - Loss should decrease for some range\n\n5. **The Ablation Debug**\n   - Remove components until it works\n   - Add back one at a time\n\n### Phase 4: Checkpoint Questions\n\nAt each stage, you should be able to answer:\n\n**Understanding:**\n- Why does this component exist?\n- What would happen without it?\n- What alternatives were considered?\n\n**Implementation:**\n- Why this specific implementation choice?\n- Where could numerical issues arise?\n- What's the computational complexity?\n\n**Debugging:**\n- What would it look like if this was broken?\n- How would you test this in isolation?\n- What are the most likely bugs?\n\n## Output Format\n\nFor each implementation session, provide:\n\n```markdown\n## Today's Implementation Goal\n[Specific component we're building]\n\n## Prerequisites Check\n- [ ] Previous components working\n- [ ] Understand what we're building\n- [ ] Know expected behavior\n\n## Implementation\n\n### Code\n[Code blocks with extensive comments]\n\n### Checkpoint Questions\n1. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n2. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n### Verification Steps\n- [ ] Test 1: [What to check]\n- [ ] Test 2: [What to check]\n\n### Common Bugs at This Stage\n1. [Bug pattern]: [How to identify and fix]\n\n## What's Next\n[Preview of next component and how it connects]\n```\n\n## Tips for Specific Paper Types\n\n### Transformer-based\n- Attention mask shapes are the #1 bug source\n- Verify positional encoding is applied correctly\n- Check layer norm placement (pre vs post)\n\n### RL/Policy Gradient\n- Sign errors in policy gradient are silent killers\n- Advantage normalization matters\n- Verify discount factor handling\n\n### Generative Models\n- KL term balancing is finicky\n- Check latent space distribution\n- Verify reconstruction looks reasonable before training\n\n### Computer Vision\n- Normalization (ImageNet stats, batch norm) is crucial\n- Data augmentation can make or break results\n- Verify input preprocessing matches paper exactly\n\n## Success Criteria\n\nYou're done when:\n\n1. **Numbers match** - Within reasonable variance of paper's results\n2. **Understanding is deep** - You can explain every line of code\n3. **You found the gotchas** - You know what breaks and why\n4. **You could modify it** - Confident to try your own variations\n\n## Anti-Patterns to Avoid\n\n- ‚ùå Copying code you don't understand\n- ‚ùå Skipping checkpoint questions\n- ‚ùå Using pre-built components for core algorithm\n- ‚ùå Ignoring discrepancies with paper\n- ‚ùå Moving on before current step works"
              },
              {
                "name": "ios-app-icon-generator",
                "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons.",
                "path": "skills/ios-app-icon-generator/SKILL.md",
                "frontmatter": {
                  "name": "ios-app-icon-generator",
                  "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons."
                },
                "content": "# iOS App Icon Generator\n\nCreate beautiful, production-ready iOS app icons through a two-phase creative process.\n\n## Phase 1: Visual Philosophy\n\nBefore drawing anything, develop a 2-3 paragraph **Icon Philosophy** that articulates:\n\n- **Core concept**: What single idea or feeling should the icon convey?\n- **Visual metaphor**: What shape, object, or abstraction represents the app's purpose?\n- **Color psychology**: What palette evokes the right emotional response?\n- **Silhouette test**: Will it be recognizable as a tiny black shape?\n\nWrite this philosophy out. It guides every design decision.\n\n### Design Principles\n\nIcons that work follow these rules:\n\n- **Simplicity**: One focal element. No more than 2-3 colors. No text (illegible at small sizes).\n- **Distinctiveness**: Must stand out in a grid of 30 other icons. Avoid generic symbols (gears, checkmarks, clouds).\n- **Scalability**: The 16x16 notification icon must read as clearly as the 1024x1024 App Store version.\n- **No photography**: Apple's guidelines discourage photos. Use illustration, geometry, or abstract forms.\n- **Optical balance**: Center of visual weight, not geometric center. Curves feel heavier than straight lines.\n\n## Phase 2: Icon Generation\n\nGenerate the icon as a **self-contained HTML file** with embedded SVG that:\n\n1. Renders the icon design at 1024x1024 (the master size)\n2. Includes iOS-style rounded corners (superellipse, not CSS border-radius)\n3. Shows a preview grid of all sizes to verify readability\n4. Provides a download mechanism for each size\n\n### Required Sizes\n\nGenerate all iOS app icon sizes:\n\n| Size | Purpose |\n|------|---------|\n| 1024x1024 | App Store |\n| 180x180 | iPhone (@3x) |\n| 167x167 | iPad Pro (@2x) |\n| 152x152 | iPad (@2x) |\n| 120x120 | iPhone (@2x) |\n| 87x87 | Spotlight (@3x) |\n| 80x80 | Spotlight (@2x) |\n| 76x76 | iPad (@1x) |\n| 60x60 | iPhone (@1x) |\n| 58x58 | Settings (@2x) |\n| 40x40 | Spotlight (@1x) |\n| 29x29 | Settings (@1x) |\n| 20x20 | Notification (@1x) |\n\n### HTML Artifact Structure\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>App Icon: [Name]</title>\n  <style>\n    /* Dark interface, icon grid layout, download buttons */\n  </style>\n</head>\n<body>\n  <!-- Philosophy statement -->\n  <!-- Master SVG at 1024x1024 -->\n  <!-- Preview grid showing all sizes -->\n  <!-- Download buttons (use canvas to convert SVG ‚Üí PNG) -->\n  <script>\n    // SVG ‚Üí Canvas ‚Üí PNG download logic\n  </script>\n</body>\n</html>\n```\n\n### SVG Guidelines\n\n- Use `viewBox=\"0 0 1024 1024\"` for the master\n- Apply the iOS squircle mask (superellipse with n‚âà5)\n- Use gradients sparingly but effectively\n- Ensure googd stroke widths scale proportionally\n- Test: zoom browser to 25% - is the icon still clear?\n\n### iOS Squircle Mask\n\nThe iOS icon shape is NOT a rounded rectangle. Use this superellipse path or approximate with:\n\n```svg\n<clipPath id=\"ios-squircle\">\n  <path d=\"M512,1024 C252,1024 0,772 0,512 C0,252 252,0 512,0 C772,0 1024,252 1024,512 C1024,772 772,1024 512,1024 Z\" />\n</clipPath>\n```\n\nOr generate programmatically with the superellipse formula: `|x/a|^n + |y/b|^n = 1` where n ‚âà 5.\n\n## Process\n\n1. Ask about the app's purpose, name, and any existing brand colors\n2. Write the Icon Philosophy\n3. Describe 2-3 concept directions with rationale\n4. Get user approval on a direction\n5. Generate the HTML artifact with full icon set\n6. Iterate based on feedback\n\n## Quality Bar\n\nThe output should look like it belongs on a top-10 App Store chart. Every icon in that grid was crafted by a professional designer - yours should be indistinguishable from theirs.\n\nAvoid:\n- Glossy/skeuomorphic styles (outdated since iOS 7)\n- Thin hairline details (disappear at small sizes)\n- Overly complex illustrations\n- Generic clip-art aesthetics\n- Centered-circle-on-gradient laziness"
              },
              {
                "name": "paper-to-intuition",
                "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams.",
                "path": "skills/paper-to-intuition/SKILL.md",
                "frontmatter": {
                  "name": "paper-to-intuition",
                  "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams."
                },
                "content": "# Paper to Intuition\n\nTransform dense academic papers into genuine understanding through layered explanation and visual intuition.\n\n## Process\n\n1. **Get the paper** - Ask for the arXiv link, PDF, or paper title\n2. **Extract the core** - Identify the single key insight (one sentence)\n3. **Build the ladder** - Create explanations at 4 levels\n4. **Visualize intuition** - Generate interactive diagrams\n5. **Stress test understanding** - \"What breaks if we remove X?\"\n\n## The Explanation Ladder\n\nGenerate explanations at each level, with each building on the last:\n\n### Level 1: ELI5 (1 paragraph)\n- No jargon, no equations\n- Use familiar analogies from everyday life\n- A curious 10-year-old should roughly get it\n\n### Level 2: Undergraduate (2-3 paragraphs)\n- Assume calculus, basic linear algebra, intro ML\n- Introduce key terms with definitions\n- Connect to textbook concepts they'd know\n\n### Level 3: Graduate (3-4 paragraphs)\n- Assume ML fundamentals, optimization, probability\n- Discuss relationship to prior work\n- Explain why naive approaches don't work\n- Cover the key equations with plain-English annotations\n\n### Level 4: Researcher (2-3 paragraphs)\n- Assume field expertise\n- Subtle technical contributions\n- Limitations and open questions\n- How this changes what's possible\n\n## Key Equations Breakdown\n\nFor each important equation:\n\n```\n[Equation in LaTeX]\n\nIn words: [Plain English translation]\n\nEach term:\n- [symbol]: [what it represents] [why it's there]\n\nIntuition: [Why this mathematical form? What would change if we used a different form?]\n```\n\n## Visual Intuition Artifact\n\nGenerate a self-contained HTML file with:\n\n- **Architecture diagram** - Boxes and arrows showing information flow\n- **Interactive sliders** - Manipulate key parameters, see effects\n- **Before/after comparisons** - What the method improves over baselines\n- **Failure case visualization** - When and why it breaks down\n\nUse SVG for diagrams, vanilla JavaScript for interactivity. Dark theme, clean typography.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>[Paper Name] - Visual Intuition</title>\n  <style>\n    :root { --bg: #1a1a2e; --text: #eee; --accent: #4f8cff; }\n    /* Clean, research-aesthetic styling */\n  </style>\n</head>\n<body>\n  <h1>[Paper Title]</h1>\n  <p class=\"tldr\">[One-sentence insight]</p>\n\n  <section id=\"architecture\">\n    <svg><!-- Information flow diagram --></svg>\n  </section>\n\n  <section id=\"interactive\">\n    <!-- Parameter sliders with live updates -->\n  </section>\n\n  <section id=\"comparisons\">\n    <!-- Before/after, ablations -->\n  </section>\n</body>\n</html>\n```\n\n## The \"What Breaks?\" Analysis\n\nFor each major component, explain:\n\n1. **What it does** - The role this component plays\n2. **What breaks without it** - Concrete failure mode\n3. **Why this solution** - Alternatives considered, why this won\n4. **The tradeoff** - What we pay for this choice (compute, complexity, assumptions)\n\n## Output Structure\n\nDeliver as a structured document:\n\n```markdown\n# [Paper Title]\n\n**TL;DR:** [One sentence]\n\n**Why it matters:** [One paragraph on significance]\n\n## The Explanation Ladder\n\n### ELI5\n[...]\n\n### Undergraduate Level\n[...]\n\n### Graduate Level\n[...]\n\n### Researcher Level\n[...]\n\n## Key Equations\n\n### Equation 1: [Name]\n[Breakdown as specified above]\n\n## What Breaks If We Remove...\n\n### [Component 1]\n[Analysis]\n\n### [Component 2]\n[Analysis]\n\n## Visual Intuition\n\n[Link to or embed HTML artifact]\n\n## Further Reading\n\n- [Prerequisite paper 1]\n- [Follow-up work 1]\n```\n\n## Quality Standards\n\n- Every analogy must be accurate, not just catchy\n- Equations must be explained, not just translated\n- Visuals must reveal structure, not just decorate\n- The researcher-level section should contain insight, not just summary\n- Admit when something is genuinely confusing or poorly explained in the original paper"
              },
              {
                "name": "research-question-refiner",
                "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis.",
                "path": "skills/research-question-refiner/SKILL.md",
                "frontmatter": {
                  "name": "research-question-refiner",
                  "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis."
                },
                "content": "# Research Question Refiner\n\nTransform \"I'm interested in X\" into \"I will investigate whether Y under conditions Z, measuring W.\"\n\n## The Problem\n\nMost research ideas fail not because they're bad, but because they're:\n- Too vague to act on\n- Too ambitious to complete\n- Too incremental to matter\n- Missing a clear success criterion\n\nThis skill fixes that.\n\n## Process\n\n### Stage 1: Excavate the Interest\n\nStart by understanding what's actually pulling at you:\n\n**Questions to ask:**\n1. What sparked this interest? (Paper, conversation, problem you encountered?)\n2. What's the version that excites you most?\n3. What would be cool if it worked?\n4. Who would care about the answer?\n\n**Output:** A paragraph capturing the raw interest, unfiltered.\n\n### Stage 2: Map the Territory\n\nBefore scoping, understand the landscape:\n\n**What's Known:**\n- What's the current state-of-the-art?\n- What are the established approaches?\n- What have people tried that didn't work?\n\n**What's Unknown:**\n- What are the acknowledged open problems?\n- What assumptions does current work make?\n- Where do methods fail?\n\n**What's Controversial:**\n- Where do researchers disagree?\n- What's claimed but not convincingly shown?\n- What's believed but not rigorously tested?\n\n**Output:** A structured map with citations/references for each area.\n\n### Stage 3: Find the Gap\n\nA good research question lives in a gap that is:\n\n| Property | Too Little | Just Right | Too Much |\n|----------|-----------|------------|----------|\n| **Novelty** | Redoing existing work | New angle or combination | No foundation to build on |\n| **Difficulty** | Trivial to answer | Challenging but doable | Requires breakthroughs |\n| **Impact** | No one cares | Community would update beliefs | Nobel prize (unrealistic) |\n| **Scope** | One experiment | Thesis chapter / paper | Multiple PhDs |\n\n**Gap-finding questions:**\n- What would change if we relaxed assumption X?\n- What if we applied method A to domain B?\n- What's between approach X and approach Y?\n- What fails in setting Z that works elsewhere?\n\n**Output:** 3-5 candidate gaps, each as one sentence.\n\n### Stage 4: Refine to Concrete Question\n\nFor each candidate gap, sharpen into a question:\n\n**The Formula:**\n```\n[Action verb] + [specific phenomenon] + [under conditions] + [measurable outcome]\n```\n\n**Examples of refinement:**\n\n‚ùå Vague: \"How can we make transformers more efficient?\"\n‚úÖ Concrete: \"Does structured sparsity in attention patterns preserve performance on long-context tasks while reducing compute by >50%?\"\n\n‚ùå Vague: \"Can robots learn from humans better?\"\n‚úÖ Concrete: \"Does incorporating gaze direction in demonstrations improve sample efficiency for manipulation tasks compared to kinesthetic teaching alone?\"\n\n‚ùå Vague: \"What makes language models hallucinate?\"\n‚úÖ Concrete: \"Do retrieval-augmented models hallucinate less on factual questions when retrieval confidence is used to modulate generation temperature?\"\n\n### Stage 5: Feasibility Check\n\nFor each refined question, assess:\n\n**Resources Required:**\n- Compute: GPU-hours estimate\n- Data: Available or needs collection?\n- Time: Weeks/months realistically\n- Expertise: What skills are needed?\n\n**Risk Assessment:**\n- What's the probability this works at all?\n- What if the hypothesis is wrong? (Is negative result publishable?)\n- What could go wrong technically?\n- What could invalidate the whole direction?\n\n**Dependencies:**\n- Does this require other work to finish first?\n- Are there rate-limiting steps?\n- What can be parallelized?\n\n### Stage 6: The Litmus Tests\n\nA good research question passes all of these:\n\n**The Advisor Test:**\n> \"If I pitched this in 2 minutes, would a busy professor say 'yes, go do that' rather than 'hmm, let's talk more'?\"\n\n**The Paper Test:**\n> \"Can I envision the title, abstract, and figure 1 of the resulting paper?\"\n\n**The Null Result Test:**\n> \"If my hypothesis is wrong, would that still be interesting to report?\"\n\n**The Motivation Test:**\n> \"Am I actually excited to work on this for 6+ months?\"\n\n**The Explanation Test:**\n> \"Can I explain why this matters to a smart non-expert in 60 seconds?\"\n\n## Output Format\n\nDeliver a Research Question Brief:\n\n```markdown\n# Research Question Brief\n\n## The Interest (Raw)\n[Original unfiltered interest]\n\n## Territory Map\n\n### What's Known\n- [Point 1] ([citation])\n- [Point 2] ([citation])\n\n### What's Unknown\n- [Open question 1]\n- [Open question 2]\n\n### What's Controversial\n- [Debate 1]\n\n## Candidate Gaps\n1. [Gap 1]\n2. [Gap 2]\n3. [Gap 3]\n\n## Refined Questions\n\n### Question 1: [Title]\n**Statement:** [Precise question]\n**Hypothesis:** [What you expect to find]\n**Feasibility:** [Brief assessment]\n**If it works:** [Impact]\n**If it doesn't:** [What we still learn]\n\n### Question 2: [Title]\n[Same structure]\n\n## Recommendation\n[Which question to pursue and why]\n\n## Immediate Next Steps\n1. [Concrete action 1]\n2. [Concrete action 2]\n3. [Concrete action 3]\n```\n\n## Common Failure Modes\n\n**The Kitchen Sink:** Trying to answer too many questions at once\n‚Üí Fix: Ruthlessly cut until there's ONE core question\n\n**The Solution in Search of a Problem:** Starting with a method, not a question\n‚Üí Fix: Ask \"Who has this problem? Why hasn't it been solved?\"\n\n**The Incremental Trap:** Small delta on existing work\n‚Üí Fix: Ask \"Would this change how people think?\"\n\n**The Impossible Dream:** Beautiful question, can't be answered\n‚Üí Fix: Ask \"What's the minimal version that's still interesting?\"\n\n**The Boring Sure Thing:** Will definitely work, nobody cares\n‚Üí Fix: Add ambition until there's meaningful risk"
              },
              {
                "name": "research-taste-developer",
                "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently.",
                "path": "skills/research-taste-developer/SKILL.md",
                "frontmatter": {
                  "name": "research-taste-developer",
                  "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently."
                },
                "content": "# Research Taste Developer\n\nResearch taste is the ability to distinguish work that matters from work that doesn't - before the community tells you. This skill helps you develop that instinct.\n\n## What is Research Taste?\n\nIt's the intuition that lets experienced researchers:\n- Pick problems that turn out to be important\n- Know when an idea is \"close\" vs. \"far\" from working\n- Recognize a good result even with imperfect execution\n- Predict which papers will be remembered in 5 years\n\nTaste isn't magic - it's pattern recognition from deep exposure. This skill accelerates that exposure.\n\n## Process\n\n### Phase 1: Analyze the Field\n\nPick a specific subfield. We'll study what \"good\" looks like there.\n\n**Questions to investigate:**\n1. What are the 10 most-cited papers of the last 5 years?\n2. What are the 5 papers experts say \"changed how we think\"?\n3. What are the best papers from top venues (NeurIPS, ICML, CVPR, etc.)?\n4. What got awards? What got invited talks?\n\n**For each landmark paper, analyze:**\n- What was the state before this paper?\n- What's the single core insight?\n- What specifically made people cite it?\n- Was it obvious in hindsight?\n\n### Phase 2: Pattern Recognition\n\nLook for what the great papers have in common:\n\n**The Patterns of Impact:**\n\n#### 1. The New Primitive\nPapers that introduce a building block others build on.\n- Examples: Attention mechanism, ResNet skip connections, Dropout\n- Pattern: Simple idea, surprisingly general applicability\n- Why it works: Reduces friction for future work\n\n#### 2. The Surprising Connection\nPapers that link two previously separate areas.\n- Examples: VAE (variational inference + neural nets), NeRF (neural nets + ray marching)\n- Pattern: \"X, but for Y\" where the combination is non-obvious\n- Why it works: Cross-pollinates communities\n\n#### 3. The Scaling Insight\nPapers showing that scale changes qualitative behavior.\n- Examples: GPT-3, Chinchilla\n- Pattern: What everyone \"knew\" was wrong at sufficient scale\n- Why it works: Forces field to update beliefs\n\n#### 4. The Rigorous Foundation\nPapers that formalize what was previously folklore.\n- Examples: Theoretical convergence proofs, generalization bounds\n- Pattern: Makes hand-wavy intuitions precise\n- Why it works: Enables confident building\n\n#### 5. The Elegant Solution\nPapers that solve a problem far more simply than expected.\n- Examples: Simple baseline papers, \"X is all you need\"\n- Pattern: Previous solutions were overcomplicated\n- Why it works: Shifts field's complexity assumptions\n\n### Phase 3: Anti-Patterns\n\nLearn to recognize work that won't age well:\n\n**The Incremental Treadmill:**\n- Pattern: +0.5% on benchmark with architectural tweak\n- Why it fails: No one remembers or uses it\n- Exception: When it reveals something fundamental\n\n**The Method Mashing:**\n- Pattern: \"We combine A, B, C, and D\"\n- Why it fails: No insight about why the combination works\n- Exception: When combination reveals unexpected interaction\n\n**The Benchmark Overfitter:**\n- Pattern: Method that works only on specific benchmarks\n- Why it fails: Doesn't transfer, forgotten when benchmarks change\n- Exception: When it exposes benchmark weaknesses\n\n**The Complexity Monster:**\n- Pattern: Works but requires 47 hyperparameters and 3 loss terms\n- Why it fails: No one can reproduce or build on it\n- Exception: Rarely\n\n**The Solution Without a Problem:**\n- Pattern: Novel method without compelling use case\n- Why it fails: \"Interesting but why?\"\n- Exception: When use case emerges later (rare)\n\n### Phase 4: Develop Your Own Taste\n\n**Exercise 1: Prediction Game**\nBefore reading a paper, predict based on title/abstract:\n- Will this paper be cited >100 times in 5 years?\n- Write down your prediction and reasoning\n- Track your accuracy over time\n- Analyze where your predictions went wrong\n\n**Exercise 2: Explain the Gap**\nFor any two papers in citation count:\n- Paper A: 2000 citations\n- Paper B: 50 citations (same venue, same year)\n- What explains the difference?\n- Write a paragraph explanation\n\n**Exercise 3: The Time Machine**\nPick a highly-cited paper. Go back to when it was published:\n- What was the state of the field?\n- Would you have recognized its importance?\n- What signals would you have looked for?\n\n**Exercise 4: Design a Hit**\nGiven current state of a field:\n- What's the most important open problem?\n- What would a \"great paper\" on this look like?\n- What would make people cite it?\n\n### Phase 5: Meta-Principles\n\nWhat top researchers seem to do differently:\n\n**Problem Selection:**\n- Work on problems that are \"ready\" (pieces exist, no one assembled them)\n- Avoid problems that are stuck for fundamental reasons\n- Pick problems where you have unfair advantages\n\n**Execution Taste:**\n- Know when to stop polishing (diminishing returns)\n- Know when result is \"strong enough\" to share\n- Prefer simple-that-works over complex-that-works-slightly-better\n\n**Communication Taste:**\n- Lead with the insight, not the method\n- Make contribution obvious in first 2 minutes\n- Anticipate and address likely objections\n\n**Portfolio Taste:**\n- Mix safe and risky projects\n- Build a coherent research identity\n- Create compound interest (each paper enables the next)\n\n## Output: Taste Development Report\n\n```markdown\n# Research Taste Analysis: [Field/Subfield]\n\n## Landmark Paper Analysis\n\n### [Paper 1 Title] ([Year])\n- **Pre-existing state:** [What was true before]\n- **Core insight:** [One sentence]\n- **Why it's cited:** [Specific reason]\n- **Pattern type:** [New Primitive / Connection / etc.]\n\n### [Paper 2 Title]\n[Same structure]\n\n## Pattern Distribution\nIn this subfield, highly-cited papers tend to be:\n- [X]% New Primitives\n- [Y]% Surprising Connections\n- [Z]% Other\n\n## Anti-Pattern Warnings\nThe following patterns are common but don't lead to impact:\n1. [Anti-pattern common in this field]\n2. [Another one]\n\n## Taste Heuristics for [Field]\nWhen evaluating a paper in this field, ask:\n1. [Field-specific question that distinguishes good from meh]\n2. [Another one]\n3. [Another one]\n\n## Current Opportunities\nBased on this analysis, promising directions seem to be:\n1. [Direction 1]: [Why it's ripe]\n2. [Direction 2]: [Why it's ripe]\n\n## Your Taste Development Exercises\n1. [Specific exercise for this field]\n2. [Another one]\n```\n\n## The Ultimate Test\n\nYou have good taste when:\n- You're bored by work others find impressive (correctly predicting it won't matter)\n- You're excited by work others overlook (correctly predicting it will matter)\n- Your intuitions about importance are calibrated with reality\n- You can articulate *why* something is good, not just that it is\n\nThis takes years. But deliberate practice - not just reading, but *analyzing* - accelerates it dramatically."
              },
              {
                "name": "reviewer-2-simulator",
                "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims.",
                "path": "skills/reviewer-2-simulator/SKILL.md",
                "frontmatter": {
                  "name": "reviewer-2-simulator",
                  "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims."
                },
                "content": "# Reviewer 2 Simulator\n\nChannel the energy of the harshest (but fair) reviewer to find weaknesses before your actual reviewers do.\n\n## The Mindset\n\nReviewer 2 is:\n- Skeptical but not hostile\n- Technically rigorous\n- Short on time (will skim, not read carefully)\n- Looking for reasons to reject (high-volume venues)\n- But wants to champion good work\n\nReviewer 2 is NOT:\n- Trying to be mean\n- Unfamiliar with the field (usually)\n- Unable to be convinced by good arguments\n\n## Process\n\n### Phase 1: First Pass (5-minute skim)\n\nRead like a busy reviewer would:\n- Title and abstract\n- Figures and captions\n- Section headers\n- Conclusion\n\n**First-pass questions:**\n1. Can I understand the contribution from abstract alone?\n2. Do the figures tell the story?\n3. Is this obviously incremental or obviously interesting?\n4. Any immediate red flags?\n\n### Phase 2: Deep Read Critique\n\nGo section by section:\n\n#### Abstract\n- [ ] Clear problem statement?\n- [ ] Specific contribution (not vague \"we propose...\")?\n- [ ] Key result with number?\n- [ ] Any overclaims?\n\n**Common issues:**\n- \"We achieve state-of-the-art\" without specifying where/what\n- \"Novel\" without explaining what's actually new\n- Claims not supported in the paper\n\n#### Introduction\n- [ ] Motivation compelling?\n- [ ] Gap in prior work clearly identified?\n- [ ] Contribution stated precisely?\n- [ ] Paper organization clear?\n\n**Common issues:**\n- Straw-man characterization of prior work\n- Gap is manufactured, not real\n- Contribution buried in paragraph 4\n\n#### Related Work\n- [ ] Comprehensive coverage?\n- [ ] Fair characterization of prior work?\n- [ ] Clear differentiation from closest work?\n- [ ] Missing obvious citations?\n\n**Common issues:**\n- Missing direct competitors\n- Misrepresenting prior work to look better\n- No clear statement of difference from closest work\n\n#### Method\n- [ ] Technically sound?\n- [ ] Reproducible from description?\n- [ ] Assumptions stated explicitly?\n- [ ] Notation consistent?\n\n**Common issues:**\n- Hand-wavy justification\n- Critical details in appendix (or missing entirely)\n- Unstated assumptions\n- Notation changes mid-paper\n\n#### Experiments\n- [ ] Baselines appropriate and strong?\n- [ ] Metrics justified?\n- [ ] Ablations support claims?\n- [ ] Statistical significance addressed?\n- [ ] Error bars / variance reported?\n\n**Common issues:**\n- Weak or outdated baselines\n- Metric chosen to favor method\n- Missing ablations for key components\n- Single seed results\n- Cherry-picked examples\n\n#### Results/Analysis\n- [ ] Claims supported by evidence?\n- [ ] Alternative explanations considered?\n- [ ] Limitations acknowledged?\n- [ ] Failure cases shown?\n\n**Common issues:**\n- Overclaiming from marginal improvements\n- Ignoring results that don't fit narrative\n- No discussion of when method fails\n\n#### Conclusion\n- [ ] Restates contribution accurately?\n- [ ] Future work is genuine (not hand-wavy)?\n- [ ] Doesn't introduce new claims?\n\n### Phase 3: The Killer Questions\n\nThese are the questions that sink papers:\n\n**Novelty:**\n- \"How is this different from [X]?\" (where X is obvious prior work)\n- \"Why couldn't you just do [simpler thing]?\"\n- \"What's the actual technical contribution?\"\n\n**Significance:**\n- \"Why should anyone care about this?\"\n- \"What changes if this paper exists vs. doesn't?\"\n- \"Is this solving a real problem or a made-up one?\"\n\n**Soundness:**\n- \"How do you know [claim]?\"\n- \"What if [assumption] is violated?\"\n- \"Did you try [obvious baseline]?\"\n\n**Clarity:**\n- \"What exactly do you mean by [term]?\"\n- \"How would someone reproduce this?\"\n- \"Why is [unexplained design choice] the right choice?\"\n\n### Phase 4: Scoring\n\nRate on standard conference criteria:\n\n| Criterion | Score (1-5) | Justification |\n|-----------|-------------|---------------|\n| **Novelty** | | How new is this? |\n| **Significance** | | How much does it matter? |\n| **Soundness** | | Is it technically correct? |\n| **Clarity** | | Is it well-written? |\n| **Reproducibility** | | Could I implement this? |\n\n**Overall Recommendation:**\n- Strong Accept: Top 5%, must be in conference\n- Weak Accept: Above threshold, would be OK to accept\n- Borderline: Could go either way\n- Weak Reject: Below threshold, but not fatally flawed\n- Strong Reject: Fundamental issues\n\n## Output Format\n\n```markdown\n# Reviewer 2 Report: [Paper Title]\n\n## Summary (2-3 sentences)\n[What the paper does and claims]\n\n## Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n## Weaknesses\n\n### Major Issues (any one is grounds for rejection)\n1. **[Issue Title]**\n   - What's wrong: [Description]\n   - Why it matters: [Impact on claims]\n   - How to fix: [Concrete suggestion]\n\n### Minor Issues (should be fixed but not fatal)\n1. **[Issue Title]**\n   - [Description and suggestion]\n\n### Nitpicks (take or leave)\n- [Small thing 1]\n- [Small thing 2]\n\n## Questions for Authors\n1. [Question that must be answered]\n2. [Question that would strengthen paper]\n\n## Missing References\n- [Paper 1]: [Why it should be cited]\n- [Paper 2]: [Why it should be cited]\n\n## Scores\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Novelty | X/5 | |\n| Significance | X/5 | |\n| Soundness | X/5 | |\n| Clarity | X/5 | |\n\n## Overall Assessment\n**Recommendation:** [Accept/Reject with confidence]\n\n**In one sentence:** [The core issue or strength]\n\n## Author Rebuttal Priorities\nIf I were the author, I would address these in order:\n1. [Most important thing to address]\n2. [Second most important]\n3. [Third]\n```\n\n## Calibration Notes\n\n**Reviewer 2 is harsh but fair:**\n- Points out real issues, not imagined ones\n- Suggests fixes, not just complaints\n- Acknowledges strengths genuinely\n- Would update opinion if given good rebuttal\n\n**Reviewer 2 is NOT:**\n- Dismissive without reason\n- Demanding impossible experiments\n- Rejecting due to missing tangential work\n- Penalizing for honest limitations"
              },
              {
                "name": "ted-mosby",
                "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured.",
                "path": "skills/ted-mosby/SKILL.md",
                "frontmatter": {
                  "name": "ted-mosby",
                  "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured."
                },
                "content": "# Ted Mosby - Architecture Wiki Generator\n\nGenerate comprehensive architectural documentation for any codebase with source code traceability (file:line references).\n\n## Overview\n\nTed Mosby creates architectural wikis that help developers understand codebases. Every concept links directly to source code, so you can navigate from documentation to implementation.\n\n**Output includes:**\n- Architecture overview with Mermaid diagrams\n- Module documentation with source traceability\n- Data flow documentation\n- Getting started guides\n- Interactive static site with search, keyboard nav, dark mode\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Document a codebase or project architecture\n- Generate a wiki or documentation site\n- Create architecture diagrams with source references\n- Understand and document how a project is structured\n- Produce navigable documentation with file:line traceability\n\n**Trigger phrases:**\n- \"Generate docs for this project\"\n- \"Create architecture documentation\"\n- \"Document this codebase\"\n- \"Make a wiki for this repo\"\n- \"Help me understand this project's structure\"\n\n## Prerequisites\n\n### Required\n- Node.js >= 18.0.0\n- Anthropic API key (`ANTHROPIC_API_KEY` environment variable)\n\n### Check Prerequisites\n```bash\n# Verify Node.js version\nnode --version  # Should be >= 18.0.0\n\n# Verify API key is set\necho $ANTHROPIC_API_KEY  # Should show your key\n```\n\n### Install Ted Mosby\n```bash\nnpm install -g ted-mosby\n```\n\n## Quick Start Commands\n\n### Basic Wiki Generation\n```bash\n# Generate wiki for current directory\nted-mosby generate -r .\n\n# Generate wiki for a specific project\nted-mosby generate -r ./my-project\n\n# Generate wiki for a GitHub repository\nted-mosby generate -r https://github.com/user/repo\n```\n\n### With Interactive Site\n```bash\n# Generate wiki + interactive static site\nted-mosby generate -r ./my-project --site\n\n# Custom title and theme\nted-mosby generate -r ./my-project --site --site-title \"My Project Docs\" --theme dark\n\n# Generate site only (if wiki already exists)\nted-mosby generate -r ./my-project --site-only\n```\n\n### Other Useful Options\n```bash\n# Focus on specific subdirectory\nted-mosby generate -r ./my-project -p src/core\n\n# Custom output directory\nted-mosby generate -r ./my-project -o ./docs/architecture\n\n# Verbose output (see agent progress)\nted-mosby generate -r ./my-project -v\n\n# Estimate time/cost before running (dry run)\nted-mosby generate -r ./my-project -e\n```\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nBefore running Ted Mosby, clarify with the user:\n\n1. **Target path** - What directory or repo to document?\n2. **Output location** - Where should the wiki go? (default: `./wiki`)\n3. **Site generation** - Do they want an interactive static site?\n4. **Focus area** - Any specific subdirectory to focus on?\n5. **Theme preference** - Light, dark, or auto?\n\n### Step 2: Pre-flight Checks\n\nVerify the environment is ready:\n\n```bash\n# Check Node.js version\nnode --version\n\n# Verify ted-mosby is installed\nwhich ted-mosby || echo \"Run: npm install -g ted-mosby\"\n\n# Check API key\n[ -z \"$ANTHROPIC_API_KEY\" ] && echo \"Set ANTHROPIC_API_KEY environment variable\"\n```\n\n### Step 3: Run Generation\n\nChoose the appropriate command based on user needs:\n\n| User Wants | Command |\n|------------|---------|\n| Basic wiki only | `ted-mosby generate -r ./project` |\n| Wiki + interactive site | `ted-mosby generate -r ./project --site` |\n| Site with custom title | `ted-mosby generate -r ./project --site --site-title \"Docs\"` |\n| Dark theme site | `ted-mosby generate -r ./project --site --theme dark` |\n| Focus on subdirectory | `ted-mosby generate -r ./project -p src/core` |\n| Large codebase | `ted-mosby generate -r ./project --max-chunks 5000` |\n| Quick iteration | `ted-mosby generate -r ./project --skip-index` |\n\n### Step 4: Review Output\n\nAfter generation completes:\n\n1. **Wiki location:** `./wiki/README.md` (or custom output dir)\n2. **Site location:** `./wiki/site/index.html` (if `--site` used)\n3. **Open site:** Open `index.html` in browser\n\n### Step 5: Fix Issues (if needed)\n\nIf there are broken links or missing pages:\n\n```bash\n# Check for and generate missing pages\nted-mosby continue -r ./my-project -o ./wiki\n\n# Verify only (don't generate)\nted-mosby continue -r ./my-project -o ./wiki --verify-only\n```\n\n## Output Structure\n\n```\nwiki/\n‚îú‚îÄ‚îÄ README.md                    # Navigation entry point\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ overview.md              # System architecture + Mermaid diagrams\n‚îÇ   ‚îî‚îÄ‚îÄ data-flow.md             # Data flow documentation\n‚îú‚îÄ‚îÄ components/\n‚îÇ   ‚îî‚îÄ‚îÄ {module}/\n‚îÇ       ‚îî‚îÄ‚îÄ index.md             # Per-module documentation\n‚îú‚îÄ‚îÄ guides/\n‚îÇ   ‚îî‚îÄ‚îÄ getting-started.md       # Quick start guide\n‚îú‚îÄ‚îÄ glossary.md                  # Concept index\n‚îî‚îÄ‚îÄ site/                        # (with --site flag)\n    ‚îú‚îÄ‚îÄ index.html               # Interactive site entry\n    ‚îú‚îÄ‚îÄ styles.css\n    ‚îî‚îÄ‚îÄ scripts.js\n```\n\n## Source Traceability\n\nEvery architectural concept includes clickable source references:\n\n```markdown\n## Authentication Flow\n\nThe authentication system uses JWT tokens for stateless auth.\n\n**Source:** [`src/auth/jwt-provider.ts:23-67`](../../../src/auth/jwt-provider.ts#L23-L67)\n```\n\nThis allows developers to navigate directly from documentation to implementation.\n\n## Interactive Site Features\n\nWhen `--site` is used, the generated site includes:\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Instant search across all pages (Cmd/Ctrl+K) |\n| Keyboard navigation | Arrow keys, vim-style (j/k/h/l) |\n| Dark/light mode | Respects system preference or manual toggle |\n| Table of contents | Auto-generated from headings |\n| Mobile responsive | Works on all devices |\n| Offline capable | No server required |\n| Mermaid diagrams | Rendered automatically |\n\n## Command Reference\n\n### `generate` - Create wiki documentation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path/url>` | Repository path or GitHub URL (required) | - |\n| `-o, --output <dir>` | Output directory for wiki | `./wiki` |\n| `-p, --path <path>` | Focus on specific directory | - |\n| `-s, --site` | Generate interactive static site | - |\n| `--site-only` | Generate site only (skip wiki) | - |\n| `--site-title <title>` | Custom site title | Project name |\n| `--theme <theme>` | Site theme: light, dark, auto | `auto` |\n| `-v, --verbose` | Show detailed progress | - |\n| `-e, --estimate` | Estimate time/cost (dry run) | - |\n| `--max-chunks <n>` | Limit indexed chunks (for large repos) | unlimited |\n| `--skip-index` | Use cached embeddings index | - |\n| `--direct-api` | Use Anthropic API directly | - |\n| `-m, --model <model>` | Claude model to use | `claude-sonnet-4-20250514` |\n| `--max-turns <n>` | Limit agent iterations | 200 |\n\n### `continue` - Resume/fix wiki generation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path>` | Repository path (required) | - |\n| `-o, --output <dir>` | Wiki output directory | `./wiki` |\n| `--verify-only` | Only check, don't generate | - |\n| `--skip-index` | Use cached embeddings index | - |\n| `-v, --verbose` | Show detailed progress | - |\n\n## Large Codebase Options\n\nFor repositories with 10,000+ files:\n\n```bash\n# Limit indexed chunks (reduces memory usage)\nted-mosby generate -r ./large-project --max-chunks 5000\n\n# Reduce search results per query\nted-mosby generate -r ./large-project --max-results 5\n\n# Batched processing (for very large repos)\nted-mosby generate -r ./large-project --batch-size 3000\n```\n\n## Typical Runtime\n\n| Codebase Size | Approximate Time |\n|---------------|------------------|\n| Small (<50 files) | 1-2 minutes |\n| Medium (50-200 files) | 2-5 minutes |\n| Large (200+ files) | 5-10 minutes |\n\nUse `--estimate` to get a cost/time estimate before running.\n\n## Troubleshooting\n\n### \"Credit balance is too low\" error\nUse direct API mode:\n```bash\nted-mosby generate -r ./my-project --direct-api\n```\n\n### Out of memory on large repos\nLimit indexed chunks:\n```bash\nted-mosby generate -r ./large-project --max-chunks 5000 --batch-size 3000\n```\n\n### Slow re-runs during development\nSkip re-indexing:\n```bash\nted-mosby generate -r ./my-project --skip-index\n```\n\n### Missing pages / broken links\nUse the continue command:\n```bash\nted-mosby continue -r ./my-project -o ./wiki\n```\n\n## Example Conversation\n\n**User:** \"Can you document this project's architecture?\"\n\n**Assistant:** I'll use Ted Mosby to generate architectural documentation for your project.\n\nFirst, let me verify the prerequisites are in place, then generate the wiki with an interactive site:\n\n```bash\n# Generate wiki with interactive site\nted-mosby generate -r . --site --site-title \"Project Architecture\"\n```\n\nThis will create:\n- `wiki/README.md` - Main navigation\n- `wiki/architecture/overview.md` - Architecture diagrams\n- `wiki/site/index.html` - Interactive documentation site\n\n## Resources\n\n- [Ted Mosby GitHub](https://github.com/your-username/ted-mosby)\n- [Build an Agent Workshop](https://buildanagentworkshop.com)"
              },
              {
                "name": "turn-this-feature-into-a-blog-post",
                "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\".",
                "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
                "frontmatter": {
                  "name": "turn-this-feature-into-a-blog-post",
                  "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\"."
                },
                "content": "# Turn This Feature Into a Blog Post\n\nGenerate a Markdown blog post that explains a code implementation in an engaging, educational way.\n\n## Process\n\n1. **Analyze the implementation** - Read and understand all relevant code files, tracing the feature from entry point to completion\n2. **Identify the narrative** - Find the core problem being solved and why it matters\n3. **Structure the post** - Organize as What ‚Üí Why ‚Üí How (from first principles)\n4. **Write accessibly** - Use friendly, conversational language while maintaining technical authority\n5. **Output Markdown** - Create a complete `.md` file ready for publishing\n\n## Blog Post Structure\n\n### Title\n- Clear, specific, and searchable\n- Format: \"How We Built [Feature]\" or \"Building [Feature]: A Deep Dive\"\n\n### Introduction (2-3 paragraphs)\n- Hook the reader with the problem or outcome\n- Briefly explain what the feature does\n- Preview what readers will learn\n\n### The What (1-2 sections)\n- Describe the feature from the user's perspective\n- Include screenshots or diagrams if applicable\n- Keep technical jargon minimal\n\n### The Why (1-2 sections)\n- Explain the problem this solves\n- Discuss alternatives considered and why this approach won\n- Connect to broader engineering principles\n\n### The How (2-4 sections)\n- Walk through the implementation from first principles\n- Include relevant code snippets with explanations\n- Explain non-obvious decisions\n- Build up complexity gradually\n\n### Conclusion\n- Summarize key takeaways\n- Mention potential future improvements\n- Invite engagement (questions, feedback)\n\n## Writing Style\n\n- **Friendly but authoritative** - Write like a knowledgeable colleague explaining over coffee\n- **First-person plural** - Use \"we\" to create shared ownership\n- **Active voice** - \"We built\" not \"It was built\"\n- **Show, don't just tell** - Use code examples liberally\n- **Explain the \"why\"** - Every code block should have context\n- **Avoid jargon walls** - Define terms on first use\n\n## Code Snippets\n\n- Include only relevant portions, not entire files\n- Add comments for non-obvious lines\n- Use syntax highlighting with language tags\n- Provide context before each snippet\n\n## Output\n\nSave the blog post as a Markdown file with:\n- Kebab-case filename matching the title\n- Frontmatter with title, date, author, and tags (if appropriate for the target platform)\n- Properly formatted headers, code blocks, and lists"
              }
            ]
          },
          {
            "name": "documentation-skills",
            "description": "Skills for generating architectural documentation and wikis with source code traceability",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add GhostScientist/skills",
              "/plugin install documentation-skills@GhostScientist-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-10T22:28:06Z",
              "created_at": "2025-12-08T15:22:00Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "create-watchos-version",
                "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation.",
                "path": "skills/create-watchos-version/SKILL.md",
                "frontmatter": {
                  "name": "create-watchos-version",
                  "description": "Analyzes existing iOS/macOS/Apple platform projects to create a comprehensive, phased plan for building a watchOS companion or standalone app. Use when users want to add watchOS support to an existing Apple platform app, create a Watch app version of their iOS app, or build watchOS features. The skill digests project architecture, identifies patterns, analyzes API compatibility, searches for current watchOS documentation, and produces a detailed implementation plan with API availability warnings before any code generation."
                },
                "content": "# Create watchOS Version\n\nAnalyzes existing Apple platform projects and creates detailed, phased implementation plans for watchOS apps that are elegant, top-tier experiences‚Äînot naive skins of the parent app.\n\n## Workflow\n\n1. **Project Discovery** - Analyze project structure, patterns, architecture\n2. **Feature Mapping** - Identify watchOS-suitable features and priorities\n3. **API Compatibility** - Search web for current watchOS API availability\n4. **Architecture Planning** - Design watchOS-specific architecture\n5. **Plan Generation** - Create phased plan with warnings and alternatives\n6. **User Review** - Present plan for approval before implementation\n\n## Phase 1: Project Discovery\n\nScan project root for:\n\n```\n‚îú‚îÄ‚îÄ App Architecture (SwiftUI, UIKit, AppKit, hybrid)\n‚îú‚îÄ‚îÄ Data Layer (Core Data, SwiftData, Realm, custom)\n‚îú‚îÄ‚îÄ Networking (URLSession, Alamofire, custom)\n‚îú‚îÄ‚îÄ State Management (ObservableObject, TCA, Redux-like)\n‚îú‚îÄ‚îÄ Navigation (NavigationStack, Coordinator)\n‚îú‚îÄ‚îÄ Shared Frameworks (SPM packages, shared targets)\n‚îú‚îÄ‚îÄ Assets (colors, images, SF Symbols)\n‚îú‚îÄ‚îÄ Existing Watch Target (if any)\n‚îî‚îÄ‚îÄ Minimum iOS Version (affects watchOS targeting)\n```\n\nKey files: `*.xcodeproj`, `Package.swift`, `Info.plist`, App entry points, ViewModels, Models.\n\n## Phase 2: Feature Mapping\n\n**Glanceable (High Priority)**: Status displays, counters, progress, recent items, quick stats\n\n**Quick Actions (High Priority)**: Single-tap toggles, shortcuts, haptic confirmations\n\n**Complications/Widgets (Critical)**: Map data to WidgetKit families‚ÄîaccessoryCircular, accessoryRectangular, accessoryInline, accessoryCorner. Consider Smart Stack relevance.\n\n**Background**: HealthKit integration, background refresh, Watch Connectivity sync\n\n**Defer/Exclude**: Complex data entry, long-form content, sustained screen time features\n\n## Phase 3: API Compatibility\n\n**CRITICAL**: Always search web for current watchOS docs before finalizing. APIs change frequently.\n\nSearch: `[FrameworkName] watchOS availability site:developer.apple.com`\n\n### Quick Reference\n\n**Available**: SwiftUI, SwiftData (10+), WidgetKit (9+), HealthKit, WorkoutKit, CoreLocation (limited), WatchConnectivity, CloudKit, CoreMotion, AVFoundation (audio), CoreBluetooth, Combine, Swift Concurrency\n\n**Unavailable/Limited**: UIKit, WebKit, MapKit (limited), CoreImage (limited), ARKit, RealityKit, StoreKit (limited), Background URLSession (limited)\n\nSee `references/api-compatibility.md` for detailed compatibility matrix.\n\n## Phase 4: Architecture\n\n### Version Targeting\n\n```\niOS 16+ ‚Üí watchOS 9+  (WidgetKit complications)\niOS 17+ ‚Üí watchOS 10+ (SwiftData, Smart Stack)\niOS 18+ ‚Üí watchOS 11+ (Live Activities on Watch)\n```\n\n### Structure\n\n```\nShared/\n‚îú‚îÄ‚îÄ Models/           # Pure Swift, shared via target membership\n‚îú‚îÄ‚îÄ Services/         # Platform-agnostic logic\n‚îî‚îÄ‚îÄ Utilities/\n\nWatchApp/\n‚îú‚îÄ‚îÄ App.swift\n‚îú‚îÄ‚îÄ Views/\n‚îú‚îÄ‚îÄ ViewModels/\n‚îú‚îÄ‚îÄ Complications/\n‚îî‚îÄ‚îÄ WatchConnectivity/\n```\n\n### Design Principles\n\n1. **Glanceability** - Visible within 2 seconds\n2. **Minimal Interaction** - 1-3 taps max\n3. **Context Awareness** - Time, location, activity\n4. **Battery Conscious** - Efficient refresh, TimelineSchedule\n5. **Haptic Feedback** - Confirm actions appropriately\n\n### SwiftUI Gotchas\n\n- Avoid nested TabViews (memory leaks)\n- Use TimelineSchedule for efficient metric updates\n- Check `isLuminanceReduced` to reduce work when dimmed\n- Don't use data-driven high-frequency UI refreshes\n\n## Phase 5: Plan Generation\n\nUse template in `references/plan-template.md` to generate:\n\n1. Executive Summary\n2. ‚ö†Ô∏è API Compatibility Warnings table\n3. Phased implementation tasks\n4. Testing checklist\n\n## Phase 6: User Review\n\nPresent plan and ask for approval before implementing:\n\n> \"I've analyzed your project and created a watchOS plan. Before proceeding:\n> 1. **API Warnings**: [N] APIs unavailable‚Äîalternatives documented.\n> 2. **Recommended Features**: [list] prioritized for Watch.\n> 3. **Scope**: [N] phases.\n> \n> Proceed with implementation, or adjust the plan?\"\n\n**Do not implement until user approves.**\n\n## Best Practices Reference\n\n### Watch Connectivity\n\n```swift\nguard WCSession.default.activationState == .activated else { return }\n// sendMessage: immediate, requires reachability\n// transferUserInfo: queued, guaranteed\n// transferCurrentComplicationUserInfo: complication priority\n```\n\n### Complications\n\n```swift\n// Use appropriate reload policy\nTimeline(entries: entries, policy: .after(nextUpdateDate))\n// Use .never for static complications\n```\n\n### Battery Efficiency\n\n- Timeline-based over active refresh\n- Check `isLuminanceReduced`\n- Batch Watch Connectivity transfers\n- Significant location change vs continuous updates"
              },
              {
                "name": "experiment-design-checklist",
                "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates.",
                "path": "skills/experiment-design-checklist/SKILL.md",
                "frontmatter": {
                  "name": "experiment-design-checklist",
                  "description": "Generates a rigorous experiment design given a hypothesis. Use when asked to design experiments, plan experiments, create an experimental setup, or figure out how to test a research hypothesis. Covers controls, baselines, ablations, metrics, statistical tests, and compute estimates."
                },
                "content": "# Experiment Design Checklist\n\nPrevent the \"I ran experiments for 3 months and they're meaningless\" disaster through rigorous upfront design.\n\n## The Core Principle\n\nBefore running ANY experiment, you should be able to answer:\n1. What specific claim will this experiment support or refute?\n2. What would convince a skeptical reviewer?\n3. What could go wrong that would invalidate the results?\n\n## Process\n\n### Step 1: State the Hypothesis Precisely\n\nConvert your research question into falsifiable predictions:\n\n**Template:**\n```\nIf [intervention/method], then [measurable outcome], because [mechanism].\n```\n\n**Examples:**\n- \"If we add auxiliary contrastive loss, then downstream task accuracy increases by >2%, because representations become more separable.\"\n- \"If we use learned positional encodings, then performance on sequences >4096 tokens improves, because the model can extrapolate beyond training length.\"\n\n**Null hypothesis:** What does \"no effect\" look like? This is what you're trying to reject.\n\n### Step 2: Identify Variables\n\n**Independent Variables (what you manipulate):**\n| Variable | Levels | Rationale |\n|----------|--------|-----------|\n| [Var 1] | [Level A, B, C] | [Why these levels] |\n\n**Dependent Variables (what you measure):**\n| Metric | How Measured | Why This Metric |\n|--------|--------------|-----------------|\n| [Metric 1] | [Procedure] | [Justification] |\n\n**Control Variables (what you hold constant):**\n| Variable | Fixed Value | Why Fixed |\n|----------|-------------|-----------|\n| [Var 1] | [Value] | [Prevents confound X] |\n\n### Step 3: Choose Baselines\n\nEvery experiment needs comparisons. No result is meaningful in isolation.\n\n**Baseline Hierarchy:**\n\n1. **Random/Trivial Baseline**\n   - What does random chance achieve?\n   - Sanity check that the task isn't trivial\n\n2. **Simple Baseline**\n   - Simplest reasonable approach\n   - Often embarrassingly effective\n\n3. **Standard Baseline**\n   - Well-known method from literature\n   - Apples-to-apples comparison\n\n4. **State-of-the-Art Baseline**\n   - Current best published result\n   - Only if you're claiming SOTA\n\n5. **Ablated Self**\n   - Your method minus key components\n   - Shows each component contributes\n\n**For each baseline, document:**\n- Source (paper, implementation)\n- Hyperparameters used\n- Whether you re-ran or used reported numbers\n- Any modifications made\n\n### Step 4: Design Ablations\n\nAblations answer: \"Is each component necessary?\"\n\n**Ablation Template:**\n| Variant | What's Removed/Changed | Expected Effect | If No Effect... |\n|---------|----------------------|-----------------|-----------------|\n| Full Model | Nothing | Best performance | - |\n| w/o Component A | Remove A | Performance drops X% | A isn't helping |\n| w/o Component B | Remove B | Performance drops Y% | B isn't helping |\n| Component A only | Only A, no B | Shows A's isolated contribution | - |\n\n**Good ablations are:**\n- Surgical (one change at a time)\n- Interpretable (clear what was changed)\n- Informative (result tells you something)\n\n### Step 5: Address Confounds\n\nThings that could explain your results OTHER than your hypothesis:\n\n**Common Confounds:**\n\n| Confound | How to Check | How to Control |\n|----------|--------------|----------------|\n| Hyperparameter tuning advantage | Same tuning budget for all | Report tuning procedure |\n| Compute advantage | Matched FLOPs/params | Report compute used |\n| Data leakage | Check train/test overlap | Strict separation |\n| Random seed luck | Multiple seeds | Report variance |\n| Implementation bugs (baseline) | Verify baseline numbers | Use official implementations |\n| Cherry-picked examples | Random or systematic selection | Pre-register selection criteria |\n\n### Step 6: Statistical Rigor\n\n**Sample Size:**\n- How many random seeds? (Minimum: 3, better: 5+)\n- How many data splits? (If applicable)\n- Power analysis: Can you detect expected effect size?\n\n**What to Report:**\n- Mean ¬± standard deviation (or standard error)\n- Confidence intervals where appropriate\n- Statistical significance tests if claiming \"better\"\n\n**Appropriate Tests:**\n| Comparison | Test | Assumptions |\n|------------|------|-------------|\n| Two methods, normal data | t-test | Normality, equal variance |\n| Two methods, unknown dist | Mann-Whitney U | Ordinal data |\n| Multiple methods | ANOVA + post-hoc | Normality |\n| Multiple methods, unknown | Kruskal-Wallis | Ordinal data |\n| Paired comparisons | Wilcoxon signed-rank | Same test instances |\n\n**Avoid:**\n- p-hacking (running until significant)\n- Multiple comparison problems (Bonferroni correct)\n- Reporting only favorable metrics\n\n### Step 7: Compute Budget\n\nBefore running, estimate:\n\n| Component | Estimate | Notes |\n|-----------|----------|-------|\n| Single training run | X GPU-hours | [Details] |\n| Hyperparameter search | Y runs √ó X hours | [Search strategy] |\n| Baselines | Z runs √ó W hours | [Which baselines] |\n| Ablations | N variants √ó X hours | [Which ablations] |\n| Seeds | M seeds √ó above | [How many seeds] |\n| **Total** | **T GPU-hours** | Buffer: 1.5-2x |\n\n**Go/No-Go Decision:** Is this feasible with available resources?\n\n### Step 8: Pre-Registration (Optional but Recommended)\n\nWrite down BEFORE running:\n- Exact hypotheses\n- Primary metrics (not chosen post-hoc)\n- Analysis plan\n- What would constitute \"success\"\n\nThis prevents unconscious goal-post moving.\n\n## Output: Experiment Design Document\n\n```markdown\n# Experiment Design: [Title]\n\n## Hypothesis\n[Precise statement]\n\n## Variables\n### Independent\n[Table]\n\n### Dependent\n[Table]\n\n### Controls\n[Table]\n\n## Baselines\n1. [Baseline 1]: [Source, details]\n2. [Baseline 2]: [Source, details]\n\n## Ablations\n[Table]\n\n## Confound Mitigation\n[Table]\n\n## Statistical Plan\n- Seeds: [N]\n- Tests: [Which tests for which comparisons]\n- Significance threshold: [Œ± level]\n\n## Compute Budget\n[Table with total estimate]\n\n## Success Criteria\n- Primary: [What must be true]\n- Secondary: [Nice to have]\n\n## Timeline\n- Phase 1: [What, when]\n- Phase 2: [What, when]\n\n## Known Risks\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n```\n\n## Red Flags in Experiment Design\n\nüö© \"We'll figure out the metrics later\"\nüö© \"One run should be enough\"\nüö© \"We don't need baselines, it's obviously better\"\nüö© \"Let's just see what happens\"\nüö© \"We can always run more if it's not significant\"\nüö© No compute estimate before starting\nüö© Vague success criteria"
              },
              {
                "name": "hugging-face-space-deployer",
                "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons.",
                "path": "skills/hugging-face-space-deployer/SKILL.md",
                "frontmatter": {
                  "name": "hugging-face-space-deployer",
                  "description": "Create, configure, and deploy Hugging Face Spaces for showcasing ML models. Supports Gradio, Streamlit, and Docker SDKs with templates for common use cases like chat interfaces, image generation, and model comparisons."
                },
                "content": "# Hugging Face Space Deployer\n\nA skill for AI engineers to create, configure, and deploy interactive ML demos on Hugging Face Spaces.\n\n## CRITICAL: Pre-Deployment Checklist\n\n**Before writing ANY code, gather this information about the model:**\n\n### 1. Check Model Type (LoRA Adapter vs Full Model)\n\n**Use the HF MCP tool to inspect the model files:**\n```\nhf-skills - Hub Repo Details (repo_ids: [\"username/model\"], repo_type: \"model\")\n```\n\n**Look for these indicators:**\n\n| Files Present | Model Type | Action Required |\n|---------------|------------|-----------------|\n| `model.safetensors` or `pytorch_model.bin` | Full model | Load directly with `AutoModelForCausalLM` |\n| `adapter_model.safetensors` + `adapter_config.json` | LoRA/PEFT adapter | Must load base model first, then apply adapter with `peft` |\n| Only config files, no weights | Broken/incomplete | Ask user to verify |\n\n**If adapter_config.json exists, check for `base_model_name_or_path` to identify the base model.**\n\n### 2. Check Inference API Availability\n\nVisit the model page on HF Hub and look for \"Inference Providers\" widget on the right side.\n\n**Indicators that model HAS Inference API:**\n- Inference widget visible on model page\n- Model from known provider: `meta-llama`, `mistralai`, `HuggingFaceH4`, `google`, `stabilityai`, `Qwen`\n- High download count (>10,000) with standard architecture\n\n**Indicators that model DOES NOT have Inference API:**\n- Personal namespace (e.g., `GhostScientist/my-model`)\n- LoRA/PEFT adapter (adapters never have direct Inference API)\n- Missing `pipeline_tag` in model metadata\n- No inference widget on model page\n\n### 3. Check Model Metadata\n\n- Ensure `pipeline_tag` is set (e.g., `text-generation`)\n- Add `conversational` tag for chat models\n\n### 4. Determine Hardware Needs\n\n| Model Size | Recommended Hardware |\n|------------|---------------------|\n| < 3B parameters | ZeroGPU (free) or CPU |\n| 3B - 7B parameters | ZeroGPU or T4 |\n| > 7B parameters | A10G or A100 |\n\n### 5. Ask User If Unclear\n\n**If you cannot determine the model type, ASK THE USER:**\n\n> \"I'm analyzing your model to determine the best deployment strategy. I found:\n> - [what you found about files]\n> - [what you found about inference API]\n>\n> Is this model:\n> 1. A full model you trained/uploaded?\n> 2. A LoRA/PEFT adapter on top of another model?\n> 3. Something else?\n>\n> Also, would you prefer:\n> A. Free deployment with ZeroGPU (may have queue times)\n> B. Paid GPU for faster response (~$0.60/hr)\"\n\n## Hardware Options\n\n| Hardware | Use Case | Cost |\n|----------|----------|------|\n| `cpu-basic` | Simple demos, Inference API apps | Free |\n| `cpu-upgrade` | Faster CPU inference | ~$0.03/hr |\n| **`zero-a10g`** | **Models needing GPU on-demand (recommended for most)** | **Free (with quota)** |\n| `t4-small` | Small GPU models (<7B) | ~$0.60/hr |\n| `t4-medium` | Medium GPU models | ~$0.90/hr |\n| `a10g-small` | Large models (7B-13B) | ~$1.50/hr |\n| `a10g-large` | Very large models (30B+) | ~$3.15/hr |\n| `a100-large` | Largest models | ~$4.50/hr |\n\n**ZeroGPU Note:** ZeroGPU (`zero-a10g`) provides free GPU access on-demand. The Space runs on CPU, and when a user triggers inference, a GPU is allocated temporarily (~60-120 seconds). **After deployment, you must manually set the runtime to \"ZeroGPU\" in Space Settings > Hardware.**\n\n## Deployment Decision Tree\n\n```\nAnalyze Model\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have adapter_config.json?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a LoRA adapter\n‚îÇ       ‚îú‚îÄ‚îÄ Find base_model_name_or_path in adapter_config.json\n‚îÇ       ‚îî‚îÄ‚îÄ Use Template 3 (LoRA + ZeroGPU)\n‚îÇ\n‚îú‚îÄ‚îÄ Does it have model.safetensors or pytorch_model.bin?\n‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí It's a full model\n‚îÇ       ‚îú‚îÄ‚îÄ Is it from a major provider with inference widget?\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ YES ‚Üí Use Inference API (Template 1)\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ NO ‚Üí Use ZeroGPU (Template 2)\n‚îÇ\n‚îî‚îÄ‚îÄ Neither found?\n    ‚îî‚îÄ‚îÄ ASK USER - model may be incomplete\n```\n\n## Dependencies\n\n**For Inference API (cpu-basic, free):**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**For ZeroGPU full models (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**For ZeroGPU LoRA adapters (zero-a10g, free with quota):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n## CLI Commands (CORRECT Syntax)\n\n```bash\n# Create Space\nhf repo create my-space-name --repo-type space --space-sdk gradio\n\n# Upload files\nhf upload username/space-name ./local-folder --repo-type space\n\n# Download model files to inspect\nhf download username/model-name --local-dir ./model-check --dry-run\n\n# Check what files exist in a model\nhf download username/model-name --local-dir /tmp/check --dry-run 2>&1 | grep -E '\\.(safetensors|bin|json)'\n```\n\n## Template 1: Inference API (For Supported Models)\n\n**Use when:** Model has inference widget, is from major provider, or explicitly supports serverless API.\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nMODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"  # Must support Inference API!\nclient = InferenceClient(MODEL_ID)\n\ndef respond(message, history, system_message, max_tokens, temperature, top_p):\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    response = \"\"\n    for token in client.chat_completion(\n        messages,\n        max_tokens=max_tokens,\n        stream=True,\n        temperature=temperature,\n        top_p=top_p,\n    ):\n        delta = token.choices[0].delta.content or \"\"\n        response += delta\n        yield response\n\ndemo = gr.ChatInterface(\n    respond,\n    title=\"Chat Assistant\",\n    description=\"Powered by Hugging Face Inference API\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\"),\n        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Write a Python function to sort a list\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Chat App\nemoji: üí¨\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n```\n\n## Template 2: ZeroGPU Full Model (For Models Without Inference API)\n\n**Use when:** Full model (has model.safetensors) but no Inference API support.\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"username/my-full-model\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Global model - loaded lazily on first GPU call for faster Space startup\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for user_msg, assistant_msg in history:\n        if user_msg:\n            messages.append({\"role\": \"user\", \"content\": user_msg})\n        if assistant_msg:\n            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Model\",\n    description=\"Powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me write some code\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt:**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Model\nemoji: ü§ñ\ncolorFrom: blue\ncolorTo: purple\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Template 3: ZeroGPU LoRA Adapter (CRITICAL FOR FINE-TUNED MODELS)\n\n**Use when:** Model has `adapter_config.json` and `adapter_model.safetensors` (NOT `model.safetensors`)\n\n**You MUST identify the base model from `adapter_config.json` field `base_model_name_or_path`**\n\n```python\nimport gradio as gr\nimport spaces\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Your LoRA adapter\nADAPTER_ID = \"username/my-lora-adapter\"\n# Base model (from adapter_config.json -> base_model_name_or_path)\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n\n# Load tokenizer at startup\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Global model - loaded lazily on first GPU call\nmodel = None\n\ndef load_model():\n    global model\n    if model is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            BASE_MODEL_ID,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n        model = model.merge_and_unload()  # Merge for faster inference\n    return model\n\n@spaces.GPU(duration=120)\ndef generate_response(message, history, system_message, max_tokens, temperature, top_p):\n    model = load_model()\n\n    messages = [{\"role\": \"system\", \"content\": system_message}]\n\n    for item in history:\n        if isinstance(item, (list, tuple)) and len(item) == 2:\n            user_msg, assistant_msg = item\n            if user_msg:\n                messages.append({\"role\": \"user\", \"content\": user_msg})\n            if assistant_msg:\n                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\n    messages.append({\"role\": \"user\", \"content\": message})\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=int(max_tokens),\n            temperature=float(temperature),\n            top_p=float(top_p),\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    )\n    return response\n\ndemo = gr.ChatInterface(\n    generate_response,\n    title=\"My Fine-Tuned Model\",\n    description=\"LoRA fine-tuned model powered by ZeroGPU (free!)\",\n    additional_inputs=[\n        gr.Textbox(value=\"You are a helpful assistant.\", label=\"System message\", lines=2),\n        gr.Slider(minimum=64, maximum=2048, value=512, step=64, label=\"Max tokens\"),\n        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.1, label=\"Temperature\"),\n        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p\"),\n    ],\n    examples=[\n        [\"Hello! How are you?\"],\n        [\"Help me with a coding task\"],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n**requirements.txt (MUST include peft):**\n```\ngradio>=5.0.0\ntorch\ntransformers\naccelerate\nspaces\npeft\n```\n\n**README.md:**\n```yaml\n---\ntitle: My Fine-Tuned Model\nemoji: üîß\ncolorFrom: green\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.9.1\napp_file: app.py\npinned: false\nlicense: apache-2.0\nsuggested_hardware: zero-a10g\n---\n```\n\n## Post-Deployment Steps\n\n**After uploading your Space files:**\n\n### 1. Set the Runtime Hardware (REQUIRED for GPU models)\n\n- Go to: `https://huggingface.co/spaces/USERNAME/SPACE_NAME/settings`\n- Under \"Space Hardware\", select the appropriate option:\n  - **ZeroGPU** for free on-demand GPU (recommended)\n  - Or a dedicated GPU tier if needed\n\n### 2. Verify the Space is Running\n\n- Check the Space URL for any build errors\n- Review container logs in Settings if issues occur\n\n### 3. Common Post-Deploy Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| \"No API found\" error | Hardware mismatch | Set runtime to ZeroGPU in Settings |\n| Model not loading | LoRA vs full model confusion | Check if it's an adapter, use correct template |\n| Inference API errors | Model not on serverless | Load directly with transformers instead |\n\n## Detecting Model Type - Quick Reference\n\n### Full Model\nFiles include: `model.safetensors`, `pytorch_model.bin`, or sharded versions\n```python\n# Can load directly\nmodel = AutoModelForCausalLM.from_pretrained(\"username/model\")\n```\n\n### LoRA/PEFT Adapter\nFiles include: `adapter_config.json`, `adapter_model.safetensors`\n```python\n# Must load base model first, then apply adapter\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model-id\")\nmodel = PeftModel.from_pretrained(base_model, \"username/adapter\")\nmodel = model.merge_and_unload()  # Optional: merge for faster inference\n```\n\n### Inference API Available\nModel page shows \"Inference Providers\" widget on the right side\n```python\n# Can use InferenceClient (simplest approach)\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\"username/model\")\n```\n\n## Fixing Missing pipeline_tag (To Enable Inference API)\n\nIf a model doesn't have an inference widget but should, it may be missing metadata:\n\n```bash\n# Download the README\nhf download username/model-name README.md --local-dir /tmp/fix\n\n# Edit to add pipeline_tag in YAML frontmatter:\n# ---\n# pipeline_tag: text-generation\n# tags:\n# - conversational\n# ---\n\n# Upload the fix\nhf upload username/model-name /tmp/fix/README.md README.md\n```\n\n**Note:** Even with correct tags, custom models may not get Inference API - it depends on HF's infrastructure decisions.\n\n## CRITICAL: Gradio 5.x Requirements\n\n### Examples Format (MUST be nested lists)\n```python\n# CORRECT:\nexamples=[\n    [\"Example 1\"],\n    [\"Example 2\"],\n]\n\n# WRONG (causes ValueError):\nexamples=[\n    \"Example 1\",\n    \"Example 2\",\n]\n```\n\n### Version Requirements\n```\ngradio>=5.0.0\nhuggingface_hub>=0.26.0\n```\n\nDo NOT use `gradio==4.44.0` - causes `ImportError: cannot import name 'HfFolder'`\n\n## Troubleshooting\n\n### \"No API found\" Error\n**Cause:** Gradio app isn't exposing API correctly, often due to hardware mismatch\n**Fix:** Go to Space Settings and set runtime to \"ZeroGPU\" or appropriate GPU tier\n\n### \"OSError: does not appear to have a file named pytorch_model.bin, model.safetensors\"\n**Cause:** Trying to load a LoRA adapter as a full model\n**Fix:** Check for `adapter_config.json` - if present, use PEFT to load:\n```python\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\nmodel = PeftModel.from_pretrained(base_model, \"adapter-id\")\n```\n\n### Inference API Not Available\n**Cause:** Model doesn't have pipeline_tag or isn't deployed to serverless\n**Fix:** Either:\n  a. Add `pipeline_tag: text-generation` to model's README.md\n  b. Or load model directly with transformers instead of InferenceClient\n\n### `ImportError: cannot import name 'HfFolder'`\n**Cause:** gradio/huggingface_hub version mismatch\n**Fix:** Use `gradio>=5.0.0` and `huggingface_hub>=0.26.0`\n\n### `ValueError: examples must be nested list`\n**Cause:** Gradio 5.x format change\n**Fix:** Use `[[\"ex1\"], [\"ex2\"]]` not `[\"ex1\", \"ex2\"]`\n\n### Space builds but model doesn't load\n**Cause:** Missing `peft` for adapters, or wrong base model\n**Fix:** Check adapter_config.json for correct base_model_name_or_path\n\n## Workflow Summary\n\n1. **Analyze model** (check for adapter_config.json, model files, inference widget)\n2. **Determine strategy** (Inference API vs ZeroGPU, full model vs LoRA)\n3. **Ask user if unclear** about model type or cost preferences\n4. **Generate correct template** based on analysis\n5. **Create Space** with correct requirements and README\n6. **Upload files** using `hf upload`\n7. **Set hardware** in Space Settings (ZeroGPU for free GPU access)\n8. **Monitor build logs** for any issues"
              },
              {
                "name": "implement-paper-from-scratch",
                "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions.",
                "path": "skills/implement-paper-from-scratch/SKILL.md",
                "frontmatter": {
                  "name": "implement-paper-from-scratch",
                  "description": "Guides you through implementing a research paper step-by-step from scratch. Use when asked to implement a paper, code up a paper, reproduce research results, or build a model from a paper. Focuses on building understanding through implementation with checkpoint questions."
                },
                "content": "# Implement Paper From Scratch\n\nThe best way to truly understand a paper is to implement it. This skill guides you through that process methodically.\n\n## Philosophy\n\n- **No copy-pasting from reference implementations** - We build understanding, not just working code\n- **Checkpoint questions verify understanding** - You should be able to answer \"why\" at each step\n- **Minimal dependencies** - Use NumPy/PyTorch fundamentals, not high-level wrappers\n- **Deliberate debugging** - Bugs are learning opportunities, not obstacles\n\n## Process\n\n### Phase 1: Pre-Implementation Analysis\n\nBefore writing any code:\n\n1. **Identify the core algorithm** - Strip away ablations, extensions, bells and whistles. What's the minimal version?\n\n2. **List the components** - Break into modules:\n   - Data pipeline\n   - Model architecture\n   - Loss function(s)\n   - Training loop\n   - Evaluation metrics\n\n3. **Find the tricky parts** - What's non-obvious?\n   - Custom layers or operations\n   - Numerical stability concerns\n   - Hyperparameter sensitivity\n   - Implementation details buried in appendices\n\n4. **Gather reference numbers** - What should we expect?\n   - Training loss trajectory\n   - Validation metrics at convergence\n   - Compute requirements (if stated)\n\n### Phase 2: Scaffolded Implementation\n\nBuild up the implementation in this order:\n\n#### Step 1: Data\n```python\n# Start with synthetic/toy data\n# Verify shapes and types before touching real data\n```\n\n**Checkpoint:** Can you describe what each tensor represents and its expected shape?\n\n#### Step 2: Model Architecture\n```python\n# Build layer by layer\n# Print shapes at each stage\n# Verify parameter counts match paper\n```\n\n**Checkpoint:** If you randomly initialize and do a forward pass, do the output shapes match what the paper describes?\n\n#### Step 3: Loss Function\n```python\n# Implement exactly as described\n# Test with known inputs/outputs\n# Check gradient flow\n```\n\n**Checkpoint:** Can you explain each term in the loss and why it's there?\n\n#### Step 4: Training Loop\n```python\n# Minimal loop first (no logging, checkpointing, etc.)\n# Verify loss decreases on tiny overfit test\n# Then add bells and whistles\n```\n\n**Checkpoint:** Can you overfit a single batch? If not, something is broken.\n\n#### Step 5: Evaluation\n```python\n# Implement paper's exact metrics\n# Compare against reported numbers\n```\n\n**Checkpoint:** On the same data split, how close are you to paper's numbers?\n\n### Phase 3: The Debugging Gauntlet\n\nWhen it doesn't work (and it won't at first):\n\n1. **The Overfit Test**\n   - Can you memorize 1 example? 10? 100?\n   - If not, architecture or gradient bug\n\n2. **The Gradient Check**\n   - Are gradients flowing to all parameters?\n   - Any NaN or exploding gradients?\n\n3. **The Initialization Check**\n   - Match paper's initialization exactly\n   - This matters more than people think\n\n4. **The Learning Rate Sweep**\n   - Log scale: 1e-5 to 1e-1\n   - Loss should decrease for some range\n\n5. **The Ablation Debug**\n   - Remove components until it works\n   - Add back one at a time\n\n### Phase 4: Checkpoint Questions\n\nAt each stage, you should be able to answer:\n\n**Understanding:**\n- Why does this component exist?\n- What would happen without it?\n- What alternatives were considered?\n\n**Implementation:**\n- Why this specific implementation choice?\n- Where could numerical issues arise?\n- What's the computational complexity?\n\n**Debugging:**\n- What would it look like if this was broken?\n- How would you test this in isolation?\n- What are the most likely bugs?\n\n## Output Format\n\nFor each implementation session, provide:\n\n```markdown\n## Today's Implementation Goal\n[Specific component we're building]\n\n## Prerequisites Check\n- [ ] Previous components working\n- [ ] Understand what we're building\n- [ ] Know expected behavior\n\n## Implementation\n\n### Code\n[Code blocks with extensive comments]\n\n### Checkpoint Questions\n1. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n2. [Question]\n   <details><summary>Answer</summary>[Answer]</details>\n\n### Verification Steps\n- [ ] Test 1: [What to check]\n- [ ] Test 2: [What to check]\n\n### Common Bugs at This Stage\n1. [Bug pattern]: [How to identify and fix]\n\n## What's Next\n[Preview of next component and how it connects]\n```\n\n## Tips for Specific Paper Types\n\n### Transformer-based\n- Attention mask shapes are the #1 bug source\n- Verify positional encoding is applied correctly\n- Check layer norm placement (pre vs post)\n\n### RL/Policy Gradient\n- Sign errors in policy gradient are silent killers\n- Advantage normalization matters\n- Verify discount factor handling\n\n### Generative Models\n- KL term balancing is finicky\n- Check latent space distribution\n- Verify reconstruction looks reasonable before training\n\n### Computer Vision\n- Normalization (ImageNet stats, batch norm) is crucial\n- Data augmentation can make or break results\n- Verify input preprocessing matches paper exactly\n\n## Success Criteria\n\nYou're done when:\n\n1. **Numbers match** - Within reasonable variance of paper's results\n2. **Understanding is deep** - You can explain every line of code\n3. **You found the gotchas** - You know what breaks and why\n4. **You could modify it** - Confident to try your own variations\n\n## Anti-Patterns to Avoid\n\n- ‚ùå Copying code you don't understand\n- ‚ùå Skipping checkpoint questions\n- ‚ùå Using pre-built components for core algorithm\n- ‚ùå Ignoring discrepancies with paper\n- ‚ùå Moving on before current step works"
              },
              {
                "name": "ios-app-icon-generator",
                "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons.",
                "path": "skills/ios-app-icon-generator/SKILL.md",
                "frontmatter": {
                  "name": "ios-app-icon-generator",
                  "description": "Generates a complete iOS app icon set with all required sizes. Use when asked to create an app icon, design an iOS icon, generate app store artwork, or make an icon for an iPhone/iPad app. Follows a philosophy-first approach - first defining the visual identity and concept, then producing production-ready icons."
                },
                "content": "# iOS App Icon Generator\n\nCreate beautiful, production-ready iOS app icons through a two-phase creative process.\n\n## Phase 1: Visual Philosophy\n\nBefore drawing anything, develop a 2-3 paragraph **Icon Philosophy** that articulates:\n\n- **Core concept**: What single idea or feeling should the icon convey?\n- **Visual metaphor**: What shape, object, or abstraction represents the app's purpose?\n- **Color psychology**: What palette evokes the right emotional response?\n- **Silhouette test**: Will it be recognizable as a tiny black shape?\n\nWrite this philosophy out. It guides every design decision.\n\n### Design Principles\n\nIcons that work follow these rules:\n\n- **Simplicity**: One focal element. No more than 2-3 colors. No text (illegible at small sizes).\n- **Distinctiveness**: Must stand out in a grid of 30 other icons. Avoid generic symbols (gears, checkmarks, clouds).\n- **Scalability**: The 16x16 notification icon must read as clearly as the 1024x1024 App Store version.\n- **No photography**: Apple's guidelines discourage photos. Use illustration, geometry, or abstract forms.\n- **Optical balance**: Center of visual weight, not geometric center. Curves feel heavier than straight lines.\n\n## Phase 2: Icon Generation\n\nGenerate the icon as a **self-contained HTML file** with embedded SVG that:\n\n1. Renders the icon design at 1024x1024 (the master size)\n2. Includes iOS-style rounded corners (superellipse, not CSS border-radius)\n3. Shows a preview grid of all sizes to verify readability\n4. Provides a download mechanism for each size\n\n### Required Sizes\n\nGenerate all iOS app icon sizes:\n\n| Size | Purpose |\n|------|---------|\n| 1024x1024 | App Store |\n| 180x180 | iPhone (@3x) |\n| 167x167 | iPad Pro (@2x) |\n| 152x152 | iPad (@2x) |\n| 120x120 | iPhone (@2x) |\n| 87x87 | Spotlight (@3x) |\n| 80x80 | Spotlight (@2x) |\n| 76x76 | iPad (@1x) |\n| 60x60 | iPhone (@1x) |\n| 58x58 | Settings (@2x) |\n| 40x40 | Spotlight (@1x) |\n| 29x29 | Settings (@1x) |\n| 20x20 | Notification (@1x) |\n\n### HTML Artifact Structure\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>App Icon: [Name]</title>\n  <style>\n    /* Dark interface, icon grid layout, download buttons */\n  </style>\n</head>\n<body>\n  <!-- Philosophy statement -->\n  <!-- Master SVG at 1024x1024 -->\n  <!-- Preview grid showing all sizes -->\n  <!-- Download buttons (use canvas to convert SVG ‚Üí PNG) -->\n  <script>\n    // SVG ‚Üí Canvas ‚Üí PNG download logic\n  </script>\n</body>\n</html>\n```\n\n### SVG Guidelines\n\n- Use `viewBox=\"0 0 1024 1024\"` for the master\n- Apply the iOS squircle mask (superellipse with n‚âà5)\n- Use gradients sparingly but effectively\n- Ensure googd stroke widths scale proportionally\n- Test: zoom browser to 25% - is the icon still clear?\n\n### iOS Squircle Mask\n\nThe iOS icon shape is NOT a rounded rectangle. Use this superellipse path or approximate with:\n\n```svg\n<clipPath id=\"ios-squircle\">\n  <path d=\"M512,1024 C252,1024 0,772 0,512 C0,252 252,0 512,0 C772,0 1024,252 1024,512 C1024,772 772,1024 512,1024 Z\" />\n</clipPath>\n```\n\nOr generate programmatically with the superellipse formula: `|x/a|^n + |y/b|^n = 1` where n ‚âà 5.\n\n## Process\n\n1. Ask about the app's purpose, name, and any existing brand colors\n2. Write the Icon Philosophy\n3. Describe 2-3 concept directions with rationale\n4. Get user approval on a direction\n5. Generate the HTML artifact with full icon set\n6. Iterate based on feedback\n\n## Quality Bar\n\nThe output should look like it belongs on a top-10 App Store chart. Every icon in that grid was crafted by a professional designer - yours should be indistinguishable from theirs.\n\nAvoid:\n- Glossy/skeuomorphic styles (outdated since iOS 7)\n- Thin hairline details (disappear at small sizes)\n- Overly complex illustrations\n- Generic clip-art aesthetics\n- Centered-circle-on-gradient laziness"
              },
              {
                "name": "paper-to-intuition",
                "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams.",
                "path": "skills/paper-to-intuition/SKILL.md",
                "frontmatter": {
                  "name": "paper-to-intuition",
                  "description": "Transforms an academic paper into deep, multi-layered understanding. Use when asked to explain a paper, break down a research paper, understand an arXiv paper, or build intuition for a technical concept from a paper. Generates explanations at multiple levels plus visual intuition diagrams."
                },
                "content": "# Paper to Intuition\n\nTransform dense academic papers into genuine understanding through layered explanation and visual intuition.\n\n## Process\n\n1. **Get the paper** - Ask for the arXiv link, PDF, or paper title\n2. **Extract the core** - Identify the single key insight (one sentence)\n3. **Build the ladder** - Create explanations at 4 levels\n4. **Visualize intuition** - Generate interactive diagrams\n5. **Stress test understanding** - \"What breaks if we remove X?\"\n\n## The Explanation Ladder\n\nGenerate explanations at each level, with each building on the last:\n\n### Level 1: ELI5 (1 paragraph)\n- No jargon, no equations\n- Use familiar analogies from everyday life\n- A curious 10-year-old should roughly get it\n\n### Level 2: Undergraduate (2-3 paragraphs)\n- Assume calculus, basic linear algebra, intro ML\n- Introduce key terms with definitions\n- Connect to textbook concepts they'd know\n\n### Level 3: Graduate (3-4 paragraphs)\n- Assume ML fundamentals, optimization, probability\n- Discuss relationship to prior work\n- Explain why naive approaches don't work\n- Cover the key equations with plain-English annotations\n\n### Level 4: Researcher (2-3 paragraphs)\n- Assume field expertise\n- Subtle technical contributions\n- Limitations and open questions\n- How this changes what's possible\n\n## Key Equations Breakdown\n\nFor each important equation:\n\n```\n[Equation in LaTeX]\n\nIn words: [Plain English translation]\n\nEach term:\n- [symbol]: [what it represents] [why it's there]\n\nIntuition: [Why this mathematical form? What would change if we used a different form?]\n```\n\n## Visual Intuition Artifact\n\nGenerate a self-contained HTML file with:\n\n- **Architecture diagram** - Boxes and arrows showing information flow\n- **Interactive sliders** - Manipulate key parameters, see effects\n- **Before/after comparisons** - What the method improves over baselines\n- **Failure case visualization** - When and why it breaks down\n\nUse SVG for diagrams, vanilla JavaScript for interactivity. Dark theme, clean typography.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <title>[Paper Name] - Visual Intuition</title>\n  <style>\n    :root { --bg: #1a1a2e; --text: #eee; --accent: #4f8cff; }\n    /* Clean, research-aesthetic styling */\n  </style>\n</head>\n<body>\n  <h1>[Paper Title]</h1>\n  <p class=\"tldr\">[One-sentence insight]</p>\n\n  <section id=\"architecture\">\n    <svg><!-- Information flow diagram --></svg>\n  </section>\n\n  <section id=\"interactive\">\n    <!-- Parameter sliders with live updates -->\n  </section>\n\n  <section id=\"comparisons\">\n    <!-- Before/after, ablations -->\n  </section>\n</body>\n</html>\n```\n\n## The \"What Breaks?\" Analysis\n\nFor each major component, explain:\n\n1. **What it does** - The role this component plays\n2. **What breaks without it** - Concrete failure mode\n3. **Why this solution** - Alternatives considered, why this won\n4. **The tradeoff** - What we pay for this choice (compute, complexity, assumptions)\n\n## Output Structure\n\nDeliver as a structured document:\n\n```markdown\n# [Paper Title]\n\n**TL;DR:** [One sentence]\n\n**Why it matters:** [One paragraph on significance]\n\n## The Explanation Ladder\n\n### ELI5\n[...]\n\n### Undergraduate Level\n[...]\n\n### Graduate Level\n[...]\n\n### Researcher Level\n[...]\n\n## Key Equations\n\n### Equation 1: [Name]\n[Breakdown as specified above]\n\n## What Breaks If We Remove...\n\n### [Component 1]\n[Analysis]\n\n### [Component 2]\n[Analysis]\n\n## Visual Intuition\n\n[Link to or embed HTML artifact]\n\n## Further Reading\n\n- [Prerequisite paper 1]\n- [Follow-up work 1]\n```\n\n## Quality Standards\n\n- Every analogy must be accurate, not just catchy\n- Equations must be explained, not just translated\n- Visuals must reveal structure, not just decorate\n- The researcher-level section should contain insight, not just summary\n- Admit when something is genuinely confusing or poorly explained in the original paper"
              },
              {
                "name": "research-question-refiner",
                "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis.",
                "path": "skills/research-question-refiner/SKILL.md",
                "frontmatter": {
                  "name": "research-question-refiner",
                  "description": "Helps transform a vague research interest into a concrete, tractable research question. Use when asked to refine a research idea, develop a research question, scope a research project, or figure out what to work on. Walks through systematic refinement with feasibility analysis."
                },
                "content": "# Research Question Refiner\n\nTransform \"I'm interested in X\" into \"I will investigate whether Y under conditions Z, measuring W.\"\n\n## The Problem\n\nMost research ideas fail not because they're bad, but because they're:\n- Too vague to act on\n- Too ambitious to complete\n- Too incremental to matter\n- Missing a clear success criterion\n\nThis skill fixes that.\n\n## Process\n\n### Stage 1: Excavate the Interest\n\nStart by understanding what's actually pulling at you:\n\n**Questions to ask:**\n1. What sparked this interest? (Paper, conversation, problem you encountered?)\n2. What's the version that excites you most?\n3. What would be cool if it worked?\n4. Who would care about the answer?\n\n**Output:** A paragraph capturing the raw interest, unfiltered.\n\n### Stage 2: Map the Territory\n\nBefore scoping, understand the landscape:\n\n**What's Known:**\n- What's the current state-of-the-art?\n- What are the established approaches?\n- What have people tried that didn't work?\n\n**What's Unknown:**\n- What are the acknowledged open problems?\n- What assumptions does current work make?\n- Where do methods fail?\n\n**What's Controversial:**\n- Where do researchers disagree?\n- What's claimed but not convincingly shown?\n- What's believed but not rigorously tested?\n\n**Output:** A structured map with citations/references for each area.\n\n### Stage 3: Find the Gap\n\nA good research question lives in a gap that is:\n\n| Property | Too Little | Just Right | Too Much |\n|----------|-----------|------------|----------|\n| **Novelty** | Redoing existing work | New angle or combination | No foundation to build on |\n| **Difficulty** | Trivial to answer | Challenging but doable | Requires breakthroughs |\n| **Impact** | No one cares | Community would update beliefs | Nobel prize (unrealistic) |\n| **Scope** | One experiment | Thesis chapter / paper | Multiple PhDs |\n\n**Gap-finding questions:**\n- What would change if we relaxed assumption X?\n- What if we applied method A to domain B?\n- What's between approach X and approach Y?\n- What fails in setting Z that works elsewhere?\n\n**Output:** 3-5 candidate gaps, each as one sentence.\n\n### Stage 4: Refine to Concrete Question\n\nFor each candidate gap, sharpen into a question:\n\n**The Formula:**\n```\n[Action verb] + [specific phenomenon] + [under conditions] + [measurable outcome]\n```\n\n**Examples of refinement:**\n\n‚ùå Vague: \"How can we make transformers more efficient?\"\n‚úÖ Concrete: \"Does structured sparsity in attention patterns preserve performance on long-context tasks while reducing compute by >50%?\"\n\n‚ùå Vague: \"Can robots learn from humans better?\"\n‚úÖ Concrete: \"Does incorporating gaze direction in demonstrations improve sample efficiency for manipulation tasks compared to kinesthetic teaching alone?\"\n\n‚ùå Vague: \"What makes language models hallucinate?\"\n‚úÖ Concrete: \"Do retrieval-augmented models hallucinate less on factual questions when retrieval confidence is used to modulate generation temperature?\"\n\n### Stage 5: Feasibility Check\n\nFor each refined question, assess:\n\n**Resources Required:**\n- Compute: GPU-hours estimate\n- Data: Available or needs collection?\n- Time: Weeks/months realistically\n- Expertise: What skills are needed?\n\n**Risk Assessment:**\n- What's the probability this works at all?\n- What if the hypothesis is wrong? (Is negative result publishable?)\n- What could go wrong technically?\n- What could invalidate the whole direction?\n\n**Dependencies:**\n- Does this require other work to finish first?\n- Are there rate-limiting steps?\n- What can be parallelized?\n\n### Stage 6: The Litmus Tests\n\nA good research question passes all of these:\n\n**The Advisor Test:**\n> \"If I pitched this in 2 minutes, would a busy professor say 'yes, go do that' rather than 'hmm, let's talk more'?\"\n\n**The Paper Test:**\n> \"Can I envision the title, abstract, and figure 1 of the resulting paper?\"\n\n**The Null Result Test:**\n> \"If my hypothesis is wrong, would that still be interesting to report?\"\n\n**The Motivation Test:**\n> \"Am I actually excited to work on this for 6+ months?\"\n\n**The Explanation Test:**\n> \"Can I explain why this matters to a smart non-expert in 60 seconds?\"\n\n## Output Format\n\nDeliver a Research Question Brief:\n\n```markdown\n# Research Question Brief\n\n## The Interest (Raw)\n[Original unfiltered interest]\n\n## Territory Map\n\n### What's Known\n- [Point 1] ([citation])\n- [Point 2] ([citation])\n\n### What's Unknown\n- [Open question 1]\n- [Open question 2]\n\n### What's Controversial\n- [Debate 1]\n\n## Candidate Gaps\n1. [Gap 1]\n2. [Gap 2]\n3. [Gap 3]\n\n## Refined Questions\n\n### Question 1: [Title]\n**Statement:** [Precise question]\n**Hypothesis:** [What you expect to find]\n**Feasibility:** [Brief assessment]\n**If it works:** [Impact]\n**If it doesn't:** [What we still learn]\n\n### Question 2: [Title]\n[Same structure]\n\n## Recommendation\n[Which question to pursue and why]\n\n## Immediate Next Steps\n1. [Concrete action 1]\n2. [Concrete action 2]\n3. [Concrete action 3]\n```\n\n## Common Failure Modes\n\n**The Kitchen Sink:** Trying to answer too many questions at once\n‚Üí Fix: Ruthlessly cut until there's ONE core question\n\n**The Solution in Search of a Problem:** Starting with a method, not a question\n‚Üí Fix: Ask \"Who has this problem? Why hasn't it been solved?\"\n\n**The Incremental Trap:** Small delta on existing work\n‚Üí Fix: Ask \"Would this change how people think?\"\n\n**The Impossible Dream:** Beautiful question, can't be answered\n‚Üí Fix: Ask \"What's the minimal version that's still interesting?\"\n\n**The Boring Sure Thing:** Will definitely work, nobody cares\n‚Üí Fix: Add ambition until there's meaningful risk"
              },
              {
                "name": "research-taste-developer",
                "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently.",
                "path": "skills/research-taste-developer/SKILL.md",
                "frontmatter": {
                  "name": "research-taste-developer",
                  "description": "Develops intuition for what makes research \"good\" versus \"incremental.\" Use when asked about research taste, how to identify good research, what makes a paper impactful, how to develop research intuition, or how to pick important problems. Analyzes patterns in highly-cited work and what top researchers do differently."
                },
                "content": "# Research Taste Developer\n\nResearch taste is the ability to distinguish work that matters from work that doesn't - before the community tells you. This skill helps you develop that instinct.\n\n## What is Research Taste?\n\nIt's the intuition that lets experienced researchers:\n- Pick problems that turn out to be important\n- Know when an idea is \"close\" vs. \"far\" from working\n- Recognize a good result even with imperfect execution\n- Predict which papers will be remembered in 5 years\n\nTaste isn't magic - it's pattern recognition from deep exposure. This skill accelerates that exposure.\n\n## Process\n\n### Phase 1: Analyze the Field\n\nPick a specific subfield. We'll study what \"good\" looks like there.\n\n**Questions to investigate:**\n1. What are the 10 most-cited papers of the last 5 years?\n2. What are the 5 papers experts say \"changed how we think\"?\n3. What are the best papers from top venues (NeurIPS, ICML, CVPR, etc.)?\n4. What got awards? What got invited talks?\n\n**For each landmark paper, analyze:**\n- What was the state before this paper?\n- What's the single core insight?\n- What specifically made people cite it?\n- Was it obvious in hindsight?\n\n### Phase 2: Pattern Recognition\n\nLook for what the great papers have in common:\n\n**The Patterns of Impact:**\n\n#### 1. The New Primitive\nPapers that introduce a building block others build on.\n- Examples: Attention mechanism, ResNet skip connections, Dropout\n- Pattern: Simple idea, surprisingly general applicability\n- Why it works: Reduces friction for future work\n\n#### 2. The Surprising Connection\nPapers that link two previously separate areas.\n- Examples: VAE (variational inference + neural nets), NeRF (neural nets + ray marching)\n- Pattern: \"X, but for Y\" where the combination is non-obvious\n- Why it works: Cross-pollinates communities\n\n#### 3. The Scaling Insight\nPapers showing that scale changes qualitative behavior.\n- Examples: GPT-3, Chinchilla\n- Pattern: What everyone \"knew\" was wrong at sufficient scale\n- Why it works: Forces field to update beliefs\n\n#### 4. The Rigorous Foundation\nPapers that formalize what was previously folklore.\n- Examples: Theoretical convergence proofs, generalization bounds\n- Pattern: Makes hand-wavy intuitions precise\n- Why it works: Enables confident building\n\n#### 5. The Elegant Solution\nPapers that solve a problem far more simply than expected.\n- Examples: Simple baseline papers, \"X is all you need\"\n- Pattern: Previous solutions were overcomplicated\n- Why it works: Shifts field's complexity assumptions\n\n### Phase 3: Anti-Patterns\n\nLearn to recognize work that won't age well:\n\n**The Incremental Treadmill:**\n- Pattern: +0.5% on benchmark with architectural tweak\n- Why it fails: No one remembers or uses it\n- Exception: When it reveals something fundamental\n\n**The Method Mashing:**\n- Pattern: \"We combine A, B, C, and D\"\n- Why it fails: No insight about why the combination works\n- Exception: When combination reveals unexpected interaction\n\n**The Benchmark Overfitter:**\n- Pattern: Method that works only on specific benchmarks\n- Why it fails: Doesn't transfer, forgotten when benchmarks change\n- Exception: When it exposes benchmark weaknesses\n\n**The Complexity Monster:**\n- Pattern: Works but requires 47 hyperparameters and 3 loss terms\n- Why it fails: No one can reproduce or build on it\n- Exception: Rarely\n\n**The Solution Without a Problem:**\n- Pattern: Novel method without compelling use case\n- Why it fails: \"Interesting but why?\"\n- Exception: When use case emerges later (rare)\n\n### Phase 4: Develop Your Own Taste\n\n**Exercise 1: Prediction Game**\nBefore reading a paper, predict based on title/abstract:\n- Will this paper be cited >100 times in 5 years?\n- Write down your prediction and reasoning\n- Track your accuracy over time\n- Analyze where your predictions went wrong\n\n**Exercise 2: Explain the Gap**\nFor any two papers in citation count:\n- Paper A: 2000 citations\n- Paper B: 50 citations (same venue, same year)\n- What explains the difference?\n- Write a paragraph explanation\n\n**Exercise 3: The Time Machine**\nPick a highly-cited paper. Go back to when it was published:\n- What was the state of the field?\n- Would you have recognized its importance?\n- What signals would you have looked for?\n\n**Exercise 4: Design a Hit**\nGiven current state of a field:\n- What's the most important open problem?\n- What would a \"great paper\" on this look like?\n- What would make people cite it?\n\n### Phase 5: Meta-Principles\n\nWhat top researchers seem to do differently:\n\n**Problem Selection:**\n- Work on problems that are \"ready\" (pieces exist, no one assembled them)\n- Avoid problems that are stuck for fundamental reasons\n- Pick problems where you have unfair advantages\n\n**Execution Taste:**\n- Know when to stop polishing (diminishing returns)\n- Know when result is \"strong enough\" to share\n- Prefer simple-that-works over complex-that-works-slightly-better\n\n**Communication Taste:**\n- Lead with the insight, not the method\n- Make contribution obvious in first 2 minutes\n- Anticipate and address likely objections\n\n**Portfolio Taste:**\n- Mix safe and risky projects\n- Build a coherent research identity\n- Create compound interest (each paper enables the next)\n\n## Output: Taste Development Report\n\n```markdown\n# Research Taste Analysis: [Field/Subfield]\n\n## Landmark Paper Analysis\n\n### [Paper 1 Title] ([Year])\n- **Pre-existing state:** [What was true before]\n- **Core insight:** [One sentence]\n- **Why it's cited:** [Specific reason]\n- **Pattern type:** [New Primitive / Connection / etc.]\n\n### [Paper 2 Title]\n[Same structure]\n\n## Pattern Distribution\nIn this subfield, highly-cited papers tend to be:\n- [X]% New Primitives\n- [Y]% Surprising Connections\n- [Z]% Other\n\n## Anti-Pattern Warnings\nThe following patterns are common but don't lead to impact:\n1. [Anti-pattern common in this field]\n2. [Another one]\n\n## Taste Heuristics for [Field]\nWhen evaluating a paper in this field, ask:\n1. [Field-specific question that distinguishes good from meh]\n2. [Another one]\n3. [Another one]\n\n## Current Opportunities\nBased on this analysis, promising directions seem to be:\n1. [Direction 1]: [Why it's ripe]\n2. [Direction 2]: [Why it's ripe]\n\n## Your Taste Development Exercises\n1. [Specific exercise for this field]\n2. [Another one]\n```\n\n## The Ultimate Test\n\nYou have good taste when:\n- You're bored by work others find impressive (correctly predicting it won't matter)\n- You're excited by work others overlook (correctly predicting it will matter)\n- Your intuitions about importance are calibrated with reality\n- You can articulate *why* something is good, not just that it is\n\nThis takes years. But deliberate practice - not just reading, but *analyzing* - accelerates it dramatically."
              },
              {
                "name": "reviewer-2-simulator",
                "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims.",
                "path": "skills/reviewer-2-simulator/SKILL.md",
                "frontmatter": {
                  "name": "reviewer-2-simulator",
                  "description": "Critiques your paper draft as a skeptical reviewer would. Use when asked to review a paper draft, find weaknesses in a paper, prepare for peer review, anticipate reviewer criticism, or stress-test research before submission. Identifies weak claims, missing baselines, unclear explanations, and overclaims."
                },
                "content": "# Reviewer 2 Simulator\n\nChannel the energy of the harshest (but fair) reviewer to find weaknesses before your actual reviewers do.\n\n## The Mindset\n\nReviewer 2 is:\n- Skeptical but not hostile\n- Technically rigorous\n- Short on time (will skim, not read carefully)\n- Looking for reasons to reject (high-volume venues)\n- But wants to champion good work\n\nReviewer 2 is NOT:\n- Trying to be mean\n- Unfamiliar with the field (usually)\n- Unable to be convinced by good arguments\n\n## Process\n\n### Phase 1: First Pass (5-minute skim)\n\nRead like a busy reviewer would:\n- Title and abstract\n- Figures and captions\n- Section headers\n- Conclusion\n\n**First-pass questions:**\n1. Can I understand the contribution from abstract alone?\n2. Do the figures tell the story?\n3. Is this obviously incremental or obviously interesting?\n4. Any immediate red flags?\n\n### Phase 2: Deep Read Critique\n\nGo section by section:\n\n#### Abstract\n- [ ] Clear problem statement?\n- [ ] Specific contribution (not vague \"we propose...\")?\n- [ ] Key result with number?\n- [ ] Any overclaims?\n\n**Common issues:**\n- \"We achieve state-of-the-art\" without specifying where/what\n- \"Novel\" without explaining what's actually new\n- Claims not supported in the paper\n\n#### Introduction\n- [ ] Motivation compelling?\n- [ ] Gap in prior work clearly identified?\n- [ ] Contribution stated precisely?\n- [ ] Paper organization clear?\n\n**Common issues:**\n- Straw-man characterization of prior work\n- Gap is manufactured, not real\n- Contribution buried in paragraph 4\n\n#### Related Work\n- [ ] Comprehensive coverage?\n- [ ] Fair characterization of prior work?\n- [ ] Clear differentiation from closest work?\n- [ ] Missing obvious citations?\n\n**Common issues:**\n- Missing direct competitors\n- Misrepresenting prior work to look better\n- No clear statement of difference from closest work\n\n#### Method\n- [ ] Technically sound?\n- [ ] Reproducible from description?\n- [ ] Assumptions stated explicitly?\n- [ ] Notation consistent?\n\n**Common issues:**\n- Hand-wavy justification\n- Critical details in appendix (or missing entirely)\n- Unstated assumptions\n- Notation changes mid-paper\n\n#### Experiments\n- [ ] Baselines appropriate and strong?\n- [ ] Metrics justified?\n- [ ] Ablations support claims?\n- [ ] Statistical significance addressed?\n- [ ] Error bars / variance reported?\n\n**Common issues:**\n- Weak or outdated baselines\n- Metric chosen to favor method\n- Missing ablations for key components\n- Single seed results\n- Cherry-picked examples\n\n#### Results/Analysis\n- [ ] Claims supported by evidence?\n- [ ] Alternative explanations considered?\n- [ ] Limitations acknowledged?\n- [ ] Failure cases shown?\n\n**Common issues:**\n- Overclaiming from marginal improvements\n- Ignoring results that don't fit narrative\n- No discussion of when method fails\n\n#### Conclusion\n- [ ] Restates contribution accurately?\n- [ ] Future work is genuine (not hand-wavy)?\n- [ ] Doesn't introduce new claims?\n\n### Phase 3: The Killer Questions\n\nThese are the questions that sink papers:\n\n**Novelty:**\n- \"How is this different from [X]?\" (where X is obvious prior work)\n- \"Why couldn't you just do [simpler thing]?\"\n- \"What's the actual technical contribution?\"\n\n**Significance:**\n- \"Why should anyone care about this?\"\n- \"What changes if this paper exists vs. doesn't?\"\n- \"Is this solving a real problem or a made-up one?\"\n\n**Soundness:**\n- \"How do you know [claim]?\"\n- \"What if [assumption] is violated?\"\n- \"Did you try [obvious baseline]?\"\n\n**Clarity:**\n- \"What exactly do you mean by [term]?\"\n- \"How would someone reproduce this?\"\n- \"Why is [unexplained design choice] the right choice?\"\n\n### Phase 4: Scoring\n\nRate on standard conference criteria:\n\n| Criterion | Score (1-5) | Justification |\n|-----------|-------------|---------------|\n| **Novelty** | | How new is this? |\n| **Significance** | | How much does it matter? |\n| **Soundness** | | Is it technically correct? |\n| **Clarity** | | Is it well-written? |\n| **Reproducibility** | | Could I implement this? |\n\n**Overall Recommendation:**\n- Strong Accept: Top 5%, must be in conference\n- Weak Accept: Above threshold, would be OK to accept\n- Borderline: Could go either way\n- Weak Reject: Below threshold, but not fatally flawed\n- Strong Reject: Fundamental issues\n\n## Output Format\n\n```markdown\n# Reviewer 2 Report: [Paper Title]\n\n## Summary (2-3 sentences)\n[What the paper does and claims]\n\n## Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n## Weaknesses\n\n### Major Issues (any one is grounds for rejection)\n1. **[Issue Title]**\n   - What's wrong: [Description]\n   - Why it matters: [Impact on claims]\n   - How to fix: [Concrete suggestion]\n\n### Minor Issues (should be fixed but not fatal)\n1. **[Issue Title]**\n   - [Description and suggestion]\n\n### Nitpicks (take or leave)\n- [Small thing 1]\n- [Small thing 2]\n\n## Questions for Authors\n1. [Question that must be answered]\n2. [Question that would strengthen paper]\n\n## Missing References\n- [Paper 1]: [Why it should be cited]\n- [Paper 2]: [Why it should be cited]\n\n## Scores\n| Criterion | Score | Notes |\n|-----------|-------|-------|\n| Novelty | X/5 | |\n| Significance | X/5 | |\n| Soundness | X/5 | |\n| Clarity | X/5 | |\n\n## Overall Assessment\n**Recommendation:** [Accept/Reject with confidence]\n\n**In one sentence:** [The core issue or strength]\n\n## Author Rebuttal Priorities\nIf I were the author, I would address these in order:\n1. [Most important thing to address]\n2. [Second most important]\n3. [Third]\n```\n\n## Calibration Notes\n\n**Reviewer 2 is harsh but fair:**\n- Points out real issues, not imagined ones\n- Suggests fixes, not just complaints\n- Acknowledges strengths genuinely\n- Would update opinion if given good rebuttal\n\n**Reviewer 2 is NOT:**\n- Dismissive without reason\n- Demanding impossible experiments\n- Rejecting due to missing tangential work\n- Penalizing for honest limitations"
              },
              {
                "name": "ted-mosby",
                "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured.",
                "path": "skills/ted-mosby/SKILL.md",
                "frontmatter": {
                  "name": "ted-mosby",
                  "description": "Generate architectural wikis with source code traceability. Creates comprehensive documentation including architecture overviews, module docs, data flow diagrams, and interactive static sites. Use when asked to document a codebase, generate architecture docs, create a wiki, or explain how a project is structured."
                },
                "content": "# Ted Mosby - Architecture Wiki Generator\n\nGenerate comprehensive architectural documentation for any codebase with source code traceability (file:line references).\n\n## Overview\n\nTed Mosby creates architectural wikis that help developers understand codebases. Every concept links directly to source code, so you can navigate from documentation to implementation.\n\n**Output includes:**\n- Architecture overview with Mermaid diagrams\n- Module documentation with source traceability\n- Data flow documentation\n- Getting started guides\n- Interactive static site with search, keyboard nav, dark mode\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Document a codebase or project architecture\n- Generate a wiki or documentation site\n- Create architecture diagrams with source references\n- Understand and document how a project is structured\n- Produce navigable documentation with file:line traceability\n\n**Trigger phrases:**\n- \"Generate docs for this project\"\n- \"Create architecture documentation\"\n- \"Document this codebase\"\n- \"Make a wiki for this repo\"\n- \"Help me understand this project's structure\"\n\n## Prerequisites\n\n### Required\n- Node.js >= 18.0.0\n- Anthropic API key (`ANTHROPIC_API_KEY` environment variable)\n\n### Check Prerequisites\n```bash\n# Verify Node.js version\nnode --version  # Should be >= 18.0.0\n\n# Verify API key is set\necho $ANTHROPIC_API_KEY  # Should show your key\n```\n\n### Install Ted Mosby\n```bash\nnpm install -g ted-mosby\n```\n\n## Quick Start Commands\n\n### Basic Wiki Generation\n```bash\n# Generate wiki for current directory\nted-mosby generate -r .\n\n# Generate wiki for a specific project\nted-mosby generate -r ./my-project\n\n# Generate wiki for a GitHub repository\nted-mosby generate -r https://github.com/user/repo\n```\n\n### With Interactive Site\n```bash\n# Generate wiki + interactive static site\nted-mosby generate -r ./my-project --site\n\n# Custom title and theme\nted-mosby generate -r ./my-project --site --site-title \"My Project Docs\" --theme dark\n\n# Generate site only (if wiki already exists)\nted-mosby generate -r ./my-project --site-only\n```\n\n### Other Useful Options\n```bash\n# Focus on specific subdirectory\nted-mosby generate -r ./my-project -p src/core\n\n# Custom output directory\nted-mosby generate -r ./my-project -o ./docs/architecture\n\n# Verbose output (see agent progress)\nted-mosby generate -r ./my-project -v\n\n# Estimate time/cost before running (dry run)\nted-mosby generate -r ./my-project -e\n```\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nBefore running Ted Mosby, clarify with the user:\n\n1. **Target path** - What directory or repo to document?\n2. **Output location** - Where should the wiki go? (default: `./wiki`)\n3. **Site generation** - Do they want an interactive static site?\n4. **Focus area** - Any specific subdirectory to focus on?\n5. **Theme preference** - Light, dark, or auto?\n\n### Step 2: Pre-flight Checks\n\nVerify the environment is ready:\n\n```bash\n# Check Node.js version\nnode --version\n\n# Verify ted-mosby is installed\nwhich ted-mosby || echo \"Run: npm install -g ted-mosby\"\n\n# Check API key\n[ -z \"$ANTHROPIC_API_KEY\" ] && echo \"Set ANTHROPIC_API_KEY environment variable\"\n```\n\n### Step 3: Run Generation\n\nChoose the appropriate command based on user needs:\n\n| User Wants | Command |\n|------------|---------|\n| Basic wiki only | `ted-mosby generate -r ./project` |\n| Wiki + interactive site | `ted-mosby generate -r ./project --site` |\n| Site with custom title | `ted-mosby generate -r ./project --site --site-title \"Docs\"` |\n| Dark theme site | `ted-mosby generate -r ./project --site --theme dark` |\n| Focus on subdirectory | `ted-mosby generate -r ./project -p src/core` |\n| Large codebase | `ted-mosby generate -r ./project --max-chunks 5000` |\n| Quick iteration | `ted-mosby generate -r ./project --skip-index` |\n\n### Step 4: Review Output\n\nAfter generation completes:\n\n1. **Wiki location:** `./wiki/README.md` (or custom output dir)\n2. **Site location:** `./wiki/site/index.html` (if `--site` used)\n3. **Open site:** Open `index.html` in browser\n\n### Step 5: Fix Issues (if needed)\n\nIf there are broken links or missing pages:\n\n```bash\n# Check for and generate missing pages\nted-mosby continue -r ./my-project -o ./wiki\n\n# Verify only (don't generate)\nted-mosby continue -r ./my-project -o ./wiki --verify-only\n```\n\n## Output Structure\n\n```\nwiki/\n‚îú‚îÄ‚îÄ README.md                    # Navigation entry point\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ overview.md              # System architecture + Mermaid diagrams\n‚îÇ   ‚îî‚îÄ‚îÄ data-flow.md             # Data flow documentation\n‚îú‚îÄ‚îÄ components/\n‚îÇ   ‚îî‚îÄ‚îÄ {module}/\n‚îÇ       ‚îî‚îÄ‚îÄ index.md             # Per-module documentation\n‚îú‚îÄ‚îÄ guides/\n‚îÇ   ‚îî‚îÄ‚îÄ getting-started.md       # Quick start guide\n‚îú‚îÄ‚îÄ glossary.md                  # Concept index\n‚îî‚îÄ‚îÄ site/                        # (with --site flag)\n    ‚îú‚îÄ‚îÄ index.html               # Interactive site entry\n    ‚îú‚îÄ‚îÄ styles.css\n    ‚îî‚îÄ‚îÄ scripts.js\n```\n\n## Source Traceability\n\nEvery architectural concept includes clickable source references:\n\n```markdown\n## Authentication Flow\n\nThe authentication system uses JWT tokens for stateless auth.\n\n**Source:** [`src/auth/jwt-provider.ts:23-67`](../../../src/auth/jwt-provider.ts#L23-L67)\n```\n\nThis allows developers to navigate directly from documentation to implementation.\n\n## Interactive Site Features\n\nWhen `--site` is used, the generated site includes:\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Instant search across all pages (Cmd/Ctrl+K) |\n| Keyboard navigation | Arrow keys, vim-style (j/k/h/l) |\n| Dark/light mode | Respects system preference or manual toggle |\n| Table of contents | Auto-generated from headings |\n| Mobile responsive | Works on all devices |\n| Offline capable | No server required |\n| Mermaid diagrams | Rendered automatically |\n\n## Command Reference\n\n### `generate` - Create wiki documentation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path/url>` | Repository path or GitHub URL (required) | - |\n| `-o, --output <dir>` | Output directory for wiki | `./wiki` |\n| `-p, --path <path>` | Focus on specific directory | - |\n| `-s, --site` | Generate interactive static site | - |\n| `--site-only` | Generate site only (skip wiki) | - |\n| `--site-title <title>` | Custom site title | Project name |\n| `--theme <theme>` | Site theme: light, dark, auto | `auto` |\n| `-v, --verbose` | Show detailed progress | - |\n| `-e, --estimate` | Estimate time/cost (dry run) | - |\n| `--max-chunks <n>` | Limit indexed chunks (for large repos) | unlimited |\n| `--skip-index` | Use cached embeddings index | - |\n| `--direct-api` | Use Anthropic API directly | - |\n| `-m, --model <model>` | Claude model to use | `claude-sonnet-4-20250514` |\n| `--max-turns <n>` | Limit agent iterations | 200 |\n\n### `continue` - Resume/fix wiki generation\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `-r, --repo <path>` | Repository path (required) | - |\n| `-o, --output <dir>` | Wiki output directory | `./wiki` |\n| `--verify-only` | Only check, don't generate | - |\n| `--skip-index` | Use cached embeddings index | - |\n| `-v, --verbose` | Show detailed progress | - |\n\n## Large Codebase Options\n\nFor repositories with 10,000+ files:\n\n```bash\n# Limit indexed chunks (reduces memory usage)\nted-mosby generate -r ./large-project --max-chunks 5000\n\n# Reduce search results per query\nted-mosby generate -r ./large-project --max-results 5\n\n# Batched processing (for very large repos)\nted-mosby generate -r ./large-project --batch-size 3000\n```\n\n## Typical Runtime\n\n| Codebase Size | Approximate Time |\n|---------------|------------------|\n| Small (<50 files) | 1-2 minutes |\n| Medium (50-200 files) | 2-5 minutes |\n| Large (200+ files) | 5-10 minutes |\n\nUse `--estimate` to get a cost/time estimate before running.\n\n## Troubleshooting\n\n### \"Credit balance is too low\" error\nUse direct API mode:\n```bash\nted-mosby generate -r ./my-project --direct-api\n```\n\n### Out of memory on large repos\nLimit indexed chunks:\n```bash\nted-mosby generate -r ./large-project --max-chunks 5000 --batch-size 3000\n```\n\n### Slow re-runs during development\nSkip re-indexing:\n```bash\nted-mosby generate -r ./my-project --skip-index\n```\n\n### Missing pages / broken links\nUse the continue command:\n```bash\nted-mosby continue -r ./my-project -o ./wiki\n```\n\n## Example Conversation\n\n**User:** \"Can you document this project's architecture?\"\n\n**Assistant:** I'll use Ted Mosby to generate architectural documentation for your project.\n\nFirst, let me verify the prerequisites are in place, then generate the wiki with an interactive site:\n\n```bash\n# Generate wiki with interactive site\nted-mosby generate -r . --site --site-title \"Project Architecture\"\n```\n\nThis will create:\n- `wiki/README.md` - Main navigation\n- `wiki/architecture/overview.md` - Architecture diagrams\n- `wiki/site/index.html` - Interactive documentation site\n\n## Resources\n\n- [Ted Mosby GitHub](https://github.com/your-username/ted-mosby)\n- [Build an Agent Workshop](https://buildanagentworkshop.com)"
              },
              {
                "name": "turn-this-feature-into-a-blog-post",
                "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\".",
                "path": "skills/turn-this-feature-into-a-blog-post/SKILL.md",
                "frontmatter": {
                  "name": "turn-this-feature-into-a-blog-post",
                  "description": "Generates a technical blog post from code implementation. Use when asked to write a blog post about a feature, explain an implementation for a blog, document code as a blog article, or create technical content from source code. Triggers on phrases like \"write a blog post about\", \"turn this into a blog\", \"create a technical article\", or \"explain this for a blog\"."
                },
                "content": "# Turn This Feature Into a Blog Post\n\nGenerate a Markdown blog post that explains a code implementation in an engaging, educational way.\n\n## Process\n\n1. **Analyze the implementation** - Read and understand all relevant code files, tracing the feature from entry point to completion\n2. **Identify the narrative** - Find the core problem being solved and why it matters\n3. **Structure the post** - Organize as What ‚Üí Why ‚Üí How (from first principles)\n4. **Write accessibly** - Use friendly, conversational language while maintaining technical authority\n5. **Output Markdown** - Create a complete `.md` file ready for publishing\n\n## Blog Post Structure\n\n### Title\n- Clear, specific, and searchable\n- Format: \"How We Built [Feature]\" or \"Building [Feature]: A Deep Dive\"\n\n### Introduction (2-3 paragraphs)\n- Hook the reader with the problem or outcome\n- Briefly explain what the feature does\n- Preview what readers will learn\n\n### The What (1-2 sections)\n- Describe the feature from the user's perspective\n- Include screenshots or diagrams if applicable\n- Keep technical jargon minimal\n\n### The Why (1-2 sections)\n- Explain the problem this solves\n- Discuss alternatives considered and why this approach won\n- Connect to broader engineering principles\n\n### The How (2-4 sections)\n- Walk through the implementation from first principles\n- Include relevant code snippets with explanations\n- Explain non-obvious decisions\n- Build up complexity gradually\n\n### Conclusion\n- Summarize key takeaways\n- Mention potential future improvements\n- Invite engagement (questions, feedback)\n\n## Writing Style\n\n- **Friendly but authoritative** - Write like a knowledgeable colleague explaining over coffee\n- **First-person plural** - Use \"we\" to create shared ownership\n- **Active voice** - \"We built\" not \"It was built\"\n- **Show, don't just tell** - Use code examples liberally\n- **Explain the \"why\"** - Every code block should have context\n- **Avoid jargon walls** - Define terms on first use\n\n## Code Snippets\n\n- Include only relevant portions, not entire files\n- Add comments for non-obvious lines\n- Use syntax highlighting with language tags\n- Provide context before each snippet\n\n## Output\n\nSave the blog post as a Markdown file with:\n- Kebab-case filename matching the title\n- Frontmatter with title, date, author, and tags (if appropriate for the target platform)\n- Properly formatted headers, code blocks, and lists"
              }
            ]
          }
        ]
      }
    }
  ]
}