{
  "owner": {
    "id": "kjgarza",
    "display_name": "Kristian Garza",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/1092861?u=c2a7f6bda863411cbcbb5acb483c1a3e1273f7eb&v=4",
    "url": "https://github.com/kjgarza",
    "bio": "AI Engineer | Making research more open, efficient, and effective through code and AI",
    "stats": {
      "total_repos": 1,
      "total_plugins": 4,
      "total_commands": 13,
      "total_skills": 31,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "kjgarza/marketplace-claude",
      "url": "https://github.com/kjgarza/marketplace-claude",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-11T12:23:28Z",
        "created_at": "2026-01-01T15:35:08Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2514
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 117
        },
        {
          "path": "CITATION.CFF",
          "type": "blob",
          "size": 486
        },
        {
          "path": "LICENSE.md",
          "type": "blob",
          "size": 1066
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 594
        },
        {
          "path": "plugins/kjgarza-base/README.md",
          "type": "blob",
          "size": 677
        },
        {
          "path": "plugins/kjgarza-base/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/file-organising",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/file-organising/SKILL.md",
          "type": "blob",
          "size": 11311
        },
        {
          "path": "plugins/kjgarza-base/skills/image-processing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/image-processing/SKILL.md",
          "type": "blob",
          "size": 5004
        },
        {
          "path": "plugins/kjgarza-base/skills/image-processing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/image-processing/references/imagemagick-batch.md",
          "type": "blob",
          "size": 12188
        },
        {
          "path": "plugins/kjgarza-base/skills/image-processing/references/imagemagick-editing.md",
          "type": "blob",
          "size": 13181
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/SKILL.md",
          "type": "blob",
          "size": 5312
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/references/feature-matrix.md",
          "type": "blob",
          "size": 4022
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/references/project-types.md",
          "type": "blob",
          "size": 3863
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/scripts/validate_name.py",
          "type": "blob",
          "size": 2298
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/.gitignore.tmpl",
          "type": "blob",
          "size": 168
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/package.json.tmpl",
          "type": "blob",
          "size": 218
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src/index.js.tmpl",
          "type": "blob",
          "size": 401
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src/middleware",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src/middleware/error-handler.js.tmpl",
          "type": "blob",
          "size": 430
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src/routes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-js/src/routes/health.js.tmpl",
          "type": "blob",
          "size": 196
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/.gitignore.tmpl",
          "type": "blob",
          "size": 194
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/package.json.tmpl",
          "type": "blob",
          "size": 376
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src/index.ts.tmpl",
          "type": "blob",
          "size": 401
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src/middleware",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src/middleware/error-handler.ts.tmpl",
          "type": "blob",
          "size": 304
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src/routes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/src/routes/health.ts.tmpl",
          "type": "blob",
          "size": 196
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/node-ts/tsconfig.json.tmpl",
          "type": "blob",
          "size": 484
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/.gitignore.tmpl",
          "type": "blob",
          "size": 262
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/pyproject.toml.tmpl",
          "type": "blob",
          "size": 425
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src/__init__.py.tmpl",
          "type": "blob",
          "size": 35
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src/main.py.tmpl",
          "type": "blob",
          "size": 751
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src/routes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src/routes/__init__.py.tmpl",
          "type": "blob",
          "size": 18
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/api/python/src/routes/health.py.tmpl",
          "type": "blob",
          "size": 293
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js/.gitignore.tmpl",
          "type": "blob",
          "size": 124
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js/package.json.tmpl",
          "type": "blob",
          "size": 237
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js/src/cli.js.tmpl",
          "type": "blob",
          "size": 139
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-js/src/index.js.tmpl",
          "type": "blob",
          "size": 370
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/.gitignore.tmpl",
          "type": "blob",
          "size": 145
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/package.json.tmpl",
          "type": "blob",
          "size": 398
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/src/cli.ts.tmpl",
          "type": "blob",
          "size": 110
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/src/index.ts.tmpl",
          "type": "blob",
          "size": 378
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/node-ts/tsconfig.json.tmpl",
          "type": "blob",
          "size": 431
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/.gitignore.tmpl",
          "type": "blob",
          "size": 231
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/pyproject.toml.tmpl",
          "type": "blob",
          "size": 381
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/src/__init__.py.tmpl",
          "type": "blob",
          "size": 35
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/src/__main__.py.tmpl",
          "type": "blob",
          "size": 119
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/cli/python/src/cli.py.tmpl",
          "type": "blob",
          "size": 374
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/ci-cd",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/ci-cd/github-actions-node.yml.tmpl",
          "type": "blob",
          "size": 586
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/ci-cd/github-actions-python.yml.tmpl",
          "type": "blob",
          "size": 557
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docker/Dockerfile.node.tmpl",
          "type": "blob",
          "size": 684
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docker/Dockerfile.python.tmpl",
          "type": "blob",
          "size": 581
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docker/docker-compose.yml.tmpl",
          "type": "blob",
          "size": 134
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docs/CONTRIBUTING.md.tmpl",
          "type": "blob",
          "size": 697
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docs/LICENSE.tmpl",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/docs/README.md.tmpl",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/linting",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/linting/.prettierrc.tmpl",
          "type": "blob",
          "size": 107
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/linting/eslint.config.js.tmpl",
          "type": "blob",
          "size": 326
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/linting/ruff.toml.tmpl",
          "type": "blob",
          "size": 200
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/testing/conftest.py.tmpl",
          "type": "blob",
          "size": 185
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/testing/jest.config.js.tmpl",
          "type": "blob",
          "size": 341
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/testing/pytest.ini.tmpl",
          "type": "blob",
          "size": 112
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/features/testing/vitest.config.ts.tmpl",
          "type": "blob",
          "size": 254
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/.gitignore.tmpl",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/index.html.tmpl",
          "type": "blob",
          "size": 310
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/package.json.tmpl",
          "type": "blob",
          "size": 353
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/src/App.jsx.tmpl",
          "type": "blob",
          "size": 414
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/src/index.css.tmpl",
          "type": "blob",
          "size": 854
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/src/main.jsx.tmpl",
          "type": "blob",
          "size": 225
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-js/vite.config.js.tmpl",
          "type": "blob",
          "size": 133
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/.gitignore.tmpl",
          "type": "blob",
          "size": 211
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/index.html.tmpl",
          "type": "blob",
          "size": 310
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/package.json.tmpl",
          "type": "blob",
          "size": 458
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/src/App.tsx.tmpl",
          "type": "blob",
          "size": 414
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/src/index.css.tmpl",
          "type": "blob",
          "size": 1137
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/src/main.tsx.tmpl",
          "type": "blob",
          "size": 226
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/tsconfig.json.tmpl",
          "type": "blob",
          "size": 546
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/react-ts/vite.config.ts.tmpl",
          "type": "blob",
          "size": 133
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla/.gitignore.tmpl",
          "type": "blob",
          "size": 153
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla/index.html.tmpl",
          "type": "blob",
          "size": 545
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla/main.js.tmpl",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla/package.json.tmpl",
          "type": "blob",
          "size": 239
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vanilla/style.css.tmpl",
          "type": "blob",
          "size": 854
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/.gitignore.tmpl",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/index.html.tmpl",
          "type": "blob",
          "size": 308
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/package.json.tmpl",
          "type": "blob",
          "size": 321
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/src/App.vue.tmpl",
          "type": "blob",
          "size": 509
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/src/main.js.tmpl",
          "type": "blob",
          "size": 111
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/src/style.css.tmpl",
          "type": "blob",
          "size": 691
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-js/vite.config.js.tmpl",
          "type": "blob",
          "size": 127
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/.gitignore.tmpl",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/index.html.tmpl",
          "type": "blob",
          "size": 308
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/package.json.tmpl",
          "type": "blob",
          "size": 388
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/src/App.vue.tmpl",
          "type": "blob",
          "size": 519
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/src/main.ts.tmpl",
          "type": "blob",
          "size": 111
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/src/style.css.tmpl",
          "type": "blob",
          "size": 691
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/src/vite-env.d.ts.tmpl",
          "type": "blob",
          "size": 186
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/tsconfig.json.tmpl",
          "type": "blob",
          "size": 547
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/frontend/vue-ts/vite.config.ts.tmpl",
          "type": "blob",
          "size": 127
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/.gitignore.tmpl",
          "type": "blob",
          "size": 196
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/apps",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/apps/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/package.json.tmpl",
          "type": "blob",
          "size": 288
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/packages",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/packages/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/pnpm-workspace.yaml.tmpl",
          "type": "blob",
          "size": 40
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/node/turbo.json.tmpl",
          "type": "blob",
          "size": 361
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/.gitignore.tmpl",
          "type": "blob",
          "size": 276
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/apps",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/apps/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/packages",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/packages/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/project-scaffold/templates/monorepo/python/pyproject.toml.tmpl",
          "type": "blob",
          "size": 300
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 17837
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/__pycache__",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/__pycache__/quick_validate.cpython-312.pyc",
          "type": "blob",
          "size": 4020
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10863
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/monorepo-generator.skill",
          "type": "blob",
          "size": 24601
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/kjgarza-base/skills/skill-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 3523
        },
        {
          "path": "plugins/kjgarza-product",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 807
        },
        {
          "path": "plugins/kjgarza-product/README.md",
          "type": "blob",
          "size": 2098
        },
        {
          "path": "plugins/kjgarza-product/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/agents/product-manager.md",
          "type": "blob",
          "size": 7126
        },
        {
          "path": "plugins/kjgarza-product/agents/scholarly-comms-researcher.md",
          "type": "blob",
          "size": 8030
        },
        {
          "path": "plugins/kjgarza-product/agents/senior-dev-advisor.md",
          "type": "blob",
          "size": 9431
        },
        {
          "path": "plugins/kjgarza-product/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/commands/analyze-feature-request.md",
          "type": "blob",
          "size": 5079
        },
        {
          "path": "plugins/kjgarza-product/commands/analyze-intel.md",
          "type": "blob",
          "size": 9343
        },
        {
          "path": "plugins/kjgarza-product/commands/create-prd.md",
          "type": "blob",
          "size": 6450
        },
        {
          "path": "plugins/kjgarza-product/commands/create-user-stories.md",
          "type": "blob",
          "size": 13341
        },
        {
          "path": "plugins/kjgarza-product/commands/facilitate-design-critique.md",
          "type": "blob",
          "size": 10475
        },
        {
          "path": "plugins/kjgarza-product/commands/plan-usability-test.md",
          "type": "blob",
          "size": 14168
        },
        {
          "path": "plugins/kjgarza-product/commands/prioritize-backlog.md",
          "type": "blob",
          "size": 8535
        },
        {
          "path": "plugins/kjgarza-product/commands/search-user-research.md",
          "type": "blob",
          "size": 5475
        },
        {
          "path": "plugins/kjgarza-product/output-styles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/output-styles/product-manager.md",
          "type": "blob",
          "size": 8762
        },
        {
          "path": "plugins/kjgarza-product/output-styles/technical-lead.md",
          "type": "blob",
          "size": 7913
        },
        {
          "path": "plugins/kjgarza-product/settings.local.json",
          "type": "blob",
          "size": 3063
        },
        {
          "path": "plugins/kjgarza-product/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/SKILL.md",
          "type": "blob",
          "size": 10150
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml.md",
          "type": "blob",
          "size": 23572
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/__init__.py",
          "type": "blob",
          "size": 65
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/document.py",
          "type": "blob",
          "size": 50409
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates/comments.xml",
          "type": "blob",
          "size": 2635
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates/commentsExtended.xml",
          "type": "blob",
          "size": 2643
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates/commentsExtensible.xml",
          "type": "blob",
          "size": 2739
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates/commentsIds.xml",
          "type": "blob",
          "size": 2651
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/templates/people.xml",
          "type": "blob",
          "size": 147
        },
        {
          "path": "plugins/kjgarza-product/skills/docx/scripts/utilities.py",
          "type": "blob",
          "size": 13694
        },
        {
          "path": "plugins/kjgarza-product/skills/gdrive.skill",
          "type": "blob",
          "size": 4868
        },
        {
          "path": "plugins/kjgarza-product/skills/new-yorker-style.skill",
          "type": "blob",
          "size": 7258
        },
        {
          "path": "plugins/kjgarza-product/skills/new-yorker-style",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/new-yorker-style/SKILL.md",
          "type": "blob",
          "size": 7138
        },
        {
          "path": "plugins/kjgarza-product/skills/new-yorker-style/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/new-yorker-style/references/style-details.md",
          "type": "blob",
          "size": 7449
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/SKILL.md",
          "type": "blob",
          "size": 7068
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/check_bounding_boxes.py",
          "type": "blob",
          "size": 3139
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/check_bounding_boxes_test.py",
          "type": "blob",
          "size": 8818
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/check_fillable_fields.py",
          "type": "blob",
          "size": 362
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/convert_pdf_to_images.py",
          "type": "blob",
          "size": 1123
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/create_validation_image.py",
          "type": "blob",
          "size": 1603
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/extract_form_field_info.py",
          "type": "blob",
          "size": 6127
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/fill_fillable_fields.py",
          "type": "blob",
          "size": 4863
        },
        {
          "path": "plugins/kjgarza-product/skills/pdf/scripts/fill_pdf_form_with_annotations.py",
          "type": "blob",
          "size": 3596
        },
        {
          "path": "plugins/kjgarza-product/skills/planning-with-files",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/planning-with-files/SKILL.md",
          "type": "blob",
          "size": 3876
        },
        {
          "path": "plugins/kjgarza-product/skills/planning-with-files/examples.md",
          "type": "blob",
          "size": 4426
        },
        {
          "path": "plugins/kjgarza-product/skills/planning-with-files/reference.md",
          "type": "blob",
          "size": 3478
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/SKILL.md",
          "type": "blob",
          "size": 25551
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts/html2pptx.js",
          "type": "blob",
          "size": 37795
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts/inventory.py",
          "type": "blob",
          "size": 38126
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts/rearrange.py",
          "type": "blob",
          "size": 8514
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts/replace.py",
          "type": "blob",
          "size": 13594
        },
        {
          "path": "plugins/kjgarza-product/skills/pptx/scripts/thumbnail.py",
          "type": "blob",
          "size": 15484
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/SKILL.md",
          "type": "blob",
          "size": 4047
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/critique-guidelines.md",
          "type": "blob",
          "size": 13874
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/design-principles.md",
          "type": "blob",
          "size": 7338
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/design-processes.md",
          "type": "blob",
          "size": 9462
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/lifecycle-guidance.md",
          "type": "blob",
          "size": 11076
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/prioritization-frameworks.md",
          "type": "blob",
          "size": 11506
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/requirements.md",
          "type": "blob",
          "size": 14628
        },
        {
          "path": "plugins/kjgarza-product/skills/product-frameworks/references/usability-testing.md",
          "type": "blob",
          "size": 18331
        },
        {
          "path": "plugins/kjgarza-product/skills/project-bootstrapping",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/project-bootstrapping/SKILL.md",
          "type": "blob",
          "size": 4233
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-academic-outputs-with-dimensions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-academic-outputs-with-dimensions/SKILL.md",
          "type": "blob",
          "size": 5929
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-academic-outputs-with-dimensions/dsl-reference.md",
          "type": "blob",
          "size": 5978
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-coda",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-coda/SKILL.md",
          "type": "blob",
          "size": 5576
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-coda/coda-reference.md",
          "type": "blob",
          "size": 7537
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/SKILL.md",
          "type": "blob",
          "size": 3774
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/digital-science-reference.md",
          "type": "blob",
          "size": 2371
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/scripts/check_rclone.sh",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/scripts/download_gdrive.sh",
          "type": "blob",
          "size": 8122
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/scripts/search_gdrive.sh",
          "type": "blob",
          "size": 3863
        },
        {
          "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/scripts/sync_standups.sh",
          "type": "blob",
          "size": 9680
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 17837
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/__pycache__",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/__pycache__/quick_validate.cpython-312.pyc",
          "type": "blob",
          "size": 4020
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10863
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/monorepo-generator.skill",
          "type": "blob",
          "size": 24601
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/kjgarza-product/skills/skill-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 3523
        },
        {
          "path": "plugins/kjgarza-product/skills/xlsx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/kjgarza-product/skills/xlsx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/kjgarza-product/skills/xlsx/SKILL.md",
          "type": "blob",
          "size": 10632
        },
        {
          "path": "plugins/kjgarza-product/skills/xlsx/recalc.py",
          "type": "blob",
          "size": 6408
        },
        {
          "path": "plugins/scholarly-comms-researcher",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 748
        },
        {
          "path": "plugins/scholarly-comms-researcher/README.md",
          "type": "blob",
          "size": 6802
        },
        {
          "path": "plugins/scholarly-comms-researcher/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/agents/scholarly-comms-researcher.md",
          "type": "blob",
          "size": 7987
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/SKILL.md",
          "type": "blob",
          "size": 19800
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/assets/review_template.md",
          "type": "blob",
          "size": 12982
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/references/citation_styles.md",
          "type": "blob",
          "size": 6112
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/references/database_strategies.md",
          "type": "blob",
          "size": 12651
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/scripts/generate_pdf.py",
          "type": "blob",
          "size": 5410
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/scripts/search_databases.py",
          "type": "blob",
          "size": 9036
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/literature-review/scripts/verify_citations.py",
          "type": "blob",
          "size": 7050
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/SKILL.md",
          "type": "blob",
          "size": 10153
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/references/evaluation_framework.md",
          "type": "blob",
          "size": 19950
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/scripts/calculate_scores.py",
          "type": "blob",
          "size": 11551
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-brainstorming/SKILL.md",
          "type": "blob",
          "size": 7973
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-brainstorming/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-brainstorming/references/brainstorming_methods.md",
          "type": "blob",
          "size": 12065
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/SKILL.md",
          "type": "blob",
          "size": 22004
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/common_biases.md",
          "type": "blob",
          "size": 11093
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/evidence_hierarchy.md",
          "type": "blob",
          "size": 13444
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/experimental_design.md",
          "type": "blob",
          "size": 16068
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/logical_fallacies.md",
          "type": "blob",
          "size": 18014
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/scientific_method.md",
          "type": "blob",
          "size": 6100
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/references/statistical_pitfalls.md",
          "type": "blob",
          "size": 15726
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/SKILL.md",
          "type": "blob",
          "size": 25298
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/assets/color_palettes.py",
          "type": "blob",
          "size": 5754
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/assets/nature.mplstyle",
          "type": "blob",
          "size": 1449
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/assets/presentation.mplstyle",
          "type": "blob",
          "size": 1309
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/assets/publication.mplstyle",
          "type": "blob",
          "size": 1474
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/references/color_palettes.md",
          "type": "blob",
          "size": 9861
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/references/journal_requirements.md",
          "type": "blob",
          "size": 9533
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/references/matplotlib_examples.md",
          "type": "blob",
          "size": 18103
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/references/publication_guidelines.md",
          "type": "blob",
          "size": 8556
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/scripts/figure_export.py",
          "type": "blob",
          "size": 10904
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/scripts/style_presets.py",
          "type": "blob",
          "size": 12199
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/SKILL.md",
          "type": "blob",
          "size": 16576
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references/citation_styles.md",
          "type": "blob",
          "size": 23705
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references/figures_tables.md",
          "type": "blob",
          "size": 27883
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references/imrad_structure.md",
          "type": "blob",
          "size": 23921
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references/reporting_guidelines.md",
          "type": "blob",
          "size": 25012
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/references/writing_principles.md",
          "type": "blob",
          "size": 27593
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/SKILL.md",
          "type": "blob",
          "size": 15011
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/competing-risks.md",
          "type": "blob",
          "size": 11986
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/cox-models.md",
          "type": "blob",
          "size": 5238
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/data-handling.md",
          "type": "blob",
          "size": 12094
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/ensemble-models.md",
          "type": "blob",
          "size": 9529
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/evaluation-metrics.md",
          "type": "blob",
          "size": 10850
        },
        {
          "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/references/svm-models.md",
          "type": "blob",
          "size": 11096
        },
        {
          "path": "plugins/senior-software-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 765
        },
        {
          "path": "plugins/senior-software-developer/README.md",
          "type": "blob",
          "size": 13281
        },
        {
          "path": "plugins/senior-software-developer/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/agents/senior-developer.md",
          "type": "blob",
          "size": 9572
        },
        {
          "path": "plugins/senior-software-developer/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/commands/architecture-review.md",
          "type": "blob",
          "size": 6128
        },
        {
          "path": "plugins/senior-software-developer/commands/code-review.md",
          "type": "blob",
          "size": 6936
        },
        {
          "path": "plugins/senior-software-developer/commands/refactor-strategy.md",
          "type": "blob",
          "size": 9601
        },
        {
          "path": "plugins/senior-software-developer/commands/system-design.md",
          "type": "blob",
          "size": 9251
        },
        {
          "path": "plugins/senior-software-developer/commands/technical-debt-audit.md",
          "type": "blob",
          "size": 10314
        },
        {
          "path": "plugins/senior-software-developer/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/hooks/hooks.json",
          "type": "blob",
          "size": 2076
        },
        {
          "path": "plugins/senior-software-developer/output-styles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/output-styles/technical-lead.md",
          "type": "blob",
          "size": 7913
        },
        {
          "path": "plugins/senior-software-developer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/LICENSE.txt",
          "type": "blob",
          "size": 1056
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/SKILL.md",
          "type": "blob",
          "size": 13036
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/background.ts",
          "type": "blob",
          "size": 8763
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/content-script",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/content-script/main.ts",
          "type": "blob",
          "size": 7341
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/package.json",
          "type": "blob",
          "size": 931
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/postcss.config.js",
          "type": "blob",
          "size": 81
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/sidepanel",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/sidepanel/App.tsx",
          "type": "blob",
          "size": 5098
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/sidepanel/index.html",
          "type": "blob",
          "size": 329
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/sidepanel/main.tsx",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/sidepanel/style.css",
          "type": "blob",
          "size": 2043
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/tailwind.config.js",
          "type": "blob",
          "size": 1521
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/tsconfig.json",
          "type": "blob",
          "size": 436
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/assets/templates/wxt.config.ts",
          "type": "blob",
          "size": 2489
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/references/messaging.md",
          "type": "blob",
          "size": 14940
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/references/mv3-permissions.md",
          "type": "blob",
          "size": 8248
        },
        {
          "path": "plugins/senior-software-developer/skills/chrome-extension-builder/references/wxt-config.md",
          "type": "blob",
          "size": 8463
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/SKILL.md",
          "type": "blob",
          "size": 13249
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/templates/conversational.py.template",
          "type": "blob",
          "size": 8185
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/templates/main.py.template",
          "type": "blob",
          "size": 12728
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/templates/pyproject.toml.template",
          "type": "blob",
          "size": 2092
        },
        {
          "path": "plugins/senior-software-developer/skills/cli-generator/templates/responses.py.template",
          "type": "blob",
          "size": 3648
        },
        {
          "path": "plugins/senior-software-developer/skills/detect-code-smells",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/detect-code-smells/SKILL.md",
          "type": "blob",
          "size": 8355
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/SKILL.md",
          "type": "blob",
          "size": 9092
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/reference/evaluation.md",
          "type": "blob",
          "size": 21663
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/reference/mcp_best_practices.md",
          "type": "blob",
          "size": 7330
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/reference/node_mcp_server.md",
          "type": "blob",
          "size": 28550
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/reference/python_mcp_server.md",
          "type": "blob",
          "size": 25099
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/scripts/connections.py",
          "type": "blob",
          "size": 4875
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/scripts/evaluation.py",
          "type": "blob",
          "size": 12579
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/scripts/example_evaluation.xml",
          "type": "blob",
          "size": 1194
        },
        {
          "path": "plugins/senior-software-developer/skills/mcp-builder/scripts/requirements.txt",
          "type": "blob",
          "size": 29
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/SKILL.md",
          "type": "blob",
          "size": 10591
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/api/package.json",
          "type": "blob",
          "size": 550
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/api/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/api/src/index.ts",
          "type": "blob",
          "size": 825
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/api/tsconfig.json",
          "type": "blob",
          "size": 217
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/app",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/app/globals.css",
          "type": "blob",
          "size": 1196
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/app/layout.tsx",
          "type": "blob",
          "size": 444
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/app/page.tsx",
          "type": "blob",
          "size": 1039
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/next.config.ts",
          "type": "blob",
          "size": 197
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/package.json",
          "type": "blob",
          "size": 720
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/tailwind.config.ts",
          "type": "blob",
          "size": 1359
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/apps/web/tsconfig.json",
          "type": "blob",
          "size": 258
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/package.json",
          "type": "blob",
          "size": 464
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/src/artifact.tsx",
          "type": "blob",
          "size": 786
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/src/index.ts",
          "type": "blob",
          "size": 55
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/src/message.tsx",
          "type": "blob",
          "size": 772
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ai-elements/tsconfig.json",
          "type": "blob",
          "size": 181
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/config/package.json",
          "type": "blob",
          "size": 271
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/config/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/config/src/index.ts",
          "type": "blob",
          "size": 681
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/config/tsconfig.json",
          "type": "blob",
          "size": 172
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/drizzle.config.ts",
          "type": "blob",
          "size": 213
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/package.json",
          "type": "blob",
          "size": 542
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/src/index.ts",
          "type": "blob",
          "size": 457
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/src/schema.ts",
          "type": "blob",
          "size": 430
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/db/tsconfig.json",
          "type": "blob",
          "size": 172
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/eslint-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/eslint-config/index.js",
          "type": "blob",
          "size": 540
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/eslint-config/package.json",
          "type": "blob",
          "size": 430
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config/base.json",
          "type": "blob",
          "size": 439
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config/nextjs.json",
          "type": "blob",
          "size": 263
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config/node.json",
          "type": "blob",
          "size": 155
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config/package.json",
          "type": "blob",
          "size": 108
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/typescript-config/react-library.json",
          "type": "blob",
          "size": 182
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/package.json",
          "type": "blob",
          "size": 522
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/src/components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/src/components/button.tsx",
          "type": "blob",
          "size": 1810
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/src/index.ts",
          "type": "blob",
          "size": 37
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/ui/tsconfig.json",
          "type": "blob",
          "size": 181
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils/package.json",
          "type": "blob",
          "size": 302
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils/src/cn.ts",
          "type": "blob",
          "size": 169
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils/src/index.ts",
          "type": "blob",
          "size": 27
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/packages/utils/tsconfig.json",
          "type": "blob",
          "size": 181
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/root",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/root/.gitignore",
          "type": "blob",
          "size": 496
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/root/turbo.json",
          "type": "blob",
          "size": 486
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/jobs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/jobs/package.json",
          "type": "blob",
          "size": 516
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/jobs/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/jobs/src/index.ts",
          "type": "blob",
          "size": 1062
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/jobs/tsconfig.json",
          "type": "blob",
          "size": 217
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines/package.json",
          "type": "blob",
          "size": 413
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines/src/etl",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines/src/etl/example.ts",
          "type": "blob",
          "size": 1992
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/assets/services/pipelines/tsconfig.json",
          "type": "blob",
          "size": 239
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/monorepo-generator/references/package-managers.md",
          "type": "blob",
          "size": 3360
        },
        {
          "path": "plugins/senior-software-developer/skills/security-pattern-check",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/security-pattern-check/SKILL.md",
          "type": "blob",
          "size": 15034
        },
        {
          "path": "plugins/senior-software-developer/skills/suggest-performance-fix",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/suggest-performance-fix/SKILL.md",
          "type": "blob",
          "size": 10474
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/SKILL.md",
          "type": "blob",
          "size": 10289
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/README.md",
          "type": "blob",
          "size": 3460
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/command-extension",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/command-extension/README.md",
          "type": "blob",
          "size": 1567
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/command-extension/extension.ts",
          "type": "blob",
          "size": 1340
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/command-extension/package.json",
          "type": "blob",
          "size": 924
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/webview-extension",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/webview-extension/README.md",
          "type": "blob",
          "size": 3470
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/webview-extension/extension.ts",
          "type": "blob",
          "size": 4711
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/assets/templates/webview-extension/package.json",
          "type": "blob",
          "size": 784
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/references/activation-events.md",
          "type": "blob",
          "size": 11486
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/references/best-practices.md",
          "type": "blob",
          "size": 14760
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/references/common-apis.md",
          "type": "blob",
          "size": 16856
        },
        {
          "path": "plugins/senior-software-developer/skills/vscode-extension-builder/references/extension-anatomy.md",
          "type": "blob",
          "size": 13449
        }
      ],
      "marketplace": {
        "name": "marketplace-claude",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Kristian Garza",
          "email": "kj.garza@gmail.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "kjgarza-base",
            "description": "Base MCP server configuration with puppeteer, fetch, sequential thinking, and context7 documentation servers",
            "source": "./plugins/kjgarza-base",
            "category": "utilities",
            "version": "1.0.0",
            "author": {
              "name": "Kristian Garza",
              "email": "kj.garza@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add kjgarza/marketplace-claude",
              "/plugin install kjgarza-base@marketplace-claude"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-11T12:23:28Z",
              "created_at": "2026-01-01T15:35:08Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "file-organizer",
                "description": "Intelligently organizes your files and folders across your computer by understanding context, finding duplicates, suggesting better structures, and automating cleanup tasks. Reduces cognitive load and keeps your digital workspace tidy without manual effort.",
                "path": "plugins/kjgarza-base/skills/file-organising/SKILL.md",
                "frontmatter": {
                  "name": "file-organizer",
                  "description": "Intelligently organizes your files and folders across your computer by understanding context, finding duplicates, suggesting better structures, and automating cleanup tasks. Reduces cognitive load and keeps your digital workspace tidy without manual effort."
                },
                "content": "# File Organizer\n\nThis skill acts as your personal organization assistant, helping you maintain a clean, logical file structure across your computer without the mental overhead of constant manual organization.\n\n## When to Use This Skill\n\n- Your Downloads folder is a chaotic mess\n- You can't find files because they're scattered everywhere\n- You have duplicate files taking up space\n- Your folder structure doesn't make sense anymore\n- You want to establish better organization habits\n- You're starting a new project and need a good structure\n- You're cleaning up before archiving old projects\n\n## What This Skill Does\n\n1. **Analyzes Current Structure**: Reviews your folders and files to understand what you have\n2. **Finds Duplicates**: Identifies duplicate files across your system\n3. **Suggests Organization**: Proposes logical folder structures based on your content\n4. **Automates Cleanup**: Moves, renames, and organizes files with your approval\n5. **Maintains Context**: Makes smart decisions based on file types, dates, and content\n6. **Reduces Clutter**: Identifies old files you probably don't need anymore\n\n## How to Use\n\n### From Your Home Directory\n\n```\ncd ~\n```\n\nThen run Claude Code and ask for help:\n\n```\nHelp me organize my Downloads folder\n```\n\n```\nFind duplicate files in my Documents folder\n```\n\n```\nReview my project directories and suggest improvements\n```\n\n### Specific Organization Tasks\n\n```\nOrganize these downloads into proper folders based on what they are\n```\n\n```\nFind duplicate files and help me decide which to keep\n```\n\n```\nClean up old files I haven't touched in 6+ months\n```\n\n```\nCreate a better folder structure for my [work/projects/photos/etc]\n```\n\n## Instructions\n\nWhen a user requests file organization help:\n\n1. **Understand the Scope**\n   \n   Ask clarifying questions:\n   - Which directory needs organization? (Downloads, Documents, entire home folder?)\n   - What's the main problem? (Can't find things, duplicates, too messy, no structure?)\n   - Any files or folders to avoid? (Current projects, sensitive data?)\n   - How aggressively to organize? (Conservative vs. comprehensive cleanup)\n\n2. **Analyze Current State**\n   \n   Review the target directory:\n   ```bash\n   # Get overview of current structure\n   ls -la [target_directory]\n   \n   # Check file types and sizes\n   find [target_directory] -type f -exec file {} \\; | head -20\n   \n   # Identify largest files\n   du -sh [target_directory]/* | sort -rh | head -20\n   \n   # Count file types\n   find [target_directory] -type f | sed 's/.*\\.//' | sort | uniq -c | sort -rn\n   ```\n   \n   Summarize findings:\n   - Total files and folders\n   - File type breakdown\n   - Size distribution\n   - Date ranges\n   - Obvious organization issues\n\n3. **Identify Organization Patterns**\n   \n   Based on the files, determine logical groupings:\n   \n   **By Type**:\n   - Documents (PDFs, DOCX, TXT)\n   - Images (JPG, PNG, SVG)\n   - Videos (MP4, MOV)\n   - Archives (ZIP, TAR, DMG)\n   - Code/Projects (directories with code)\n   - Spreadsheets (XLSX, CSV)\n   - Presentations (PPTX, KEY)\n   \n   **By Purpose**:\n   - Work vs. Personal\n   - Active vs. Archive\n   - Project-specific\n   - Reference materials\n   - Temporary/scratch files\n   \n   **By Date**:\n   - Current year/month\n   - Previous years\n   - Very old (archive candidates)\n\n4. **Find Duplicates**\n   \n   When requested, search for duplicates:\n   ```bash\n   # Find exact duplicates by hash\n   find [directory] -type f -exec md5 {} \\; | sort | uniq -d\n   \n   # Find files with same name\n   find [directory] -type f -printf '%f\\n' | sort | uniq -d\n   \n   # Find similar-sized files\n   find [directory] -type f -printf '%s %p\\n' | sort -n\n   ```\n   \n   For each set of duplicates:\n   - Show all file paths\n   - Display sizes and modification dates\n   - Recommend which to keep (usually newest or best-named)\n   - **Important**: Always ask for confirmation before deleting\n\n5. **Propose Organization Plan**\n   \n   Present a clear plan before making changes:\n   \n   ```markdown\n   # Organization Plan for [Directory]\n   \n   ## Current State\n   - X files across Y folders\n   - [Size] total\n   - File types: [breakdown]\n   - Issues: [list problems]\n   \n   ## Proposed Structure\n   \n   ```\n   [Directory]/\n    Work/\n       Projects/\n       Documents/\n       Archive/\n    Personal/\n       Photos/\n       Documents/\n       Media/\n    Downloads/\n        To-Sort/\n        Archive/\n   ```\n   \n   ## Changes I'll Make\n   \n   1. **Create new folders**: [list]\n   2. **Move files**:\n      - X PDFs  Work/Documents/\n      - Y images  Personal/Photos/\n      - Z old files  Archive/\n   3. **Rename files**: [any renaming patterns]\n   4. **Delete**: [duplicates or trash files]\n   \n   ## Files Needing Your Decision\n   \n   - [List any files you're unsure about]\n   \n   Ready to proceed? (yes/no/modify)\n   ```\n\n6. **Execute Organization**\n   \n   After approval, organize systematically:\n   \n   ```bash\n   # Create folder structure\n   mkdir -p \"path/to/new/folders\"\n   \n   # Move files with clear logging\n   mv \"old/path/file.pdf\" \"new/path/file.pdf\"\n   \n   # Rename files with consistent patterns\n   # Example: \"YYYY-MM-DD - Description.ext\"\n   ```\n   \n   **Important Rules**:\n   - Always confirm before deleting anything\n   - Log all moves for potential undo\n   - Preserve original modification dates\n   - Handle filename conflicts gracefully\n   - Stop and ask if you encounter unexpected situations\n\n7. **Provide Summary and Maintenance Tips**\n   \n   After organizing:\n   \n   ```markdown\n   # Organization Complete! \n   \n   ## What Changed\n   \n   - Created [X] new folders\n   - Organized [Y] files\n   - Freed [Z] GB by removing duplicates\n   - Archived [W] old files\n   \n   ## New Structure\n   \n   [Show the new folder tree]\n   \n   ## Maintenance Tips\n   \n   To keep this organized:\n   \n   1. **Weekly**: Sort new downloads\n   2. **Monthly**: Review and archive completed projects\n   3. **Quarterly**: Check for new duplicates\n   4. **Yearly**: Archive old files\n   \n   ## Quick Commands for You\n   \n   ```bash\n   # Find files modified this week\n   find . -type f -mtime -7\n   \n   # Sort downloads by type\n   [custom command for their setup]\n   \n   # Find duplicates\n   [custom command]\n   ```\n   \n   Want to organize another folder?\n   ```\n\n## Examples\n\n### Example 1: Organizing Downloads (From Justin Dielmann)\n\n**User**: \"My Downloads folder is a mess with 500+ files. Help me organize it.\"\n\n**Process**:\n1. Analyzes Downloads folder\n2. Finds patterns: work docs, personal photos, installers, random PDFs\n3. Proposes structure:\n   - Downloads/\n     - Work/\n     - Personal/\n     - Installers/ (DMG, PKG files)\n     - Archive/\n     - ToSort/ (things needing decisions)\n4. Asks for confirmation\n5. Moves files intelligently based on content and names\n6. Results: 500 files  5 organized folders\n\n### Example 2: Finding and Removing Duplicates\n\n**User**: \"Find duplicate files in my Documents and help me decide which to keep.\"\n\n**Output**:\n```markdown\n# Found 23 Sets of Duplicates (156 MB total)\n\n## Duplicate Set 1: \"proposal.pdf\"\n- `/Documents/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Documents/old/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Desktop/proposal.pdf` (2.3 MB, modified: 2024-03-10)\n\n**Recommendation**: Keep `/Documents/proposal.pdf` (most recent in correct location)\nDelete the other 2 copies?\n\n[Continue for all duplicates...]\n```\n\n### Example 3: Restructuring Projects Folder\n\n**User**: \"Review my ~/Projects directory and suggest improvements.\"\n\n**Output**:\n```markdown\n# Analysis of ~/Projects\n\n## Current Structure Issues\n- Mix of active and archived projects (3+ years old)\n- No consistent naming convention\n- Some projects at root, others in random subfolders\n- Duplicate folders (project-name, project-name-old, project-name-v2)\n\n## Proposed Structure\n\n```\nProjects/\n Active/\n    client-work/\n    side-projects/\n    learning/\n Archive/\n    2022/\n    2023/\n    2024/\n Templates/\n```\n\n## Specific Changes\n1. Move 12 projects not touched since 2022  Archive/\n2. Consolidate 4 duplicate project folders\n3. Rename projects to consistent format: \"client-name-project-name\"\n4. Create Archive for old work\n\nWant me to implement this?\n```\n\n### Example 4: Organizing Photos by Date\n\n**User**: \"Organize my photo folders by year and month.\"\n\n**Output**: Creates structure like:\n```\nPhotos/\n 2023/\n    01-January/\n    02-February/\n    ...\n 2024/\n    01-January/\n    ...\n Unsorted/\n```\n\nThen moves photos based on EXIF data or file modification dates.\n\n## Common Organization Tasks\n\n### Downloads Cleanup\n```\nOrganize my Downloads folder - move documents to Documents, \nimages to Pictures, keep installers separate, and archive files \nolder than 3 months.\n```\n\n### Project Organization\n```\nReview my Projects folder structure and help me separate active \nprojects from old ones I should archive.\n```\n\n### Duplicate Removal\n```\nFind all duplicate files in my Documents folder and help me \ndecide which ones to keep.\n```\n\n### Desktop Cleanup\n```\nMy Desktop is covered in files. Help me organize everything into \nmy Documents folder properly.\n```\n\n### Photo Organization\n```\nOrganize all photos in this folder by date (year/month) based \non when they were taken.\n```\n\n### Work/Personal Separation\n```\nHelp me separate my work files from personal files across my \nDocuments folder.\n```\n\n## Pro Tips\n\n1. **Start Small**: Begin with one messy folder (like Downloads) to build trust\n2. **Regular Maintenance**: Run weekly cleanup on Downloads\n3. **Consistent Naming**: Use \"YYYY-MM-DD - Description\" format for important files\n4. **Archive Aggressively**: Move old projects to Archive instead of deleting\n5. **Keep Active Separate**: Maintain clear boundaries between active and archived work\n6. **Trust the Process**: Let Claude handle the cognitive load of where things go\n\n## Best Practices\n\n### Folder Naming\n- Use clear, descriptive names\n- Avoid spaces (use hyphens or underscores)\n- Be specific: \"client-proposals\" not \"docs\"\n- Use prefixes for ordering: \"01-current\", \"02-archive\"\n\n### File Naming\n- Include dates: \"2024-10-17-meeting-notes.md\"\n- Be descriptive: \"q3-financial-report.xlsx\"\n- Avoid version numbers in names (use version control instead)\n- Remove download artifacts: \"document-final-v2 (1).pdf\"  \"document.pdf\"\n\n### When to Archive\n- Projects not touched in 6+ months\n- Completed work that might be referenced later\n- Old versions after migration to new systems\n- Files you're hesitant to delete (archive first)\n\n## Related Use Cases\n\n- Setting up organization for a new computer\n- Preparing files for backup/archiving\n- Cleaning up before storage cleanup\n- Organizing shared team folders\n- Structuring new project directories"
              },
              {
                "name": "media-processing",
                "description": "Process multimedia files with  ImageMagick (image manipulation, format conversion, batch processing, effects, composition). Use when converting media formats, encoding images",
                "path": "plugins/kjgarza-base/skills/image-processing/SKILL.md",
                "frontmatter": {
                  "name": "media-processing",
                  "description": "Process multimedia files with  ImageMagick (image manipulation, format conversion, batch processing, effects, composition). Use when converting media formats, encoding images",
                  "license": "MIT"
                },
                "content": "# Media Processing Skill\n\nProcess images using ImageMagick command-line tools for conversion, optimization, streaming, and manipulation tasks.\n\n## When to Use This Skill\n\nUse when:\n- Converting media formats (video, audio, images)\n- Encoding video with codecs (H.264, H.265, VP9, AV1)\n- Processing images (resize, crop, effects, watermarks)\n- Extracting audio from video\n- Creating streaming manifests (HLS/DASH)\n- Generating thumbnails and previews\n- Batch processing media files\n- Optimizing file sizes and quality\n- Applying filters and effects\n- Creating composite images or videos\n\n## Tool Selection Guide\n\n\n### ImageMagick: Image Processing\nUse ImageMagick for:\n- Image format conversion (PNG, JPEG, WebP, GIF)\n- Resizing, cropping, transformations\n- Batch image processing (mogrify)\n- Visual effects (blur, sharpen, sepia)\n- Text overlays and watermarks\n- Image composition and montages\n- Color adjustments, filters\n- Thumbnail generation\n\n### Decision Matrix\n\n| Task | Tool | Why |\n|------|------|-----|\n| Image resize | ImageMagick | Optimized for still images |\n| Batch images | ImageMagick | mogrify for in-place edits |\n| Image effects | ImageMagick | Rich filter library |\n\n## Installation\n\n### macOS\n```bash\nbrew install ffmpeg imagemagick\n```\n\n### Ubuntu/Debian\n```bash\nsudo apt-get install ffmpeg imagemagick\n```\n\n### Windows\n```bash\n# Using winget\nwinget install ffmpeg\nwinget install ImageMagick.ImageMagick\n\n# Or download binaries\n# FFmpeg: https://ffmpeg.org/download.html\n# ImageMagick: https://imagemagick.org/script/download.php\n```\n\n### Verify Installation\n```bash\nmagick -version\n# or\nconvert -version\n```\n\n## Quick Start Examples\n\n### Image Processing\n```bash\n# Convert format\nmagick input.png output.jpg\n\n# Resize maintaining aspect ratio\nmagick input.jpg -resize 800x600 output.jpg\n\n# Create square thumbnail\nmagick input.jpg -resize 200x200^ -gravity center -extent 200x200 thumb.jpg\n```\n\n### Batch Image Resize\n```bash\n# Resize all JPEGs to 800px width\nmogrify -resize 800x -quality 85 *.jpg\n\n# Output to separate directory\nmogrify -path ./output -resize 800x600 *.jpg\n```\n\n### Image Watermark\n```bash\n# Add watermark to corner\nmagick input.jpg watermark.png -gravity southeast \\\n  -geometry +10+10 -composite output.jpg\n```\n\n## Common Workflows\n\n### Create Responsive Images\n```bash\n# Generate multiple sizes\nfor size in 320 640 1024 1920; do\n  magick input.jpg -resize ${size}x -quality 85 \"output-${size}w.jpg\"\ndone\n```\n\n### Batch Image Optimization\n```bash\n# Convert PNG to optimized JPEG\nmogrify -path ./optimized -format jpg -quality 85 -strip *.png\n```\n\n### Image Blur Effect\n```bash\n# Gaussian blur\nmagick input.jpg -gaussian-blur 0x8 output.jpg\n```\n\n## Advanced Techniques\n\n### Complex Image Pipeline\n```bash\n# Resize, crop, border, adjust\nmagick input.jpg \\\n  -resize 1000x1000^ \\\n  -gravity center \\\n  -crop 1000x1000+0+0 +repage \\\n  -bordercolor black -border 5x5 \\\n  -brightness-contrast 5x10 \\\n  -quality 90 \\\n  output.jpg\n```\n\n### Animated GIF from Images\n```bash\n# Create with delay\nmagick -delay 100 -loop 0 frame*.png animated.gif\n\n# Optimize size\nmagick animated.gif -fuzz 5% -layers Optimize optimized.gif\n```\n\n## Media Analysis\n\n### Image Information\n```bash\n# Basic info\nidentify image.jpg\n\n# Detailed format\nidentify -verbose image.jpg\n\n# Custom format\nidentify -format \"%f: %wx%h %b\\n\" image.jpg\n```\n\n## Performance Tips\n\n1. **Copy streams when possible** - Avoid re-encoding with `-c copy`\n2. **Appropriate presets** - Balance speed vs compression\n3. **Batch with mogrify** - In-place image processing\n4. **Strip metadata** - Reduce file size with `-strip`\n5. **Progressive JPEG** - Better web loading with `-interlace Plane`\n6. **Limit memory** - Prevent crashes on large batches\n7. **Test on samples** - Verify settings before batch\n8.  **Parallel processing** - Use GNU Parallel for multiple files\n\n## Reference Documentation\n\nDetailed guides in `references/`:\n\n- **imagemagick-editing.md** - Format conversion, effects, transformations\n- **imagemagick-batch.md** - Batch processing, mogrify, parallel operations\n\n## Common Parameters\n\n### ImageMagick Geometry\n- `800x600` - Fit within (maintains aspect)\n- `800x600!` - Force exact size\n- `800x600^` - Fill (may crop)\n- `800x` - Width only\n- `x600` - Height only\n- `50%` - Scale percentage\n\n## Troubleshooting\n\n**ImageMagick \"not authorized\"**\n```bash\n# Edit policy file\nsudo nano /etc/ImageMagick-7/policy.xml\n# Change <policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" />\n# to <policy domain=\"coder\" rights=\"read|write\" pattern=\"PDF\" />\n```\n\n**Memory errors**\n```bash\n# Limit memory usage\nmagick -limit memory 2GB -limit map 4GB input.jpg output.jpg\n```\n\n## Resources\n\n- ImageMagick: https://imagemagick.org/\n- ImageMagick Usage: https://imagemagick.org/Usage/"
              },
              {
                "name": "project-scaffold",
                "description": "Scaffold new software projects with best practices. Use this skill when users want to create a new project, initialize a codebase, scaffold an application, start a new repo, or set up a project from scratch. Supports frontend (React, Vue, vanilla), CLI tools, REST APIs, and monorepos in TypeScript/JavaScript or Python.\n",
                "path": "plugins/kjgarza-base/skills/project-scaffold/SKILL.md",
                "frontmatter": {
                  "name": "project-scaffold",
                  "description": "Scaffold new software projects with best practices. Use this skill when users want to create a new project, initialize a codebase, scaffold an application, start a new repo, or set up a project from scratch. Supports frontend (React, Vue, vanilla), CLI tools, REST APIs, and monorepos in TypeScript/JavaScript or Python.\n"
                },
                "content": "# Project Scaffolder\n\nScaffold production-ready projects with modern tooling and configurable features.\n\n## Trigger Conditions\n\nUse this skill when the user:\n- Wants to \"create a new project\" or \"start a new app\"\n- Asks to \"scaffold\" or \"initialize\" a project\n- Says \"set up a new [frontend/CLI/API/monorepo]\"\n- Requests a \"boilerplate\" or \"starter template\"\n\n## Interactive Workflow\n\nFollow these steps in order. Use the AskUserQuestion tool for each step.\n\n### Step 1: Project Name\n\nAsk for the project name. Validate that it:\n- Uses lowercase letters, numbers, and hyphens only (kebab-case)\n- Starts with a letter\n- Is between 2-50 characters\n- Does not start with a number\n\nExample prompt:\n```\nWhat would you like to name your project?\n(Use lowercase with hyphens, e.g., my-awesome-app)\n```\n\n### Step 2: Project Type\n\nAsk which type of project to create:\n\n| Type | Description |\n|------|-------------|\n| **Frontend** | Static site or single-page application |\n| **CLI** | Command-line tool |\n| **API** | REST API server |\n| **Monorepo** | Multi-package workspace |\n\n### Step 3: Language/Framework\n\nBased on project type, present these options:\n\n**Frontend:**\n1. React with TypeScript (Recommended)\n2. React with JavaScript\n3. Vue with TypeScript\n4. Vue with JavaScript\n5. Vanilla (no framework)\n\n**CLI:**\n1. TypeScript/Node.js with Commander (Recommended)\n2. JavaScript/Node.js with Commander\n3. Python with Click\n\n**API:**\n1. TypeScript/Express (Recommended)\n2. JavaScript/Express\n3. Python/FastAPI\n\n**Monorepo:**\n1. Node.js with pnpm + Turborepo (Recommended)\n2. Python with uv workspaces\n\n### Step 4: Features\n\nAsk which features to include (allow multiple selections):\n\n| Feature | Node.js | Python |\n|---------|---------|--------|\n| Testing | Vitest / Jest | Pytest |\n| Linting | ESLint + Prettier | Ruff |\n| CI/CD | GitHub Actions | GitHub Actions |\n| Docker | Dockerfile + compose | Dockerfile + compose |\n| Documentation | README, CONTRIBUTING, LICENSE | README, CONTRIBUTING, LICENSE |\n\n### Step 5: Confirmation\n\nSummarize all selections and ask for confirmation:\n\n```\nI'll create a [LANGUAGE] [TYPE] project named '[NAME]' with:\n- [Feature 1]\n- [Feature 2]\n- ...\n\nProceed? (yes to confirm, or specify changes)\n```\n\n## Generation Process\n\nAfter confirmation, generate the project:\n\n1. **Create project directory** at the specified path (default: current directory)\n\n2. **Copy base templates** from the appropriate `templates/[type]/[language]/` directory\n\n3. **Process template variables** - Replace these placeholders in all `.tmpl` files:\n   - `{{PROJECT_NAME}}` - kebab-case name (e.g., `my-app`)\n   - `{{PROJECT_NAME_PASCAL}}` - PascalCase (e.g., `MyApp`)\n   - `{{PROJECT_NAME_SNAKE}}` - snake_case (e.g., `my_app`)\n   - `{{AUTHOR}}` - From `git config user.name` or \"Your Name\"\n   - `{{YEAR}}` - Current year\n\n4. **Add feature files** from `templates/features/[feature]/` for each selected feature\n\n5. **Create .gitignore** appropriate for the language\n\n6. **Initialize git** with `git init`\n\n7. **Display next steps**:\n   ```\n   Project created successfully!\n\n   Next steps:\n     cd [project-name]\n     [install command]  # npm install / pip install -e .\n     [dev command]      # npm run dev / python -m [name]\n   ```\n\n## Template Locations\n\nBase project templates:\n- `templates/frontend/react-ts/` - React + TypeScript + Vite\n- `templates/frontend/react-js/` - React + JavaScript + Vite\n- `templates/frontend/vue-ts/` - Vue + TypeScript + Vite\n- `templates/frontend/vue-js/` - Vue + JavaScript + Vite\n- `templates/frontend/vanilla/` - Vanilla JS + Vite\n- `templates/cli/node-ts/` - Node.js + TypeScript + Commander\n- `templates/cli/node-js/` - Node.js + JavaScript + Commander\n- `templates/cli/python/` - Python + Click\n- `templates/api/node-ts/` - Express + TypeScript\n- `templates/api/node-js/` - Express + JavaScript\n- `templates/api/python/` - FastAPI + Uvicorn\n- `templates/monorepo/node/` - pnpm + Turborepo\n- `templates/monorepo/python/` - uv workspaces\n\nFeature templates:\n- `templates/features/testing/` - Test configs (vitest, jest, pytest)\n- `templates/features/linting/` - Lint configs (eslint, prettier, ruff)\n- `templates/features/ci-cd/` - GitHub Actions workflows\n- `templates/features/docker/` - Dockerfile and docker-compose\n- `templates/features/docs/` - README, CONTRIBUTING, LICENSE\n\n## File Naming\n\nTemplate files use `.tmpl` extension. When copying:\n- Remove `.tmpl` extension (e.g., `package.json.tmpl`  `package.json`)\n- Process variable substitutions\n- Preserve directory structure\n\n## Best Practices Applied\n\nAll generated projects include:\n- **Modern tooling**: Vite for frontend, tsx for Node, uv for Python\n- **Type safety**: TypeScript by default, type hints for Python\n- **Sensible defaults**: Works out of the box with minimal config\n- **Clean structure**: Organized src/ directory, clear separation of concerns\n- **Git-ready**: Appropriate .gitignore, ready for version control"
              },
              {
                "name": "skill-creator",
                "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                "path": "plugins/kjgarza-base/skills/skill-creator/SKILL.md",
                "frontmatter": {
                  "name": "skill-creator",
                  "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again"
              }
            ]
          },
          {
            "name": "scholarly-comms-researcher",
            "description": "Expert agent for scholarly communication research with Dimensions database integration and academic search capabilities",
            "source": "./plugins/scholarly-comms-researcher",
            "category": "documentation",
            "version": "1.0.0",
            "author": {
              "name": "Kristian Garza",
              "email": "kj.garza@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add kjgarza/marketplace-claude",
              "/plugin install scholarly-comms-researcher@marketplace-claude"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-11T12:23:28Z",
              "created_at": "2026-01-01T15:35:08Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "literature-review",
                "description": "Conduct comprehensive, systematic literature reviews using multiple academic databases (PubMed, arXiv, bioRxiv, Semantic Scholar, etc.). This skill should be used when conducting systematic literature reviews, meta-analyses, research synthesis, or comprehensive literature searches across biomedical, scientific, and technical domains. Creates professionally formatted markdown documents and PDFs with verified citations in multiple citation styles (APA, Nature, Vancouver, etc.).",
                "path": "plugins/scholarly-comms-researcher/skills/literature-review/SKILL.md",
                "frontmatter": {
                  "name": "literature-review",
                  "description": "Conduct comprehensive, systematic literature reviews using multiple academic databases (PubMed, arXiv, bioRxiv, Semantic Scholar, etc.). This skill should be used when conducting systematic literature reviews, meta-analyses, research synthesis, or comprehensive literature searches across biomedical, scientific, and technical domains. Creates professionally formatted markdown documents and PDFs with verified citations in multiple citation styles (APA, Nature, Vancouver, etc.)."
                },
                "content": "# Literature Review\n\n## Overview\n\nConduct systematic, comprehensive literature reviews following rigorous academic methodology. Search multiple literature databases, synthesize findings thematically, verify all citations for accuracy, and generate professional output documents in markdown and PDF formats.\n\nThis skill integrates with multiple scientific skills for database access (gget, bioservices, datacommons-client) and provides specialized tools for citation verification, result aggregation, and document generation.\n\n## When to Use This Skill\n\nUse this skill when:\n- Conducting a systematic literature review for research or publication\n- Synthesizing current knowledge on a specific topic across multiple sources\n- Performing meta-analysis or scoping reviews\n- Writing the literature review section of a research paper or thesis\n- Investigating the state of the art in a research domain\n- Identifying research gaps and future directions\n- Requiring verified citations and professional formatting\n\n## Core Workflow\n\nLiterature reviews follow a structured, multi-phase workflow:\n\n### Phase 1: Planning and Scoping\n\n1. **Define Research Question**: Use PICO framework (Population, Intervention, Comparison, Outcome) for clinical/biomedical reviews\n   - Example: \"What is the efficacy of CRISPR-Cas9 (I) for treating sickle cell disease (P) compared to standard care (C)?\"\n\n2. **Establish Scope and Objectives**:\n   - Define clear, specific research questions\n   - Determine review type (narrative, systematic, scoping, meta-analysis)\n   - Set boundaries (time period, geographic scope, study types)\n\n3. **Develop Search Strategy**:\n   - Identify 2-4 main concepts from research question\n   - List synonyms, abbreviations, and related terms for each concept\n   - Plan Boolean operators (AND, OR, NOT) to combine terms\n   - Select minimum 3 complementary databases\n\n4. **Set Inclusion/Exclusion Criteria**:\n   - Date range (e.g., last 10 years: 2015-2024)\n   - Language (typically English, or specify multilingual)\n   - Publication types (peer-reviewed, preprints, reviews)\n   - Study designs (RCTs, observational, in vitro, etc.)\n   - Document all criteria clearly\n\n### Phase 2: Systematic Literature Search\n\n1. **Multi-Database Search**:\n\n   Select databases appropriate for the domain:\n\n   **Biomedical & Life Sciences:**\n   - Use `gget` skill: `gget search pubmed \"search terms\"` for PubMed/PMC\n   - Use `gget` skill: `gget search biorxiv \"search terms\"` for preprints\n   - Use `bioservices` skill for ChEMBL, KEGG, UniProt, etc.\n\n   **General Scientific Literature:**\n   - Search arXiv via direct API (preprints in physics, math, CS, q-bio)\n   - Search Semantic Scholar via API (200M+ papers, cross-disciplinary)\n   - Use Google Scholar for comprehensive coverage (manual or careful scraping)\n\n   **Specialized Databases:**\n   - Use `gget alphafold` for protein structures\n   - Use `gget cosmic` for cancer genomics\n   - Use `datacommons-client` for demographic/statistical data\n   - Use specialized databases as appropriate for the domain\n\n2. **Document Search Parameters**:\n   ```markdown\n   ## Search Strategy\n\n   ### Database: PubMed\n   - **Date searched**: 2024-10-25\n   - **Date range**: 2015-01-01 to 2024-10-25\n   - **Search string**:\n     ```\n     (\"CRISPR\"[Title] OR \"Cas9\"[Title])\n     AND (\"sickle cell\"[MeSH] OR \"SCD\"[Title/Abstract])\n     AND 2015:2024[Publication Date]\n     ```\n   - **Results**: 247 articles\n   ```\n\n   Repeat for each database searched.\n\n3. **Export and Aggregate Results**:\n   - Export results in JSON format from each database\n   - Combine all results into a single file\n   - Use `scripts/search_databases.py` for post-processing:\n     ```bash\n     python search_databases.py combined_results.json \\\n       --deduplicate \\\n       --format markdown \\\n       --output aggregated_results.md\n     ```\n\n### Phase 3: Screening and Selection\n\n1. **Deduplication**:\n   ```bash\n   python search_databases.py results.json --deduplicate --output unique_results.json\n   ```\n   - Removes duplicates by DOI (primary) or title (fallback)\n   - Document number of duplicates removed\n\n2. **Title Screening**:\n   - Review all titles against inclusion/exclusion criteria\n   - Exclude obviously irrelevant studies\n   - Document number excluded at this stage\n\n3. **Abstract Screening**:\n   - Read abstracts of remaining studies\n   - Apply inclusion/exclusion criteria rigorously\n   - Document reasons for exclusion\n\n4. **Full-Text Screening**:\n   - Obtain full texts of remaining studies\n   - Conduct detailed review against all criteria\n   - Document specific reasons for exclusion\n   - Record final number of included studies\n\n5. **Create PRISMA Flow Diagram**:\n   ```\n   Initial search: n = X\n    After deduplication: n = Y\n    After title screening: n = Z\n    After abstract screening: n = A\n    Included in review: n = B\n   ```\n\n### Phase 4: Data Extraction and Quality Assessment\n\n1. **Extract Key Data** from each included study:\n   - Study metadata (authors, year, journal, DOI)\n   - Study design and methods\n   - Sample size and population characteristics\n   - Key findings and results\n   - Limitations noted by authors\n   - Funding sources and conflicts of interest\n\n2. **Assess Study Quality**:\n   - **For RCTs**: Use Cochrane Risk of Bias tool\n   - **For observational studies**: Use Newcastle-Ottawa Scale\n   - **For systematic reviews**: Use AMSTAR 2\n   - Rate each study: High, Moderate, Low, or Very Low quality\n   - Consider excluding very low-quality studies\n\n3. **Organize by Themes**:\n   - Identify 3-5 major themes across studies\n   - Group studies by theme (studies may appear in multiple themes)\n   - Note patterns, consensus, and controversies\n\n### Phase 5: Synthesis and Analysis\n\n1. **Create Review Document** from template:\n   ```bash\n   cp assets/review_template.md my_literature_review.md\n   ```\n\n2. **Write Thematic Synthesis** (NOT study-by-study summaries):\n   - Organize Results section by themes or research questions\n   - Synthesize findings across multiple studies within each theme\n   - Compare and contrast different approaches and results\n   - Identify consensus areas and points of controversy\n   - Highlight the strongest evidence\n\n   Example structure:\n   ```markdown\n   #### 3.3.1 Theme: CRISPR Delivery Methods\n\n   Multiple delivery approaches have been investigated for therapeutic\n   gene editing. Viral vectors (AAV) were used in 15 studies^1-15^ and\n   showed high transduction efficiency (65-85%) but raised immunogenicity\n   concerns^3,7,12^. In contrast, lipid nanoparticles demonstrated lower\n   efficiency (40-60%) but improved safety profiles^16-23^.\n   ```\n\n3. **Critical Analysis**:\n   - Evaluate methodological strengths and limitations across studies\n   - Assess quality and consistency of evidence\n   - Identify knowledge gaps and methodological gaps\n   - Note areas requiring future research\n\n4. **Write Discussion**:\n   - Interpret findings in broader context\n   - Discuss clinical, practical, or research implications\n   - Acknowledge limitations of the review itself\n   - Compare with previous reviews if applicable\n   - Propose specific future research directions\n\n### Phase 6: Citation Verification\n\n**CRITICAL**: All citations must be verified for accuracy before final submission.\n\n1. **Verify All DOIs**:\n   ```bash\n   python scripts/verify_citations.py my_literature_review.md\n   ```\n\n   This script:\n   - Extracts all DOIs from the document\n   - Verifies each DOI resolves correctly\n   - Retrieves metadata from CrossRef\n   - Generates verification report\n   - Outputs properly formatted citations\n\n2. **Review Verification Report**:\n   - Check for any failed DOIs\n   - Verify author names, titles, and publication details match\n   - Correct any errors in the original document\n   - Re-run verification until all citations pass\n\n3. **Format Citations Consistently**:\n   - Choose one citation style and use throughout (see `references/citation_styles.md`)\n   - Common styles: APA, Nature, Vancouver, Chicago, IEEE\n   - Use verification script output to format citations correctly\n   - Ensure in-text citations match reference list format\n\n### Phase 7: Document Generation\n\n1. **Generate PDF**:\n   ```bash\n   python scripts/generate_pdf.py my_literature_review.md \\\n     --citation-style apa \\\n     --output my_review.pdf\n   ```\n\n   Options:\n   - `--citation-style`: apa, nature, chicago, vancouver, ieee\n   - `--no-toc`: Disable table of contents\n   - `--no-numbers`: Disable section numbering\n   - `--check-deps`: Check if pandoc/xelatex are installed\n\n2. **Review Final Output**:\n   - Check PDF formatting and layout\n   - Verify all sections are present\n   - Ensure citations render correctly\n   - Check that figures/tables appear properly\n   - Verify table of contents is accurate\n\n3. **Quality Checklist**:\n   - [ ] All DOIs verified with verify_citations.py\n   - [ ] Citations formatted consistently\n   - [ ] PRISMA flow diagram included (for systematic reviews)\n   - [ ] Search methodology fully documented\n   - [ ] Inclusion/exclusion criteria clearly stated\n   - [ ] Results organized thematically (not study-by-study)\n   - [ ] Quality assessment completed\n   - [ ] Limitations acknowledged\n   - [ ] References complete and accurate\n   - [ ] PDF generates without errors\n\n## Database-Specific Search Guidance\n\n### PubMed / PubMed Central\n\nAccess via `gget` skill:\n```bash\n# Search PubMed\ngget search pubmed \"CRISPR gene editing\" -l 100\n\n# Search with filters\n# Use PubMed Advanced Search Builder to construct complex queries\n# Then execute via gget or direct Entrez API\n```\n\n**Search tips**:\n- Use MeSH terms: `\"sickle cell disease\"[MeSH]`\n- Field tags: `[Title]`, `[Title/Abstract]`, `[Author]`\n- Date filters: `2020:2024[Publication Date]`\n- Boolean operators: AND, OR, NOT\n- See MeSH browser: https://meshb.nlm.nih.gov/search\n\n### bioRxiv / medRxiv\n\nAccess via `gget` skill:\n```bash\ngget search biorxiv \"CRISPR sickle cell\" -l 50\n```\n\n**Important considerations**:\n- Preprints are not peer-reviewed\n- Verify findings with caution\n- Check if preprint has been published (CrossRef)\n- Note preprint version and date\n\n### arXiv\n\nAccess via direct API or WebFetch:\n```python\n# Example search categories:\n# q-bio.QM (Quantitative Methods)\n# q-bio.GN (Genomics)\n# q-bio.MN (Molecular Networks)\n# cs.LG (Machine Learning)\n# stat.ML (Machine Learning Statistics)\n\n# Search format: category AND terms\nsearch_query = \"cat:q-bio.QM AND ti:\\\"single cell sequencing\\\"\"\n```\n\n### Semantic Scholar\n\nAccess via direct API (requires API key, or use free tier):\n- 200M+ papers across all fields\n- Excellent for cross-disciplinary searches\n- Provides citation graphs and paper recommendations\n- Use for finding highly influential papers\n\n### Specialized Biomedical Databases\n\nUse appropriate skills:\n- **ChEMBL**: `bioservices` skill for chemical bioactivity\n- **UniProt**: `gget` or `bioservices` skill for protein information\n- **KEGG**: `bioservices` skill for pathways and genes\n- **COSMIC**: `gget` skill for cancer mutations\n- **AlphaFold**: `gget alphafold` for protein structures\n- **PDB**: `gget` or direct API for experimental structures\n\n### Citation Chaining\n\nExpand search via citation networks:\n\n1. **Forward citations** (papers citing key papers):\n   - Use Google Scholar \"Cited by\"\n   - Use Semantic Scholar or OpenAlex APIs\n   - Identifies newer research building on seminal work\n\n2. **Backward citations** (references from key papers):\n   - Extract references from included papers\n   - Identify highly cited foundational work\n   - Find papers cited by multiple included studies\n\n## Citation Style Guide\n\nDetailed formatting guidelines are in `references/citation_styles.md`. Quick reference:\n\n### APA (7th Edition)\n- In-text: (Smith et al., 2023)\n- Reference: Smith, J. D., Johnson, M. L., & Williams, K. R. (2023). Title. *Journal*, *22*(4), 301-318. https://doi.org/10.xxx/yyy\n\n### Nature\n- In-text: Superscript numbers^1,2^\n- Reference: Smith, J. D., Johnson, M. L. & Williams, K. R. Title. *Nat. Rev. Drug Discov.* **22**, 301-318 (2023).\n\n### Vancouver\n- In-text: Superscript numbers^1,2^\n- Reference: Smith JD, Johnson ML, Williams KR. Title. Nat Rev Drug Discov. 2023;22(4):301-18.\n\n**Always verify citations** with verify_citations.py before finalizing.\n\n## Best Practices\n\n### Search Strategy\n1. **Use multiple databases** (minimum 3): Ensures comprehensive coverage\n2. **Include preprint servers**: Captures latest unpublished findings\n3. **Document everything**: Search strings, dates, result counts for reproducibility\n4. **Test and refine**: Run pilot searches, review results, adjust search terms\n\n### Screening and Selection\n1. **Use clear criteria**: Document inclusion/exclusion criteria before screening\n2. **Screen systematically**: Title  Abstract  Full text\n3. **Document exclusions**: Record reasons for excluding studies\n4. **Consider dual screening**: For systematic reviews, have two reviewers screen independently\n\n### Synthesis\n1. **Organize thematically**: Group by themes, NOT by individual studies\n2. **Synthesize across studies**: Compare, contrast, identify patterns\n3. **Be critical**: Evaluate quality and consistency of evidence\n4. **Identify gaps**: Note what's missing or understudied\n\n### Quality and Reproducibility\n1. **Assess study quality**: Use appropriate quality assessment tools\n2. **Verify all citations**: Run verify_citations.py script\n3. **Document methodology**: Provide enough detail for others to reproduce\n4. **Follow guidelines**: Use PRISMA for systematic reviews\n\n### Writing\n1. **Be objective**: Present evidence fairly, acknowledge limitations\n2. **Be systematic**: Follow structured template\n3. **Be specific**: Include numbers, statistics, effect sizes where available\n4. **Be clear**: Use clear headings, logical flow, thematic organization\n\n## Common Pitfalls to Avoid\n\n1. **Single database search**: Misses relevant papers; always search multiple databases\n2. **No search documentation**: Makes review irreproducible; document all searches\n3. **Study-by-study summary**: Lacks synthesis; organize thematically instead\n4. **Unverified citations**: Leads to errors; always run verify_citations.py\n5. **Too broad search**: Yields thousands of irrelevant results; refine with specific terms\n6. **Too narrow search**: Misses relevant papers; include synonyms and related terms\n7. **Ignoring preprints**: Misses latest findings; include bioRxiv, medRxiv, arXiv\n8. **No quality assessment**: Treats all evidence equally; assess and report quality\n9. **Publication bias**: Only positive results published; note potential bias\n10. **Outdated search**: Field evolves rapidly; clearly state search date\n\n## Example Workflow\n\nComplete workflow for a biomedical literature review:\n\n```bash\n# 1. Create review document from template\ncp assets/review_template.md crispr_sickle_cell_review.md\n\n# 2. Search multiple databases using appropriate skills\n# - Use gget skill for PubMed, bioRxiv\n# - Use direct API access for arXiv, Semantic Scholar\n# - Export results in JSON format\n\n# 3. Aggregate and process results\npython scripts/search_databases.py combined_results.json \\\n  --deduplicate \\\n  --rank citations \\\n  --year-start 2015 \\\n  --year-end 2024 \\\n  --format markdown \\\n  --output search_results.md \\\n  --summary\n\n# 4. Screen results and extract data\n# - Manually screen titles, abstracts, full texts\n# - Extract key data into the review document\n# - Organize by themes\n\n# 5. Write the review following template structure\n# - Introduction with clear objectives\n# - Detailed methodology section\n# - Results organized thematically\n# - Critical discussion\n# - Clear conclusions\n\n# 6. Verify all citations\npython scripts/verify_citations.py crispr_sickle_cell_review.md\n\n# Review the citation report\ncat crispr_sickle_cell_review_citation_report.json\n\n# Fix any failed citations and re-verify\npython scripts/verify_citations.py crispr_sickle_cell_review.md\n\n# 7. Generate professional PDF\npython scripts/generate_pdf.py crispr_sickle_cell_review.md \\\n  --citation-style nature \\\n  --output crispr_sickle_cell_review.pdf\n\n# 8. Review final PDF and markdown outputs\n```\n\n## Integration with Other Skills\n\nThis skill works seamlessly with other scientific skills:\n\n### Database Access Skills\n- **gget**: PubMed, bioRxiv, COSMIC, AlphaFold, Ensembl, UniProt\n- **bioservices**: ChEMBL, KEGG, Reactome, UniProt, PubChem\n- **datacommons-client**: Demographics, economics, health statistics\n\n### Analysis Skills\n- **pydeseq2**: RNA-seq differential expression (for methods sections)\n- **scanpy**: Single-cell analysis (for methods sections)\n- **anndata**: Single-cell data (for methods sections)\n- **biopython**: Sequence analysis (for background sections)\n\n### Visualization Skills\n- **matplotlib**: Generate figures and plots for review\n- **seaborn**: Statistical visualizations\n\n### Writing Skills\n- **brand-guidelines**: Apply institutional branding to PDF\n- **internal-comms**: Adapt review for different audiences\n\n## Resources\n\n### Bundled Resources\n\n**Scripts:**\n- `scripts/verify_citations.py`: Verify DOIs and generate formatted citations\n- `scripts/generate_pdf.py`: Convert markdown to professional PDF\n- `scripts/search_databases.py`: Process, deduplicate, and format search results\n\n**References:**\n- `references/citation_styles.md`: Detailed citation formatting guide (APA, Nature, Vancouver, Chicago, IEEE)\n- `references/database_strategies.md`: Comprehensive database search strategies\n\n**Assets:**\n- `assets/review_template.md`: Complete literature review template with all sections\n\n### External Resources\n\n**Guidelines:**\n- PRISMA (Systematic Reviews): http://www.prisma-statement.org/\n- Cochrane Handbook: https://training.cochrane.org/handbook\n- AMSTAR 2 (Review Quality): https://amstar.ca/\n\n**Tools:**\n- MeSH Browser: https://meshb.nlm.nih.gov/search\n- PubMed Advanced Search: https://pubmed.ncbi.nlm.nih.gov/advanced/\n- Boolean Search Guide: https://www.ncbi.nlm.nih.gov/books/NBK3827/\n\n**Citation Styles:**\n- APA Style: https://apastyle.apa.org/\n- Nature Portfolio: https://www.nature.com/nature-portfolio/editorial-policies/reporting-standards\n- NLM/Vancouver: https://www.nlm.nih.gov/bsd/uniform_requirements.html\n\n## Dependencies\n\n### Required Python Packages\n```bash\nuv pip install requests  # For citation verification\n```\n\n### Required System Tools\n```bash\n# For PDF generation\nbrew install pandoc  # macOS\napt-get install pandoc  # Linux\n\n# For LaTeX (PDF generation)\nbrew install --cask mactex  # macOS\napt-get install texlive-xetex  # Linux\n```\n\nCheck dependencies:\n```bash\npython scripts/generate_pdf.py --check-deps\n```\n\n## Summary\n\nThis literature-review skill provides:\n\n1. **Systematic methodology** following academic best practices\n2. **Multi-database integration** via existing scientific skills\n3. **Citation verification** ensuring accuracy and credibility\n4. **Professional output** in markdown and PDF formats\n5. **Comprehensive guidance** covering the entire review process\n6. **Quality assurance** with verification and validation tools\n7. **Reproducibility** through detailed documentation requirements\n\nConduct thorough, rigorous literature reviews that meet academic standards and provide comprehensive synthesis of current knowledge in any domain."
              },
              {
                "name": "scholar-evaluation",
                "description": "Systematic framework for evaluating scholarly and research work based on the ScholarEval methodology. This skill should be used when assessing research papers, evaluating literature reviews, scoring research methodologies, analyzing scientific writing quality, or applying structured evaluation criteria to academic work. Provides comprehensive assessment across multiple dimensions including problem formulation, literature review, methodology, data collection, analysis, results interpretation, and scholarly writing quality.",
                "path": "plugins/scholarly-comms-researcher/skills/scholar-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "scholar-evaluation",
                  "description": "Systematic framework for evaluating scholarly and research work based on the ScholarEval methodology. This skill should be used when assessing research papers, evaluating literature reviews, scoring research methodologies, analyzing scientific writing quality, or applying structured evaluation criteria to academic work. Provides comprehensive assessment across multiple dimensions including problem formulation, literature review, methodology, data collection, analysis, results interpretation, and scholarly writing quality."
                },
                "content": "# Scholar Evaluation\n\n## Overview\n\nApply the ScholarEval framework to systematically evaluate scholarly and research work. This skill provides structured evaluation methodology based on peer-reviewed research assessment criteria, enabling comprehensive analysis of academic papers, research proposals, literature reviews, and scholarly writing across multiple quality dimensions.\n\n## When to Use This Skill\n\nUse this skill when:\n- Evaluating research papers for quality and rigor\n- Assessing literature review comprehensiveness and quality\n- Reviewing research methodology design\n- Scoring data analysis approaches\n- Evaluating scholarly writing and presentation\n- Providing structured feedback on academic work\n- Benchmarking research quality against established criteria\n\n## Evaluation Workflow\n\n### Step 1: Initial Assessment and Scope Definition\n\nBegin by identifying the type of scholarly work being evaluated and the evaluation scope:\n\n**Work Types:**\n- Full research paper (empirical, theoretical, or review)\n- Research proposal or protocol\n- Literature review (systematic, narrative, or scoping)\n- Thesis or dissertation chapter\n- Conference abstract or short paper\n\n**Evaluation Scope:**\n- Comprehensive (all dimensions)\n- Targeted (specific aspects like methodology or writing)\n- Comparative (benchmarking against other work)\n\nAsk the user to clarify if the scope is ambiguous.\n\n### Step 2: Dimension-Based Evaluation\n\nSystematically evaluate the work across the ScholarEval dimensions. For each applicable dimension, assess quality, identify strengths and weaknesses, and provide scores where appropriate.\n\nRefer to `references/evaluation_framework.md` for detailed criteria and rubrics for each dimension.\n\n**Core Evaluation Dimensions:**\n\n1. **Problem Formulation & Research Questions**\n   - Clarity and specificity of research questions\n   - Theoretical or practical significance\n   - Feasibility and scope appropriateness\n   - Novelty and contribution potential\n\n2. **Literature Review**\n   - Comprehensiveness of coverage\n   - Critical synthesis vs. mere summarization\n   - Identification of research gaps\n   - Currency and relevance of sources\n   - Proper contextualization\n\n3. **Methodology & Research Design**\n   - Appropriateness for research questions\n   - Rigor and validity\n   - Reproducibility and transparency\n   - Ethical considerations\n   - Limitations acknowledgment\n\n4. **Data Collection & Sources**\n   - Quality and appropriateness of data\n   - Sample size and representativeness\n   - Data collection procedures\n   - Source credibility and reliability\n\n5. **Analysis & Interpretation**\n   - Appropriateness of analytical methods\n   - Rigor of analysis\n   - Logical coherence\n   - Alternative explanations considered\n   - Results-claims alignment\n\n6. **Results & Findings**\n   - Clarity of presentation\n   - Statistical or qualitative rigor\n   - Visualization quality\n   - Interpretation accuracy\n   - Implications discussion\n\n7. **Scholarly Writing & Presentation**\n   - Clarity and organization\n   - Academic tone and style\n   - Grammar and mechanics\n   - Logical flow\n   - Accessibility to target audience\n\n8. **Citations & References**\n   - Citation completeness\n   - Source quality and appropriateness\n   - Citation accuracy\n   - Balance of perspectives\n   - Adherence to citation standards\n\n### Step 3: Scoring and Rating\n\nFor each evaluated dimension, provide:\n\n**Qualitative Assessment:**\n- Key strengths (2-3 specific points)\n- Areas for improvement (2-3 specific points)\n- Critical issues (if any)\n\n**Quantitative Scoring (Optional):**\nUse a 5-point scale where applicable:\n- 5: Excellent - Exemplary quality, publishable in top venues\n- 4: Good - Strong quality with minor improvements needed\n- 3: Adequate - Acceptable quality with notable areas for improvement\n- 2: Needs Improvement - Significant revisions required\n- 1: Poor - Fundamental issues requiring major revision\n\nTo calculate aggregate scores programmatically, use `scripts/calculate_scores.py`.\n\n### Step 4: Synthesize Overall Assessment\n\nProvide an integrated evaluation summary:\n\n1. **Overall Quality Assessment** - Holistic judgment of the work's scholarly merit\n2. **Major Strengths** - 3-5 key strengths across dimensions\n3. **Critical Weaknesses** - 3-5 primary areas requiring attention\n4. **Priority Recommendations** - Ranked list of improvements by impact\n5. **Publication Readiness** (if applicable) - Assessment of suitability for target venues\n\n### Step 5: Provide Actionable Feedback\n\nTransform evaluation findings into constructive, actionable feedback:\n\n**Feedback Structure:**\n- **Specific** - Reference exact sections, paragraphs, or page numbers\n- **Actionable** - Provide concrete suggestions for improvement\n- **Prioritized** - Rank recommendations by importance and feasibility\n- **Balanced** - Acknowledge strengths while addressing weaknesses\n- **Evidence-based** - Ground feedback in evaluation criteria\n\n**Feedback Format Options:**\n- Structured report with dimension-by-dimension analysis\n- Annotated comments mapped to specific document sections\n- Executive summary with key findings and recommendations\n- Comparative analysis against benchmark standards\n\n### Step 6: Contextual Considerations\n\nAdjust evaluation approach based on:\n\n**Stage of Development:**\n- Early draft: Focus on conceptual and structural issues\n- Advanced draft: Focus on refinement and polish\n- Final submission: Comprehensive quality check\n\n**Purpose and Venue:**\n- Journal article: High standards for rigor and contribution\n- Conference paper: Balance novelty with presentation clarity\n- Student work: Educational feedback with developmental focus\n- Grant proposal: Emphasis on feasibility and impact\n\n**Discipline-Specific Norms:**\n- STEM fields: Emphasis on reproducibility and statistical rigor\n- Social sciences: Balance quantitative and qualitative standards\n- Humanities: Focus on argumentation and scholarly interpretation\n\n## Resources\n\n### references/evaluation_framework.md\n\nDetailed evaluation criteria, rubrics, and quality indicators for each ScholarEval dimension. Load this reference when conducting evaluations to access specific assessment guidelines and scoring rubrics.\n\nSearch patterns for quick access:\n- \"Problem Formulation criteria\"\n- \"Literature Review rubric\"\n- \"Methodology assessment\"\n- \"Data quality indicators\"\n- \"Analysis rigor standards\"\n- \"Writing quality checklist\"\n\n### scripts/calculate_scores.py\n\nPython script for calculating aggregate evaluation scores from dimension-level ratings. Supports weighted averaging, threshold analysis, and score visualization.\n\nUsage:\n```python\npython scripts/calculate_scores.py --scores <dimension_scores.json> --output <report.txt>\n```\n\n## Best Practices\n\n1. **Maintain Objectivity** - Base evaluations on established criteria, not personal preferences\n2. **Be Comprehensive** - Evaluate all applicable dimensions systematically\n3. **Provide Evidence** - Support assessments with specific examples from the work\n4. **Stay Constructive** - Frame weaknesses as opportunities for improvement\n5. **Consider Context** - Adjust expectations based on work stage and purpose\n6. **Document Rationale** - Explain the reasoning behind assessments and scores\n7. **Encourage Strengths** - Explicitly acknowledge what the work does well\n8. **Prioritize Feedback** - Focus on high-impact improvements first\n\n## Example Evaluation Workflow\n\n**User Request:** \"Evaluate this research paper on machine learning for drug discovery\"\n\n**Response Process:**\n1. Identify work type (empirical research paper) and scope (comprehensive evaluation)\n2. Load `references/evaluation_framework.md` for detailed criteria\n3. Systematically assess each dimension:\n   - Problem formulation: Clear research question about ML model performance\n   - Literature review: Comprehensive coverage of recent ML and drug discovery work\n   - Methodology: Appropriate deep learning architecture with validation procedures\n   - [Continue through all dimensions...]\n4. Calculate dimension scores and overall assessment\n5. Synthesize findings into structured report highlighting:\n   - Strong methodology and reproducible code\n   - Needs more diverse dataset evaluation\n   - Writing could improve clarity in results section\n6. Provide prioritized recommendations with specific suggestions\n\n## Notes\n\n- Evaluation rigor should match the work's purpose and stage\n- Some dimensions may not apply to all work types (e.g., data collection for purely theoretical papers)\n- Cultural and disciplinary differences in scholarly norms should be considered\n- This framework complements, not replaces, domain-specific expertise\n\n## Citation\n\nThis skill is based on the ScholarEval framework introduced in:\n\n**Moussa, H. N., Da Silva, P. Q., Adu-Ampratwum, D., East, A., Lu, Z., Puccetti, N., Xue, M., Sun, H., Majumder, B. P., & Kumar, S. (2025).** _ScholarEval: Research Idea Evaluation Grounded in Literature_. arXiv preprint arXiv:2510.16234. [https://arxiv.org/abs/2510.16234](https://arxiv.org/abs/2510.16234)\n\n**Abstract:** ScholarEval is a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness (the empirical validity of proposed methods based on existing literature) and contribution (the degree of advancement made by the idea across different dimensions relative to prior research). The framework achieves significantly higher coverage of expert-annotated evaluation points and is consistently preferred over baseline systems in terms of evaluation actionability, depth, and evidence support."
              },
              {
                "name": "scientific-brainstorming",
                "description": "Research ideation partner. Generate hypotheses, explore interdisciplinary connections, challenge assumptions, develop methodologies, identify research gaps, for creative scientific problem-solving.",
                "path": "plugins/scholarly-comms-researcher/skills/scientific-brainstorming/SKILL.md",
                "frontmatter": {
                  "name": "scientific-brainstorming",
                  "description": "Research ideation partner. Generate hypotheses, explore interdisciplinary connections, challenge assumptions, develop methodologies, identify research gaps, for creative scientific problem-solving."
                },
                "content": "# Scientific Brainstorming\n\n## Overview\n\nScientific brainstorming is a conversational process for generating novel research ideas. Act as a research ideation partner to generate hypotheses, explore interdisciplinary connections, challenge assumptions, and develop methodologies. Apply this skill for creative scientific problem-solving.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Generating novel research ideas or directions\n- Exploring interdisciplinary connections and analogies\n- Challenging assumptions in existing research frameworks\n- Developing new methodological approaches\n- Identifying research gaps or opportunities\n- Overcoming creative blocks in problem-solving\n- Brainstorming experimental designs or study plans\n\n## Core Principles\n\nWhen engaging in scientific brainstorming:\n\n1. **Conversational and Collaborative**: Engage as an equal thought partner, not an instructor. Ask questions, build on ideas together, and maintain a natural dialogue.\n\n2. **Intellectually Curious**: Show genuine interest in the scientist's work. Ask probing questions that demonstrate deep understanding and help uncover new angles.\n\n3. **Creatively Challenging**: Push beyond obvious ideas. Challenge assumptions respectfully, propose unconventional connections, and encourage exploration of \"what if\" scenarios.\n\n4. **Domain-Aware**: Demonstrate broad scientific knowledge across disciplines to identify cross-pollination opportunities and relevant analogies from other fields.\n\n5. **Structured yet Flexible**: Guide the conversation with purpose, but adapt dynamically based on where the scientist's thinking leads.\n\n## Brainstorming Workflow\n\n### Phase 1: Understanding the Context\n\nBegin by deeply understanding what the scientist is working on. This phase establishes the foundation for productive ideation.\n\n**Approach:**\n- Ask open-ended questions about their current research, interests, or challenge\n- Understand their field, methodology, and constraints\n- Identify what they're trying to achieve and what obstacles they face\n- Listen for implicit assumptions or unexplored angles\n\n**Example questions:**\n- \"What aspect of your research are you most excited about right now?\"\n- \"What problem keeps you up at night?\"\n- \"What assumptions are you making that might be worth questioning?\"\n- \"Are there any unexpected findings that don't fit your current model?\"\n\n**Transition:** Once the context is clear, acknowledge understanding and suggest moving into active ideation.\n\n### Phase 2: Divergent Exploration\n\nHelp the scientist generate a wide range of ideas without judgment. The goal is quantity and diversity, not immediate feasibility.\n\n**Techniques to employ:**\n\n1. **Cross-Domain Analogies**\n   - Draw parallels from other scientific fields\n   - \"How might concepts from [field X] apply to your problem?\"\n   - Connect biological systems to social networks, physics to economics, etc.\n\n2. **Assumption Reversal**\n   - Identify core assumptions and flip them\n   - \"What if the opposite were true?\"\n   - \"What if you had unlimited resources/time/data?\"\n\n3. **Scale Shifting**\n   - Explore the problem at different scales (molecular, cellular, organismal, population, ecosystem)\n   - Consider temporal scales (milliseconds to millennia)\n\n4. **Constraint Removal/Addition**\n   - Remove apparent constraints: \"What if you could measure anything?\"\n   - Add new constraints: \"What if you had to solve this with 1800s technology?\"\n\n5. **Interdisciplinary Fusion**\n   - Suggest combining methodologies from different fields\n   - Propose collaborations that bridge disciplines\n\n6. **Technology Speculation**\n   - Imagine emerging technologies applied to the problem\n   - \"What becomes possible with CRISPR/AI/quantum computing/etc.?\"\n\n**Interaction style:**\n- Rapid-fire idea generation with the scientist\n- Build on their suggestions with \"Yes, and...\"\n- Encourage wild ideas explicitly: \"What's the most radical approach imaginable?\"\n- Consult references/brainstorming_methods.md for additional structured techniques\n\n### Phase 3: Connection Making\n\nHelp identify patterns, themes, and unexpected connections among the generated ideas.\n\n**Approach:**\n- Look for common threads across different ideas\n- Identify which ideas complement or enhance each other\n- Find surprising connections between seemingly unrelated concepts\n- Map relationships between ideas visually (if helpful)\n\n**Prompts:**\n- \"I notice several ideas involve [theme]what if we combined them?\"\n- \"These three approaches share [commonality]is there something deeper there?\"\n- \"What's the most unexpected connection you're seeing?\"\n\n### Phase 4: Critical Evaluation\n\nShift to constructively evaluating the most promising ideas while maintaining creative momentum.\n\n**Balance:**\n- Be critical but not dismissive\n- Identify both strengths and challenges\n- Consider feasibility while preserving innovative elements\n- Suggest modifications to make wild ideas more tractable\n\n**Questions to explore:**\n- \"What would it take to actually test this?\"\n- \"What's the first small experiment to run?\"\n- \"What existing data or tools could be leveraged?\"\n- \"Who else would need to be involved?\"\n- \"What's the biggest obstacle, and how might it be overcome?\"\n\n### Phase 5: Synthesis and Next Steps\n\nHelp crystallize insights and create concrete paths forward.\n\n**Deliverables:**\n- Summarize the most promising directions identified\n- Highlight novel connections or perspectives discovered\n- Suggest immediate next steps (literature search, pilot experiments, collaborations)\n- Capture key questions that emerged for future exploration\n- Identify resources or expertise that would be valuable\n\n**Close with encouragement:**\n- Acknowledge the creative work done\n- Reinforce the value of the ideas generated\n- Offer to continue the brainstorming in future sessions\n\n## Adaptive Techniques\n\n### When the Scientist Is Stuck\n\n- Break the problem into smaller pieces\n- Change the framing entirely (\"Instead of asking X, what if we asked Y?\")\n- Tell a story or analogy that might spark new thinking\n- Suggest taking a \"vacation\" from the problem to explore tangential ideas\n\n### When Ideas Are Too Safe\n\n- Explicitly encourage risk-taking: \"What's an idea so bold it makes you nervous?\"\n- Play devil's advocate to the conservative approach\n- Ask about failed or abandoned approaches and why they might actually work\n- Propose intentionally provocative \"what ifs\"\n\n### When Energy Lags\n\n- Inject enthusiasm about interesting ideas\n- Share genuine curiosity about a particular direction\n- Ask about something that excites them personally\n- Take a brief tangent into a related but different topic\n\n## Resources\n\n### references/brainstorming_methods.md\n\nContains detailed descriptions of structured brainstorming methodologies that can be consulted when standard techniques need supplementation:\n- SCAMPER framework (Substitute, Combine, Adapt, Modify, Put to another use, Eliminate, Reverse)\n- Six Thinking Hats for multi-perspective analysis\n- Morphological analysis for systematic exploration\n- TRIZ principles for inventive problem-solving\n- Biomimicry approaches for nature-inspired solutions\n\nConsult this file when the scientist requests a specific methodology or when the brainstorming session would benefit from a more structured approach.\n\n## Notes\n\n- This is a **conversation**, not a lecture. The scientist should be doing at least 50% of the talking.\n- Avoid jargon from fields outside the scientist's expertise unless explaining it clearly.\n- Be comfortable with silencegive space for thinking.\n- Remember that the best brainstorming often feels playful and exploratory.\n- The goal is not to solve everything, but to open new possibilities."
              },
              {
                "name": "scientific-critical-thinking",
                "description": "Evaluate research rigor. Assess methodology, experimental design, statistical validity, biases, confounding, evidence quality (GRADE, Cochrane ROB), for critical analysis of scientific claims.",
                "path": "plugins/scholarly-comms-researcher/skills/scientific-critical-thinking/SKILL.md",
                "frontmatter": {
                  "name": "scientific-critical-thinking",
                  "description": "Evaluate research rigor. Assess methodology, experimental design, statistical validity, biases, confounding, evidence quality (GRADE, Cochrane ROB), for critical analysis of scientific claims."
                },
                "content": "# Scientific Critical Thinking\n\n## Overview\n\nCritical thinking is a systematic process for evaluating scientific rigor. Assess methodology, experimental design, statistical validity, biases, confounding, and evidence quality using GRADE and Cochrane ROB frameworks. Apply this skill for critical analysis of scientific claims.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Evaluating research methodology and experimental design\n- Assessing statistical validity and evidence quality\n- Identifying biases and confounding in studies\n- Reviewing scientific claims and conclusions\n- Conducting systematic reviews or meta-analyses\n- Applying GRADE or Cochrane risk of bias assessments\n- Providing critical analysis of research papers\n\n## Core Capabilities\n\n### 1. Methodology Critique\n\nEvaluate research methodology for rigor, validity, and potential flaws.\n\n**Apply when:**\n- Reviewing research papers\n- Assessing experimental designs\n- Evaluating study protocols\n- Planning new research\n\n**Evaluation framework:**\n\n1. **Study Design Assessment**\n   - Is the design appropriate for the research question?\n   - Can the design support causal claims being made?\n   - Are comparison groups appropriate and adequate?\n   - Consider whether experimental, quasi-experimental, or observational design is justified\n\n2. **Validity Analysis**\n   - **Internal validity:** Can we trust the causal inference?\n     - Check randomization quality\n     - Evaluate confounding control\n     - Assess selection bias\n     - Review attrition/dropout patterns\n   - **External validity:** Do results generalize?\n     - Evaluate sample representativeness\n     - Consider ecological validity of setting\n     - Assess whether conditions match target application\n   - **Construct validity:** Do measures capture intended constructs?\n     - Review measurement validation\n     - Check operational definitions\n     - Assess whether measures are direct or proxy\n   - **Statistical conclusion validity:** Are statistical inferences sound?\n     - Verify adequate power/sample size\n     - Check assumption compliance\n     - Evaluate test appropriateness\n\n3. **Control and Blinding**\n   - Was randomization properly implemented (sequence generation, allocation concealment)?\n   - Was blinding feasible and implemented (participants, providers, assessors)?\n   - Are control conditions appropriate (placebo, active control, no treatment)?\n   - Could performance or detection bias affect results?\n\n4. **Measurement Quality**\n   - Are instruments validated and reliable?\n   - Are measures objective when possible, or subjective with acknowledged limitations?\n   - Is outcome assessment standardized?\n   - Are multiple measures used to triangulate findings?\n\n**Reference:** See `references/scientific_method.md` for detailed principles and `references/experimental_design.md` for comprehensive design checklist.\n\n### 2. Bias Detection\n\nIdentify and evaluate potential sources of bias that could distort findings.\n\n**Apply when:**\n- Reviewing published research\n- Designing new studies\n- Interpreting conflicting evidence\n- Assessing research quality\n\n**Systematic bias review:**\n\n1. **Cognitive Biases (Researcher)**\n   - **Confirmation bias:** Are only supporting findings highlighted?\n   - **HARKing:** Were hypotheses stated a priori or formed after seeing results?\n   - **Publication bias:** Are negative results missing from literature?\n   - **Cherry-picking:** Is evidence selectively reported?\n   - Check for preregistration and analysis plan transparency\n\n2. **Selection Biases**\n   - **Sampling bias:** Is sample representative of target population?\n   - **Volunteer bias:** Do participants self-select in systematic ways?\n   - **Attrition bias:** Is dropout differential between groups?\n   - **Survivorship bias:** Are only \"survivors\" visible in sample?\n   - Examine participant flow diagrams and compare baseline characteristics\n\n3. **Measurement Biases**\n   - **Observer bias:** Could expectations influence observations?\n   - **Recall bias:** Are retrospective reports systematically inaccurate?\n   - **Social desirability:** Are responses biased toward acceptability?\n   - **Instrument bias:** Do measurement tools systematically err?\n   - Evaluate blinding, validation, and measurement objectivity\n\n4. **Analysis Biases**\n   - **P-hacking:** Were multiple analyses conducted until significance emerged?\n   - **Outcome switching:** Were non-significant outcomes replaced with significant ones?\n   - **Selective reporting:** Are all planned analyses reported?\n   - **Subgroup fishing:** Were subgroup analyses conducted without correction?\n   - Check for study registration and compare to published outcomes\n\n5. **Confounding**\n   - What variables could affect both exposure and outcome?\n   - Were confounders measured and controlled (statistically or by design)?\n   - Could unmeasured confounding explain findings?\n   - Are there plausible alternative explanations?\n\n**Reference:** See `references/common_biases.md` for comprehensive bias taxonomy with detection and mitigation strategies.\n\n### 3. Statistical Analysis Evaluation\n\nCritically assess statistical methods, interpretation, and reporting.\n\n**Apply when:**\n- Reviewing quantitative research\n- Evaluating data-driven claims\n- Assessing clinical trial results\n- Reviewing meta-analyses\n\n**Statistical review checklist:**\n\n1. **Sample Size and Power**\n   - Was a priori power analysis conducted?\n   - Is sample adequate for detecting meaningful effects?\n   - Is the study underpowered (common problem)?\n   - Do significant results from small samples raise flags for inflated effect sizes?\n\n2. **Statistical Tests**\n   - Are tests appropriate for data type and distribution?\n   - Were test assumptions checked and met?\n   - Are parametric tests justified, or should non-parametric alternatives be used?\n   - Is the analysis matched to study design (e.g., paired vs. independent)?\n\n3. **Multiple Comparisons**\n   - Were multiple hypotheses tested?\n   - Was correction applied (Bonferroni, FDR, other)?\n   - Are primary outcomes distinguished from secondary/exploratory?\n   - Could findings be false positives from multiple testing?\n\n4. **P-Value Interpretation**\n   - Are p-values interpreted correctly (probability of data if null is true)?\n   - Is non-significance incorrectly interpreted as \"no effect\"?\n   - Is statistical significance conflated with practical importance?\n   - Are exact p-values reported, or only \"p < .05\"?\n   - Is there suspicious clustering just below .05?\n\n5. **Effect Sizes and Confidence Intervals**\n   - Are effect sizes reported alongside significance?\n   - Are confidence intervals provided to show precision?\n   - Is the effect size meaningful in practical terms?\n   - Are standardized effect sizes interpreted with field-specific context?\n\n6. **Missing Data**\n   - How much data is missing?\n   - Is missing data mechanism considered (MCAR, MAR, MNAR)?\n   - How is missing data handled (deletion, imputation, maximum likelihood)?\n   - Could missing data bias results?\n\n7. **Regression and Modeling**\n   - Is the model overfitted (too many predictors, no cross-validation)?\n   - Are predictions made outside the data range (extrapolation)?\n   - Are multicollinearity issues addressed?\n   - Are model assumptions checked?\n\n8. **Common Pitfalls**\n   - Correlation treated as causation\n   - Ignoring regression to the mean\n   - Base rate neglect\n   - Texas sharpshooter fallacy (pattern finding in noise)\n   - Simpson's paradox (confounding by subgroups)\n\n**Reference:** See `references/statistical_pitfalls.md` for detailed pitfalls and correct practices.\n\n### 4. Evidence Quality Assessment\n\nEvaluate the strength and quality of evidence systematically.\n\n**Apply when:**\n- Weighing evidence for decisions\n- Conducting literature reviews\n- Comparing conflicting findings\n- Determining confidence in conclusions\n\n**Evidence evaluation framework:**\n\n1. **Study Design Hierarchy**\n   - Systematic reviews/meta-analyses (highest for intervention effects)\n   - Randomized controlled trials\n   - Cohort studies\n   - Case-control studies\n   - Cross-sectional studies\n   - Case series/reports\n   - Expert opinion (lowest)\n\n   **Important:** Higher-level designs aren't always better quality. A well-designed observational study can be stronger than a poorly-conducted RCT.\n\n2. **Quality Within Design Type**\n   - Risk of bias assessment (use appropriate tool: Cochrane ROB, Newcastle-Ottawa, etc.)\n   - Methodological rigor\n   - Transparency and reporting completeness\n   - Conflicts of interest\n\n3. **GRADE Considerations (if applicable)**\n   - Start with design type (RCT = high, observational = low)\n   - **Downgrade for:**\n     - Risk of bias\n     - Inconsistency across studies\n     - Indirectness (wrong population/intervention/outcome)\n     - Imprecision (wide confidence intervals, small samples)\n     - Publication bias\n   - **Upgrade for:**\n     - Large effect sizes\n     - Dose-response relationships\n     - Confounders would reduce (not increase) effect\n\n4. **Convergence of Evidence**\n   - **Stronger when:**\n     - Multiple independent replications\n     - Different research groups and settings\n     - Different methodologies converge on same conclusion\n     - Mechanistic and empirical evidence align\n   - **Weaker when:**\n     - Single study or research group\n     - Contradictory findings in literature\n     - Publication bias evident\n     - No replication attempts\n\n5. **Contextual Factors**\n   - Biological/theoretical plausibility\n   - Consistency with established knowledge\n   - Temporality (cause precedes effect)\n   - Specificity of relationship\n   - Strength of association\n\n**Reference:** See `references/evidence_hierarchy.md` for detailed hierarchy, GRADE system, and quality assessment tools.\n\n### 5. Logical Fallacy Identification\n\nDetect and name logical errors in scientific arguments and claims.\n\n**Apply when:**\n- Evaluating scientific claims\n- Reviewing discussion/conclusion sections\n- Assessing popular science communication\n- Identifying flawed reasoning\n\n**Common fallacies in science:**\n\n1. **Causation Fallacies**\n   - **Post hoc ergo propter hoc:** \"B followed A, so A caused B\"\n   - **Correlation = causation:** Confusing association with causality\n   - **Reverse causation:** Mistaking cause for effect\n   - **Single cause fallacy:** Attributing complex outcomes to one factor\n\n2. **Generalization Fallacies**\n   - **Hasty generalization:** Broad conclusions from small samples\n   - **Anecdotal fallacy:** Personal stories as proof\n   - **Cherry-picking:** Selecting only supporting evidence\n   - **Ecological fallacy:** Group patterns applied to individuals\n\n3. **Authority and Source Fallacies**\n   - **Appeal to authority:** \"Expert said it, so it's true\" (without evidence)\n   - **Ad hominem:** Attacking person, not argument\n   - **Genetic fallacy:** Judging by origin, not merits\n   - **Appeal to nature:** \"Natural = good/safe\"\n\n4. **Statistical Fallacies**\n   - **Base rate neglect:** Ignoring prior probability\n   - **Texas sharpshooter:** Finding patterns in random data\n   - **Multiple comparisons:** Not correcting for multiple tests\n   - **Prosecutor's fallacy:** Confusing P(E|H) with P(H|E)\n\n5. **Structural Fallacies**\n   - **False dichotomy:** \"Either A or B\" when more options exist\n   - **Moving goalposts:** Changing evidence standards after they're met\n   - **Begging the question:** Circular reasoning\n   - **Straw man:** Misrepresenting arguments to attack them\n\n6. **Science-Specific Fallacies**\n   - **Galileo gambit:** \"They laughed at Galileo, so my fringe idea is correct\"\n   - **Argument from ignorance:** \"Not proven false, so true\"\n   - **Nirvana fallacy:** Rejecting imperfect solutions\n   - **Unfalsifiability:** Making untestable claims\n\n**When identifying fallacies:**\n- Name the specific fallacy\n- Explain why the reasoning is flawed\n- Identify what evidence would be needed for valid inference\n- Note that fallacious reasoning doesn't prove the conclusion falsejust that this argument doesn't support it\n\n**Reference:** See `references/logical_fallacies.md` for comprehensive fallacy catalog with examples and detection strategies.\n\n### 6. Research Design Guidance\n\nProvide constructive guidance for planning rigorous studies.\n\n**Apply when:**\n- Helping design new experiments\n- Planning research projects\n- Reviewing research proposals\n- Improving study protocols\n\n**Design process:**\n\n1. **Research Question Refinement**\n   - Ensure question is specific, answerable, and falsifiable\n   - Verify it addresses a gap or contradiction in literature\n   - Confirm feasibility (resources, ethics, time)\n   - Define variables operationally\n\n2. **Design Selection**\n   - Match design to question (causal  experimental; associational  observational)\n   - Consider feasibility and ethical constraints\n   - Choose between-subjects, within-subjects, or mixed designs\n   - Plan factorial designs if testing multiple factors\n\n3. **Bias Minimization Strategy**\n   - Implement randomization when possible\n   - Plan blinding at all feasible levels (participants, providers, assessors)\n   - Identify and plan to control confounds (randomization, matching, stratification, statistical adjustment)\n   - Standardize all procedures\n   - Plan to minimize attrition\n\n4. **Sample Planning**\n   - Conduct a priori power analysis (specify expected effect, desired power, alpha)\n   - Account for attrition in sample size\n   - Define clear inclusion/exclusion criteria\n   - Consider recruitment strategy and feasibility\n   - Plan for sample representativeness\n\n5. **Measurement Strategy**\n   - Select validated, reliable instruments\n   - Use objective measures when possible\n   - Plan multiple measures of key constructs (triangulation)\n   - Ensure measures are sensitive to expected changes\n   - Establish inter-rater reliability procedures\n\n6. **Analysis Planning**\n   - Prespecify all hypotheses and analyses\n   - Designate primary outcome clearly\n   - Plan statistical tests with assumption checks\n   - Specify how missing data will be handled\n   - Plan to report effect sizes and confidence intervals\n   - Consider multiple comparison corrections\n\n7. **Transparency and Rigor**\n   - Preregister study and analysis plan\n   - Use reporting guidelines (CONSORT, STROBE, PRISMA)\n   - Plan to report all outcomes, not just significant ones\n   - Distinguish confirmatory from exploratory analyses\n   - Commit to data/code sharing\n\n**Reference:** See `references/experimental_design.md` for comprehensive design checklist covering all stages from question to dissemination.\n\n### 7. Claim Evaluation\n\nSystematically evaluate scientific claims for validity and support.\n\n**Apply when:**\n- Assessing conclusions in papers\n- Evaluating media reports of research\n- Reviewing abstract or introduction claims\n- Checking if data support conclusions\n\n**Claim evaluation process:**\n\n1. **Identify the Claim**\n   - What exactly is being claimed?\n   - Is it a causal claim, associational claim, or descriptive claim?\n   - How strong is the claim (proven, likely, suggested, possible)?\n\n2. **Assess the Evidence**\n   - What evidence is provided?\n   - Is evidence direct or indirect?\n   - Is evidence sufficient for the strength of claim?\n   - Are alternative explanations ruled out?\n\n3. **Check Logical Connection**\n   - Do conclusions follow from the data?\n   - Are there logical leaps?\n   - Is correlational data used to support causal claims?\n   - Are limitations acknowledged?\n\n4. **Evaluate Proportionality**\n   - Is confidence proportional to evidence strength?\n   - Are hedging words used appropriately?\n   - Are limitations downplayed?\n   - Is speculation clearly labeled?\n\n5. **Check for Overgeneralization**\n   - Do claims extend beyond the sample studied?\n   - Are population restrictions acknowledged?\n   - Is context-dependence recognized?\n   - Are caveats about generalization included?\n\n6. **Red Flags**\n   - Causal language from correlational studies\n   - \"Proves\" or absolute certainty\n   - Cherry-picked citations\n   - Ignoring contradictory evidence\n   - Dismissing limitations\n   - Extrapolation beyond data\n\n**Provide specific feedback:**\n- Quote the problematic claim\n- Explain what evidence would be needed to support it\n- Suggest appropriate hedging language if warranted\n- Distinguish between data (what was found) and interpretation (what it means)\n\n## Application Guidelines\n\n### General Approach\n\n1. **Be Constructive**\n   - Identify strengths as well as weaknesses\n   - Suggest improvements rather than just criticizing\n   - Distinguish between fatal flaws and minor limitations\n   - Recognize that all research has limitations\n\n2. **Be Specific**\n   - Point to specific instances (e.g., \"Table 2 shows...\" or \"In the Methods section...\")\n   - Quote problematic statements\n   - Provide concrete examples of issues\n   - Reference specific principles or standards violated\n\n3. **Be Proportionate**\n   - Match criticism severity to issue importance\n   - Distinguish between major threats to validity and minor concerns\n   - Consider whether issues affect primary conclusions\n   - Acknowledge uncertainty in your own assessments\n\n4. **Apply Consistent Standards**\n   - Use same criteria across all studies\n   - Don't apply stricter standards to findings you dislike\n   - Acknowledge your own potential biases\n   - Base judgments on methodology, not results\n\n5. **Consider Context**\n   - Acknowledge practical and ethical constraints\n   - Consider field-specific norms for effect sizes and methods\n   - Recognize exploratory vs. confirmatory contexts\n   - Account for resource limitations in evaluating studies\n\n### When Providing Critique\n\n**Structure feedback as:**\n\n1. **Summary:** Brief overview of what was evaluated\n2. **Strengths:** What was done well (important for credibility and learning)\n3. **Concerns:** Issues organized by severity\n   - Critical issues (threaten validity of main conclusions)\n   - Important issues (affect interpretation but not fatally)\n   - Minor issues (worth noting but don't change conclusions)\n4. **Specific Recommendations:** Actionable suggestions for improvement\n5. **Overall Assessment:** Balanced conclusion about evidence quality and what can be concluded\n\n**Use precise terminology:**\n- Name specific biases, fallacies, and methodological issues\n- Reference established standards and guidelines\n- Cite principles from scientific methodology\n- Use technical terms accurately\n\n### When Uncertain\n\n- **Acknowledge uncertainty:** \"This could be X or Y; additional information needed is Z\"\n- **Ask clarifying questions:** \"Was [methodological detail] done? This affects interpretation.\"\n- **Provide conditional assessments:** \"If X was done, then Y follows; if not, then Z is concern\"\n- **Note what additional information would resolve uncertainty**\n\n## Reference Materials\n\nThis skill includes comprehensive reference materials that provide detailed frameworks for critical evaluation:\n\n- **`references/scientific_method.md`** - Core principles of scientific methodology, the scientific process, critical evaluation criteria, red flags in scientific claims, causal inference standards, peer review, and open science principles\n\n- **`references/common_biases.md`** - Comprehensive taxonomy of cognitive, experimental, methodological, statistical, and analysis biases with detection and mitigation strategies\n\n- **`references/statistical_pitfalls.md`** - Common statistical errors and misinterpretations including p-value misunderstandings, multiple comparisons problems, sample size issues, effect size mistakes, correlation/causation confusion, regression pitfalls, and meta-analysis issues\n\n- **`references/evidence_hierarchy.md`** - Traditional evidence hierarchy, GRADE system, study quality assessment criteria, domain-specific considerations, evidence synthesis principles, and practical decision frameworks\n\n- **`references/logical_fallacies.md`** - Logical fallacies common in scientific discourse organized by type (causation, generalization, authority, relevance, structure, statistical) with examples and detection strategies\n\n- **`references/experimental_design.md`** - Comprehensive experimental design checklist covering research questions, hypotheses, study design selection, variables, sampling, blinding, randomization, control groups, procedures, measurement, bias minimization, data management, statistical planning, ethical considerations, validity threats, and reporting standards\n\n**When to consult references:**\n- Load references into context when detailed frameworks are needed\n- Use grep to search references for specific topics: `grep -r \"pattern\" references/`\n- References provide depth; SKILL.md provides procedural guidance\n- Consult references for comprehensive lists, detailed criteria, and specific examples\n\n## Remember\n\n**Scientific critical thinking is about:**\n- Systematic evaluation using established principles\n- Constructive critique that improves science\n- Proportional confidence to evidence strength\n- Transparency about uncertainty and limitations\n- Consistent application of standards\n- Recognition that all research has limitations\n- Balance between skepticism and openness to evidence\n\n**Always distinguish between:**\n- Data (what was observed) and interpretation (what it means)\n- Correlation and causation\n- Statistical significance and practical importance\n- Exploratory and confirmatory findings\n- What is known and what is uncertain\n- Evidence against a claim and evidence for the null\n\n**Goals of critical thinking:**\n1. Identify strengths and weaknesses accurately\n2. Determine what conclusions are supported\n3. Recognize limitations and uncertainties\n4. Suggest improvements for future work\n5. Advance scientific understanding"
              },
              {
                "name": "scientific-visualization",
                "description": "Create publication figures with matplotlib/seaborn/plotly. Multi-panel layouts, error bars, significance markers, colorblind-safe, export PDF/EPS/TIFF, for journal-ready scientific plots.",
                "path": "plugins/scholarly-comms-researcher/skills/scientific-visualization/SKILL.md",
                "frontmatter": {
                  "name": "scientific-visualization",
                  "description": "Create publication figures with matplotlib/seaborn/plotly. Multi-panel layouts, error bars, significance markers, colorblind-safe, export PDF/EPS/TIFF, for journal-ready scientific plots."
                },
                "content": "# Scientific Visualization\n\n## Overview\n\nScientific visualization transforms data into clear, accurate figures for publication. Create journal-ready plots with multi-panel layouts, error bars, significance markers, and colorblind-safe palettes. Export as PDF/EPS/TIFF using matplotlib, seaborn, and plotly for manuscripts.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Creating plots or visualizations for scientific manuscripts\n- Preparing figures for journal submission (Nature, Science, Cell, PLOS, etc.)\n- Ensuring figures are colorblind-friendly and accessible\n- Making multi-panel figures with consistent styling\n- Exporting figures at correct resolution and format\n- Following specific publication guidelines\n- Improving existing figures to meet publication standards\n- Creating figures that need to work in both color and grayscale\n\n## Quick Start Guide\n\n### Basic Publication-Quality Figure\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Apply publication style (from scripts/style_presets.py)\nfrom style_presets import apply_publication_style\napply_publication_style('default')\n\n# Create figure with appropriate size (single column = 3.5 inches)\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\n\n# Plot data\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), label='sin(x)')\nax.plot(x, np.cos(x), label='cos(x)')\n\n# Proper labeling with units\nax.set_xlabel('Time (seconds)')\nax.set_ylabel('Amplitude (mV)')\nax.legend(frameon=False)\n\n# Remove unnecessary spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Save in publication formats (from scripts/figure_export.py)\nfrom figure_export import save_publication_figure\nsave_publication_figure(fig, 'figure1', formats=['pdf', 'png'], dpi=300)\n```\n\n### Using Pre-configured Styles\n\nApply journal-specific styles using the matplotlib style files in `assets/`:\n\n```python\nimport matplotlib.pyplot as plt\n\n# Option 1: Use style file directly\nplt.style.use('assets/nature.mplstyle')\n\n# Option 2: Use style_presets.py helper\nfrom style_presets import configure_for_journal\nconfigure_for_journal('nature', figure_width='single')\n\n# Now create figures - they'll automatically match Nature specifications\nfig, ax = plt.subplots()\n# ... your plotting code ...\n```\n\n### Quick Start with Seaborn\n\nFor statistical plots, use seaborn with publication styling:\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom style_presets import apply_publication_style\n\n# Apply publication style\napply_publication_style('default')\nsns.set_theme(style='ticks', context='paper', font_scale=1.1)\nsns.set_palette('colorblind')\n\n# Create statistical comparison figure\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsns.boxplot(data=df, x='treatment', y='response', \n            order=['Control', 'Low', 'High'], palette='Set2', ax=ax)\nsns.stripplot(data=df, x='treatment', y='response',\n              order=['Control', 'Low', 'High'], \n              color='black', alpha=0.3, size=3, ax=ax)\nax.set_ylabel('Response (M)')\nsns.despine()\n\n# Save figure\nfrom figure_export import save_publication_figure\nsave_publication_figure(fig, 'treatment_comparison', formats=['pdf', 'png'], dpi=300)\n```\n\n## Core Principles and Best Practices\n\n### 1. Resolution and File Format\n\n**Critical requirements** (detailed in `references/publication_guidelines.md`):\n- **Raster images** (photos, microscopy): 300-600 DPI\n- **Line art** (graphs, plots): 600-1200 DPI or vector format\n- **Vector formats** (preferred): PDF, EPS, SVG\n- **Raster formats**: TIFF, PNG (never JPEG for scientific data)\n\n**Implementation:**\n```python\n# Use the figure_export.py script for correct settings\nfrom figure_export import save_publication_figure\n\n# Saves in multiple formats with proper DPI\nsave_publication_figure(fig, 'myfigure', formats=['pdf', 'png'], dpi=300)\n\n# Or save for specific journal requirements\nfrom figure_export import save_for_journal\nsave_for_journal(fig, 'figure1', journal='nature', figure_type='combination')\n```\n\n### 2. Color Selection - Colorblind Accessibility\n\n**Always use colorblind-friendly palettes** (detailed in `references/color_palettes.md`):\n\n**Recommended: Okabe-Ito palette** (distinguishable by all types of color blindness):\n```python\n# Option 1: Use assets/color_palettes.py\nfrom color_palettes import OKABE_ITO_LIST, apply_palette\napply_palette('okabe_ito')\n\n# Option 2: Manual specification\nokabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442',\n             '#0072B2', '#D55E00', '#CC79A7', '#000000']\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=okabe_ito)\n```\n\n**For heatmaps/continuous data:**\n- Use perceptually uniform colormaps: `viridis`, `plasma`, `cividis`\n- Avoid red-green diverging maps (use `PuOr`, `RdBu`, `BrBG` instead)\n- Never use `jet` or `rainbow` colormaps\n\n**Always test figures in grayscale** to ensure interpretability.\n\n### 3. Typography and Text\n\n**Font guidelines** (detailed in `references/publication_guidelines.md`):\n- Sans-serif fonts: Arial, Helvetica, Calibri\n- Minimum sizes at **final print size**:\n  - Axis labels: 7-9 pt\n  - Tick labels: 6-8 pt\n  - Panel labels: 8-12 pt (bold)\n- Sentence case for labels: \"Time (hours)\" not \"TIME (HOURS)\"\n- Always include units in parentheses\n\n**Implementation:**\n```python\n# Set fonts globally\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'sans-serif'\nmpl.rcParams['font.sans-serif'] = ['Arial', 'Helvetica']\nmpl.rcParams['font.size'] = 8\nmpl.rcParams['axes.labelsize'] = 9\nmpl.rcParams['xtick.labelsize'] = 7\nmpl.rcParams['ytick.labelsize'] = 7\n```\n\n### 4. Figure Dimensions\n\n**Journal-specific widths** (detailed in `references/journal_requirements.md`):\n- **Nature**: Single 89 mm, Double 183 mm\n- **Science**: Single 55 mm, Double 175 mm\n- **Cell**: Single 85 mm, Double 178 mm\n\n**Check figure size compliance:**\n```python\nfrom figure_export import check_figure_size\n\nfig = plt.figure(figsize=(3.5, 3))  # 89 mm for Nature\ncheck_figure_size(fig, journal='nature')\n```\n\n### 5. Multi-Panel Figures\n\n**Best practices:**\n- Label panels with bold letters: **A**, **B**, **C** (uppercase for most journals, lowercase for Nature)\n- Maintain consistent styling across all panels\n- Align panels along edges where possible\n- Use adequate white space between panels\n\n**Example implementation** (see `references/matplotlib_examples.md` for complete code):\n```python\nfrom string import ascii_uppercase\n\nfig = plt.figure(figsize=(7, 4))\ngs = fig.add_gridspec(2, 2, hspace=0.4, wspace=0.4)\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\n# ... create other panels ...\n\n# Add panel labels\nfor i, ax in enumerate([ax1, ax2, ...]):\n    ax.text(-0.15, 1.05, ascii_uppercase[i], transform=ax.transAxes,\n            fontsize=10, fontweight='bold', va='top')\n```\n\n## Common Tasks\n\n### Task 1: Create a Publication-Ready Line Plot\n\nSee `references/matplotlib_examples.md` Example 1 for complete code.\n\n**Key steps:**\n1. Apply publication style\n2. Set appropriate figure size for target journal\n3. Use colorblind-friendly colors\n4. Add error bars with correct representation (SEM, SD, or CI)\n5. Label axes with units\n6. Remove unnecessary spines\n7. Save in vector format\n\n**Using seaborn for automatic confidence intervals:**\n```python\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.lineplot(data=timeseries, x='time', y='measurement',\n             hue='treatment', errorbar=('ci', 95), \n             markers=True, ax=ax)\nax.set_xlabel('Time (hours)')\nax.set_ylabel('Measurement (AU)')\nsns.despine()\n```\n\n### Task 2: Create a Multi-Panel Figure\n\nSee `references/matplotlib_examples.md` Example 2 for complete code.\n\n**Key steps:**\n1. Use `GridSpec` for flexible layout\n2. Ensure consistent styling across panels\n3. Add bold panel labels (A, B, C, etc.)\n4. Align related panels\n5. Verify all text is readable at final size\n\n### Task 3: Create a Heatmap with Proper Colormap\n\nSee `references/matplotlib_examples.md` Example 4 for complete code.\n\n**Key steps:**\n1. Use perceptually uniform colormap (`viridis`, `plasma`, `cividis`)\n2. Include labeled colorbar\n3. For diverging data, use colorblind-safe diverging map (`RdBu_r`, `PuOr`)\n4. Set appropriate center value for diverging maps\n5. Test appearance in grayscale\n\n**Using seaborn for correlation matrices:**\n```python\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(5, 4))\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f',\n            cmap='RdBu_r', center=0, square=True,\n            linewidths=1, cbar_kws={'shrink': 0.8}, ax=ax)\n```\n\n### Task 4: Prepare Figure for Specific Journal\n\n**Workflow:**\n1. Check journal requirements: `references/journal_requirements.md`\n2. Configure matplotlib for journal:\n   ```python\n   from style_presets import configure_for_journal\n   configure_for_journal('nature', figure_width='single')\n   ```\n3. Create figure (will auto-size correctly)\n4. Export with journal specifications:\n   ```python\n   from figure_export import save_for_journal\n   save_for_journal(fig, 'figure1', journal='nature', figure_type='line_art')\n   ```\n\n### Task 5: Fix an Existing Figure to Meet Publication Standards\n\n**Checklist approach** (full checklist in `references/publication_guidelines.md`):\n\n1. **Check resolution**: Verify DPI meets journal requirements\n2. **Check file format**: Use vector for plots, TIFF/PNG for images\n3. **Check colors**: Ensure colorblind-friendly\n4. **Check fonts**: Minimum 6-7 pt at final size, sans-serif\n5. **Check labels**: All axes labeled with units\n6. **Check size**: Matches journal column width\n7. **Test grayscale**: Figure interpretable without color\n8. **Remove chart junk**: No unnecessary grids, 3D effects, shadows\n\n### Task 6: Create Colorblind-Friendly Visualizations\n\n**Strategy:**\n1. Use approved palettes from `assets/color_palettes.py`\n2. Add redundant encoding (line styles, markers, patterns)\n3. Test with colorblind simulator\n4. Ensure grayscale compatibility\n\n**Example:**\n```python\nfrom color_palettes import apply_palette\nimport matplotlib.pyplot as plt\n\napply_palette('okabe_ito')\n\n# Add redundant encoding beyond color\nline_styles = ['-', '--', '-.', ':']\nmarkers = ['o', 's', '^', 'v']\n\nfor i, (data, label) in enumerate(datasets):\n    plt.plot(x, data, linestyle=line_styles[i % 4],\n             marker=markers[i % 4], label=label)\n```\n\n## Statistical Rigor\n\n**Always include:**\n- Error bars (SD, SEM, or CI - specify which in caption)\n- Sample size (n) in figure or caption\n- Statistical significance markers (*, **, ***)\n- Individual data points when possible (not just summary statistics)\n\n**Example with statistics:**\n```python\n# Show individual points with summary statistics\nax.scatter(x_jittered, individual_points, alpha=0.4, s=8)\nax.errorbar(x, means, yerr=sems, fmt='o', capsize=3)\n\n# Mark significance\nax.text(1.5, max_y * 1.1, '***', ha='center', fontsize=8)\n```\n\n## Working with Different Plotting Libraries\n\n### Matplotlib\n- Most control over publication details\n- Best for complex multi-panel figures\n- Use provided style files for consistent formatting\n- See `references/matplotlib_examples.md` for extensive examples\n\n### Seaborn\n\nSeaborn provides a high-level, dataset-oriented interface for statistical graphics, built on matplotlib. It excels at creating publication-quality statistical visualizations with minimal code while maintaining full compatibility with matplotlib customization.\n\n**Key advantages for scientific visualization:**\n- Automatic statistical estimation and confidence intervals\n- Built-in support for multi-panel figures (faceting)\n- Colorblind-friendly palettes by default\n- Dataset-oriented API using pandas DataFrames\n- Semantic mapping of variables to visual properties\n\n#### Quick Start with Publication Style\n\nAlways apply matplotlib publication styles first, then configure seaborn:\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom style_presets import apply_publication_style\n\n# Apply publication style\napply_publication_style('default')\n\n# Configure seaborn for publication\nsns.set_theme(style='ticks', context='paper', font_scale=1.1)\nsns.set_palette('colorblind')  # Use colorblind-safe palette\n\n# Create figure\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\nsns.scatterplot(data=df, x='time', y='response', \n                hue='treatment', style='condition', ax=ax)\nsns.despine()  # Remove top and right spines\n```\n\n#### Common Plot Types for Publications\n\n**Statistical comparisons:**\n```python\n# Box plot with individual points for transparency\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsns.boxplot(data=df, x='treatment', y='response', \n            order=['Control', 'Low', 'High'], palette='Set2', ax=ax)\nsns.stripplot(data=df, x='treatment', y='response',\n              order=['Control', 'Low', 'High'], \n              color='black', alpha=0.3, size=3, ax=ax)\nax.set_ylabel('Response (M)')\nsns.despine()\n```\n\n**Distribution analysis:**\n```python\n# Violin plot with split comparison\nfig, ax = plt.subplots(figsize=(4, 3))\nsns.violinplot(data=df, x='timepoint', y='expression',\n               hue='treatment', split=True, inner='quartile', ax=ax)\nax.set_ylabel('Gene Expression (AU)')\nsns.despine()\n```\n\n**Correlation matrices:**\n```python\n# Heatmap with proper colormap and annotations\nfig, ax = plt.subplots(figsize=(5, 4))\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))  # Show only lower triangle\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f',\n            cmap='RdBu_r', center=0, square=True,\n            linewidths=1, cbar_kws={'shrink': 0.8}, ax=ax)\nplt.tight_layout()\n```\n\n**Time series with confidence bands:**\n```python\n# Line plot with automatic CI calculation\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.lineplot(data=timeseries, x='time', y='measurement',\n             hue='treatment', style='replicate',\n             errorbar=('ci', 95), markers=True, dashes=False, ax=ax)\nax.set_xlabel('Time (hours)')\nax.set_ylabel('Measurement (AU)')\nsns.despine()\n```\n\n#### Multi-Panel Figures with Seaborn\n\n**Using FacetGrid for automatic faceting:**\n```python\n# Create faceted plot\ng = sns.relplot(data=df, x='dose', y='response',\n                hue='treatment', col='cell_line', row='timepoint',\n                kind='line', height=2.5, aspect=1.2,\n                errorbar=('ci', 95), markers=True)\ng.set_axis_labels('Dose (M)', 'Response (AU)')\ng.set_titles('{row_name} | {col_name}')\nsns.despine()\n\n# Save with correct DPI\nfrom figure_export import save_publication_figure\nsave_publication_figure(g.figure, 'figure_facets', \n                       formats=['pdf', 'png'], dpi=300)\n```\n\n**Combining seaborn with matplotlib subplots:**\n```python\n# Create custom multi-panel layout\nfig, axes = plt.subplots(2, 2, figsize=(7, 6))\n\n# Panel A: Scatter with regression\nsns.regplot(data=df, x='predictor', y='response', ax=axes[0, 0])\naxes[0, 0].text(-0.15, 1.05, 'A', transform=axes[0, 0].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel B: Distribution comparison\nsns.violinplot(data=df, x='group', y='value', ax=axes[0, 1])\naxes[0, 1].text(-0.15, 1.05, 'B', transform=axes[0, 1].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel C: Heatmap\nsns.heatmap(correlation_data, cmap='viridis', ax=axes[1, 0])\naxes[1, 0].text(-0.15, 1.05, 'C', transform=axes[1, 0].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel D: Time series\nsns.lineplot(data=timeseries, x='time', y='signal', \n             hue='condition', ax=axes[1, 1])\naxes[1, 1].text(-0.15, 1.05, 'D', transform=axes[1, 1].transAxes,\n                fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nsns.despine()\n```\n\n#### Color Palettes for Publications\n\nSeaborn includes several colorblind-safe palettes:\n\n```python\n# Use built-in colorblind palette (recommended)\nsns.set_palette('colorblind')\n\n# Or specify custom colorblind-safe colors (Okabe-Ito)\nokabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442',\n             '#0072B2', '#D55E00', '#CC79A7', '#000000']\nsns.set_palette(okabe_ito)\n\n# For heatmaps and continuous data\nsns.heatmap(data, cmap='viridis')  # Perceptually uniform\nsns.heatmap(corr, cmap='RdBu_r', center=0)  # Diverging, centered\n```\n\n#### Choosing Between Axes-Level and Figure-Level Functions\n\n**Axes-level functions** (e.g., `scatterplot`, `boxplot`, `heatmap`):\n- Use when building custom multi-panel layouts\n- Accept `ax=` parameter for precise placement\n- Better integration with matplotlib subplots\n- More control over figure composition\n\n```python\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\nsns.scatterplot(data=df, x='x', y='y', hue='group', ax=ax)\n```\n\n**Figure-level functions** (e.g., `relplot`, `catplot`, `displot`):\n- Use for automatic faceting by categorical variables\n- Create complete figures with consistent styling\n- Great for exploratory analysis\n- Use `height` and `aspect` for sizing\n\n```python\ng = sns.relplot(data=df, x='x', y='y', col='category', kind='scatter')\n```\n\n#### Statistical Rigor with Seaborn\n\nSeaborn automatically computes and displays uncertainty:\n\n```python\n# Line plot: shows mean  95% CI by default\nsns.lineplot(data=df, x='time', y='value', hue='treatment',\n             errorbar=('ci', 95))  # Can change to 'sd', 'se', etc.\n\n# Bar plot: shows mean with bootstrapped CI\nsns.barplot(data=df, x='treatment', y='response',\n            errorbar=('ci', 95), capsize=0.1)\n\n# Always specify error type in figure caption:\n# \"Error bars represent 95% confidence intervals\"\n```\n\n#### Best Practices for Publication-Ready Seaborn Figures\n\n1. **Always set publication theme first:**\n   ```python\n   sns.set_theme(style='ticks', context='paper', font_scale=1.1)\n   ```\n\n2. **Use colorblind-safe palettes:**\n   ```python\n   sns.set_palette('colorblind')\n   ```\n\n3. **Remove unnecessary elements:**\n   ```python\n   sns.despine()  # Remove top and right spines\n   ```\n\n4. **Control figure size appropriately:**\n   ```python\n   # Axes-level: use matplotlib figsize\n   fig, ax = plt.subplots(figsize=(3.5, 2.5))\n   \n   # Figure-level: use height and aspect\n   g = sns.relplot(..., height=3, aspect=1.2)\n   ```\n\n5. **Show individual data points when possible:**\n   ```python\n   sns.boxplot(...)  # Summary statistics\n   sns.stripplot(..., alpha=0.3)  # Individual points\n   ```\n\n6. **Include proper labels with units:**\n   ```python\n   ax.set_xlabel('Time (hours)')\n   ax.set_ylabel('Expression (AU)')\n   ```\n\n7. **Export at correct resolution:**\n   ```python\n   from figure_export import save_publication_figure\n   save_publication_figure(fig, 'figure_name', \n                          formats=['pdf', 'png'], dpi=300)\n   ```\n\n#### Advanced Seaborn Techniques\n\n**Pairwise relationships for exploratory analysis:**\n```python\n# Quick overview of all relationships\ng = sns.pairplot(data=df, hue='condition', \n                 vars=['gene1', 'gene2', 'gene3'],\n                 corner=True, diag_kind='kde', height=2)\n```\n\n**Hierarchical clustering heatmap:**\n```python\n# Cluster samples and features\ng = sns.clustermap(expression_data, method='ward', \n                   metric='euclidean', z_score=0,\n                   cmap='RdBu_r', center=0, \n                   figsize=(10, 8), \n                   row_colors=condition_colors,\n                   cbar_kws={'label': 'Z-score'})\n```\n\n**Joint distributions with marginals:**\n```python\n# Bivariate distribution with context\ng = sns.jointplot(data=df, x='gene1', y='gene2',\n                  hue='treatment', kind='scatter',\n                  height=6, ratio=4, marginal_kws={'kde': True})\n```\n\n#### Common Seaborn Issues and Solutions\n\n**Issue: Legend outside plot area**\n```python\ng = sns.relplot(...)\ng._legend.set_bbox_to_anchor((0.9, 0.5))\n```\n\n**Issue: Overlapping labels**\n```python\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n```\n\n**Issue: Text too small at final size**\n```python\nsns.set_context('paper', font_scale=1.2)  # Increase if needed\n```\n\n#### Additional Resources\n\nFor more detailed seaborn information, see:\n- `scientific-packages/seaborn/SKILL.md` - Comprehensive seaborn documentation\n- `scientific-packages/seaborn/references/examples.md` - Practical use cases\n- `scientific-packages/seaborn/references/function_reference.md` - Complete API reference\n- `scientific-packages/seaborn/references/objects_interface.md` - Modern declarative API\n\n### Plotly\n- Interactive figures for exploration\n- Export static images for publication\n- Configure for publication quality:\n```python\nfig.update_layout(\n    font=dict(family='Arial, sans-serif', size=10),\n    plot_bgcolor='white',\n    # ... see matplotlib_examples.md Example 8\n)\nfig.write_image('figure.png', scale=3)  # scale=3 gives ~300 DPI\n```\n\n## Resources\n\n### References Directory\n\n**Load these as needed for detailed information:**\n\n- **`publication_guidelines.md`**: Comprehensive best practices\n  - Resolution and file format requirements\n  - Typography guidelines\n  - Layout and composition rules\n  - Statistical rigor requirements\n  - Complete publication checklist\n\n- **`color_palettes.md`**: Color usage guide\n  - Colorblind-friendly palette specifications with RGB values\n  - Sequential and diverging colormap recommendations\n  - Testing procedures for accessibility\n  - Domain-specific palettes (genomics, microscopy)\n\n- **`journal_requirements.md`**: Journal-specific specifications\n  - Technical requirements by publisher\n  - File format and DPI specifications\n  - Figure dimension requirements\n  - Quick reference table\n\n- **`matplotlib_examples.md`**: Practical code examples\n  - 10 complete working examples\n  - Line plots, bar plots, heatmaps, multi-panel figures\n  - Journal-specific figure examples\n  - Tips for each library (matplotlib, seaborn, plotly)\n\n### Scripts Directory\n\n**Use these helper scripts for automation:**\n\n- **`figure_export.py`**: Export utilities\n  - `save_publication_figure()`: Save in multiple formats with correct DPI\n  - `save_for_journal()`: Use journal-specific requirements automatically\n  - `check_figure_size()`: Verify dimensions meet journal specs\n  - Run directly: `python scripts/figure_export.py` for examples\n\n- **`style_presets.py`**: Pre-configured styles\n  - `apply_publication_style()`: Apply preset styles (default, nature, science, cell)\n  - `set_color_palette()`: Quick palette switching\n  - `configure_for_journal()`: One-command journal configuration\n  - Run directly: `python scripts/style_presets.py` to see examples\n\n### Assets Directory\n\n**Use these files in figures:**\n\n- **`color_palettes.py`**: Importable color definitions\n  - All recommended palettes as Python constants\n  - `apply_palette()` helper function\n  - Can be imported directly into notebooks/scripts\n\n- **Matplotlib style files**: Use with `plt.style.use()`\n  - `publication.mplstyle`: General publication quality\n  - `nature.mplstyle`: Nature journal specifications\n  - `presentation.mplstyle`: Larger fonts for posters/slides\n\n## Workflow Summary\n\n**Recommended workflow for creating publication figures:**\n\n1. **Plan**: Determine target journal, figure type, and content\n2. **Configure**: Apply appropriate style for journal\n   ```python\n   from style_presets import configure_for_journal\n   configure_for_journal('nature', 'single')\n   ```\n3. **Create**: Build figure with proper labels, colors, statistics\n4. **Verify**: Check size, fonts, colors, accessibility\n   ```python\n   from figure_export import check_figure_size\n   check_figure_size(fig, journal='nature')\n   ```\n5. **Export**: Save in required formats\n   ```python\n   from figure_export import save_for_journal\n   save_for_journal(fig, 'figure1', 'nature', 'combination')\n   ```\n6. **Review**: View at final size in manuscript context\n\n## Common Pitfalls to Avoid\n\n1. **Font too small**: Text unreadable when printed at final size\n2. **JPEG format**: Never use JPEG for graphs/plots (creates artifacts)\n3. **Red-green colors**: ~8% of males cannot distinguish\n4. **Low resolution**: Pixelated figures in publication\n5. **Missing units**: Always label axes with units\n6. **3D effects**: Distorts perception, avoid completely\n7. **Chart junk**: Remove unnecessary gridlines, decorations\n8. **Truncated axes**: Start bar charts at zero unless scientifically justified\n9. **Inconsistent styling**: Different fonts/colors across figures in same manuscript\n10. **No error bars**: Always show uncertainty\n\n## Final Checklist\n\nBefore submitting figures, verify:\n\n- [ ] Resolution meets journal requirements (300+ DPI)\n- [ ] File format is correct (vector for plots, TIFF for images)\n- [ ] Figure size matches journal specifications\n- [ ] All text readable at final size (6 pt)\n- [ ] Colors are colorblind-friendly\n- [ ] Figure works in grayscale\n- [ ] All axes labeled with units\n- [ ] Error bars present with definition in caption\n- [ ] Panel labels present and consistent\n- [ ] No chart junk or 3D effects\n- [ ] Fonts consistent across all figures\n- [ ] Statistical significance clearly marked\n- [ ] Legend is clear and complete\n\nUse this skill to ensure scientific figures meet the highest publication standards while remaining accessible to all readers."
              },
              {
                "name": "scientific-writing",
                "description": "Write scientific manuscripts. IMRAD structure, citations (APA/AMA/Vancouver), figures/tables, reporting guidelines (CONSORT/STROBE/PRISMA), abstracts, for research papers and journal submissions.",
                "path": "plugins/scholarly-comms-researcher/skills/scientific-writing/SKILL.md",
                "frontmatter": {
                  "name": "scientific-writing",
                  "description": "Write scientific manuscripts. IMRAD structure, citations (APA/AMA/Vancouver), figures/tables, reporting guidelines (CONSORT/STROBE/PRISMA), abstracts, for research papers and journal submissions."
                },
                "content": "# Scientific Writing\n\n## Overview\n\nScientific writing is a process for communicating research with precision and clarity. Write manuscripts using IMRAD structure, citations (APA/AMA/Vancouver), figures/tables, and reporting guidelines (CONSORT/STROBE/PRISMA). Apply this skill for research papers and journal submissions.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Writing or revising any section of a scientific manuscript (abstract, introduction, methods, results, discussion)\n- Structuring a research paper using IMRAD or other standard formats\n- Formatting citations and references in specific styles (APA, AMA, Vancouver, Chicago, IEEE)\n- Creating, formatting, or improving figures, tables, and data visualizations\n- Applying study-specific reporting guidelines (CONSORT for trials, STROBE for observational studies, PRISMA for reviews)\n- Drafting abstracts that meet journal requirements (structured or unstructured)\n- Preparing manuscripts for submission to specific journals\n- Improving writing clarity, conciseness, and precision\n- Ensuring proper use of field-specific terminology and nomenclature\n- Addressing reviewer comments and revising manuscripts\n\n## Core Capabilities\n\n### 1. Manuscript Structure and Organization\n\n**IMRAD Format**: Guide papers through the standard Introduction, Methods, Results, And Discussion structure used across most scientific disciplines. This includes:\n- **Introduction**: Establish research context, identify gaps, state objectives\n- **Methods**: Detail study design, populations, procedures, and analysis approaches\n- **Results**: Present findings objectively without interpretation\n- **Discussion**: Interpret results, acknowledge limitations, propose future directions\n\nFor detailed guidance on IMRAD structure, refer to `references/imrad_structure.md`.\n\n**Alternative Structures**: Support discipline-specific formats including:\n- Review articles (narrative, systematic, scoping)\n- Case reports and case series\n- Meta-analyses and pooled analyses\n- Theoretical/modeling papers\n- Methods papers and protocols\n\n### 2. Section-Specific Writing Guidance\n\n**Abstract Composition**: Craft concise, standalone summaries (100-250 words) that capture the paper's purpose, methods, results, and conclusions. Support both structured abstracts (with labeled sections) and unstructured single-paragraph formats.\n\n**Introduction Development**: Build compelling introductions that:\n- Establish the research problem's importance\n- Review relevant literature systematically\n- Identify knowledge gaps or controversies\n- State clear research questions or hypotheses\n- Explain the study's novelty and significance\n\n**Methods Documentation**: Ensure reproducibility through:\n- Detailed participant/sample descriptions\n- Clear procedural documentation\n- Statistical methods with justification\n- Equipment and materials specifications\n- Ethical approval and consent statements\n\n**Results Presentation**: Present findings with:\n- Logical flow from primary to secondary outcomes\n- Integration with figures and tables\n- Statistical significance with effect sizes\n- Objective reporting without interpretation\n\n**Discussion Construction**: Synthesize findings by:\n- Relating results to research questions\n- Comparing with existing literature\n- Acknowledging limitations honestly\n- Proposing mechanistic explanations\n- Suggesting practical implications and future research\n\n### 3. Citation and Reference Management\n\nApply citation styles correctly across disciplines. For comprehensive style guides, refer to `references/citation_styles.md`.\n\n**Major Citation Styles:**\n- **AMA (American Medical Association)**: Numbered superscript citations, common in medicine\n- **Vancouver**: Numbered citations in square brackets, biomedical standard\n- **APA (American Psychological Association)**: Author-date in-text citations, common in social sciences\n- **Chicago**: Notes-bibliography or author-date, humanities and sciences\n- **IEEE**: Numbered square brackets, engineering and computer science\n\n**Best Practices:**\n- Cite primary sources when possible\n- Include recent literature (last 5-10 years for active fields)\n- Balance citation distribution across introduction and discussion\n- Verify all citations against original sources\n- Use reference management software (Zotero, Mendeley, EndNote)\n\n### 4. Figures and Tables\n\nCreate effective data visualizations that enhance comprehension. For detailed best practices, refer to `references/figures_tables.md`.\n\n**When to Use Tables vs. Figures:**\n- **Tables**: Precise numerical data, complex datasets, multiple variables requiring exact values\n- **Figures**: Trends, patterns, relationships, comparisons best understood visually\n\n**Design Principles:**\n- Make each table/figure self-explanatory with complete captions\n- Use consistent formatting and terminology across all display items\n- Label all axes, columns, and rows with units\n- Include sample sizes (n) and statistical annotations\n- Follow the \"one table/figure per 1000 words\" guideline\n- Avoid duplicating information between text, tables, and figures\n\n**Common Figure Types:**\n- Bar graphs: Comparing discrete categories\n- Line graphs: Showing trends over time\n- Scatterplots: Displaying correlations\n- Box plots: Showing distributions and outliers\n- Heatmaps: Visualizing matrices and patterns\n\n### 5. Reporting Guidelines by Study Type\n\nEnsure completeness and transparency by following established reporting standards. For comprehensive guideline details, refer to `references/reporting_guidelines.md`.\n\n**Key Guidelines:**\n- **CONSORT**: Randomized controlled trials\n- **STROBE**: Observational studies (cohort, case-control, cross-sectional)\n- **PRISMA**: Systematic reviews and meta-analyses\n- **STARD**: Diagnostic accuracy studies\n- **TRIPOD**: Prediction model studies\n- **ARRIVE**: Animal research\n- **CARE**: Case reports\n- **SQUIRE**: Quality improvement studies\n- **SPIRIT**: Study protocols for clinical trials\n- **CHEERS**: Economic evaluations\n\nEach guideline provides checklists ensuring all critical methodological elements are reported.\n\n### 6. Writing Principles and Style\n\nApply fundamental scientific writing principles. For detailed guidance, refer to `references/writing_principles.md`.\n\n**Clarity**:\n- Use precise, unambiguous language\n- Define technical terms and abbreviations at first use\n- Maintain logical flow within and between paragraphs\n- Use active voice when appropriate for clarity\n\n**Conciseness**:\n- Eliminate redundant words and phrases\n- Favor shorter sentences (15-20 words average)\n- Remove unnecessary qualifiers\n- Respect word limits strictly\n\n**Accuracy**:\n- Report exact values with appropriate precision\n- Use consistent terminology throughout\n- Distinguish between observations and interpretations\n- Acknowledge uncertainty appropriately\n\n**Objectivity**:\n- Present results without bias\n- Avoid overstating findings or implications\n- Acknowledge conflicting evidence\n- Maintain professional, neutral tone\n\n### 7. Journal-Specific Formatting\n\nAdapt manuscripts to journal requirements:\n- Follow author guidelines for structure, length, and format\n- Apply journal-specific citation styles\n- Meet figure/table specifications (resolution, file formats, dimensions)\n- Include required statements (funding, conflicts of interest, data availability, ethical approval)\n- Adhere to word limits for each section\n- Format according to template requirements when provided\n\n### 8. Field-Specific Language and Terminology\n\nAdapt language, terminology, and conventions to match the specific scientific discipline. Each field has established vocabulary, preferred phrasings, and domain-specific conventions that signal expertise and ensure clarity for the target audience.\n\n**Identify Field-Specific Linguistic Conventions:**\n- Review terminology used in recent high-impact papers in the target journal\n- Note field-specific abbreviations, units, and notation systems\n- Identify preferred terms (e.g., \"participants\" vs. \"subjects,\" \"compound\" vs. \"drug,\" \"specimens\" vs. \"samples\")\n- Observe how methods, organisms, or techniques are typically described\n\n**Biomedical and Clinical Sciences:**\n- Use precise anatomical and clinical terminology (e.g., \"myocardial infarction\" not \"heart attack\" in formal writing)\n- Follow standardized disease nomenclature (ICD, DSM, SNOMED-CT)\n- Specify drug names using generic names first, brand names in parentheses if needed\n- Use \"patients\" for clinical studies, \"participants\" for community-based research\n- Follow Human Genome Variation Society (HGVS) nomenclature for genetic variants\n- Report lab values with standard units (SI units in most international journals)\n\n**Molecular Biology and Genetics:**\n- Use italics for gene symbols (e.g., *TP53*), regular font for proteins (e.g., p53)\n- Follow species-specific gene nomenclature (uppercase for human: *BRCA1*; sentence case for mouse: *Brca1*)\n- Specify organism names in full at first mention, then use accepted abbreviations (e.g., *Escherichia coli*, then *E. coli*)\n- Use standard genetic notation (e.g., +/+, +/-, -/- for genotypes)\n- Employ established terminology for molecular techniques (e.g., \"quantitative PCR\" or \"qPCR,\" not \"real-time PCR\")\n\n**Chemistry and Pharmaceutical Sciences:**\n- Follow IUPAC nomenclature for chemical compounds\n- Use systematic names for novel compounds, common names for well-known substances\n- Specify chemical structures using standard notation (e.g., SMILES, InChI for databases)\n- Report concentrations with appropriate units (mM, M, nM, or % w/v, v/v)\n- Describe synthesis routes using accepted reaction nomenclature\n- Use terms like \"bioavailability,\" \"pharmacokinetics,\" \"IC50\" consistently with field definitions\n\n**Ecology and Environmental Sciences:**\n- Use binomial nomenclature for species (italicized: *Homo sapiens*)\n- Specify taxonomic authorities at first species mention when relevant\n- Employ standardized habitat and ecosystem classifications\n- Use consistent terminology for ecological metrics (e.g., \"species richness,\" \"Shannon diversity index\")\n- Describe sampling methods with field-standard terms (e.g., \"transect,\" \"quadrat,\" \"mark-recapture\")\n\n**Physics and Engineering:**\n- Follow SI units consistently unless field conventions dictate otherwise\n- Use standard notation for physical quantities (scalars vs. vectors, tensors)\n- Employ established terminology for phenomena (e.g., \"quantum entanglement,\" \"laminar flow\")\n- Specify equipment with model numbers and manufacturers when relevant\n- Use mathematical notation consistent with field standards (e.g.,  for reduced Planck constant)\n\n**Neuroscience:**\n- Use standardized brain region nomenclature (e.g., refer to atlases like Allen Brain Atlas)\n- Specify coordinates for brain regions using established stereotaxic systems\n- Follow conventions for neural terminology (e.g., \"action potential\" not \"spike\" in formal writing)\n- Use \"neural activity,\" \"neuronal firing,\" \"brain activation\" appropriately based on measurement method\n- Describe recording techniques with proper specificity (e.g., \"whole-cell patch clamp,\" \"extracellular recording\")\n\n**Social and Behavioral Sciences:**\n- Use person-first language when appropriate (e.g., \"people with schizophrenia\" not \"schizophrenics\")\n- Employ standardized psychological constructs and validated assessment names\n- Follow APA guidelines for reducing bias in language\n- Specify theoretical frameworks using established terminology\n- Use \"participants\" rather than \"subjects\" for human research\n\n**General Principles:**\n\n**Match Audience Expertise:**\n- For specialized journals: Use field-specific terminology freely, define only highly specialized or novel terms\n- For broad-impact journals (e.g., *Nature*, *Science*): Define more technical terms, provide context for specialized concepts\n- For interdisciplinary audiences: Balance precision with accessibility, define terms at first use\n\n**Define Technical Terms Strategically:**\n- Define abbreviations at first use: \"messenger RNA (mRNA)\"\n- Provide brief explanations for specialized techniques when writing for broader audiences\n- Avoid over-defining terms well-known to the target audience (signals unfamiliarity with field)\n- Create a glossary if numerous specialized terms are unavoidable\n\n**Maintain Consistency:**\n- Use the same term for the same concept throughout (don't alternate between \"medication,\" \"drug,\" and \"pharmaceutical\")\n- Follow a consistent system for abbreviations (decide on \"PCR\" or \"polymerase chain reaction\" after first definition)\n- Apply the same nomenclature system throughout (especially for genes, species, chemicals)\n\n**Avoid Field Mixing Errors:**\n- Don't use clinical terminology for basic science (e.g., don't call mice \"patients\")\n- Avoid colloquialisms or overly general terms in place of precise field terminology\n- Don't import terminology from adjacent fields without ensuring proper usage\n\n**Verify Terminology Usage:**\n- Consult field-specific style guides and nomenclature resources\n- Check how terms are used in recent papers from the target journal\n- Use domain-specific databases and ontologies (e.g., Gene Ontology, MeSH terms)\n- When uncertain, cite a key reference that establishes terminology\n\n### 9. Common Pitfalls to Avoid\n\n**Top Rejection Reasons:**\n1. Inappropriate, incomplete, or insufficiently described statistics\n2. Over-interpretation of results or unsupported conclusions\n3. Poorly described methods affecting reproducibility\n4. Small, biased, or inappropriate samples\n5. Poor writing quality or difficult-to-follow text\n6. Inadequate literature review or context\n7. Figures and tables that are unclear or poorly designed\n8. Failure to follow reporting guidelines\n\n**Writing Quality Issues:**\n- Mixing tenses inappropriately (use past tense for methods/results, present for established facts)\n- Excessive jargon or undefined acronyms\n- Paragraph breaks that disrupt logical flow\n- Missing transitions between sections\n- Inconsistent notation or terminology\n\n## Workflow for Manuscript Development\n\n**Stage 1: Planning**\n1. Identify target journal and review author guidelines\n2. Determine applicable reporting guideline (CONSORT, STROBE, etc.)\n3. Outline manuscript structure (usually IMRAD)\n4. Plan figures and tables as the backbone of the paper\n\n**Stage 2: Drafting**\n1. Start with figures and tables (the core data story)\n2. Write Methods (often easiest to draft first)\n3. Draft Results (describing figures/tables objectively)\n4. Compose Discussion (interpreting findings)\n5. Write Introduction (setting up the research question)\n6. Craft Abstract (synthesizing the complete story)\n7. Create Title (concise and descriptive)\n\n**Stage 3: Revision**\n1. Check logical flow and \"red thread\" throughout\n2. Verify consistency in terminology and notation\n3. Ensure figures/tables are self-explanatory\n4. Confirm adherence to reporting guidelines\n5. Verify all citations are accurate and properly formatted\n6. Check word counts for each section\n7. Proofread for grammar, spelling, and clarity\n\n**Stage 4: Final Preparation**\n1. Format according to journal requirements\n2. Prepare supplementary materials\n3. Write cover letter highlighting significance\n4. Complete submission checklists\n5. Gather all required statements and forms\n\n## Integration with Other Scientific Skills\n\nThis skill works effectively with:\n- **Data analysis skills**: For generating results to report\n- **Statistical analysis**: For determining appropriate statistical presentations\n- **Literature review skills**: For contextualizing research\n- **Figure creation tools**: For developing publication-quality visualizations\n\n## References\n\nThis skill includes comprehensive reference files covering specific aspects of scientific writing:\n\n- `references/imrad_structure.md`: Detailed guide to IMRAD format and section-specific content\n- `references/citation_styles.md`: Complete citation style guides (APA, AMA, Vancouver, Chicago, IEEE)\n- `references/figures_tables.md`: Best practices for creating effective data visualizations\n- `references/reporting_guidelines.md`: Study-specific reporting standards and checklists\n- `references/writing_principles.md`: Core principles of effective scientific communication\n\nLoad these references as needed when working on specific aspects of scientific writing."
              },
              {
                "name": "scikit-survival",
                "description": "Comprehensive toolkit for survival analysis and time-to-event modeling in Python using scikit-survival. Use this skill when working with censored survival data, performing time-to-event analysis, fitting Cox models, Random Survival Forests, Gradient Boosting models, or Survival SVMs, evaluating survival predictions with concordance index or Brier score, handling competing risks, or implementing any survival analysis workflow with the scikit-survival library.",
                "path": "plugins/scholarly-comms-researcher/skills/scikit-survival/SKILL.md",
                "frontmatter": {
                  "name": "scikit-survival",
                  "description": "Comprehensive toolkit for survival analysis and time-to-event modeling in Python using scikit-survival. Use this skill when working with censored survival data, performing time-to-event analysis, fitting Cox models, Random Survival Forests, Gradient Boosting models, or Survival SVMs, evaluating survival predictions with concordance index or Brier score, handling competing risks, or implementing any survival analysis workflow with the scikit-survival library."
                },
                "content": "# scikit-survival: Survival Analysis in Python\n\n## Overview\n\nscikit-survival is a Python library for survival analysis built on top of scikit-learn. It provides specialized tools for time-to-event analysis, handling the unique challenge of censored data where some observations are only partially known.\n\nSurvival analysis aims to establish connections between covariates and the time of an event, accounting for censored records (particularly right-censored data from studies where participants don't experience events during observation periods).\n\n## When to Use This Skill\n\nUse this skill when:\n- Performing survival analysis or time-to-event modeling\n- Working with censored data (right-censored, left-censored, or interval-censored)\n- Fitting Cox proportional hazards models (standard or penalized)\n- Building ensemble survival models (Random Survival Forests, Gradient Boosting)\n- Training Survival Support Vector Machines\n- Evaluating survival model performance (concordance index, Brier score, time-dependent AUC)\n- Estimating Kaplan-Meier or Nelson-Aalen curves\n- Analyzing competing risks\n- Preprocessing survival data or handling missing values in survival datasets\n- Conducting any analysis using the scikit-survival library\n\n## Core Capabilities\n\n### 1. Model Types and Selection\n\nscikit-survival provides multiple model families, each suited for different scenarios:\n\n#### Cox Proportional Hazards Models\n**Use for**: Standard survival analysis with interpretable coefficients\n- `CoxPHSurvivalAnalysis`: Basic Cox model\n- `CoxnetSurvivalAnalysis`: Penalized Cox with elastic net for high-dimensional data\n- `IPCRidge`: Ridge regression for accelerated failure time models\n\n**See**: `references/cox-models.md` for detailed guidance on Cox models, regularization, and interpretation\n\n#### Ensemble Methods\n**Use for**: High predictive performance with complex non-linear relationships\n- `RandomSurvivalForest`: Robust, non-parametric ensemble method\n- `GradientBoostingSurvivalAnalysis`: Tree-based boosting for maximum performance\n- `ComponentwiseGradientBoostingSurvivalAnalysis`: Linear boosting with feature selection\n- `ExtraSurvivalTrees`: Extremely randomized trees for additional regularization\n\n**See**: `references/ensemble-models.md` for comprehensive guidance on ensemble methods, hyperparameter tuning, and when to use each model\n\n#### Survival Support Vector Machines\n**Use for**: Medium-sized datasets with margin-based learning\n- `FastSurvivalSVM`: Linear SVM optimized for speed\n- `FastKernelSurvivalSVM`: Kernel SVM for non-linear relationships\n- `HingeLossSurvivalSVM`: SVM with hinge loss\n- `ClinicalKernelTransform`: Specialized kernel for clinical + molecular data\n\n**See**: `references/svm-models.md` for detailed SVM guidance, kernel selection, and hyperparameter tuning\n\n#### Model Selection Decision Tree\n\n```\nStart\n High-dimensional data (p > n)?\n   Yes  CoxnetSurvivalAnalysis (elastic net)\n   No  Continue\n\n Need interpretable coefficients?\n   Yes  CoxPHSurvivalAnalysis or ComponentwiseGradientBoostingSurvivalAnalysis\n   No  Continue\n\n Complex non-linear relationships expected?\n   Yes\n     Large dataset (n > 1000)  GradientBoostingSurvivalAnalysis\n     Medium dataset  RandomSurvivalForest or FastKernelSurvivalSVM\n     Small dataset  RandomSurvivalForest\n   No  CoxPHSurvivalAnalysis or FastSurvivalSVM\n\n For maximum performance  Try multiple models and compare\n```\n\n### 2. Data Preparation and Preprocessing\n\nBefore modeling, properly prepare survival data:\n\n#### Creating Survival Outcomes\n```python\nfrom sksurv.util import Surv\n\n# From separate arrays\ny = Surv.from_arrays(event=event_array, time=time_array)\n\n# From DataFrame\ny = Surv.from_dataframe('event', 'time', df)\n```\n\n#### Essential Preprocessing Steps\n1. **Handle missing values**: Imputation strategies for features\n2. **Encode categorical variables**: One-hot encoding or label encoding\n3. **Standardize features**: Critical for SVMs and regularized Cox models\n4. **Validate data quality**: Check for negative times, sufficient events per feature\n5. **Train-test split**: Maintain similar censoring rates across splits\n\n**See**: `references/data-handling.md` for complete preprocessing workflows, data validation, and best practices\n\n### 3. Model Evaluation\n\nProper evaluation is critical for survival models. Use appropriate metrics that account for censoring:\n\n#### Concordance Index (C-index)\nPrimary metric for ranking/discrimination:\n- **Harrell's C-index**: Use for low censoring (<40%)\n- **Uno's C-index**: Use for moderate to high censoring (>40%) - more robust\n\n```python\nfrom sksurv.metrics import concordance_index_censored, concordance_index_ipcw\n\n# Harrell's C-index\nc_harrell = concordance_index_censored(y_test['event'], y_test['time'], risk_scores)[0]\n\n# Uno's C-index (recommended)\nc_uno = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n```\n\n#### Time-Dependent AUC\nEvaluate discrimination at specific time points:\n\n```python\nfrom sksurv.metrics import cumulative_dynamic_auc\n\ntimes = [365, 730, 1095]  # 1, 2, 3 years\nauc, mean_auc = cumulative_dynamic_auc(y_train, y_test, risk_scores, times)\n```\n\n#### Brier Score\nAssess both discrimination and calibration:\n\n```python\nfrom sksurv.metrics import integrated_brier_score\n\nibs = integrated_brier_score(y_train, y_test, survival_functions, times)\n```\n\n**See**: `references/evaluation-metrics.md` for comprehensive evaluation guidance, metric selection, and using scorers with cross-validation\n\n### 4. Competing Risks Analysis\n\nHandle situations with multiple mutually exclusive event types:\n\n```python\nfrom sksurv.nonparametric import cumulative_incidence_competing_risks\n\n# Estimate cumulative incidence for each event type\ntime_points, cif_event1, cif_event2 = cumulative_incidence_competing_risks(y)\n```\n\n**Use competing risks when**:\n- Multiple mutually exclusive event types exist (e.g., death from different causes)\n- Occurrence of one event prevents others\n- Need probability estimates for specific event types\n\n**See**: `references/competing-risks.md` for detailed competing risks methods, cause-specific hazard models, and interpretation\n\n### 5. Non-parametric Estimation\n\nEstimate survival functions without parametric assumptions:\n\n#### Kaplan-Meier Estimator\n```python\nfrom sksurv.nonparametric import kaplan_meier_estimator\n\ntime, survival_prob = kaplan_meier_estimator(y['event'], y['time'])\n```\n\n#### Nelson-Aalen Estimator\n```python\nfrom sksurv.nonparametric import nelson_aalen_estimator\n\ntime, cumulative_hazard = nelson_aalen_estimator(y['event'], y['time'])\n```\n\n## Typical Workflows\n\n### Workflow 1: Standard Survival Analysis\n\n```python\nfrom sksurv.datasets import load_breast_cancer\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\nfrom sksurv.metrics import concordance_index_ipcw\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Load and prepare data\nX, y = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Preprocess\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 3. Fit model\nestimator = CoxPHSurvivalAnalysis()\nestimator.fit(X_train_scaled, y_train)\n\n# 4. Predict\nrisk_scores = estimator.predict(X_test_scaled)\n\n# 5. Evaluate\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\nprint(f\"C-index: {c_index:.3f}\")\n```\n\n### Workflow 2: High-Dimensional Data with Feature Selection\n\n```python\nfrom sksurv.linear_model import CoxnetSurvivalAnalysis\nfrom sklearn.model_selection import GridSearchCV\nfrom sksurv.metrics import as_concordance_index_ipcw_scorer\n\n# 1. Use penalized Cox for feature selection\nestimator = CoxnetSurvivalAnalysis(l1_ratio=0.9)  # Lasso-like\n\n# 2. Tune regularization with cross-validation\nparam_grid = {'alpha_min_ratio': [0.01, 0.001]}\ncv = GridSearchCV(estimator, param_grid,\n                  scoring=as_concordance_index_ipcw_scorer(), cv=5)\ncv.fit(X, y)\n\n# 3. Identify selected features\nbest_model = cv.best_estimator_\nselected_features = np.where(best_model.coef_ != 0)[0]\n```\n\n### Workflow 3: Ensemble Method for Maximum Performance\n\n```python\nfrom sksurv.ensemble import GradientBoostingSurvivalAnalysis\nfrom sklearn.model_selection import GridSearchCV\n\n# 1. Define parameter grid\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7]\n}\n\n# 2. Grid search\ngbs = GradientBoostingSurvivalAnalysis()\ncv = GridSearchCV(gbs, param_grid, cv=5,\n                  scoring=as_concordance_index_ipcw_scorer(), n_jobs=-1)\ncv.fit(X_train, y_train)\n\n# 3. Evaluate best model\nbest_model = cv.best_estimator_\nrisk_scores = best_model.predict(X_test)\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n```\n\n### Workflow 4: Comprehensive Model Comparison\n\n```python\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\nfrom sksurv.svm import FastSurvivalSVM\nfrom sksurv.metrics import concordance_index_ipcw, integrated_brier_score\n\n# Define models\nmodels = {\n    'Cox': CoxPHSurvivalAnalysis(),\n    'RSF': RandomSurvivalForest(n_estimators=100, random_state=42),\n    'GBS': GradientBoostingSurvivalAnalysis(random_state=42),\n    'SVM': FastSurvivalSVM(random_state=42)\n}\n\n# Evaluate each model\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    risk_scores = model.predict(X_test_scaled)\n    c_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n    results[name] = c_index\n    print(f\"{name}: C-index = {c_index:.3f}\")\n\n# Select best model\nbest_model_name = max(results, key=results.get)\nprint(f\"\\nBest model: {best_model_name}\")\n```\n\n## Integration with scikit-learn\n\nscikit-survival fully integrates with scikit-learn's ecosystem:\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n# Use pipelines\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', CoxPHSurvivalAnalysis())\n])\n\n# Use cross-validation\nscores = cross_val_score(pipeline, X, y, cv=5,\n                         scoring=as_concordance_index_ipcw_scorer())\n\n# Use grid search\nparam_grid = {'model__alpha': [0.1, 1.0, 10.0]}\ncv = GridSearchCV(pipeline, param_grid, cv=5)\ncv.fit(X, y)\n```\n\n## Best Practices\n\n1. **Always standardize features** for SVMs and regularized Cox models\n2. **Use Uno's C-index** instead of Harrell's when censoring > 40%\n3. **Report multiple evaluation metrics** (C-index, integrated Brier score, time-dependent AUC)\n4. **Check proportional hazards assumption** for Cox models\n5. **Use cross-validation** for hyperparameter tuning with appropriate scorers\n6. **Validate data quality** before modeling (check for negative times, sufficient events per feature)\n7. **Compare multiple model types** to find best performance\n8. **Use permutation importance** for Random Survival Forests (not built-in importance)\n9. **Consider competing risks** when multiple event types exist\n10. **Document censoring mechanism** and rates in analysis\n\n## Common Pitfalls to Avoid\n\n1. **Using Harrell's C-index with high censoring**  Use Uno's C-index\n2. **Not standardizing features for SVMs**  Always standardize\n3. **Forgetting to pass y_train to concordance_index_ipcw**  Required for IPCW calculation\n4. **Treating competing events as censored**  Use competing risks methods\n5. **Not checking for sufficient events per feature**  Rule of thumb: 10+ events per feature\n6. **Using built-in feature importance for RSF**  Use permutation importance\n7. **Ignoring proportional hazards assumption**  Validate or use alternative models\n8. **Not using appropriate scorers in cross-validation**  Use as_concordance_index_ipcw_scorer()\n\n## Reference Files\n\nThis skill includes detailed reference files for specific topics:\n\n- **`references/cox-models.md`**: Complete guide to Cox proportional hazards models, penalized Cox (CoxNet), IPCRidge, regularization strategies, and interpretation\n- **`references/ensemble-models.md`**: Random Survival Forests, Gradient Boosting, hyperparameter tuning, feature importance, and model selection\n- **`references/evaluation-metrics.md`**: Concordance index (Harrell's vs Uno's), time-dependent AUC, Brier score, comprehensive evaluation pipelines\n- **`references/data-handling.md`**: Data loading, preprocessing workflows, handling missing data, feature encoding, validation checks\n- **`references/svm-models.md`**: Survival Support Vector Machines, kernel selection, clinical kernel transform, hyperparameter tuning\n- **`references/competing-risks.md`**: Competing risks analysis, cumulative incidence functions, cause-specific hazard models\n\nLoad these reference files when detailed information is needed for specific tasks.\n\n## Additional Resources\n\n- **Official Documentation**: https://scikit-survival.readthedocs.io/\n- **GitHub Repository**: https://github.com/sebp/scikit-survival\n- **Built-in Datasets**: Use `sksurv.datasets` for practice datasets (GBSG2, WHAS500, veterans lung cancer, etc.)\n- **API Reference**: Complete list of classes and functions at https://scikit-survival.readthedocs.io/en/stable/api/index.html\n\n## Quick Reference: Key Imports\n\n```python\n# Models\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis, IPCRidge\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\nfrom sksurv.svm import FastSurvivalSVM, FastKernelSurvivalSVM\nfrom sksurv.tree import SurvivalTree\n\n# Evaluation metrics\nfrom sksurv.metrics import (\n    concordance_index_censored,\n    concordance_index_ipcw,\n    cumulative_dynamic_auc,\n    brier_score,\n    integrated_brier_score,\n    as_concordance_index_ipcw_scorer,\n    as_integrated_brier_score_scorer\n)\n\n# Non-parametric estimation\nfrom sksurv.nonparametric import (\n    kaplan_meier_estimator,\n    nelson_aalen_estimator,\n    cumulative_incidence_competing_risks\n)\n\n# Data handling\nfrom sksurv.util import Surv\nfrom sksurv.preprocessing import OneHotEncoder, encode_categorical\nfrom sksurv.datasets import load_gbsg2, load_breast_cancer, load_veterans_lung_cancer\n\n# Kernels\nfrom sksurv.kernels import ClinicalKernelTransform\n```"
              }
            ]
          },
          {
            "name": "senior-software-developer",
            "description": "Expert-level development assistance for architecture, code review, system design, and technical leadership",
            "source": "./plugins/senior-software-developer",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Claude Code Team"
            },
            "install_commands": [
              "/plugin marketplace add kjgarza/marketplace-claude",
              "/plugin install senior-software-developer@marketplace-claude"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-11T12:23:28Z",
              "created_at": "2026-01-01T15:35:08Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/architecture-review",
                "description": null,
                "path": "plugins/senior-software-developer/commands/architecture-review.md",
                "frontmatter": null,
                "content": "# Architecture Review\n\n## Overview\nConduct a comprehensive architecture review of your codebase, evaluating system design, scalability, maintainability, and adherence to best practices. This command provides senior-level analysis with actionable recommendations.\n\n## Steps\n\n### 1. Understand Context\n- Identify the type of system (web app, microservices, monolith, distributed system, etc.)\n- Determine the technology stack and frameworks in use\n- Understand the business domain and requirements\n- Review any existing architecture documentation\n\n### 2. Analyze System Structure\n- **Module Organization**: Evaluate how code is organized into modules, packages, or services\n- **Separation of Concerns**: Check if responsibilities are properly separated\n- **Dependencies**: Map out dependencies between components\n- **Coupling**: Identify tight coupling that may hinder maintainability\n- **Cohesion**: Assess if related functionality is grouped appropriately\n\n### 3. Evaluate Architectural Patterns\n- Identify design patterns in use (MVC, MVVM, Repository, Factory, etc.)\n- Assess if patterns are applied correctly and consistently\n- Look for anti-patterns (God objects, circular dependencies, etc.)\n- Evaluate if the chosen patterns fit the problem domain\n\n### 4. Review Scalability & Performance\n- **Horizontal Scalability**: Can the system scale by adding more instances?\n- **Vertical Scalability**: Are there bottlenecks that limit scaling up?\n- **Caching Strategy**: Is caching used appropriately?\n- **Database Design**: Review query patterns, indexes, and data models\n- **Asynchronous Processing**: Are long-running tasks handled asynchronously?\n- **Rate Limiting**: Are there protections against abuse?\n\n### 5. Assess Reliability & Resilience\n- **Error Handling**: Is error handling comprehensive and graceful?\n- **Fault Tolerance**: How does the system handle failures?\n- **Circuit Breakers**: Are there protections against cascading failures?\n- **Retry Logic**: Are transient failures handled appropriately?\n- **Monitoring**: Is the system observable and debuggable?\n- **Health Checks**: Are health endpoints implemented?\n\n### 6. Security Review\n- **Authentication**: How are users authenticated?\n- **Authorization**: Is access control properly implemented?\n- **Input Validation**: Are inputs sanitized and validated?\n- **Secrets Management**: How are sensitive credentials stored?\n- **API Security**: Are APIs protected against common attacks (SQL injection, XSS, CSRF)?\n- **Data Encryption**: Is sensitive data encrypted at rest and in transit?\n\n### 7. Code Quality Assessment\n- **Testing**: Review test coverage and quality\n- **Documentation**: Assess if code and APIs are well-documented\n- **Code Complexity**: Identify overly complex methods or classes\n- **Code Duplication**: Look for DRY violations\n- **Naming Conventions**: Check for clear, consistent naming\n- **Type Safety**: Evaluate use of types and interfaces\n\n### 8. DevOps & Deployment\n- **CI/CD Pipeline**: Review automated testing and deployment\n- **Infrastructure as Code**: Is infrastructure versioned and reproducible?\n- **Containerization**: Is Docker or similar technology used appropriately?\n- **Configuration Management**: How are environment-specific configs handled?\n- **Rollback Strategy**: Can deployments be easily rolled back?\n\n### 9. Generate Recommendations\nProvide prioritized recommendations in three categories:\n\n**Critical Issues** (fix immediately):\n- Security vulnerabilities\n- Data loss risks\n- Scalability blockers\n\n**Important Improvements** (plan for next sprint):\n- Performance optimizations\n- Code quality issues\n- Missing tests\n\n**Nice-to-Haves** (technical debt backlog):\n- Refactoring opportunities\n- Documentation gaps\n- Tool upgrades\n\n### 10. Create Action Plan\n- Estimate effort for each recommendation (small/medium/large)\n- Identify quick wins vs. long-term improvements\n- Suggest phased approach for major refactoring\n- Highlight dependencies between improvements\n\n## Checklist\n\n- [ ] System context and requirements understood\n- [ ] Module structure and organization reviewed\n- [ ] Design patterns and anti-patterns identified\n- [ ] Scalability and performance assessed\n- [ ] Reliability and error handling evaluated\n- [ ] Security vulnerabilities checked\n- [ ] Code quality metrics reviewed\n- [ ] Testing coverage analyzed\n- [ ] DevOps practices evaluated\n- [ ] Prioritized recommendations provided\n- [ ] Action plan with estimates created\n\n## Examples\n\n### Example 1: Review a Microservices Architecture\n```\n/architecture-review\n\nPlease review the architecture of our e-commerce platform:\n- 5 microservices (auth, products, orders, payments, notifications)\n- Node.js with Express\n- PostgreSQL databases per service\n- RabbitMQ for inter-service communication\n- Deployed on Kubernetes\n```\n\n### Example 2: Review a Monolithic Application\n```\n/architecture-review\n\nAnalyze our legacy Django monolith:\n- 50k lines of Python code\n- Single PostgreSQL database\n- Redis for caching\n- Considering breaking into microservices\n```\n\n## Configuration\n\n### Scope Options\nYou can focus the review on specific areas:\n- `--focus=security`: Deep dive on security issues\n- `--focus=performance`: Emphasize scalability and performance\n- `--focus=maintainability`: Focus on code quality and structure\n- `--focus=cost`: Analyze infrastructure cost optimization\n\n### Output Format\n- `--format=summary`: High-level executive summary\n- `--format=detailed`: Comprehensive analysis (default)\n- `--format=checklist`: Actionable task list\n\n## Best Practices\n\n1. **Be Holistic**: Consider technical, business, and team constraints\n2. **Prioritize**: Not all issues are equally important\n3. **Be Pragmatic**: Perfect is the enemy of good\n4. **Consider Context**: What works for one system may not work for another\n5. **Think Long-term**: Balance immediate needs with future maintainability\n\n## Related Commands\n\n- `/code-review`: Review specific code changes or files\n- `/system-design`: Design a new system or feature\n- `/refactor-strategy`: Create a plan for large-scale refactoring\n- `/performance-audit`: Deep dive on performance optimization\n\n"
              },
              {
                "name": "/code-review",
                "description": null,
                "path": "plugins/senior-software-developer/commands/code-review.md",
                "frontmatter": null,
                "content": "# Code Review\n\n\n## Overview\nPerform a thorough, senior-level code review focusing on design, maintainability, performance, security, and best practices. Provides constructive feedback with specific suggestions for improvement.\n\n## Steps\n\n### 1. Understand the Change\n- What problem is this code solving?\n- What is the scope and impact of the change?\n- Are there any related files or systems affected?\n- Review any associated requirements, tickets, or PRs\n\n### 2. Review Design & Architecture\n- **Fits into Architecture**: Does this change align with the overall system design?\n- **Design Patterns**: Are appropriate patterns used?\n- **Abstraction Level**: Is the abstraction appropriate (not too abstract, not too concrete)?\n- **SOLID Principles**: \n  - Single Responsibility: Does each class/function have one reason to change?\n  - Open/Closed: Open for extension, closed for modification?\n  - Liskov Substitution: Are inheritance relationships correct?\n  - Interface Segregation: Are interfaces focused and minimal?\n  - Dependency Inversion: Depend on abstractions, not concretions?\n\n### 3. Evaluate Code Quality\n- **Readability**: Is the code easy to understand?\n- **Naming**: Are variables, functions, and classes well-named?\n- **Complexity**: Are functions/methods too complex? (Check cyclomatic complexity)\n- **Length**: Are files or functions too long?\n- **DRY**: Is there code duplication?\n- **Magic Numbers**: Are constants properly named and defined?\n- **Comments**: Are comments helpful (explain why, not what)?\n\n### 4. Check Logic & Correctness\n- **Edge Cases**: Are edge cases handled?\n- **Error Handling**: Are errors caught and handled appropriately?\n- **Null Safety**: Are null/undefined values handled?\n- **Type Safety**: Are types used correctly?\n- **Logic Errors**: Are there any logical mistakes?\n- **Off-by-one Errors**: Check loop boundaries and array indices\n\n### 5. Performance Review\n- **Algorithmic Complexity**: What's the time/space complexity?\n- **Unnecessary Operations**: Are there redundant calculations?\n- **Database Queries**: N+1 queries? Missing indexes?\n- **Caching**: Should results be cached?\n- **Memory Leaks**: Are resources properly cleaned up?\n- **Lazy Loading**: Should data be loaded on-demand?\n\n### 6. Security Assessment\n- **Input Validation**: Are all inputs validated and sanitized?\n- **SQL Injection**: Are queries parameterized?\n- **XSS**: Is output escaped appropriately?\n- **Authentication**: Are auth checks present where needed?\n- **Authorization**: Are permission checks correct?\n- **Sensitive Data**: Is PII/credentials handled securely?\n- **Rate Limiting**: Are there protections against abuse?\n\n### 7. Testing Review\n- **Test Coverage**: Are new features tested?\n- **Test Quality**: Do tests verify correct behavior?\n- **Edge Cases**: Are edge cases tested?\n- **Mocking**: Are external dependencies properly mocked?\n- **Test Naming**: Are test names descriptive?\n- **Assertions**: Are assertions specific and meaningful?\n\n### 8. API & Interface Design\n- **API Design**: Is the API intuitive and consistent?\n- **Backward Compatibility**: Does this break existing contracts?\n- **Versioning**: Is versioning handled if needed?\n- **Documentation**: Is the API documented?\n- **Error Messages**: Are error messages helpful?\n\n### 9. Dependencies & Imports\n- **Necessary Dependencies**: Are all dependencies needed?\n- **Version Constraints**: Are versions pinned appropriately?\n- **Security Vulnerabilities**: Any known CVEs?\n- **License Compatibility**: Are licenses compatible?\n- **Import Organization**: Are imports organized logically?\n\n### 10. Documentation\n- **Code Documentation**: Are complex parts explained?\n- **API Documentation**: Is public API documented?\n- **README Updates**: Does README need updating?\n- **Changelog**: Should CHANGELOG be updated?\n- **Migration Guides**: Are breaking changes documented?\n\n### 11. Provide Feedback\nStructure feedback as:\n\n**Critical Issues** (must fix before merge):\n- Security vulnerabilities\n- Bugs or logic errors\n- Breaking changes without migration path\n\n**Important Suggestions** (should address):\n- Design improvements\n- Performance issues\n- Missing tests\n- Code quality concerns\n\n**Nitpicks** (optional improvements):\n- Minor style issues\n- Better naming suggestions\n- Refactoring opportunities\n\n**Praise** (positive feedback):\n- Well-designed solutions\n- Good test coverage\n- Clear documentation\n\n## Checklist\n\n- [ ] Purpose and scope understood\n- [ ] Architecture alignment verified\n- [ ] Code quality assessed\n- [ ] Logic and correctness verified\n- [ ] Performance implications considered\n- [ ] Security vulnerabilities checked\n- [ ] Test coverage reviewed\n- [ ] API design evaluated\n- [ ] Dependencies reviewed\n- [ ] Documentation checked\n- [ ] Constructive feedback provided\n\n## Examples\n\n### Example 1: Review a Pull Request\n```\n/code-review\n\nPlease review this PR that adds user authentication:\n- Implements JWT-based auth\n- Adds login/logout endpoints\n- Includes middleware for protected routes\n- 85% test coverage\n```\n\n### Example 2: Review a Specific File\n```\n/code-review src/services/payment-processor.ts\n\nFocus on security and error handling for payment processing logic.\n```\n\n### Example 3: Review Recent Changes\n```\n/code-review --since=HEAD~3\n\nReview the last 3 commits in the feature branch.\n```\n\n## Configuration\n\n### Focus Areas\n- `--focus=security`: Emphasize security review\n- `--focus=performance`: Deep dive on performance\n- `--focus=testing`: Focus on test coverage and quality\n- `--focus=design`: Emphasize architecture and design patterns\n\n### Strictness Level\n- `--strict`: Flag all issues including nitpicks\n- `--normal`: Balanced review (default)\n- `--lenient`: Focus only on critical issues\n\n## Best Practices\n\n1. **Be Constructive**: Suggest improvements, don't just criticize\n2. **Explain Why**: Help the author learn, not just fix issues\n3. **Prioritize**: Distinguish between must-fix and nice-to-have\n4. **Acknowledge Good Work**: Give positive feedback too\n5. **Be Specific**: Point to exact lines and provide examples\n6. **Consider Context**: Understand constraints and tradeoffs\n7. **Ask Questions**: If something is unclear, ask for clarification\n\n## Common Issues to Watch For\n\n### Logic Issues\n- Off-by-one errors in loops\n- Incorrect boolean logic\n- Race conditions\n- Unhandled edge cases\n\n### Security Issues\n- Unvalidated user input\n- Hardcoded credentials\n- Missing authentication checks\n- Insecure random number generation\n\n### Performance Issues\n- N+1 query problems\n- Unnecessary database calls\n- Memory leaks\n- Blocking I/O operations\n\n### Maintainability Issues\n- God classes/functions\n- Deep nesting\n- Complex conditionals\n- Tight coupling\n\n## Related Commands\n\n- `/architecture-review`: Review overall system architecture\n- `/refactor-strategy`: Plan refactoring for legacy code\n- `/security-audit`: Deep security analysis\n- `/performance-audit`: Detailed performance review\n\n"
              },
              {
                "name": "/refactor-strategy",
                "description": null,
                "path": "plugins/senior-software-developer/commands/refactor-strategy.md",
                "frontmatter": null,
                "content": "# Refactor Strategy\n\n## Overview\nCreate a comprehensive, risk-mitigated strategy for refactoring legacy code or large-scale architectural changes. This command helps plan refactoring that can be done incrementally without breaking production systems.\n\n## Steps\n\n### 1. Assess Current State\n\n#### Understand the Problem\n- What are the pain points with the current code?\n- Why does it need refactoring?\n- What specific issues are you trying to solve?\n- What is the business impact?\n\n#### Analyze the Codebase\n- Code complexity metrics (cyclomatic complexity, nesting depth)\n- Code duplication\n- Test coverage\n- Dependencies and coupling\n- Technical debt accumulation\n- Performance bottlenecks\n\n#### Identify Constraints\n- Must maintain backward compatibility?\n- Zero downtime requirement?\n- Team size and expertise\n- Time and budget constraints\n- External dependencies\n\n### 2. Define Target State\n\n#### Desired Outcome\n- What does \"good\" look like?\n- What architectural pattern are you moving toward?\n- What will be better after refactoring?\n- How will you measure success?\n\n#### Success Metrics\n- Code quality metrics improvement\n- Performance improvements\n- Reduction in bugs/incidents\n- Developer velocity increase\n- Test coverage increase\n\n### 3. Risk Assessment\n\nIdentify risks:\n- **Breaking Changes**: What could break?\n- **Data Migration**: Does data need to be migrated?\n- **Downtime**: Will there be service interruption?\n- **Performance**: Could performance degrade?\n- **Team Knowledge**: Does the team understand the changes?\n- **Dependencies**: What external systems are affected?\n\nFor each risk:\n- **Likelihood**: High/Medium/Low\n- **Impact**: High/Medium/Low\n- **Mitigation**: How to reduce risk?\n\n### 4. Choose Refactoring Strategy\n\n#### Strangler Fig Pattern (Recommended for Large Changes)\n- Build new system alongside old\n- Gradually route traffic to new system\n- Decommission old system once fully replaced\n- **Pros**: Low risk, incremental, can rollback\n- **Cons**: Temporary complexity, longer timeline\n\n#### Big Bang Refactor\n- Rewrite everything at once\n- Deploy all changes together\n- **Pros**: Clean break, simpler during refactor\n- **Cons**: High risk, long development, difficult rollback\n\n#### Branch by Abstraction\n- Create abstraction layer\n- Implement new version behind abstraction\n- Switch implementations\n- Remove abstraction once complete\n- **Pros**: No branching issues, incremental\n- **Cons**: Requires careful abstraction design\n\n#### Feature Flags\n- Deploy both old and new code\n- Use flags to control which version runs\n- Gradually roll out new version\n- **Pros**: Easy rollback, gradual rollout\n- **Cons**: Code complexity during transition\n\n### 5. Break Down Into Phases\n\nCreate incremental phases:\n\n**Phase 1: Foundation**\n- Set up new structure\n- Add comprehensive tests\n- Create abstractions\n- No user-facing changes\n\n**Phase 2: Parallel Implementation**\n- Implement new version alongside old\n- Verify correctness\n- Shadow traffic for testing\n\n**Phase 3: Gradual Migration**\n- Route small percentage of traffic\n- Monitor metrics\n- Increase percentage if stable\n\n**Phase 4: Complete Migration**\n- Route all traffic to new version\n- Keep old version for rollback\n- Monitor for issues\n\n**Phase 5: Cleanup**\n- Remove old code\n- Remove feature flags\n- Update documentation\n\nEach phase should:\n- Be independently deployable\n- Take 1-4 weeks\n- Have clear success criteria\n- Be reversible\n\n### 6. Testing Strategy\n\n#### Before Refactoring\n- Add characterization tests (test current behavior)\n- Increase test coverage of areas being changed\n- Document current behavior\n\n#### During Refactoring\n- Maintain or increase test coverage\n- Add tests for new code\n- Ensure tests pass after each change\n- Use TDD for new implementations\n\n#### After Refactoring\n- Run full regression test suite\n- Performance testing\n- Load testing\n- Security testing\n- User acceptance testing\n\n#### Testing Techniques\n- **Parallel Run**: Run old and new code, compare outputs\n- **Shadow Traffic**: Send copy of real traffic to new code\n- **Canary Deployment**: Deploy to small percentage of users\n- **Blue-Green Deployment**: Switch between old and new versions\n- **A/B Testing**: Compare metrics between versions\n\n### 7. Data Migration Plan (if applicable)\n\n#### Assess Data Changes\n- Schema changes needed?\n- Data transformations required?\n- Historical data migration?\n\n#### Migration Strategy\n- **Dual Writes**: Write to both old and new datastores\n- **Backfill**: Migrate historical data in background\n- **Validation**: Verify data consistency\n- **Rollback**: Plan for reverting data changes\n\n#### Steps\n1. Add dual writes (write to old and new)\n2. Backfill historical data\n3. Validate data consistency\n4. Switch reads to new datastore\n5. Stop writes to old datastore\n6. Archive old data\n\n### 8. Deployment Strategy\n\n#### Gradual Rollout\n- Deploy to dev environment\n- Deploy to staging\n- Deploy to production (1% of traffic)\n- Monitor metrics\n- Increase to 10%, 50%, 100%\n\n#### Monitoring During Rollout\n- Error rates\n- Response times\n- Resource utilization\n- Business metrics\n- User feedback\n\n#### Rollback Plan\n- Automated rollback triggers\n- Manual rollback procedure\n- How quickly can you rollback?\n- Data rollback considerations\n\n### 9. Communication Plan\n\n#### Team Communication\n- Share refactoring plan with team\n- Conduct design review\n- Pair programming for complex parts\n- Daily standups during critical phases\n- Retrospectives after each phase\n\n#### Stakeholder Communication\n- Explain business value\n- Communicate timeline\n- Set expectations for disruptions\n- Report progress regularly\n- Celebrate milestones\n\n#### Documentation\n- Update architecture docs\n- Create runbooks\n- Document new patterns\n- Update onboarding materials\n- Record decisions and tradeoffs\n\n### 10. Implementation Guidelines\n\n#### Code Quality Standards\n- Follow existing code style\n- Use consistent naming\n- Add meaningful comments\n- Keep functions small\n- Reduce complexity\n\n#### Incremental Changes\n- Small, frequent commits\n- Each commit should compile and pass tests\n- Frequent deployments\n- Quick feedback loops\n\n#### Continuous Monitoring\n- Set up dashboards\n- Create alerts for anomalies\n- Track key metrics\n- Log important events\n\n### 11. Create Refactoring Backlog\n\nBreak work into stories:\n- Each story should be completable in 1-3 days\n- Stories should be independently valuable\n- Prioritize by risk and value\n- Include testing and documentation tasks\n\nExample stories:\n1. Add characterization tests for UserService\n2. Extract UserRepository interface\n3. Implement new UserRepository with PostgreSQL\n4. Add feature flag for UserRepository selection\n5. Deploy with flag disabled\n6. Enable flag for 1% of traffic\n7. Monitor and increase to 100%\n8. Remove old implementation\n\n### 12. Post-Refactoring Review\n\nAfter completion:\n- Measure against success metrics\n- Conduct retrospective\n- Document lessons learned\n- Identify remaining technical debt\n- Plan next refactoring iteration\n\n## Checklist\n\n- [ ] Current state assessed and documented\n- [ ] Target state clearly defined\n- [ ] Risks identified and mitigation planned\n- [ ] Refactoring strategy selected\n- [ ] Work broken into incremental phases\n- [ ] Testing strategy defined\n- [ ] Data migration plan created (if needed)\n- [ ] Deployment strategy planned\n- [ ] Communication plan established\n- [ ] Implementation guidelines set\n- [ ] Refactoring backlog created\n- [ ] Monitoring and rollback procedures defined\n\n## Examples\n\n### Example 1: Refactor Monolith to Microservices\n```\n/refactor-strategy\n\nPlan migration of our monolithic e-commerce app to microservices:\n- Current: Single Django app, 100k LOC, PostgreSQL\n- Target: 5 microservices (users, products, orders, payments, notifications)\n- Constraints: Zero downtime, maintain existing APIs\n- Timeline: 6 months\n```\n\n### Example 2: Replace Legacy Authentication System\n```\n/refactor-strategy\n\nMigrate from custom auth to Auth0:\n- Current: Custom JWT implementation, user database\n- Target: Auth0 with OAuth 2.0\n- 50k active users\n- Must migrate existing user accounts\n- Maintain current user experience\n```\n\n### Example 3: Database Migration\n```\n/refactor-strategy\n\nMigrate from MySQL to PostgreSQL:\n- 500GB database\n- 24/7 uptime requirement\n- Complex queries using MySQL-specific features\n- Need to test query performance\n```\n\n## Best Practices\n\n1. **Test First**: Add tests before refactoring\n2. **Incremental Changes**: Small, frequent deployments\n3. **Measure Everything**: Track metrics before and after\n4. **Communicate Often**: Keep team and stakeholders informed\n5. **Plan for Rollback**: Always have an escape hatch\n6. **Pair Program**: Complex refactoring benefits from collaboration\n7. **Document Decisions**: Explain why, not just what\n8. **Celebrate Progress**: Acknowledge milestones\n9. **Learn and Iterate**: Adjust strategy based on feedback\n10. **Don't Gold-Plate**: Focus on solving the problem, not perfection\n\n## Anti-Patterns to Avoid\n\n- **Big Bang Refactor**: Rewriting everything at once\n- **Refactor Without Tests**: Changing code without safety net\n- **Scope Creep**: Adding features during refactoring\n- **Perfectionism**: Spending too long on diminishing returns\n- **No Monitoring**: Not tracking impact of changes\n- **Ignoring Feedback**: Not adapting based on results\n- **Solo Refactoring**: Not involving team in major changes\n\n## Related Commands\n\n- `/architecture-review`: Assess current architecture\n- `/code-review`: Review refactored code\n- `/system-design`: Design target architecture\n- `/technical-debt-audit`: Identify areas needing refactoring\n\n"
              },
              {
                "name": "/system-design",
                "description": null,
                "path": "plugins/senior-software-developer/commands/system-design.md",
                "frontmatter": null,
                "content": "# System Design\n\n## Overview\nDesign a robust, scalable system from scratch or plan major architectural changes. This command guides you through senior-level system design considerations including requirements, architecture, scalability, reliability, and tradeoffs.\n\n## Steps\n\n### 1. Gather Requirements\n\n#### Functional Requirements\n- What features does the system need?\n- What are the core use cases?\n- What operations will users perform?\n- What data needs to be stored and retrieved?\n- What are the inputs and outputs?\n\n#### Non-Functional Requirements\n- **Scale**: How many users? Requests per second? Data volume?\n- **Performance**: Latency requirements? Throughput targets?\n- **Availability**: Uptime requirements (99.9%, 99.99%, etc.)?\n- **Consistency**: Strong consistency or eventual consistency?\n- **Durability**: Data loss tolerance?\n- **Security**: Authentication, authorization, encryption needs?\n- **Compliance**: GDPR, HIPAA, SOC2, etc.?\n\n#### Constraints\n- Budget limitations\n- Timeline constraints\n- Team expertise\n- Technology restrictions\n- Regulatory requirements\n\n### 2. Capacity Estimation\n\nCalculate:\n- **Traffic Estimates**: QPS (queries per second), peak vs. average\n- **Storage Estimates**: Data size per record  number of records  growth rate\n- **Bandwidth Estimates**: Request size  QPS\n- **Memory Estimates**: Cache size, in-memory data structures\n- **Compute Requirements**: CPU/RAM per instance  number of instances\n\n### 3. Define APIs\n\nDesign clean, RESTful APIs:\n- **Endpoints**: What operations are exposed?\n- **Request/Response**: What data is sent and received?\n- **Authentication**: How are requests authenticated?\n- **Rate Limiting**: What are the limits?\n- **Versioning**: How will APIs evolve?\n- **Error Handling**: What error codes and messages?\n\nExample:\n```\nPOST /api/v1/users\nGET /api/v1/users/{id}\nPUT /api/v1/users/{id}\nDELETE /api/v1/users/{id}\nGET /api/v1/users?page=1&limit=20\n```\n\n### 4. Data Model Design\n\n#### Database Selection\n- **Relational (SQL)**: PostgreSQL, MySQL\n  - Use for: Structured data, ACID transactions, complex queries\n- **NoSQL Document**: MongoDB, Couchbase\n  - Use for: Flexible schema, hierarchical data\n- **NoSQL Key-Value**: Redis, DynamoDB\n  - Use for: Simple lookups, caching, sessions\n- **NoSQL Wide-Column**: Cassandra, HBase\n  - Use for: High write throughput, time-series data\n- **Graph**: Neo4j, Amazon Neptune\n  - Use for: Relationship-heavy data (social graphs, recommendations)\n\n#### Schema Design\n- Define entities and relationships\n- Identify primary keys and foreign keys\n- Plan for indexes\n- Consider partitioning/sharding strategy\n- Design for query patterns\n\n### 5. High-Level Architecture\n\n#### Architecture Patterns\n- **Monolithic**: Single deployable unit\n  - Pros: Simple, easy to develop/deploy initially\n  - Cons: Hard to scale, couples everything together\n- **Microservices**: Independent services\n  - Pros: Scalable, independent deployment, technology diversity\n  - Cons: Complex, distributed system challenges\n- **Serverless**: Event-driven functions\n  - Pros: Auto-scaling, pay-per-use, no server management\n  - Cons: Cold starts, vendor lock-in, complex debugging\n- **Event-Driven**: Async message passing\n  - Pros: Decoupled, resilient, scalable\n  - Cons: Complexity, eventual consistency\n\n#### Core Components\n- **Load Balancer**: Distribute traffic (nginx, HAProxy, ALB)\n- **API Gateway**: Single entry point, rate limiting, auth\n- **Application Servers**: Business logic\n- **Cache Layer**: Redis, Memcached\n- **Message Queue**: RabbitMQ, Kafka, SQS\n- **Database**: Primary and replica(s)\n- **Object Storage**: S3, GCS for files/media\n- **CDN**: CloudFront, Cloudflare for static assets\n- **Search**: Elasticsearch, Algolia\n- **Monitoring**: Prometheus, Grafana, DataDog\n\n### 6. Detailed Component Design\n\nFor each major component:\n- **Responsibility**: What does it do?\n- **APIs**: How do other components interact with it?\n- **Data**: What data does it store/process?\n- **Scale**: How does it scale?\n- **Failure Modes**: What happens when it fails?\n\n### 7. Scalability Strategy\n\n#### Horizontal Scaling\n- Stateless services (can add more instances)\n- Load balancing across instances\n- Database sharding\n- Read replicas for databases\n\n#### Vertical Scaling\n- Increase CPU/RAM of instances\n- More limited than horizontal scaling\n\n#### Caching Strategy\n- **Cache Aside**: App reads from cache, falls back to DB\n- **Write-Through**: Writes go to cache and DB simultaneously\n- **Write-Behind**: Writes go to cache, async to DB\n- What to cache: Hot data, expensive queries\n- Cache invalidation: TTL, event-based\n\n#### Database Scaling\n- **Replication**: Master-slave for read scaling\n- **Sharding**: Partition data across databases\n- **Denormalization**: Trade storage for read performance\n- **CQRS**: Separate read and write models\n\n### 8. Reliability & Resilience\n\n#### Availability\n- **Redundancy**: No single point of failure\n- **Replication**: Multiple copies of data\n- **Health Checks**: Detect and remove unhealthy instances\n- **Auto-scaling**: Handle traffic spikes\n\n#### Failure Handling\n- **Retry Logic**: Exponential backoff for transient failures\n- **Circuit Breaker**: Stop calling failing services\n- **Fallback**: Degrade gracefully\n- **Timeouts**: Don't wait forever\n- **Idempotency**: Safe to retry operations\n\n#### Disaster Recovery\n- **Backups**: Regular automated backups\n- **Multi-region**: Deploy across regions\n- **RPO/RTO**: Recovery Point/Time Objectives\n- **Runbooks**: Incident response procedures\n\n### 9. Security Design\n\n- **Authentication**: OAuth 2.0, JWT, SSO\n- **Authorization**: RBAC, ABAC, policies\n- **Encryption**: TLS in transit, encryption at rest\n- **Secrets Management**: Vault, AWS Secrets Manager\n- **Network Security**: VPC, security groups, WAF\n- **Rate Limiting**: Prevent abuse\n- **Audit Logging**: Track security events\n- **Penetration Testing**: Regular security assessments\n\n### 10. Monitoring & Observability\n\n- **Metrics**: Response times, error rates, throughput\n- **Logging**: Centralized logging (ELK, Splunk)\n- **Tracing**: Distributed tracing (Jaeger, Zipkin)\n- **Alerting**: PagerDuty, OpsGenie\n- **Dashboards**: Real-time system health visibility\n- **SLOs/SLIs**: Define and track service levels\n\n### 11. Tradeoffs & Alternatives\n\nFor each major decision, document:\n- Why this approach?\n- What alternatives were considered?\n- What are the tradeoffs?\n- What could change this decision?\n\nExample: \"Chose MongoDB over PostgreSQL because...\"\n\n### 12. Create Design Artifacts\n\nGenerate:\n- **Architecture Diagram**: High-level component view\n- **Sequence Diagrams**: Key user flows\n- **Data Flow Diagram**: How data moves through system\n- **Deployment Diagram**: Infrastructure layout\n- **API Specification**: OpenAPI/Swagger docs\n\n## Checklist\n\n- [ ] Functional requirements defined\n- [ ] Non-functional requirements specified\n- [ ] Capacity estimates calculated\n- [ ] APIs designed\n- [ ] Data model created\n- [ ] Architecture pattern selected\n- [ ] Core components identified\n- [ ] Scalability strategy defined\n- [ ] Reliability measures planned\n- [ ] Security designed\n- [ ] Monitoring strategy defined\n- [ ] Tradeoffs documented\n- [ ] Design artifacts created\n\n## Examples\n\n### Example 1: Design a URL Shortener\n```\n/system-design\n\nDesign a URL shortening service like bit.ly:\n- 100M URLs shortened per day\n- Read-heavy (100:1 read-to-write ratio)\n- Low latency (<100ms)\n- High availability (99.99%)\n- Custom short URLs supported\n```\n\n### Example 2: Design a Chat System\n```\n/system-design\n\nDesign a real-time chat application:\n- Support 1M concurrent users\n- 1-on-1 and group chats\n- Message history\n- Read receipts\n- File sharing\n- End-to-end encryption\n```\n\n### Example 3: Design a Video Streaming Platform\n```\n/system-design\n\nDesign a video streaming service:\n- 10M daily active users\n- Upload and stream videos\n- Different quality options (480p, 720p, 1080p, 4K)\n- Recommendations based on viewing history\n- Social features (likes, comments, shares)\n```\n\n## Best Practices\n\n1. **Start High-Level**: Begin with big picture, then drill down\n2. **Ask Clarifying Questions**: Don't assume, verify requirements\n3. **Calculate Numbers**: Estimate scale and capacity\n4. **Consider Tradeoffs**: No perfect solution, document choices\n5. **Think About Failure**: How does each component fail?\n6. **Be Pragmatic**: Balance perfection with deadlines and budget\n7. **Iterate**: Design is iterative, refine as you go\n8. **Document Decisions**: Explain the \"why\" behind choices\n\n## Common Patterns & Techniques\n\n### Load Distribution\n- Round-robin\n- Least connections\n- Consistent hashing\n- Geo-based routing\n\n### Data Consistency\n- Eventual consistency\n- Strong consistency\n- Causal consistency\n- Read-your-writes consistency\n\n### Communication\n- Synchronous (REST, gRPC)\n- Asynchronous (message queues)\n- Pub/Sub (event-driven)\n- WebSockets (real-time)\n\n### Caching\n- CDN (edge caching)\n- Application cache (Redis)\n- Database query cache\n- Object cache\n\n## Related Commands\n\n- `/architecture-review`: Review existing architecture\n- `/refactor-strategy`: Plan migration from old to new design\n- `/capacity-planning`: Deep dive on capacity estimation\n- `/database-design`: Focus on data modeling\n\n"
              },
              {
                "name": "/technical-debt-audit",
                "description": null,
                "path": "plugins/senior-software-developer/commands/technical-debt-audit.md",
                "frontmatter": null,
                "content": "# Technical Debt Audit\n\n## Overview\nConduct a comprehensive audit of technical debt across your codebase, prioritizing issues and creating a roadmap for addressing accumulated debt. This command helps identify, quantify, and prioritize technical debt for systematic reduction.\n\n## Steps\n\n### 1. Define Technical Debt Categories\n\n#### Code Quality Debt\n- Complex, hard-to-understand code\n- Code duplication (DRY violations)\n- Poor naming and inconsistent conventions\n- Lack of comments or outdated documentation\n- Long functions/classes (God objects)\n- Deep nesting and high cyclomatic complexity\n\n#### Architecture Debt\n- Violations of SOLID principles\n- Tight coupling between components\n- Missing abstractions\n- Circular dependencies\n- Inappropriate design patterns\n- Monolithic components that should be split\n\n#### Testing Debt\n- Low or no test coverage\n- Brittle tests (too tightly coupled to implementation)\n- Missing integration/E2E tests\n- Slow test suites\n- Flaky tests\n- Outdated test data\n\n#### Documentation Debt\n- Missing or outdated README\n- No API documentation\n- Undocumented configuration\n- No architecture diagrams\n- Missing runbooks\n- Stale inline comments\n\n#### Infrastructure Debt\n- Manual deployment processes\n- No infrastructure as code\n- Outdated dependencies\n- Security vulnerabilities (CVEs)\n- Inefficient resource usage\n- No monitoring/alerting\n\n#### Performance Debt\n- N+1 query problems\n- Missing database indexes\n- Inefficient algorithms\n- Memory leaks\n- No caching strategy\n- Unoptimized assets\n\n#### Security Debt\n- Known vulnerabilities\n- Missing authentication/authorization\n- Hardcoded secrets\n- Lack of input validation\n- No security audits\n- Outdated security practices\n\n### 2. Measure Technical Debt\n\n#### Automated Analysis Tools\n- **Code Quality**: SonarQube, CodeClimate, ESLint, Pylint\n- **Security**: Snyk, Dependabot, npm audit, OWASP ZAP\n- **Test Coverage**: Coverage.py, Istanbul, JaCoCo\n- **Complexity**: McCabe complexity, cognitive complexity\n- **Dependencies**: npm outdated, pip list --outdated\n\n#### Manual Assessment\n- Code review sessions\n- Team surveys on pain points\n- Developer velocity tracking\n- Bug/incident analysis\n- Time spent on maintenance vs features\n\n#### Key Metrics\n- **Code Churn**: Frequency of changes to files\n- **Bug Density**: Bugs per 1000 lines of code\n- **Test Coverage**: Percentage of code covered by tests\n- **Cyclomatic Complexity**: Code complexity score\n- **Dependency Age**: How outdated are dependencies?\n- **Build Time**: Time to build and test\n- **Deployment Frequency**: How often can you deploy?\n- **Mean Time to Recovery**: How quickly can you fix issues?\n\n### 3. Quantify Impact\n\nFor each debt item, assess:\n\n#### Severity\n- **Critical**: Blocks progress, frequent source of bugs\n- **High**: Significant impact on velocity or quality\n- **Medium**: Noticeable but manageable impact\n- **Low**: Minor inconvenience\n\n#### Frequency\n- How often does this cause problems?\n- How many developers are affected?\n- Is it getting worse over time?\n\n#### Cost\n- **Ongoing Cost**: Time wasted per week/month\n- **Opportunity Cost**: Features not built due to debt\n- **Risk Cost**: Potential for major incidents\n- **Team Morale Cost**: Developer frustration\n\nExample: \"Complex authentication code causes 5 hours/week of debugging and is involved in 40% of security bugs\"\n\n### 4. Identify Root Causes\n\nWhy did debt accumulate?\n- **Deadline Pressure**: Rushed to ship, skipped quality\n- **Lack of Knowledge**: Team didn't know better approach\n- **Legacy System**: Inherited from previous team\n- **Technology Change**: Framework/library became outdated\n- **Requirements Change**: Original design no longer fits\n- **Lack of Refactoring**: No time allocated for cleanup\n\nUnderstanding root causes helps prevent future accumulation.\n\n### 5. Create Debt Inventory\n\nBuild a comprehensive list with:\n- **Item**: Description of debt\n- **Category**: Code/Architecture/Testing/etc.\n- **Location**: File/module/component\n- **Severity**: Critical/High/Medium/Low\n- **Effort**: Hours/days to fix\n- **Impact**: What improves when fixed?\n- **Dependencies**: What must be fixed first?\n\nExample format:\n```\n| Item | Category | Location | Severity | Effort | Impact |\n|------|----------|----------|----------|--------|--------|\n| UserService is 2000 lines | Architecture | src/services/user.ts | High | 5 days | Easier to maintain, better testability |\n| No integration tests | Testing | tests/ | Critical | 10 days | Catch more bugs, confidence in deploys |\n| Hardcoded API keys | Security | config/* | Critical | 1 day | Security compliance, rotate secrets |\n```\n\n### 6. Prioritization Framework\n\nUse a scoring system to prioritize:\n\n**Priority Score = (Impact  Frequency  Severity) / Effort**\n\n#### Impact (1-10)\n- How much improvement when fixed?\n- Developer velocity increase\n- Bug reduction\n- Performance gain\n\n#### Frequency (1-10)\n- How often is this pain felt?\n- Number of developers affected\n- Frequency of related bugs\n\n#### Severity (1-10)\n- How bad is it when it manifests?\n- Can it cause outages?\n- Does it block work?\n\n#### Effort (story points or days)\n- How long to fix?\n- Complexity of the fix\n- Risk of breaking things\n\n#### Quick Wins\nHigh priority score with low effort:\n- Update outdated dependencies\n- Add missing indexes\n- Extract large functions\n- Add critical tests\n\n#### Long-term Projects\nHigh impact but high effort:\n- Major refactoring\n- Architecture changes\n- Test suite overhaul\n- Database migration\n\n### 7. Create Reduction Roadmap\n\n#### Phase 1: Critical & Quick Wins (Month 1)\n- Security vulnerabilities\n- High-impact, low-effort items\n- Blocking issues\n\n#### Phase 2: High-Impact Items (Months 2-3)\n- Architecture improvements\n- Test coverage increase\n- Performance optimizations\n\n#### Phase 3: Medium-Priority Items (Months 4-6)\n- Code quality improvements\n- Documentation updates\n- Infrastructure modernization\n\n#### Phase 4: Nice-to-Haves (Ongoing)\n- Minor refactoring\n- Style consistency\n- Legacy cleanup\n\n### 8. Establish Debt Prevention Practices\n\n#### Definition of Done\nAdd to your DoD:\n- [ ] Tests written and passing\n- [ ] Code reviewed\n- [ ] Documentation updated\n- [ ] No new lint warnings\n- [ ] Performance impact assessed\n\n#### Code Review Guidelines\n- Flag new technical debt\n- Suggest improvements\n- Don't accumulate debt for deadlines without explicit tradeoff discussion\n\n#### Regular Refactoring Time\n- Dedicate 20% of sprint to debt reduction\n- \"Tech debt Fridays\"\n- Include refactoring in estimates\n\n#### Monitoring and Metrics\n- Track debt metrics over time\n- Set targets (e.g., maintain >80% test coverage)\n- Review metrics in retrospectives\n\n#### Team Culture\n- Make it safe to identify and discuss debt\n- Celebrate debt reduction\n- Share learnings\n- Don't blame for past decisions\n\n### 9. Implement Boy Scout Rule\n\n\"Leave the code better than you found it\"\n\nWhen working on a feature:\n- If you touch a file, improve it slightly\n- Extract a function, add a test, improve naming\n- Small, incremental improvements\n- Debt reduction becomes part of daily work\n\n### 10. Track Progress\n\n#### Metrics Dashboard\n- Code coverage trend\n- Complexity trend\n- Vulnerability count\n- Build time\n- Deployment frequency\n- Debt backlog burn-down\n\n#### Regular Reviews\n- Monthly debt review meetings\n- Quarterly retrospectives\n- Annual architecture review\n\n#### Celebrate Wins\n- Share improvements with team\n- Quantify impact (e.g., \"tests now run 50% faster\")\n- Recognize contributors\n\n## Checklist\n\n- [ ] Technical debt categories defined\n- [ ] Automated analysis tools run\n- [ ] Manual assessment conducted\n- [ ] Key metrics collected\n- [ ] Impact quantified for each item\n- [ ] Root causes identified\n- [ ] Comprehensive debt inventory created\n- [ ] Prioritization framework applied\n- [ ] Reduction roadmap created\n- [ ] Prevention practices established\n- [ ] Boy Scout Rule adopted\n- [ ] Progress tracking set up\n\n## Examples\n\n### Example 1: Audit an E-commerce Platform\n```\n/technical-debt-audit\n\nAudit our e-commerce platform:\n- 5 years old, original team left\n- Test coverage ~40%\n- Deployment takes 2 hours\n- Frequent production issues\n- Team velocity declining\n```\n\n### Example 2: Audit After Rapid Growth\n```\n/technical-debt-audit\n\nWe grew from 2 to 15 engineers in 1 year. Codebase has:\n- Inconsistent patterns across teams\n- Duplicate implementations of similar features\n- No shared component library\n- Documentation is sparse\n```\n\n### Example 3: Inherited Legacy System\n```\n/technical-debt-audit\n\nInherited PHP monolith:\n- 150k lines of code\n- PHP 5.6 (EOL)\n- No tests\n- jQuery spaghetti\n- Need modernization plan\n```\n\n## Best Practices\n\n1. **Be Objective**: Use metrics, not opinions\n2. **Be Comprehensive**: Don't just focus on code quality\n3. **Involve the Team**: Get input from all developers\n4. **Quantify Impact**: Use data to prioritize\n5. **Start Small**: Pick quick wins to build momentum\n6. **Make It Ongoing**: Debt audit is not one-time\n7. **Communicate Up**: Help leadership understand cost of debt\n8. **Don't Blame**: Focus on improvement, not fault\n9. **Track Trends**: Is debt increasing or decreasing?\n10. **Balance**: Some debt is acceptable tradeoff\n\n## Common Pitfalls\n\n- **Analysis Paralysis**: Spending too long analyzing, not fixing\n- **Perfectionism**: Trying to fix everything at once\n- **No Follow-through**: Creating plan but not executing\n- **Ignoring Root Causes**: Fixing symptoms without addressing why debt accumulates\n- **No Time Allocated**: Expecting debt reduction to happen \"when there's time\"\n- **Tool Over-reliance**: Trusting automated tools without context\n- **Neglecting Documentation**: Only focusing on code\n\n## Output Format\n\nThe audit should produce:\n1. **Executive Summary**: High-level findings and recommendations\n2. **Debt Inventory**: Detailed list of all debt items\n3. **Prioritization Matrix**: Visual representation of priority scores\n4. **Roadmap**: Timeline for addressing debt\n5. **Metrics Dashboard**: Current state and targets\n6. **Prevention Plan**: How to avoid future debt accumulation\n\n## Related Commands\n\n- `/architecture-review`: Deep dive on architecture debt\n- `/code-review`: Review specific code for quality issues\n- `/refactor-strategy`: Plan how to address major debt items\n- `/security-audit`: Focus on security-related debt\n\n"
              }
            ],
            "skills": [
              {
                "name": "chrome-extension-builder",
                "description": "Scaffold and setup Chrome MV3 extensions using WXT framework with React, TypeScript, and shadcn-UI. Use when creating new browser extensions, setting up content scripts, background service workers, side panels, popups, or configuring native messaging. Supports Google Docs/Overleaf integrations, DOM extraction, and cross-context communication patterns.",
                "path": "plugins/senior-software-developer/skills/chrome-extension-builder/SKILL.md",
                "frontmatter": {
                  "name": "chrome-extension-builder",
                  "description": "Scaffold and setup Chrome MV3 extensions using WXT framework with React, TypeScript, and shadcn-UI. Use when creating new browser extensions, setting up content scripts, background service workers, side panels, popups, or configuring native messaging. Supports Google Docs/Overleaf integrations, DOM extraction, and cross-context communication patterns."
                },
                "content": "# Chrome Extension Builder\n\nScaffold production-ready Chrome MV3 extensions using WXT + React + shadcn-UI.\n\n## Quick Start\n\n```bash\npnpm dlx wxt@latest init <project-name> --template react\ncd <project-name>\npnpm install\npnpm dev\n```\n\n---\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nAsk user about extension type and features:\n\n| Component | Purpose | When to Include |\n|-----------|---------|-----------------|\n| **Background** | Service worker, messaging hub, native messaging | Always |\n| **Content Script** | DOM manipulation, page extraction | Interacting with web pages |\n| **Side Panel** | Persistent UI alongside pages | Complex UIs, suggestion panels |\n| **Popup** | Quick actions, settings access | Simple interactions |\n| **Options Page** | Extension configuration | User preferences |\n| **DevTools Panel** | Developer debugging tools | Development tooling |\n\n### Step 2: Create Project Structure\n\n```\n/extension\n entrypoints/\n    background.ts           # Service worker\n    popup/                   # Popup UI (optional)\n       index.html\n       App.tsx\n       style.css\n    sidepanel/              # Side panel UI (optional)\n       index.html\n       App.tsx\n       style.css\n    options/                # Options page (optional)\n        index.html\n        App.tsx\n content/                    # Content scripts\n    main.ts                 # Primary content script\n    [site-name].ts          # Site-specific scripts\n lib/                        # Shared utilities\n    storage.ts              # Storage wrapper\n    messaging.ts            # Message protocol\n    types.ts                # Shared types\n components/                 # React components (shadcn-ui)\n    ui/\n wxt.config.ts              # WXT configuration\n tailwind.config.js         # Tailwind CSS\n package.json\n tsconfig.json\n```\n\n### Step 3: Configure WXT\n\n**wxt.config.ts:**\n```typescript\nimport { defineConfig } from 'wxt';\n\nexport default defineConfig({\n  modules: ['@wxt-dev/module-react'],\n  manifest: {\n    name: 'Extension Name',\n    version: '0.1.0',\n    permissions: ['storage', 'activeTab', 'scripting'],\n    host_permissions: ['https://example.com/*'],\n  },\n});\n```\n\nSee [WXT Configuration Reference](./references/wxt-config.md) for complete options.\n\n### Step 4: Implement Components\n\n#### Background Service Worker\n\n```typescript\n// entrypoints/background.ts\nexport default defineBackground(() => {\n  console.log('Extension loaded');\n\n  // Message handler\n  browser.runtime.onMessage.addListener((message, sender, sendResponse) => {\n    if (message.type === 'GET_DATA') {\n      // Handle message\n      sendResponse({ success: true, data: {} });\n    }\n    return true; // Keep channel open for async response\n  });\n});\n```\n\n#### Content Script\n\n```typescript\n// content/main.ts\nexport default defineContentScript({\n  matches: ['https://example.com/*'],\n  main(ctx) {\n    console.log('Content script loaded on', window.location.href);\n\n    // Extract page data\n    const pageData = extractPageContent();\n\n    // Send to background\n    browser.runtime.sendMessage({ type: 'PAGE_DATA', data: pageData });\n  },\n});\n```\n\n#### Side Panel with React\n\n```tsx\n// entrypoints/sidepanel/App.tsx\nimport { useState, useEffect } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { Card } from '@/components/ui/card';\n\nexport default function App() {\n  const [data, setData] = useState<DataType[]>([]);\n  const [loading, setLoading] = useState(false);\n\n  useEffect(() => {\n    // Listen for messages from background\n    browser.runtime.onMessage.addListener((message) => {\n      if (message.type === 'NEW_DATA') {\n        setData(prev => [message.data, ...prev]);\n      }\n    });\n  }, []);\n\n  const handleAction = async () => {\n    setLoading(true);\n    const response = await browser.runtime.sendMessage({ type: 'RUN_ACTION' });\n    setLoading(false);\n  };\n\n  return (\n    <div className=\"p-4\">\n      <Button onClick={handleAction} disabled={loading}>\n        {loading ? 'Processing...' : 'Run Action'}\n      </Button>\n      {data.map((item) => (\n        <Card key={item.id} className=\"mt-2 p-3\">\n          {item.content}\n        </Card>\n      ))}\n    </div>\n  );\n}\n```\n\n---\n\n## Common Patterns\n\n### Storage Wrapper\n\n```typescript\n// lib/storage.ts\nimport { storage } from 'wxt/storage';\n\nexport interface DocState {\n  docId: string;\n  title: string;\n  lastRunAt: number;\n  items: Item[];\n  dismissedIds: string[];\n}\n\nconst docStateKey = (id: string) => `local:doc:${id}` as const;\n\nexport const docStorage = {\n  async get(docId: string): Promise<DocState | null> {\n    return storage.getItem<DocState>(docStateKey(docId));\n  },\n\n  async set(docId: string, state: DocState): Promise<void> {\n    await storage.setItem(docStateKey(docId), state);\n  },\n\n  watch(docId: string, callback: (state: DocState | null) => void) {\n    return storage.watch<DocState>(docStateKey(docId), callback);\n  },\n};\n```\n\n### Message Protocol\n\n```typescript\n// lib/messaging.ts\nexport const PROTOCOL_VERSION = '1.0.0';\n\nexport type MessageType =\n  | { type: 'DOC_OPEN'; doc: DocPayload }\n  | { type: 'DOC_CHUNK'; docId: string; chunk: string; index: number }\n  | { type: 'DOC_DONE'; docId: string }\n  | { type: 'SUGGESTIONS'; docId: string; items: Suggestion[] }\n  | { type: 'INSERT_FIX'; suggestionId: string }\n  | { type: 'ERROR'; code: string; message: string };\n\nexport async function sendMessage<T extends MessageType>(\n  message: T\n): Promise<MessageResponse<T>> {\n  return browser.runtime.sendMessage({ ...message, protocolVersion: PROTOCOL_VERSION });\n}\n```\n\n### Native Messaging (Optional)\n\n```typescript\n// lib/nativeAdapter.ts\nexport interface NativeAdapter {\n  connect(): Promise<void>;\n  send(message: unknown): void;\n  onMessage(callback: (msg: unknown) => void): void;\n  disconnect(): void;\n}\n\nexport function createNativeAdapter(appName: string): NativeAdapter {\n  let port: browser.Runtime.Port | null = null;\n\n  return {\n    connect() {\n      port = browser.runtime.connectNative(appName);\n      return Promise.resolve();\n    },\n    send(message) {\n      port?.postMessage(message);\n    },\n    onMessage(callback) {\n      port?.onMessage.addListener(callback);\n    },\n    disconnect() {\n      port?.disconnect();\n      port = null;\n    },\n  };\n}\n\n// Mock adapter for development\nexport function createMockAdapter(): NativeAdapter {\n  return {\n    async connect() {},\n    send(message) {\n      // Simulate response after delay\n      setTimeout(() => {\n        // Return mock data\n      }, 1000);\n    },\n    onMessage(callback) {},\n    disconnect() {},\n  };\n}\n```\n\n### Content Script Insertion\n\n```typescript\n// content/insert.ts\nexport function insertText(text: string): boolean {\n  const selection = window.getSelection();\n  if (!selection || selection.rangeCount === 0) return false;\n\n  const range = selection.getRangeAt(0);\n  range.deleteContents();\n  range.insertNode(document.createTextNode(text));\n\n  // Collapse selection to end\n  range.collapse(false);\n  selection.removeAllRanges();\n  selection.addRange(range);\n\n  return true;\n}\n\n// For contenteditable elements (Google Docs workaround)\nexport async function insertViaClipboard(text: string): Promise<boolean> {\n  try {\n    await navigator.clipboard.writeText(text);\n    document.execCommand('paste');\n    return true;\n  } catch {\n    return false;\n  }\n}\n```\n\n### Shadow DOM UI in Content Scripts\n\n```typescript\n// content/overlay.ts\nimport { createShadowRootUi } from 'wxt/content-script-ui/shadow-root';\nimport { createRoot } from 'react-dom/client';\nimport Overlay from './Overlay';\n\nexport default defineContentScript({\n  matches: ['https://example.com/*'],\n  cssInjectionMode: 'ui',\n\n  async main(ctx) {\n    const ui = await createShadowRootUi(ctx, {\n      name: 'my-overlay',\n      position: 'inline',\n      anchor: 'body',\n      onMount: (container) => {\n        const root = createRoot(container);\n        root.render(<Overlay />);\n        return root;\n      },\n      onRemove: (root) => {\n        root?.unmount();\n      },\n    });\n\n    ui.mount();\n  },\n});\n```\n\n---\n\n## Site-Specific Content Scripts\n\n### Google Docs Extraction\n\n```typescript\n// content/gdocs.ts\nexport default defineContentScript({\n  matches: ['https://docs.google.com/document/*'],\n\n  main(ctx) {\n    const docId = extractDocId(window.location.href);\n    const title = document.title.replace(' - Google Docs', '');\n\n    // Google Docs renders text in .kix-lineview elements\n    const lines = document.querySelectorAll('.kix-lineview');\n    const text = Array.from(lines)\n      .map(el => el.textContent || '')\n      .join('\\n');\n\n    const cursorContext = getCursorContext();\n    const headings = extractHeadings();\n\n    browser.runtime.sendMessage({\n      type: 'DOC_OPEN',\n      doc: { docId, title, text, cursorContext, headings },\n    });\n  },\n});\n\nfunction extractDocId(url: string): string {\n  const match = url.match(/\\/document\\/d\\/([a-zA-Z0-9-_]+)/);\n  return match?.[1] || '';\n}\n\nfunction getCursorContext(): { before: string; after: string } {\n  // Implementation depends on Google Docs DOM structure\n  return { before: '', after: '' };\n}\n\nfunction extractHeadings(): { text: string; start: number }[] {\n  // Extract heading elements\n  return [];\n}\n```\n\n### Overleaf Extraction\n\n```typescript\n// content/overleaf.ts\nexport default defineContentScript({\n  matches: ['https://www.overleaf.com/project/*'],\n\n  main(ctx) {\n    const projectId = extractProjectId(window.location.href);\n\n    // Overleaf uses CodeMirror - access editor content\n    const editor = document.querySelector('.cm-content');\n    const text = editor?.textContent || '';\n\n    browser.runtime.sendMessage({\n      type: 'DOC_OPEN',\n      doc: { docId: projectId, title: document.title, text },\n    });\n  },\n});\n\nfunction extractProjectId(url: string): string {\n  const match = url.match(/\\/project\\/([a-f0-9]+)/);\n  return match?.[1] || '';\n}\n```\n\n---\n\n## UI Setup with shadcn-ui\n\n### Initialize Tailwind + shadcn\n\n```bash\n# After WXT init\npnpm add -D tailwindcss postcss autoprefixer\npnpm dlx tailwindcss init -p\n\n# Add shadcn-ui\npnpm dlx shadcn@latest init\npnpm dlx shadcn@latest add button card toast badge\n```\n\n**tailwind.config.js:**\n```javascript\n/** @type {import('tailwindcss').Config} */\nexport default {\n  darkMode: ['class'],\n  content: [\n    './entrypoints/**/*.{ts,tsx,html}',\n    './components/**/*.{ts,tsx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n};\n```\n\n---\n\n## Testing\n\n### Unit Tests (Vitest)\n\n```typescript\n// lib/__tests__/storage.test.ts\nimport { describe, it, expect, vi } from 'vitest';\nimport { docStorage } from '../storage';\n\nvi.mock('wxt/storage', () => ({\n  storage: {\n    getItem: vi.fn(),\n    setItem: vi.fn(),\n  },\n}));\n\ndescribe('docStorage', () => {\n  it('should get doc state by id', async () => {\n    const state = await docStorage.get('doc-123');\n    expect(state).toBeDefined();\n  });\n});\n```\n\n### E2E Tests (Playwright)\n\n```typescript\n// e2e/extension.spec.ts\nimport { test, expect, chromium } from '@playwright/test';\n\ntest('extension loads side panel', async () => {\n  const pathToExtension = './dist/chrome-mv3';\n\n  const context = await chromium.launchPersistentContext('', {\n    headless: false,\n    args: [\n      `--disable-extensions-except=${pathToExtension}`,\n      `--load-extension=${pathToExtension}`,\n    ],\n  });\n\n  const page = await context.newPage();\n  await page.goto('https://docs.google.com/document/d/test');\n\n  // Test content script injection\n  // Test side panel interaction\n});\n```\n\n---\n\n## Build & Distribution\n\n```json\n{\n  \"scripts\": {\n    \"dev\": \"wxt\",\n    \"dev:firefox\": \"wxt -b firefox\",\n    \"build\": \"wxt build\",\n    \"build:firefox\": \"wxt build -b firefox\",\n    \"zip\": \"wxt zip\",\n    \"test\": \"vitest\",\n    \"lint\": \"eslint .\"\n  }\n}\n```\n\n**Environment Variables:**\n```bash\n# .env.development\nUSE_MOCK_NATIVE=true\n\n# .env.production\nUSE_MOCK_NATIVE=false\n```\n\n---\n\n## Reference Files\n\n- [WXT Configuration Reference](./references/wxt-config.md) - Complete wxt.config.ts options\n- [MV3 Permissions Reference](./references/mv3-permissions.md) - Manifest permissions guide\n- [Messaging Patterns](./references/messaging.md) - Cross-context communication\n\n## Assets\n\n- [Template: Side Panel React](./assets/templates/sidepanel/) - Side panel starter\n- [Template: Content Script](./assets/templates/content-script/) - Content script starter"
              },
              {
                "name": "cli-generator",
                "description": "Generate AI-friendly Python CLIs using Click, Pydantic, and uv. Use when user wants to create a new CLI tool that follows best practices for agentic coding environments.",
                "path": "plugins/senior-software-developer/skills/cli-generator/SKILL.md",
                "frontmatter": {
                  "name": "cli-generator",
                  "description": "Generate AI-friendly Python CLIs using Click, Pydantic, and uv. Use when user wants to create a new CLI tool that follows best practices for agentic coding environments.",
                  "allowed-tools": "Read, Grep, Glob, Bash, Edit, Write"
                },
                "content": "# AI-Friendly CLI Generator Skill\n\nGenerate Python command-line interfaces optimized for AI agents and agentic coding environments.\n\n## Core Principle: Every Output is a Prompt\n\nIn an agentic coding environment, every interaction with a CLI tool is a turn in a conversation. The tool's outputwhether it succeeds or failsshould be designed as a helpful, guiding prompt for the agent's next action.\n\n## Tech Stack\n\n- **Python** - Primary language\n- **Click** - CLI framework\n- **Pydantic** - Data validation and response models\n- **Rich** - Terminal formatting and tables\n- **uv** - Package management\n\n## Project Structure\n\n```\nmy-cli/\n pyproject.toml\n README.md\n src/\n    my_cli/\n        __init__.py\n        main.py              # CLI entry point\n        commands/            # Command modules\n           __init__.py\n        models/\n           __init__.py\n           responses.py     # Pydantic response models\n        output/\n           __init__.py\n           conversational.py # AI-friendly output\n        core/\n            __init__.py\n            client.py        # API client\n            config.py        # Configuration\n tests/\n```\n\n## Quick Start\n\n1. Create project directory:\n```bash\nmkdir my-cli && cd my-cli\n```\n\n2. Initialize with uv:\n```bash\nuv init\n```\n\n3. Add dependencies to `pyproject.toml`:\n```toml\ndependencies = [\n    \"click>=8.1.0\",\n    \"rich>=13.0.0\",\n    \"pydantic>=2.0.0\",\n]\n```\n\n4. Create the source structure:\n```bash\nmkdir -p src/my_cli/{commands,models,output,core}\ntouch src/my_cli/__init__.py\ntouch src/my_cli/{commands,models,output,core}/__init__.py\n```\n\n5. Copy templates from `templates/` directory\n\n## AI-Friendly Output Patterns\n\n### Pattern 1: Success Output\n\nA successful output confirms the action AND suggests next steps with exact commands:\n\n**Bad (Traditional):**\n```\nSuccess!\n```\n\n**Good (AI-Friendly):**\n```\n Found 4 documents matching 'AI'\n\n Available Resources:\n   Total documents: 4\n   First document ID: 2oLo0Z72BR\n   First document name: AI experience design\n\n Results:\n\n Name                         ID          Updated    \n\n AI experience design         2oLo0Z72BR  2025-11-26 \n\n\n What's next? Try these commands:\n  1.  mycli show 2oLo0Z72BR - View document details\n  2.  mycli export 2oLo0Z72BR --format json - Export as JSON\n```\n\n### Pattern 2: Error Output (Three Parts)\n\nEvery error must include:\n1. **What went wrong** - Clear description\n2. **How to fix** - Step-by-step instructions\n3. **What's next** - Commands to try after fixing\n\n**Example:**\n```\n Command failed\n   Authentication error\n\n What went wrong:\n  The Coda API returned an error: API key is invalid or expired.\n\n How to fix:\n  1. Check your internet connection\n  2. Verify your API key is correct\n  3. Try regenerating your API token\n\n What's next:\n   mycli auth test - Test your authentication\n   mycli auth setup - Re-run interactive setup\n```\n\n### Pattern 3: Help Text with Examples\n\nAlways include working examples in `--help`:\n\n```python\n@click.command(\n    epilog=\"\"\"\nExamples:\n    # Search for documents\n    mycli search \"machine learning\"\n\n    # Export a table as JSON\n    mycli export DOC_ID TABLE_ID --format json\n\n    # List all your documents\n    mycli list --mine\n\"\"\"\n)\ndef search(query: str):\n    \"\"\"Search for documents matching a query.\"\"\"\n    pass\n```\n\n## Code Patterns\n\n### Response Models (`models/responses.py`)\n\n```python\n\"\"\"Pydantic models for CLI command responses.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\nfrom pydantic import BaseModel, Field\n\n\nclass Suggestion(BaseModel):\n    \"\"\"A suggested next command with description.\"\"\"\n    command: str = Field(..., description=\"The exact command to run\")\n    description: str = Field(..., description=\"What the command does\")\n    category: Optional[str] = Field(None, description=\"Category: view, export, search, etc.\")\n\n\nclass ErrorDetail(BaseModel):\n    \"\"\"Detailed error following 'what/how/next' pattern.\"\"\"\n    what_went_wrong: str = Field(..., description=\"Clear explanation of the failure\")\n    how_to_fix: List[str] = Field(..., description=\"Step-by-step fix instructions\")\n    whats_next: List[Suggestion] = Field(..., description=\"Commands to try after fixing\")\n    error_code: Optional[str] = Field(None, description=\"Machine-readable error code\")\n\n\nclass CommandResult(BaseModel):\n    \"\"\"Result of a CLI command with conversational context.\"\"\"\n    success: bool = Field(..., description=\"Whether command succeeded\")\n    message: str = Field(..., description=\"Primary result message\")\n    context: Dict[str, Any] = Field(default_factory=dict, description=\"Resource IDs and metadata\")\n    data: Optional[List[Any]] = Field(None, description=\"Structured data results\")\n    suggestions: List[Suggestion] = Field(default_factory=list, description=\"Suggested next commands\")\n    error: Optional[ErrorDetail] = Field(None, description=\"Error details if failed\")\n```\n\n### Conversational Output (`output/conversational.py`)\n\n```python\n\"\"\"Conversational output following 'Every Output is a Prompt' pattern.\"\"\"\n\nfrom typing import Any, Optional, List\nfrom rich.console import Console\nfrom rich.table import Table\nfrom .responses import CommandResult, Suggestion\n\n\nclass ConversationalOutput:\n    \"\"\"Output manager that makes every interaction conversational.\"\"\"\n\n    def __init__(self, console: Console, show_suggestions: bool = True):\n        self.console = console\n        self.show_suggestions = show_suggestions\n\n    def success(self, result: CommandResult) -> None:\n        \"\"\"Display success with context and suggestions.\"\"\"\n        # Main success message\n        self.console.print(f\" {result.message}\", style=\"bold green\")\n\n        # Show context (resource IDs, counts, etc.)\n        if result.context:\n            self.console.print(\"\\n Available Resources:\", style=\"bold blue\")\n            for key, value in result.context.items():\n                self.console.print(f\"   {key}: [cyan]{value}[/cyan]\")\n\n        # Show data in table format\n        if result.data:\n            self._render_data(result.data)\n\n        # Show suggested next commands\n        if self.show_suggestions and result.suggestions:\n            self._render_suggestions(result.suggestions)\n\n    def error(self, result: CommandResult) -> None:\n        \"\"\"Display error with three-part pattern.\"\"\"\n        if not result.error:\n            self.console.print(f\" {result.message}\", style=\"bold red\")\n            return\n\n        error = result.error\n\n        # What went wrong\n        self.console.print(\" Command failed\", style=\"bold red\")\n        self.console.print(f\"   {result.message}\")\n        self.console.print(\"\\n What went wrong:\", style=\"bold yellow\")\n        self.console.print(f\"  {error.what_went_wrong}\")\n\n        # How to fix\n        if error.how_to_fix:\n            self.console.print(\"\\n How to fix:\", style=\"bold green\")\n            for i, step in enumerate(error.how_to_fix, 1):\n                self.console.print(f\"  {i}. {step}\")\n\n        # What's next\n        if error.whats_next:\n            self.console.print(\"\\n What's next:\", style=\"bold blue\")\n            for suggestion in error.whats_next:\n                self.console.print(\n                    f\"   [cyan]{suggestion.command}[/cyan] - {suggestion.description}\"\n                )\n\n    def _render_data(self, data: List[Any]) -> None:\n        \"\"\"Render structured data as a table.\"\"\"\n        if not data:\n            return\n\n        self.console.print(\"\\n Results:\", style=\"bold blue\")\n        table = Table(show_header=True, header_style=\"bold magenta\")\n\n        # Build table from first item's keys\n        if isinstance(data[0], dict):\n            for key in list(data[0].keys())[:5]:  # Limit columns\n                table.add_column(key.replace(\"_\", \" \").title())\n\n            for item in data[:10]:  # Limit rows\n                table.add_row(*[str(v)[:40] for v in list(item.values())[:5]])\n\n        self.console.print(table)\n\n    def _render_suggestions(self, suggestions: List[Suggestion]) -> None:\n        \"\"\"Render suggested next commands.\"\"\"\n        self.console.print(\"\\n What's next? Try these commands:\", style=\"bold yellow\")\n\n        emoji_map = {\n            \"view\": \"\", \"export\": \"\", \"search\": \"\",\n            \"create\": \"\", \"edit\": \"\", \"auth\": \"\",\n        }\n\n        for i, s in enumerate(suggestions[:5], 1):\n            emoji = emoji_map.get(s.category, \"\")\n            self.console.print(f\"  {i}. {emoji}[cyan]{s.command}[/cyan] - {s.description}\")\n```\n\n### Main CLI Entry Point (`main.py`)\n\n```python\n\"\"\"Main CLI entry point.\"\"\"\n\nimport click\nfrom rich.console import Console\n\nfrom .models.responses import CommandResult, Suggestion, ErrorDetail\nfrom .output.conversational import ConversationalOutput\n\nconsole = Console()\noutput = ConversationalOutput(console)\n\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"My CLI tool - AI-friendly command interface.\n\n    Examples:\n        mycli search \"query\"\n        mycli show RESOURCE_ID\n        mycli export RESOURCE_ID --format json\n    \"\"\"\n    pass\n\n\n@cli.command(epilog=\"\"\"\nExamples:\n    mycli search \"machine learning\"\n    mycli search \"climate\" --limit 5\n\"\"\")\n@click.argument(\"query\")\n@click.option(\"--limit\", default=10, help=\"Maximum results to return\")\ndef search(query: str, limit: int):\n    \"\"\"Search for resources matching a query.\"\"\"\n    try:\n        # Your search logic here\n        results = []  # fetch_results(query, limit)\n\n        result = CommandResult(\n            success=True,\n            message=f\"Found {len(results)} results for '{query}'\",\n            context={\n                \"Query\": query,\n                \"Total results\": len(results),\n            },\n            data=results,\n            suggestions=[\n                Suggestion(\n                    command=f\"mycli show {results[0]['id']}\" if results else \"mycli list\",\n                    description=\"View details\" if results else \"List all resources\",\n                    category=\"view\"\n                ),\n                Suggestion(\n                    command=f\"mycli export {results[0]['id']} --format json\" if results else \"mycli search 'other'\",\n                    description=\"Export as JSON\" if results else \"Try another search\",\n                    category=\"export\" if results else \"search\"\n                ),\n            ]\n        )\n        output.success(result)\n\n    except Exception as e:\n        result = CommandResult(\n            success=False,\n            message=\"Search failed\",\n            error=ErrorDetail(\n                what_went_wrong=str(e),\n                how_to_fix=[\n                    \"Check your query syntax\",\n                    \"Verify your authentication\",\n                ],\n                whats_next=[\n                    Suggestion(command=\"mycli auth test\", description=\"Test authentication\", category=\"auth\"),\n                ]\n            )\n        )\n        output.error(result)\n\n\nif __name__ == \"__main__\":\n    cli()\n```\n\n## pyproject.toml Template\n\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"my-cli\"\nversion = \"0.1.0\"\ndescription = \"AI-friendly CLI tool\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"click>=8.1.0\",\n    \"rich>=13.0.0\",\n    \"pydantic>=2.0.0\",\n    \"python-dotenv>=1.0.0\",\n]\n\n[project.scripts]\nmycli = \"my_cli.main:cli\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/my_cli\"]\n```\n\n## Reference Implementation\n\nSee the `coda-cli` project for a complete working example:\n- Location: `.claude/skills/coda/scripts/coda-cli/`\n- Key files:\n  - `src/coda_cli/output/conversational.py` - Full output implementation\n  - `src/coda_cli/models/responses.py` - Complete response models\n  - `pyproject.toml` - Project configuration\n\n## Checklist for New CLIs\n\n- [ ] Every success output includes suggested next commands\n- [ ] Every error includes: what went wrong, how to fix, what's next\n- [ ] All commands have `epilog` with usage examples\n- [ ] Response models use Pydantic for validation\n- [ ] Rich is used for formatted terminal output\n- [ ] Context includes resource IDs for follow-up commands\n- [ ] Table output is limited to prevent overwhelming agents"
              },
              {
                "name": "detect-code-smells",
                "description": "Detect common code smells and anti-patterns providing feedback on quality issues a senior developer would catch during review. Use when user opens/views code files, asks for code review or quality assessment, mentions code quality/refactoring/improvements, when files contain code smell patterns, or during code review discussions.",
                "path": "plugins/senior-software-developer/skills/detect-code-smells/SKILL.md",
                "frontmatter": {
                  "name": "detect-code-smells",
                  "description": "Detect common code smells and anti-patterns providing feedback on quality issues a senior developer would catch during review. Use when user opens/views code files, asks for code review or quality assessment, mentions code quality/refactoring/improvements, when files contain code smell patterns, or during code review discussions."
                },
                "content": "# Detect Code Smells\n\nDetect common code smells and anti-patterns in code, providing immediate feedback on quality issues.\n\n## Code Smell Categories\n\n### 1. Complexity Smells\n- **Long Method**: Functions/methods > 50 lines\n- **Long Parameter List**: > 4 parameters\n- **Complex Conditionals**: Deeply nested if/else, complex boolean expressions\n- **High Cyclomatic Complexity**: > 10 branches\n- **Deep Nesting**: > 4 levels of indentation\n\n### 2. Duplication Smells\n- **Duplicate Code**: Repeated code blocks\n- **Similar Functions**: Functions with nearly identical logic\n- **Magic Numbers**: Hardcoded numbers without explanation\n- **String Duplication**: Repeated string literals\n\n### 3. Naming Smells\n- **Unclear Names**: Single letter variables (except loop counters)\n- **Hungarian Notation**: Unnecessary type prefixes\n- **Inconsistent Naming**: Mixed camelCase/snake_case\n- **Abbreviated Names**: Unclear abbreviations (mgr, ctx, tmp)\n- **Misleading Names**: Name doesn't match behavior\n\n### 4. Object-Oriented Smells\n- **God Class**: Class > 500 lines or too many responsibilities\n- **Data Class**: Class with only getters/setters\n- **Feature Envy**: Method uses more of another class than its own\n- **Inappropriate Intimacy**: Classes too dependent on internal details\n- **Lazy Class**: Class doing too little to justify existence\n\n### 5. Functional Smells\n- **Side Effects**: Function modifies external state unexpectedly\n- **Non-Pure Functions**: Functions with hidden dependencies\n- **Mutability Issues**: Unexpected mutation of objects\n- **Callback Hell**: Deeply nested callbacks\n\n### 6. Architecture Smells\n- **Circular Dependencies**: Module A depends on B, B depends on A\n- **Missing Abstraction**: Concrete implementations without interfaces\n- **Tight Coupling**: Hard dependencies on specific implementations\n- **Leaky Abstraction**: Implementation details exposed through interface\n\n## Detection Process\n\n1. Parse file - Analyze syntax tree and structure\n2. Identify patterns - Look for known code smell patterns\n3. Calculate metrics - Measure complexity, length, duplication\n4. Assess severity - Determine impact of each smell\n5. Generate report - Provide actionable feedback\n\n## Analysis Techniques\n- Abstract Syntax Tree (AST) parsing\n- Pattern matching against known smells\n- Metric calculation (LOC, complexity, coupling)\n- Comparison with language-specific best practices\n- Context-aware analysis (test files have different standards)\n\n## Return Value\n\nReturn object:\n```json\n{\n  \"file\": \"path/to/file.js\",\n  \"language\": \"javascript\",\n  \"overallScore\": 7.5,\n  \"smells\": [\n    {\n      \"type\": \"Long Method\",\n      \"severity\": \"warning\",\n      \"location\": {\n        \"line\": 42,\n        \"endLine\": 95,\n        \"function\": \"processUserData\"\n      },\n      \"description\": \"Function 'processUserData' is 53 lines long\",\n      \"suggestion\": \"Extract smaller functions for validation, transformation, and persistence\",\n      \"impact\": \"Harder to understand, test, and maintain\",\n      \"effort\": \"medium\"\n    },\n    {\n      \"type\": \"Complex Conditional\",\n      \"severity\": \"warning\",\n      \"location\": {\n        \"line\": 67,\n        \"column\": 8\n      },\n      \"description\": \"Nested conditional with 5 levels of nesting\",\n      \"suggestion\": \"Extract conditions into well-named boolean variables or separate functions\",\n      \"impact\": \"Difficult to understand logic flow\",\n      \"effort\": \"small\"\n    }\n  ],\n  \"metrics\": {\n    \"linesOfCode\": 234,\n    \"averageComplexity\": 4.2,\n    \"maxComplexity\": 12,\n    \"duplicationPercentage\": 8.5\n  },\n  \"recommendations\": [\n    \"Extract 'processUserData' into smaller single-purpose functions\",\n    \"Replace complex conditional at line 67 with early returns\",\n    \"Consider extracting repeated validation logic into a helper function\"\n  ]\n}\n```\n\n## Examples\n\n### Example 1: Detecting Long Method\n**Context**: User opens `user-service.js` containing a 150-line function\n\n**Autonomous Invocation**: Skill automatically analyzes the file and reports:\n```\nCode Smell Detected: Long Method\n\nFunction 'handleUserRegistration' (lines 23-172) is 150 lines long.\n\nImpact: This function is doing too many things, making it:\n- Hard to understand at a glance\n- Difficult to test thoroughly\n- Prone to bugs when modified\n- Challenging to reuse parts of its logic\n\nSuggestion: Extract into smaller functions:\n- validateRegistrationData() - lines 23-45\n- checkEmailAvailability() - lines 46-68\n- createUserAccount() - lines 69-98\n- sendWelcomeEmail() - lines 99-130\n- logRegistrationEvent() - lines 131-172\n\nThis follows the Single Responsibility Principle and makes each piece independently testable.\n```\n\n### Example 2: Detecting Complex Conditional\n**Context**: User working on file with deeply nested if/else\n\n**Autonomous Invocation**: Skill detects and suggests:\n```\nCode Smell Detected: Complex Conditional\n\nLines 45-72 contain a deeply nested conditional (5 levels deep) with multiple boolean conditions.\n\nCurrent pattern:\nif (user) {\n  if (user.isActive) {\n    if (user.hasPermission('admin')) {\n      if (validateToken(user.token)) {\n        // ... more nesting\n      }\n    }\n  }\n}\n\nRefactoring suggestion using guard clauses:\nif (!user) return null;\nif (!user.isActive) return null;\nif (!user.hasPermission('admin')) return null;\nif (!validateToken(user.token)) return null;\n\n// Now do the work without nesting\n\nBenefits: Reduces cognitive load, clearer failure conditions, easier to test edge cases.\n```\n\n### Example 3: Detecting Code Duplication\n**Context**: User reviewing file with repeated code blocks\n\n**Autonomous Invocation**: Skill identifies DRY violation:\n```\nCode Smell Detected: Duplicate Code\n\nFound 3 nearly identical code blocks:\n- Lines 34-42 (in fetchUserById)\n- Lines 78-86 (in fetchUserByEmail)\n- Lines 112-120 (in fetchUserByUsername)\n\nAll three blocks implement the same error handling and response formatting logic.\n\nSuggestion: Extract common logic into a helper function:\n\nfunction handleUserFetchResponse(result, errorMessage) {\n  if (!result) {\n    throw new Error(errorMessage);\n  }\n  return formatUserResponse(result);\n}\n\nThen simplify each call site:\nconst user = await db.query(...);\nreturn handleUserFetchResponse(user, 'User not found');\n\nImpact: Reduces maintenance burden, ensures consistent behavior, fewer bugs.\n```\n\n## Error Handling\n\n- If file cannot be parsed: Return error with details about syntax issue\n- If language not supported: Suggest manual review or generic analysis\n- If file is too large (> 10k lines): Warn about God Object and suggest splitting\n- If file is test file: Apply different standards (longer functions acceptable)\n- If file is generated: Skip analysis or warn user\n\n## Context Awareness\n\n### Test Files\n- Allow longer functions (tests often have setup/teardown)\n- Allow more duplication (explicit tests > DRY)\n- Different naming conventions acceptable\n\n### Configuration Files\n- JSON/YAML: Check for structural issues\n- Don't apply code smell rules meant for logic\n\n### Legacy Code\n- Flag issues but acknowledge inherited constraints\n- Prioritize critical issues over stylistic ones\n- Suggest incremental improvement approach\n\n### Generated Code\n- Identify if file is auto-generated\n- Suggest improving generator rather than generated code\n- Skip some smell checks\n\n## Integration with Development Workflow\n\n- **Non-intrusive**: Provides info but doesn't block work\n- **Actionable**: Specific suggestions with examples\n- **Educational**: Explains why something is a smell\n- **Prioritized**: Critical issues highlighted over minor ones\n- **Context-sensitive**: Understands different file types\n\n## Related Skills\n- `suggest-performance-fix`: Focuses on performance issues\n- `security-pattern-check`: Focuses on security concerns\n\n## Notes\n\nThis skill acts as a senior developer looking over your shoulder, catching issues that might be missed during fast-paced development. It doesn't replace human code review but augments it by catching obvious issues early."
              },
              {
                "name": "mcp-builder",
                "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
                "path": "plugins/senior-software-developer/skills/mcp-builder/SKILL.md",
                "frontmatter": {
                  "name": "mcp-builder",
                  "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n##  High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by clientsome clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nStart with the sitemap to find relevant pages: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [ Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [ Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n##  Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Start with sitemap at `https://modelcontextprotocol.io/sitemap.xml`, then fetch specific pages with `.md` suffix\n- [ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [ Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts"
              },
              {
                "name": "monorepo-generator",
                "description": "Generate production-ready monorepo structures for full-stack applications including frontends (Next.js, React), APIs (Hono, Express), and data pipelines. Use when creating new monorepo projects, scaffolding multi-project architectures (web apps, APIs, workers, CLI tools), setting up shared packages, or configuring workspace tooling with Bun, PNPM, or Yarn.",
                "path": "plugins/senior-software-developer/skills/monorepo-generator/SKILL.md",
                "frontmatter": {
                  "name": "monorepo-generator",
                  "description": "Generate production-ready monorepo structures for full-stack applications including frontends (Next.js, React), APIs (Hono, Express), and data pipelines. Use when creating new monorepo projects, scaffolding multi-project architectures (web apps, APIs, workers, CLI tools), setting up shared packages, or configuring workspace tooling with Bun, PNPM, or Yarn."
                },
                "content": "# Monorepo Generator\n\nGenerate scalable monorepo structures for full-stack applications with shared packages.\n\n## Quick Start\n\n```bash\nmkdir my-monorepo && cd my-monorepo\nbun init -y  # or: pnpm init / npm init -y\n```\n\n---\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nAsk user what project types they need:\n\n| Type | Examples | Common Stack |\n|------|----------|--------------|\n| **Frontend** | Web app, admin dashboard, docs site | Next.js 15, React 19, Tailwind |\n| **API** | REST API, GraphQL server | Hono, Express, Fastify |\n| **Pipeline** | ETL jobs, cron tasks, workers | Node scripts, Python |\n| **CLI** | Developer tools, automation | Commander, Yargs |\n| **Mobile** | React Native app | Expo, React Native |\n\n### Step 2: Choose Package Manager\n\n- **Bun** (recommended): Fastest, built-in workspaces\n- **PNPM**: Efficient disk usage, strict dependencies\n- **Yarn**: Mature ecosystem, plug'n'play option\n\nSee [Package Manager Guide](./references/package-managers.md) for configs.\n\n### Step 3: Create Directory Structure\n\n```\n/monorepo/\n apps/                       # Deployable applications\n    web/                    # Next.js frontend\n    api/                    # Hono/Express API\n    admin/                  # Admin dashboard\n    docs/                   # Documentation site\n\n services/                   # Backend services & workers\n    jobs/                   # Scheduled jobs/cron\n    workers/                # Background workers\n    pipelines/              # Data pipelines\n\n packages/                   # Shared libraries\n    ui/                     # React components (shadcn/ui)\n    utils/                  # Shared utilities\n    db/                     # Database client & schemas\n    api-client/             # Generated API client\n    config/                 # Shared configuration\n    eslint-config/          # Shared ESLint\n    typescript-config/      # Shared TS configs\n\n package.json                # Workspace root\n turbo.json                  # Turborepo config\n .gitignore\n```\n\nAdapt structure based on needsnot all projects need all directories.\n\n### Step 4: Create Root Configuration\n\n**Root package.json:**\n```json\n{\n  \"name\": \"monorepo\",\n  \"private\": true,\n  \"workspaces\": [\"apps/*\", \"services/*\", \"packages/*\"],\n  \"scripts\": {\n    \"dev\": \"turbo dev\",\n    \"build\": \"turbo build\",\n    \"lint\": \"turbo lint\",\n    \"type-check\": \"turbo type-check\",\n    \"test\": \"turbo test\"\n  },\n  \"devDependencies\": {\n    \"turbo\": \"^2.3.0\",\n    \"typescript\": \"^5.7.0\"\n  },\n  \"packageManager\": \"bun@1.1.0\"\n}\n```\n\n**turbo.json:**\n```json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"tasks\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"dist/**\", \".next/**\"]\n    },\n    \"dev\": { \"cache\": false, \"persistent\": true },\n    \"start\": { \"dependsOn\": [\"build\"] },\n    \"lint\": { \"dependsOn\": [\"^lint\"] },\n    \"type-check\": { \"dependsOn\": [\"^type-check\"] },\n    \"test\": { \"dependsOn\": [\"^build\"] }\n  }\n}\n```\n\n---\n\n## Project Templates\n\n### Frontend App (Next.js)\n\n```json\n// apps/web/package.json\n{\n  \"name\": \"web\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev --turbopack\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"@monorepo/ui\": \"workspace:*\",\n    \"@monorepo/utils\": \"workspace:*\",\n    \"next\": \"^15.0.0\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\"\n  },\n  \"devDependencies\": {\n    \"@monorepo/typescript-config\": \"workspace:*\",\n    \"tailwindcss\": \"^4.0.0\",\n    \"typescript\": \"^5.7.0\"\n  }\n}\n```\n\n```typescript\n// apps/web/next.config.ts\nimport type { NextConfig } from 'next';\n\nconst config: NextConfig = {\n  transpilePackages: ['@monorepo/ui', '@monorepo/utils'],\n};\n\nexport default config;\n```\n\n### API Server (Hono)\n\n```json\n// apps/api/package.json\n{\n  \"name\": \"api\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"bun run --hot src/index.ts\",\n    \"build\": \"bun build src/index.ts --outdir dist --target node\",\n    \"start\": \"bun run dist/index.js\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"@monorepo/db\": \"workspace:*\",\n    \"@monorepo/utils\": \"workspace:*\",\n    \"hono\": \"^4.6.0\",\n    \"@hono/node-server\": \"^1.13.0\"\n  },\n  \"devDependencies\": {\n    \"@monorepo/typescript-config\": \"workspace:*\",\n    \"@types/node\": \"^22.0.0\",\n    \"typescript\": \"^5.7.0\"\n  }\n}\n```\n\n```typescript\n// apps/api/src/index.ts\nimport { serve } from '@hono/node-server';\nimport { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { logger } from 'hono/logger';\n\nconst app = new Hono();\n\napp.use('*', logger());\napp.use('*', cors());\n\napp.get('/', (c) => c.json({ status: 'ok' }));\napp.get('/health', (c) => c.json({ healthy: true }));\n\n// Routes\napp.route('/api/users', usersRouter);\napp.route('/api/items', itemsRouter);\n\nserve({ fetch: app.fetch, port: 3001 }, (info) => {\n  console.log(`API running on http://localhost:${info.port}`);\n});\n```\n\n### Background Worker/Job\n\n```json\n// services/jobs/package.json\n{\n  \"name\": \"jobs\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"bun run --watch src/index.ts\",\n    \"build\": \"bun build src/index.ts --outdir dist --target node\",\n    \"start\": \"bun run dist/index.js\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"@monorepo/db\": \"workspace:*\",\n    \"@monorepo/utils\": \"workspace:*\",\n    \"cron\": \"^3.1.0\"\n  },\n  \"devDependencies\": {\n    \"@monorepo/typescript-config\": \"workspace:*\",\n    \"@types/cron\": \"^2.4.0\",\n    \"typescript\": \"^5.7.0\"\n  }\n}\n```\n\n```typescript\n// services/jobs/src/index.ts\nimport { CronJob } from 'cron';\nimport { db } from '@monorepo/db';\n\n// Run every hour\nconst hourlyJob = new CronJob('0 * * * *', async () => {\n  console.log('Running hourly job...');\n  // Your job logic here\n});\n\n// Run daily at midnight\nconst dailyJob = new CronJob('0 0 * * *', async () => {\n  console.log('Running daily cleanup...');\n  // Cleanup logic here\n});\n\nhourlyJob.start();\ndailyJob.start();\n\nconsole.log('Job scheduler started');\n```\n\n### Data Pipeline\n\n```json\n// services/pipelines/package.json\n{\n  \"name\": \"pipelines\",\n  \"private\": true,\n  \"scripts\": {\n    \"etl:users\": \"bun run src/etl/users.ts\",\n    \"etl:analytics\": \"bun run src/etl/analytics.ts\",\n    \"build\": \"bun build src/**/*.ts --outdir dist\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"@monorepo/db\": \"workspace:*\",\n    \"@monorepo/utils\": \"workspace:*\",\n    \"zod\": \"^3.23.0\"\n  },\n  \"devDependencies\": {\n    \"@monorepo/typescript-config\": \"workspace:*\",\n    \"typescript\": \"^5.7.0\"\n  }\n}\n```\n\n```typescript\n// services/pipelines/src/etl/users.ts\nimport { db } from '@monorepo/db';\nimport { z } from 'zod';\n\nconst UserSchema = z.object({\n  id: z.string(),\n  email: z.string().email(),\n  createdAt: z.coerce.date(),\n});\n\nasync function extractUsers() {\n  // Extract from source\n}\n\nasync function transformUsers(raw: unknown[]) {\n  return raw.map((r) => UserSchema.parse(r));\n}\n\nasync function loadUsers(users: z.infer<typeof UserSchema>[]) {\n  // Load to destination\n}\n\nasync function main() {\n  console.log('Starting ETL pipeline...');\n  const raw = await extractUsers();\n  const transformed = await transformUsers(raw);\n  await loadUsers(transformed);\n  console.log(`Processed ${transformed.length} users`);\n}\n\nmain().catch(console.error);\n```\n\n---\n\n## Shared Packages\n\n### packages/utils\n\n```typescript\n// packages/utils/src/index.ts\nexport { cn } from './cn';\nexport { formatDate, parseDate } from './date';\nexport { sleep, retry } from './async';\nexport type { Result, AsyncResult } from './types';\n```\n\n### packages/db\n\n```json\n// packages/db/package.json\n{\n  \"name\": \"@monorepo/db\",\n  \"private\": true,\n  \"exports\": { \".\": \"./src/index.ts\" },\n  \"dependencies\": {\n    \"drizzle-orm\": \"^0.36.0\",\n    \"postgres\": \"^3.4.0\"\n  },\n  \"devDependencies\": {\n    \"drizzle-kit\": \"^0.28.0\",\n    \"@monorepo/typescript-config\": \"workspace:*\"\n  }\n}\n```\n\n### packages/config\n\n```typescript\n// packages/config/src/index.ts\nimport { z } from 'zod';\n\nconst envSchema = z.object({\n  NODE_ENV: z.enum(['development', 'production', 'test']).default('development'),\n  DATABASE_URL: z.string().url(),\n  API_URL: z.string().url().optional(),\n  LOG_LEVEL: z.enum(['debug', 'info', 'warn', 'error']).default('info'),\n});\n\nexport const config = envSchema.parse(process.env);\nexport type Config = z.infer<typeof envSchema>;\n```\n\n---\n\n## TypeScript Configs\n\n```json\n// packages/typescript-config/base.json\n{\n  \"$schema\": \"https://json.schemastore.org/tsconfig\",\n  \"compilerOptions\": {\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"declaration\": true,\n    \"declarationMap\": true\n  }\n}\n```\n\n```json\n// packages/typescript-config/node.json (for APIs/services)\n{\n  \"extends\": \"./base.json\",\n  \"compilerOptions\": {\n    \"lib\": [\"ES2022\"],\n    \"noEmit\": true\n  }\n}\n```\n\n```json\n// packages/typescript-config/nextjs.json\n{\n  \"extends\": \"./base.json\",\n  \"compilerOptions\": {\n    \"lib\": [\"DOM\", \"DOM.Iterable\", \"ES2022\"],\n    \"jsx\": \"preserve\",\n    \"noEmit\": true,\n    \"plugins\": [{ \"name\": \"next\" }]\n  }\n}\n```\n\n---\n\n## Adding New Projects\n\n```bash\n# New frontend\nmkdir -p apps/dashboard && cp -r apps/web/* apps/dashboard/\n\n# New API\nmkdir -p apps/gateway && cp -r apps/api/* apps/gateway/\n\n# New service\nmkdir -p services/notifications\n```\n\nUpdate `package.json` name and dependencies for each new project.\n\n---\n\n## Deployment\n\n| Project Type | Deployment Options |\n|--------------|-------------------|\n| Frontend | Vercel, Netlify, Cloudflare Pages |\n| API | Railway, Fly.io, AWS Lambda, Docker |\n| Workers | Railway, Render, AWS ECS |\n| Jobs | Cron service, GitHub Actions, Temporal |\n\nEach project deploys independently from its directory.\n\n---\n\n## Resources\n\n- [Package Manager Guide](./references/package-managers.md) - Bun/PNPM/Yarn configs\n- [assets/](./assets/) - Template files for scaffolding"
              },
              {
                "name": "security-pattern-check",
                "description": "Identify security vulnerabilities and anti-patterns providing feedback on security issues a senior developer would catch. Use when user mentions security/vulnerability/safety concerns, code involves user input/authentication/data access, working with sensitive data (passwords/PII/financial), code includes SQL queries/file operations/external API calls, user asks about security best practices, or security-sensitive files are being modified (auth, payment, data access).",
                "path": "plugins/senior-software-developer/skills/security-pattern-check/SKILL.md",
                "frontmatter": {
                  "name": "security-pattern-check",
                  "description": "Identify security vulnerabilities and anti-patterns providing feedback on security issues a senior developer would catch. Use when user mentions security/vulnerability/safety concerns, code involves user input/authentication/data access, working with sensitive data (passwords/PII/financial), code includes SQL queries/file operations/external API calls, user asks about security best practices, or security-sensitive files are being modified (auth, payment, data access)."
                },
                "content": "# Security Pattern Check\n\nIdentify security vulnerabilities and anti-patterns in code, providing immediate feedback on security issues.\n\n## Security Issue Categories\n\n### 1. Injection Vulnerabilities\n- **SQL Injection**: Unparameterized queries with user input\n- **NoSQL Injection**: Unsanitized MongoDB queries\n- **Command Injection**: Shell commands with user input\n- **LDAP Injection**: Unescaped LDAP queries\n- **XPath Injection**: Dynamic XPath expressions\n\n### 2. Authentication & Authorization\n- **Missing Authentication**: Unprotected endpoints\n- **Weak Passwords**: No password strength requirements\n- **Hardcoded Credentials**: Passwords/keys in code\n- **Insecure Password Storage**: Plaintext or weak hashing (MD5, SHA1)\n- **Missing Authorization**: No permission checks\n- **Broken Access Control**: Users accessing resources they shouldn't\n\n### 3. Sensitive Data Exposure\n- **Logging Sensitive Data**: Passwords, tokens in logs\n- **Insecure Storage**: Unencrypted sensitive data\n- **Missing Encryption**: No TLS/SSL\n- **Weak Encryption**: DES, RC4, custom crypto\n- **Exposed Secrets**: API keys, tokens in code/config\n- **Information Disclosure**: Stack traces, verbose errors to users\n\n### 4. Cross-Site Scripting (XSS)\n- **Reflected XSS**: Echoing user input in HTML\n- **Stored XSS**: Saving unsanitized input to database\n- **DOM XSS**: Client-side script injection\n- **Missing Output Encoding**: Not escaping HTML/JavaScript\n\n### 5. Cross-Site Request Forgery (CSRF)\n- **Missing CSRF Tokens**: State-changing requests without tokens\n- **Incorrect Token Validation**: Weak or missing validation\n- **GET for State Changes**: Using GET for mutations\n\n### 6. Security Misconfiguration\n- **Debug Mode in Production**: Verbose error messages\n- **Default Credentials**: Using default admin/admin\n- **Unnecessary Services**: Unused endpoints or features enabled\n- **Missing Security Headers**: No CSP, X-Frame-Options, etc.\n- **Directory Listing**: Exposing file structure\n- **Improper Error Handling**: Revealing system details\n\n### 7. Cryptographic Issues\n- **Weak Random Number Generation**: Math.random() for security\n- **Insufficient Key Length**: Short encryption keys\n- **Weak Hash Functions**: MD5, SHA1 for security purposes\n- **ECB Mode**: Using ECB instead of CBC/GCM\n- **No Salt**: Password hashing without salt\n- **Predictable Tokens**: Sequential or guessable tokens\n\n### 8. API Security\n- **Missing Rate Limiting**: No protection against abuse\n- **Excessive Data Exposure**: Returning more data than needed\n- **Missing Input Validation**: Not validating request parameters\n- **Improper Assets Management**: Exposing old/unused endpoints\n- **Security Misconfiguration**: CORS allowing any origin\n\n### 9. Component Vulnerabilities\n- **Known Vulnerable Dependencies**: Using libraries with CVEs\n- **Outdated Packages**: Using old versions with known issues\n- **Unverified Dependencies**: Not checking package integrity\n\n### 10. File & Resource Issues\n- **Path Traversal**: User-controlled file paths\n- **Unrestricted File Upload**: No validation on uploads\n- **XML External Entity (XXE)**: Parsing untrusted XML\n- **Insecure Deserialization**: Deserializing untrusted data\n- **Resource Exhaustion**: No limits on resource usage\n\n## Analysis Process\n\n1. Identify input points - Find user input, external data\n2. Trace data flow - Follow data through application\n3. Check validation - Verify input sanitization\n4. Review auth/authz - Check access controls\n5. Examine crypto - Review encryption usage\n6. Scan dependencies - Check for known CVEs\n7. Assess configuration - Review security settings\n8. Generate report - Prioritize by risk\n\n## Return Value\n\nReturn object:\n```json\n{\n  \"file\": \"api/auth.js\",\n  \"securityScore\": 6.5,\n  \"riskLevel\": \"high\",\n  \"vulnerabilities\": [\n    {\n      \"type\": \"SQL Injection\",\n      \"severity\": \"critical\",\n      \"cwe\": \"CWE-89\",\n      \"owasp\": \"A03:2021  Injection\",\n      \"location\": {\n        \"line\": 34,\n        \"function\": \"loginUser\"\n      },\n      \"description\": \"User input directly interpolated into SQL query\",\n      \"risk\": {\n        \"confidentiality\": \"high\",\n        \"integrity\": \"high\",\n        \"availability\": \"medium\",\n        \"exploitability\": \"easy\"\n      },\n      \"exploitScenario\": \"Attacker can bypass authentication or extract database contents\",\n      \"codeExample\": \"// Current vulnerable code...\",\n      \"fix\": {\n        \"approach\": \"Use parameterized queries\",\n        \"codeExample\": \"// Secure version...\",\n        \"effort\": \"small\"\n      },\n      \"references\": [\n        \"https://owasp.org/www-community/attacks/SQL_Injection\",\n        \"https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html\"\n      ]\n    }\n  ],\n  \"recommendations\": [\n    \"Implement parameterized queries for all database access\",\n    \"Add input validation middleware\",\n    \"Enable security headers (CSP, X-Frame-Options)\",\n    \"Rotate and secure API keys using environment variables\"\n  ],\n  \"compliance\": {\n    \"owasp\": [\"A03:2021\", \"A07:2021\"],\n    \"pciDss\": [\"6.5.1\"],\n    \"cwe\": [\"CWE-89\", \"CWE-798\"]\n  }\n}\n```\n\n## Examples\n\n### Example 1: SQL Injection Detection\n**Context**: User has login function with string concatenation\n\n**Autonomous Invocation**: Skill detects vulnerability:\n```\n CRITICAL SECURITY VULNERABILITY DETECTED: SQL Injection\n\nLocation: api/auth.js, line 34\n\nVulnerable Code:\nasync function loginUser(username, password) {\n  const query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;\n  const user = await db.query(query);\n  return user;\n}\n\nVulnerability: SQL Injection (CWE-89, OWASP A03:2021)\n\nRisk Assessment:\n- Severity: CRITICAL\n- Exploitability: Easy (no special tools needed)\n- Impact: Complete database compromise\n\nAttack Scenario:\nAn attacker could input:\n  username: admin' --\n  password: anything\n\nResulting query:\n  SELECT * FROM users WHERE username = 'admin' --' AND password = 'anything'\n\nThe '--' comments out the rest, bypassing password check!\n\nWorse attack:\n  username: admin'; DROP TABLE users; --\n  password: anything\n\nThis could DELETE YOUR ENTIRE USERS TABLE!\n\nSecure Fix (Parameterized Query):\nasync function loginUser(username, password) {\n  // Use parameterized query (prevents injection)\n  const query = 'SELECT * FROM users WHERE username = ? AND password = ?';\n  const user = await db.query(query, [username, password]);\n  return user;\n}\n\nAdditional Security Measures:\n1. Hash passwords (never store plaintext)\n   - Use bcrypt, scrypt, or Argon2\n   - Example: const hashedPassword = await bcrypt.hash(password, 10);\n\n2. Add rate limiting to prevent brute force\n   - Limit login attempts per IP\n   - Add exponential backoff after failures\n\n3. Use prepared statements consistently across all queries\n\n4. Implement logging for failed login attempts\n\n5. Consider multi-factor authentication\n\nReferences:\n- OWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection\n- SQL Injection Prevention: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html\n\nEstimated effort to fix: 30 minutes\nPriority: FIX IMMEDIATELY (before deploying to production)\n```\n\n### Example 2: Hardcoded Secrets\n**Context**: User has API key directly in code\n\n**Autonomous Invocation**: Skill identifies exposure:\n```\n SECURITY ISSUE DETECTED: Hardcoded Secrets\n\nLocation: services/payment.js, line 12\n\nVulnerable Code:\nconst STRIPE_API_KEY = 'sk_live_51HqK2bL4z8pQxYz...';\n\nconst stripe = require('stripe')(STRIPE_API_KEY);\n\nProblem: Exposed Credentials (CWE-798)\n\nRisks:\n1. API key visible in version control history (git)\n2. Exposed to anyone with code access\n3. Cannot rotate key without code change\n4. May be accidentally shared (screenshots, pairing, etc.)\n5. If key is compromised, attacker can make payments with your account\n\nThis is a LIVE API key - production payments are at risk!\n\nSecure Fix:\n1. Remove key from code immediately:\n   git rm --cached services/payment.js\n   git commit -m \"Remove hardcoded API key\"\n\n2. Rotate the compromised key in Stripe dashboard\n   - Generate new key\n   - Revoke old key\n\n3. Use environment variables:\n   // .env file (add to .gitignore!)\n   STRIPE_API_KEY=sk_live_51HqK2bL4z8pQxYz...\n\n   // services/payment.js\n   const stripe = require('stripe')(process.env.STRIPE_API_KEY);\n\n4. For production, use secret management:\n   - AWS Secrets Manager\n   - Azure Key Vault\n   - HashiCorp Vault\n   - Kubernetes Secrets\n\n5. Add to .gitignore:\n   .env\n   .env.local\n   .env.*.local\n\n6. Create .env.example (without real values):\n   STRIPE_API_KEY=your_stripe_key_here\n\nBest Practices:\n- Never commit secrets to git\n- Use different keys for dev/staging/prod\n- Rotate keys regularly\n- Monitor key usage for anomalies\n- Use secret scanning tools (git-secrets, truffleHog)\n\nImmediate Actions:\n1.  Rotate Stripe API key NOW\n2.  Move to environment variables\n3.  Check git history for other exposed secrets\n4.  Review access logs for suspicious activity\n\nEstimated effort: 15 minutes\nPriority: CRITICAL - Fix immediately\n```\n\n### Example 3: Missing Input Validation\n**Context**: User has API endpoint accepting user input\n\n**Autonomous Invocation**: Skill identifies missing validation:\n```\n  SECURITY ISSUE DETECTED: Missing Input Validation\n\nLocation: api/users.js, line 45\n\nVulnerable Code:\napp.post('/api/users', async (req, res) => {\n  const { username, email, age } = req.body;\n  \n  const user = await db.createUser({ username, email, age });\n  res.json(user);\n});\n\nProblems:\n1. No validation on input types\n2. No length limits\n3. No format validation (email)\n4. No sanitization (XSS risk)\n\nRisk: Moderate to High\n- Data integrity issues\n- Potential for XSS\n- Database errors from unexpected input\n- Business logic bypass\n\nAttack Scenarios:\n\n1. Type Confusion:\n   POST { \"age\": \"definitely not a number\" }\n   Could cause database errors or logic bugs\n\n2. Extremely Long Input:\n   POST { \"username\": \"A\".repeat(1000000) }\n   Could cause memory issues or DoS\n\n3. XSS Injection:\n   POST { \"username\": \"<script>alert('xss')</script>\" }\n   If username is displayed in HTML, executes JavaScript\n\n4. Email Format:\n   POST { \"email\": \"not-an-email\" }\n   Breaks email functionality\n\nSecure Fix:\n// Option 1: Manual validation\napp.post('/api/users', async (req, res) => {\n  const { username, email, age } = req.body;\n  \n  // Validate types\n  if (typeof username !== 'string' || typeof email !== 'string') {\n    return res.status(400).json({ error: 'Invalid input types' });\n  }\n  \n  // Validate lengths\n  if (username.length < 3 || username.length > 50) {\n    return res.status(400).json({ error: 'Username must be 3-50 characters' });\n  }\n  \n  // Validate format\n  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n  if (!emailRegex.test(email)) {\n    return res.status(400).json({ error: 'Invalid email format' });\n  }\n  \n  // Validate ranges\n  const ageNum = parseInt(age);\n  if (isNaN(ageNum) || ageNum < 13 || ageNum > 120) {\n    return res.status(400).json({ error: 'Age must be between 13 and 120' });\n  }\n  \n  // Sanitize for XSS (if needed for display)\n  const sanitizedUsername = escapeHtml(username);\n  \n  const user = await db.createUser({\n    username: sanitizedUsername,\n    email,\n    age: ageNum\n  });\n  \n  res.json(user);\n});\n\n// Option 2: Use validation library (Recommended)\nconst { body, validationResult } = require('express-validator');\n\napp.post('/api/users',\n  // Validation middleware\n  body('username')\n    .isString()\n    .trim()\n    .isLength({ min: 3, max: 50 })\n    .escape(), // Prevents XSS\n  body('email')\n    .isEmail()\n    .normalizeEmail(),\n  body('age')\n    .isInt({ min: 13, max: 120 }),\n  \n  async (req, res) => {\n    // Check validation results\n    const errors = validationResult(req);\n    if (!errors.isEmpty()) {\n      return res.status(400).json({ errors: errors.array() });\n    }\n    \n    const { username, email, age } = req.body;\n    const user = await db.createUser({ username, email, age });\n    res.json(user);\n  }\n);\n\nAdditional Security Measures:\n1. Rate limiting (prevent abuse)\n   app.use(rateLimit({ windowMs: 15 * 60 * 1000, max: 100 }));\n\n2. Schema validation (Joi, Yup, Zod)\n   const schema = Joi.object({\n     username: Joi.string().min(3).max(50).required(),\n     email: Joi.string().email().required(),\n     age: Joi.number().integer().min(13).max(120).required()\n   });\n\n3. Content-Type validation\n   Only accept application/json, reject others\n\n4. Size limits\n   app.use(express.json({ limit: '10kb' }));\n\nBest Practices:\n- Whitelist valid input, don't blacklist bad input\n- Validate on server side (client validation is not secure)\n- Fail securely (reject invalid input)\n- Provide clear error messages (but don't reveal system details)\n- Log suspicious validation failures\n\nEstimated effort: 1 hour\nPriority: HIGH - Add before next deployment\n```\n\n## Error Handling\n\n- If language not supported: Provide generic security guidance\n- If context insufficient: Ask for framework/usage details\n- If no issues found: Confirm and suggest security best practices\n- If false positive suspected: Provide rationale and allow override\n\n## Risk Scoring\n\nIssues scored by CVSS-like criteria:\n- **Critical (9.0-10.0)**: SQL injection, RCE, auth bypass\n- **High (7.0-8.9)**: XSS, CSRF, sensitive data exposure\n- **Medium (4.0-6.9)**: Missing security headers, weak crypto\n- **Low (0.1-3.9)**: Information disclosure, minor config issues\n\n## Integration with Workflow\n\n- **Proactive**: Catches vulnerabilities during development\n- **Educational**: Explains attack scenarios and impact\n- **Standards-Based**: References OWASP, CWE, compliance requirements\n- **Actionable**: Provides specific secure code examples\n- **Prioritized**: Critical issues highlighted first\n\n## Compliance Mapping\n\nMaps findings to:\n- OWASP Top 10 (2021)\n- CWE (Common Weakness Enumeration)\n- PCI-DSS requirements\n- HIPAA security rules\n- GDPR data protection requirements\n\n## Related Skills\n- `detect-code-smells`: General code quality\n- `suggest-performance-fix`: Performance optimization\n\n## Notes\n\nThis skill embodies the senior developer mindset: \"Security is not an afterthought.\" It catches common vulnerabilities early when they're cheap to fix, rather than in production where they're expensive disasters.\n\nSecurity principles:\n1. Defense in depth (multiple layers)\n2. Least privilege (minimal permissions)\n3. Fail securely (deny by default)\n4. Don't trust user input (validate everything)\n5. Keep it simple (complex systems are insecure)"
              },
              {
                "name": "suggest-performance-fix",
                "description": "Identify performance issues and bottlenecks suggesting optimizations a senior developer would recommend. Use when user mentions performance/optimization/speed issues, asks \"why is this slow?\" or similar questions, code contains obvious performance anti-patterns (N+1 queries, unnecessary loops, etc.), user works on performance-critical code paths, performance profiling shows bottlenecks, or large data processing or database operations detected.",
                "path": "plugins/senior-software-developer/skills/suggest-performance-fix/SKILL.md",
                "frontmatter": {
                  "name": "suggest-performance-fix",
                  "description": "Identify performance issues and bottlenecks suggesting optimizations a senior developer would recommend. Use when user mentions performance/optimization/speed issues, asks \"why is this slow?\" or similar questions, code contains obvious performance anti-patterns (N+1 queries, unnecessary loops, etc.), user works on performance-critical code paths, performance profiling shows bottlenecks, or large data processing or database operations detected."
                },
                "content": "# Suggest Performance Fix\n\nIdentify performance issues and bottlenecks in code, suggesting specific optimizations.\n\n## Performance Issue Categories\n\n### 1. Algorithmic Complexity\n- **Nested Loops**: O(n) or worse time complexity\n- **Inefficient Search**: Linear search when hash lookup possible\n- **Redundant Calculations**: Computing same value multiple times\n- **Inefficient Sorting**: Using bubble sort instead of quicksort/mergesort\n- **String Concatenation**: Building strings in loops\n\n### 2. Database Performance\n- **N+1 Query Problem**: Loading related data in a loop\n- **Missing Indexes**: Queries on unindexed columns\n- **SELECT ***: Fetching unnecessary columns\n- **No Query Pagination**: Loading all records at once\n- **Missing Connection Pooling**: Creating new connection per query\n- **Lack of Prepared Statements**: Re-parsing queries\n\n### 3. Memory Issues\n- **Memory Leaks**: Objects not released/garbage collected\n- **Excessive Allocations**: Creating unnecessary objects\n- **Large In-Memory Data**: Loading entire datasets into RAM\n- **Caching Issues**: Not caching expensive operations\n- **Deep Cloning**: Expensive deep copies when shallow would work\n\n### 4. I/O Performance\n- **Synchronous I/O**: Blocking operations in async context\n- **No Batching**: Multiple small I/O operations instead of batch\n- **Missing Compression**: Large payloads without compression\n- **No Streaming**: Loading entire files into memory\n- **Excessive File I/O**: Reading/writing files repeatedly\n\n### 5. Frontend/API Performance\n- **No Lazy Loading**: Loading all data upfront\n- **Missing CDN**: Serving static assets from origin\n- **Large Bundle Size**: Shipping unnecessary code\n- **Render Blocking**: Blocking page render with scripts\n- **Missing Caching Headers**: Not utilizing browser cache\n- **Excessive API Calls**: Multiple calls when one would suffice\n\n### 6. Concurrency Issues\n- **Missing Parallelization**: Sequential when parallel possible\n- **Race Conditions**: Unprotected shared state\n- **Lock Contention**: Too much synchronization\n- **Thread Pool Exhaustion**: Not limiting concurrent operations\n\n## Analysis Process\n\n1. Identify hotspots - Find expensive operations\n2. Measure complexity - Calculate time/space complexity\n3. Check patterns - Match against known anti-patterns\n4. Estimate impact - Predict performance gain from fix\n5. Prioritize - Focus on high-impact optimizations\n6. Generate fix - Suggest specific code improvements\n\n## Performance Analysis Techniques\n- Complexity analysis (Big O notation)\n- Pattern matching for known anti-patterns\n- Static analysis of queries and loops\n- Resource usage estimation\n- Comparison with best practices\n\n## Return Value\n\nReturn object:\n```json\n{\n  \"file\": \"api/users.js\",\n  \"issues\": [\n    {\n      \"type\": \"N+1 Query\",\n      \"severity\": \"critical\",\n      \"location\": {\n        \"line\": 45,\n        \"function\": \"getUsersWithPosts\"\n      },\n      \"description\": \"Loading posts for each user in a loop (N+1 query pattern)\",\n      \"currentPerformance\": {\n        \"timeComplexity\": \"O(n)\",\n        \"queriesExecuted\": \"1 + n\",\n        \"estimatedLatency\": \"50ms + (n  20ms)\"\n      },\n      \"optimizedPerformance\": {\n        \"timeComplexity\": \"O(1)\",\n        \"queriesExecuted\": \"2\",\n        \"estimatedLatency\": \"70ms (constant)\"\n      },\n      \"impact\": \"high\",\n      \"effort\": \"small\",\n      \"suggestedFix\": {\n        \"approach\": \"Use JOIN or eager loading\",\n        \"codeExample\": \"// Before...\\n// After...\",\n        \"estimatedSpeedup\": \"10-100x faster depending on n\"\n      }\n    }\n  ],\n  \"recommendations\": [\n    {\n      \"priority\": 1,\n      \"title\": \"Fix N+1 query in getUsersWithPosts\",\n      \"impact\": \"80% reduction in API response time\",\n      \"effort\": \"30 minutes\"\n    }\n  ]\n}\n```\n\n## Examples\n\n### Example 1: N+1 Query Detection\n**Context**: User has API endpoint that loads users and their posts\n\n**Autonomous Invocation**: Skill detects the pattern and reports:\n```\nPerformance Issue Detected: N+1 Query Problem\n\nLocation: api/users.js, line 45\n\nCurrent Code:\nasync function getUsersWithPosts() {\n  const users = await db.query('SELECT * FROM users');\n  \n  for (const user of users) {\n    user.posts = await db.query('SELECT * FROM posts WHERE user_id = ?', [user.id]);\n  }\n  \n  return users;\n}\n\nProblem:\n- Executes 1 query to get users\n- Then executes n additional queries (one per user) to get posts\n- With 100 users, this is 101 database queries!\n\nPerformance Impact:\n- Current: ~50ms + (100  20ms) = 2,050ms (2 seconds!)\n- Each additional user adds 20ms\n\nOptimized Solution:\nasync function getUsersWithPosts() {\n  const users = await db.query('SELECT * FROM users');\n  const userIds = users.map(u => u.id);\n  \n  // Single query to get all posts\n  const posts = await db.query(\n    'SELECT * FROM posts WHERE user_id IN (?)',\n    [userIds]\n  );\n  \n  // Group posts by user_id in application code\n  const postsByUser = posts.reduce((acc, post) => {\n    if (!acc[post.user_id]) acc[post.user_id] = [];\n    acc[post.user_id].push(post);\n    return acc;\n  }, {});\n  \n  users.forEach(user => {\n    user.posts = postsByUser[user.id] || [];\n  });\n  \n  return users;\n}\n\nAlternative (SQL JOIN):\nconst results = await db.query(`\n  SELECT \n    u.*,\n    p.id as post_id,\n    p.title as post_title,\n    p.content as post_content\n  FROM users u\n  LEFT JOIN posts p ON p.user_id = u.id\n`);\n\n// Then reshape results in application code\n\nPerformance Improvement:\n- Optimized: ~70ms (constant, regardless of user count)\n- 30x faster with 100 users\n- 100x faster with 1000 users\n\nEstimated effort: 30 minutes\nImpact: Critical for scalability\n```\n\n### Example 2: Inefficient Algorithm\n**Context**: User has sorting implementation in code\n\n**Autonomous Invocation**: Skill identifies algorithmic issue:\n```\nPerformance Issue Detected: Inefficient Algorithm\n\nLocation: utils/sort.js, line 23\n\nCurrent Code:\nfunction sortUsers(users) {\n  // Bubble sort implementation\n  for (let i = 0; i < users.length; i++) {\n    for (let j = 0; j < users.length - 1; j++) {\n      if (users[j].name > users[j + 1].name) {\n        [users[j], users[j + 1]] = [users[j + 1], users[j]];\n      }\n    }\n  }\n  return users;\n}\n\nProblem:\n- Time Complexity: O(n) - quadratic growth\n- With 1,000 users: ~1,000,000 comparisons\n- With 10,000 users: ~100,000,000 comparisons\n\nPerformance Impact:\n- 100 users: ~5ms\n- 1,000 users: ~450ms\n- 10,000 users: ~45 seconds (!!!)\n\nOptimized Solution:\nfunction sortUsers(users) {\n  return users.sort((a, b) => a.name.localeCompare(b.name));\n}\n\nBuilt-in Array.sort() uses optimized algorithm (typically Timsort):\n- Time Complexity: O(n log n)\n- With 10,000 users: ~130,000 comparisons (vs 100M)\n\nPerformance Improvement:\n- 100 users: 5ms  <1ms\n- 1,000 users: 450ms  3ms (150x faster)\n- 10,000 users: 45s  40ms (1,125x faster!)\n\nAdditional Optimization (for very large datasets):\n// If sorting is a frequent operation, consider:\n1. Maintain sorted order during insertion\n2. Use indexed database column and sort in DB\n3. Cache sorted results\n\nEstimated effort: 5 minutes\nImpact: Critical at scale\n```\n\n### Example 3: Missing Database Index\n**Context**: User reports slow query performance\n\n**Autonomous Invocation**: Skill analyzes query:\n```\nPerformance Issue Detected: Missing Database Index\n\nLocation: repositories/user-repo.js, line 67\n\nCurrent Code:\nasync function findUsersByStatus(status) {\n  return await db.query(\n    'SELECT * FROM users WHERE status = ?',\n    [status]\n  );\n}\n\nProblem:\n- Query filters on 'status' column\n- No index on 'status' column\n- Database must scan entire table (table scan)\n\nPerformance Impact (with 100,000 users):\n- Current: ~500ms (full table scan)\n- After index: ~5ms (index lookup)\n- 100x improvement!\n\nQuery Execution Plan (Current):\n- Type: ALL (full table scan)\n- Rows examined: 100,000\n- Rows returned: ~5,000\n\nRecommended Fix:\n1. Add database index:\n   CREATE INDEX idx_users_status ON users(status);\n\n2. Verify index is being used:\n   EXPLAIN SELECT * FROM users WHERE status = 'active';\n\nQuery Execution Plan (After Index):\n- Type: ref (index lookup)\n- Rows examined: ~5,000 (only matching rows)\n- Rows returned: ~5,000\n\nAdditional Considerations:\n- Index adds ~5MB storage (minimal cost)\n- Slightly slower writes (updates must update index)\n- Worth it for read-heavy columns\n\nIf multiple filters are common:\n   CREATE INDEX idx_users_status_created \n   ON users(status, created_at);\n\nThis supports queries like:\n   WHERE status = 'active' AND created_at > '2024-01-01'\n\nEstimated effort: 5 minutes\nImpact: Critical for query performance\n```\n\n## Error Handling\n\n- If file type not recognized: Skip analysis or use generic patterns\n- If measurements missing: Provide theoretical analysis only\n- If context insufficient: Ask for more information about use case\n- If no issues found: Provide confirmation that code looks performant\n\n## Priority Matrix\n\nIssues are prioritized by:\n- **Critical**: 10x+ improvement possible, high traffic code path\n- **High**: 5-10x improvement, medium traffic\n- **Medium**: 2-5x improvement or low traffic\n- **Low**: < 2x improvement, optimization edge cases\n\n## Integration with Workflow\n\n- **Proactive**: Catches issues before they reach production\n- **Educational**: Explains complexity and tradeoffs\n- **Measurable**: Provides concrete performance estimates\n- **Actionable**: Specific code examples for fixes\n- **Prioritized**: Focuses on high-impact optimizations\n\n## Related Skills\n- `detect-code-smells`: General code quality issues\n- `security-pattern-check`: Security-focused analysis\n\n## Notes\n\nFollowing the senior developer principle: \"Premature optimization is the root of all evil, but that doesn't mean ignore obvious issues.\" This skill focuses on clear performance anti-patterns, not micro-optimizations.\n\nPerformance optimization should be:\n1. Measured (know the current performance)\n2. Targeted (fix the actual bottleneck)\n3. Verified (confirm the improvement)\n4. Balanced (against code complexity)"
              },
              {
                "name": "vscode-extension-builder",
                "description": "Comprehensive guide for creating VS Code extensions from scratch, including project scaffolding, API usage, activation events, and packaging. Use when user wants to create/build/generate/develop a VS Code extension or plugin, asks about VS Code extension development, needs help with VS Code Extension API, discusses extension architecture, wants to add commands/webviews/language support, or mentions scaffolding a VS Code project.",
                "path": "plugins/senior-software-developer/skills/vscode-extension-builder/SKILL.md",
                "frontmatter": {
                  "name": "vscode-extension-builder",
                  "description": "Comprehensive guide for creating VS Code extensions from scratch, including project scaffolding, API usage, activation events, and packaging. Use when user wants to create/build/generate/develop a VS Code extension or plugin, asks about VS Code extension development, needs help with VS Code Extension API, discusses extension architecture, wants to add commands/webviews/language support, or mentions scaffolding a VS Code project."
                },
                "content": "# VS Code Extension Builder\n\nBuild professional VS Code extensions with proper architecture, best practices, and complete tooling support.\n\n## Quick Start\n\nFor immediate extension creation:\n\n1. **Initialize**: Run `npx --package yo --package generator-code -- yo code`\n2. **Choose type**: New Extension (TypeScript)\n3. **Fill details**: Name, identifier, description\n4. **Develop**: Open in VS Code, press F5 to debug\n5. **Test**: Run commands in Extension Development Host\n6. **Package**: Run `vsce package` when ready\n\nFor detailed guidance, follow the workflow below.\n\n## Extension Types\n\nChoose the type that matches your needs:\n\n- **Command Extension**: Add commands to Command Palette (simplest, most common)\n- **Language Support**: Syntax highlighting, IntelliSense, formatting\n- **Webview Extension**: Custom UI panels with HTML/CSS/JS\n- **Tree View**: Custom sidebar views with hierarchical data\n- **Debugger**: Add debugging support for languages\n- **Theme**: Color themes, file icon themes\n- **Snippet Provider**: Code snippets for languages\n\n## Core Workflow\n\n### 1. Gather Requirements\n\nAsk user about:\n- **Purpose**: What should the extension do?\n- **Type**: Which extension type? (command, language, webview, etc.)\n- **Features**: Specific functionality needed\n- **UI**: Commands, views, panels, status bar items?\n- **Activation**: When should it activate?\n\n### 2. Choose Extension Type & Architecture\n\nBased on requirements, select appropriate pattern:\n\n**Simple Command Extension** (most common):\n- Single responsibility\n- Command Palette integration\n- Quick to build\n\n**Language Extension**:\n- Syntax highlighting (TextMate grammar)\n- Language server for IntelliSense\n- Complex but powerful\n\n**Webview Extension**:\n- Custom UI needed\n- Rich interactions\n- More complex state management\n\nSee [extension-anatomy.md](references/extension-anatomy.md) for detailed structure.\n\n### 3. Initialize Project\n\n**Option A: Use Yeoman Generator (Recommended)**\n```bash\nnpx --package yo --package generator-code -- yo code\n```\n\nFill in:\n- Type: New Extension (TypeScript)\n- Name: User-friendly name\n- Identifier: lowercase-with-hyphens\n- Description: Clear purpose\n- Git: Yes\n- Bundler: esbuild (recommended) or webpack\n- Package manager: npm\n\n**Option B: Use Templates**\n\nFor specific patterns, copy from `assets/templates/`:\n- `command-extension/` - Command-based extension\n- `language-support/` - Language extension starter\n- `webview-extension/` - Webview-based extension\n\n### 4. Implement Core Functionality\n\n**For Command Extensions:**\n\n1. Define command in `package.json`:\n```json\n{\n  \"contributes\": {\n    \"commands\": [{\n      \"command\": \"extension.commandId\",\n      \"title\": \"Command Title\"\n    }]\n  }\n}\n```\n\n2. Register command in `extension.ts`:\n```typescript\nexport function activate(context: vscode.ExtensionContext) {\n  let disposable = vscode.commands.registerCommand('extension.commandId', () => {\n    vscode.window.showInformationMessage('Hello from Extension!');\n  });\n  context.subscriptions.push(disposable);\n}\n```\n\n**For Language Extensions:**\nSee [common-apis.md](references/common-apis.md) for language features APIs.\n\n**For Webview Extensions:**\nSee [common-apis.md](references/common-apis.md) for webview creation patterns.\n\n### 5. Configure Activation & Contributions\n\n**Activation Events** determine when your extension loads:\n- `onCommand`: When command is invoked\n- `onLanguage`: When file type opens\n- `onView`: When tree view becomes visible\n- `*`: On startup (avoid if possible)\n\nSee [activation-events.md](references/activation-events.md) for complete reference.\n\n**Contributions** declare extension capabilities in `package.json`:\n- `commands`: Command Palette entries\n- `menus`: Context menu items\n- `keybindings`: Keyboard shortcuts\n- `languages`: Language support\n- `views`: Tree views\n- `configuration`: Settings\n\n### 6. Test & Debug\n\n**Local Testing:**\n1. Press `F5` in VS Code to launch Extension Development Host\n2. Test commands and features\n3. Check Debug Console for logs\n4. Set breakpoints for debugging\n\n**Automated Testing:**\n- Unit tests: Test business logic\n- Integration tests: Test VS Code API interactions\n- Use `@vscode/test-electron` for testing\n\n**Common Issues:**\n- Command not appearing: Check `contributes.commands` and activation events\n- Extension not activating: Verify activation events in `package.json`\n- API errors: Check VS Code API version compatibility\n\n### 7. Package & Distribute\n\n**Prepare for Publishing:**\n1. Update README.md with features and usage\n2. Add extension icon (128x128 PNG)\n3. Set repository URL in package.json\n4. Add LICENSE file\n5. Test thoroughly\n\n**Package Extension:**\n```bash\nnpm install -g @vscode/vsce\nvsce package\n```\n\nCreates `.vsix` file for distribution.\n\n**Publish to Marketplace:**\n```bash\nvsce publish\n```\n\nRequires Azure DevOps personal access token.\n\n## Common Patterns\n\n### Pattern 1: Simple Command\n\nQuick command that shows information:\n\n```typescript\nvscode.commands.registerCommand('extension.showInfo', () => {\n  vscode.window.showInformationMessage('Information message');\n});\n```\n\n### Pattern 2: Command with User Input\n\nGet input before executing:\n\n```typescript\nvscode.commands.registerCommand('extension.greet', async () => {\n  const name = await vscode.window.showInputBox({\n    prompt: 'Enter your name'\n  });\n  if (name) {\n    vscode.window.showInformationMessage(`Hello, ${name}!`);\n  }\n});\n```\n\n### Pattern 3: File Operation Command\n\nWork with active editor:\n\n```typescript\nvscode.commands.registerCommand('extension.processFile', () => {\n  const editor = vscode.window.activeTextEditor;\n  if (!editor) {\n    vscode.window.showErrorMessage('No active editor');\n    return;\n  }\n  \n  const document = editor.document;\n  const text = document.getText();\n  // Process text...\n});\n```\n\n### Pattern 4: Status Bar Item\n\nShow persistent status:\n\n```typescript\nconst statusBarItem = vscode.window.createStatusBarItem(\n  vscode.StatusBarAlignment.Right,\n  100\n);\nstatusBarItem.text = \"$(check) Ready\";\nstatusBarItem.show();\ncontext.subscriptions.push(statusBarItem);\n```\n\n## Reference Navigation\n\nLoad these references as needed:\n\n- **[extension-anatomy.md](references/extension-anatomy.md)**: When you need details about:\n  - Extension structure and file organization\n  - `package.json` manifest fields\n  - Entry point and lifecycle hooks\n  - Extension context and disposables\n\n- **[common-apis.md](references/common-apis.md)**: When implementing:\n  - Window and editor operations\n  - Workspace and file system access\n  - Language features (IntelliSense, diagnostics)\n  - Webview creation and messaging\n  - Tree views and custom UI\n\n- **[activation-events.md](references/activation-events.md)**: When configuring:\n  - When extension should load\n  - Performance optimization\n  - Lazy loading strategies\n\n- **[best-practices.md](references/best-practices.md)**: When considering:\n  - UX guidelines and design patterns\n  - Performance optimization\n  - Security considerations\n  - Testing strategies\n  - Publishing guidelines\n\n## Key Principles\n\n### Performance\n- **Lazy load**: Use specific activation events, not `*`\n- **Async operations**: Use async/await for I/O\n- **Dispose resources**: Clean up subscriptions\n- **Minimize startup**: Defer heavy operations\n\n### User Experience\n- **Clear commands**: Descriptive titles and categories\n- **Feedback**: Show progress for long operations\n- **Error handling**: Helpful error messages\n- **Consistent UI**: Follow VS Code conventions\n\n### Code Quality\n- **TypeScript**: Use strict mode for type safety\n- **Error handling**: Try-catch for all operations\n- **Logging**: Use console.log for debugging\n- **Testing**: Write tests for critical functionality\n\n## Troubleshooting\n\n### Extension Not Appearing\n- Verify `package.json` syntax (valid JSON)\n- Check `main` field points to compiled output\n- Ensure activation events are correct\n- Reload window: `Developer: Reload Window`\n\n### Command Not Working\n- Check command ID matches in `package.json` and code\n- Verify activation event includes the command\n- Check Debug Console for errors\n- Ensure command is registered in `activate()`\n\n### Build Errors\n- Run `npm install` to install dependencies\n- Check TypeScript configuration\n- Verify VS Code API version compatibility\n- Update `@types/vscode` if needed\n\n## Examples by Use Case\n\n### Add Command to Format Code\n1. Type: Command extension\n2. Activation: `onCommand`\n3. Implementation: Get editor text, format, replace\n4. UI: Command Palette entry\n\n### Add Syntax Highlighting\n1. Type: Language extension\n2. Activation: `onLanguage:mylang`\n3. Implementation: TextMate grammar in JSON\n4. UI: Automatic on file open\n\n### Add Custom Sidebar View\n1. Type: Tree view extension\n2. Activation: `onView:myView`\n3. Implementation: TreeDataProvider interface\n4. UI: Activity bar icon + sidebar panel\n\n### Add Quick Pick Menu\n1. Type: Command extension with UI\n2. Activation: `onCommand`\n3. Implementation: `showQuickPick` with items\n4. UI: Searchable dropdown menu\n\n## Resources in This Skill\n\n- **references/**: Detailed documentation (load as needed)\n- **assets/templates/**: Starting templates for common patterns\n- **Official docs**: https://code.visualstudio.com/api\n\n## Related Skills\n\nFor code quality and architecture review of your extension code:\n- `detect-code-smells`: Check extension code quality\n- `security-pattern-check`: Security review for extensions\n- `suggest-performance-fix`: Optimize extension performance\n\n## Notes\n\nThis skill provides the complete workflow for VS Code extension development, from initial concept to published extension. Use progressive disclosure: start with Quick Start for simple cases, dive into references for complex requirements. Templates in `assets/` provide copy-paste starting points for common patterns."
              }
            ]
          },
          {
            "name": "kjgarza-product",
            "description": "Product management toolkit with document processing, integrations, and research capabilities",
            "source": "./plugins/kjgarza-product",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "Kristian Garza",
              "email": "kj.garza@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add kjgarza/marketplace-claude",
              "/plugin install kjgarza-product@marketplace-claude"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-11T12:23:28Z",
              "created_at": "2026-01-01T15:35:08Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/analyze-feature-request",
                "description": "Analyze a feature request using RICE, Kano, and other PM frameworks",
                "path": "plugins/kjgarza-product/commands/analyze-feature-request.md",
                "frontmatter": {
                  "description": "Analyze a feature request using RICE, Kano, and other PM frameworks",
                  "argument-hint": [
                    "feature request description"
                  ]
                },
                "content": "# Analyze Feature Request\n\nEvaluate this feature request using PM frameworks: **$ARGUMENTS**\n\n## Your Task\n\nAnalyze the feature request above to determine its viability, impact, and strategic fit. Use the frameworks below to produce a data-driven recommendation on whether to pursue this opportunity.\n\n## Steps\n\n1. **Gather Context**\n   - Parse the feature request \"$ARGUMENTS\" for key details\n   - Search for related user research in Dovetail\n   - Check Coda for any existing PRDs or roadmap items\n   - Review any relevant technical documentation\n\n2. **Apply Product Frameworks**\n   - Use RICE scoring (Reach, Impact, Confidence, Effort)\n   - Apply Kano Model to categorize feature type (Basic/Performance/Delighter)\n   - Apply Jobs-to-be-Done framework to understand user needs\n   - Consider product lifecycle stage for context-appropriate prioritization\n   - Evaluate against current product strategy and OKRs\n   - Apply 80/20 rule: Is this in the vital 20% that drives value?\n   - Consider competitive landscape\n   \n   **Reference**: See [product-frameworks](../skills/product-frameworks/) skill for detailed guidance on:\n   - Prioritization frameworks (RICE, Kano, Pareto)\n   - Lifecycle-aware prioritization strategies\n\n3. **Analyze Impact**\n   - Estimate user reach (how many users affected)\n   - Assess impact on user value (1-3 scale: minimal, moderate, massive)\n   - Evaluate business impact (revenue, retention, acquisition)\n   - Consider strategic alignment with product vision\n\n4. **Assess Feasibility**\n   - Technical complexity and dependencies\n   - Resource requirements (engineering, design, PM time)\n   - Timeline estimates\n   - Risk factors and constraints\n\n5. **Generate Recommendation**\n   - Present RICE score with breakdown\n   - Recommend priority level (P0-P3)\n   - Suggest next steps (build, research more, defer, decline)\n   - Identify key assumptions to validate\n\n## Output Format\n\nPresent the analysis in a structured format:\n\n### Feature Summary\n- **Request**: [Brief description]\n- **Requested by**: [Source]\n- **Strategic alignment**: [How it fits strategy]\n\n### RICE Score Analysis\n- **Reach**: [Number of users/quarter]\n- **Impact**: [0.25-3 scale: 3=Massive, 2=High, 1=Medium, 0.5=Low, 0.25=Minimal]\n- **Confidence**: [Percentage: 100%=High, 80%=Medium, 50%=Low]\n- **Effort**: [Person-months estimate]\n- **Final RICE Score**: [Calculated: (Reach  Impact  Confidence) / Effort]\n\n### Kano Model Classification\n- **Category**: [Basic/Performance/Delighter]\n- **Rationale**: [Why this classification]\n- **Implication**: [What this means for prioritization]\n\n### Lifecycle Context\n- **Product stage**: [Introduction/Growth/Maturity/Decline]\n- **Stage-appropriate priority**: [How lifecycle affects this feature's priority]\n\n### Supporting Evidence\n- User research insights from Dovetail\n- Competitive analysis\n- Usage data or analytics\n- Strategic objectives\n\n### Recommendation\n- **Priority**: [P0/P1/P2/P3]\n- **Decision**: [Build/Research/Defer/Decline]\n- **Rationale**: [Key reasons for recommendation]\n- **Next steps**: [Actionable items]\n- **Risks**: [Key risks and mitigations]\n\n## Examples\n\n### Example 1: User-Requested Feature\n```\nUser: \"We've had 10 customers request bulk export functionality\"\n\n[Command analyzes]:\n- Searches Dovetail for export-related feedback\n- Checks Coda for roadmap and PRD status\n- Calculates RICE score\n- Provides prioritization recommendation\n```\n\n### Example 2: Competitive Feature\n```\nUser: \"Competitor X just launched real-time collaboration. Should we build this?\"\n\n[Command analyzes]:\n- Reviews strategic positioning\n- Assesses user demand from research\n- Evaluates effort vs. impact\n- Recommends whether to pursue\n```\n\n## Best Practices\n\n- Ground analysis in user research data, not just opinions\n- Be transparent about confidence levels and assumptions\n- Consider opportunity cost (what else could we build)\n- Include both quantitative and qualitative factors\n- Make clear, actionable recommendations\n- Document key assumptions for future reference\n\n## Related Commands\n\n- [/create-prd](create-prd.md) - Create a PRD if feature is approved\n- [/prioritize-backlog](prioritize-backlog.md) - Compare against other backlog items\n- [/search-user-research](search-user-research.md) - Find supporting evidence in Dovetail\n\n## Framework References\n\nFor deeper understanding of the frameworks used:\n- **RICE Scoring**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n- **Kano Model**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n- **Pareto Principle**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n- **Lifecycle Guidance**: [lifecycle-guidance.md](../skills/product-frameworks/references/lifecycle-guidance.md)\n- **Jobs-to-be-Done**: [requirements.md](../skills/product-frameworks/references/requirements.md)"
              },
              {
                "name": "/analyze-intel",
                "description": "Analyze the relevance of new information (text, link, or document) to the current product/project",
                "path": "plugins/kjgarza-product/commands/analyze-intel.md",
                "frontmatter": {
                  "description": "Analyze the relevance of new information (text, link, or document) to the current product/project",
                  "argument-hint": [
                    "URL",
                    "text",
                    "or description of new information"
                  ]
                },
                "content": "# Analyze Intelligence\n\nAnalyze the relevance and implications of: **$ARGUMENTS**\n\n## Purpose\n\nThis command helps product teams quickly assess whether new informationmarket news, competitor updates, research papers, user feedback, technology announcements, or any other signalis relevant to their product strategy and requires action.\n\n## Your Task\n\nSystematically analyze the provided information and produce a structured relevance assessment with clear recommendations on whether and how to act as well as a few comments to share with the team.\n\n## Steps\n\n### 1. Resolve the Information\n\n**If URL provided:**\n- Fetch and extract the content using WebFetch\n- Identify the source type (news article, blog post, research paper, documentation, social media, etc.)\n- Note publication date and author credibility\n\n**If text provided:**\n- Parse the key claims and information\n- Identify the source if mentioned\n- Note any context clues\n\n**If reference to a document:**\n- Read the document from the filesystem\n- Extract key points and claims\n\n### 2. Understand Current Product Context\n\n- Review CLAUDE.md for project context\n- Check recent standup notes or documentation for current priorities\n- Identify the product's:\n  - Current stage (ideation, development, launch, growth, maturity)\n  - Target users and personas\n  - Key value propositions\n  - Active hypotheses being tested\n  - Known risks and concerns\n\n### 3. Assess Relevance\n\nScore the information across multiple dimensions:\n\n| Dimension | Score (1-5) | Criteria |\n|-----------|-------------|----------|\n| **Direct Relevance** | | Does this directly affect our product, users, or market? |\n| **Timing Urgency** | | Do we need to act now, soon, or can this wait? |\n| **Competitive Impact** | | Does this change our competitive position? |\n| **Strategic Alignment** | | Does this affect our strategy or validate/invalidate assumptions? |\n| **User Impact** | | Does this change what our users need or expect? |\n\n**Overall Relevance Score:** Average of dimensions (1-5)\n- **5 - Critical:** Requires immediate attention and likely action\n- **4 - High:** Should be discussed this week, may require action\n- **3 - Moderate:** Worth noting, discuss when relevant\n- **2 - Low:** Tangentially related, file for future reference\n- **1 - Minimal:** Not relevant to current product focus\n\n### 4. Identify Implications\n\nFor relevant information, analyze:\n\n**Opportunities:**\n- Does this create new possibilities for the product?\n- Can we leverage this for competitive advantage?\n- Does this validate our direction?\n\n**Threats:**\n- Does this invalidate assumptions we're making?\n- Does this create competitive pressure?\n- Does this introduce new risks?\n\n**Dependencies:**\n- Does this affect our technical approach?\n- Does this change our timeline?\n- Does this require new capabilities?\n\n### 5. Generate Recommendations\n\nBased on relevance and implications, recommend:\n\n**Action Level:**\n-  **Act Now**  Immediate response required\n-  **Act Soon**  Address within 1-2 weeks\n-  **Monitor**  Track but no immediate action\n-  **Archive**  Note for reference, no action needed\n\n**Specific Actions:**\n- Who should be informed?\n- What decisions need to be made?\n- What additional research is needed?\n- How does this affect current priorities?\n\n## Output Format\n\n### Intelligence Brief\n\n```markdown\n## Summary\n[2-3 sentence summary of the information and its source]\n\n## Source Assessment\n- **Type:** [News/Research/Competitor/Technology/User Feedback/Regulatory/Other]\n- **Credibility:** [High/Medium/Low]  [Rationale]\n- **Freshness:** [Publication date and timeliness]\n\n## Relevance Assessment\n\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| Direct Relevance | X/5 | [Brief explanation] |\n| Timing Urgency | X/5 | [Brief explanation] |\n| Competitive Impact | X/5 | [Brief explanation] |\n| Strategic Alignment | X/5 | [Brief explanation] |\n| User Impact | X/5 | [Brief explanation] |\n\n**Overall Score:** X/5  [Critical/High/Moderate/Low/Minimal]\n\n## Key Implications\n\n### Opportunities\n- [Opportunity 1]\n- [Opportunity 2]\n\n### Threats\n- [Threat 1]\n- [Threat 2]\n\n### Assumptions Affected\n- [Assumption that may need revisiting]\n\n## Recommendations\n\n**Action Level:** [ Act Now /  Act Soon /  Monitor /  Archive]\n\n**Recommended Actions:**\n1. [Specific action with owner if known]\n2. [Specific action]\n\n**Discussion Points:**\n- [Question to raise with team]\n\n**Related To:**\n- [Current initiative or workstream this relates to]\n```\n\n## Information Type Frameworks\n\n### Competitor Intelligence\n\nWhen analyzing competitor news:\n- What specifically did they ship/announce?\n- Does this overlap with our roadmap?\n- Does this change user expectations in our market?\n- What's their apparent strategy?\n- How does their approach differ from ours?\n\n### Technology Updates\n\nWhen analyzing technology changes:\n- Does this enable something we couldn't do before?\n- Does this obsolete our current approach?\n- What's the adoption timeline?\n- What's the integration effort?\n\n### User Research / Feedback\n\nWhen analyzing user signals:\n- Does this validate or challenge our assumptions?\n- Is this representative or anecdotal?\n- How does this compare to our current understanding?\n- What questions does this raise?\n\n### Market / Industry News\n\nWhen analyzing market changes:\n- Does this expand or contract our addressable market?\n- Does this change buyer behavior or priorities?\n- Are there regulatory implications?\n- What's the timeline for impact?\n\n### Academic Research\n\nWhen analyzing research papers:\n- What problem does this solve?\n- Is this applicable to our domain?\n- What's the maturity level (theory vs. applied)?\n- Who are the authors and institutions?\n\n## Examples\n\n### Example 1: Competitor Announcement\n```\nUser: /analyze-intel https://techcrunch.com/competitor-launches-ai-writing-assistant\n\n[Command analyzes]:\n- Fetches article content\n- Identifies competitor and feature set\n- Compares to current product roadmap\n- Assesses overlap and differentiation\n- Recommends team discussion points\n```\n\n### Example 2: User Feedback\n```\nUser: /analyze-intel \"Three users this week mentioned they want real-time collaboration in the editor\"\n\n[Command analyzes]:\n- Parses the feedback\n- Checks against current user research\n- Assesses alignment with roadmap\n- Evaluates effort vs. impact\n- Recommends whether to investigate further\n```\n\n### Example 3: Technology Change\n```\nUser: /analyze-intel OpenAI just released a new model with 2x context window\n\n[Command analyzes]:\n- Identifies technical implications\n- Assesses impact on current architecture\n- Evaluates migration effort\n- Recommends adoption timeline\n```\n\n### Example 4: Research Paper\n```\nUser: /analyze-intel https://arxiv.org/abs/2024.12345 - new approach to claim detection\n\n[Command analyzes]:\n- Fetches and parses paper abstract/content\n- Assesses applicability to product\n- Compares to current approach\n- Evaluates implementation feasibility\n- Recommends whether to investigate or adopt\n```\n\n## Decision Tree\n\n```\nIs the information directly about our product/users/market?\n Yes  High baseline relevance, proceed with full analysis\n No  Is it about adjacent space or enabling technology?\n     Yes  Moderate baseline, assess indirect implications\n     No  Low baseline, quick scan for any connection\n         Connection found  Elevate to moderate, analyze\n         No connection  Archive with minimal analysis\n```\n\n## Integration with Product Process\n\nAfter analysis, consider:\n\n- **If high relevance:** Add to next standup/planning discussion\n- **If affects roadmap:** Update backlog, use [/prioritize-backlog](prioritize-backlog.md)\n- **If raises questions:** Use [/search-user-research](search-user-research.md) for context\n- **If new feature idea:** Use [/analyze-feature-request](analyze-feature-request.md)\n- **If validated need:** Use [/create-prd](create-prd.md) to document\n\n## Quality Checklist\n\nBefore finalizing analysis:\n- [ ] Source credibility assessed\n- [ ] Information recency noted\n- [ ] Current product context considered\n- [ ] All relevance dimensions scored\n- [ ] Implications (opportunities + threats) identified\n- [ ] Clear action recommendation provided\n- [ ] Owner/stakeholder identified if action needed\n- [ ] Related initiatives or workstreams noted\n\n## Common Pitfalls\n\n1. **Overreacting to noise**\n   - Solution: Assess source credibility and sample size\n\n2. **Underreacting to weak signals**\n   - Solution: Look for patterns across multiple signals\n\n3. **Competitor obsession**\n   - Solution: Always bring back to user value\n\n4. **Analysis paralysis**\n   - Solution: Time-box analysis, make a call\n\n5. **Ignoring timing**\n   - Solution: Explicitly assess urgency vs. importance\n\n## Related Commands\n\n- [/analyze-feature-request](analyze-feature-request.md)  Deep dive on feature ideas\n- [/prioritize-backlog](prioritize-backlog.md)  Reprioritize if needed\n- [/search-user-research](search-user-research.md)  Find supporting context\n- [/create-prd](create-prd.md)  Document if action needed"
              },
              {
                "name": "/create-prd",
                "description": "Generate a comprehensive PRD for a feature or product initiative",
                "path": "plugins/kjgarza-product/commands/create-prd.md",
                "frontmatter": {
                  "description": "Generate a comprehensive PRD for a feature or product initiative",
                  "argument-hint": [
                    "feature or product name"
                  ]
                },
                "content": "# Create Product Requirements Document (PRD)\n\nGenerate a comprehensive PRD for: **$ARGUMENTS**\n\n## Your Task\n\nCreate a complete Product Requirements Document for the feature/product above. Follow the structure below to produce a PRD that incorporates user research, technical considerations, success metrics, and a clear execution plan.\n\n## Steps\n\n1. **Gather Requirements for \"$ARGUMENTS\"**\n   - Interview stakeholders about feature goals\n   - Search Dovetail for relevant user research\n   - Review Coda for related product documentation\n   - Identify technical constraints and dependencies\n\n2. **Define Problem & Opportunity**\n   - Articulate the user problem or opportunity\n   - Apply Jobs-to-be-Done framework to understand user's underlying goal\n   - Apply Double Diamond process: ensure problem is well-defined before jumping to solution\n   - Quantify market opportunity\n   - Define success criteria\n   \n   **Reference**: See [product-frameworks](../skills/product-frameworks/) skill for:\n   - Jobs-to-be-Done framing ([requirements.md](../skills/product-frameworks/references/requirements.md))\n   - Double Diamond process ([design-processes.md](../skills/product-frameworks/references/design-processes.md))\n\n3. **Specify Solution**\n   - Detail functional requirements\n   - Define user stories and acceptance criteria\n   - Create user flow diagrams (if needed)\n   - Specify edge cases and error states\n\n4. **Plan Execution**\n   - Break down into phases/milestones\n   - Identify dependencies and risks\n   - Estimate timeline and resources\n   - Define launch criteria\n\n5. **Define Metrics**\n   - Specify success metrics (North Star, KPIs)\n   - Define measurement approach\n   - Set targets and goals\n   - Plan experimentation strategy\n\n## PRD Structure\n\nUse the comprehensive PRD template from the [product-frameworks](../skills/product-frameworks/) skill ([requirements.md](../skills/product-frameworks/references/requirements.md)).\n\n### 1. Overview\n- **Title**: Feature/Product name\n- **Owner**: PM responsible\n- **Status**: Draft/Review/Approved\n- **Last updated**: Date\n- **Stakeholders**: Key people involved\n\n### 2. Problem Statement\n- What user problem are we solving?\n- What job is the user trying to do? (Jobs-to-be-Done)\n- Why is this important now?\n- What happens if we don't solve this?\n- User research evidence (link to Dovetail)\n\n### 3. Goals & Success Metrics\n- Business objectives\n- User value objectives\n- Key metrics (with targets)\n- How we'll measure success\n- North Star metric (if applicable)\n\n### 4. User Stories & Requirements\n- User personas affected\n- User stories in standard format:\n  - \"As a [user role], I want [goal] so that [benefit]\"\n  - Include acceptance criteria for each story\n- Functional requirements\n- Non-functional requirements (performance, security, etc.)\n- MoSCoW prioritization (Must/Should/Could/Won't have)\n- Out of scope (what we're NOT building)\n\n**Reference**: See [requirements.md](../skills/product-frameworks/references/requirements.md) for:\n- Detailed PRD structure guidance\n- User story format and examples\n- Acceptance criteria best practices\n\n### 5. User Experience\n- User flows (key paths)\n- Wireframes or mockups (if available)\n- Edge cases and error handling\n- Accessibility requirements\n\n### 6. Technical Considerations\n- Architecture overview\n- APIs or integrations needed\n- Data models\n- Performance requirements\n- Security considerations\n\n### 7. Dependencies & Risks\n- Internal dependencies (teams, systems)\n- External dependencies (vendors, partners)\n- Technical risks\n- Business risks\n- Mitigation strategies\n\n### 8. Timeline & Phases\n- Phase 1: MVP (minimum viable product)\n- Phase 2: Enhancements\n- Phase 3: Scale\n- Key milestones and dates\n\n### 9. Launch Plan\n- Beta testing approach\n- Rollout strategy (% of users)\n- Communication plan\n- Support readiness\n\n### 10. Open Questions\n- Unresolved decisions\n- Items needing research\n- Assumptions to validate\n\n### 11. Appendices (Optional)\n- Market research\n- Customer feedback snippets\n- Technical diagrams\n- Competitive analysis\n\n## Framework References\n\nThe PRD structure follows industry best practices. For detailed guidance:\n- **Complete PRD template**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **User story format**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **Jobs-to-be-Done framing**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **Double Diamond context**: [design-processes.md](../skills/product-frameworks/references/design-processes.md)\n\n## Output\n\nThe command will:\n1. Create a new Markdown file for the PRD\n2. Populate it with the structured content\n3. Link to relevant Dovetail insights\n4. Suggest saving to Coda for team collaboration\n5. Generate a checklist for PRD review\n\n## Examples\n\n### Example 1: New Feature PRD\n```\nUser: \"Create a PRD for the bulk export feature we discussed\"\n\n[Command generates]:\n- Searches Dovetail for export-related research\n- Checks Coda for related docs\n- Creates structured PRD with all sections\n- Includes RICE score from previous analysis\n```\n\n### Example 2: Product Enhancement PRD\n```\nUser: \"We need a PRD for improving search performance\"\n\n[Command generates]:\n- Reviews user feedback about search\n- Documents performance requirements\n- Creates phased rollout plan\n- Defines success metrics\n```\n\n## Review Checklist\n\nBefore finalizing, verify:\n- [ ] Problem statement is clear and evidence-based\n- [ ] Success metrics are specific and measurable\n- [ ] User stories cover all key scenarios\n- [ ] Technical feasibility confirmed with engineering\n- [ ] Dependencies and risks identified\n- [ ] Timeline is realistic\n- [ ] Launch plan is detailed\n- [ ] Stakeholders reviewed and approved\n\n## Best Practices\n\n- Start with \"why\" before \"what\"\n- Ground requirements in user research\n- Be specific about success metrics\n- Include both happy paths and edge cases\n- Make assumptions explicit\n- Keep it living document (update as you learn)\n- Use visuals where helpful (flows, mockups)\n- Get cross-functional review (eng, design, data)\n\n## Related Commands\n\n- [/analyze-feature-request](analyze-feature-request.md) - Before creating PRD\n- `/update-roadmap` - After PRD approval\n- [/search-user-research](search-user-research.md) - Find supporting research"
              },
              {
                "name": "/create-user-stories",
                "description": "Transform requirements into user stories with acceptance criteria",
                "path": "plugins/kjgarza-product/commands/create-user-stories.md",
                "frontmatter": {
                  "description": "Transform requirements into user stories with acceptance criteria",
                  "argument-hint": [
                    "feature or requirement description"
                  ]
                },
                "content": "# Create User Stories\n\nTransform the following requirement into well-formed user stories: **$ARGUMENTS**\n\n## Your Task\n\nAnalyze the requirement above and create comprehensive user stories that capture user needs, goals, and value. Follow the framework below to produce clear, actionable stories with proper acceptance criteria.\n\n## Standard User Story Format\n\n```\nAs a [user role], I want [goal/desire] so that [benefit/value].\n```\n\nThis format encapsulates:\n- **Who**: The type of user (role or persona)\n- **What**: The desired action or feature\n- **Why**: The reason or value behind the request\n\n## Steps\n\n### 1. Understand the Requirement\n\nBased on \"$ARGUMENTS\", determine:\n- What problem are we solving?\n- Who is this for?\n- What job is the user trying to do? (Jobs-to-be-Done)\n- What value does this provide?\n- What constraints or context apply?\n\n**Gather context**:\n- Review feature request or PRD\n- Search Dovetail for user research\n- Identify affected user personas\n- Understand user workflows\n\n### 2. Identify User Roles\n\nDefine who the stories are for. Use established personas when available.\n\n**Examples**:\n- New user vs. power user\n- Administrator vs. team member\n- Free tier vs. paid customer\n- Mobile user vs. desktop user\n- Internal user vs. external customer\n\n**Be specific**: \"Marketing manager\" is better than \"user\"\n\n### 3. Write User Stories\n\nFor each piece of functionality, write a story following the template.\n\n**Example 1**:\n```\nAs a power user, I want a spell-checker so that I don't have to \nworry about spelling mistakes.\n```\n\n**Example 2**:\n```\nAs an administrator, I want to bulk-assign permissions so that I can \nonboard new team members quickly without clicking through each user.\n```\n\n**Example 3**:\n```\nAs a mobile user, I want offline access to my recent documents so that \nI can review work during my commute without internet.\n```\n\n### 4. Write Acceptance Criteria\n\nFor each story, define conditions of satisfaction  how we know it's done correctly.\n\n**Format Options**:\n\n**Checklist Format**:\n```\nUser Story: As a power user, I want a spell-checker\n\nAcceptance Criteria:\n- Spell-checker underlines misspelled words in red\n- Right-click on underlined word shows correction suggestions\n- User can add words to personal dictionary\n- Spell-check works in real-time as user types\n- Works for English, Spanish, and French languages\n```\n\n**Given/When/Then Format** (Behavior-Driven Development style):\n```\nUser Story: As an administrator, I want to bulk-assign permissions\n\nAcceptance Criteria:\n- Given: I have selected 10 users\n  When: I choose \"Assign permissions\" and select \"Editor\" role\n  Then: All 10 users receive Editor permissions\n  \n- Given: Bulk operation is in progress\n  When: The operation is processing\n  Then: I see a progress indicator\n  \n- Given: Bulk operation completes\n  When: All users are updated\n  Then: I receive a success confirmation with count\n```\n\n### 5. Break Down Epics\n\nIf a story is too large to complete in one sprint, it's an \"epic\" that should be broken down.\n\n**Epic Example**:\n```\nAs a user, I want collaboration features so that my team can work together.\n```\n\nThis is too broad. Break it down:\n\n**Story 1**:\n```\nAs a team member, I want to share documents with colleagues so that \nwe can review work together.\n```\n\n**Story 2**:\n```\nAs a document owner, I want to control who can edit vs. view so that \nI can protect sensitive content.\n```\n\n**Story 3**:\n```\nAs a collaborator, I want to see who's currently viewing a document \nso that I know when to coordinate with them.\n```\n\n**Story 4**:\n```\nAs a team member, I want real-time updates when others edit so that \nI see changes immediately.\n```\n\n### 6. Prioritize Stories\n\nUse prioritization frameworks to order stories:\n\n**MoSCoW Method**:\n- **Must have**: Critical for launch\n- **Should have**: Important but not critical\n- **Could have**: Nice to have\n- **Won't have**: Out of scope\n\n**Value to User**:\n- Which stories deliver the most user value?\n- Which solve the biggest pain points?\n\n**Dependencies**:\n- Which stories must be built first?\n- What's the logical implementation order?\n\n**Reference**: See [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n\n## User Story Guidelines\n\n### Keep Stories Small and Independent\n\n**INVEST Criteria**:\n- **Independent**: Can be implemented without depending on other stories\n- **Negotiable**: Details can be discussed and refined\n- **Valuable**: Provides clear value to users\n- **Estimable**: Team can estimate effort\n- **Small**: Completable in one sprint\n- **Testable**: Can verify it works correctly\n\n### The \"So That\" Clause is Critical\n\nIf you can't articulate the benefit, question whether the feature is justified.\n\n**Without \"so that\"**: \"As a user, I want a dashboard\"\n**With \"so that\"**: \"As a user, I want a dashboard so that I can see my key metrics at a glance without navigating through multiple pages\"\n\nThe benefit clarifies value and helps with prioritization.\n\n### Avoid Solution-Focused Stories\n\nFocus on user needs, not implementation details.\n\n**Bad (solution-focused)**: \n\"As a user, I want a dropdown menu for selecting categories\"\n\n**Good (need-focused)**: \n\"As a user, I want to categorize my documents so that I can organize and find them easily\"\n\nThe design team can then decide if a dropdown, tags, folders, or another UI is best.\n\n### Use Active Voice\n\nWrite clearly and directly.\n\n**Good**: \"As a user, I want to export data\"\n**Avoid**: \"Data should be exportable by the user\"\n\n## Jobs-to-be-Done Integration\n\nFrame stories around the job users are trying to accomplish.\n\n**Traditional approach**: \"User wants a drill\"\n\n**Jobs-to-be-Done reframe**: \n\"User needs to hang a picture\" (which requires a hole in the wall)\n\n**Better user story**:\n```\nAs a homeowner, I want to make precise holes in walls so that \nI can hang pictures securely without damaging the wall.\n```\n\nThis opens up solution space (drill, nail gun, adhesive hooks, etc.).\n\n**Application**:\n- Understand the underlying job\n- Consider functional, emotional, and social dimensions\n- Write stories that focus on the job outcome\n\n**Reference**: See [requirements.md](../skills/product-frameworks/references/requirements.md) for Jobs-to-be-Done framework.\n\n## Output Format\n\n### User Story Card\n\n```markdown\n### Story: [Short title]\n\n**As a** [user role]\n**I want** [goal/action]\n**So that** [benefit/value]\n\n**Acceptance Criteria:**\n- [ ] Criterion 1\n- [ ] Criterion 2\n- [ ] Criterion 3\n\n**Priority:** [Must/Should/Could/Won't] or [P0/P1/P2/P3]\n\n**Estimate:** [Story points or t-shirt size]\n\n**Dependencies:** [Other stories this depends on, if any]\n\n**Notes:** [Additional context, constraints, or considerations]\n```\n\n### Story Set for a Feature\n\nWhen creating stories for a feature, provide:\n\n**Feature**: [Feature name]\n\n**User Need**: [Underlying user problem or job-to-be-done]\n\n**Stories**:\n\n1. **Story Title** (Must have)\n   - As a [role], I want [goal] so that [benefit]\n   - Acceptance: [criteria]\n\n2. **Story Title** (Should have)\n   - As a [role], I want [goal] so that [benefit]\n   - Acceptance: [criteria]\n\n3. **Story Title** (Could have)\n   - As a [role], I want [goal] so that [benefit]\n   - Acceptance: [criteria]\n\n**Out of Scope**:\n- [What we're NOT doing in this iteration]\n\n## Examples\n\n### Example 1: Export Feature Stories\n\n**Feature**: Bulk document export\n\n**User Need**: Users need to download multiple documents at once for offline work or compliance reporting.\n\n**Stories**:\n\n1. **Multi-select documents** (Must have)\n   ```\n   As an administrator, I want to select multiple documents at once \n   so that I can export them in bulk without clicking each individually.\n   \n   Acceptance Criteria:\n   - User can select documents via checkboxes\n   - \"Select all\" option available\n   - Can filter list and select filtered subset\n   - Selected count displays: \"N documents selected\"\n   - Max 1000 documents per selection\n   ```\n\n2. **Batch export to ZIP** (Must have)\n   ```\n   As an administrator, I want to export selected documents as a single ZIP \n   so that I can download everything at once.\n   \n   Acceptance Criteria:\n   - Given: User has selected documents\n     When: User clicks \"Export selected\"\n     Then: System generates ZIP file with all documents\n   - ZIP preserves folder structure\n   - ZIP includes metadata manifest\n   - Progress indicator shows during generation\n   - Export completes within 2 minutes for 100 documents\n   ```\n\n3. **Export with filters** (Should have)\n   ```\n   As an administrator, I want to export documents matching specific criteria \n   so that I can get exactly what I need for reports.\n   \n   Acceptance Criteria:\n   - Can filter by date range, category, owner before export\n   - Export includes only filtered documents\n   - Filter state is clear in export manifest\n   ```\n\n4. **Schedule automated exports** (Could have)\n   ```\n   As an administrator, I want to schedule recurring exports \n   so that I can automate monthly compliance reporting.\n   \n   Acceptance Criteria:\n   - Can set export schedule (daily/weekly/monthly)\n   - Can define export criteria and recipient email\n   - Receives notification when export completes\n   ```\n\n**Out of Scope for v1**:\n- Export to formats other than ZIP\n- Cloud storage integration (Google Drive, Dropbox)\n- Exports >1000 documents\n\n### Example 2: Onboarding Flow Stories\n\n**Feature**: First-time user onboarding\n\n**User Need**: New users need to understand the product and get to value quickly.\n\n**Stories**:\n\n1. **Welcome screen** (Must have)\n   ```\n   As a new user, I want to see a welcome message explaining the product \n   so that I understand what it does before diving in.\n   \n   Acceptance Criteria:\n   - Welcome screen appears on first login only\n   - Clear 1-sentence value proposition\n   - \"Get started\" CTA button\n   - \"Skip\" option for returning users who cleared cookies\n   ```\n\n2. **Quick setup wizard** (Must have)\n   ```\n   As a new user, I want a guided setup process \n   so that I can configure essential settings without feeling lost.\n   \n   Acceptance Criteria:\n   - 3-step wizard: Profile  Preferences  First action\n   - Progress indicator shows \"Step 2 of 3\"\n   - Can go back to previous step\n   - Can skip and complete later\n   - Takes <2 minutes to complete\n   ```\n\n3. **Sample data** (Should have)\n   ```\n   As a new user, I want to see example data in the product \n   so that I can understand how it works before adding my own.\n   \n   Acceptance Criteria:\n   - Sample project with realistic data loads on first use\n   - Clear indication it's sample data (\"Try editing this example\")\n   - Easy to delete sample and start fresh\n   - Sample showcases key features\n   ```\n\n## Best Practices\n\n### Do:\n- Write from user perspective, not system perspective\n- Include the \"so that\" clause to clarify value\n- Keep stories small and completable in one sprint\n- Write acceptance criteria before development starts\n- Collaborate with design and engineering on stories\n- Refine stories during backlog grooming\n- Use consistent format across all stories\n\n### Don't:\n- Write stories as technical tasks (\"Refactor database schema\")\n- Make stories too large (break down epics)\n- Skip the \"why\" (the benefit clause)\n- Write acceptance criteria that are vague or untestable\n- Create stories without consulting user research\n- Assume one story format fits all situations\n\n## Common Pitfalls\n\n**Too technical**: \n\"As a developer, I want to implement caching\"\n Technical task, not user story. Put in engineering backlog.\n\n**Too vague**: \n\"As a user, I want the app to be fast\"\n Not actionable. Be specific about what should be fast and measurable criteria.\n\n**No clear value**: \n\"As a user, I want a settings page\"\n Missing the \"so that\". Why do they need it? What settings matter?\n\n**Solution, not need**: \n\"As a user, I want a carousel on the homepage\"\n Prescribes solution. Better: \"As a user, I want to discover new content easily...\"\n\n## Story Writing Checklist\n\nBefore finalizing a story:\n- [ ] Follows \"As a/I want/So that\" format\n- [ ] User role is specific (not just \"user\")\n- [ ] Goal is clear and actionable\n- [ ] Benefit explains the \"why\"\n- [ ] Acceptance criteria are testable\n- [ ] Story is small enough for one sprint\n- [ ] Priority is assigned\n- [ ] Dependencies identified\n- [ ] Team reviewed and understood the story\n\n## Related Commands\n\n- [/create-prd](create-prd.md) - Stories feed into PRD documentation\n- [/prioritize-backlog](prioritize-backlog.md) - Use stories in backlog prioritization\n- [/analyze-feature-request](analyze-feature-request.md) - Validate need before creating stories\n\n## Framework References\n\nFor comprehensive guidance on user stories and requirements:\n- **User story format and examples**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **Jobs-to-be-Done framework**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **PRD structure and requirements**: [requirements.md](../skills/product-frameworks/references/requirements.md)\n- **MoSCoW prioritization**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)"
              },
              {
                "name": "/facilitate-design-critique",
                "description": "Facilitate a structured design critique session with constructive feedback",
                "path": "plugins/kjgarza-product/commands/facilitate-design-critique.md",
                "frontmatter": {
                  "description": "Facilitate a structured design critique session with constructive feedback",
                  "argument-hint": [
                    "design artifact or link"
                  ]
                },
                "content": "# Facilitate Design Critique\n\nProvide structured design critique for: **$ARGUMENTS**\n\n## Your Task\n\nReview the design artifact above and provide constructive feedback using established critique frameworks. Structure your feedback to improve design quality while focusing on goals and user needs.\n\n## Purpose\n\nDesign critique is a structured process to collaboratively improve design work by providing feedback tied to goals and user needs. The aim is to enhance the design, not judge the designer.\n\n## Steps\n\n### 1. Pre-Critique Preparation\n\n**Define Scope for \"$ARGUMENTS\"**:\n- What artifacts are being critiqued? (wireframes, mockups, prototype, flows)\n- What stage is the design in? (early exploration, near-final)\n- What specific feedback questions does the designer have?\n\n**Set Objectives**:\n- What user goals should this design achieve?\n- What business goals?\n- What constraints or requirements apply?\n\n**Share Materials**:\n- Distribute design artifacts in advance\n- Include context (user research, requirements, design rationale)\n- Give reviewers time to examine before the session\n\n**Invite Participants**:\n- Include cross-functional perspectives (engineering, product, design, QA)\n- Aim for 4-8 participants (enough diversity, not too crowded)\n- Ensure all understand critique guidelines\n\n### 2. Session Structure (45-60 minutes)\n\n**Designer Presents (5-10 minutes)**:\n- Context: problem being solved, user needs, goals\n- Design rationale: key decisions and trade-offs\n- Specific questions for feedback\n\n**Clarifying Questions (5 minutes)**:\n- Ask questions to understand, not to critique yet\n- \"What user segment is this for?\"\n- \"What's the expected flow into this screen?\"\n\n**Feedback Round (20-30 minutes)**:\n- Use structured facilitation method (see below)\n- Focus on objectives and goals\n- Balance positives and areas for improvement\n- Tie feedback to user/business impact\n\n**Designer Responds (5 minutes)**:\n- Designer can clarify intent\n- Ask follow-up questions\n- Note action items\n\n**Wrap-up (5 minutes)**:\n- Summarize key feedback themes\n- Identify action items with owners\n- Decide what will change vs. what won't\n- Thank participants\n\n### 3. Document and Follow-up\n\n**Capture**:\n- Key feedback points\n- Action items with owners\n- Decisions made\n- Open questions\n\n**Designer Iterates**:\n- Incorporate feedback\n- Make design changes\n- Prepare for next review if needed\n\n## Facilitation Methods\n\nChoose a method that fits your team culture and session goals.\n\n### Round-Robin\nEach person gives feedback in turn. Ensures all voices are heard.\n\n**Process**:\n1. Go around the table/video call\n2. Each person shares 1-3 observations\n3. Can pass if nothing new to add\n4. Multiple rounds if needed\n\n### I Like, I Wish, What If\nStructured feedback format that balances positive and constructive.\n\n**Format**:\n- **I like**: What's working well\n- **I wish**: What could be improved (framed as aspiration)\n- **What if**: Suggestions for exploration\n\n**Example**:\n- \"I like how the navigation is clean and minimal\"\n- \"I wish the key CTA was more prominent\"\n- \"What if we tried a contrasting color for the primary action?\"\n\n### Goal-Based Critique\nFrame all feedback relative to specific goals.\n\n**Process**:\n1. List the design goals\n2. For each goal, discuss: Does the design achieve it?\n3. Where gaps exist, suggest improvements\n4. Ignore issues unrelated to stated goals (out of scope)\n\n## Critique Ground Rules\n\nEstablish these rules at the start of each session:\n\n### 1. Tie Feedback to Goals\nAlways relate comments to whether the design meets user needs and business goals.\n\n**Good**: \"This element might not align with the goal of quick signup, because the extra step adds friction\"\n**Bad**: \"I don't like this color\"\n\n### 2. Be Specific and Candid\nPoint out exactly what and where you see an issue or something good.\n\n**Good**: \"The contrast between text and background measures 2.8:1, which doesn't meet WCAG AA standards (4.5:1 required)\"\n**Bad**: \"The colors don't work\"\n\n### 3. Balance Positives and Negatives\nNote what is working well, not only problems.\n\n**Example**: \"The information hierarchy is clear (positive). The submit button is small and might be missed (issue).\"\n\n### 4. Problems Before Solutions\nIdentify what isn't working and why, then suggest fixes.\n\n**Approach**:\n1. \"Users might overlook the Save button\" (identify problem)\n2. \"It's at the bottom and blends in\" (diagnose why)\n3. \"What if we made it more prominent or moved it to the header?\" (suggest)\n\n### 5. Suggest, Don't Mandate\nPhrase ideas as suggestions (\"What if...\") not commands (\"Change this to...\").\n\n**Good**: \"What if we tried making the CTA button larger?\"\n**Bad**: \"Make that button bigger\"\n\n### 6. Critique the Work, Not the Person\nFocus on the design artifact, never on the designer's abilities.\n\n**Good**: \"This button placement might make it hard for users to find\"\n**Bad**: \"You should have known to put the button there\"\n\n## Using Frameworks in Critique\n\nReference established design principles and frameworks to ground feedback objectively:\n\n### Design Principles\n\n**Hick's Law**: \"This screen presents 8 equally-weighted actions, which might increase decision time. Could we highlight the primary action?\"\n\n**Fitts's Law**: \"The submit button is small and far from the form fields. Making it larger and closer might improve completion rate.\"\n\n**Affordances**: \"These text elements look like links but aren't clickable. That might confuse users.\"\n\n**Feedback principle**: \"When users click save, there's no visible confirmation. Adding feedback would reassure users.\"\n\n**Reference**: See [design-principles.md](../skills/product-frameworks/references/design-principles.md) for detailed heuristics.\n\n### Product Frameworks\n\n**Jobs-to-be-Done**: \"The job users are trying to do is quickly find articles. This design requires 3 clicks to start a search. Could we make search more prominent?\"\n\n**User-Centered Design**: \"Have we tested this flow with actual users? It's unclear if this matches their mental model.\"\n\n## Anti-Patterns to Avoid\n\n### Design by Committee\n**Problem**: Critique becomes design session where everyone redesigns together.\n**Solution**: Identify problems and directions, but let designer synthesize.\n\n### Personal Preference Disguised as Feedback\n**Problem**: \"I don't like blue\" without tying to goals or evidence.\n**Solution**: Require feedback to reference goals, user needs, or design principles.\n\n### Too Late Feedback\n**Problem**: Fundamental concerns raised at final review when too late to pivot.\n**Solution**: Critique early and often, especially for major decisions.\n\n### Vague Feedback\n**Problem**: \"Make it pop\", \"Needs more polish\", \"Something feels off\"\n**Solution**: Push for specificity. \"What exactly feels off? Can you point to it?\"\n\n## Output Format\n\nAt the end of the session, summarize:\n\n### Feedback Summary\n- **What's working well**: [List positives]\n- **Areas for improvement**: [List issues with severity]\n- **Key themes**: [Patterns across feedback]\n\n### Action Items\n- [Action item 1] - Owner: [Name] - Due: [Date]\n- [Action item 2] - Owner: [Name] - Due: [Date]\n\n### Decisions\n- [Decision 1]: [What was decided and why]\n- [Decision 2]: [What was decided and why]\n\n### Open Questions\n- [Question 1]: [What needs to be resolved]\n- [Question 2]: [What needs to be resolved]\n\n## Examples\n\n### Example 1: Early Wireframe Critique\n```\nContext: Mobile app onboarding flow, early wireframes\n\nDesigner presents: 3-step onboarding concept\nKey question: \"Is the flow intuitive for first-time users?\"\n\nFeedback (I Like, I Wish, What If):\n- I like the progressive disclosure approach\n- I wish the value proposition was clearer in step 1\n- What if we added illustrations to make each step more engaging?\n\nAction items:\n- Designer: Add clearer value prop to step 1\n- PM: Validate step sequence with user research\n- Next critique: High-fidelity designs with updated flow\n```\n\n### Example 2: High-Fidelity UI Critique\n```\nContext: Dashboard redesign, near-final mockups\n\nGoal-based critique:\nGoal 1: Help users quickly identify key metrics\n-  Card layout makes scanning easy\n-  Concern: Too many metrics shown at once (Hick's Law)\n- Suggestion: Let users customize which metrics are visible\n\nGoal 2: Improve mobile experience\n-  Responsive breakpoints work well\n-  Touch targets on small screens below 44x44px (Fitts's Law)\n- Suggestion: Increase button sizes on mobile\n\nAction items:\n- Designer: Increase mobile touch targets to 44x44px minimum\n- Engineering: Assess feasibility of customizable metrics\n- Next: Usability test with 5 users\n```\n\n## Best Practices\n\n- **Critique early and often**: Don't wait until designs are \"perfect\"\n- **Time-box discussions**: Keep sessions focused and on schedule\n- **Stay objective**: Use frameworks and principles to ground feedback\n- **Create safety**: Establish that all feedback is about improving the work\n- **Document decisions**: Capture rationale for future reference\n- **Follow through**: Ensure action items are completed\n\n## Facilitation Checklist\n\n**Before session**:\n- [ ] Scope and objectives defined\n- [ ] Materials shared in advance\n- [ ] Diverse perspectives invited\n- [ ] Time allocated (45-60 min)\n- [ ] Ground rules ready to review\n\n**During session**:\n- [ ] Designer presents context and questions\n- [ ] Clarifying questions before feedback\n- [ ] Feedback tied to goals\n- [ ] Specific observations, not vague opinions\n- [ ] Balance of positives and improvements\n- [ ] Solutions offered as suggestions\n- [ ] Respectful, professional tone maintained\n\n**After session**:\n- [ ] Action items documented\n- [ ] Decisions captured\n- [ ] Next steps clear\n- [ ] Designer has autonomy to synthesize feedback\n\n## Related Commands\n\n- [/plan-usability-test](plan-usability-test.md) - Validate design decisions with users\n- [/create-user-stories](create-user-stories.md) - Ensure designs meet user needs\n\n## Framework References\n\nFor comprehensive critique guidelines:\n- **Complete critique practices**: [critique-guidelines.md](../skills/product-frameworks/references/critique-guidelines.md)\n- **Design principles for feedback**: [design-principles.md](../skills/product-frameworks/references/design-principles.md)\n- **User-centered design philosophy**: [design-processes.md](../skills/product-frameworks/references/design-processes.md)"
              },
              {
                "name": "/plan-usability-test",
                "description": "Plan a structured usability test for validating designs with users",
                "path": "plugins/kjgarza-product/commands/plan-usability-test.md",
                "frontmatter": {
                  "description": "Plan a structured usability test for validating designs with users",
                  "argument-hint": [
                    "feature or design to test"
                  ]
                },
                "content": "# Plan Usability Test\n\nCreate a usability test plan for: **$ARGUMENTS**\n\n## Your Task\n\nDesign a comprehensive usability test for the feature/design above. Produce a complete test plan including goals, participant criteria, task scenarios, and success metrics that will validate design decisions with real users.\n\n## Purpose\n\nUsability testing involves observing real users as they interact with your product to identify usability problems, discover improvement opportunities, and validate design decisions. Proper planning is essential for effective testing.\n\n## Steps\n\n### 1. Define Test Goals\n\nBased on \"$ARGUMENTS\", articulate what questions the usability test should answer. Prioritize top goals so the test stays focused.\n\n**Good goals**:\n- \"Can first-time users find the account settings?\"\n- \"Is the checkout process easy to complete?\"\n- \"Do users understand the dashboard metrics?\"\n- \"Can users successfully complete a document export?\"\n\n**Poor goals**:\n- \"Test the whole app\" (too broad)\n- \"See if users like it\" (too vague)\n- \"Make sure it works\" (that's QA testing)\n\n**Limit to 3-5 key goals** per session to ensure adequate time per task.\n\n### 2. Choose Test Format\n\n#### Moderated vs. Unmoderated\n\n**Moderated Tests**:\n- Facilitator present (in-person or video call)\n- Can probe and ask follow-up questions\n- Yields deeper insights\n- More time-intensive, smaller sample\n\n**When to use**: Early validation, complex workflows, exploring \"why\" behind behavior\n\n**Unmoderated Tests**:\n- Users complete tasks independently\n- Faster data collection, more users\n- Lower cost per participant\n- Can't probe in real-time\n\n**When to use**: Specific task validation, quantitative metrics, large samples\n\n#### In-Person vs. Remote\n\n**In-Person**:\n- Observe body language and environment\n- Easier rapport building\n- Limited geographic reach\n\n**When to use**: Complex products, users needing help, observing physical context\n\n**Remote**:\n- Test users anywhere\n- Easier scheduling\n- Users in natural environment\n- Requires reliable technology\n\n**When to use**: Distributed users, budget constraints, web/mobile products\n\n#### Lab vs. Field\n\n**Lab/Controlled**:\n- Consistent setup\n- Fewer variables\n- Convenient for facilitator\n\n**When to use**: Most usability testing\n\n**Field Testing**:\n- Users in actual context\n- Reveals context-specific issues\n- Harder to control\n\n**When to use**: Context-dependent products (mobile apps in stores, field tools)\n\n### 3. Recruit Participants\n\n#### Define Target Audience\nIdentify characteristics matching your user base (use personas).\n\n**Screening criteria examples**:\n- Role/job title\n- Experience level with similar products\n- Frequency of use\n- Demographics (age, location, tech-savviness)\n- Specific behaviors or needs\n\n#### How Many Participants?\n\n**Nielsen's recommendation**: 5 users per distinct user group uncover ~85% of usability issues.\n\n**Considerations**:\n- **5 users**: Good for qualitative insight, finding major issues\n- **8-10 users**: Better coverage, more confidence\n- **Multiple segments**: 5 users per segment (e.g., 5 new + 5 power users = 10 total)\n\n#### Recruitment Methods\n- Customer database or CRM\n- User research panels (UserTesting, Respondent.io)\n- Social media and forums\n- Website intercept\n- Referrals\n\n#### Create Screener Survey\nShort questionnaire to filter candidates matching criteria.\n\n**Example questions**:\n- \"How often do you use project management software?\" (Daily/Weekly/Monthly/Never)\n- \"What tools do you currently use for [task]?\"\n- \"Have you participated in user research in the past 6 months?\" (to get fresh perspectives)\n\n### 4. Prepare Task Scenarios\n\nWrite realistic tasks derived from test goals. Present as scenarios or goals, not step-by-step instructions.\n\n#### Task Writing Principles\n\n**Good task**: \n\"You want to buy a specific book and have it delivered by Friday. Use this website to do that.\"\n- Goal-oriented\n- Realistic context\n- No clues about how\n\n**Bad task**: \n\"Click 'Search', enter a book title, click 'Add to cart', then checkout\"\n- Prescriptive (tells exactly what to do)\n- Tests instruction-following, not usability\n\n#### Task Types\n\n**Exploratory tasks**: \n\"Imagine you just signed up. Explore the app and share your first impressions.\"\n\n**Specific tasks**: \n\"Find how much storage space you're currently using.\"\n\"Change your password.\"\n\n**Comparison tasks**: \n\"Which of these two checkout flows do you find clearer?\"\n\n#### Task Quantity\n- **3-5 tasks** for 30-minute session\n- **5-8 tasks** for 60-minute session\n- Prioritize most critical tasks first\n\n### 5. Pilot Test\n\nConduct trial run with colleague or one user before real sessions.\n\n**Validates**:\n- Tasks are clear and achievable\n- Prototype/system works\n- Session timing is appropriate\n- Screener identifies right participants\n\n**Adjust based on pilot**:\n- Reword confusing tasks\n- Fix technical issues\n- Adjust time allocation\n- Refine facilitation\n\n### 6. Setup Logistics\n\n#### For In-Person Tests\n- Reserve quiet, private space\n- Set up recording equipment (if documenting)\n- Test all hardware/software beforehand\n- Prepare comfortable seating, offer water\n\n#### For Remote Tests\n- Choose reliable platform (Zoom, Teams, UserTesting)\n- Verify prototype/site is accessible\n- Send connection details in advance\n- Have backup communication method\n\n#### General Setup\n- Prepare consent forms (if recording)\n- Set up note-taking method\n- Optional: Second person to take notes\n- Prepare compensation (if applicable)\n\n## Session Structure (60 minutes typical)\n\n### Introduction (5 minutes)\n- Make participant comfortable\n- Introduce yourself and purpose\n- \"We're testing the product, not you\"\n- Confirm consent to proceed/record\n- Explain they can stop anytime\n\n### Context Questions (5 minutes)\n- \"Tell me about your current workflow for [task]\"\n- \"Have you used similar apps?\"\n- \"What's most important when [doing task]?\"\n\n### Explain Think-Aloud (2 minutes)\n- \"Please talk me through what you're thinking\"\n- \"If you're looking for something, tell me what\"\n- \"Share any reactions or questions out loud\"\n- Demonstrate with quick example\n\n### Run Tasks (30-40 minutes)\n- Present one scenario at a time\n- Observe without interfering\n- Only intervene if completely stuck\n- Prompt to keep thinking aloud: \"What are you thinking?\"\n- Stay neutral  no hints or reactions\n\n**After each task**:\n- \"How was that experience?\"\n- \"Anything confusing or frustrating?\"\n- \"On a scale of 1-5, how difficult was that?\"\n\n### Post-Task Discussion (5-10 minutes)\n- \"What was the most frustrating part?\"\n- \"What did you like most?\"\n- \"If you could change one thing, what would it be?\"\n- \"Would you use this? Why or why not?\"\n- \"How does this compare to [current tool]?\"\n\n### Closing (2 minutes)\n- Thank participant\n- Explain how input will be used\n- Process compensation\n- Answer their questions\n\n## Metrics to Collect\n\n### Task Success Rate\nDid the user complete the task correctly?\n\n**Levels**:\n- **Success**: Completed correctly, independently\n- **Partial success**: Completed with hints or after errors\n- **Failure**: Could not complete\n\n**Formula**: (# successful tasks) / (total # attempted)\n\n### Time on Task\nHow long to complete successfully?\n\nUseful for comparing designs or tracking improvements.\n\n### Error Rate\nHow many errors did users make?\n\n**Types**: Wrong action, recovery needed, help requested\n\n### Satisfaction Ratings\nAsk users to rate after each task or overall.\n\n**Scales**:\n- Difficulty: 1 (very easy) to 5 (very difficult)\n- Confidence: 1 (not confident) to 5 (very confident)\n- Satisfaction: 1 (very unsatisfied) to 5 (very satisfied)\n\n### System Usability Scale (SUS)\nStandardized 10-question survey for overall usability.\n\n**Score**: 0-100 scale (68 is average)\n\n**When to use**: Post-session for overall assessment\n\n## Test Plan Document\n\nCreate a written test plan including:\n\n### 1. Test Overview\n- Study goals (3-5 questions to answer)\n- What's being tested (product/feature)\n- Test date and duration\n- Facilitator and observers\n\n### 2. Methodology\n- Test format (moderated/unmoderated, remote/in-person)\n- Number of participants\n- Session length\n- Recording approach\n\n### 3. Participants\n- Target audience description\n- Screening criteria\n- Recruitment approach\n- Sample size and segmentation\n\n### 4. Task Scenarios\n- List of tasks with descriptions\n- Expected completion criteria\n- Priority order\n\n### 5. Metrics\n- What will be measured (success rate, time, satisfaction)\n- How data will be collected\n- Success criteria for each task\n\n### 6. Session Protocol\n- Introduction script\n- Think-aloud instructions\n- Task presentation order\n- Post-task questions\n- Closing script\n\n### 7. Logistics\n- Location/platform\n- Equipment needs\n- Participant compensation\n- Team roles (facilitator, note-taker, observers)\n\n## Output Format\n\nWhen planning a usability test, provide:\n\n### Test Plan Summary\n- **Goals**: [3-5 key questions]\n- **Format**: [Moderated/unmoderated, remote/in-person]\n- **Participants**: [N users, segmentation]\n- **Timeline**: [Test dates]\n\n### Task Scenarios\n1. **Task 1**: [Scenario description]\n   - Goal: [What user should accomplish]\n   - Success criteria: [How we know they succeeded]\n   \n2. **Task 2**: [Scenario description]\n   - Goal: [What user should accomplish]\n   - Success criteria: [How we know they succeeded]\n\n### Screening Questions\n- Question 1: [Text]\n- Question 2: [Text]\n- [etc.]\n\n### Metrics to Collect\n- Task success rate (target: >80%)\n- Time on task (target: <2 minutes for key tasks)\n- Error rate (target: <20%)\n- Satisfaction (target: >4/5)\n\n### Logistics Checklist\n- [ ] Participants recruited\n- [ ] Session scheduled\n- [ ] Platform/space confirmed\n- [ ] Recording equipment tested\n- [ ] Consent forms prepared\n- [ ] Note-taking method ready\n- [ ] Compensation arranged\n\n## Examples\n\n### Example 1: E-commerce Checkout Test\n```\nGoal: Validate new checkout flow reduces abandonment\n\nFormat: Remote moderated, 8 participants (4 new, 4 returning customers)\n\nTasks:\n1. \"You want to buy this jacket and have it by Friday. Complete your purchase.\"\n2. \"You realize you need a different size. Update your order before it ships.\"\n3. \"You're not ready to buy. Save the item for later.\"\n\nSuccess Metrics:\n- >90% complete checkout without assistance\n- <2 minutes average time\n- <1 error per user\n- >4/5 satisfaction rating\n\nNext Steps:\n- Recruit via customer panel\n- Test next week (Mon-Wed)\n- Analyze and report findings by Friday\n```\n\n### Example 2: Dashboard Usability Test\n```\nGoal: Determine if users can find and understand key metrics\n\nFormat: In-person unmoderated, 6 users (mixed experience levels)\n\nTasks:\n1. \"Find how many active projects you have.\"\n2. \"Identify which project is closest to deadline.\"\n3. \"Export the last month's activity report.\"\n\nSuccess Metrics:\n- >80% task success\n- <30 seconds to find each metric\n- >3.5/5 confidence rating\n\nInsights Sought:\n- Do users understand metric labels?\n- Is information hierarchy clear?\n- Are export options discoverable?\n```\n\n## Analysis Guidance\n\nAfter testing, analyze findings:\n\n### 1. Compile Data\n- Review notes and recordings\n- Calculate metrics (success rates, times, etc.)\n- Extract quotes and observations\n\n### 2. Identify Patterns\n- Issues occurring across multiple users (high priority)\n- Single-user issues (edge cases)\n- Positive findings (what worked well)\n\n### 3. Severity Rating\n- **Critical**: Prevents completion, affects many users\n- **High**: Significant frustration, affects most users\n- **Medium**: Minor frustration, affects some users\n- **Low**: Cosmetic, affects few users\n\n### 4. Create Report\n- Executive summary with key findings\n- Detailed findings with evidence (quotes, clips)\n- Severity and frequency for each issue\n- Recommendations with action items\n\n**Reference**: See [usability-testing.md](../skills/product-frameworks/references/usability-testing.md) for detailed analysis guidance.\n\n## Best Practices\n\n- **Test early and often**: Don't wait for final designs\n- **Focus on goals**: Every task should answer a specific question\n- **Stay neutral**: Don't lead participants or react to their actions\n- **Let them struggle**: That's where you learn about problems\n- **Document everything**: Take notes, record sessions (with consent)\n- **Act on findings**: Usability testing is only valuable if you iterate\n\n## Common Mistakes to Avoid\n\n- **Leading questions**: \"Isn't this easy?\" biases response\n- **Helping too much**: Guiding users hides usability problems\n- **Testing too late**: Waiting until final design makes fixes expensive\n- **Testing only once**: Need to test  fix  re-test\n- **Ignoring findings**: Conducting tests but not acting wastes effort\n\n## Usability Testing Checklist\n\n**Planning Phase**:\n- [ ] Test goals defined (3-5 key questions)\n- [ ] Format chosen and justified\n- [ ] Target audience defined\n- [ ] Screening criteria created\n- [ ] Task scenarios written\n- [ ] Pilot test completed\n\n**Recruitment Phase**:\n- [ ] 5+ participants per segment recruited\n- [ ] Screener survey deployed\n- [ ] Sessions scheduled\n- [ ] Confirmation emails sent\n- [ ] Compensation arranged\n\n**Preparation Phase**:\n- [ ] Test plan documented\n- [ ] Space/platform configured\n- [ ] Recording equipment tested\n- [ ] Consent forms prepared\n- [ ] Facilitator prepared\n- [ ] Observers briefed\n\n**Ready to Test**:\n- [ ] All logistics confirmed\n- [ ] Backup plans in place\n- [ ] Team roles assigned\n- [ ] Post-test analysis plan ready\n\n## Related Commands\n\n- [/facilitate-design-critique](facilitate-design-critique.md) - Review designs before testing\n- [/create-user-stories](create-user-stories.md) - Ensure test scenarios match user needs\n- [/search-user-research](search-user-research.md) - Build on existing research\n\n## Framework References\n\nFor comprehensive usability testing guidance:\n- **Complete testing methodology**: [usability-testing.md](../skills/product-frameworks/references/usability-testing.md)\n- **User-centered design context**: [design-processes.md](../skills/product-frameworks/references/design-processes.md)"
              },
              {
                "name": "/prioritize-backlog",
                "description": "Prioritize backlog items using RICE and other PM frameworks",
                "path": "plugins/kjgarza-product/commands/prioritize-backlog.md",
                "frontmatter": {
                  "description": "Prioritize backlog items using RICE and other PM frameworks",
                  "argument-hint": [
                    "backlog items or context"
                  ]
                },
                "content": "# Prioritize Backlog\n\nApply prioritization frameworks to: **$ARGUMENTS**\n\n## Your Task\n\nSystematically prioritize the backlog items above using proven PM frameworks. Produce a ranked list with RICE scores, priority tiers, and actionable recommendations for sprint/quarter planning.\n\n## Steps\n\n1. **Gather Backlog Items**\n   - Parse \"$ARGUMENTS\" for backlog items to prioritize\n   - Search Coda for roadmap and backlog documents\n   - Review Dovetail for user research supporting items\n   - Identify stakeholder requests\n\n2. **Score Each Item**\n   - Apply RICE framework (Reach, Impact, Confidence, Effort)\n   - Consider strategic alignment\n   - Factor in dependencies\n   - Assess urgency vs. importance\n\n3. **Apply Prioritization Framework**\n   \n   **RICE Scoring (Primary):**\n   - **Reach**: How many users per quarter?\n   - **Impact**: Minimal (0.25), Low (0.5), Medium (1), High (2), Massive (3)\n   - **Confidence**: High (100%), Medium (80%), Low (50%)\n   - **Effort**: Person-months estimate\n   - **Score**: (Reach  Impact  Confidence) / Effort\n   \n   **Kano Model (Context):**\n   - Categorize features as Basic/Performance/Delighter\n   - Ensure all Basics are covered first\n   - Balance Performance features with some Delighters\n   \n   **Pareto Principle (80/20 Rule):**\n   - Identify the vital 20% of features driving 80% of value\n   - Focus on optimizing high-impact features\n   \n   **Lifecycle-Aware Prioritization:**\n   - Adjust strategy based on product stage (Introduction/Growth/Maturity/Decline)\n   - Introduction: Focus on must-haves and validation\n   - Growth: Balance quick wins with strategic bets\n   - Maturity: Apply Pareto heavily, optimize vital 20%\n   \n   **Reference**: See [product-frameworks](../skills/product-frameworks/) skill for detailed frameworks:\n   - [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md) for RICE, MoSCoW, Kano, Pareto\n   - [lifecycle-guidance.md](../skills/product-frameworks/references/lifecycle-guidance.md) for stage-specific strategies\n\n4. **Consider Additional Factors**\n   - Strategic initiatives and OKRs\n   - Technical debt and maintenance\n   - Competitive pressure\n   - Regulatory or security requirements\n   - Quick wins vs. long-term investments\n\n5. **Generate Prioritized List**\n   - Rank by RICE score\n   - Group into priority tiers (P0-P3)\n   - Suggest sprint/quarter assignments\n   - Identify trade-offs and dependencies\n\n## Prioritization Frameworks\n\n### RICE Framework (Primary)\nBest for comparing diverse features objectively. Provides quantitative scoring for data-driven decisions.\n\n**Detailed guidance**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n\n### Kano Model (Supplementary)\nCategorizes features by customer satisfaction impact:\n- **Basic Features (Must-haves)**: Absence causes dissatisfaction, presence is expected\n- **Performance Features (Satisfiers)**: More is better, linear satisfaction\n- **Delighters (Exciters)**: Unexpected features that create wow moments\n\n**When to use**: Ensure basic features are covered before adding delighters. In mature products, focus on performance features and sprinkle delighters.\n\n**Detailed guidance**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n\n### Pareto Principle (80/20 Rule)\nIdentify the 20% of features that drive 80% of user value.\n\n**Application**: \n- Focus optimization on high-usage features\n- Consider deprecating rarely-used features\n- In mature products, ruthlessly focus on the vital few\n\n**Detailed guidance**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n\n### Lifecycle-Aware Prioritization\nAdjust prioritization based on product stage:\n\n**Introduction**: \n- Must-haves only, keep scope tight\n- High confidence items, minimize risk\n- Focus on product-market fit validation\n\n**Growth**:\n- Balance quick wins with strategic bets\n- Invest in differentiation and scalability\n- High reach features matter more\n\n**Maturity**:\n- Pareto principle heavily (optimize vital 20%)\n- Retention over acquisition features\n- Efficiency and cost reduction\n\n**Decline**:\n- Minimal investment (harvest) or bold bets (reinvention)\n- Only critical fixes unless pivoting\n\n**Detailed guidance**: [lifecycle-guidance.md](../skills/product-frameworks/references/lifecycle-guidance.md)\n\n### MoSCoW Method (Supporting)\n- **Must have**: Critical for launch\n- **Should have**: Important but not critical\n- **Could have**: Nice to have\n- **Won't have**: Out of scope\n\n### Value vs. Effort Matrix\nPlot items on 2x2 matrix:\n- High value, low effort  Do first (quick wins)\n- High value, high effort  Plan carefully\n- Low value, low effort  Do if time permits\n- Low value, high effort  Don't do\n\n## Output Format\n\n### Prioritized Backlog\n\n| Rank | Feature | RICE Score | Priority | Quarter | Rationale |\n|------|---------|------------|----------|---------|-----------|\n| 1 | Feature A | 85 | P0 | Q1 | High user demand, low effort |\n| 2 | Feature B | 72 | P0 | Q1 | Strategic initiative |\n| 3 | Feature C | 45 | P1 | Q2 | Good impact, higher effort |\n\n### Priority Tiers\n\n**P0 - Critical (Do Now)**\n- Features essential for product strategy\n- High RICE scores (>70)\n- Blocking other initiatives\n\n**P1 - High Priority (Do Next)**\n- Important features with strong justification\n- RICE scores 40-70\n- Clear user value\n\n**P2 - Medium Priority (Do Later)**\n- Nice-to-have features\n- RICE scores 20-40\n- Enhancement opportunities\n\n**P3 - Low Priority (Consider)**\n- Nice-to-have with unclear impact\n- RICE scores <20\n- Defer or decline candidates\n\n### Supporting Evidence\nFor each high-priority item:\n- User research insights (Dovetail links)\n- Usage data or analytics\n- Competitive context\n- Strategic alignment\n\n### Recommendations\n- Suggested sprint planning\n- Resource allocation advice\n- Risk callouts\n- Trade-off decisions\n\n## Examples\n\n### Example 1: Sprint Planning\n```\nUser: \"We have 25 backlog items. Help me prioritize for next sprint.\"\n\n[Command analyzes]:\n- Scores each item with RICE\n- Searches Dovetail for supporting research\n- Considers team capacity\n- Recommends top 5-7 items for sprint\n```\n\n### Example 2: Quarterly Roadmap\n```\nUser: \"Prioritize our backlog for Q2 planning\"\n\n[Command generates]:\n- Full RICE scoring for all items\n- Groups into quarters\n- Identifies dependencies\n- Suggests quarterly themes\n```\n\n## Best Practices\n\n### Do:\n- Be transparent about scoring methodology\n- Ground decisions in user research and data\n- Consider opportunity cost\n- Review and update priorities regularly\n- Communicate trade-offs clearly\n- Factor in team velocity and capacity\n\n### Don't:\n- Prioritize solely by stakeholder requests\n- Ignore technical debt\n- Overlook quick wins\n- Forget to validate assumptions\n- Neglect to communicate decisions\n\n## Common Pitfalls\n\n1. **HiPPO (Highest Paid Person's Opinion)**\n   - Solution: Use data-driven frameworks like RICE\n\n2. **Feature Bloat**\n   - Solution: Ruthlessly cut low-impact items\n\n3. **Ignoring Technical Debt**\n   - Solution: Reserve 20% capacity for maintenance\n\n4. **Analysis Paralysis**\n   - Solution: Set time limits for prioritization exercises\n\n5. **Shifting Priorities**\n   - Solution: Define stable planning horizons (quarters)\n\n## Related Commands\n\n- [/analyze-feature-request](analyze-feature-request.md) - Deep dive on specific features\n- [/create-prd](create-prd.md) - Document high-priority features\n- [/search-user-research](search-user-research.md) - Find supporting evidence\n\n## Framework References\n\nFor detailed guidance on prioritization methods:\n- **RICE, MoSCoW, Kano, Pareto**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md)\n- **Lifecycle-aware strategies**: [lifecycle-guidance.md](../skills/product-frameworks/references/lifecycle-guidance.md)\n- **Combining frameworks**: [prioritization-frameworks.md](../skills/product-frameworks/references/prioritization-frameworks.md) (Decision Framework section)\n\n## Checklist\n\nBefore finalizing prioritization:\n- [ ] All items scored with consistent framework\n- [ ] User research consulted for evidence\n- [ ] Strategic alignment verified\n- [ ] Dependencies identified\n- [ ] Team capacity considered\n- [ ] Trade-offs documented\n- [ ] Stakeholders aligned\n- [ ] Rationale documented for decisions"
              },
              {
                "name": "/search-user-research",
                "description": "Search and analyze user research from Dovetail for insights",
                "path": "plugins/kjgarza-product/commands/search-user-research.md",
                "frontmatter": {
                  "description": "Search and analyze user research from Dovetail for insights",
                  "argument-hint": [
                    "search topic or question"
                  ]
                },
                "content": "# Search User Research\n\nFind and analyze user research insights about: **$ARGUMENTS**\n\n## Your Task\n\nSearch Dovetail for user research related to the topic above. Synthesize findings into actionable insights that support evidence-based product decisions.\n\n## Steps\n\n1. **Define Search Query**\n   - Using \"$ARGUMENTS\" as the primary search topic\n   - Identify related key search terms and synonyms\n   - Specify time range (if relevant)\n   - Define scope (projects, tags, insights)\n\n2. **Search Dovetail**\n   - Query Dovetail MCP for research matching \"$ARGUMENTS\"\n   - Search across projects, insights, and highlights\n   - Filter by tags and categories\n   - Retrieve matching research data\n\n3. **Analyze Findings**\n   - Identify common themes and patterns\n   - Extract key insights and quotes\n   - Quantify frequency of mentions\n   - Note conflicting evidence\n\n4. **Synthesize Results**\n   - Summarize key findings\n   - Highlight most relevant insights\n   - Link to specific research artifacts\n   - Provide actionable recommendations\n\n5. **Present Evidence**\n   - Structure findings clearly\n   - Include direct quotes from users\n   - Show frequency of themes\n   - Link to source material in Dovetail\n\n## Search Patterns\n\n### By Topic\nSearch for specific product areas or features:\n- \"onboarding experience\"\n- \"search functionality\"\n- \"collaboration features\"\n- \"pricing concerns\"\n\n### By User Segment\nSearch for specific user types:\n- \"enterprise customers\"\n- \"academic users\"\n- \"new users\"\n- \"power users\"\n\n### By Problem/Pain Point\nSearch for user challenges:\n- \"pain points\"\n- \"frustrations\"\n- \"confusing\"\n- \"difficult to use\"\n\n### By Opportunity\nSearch for positive signals:\n- \"feature requests\"\n- \"wishlist\"\n- \"would love to\"\n- \"missing functionality\"\n\n## Output Format\n\n### Research Summary\n\n**Search Query**: [What was searched]  \n**Projects Searched**: [Number and names]  \n**Date Range**: [If applicable]  \n**Total Insights Found**: [Count]\n\n### Key Findings\n\n#### Theme 1: [Primary theme]\n- **Frequency**: [How often mentioned]\n- **Sentiment**: [Positive/Negative/Neutral]\n- **Key Insights**:\n  - Insight 1 with context\n  - Insight 2 with context\n- **Representative Quotes**:\n  > \"Direct quote from user research\"\n  > \"Another relevant quote\"\n- **Source**: [Link to Dovetail project/insight]\n\n#### Theme 2: [Secondary theme]\n[Same structure as above]\n\n### Patterns Observed\n- Pattern 1: Description\n- Pattern 2: Description\n- Pattern 3: Description\n\n### Conflicting Evidence\n[Note any contradictory findings]\n\n### Gaps in Research\n[Areas where more research is needed]\n\n### Recommendations\nBased on the research findings:\n1. Recommendation 1 with supporting evidence\n2. Recommendation 2 with supporting evidence\n3. Recommendation 3 with supporting evidence\n\n### Next Steps\n- [ ] Additional research needed on [topic]\n- [ ] Validate findings with [method]\n- [ ] Incorporate into [PRD/roadmap]\n\n## Examples\n\n### Example 1: Feature Validation\n```\nUser: \"Search our research about export functionality\"\n\n[Command searches]:\n- Queries Dovetail for \"export\" mentions\n- Identifies 15 relevant insights across 5 projects\n- Extracts themes: bulk export, format options, scheduling\n- Shows frequency and user quotes\n- Recommends feature scope based on evidence\n```\n\n### Example 2: Problem Exploration\n```\nUser: \"What are users saying about our onboarding?\"\n\n[Command analyzes]:\n- Searches for \"onboarding\" across all projects\n- Identifies pain points and friction\n- Quantifies issue frequency\n- Provides specific improvement recommendations\n```\n\n### Example 3: Segment Understanding\n```\nUser: \"What challenges do enterprise customers face?\"\n\n[Command finds]:\n- Filters for enterprise-tagged research\n- Extracts common themes\n- Compares with SMB customer feedback\n- Highlights unique enterprise needs\n```\n\n## Use Cases\n\n### Product Discovery\n- Validate new feature ideas\n- Identify user pain points\n- Discover unmet needs\n- Understand user workflows\n\n### Feature Prioritization\n- Assess demand for features\n- Quantify problem severity\n- Compare user segment needs\n- Support RICE scoring\n\n### PRD Development\n- Ground requirements in user needs\n- Include user quotes\n- Validate solution approaches\n- Define success criteria\n\n### Competitive Analysis\n- Understand why users choose competitors\n- Identify feature gaps\n- Assess positioning opportunities\n- Validate differentiation strategy\n\n## Best Practices\n\n### Do:\n- Search broadly first, then narrow down\n- Look for patterns across multiple users\n- Include direct quotes for context\n- Note the source and date of research\n- Identify gaps in current research\n- Consider user segment differences\n\n### Don't:\n- Cherry-pick quotes that fit your bias\n- Over-rely on single user feedback\n- Ignore contradictory evidence\n- Forget to validate old research\n- Take quotes out of context\n\n## Integration with Other Commands\n\n- Use before [/analyze-feature-request](analyze-feature-request.md) for evidence\n- Inform [/create-prd](create-prd.md) with user insights\n- Support [/prioritize-backlog](prioritize-backlog.md) decisions\n- Ground `/competitive-analysis` in user needs\n\n## Related Commands\n\n- [/analyze-feature-request](analyze-feature-request.md) - Analyze specific features\n- [/create-prd](create-prd.md) - Document requirements\n- [/prioritize-backlog](prioritize-backlog.md) - Data-driven prioritization"
              }
            ],
            "skills": [
              {
                "name": "docx",
                "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
                "path": "plugins/kjgarza-product/skills/docx/SKILL.md",
                "frontmatter": {
                  "name": "docx",
                  "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
              },
              {
                "name": "new-yorker-style",
                "description": "Write prose in The New Yorker's distinctive literary style. Applies the magazine's house conventions (diaeresis, British spellings, serial comma), elegant sentence construction, and commitment to clarity. Use when writing essays, articles, profiles, long-form journalism, or any sophisticated prose. Triggers on requests for \"New Yorker style,\" literary writing, magazine-quality prose, or elegant nonfiction.",
                "path": "plugins/kjgarza-product/skills/new-yorker-style/SKILL.md",
                "frontmatter": {
                  "name": "new-yorker-style",
                  "description": "Write prose in The New Yorker's distinctive literary style. Applies the magazine's house conventions (diaeresis, British spellings, serial comma), elegant sentence construction, and commitment to clarity. Use when writing essays, articles, profiles, long-form journalism, or any sophisticated prose. Triggers on requests for \"New Yorker style,\" literary writing, magazine-quality prose, or elegant nonfiction."
                },
                "content": "# The New Yorker Writing Style\n\nWrite with clarity, harmony, truth, and unfailing courtesy to the reader. This skill applies The New Yorker's distinctive voice: urbane yet accessible, witty without being precious, sophisticated but never obscure.\n\n## Core Philosophy\n\nVed Mehta, staff writer from 19611994, distilled the style: **\"Clarity, harmony, truth and unfailing courtesy to the reader.\"**\n\nDavid Foster Wallace articulated what this means in practice:\n> \"In the broadest possible sense, writing well means to communicate clearly and interestingly and in a way that feels alive to the reader. Where there's some kind of relationship between the writer and the readereven though it's mediated by a kind of textthere's an electricity about it.\"\n\nThe reader cannot read your mind. Every word must earn its place.\n\n## Voice and Tone\n\n### The New Yorker Sound\n- **Urbane but not elitist**: Assume an intelligent, curious reader\n- **Literary yet conversational**: Balance formality with warmth\n- **Witty without straining**: Humor emerges from observation, not forced cleverness\n- **Precise but not pedantic**: Choose the exact word, not the impressive one\n\n### What to Avoid\n- Sentimentality or sensationalism\n- Jargon without explanation\n- Clichs and vogue expressions\n- Excessive qualification (\"very,\" \"really,\" \"quite\")\n- The passive voice when active would serve\n\n## Sentence Craft\n\n### Rhythm and Variation\nFollow a long, complex sentence with a short, punchy one:\n> \"By 2000, the investigation of the helicopter-conversion industry was winding down, with disappointing results for Wales and the U.S. Attorney's Office. **Only one case remained.**\"\n\nBegin paragraphs with short sentences when appropriate:\n> \"**Progress came slowly.** Anderson remained the only suspect; in 2004, the Seattle Times reported that the F.B.I. had searched Anderson's home.\"\n\n### Sentence Openers\nStart sentences with light openerstransitional words, subordinate clauses, or scene-setting phrases:\n> \"**Wales,** an Assistant United States Attorney in Seattle, had planned to have dinner with his girlfriend. **But** that afternoon Wales called and said he had projects to work on at home.\"\n\n### Linking and Flow\nLink the first sentence of a paragraph to the last sentence of the preceding one:\n> \"Neither Wales's romantic life nor the fender bender yielded **promising leads** in the murder investigation.\n>\n> Wales's work on gun control also failed to **produce suspects.**\"\n\nConvey chronology through transition phrases rather than bare dates:\n- \"Three months before his death...\"\n- \"About fifteen minutes later...\"\n- \"Two weeks after the murder...\"\n- \"Meanwhile...\"\n- \"Not long after the meeting...\"\n\n## Punctuation\n\nThe New Yorker follows \"close punctuation\"commas fall with precision, as E.B. White observed, \"like knives in a circus act, outlining the victim.\"\n\n### The Serial Comma\nAlways use it:\n> \"She bought apples, oranges, and pears.\"\n\n### Em Dashes\nSet off explanatory phrases with em dashes for emphasis and clarity:\n> \"The proposal brought out the full might of the gun-control lobby, which spent four million dollars**primarily on television advertisements and direct-mail appeals**and voters rejected the measure.\"\n\nUse dashes when commas would create ambiguity.\n\n### Semicolons\nReserve for parallel constructions:\n> \"United States Attorneys establish the priorities for each of the nation's ninety-four judicial districts and announce significant indictments; Assistant U.S. Attorneys are more like civil servants; they perform the day-to-day work.\"\n\n### Colons\nUse to introduce an explanation that could stand as a complete sentence:\n> \"The phrase was partly a joke, a bit of feigned grandiosity to justify a tendency toward excessive meticulousness: **Wales did things slowly.**\"\n\n### Hyphens\nHyphenate phrasal adjectives for clarity:\n- cell-phone towers\n- gun-control initiative\n- death-penalty case\n- law-enforcement official\n- forty-year-old pilot\n- highest-ranking official\n\n## House Style Conventions\n\n### The Diaeresis\nThe New Yorker uses the diaeresis (not umlaut) over the second vowel in doubled-vowel pairs to indicate a new syllable:\n- coperate (not cooperate or co-operate)\n- relect\n- remerge\n- rexamine\n- preminent\n\nAlso retain accents in borrowed words:\n- nave\n- lite\n- rsum\n- caf\n\n### British-Influenced Spellings\nDouble consonants where American usage would not:\n- focussed (not focused)\n- travelling (not traveling)\n- marvellous (not marvelous)\n- kidnapped\n- worshipped\n\n### Distinctive Spellings\n- theatre (not theater)\n- carrousel (not carousel)\n- vender (not vendor)\n- teen-ager (hyphenated)\n- per cent (two words)\n- good-by (hyphenated)\n\n### Recent Updates (2025)\n- inbox (formerly in-box)\n- website (formerly Web site)\n- internet (formerly Internet)\n- cellphone (formerly cell phone)\n\n## The Writing Process\n\n### Draft with Deliberation\nWallace advocated writing by hand for early drafts: \"Writing makes me slow down in a way that helps me pay attention.\" Whether handwritten or typed, the first draft should emerge slowly enough for genuine thought.\n\n### Revision as Respect\n> \"One of the things that's good about writing and practicing writing is it's a great remedy for my natural self-involvement and self-centeredness... I never forget there's someone on the end of the line, that I owe that person certain allegiances.\"\n\nEach revision asks: Does this serve the reader?\n\n### Earning Interest\nWallace's humbling insight for every writer:\n> \"I am not, in and of myself, interesting to a reader. If I want to seem interesting, work has to be done in order to make myself interesting.\"\n\n## Fact-Checking Ethic\n\nIn 1927, The New Yorker ran an article about Edna St. Vincent Millay with multiple errors, and her mother threatened to sue. The magazine developed the most rigorous fact-checking process in American journalism.\n\nApply this standard:\n- Verify every name, date, title, and statistic\n- Question every quotationis this precisely what was said?\n- Check obvious \"facts\" hardest; familiarity breeds false confidence\n- When uncertain, investigate rather than assume\n\n## Quick Reference\n\n| Element | New Yorker Style |\n|---------|------------------|\n| Serial comma | Always |\n| Theatre/theater | theatre |\n| Focused/focussed | focussed |\n| Traveled/travelled | travelled |\n| Cooperate | coperate |\n| Naive | nave |\n| Elite | lite |\n| Teenager | teen-ager |\n| Percent | per cent |\n| Website | website (as of 2025) |\n| Internet | internet (as of 2025) |\n\n## Further Reading\n\nFor detailed spelling lists and additional punctuation rules, see [references/style-details.md](references/style-details.md)."
              },
              {
                "name": "pdf",
                "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
                "path": "plugins/kjgarza-product/skills/pdf/SKILL.md",
                "frontmatter": {
                  "name": "pdf",
                  "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md"
              },
              {
                "name": "planning-with-files",
                "description": "Transforms workflow to use Manus-style persistent markdown files for planning, progress tracking, and knowledge storage. Use when starting complex tasks, multi-step projects, research tasks, or when the user mentions planning, organizing work, tracking progress, or wants structured output.",
                "path": "plugins/kjgarza-product/skills/planning-with-files/SKILL.md",
                "frontmatter": {
                  "name": "planning-with-files",
                  "description": "Transforms workflow to use Manus-style persistent markdown files for planning, progress tracking, and knowledge storage. Use when starting complex tasks, multi-step projects, research tasks, or when the user mentions planning, organizing work, tracking progress, or wants structured output."
                },
                "content": "# Planning with Files\n\nWork like Manus: Use persistent markdown files as your \"working memory on disk.\"\n\n## Quick Start\n\nBefore ANY complex task:\n\n1. **Create `task_plan.md`** in the working directory\n2. **Define phases** with checkboxes\n3. **Update after each phase** - mark [x] and change status\n4. **Read before deciding** - refresh goals in attention window\n\n## The 3-File Pattern\n\nFor every non-trivial task, create THREE files:\n\n| File | Purpose | When to Update |\n|------|---------|----------------|\n| `task_plan.md` | Track phases and progress | After each phase |\n| `notes.md` | Store findings and research | During research |\n| `[deliverable].md` | Final output | At completion |\n\n## Core Workflow\n\n```\nLoop 1: Create task_plan.md with goal and phases\nLoop 2: Research  save to notes.md  update task_plan.md\nLoop 3: Read notes.md  create deliverable  update task_plan.md\nLoop 4: Deliver final output\n```\n\n### The Loop in Detail\n\n**Before each major action:**\n```bash\nRead task_plan.md  # Refresh goals in attention window\n```\n\n**After each phase:**\n```bash\nEdit task_plan.md  # Mark [x], update status\n```\n\n**When storing information:**\n```bash\nWrite notes.md     # Don't stuff context, store in file\n```\n\n## task_plan.md Template\n\nCreate this file FIRST for any complex task:\n\n```markdown\n# Task Plan: [Brief Description]\n\n## Goal\n[One sentence describing the end state]\n\n## Phases\n- [ ] Phase 1: Plan and setup\n- [ ] Phase 2: Research/gather information\n- [ ] Phase 3: Execute/build\n- [ ] Phase 4: Review and deliver\n\n## Key Questions\n1. [Question to answer]\n2. [Question to answer]\n\n## Decisions Made\n- [Decision]: [Rationale]\n\n## Errors Encountered\n- [Error]: [Resolution]\n\n## Status\n**Currently in Phase X** - [What I'm doing now]\n```\n\n## notes.md Template\n\nFor research and findings:\n\n```markdown\n# Notes: [Topic]\n\n## Sources\n\n### Source 1: [Name]\n- URL: [link]\n- Key points:\n  - [Finding]\n  - [Finding]\n\n## Synthesized Findings\n\n### [Category]\n- [Finding]\n- [Finding]\n```\n\n## Critical Rules\n\n### 1. ALWAYS Create Plan First\nNever start a complex task without `task_plan.md`. This is non-negotiable.\n\n### 2. Read Before Decide\nBefore any major decision, read the plan file. This keeps goals in your attention window.\n\n### 3. Update After Act\nAfter completing any phase, immediately update the plan file:\n- Mark completed phases with [x]\n- Update the Status section\n- Log any errors encountered\n\n### 4. Store, Don't Stuff\nLarge outputs go to files, not context. Keep only paths in working memory.\n\n### 5. Log All Errors\nEvery error goes in the \"Errors Encountered\" section. This builds knowledge for future tasks.\n\n## When to Use This Pattern\n\n**Use 3-file pattern for:**\n- Multi-step tasks (3+ steps)\n- Research tasks\n- Building/creating something\n- Tasks spanning multiple tool calls\n- Anything requiring organization\n\n**Skip for:**\n- Simple questions\n- Single-file edits\n- Quick lookups\n\n## Anti-Patterns to Avoid\n\n| Don't | Do Instead |\n|-------|------------|\n| Use TodoWrite for persistence | Create `task_plan.md` file |\n| State goals once and forget | Re-read plan before each decision |\n| Hide errors and retry | Log errors to plan file |\n| Stuff everything in context | Store large content in files |\n| Start executing immediately | Create plan file FIRST |\n\n## Advanced Patterns\n\nSee [reference.md](reference.md) for:\n- Attention manipulation techniques\n- Error recovery patterns\n- Context optimization from Manus\n\nSee [examples.md](examples.md) for:\n- Real task examples\n- Complex workflow patterns"
              },
              {
                "name": "pptx",
                "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
                "path": "plugins/kjgarza-product/skills/pptx/SKILL.md",
                "frontmatter": {
                  "name": "pptx",
                  "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n-  State your content-informed design approach BEFORE writing code\n-  Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n-  Create clear visual hierarchy through size, weight, and color\n-  Ensure readability: strong contrast, appropriately sized text, clean alignment\n-  Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90 or 270\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (33, 44 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt  405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (56)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
              },
              {
                "name": "product-frameworks-for-design-and-management",
                "description": "Comprehensive product design and management frameworks including UX heuristics (Hick's Law, Fitts's Law), design processes (Double Diamond, Lean UX, Agile, User-Centered Design), prioritization methods (RICE, MoSCoW, Kano Model, Pareto Principle), product lifecycle guidance, PRD templates, user story formats, design critique guidelines, and usability testing checklists. Use when analyzing features, creating product documentation, facilitating design reviews, planning user research, applying UX principles to interfaces, or making product decisions that require framework-based guidance.",
                "path": "plugins/kjgarza-product/skills/product-frameworks/SKILL.md",
                "frontmatter": {
                  "name": "product-frameworks-for-design-and-management",
                  "description": "Comprehensive product design and management frameworks including UX heuristics (Hick's Law, Fitts's Law), design processes (Double Diamond, Lean UX, Agile, User-Centered Design), prioritization methods (RICE, MoSCoW, Kano Model, Pareto Principle), product lifecycle guidance, PRD templates, user story formats, design critique guidelines, and usability testing checklists. Use when analyzing features, creating product documentation, facilitating design reviews, planning user research, applying UX principles to interfaces, or making product decisions that require framework-based guidance."
                },
                "content": "# Product Frameworks\n\n## Overview\n\nComprehensive product design and management frameworks to guide strategic decisions, improve user experience, and structure product workflows.\n\n**Reference Files:**\n- [references/design-principles.md](references/design-principles.md)  Hick's Law, Fitts's Law, affordances, feedback\n- [references/design-processes.md](references/design-processes.md)  Double Diamond, Agile, Lean UX, User-Centered Design\n- [references/prioritization-frameworks.md](references/prioritization-frameworks.md)  RICE, MoSCoW, Kano Model, Pareto Principle\n- [references/lifecycle-guidance.md](references/lifecycle-guidance.md)  Product stage strategies\n- [references/requirements.md](references/requirements.md)  PRD templates, user stories, acceptance criteria\n- [references/critique-guidelines.md](references/critique-guidelines.md)  Design review facilitation\n- [references/usability-testing.md](references/usability-testing.md)  Test planning and analysis\n\n## Quick Start\n\n1. **Identify the domain**  Determine which area of product/design work you're addressing\n2. **Load the relevant reference**  Read the specific reference file for that domain\n3. **Apply the frameworks**  Use the guidance to inform decisions or structure workflows\n\n## When to Use\n\n| Need | Reference File |\n|------|----------------|\n| UI/UX decisions | `design-principles.md` |\n| Planning discovery work | `design-processes.md` |\n| Evaluating features | `prioritization-frameworks.md` |\n| Product stage strategy | `lifecycle-guidance.md` |\n| Creating PRDs or user stories | `requirements.md` |\n| Running design reviews | `critique-guidelines.md` |\n| Planning user research | `usability-testing.md` |\n\n## Example Workflows\n\n**Feature Evaluation:**\n1. Load `prioritization-frameworks.md` for RICE and Kano Model\n2. Load `lifecycle-guidance.md` for stage-specific priorities\n3. Apply both for informed prioritization\n\n**Interface Design Feedback:**\n1. Load `design-principles.md` for Hick's Law and Fitts's Law\n2. Evaluate design against heuristics\n3. Load `critique-guidelines.md` to structure feedback\n\n**Discovery Phase Planning:**\n1. Load `design-processes.md` for Double Diamond\n2. Load `usability-testing.md` for research planning\n3. Structure discovery using both\n\n## Framework Summaries\n\n**Design Principles:** Hick's Law (choice overload), Fitts's Law (target sizing), affordances, feedback loops, system status visibility\n\n**Design Processes:** Double Diamond (DiscoverDefineDevelopDeliver), Agile, Lean UX, User-Centered Design\n\n**Prioritization:** RICE Scoring, MoSCoW (Must/Should/Could/Won't), Kano Model, 80/20 Rule\n\n**Lifecycle:** Introduction, Growth, Maturity, Decline stage strategies\n\n**Documentation:** PRD structure, user story format, acceptance criteria\n\n## Best Practices\n\n**Progressive Loading:** Don't load all references at once. Start with the most relevant file, then add others as needed.\n\n**Context-Specific Application:** Frameworks are guidance, not rigid rules. Apply with judgment based on your product context.\n\n**Complementary Frameworks:**\n- Kano Model + RICE = comprehensive prioritization\n- Double Diamond + Lean UX = structured but iterative discovery\n- User-Centered Design + Usability Testing = validated user focus\n\n**Shared Language:** Use framework terminology to align teams and communicate with stakeholders."
              },
              {
                "name": "project-bootstrapping",
                "description": "Sets up new projects or improves existing projects with development best practices, tooling, documentation, and workflow automation. Use when user wants to start a new project, improve project structure, add development tooling, or establish professional workflows.",
                "path": "plugins/kjgarza-product/skills/project-bootstrapping/SKILL.md",
                "frontmatter": {
                  "name": "project-bootstrapping",
                  "description": "Sets up new projects or improves existing projects with development best practices, tooling, documentation, and workflow automation. Use when user wants to start a new project, improve project structure, add development tooling, or establish professional workflows."
                },
                "content": "# Overview\n\nSets up new projects or improves existing projects with development best practices, tooling, documentation, and workflow automation.\n\n## When to Use\n\n- \"set up a new project\"\n- \"bootstrap this project\"\n- \"add best practices\"\n- \"improve project structure\"\n- \"set up development tooling\"\n- \"initialize project properly\"\n\n## What It Sets Up\n\n### 1. Project Structure\n- Standard directories (docs/, .github/, .cursor/, .claude/)\n- Logical file organization\n- Structure improvements\n\n### 2. Git Configuration\n- Comprehensive `.gitignore`\n- `.gitattributes` for line endings/diffs\n- Git hooks (pre-commit, commit-msg)\n- Branch protection patterns\n\n\n### 3. Documentation\n- Comprehensive `README.md`\n- `CONTRIBUTING.md`\n- Code documentation (JSDoc, docstrings)\n- `CHANGELOG.md` structure\n- Architecture docs if complex\n- MIT License file\n\n### 4. Testing Setup\n- Identify/suggest testing framework\n- Test structure and conventions\n- Example/template tests\n- Configure test runners\n- Coverage reporting\n- Testing scripts/commands\n\n### 5. Code Quality Tools\n- Linters (ESLint, Pylint, etc.)\n- Formatters (Prettier, Black, etc.)\n- Type checking (TypeScript, mypy, etc.)\n- Pre-commit hooks for quality\n- Editor configs (.editorconfig)\n- Code quality badges\n\n### 6. Dependencies Management\n- Package manager configuration\n- Organize dependencies\n- Check security vulnerabilities\n- Set up dependency updates (Dependabot, Renovate)\n- Create lock files\n- Document dependency choices\n\n### 7. Development Workflow\n- Useful npm scripts / Makefile targets\n- Environment variable templates (.env.example)\n- Docker configuration if appropriate\n- Development startup scripts\n- Hot-reload / watch modes\n- Document development workflow\n\n### 8. CI/CD Setup\n- GitHub Actions / GitLab CI config\n- Automated testing\n- Automated deployment (if applicable)\n- Status badges\n- Release automation\n- Branch protection\n\n## Approach\n\n### Discovery Phase\n\nAsk clarifying questions:\n1. **Project type**: New or existing?\n2. **Primary purpose**: Web app, library, CLI tool?\n3. **Language/framework**: JS/TS, Python, Go, etc.?\n4. **Collaboration**: Personal or team?\n5. **Deployment target**: Server, cloud, mobile, desktop?\n6. **Preferences**: Specific tools/frameworks?\n7. **Scope**: Full setup or specific areas?\n\n### Implementation Phase\n\n1. **Analyze existing** structure (if existing project)\n2. **Create plan** based on answers\n3. **Show plan** and get approval\n4. **Implement systematically** (one area at a time)\n5. **Verify completeness**\n6. **Provide handoff** documentation\n\n## Customization\n\nAdapts to:\n- **Language ecosystem**: Node.js vs Python vs Go vs Rust\n- **Project size**: Small script vs large app\n- **Team size**: Solo vs collaborative\n- **Maturity**: Startup speed vs enterprise standards\n\n## Tools Used\n\n- **AskUserQuestion**: Gather requirements\n- **Write**: Create configuration files, documentation\n- **Edit**: Update existing files\n- **Bash**: Initialize tools (git init, npm init)\n- **Read**: Analyze existing structure\n- **Glob**: Find files to update\n\n## Success Criteria\n\n- All standard files present and configured\n- Clear and complete documentation\n- Documented development workflow\n- Automated quality tooling (pre-commit hooks)\n- Easy test execution\n- Follows language/framework conventions\n- Quick developer onboarding\n- No obvious best practices missing\n\n## Templates\n\n- Node.js/TypeScript web app\n- Python CLI tool\n- Python web API (FastAPI/Flask)\n- React/Next.js app\n- Go service\n- Rust CLI/library\n\n## Integration\n\n- **feature-planning**: For planning custom features\n- **code-auditor**: For validating setup quality\n- **codebase-documenter**: For generating detailed docs\n\n## Scope Control\n\n- **Full bootstrap**: Everything from scratch\n- **Partial setup**: Specific areas only (e.g., \"just add testing\")\n- **Improvement pass**: Enhance existing project\n- **Audit + fix**: Check what's missing and add it"
              },
              {
                "name": "searching-academic-outputs-with-dimensions",
                "description": "Search for academic literature, empirical evidence, and scholarly research using the Dimensions database. Use when seeking research papers to support product decisions, find empirical studies, conduct literature reviews, explore funding patterns, validate hypotheses with academic sources, or discover research trends. Supports publications, grants, patents, clinical trials, and researcher profiles. Triggers on requests for \"academic evidence\", \"empirical research\", \"find studies\", \"literature search\", or \"research to support decisions\".",
                "path": "plugins/kjgarza-product/skills/searching-academic-outputs-with-dimensions/SKILL.md",
                "frontmatter": {
                  "name": "searching-academic-outputs-with-dimensions",
                  "description": "Search for academic literature, empirical evidence, and scholarly research using the Dimensions database. Use when seeking research papers to support product decisions, find empirical studies, conduct literature reviews, explore funding patterns, validate hypotheses with academic sources, or discover research trends. Supports publications, grants, patents, clinical trials, and researcher profiles. Triggers on requests for \"academic evidence\", \"empirical research\", \"find studies\", \"literature search\", or \"research to support decisions\".",
                  "allowed-tools": "Read, Grep, Glob, Bash, Edit, Write"
                },
                "content": "# Overview\n\nQuery the Dimensions academic database to find publications, grants, patents, clinical trials, and researchers using the kestrel CLI.\n\nQueries use the Dimensions Search Language (DSL) piped to kestrel.\n\n**Reference Documentation:**\n- [dsl-reference.md](dsl-reference.md)  Full DSL syntax, operators, fields, and example queries\n\n**Authentication:** Requires API key from https://app.dimensions.ai/account/tokens. Configure credentials in `~/.dimensions/dsl.ini` or use environment variables (`DIMENSIONS_KEY`, `DIMENSIONS_ENDPOINT`). See Authentication section in [dsl-reference.md](dsl-reference.md) for details.\n\n# Quick Start\n\n```bash\n# Search publications\necho 'search publications for \"machine learning\" return publications limit 10' | kestrel\n\n# Search grants\necho 'search grants for \"artificial intelligence\" return grants[title+funding_usd] limit 10' | kestrel\n\n# Find researchers\necho 'search researchers for \"John Smith\" return researchers[first_name+last_name+current_research_org] limit 10' | kestrel\n```\n\n# Query Structure\n\n```\nsearch <source> [for \"<terms>\"] [where <filters>] return <result> [limit N]\n```\n\n# Available Sources\n\n| Source | Description |\n|--------|-------------|\n| `publications` | Research papers, articles, books |\n| `grants` | Research funding awards |\n| `patents` | Patent applications and grants |\n| `clinical_trials` | Clinical trial records |\n| `researchers` | Researcher profiles |\n| `datasets` | Research datasets |\n\n# Common Query Patterns\n\n| Task | Query |\n|------|-------|\n| Search by keyword | `search publications for \"CRISPR\" return publications limit 20` |\n| Filter by year | `search publications where year=2024 return publications limit 10` |\n| Search title only | `search publications in title_only for \"climate change\" return publications limit 10` |\n| Highly cited papers | `search publications for \"AI\" where times_cited>100 return publications sort by times_cited desc limit 10` |\n| Grants by funder | `search grants where funders.name~\"NIH\" return grants[title+funding_usd] limit 10` |\n| Recent patents | `search patents where filing_year>=2023 return patents[title+assignees] limit 10` |\n| Clinical trials | `search clinical_trials for \"diabetes\" return clinical_trials[title+phase] limit 10` |\n| Aggregations | `search publications for \"robotics\" return research_orgs aggregate count sort by count desc limit 10` |\n\n# Return Specific Fields\n\nUse `+` to combine fields:\n```bash\necho 'search publications for \"quantum\" return publications[id+title+doi+year+times_cited] limit 10' | kestrel\n```\n\n# Filter Operators\n\n| Operator | Example |\n|----------|---------|\n| `=` equals | `year=2024` |\n| `>` greater than | `times_cited>100` |\n| `~` contains | `journal.title~\"Nature\"` |\n| `in` range | `year in [2020:2024]` |\n| `and/or` combine | `year=2024 and type=\"article\"` |\n\n# Boolean Search Operators\n\nBoolean operators in search terms must be **UPPERCASE** and **inside** the quotes:\n\n| Pattern | Example |\n|---------|---------|\n| AND (both required) | `for \"machine learning AND healthcare\"` |\n| OR (either matches) | `for \"python OR java\"` |\n| NOT (exclude) | `for \"AI NOT robotics\"` |\n| Grouped | `for \"(cancer OR tumor) AND treatment\"` |\n| Exact phrase | `for \"\\\"peer feedback\\\"\"` (escaped quotes) |\n| Proximity | `for \"\\\"formal model\\\"~10\"` (within 10 words) |\n\n**Important**: Lowercase `and/or` in the `where` clause combines filters, but search terms require UPPERCASE `AND/OR/NOT`.\n\n# Tips for Precise Results\n\n1. **Use targeted search indexes** for relevant results:\n   ```bash\n   # Most specific - title only\n   echo 'search publications in title_only for \"machine learning\" return publications limit 20' | kestrel\n\n   # Good balance - title and abstract\n   echo 'search publications in title_abstract_only for \"peer feedback AND writing\" return publications limit 20' | kestrel\n   ```\n\n2. **Use phrase searches** for exact multi-word terms:\n   ```bash\n   echo 'search publications in title_abstract_only for \"\\\"formative assessment\\\" AND \\\"higher education\\\"\" return publications limit 20' | kestrel\n   ```\n\n3. **Combine search with filters** to narrow scope:\n   ```bash\n   echo 'search publications in title_abstract_only for \"\\\"peer review\\\"\" where year>=2020 and times_cited>10 return publications[title+doi+times_cited+year] limit 20' | kestrel\n   ```\n\n4. **Filter by citation count** to find influential papers:\n   ```bash\n   echo 'search publications for \"cognitive load\" where times_cited>50 return publications sort by times_cited desc limit 10' | kestrel\n   ```\n\n# General Tips\n\n- Always use double quotes around search terms: `for \"search term\"`\n- Use `~` for partial string matching: `where journal.title~\"Nature\"`\n- Use `limit` to control result size\n- See [dsl-reference.md](dsl-reference.md) for complete syntax and all available fields\n\n# Error Handling\n\n| Error | Solution |\n|-------|----------|\n| kestrel not found | Install from ~/aves/kestrel: `cd ~/aves/kestrel && uv pip install .` |\n| Configuration error (exit code 2) | Set `DIMENSIONS_KEY` env var or create `~/.dimensions/dsl.ini` |\n| Invalid credentials | Get new key from https://app.dimensions.ai/account/tokens |\n| Query syntax error (exit code 1) | Check DSL syntax in [dsl-reference.md](dsl-reference.md) |"
              },
              {
                "name": "searching-documents-with-coda",
                "description": "Search and extract content from Coda documents including PRDs, roadmaps, competitive analyses, and research docs. Use when user wants to find documents in Coda, list pages and tables, export content as Markdown/HTML/JSON/CSV, or access structured data from Coda workspace. Triggers on \"find in Coda\", \"search Coda\", \"Coda document\", \"export from Coda\", or references to product documentation.",
                "path": "plugins/kjgarza-product/skills/searching-documents-with-coda/SKILL.md",
                "frontmatter": {
                  "name": "searching-documents-with-coda",
                  "description": "Search and extract content from Coda documents including PRDs, roadmaps, competitive analyses, and research docs. Use when user wants to find documents in Coda, list pages and tables, export content as Markdown/HTML/JSON/CSV, or access structured data from Coda workspace. Triggers on \"find in Coda\", \"search Coda\", \"Coda document\", \"export from Coda\", or references to product documentation.",
                  "allowed-tools": "Read, Grep, Glob, Bash, Edit, Write"
                },
                "content": "# Overview\n\nQuery and export content from Coda documents using the `coda` CLI. Supports searching documents, listing pages and tables, exporting page content as Markdown/HTML, and exporting table data as JSON/CSV.\n\n**Reference Documentation:**\n- [coda-reference.md](coda-reference.md)  Full command reference with all options\n- [digital-science-reference.md](digital-science-reference.md)  Digital Science documentation links\n\n**Authentication:** Requires `CODA_API_KEY` environment variable. Get your key at https://coda.io/account\n\n# Quick Start\n\n```bash\n# Verify authentication\ncoda auth status\n\n# Search for documents\ncoda docs search \"PRD\"\n\n# List pages in a document\ncoda pages list DOC_ID\n\n# Export page as Markdown\ncoda pages export DOC_ID PAGE_ID --format markdown\n\n# Export table data as JSON\ncoda export DOC_ID TABLE_ID --format json\n```\n\n# Command Reference\n\n## Document Commands\n\n| Command | Description |\n|---------|-------------|\n| `coda docs list` | List all accessible documents |\n| `coda docs list --query \"PRD\"` | Search documents by name |\n| `coda docs list --mine` | Show only your documents |\n| `coda docs list --limit 10` | Limit results |\n| `coda docs search \"roadmap\"` | Search documents by query |\n| `coda docs show DOC_ID` | Show document details |\n\n## Page Commands\n\n| Command | Description |\n|---------|-------------|\n| `coda pages list DOC_ID` | List all pages in document |\n| `coda pages show DOC_ID PAGE_ID` | Show page details |\n| `coda pages export DOC_ID PAGE_ID` | Export page as HTML (default) |\n| `coda pages export DOC_ID PAGE_ID --format markdown` | Export page as Markdown |\n| `coda pages export DOC_ID PAGE_ID -o page.md` | Save to file |\n\n## Table Commands\n\n| Command | Description |\n|---------|-------------|\n| `coda tables list DOC_ID` | List all tables in document |\n| `coda columns list DOC_ID TABLE_ID` | Show table schema/columns |\n| `coda rows list DOC_ID TABLE_ID` | List all rows in table |\n| `coda rows list DOC_ID TABLE_ID --limit 10` | Limit rows returned |\n| `coda rows list DOC_ID TABLE_ID --query \"filter\"` | Filter rows |\n| `coda rows show DOC_ID TABLE_ID ROW_ID` | Show specific row details |\n\n## Export Commands\n\n| Command | Description |\n|---------|-------------|\n| `coda export DOC_ID TABLE_ID` | Export table as JSON (console) |\n| `coda export DOC_ID TABLE_ID --format csv` | Export as CSV |\n| `coda export DOC_ID TABLE_ID -o data.json` | Save to file |\n\n## Authentication Commands\n\n| Command | Description |\n|---------|-------------|\n| `coda auth status` | Show current configuration |\n| `coda auth test` | Test API connection |\n\n# Common Workflows\n\n## Find and Read a Document\n\n```bash\n# 1. Search for the document\ncoda docs search \"product roadmap\"\n\n# 2. List pages in the document\ncoda pages list DOC_ID\n\n# 3. Export page content as Markdown\ncoda pages export DOC_ID PAGE_ID --format markdown\n```\n\n## Extract Table Data\n\n```bash\n# 1. List tables in document\ncoda tables list DOC_ID\n\n# 2. View table schema\ncoda columns list DOC_ID TABLE_ID\n\n# 3. Export table data\ncoda export DOC_ID TABLE_ID --format json -o data.json\n```\n\n## Browse Table Rows\n\n```bash\n# List first 20 rows\ncoda rows list DOC_ID TABLE_ID --limit 20\n\n# Filter rows\ncoda rows list DOC_ID TABLE_ID --query \"status:active\"\n\n# Show specific row details\ncoda rows show DOC_ID TABLE_ID ROW_ID\n```\n\n# Common Searches\n\n| Task | Command |\n|------|---------|\n| Find PRDs | `coda docs search \"PRD\"` |\n| Find roadmaps | `coda docs search \"roadmap\"` |\n| Find competitive analysis | `coda docs search \"competitive\"` |\n| Find research docs | `coda docs search \"research\"` |\n| Find Overleaf/Nova docs | `coda docs search \"overleaf\"` or `\"nova\"` |\n| Find Reflect docs | `coda docs search \"reflect\"` |\n\n# Global Options\n\n| Option | Description |\n|--------|-------------|\n| `--format conversational` | Human-readable output (default) |\n| `--format json` | Machine-readable JSON output |\n| `--format csv` | CSV output |\n| `--format table` | Tabular output |\n| `--ai-snippets` | Show AI agent code snippets |\n\n# Tips\n\n- **Document IDs** are in the URL: `coda.io/d/Doc-Name_d<DOC_ID>`\n- **Page IDs** start with `canvas-` (found via `pages list`)\n- **Table IDs** start with `grid-` or `table-` (found via `tables list`)\n- **Row IDs** are shown in `rows list` output\n- Use `--format json` for machine-readable output\n- Export pages as Markdown for easy reading\n\n# Installation\n\nIf `coda` command is not found, install from source:\n\n```bash\ncd ~/aves/coda-cli && uv tool install .\n```\n\nOr run directly without global install:\n\n```bash\ncd ~/aves/coda-cli && uv run coda <command>\n```\n\n# Error Handling\n\n| Error | Solution |\n|-------|----------|\n| `command not found: coda` | Install: `cd ~/aves/coda-cli && uv tool install .` |\n| `CODA_API_KEY not set` | Run `export CODA_API_KEY=\"your-key\"` |\n| `Invalid API key` | Get new key at https://coda.io/account |\n| `Document not found` | Verify DOC_ID from URL or search results |\n| `Table not found` | Run `coda tables list DOC_ID` for valid IDs |\n| `Page not found` | Run `coda pages list DOC_ID` for valid IDs |"
              },
              {
                "name": "searching-documents-with-google-drive",
                "description": "Search and download documents from Google Drive using rclone. Exports Google Docs as Markdown by default. Use when users ask to find files in Google Drive, download documents from Drive, export Google Docs, or sync Drive content locally. Triggers on requests mentioning Google Drive, gdrive, or document downloads from cloud storage.",
                "path": "plugins/kjgarza-product/skills/searching-documents-with-google-drive/SKILL.md",
                "frontmatter": {
                  "name": "searching-documents-with-google-drive",
                  "description": "Search and download documents from Google Drive using rclone. Exports Google Docs as Markdown by default. Use when users ask to find files in Google Drive, download documents from Drive, export Google Docs, or sync Drive content locally. Triggers on requests mentioning Google Drive, gdrive, or document downloads from cloud storage."
                },
                "content": "# Google Drive Document Access\n\n## Overview\n\nSearch for and download documents from Google Drive using rclone. Google Docs are automatically exported as Markdown for easy reading and processing.\n\nScripts are located at `.claude/searching-documents-with-google-drive/gdrive/scripts/`.\n\n**Reference Documentation:**\n- [scripts/](scripts/)  Shell scripts for search, download, and sync operations\n- [digital-science-reference.md](digital-science-reference.md)  Digital Science documentation links\n\n**Authentication:** See the Authentication section in [digital-science-reference.md](digital-science-reference.md) for rclone setup. The remote must be named `gdrive`.\n\n## Quick Start\n\n```bash\n# Check rclone is configured\n./scripts/check_rclone.sh\n\n# Search for files\n./scripts/search_gdrive.sh \"meeting notes\"\n\n# Search shared files\n./scripts/search_gdrive.sh \"standup\" --shared\n\n# Download a file (exports Google Docs as Markdown)\n./scripts/download_gdrive.sh \"Documents/Report\" ./output\n\n# Download by file ID\n./scripts/download_gdrive.sh --id \"FILE_ID\" ./output\n\n# Download matching shared files\n./scripts/download_gdrive.sh --shared --include \"pattern*\" ./output\n```\n\n## Common Tasks\n\n| Task | Command |\n|------|---------|\n| Check setup | `./scripts/check_rclone.sh` |\n| Search files | `./scripts/search_gdrive.sh \"pattern\"` |\n| Search shared files | `./scripts/search_gdrive.sh \"pattern\" --shared` |\n| Download file | `./scripts/download_gdrive.sh \"path/to/file\" ./output` |\n| Download by ID | `./scripts/download_gdrive.sh --id \"FILE_ID\" ./output` |\n| Download folder | `./scripts/download_gdrive.sh \"Folder\" ./output --bulk` |\n| Download shared files | `./scripts/download_gdrive.sh --shared --include \"pattern*\" ./output` |\n| Limit downloads | `./scripts/download_gdrive.sh --shared --include \"pattern*\" ./output --limit 20` |\n\n## Export Formats\n\nGoogle Docs can be exported in different formats:\n\n| Format | Flag | Use Case |\n|--------|------|----------|\n| Markdown | `--format md` (default) | Easy reading and processing |\n| Word | `--format docx` | Office compatibility |\n| PDF | `--format pdf` | Final documents |\n| Plain text | `--format txt` | Minimal formatting |\n| HTML | `--format html` | Web content |\n\nExample: `./scripts/download_gdrive.sh \"Report\" ./output --format docx`\n\n## File ID Extraction\n\nExtract file IDs from Google Drive URLs:\n- `https://docs.google.com/document/d/FILE_ID/edit`  use FILE_ID\n- `https://drive.google.com/file/d/FILE_ID/view`  use FILE_ID\n\n## Preview Before Download\n\nUse dry-run to see what would be downloaded:\n```bash\n./scripts/download_gdrive.sh \"Large Folder\" ./output --bulk --dry-run\n```\n\n## Direct rclone Commands\n\nFor advanced use cases:\n```bash\nrclone ls gdrive:\"Documents\"           # List files\nrclone lsjson gdrive:\"path\" --no-traverse  # Get file info as JSON\nrclone sync gdrive:\"Source\" ./local --drive-export-formats md  # Sync folder\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| \"remote not configured\" | Run `rclone config` to set up gdrive remote |\n| \"Cannot connect\" | Run `rclone config reconnect gdrive:` |\n| Empty search results | Check path spelling; try searching from root |\n| File not downloading | Verify file permissions; check if file is in Trash |\n| Shared files not found | Use `--shared` flag |\n| Too many files | Use `--limit N` to download only N most recent |"
              },
              {
                "name": "skill-creator",
                "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                "path": "plugins/kjgarza-product/skills/skill-creator/SKILL.md",
                "frontmatter": {
                  "name": "skill-creator",
                  "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again"
              },
              {
                "name": "xlsx",
                "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
                "path": "plugins/kjgarza-product/skills/xlsx/SKILL.md",
                "frontmatter": {
                  "name": "xlsx",
                  "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n###  WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n###  CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections"
              }
            ]
          }
        ]
      }
    }
  ]
}