{
  "owner": {
    "id": "kcns008",
    "display_name": "Chin K",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/3204061?u=857b4d988be031603f33baf9c599b70c7cbbf7f0&v=4",
    "url": "https://github.com/kcns008",
    "bio": "Consultant",
    "stats": {
      "total_repos": 1,
      "total_plugins": 8,
      "total_commands": 29,
      "total_skills": 0,
      "total_stars": 4,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "kcns008/cluster-code",
      "url": "https://github.com/kcns008/cluster-code",
      "description": "Manage K8s - AI-powered CLI tool for building, maintaining, and troubleshooting Kubernetes and OpenShift clusters",
      "homepage": "",
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2026-01-09T20:48:20Z",
        "created_at": "2025-10-27T03:14:59Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 4707
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/agents/architecture-review.md",
          "type": "blob",
          "size": 10836
        },
        {
          "path": ".claude/agents/cluster-analyzer.md",
          "type": "blob",
          "size": 4839
        },
        {
          "path": ".claude/agents/network-inspector.md",
          "type": "blob",
          "size": 8502
        },
        {
          "path": ".claude/agents/pod-doctor.md",
          "type": "blob",
          "size": 6908
        },
        {
          "path": ".claude/agents/refactoring-assistant.md",
          "type": "blob",
          "size": 17186
        },
        {
          "path": ".claude/agents/test-generator.md",
          "type": "blob",
          "size": 16199
        },
        {
          "path": ".claude/cluster-config.example.json",
          "type": "blob",
          "size": 860
        },
        {
          "path": ".claude/cluster-config.template.json",
          "type": "blob",
          "size": 4653
        },
        {
          "path": ".claude/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/commands/cluster-diagnose.md",
          "type": "blob",
          "size": 4010
        },
        {
          "path": ".claude/commands/cluster-init.md",
          "type": "blob",
          "size": 1800
        },
        {
          "path": ".claude/commands/cluster-status.md",
          "type": "blob",
          "size": 2412
        },
        {
          "path": ".claude/commands/commit-push-pr.md",
          "type": "blob",
          "size": 795
        },
        {
          "path": ".claude/commands/dedupe.md",
          "type": "blob",
          "size": 1876
        },
        {
          "path": ".claude/commands/pod-logs.md",
          "type": "blob",
          "size": 4031
        },
        {
          "path": ".claude/commands/resource-describe.md",
          "type": "blob",
          "size": 6409
        },
        {
          "path": ".claude/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/post-tool-use-tracker.sh",
          "type": "blob",
          "size": 3954
        },
        {
          "path": ".claude/hooks/skill-activation-prompt.sh",
          "type": "blob",
          "size": 4410
        },
        {
          "path": ".claude/skill-rules.json",
          "type": "blob",
          "size": 2637
        },
        {
          "path": ".claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/cluster-dev-guidelines.md",
          "type": "blob",
          "size": 19140
        },
        {
          "path": ".claude/skills/k8s-gitops",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/k8s-gitops/SKILL.md",
          "type": "blob",
          "size": 30872
        },
        {
          "path": ".claude/skills/k8s-manifests",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/k8s-manifests/SKILL.md",
          "type": "blob",
          "size": 28120
        },
        {
          "path": ".claude/skills/k8s-operations",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/k8s-operations/SKILL.md",
          "type": "blob",
          "size": 28093
        },
        {
          "path": ".claude/skills/k8s-security",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/k8s-security/SKILL.md",
          "type": "blob",
          "size": 23991
        },
        {
          "path": ".claude/skills/k8s-troubleshooting",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/k8s-troubleshooting/SKILL.md",
          "type": "blob",
          "size": 48485
        },
        {
          "path": ".claude/skills/openshift-popeye-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/openshift-popeye-analysis/SKILL.md",
          "type": "blob",
          "size": 42383
        },
        {
          "path": ".claude/skills/osgrep-skill.md",
          "type": "blob",
          "size": 11145
        },
        {
          "path": ".claude/skills/skill-developer.md",
          "type": "blob",
          "size": 13598
        },
        {
          "path": ".devcontainer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".devcontainer/Dockerfile",
          "type": "blob",
          "size": 2507
        },
        {
          "path": ".devcontainer/devcontainer.json",
          "type": "blob",
          "size": 1662
        },
        {
          "path": ".devcontainer/init-firewall.sh",
          "type": "blob",
          "size": 4578
        },
        {
          "path": ".eslintrc.json",
          "type": "blob",
          "size": 454
        },
        {
          "path": ".gitattributes",
          "type": "blob",
          "size": 35
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE/bug_report.yml",
          "type": "blob",
          "size": 5851
        },
        {
          "path": ".github/ISSUE_TEMPLATE/config.yml",
          "type": "blob",
          "size": 619
        },
        {
          "path": ".github/ISSUE_TEMPLATE/documentation.yml",
          "type": "blob",
          "size": 3489
        },
        {
          "path": ".github/ISSUE_TEMPLATE/feature_request.yml",
          "type": "blob",
          "size": 4208
        },
        {
          "path": ".github/ISSUE_TEMPLATE/model_behavior.yml",
          "type": "blob",
          "size": 6952
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/auto-close-duplicates.yml",
          "type": "blob",
          "size": 847
        },
        {
          "path": ".github/workflows/backfill-duplicate-comments.yml",
          "type": "blob",
          "size": 1143
        },
        {
          "path": ".github/workflows/claude-dedupe-issues.yml",
          "type": "blob",
          "size": 2862
        },
        {
          "path": ".github/workflows/claude-issue-triage.yml",
          "type": "blob",
          "size": 4829
        },
        {
          "path": ".github/workflows/claude.yml",
          "type": "blob",
          "size": 1206
        },
        {
          "path": ".github/workflows/issue-opened-dispatch.yml",
          "type": "blob",
          "size": 738
        },
        {
          "path": ".github/workflows/lock-closed-issues.yml",
          "type": "blob",
          "size": 3121
        },
        {
          "path": ".github/workflows/log-issue-events.yml",
          "type": "blob",
          "size": 1403
        },
        {
          "path": ".github/workflows/npm-publish.yml",
          "type": "blob",
          "size": 1616
        },
        {
          "path": ".github/workflows/oncall-triage.yml",
          "type": "blob",
          "size": 5742
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 412
        },
        {
          "path": ".npmignore",
          "type": "blob",
          "size": 400
        },
        {
          "path": ".prettierrc.json",
          "type": "blob",
          "size": 132
        },
        {
          "path": ".vscode",
          "type": "tree",
          "size": null
        },
        {
          "path": ".vscode/extensions.json",
          "type": "blob",
          "size": 155
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 46444
        },
        {
          "path": "CLAUDE_INFRASTRUCTURE_INTEGRATION.md",
          "type": "blob",
          "size": 12263
        },
        {
          "path": "CLUSTER_CODE_IMPLEMENTATION.md",
          "type": "blob",
          "size": 9561
        },
        {
          "path": "DEVELOPMENT.md",
          "type": "blob",
          "size": 7013
        },
        {
          "path": "IMPLEMENTATION_SUMMARY.md",
          "type": "blob",
          "size": 8263
        },
        {
          "path": "IMPROVEMENTS.md",
          "type": "blob",
          "size": 32309
        },
        {
          "path": "LICENSE.md",
          "type": "blob",
          "size": 150
        },
        {
          "path": "NPM_PUBLISHING_GUIDE.md",
          "type": "blob",
          "size": 4684
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 4011
        },
        {
          "path": "SECURITY.md",
          "type": "blob",
          "size": 693
        },
        {
          "path": "Script",
          "type": "tree",
          "size": null
        },
        {
          "path": "Script/run_devcontainer_claude_code.ps1",
          "type": "blob",
          "size": 5319
        },
        {
          "path": "USAGE.md",
          "type": "blob",
          "size": 507
        },
        {
          "path": "bin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bin/cluster-code.js",
          "type": "blob",
          "size": 217
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/Gemfile",
          "type": "blob",
          "size": 611
        },
        {
          "path": "docs/PROVIDERS.md",
          "type": "blob",
          "size": 8790
        },
        {
          "path": "docs/README.md",
          "type": "blob",
          "size": 5716
        },
        {
          "path": "docs/_config.yml",
          "type": "blob",
          "size": 3297
        },
        {
          "path": "docs/api.md",
          "type": "blob",
          "size": 639
        },
        {
          "path": "docs/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/api/commands.md",
          "type": "blob",
          "size": 15977
        },
        {
          "path": "docs/guides.md",
          "type": "blob",
          "size": 909
        },
        {
          "path": "docs/guides",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/guides/getting-started.md",
          "type": "blob",
          "size": 12412
        },
        {
          "path": "docs/guides/github-copilot-setup.md",
          "type": "blob",
          "size": 8141
        },
        {
          "path": "docs/guides/installation.md",
          "type": "blob",
          "size": 10916
        },
        {
          "path": "docs/guides/llm-providers.md",
          "type": "blob",
          "size": 15531
        },
        {
          "path": "docs/guides/pufferlib-rl.md",
          "type": "blob",
          "size": 6363
        },
        {
          "path": "docs/index.md",
          "type": "blob",
          "size": 9364
        },
        {
          "path": "docs/local-llm-support.md",
          "type": "blob",
          "size": 10071
        },
        {
          "path": "docs/tutorials.md",
          "type": "blob",
          "size": 765
        },
        {
          "path": "docs/tutorials",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/first-cluster.md",
          "type": "blob",
          "size": 13980
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/cluster-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/cluster-templates/aks",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/cluster-templates/aks/production-cluster.yaml",
          "type": "blob",
          "size": 5874
        },
        {
          "path": "examples/cluster-templates/aro",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/cluster-templates/aro/production-cluster.yaml",
          "type": "blob",
          "size": 9762
        },
        {
          "path": "examples/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/hooks/bash_command_validator_example.py",
          "type": "blob",
          "size": 2078
        },
        {
          "path": "examples/provider-config-examples.md",
          "type": "blob",
          "size": 7688
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 2262
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/README.md",
          "type": "blob",
          "size": 4109
        },
        {
          "path": "plugins/agent-sdk-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-sdk-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-sdk-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 190
        },
        {
          "path": "plugins/agent-sdk-dev/README.md",
          "type": "blob",
          "size": 6397
        },
        {
          "path": "plugins/agent-sdk-dev/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-sdk-dev/agents/agent-sdk-verifier-py.md",
          "type": "blob",
          "size": 5206
        },
        {
          "path": "plugins/agent-sdk-dev/agents/agent-sdk-verifier-ts.md",
          "type": "blob",
          "size": 5419
        },
        {
          "path": "plugins/agent-sdk-dev/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-sdk-dev/commands/new-sdk-app.md",
          "type": "blob",
          "size": 7846
        },
        {
          "path": "plugins/cloud-aws",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-aws/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-aws/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1666
        },
        {
          "path": "plugins/cloud-aws/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-aws/commands/aws-cluster-create.md",
          "type": "blob",
          "size": 13807
        },
        {
          "path": "plugins/cloud-aws/commands/aws-cluster-delete.md",
          "type": "blob",
          "size": 17965
        },
        {
          "path": "plugins/cloud-aws/commands/aws-cluster-upgrade.md",
          "type": "blob",
          "size": 22311
        },
        {
          "path": "plugins/cloud-aws/mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-aws/mcp/aws-operations.json",
          "type": "blob",
          "size": 11657
        },
        {
          "path": "plugins/cloud-azure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-azure/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-azure/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1819
        },
        {
          "path": "plugins/cloud-azure/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-azure/commands/azure-cluster-connect.md",
          "type": "blob",
          "size": 8579
        },
        {
          "path": "plugins/cloud-azure/commands/azure-cluster-create.md",
          "type": "blob",
          "size": 10337
        },
        {
          "path": "plugins/cloud-azure/commands/azure-cluster-delete.md",
          "type": "blob",
          "size": 14581
        },
        {
          "path": "plugins/cloud-azure/commands/azure-cluster-list.md",
          "type": "blob",
          "size": 6231
        },
        {
          "path": "plugins/cloud-azure/commands/azure-cluster-upgrade.md",
          "type": "blob",
          "size": 12781
        },
        {
          "path": "plugins/cloud-azure/mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-azure/mcp/azure-operations.json",
          "type": "blob",
          "size": 11963
        },
        {
          "path": "plugins/cloud-gcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-gcp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-gcp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 877
        },
        {
          "path": "plugins/cloud-gcp/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cloud-gcp/commands/gcp-cluster-create.md",
          "type": "blob",
          "size": 20230
        },
        {
          "path": "plugins/cloud-gcp/commands/gcp-cluster-delete.md",
          "type": "blob",
          "size": 20850
        },
        {
          "path": "plugins/cloud-gcp/commands/gcp-cluster-upgrade.md",
          "type": "blob",
          "size": 21730
        },
        {
          "path": "plugins/cluster-core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-core/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-core/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1022
        },
        {
          "path": "plugins/cluster-core/README.md",
          "type": "blob",
          "size": 6966
        },
        {
          "path": "plugins/cluster-core/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-core/agents/workload-analyzer.md",
          "type": "blob",
          "size": 10562
        },
        {
          "path": "plugins/cluster-core/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-core/commands/backup-cluster.md",
          "type": "blob",
          "size": 17091
        },
        {
          "path": "plugins/cluster-core/commands/multi-cluster-context.md",
          "type": "blob",
          "size": 16622
        },
        {
          "path": "plugins/cluster-core/commands/node-cordon.md",
          "type": "blob",
          "size": 9034
        },
        {
          "path": "plugins/cluster-core/commands/node-drain.md",
          "type": "blob",
          "size": 4812
        },
        {
          "path": "plugins/cluster-core/commands/node-status.md",
          "type": "blob",
          "size": 4117
        },
        {
          "path": "plugins/cluster-core/commands/node-uncordon.md",
          "type": "blob",
          "size": 13687
        },
        {
          "path": "plugins/cluster-core/commands/pvc-status.md",
          "type": "blob",
          "size": 8112
        },
        {
          "path": "plugins/cluster-core/commands/restore-cluster.md",
          "type": "blob",
          "size": 16835
        },
        {
          "path": "plugins/cluster-core/commands/service-describe.md",
          "type": "blob",
          "size": 8136
        },
        {
          "path": "plugins/cluster-openshift",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-openshift/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-openshift/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1398
        },
        {
          "path": "plugins/cluster-openshift/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/cluster-openshift/commands/operator-install.md",
          "type": "blob",
          "size": 14837
        },
        {
          "path": "plugins/cluster-openshift/commands/routes-analyze.md",
          "type": "blob",
          "size": 11934
        },
        {
          "path": "plugins/code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-review/README.md",
          "type": "blob",
          "size": 7321
        },
        {
          "path": "plugins/code-review/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-review/commands/code-review.md",
          "type": "blob",
          "size": 6008
        },
        {
          "path": "plugins/commit-commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commit-commands/README.md",
          "type": "blob",
          "size": 5908
        },
        {
          "path": "plugins/commit-commands/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commit-commands/commands/clean_gone.md",
          "type": "blob",
          "size": 1865
        },
        {
          "path": "plugins/commit-commands/commands/commit-push-pr.md",
          "type": "blob",
          "size": 796
        },
        {
          "path": "plugins/commit-commands/commands/commit.md",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/feature-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feature-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feature-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 291
        },
        {
          "path": "plugins/feature-dev/README.md",
          "type": "blob",
          "size": 11697
        },
        {
          "path": "plugins/feature-dev/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feature-dev/agents/code-architect.md",
          "type": "blob",
          "size": 2259
        },
        {
          "path": "plugins/feature-dev/agents/code-explorer.md",
          "type": "blob",
          "size": 2115
        },
        {
          "path": "plugins/feature-dev/agents/code-reviewer.md",
          "type": "blob",
          "size": 2994
        },
        {
          "path": "plugins/feature-dev/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feature-dev/commands/feature-dev.md",
          "type": "blob",
          "size": 5097
        },
        {
          "path": "plugins/gitops",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gitops/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gitops/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1824
        },
        {
          "path": "plugins/gitops/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gitops/commands/argocd-sync.md",
          "type": "blob",
          "size": 12504
        },
        {
          "path": "plugins/gitops/commands/flux-reconcile.md",
          "type": "blob",
          "size": 12618
        },
        {
          "path": "plugins/gitops/commands/helm-deploy.md",
          "type": "blob",
          "size": 14515
        },
        {
          "path": "plugins/gitops/commands/kustomize-apply.md",
          "type": "blob",
          "size": 14161
        },
        {
          "path": "plugins/k8sgpt-analyzers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/k8sgpt-analyzers/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/k8sgpt-analyzers/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 945
        },
        {
          "path": "plugins/k8sgpt-analyzers/analyzers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/k8sgpt-analyzers/analyzers/pod-analyzer.md",
          "type": "blob",
          "size": 8323
        },
        {
          "path": "plugins/pr-review-toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/pr-review-toolkit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/pr-review-toolkit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 282
        },
        {
          "path": "plugins/pr-review-toolkit/README.md",
          "type": "blob",
          "size": 7528
        },
        {
          "path": "plugins/pr-review-toolkit/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/pr-review-toolkit/agents/code-reviewer.md",
          "type": "blob",
          "size": 3985
        },
        {
          "path": "plugins/pr-review-toolkit/agents/code-simplifier.md",
          "type": "blob",
          "size": 5230
        },
        {
          "path": "plugins/pr-review-toolkit/agents/comment-analyzer.md",
          "type": "blob",
          "size": 5725
        },
        {
          "path": "plugins/pr-review-toolkit/agents/pr-test-analyzer.md",
          "type": "blob",
          "size": 4985
        },
        {
          "path": "plugins/pr-review-toolkit/agents/silent-failure-hunter.md",
          "type": "blob",
          "size": 7807
        },
        {
          "path": "plugins/pr-review-toolkit/agents/type-design-analyzer.md",
          "type": "blob",
          "size": 5368
        },
        {
          "path": "plugins/pr-review-toolkit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/pr-review-toolkit/commands/review-pr.md",
          "type": "blob",
          "size": 4997
        },
        {
          "path": "plugins/security-guidance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/security-guidance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/security-guidance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 306
        },
        {
          "path": "plugins/security-guidance/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/security-guidance/hooks/hooks.json",
          "type": "blob",
          "size": 382
        },
        {
          "path": "plugins/security-guidance/hooks/security_reminder_hook.py",
          "type": "blob",
          "size": 10767
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/auto-close-duplicates.ts",
          "type": "blob",
          "size": 7753
        },
        {
          "path": "scripts/backfill-duplicate-comments.ts",
          "type": "blob",
          "size": 6830
        },
        {
          "path": "src",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/agent/agent-client.ts",
          "type": "blob",
          "size": 28973
        },
        {
          "path": "src/agent/agent-session.ts",
          "type": "blob",
          "size": 20777
        },
        {
          "path": "src/agent/copilot-agent.ts",
          "type": "blob",
          "size": 8149
        },
        {
          "path": "src/agent/index.ts",
          "type": "blob",
          "size": 627
        },
        {
          "path": "src/agent/mcp-kubernetes.ts",
          "type": "blob",
          "size": 18654
        },
        {
          "path": "src/auth",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/auth/credential-store.ts",
          "type": "blob",
          "size": 8534
        },
        {
          "path": "src/auth/github-auth.ts",
          "type": "blob",
          "size": 11172
        },
        {
          "path": "src/auth/index.ts",
          "type": "blob",
          "size": 357
        },
        {
          "path": "src/chat",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/chat/index.ts",
          "type": "blob",
          "size": 5442
        },
        {
          "path": "src/cli.ts",
          "type": "blob",
          "size": 21755
        },
        {
          "path": "src/cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/cli/github-commands.ts",
          "type": "blob",
          "size": 8108
        },
        {
          "path": "src/cli/index.ts",
          "type": "blob",
          "size": 418
        },
        {
          "path": "src/cli/setup-wizard.ts",
          "type": "blob",
          "size": 11126
        },
        {
          "path": "src/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/commands/config.ts",
          "type": "blob",
          "size": 1444
        },
        {
          "path": "src/commands/diagnose.ts",
          "type": "blob",
          "size": 4988
        },
        {
          "path": "src/commands/index.ts",
          "type": "blob",
          "size": 582
        },
        {
          "path": "src/commands/info.ts",
          "type": "blob",
          "size": 3823
        },
        {
          "path": "src/commands/init.ts",
          "type": "blob",
          "size": 13724
        },
        {
          "path": "src/commands/provider.ts",
          "type": "blob",
          "size": 9010
        },
        {
          "path": "src/commands/rl.ts",
          "type": "blob",
          "size": 18611
        },
        {
          "path": "src/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/config/index.ts",
          "type": "blob",
          "size": 7908
        },
        {
          "path": "src/config/model-selector.ts",
          "type": "blob",
          "size": 8017
        },
        {
          "path": "src/index.ts",
          "type": "blob",
          "size": 227
        },
        {
          "path": "src/interactive",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/interactive/index.ts",
          "type": "blob",
          "size": 64
        },
        {
          "path": "src/interactive/session.ts",
          "type": "blob",
          "size": 11814
        },
        {
          "path": "src/llm",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/llm/client.ts",
          "type": "blob",
          "size": 2861
        },
        {
          "path": "src/llm/index.ts",
          "type": "blob",
          "size": 279
        },
        {
          "path": "src/llm/provider.ts",
          "type": "blob",
          "size": 11281
        },
        {
          "path": "src/providers",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/providers/copilot-provider.ts",
          "type": "blob",
          "size": 9145
        },
        {
          "path": "src/providers/index.ts",
          "type": "blob",
          "size": 221
        },
        {
          "path": "src/pufferlib",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/pufferlib/checker.ts",
          "type": "blob",
          "size": 4324
        },
        {
          "path": "src/pufferlib/config.ts",
          "type": "blob",
          "size": 1512
        },
        {
          "path": "src/pufferlib/environments",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/pufferlib/environments/cluster_env.py",
          "type": "blob",
          "size": 22471
        },
        {
          "path": "src/pufferlib/index.ts",
          "type": "blob",
          "size": 265
        },
        {
          "path": "src/pufferlib/setup.ts",
          "type": "blob",
          "size": 5850
        },
        {
          "path": "src/tui",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/tui/agent-integration.ts",
          "type": "blob",
          "size": 2989
        },
        {
          "path": "src/tui/index.ts",
          "type": "blob",
          "size": 15078
        },
        {
          "path": "src/tui/layout.ts",
          "type": "blob",
          "size": 6837
        },
        {
          "path": "src/tui/state.ts",
          "type": "blob",
          "size": 4817
        },
        {
          "path": "src/types",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/types/index.ts",
          "type": "blob",
          "size": 2646
        },
        {
          "path": "src/types/optional-deps.d.ts",
          "type": "blob",
          "size": 1016
        },
        {
          "path": "src/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/utils/cli-context.ts",
          "type": "blob",
          "size": 12437
        },
        {
          "path": "src/utils/cli-version-manager.ts",
          "type": "blob",
          "size": 13766
        },
        {
          "path": "src/utils/cluster-detector.ts",
          "type": "blob",
          "size": 9245
        },
        {
          "path": "src/utils/kubectl.ts",
          "type": "blob",
          "size": 8848
        },
        {
          "path": "src/utils/logger.ts",
          "type": "blob",
          "size": 1782
        },
        {
          "path": "src/utils/oauth-server.ts",
          "type": "blob",
          "size": 6779
        },
        {
          "path": "tmp_verification",
          "type": "tree",
          "size": null
        },
        {
          "path": "tmp_verification/package.json",
          "type": "blob",
          "size": 320
        },
        {
          "path": "tsconfig.json",
          "type": "blob",
          "size": 852
        }
      ],
      "marketplace": {
        "name": "cluster-code-plugins",
        "version": "1.0.0",
        "description": "Bundled plugins for Cluster Code including Kubernetes cluster operations, AI-powered diagnostics, and troubleshooting tools",
        "owner_info": {
          "name": "Cluster Code Team",
          "email": "support@cluster-code.io"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "cluster-core",
            "description": "Core Kubernetes cluster operations and resource management commands",
            "source": "./plugins/cluster-core",
            "category": "cluster-management",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install cluster-core@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/backup-cluster",
                "description": "Create comprehensive cluster backup using Velero",
                "path": "plugins/cluster-core/commands/backup-cluster.md",
                "frontmatter": {
                  "name": "backup-cluster",
                  "description": "Create comprehensive cluster backup using Velero",
                  "category": "backup-restore",
                  "parameters": [
                    {
                      "name": "backup-name",
                      "description": "Name for this backup (auto-generated if not provided)",
                      "required": false
                    },
                    {
                      "name": "include-namespaces",
                      "description": "Comma-separated list of namespaces to backup",
                      "required": false
                    },
                    {
                      "name": "exclude-namespaces",
                      "description": "Comma-separated list of namespaces to exclude",
                      "required": false
                    },
                    {
                      "name": "include-resources",
                      "description": "Resources to include (e.g., pods,deployments)",
                      "required": false
                    },
                    {
                      "name": "exclude-resources",
                      "description": "Resources to exclude",
                      "required": false
                    },
                    {
                      "name": "selector",
                      "description": "Label selector to filter resources",
                      "required": false
                    },
                    {
                      "name": "snapshot-volumes",
                      "description": "Create volume snapshots (requires CSI support)",
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "ttl",
                      "description": "Backup retention time (e.g., 720h for 30 days)",
                      "default": "720h"
                    },
                    {
                      "name": "wait",
                      "description": "Wait for backup to complete",
                      "type": "boolean",
                      "default": true
                    }
                  ],
                  "tags": [
                    "backup",
                    "disaster-recovery",
                    "velero",
                    "data-protection"
                  ]
                },
                "content": "# Cluster Backup\n\nCreate comprehensive Kubernetes cluster backups using Velero, including resources, configurations, and persistent volumes.\n\n## Overview\n\nThis command creates full cluster backups with:\n\n- **Resource Backup**: All Kubernetes resources (pods, deployments, configmaps, etc.)\n- **Persistent Volume Snapshots**: Volume data using CSI or cloud-native snapshots\n- **Selective Backup**: Namespace, resource type, and label filtering\n- **Automated Retention**: Configurable backup expiration\n- **Cloud Integration**: Works with AWS, Azure, GCP, and S3-compatible storage\n\n## Prerequisites\n\n- `velero` CLI installed (`velero version` >= 1.12.0)\n- Velero server installed in cluster\n- Cloud storage bucket configured (S3, GCS, Azure Blob)\n- Volume snapshot provider configured (for PV snapshots)\n- `kubectl` access to cluster\n\n## Initial Velero Setup\n\nIf Velero is not yet installed, set it up first:\n\n### AWS (S3 + EBS Snapshots)\n\n```bash\n# Create S3 bucket\naws s3 mb s3://my-velero-backups --region us-west-2\n\n# Create IAM user with permissions\ncat > velero-policy.json <<EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:*\", \"ec2:DescribeVolumes\", \"ec2:DescribeSnapshots\",\n                       \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:CreateSnapshot\"],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nEOF\n\naws iam create-policy --policy-name VeleroPolicy --policy-document file://velero-policy.json\n\n# Install Velero\nvelero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.8.0 \\\n    --bucket my-velero-backups \\\n    --backup-location-config region=us-west-2 \\\n    --snapshot-location-config region=us-west-2 \\\n    --secret-file ./credentials-velero\n```\n\n### Azure (Blob Storage + Disk Snapshots)\n\n```bash\n# Create resource group and storage account\naz storage account create \\\n    --name velerostorage \\\n    --resource-group velero-rg \\\n    --sku Standard_GRS \\\n    --encryption-services blob\n\n# Install Velero\nvelero install \\\n    --provider azure \\\n    --plugins velero/velero-plugin-for-microsoft-azure:v1.8.0 \\\n    --bucket velero-backups \\\n    --secret-file ./credentials-velero \\\n    --backup-location-config resourceGroup=velero-rg,storageAccount=velerostorage \\\n    --snapshot-location-config resourceGroup=velero-rg\n```\n\n### GCP (GCS + Disk Snapshots)\n\n```bash\n# Create GCS bucket\ngsutil mb gs://my-velero-backups/\n\n# Install Velero\nvelero install \\\n    --provider gcp \\\n    --plugins velero/velero-plugin-for-gcp:v1.8.0 \\\n    --bucket my-velero-backups \\\n    --secret-file ./credentials-velero\n```\n\n## Workflow\n\n### Phase 1: Pre-backup Validation\n\n#### 1.1 Verify Velero Installation\n\n```bash\nBACKUP_NAME=\"${BACKUP_NAME:-cluster-backup-$(date +%Y%m%d-%H%M%S)}\"\nSNAPSHOT_VOLUMES=\"${SNAPSHOT_VOLUMES:-true}\"\nTTL=\"${TTL:-720h}\"\nWAIT=\"${WAIT:-true}\"\n\necho \"üîç Validating Velero installation...\"\necho \"\"\n\n# Check if Velero CLI is installed\nif ! command -v velero &>/dev/null; then\n    echo \"‚ùå ERROR: Velero CLI not found\"\n    echo \"\"\n    echo \"Install Velero CLI:\"\n    echo \"  # macOS\"\n    echo \"  brew install velero\"\n    echo \"\"\n    echo \"  # Linux\"\n    echo \"  wget https://github.com/vmware-tanzu/velero/releases/latest/download/velero-linux-amd64.tar.gz\"\n    echo \"  tar -xvf velero-linux-amd64.tar.gz\"\n    echo \"  sudo mv velero-linux-amd64/velero /usr/local/bin/\"\n    exit 1\nfi\n\nVELERO_VERSION=$(velero version --client-only 2>/dev/null | grep \"Client\" | awk '{print $2}')\necho \"‚úÖ Velero CLI: $VELERO_VERSION\"\n\n# Check if Velero is deployed in cluster\nif ! kubectl get namespace velero &>/dev/null; then\n    echo \"‚ùå ERROR: Velero not installed in cluster\"\n    echo \"\"\n    echo \"Install Velero in your cluster first. See examples in this command.\"\n    exit 1\nfi\n\n# Check Velero server status\nVELERO_POD=$(kubectl get pods -n velero -l app.kubernetes.io/name=velero -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n\nif [[ -z \"$VELERO_POD\" ]]; then\n    echo \"‚ùå ERROR: Velero pod not found\"\n    exit 1\nfi\n\nVELERO_STATUS=$(kubectl get pod -n velero \"$VELERO_POD\" -o jsonpath='{.status.phase}')\nif [[ \"$VELERO_STATUS\" != \"Running\" ]]; then\n    echo \"‚ùå ERROR: Velero pod not running (status: $VELERO_STATUS)\"\n    exit 1\nfi\n\necho \"‚úÖ Velero server: Running\"\n\n# Check backup locations\nBACKUP_LOCATIONS=$(velero backup-location get -o json 2>/dev/null | jq -r '.items[] | select(.status.phase==\"Available\") | .metadata.name' | wc -l)\n\nif [[ $BACKUP_LOCATIONS -eq 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: No backup locations available\"\n    echo \"\"\n    velero backup-location get\n    echo \"\"\n    echo \"Configure a backup location before continuing\"\n    exit 1\nfi\n\necho \"‚úÖ Backup locations: $BACKUP_LOCATIONS available\"\n\n# Check snapshot locations (if snapshots enabled)\nif [[ \"$SNAPSHOT_VOLUMES\" == \"true\" ]]; then\n    SNAPSHOT_LOCATIONS=$(velero snapshot-location get -o json 2>/dev/null | jq -r '.items[] | select(.status.phase==\"Available\") | .metadata.name' | wc -l)\n\n    if [[ $SNAPSHOT_LOCATIONS -eq 0 ]]; then\n        echo \"‚ö†Ô∏è  WARNING: No snapshot locations available\"\n        echo \"   Volume snapshots will be skipped\"\n        SNAPSHOT_VOLUMES=\"false\"\n    else\n        echo \"‚úÖ Snapshot locations: $SNAPSHOT_LOCATIONS available\"\n    fi\nfi\n\necho \"\"\n```\n\n#### 1.2 Analyze Cluster Resources\n\n```bash\necho \"üìä Analyzing cluster resources...\"\necho \"\"\n\n# Count resources\nNAMESPACE_COUNT=$(kubectl get namespaces --no-headers | wc -l)\nPOD_COUNT=$(kubectl get pods --all-namespaces --no-headers | wc -l)\nPV_COUNT=$(kubectl get pv --no-headers | wc -l)\nPVC_COUNT=$(kubectl get pvc --all-namespaces --no-headers | wc -l)\nDEPLOYMENT_COUNT=$(kubectl get deployments --all-namespaces --no-headers | wc -l)\nSERVICE_COUNT=$(kubectl get services --all-namespaces --no-headers | wc -l)\nCONFIGMAP_COUNT=$(kubectl get configmaps --all-namespaces --no-headers | wc -l)\nSECRET_COUNT=$(kubectl get secrets --all-namespaces --no-headers | wc -l)\n\necho \"Cluster resources:\"\necho \"  Namespaces: $NAMESPACE_COUNT\"\necho \"  Pods: $POD_COUNT\"\necho \"  Deployments: $DEPLOYMENT_COUNT\"\necho \"  Services: $SERVICE_COUNT\"\necho \"  ConfigMaps: $CONFIGMAP_COUNT\"\necho \"  Secrets: $SECRET_COUNT\"\necho \"  Persistent Volumes: $PV_COUNT\"\necho \"  Persistent Volume Claims: $PVC_COUNT\"\n\n# Estimate backup size\nif [[ $PV_COUNT -gt 0 ]]; then\n    echo \"\"\n    echo \"Persistent Volume sizes:\"\n    kubectl get pv -o custom-columns=NAME:.metadata.name,SIZE:.spec.capacity.storage,STORAGECLASS:.spec.storageClassName --no-headers | head -10\n\n    TOTAL_PV_SIZE=$(kubectl get pv -o json | jq -r '[.items[].spec.capacity.storage] | map(gsub(\"Gi\";\"\") | tonumber) | add')\n    if [[ -n \"$TOTAL_PV_SIZE\" && \"$TOTAL_PV_SIZE\" != \"null\" ]]; then\n        echo \"\"\n        echo \"  Total PV capacity: ~${TOTAL_PV_SIZE}Gi\"\n    fi\nfi\n\necho \"\"\n```\n\n### Phase 2: Build Backup Command\n\n#### 2.1 Construct Velero Backup Command\n\n```bash\necho \"üõ†Ô∏è  Building backup command...\"\necho \"\"\n\nBACKUP_CMD=\"velero backup create \\\"$BACKUP_NAME\\\"\"\n\n# Namespace inclusion/exclusion\nif [[ -n \"${INCLUDE_NAMESPACES}\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --include-namespaces=\\\"${INCLUDE_NAMESPACES}\\\"\"\n    echo \"  Including namespaces: ${INCLUDE_NAMESPACES}\"\nelif [[ -n \"${EXCLUDE_NAMESPACES}\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --exclude-namespaces=\\\"${EXCLUDE_NAMESPACES}\\\"\"\n    echo \"  Excluding namespaces: ${EXCLUDE_NAMESPACES}\"\nelse\n    # Exclude Velero namespace by default\n    BACKUP_CMD=\"$BACKUP_CMD --exclude-namespaces=velero\"\n    echo \"  Backing up: All namespaces (except velero)\"\nfi\n\n# Resource inclusion/exclusion\nif [[ -n \"${INCLUDE_RESOURCES}\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --include-resources=\\\"${INCLUDE_RESOURCES}\\\"\"\n    echo \"  Including resources: ${INCLUDE_RESOURCES}\"\nfi\n\nif [[ -n \"${EXCLUDE_RESOURCES}\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --exclude-resources=\\\"${EXCLUDE_RESOURCES}\\\"\"\n    echo \"  Excluding resources: ${EXCLUDE_RESOURCES}\"\nfi\n\n# Label selector\nif [[ -n \"${SELECTOR}\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --selector=\\\"${SELECTOR}\\\"\"\n    echo \"  Label selector: ${SELECTOR}\"\nfi\n\n# Snapshot volumes\nif [[ \"$SNAPSHOT_VOLUMES\" == \"true\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --snapshot-volumes=true\"\n    echo \"  Volume snapshots: Enabled\"\nelse\n    BACKUP_CMD=\"$BACKUP_CMD --snapshot-volumes=false\"\n    echo \"  Volume snapshots: Disabled\"\nfi\n\n# TTL\nBACKUP_CMD=\"$BACKUP_CMD --ttl=\\\"$TTL\\\"\"\necho \"  Retention (TTL): $TTL\"\n\n# Wait for completion\nif [[ \"$WAIT\" == \"true\" ]]; then\n    BACKUP_CMD=\"$BACKUP_CMD --wait\"\nfi\n\necho \"\"\necho \"Backup command:\"\necho \"  $BACKUP_CMD\"\necho \"\"\n```\n\n### Phase 3: Create Backup\n\n#### 3.1 Execute Backup\n\n```bash\necho \"üíæ Creating backup...\"\necho \"\"\n\nBACKUP_START=$(date +%s)\n\n# Execute backup\nif eval \"$BACKUP_CMD\"; then\n    echo \"\"\n    echo \"‚úÖ Backup creation initiated: $BACKUP_NAME\"\nelse\n    echo \"\"\n    echo \"‚ùå Backup creation failed\"\n    exit 1\nfi\n\n# Monitor backup progress\nif [[ \"$WAIT\" == \"true\" ]]; then\n    echo \"\"\n    echo \"‚è≥ Waiting for backup to complete...\"\n    echo \"\"\n\n    # Backup is already complete due to --wait flag\n    # Just show the final status\n    sleep 2\nfi\n```\n\n### Phase 4: Verification\n\n#### 4.1 Check Backup Status\n\n```bash\necho \"\"\necho \"üîç Verifying backup...\"\necho \"\"\n\n# Get backup details\nBACKUP_INFO=$(velero backup describe \"$BACKUP_NAME\" --details 2>/dev/null)\n\nif [[ -z \"$BACKUP_INFO\" ]]; then\n    echo \"‚ùå Could not retrieve backup details\"\n    exit 1\nfi\n\n# Extract key information\nBACKUP_STATUS=$(velero backup get \"$BACKUP_NAME\" -o json 2>/dev/null | jq -r '.status.phase')\nBACKUP_ERRORS=$(velero backup get \"$BACKUP_NAME\" -o json 2>/dev/null | jq -r '.status.errors // 0')\nBACKUP_WARNINGS=$(velero backup get \"$BACKUP_NAME\" -o json 2>/dev/null | jq -r '.status.warnings // 0')\n\necho \"Backup Status: $BACKUP_STATUS\"\necho \"Errors: $BACKUP_ERRORS\"\necho \"Warnings: $BACKUP_WARNINGS\"\n\nif [[ \"$BACKUP_STATUS\" == \"Completed\" ]]; then\n    echo \"‚úÖ Backup completed successfully\"\nelif [[ \"$BACKUP_STATUS\" == \"PartiallyFailed\" ]]; then\n    echo \"‚ö†Ô∏è  Backup partially failed\"\n    echo \"\"\n    echo \"Check errors with:\"\n    echo \"  velero backup logs $BACKUP_NAME\"\nelif [[ \"$BACKUP_STATUS\" == \"Failed\" ]]; then\n    echo \"‚ùå Backup failed\"\n    echo \"\"\n    echo \"Check errors with:\"\n    echo \"  velero backup logs $BACKUP_NAME\"\n    exit 1\nelif [[ \"$BACKUP_STATUS\" == \"InProgress\" ]]; then\n    echo \"‚è≥ Backup still in progress\"\nelse\n    echo \"‚ùì Unknown backup status: $BACKUP_STATUS\"\nfi\n\necho \"\"\n```\n\n#### 4.2 Display Backup Details\n\n```bash\necho \"üìã Backup Details:\"\necho \"\"\n\nvelero backup describe \"$BACKUP_NAME\" | head -50\n\necho \"\"\necho \"Resource summary:\"\nBACKUP_JSON=$(velero backup get \"$BACKUP_NAME\" -o json 2>/dev/null)\n\n# Show backed up resources\necho \"$BACKUP_JSON\" | jq -r '.status.progress.itemsBackedUp // 0' | \\\n    xargs -I {} echo \"  Items backed up: {}\"\n\necho \"$BACKUP_JSON\" | jq -r '.status.progress.totalItems // 0' | \\\n    xargs -I {} echo \"  Total items: {}\"\n\n# Show volume snapshots\nif [[ \"$SNAPSHOT_VOLUMES\" == \"true\" ]]; then\n    SNAPSHOT_COUNT=$(echo \"$BACKUP_JSON\" | jq -r '.status.volumeSnapshotsCompleted // 0')\n    echo \"  Volume snapshots: $SNAPSHOT_COUNT\"\nfi\n\n# Calculate backup duration\nBACKUP_END=$(date +%s)\nBACKUP_DURATION=$(( BACKUP_END - BACKUP_START ))\necho \"\"\necho \"Backup duration: ${BACKUP_DURATION}s\"\n\n# Show backup location\nBACKUP_LOCATION=$(echo \"$BACKUP_JSON\" | jq -r '.spec.storageLocation')\necho \"Storage location: $BACKUP_LOCATION\"\n\n# Show backup size (if available)\nBACKUP_SIZE=$(velero backup describe \"$BACKUP_NAME\" --details 2>/dev/null | grep \"Resource List\" -A 100 | grep \"Total\" | awk '{print $2}' || echo \"Unknown\")\necho \"Backup size: $BACKUP_SIZE\"\n\necho \"\"\n```\n\n### Phase 5: Summary and Next Steps\n\n```bash\necho \"‚úÖ BACKUP COMPLETE\"\necho \"==================\"\necho \"\"\necho \"Backup name: $BACKUP_NAME\"\necho \"Status: $BACKUP_STATUS\"\necho \"Retention: $TTL\"\necho \"\"\necho \"Backup includes:\"\nif [[ -n \"${INCLUDE_NAMESPACES}\" ]]; then\n    echo \"  ‚Ä¢ Namespaces: ${INCLUDE_NAMESPACES}\"\nelse\n    echo \"  ‚Ä¢ All namespaces (except velero)\"\nfi\n\nif [[ \"$SNAPSHOT_VOLUMES\" == \"true\" ]]; then\n    echo \"  ‚Ä¢ Persistent volume snapshots\"\nfi\n\necho \"\"\necho \"View backup details:\"\necho \"  velero backup describe $BACKUP_NAME\"\necho \"\"\necho \"View backup logs:\"\necho \"  velero backup logs $BACKUP_NAME\"\necho \"\"\necho \"Download backup:\"\necho \"  velero backup download $BACKUP_NAME\"\necho \"\"\necho \"Restore from this backup:\"\necho \"  cluster-code restore-cluster --backup-name $BACKUP_NAME\"\necho \"  # or\"\necho \"  velero restore create --from-backup $BACKUP_NAME\"\necho \"\"\necho \"List all backups:\"\necho \"  velero backup get\"\necho \"\"\necho \"Delete this backup:\"\necho \"  velero backup delete $BACKUP_NAME\"\necho \"\"\n\nif [[ $BACKUP_ERRORS -gt 0 || $BACKUP_WARNINGS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  Review errors and warnings:\"\n    echo \"  velero backup logs $BACKUP_NAME | grep -E 'error|warning'\"\n    echo \"\"\nfi\n```\n\n## Backup Strategies\n\n### Full Cluster Backup (default)\n\nBacks up all resources and volumes:\n\n```bash\ncluster-code backup-cluster --backup-name full-backup-$(date +%Y%m%d)\n```\n\n### Namespace-Specific Backup\n\nBackup specific namespaces:\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name prod-backup \\\n  --include-namespaces production,staging\n```\n\n### Application-Specific Backup\n\nBackup by label selector:\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name app-backup \\\n  --selector app=myapp,tier=frontend\n```\n\n### Configuration-Only Backup\n\nExclude volumes for faster backup:\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name config-backup \\\n  --snapshot-volumes=false\n```\n\n### Scheduled Backups\n\nCreate a Velero schedule for automated backups:\n\n```bash\n# Daily backups at 3 AM, retain for 30 days\nvelero schedule create daily-backup \\\n  --schedule=\"0 3 * * *\" \\\n  --ttl 720h\n\n# Weekly full backups on Sundays\nvelero schedule create weekly-backup \\\n  --schedule=\"0 2 * * 0\" \\\n  --ttl 2160h\n```\n\n## Examples\n\n### Example 1: Full cluster backup\n\n```bash\ncluster-code backup-cluster --backup-name production-full-backup\n```\n\n### Example 2: Backup specific namespaces\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name app-backup \\\n  --include-namespaces default,myapp,monitoring\n```\n\n### Example 3: Backup without volume snapshots\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name config-only \\\n  --snapshot-volumes=false\n```\n\n### Example 4: Backup with custom retention\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name quarterly-backup \\\n  --ttl 2160h\n```\n\n### Example 5: Backup specific resources\n\n```bash\ncluster-code backup-cluster \\\n  --backup-name deployments-backup \\\n  --include-resources deployments,services,configmaps\n```\n\n## Common Issues\n\n### Issue: \"BackupStorageLocation is unavailable\"\n\n**Solution**: Check backup location configuration:\n\n```bash\nvelero backup-location get\nkubectl describe backupstoragelocation -n velero\n```\n\n### Issue: Volume snapshots failing\n\n**Solution**: Verify snapshot configuration:\n\n```bash\nvelero snapshot-location get\nkubectl get volumesnapshotclass\n```\n\n### Issue: Backup stuck in \"InProgress\"\n\n**Solution**: Check Velero logs:\n\n```bash\nkubectl logs -n velero -l app.kubernetes.io/name=velero\n```\n\n## Best Practices\n\n1. **Regular backups** - Schedule daily backups for production\n2. **Test restores** - Regularly test restore procedures\n3. **Offsite storage** - Use cloud storage in different region\n4. **Retention policy** - Balance storage costs with recovery needs\n5. **Monitor backups** - Set up alerts for backup failures\n6. **Namespace organization** - Group related resources for selective backups\n7. **Document procedures** - Maintain runbooks for disaster recovery\n\n## Related Commands\n\n- `restore-cluster`: Restore from backup\n- `cluster-diagnose`: Check cluster health before backup\n- `aws-cluster-delete`: Delete cluster (after backup)\n- `velero backup get`: List all backups\n- `velero schedule create`: Schedule automated backups"
              },
              {
                "name": "/multi-cluster-context",
                "description": "Manage and switch between multiple Kubernetes cluster contexts",
                "path": "plugins/cluster-core/commands/multi-cluster-context.md",
                "frontmatter": {
                  "name": "multi-cluster-context",
                  "description": "Manage and switch between multiple Kubernetes cluster contexts",
                  "category": "cluster-management",
                  "parameters": [
                    {
                      "name": "action",
                      "description": "Action to perform (list, switch, current, rename, delete)",
                      "required": true
                    },
                    {
                      "name": "context",
                      "description": "Context name (for switch, rename, delete actions)",
                      "required": false
                    },
                    {
                      "name": "new-name",
                      "description": "New context name (for rename action)",
                      "required": false
                    }
                  ],
                  "tags": [
                    "contexts",
                    "multi-cluster",
                    "kubectl",
                    "cluster-management"
                  ]
                },
                "content": "# Multi-Cluster Context Management\n\nManage multiple Kubernetes cluster contexts with an enhanced interface for switching between clusters.\n\n## Overview\n\nThis command provides an improved experience for managing kubectl contexts across multiple clusters:\n\n- **List contexts** - View all configured contexts with cluster details\n- **Switch contexts** - Quickly change active cluster\n- **Show current** - Display active context information\n- **Rename contexts** - Give contexts meaningful names\n- **Delete contexts** - Remove unused contexts\n\n## Prerequisites\n\n- `kubectl` installed and configured\n- At least one Kubernetes cluster context configured\n\n## Workflow\n\n### Phase 1: Parse Action\n\n```bash\nACTION=\"${ACTION}\"\nCONTEXT_NAME=\"${CONTEXT}\"\nNEW_NAME=\"${NEW_NAME}\"\n\ncase \"$ACTION\" in\n    list|ls)\n        ACTION_TYPE=\"list\"\n        ;;\n    switch|use)\n        ACTION_TYPE=\"switch\"\n        if [[ -z \"$CONTEXT_NAME\" ]]; then\n            echo \"‚ùå ERROR: Context name required for switch action\"\n            exit 1\n        fi\n        ;;\n    current|show)\n        ACTION_TYPE=\"current\"\n        ;;\n    rename|mv)\n        ACTION_TYPE=\"rename\"\n        if [[ -z \"$CONTEXT_NAME\" || -z \"$NEW_NAME\" ]]; then\n            echo \"‚ùå ERROR: Both context and new-name required for rename action\"\n            exit 1\n        fi\n        ;;\n    delete|rm)\n        ACTION_TYPE=\"delete\"\n        if [[ -z \"$CONTEXT_NAME\" ]]; then\n            echo \"‚ùå ERROR: Context name required for delete action\"\n            exit 1\n        fi\n        ;;\n    *)\n        echo \"‚ùå ERROR: Unknown action: $ACTION\"\n        echo \"\"\n        echo \"Valid actions: list, switch, current, rename, delete\"\n        exit 1\n        ;;\nesac\n```\n\n### Phase 2: Execute Action\n\n#### 2.1 List Contexts\n\n```bash\nif [[ \"$ACTION_TYPE\" == \"list\" ]]; then\n    echo \"üìã KUBERNETES CONTEXTS\"\n    echo \"======================\"\n    echo \"\"\n\n    # Get current context\n    CURRENT_CONTEXT=$(kubectl config current-context 2>/dev/null || echo \"none\")\n\n    # Get all contexts\n    CONTEXTS=$(kubectl config get-contexts -o name 2>/dev/null)\n\n    if [[ -z \"$CONTEXTS\" ]]; then\n        echo \"No contexts found\"\n        echo \"\"\n        echo \"Add a cluster context:\"\n        echo \"  ‚Ä¢ AWS EKS: aws eks update-kubeconfig --name <cluster> --region <region>\"\n        echo \"  ‚Ä¢ Azure AKS: az aks get-credentials --name <cluster> --resource-group <rg>\"\n        echo \"  ‚Ä¢ GCP GKE: gcloud container clusters get-credentials <cluster> --zone <zone>\"\n        exit 0\n    fi\n\n    echo \"Available contexts:\"\n    echo \"\"\n\n    # Format: NAME | CLUSTER | USER | NAMESPACE | CURRENT\n    printf \"%-40s %-30s %-20s %-15s %s\\n\" \"CONTEXT\" \"CLUSTER\" \"USER\" \"NAMESPACE\" \"CURRENT\"\n    printf \"%-40s %-30s %-20s %-15s %s\\n\" \"-------\" \"-------\" \"----\" \"---------\" \"-------\"\n\n    while IFS= read -r CTX; do\n        # Get context details\n        CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CTX\\\")].context.cluster}\" 2>/dev/null)\n        USER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CTX\\\")].context.user}\" 2>/dev/null)\n        NAMESPACE=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CTX\\\")].context.namespace}\" 2>/dev/null)\n\n        # Truncate long names\n        CTX_DISPLAY=$(echo \"$CTX\" | cut -c1-39)\n        CLUSTER_DISPLAY=$(echo \"$CLUSTER\" | cut -c1-29)\n        USER_DISPLAY=$(echo \"$USER\" | cut -c1-19)\n        NAMESPACE_DISPLAY=${NAMESPACE:-default}\n\n        # Mark current context\n        if [[ \"$CTX\" == \"$CURRENT_CONTEXT\" ]]; then\n            CURRENT_MARK=\"*\"\n        else\n            CURRENT_MARK=\"\"\n        fi\n\n        printf \"%-40s %-30s %-20s %-15s %s\\n\" \\\n            \"$CTX_DISPLAY\" \\\n            \"$CLUSTER_DISPLAY\" \\\n            \"$USER_DISPLAY\" \\\n            \"$NAMESPACE_DISPLAY\" \\\n            \"$CURRENT_MARK\"\n    done <<< \"$CONTEXTS\"\n\n    echo \"\"\n    echo \"Current context: $CURRENT_CONTEXT\"\n    echo \"\"\n    echo \"Switch context:\"\n    echo \"  cluster-code multi-cluster-context --action switch --context <name>\"\n    echo \"\"\n\n    # Show cluster versions if possible\n    echo \"Cluster versions:\"\n    echo \"\"\n\n    for CTX in $CONTEXTS; do\n        CTX_SHORT=$(echo \"$CTX\" | cut -c1-30)\n        # Try to get version (this may fail if cluster is unreachable)\n        VERSION=$(kubectl --context=\"$CTX\" version --short 2>/dev/null | grep \"Server Version\" | awk '{print $3}' || echo \"unreachable\")\n\n        if [[ \"$VERSION\" != \"unreachable\" ]]; then\n            printf \"  %-30s %s\\n\" \"$CTX_SHORT:\" \"$VERSION\"\n        fi\n    done\n\n    echo \"\"\nfi\n```\n\n#### 2.2 Switch Context\n\n```bash\nif [[ \"$ACTION_TYPE\" == \"switch\" ]]; then\n    echo \"üîÑ Switching context...\"\n    echo \"\"\n\n    # Check if context exists\n    if ! kubectl config get-contexts \"$CONTEXT_NAME\" &>/dev/null; then\n        echo \"‚ùå ERROR: Context not found: $CONTEXT_NAME\"\n        echo \"\"\n        echo \"Available contexts:\"\n        kubectl config get-contexts -o name\n        exit 1\n    fi\n\n    # Get previous context\n    PREVIOUS_CONTEXT=$(kubectl config current-context 2>/dev/null || echo \"none\")\n\n    # Switch context\n    if kubectl config use-context \"$CONTEXT_NAME\" &>/dev/null; then\n        echo \"‚úÖ Switched context\"\n        echo \"\"\n        echo \"Previous: $PREVIOUS_CONTEXT\"\n        echo \"Current:  $CONTEXT_NAME\"\n\n        # Get cluster info\n        CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CONTEXT_NAME\\\")].context.cluster}\")\n        NAMESPACE=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CONTEXT_NAME\\\")].context.namespace}\" || echo \"default\")\n\n        echo \"Cluster:  $CLUSTER\"\n        echo \"Namespace: $NAMESPACE\"\n        echo \"\"\n\n        # Test connection\n        echo \"Testing connection...\"\n        if kubectl cluster-info --request-timeout=5s &>/dev/null; then\n            echo \"‚úÖ Cluster reachable\"\n\n            # Show cluster version\n            SERVER_VERSION=$(kubectl version --short 2>/dev/null | grep \"Server Version\" | awk '{print $3}')\n            if [[ -n \"$SERVER_VERSION\" ]]; then\n                echo \"   Version: $SERVER_VERSION\"\n            fi\n\n            # Show node count\n            NODE_COUNT=$(kubectl get nodes --no-headers 2>/dev/null | wc -l)\n            if [[ $NODE_COUNT -gt 0 ]]; then\n                echo \"   Nodes: $NODE_COUNT\"\n            fi\n        else\n            echo \"‚ö†Ô∏è  WARNING: Cluster not reachable\"\n        fi\n\n        echo \"\"\n        echo \"You are now connected to: $CONTEXT_NAME\"\n    else\n        echo \"‚ùå Failed to switch context\"\n        exit 1\n    fi\nfi\n```\n\n#### 2.3 Show Current Context\n\n```bash\nif [[ \"$ACTION_TYPE\" == \"current\" ]]; then\n    echo \"üìç CURRENT CONTEXT\"\n    echo \"==================\"\n    echo \"\"\n\n    CURRENT_CONTEXT=$(kubectl config current-context 2>/dev/null)\n\n    if [[ -z \"$CURRENT_CONTEXT\" ]]; then\n        echo \"No current context set\"\n        exit 0\n    fi\n\n    echo \"Context: $CURRENT_CONTEXT\"\n    echo \"\"\n\n    # Get detailed information\n    CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CURRENT_CONTEXT\\\")].context.cluster}\")\n    USER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CURRENT_CONTEXT\\\")].context.user}\")\n    NAMESPACE=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CURRENT_CONTEXT\\\")].context.namespace}\" || echo \"default\")\n\n    echo \"Details:\"\n    echo \"  Cluster: $CLUSTER\"\n    echo \"  User: $USER\"\n    echo \"  Namespace: $NAMESPACE\"\n    echo \"\"\n\n    # Get cluster endpoint\n    ENDPOINT=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name==\\\"$CLUSTER\\\")].cluster.server}\")\n    echo \"  Endpoint: $ENDPOINT\"\n    echo \"\"\n\n    # Test connection and get cluster info\n    echo \"Cluster Information:\"\n    if kubectl cluster-info --request-timeout=5s &>/dev/null; then\n        # Version\n        SERVER_VERSION=$(kubectl version --short 2>/dev/null | grep \"Server Version\" | awk '{print $3}')\n        echo \"  Kubernetes Version: $SERVER_VERSION\"\n\n        # Nodes\n        NODE_COUNT=$(kubectl get nodes --no-headers 2>/dev/null | wc -l)\n        echo \"  Nodes: $NODE_COUNT\"\n\n        # Namespaces\n        NS_COUNT=$(kubectl get namespaces --no-headers 2>/dev/null | wc -l)\n        echo \"  Namespaces: $NS_COUNT\"\n\n        # Pods\n        POD_COUNT=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | wc -l)\n        echo \"  Total Pods: $POD_COUNT\"\n\n        echo \"\"\n        echo \"Recent cluster activity:\"\n        kubectl get events --all-namespaces --sort-by='.lastTimestamp' --field-selector type=Warning | tail -5\n    else\n        echo \"  ‚ö†Ô∏è  Cluster not reachable\"\n    fi\n\n    echo \"\"\nfi\n```\n\n#### 2.4 Rename Context\n\n```bash\nif [[ \"$ACTION_TYPE\" == \"rename\" ]]; then\n    echo \"‚úèÔ∏è  Renaming context...\"\n    echo \"\"\n\n    # Check if old context exists\n    if ! kubectl config get-contexts \"$CONTEXT_NAME\" &>/dev/null; then\n        echo \"‚ùå ERROR: Context not found: $CONTEXT_NAME\"\n        exit 1\n    fi\n\n    # Check if new name already exists\n    if kubectl config get-contexts \"$NEW_NAME\" &>/dev/null; then\n        echo \"‚ùå ERROR: Context already exists: $NEW_NAME\"\n        exit 1\n    fi\n\n    # Rename context\n    if kubectl config rename-context \"$CONTEXT_NAME\" \"$NEW_NAME\" &>/dev/null; then\n        echo \"‚úÖ Context renamed\"\n        echo \"   Old name: $CONTEXT_NAME\"\n        echo \"   New name: $NEW_NAME\"\n        echo \"\"\n\n        # Check if this was the current context\n        CURRENT_CONTEXT=$(kubectl config current-context 2>/dev/null)\n        if [[ \"$CURRENT_CONTEXT\" == \"$NEW_NAME\" ]]; then\n            echo \"This is your current context\"\n        fi\n    else\n        echo \"‚ùå Failed to rename context\"\n        exit 1\n    fi\n\n    echo \"\"\nfi\n```\n\n#### 2.5 Delete Context\n\n```bash\nif [[ \"$ACTION_TYPE\" == \"delete\" ]]; then\n    echo \"üóëÔ∏è  Deleting context...\"\n    echo \"\"\n\n    # Check if context exists\n    if ! kubectl config get-contexts \"$CONTEXT_NAME\" &>/dev/null; then\n        echo \"‚ùå ERROR: Context not found: $CONTEXT_NAME\"\n        exit 1\n    fi\n\n    # Check if this is the current context\n    CURRENT_CONTEXT=$(kubectl config current-context 2>/dev/null)\n    if [[ \"$CURRENT_CONTEXT\" == \"$CONTEXT_NAME\" ]]; then\n        echo \"‚ö†Ô∏è  WARNING: This is your current context\"\n        echo \"\"\n        echo \"Are you sure you want to delete it? (yes/no)\"\n        read -r CONFIRM\n        if [[ \"$CONFIRM\" != \"yes\" ]]; then\n            echo \"Deletion cancelled\"\n            exit 0\n        fi\n    fi\n\n    # Get context details before deletion\n    CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CONTEXT_NAME\\\")].context.cluster}\")\n    USER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CONTEXT_NAME\\\")].context.user}\")\n\n    echo \"Context details:\"\n    echo \"  Name: $CONTEXT_NAME\"\n    echo \"  Cluster: $CLUSTER\"\n    echo \"  User: $USER\"\n    echo \"\"\n    echo \"This will delete the context entry only.\"\n    echo \"The cluster, user, and credentials will remain.\"\n    echo \"\"\n    echo \"Proceed with deletion? (yes/no)\"\n    read -r FINAL_CONFIRM\n\n    if [[ \"$FINAL_CONFIRM\" != \"yes\" ]]; then\n        echo \"Deletion cancelled\"\n        exit 0\n    fi\n\n    # Delete context\n    if kubectl config delete-context \"$CONTEXT_NAME\" &>/dev/null; then\n        echo \"‚úÖ Context deleted: $CONTEXT_NAME\"\n\n        # If this was current context, show available contexts\n        if [[ \"$CURRENT_CONTEXT\" == \"$CONTEXT_NAME\" ]]; then\n            echo \"\"\n            echo \"No current context set. Available contexts:\"\n            kubectl config get-contexts -o name\n        fi\n    else\n        echo \"‚ùå Failed to delete context\"\n        exit 1\n    fi\n\n    echo \"\"\nfi\n```\n\n## Quick Reference\n\n### List all contexts\n\n```bash\ncluster-code multi-cluster-context --action list\n```\n\n### Switch to a context\n\n```bash\ncluster-code multi-cluster-context --action switch --context my-cluster\n```\n\n### Show current context\n\n```bash\ncluster-code multi-cluster-context --action current\n```\n\n### Rename a context\n\n```bash\ncluster-code multi-cluster-context --action rename \\\n  --context old-name \\\n  --new-name new-name\n```\n\n### Delete a context\n\n```bash\ncluster-code multi-cluster-context --action delete --context old-cluster\n```\n\n## Examples\n\n### Example 1: List contexts with details\n\n```bash\ncluster-code multi-cluster-context --action list\n```\n\nOutput:\n```\nüìã KUBERNETES CONTEXTS\n======================\n\nCONTEXT                           CLUSTER                       USER                 NAMESPACE       CURRENT\n-------                           -------                       ----                 ---------       -------\ngke_proj_us-central1_prod        gke_proj_us-central1_prod     gke_user             default         *\neks-staging                      eks-staging.us-west-2         eks-user             staging\naks-dev                          aks-dev                       aks-user             dev\n\nCurrent context: gke_proj_us-central1_prod\n```\n\n### Example 2: Switch between environments\n\n```bash\n# Switch to staging\ncluster-code multi-cluster-context --action switch --context eks-staging\n\n# Verify\ncluster-code multi-cluster-context --action current\n```\n\n### Example 3: Organize contexts with meaningful names\n\n```bash\n# Rename cloud provider contexts\ncluster-code multi-cluster-context --action rename \\\n  --context gke_myproject_us-central1-a_prod-cluster \\\n  --new-name prod-gke\n\ncluster-code multi-cluster-context --action rename \\\n  --context arn:aws:eks:us-west-2:123456:cluster/staging \\\n  --new-name staging-eks\n```\n\n## Multi-Cluster Workflows\n\n### Cross-Cluster Resource Management\n\n```bash\n# List pods across all clusters\nfor ctx in $(kubectl config get-contexts -o name); do\n  echo \"=== $ctx ===\"\n  kubectl --context=$ctx get pods --all-namespaces\ndone\n```\n\n### Backup Multiple Clusters\n\n```bash\n# Backup all production clusters\nfor ctx in $(kubectl config get-contexts -o name | grep prod); do\n  cluster-code multi-cluster-context --action switch --context $ctx\n  cluster-code backup-cluster --backup-name \"${ctx}-backup-$(date +%Y%m%d)\"\ndone\n```\n\n### Health Check All Clusters\n\n```bash\n# Check all cluster health\nfor ctx in $(kubectl config get-contexts -o name); do\n  echo \"Checking $ctx...\"\n  cluster-code multi-cluster-context --action switch --context $ctx\n  cluster-code cluster-diagnose\ndone\n```\n\n## Tips and Best Practices\n\n1. **Use descriptive names** - Rename contexts to be environment/region specific\n   - Good: `prod-gke-us`, `staging-eks-eu`\n   - Bad: `gke_project123_zone-a_cluster-xyz`\n\n2. **Organize by purpose**\n   - Prefix with environment: `prod-`, `staging-`, `dev-`\n   - Include cloud provider: `-eks`, `-aks`, `-gke`\n   - Add region: `-us-east`, `-eu-west`\n\n3. **Set default namespaces** per context:\n   ```bash\n   kubectl config set-context prod-gke --namespace=production\n   ```\n\n4. **Use kubectx** for faster switching (optional companion tool):\n   ```bash\n   # Install kubectx\n   brew install kubectx\n\n   # Quick switch\n   kubectx prod-gke\n   ```\n\n5. **Visual indicators** - Use shell prompt to show current context:\n   ```bash\n   # Add to .bashrc or .zshrc\n   PROMPT='$(kubectl config current-context):$PROMPT'\n   ```\n\n## Context Configuration File\n\nContexts are stored in `~/.kube/config`:\n\n```yaml\napiVersion: v1\nkind: Config\ncontexts:\n- context:\n    cluster: my-cluster\n    user: my-user\n    namespace: default\n  name: my-context\nclusters:\n- cluster:\n    server: https://api.cluster.example.com\n  name: my-cluster\nusers:\n- name: my-user\n  user:\n    token: xxx\n```\n\n## Troubleshooting\n\n### Issue: Context not found\n\n**Solution**: List available contexts:\n```bash\nkubectl config get-contexts\n```\n\n### Issue: Unable to connect to cluster\n\n**Solution**: Check cluster credentials and endpoint:\n```bash\nkubectl cluster-info\nkubectl config view --minify\n```\n\n### Issue: Wrong namespace\n\n**Solution**: Set default namespace for context:\n```bash\nkubectl config set-context --current --namespace=my-namespace\n```\n\n## Related Commands\n\n- `kubectl config`: Direct kubectl configuration management\n- `cluster-diagnose`: Verify cluster health\n- `backup-cluster`: Backup current cluster\n- Cloud-specific commands:\n  - `aws-cluster-create`\n  - `azure-cluster-create`\n  - `gcp-cluster-create`"
              },
              {
                "name": "/node-cordon",
                "description": "Mark one or more nodes as unschedulable to prevent new pods",
                "path": "plugins/cluster-core/commands/node-cordon.md",
                "frontmatter": {
                  "name": "node-cordon",
                  "description": "Mark one or more nodes as unschedulable to prevent new pods",
                  "category": "node-management",
                  "parameters": [
                    {
                      "name": "node",
                      "description": "Node name or pattern to cordon (supports wildcards)",
                      "required": true
                    },
                    {
                      "name": "selector",
                      "description": "Label selector to cordon multiple nodes (e.g., node-role.kubernetes.io/worker)",
                      "required": false
                    }
                  ],
                  "tags": [
                    "nodes",
                    "maintenance",
                    "scheduling"
                  ]
                },
                "content": "# Node Cordon\n\nMark nodes as unschedulable to prevent new pods from being scheduled while allowing existing pods to continue running.\n\n## Overview\n\nCordoning a node is typically the first step in node maintenance:\n\n1. **Cordon** - Mark unschedulable (this command)\n2. **Drain** - Safely evict existing pods\n3. **Perform maintenance** - Upgrade, patch, or replace node\n4. **Uncordon** - Mark schedulable again\n\n**Use cases:**\n- Preparing nodes for maintenance or upgrades\n- Isolating problematic nodes\n- Gradual node pool replacement\n- Testing pod rescheduling\n\n## Prerequisites\n\n- `kubectl` configured with cluster access\n- Appropriate RBAC permissions:\n  - `nodes/get`\n  - `nodes/update`\n\n## Workflow\n\n### Phase 1: Identify Nodes\n\n#### 1.1 Parse Node Selection\n\n```bash\nNODE_INPUT=\"${NODE}\"\nLABEL_SELECTOR=\"${LABEL_SELECTOR}\"\n\necho \"üîç Identifying nodes to cordon...\"\n\nif [[ -n \"$LABEL_SELECTOR\" ]]; then\n    # Select nodes by label\n    echo \"   Using label selector: $LABEL_SELECTOR\"\n\n    NODES=$(kubectl get nodes -l \"$LABEL_SELECTOR\" -o jsonpath='{.items[*].metadata.name}')\n\n    if [[ -z \"$NODES\" ]]; then\n        echo \"‚ùå No nodes found matching selector: $LABEL_SELECTOR\"\n        exit 1\n    fi\n\n    NODE_COUNT=$(echo \"$NODES\" | wc -w)\n    echo \"   Found $NODE_COUNT nodes:\"\n    echo \"$NODES\" | tr ' ' '\\n' | sed 's/^/     - /'\n\nelif [[ \"$NODE_INPUT\" == *\"*\"* ]]; then\n    # Wildcard pattern matching\n    PATTERN=$(echo \"$NODE_INPUT\" | sed 's/\\*/.*/')\n    echo \"   Using pattern: $NODE_INPUT\"\n\n    NODES=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | grep -E \"^${PATTERN}$\")\n\n    if [[ -z \"$NODES\" ]]; then\n        echo \"‚ùå No nodes found matching pattern: $NODE_INPUT\"\n        exit 1\n    fi\n\n    NODE_COUNT=$(echo \"$NODES\" | wc -l)\n    echo \"   Found $NODE_COUNT nodes:\"\n    echo \"$NODES\" | sed 's/^/     - /'\n\nelse\n    # Single node name\n    NODES=\"$NODE_INPUT\"\n\n    # Verify node exists\n    if ! kubectl get node \"$NODES\" &>/dev/null; then\n        echo \"‚ùå Node not found: $NODES\"\n        echo \"\"\n        echo \"Available nodes:\"\n        kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,ROLES:.metadata.labels.node-role\\\\.kubernetes\\\\.io/*\n        exit 1\n    fi\n\n    NODE_COUNT=1\n    echo \"   Target node: $NODES\"\nfi\n\necho \"\"\n```\n\n### Phase 2: Pre-cordon Analysis\n\n#### 2.1 Check Node Status\n\n```bash\necho \"üìä Analyzing nodes...\"\necho \"\"\n\nfor NODE_NAME in $NODES; do\n    echo \"Node: $NODE_NAME\"\n\n    # Get node status\n    NODE_INFO=$(kubectl get node \"$NODE_NAME\" -o json)\n\n    READY_STATUS=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"Ready\") | .status')\n    SCHEDULABLE=$(echo \"$NODE_INFO\" | jq -r '.spec.unschedulable // false')\n\n    echo \"  Ready: $READY_STATUS\"\n    echo \"  Schedulable: $([ \"$SCHEDULABLE\" == \"false\" ] && echo \"Yes\" || echo \"No (already cordoned)\")\"\n\n    # Count pods\n    POD_COUNT=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=\"$NODE_NAME\" --no-headers 2>/dev/null | wc -l)\n    echo \"  Running Pods: $POD_COUNT\"\n\n    # Get node role\n    ROLE=$(echo \"$NODE_INFO\" | jq -r '.metadata.labels[\"node-role.kubernetes.io/control-plane\"] // .metadata.labels[\"node-role.kubernetes.io/master\"] // \"worker\"')\n    if [[ \"$ROLE\" != \"worker\" ]]; then\n        echo \"  Role: control-plane/master\"\n    fi\n\n    # Check for taints\n    TAINTS=$(echo \"$NODE_INFO\" | jq -r '.spec.taints // [] | length')\n    if [[ $TAINTS -gt 0 ]]; then\n        echo \"  Taints: $TAINTS\"\n    fi\n\n    echo \"\"\ndone\n```\n\n### Phase 3: Cordon Nodes\n\n#### 3.1 Execute Cordon\n\n```bash\necho \"üîí Cordoning nodes...\"\necho \"\"\n\nSUCCESS_COUNT=0\nALREADY_CORDONED=0\nFAILED_COUNT=0\n\nfor NODE_NAME in $NODES; do\n    # Check if already cordoned\n    SCHEDULABLE=$(kubectl get node \"$NODE_NAME\" -o jsonpath='{.spec.unschedulable}' 2>/dev/null)\n\n    if [[ \"$SCHEDULABLE\" == \"true\" ]]; then\n        echo \"  ‚ÑπÔ∏è  $NODE_NAME - already cordoned\"\n        ALREADY_CORDONED=$((ALREADY_CORDONED + 1))\n        continue\n    fi\n\n    # Cordon the node\n    if kubectl cordon \"$NODE_NAME\" &>/dev/null; then\n        echo \"  ‚úÖ $NODE_NAME - cordoned\"\n        SUCCESS_COUNT=$((SUCCESS_COUNT + 1))\n    else\n        echo \"  ‚ùå $NODE_NAME - failed to cordon\"\n        FAILED_COUNT=$((FAILED_COUNT + 1))\n    fi\ndone\n\necho \"\"\necho \"Results:\"\necho \"  Successfully cordoned: $SUCCESS_COUNT\"\nif [[ $ALREADY_CORDONED -gt 0 ]]; then\n    echo \"  Already cordoned: $ALREADY_CORDONED\"\nfi\nif [[ $FAILED_COUNT -gt 0 ]]; then\n    echo \"  Failed: $FAILED_COUNT\"\nfi\n```\n\n### Phase 4: Verification\n\n#### 4.1 Verify Cordon Status\n\n```bash\necho \"\"\necho \"üîç Verifying cordon status...\"\necho \"\"\n\nkubectl get nodes $(echo $NODES | tr '\\n' ' ') -o custom-columns=\\\nNAME:.metadata.name,\\\nSTATUS:.status.conditions[-1].type,\\\nSCHEDULABLE:.spec.unschedulable,\\\nROLES:.metadata.labels.node-role\\\\.kubernetes\\\\.io/*\n\n# Check cluster capacity\necho \"\"\necho \"üìä Cluster Scheduling Capacity:\"\n\nTOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)\nSCHEDULABLE_NODES=$(kubectl get nodes -o json | jq '[.items[] | select(.spec.unschedulable != true)] | length')\nUNSCHEDULABLE_NODES=$((TOTAL_NODES - SCHEDULABLE_NODES))\n\necho \"  Total nodes: $TOTAL_NODES\"\necho \"  Schedulable: $SCHEDULABLE_NODES\"\necho \"  Unschedulable (cordoned): $UNSCHEDULABLE_NODES\"\n\nif [[ $SCHEDULABLE_NODES -eq 0 ]]; then\n    echo \"\"\n    echo \"‚ö†Ô∏è  WARNING: No schedulable nodes remaining!\"\n    echo \"   New pods cannot be scheduled until nodes are uncordoned\"\nfi\n\necho \"\"\necho \"‚úÖ CORDON COMPLETE\"\necho \"==================\"\necho \"\"\necho \"Cordoned $SUCCESS_COUNT node(s)\"\necho \"\"\necho \"Next steps:\"\necho \"\"\necho \"1. Review running pods on cordoned nodes:\"\necho \"   kubectl get pods --all-namespaces -o wide | grep -E '$(echo $NODES | tr ' ' '|')'\"\necho \"\"\necho \"2. Drain nodes to evict pods (optional):\"\nfor NODE_NAME in $NODES; do\n    echo \"   cluster-code node-drain --node $NODE_NAME\"\ndone\necho \"\"\necho \"3. Perform maintenance on nodes\"\necho \"\"\necho \"4. Uncordon nodes when ready:\"\nfor NODE_NAME in $NODES; do\n    echo \"   cluster-code node-uncordon --node $NODE_NAME\"\ndone\necho \"\"\n```\n\n## Node States\n\n| State | Schedulable | Pods Running | Use Case |\n|-------|-------------|--------------|----------|\n| **Normal** | Yes | Yes | Regular operation |\n| **Cordoned** | No | Yes | Preventing new pods, prep for drain |\n| **Drained** | No | No | Node empty, ready for maintenance |\n| **Uncordoned** | Yes | Maybe | Back in rotation |\n\n## Common Scenarios\n\n### Scenario 1: Prepare single node for maintenance\n\n```bash\n# Cordon node\ncluster-code node-cordon --node worker-01\n\n# Verify\nkubectl get node worker-01\n# Shows: SchedulingDisabled\n\n# Pods still running\nkubectl get pods -A -o wide | grep worker-01\n```\n\n### Scenario 2: Cordon all worker nodes in a pool\n\n```bash\n# Cordon by label\ncluster-code node-cordon --selector node-pool=workers-v1\n\n# Cordon by pattern\ncluster-code node-cordon --node \"worker-*\"\n```\n\n### Scenario 3: Isolate problematic node\n\n```bash\n# Cordon to prevent new pods\ncluster-code node-cordon --node problematic-node-07\n\n# Investigate issues\nkubectl describe node problematic-node-07\nkubectl logs -n kube-system <kubelet-pod>\n```\n\n## Examples\n\n### Example 1: Cordon single node\n\n```bash\ncluster-code node-cordon --node ip-10-0-1-42.ec2.internal\n```\n\n### Example 2: Cordon multiple nodes by pattern\n\n```bash\ncluster-code node-cordon --node \"gke-prod-cluster-pool-1-*\"\n```\n\n### Example 3: Cordon all nodes in a specific pool\n\n```bash\ncluster-code node-cordon --selector eks.amazonaws.com/nodegroup=workers-spot\n```\n\n### Example 4: Cordon nodes with specific instance type\n\n```bash\ncluster-code node-cordon --selector node.kubernetes.io/instance-type=t3.medium\n```\n\n## Comparison: Cordon vs Drain vs Delete\n\n| Action | Scheduling | Existing Pods | Node Exists | Reversible |\n|--------|-----------|---------------|-------------|------------|\n| **Cordon** | Disabled | Continue running | Yes | Yes (uncordon) |\n| **Drain** | Disabled | Evicted | Yes | Yes (uncordon) |\n| **Delete** | N/A | Terminated | No | No |\n\n## Important Notes\n\n1. **Cordoning is non-disruptive** - Existing pods continue running\n2. **Not permanent** - Easily reversible with `uncordon`\n3. **DaemonSets** - DaemonSet pods ignore cordon (will still schedule)\n4. **StatefulSets** - Cordoning doesn't affect existing StatefulSet pods\n5. **Cluster autoscaler** - May still scale cordoned node pools\n\n## Related Commands\n\n- `node-drain`: Safely evict pods from node\n- `node-uncordon`: Mark node as schedulable again\n- `cluster-diagnose`: Analyze node health\n- `kubectl get nodes`: View all node statuses"
              },
              {
                "name": "/node-drain",
                "description": "Safely drain a node for maintenance with pod eviction and validation",
                "path": "plugins/cluster-core/commands/node-drain.md",
                "frontmatter": {
                  "name": "node-drain",
                  "description": "Safely drain a node for maintenance with pod eviction and validation",
                  "category": "node-management",
                  "tools": [
                    "Bash(kubectl:*)"
                  ],
                  "parameters": [
                    {
                      "name": "node",
                      "description": "Node name to drain",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "force",
                      "description": "Force drain even if pods not managed by controller",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "ignore-daemonsets",
                      "description": "Ignore DaemonSet-managed pods",
                      "required": false,
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "delete-emptydir-data",
                      "description": "Delete pods using emptyDir",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "grace-period",
                      "description": "Grace period for pod termination (seconds)",
                      "required": false,
                      "type": "integer",
                      "default": 30
                    },
                    {
                      "name": "timeout",
                      "description": "Drain timeout duration",
                      "required": false,
                      "type": "string",
                      "default": "5m"
                    }
                  ],
                  "examples": [
                    "node-drain --node worker-node-1",
                    "node-drain --node worker-node-2 --force --delete-emptydir-data"
                  ]
                },
                "content": "# Node Drain\n\nSafely evict all pods from a node for maintenance, upgrades, or decommissioning.\n\n## Task Workflow\n\n### Phase 1: Pre-Drain Validation\n\n1. **Verify node exists**:\n   ```bash\n   kubectl get node $NODE_NAME || {\n     echo \"‚ùå Node not found: $NODE_NAME\"\n     exit 1\n   }\n   ```\n\n2. **Get node information**:\n   ```bash\n   NODE_INFO=$(kubectl get node $NODE_NAME -o json)\n\n   NODE_STATUS=$(echo $NODE_INFO | jq -r '.status.conditions[] | select(.type==\"Ready\") | .status')\n   NODE_ROLE=$(echo $NODE_INFO | jq -r '.metadata.labels.\"node-role.kubernetes.io/control-plane\" // \"worker\"')\n   KUBELET_VERSION=$(echo $NODE_INFO | jq -r '.status.nodeInfo.kubeletVersion')\n\n   echo \"Node: $NODE_NAME\"\n   echo \"Status: $NODE_STATUS\"\n   echo \"Role: $NODE_ROLE\"\n   echo \"Kubelet Version: $KUBELET_VERSION\"\n   echo \"\"\n   ```\n\n3. **Check pods on node**:\n   ```bash\n   PODS=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME -o json)\n   POD_COUNT=$(echo $PODS | jq '.items | length')\n\n   echo \"Pods on node: $POD_COUNT\"\n   echo \"\"\n\n   if [[ $POD_COUNT -eq 0 ]]; then\n     echo \"‚ÑπÔ∏è  No pods on node - drain not needed\"\n     exit 0\n   fi\n\n   # Show pod breakdown\n   echo \"Pod breakdown:\"\n   echo $PODS | jq -r '.items[] | \"\\(.metadata.namespace)/\\(.metadata.name)\"' | \\\n     awk -F/ '{ns[$1]++} END {for (n in ns) printf \"  %s: %d pods\\n\", n, ns[n]}'\n   echo \"\"\n\n   # Check for DaemonSets\n   DAEMONSET_PODS=$(echo $PODS | jq '[.items[] | select(.metadata.ownerReferences[]?.kind==\"DaemonSet\")] | length')\n   if [[ $DAEMONSET_PODS -gt 0 ]]; then\n     echo \"‚ÑπÔ∏è  $DAEMONSET_PODS DaemonSet pods (will be ignored with --ignore-daemonsets)\"\n   fi\n\n   # Check for standalone pods\n   STANDALONE_PODS=$(echo $PODS | jq '[.items[] | select(.metadata.ownerReferences | length == 0)] | length')\n   if [[ $STANDALONE_PODS -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  $STANDALONE_PODS standalone pods (require --force to drain)\"\n     echo $PODS | jq -r '.items[] | select(.metadata.ownerReferences | length == 0) | \"    - \\(.metadata.namespace)/\\(.metadata.name)\"'\n   fi\n   echo \"\"\n   ```\n\n### Phase 2: Cordon Node\n\n```bash\necho \"üîí Cordoning node...\"\nkubectl cordon $NODE_NAME\n\nif [[ $? -eq 0 ]]; then\n  echo \"‚úÖ Node cordoned (no new pods will be scheduled)\"\nelse\n  echo \"‚ùå Failed to cordon node\"\n  exit 1\nfi\necho \"\"\n```\n\n### Phase 3: Execute Drain\n\n```bash\necho \"üö∞ Draining node...\"\necho \"\"\n\n# Build drain command\nDRAIN_CMD=\"kubectl drain $NODE_NAME\"\n[[ \"$IGNORE_DAEMONSETS\" == \"true\" ]] && DRAIN_CMD=\"$DRAIN_CMD --ignore-daemonsets\"\n[[ \"$FORCE\" == \"true\" ]] && DRAIN_CMD=\"$DRAIN_CMD --force\"\n[[ \"$DELETE_EMPTYDIR\" == \"true\" ]] && DRAIN_CMD=\"$DRAIN_CMD --delete-emptydir-data\"\nDRAIN_CMD=\"$DRAIN_CMD --grace-period=$GRACE_PERIOD --timeout=$TIMEOUT\"\n\n# Execute drain\n$DRAIN_CMD\n\nDRAIN_EXIT=$?\n\nif [[ $DRAIN_EXIT -eq 0 ]]; then\n  echo \"\"\n  echo \"‚úÖ Node drained successfully\"\nelse\n  echo \"\"\n  echo \"‚ùå Drain failed\"\n  echo \"\"\n  echo \"Common issues:\"\n  echo \"- Standalone pods (use --force)\"\n  echo \"- PodDisruptionBudgets preventing eviction\"\n  echo \"- Pods using emptyDir (use --delete-emptydir-data)\"\n  exit 1\nfi\n```\n\n### Phase 4: Verify Drain\n\n```bash\necho \"\"\necho \"Verifying drain...\"\n\nREMAINING_PODS=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME --no-headers 2>/dev/null | grep -v DaemonSet | wc -l)\n\nif [[ $REMAINING_PODS -eq 0 ]]; then\n  echo \"‚úÖ All non-DaemonSet pods evicted\"\nelse\n  echo \"‚ö†Ô∏è  $REMAINING_PODS pods remaining\"\n  kubectl get pods --all-namespaces --field-selector spec.nodeName=$NODE_NAME\nfi\n\necho \"\"\necho \"Node is ready for maintenance\"\necho \"\"\necho \"To uncordon node when ready:\"\necho \"  kubectl uncordon $NODE_NAME\"\n```\n\n## References\n\n- https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/"
              },
              {
                "name": "/node-status",
                "description": "Show detailed node status and resource utilization",
                "path": "plugins/cluster-core/commands/node-status.md",
                "frontmatter": {
                  "name": "node-status",
                  "description": "Show detailed node status and resource utilization",
                  "args": [
                    {
                      "name": "node",
                      "description": "Specific node name (optional, shows all if not specified)",
                      "required": false,
                      "hint": "node-name"
                    },
                    {
                      "name": "detailed",
                      "description": "Show detailed node information including pods and conditions",
                      "required": false,
                      "hint": "--detailed"
                    }
                  ],
                  "tools": [
                    "Bash",
                    "Grep"
                  ],
                  "permissions": {
                    "allow": [
                      "Bash(kubectl:*)",
                      "Bash(oc:*)"
                    ]
                  },
                  "model": "sonnet",
                  "color": "green"
                },
                "content": "# Node Status Analysis\n\nI'll provide comprehensive information about cluster node health, resource utilization, and pod distribution.\n\n## Node Overview\n\n```bash\n# Basic node information\nkubectl get nodes -o wide\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\t\"}{.status.capacity.memory}{\"\\t\"}{.status.capacity.ephemeral-storage}{\"\\t\"}{.status.allocatable.cpu}{\"\\t\"}{.status.allocatable.memory}{\"\\n\"}{end}'\n```\n\n## Node Health Conditions\n\n```bash\n# Node conditions analysis\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{range .status.conditions[*]}{.type}{\"=\"}{.status}{\" - \"}{.reason}{\": \"}{.message}{\"\\n\"}{end}{\"\\n\"}'\n\n# Find unhealthy nodes\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{range .status.conditions[*]}{.type}{\"=\"}{.status}{\"\\n\"}{end}{\"\\n\"}' | grep -A 5 \"False\"\n```\n\n{{#if detailed}}\n## Detailed Node Analysis\n\n```bash\n# Resource utilization\nkubectl top nodes 2>/dev/null || echo \"Metrics server not installed\"\n\n# Pod distribution per node\nkubectl get pods --all-namespaces -o wide --sort-by=.spec.nodeName\n\n# Taints and tolerations\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\nTaints: \"}{range .spec.taints[*]}{.key}{\"=\"}{.value}{\":\"}{.effect}{\" \"}{end}{\"\\n\\n\"}{end}'\n\n# Node labels\nkubectl get nodes --show-labels\n```\n\n## Specific Node Details\n\n{{#if node}}\n**Analysis for node: {{node}}**\n\n```bash\n# Detailed node description\nkubectl describe node {{node}}\n\n# Pods running on this node\nkubectl get pods --all-namespaces --field-selector=spec.nodeName={{node}} -o wide\n\n# Resource usage for this node's pods\nkubectl top pods --all-namespaces --field-selector=spec.nodeName={{node}} 2>/dev/null || echo \"Pod metrics not available\"\n```\n{{/if}}\n{{/if}}\n\n## Node Capacity Analysis\n\n```bash\n# Cluster resource summary\nkubectl describe nodes | grep -A 5 \"Allocated resources\"\n\n# Resource pressure detection\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{range .status.conditions[*]}{.type}{\"=\"}{.status}{\"\\n\"}{end}{\"\\n\"}' | grep -A 1 \"MemoryPressure\\|DiskPressure\\|PIDPressure\"\n```\n\n## Troubleshooting Information\n\nI'll help identify:\n\n### üö® Critical Issues\n- **NotReady Nodes**: Nodes that are not responding\n- **Resource Pressure**: CPU, memory, disk, or PID pressure\n- **Network Issues**: Connectivity problems between nodes\n- **Storage Issues**: Disk space or storage class problems\n\n### üìä Performance Metrics\n- **CPU Utilization**: Current usage vs. capacity\n- **Memory Usage**: Memory consumption and available space\n- **Disk Usage**: Storage consumption on nodes\n- **Pod Density**: Number of pods per node vs. limits\n\n### üîß Optimization Opportunities\n- **Resource Balancing**: Distribute workloads more evenly\n- **Node Scaling**: Add or remove nodes based on demand\n- **Taint Management**: Optimize node taints and tolerations\n- **Pod Scheduling**: Improve pod placement strategies\n\n## Quick Actions\n\nBased on the analysis, I can help you:\n- **Drain a node**: `kubectl drain <node-name> --ignore-daemonsets`\n- **Uncordon a node**: `kubectl uncordon <node-name>`\n- **Label nodes**: `kubectl label nodes <node-name> <key>=<value>`\n- **Add taints**: `kubectl taint nodes <node-name> <key>=<value>:<effect>`\n\n## Node Maintenance\n\n### Pre-maintenance Checklist\n- Drain workloads safely\n- Check for critical applications\n- Verify backup status\n- Plan maintenance window\n\n### Post-maintenance Verification\n- Confirm node is Ready\n- Verify pod health\n- Check resource utilization\n- Monitor application performance\n\nWould you like me to proceed with the node status analysis?"
              },
              {
                "name": "/node-uncordon",
                "description": "Mark one or more nodes as schedulable to allow new pods",
                "path": "plugins/cluster-core/commands/node-uncordon.md",
                "frontmatter": {
                  "name": "node-uncordon",
                  "description": "Mark one or more nodes as schedulable to allow new pods",
                  "category": "node-management",
                  "parameters": [
                    {
                      "name": "node",
                      "description": "Node name or pattern to uncordon (supports wildcards)",
                      "required": true
                    },
                    {
                      "name": "selector",
                      "description": "Label selector to uncordon multiple nodes (e.g., node-pool=workers)",
                      "required": false
                    },
                    {
                      "name": "verify-health",
                      "description": "Verify node health before uncordoning",
                      "type": "boolean",
                      "default": true
                    }
                  ],
                  "tags": [
                    "nodes",
                    "maintenance",
                    "scheduling"
                  ]
                },
                "content": "# Node Uncordon\n\nMark nodes as schedulable to allow new pods to be scheduled after maintenance or troubleshooting.\n\n## Overview\n\nUncordoning returns nodes to normal scheduling operation:\n\n**Typical maintenance workflow:**\n1. Cordon - Mark unschedulable\n2. Drain - Evict existing pods\n3. Maintain - Perform upgrades or fixes\n4. **Uncordon** - Return to service (this command)\n\n**What uncordoning does:**\n- Removes `unschedulable` flag from node\n- Allows Kubernetes scheduler to place new pods\n- Does NOT automatically reschedule pods that were evicted\n\n**What uncordoning does NOT do:**\n- Does not move pods back to the node\n- Does not guarantee immediate pod placement\n- Does not affect existing pod placement\n\n## Prerequisites\n\n- `kubectl` configured with cluster access\n- Appropriate RBAC permissions:\n  - `nodes/get`\n  - `nodes/update`\n\n## Workflow\n\n### Phase 1: Identify Nodes\n\n#### 1.1 Parse Node Selection\n\n```bash\nNODE_INPUT=\"${NODE}\"\nLABEL_SELECTOR=\"${LABEL_SELECTOR}\"\nVERIFY_HEALTH=\"${VERIFY_HEALTH:-true}\"\n\necho \"üîç Identifying nodes to uncordon...\"\n\nif [[ -n \"$LABEL_SELECTOR\" ]]; then\n    # Select nodes by label\n    echo \"   Using label selector: $LABEL_SELECTOR\"\n\n    NODES=$(kubectl get nodes -l \"$LABEL_SELECTOR\" -o jsonpath='{.items[*].metadata.name}')\n\n    if [[ -z \"$NODES\" ]]; then\n        echo \"‚ùå No nodes found matching selector: $LABEL_SELECTOR\"\n        exit 1\n    fi\n\n    NODE_COUNT=$(echo \"$NODES\" | wc -w)\n    echo \"   Found $NODE_COUNT nodes:\"\n    echo \"$NODES\" | tr ' ' '\\n' | sed 's/^/     - /'\n\nelif [[ \"$NODE_INPUT\" == *\"*\"* ]]; then\n    # Wildcard pattern matching\n    PATTERN=$(echo \"$NODE_INPUT\" | sed 's/\\*/.*/')\n    echo \"   Using pattern: $NODE_INPUT\"\n\n    NODES=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | grep -E \"^${PATTERN}$\")\n\n    if [[ -z \"$NODES\" ]]; then\n        echo \"‚ùå No nodes found matching pattern: $NODE_INPUT\"\n        exit 1\n    fi\n\n    NODE_COUNT=$(echo \"$NODES\" | wc -l)\n    echo \"   Found $NODE_COUNT nodes:\"\n    echo \"$NODES\" | sed 's/^/     - /'\n\nelse\n    # Single node name\n    NODES=\"$NODE_INPUT\"\n\n    # Verify node exists\n    if ! kubectl get node \"$NODES\" &>/dev/null; then\n        echo \"‚ùå Node not found: $NODES\"\n        echo \"\"\n        echo \"Available nodes:\"\n        kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,SCHEDULABLE:.spec.unschedulable\n        exit 1\n    fi\n\n    NODE_COUNT=1\n    echo \"   Target node: $NODES\"\nfi\n\necho \"\"\n```\n\n### Phase 2: Pre-uncordon Health Check\n\n#### 2.1 Verify Node Health\n\n```bash\nif [[ \"$VERIFY_HEALTH\" == \"true\" ]]; then\n    echo \"üè• Verifying node health...\"\n    echo \"\"\n\n    UNHEALTHY_NODES=\"\"\n    WARNINGS=0\n\n    for NODE_NAME in $NODES; do\n        echo \"Checking: $NODE_NAME\"\n\n        NODE_INFO=$(kubectl get node \"$NODE_NAME\" -o json)\n\n        # Check Ready status\n        READY_STATUS=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"Ready\") | .status')\n        READY_REASON=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"Ready\") | .reason')\n\n        if [[ \"$READY_STATUS\" != \"True\" ]]; then\n            echo \"  ‚ö†Ô∏è  Node not Ready (Status: $READY_STATUS, Reason: $READY_REASON)\"\n            UNHEALTHY_NODES=\"$UNHEALTHY_NODES $NODE_NAME\"\n            WARNINGS=$((WARNINGS + 1))\n        else\n            echo \"  ‚úÖ Node is Ready\"\n        fi\n\n        # Check for pressure conditions\n        MEMORY_PRESSURE=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"MemoryPressure\") | .status')\n        DISK_PRESSURE=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"DiskPressure\") | .status')\n        PID_PRESSURE=$(echo \"$NODE_INFO\" | jq -r '.status.conditions[] | select(.type==\"PIDPressure\") | .status')\n\n        if [[ \"$MEMORY_PRESSURE\" == \"True\" ]]; then\n            echo \"  ‚ö†Ô∏è  Memory Pressure detected\"\n            WARNINGS=$((WARNINGS + 1))\n        fi\n\n        if [[ \"$DISK_PRESSURE\" == \"True\" ]]; then\n            echo \"  ‚ö†Ô∏è  Disk Pressure detected\"\n            WARNINGS=$((WARNINGS + 1))\n        fi\n\n        if [[ \"$PID_PRESSURE\" == \"True\" ]]; then\n            echo \"  ‚ö†Ô∏è  PID Pressure detected\"\n            WARNINGS=$((WARNINGS + 1))\n        fi\n\n        # Check kubelet version\n        KUBELET_VERSION=$(echo \"$NODE_INFO\" | jq -r '.status.nodeInfo.kubeletVersion')\n        echo \"  Kubelet: $KUBELET_VERSION\"\n\n        # Check if already schedulable\n        SCHEDULABLE=$(echo \"$NODE_INFO\" | jq -r '.spec.unschedulable // false')\n        if [[ \"$SCHEDULABLE\" == \"false\" ]]; then\n            echo \"  ‚ÑπÔ∏è  Node is already schedulable\"\n        else\n            echo \"  Status: Cordoned (will be uncordoned)\"\n        fi\n\n        echo \"\"\n    done\n\n    if [[ $WARNINGS -gt 0 ]]; then\n        echo \"‚ö†Ô∏è  Found $WARNINGS health warnings\"\n        echo \"\"\n\n        if [[ -n \"$UNHEALTHY_NODES\" ]]; then\n            echo \"Nodes not in Ready state:$UNHEALTHY_NODES\"\n            echo \"\"\n            echo \"Continue with uncordon anyway? (yes/no)\"\n            read -r CONFIRM\n\n            if [[ \"$CONFIRM\" != \"yes\" ]]; then\n                echo \"‚ùå Uncordon cancelled\"\n                exit 1\n            fi\n        fi\n    else\n        echo \"‚úÖ All nodes are healthy\"\n    fi\nelse\n    echo \"‚ö†Ô∏è  Health check skipped (--verify-health=false)\"\nfi\n\necho \"\"\n```\n\n### Phase 3: Uncordon Nodes\n\n#### 3.1 Execute Uncordon\n\n```bash\necho \"üîì Uncordoning nodes...\"\necho \"\"\n\nSUCCESS_COUNT=0\nALREADY_SCHEDULABLE=0\nFAILED_COUNT=0\n\nfor NODE_NAME in $NODES; do\n    # Check if already schedulable\n    SCHEDULABLE=$(kubectl get node \"$NODE_NAME\" -o jsonpath='{.spec.unschedulable}' 2>/dev/null)\n\n    if [[ \"$SCHEDULABLE\" != \"true\" ]]; then\n        echo \"  ‚ÑπÔ∏è  $NODE_NAME - already schedulable\"\n        ALREADY_SCHEDULABLE=$((ALREADY_SCHEDULABLE + 1))\n        continue\n    fi\n\n    # Uncordon the node\n    if kubectl uncordon \"$NODE_NAME\" &>/dev/null; then\n        echo \"  ‚úÖ $NODE_NAME - uncordoned\"\n        SUCCESS_COUNT=$((SUCCESS_COUNT + 1))\n    else\n        echo \"  ‚ùå $NODE_NAME - failed to uncordon\"\n        FAILED_COUNT=$((FAILED_COUNT + 1))\n    fi\ndone\n\necho \"\"\necho \"Results:\"\necho \"  Successfully uncordoned: $SUCCESS_COUNT\"\nif [[ $ALREADY_SCHEDULABLE -gt 0 ]]; then\n    echo \"  Already schedulable: $ALREADY_SCHEDULABLE\"\nfi\nif [[ $FAILED_COUNT -gt 0 ]]; then\n    echo \"  Failed: $FAILED_COUNT\"\nfi\n```\n\n### Phase 4: Verification and Monitoring\n\n#### 4.1 Verify Uncordon Status\n\n```bash\necho \"\"\necho \"üîç Verifying uncordon status...\"\necho \"\"\n\nkubectl get nodes $(echo $NODES | tr '\\n' ' ') -o custom-columns=\\\nNAME:.metadata.name,\\\nSTATUS:.status.conditions[-1].type,\\\nSCHEDULABLE:.spec.unschedulable,\\\nPODS:.status.allocatable.pods,\\\nROLES:.metadata.labels.node-role\\\\.kubernetes\\\\.io/*\n\necho \"\"\n```\n\n#### 4.2 Check Cluster Capacity\n\n```bash\necho \"üìä Cluster Scheduling Capacity:\"\necho \"\"\n\nTOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)\nSCHEDULABLE_NODES=$(kubectl get nodes -o json | jq '[.items[] | select(.spec.unschedulable != true)] | length')\nUNSCHEDULABLE_NODES=$((TOTAL_NODES - SCHEDULABLE_NODES))\n\necho \"  Total nodes: $TOTAL_NODES\"\necho \"  Schedulable: $SCHEDULABLE_NODES\"\necho \"  Unschedulable (cordoned): $UNSCHEDULABLE_NODES\"\n\n# Calculate percentage\nSCHEDULABLE_PCT=$(( SCHEDULABLE_NODES * 100 / TOTAL_NODES ))\necho \"  Schedulable capacity: ${SCHEDULABLE_PCT}%\"\n\nif [[ $SCHEDULABLE_PCT -eq 100 ]]; then\n    echo \"  ‚úÖ All nodes are schedulable\"\nfi\n\necho \"\"\n```\n\n#### 4.3 Monitor Pod Scheduling\n\n```bash\necho \"üîÑ Monitoring new pod placement...\"\necho \"\"\n\n# Check for pending pods that might now be scheduled\nPENDING_PODS=$(kubectl get pods --all-namespaces --field-selector status.phase=Pending --no-headers 2>/dev/null | wc -l)\n\nif [[ $PENDING_PODS -gt 0 ]]; then\n    echo \"‚ÑπÔ∏è  $PENDING_PODS pods are currently Pending\"\n    echo \"\"\n    echo \"Pending pods may now be scheduled to uncordoned nodes:\"\n    kubectl get pods --all-namespaces --field-selector status.phase=Pending -o custom-columns=\\\nNAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nAGE:.metadata.creationTimestamp | head -10\n\n    echo \"\"\n    echo \"Monitor scheduling with:\"\n    echo \"  kubectl get pods --all-namespaces -w\"\nelse\n    echo \"‚úÖ No pending pods\"\nfi\n\necho \"\"\n```\n\n### Phase 5: Summary and Next Steps\n\n```bash\necho \"‚úÖ UNCORDON COMPLETE\"\necho \"====================\"\necho \"\"\necho \"Uncordoned $SUCCESS_COUNT node(s)\"\necho \"\"\necho \"Node(s) are now available for scheduling new pods.\"\necho \"\"\necho \"Next steps:\"\necho \"\"\necho \"1. Monitor pod distribution across nodes:\"\necho \"   kubectl get pods --all-namespaces -o wide | grep '$(echo $NODES | tr ' ' '\\\\|')'\"\necho \"\"\necho \"2. Check node resource utilization:\"\necho \"   kubectl top nodes $(echo $NODES | tr '\\n' ' ')\"\necho \"\"\necho \"3. Verify cluster autoscaler behavior (if enabled):\"\necho \"   kubectl get pods -n kube-system -l app=cluster-autoscaler\"\necho \"\"\necho \"4. Monitor node events:\"\nfor NODE_NAME in $NODES; do\n    echo \"   kubectl get events --field-selector involvedObject.name=$NODE_NAME\"\ndone\necho \"\"\necho \"Note: Pods that were evicted during drain will NOT automatically\"\necho \"return to these nodes. New pods or rescheduled pods will use the\"\necho \"uncordoned nodes based on scheduler decisions.\"\necho \"\"\n```\n\n## Scheduling Behavior After Uncordon\n\n### Immediate Effects\n- Node becomes available for new pod placement\n- Scheduler can assign pending pods to the node\n- Cluster autoscaler may scale down other nodes\n\n### What Does NOT Happen\n- Existing pods do not move to uncordoned node\n- Evicted pods are not automatically recreated there\n- Pod distribution is not automatically rebalanced\n\n### To Trigger Pod Rescheduling\n\nIf you want to force pod redistribution:\n\n```bash\n# Option 1: Scale deployment to trigger new pod placement\nkubectl scale deployment <name> --replicas=0\nkubectl scale deployment <name> --replicas=<original-count>\n\n# Option 2: Restart deployment (rolling restart)\nkubectl rollout restart deployment <name>\n\n# Option 3: Use descheduler (if installed)\nkubectl create job --from=cronjob/descheduler manual-deschedule\n```\n\n## Common Scenarios\n\n### Scenario 1: Return node to service after maintenance\n\n```bash\n# Node maintenance completed\ncluster-code node-uncordon --node worker-01\n\n# Verify ready for workloads\nkubectl get node worker-01\nkubectl top node worker-01\n```\n\n### Scenario 2: Uncordon entire node pool after upgrade\n\n```bash\n# Uncordon all nodes in pool\ncluster-code node-uncordon --selector node-pool=workers-v2\n\n# Monitor pod distribution\nwatch kubectl get pods -o wide\n```\n\n### Scenario 3: Restore cluster capacity during incident\n\n```bash\n# Quickly uncordon all nodes\ncluster-code node-uncordon --node \"*\"\n\n# Check for pending pods\nkubectl get pods -A --field-selector status.phase=Pending\n```\n\n## Examples\n\n### Example 1: Uncordon single node\n\n```bash\ncluster-code node-uncordon --node ip-10-0-1-42.ec2.internal\n```\n\n### Example 2: Uncordon multiple nodes by pattern\n\n```bash\ncluster-code node-uncordon --node \"gke-prod-cluster-pool-1-*\"\n```\n\n### Example 3: Uncordon all nodes in a specific pool\n\n```bash\ncluster-code node-uncordon --selector eks.amazonaws.com/nodegroup=workers-spot\n```\n\n### Example 4: Uncordon without health check (emergency)\n\n```bash\ncluster-code node-uncordon --node worker-05 --verify-health=false\n```\n\n### Example 5: Uncordon all cordoned nodes\n\n```bash\n# Find all cordoned nodes\nCORDONED=$(kubectl get nodes -o json | jq -r '.items[] | select(.spec.unschedulable==true) | .metadata.name')\n\n# Uncordon them\nfor node in $CORDONED; do\n  cluster-code node-uncordon --node $node\ndone\n```\n\n## Important Considerations\n\n### Pod Affinity/Anti-Affinity\nUncordoning a node doesn't override pod affinity rules. Pods will only schedule if:\n- Node labels match pod affinity requirements\n- Anti-affinity rules allow placement\n\n### Taints and Tolerations\nIf node has taints, only pods with matching tolerations can schedule, even after uncordon.\n\n### Resource Availability\nScheduler only places pods if node has sufficient:\n- CPU\n- Memory\n- Ephemeral storage\n- Custom resources (GPUs, etc.)\n\n### Cluster Autoscaler\nIf cluster autoscaler is enabled:\n- May scale down uncordoned nodes if underutilized\n- May prefer newly uncordoned nodes over scaling up\n- Respects PodDisruptionBudgets\n\n## Troubleshooting\n\n### Pods not scheduling to uncordoned node\n\n**Check node conditions:**\n```bash\nkubectl describe node <node-name>\n```\n\n**Check node resources:**\n```bash\nkubectl top node <node-name>\n```\n\n**Check pod requirements:**\n```bash\nkubectl describe pod <pod-name>\n# Look for nodeSelector, affinity, tolerations\n```\n\n### Node shows as Ready but pods failing\n\n**Check kubelet logs:**\n```bash\n# For systemd-based systems\njournalctl -u kubelet -f\n\n# For containerized kubelet\nkubectl logs -n kube-system <kubelet-pod>\n```\n\n**Check node events:**\n```bash\nkubectl get events --field-selector involvedObject.name=<node-name>\n```\n\n## Related Commands\n\n- `node-cordon`: Mark node as unschedulable\n- `node-drain`: Safely evict pods from node\n- `cluster-diagnose`: Analyze cluster health\n- `kubectl describe node`: View detailed node status\n- `kubectl top nodes`: View node resource usage"
              },
              {
                "name": "/pvc-status",
                "description": null,
                "path": "plugins/cluster-core/commands/pvc-status.md",
                "frontmatter": null,
                "content": "---\nname: pvc-status\ndescription: Analyze Persistent Volume Claims and storage resources\nargs:\n  - name: pvc\n    description: Specific PVC name (optional, shows all if not specified)\n    required: false\n    hint: pvc-name\n  - name: namespace\n    description: Namespace (default: all)\n    required: false\n    hint: --namespace\n  - name: detailed\n    description: Show detailed storage analysis including capacity and utilization\n    required: false\n    hint: --detailed\ntools:\n  - Bash\n  - Grep\npermissions:\n  allow:\n    - Bash(kubectl:*)\n    - Bash(oc:*)\nmodel: sonnet\ncolor: yellow\n---\n\n# Persistent Volume Claim Analysis\n\nI'll provide comprehensive analysis of your PVCs, persistent volumes, and storage resources.\n\n## PVC Overview\n\n```bash\n# PVC status summary\nkubectl get pvc {{#if namespace}}-n {{namespace}}{{/if}} -o wide\n\n# Detailed PVC information\nkubectl get pvc {{#if namespace}}-n {{namespace}}{{/if}} -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{.status.phase}{\"\\n\"}{.spec.storageClassName}{\"\\n\"}{.status.capacity.storage}{\"\\n\"}{.spec.accessModes}{\"\\n\\n\"}{end}'\n\n# Find PVCs with issues\nkubectl get pvc {{#if namespace}}-n {{namespace}}{{/if}} --field-selector=status.phase!=Bound\n```\n\n## Storage Class Analysis\n\n```bash\n# Available storage classes\nkubectl get storageclass\n\n# Default storage class\nkubectl get storageclass -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class}{\"\\n\"}{end}' | grep \"true\"\n\n# Storage class details\nkubectl get storageclass -o yaml | grep -A 10 -E \"(provisioner|parameters|reclaimPolicy)\"\n```\n\n## Persistent Volume Analysis\n\n```bash\n# Persistent volume status\nkubectl get pv -o wide\n\n# Volume usage by PVC\nkubectl get pv -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.claimRef.namespace}{\"\\t\"}{.spec.claimRef.name}{\"\\t\"}{.status.phase}{\"\\n\"}{end}'\n\n# Unbound volumes and available capacity\nkubectl get pv --field-selector=status.phase!=Bound -o wide\n```\n\n{{#if detailed}}\n## Detailed Storage Analysis\n\n### Capacity and Utilization\n\n```bash\n# Storage capacity summary\nkubectl get pv -o jsonpath='{range .items[*]}{.spec.capacity.storage}{\"\\n\"}{end}' | awk '{sum += $1} END {print \"Total Storage Capacity: \" sum \"Gi\"}'\n\n# Used vs. Available storage\nused_storage=$(kubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.status.capacity.storage}{\"\\n\"}{end}' | sed 's/Gi//' | awk '{sum += $1} END {print sum}')\ntotal_storage=$(kubectl get pv -o jsonpath='{range .items[*]}{.spec.capacity.storage}{\"\\n\"}{end}' | sed 's/Gi//' | awk '{sum += $1} END {print sum}')\necho \"Storage Utilization: $used_storage Gi used / $total_storage Gi total\"\n\n# Storage class usage distribution\nkubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.spec.storageClassName}{\"\\n\"}{end}' | sort | uniq -c\n```\n\n### Access Mode Analysis\n\n```bash\n# Access mode distribution\nkubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.accessModes}{\"\\n\"}{end}'\n\n# ReadWriteOnce vs ReadWriteMany usage\nrwo_count=$(kubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.spec.accessModes[?(@==\"ReadWriteOnce\")]}{\"\\n\"}{end}' | wc -l)\nrwx_count=$(kubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.spec.accessModes[?(@==\"ReadWriteMany\")]}{\"\\n\"}{end}' | wc -l)\nrox_count=$(kubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.spec.accessModes[?(@==\"ReadOnlyMany\")]}{\"\\n\"}{end}' | wc -l)\n\necho \"Access Mode Usage:\"\necho \"  ReadWriteOnce: $rwo_count PVCs\"\necho \"  ReadWriteMany: $rwx_count PVCs\"\necho \"  ReadOnlyMany: $rox_count PVCs\"\n```\n{{/if}}\n\n{{#if pvc}}\n## Specific PVC Analysis\n\n**Analyzing PVC: {{pvc}} in namespace: {{namespace}}**\n\n```bash\n# PVC details\nkubectl get pvc {{pvc}} -n {{namespace}} -o yaml\n\n# Associated persistent volume\npv_name=$(kubectl get pvc {{pvc}} -n {{namespace}} -o jsonpath='{.spec.volumeName}')\nif [ ! -z \"$pv_name\" ]; then\n  echo \"Persistent Volume: $pv_name\"\n  kubectl describe pv $pv_name\nfi\n\n# Pods using this PVC\nkubectl get pods -n {{namespace}} -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{\"\\n\"}{end}' | grep -B 1 {{pvc}}\n\n# Recent events for this PVC\nkubectl get events -n {{namespace}} --field-selector=involvedObject.name={{pvc}},involvedObject.kind=PersistentVolumeClaim --sort-by='.lastTimestamp'\n```\n{{/if}}\n\n## Storage Issues Detection\n\n### Common Problems\n\n```bash\n# PVCs stuck in Pending state\nkubectl get pvc --all-namespaces --field-selector=status.phase=Pending -o wide\n\n# PVCs with storage class issues\nkubectl get pvc --all-namespaces -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.storageClassName}{\"\\t\"}{.status.phase}{\"\\n\"}{end}' | while read pvc sc phase; do\n  if [ \"$phase\" != \"Bound\" ]; then\n    echo \"PVC $pvc with storage class $sc is $phase\"\n    # Check if storage class exists\n    if ! kubectl get storageclass $sc >/dev/null 2>&1; then\n      echo \"  ‚ùå Storage class $sc does not exist\"\n    fi\n  fi\ndone\n\n# Volumes with issues\nkubectl get pv --field-selector=status.phase!=Bound -o wide\n```\n\n### Capacity Issues\n\n```bash\n# Approaching capacity limits\nkubectl get pv -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.capacity.storage}{\"\\t\"}{.status.phase}{\"\\n\"}{end}' | while read pv capacity phase; do\n  if [ \"$phase\" = \"Bound\" ]; then\n    echo \"Volume $pv: $capacity ($phase)\"\n  else\n    echo \"Volume $pv: $capacity ($phase) - AVAILABLE\"\n  fi\ndone\n\n# Storage class provisioning issues\nkubectl get storageclass -o yaml | grep -A 5 -E \"(provisioner|reclaimPolicy|volumeBindingMode)\"\n```\n\n## Storage Optimization Recommendations\n\n### üìä Capacity Planning\n- **Growth Monitoring**: Track storage usage trends\n- **Capacity Alerts**: Set alerts for storage utilization\n- **Class Selection**: Choose appropriate storage classes\n- **Size Planning**: Right-size PVCs based on actual usage\n\n### üöÄ Performance Optimization\n- **Storage Tiers**: Use appropriate storage classes for performance needs\n- **Access Patterns**: Choose access modes based on application requirements\n- **IOPS Configuration**: Configure IOPS limits for performance-sensitive workloads\n- **Volume Expansion**: Enable volume expansion for growing workloads\n\n### üîí Security and Reliability\n- **Backup Strategy**: Implement backup for persistent data\n- **Replication**: Use replicated storage for critical data\n- **Encryption**: Enable storage encryption for sensitive data\n- **Access Control**: Implement proper RBAC for storage resources\n\n### üí∞ Cost Optimization\n- **Storage Classes**: Use cost-effective storage classes for non-critical data\n- **Lifecycle Management**: Implement automatic cleanup of unused volumes\n- **Size Optimization**: Right-size volumes to avoid over-provisioning\n- **Tiered Storage**: Move infrequently accessed data to cheaper storage\n\n## Troubleshooting Guide\n\n### PVC Stuck in Pending\n1. **Check storage class**: Verify storage class exists and is functional\n2. **Check resource quotas**: Ensure quotas allow storage provisioning\n3. **Check provisioner**: Verify storage provisioner is running\n4. **Check permissions**: Ensure proper RBAC permissions\n\n### Volume Mount Issues\n1. **Check volume binding**: Ensure PV is bound to PVC\n2. **Check mount path**: Verify mount path exists in container\n3. **Check permissions**: Ensure proper permissions for mount path\n4. **Check storage class**: Verify storage class supports required features\n\n### Performance Issues\n1. **Monitor IOPS**: Check I/O operations per second\n2. **Check latency**: Monitor storage latency metrics\n3. **Review access modes**: Ensure appropriate access modes\n4. **Optimize applications**: Review application I/O patterns\n\n## Interactive Analysis\n\nI can help you:\n\n1. **Diagnose Issues**: Identify and resolve PVC and storage problems\n2. **Plan Capacity**: Analyze usage patterns and plan for growth\n3. **Optimize Performance**: Recommend storage configuration improvements\n4. **Implement Best Practices**: Apply Kubernetes storage best practices\n\nWould you like me to proceed with the comprehensive PVC analysis?"
              },
              {
                "name": "/restore-cluster",
                "description": null,
                "path": "plugins/cluster-core/commands/restore-cluster.md",
                "frontmatter": null,
                "content": "---\nname: restore-cluster\ndescription: Restore cluster from Velero backup\ncategory: backup-restore\nparameters:\n  - name: backup-name\n    description: Name of the backup to restore from\n    required: true\n  - name: restore-name\n    description: Name for this restore operation (auto-generated if not provided)\n    required: false\n  - name: include-namespaces\n    description: Comma-separated list of namespaces to restore\n    required: false\n  - name: exclude-namespaces\n    description: Comma-separated list of namespaces to exclude\n    required: false\n  - name: include-resources\n    description: Resources to restore (e.g., pods,deployments)\n    required: false\n  - name: exclude-resources\n    description: Resources to exclude from restore\n    required: false\n  - name: namespace-mappings\n    description: Map namespaces (format: old:new,old2:new2)\n    required: false\n  - name: restore-pvs\n    description: Restore persistent volumes\n    type: boolean\n    default: true\n  - name: wait\n    description: Wait for restore to complete\n    type: boolean\n    default: true\ntags:\n  - restore\n  - disaster-recovery\n  - velero\n  - data-protection\n---\n\n# Cluster Restore\n\nRestore Kubernetes cluster resources and persistent volumes from a Velero backup.\n\n## Overview\n\nThis command restores cluster state from backups with:\n\n- **Full Cluster Restore**: Restore all resources and volumes\n- **Selective Restore**: Choose specific namespaces or resources\n- **Namespace Remapping**: Restore to different namespaces\n- **Volume Restore**: Restore persistent volumes from snapshots\n- **Validation**: Pre-restore checks and post-restore verification\n\n## Prerequisites\n\n- `velero` CLI installed\n- Velero server installed and running\n- Access to backup storage location\n- Valid backup available\n- `kubectl` cluster admin access\n\n## Important Considerations\n\n### When to Restore\n\n- **Disaster Recovery**: Complete cluster failure\n- **Data Recovery**: Accidental deletion\n- **Migration**: Move workloads to new cluster\n- **Testing**: Validate backup integrity\n\n### Restore Behavior\n\n- **Creates resources**: Does not update existing resources\n- **Conflicts**: Existing resources may block restore\n- **Order**: Resources restored in dependency order\n- **Volumes**: Requires compatible storage class\n\n## Workflow\n\n### Phase 1: Pre-restore Validation\n\n#### 1.1 Verify Velero and Backup\n\n```bash\nBACKUP_NAME=\"${BACKUP_NAME}\"\nRESTORE_NAME=\"${RESTORE_NAME:-restore-$BACKUP_NAME-$(date +%Y%m%d-%H%M%S)}\"\nRESTORE_PVS=\"${RESTORE_PVS:-true}\"\nWAIT=\"${WAIT:-true}\"\n\necho \"üîç Validating restore prerequisites...\"\necho \"\"\n\n# Check Velero installation\nif ! command -v velero &>/dev/null; then\n    echo \"‚ùå ERROR: Velero CLI not found\"\n    exit 1\nfi\n\nif ! kubectl get namespace velero &>/dev/null; then\n    echo \"‚ùå ERROR: Velero not installed in cluster\"\n    exit 1\nfi\n\necho \"‚úÖ Velero installation verified\"\n\n# Check if backup exists\nif ! velero backup get \"$BACKUP_NAME\" &>/dev/null; then\n    echo \"‚ùå ERROR: Backup not found: $BACKUP_NAME\"\n    echo \"\"\n    echo \"Available backups:\"\n    velero backup get\n    exit 1\nfi\n\n# Get backup details\nBACKUP_STATUS=$(velero backup get \"$BACKUP_NAME\" -o json | jq -r '.status.phase')\nBACKUP_EXPIRATION=$(velero backup get \"$BACKUP_NAME\" -o json | jq -r '.status.expiration')\n\necho \"‚úÖ Backup found: $BACKUP_NAME\"\necho \"   Status: $BACKUP_STATUS\"\necho \"   Expires: $BACKUP_EXPIRATION\"\n\nif [[ \"$BACKUP_STATUS\" != \"Completed\" ]]; then\n    echo \"‚ö†Ô∏è  WARNING: Backup status is $BACKUP_STATUS (not Completed)\"\n    echo \"\"\n    echo \"Continue anyway? (yes/no)\"\n    read -r CONFIRM\n    if [[ \"$CONFIRM\" != \"yes\" ]]; then\n        exit 0\n    fi\nfi\n\necho \"\"\n```\n\n#### 1.2 Analyze Backup Contents\n\n```bash\necho \"üìä Analyzing backup contents...\"\necho \"\"\n\n# Show backup details\nvelero backup describe \"$BACKUP_NAME\" --details | head -60\n\nBACKUP_JSON=$(velero backup get \"$BACKUP_NAME\" -o json)\n\n# Count resources in backup\nTOTAL_ITEMS=$(echo \"$BACKUP_JSON\" | jq -r '.status.progress.totalItems // 0')\nITEMS_BACKED_UP=$(echo \"$BACKUP_JSON\" | jq -r '.status.progress.itemsBackedUp // 0')\nVOLUME_SNAPSHOTS=$(echo \"$BACKUP_JSON\" | jq -r '.status.volumeSnapshotsCompleted // 0')\n\necho \"\"\necho \"Backup contains:\"\necho \"  Total items: $TOTAL_ITEMS\"\necho \"  Items backed up: $ITEMS_BACKED_UP\"\necho \"  Volume snapshots: $VOLUME_SNAPSHOTS\"\n\n# Show namespaces in backup\necho \"\"\necho \"Namespaces in backup:\"\nvelero backup describe \"$BACKUP_NAME\" --details 2>/dev/null | \\\n    grep -A 100 \"Namespaces:\" | grep \"^  \" | head -20\n\necho \"\"\n```\n\n#### 1.3 Check Target Cluster\n\n```bash\necho \"üéØ Checking target cluster...\"\necho \"\"\n\n# Get current cluster context\nCURRENT_CONTEXT=$(kubectl config current-context)\nCURRENT_CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name==\\\"$CURRENT_CONTEXT\\\")].context.cluster}\")\n\necho \"Target cluster: $CURRENT_CLUSTER\"\necho \"Context: $CURRENT_CONTEXT\"\necho \"\"\n\n# Check for existing resources that might conflict\nif [[ -z \"${INCLUDE_NAMESPACES}\" ]]; then\n    echo \"Checking for potential conflicts...\"\n\n    # Check if key namespaces exist\n    EXISTING_NS=$(kubectl get namespaces -o json | jq -r '.items[].metadata.name' | grep -v \"^kube-\" | grep -v \"^velero$\" | wc -l)\n\n    if [[ $EXISTING_NS -gt 0 ]]; then\n        echo \"‚ö†Ô∏è  WARNING: $EXISTING_NS existing namespaces found\"\n        echo \"\"\n        echo \"Existing namespaces:\"\n        kubectl get namespaces -o custom-columns=NAME:.metadata.name,STATUS:.status.phase --no-headers | grep -v \"^kube-\" | grep -v \"^velero \" | head -10\n        echo \"\"\n        echo \"Restore may conflict with existing resources.\"\n        echo \"Consider using --include-namespaces or --namespace-mappings\"\n        echo \"\"\n        echo \"Continue with restore? (yes/no)\"\n        read -r CONFIRM\n        if [[ \"$CONFIRM\" != \"yes\" ]]; then\n            echo \"Restore cancelled\"\n            exit 0\n        fi\n    fi\nfi\n\necho \"\"\n```\n\n### Phase 2: Build Restore Command\n\n#### 2.1 Construct Velero Restore Command\n\n```bash\necho \"üõ†Ô∏è  Building restore command...\"\necho \"\"\n\nRESTORE_CMD=\"velero restore create \\\"$RESTORE_NAME\\\" --from-backup=\\\"$BACKUP_NAME\\\"\"\n\n# Namespace inclusion/exclusion\nif [[ -n \"${INCLUDE_NAMESPACES}\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --include-namespaces=\\\"${INCLUDE_NAMESPACES}\\\"\"\n    echo \"  Including namespaces: ${INCLUDE_NAMESPACES}\"\nelif [[ -n \"${EXCLUDE_NAMESPACES}\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --exclude-namespaces=\\\"${EXCLUDE_NAMESPACES}\\\"\"\n    echo \"  Excluding namespaces: ${EXCLUDE_NAMESPACES}\"\nfi\n\n# Resource inclusion/exclusion\nif [[ -n \"${INCLUDE_RESOURCES}\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --include-resources=\\\"${INCLUDE_RESOURCES}\\\"\"\n    echo \"  Including resources: ${INCLUDE_RESOURCES}\"\nfi\n\nif [[ -n \"${EXCLUDE_RESOURCES}\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --exclude-resources=\\\"${EXCLUDE_RESOURCES}\\\"\"\n    echo \"  Excluding resources: ${EXCLUDE_RESOURCES}\"\nfi\n\n# Namespace mappings\nif [[ -n \"${NAMESPACE_MAPPINGS}\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --namespace-mappings=\\\"${NAMESPACE_MAPPINGS}\\\"\"\n    echo \"  Namespace mappings: ${NAMESPACE_MAPPINGS}\"\nfi\n\n# Restore PVs\nif [[ \"$RESTORE_PVS\" == \"true\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --restore-volumes=true\"\n    echo \"  Restore volumes: Enabled\"\nelse\n    RESTORE_CMD=\"$RESTORE_CMD --restore-volumes=false\"\n    echo \"  Restore volumes: Disabled\"\nfi\n\n# Wait for completion\nif [[ \"$WAIT\" == \"true\" ]]; then\n    RESTORE_CMD=\"$RESTORE_CMD --wait\"\nfi\n\necho \"\"\necho \"Restore command:\"\necho \"  $RESTORE_CMD\"\necho \"\"\n\necho \"‚ö†Ô∏è  FINAL CONFIRMATION\"\necho \"=====================\"\necho \"\"\necho \"Ready to restore from backup: $BACKUP_NAME\"\necho \"Restore name: $RESTORE_NAME\"\necho \"\"\necho \"This will create resources in the cluster.\"\necho \"Existing resources will NOT be modified.\"\necho \"\"\necho \"Proceed with restore? (yes/no)\"\nread -r FINAL_CONFIRM\n\nif [[ \"$FINAL_CONFIRM\" != \"yes\" ]]; then\n    echo \"Restore cancelled\"\n    exit 0\nfi\n\necho \"\"\n```\n\n### Phase 3: Execute Restore\n\n#### 3.1 Perform Restore\n\n```bash\necho \"‚ôªÔ∏è  Executing restore...\"\necho \"\"\n\nRESTORE_START=$(date +%s)\n\n# Execute restore\nif eval \"$RESTORE_CMD\"; then\n    echo \"\"\n    echo \"‚úÖ Restore initiated: $RESTORE_NAME\"\nelse\n    echo \"\"\n    echo \"‚ùå Restore failed to start\"\n    exit 1\nfi\n\n# Monitor progress\nif [[ \"$WAIT\" == \"true\" ]]; then\n    echo \"\"\n    echo \"‚è≥ Waiting for restore to complete...\"\n    echo \"\"\n    sleep 2\nfi\n```\n\n### Phase 4: Verification\n\n#### 4.1 Check Restore Status\n\n```bash\necho \"\"\necho \"üîç Verifying restore...\"\necho \"\"\n\n# Get restore status\nRESTORE_STATUS=$(velero restore get \"$RESTORE_NAME\" -o json | jq -r '.status.phase')\nRESTORE_ERRORS=$(velero restore get \"$RESTORE_NAME\" -o json | jq -r '.status.errors // 0')\nRESTORE_WARNINGS=$(velero restore get \"$RESTORE_NAME\" -o json | jq -r '.status.warnings // 0')\n\necho \"Restore Status: $RESTORE_STATUS\"\necho \"Errors: $RESTORE_ERRORS\"\necho \"Warnings: $RESTORE_WARNINGS\"\n\nif [[ \"$RESTORE_STATUS\" == \"Completed\" ]]; then\n    echo \"‚úÖ Restore completed successfully\"\nelif [[ \"$RESTORE_STATUS\" == \"PartiallyFailed\" ]]; then\n    echo \"‚ö†Ô∏è  Restore partially failed\"\n    echo \"\"\n    echo \"Check errors with:\"\n    echo \"  velero restore logs $RESTORE_NAME\"\nelif [[ \"$RESTORE_STATUS\" == \"Failed\" ]]; then\n    echo \"‚ùå Restore failed\"\n    echo \"\"\n    echo \"Check errors with:\"\n    echo \"  velero restore logs $RESTORE_NAME\"\n    exit 1\nelif [[ \"$RESTORE_STATUS\" == \"InProgress\" ]]; then\n    echo \"‚è≥ Restore still in progress\"\n    echo \"\"\n    echo \"Monitor with:\"\n    echo \"  velero restore describe $RESTORE_NAME\"\nfi\n\necho \"\"\n```\n\n#### 4.2 Verify Restored Resources\n\n```bash\necho \"üìã Verifying restored resources...\"\necho \"\"\n\n# Show restore details\nvelero restore describe \"$RESTORE_NAME\" | head -60\n\n# Get restored resource counts\nRESTORE_JSON=$(velero restore get \"$RESTORE_NAME\" -o json)\n\nITEMS_RESTORED=$(echo \"$RESTORE_JSON\" | jq -r '.status.progress.itemsRestored // 0')\nTOTAL_ITEMS=$(echo \"$RESTORE_JSON\" | jq -r '.status.progress.totalItems // 0')\n\necho \"\"\necho \"Restore progress:\"\necho \"  Items restored: $ITEMS_RESTORED\"\necho \"  Total items: $TOTAL_ITEMS\"\n\n# Check pod status\necho \"\"\necho \"Checking pod status...\"\n\nRESTORED_NAMESPACES=\"${INCLUDE_NAMESPACES}\"\nif [[ -z \"$RESTORED_NAMESPACES\" ]]; then\n    # Get namespaces from backup (excluding system namespaces)\n    RESTORED_NAMESPACES=$(kubectl get namespaces -o json | \\\n        jq -r '.items[].metadata.name' | \\\n        grep -v \"^kube-\" | grep -v \"^velero$\" | \\\n        head -10 | tr '\\n' ',' | sed 's/,$//')\nfi\n\nif [[ -n \"$RESTORED_NAMESPACES\" ]]; then\n    for NS in $(echo \"$RESTORED_NAMESPACES\" | tr ',' ' '); do\n        if kubectl get namespace \"$NS\" &>/dev/null; then\n            POD_COUNT=$(kubectl get pods -n \"$NS\" --no-headers 2>/dev/null | wc -l)\n            RUNNING_PODS=$(kubectl get pods -n \"$NS\" --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)\n\n            echo \"  Namespace $NS: $RUNNING_PODS/$POD_COUNT pods running\"\n        fi\n    done\nfi\n\n# Calculate restore duration\nRESTORE_END=$(date +%s)\nRESTORE_DURATION=$(( RESTORE_END - RESTORE_START ))\n\necho \"\"\necho \"Restore duration: ${RESTORE_DURATION}s\"\necho \"\"\n```\n\n### Phase 5: Post-Restore Checks\n\n#### 5.1 Health Verification\n\n```bash\necho \"üè• Running post-restore health checks...\"\necho \"\"\n\n# Check for pods not in Running state\nPROBLEMATIC_PODS=$(kubectl get pods --all-namespaces \\\n    --field-selector=status.phase!=Running,status.phase!=Succeeded \\\n    --no-headers 2>/dev/null | wc -l)\n\nif [[ $PROBLEMATIC_PODS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  $PROBLEMATIC_PODS pods not in Running/Succeeded state\"\n    echo \"\"\n    kubectl get pods --all-namespaces \\\n        --field-selector=status.phase!=Running,status.phase!=Succeeded \\\n        -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase,REASON:.status.reason \\\n        | head -20\n    echo \"\"\nelse\n    echo \"‚úÖ All pods healthy\"\nfi\n\n# Check PVCs\nif [[ \"$RESTORE_PVS\" == \"true\" ]]; then\n    echo \"\"\n    echo \"Checking Persistent Volume Claims...\"\n\n    PVC_TOTAL=$(kubectl get pvc --all-namespaces --no-headers 2>/dev/null | wc -l)\n    PVC_BOUND=$(kubectl get pvc --all-namespaces --field-selector=status.phase=Bound --no-headers 2>/dev/null | wc -l)\n\n    if [[ $PVC_TOTAL -gt 0 ]]; then\n        echo \"  PVCs: $PVC_BOUND/$PVC_TOTAL bound\"\n\n        if [[ $PVC_BOUND -lt $PVC_TOTAL ]]; then\n            echo \"\"\n            echo \"  Unbound PVCs:\"\n            kubectl get pvc --all-namespaces --field-selector=status.phase!=Bound \\\n                -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase \\\n                | head -10\n        fi\n    else\n        echo \"  No PVCs found\"\n    fi\nfi\n\necho \"\"\n```\n\n### Phase 6: Summary\n\n```bash\necho \"‚úÖ RESTORE COMPLETE\"\necho \"===================\"\necho \"\"\necho \"Restore name: $RESTORE_NAME\"\necho \"From backup: $BACKUP_NAME\"\necho \"Status: $RESTORE_STATUS\"\necho \"\"\necho \"Restored:\"\necho \"  Items: $ITEMS_RESTORED / $TOTAL_ITEMS\"\n\nif [[ $RESTORE_ERRORS -gt 0 ]]; then\n    echo \"  Errors: $RESTORE_ERRORS\"\nfi\n\nif [[ $RESTORE_WARNINGS -gt 0 ]]; then\n    echo \"  Warnings: $RESTORE_WARNINGS\"\nfi\n\necho \"\"\necho \"View restore details:\"\necho \"  velero restore describe $RESTORE_NAME\"\necho \"\"\necho \"View restore logs:\"\necho \"  velero restore logs $RESTORE_NAME\"\necho \"\"\necho \"List all restores:\"\necho \"  velero restore get\"\necho \"\"\necho \"Delete this restore record:\"\necho \"  velero restore delete $RESTORE_NAME\"\necho \"\"\n\nif [[ $RESTORE_ERRORS -gt 0 || $RESTORE_WARNINGS -gt 0 || $PROBLEMATIC_PODS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  Post-restore actions needed:\"\n    echo \"\"\n\n    if [[ $RESTORE_ERRORS -gt 0 || $RESTORE_WARNINGS -gt 0 ]]; then\n        echo \"1. Review restore errors and warnings:\"\n        echo \"   velero restore logs $RESTORE_NAME | grep -E 'error|warning'\"\n        echo \"\"\n    fi\n\n    if [[ $PROBLEMATIC_PODS -gt 0 ]]; then\n        echo \"2. Investigate unhealthy pods:\"\n        echo \"   kubectl get pods -A --field-selector=status.phase!=Running\"\n        echo \"\"\n    fi\n\n    echo \"3. Verify application functionality\"\n    echo \"\"\nfi\n\necho \"Next steps:\"\necho \"  ‚Ä¢ Test critical application workflows\"\necho \"  ‚Ä¢ Verify data integrity\"\necho \"  ‚Ä¢ Check service endpoints and ingresses\"\necho \"  ‚Ä¢ Review logs for any errors\"\necho \"  ‚Ä¢ Update DNS/load balancer configurations if needed\"\necho \"\"\n```\n\n## Restore Scenarios\n\n### Full Cluster Restore\n\nRestore everything from backup:\n\n```bash\ncluster-code restore-cluster --backup-name full-backup-20241031\n```\n\n### Selective Namespace Restore\n\nRestore specific namespaces only:\n\n```bash\ncluster-code restore-cluster \\\n  --backup-name production-backup \\\n  --include-namespaces myapp,database\n```\n\n### Restore to Different Namespace\n\nRestore with namespace remapping:\n\n```bash\ncluster-code restore-cluster \\\n  --backup-name prod-backup \\\n  --namespace-mappings production:staging\n```\n\n### Configuration-Only Restore\n\nRestore without volumes:\n\n```bash\ncluster-code restore-cluster \\\n  --backup-name config-backup \\\n  --restore-pvs=false\n```\n\n## Examples\n\n### Example 1: Complete restore\n\n```bash\ncluster-code restore-cluster --backup-name daily-backup-20241031\n```\n\n### Example 2: Restore specific application\n\n```bash\ncluster-code restore-cluster \\\n  --backup-name weekly-backup \\\n  --include-namespaces myapp \\\n  --restore-pvs=true\n```\n\n### Example 3: Test restore in dev environment\n\n```bash\ncluster-code restore-cluster \\\n  --backup-name prod-backup \\\n  --namespace-mappings production:dev-test \\\n  --restore-name test-restore\n```\n\n## Common Issues\n\n### Issue: Resources already exist\n\n**Error**: \"the object already exists\"\n\n**Solution**: Resources aren't updated. Either delete existing resources or use different namespace:\n\n```bash\nkubectl delete namespace conflicting-namespace\n# or\ncluster-code restore-cluster --namespace-mappings old:new\n```\n\n### Issue: PVC not binding\n\n**Solution**: Check storage class compatibility:\n\n```bash\nkubectl get pvc -A\nkubectl get storageclass\n```\n\n### Issue: Pods stuck in Pending\n\n**Solution**: Check events and node resources:\n\n```bash\nkubectl describe pod <pod-name>\nkubectl get nodes\nkubectl top nodes\n```\n\n## Best Practices\n\n1. **Test restores regularly** - Validate backups work\n2. **Restore to test cluster first** - Verify before production restore\n3. **Document restore procedures** - Maintain runbooks\n4. **Check compatibility** - Ensure Kubernetes versions match\n5. **Plan for conflicts** - Use namespace mappings when needed\n6. **Verify data integrity** - Test application functionality post-restore\n7. **Monitor restoration** - Watch for errors during restore\n\n## Related Commands\n\n- `backup-cluster`: Create cluster backups\n- `cluster-diagnose`: Verify cluster health post-restore\n- `velero restore get`: List all restores\n- `velero backup get`: List available backups\n"
              },
              {
                "name": "/service-describe",
                "description": null,
                "path": "plugins/cluster-core/commands/service-describe.md",
                "frontmatter": null,
                "content": "---\nname: service-describe\ndescription: Detailed service analysis including endpoints, connectivity, and configuration\nargs:\n  - name: service\n    description: Service name to analyze (required)\n    required: true\n    hint: service-name\n  - name: namespace\n    description: Namespace (default: current)\n    required: false\n    hint: --namespace\n  - name: test\n    description: Test connectivity to the service\n    required: false\n    hint: --test\ntools:\n  - Bash\n  - Grep\npermissions:\n  allow:\n    - Bash(kubectl:*)\n    - Bash(oc:*)\nmodel: sonnet\ncolor: blue\n---\n\n# Service Analysis and Connectivity Testing\n\nI'll provide comprehensive analysis of your Kubernetes service including configuration, endpoints, and connectivity testing.\n\n## Service Configuration\n\n```bash\n# Basic service information\nkubectl get service {{service}} -n {{namespace}} -o wide\n\n# Service specification details\nkubectl get service {{service}} -n {{namespace}} -o yaml\n\n# Service selector and matching pods\nkubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.selector}'\n\n# Pods matching the service selector\nselector=$(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.selector | keys[0]}')=$(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.selector | values[0]}')\nkubectl get pods -n {{namespace}} -l $selector -o wide\n```\n\n## Endpoint Analysis\n\n```bash\n# Service endpoints\nkubectl get endpoints {{service}} -n {{namespace}} -o wide\n\n# Endpoint details with pod information\nkubectl get endpoints {{service}} -n {{namespace}} -o yaml\n\n# Check if endpoints are ready\nkubectl get endpoints {{service}} -n {{namespace}} -o jsonpath='{range .subsets[*]}{range .addresses[*]}{.ip}{\":\"}{.ports[0].port}{\" ready\\n\"}{end}{end}'\n\n# Check for unready endpoints\nkubectl get endpoints {{service}} -n {{namespace}} -o jsonpath='{range .subsets[*]}{range .notReadyAddresses[*]}{.ip}{\":\"}{.ports[0].port}{\" not ready\\n\"}{end}{end}'\n```\n\n## Service Types and Configuration\n\n```bash\n# Service type analysis\nkubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.type}{\"\\n\"}'\n\n# ClusterIP service details\nif [ \"$(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.type}')\" = \"ClusterIP\" ]; then\n  echo \"ClusterIP: $(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.clusterIP}')\"\n  echo \"Ports: $(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{range .spec.ports[*]}{.port}{\"/\"}{.targetPort}{\", \"}{end}')\"\nfi\n\n# NodePort service details\nif [ \"$(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.type}')\" = \"NodePort\" ]; then\n  echo \"NodePort: $(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{range .spec.ports[*]}{.nodePort}{\"\\n\"}{end}')\"\nfi\n\n# LoadBalancer service details\nif [ \"$(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.spec.type}')\" = \"LoadBalancer\" ]; then\n  echo \"External IP: $(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n  echo \"Hostname: $(kubectl get service {{service}} -n {{namespace}} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\nfi\n```\n\n{{#if test}}\n## Connectivity Testing\n\n```bash\n# Create a test pod for connectivity testing\nkubectl run service-test --image=nicolaka/netshoot --rm -it --restart=Never -- bash -c \"\necho 'Testing DNS resolution for {{service}}.{{namespace}}.svc.cluster.local'\nnslookup {{service}}.{{namespace}}.svc.cluster.local\n\necho 'Testing service connectivity'\n# Test each port\nkubectl get service {{service}} -n {{namespace}} -o jsonpath='{range .spec.ports[*]}{.port}{\\\" \\\"}{end}' | while read port; do\n  echo \\\"Testing port \\$port...\\\"\n  timeout 5 nc -zv {{service}}.{{namespace}}.svc.cluster.local \\$port 2>&1 || echo \\\"Connection failed on port \\$port\\\"\ndone\n\"\n```\n\n## Port Forwarding Test\n\n```bash\n# Set up port forwarding for local testing\necho \"Setting up port forwarding for {{service}}...\"\nkubectl port-forward service/{{service}} 8080:80 -n {{namespace}} &\n\n# Test local connection\nsleep 2\ncurl -s http://localhost:8080/healthz 2>/dev/null && echo \"‚úÖ Service accessible via port-forward\" || echo \"‚ùå Service not accessible via port-forward\"\n\n# Clean up\nkill %1 2>/dev/null\n```\n{{/if}}\n\n## DNS and Service Discovery\n\n```bash\n# Test DNS resolution\nkubectl run dns-test --image=busybox --rm -it --restart=Never -- nslookup {{service}}.{{namespace}}.svc.cluster.local\n\n# Test service discovery from pods\nkubectl run dns-test --image=busybox --rm -it --restart=Never -- wget -qO- {{service}}.{{namespace}}.svc.cluster.local:80/healthz 2>/dev/null || echo \"Service not reachable\"\n```\n\n## Related Resources\n\n```bash\n# Ingress resources pointing to this service\nkubectl get ingress -n {{namespace}} -o jsonpath='{range .items[*]}{range .spec.rules[*]}{range .http.paths[*]}{.backend.service.name}{\"\\n\"}{end}{end}{end}' | grep {{service}}\n\n# Endpointslice information\nkubectl get endpointslices -n {{namespace}} -l kubernetes.io/service-name={{service}}\n\n# Network policies affecting this service\nkubectl get networkpolicies -n {{namespace}} -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}' | while read policy; do\n  echo \"Checking policy $policy for service {{service}}...\"\n  kubectl get networkpolicy $policy -n {{namespace}} -o yaml | grep -q {{service}} && echo \"Policy $policy affects service {{service}}\"\ndone\n```\n\n## Service Health Analysis\n\nI'll analyze:\n\n### üîç Configuration Issues\n- **Selector Mismatches**: Service selector doesn't match pod labels\n- **Port Configuration**: Incorrect port mappings or protocols\n- **Service Type**: Inappropriate service type for use case\n- **Session Affinity**: Missing or misconfigured session affinity\n\n### üö® Connectivity Problems\n- **No Endpoints**: Service has no ready endpoints\n- **DNS Resolution**: Service not resolvable within cluster\n- **Network Policies**: Traffic blocked by network policies\n- **Firewall Rules**: External access blocked by security rules\n\n### üìä Performance Issues\n- **Load Balancing**: Uneven traffic distribution\n- **Timeout Settings**: Too short or too long timeouts\n- **Connection Limits**: Hitting connection or rate limits\n- **Resource Exhaustion**: Service endpoints overwhelmed\n\n## Troubleshooting Steps\n\n### Quick Diagnostics\n1. **Check service status**: `kubectl get service {{service}} -n {{namespace}}`\n2. **Verify endpoints**: `kubectl get endpoints {{service}} -n {{namespace}}`\n3. **Check selector labels**: Verify pod labels match service selector\n4. **Test DNS resolution**: Use nslookup to test service discovery\n\n### Deep Investigation\n1. **Pod health**: Check if backend pods are healthy\n2. **Port configuration**: Verify target ports match container ports\n3. **Network policies**: Review network policies affecting traffic\n4. **Resource limits**: Check for resource constraints\n\n### External Access Issues\n1. **Load balancer**: Verify external load balancer provisioning\n2. **Ingress configuration**: Check ingress rules and backend services\n3. **NodePort access**: Test node port accessibility from external clients\n4. **Security groups**: Verify cloud security group rules\n\n## Optimization Recommendations\n\nBased on the analysis, I can suggest:\n\n### Performance Tuning\n- **Load balancing algorithms**: Choose appropriate algorithms\n- **Health check configuration**: Optimize readiness and liveness probes\n- **Timeout settings**: Adjust based on application requirements\n- **Connection pooling**: Configure for optimal performance\n\n### Security Hardening\n- **Network policies**: Implement least-privilege network access\n- **Service mesh integration**: Add mTLS and traffic management\n- **Access control**: Implement proper RBAC and access controls\n- **Encryption**: Enable TLS for service-to-service communication\n\n### Reliability Improvements\n- **Redundancy**: Ensure multiple backend endpoints\n- **Health monitoring**: Set up comprehensive health checks\n- **Failover testing**: Test service failover scenarios\n- **Monitoring**: Implement service-level monitoring and alerting\n\nWould you like me to proceed with the detailed service analysis and connectivity testing?"
              }
            ],
            "skills": []
          },
          {
            "name": "k8sgpt-analyzers",
            "description": "AI-powered Kubernetes cluster diagnostics using enhanced K8sGPT analyzers",
            "source": "./plugins/k8sgpt-analyzers",
            "category": "diagnostics",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install k8sgpt-analyzers@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "commit-commands",
            "description": "Commands for git commit workflows including commit, push, and PR creation",
            "source": "./plugins/commit-commands",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install commit-commands@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/clean_gone",
                "description": "Cleans up all git branches marked as [gone] (branches that have been deleted on the remote but still exist locally), including removing associated worktrees.",
                "path": "plugins/commit-commands/commands/clean_gone.md",
                "frontmatter": {
                  "description": "Cleans up all git branches marked as [gone] (branches that have been deleted on the remote but still exist locally), including removing associated worktrees."
                },
                "content": "## Your Task\n\nYou need to execute the following bash commands to clean up stale local branches that have been deleted from the remote repository.\n\n## Commands to Execute\n\n1. **First, list branches to identify any with [gone] status**\n   Execute this command:\n   ```bash\n   git branch -v\n   ```\n   \n   Note: Branches with a '+' prefix have associated worktrees and must have their worktrees removed before deletion.\n\n2. **Next, identify worktrees that need to be removed for [gone] branches**\n   Execute this command:\n   ```bash\n   git worktree list\n   ```\n\n3. **Finally, remove worktrees and delete [gone] branches (handles both regular and worktree branches)**\n   Execute this command:\n   ```bash\n   # Process all [gone] branches, removing '+' prefix if present\n   git branch -v | grep '\\[gone\\]' | sed 's/^[+* ]//' | awk '{print $1}' | while read branch; do\n     echo \"Processing branch: $branch\"\n     # Find and remove worktree if it exists\n     worktree=$(git worktree list | grep \"\\\\[$branch\\\\]\" | awk '{print $1}')\n     if [ ! -z \"$worktree\" ] && [ \"$worktree\" != \"$(git rev-parse --show-toplevel)\" ]; then\n       echo \"  Removing worktree: $worktree\"\n       git worktree remove --force \"$worktree\"\n     fi\n     # Delete the branch\n     echo \"  Deleting branch: $branch\"\n     git branch -D \"$branch\"\n   done\n   ```\n\n## Expected Behavior\n\nAfter executing these commands, you will:\n\n- See a list of all local branches with their status\n- Identify and remove any worktrees associated with [gone] branches\n- Delete all branches marked as [gone]\n- Provide feedback on which worktrees and branches were removed\n\nIf no branches are marked as [gone], report that no cleanup was needed."
              },
              {
                "name": "/commit-push-pr",
                "description": "Commit, push, and open a PR",
                "path": "plugins/commit-commands/commands/commit-push-pr.md",
                "frontmatter": {
                  "allowed-tools": "Bash(git checkout --branch:*), Bash(git add:*), Bash(git status:*), Bash(git push:*), Bash(git commit:*), Bash(gh pr create:*)",
                  "description": "Commit, push, and open a PR"
                },
                "content": "## Context\n\n- Current git status: !`git status`\n- Current git diff (staged and unstaged changes): !`git diff HEAD`\n- Current branch: !`git branch --show-current`\n\n## Your task\n\nBased on the above changes:\n\n1. Create a new branch if on main\n2. Create a single commit with an appropriate message\n3. Push the branch to origin\n4. Create a pull request using `gh pr create`\n5. You have the capability to call multiple tools in a single response. You MUST do all of the above in a single message. Do not use any other tools or do anything else. Do not send any other text or messages besides these tool calls."
              },
              {
                "name": "/commit",
                "description": "Create a git commit",
                "path": "plugins/commit-commands/commands/commit.md",
                "frontmatter": {
                  "allowed-tools": "Bash(git add:*), Bash(git status:*), Bash(git commit:*)",
                  "description": "Create a git commit"
                },
                "content": "## Context\n\n- Current git status: !`git status`\n- Current git diff (staged and unstaged changes): !`git diff HEAD`\n- Current branch: !`git branch --show-current`\n- Recent commits: !`git log --oneline -10`\n\n## Your task\n\nBased on the above changes, create a single git commit.\n\nYou have the capability to call multiple tools in a single response. Stage and create the commit using a single message. Do not use any other tools or do anything else. Do not send any other text or messages besides these tool calls."
              }
            ],
            "skills": []
          },
          {
            "name": "cloud-azure",
            "description": "Azure cloud provider integration for AKS and ARO cluster management with MCP server",
            "source": "./plugins/cloud-azure",
            "category": "cloud-provider",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install cloud-azure@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/azure-cluster-connect",
                "description": "Connect to an AKS or ARO cluster and configure kubectl/oc context",
                "path": "plugins/cloud-azure/commands/azure-cluster-connect.md",
                "frontmatter": {
                  "name": "azure-cluster-connect",
                  "description": "Connect to an AKS or ARO cluster and configure kubectl/oc context",
                  "category": "cloud-management",
                  "tools": [
                    "Bash(az:*)",
                    "Bash(kubectl:*)",
                    "Bash(oc:*)",
                    "Write(.*\\.kube/config)"
                  ],
                  "parameters": [
                    {
                      "name": "name",
                      "description": "Cluster name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "resource-group",
                      "description": "Resource group name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "type",
                      "description": "Cluster type (aks or aro) - auto-detected if not specified",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "admin",
                      "description": "Get admin credentials (AKS only)",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "examples": [
                    "azure-cluster-connect --name my-aks-cluster --resource-group my-rg",
                    "azure-cluster-connect --name my-aro-cluster --resource-group my-rg --type aro",
                    "azure-cluster-connect --name prod-aks --resource-group production --admin"
                  ]
                },
                "content": "# Azure Cluster Connection\n\nConnect to an AKS or ARO cluster and configure your local kubectl/oc context for cluster management.\n\n## Task Workflow\n\n### Phase 1: Cluster Detection\n\n1. **Auto-detect cluster type** (if not specified):\n   ```bash\n   # Try AKS first\n   if az aks show --name <name> --resource-group <resource-group> &>/dev/null; then\n     CLUSTER_TYPE=\"aks\"\n   # Try ARO\n   elif az aro show --name <name> --resource-group <resource-group> &>/dev/null; then\n     CLUSTER_TYPE=\"aro\"\n   else\n     echo \"‚ùå Cluster not found in resource group\"\n     exit 1\n   fi\n   ```\n\n2. **Get cluster information**:\n   - For AKS: `az aks show --name <name> --resource-group <resource-group>`\n   - For ARO: `az aro show --name <name> --resource-group <resource-group>`\n\n3. **Check cluster state**:\n   - Verify `provisioningState` is \"Succeeded\"\n   - For AKS, check `powerState` is \"Running\"\n   - If not ready, warn user and ask if they want to continue\n\n### Phase 2: Get Credentials\n\n#### For AKS Clusters:\n\n1. **Get kubeconfig credentials**:\n   ```bash\n   az aks get-credentials \\\n     --resource-group <resource-group> \\\n     --name <cluster-name> \\\n     ${ADMIN:+--admin} \\\n     --overwrite-existing\n   ```\n\n2. **Verify connection**:\n   ```bash\n   kubectl cluster-info\n   kubectl get nodes\n   ```\n\n3. **Display connection info**:\n   ```\n   ‚úÖ Connected to AKS cluster: <cluster-name>\n\n   Context: <cluster-name>\n   API Server: <api-endpoint>\n   Kubernetes Version: <version>\n   Nodes: <node-count>\n\n   Current context set to: <cluster-name>\n   ```\n\n#### For ARO Clusters:\n\n1. **Get cluster credentials**:\n   ```bash\n   # Get kubeadmin credentials\n   CREDENTIALS=$(az aro list-credentials \\\n     --name <cluster-name> \\\n     --resource-group <resource-group> \\\n     --output json)\n\n   KUBEADMIN_USER=$(echo $CREDENTIALS | jq -r '.kubeadminUsername')\n   KUBEADMIN_PASS=$(echo $CREDENTIALS | jq -r '.kubeadminPassword')\n\n   # Get API server URL\n   API_SERVER=$(az aro show \\\n     --name <cluster-name> \\\n     --resource-group <resource-group> \\\n     --query 'apiserverProfile.url' \\\n     --output tsv)\n\n   # Get console URL\n   CONSOLE_URL=$(az aro show \\\n     --name <cluster-name> \\\n     --resource-group <resource-group> \\\n     --query 'consoleProfile.url' \\\n     --output tsv)\n   ```\n\n2. **Login with oc CLI**:\n   ```bash\n   oc login $API_SERVER \\\n     --username=$KUBEADMIN_USER \\\n     --password=$KUBEADMIN_PASS \\\n     --insecure-skip-tls-verify\n   ```\n\n3. **Verify connection**:\n   ```bash\n   oc cluster-info\n   oc get nodes\n   oc whoami\n   ```\n\n4. **Display connection info**:\n   ```\n   ‚úÖ Connected to ARO cluster: <cluster-name>\n\n   Context: <cluster-name>/<namespace>/kubeadmin\n   API Server: <api-server>\n   OpenShift Version: <version>\n   Console: <console-url>\n   Username: kubeadmin\n\n   ‚ö†Ô∏è  IMPORTANT: The kubeadmin user has cluster-admin privileges.\n   For production use, create a dedicated user with appropriate RBAC.\n\n   Current context set to: <context-name>\n   ```\n\n### Phase 3: Initialize Cluster Code\n\n1. **Automatically run cluster-code init**:\n   ```bash\n   cluster-code init --context <cluster-context>\n   ```\n\n2. **Update cluster configuration**:\n   - Set cluster type (AKS/ARO)\n   - Enable appropriate plugins\n   - Configure cloud provider settings\n\n### Phase 4: Run Initial Diagnostics\n\n1. **Quick health check**:\n   ```bash\n   cluster-code status --quick\n   ```\n\n2. **Show cluster summary**:\n   ```\n   Cluster Health Summary:\n   - API Server: ‚úÖ Healthy\n   - Nodes: ‚úÖ All Ready (5/5)\n   - System Pods: ‚úÖ All Running\n   - Resource Usage: ‚úÖ Normal (CPU: 25%, Memory: 40%)\n\n   Run full diagnostics: cluster-code diagnose\n   ```\n\n### Phase 5: Post-Connection Setup\n\n1. **For ARO clusters**, recommend creating a dedicated user:\n   ```\n   üìã ARO Post-Connection Steps:\n\n   1. Create an Azure AD user or group for cluster access:\n      https://learn.microsoft.com/en-us/azure/openshift/configure-azure-ad-cli\n\n   2. Grant cluster-admin role:\n      oc adm policy add-cluster-role-to-user cluster-admin <user@domain.com>\n\n   3. Create project and grant permissions:\n      oc new-project my-app\n      oc policy add-role-to-user admin <user@domain.com> -n my-app\n   ```\n\n2. **For AKS clusters**, recommend Azure AD integration:\n   ```\n   üìã AKS Post-Connection Steps:\n\n   1. Enable Azure AD integration (if not already enabled):\n      az aks update --resource-group <rg> --name <name> --enable-aad\n\n   2. Configure RBAC with Azure AD:\n      kubectl create clusterrolebinding <binding-name> \\\n        --clusterrole=cluster-admin \\\n        --user=<azure-ad-user>\n\n   3. Use AAD credentials for future connections:\n      az aks get-credentials --resource-group <rg> --name <name> --overwrite-existing\n   ```\n\n## Error Handling\n\n### Common Errors & Solutions:\n\n1. **Cluster not found**:\n   ```\n   ‚ùå Cluster '<name>' not found in resource group '<resource-group>'\n\n   List available clusters:\n     cluster-code azure-cluster-list\n   ```\n\n2. **Cluster not ready**:\n   ```\n   ‚ö†Ô∏è  Cluster is in '<provisioning-state>' state\n\n   Wait for cluster to be ready, then try again:\n     az <aks|aro> show --name <name> --resource-group <rg> --query provisioningState\n   ```\n\n3. **AKS cluster stopped**:\n   ```\n   ‚ö†Ô∏è  Cluster is stopped\n\n   Start the cluster:\n     az aks start --name <name> --resource-group <resource-group>\n\n   This may take 5-10 minutes.\n   ```\n\n4. **Insufficient permissions**:\n   ```\n   ‚ùå Insufficient permissions to get cluster credentials\n\n   Required RBAC roles:\n   - AKS: \"Azure Kubernetes Service Cluster User\" or \"Contributor\"\n   - ARO: \"Contributor\" or \"Owner\"\n\n   Contact your Azure administrator to request access.\n   ```\n\n5. **kubectl/oc not installed**:\n   ```\n   ‚ö†Ô∏è  kubectl/oc CLI not found\n\n   Install:\n   - kubectl: https://kubernetes.io/docs/tasks/tools/\n   - oc: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html\n   ```\n\n## Best Practices\n\n1. **Use admin credentials sparingly**:\n   - Only use `--admin` for AKS when necessary\n   - Prefer Azure AD integration for production\n   - Use RBAC for least-privilege access\n\n2. **Multiple clusters**:\n   - Use descriptive context names\n   - Switch contexts with: `kubectl config use-context <name>`\n   - View all contexts: `kubectl config get-contexts`\n\n3. **Security**:\n   - Rotate kubeadmin password (ARO): `az aro update --name <name> --resource-group <rg> --client-id <id> --client-secret <secret>`\n   - Use service principals or managed identities for automation\n   - Enable audit logging\n\n4. **ARO-specific**:\n   - Never disable the kubeadmin user until you have alternative admin access\n   - Use Azure AD or OpenShift OAuth for user management\n   - Monitor OpenShift console at the console URL\n\n## Output Format\n\nProvide clear, structured output with:\n- ‚úÖ Success confirmation\n- üìä Cluster details and status\n- ‚ö†Ô∏è  Important security warnings\n- üìã Next steps and recommendations\n- üîó Relevant URLs (console, portal)\n\n## Context Management\n\nAfter connection, show:\n\n```\nüìç Current Kubernetes Context:\n\nName: <cluster-name>\nCluster: <api-server>\nUser: <user>\nNamespace: <default-namespace>\n\nSwitch contexts:\n  kubectl config use-context <other-context>\n\nView all contexts:\n  kubectl config get-contexts\n```\n\n## References\n\n- **AKS**: https://learn.microsoft.com/en-us/azure/aks/\n- **ARO**: https://learn.microsoft.com/en-us/azure/openshift/\n- **ARO CLI**: https://learn.microsoft.com/en-us/azure/openshift/tutorial-connect-cluster\n- **kubectl**: https://kubernetes.io/docs/reference/kubectl/\n- **oc**: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/"
              },
              {
                "name": "/azure-cluster-create",
                "description": "Create a new AKS or ARO cluster on Azure",
                "path": "plugins/cloud-azure/commands/azure-cluster-create.md",
                "frontmatter": {
                  "name": "azure-cluster-create",
                  "description": "Create a new AKS or ARO cluster on Azure",
                  "category": "cloud-provisioning",
                  "tools": [
                    "Bash(az:*)",
                    "Bash(kubectl:*)",
                    "Bash(oc:*)",
                    "Write(.*\\.yaml)",
                    "Write(.*\\.json)",
                    "Write(.*\\.tf)"
                  ],
                  "parameters": [
                    {
                      "name": "type",
                      "description": "Cluster type (aks or aro)",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "name",
                      "description": "Cluster name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "resource-group",
                      "description": "Azure resource group",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "region",
                      "description": "Azure region",
                      "required": false,
                      "type": "string",
                      "default": "eastus"
                    },
                    {
                      "name": "version",
                      "description": "Kubernetes/OpenShift version",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "nodes",
                      "description": "Number of worker nodes",
                      "required": false,
                      "type": "integer",
                      "default": 3
                    },
                    {
                      "name": "vm-size",
                      "description": "VM size for nodes",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "output",
                      "description": "Output format (terraform, json, yaml)",
                      "required": false,
                      "type": "string",
                      "default": "json"
                    }
                  ],
                  "examples": [
                    "azure-cluster-create --type aks --name my-aks-cluster --resource-group my-rg --region eastus",
                    "azure-cluster-create --type aro --name my-aro-cluster --resource-group my-rg --region eastus2 --nodes 5",
                    "azure-cluster-create --type aks --name prod-cluster --resource-group production --output terraform"
                  ]
                },
                "content": "# Azure Cluster Creation\n\nYou are a specialized agent for creating Kubernetes (AKS) and OpenShift (ARO) clusters on Microsoft Azure.\n\n## Your Role\n\nGuide users through creating production-ready clusters with best practices for:\n- Network configuration and security\n- Resource sizing and node pool configuration\n- High availability and disaster recovery\n- Cost optimization\n- Compliance and governance\n\n## Task Workflow\n\n### Phase 1: Validation & Prerequisites\n\n1. **Check Azure CLI authentication**:\n   ```bash\n   az account show\n   ```\n   - Verify the correct subscription is selected\n   - If not authenticated, instruct user to run: `az login`\n\n2. **Validate parameters**:\n   - Cluster type must be 'aks' or 'aro'\n   - Cluster name must be valid (3-63 chars, alphanumeric and hyphens)\n   - Resource group must exist or create it\n   - Region must support the cluster type\n\n3. **Check available versions**:\n   - For AKS: `az aks get-versions --location <region> --output table`\n   - For ARO: `az aro get-versions --location <region> --output table`\n   - If no version specified, use the latest stable version\n\n### Phase 2: Resource Group Setup\n\n1. **Check if resource group exists**:\n   ```bash\n   az group show --name <resource-group>\n   ```\n\n2. **Create resource group if needed**:\n   ```bash\n   az group create --name <resource-group> --location <region>\n   ```\n\n### Phase 3: Cluster Creation\n\n#### For AKS Clusters:\n\n1. **Determine default values**:\n   - VM size: Standard_DS2_v2 (2 vCPUs, 7 GB RAM) for dev/test\n   - VM size: Standard_D4s_v3 (4 vCPUs, 16 GB RAM) for production\n   - Network plugin: Azure CNI for production, kubenet for dev/test\n   - Enable managed identity and RBAC by default\n\n2. **Build creation command**:\n   ```bash\n   az aks create \\\n     --resource-group <resource-group> \\\n     --name <cluster-name> \\\n     --location <region> \\\n     --kubernetes-version <version> \\\n     --node-count <nodes> \\\n     --node-vm-size <vm-size> \\\n     --network-plugin azure \\\n     --enable-managed-identity \\\n     --enable-rbac \\\n     --generate-ssh-keys \\\n     --tags \"ManagedBy=cluster-code\" \"CreatedAt=$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n   ```\n\n3. **Optional enhancements** (ask user):\n   - `--enable-cluster-autoscaler --min-count 3 --max-count 10`\n   - `--enable-azure-monitor`\n   - `--enable-defender`\n   - `--network-policy azure` (for network policies)\n   - `--load-balancer-sku standard`\n\n#### For ARO Clusters:\n\n1. **Prerequisites check**:\n   - ARO requires a virtual network with two subnets (master and worker)\n   - Service principal or managed identity\n   - Red Hat pull secret from https://cloud.redhat.com/openshift/install/azure/aro-provisioned\n\n2. **Ask user for pull secret**:\n   ```\n   ‚ö†Ô∏è  ARO clusters require a Red Hat pull secret.\n\n   Get your pull secret from: https://cloud.redhat.com/openshift/install/azure/aro-provisioned\n\n   Save it to a file (e.g., pull-secret.txt) and provide the path.\n   ```\n\n3. **Create virtual network** (if not exists):\n   ```bash\n   # Create VNet\n   az network vnet create \\\n     --resource-group <resource-group> \\\n     --name <cluster-name>-vnet \\\n     --address-prefixes 10.0.0.0/22\n\n   # Create master subnet\n   az network vnet subnet create \\\n     --resource-group <resource-group> \\\n     --vnet-name <cluster-name>-vnet \\\n     --name master-subnet \\\n     --address-prefixes 10.0.0.0/23 \\\n     --service-endpoints Microsoft.ContainerRegistry\n\n   # Create worker subnet\n   az network vnet subnet create \\\n     --resource-group <resource-group> \\\n     --vnet-name <cluster-name>-vnet \\\n     --name worker-subnet \\\n     --address-prefixes 10.0.2.0/23 \\\n     --service-endpoints Microsoft.ContainerRegistry\n   ```\n\n4. **Disable subnet private endpoint policies**:\n   ```bash\n   az network vnet subnet update \\\n     --resource-group <resource-group> \\\n     --vnet-name <cluster-name>-vnet \\\n     --name master-subnet \\\n     --disable-private-link-service-network-policies true\n   ```\n\n5. **Create ARO cluster**:\n   ```bash\n   az aro create \\\n     --resource-group <resource-group> \\\n     --name <cluster-name> \\\n     --location <region> \\\n     --vnet <cluster-name>-vnet \\\n     --master-subnet master-subnet \\\n     --worker-subnet worker-subnet \\\n     --pull-secret @pull-secret.txt \\\n     --worker-count <nodes> \\\n     --tags \"ManagedBy=cluster-code\" \"CreatedAt=$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n   ```\n\n6. **Optional ARO parameters**:\n   - `--worker-vm-size Standard_D4s_v3`\n   - `--master-vm-size Standard_D8s_v3`\n   - `--domain <custom-domain>` (for custom domain)\n\n### Phase 4: Monitor Creation Progress\n\n1. **Show creation status**:\n   ```\n   üöÄ Creating <type> cluster '<cluster-name>' in resource group '<resource-group>'...\n\n   This typically takes:\n   - AKS: 5-10 minutes\n   - ARO: 30-40 minutes\n\n   You can monitor progress with:\n   - AKS: az aks show --name <cluster-name> --resource-group <resource-group> --query provisioningState\n   - ARO: az aro show --name <cluster-name> --resource-group <resource-group> --query provisioningState\n   ```\n\n2. **Wait for completion** (or run in background):\n   - Poll every 30 seconds for status\n   - Show progress indicator\n   - Report any errors immediately\n\n### Phase 5: Post-Creation Configuration\n\n1. **Get cluster credentials**:\n   - For AKS:\n     ```bash\n     az aks get-credentials --resource-group <resource-group> --name <cluster-name>\n     ```\n   - For ARO:\n     ```bash\n     az aro list-credentials --name <cluster-name> --resource-group <resource-group>\n     az aro show --name <cluster-name> --resource-group <resource-group> --query \"consoleProfile.url\" -o tsv\n     ```\n\n2. **Verify cluster connectivity**:\n   ```bash\n   kubectl cluster-info\n   kubectl get nodes\n   ```\n\n3. **Display cluster information**:\n   ```\n   ‚úÖ Cluster created successfully!\n\n   Cluster Details:\n   - Name: <cluster-name>\n   - Type: <AKS/ARO>\n   - Resource Group: <resource-group>\n   - Region: <region>\n   - Kubernetes Version: <version>\n   - Node Count: <nodes>\n   - API Server: <api-endpoint>\n\n   Next Steps:\n   1. Initialize cluster-code: cluster-code init --context <cluster-name>\n   2. Run cluster diagnostics: cluster-code diagnose\n   3. Install required operators/tools\n   ```\n\n### Phase 6: Infrastructure as Code Output (if requested)\n\nIf user specified `--output terraform`, generate Terraform configuration:\n\n```hcl\n# terraform/main.tf\nterraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 3.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"cluster\" {\n  name     = \"<resource-group>\"\n  location = \"<region>\"\n\n  tags = {\n    ManagedBy = \"cluster-code\"\n    CreatedAt = timestamp()\n  }\n}\n\n# AKS or ARO resource based on type\n# [Generate appropriate resource blocks]\n```\n\n## Error Handling\n\n### Common Errors & Solutions:\n\n1. **QuotaExceeded**:\n   - Check subscription limits: `az vm list-usage --location <region> --output table`\n   - Request quota increase or choose different VM size\n\n2. **NetworkSecurityPerimeterConflict** (ARO):\n   - Ensure subnets have correct network policies disabled\n   - Verify service endpoints are configured\n\n3. **InvalidPullSecret** (ARO):\n   - Verify pull secret is valid JSON\n   - Re-download from Red Hat portal\n\n4. **AuthorizationFailed**:\n   - Check Azure RBAC permissions\n   - User needs \"Contributor\" role on subscription/resource group\n\n5. **RegionNotSupported**:\n   - List available regions: `az account list-locations --output table`\n   - ARO has limited region availability\n\n## Best Practices Recommendations\n\n1. **Production Clusters**:\n   - Use at least 3 nodes for HA\n   - Enable cluster autoscaler\n   - Enable Azure Monitor for monitoring\n   - Enable Azure Defender for security\n   - Use availability zones (where supported)\n   - Enable backup and disaster recovery\n\n2. **Network Security**:\n   - Use Azure CNI for better network control\n   - Enable network policies\n   - Use private clusters for sensitive workloads\n   - Configure ingress with WAF\n\n3. **Cost Optimization**:\n   - Use spot instances for non-critical workloads\n   - Enable cluster autoscaler to scale down\n   - Use appropriate VM sizes (don't over-provision)\n   - Schedule non-production cluster shutdowns\n\n4. **Compliance**:\n   - Tag all resources appropriately\n   - Enable audit logging\n   - Use Azure Policy for governance\n   - Implement RBAC properly\n\n## Output Format\n\nProvide clear, structured output with:\n- ‚úÖ Success indicators\n- ‚ö†Ô∏è  Warnings for important considerations\n- üöÄ Progress updates during long operations\n- üìã Summary of created resources\n- üîó Links to Azure Portal for verification\n- üìñ Next steps and recommendations\n\n## References\n\n- **AKS Documentation**: https://learn.microsoft.com/en-us/azure/aks/\n- **ARO Documentation**: https://learn.microsoft.com/en-us/azure/openshift/\n- **ARO Expert Guides**: https://cloud.redhat.com/experts/aro/\n- **Azure CLI Reference**: https://learn.microsoft.com/en-us/cli/azure/"
              },
              {
                "name": "/azure-cluster-delete",
                "description": "Delete AKS or ARO clusters on Azure with safety checks",
                "path": "plugins/cloud-azure/commands/azure-cluster-delete.md",
                "frontmatter": {
                  "name": "azure-cluster-delete",
                  "description": "Delete AKS or ARO clusters on Azure with safety checks",
                  "category": "cloud-lifecycle",
                  "tools": [
                    "Bash(az:*)",
                    "Bash(kubectl:*)"
                  ],
                  "parameters": [
                    {
                      "name": "name",
                      "description": "Cluster name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "resource-group",
                      "description": "Resource group name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "type",
                      "description": "Cluster type (aks or aro) - auto-detected if not specified",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "delete-resource-group",
                      "description": "Also delete the resource group",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "backup",
                      "description": "Backup resources before deletion",
                      "required": false,
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "yes",
                      "description": "Skip confirmation prompts",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "examples": [
                    "azure-cluster-delete --name my-aks --resource-group my-rg",
                    "azure-cluster-delete --name my-aro --resource-group my-rg --type aro",
                    "azure-cluster-delete --name dev-aks --resource-group dev-rg --yes --no-backup"
                  ]
                },
                "content": "# Azure Cluster Deletion\n\nSafely delete AKS or ARO clusters with comprehensive backup and safety checks.\n\n## Your Role\n\nYou are responsible for safely decommissioning Azure Kubernetes clusters while:\n- Preventing accidental data loss\n- Backing up critical resources\n- Cleaning up associated resources\n- Validating deletion safety\n- Providing rollback options\n\n## Task Workflow\n\n### Phase 1: Pre-Deletion Validation\n\n1. **Verify Azure authentication**:\n   ```bash\n   az account show || {\n     echo \"‚ùå Not authenticated to Azure\"\n     echo \"Run: az login\"\n     exit 1\n   }\n   ```\n\n2. **Auto-detect cluster type** (if not specified):\n   ```bash\n   if [[ -z \"$CLUSTER_TYPE\" ]]; then\n     # Try AKS\n     if az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP &>/dev/null; then\n       CLUSTER_TYPE=\"aks\"\n     # Try ARO\n     elif az aro show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP &>/dev/null; then\n       CLUSTER_TYPE=\"aro\"\n     else\n       echo \"‚ùå Cluster not found: $CLUSTER_NAME\"\n       exit 1\n     fi\n   fi\n   ```\n\n3. **Get cluster information**:\n   ```bash\n   echo \"üìã Cluster Information:\"\n   echo \"\"\n\n   if [[ \"$CLUSTER_TYPE\" == \"aks\" ]]; then\n     CLUSTER_INFO=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP -o json)\n\n     LOCATION=$(echo $CLUSTER_INFO | jq -r '.location')\n     K8S_VERSION=$(echo $CLUSTER_INFO | jq -r '.kubernetesVersion')\n     NODE_COUNT=$(echo $CLUSTER_INFO | jq -r '.agentPoolProfiles[].count' | awk '{sum+=$1} END {print sum}')\n     PROVISIONING_STATE=$(echo $CLUSTER_INFO | jq -r '.provisioningState')\n\n     echo \"  Type: AKS\"\n     echo \"  Name: $CLUSTER_NAME\"\n     echo \"  Resource Group: $RESOURCE_GROUP\"\n     echo \"  Location: $LOCATION\"\n     echo \"  Kubernetes Version: $K8S_VERSION\"\n     echo \"  Total Nodes: $NODE_COUNT\"\n     echo \"  State: $PROVISIONING_STATE\"\n   else\n     CLUSTER_INFO=$(az aro show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP -o json)\n\n     LOCATION=$(echo $CLUSTER_INFO | jq -r '.location')\n     OPENSHIFT_VERSION=$(echo $CLUSTER_INFO | jq -r '.clusterProfile.version')\n     PROVISIONING_STATE=$(echo $CLUSTER_INFO | jq -r '.provisioningState')\n     CONSOLE_URL=$(echo $CLUSTER_INFO | jq -r '.consoleProfile.url')\n\n     echo \"  Type: ARO\"\n     echo \"  Name: $CLUSTER_NAME\"\n     echo \"  Resource Group: $RESOURCE_GROUP\"\n     echo \"  Location: $LOCATION\"\n     echo \"  OpenShift Version: $OPENSHIFT_VERSION\"\n     echo \"  State: $PROVISIONING_STATE\"\n     echo \"  Console: $CONSOLE_URL\"\n   fi\n   echo \"\"\n   ```\n\n### Phase 2: Safety Checks\n\n1. **Check for production indicators**:\n   ```bash\n   echo \"üîç Running safety checks...\"\n   echo \"\"\n\n   WARNINGS=0\n\n   # Check cluster tags\n   TAGS=$(echo $CLUSTER_INFO | jq -r '.tags // {}')\n   ENV_TAG=$(echo $TAGS | jq -r '.Environment // .environment // \"\"')\n\n   if [[ \"$ENV_TAG\" =~ ^(prod|production|prd)$ ]]; then\n     echo \"‚ö†Ô∏è  WARNING: Cluster is tagged as PRODUCTION\"\n     WARNINGS=$((WARNINGS + 1))\n   fi\n\n   # Check node count\n   if [[ \"$CLUSTER_TYPE\" == \"aks\" && $NODE_COUNT -gt 5 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: Large cluster ($NODE_COUNT nodes)\"\n     WARNINGS=$((WARNINGS + 1))\n   fi\n\n   # Check for persistent volumes\n   PV_COUNT=$(kubectl get pv --no-headers 2>/dev/null | wc -l)\n   if [[ $PV_COUNT -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: $PV_COUNT Persistent Volumes found (data will be lost)\"\n     WARNINGS=$((WARNINGS + 1))\n   fi\n\n   # Check for load balancers\n   LB_COUNT=$(kubectl get svc --all-namespaces --no-headers 2>/dev/null | grep LoadBalancer | wc -l)\n   if [[ $LB_COUNT -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  INFO: $LB_COUNT Load Balancer services (will be deleted)\"\n   fi\n\n   echo \"\"\n   ```\n\n2. **List associated resources**:\n   ```bash\n   echo \"üîó Associated Azure Resources:\"\n   echo \"\"\n\n   # Get resources in the cluster's resource group\n   RESOURCES=$(az resource list --resource-group $RESOURCE_GROUP -o json)\n   RESOURCE_COUNT=$(echo $RESOURCES | jq 'length')\n\n   echo \"  Total resources in resource group: $RESOURCE_COUNT\"\n   echo \"\"\n   echo \"  Resource types:\"\n   echo $RESOURCES | jq -r '.[].type' | sort | uniq -c | awk '{printf \"    - %s: %d\\n\", $2, $1}'\n   echo \"\"\n\n   # Check for managed resource group (AKS creates additional RG)\n   if [[ \"$CLUSTER_TYPE\" == \"aks\" ]]; then\n     NODE_RG=$(echo $CLUSTER_INFO | jq -r '.nodeResourceGroup')\n     if [[ -n \"$NODE_RG\" && \"$NODE_RG\" != \"null\" ]]; then\n       echo \"  Managed Resource Group: $NODE_RG\"\n       NODE_RG_RESOURCES=$(az resource list --resource-group $NODE_RG -o json | jq 'length')\n       echo \"  Resources in managed RG: $NODE_RG_RESOURCES\"\n       echo \"\"\n     fi\n   fi\n   ```\n\n### Phase 3: Backup Resources\n\n1. **Create backup** (if --backup enabled):\n   ```bash\n   if [[ \"$BACKUP\" == \"true\" ]]; then\n     echo \"üíæ Backing up cluster resources...\"\n     echo \"\"\n\n     BACKUP_DIR=\"./cluster-backup-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n     mkdir -p \"$BACKUP_DIR\"\n\n     # Backup all namespaces\n     kubectl get namespaces -o yaml > \"$BACKUP_DIR/namespaces.yaml\"\n\n     # Backup all resources in each namespace\n     kubectl get namespaces --no-headers | awk '{print $1}' | while read NS; do\n       echo \"  üìÅ Backing up namespace: $NS\"\n       mkdir -p \"$BACKUP_DIR/$NS\"\n\n       # Skip system namespaces for full backup\n       if [[ ! \"$NS\" =~ ^(kube-|openshift-|default$) ]]; then\n         kubectl get all,cm,secret,pvc,ing -n $NS -o yaml > \"$BACKUP_DIR/$NS/all-resources.yaml\" 2>/dev/null\n       fi\n     done\n\n     # Backup cluster-scoped resources\n     echo \"  üåç Backing up cluster-scoped resources\"\n     kubectl get clusterrole,clusterrolebinding,sc,pv -o yaml > \"$BACKUP_DIR/cluster-resources.yaml\" 2>/dev/null\n\n     # Save cluster info\n     echo $CLUSTER_INFO | jq '.' > \"$BACKUP_DIR/cluster-info.json\"\n\n     # Create backup archive\n     tar -czf \"$BACKUP_DIR.tar.gz\" -C \"$(dirname $BACKUP_DIR)\" \"$(basename $BACKUP_DIR)\"\n     rm -rf \"$BACKUP_DIR\"\n\n     echo \"\"\n     echo \"‚úÖ Backup saved: $BACKUP_DIR.tar.gz\"\n     echo \"\"\n   fi\n   ```\n\n2. **Export important data**:\n   ```bash\n   # Export Helm releases\n   if command -v helm &>/dev/null; then\n     echo \"  üì¶ Exporting Helm releases\"\n     helm list --all-namespaces -o json > \"$BACKUP_DIR/helm-releases.json\" 2>/dev/null\n   fi\n\n   # Export ArgoCD applications (if installed)\n   if kubectl get namespace argocd &>/dev/null; then\n     echo \"  üìö Exporting ArgoCD applications\"\n     kubectl get applications -n argocd -o yaml > \"$BACKUP_DIR/argocd-apps.yaml\" 2>/dev/null\n   fi\n   ```\n\n### Phase 4: Confirmation\n\n1. **Show deletion summary**:\n   ```bash\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"‚ö†Ô∏è  DELETION SUMMARY\"\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"\"\n   echo \"Cluster to delete:\"\n   echo \"  Name: $CLUSTER_NAME\"\n   echo \"  Type: $CLUSTER_TYPE\"\n   echo \"  Resource Group: $RESOURCE_GROUP\"\n   echo \"\"\n   echo \"What will be deleted:\"\n   echo \"  ‚úó Cluster control plane\"\n   echo \"  ‚úó All worker nodes ($NODE_COUNT nodes)\"\n   echo \"  ‚úó All pods and containers\"\n   echo \"  ‚úó All persistent volumes ($PV_COUNT PVs)\"\n   echo \"  ‚úó All load balancers ($LB_COUNT LBs)\"\n   if [[ \"$CLUSTER_TYPE\" == \"aks\" ]]; then\n     echo \"  ‚úó Managed resource group: $NODE_RG\"\n   fi\n\n   if [[ \"$DELETE_RESOURCE_GROUP\" == \"true\" ]]; then\n     echo \"  ‚úó Resource group: $RESOURCE_GROUP ($RESOURCE_COUNT resources)\"\n   fi\n   echo \"\"\n\n   if [[ \"$BACKUP\" == \"true\" ]]; then\n     echo \"‚úÖ Backup created: $BACKUP_DIR.tar.gz\"\n   else\n     echo \"‚ö†Ô∏è  No backup created (use --backup to create backup)\"\n   fi\n   echo \"\"\n\n   if [[ $WARNINGS -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  $WARNINGS warning(s) detected - review carefully before proceeding\"\n     echo \"\"\n   fi\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   ```\n\n2. **Request confirmation** (unless --yes):\n   ```bash\n   if [[ \"$YES\" != \"true\" ]]; then\n     echo \"\"\n     echo \"‚ö†Ô∏è  THIS ACTION CANNOT BE UNDONE!\"\n     echo \"\"\n     read -p \"Type the cluster name to confirm deletion: \" CONFIRM_NAME\n\n     if [[ \"$CONFIRM_NAME\" != \"$CLUSTER_NAME\" ]]; then\n       echo \"\"\n       echo \"‚ùå Cluster name does not match. Deletion cancelled.\"\n       exit 0\n     fi\n\n     echo \"\"\n     read -p \"Are you absolutely sure you want to delete this cluster? [yes/NO]: \" FINAL_CONFIRM\n\n     if [[ ! \"$FINAL_CONFIRM\" =~ ^[Yy][Ee][Ss]$ ]]; then\n       echo \"\"\n       echo \"‚ùå Deletion cancelled\"\n       exit 0\n     fi\n   fi\n   ```\n\n### Phase 5: Execute Deletion\n\n1. **Delete cluster**:\n   ```bash\n   echo \"\"\n   echo \"üóëÔ∏è  Deleting cluster...\"\n   echo \"\"\n\n   START_TIME=$(date +%s)\n\n   if [[ \"$CLUSTER_TYPE\" == \"aks\" ]]; then\n     # Delete AKS cluster\n     az aks delete \\\n       --name $CLUSTER_NAME \\\n       --resource-group $RESOURCE_GROUP \\\n       --yes \\\n       --no-wait\n\n     echo \"  Deletion initiated for AKS cluster\"\n     echo \"  This typically takes 5-10 minutes\"\n   else\n     # Delete ARO cluster\n     az aro delete \\\n       --name $CLUSTER_NAME \\\n       --resource-group $RESOURCE_GROUP \\\n       --yes \\\n       --no-wait\n\n     echo \"  Deletion initiated for ARO cluster\"\n     echo \"  This typically takes 15-20 minutes\"\n   fi\n   echo \"\"\n   ```\n\n2. **Monitor deletion progress**:\n   ```bash\n   echo \"‚è≥ Monitoring deletion progress...\"\n   echo \"\"\n\n   while true; do\n     if [[ \"$CLUSTER_TYPE\" == \"aks\" ]]; then\n       STATUS=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP \\\n         --query provisioningState -o tsv 2>/dev/null)\n     else\n       STATUS=$(az aro show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP \\\n         --query provisioningState -o tsv 2>/dev/null)\n     fi\n\n     if [[ $? -ne 0 ]]; then\n       echo \"‚úÖ Cluster deleted successfully!\"\n       break\n     fi\n\n     ELAPSED=$(($(date +%s) - START_TIME))\n     ELAPSED_MIN=$((ELAPSED / 60))\n     echo \"  Status: $STATUS (elapsed: ${ELAPSED_MIN}m)\"\n     sleep 30\n   done\n\n   echo \"\"\n   ```\n\n### Phase 6: Cleanup Associated Resources\n\n1. **Delete resource group** (if --delete-resource-group):\n   ```bash\n   if [[ \"$DELETE_RESOURCE_GROUP\" == \"true\" ]]; then\n     echo \"üóëÔ∏è  Deleting resource group: $RESOURCE_GROUP\"\n     echo \"\"\n\n     az group delete \\\n       --name $RESOURCE_GROUP \\\n       --yes \\\n       --no-wait\n\n     echo \"  Resource group deletion initiated\"\n     echo \"  This may take additional time\"\n     echo \"\"\n   fi\n   ```\n\n2. **Clean up kubeconfig**:\n   ```bash\n   echo \"üßπ Cleaning up local configuration...\"\n   echo \"\"\n\n   # Remove context from kubeconfig\n   kubectl config delete-context $CLUSTER_NAME 2>/dev/null && \\\n     echo \"  ‚úÖ Removed kubectl context\"\n\n   kubectl config delete-cluster $CLUSTER_NAME 2>/dev/null && \\\n     echo \"  ‚úÖ Removed cluster from kubeconfig\"\n\n   kubectl config unset users.$CLUSTER_NAME 2>/dev/null && \\\n     echo \"  ‚úÖ Removed user credentials\"\n\n   echo \"\"\n   ```\n\n### Phase 7: Post-Deletion Report\n\n```bash\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\necho \"‚úÖ DELETION COMPLETE\"\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\necho \"\"\necho \"Deleted:\"\necho \"  ‚úì Cluster: $CLUSTER_NAME\"\necho \"  ‚úì Type: $CLUSTER_TYPE\"\necho \"  ‚úì Resource Group: $RESOURCE_GROUP\"\nif [[ \"$DELETE_RESOURCE_GROUP\" == \"true\" ]]; then\n  echo \"  ‚úì Resource Group deleted\"\nfi\necho \"\"\n\nif [[ \"$BACKUP\" == \"true\" ]]; then\n  echo \"Backup Location:\"\n  echo \"  üìÅ $BACKUP_DIR.tar.gz\"\n  echo \"\"\n  echo \"To restore from backup:\"\n  echo \"  tar -xzf $BACKUP_DIR.tar.gz\"\n  echo \"  kubectl apply -f $BACKUP_DIR/\"\n  echo \"\"\nfi\n\nTOTAL_TIME=$(($(date +%s) - START_TIME))\nTOTAL_MIN=$((TOTAL_TIME / 60))\necho \"Total Time: ${TOTAL_MIN}m\"\necho \"\"\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n```\n\n## Error Handling\n\n1. **Cluster not found**:\n   ```\n   ‚ùå Cluster not found\n\n   Verify cluster name and resource group:\n     az aks list --resource-group <rg>\n     az aro list --resource-group <rg>\n   ```\n\n2. **Deletion failed**:\n   ```\n   ‚ùå Cluster deletion failed\n\n   Check Azure portal for errors\n   View activity log:\n     az monitor activity-log list --resource-group <rg>\n\n   Retry deletion:\n     az aks delete --name <name> --resource-group <rg> --yes\n   ```\n\n3. **Resource group locked**:\n   ```\n   ‚ùå Resource group is locked\n\n   Check locks:\n     az lock list --resource-group <rg>\n\n   Remove lock:\n     az lock delete --name <lock-name> --resource-group <rg>\n   ```\n\n## Best Practices\n\n1. **Always backup production clusters**\n2. **Test deletion process in non-production first**\n3. **Export configuration for recreation**\n4. **Document deletion reason and date**\n5. **Verify backup integrity before deletion**\n6. **Keep backups for compliance period**\n7. **Remove DNS records and external dependencies**\n\n## References\n\n- **AKS**: https://learn.microsoft.com/en-us/azure/aks/\n- **ARO**: https://learn.microsoft.com/en-us/azure/openshift/"
              },
              {
                "name": "/azure-cluster-list",
                "description": "List all AKS and ARO clusters in Azure subscription",
                "path": "plugins/cloud-azure/commands/azure-cluster-list.md",
                "frontmatter": {
                  "name": "azure-cluster-list",
                  "description": "List all AKS and ARO clusters in Azure subscription",
                  "category": "cloud-management",
                  "tools": [
                    "Bash(az:*)"
                  ],
                  "parameters": [
                    {
                      "name": "resource-group",
                      "description": "Filter by resource group",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "type",
                      "description": "Filter by cluster type (aks, aro, or all)",
                      "required": false,
                      "type": "string",
                      "default": "all"
                    },
                    {
                      "name": "format",
                      "description": "Output format (table, json, yaml)",
                      "required": false,
                      "type": "string",
                      "default": "table"
                    }
                  ],
                  "examples": [
                    "azure-cluster-list",
                    "azure-cluster-list --resource-group my-rg",
                    "azure-cluster-list --type aks --format json"
                  ]
                },
                "content": "# Azure Cluster Listing\n\nList all AKS and ARO clusters in your Azure subscription with detailed information.\n\n## Task Workflow\n\n### Phase 1: Authentication Check\n\n1. **Verify Azure CLI authentication**:\n   ```bash\n   az account show --output json\n   ```\n   - Extract and display subscription ID and name\n   - If not authenticated, instruct user to run `az login`\n\n### Phase 2: List Clusters\n\n#### List AKS Clusters\n\nIf `--type` is \"aks\" or \"all\":\n\n```bash\naz aks list \\\n  ${RESOURCE_GROUP:+--resource-group $RESOURCE_GROUP} \\\n  --output json\n```\n\nExtract for each AKS cluster:\n- Name\n- Resource Group\n- Location\n- Kubernetes Version\n- Node Count\n- Provisioning State\n- FQDN\n- Power State\n\n#### List ARO Clusters\n\nIf `--type` is \"aro\" or \"all\":\n\n```bash\naz aro list \\\n  ${RESOURCE_GROUP:+--resource-group $RESOURCE_GROUP} \\\n  --output json\n```\n\nExtract for each ARO cluster:\n- Name\n- Resource Group\n- Location\n- OpenShift Version\n- Provisioning State\n- Console URL\n- API Server URL\n\n### Phase 3: Format and Display\n\n#### Table Format (Default)\n\n```\nAzure Clusters in Subscription: <subscription-name> (<subscription-id>)\n\nAKS CLUSTERS:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Name            ‚îÇ Resource Group  ‚îÇ Location ‚îÇ Version ‚îÇ Nodes ‚îÇ State    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ prod-aks-001    ‚îÇ production-rg   ‚îÇ eastus   ‚îÇ 1.28.9  ‚îÇ 5     ‚îÇ Succeeded‚îÇ\n‚îÇ dev-aks-001     ‚îÇ development-rg  ‚îÇ westus2  ‚îÇ 1.28.5  ‚îÇ 3     ‚îÇ Succeeded‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nARO CLUSTERS:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Name            ‚îÇ Resource Group  ‚îÇ Location ‚îÇ Version ‚îÇ State    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ prod-aro-001    ‚îÇ production-rg   ‚îÇ eastus2  ‚îÇ 4.15.0  ‚îÇ Succeeded‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nTotal: 3 clusters (2 AKS, 1 ARO)\n\nQuick Connect:\n  cluster-code connect azure --name prod-aks-001 --resource-group production-rg\n  cluster-code connect azure --name prod-aro-001 --resource-group production-rg\n```\n\n#### JSON Format\n\nIf `--format json`, output structured JSON:\n\n```json\n{\n  \"subscription\": {\n    \"id\": \"...\",\n    \"name\": \"...\"\n  },\n  \"clusters\": {\n    \"aks\": [\n      {\n        \"name\": \"prod-aks-001\",\n        \"resourceGroup\": \"production-rg\",\n        \"location\": \"eastus\",\n        \"kubernetesVersion\": \"1.28.9\",\n        \"nodeCount\": 5,\n        \"provisioningState\": \"Succeeded\",\n        \"fqdn\": \"prod-aks-001-dns-12345678.hcp.eastus.azmk8s.io\",\n        \"powerState\": \"Running\"\n      }\n    ],\n    \"aro\": [\n      {\n        \"name\": \"prod-aro-001\",\n        \"resourceGroup\": \"production-rg\",\n        \"location\": \"eastus2\",\n        \"openshiftVersion\": \"4.15.0\",\n        \"provisioningState\": \"Succeeded\",\n        \"consoleUrl\": \"https://console-openshift-console.apps.prod-aro-001.example.com\",\n        \"apiServerUrl\": \"https://api.prod-aro-001.example.com:6443\"\n      }\n    ]\n  },\n  \"summary\": {\n    \"total\": 3,\n    \"aks\": 2,\n    \"aro\": 1,\n    \"running\": 3,\n    \"stopped\": 0\n  }\n}\n```\n\n### Phase 4: Additional Information\n\nFor each cluster, optionally show:\n\n1. **Cost Estimate** (if requested):\n   ```bash\n   az consumption usage list \\\n     --start-date $(date -d '30 days ago' +%Y-%m-%d) \\\n     --end-date $(date +%Y-%m-%d) \\\n     | grep <cluster-name>\n   ```\n\n2. **Node Pool Details** (AKS):\n   ```bash\n   az aks nodepool list \\\n     --resource-group <resource-group> \\\n     --cluster-name <cluster-name>\n   ```\n\n3. **Add-ons Status** (AKS):\n   ```bash\n   az aks addon list \\\n     --resource-group <resource-group> \\\n     --name <cluster-name>\n   ```\n\n## Error Handling\n\n1. **No clusters found**:\n   ```\n   ‚ÑπÔ∏è  No clusters found in subscription.\n\n   Create a new cluster:\n     cluster-code azure-cluster-create --type aks --name my-cluster --resource-group my-rg\n   ```\n\n2. **Authentication error**:\n   ```\n   ‚ö†Ô∏è  Not authenticated to Azure.\n\n   Run: az login\n   ```\n\n3. **Insufficient permissions**:\n   ```\n   ‚ö†Ô∏è  Insufficient permissions to list clusters.\n\n   Required role: Reader or higher on subscription/resource group\n   ```\n\n## Output Guidelines\n\n- Use colored output for better readability (green for running, yellow for provisioning, red for failed)\n- Show cluster age (created timestamp)\n- Highlight clusters with issues or warnings\n- Provide quick connect commands\n- Show estimated monthly cost if available\n\n## References\n\n- **AKS**: https://learn.microsoft.com/en-us/azure/aks/\n- **ARO**: https://learn.microsoft.com/en-us/azure/openshift/"
              },
              {
                "name": "/azure-cluster-upgrade",
                "description": "Upgrade AKS cluster to a new Kubernetes version",
                "path": "plugins/cloud-azure/commands/azure-cluster-upgrade.md",
                "frontmatter": {
                  "name": "azure-cluster-upgrade",
                  "description": "Upgrade AKS cluster to a new Kubernetes version",
                  "category": "cloud-lifecycle",
                  "tools": [
                    "Bash(az:*)",
                    "Bash(kubectl:*)"
                  ],
                  "parameters": [
                    {
                      "name": "name",
                      "description": "Cluster name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "resource-group",
                      "description": "Resource group name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "version",
                      "description": "Target Kubernetes version",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "control-plane-only",
                      "description": "Upgrade control plane only (not node pools)",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "node-image-only",
                      "description": "Upgrade node images only",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "examples": [
                    "azure-cluster-upgrade --name my-aks --resource-group my-rg --version 1.29.0",
                    "azure-cluster-upgrade --name my-aks --resource-group my-rg --version 1.29.0 --control-plane-only",
                    "azure-cluster-upgrade --name my-aks --resource-group my-rg --node-image-only"
                  ]
                },
                "content": "# Azure AKS Cluster Upgrade\n\nSafely upgrade AKS clusters to new Kubernetes versions with comprehensive validation and rollback capabilities.\n\n## Your Role\n\nGuide users through safe cluster upgrades with:\n- Pre-upgrade validation\n- Compatibility checking\n- Staged upgrade approach\n- Health monitoring\n- Rollback procedures\n\n## Task Workflow\n\n### Phase 1: Pre-Upgrade Validation\n\n1. **Get current cluster information**:\n   ```bash\n   CLUSTER_INFO=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP -o json)\n\n   CURRENT_VERSION=$(echo $CLUSTER_INFO | jq -r '.kubernetesVersion')\n   LOCATION=$(echo $CLUSTER_INFO | jq -r '.location')\n   echo \"Current Kubernetes Version: $CURRENT_VERSION\"\n   echo \"Target Version: $TARGET_VERSION\"\n   echo \"Location: $LOCATION\"\n   echo \"\"\n   ```\n\n2. **Check available versions**:\n   ```bash\n   echo \"Checking available versions...\"\n   AVAILABLE_VERSIONS=$(az aks get-versions --location $LOCATION -o json)\n\n   # Check if target version is available\n   IS_AVAILABLE=$(echo $AVAILABLE_VERSIONS | jq -r --arg ver \"$TARGET_VERSION\" \\\n     '.orchestrators[] | select(.orchestratorVersion==$ver) | .orchestratorVersion')\n\n   if [[ -z \"$IS_AVAILABLE\" ]]; then\n     echo \"‚ùå Version $TARGET_VERSION is not available in $LOCATION\"\n     echo \"\"\n     echo \"Available versions:\"\n     echo $AVAILABLE_VERSIONS | jq -r '.orchestrators[].orchestratorVersion' | sort -V\n     exit 1\n   fi\n\n   echo \"‚úÖ Version $TARGET_VERSION is available\"\n   echo \"\"\n   ```\n\n3. **Validate upgrade path**:\n   ```bash\n   # Check if version upgrade is supported\n   CURRENT_MAJOR=$(echo $CURRENT_VERSION | cut -d. -f1)\n   CURRENT_MINOR=$(echo $CURRENT_VERSION | cut -d. -f2)\n   TARGET_MAJOR=$(echo $TARGET_VERSION | cut -d. -f1)\n   TARGET_MINOR=$(echo $TARGET_VERSION | cut -d. -f2)\n\n   MINOR_DIFF=$((TARGET_MINOR - CURRENT_MINOR))\n\n   if [[ $MINOR_DIFF -gt 1 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: Skipping minor versions is not supported\"\n     echo \"Current: $CURRENT_VERSION, Target: $TARGET_VERSION\"\n     echo \"You must upgrade one minor version at a time\"\n     echo \"\"\n     exit 1\n   fi\n\n   if [[ $MINOR_DIFF -lt 0 ]]; then\n     echo \"‚ùå Downgrade not supported\"\n     echo \"Current: $CURRENT_VERSION, Target: $TARGET_VERSION\"\n     exit 1\n   fi\n\n   echo \"‚úÖ Upgrade path validated\"\n   echo \"\"\n   ```\n\n4. **Pre-upgrade cluster health check**:\n   ```bash\n   echo \"Running pre-upgrade health checks...\"\n   echo \"\"\n\n   # Check node status\n   NODES_NOT_READY=$(kubectl get nodes --no-headers | grep -v \" Ready \" | wc -l)\n   if [[ $NODES_NOT_READY -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: $NODES_NOT_READY nodes are not Ready\"\n     kubectl get nodes\n     echo \"\"\n   fi\n\n   # Check pod health\n   PODS_NOT_RUNNING=$(kubectl get pods --all-namespaces --no-headers | \\\n     grep -v \"Running\\|Completed\" | wc -l)\n   if [[ $PODS_NOT_RUNNING -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: $PODS_NOT_RUNNING pods are not in Running state\"\n     echo \"\"\n   fi\n\n   # Check for pod disruption budgets\n   PDB_COUNT=$(kubectl get pdb --all-namespaces --no-headers 2>/dev/null | wc -l)\n   if [[ $PDB_COUNT -gt 0 ]]; then\n     echo \"‚ÑπÔ∏è  Found $PDB_COUNT Pod Disruption Budgets (will be respected during upgrade)\"\n     echo \"\"\n   fi\n\n   echo \"‚úÖ Pre-upgrade checks complete\"\n   echo \"\"\n   ```\n\n### Phase 2: Backup and Preparation\n\n1. **Backup cluster configuration**:\n   ```bash\n   echo \"üíæ Backing up cluster configuration...\"\n   BACKUP_DIR=\"./cluster-upgrade-backup-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n   mkdir -p \"$BACKUP_DIR\"\n\n   # Save cluster info\n   echo $CLUSTER_INFO > \"$BACKUP_DIR/cluster-info.json\"\n\n   # Backup critical resources\n   kubectl get all,cm,secret,pvc,ing --all-namespaces -o yaml > \"$BACKUP_DIR/all-resources.yaml\"\n\n   # Backup cluster-scoped resources\n   kubectl get clusterrole,clusterrolebinding,sc,pv -o yaml > \"$BACKUP_DIR/cluster-resources.yaml\"\n\n   echo \"‚úÖ Backup saved to $BACKUP_DIR\"\n   echo \"\"\n   ```\n\n2. **Check for deprecated APIs**:\n   ```bash\n   echo \"Checking for deprecated APIs in target version...\"\n\n   # Common deprecations (this is simplified - use pluto or kubectl-convert for real checks)\n   if [[ \"$TARGET_MINOR\" -ge 25 ]]; then\n     echo \"  Checking for PodSecurityPolicy (removed in 1.25+)\"\n     PSP_COUNT=$(kubectl get psp --no-headers 2>/dev/null | wc -l)\n     if [[ $PSP_COUNT -gt 0 ]]; then\n       echo \"  ‚ö†Ô∏è  WARNING: $PSP_COUNT PodSecurityPolicies found (deprecated)\"\n       echo \"     Migrate to Pod Security Standards before upgrading\"\n     fi\n   fi\n\n   echo \"‚úÖ API compatibility check complete\"\n   echo \"\"\n   ```\n\n### Phase 3: Upgrade Execution\n\n1. **Show upgrade plan**:\n   ```bash\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"UPGRADE PLAN\"\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"\"\n   echo \"Cluster: $CLUSTER_NAME\"\n   echo \"Current Version: $CURRENT_VERSION\"\n   echo \"Target Version: $TARGET_VERSION\"\n   echo \"\"\n\n   if [[ \"$NODE_IMAGE_ONLY\" == \"true\" ]]; then\n     echo \"Upgrade Type: Node images only\"\n   elif [[ \"$CONTROL_PLANE_ONLY\" == \"true\" ]]; then\n     echo \"Upgrade Type: Control plane only\"\n   else\n     echo \"Upgrade Type: Full upgrade (control plane + all node pools)\"\n   fi\n   echo \"\"\n\n   # Get node pools\n   NODE_POOLS=$(az aks nodepool list --cluster-name $CLUSTER_NAME \\\n     --resource-group $RESOURCE_GROUP -o json)\n   NODE_POOL_COUNT=$(echo $NODE_POOLS | jq 'length')\n\n   echo \"Node Pools: $NODE_POOL_COUNT\"\n   echo $NODE_POOLS | jq -r '.[] | \"  - \\(.name): \\(.count) nodes (K8s \\(.orchestratorVersion))\"'\n   echo \"\"\n   echo \"Estimated Time:\"\n   if [[ \"$CONTROL_PLANE_ONLY\" == \"true\" ]]; then\n     echo \"  Control plane: 10-15 minutes\"\n   else\n     TOTAL_NODES=$(echo $NODE_POOLS | jq '[.[].count] | add')\n     UPGRADE_TIME=$((15 + TOTAL_NODES * 5))\n     echo \"  Total: ~${UPGRADE_TIME} minutes\"\n   fi\n   echo \"\"\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"\"\n\n   read -p \"Proceed with upgrade? [y/N]: \" CONFIRM\n   if [[ ! \"$CONFIRM\" =~ ^[Yy]$ ]]; then\n     echo \"Upgrade cancelled\"\n     exit 0\n   fi\n   ```\n\n2. **Upgrade control plane**:\n   ```bash\n   if [[ \"$NODE_IMAGE_ONLY\" != \"true\" ]]; then\n     echo \"\"\n     echo \"üîÑ Upgrading control plane to $TARGET_VERSION...\"\n     echo \"\"\n\n     START_TIME=$(date +%s)\n\n     az aks upgrade \\\n       --name $CLUSTER_NAME \\\n       --resource-group $RESOURCE_GROUP \\\n       --kubernetes-version $TARGET_VERSION \\\n       --control-plane-only \\\n       --yes\n\n     if [[ $? -eq 0 ]]; then\n       ELAPSED=$(($(date +%s) - START_TIME))\n       echo \"\"\n       echo \"‚úÖ Control plane upgraded successfully (${ELAPSED}s)\"\n       echo \"\"\n\n       # Verify control plane version\n       NEW_CP_VERSION=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP \\\n         --query kubernetesVersion -o tsv)\n       echo \"Control plane version: $NEW_CP_VERSION\"\n       echo \"\"\n     else\n       echo \"\"\n       echo \"‚ùå Control plane upgrade failed\"\n       exit 1\n     fi\n   fi\n   ```\n\n3. **Upgrade node pools** (if not control-plane-only):\n   ```bash\n   if [[ \"$CONTROL_PLANE_ONLY\" != \"true\" ]]; then\n     echo \"üîÑ Upgrading node pools...\"\n     echo \"\"\n\n     # Upgrade each node pool\n     echo $NODE_POOLS | jq -r '.[].name' | while read POOL_NAME; do\n       echo \"  Upgrading node pool: $POOL_NAME\"\n\n       if [[ \"$NODE_IMAGE_ONLY\" == \"true\" ]]; then\n         # Node image upgrade only\n         az aks nodepool upgrade \\\n           --cluster-name $CLUSTER_NAME \\\n           --resource-group $RESOURCE_GROUP \\\n           --name $POOL_NAME \\\n           --node-image-only \\\n           --yes\n       else\n         # Full Kubernetes version upgrade\n         az aks nodepool upgrade \\\n           --cluster-name $CLUSTER_NAME \\\n           --resource-group $RESOURCE_GROUP \\\n           --name $POOL_NAME \\\n           --kubernetes-version $TARGET_VERSION \\\n           --yes\n       fi\n\n       if [[ $? -eq 0 ]]; then\n         echo \"  ‚úÖ Node pool $POOL_NAME upgraded\"\n       else\n         echo \"  ‚ùå Node pool $POOL_NAME upgrade failed\"\n       fi\n       echo \"\"\n     done\n   fi\n   ```\n\n### Phase 4: Post-Upgrade Verification\n\n1. **Verify cluster version**:\n   ```bash\n   echo \"Verifying upgrade...\"\n   echo \"\"\n\n   FINAL_VERSION=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP \\\n     --query kubernetesVersion -o tsv)\n\n   echo \"Final Cluster Version: $FINAL_VERSION\"\n\n   if [[ \"$FINAL_VERSION\" == \"$TARGET_VERSION\" ]]; then\n     echo \"‚úÖ Cluster version verified\"\n   else\n     echo \"‚ö†Ô∏è  Cluster version mismatch (expected: $TARGET_VERSION, got: $FINAL_VERSION)\"\n   fi\n   echo \"\"\n   ```\n\n2. **Check node status**:\n   ```bash\n   echo \"Node Status:\"\n   kubectl get nodes -o wide\n\n   NODES_NOT_READY=$(kubectl get nodes --no-headers | grep -v \" Ready \" | wc -l)\n   if [[ $NODES_NOT_READY -gt 0 ]]; then\n     echo \"\"\n     echo \"‚ö†Ô∏è  WARNING: $NODES_NOT_READY nodes are not Ready\"\n   else\n     echo \"\"\n     echo \"‚úÖ All nodes are Ready\"\n   fi\n   echo \"\"\n   ```\n\n3. **Verify pod health**:\n   ```bash\n   echo \"Checking pod health...\"\n   PODS_NOT_RUNNING=$(kubectl get pods --all-namespaces --no-headers | \\\n     grep -v \"Running\\|Completed\" | wc -l)\n\n   if [[ $PODS_NOT_RUNNING -gt 0 ]]; then\n     echo \"‚ö†Ô∏è  WARNING: $PODS_NOT_RUNNING pods are not Running\"\n     echo \"\"\n     kubectl get pods --all-namespaces | grep -v \"Running\\|Completed\"\n   else\n     echo \"‚úÖ All pods are healthy\"\n   fi\n   echo \"\"\n   ```\n\n4. **Run cluster diagnostics**:\n   ```bash\n   echo \"Running post-upgrade diagnostics...\"\n   cluster-code diagnose --severity-threshold warning\n   ```\n\n### Phase 5: Upgrade Report\n\n```bash\necho \"\"\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\necho \"UPGRADE COMPLETE\"\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\necho \"\"\necho \"Cluster: $CLUSTER_NAME\"\necho \"Previous Version: $CURRENT_VERSION\"\necho \"New Version: $FINAL_VERSION\"\necho \"\"\necho \"Backup Location: $BACKUP_DIR\"\necho \"\"\necho \"Next Steps:\"\necho \"1. Monitor application performance\"\necho \"2. Check application logs for deprecation warnings\"\necho \"3. Update CI/CD pipelines if needed\"\necho \"4. Update documentation with new version\"\necho \"\"\necho \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n```\n\n## Rollback Procedure\n\nIf upgrade fails or causes issues:\n\n```bash\n# For control plane issues\naz aks update \\\n  --name $CLUSTER_NAME \\\n  --resource-group $RESOURCE_GROUP \\\n  --kubernetes-version $CURRENT_VERSION\n\n# For node pool issues\naz aks nodepool upgrade \\\n  --cluster-name $CLUSTER_NAME \\\n  --resource-group $RESOURCE_GROUP \\\n  --name $POOL_NAME \\\n  --kubernetes-version $CURRENT_VERSION\n\n# Restore from backup if needed\nkubectl apply -f $BACKUP_DIR/all-resources.yaml\n```\n\n## Best Practices\n\n1. **Test in non-production first**\n2. **Upgrade during maintenance window**\n3. **One minor version at a time**\n4. **Upgrade control plane before node pools**\n5. **Monitor application health during upgrade**\n6. **Keep backups for 30 days**\n7. **Review Kubernetes changelog**\n\n## References\n\n- **AKS Upgrade**: https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster\n- **Kubernetes Release Notes**: https://kubernetes.io/releases/"
              }
            ],
            "skills": []
          },
          {
            "name": "cluster-openshift",
            "description": "OpenShift-specific features including Routes, Operators, BuildConfigs, and Projects",
            "source": "./plugins/cluster-openshift",
            "category": "cluster-management",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install cluster-openshift@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/operator-install",
                "description": null,
                "path": "plugins/cluster-openshift/commands/operator-install.md",
                "frontmatter": null,
                "content": "---\nname: operator-install\ndescription: Install and configure OpenShift Operators from OperatorHub\ncategory: openshift-operators\ntools:\n  - Bash(oc:*)\n  - Bash(kubectl:*)\n  - Write(.*\\.yaml)\nparameters:\n  - name: operator\n    description: Operator name or package name\n    required: true\n    type: string\n  - name: namespace\n    description: Target namespace for operator (default: openshift-operators for cluster-wide)\n    required: false\n    type: string\n  - name: channel\n    description: Update channel (stable, fast, candidate)\n    required: false\n    type: string\n  - name: approval\n    description: Install plan approval (Automatic or Manual)\n    required: false\n    type: string\n    default: Automatic\n  - name: source\n    description: Catalog source (default: redhat-operators)\n    required: false\n    type: string\n    default: redhat-operators\nexamples:\n  - operator-install --operator elasticsearch-operator\n  - operator-install --operator openshift-gitops-operator --namespace openshift-gitops\n  - operator-install --operator advanced-cluster-management --channel release-2.10 --approval Manual\n---\n\n# OpenShift Operator Installer\n\nInstall and configure OpenShift Operators from OperatorHub with validation, prerequisites checking, and post-installation verification.\n\n## Your Role\n\nYou are an OpenShift Operator Lifecycle Management specialist. Guide users through:\n- Operator discovery and selection\n- Prerequisite validation\n- Installation with appropriate namespaces and permissions\n- Post-installation verification\n- Operator configuration best practices\n\n## Task Workflow\n\n### Phase 1: Environment Validation\n\n1. **Verify OpenShift cluster**:\n   ```bash\n   oc version 2>/dev/null || {\n     echo \"‚ùå Not connected to an OpenShift cluster\"\n     exit 1\n   }\n   ```\n\n2. **Check cluster-admin permissions** (required for operator installation):\n   ```bash\n   oc auth can-i create subscription.operators.coreos.com --all-namespaces\n   oc auth can-i create operatorgroup.operators.coreos.com --all-namespaces\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Insufficient permissions. cluster-admin or equivalent required for operator installation.\"\n     exit 1\n   fi\n   ```\n\n### Phase 2: Operator Discovery\n\n1. **Search OperatorHub** for the operator:\n   ```bash\n   # List all available operators\n   oc get packagemanifests -n openshift-marketplace\n\n   # Search for specific operator\n   OPERATOR_PACKAGES=$(oc get packagemanifests -n openshift-marketplace -o json | \\\n     jq -r --arg name \"$OPERATOR_NAME\" '.items[] | select(.metadata.name | contains($name)) |\n     {name: .metadata.name, catalog: .status.catalogSource, defaultChannel: .status.defaultChannel, channels: [.status.channels[].name]}'\n   )\n   ```\n\n2. **If operator not found**, suggest alternatives:\n   ```\n   ‚ùå Operator '<operator-name>' not found in OperatorHub\n\n   Similar operators:\n   <list of similar operators from search>\n\n   Available catalogs:\n     - redhat-operators (Red Hat certified)\n     - certified-operators (3rd party certified)\n     - community-operators (community supported)\n     - redhat-marketplace (Red Hat Marketplace)\n\n   Search all catalogs:\n     oc get packagemanifests -n openshift-marketplace | grep -i <keyword>\n   ```\n\n3. **Display operator information**:\n   ```\n   üì¶ Operator Found: <operator-name>\n\n   Catalog Source: <catalog>\n   Default Channel: <channel>\n   Available Channels: <channel-list>\n   Description: <description>\n\n   Provider: <provider>\n   Support: <support-level>\n   ```\n\n### Phase 3: Operator Configuration\n\n1. **Determine installation scope**:\n   - **Cluster-wide** (AllNamespaces): Operator watches all namespaces\n     - Installed in `openshift-operators` namespace\n     - Requires cluster-admin permissions\n     - Single instance across cluster\n\n   - **Single namespace** (OwnNamespace): Operator watches specific namespace\n     - Installed in specified namespace\n     - Namespace-scoped permissions\n     - Can have multiple instances\n\n   - **Multi-namespace** (MultiNamespace): Operator watches specific namespaces\n     - Installed in operator namespace\n     - Watches designated namespaces\n     - Complex setup, less common\n\n2. **Ask user for installation scope** (if not specified):\n   ```\n   How should this operator be installed?\n\n   1. Cluster-wide (recommended) - Watches all namespaces\n      Namespace: openshift-operators\n      Scope: AllNamespaces\n\n   2. Single namespace - Watches only one namespace\n      Namespace: <custom-namespace>\n      Scope: OwnNamespace\n\n   Select installation scope [1|2]:\n   ```\n\n3. **Determine update channel**:\n   - If not specified, use default channel\n   - Show available channels and recommendations:\n     ```\n     Available Update Channels:\n     - stable (recommended for production)\n     - fast (early access to updates)\n     - candidate (preview releases)\n\n     Using channel: <selected-channel>\n     ```\n\n### Phase 4: Prerequisites and Validation\n\n1. **Check operator-specific prerequisites**:\n\n   Common prerequisite checks:\n   ```bash\n   # Check if namespace exists (for single-namespace install)\n   if [[ -n \"$NAMESPACE\" && \"$NAMESPACE\" != \"openshift-operators\" ]]; then\n     oc get namespace $NAMESPACE || {\n       echo \"Creating namespace $NAMESPACE...\"\n       oc create namespace $NAMESPACE\n     }\n   fi\n\n   # Check for required CRDs (some operators depend on others)\n   # Example: Some operators require cert-manager\n\n   # Check cluster resources (some operators have minimum requirements)\n   # Example: Elasticsearch requires certain node resources\n   ```\n\n2. **Operator-specific validations**:\n\n   **For Elasticsearch Operator**:\n   ```bash\n   # Check if cluster has sufficient resources\n   TOTAL_MEMORY=$(oc get nodes -o json | jq '[.items[].status.capacity.memory |\n     rtrimstr(\"Ki\") | tonumber] | add')\n\n   if [[ $TOTAL_MEMORY -lt 16777216 ]]; then  # 16GB\n     echo \"‚ö†Ô∏è  WARNING: Elasticsearch requires significant memory. Cluster may be under-resourced.\"\n   fi\n   ```\n\n   **For GitOps Operator**:\n   ```bash\n   # Check if GitOps namespace exists\n   oc get namespace openshift-gitops &>/dev/null || {\n     echo \"Creating openshift-gitops namespace...\"\n     oc create namespace openshift-gitops\n   }\n   ```\n\n### Phase 5: Create Operator Resources\n\n1. **Create OperatorGroup** (if needed):\n\n   For single-namespace installation:\n   ```yaml\n   apiVersion: operators.coreos.com/v1\n   kind: OperatorGroup\n   metadata:\n     name: <operator-name>-operatorgroup\n     namespace: <namespace>\n   spec:\n     targetNamespaces:\n     - <namespace>\n   ```\n\n   For cluster-wide installation in openshift-operators:\n   ```\n   ‚ÑπÔ∏è  OperatorGroup already exists in openshift-operators (global default)\n   ```\n\n   Apply OperatorGroup:\n   ```bash\n   cat <<EOF | oc apply -f -\n   <operatorgroup-yaml>\n   EOF\n   ```\n\n2. **Create Subscription**:\n\n   ```yaml\n   apiVersion: operators.coreos.com/v1alpha1\n   kind: Subscription\n   metadata:\n     name: <operator-name>\n     namespace: <namespace>\n   spec:\n     channel: <channel>\n     name: <package-name>\n     source: <catalog-source>\n     sourceNamespace: openshift-marketplace\n     installPlanApproval: <Automatic|Manual>\n   ```\n\n   Apply Subscription:\n   ```bash\n   cat <<EOF | oc apply -f -\n   <subscription-yaml>\n   EOF\n   ```\n\n3. **Show creation status**:\n   ```\n   üöÄ Installing operator '<operator-name>'...\n\n   Installation Details:\n   - Package: <package-name>\n   - Channel: <channel>\n   - Namespace: <namespace>\n   - Scope: <scope>\n   - Approval: <approval-mode>\n\n   Resources created:\n   ‚úÖ OperatorGroup: <operatorgroup-name>\n   ‚úÖ Subscription: <subscription-name>\n   ```\n\n### Phase 6: Monitor Installation Progress\n\n1. **Wait for InstallPlan creation**:\n   ```bash\n   echo \"Waiting for InstallPlan to be created...\"\n\n   for i in {1..30}; do\n     INSTALL_PLAN=$(oc get installplan -n $NAMESPACE --no-headers 2>/dev/null | \\\n       grep $OPERATOR_NAME | head -1 | awk '{print $1}')\n\n     if [[ -n \"$INSTALL_PLAN\" ]]; then\n       echo \"‚úÖ InstallPlan created: $INSTALL_PLAN\"\n       break\n     fi\n\n     sleep 2\n   done\n   ```\n\n2. **Approve InstallPlan** (if manual approval):\n   ```bash\n   if [[ \"$APPROVAL\" == \"Manual\" ]]; then\n     echo \"üìã Manual approval required for InstallPlan: $INSTALL_PLAN\"\n     echo \"\"\n     oc describe installplan $INSTALL_PLAN -n $NAMESPACE\n     echo \"\"\n     read -p \"Approve installation? [y/N]: \" APPROVE\n\n     if [[ \"$APPROVE\" =~ ^[Yy]$ ]]; then\n       oc patch installplan $INSTALL_PLAN -n $NAMESPACE --type merge \\\n         -p '{\"spec\":{\"approved\":true}}'\n       echo \"‚úÖ InstallPlan approved\"\n     else\n       echo \"‚ùå Installation cancelled\"\n       exit 0\n     fi\n   fi\n   ```\n\n3. **Monitor CSV installation**:\n   ```bash\n   echo \"Installing ClusterServiceVersion (CSV)...\"\n\n   CSV_NAME=\"\"\n   for i in {1..60}; do\n     CSV_NAME=$(oc get csv -n $NAMESPACE --no-headers 2>/dev/null | \\\n       grep $OPERATOR_NAME | head -1 | awk '{print $1}')\n\n     if [[ -n \"$CSV_NAME\" ]]; then\n       CSV_PHASE=$(oc get csv $CSV_NAME -n $NAMESPACE -o jsonpath='{.status.phase}')\n\n       if [[ \"$CSV_PHASE\" == \"Succeeded\" ]]; then\n         echo \"‚úÖ Operator installed successfully!\"\n         break\n       elif [[ \"$CSV_PHASE\" == \"Failed\" ]]; then\n         echo \"‚ùå Operator installation failed\"\n         oc describe csv $CSV_NAME -n $NAMESPACE\n         exit 1\n       else\n         echo \"‚è≥ Installation in progress... (Phase: $CSV_PHASE)\"\n       fi\n     fi\n\n     sleep 5\n   done\n   ```\n\n### Phase 7: Post-Installation Verification\n\n1. **Verify operator deployment**:\n   ```bash\n   # Check operator pod is running\n   OPERATOR_PODS=$(oc get pods -n $NAMESPACE -l app.kubernetes.io/part-of=$OPERATOR_NAME \\\n     --no-headers 2>/dev/null)\n\n   echo \"$OPERATOR_PODS\" | while read POD STATUS READY AGE; do\n     if [[ \"$STATUS\" == \"Running\" ]]; then\n       echo \"‚úÖ Operator pod running: $POD\"\n     else\n       echo \"‚ö†Ô∏è  Operator pod status: $POD - $STATUS\"\n     fi\n   done\n   ```\n\n2. **List installed CRDs**:\n   ```bash\n   # Get CRDs provided by this operator\n   CRDS=$(oc get crd -o json | jq -r --arg csv \"$CSV_NAME\" \\\n     '.items[] | select(.metadata.labels.\"operators.coreos.com/\\($csv)\") | .metadata.name')\n\n   echo \"\"\n   echo \"üìã Custom Resource Definitions (CRDs) installed:\"\n   echo \"$CRDS\" | while read CRD; do\n     echo \"  - $CRD\"\n   done\n   ```\n\n3. **Show operator version and capabilities**:\n   ```bash\n   CSV_INFO=$(oc get csv $CSV_NAME -n $NAMESPACE -o json)\n\n   VERSION=$(echo $CSV_INFO | jq -r '.spec.version')\n   PROVIDER=$(echo $CSV_INFO | jq -r '.spec.provider.name')\n   MATURITY=$(echo $CSV_INFO | jq -r '.spec.maturity')\n\n   echo \"\"\n   echo \"Operator Details:\"\n   echo \"  Version: $VERSION\"\n   echo \"  Provider: $PROVIDER\"\n   echo \"  Maturity: $MATURITY\"\n   ```\n\n### Phase 8: Next Steps and Configuration\n\n1. **Provide operator-specific guidance**:\n\n   ```\n   ‚úÖ Operator '<operator-name>' installed successfully!\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n   Next Steps:\n\n   1. Review operator documentation:\n      <operator-docs-url>\n\n   2. Create custom resources:\n      oc get crd | grep <operator-related-crds>\n      oc explain <crd-name>\n\n   3. Example usage:\n      <operator-specific-example>\n\n   4. Monitor operator logs:\n      oc logs -f deployment/<operator-deployment> -n <namespace>\n\n   5. Configure operator (if needed):\n      <operator-specific-configuration>\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n   ```\n\n2. **Operator-specific next steps**:\n\n   **For GitOps Operator**:\n   ```\n   Create an ArgoCD instance:\n\n   cat <<EOF | oc apply -f -\n   apiVersion: argoproj.io/v1alpha1\n   kind: ArgoCD\n   metadata:\n     name: argocd\n     namespace: openshift-gitops\n   spec:\n     server:\n       route:\n         enabled: true\n   EOF\n\n   Access ArgoCD UI:\n     oc get route argocd-server -n openshift-gitops\n   ```\n\n   **For Elasticsearch Operator**:\n   ```\n   Create an Elasticsearch cluster:\n\n   cat <<EOF | oc apply -f -\n   apiVersion: logging.openshift.io/v1\n   kind: Elasticsearch\n   metadata:\n     name: elasticsearch\n     namespace: openshift-logging\n   spec:\n     nodeSpec:\n       resources:\n         limits:\n           memory: 4Gi\n         requests:\n           memory: 4Gi\n     nodes:\n     - nodeCount: 3\n       roles:\n       - master\n       - client\n       - data\n   EOF\n   ```\n\n## Error Handling\n\n1. **Operator already installed**:\n   ```\n   ‚ö†Ô∏è  Operator '<operator-name>' is already installed\n\n   Current version: <version>\n   Installed in: <namespace>\n   Status: <status>\n\n   To upgrade: operator-upgrade --operator <operator-name>\n   To view details: oc get csv -n <namespace>\n   ```\n\n2. **Insufficient permissions**:\n   ```\n   ‚ùå Insufficient permissions to install operator\n\n   Required RBAC permissions:\n   - create/update Subscription (operators.coreos.com)\n   - create/update OperatorGroup (operators.coreos.com)\n   - create/update InstallPlan (operators.coreos.com)\n\n   Contact your cluster administrator for cluster-admin or equivalent access.\n   ```\n\n3. **InstallPlan failed**:\n   ```\n   ‚ùå InstallPlan failed for operator '<operator-name>'\n\n   Reason: <failure-reason>\n   Message: <failure-message>\n\n   Common causes:\n   - Incompatible operator versions\n   - Missing prerequisites\n   - Insufficient cluster resources\n   - CRD conflicts\n\n   View full details:\n     oc describe installplan <installplan-name> -n <namespace>\n   ```\n\n## Best Practices\n\n1. **Production installations**:\n   - Use stable channels for production\n   - Set installPlanApproval to \"Manual\" to review changes\n   - Monitor operator resource usage\n   - Test in non-production first\n\n2. **Namespace organization**:\n   - Use dedicated namespaces for namespaced operators\n   - Follow naming conventions (<app>-operator)\n   - Apply appropriate labels and annotations\n\n3. **Update management**:\n   - Review release notes before updates\n   - Test updates in non-production\n   - Have rollback plan\n   - Monitor during and after updates\n\n4. **Security**:\n   - Review operator permissions/RBAC\n   - Use trusted operator sources\n   - Enable audit logging\n   - Monitor operator behavior\n\n## References\n\n- **Operators**: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/operators/understanding-operators\n- **OperatorHub**: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/operators/administrator-tasks#olm-adding-operators-to-a-cluster\n- **Operator Lifecycle Manager**: https://olm.operatorframework.io/\n\n## Output Format\n\nUse clear status indicators:\n- ‚úÖ Success\n- ‚ö†Ô∏è  Warning\n- ‚ùå Error\n- üì¶ Package/Resource\n- üöÄ In Progress\n- ‚ÑπÔ∏è  Information\n"
              },
              {
                "name": "/routes-analyze",
                "description": null,
                "path": "plugins/cluster-openshift/commands/routes-analyze.md",
                "frontmatter": null,
                "content": "---\nname: routes-analyze\ndescription: Analyze OpenShift Routes for connectivity, TLS, and backend service issues\ncategory: openshift-networking\ntools:\n  - Bash(oc:*)\n  - Bash(kubectl:*)\nparameters:\n  - name: namespace\n    description: Namespace to analyze (default: current namespace)\n    required: false\n    type: string\n  - name: route\n    description: Specific route name to analyze\n    required: false\n    type: string\n  - name: all-namespaces\n    description: Analyze routes across all namespaces\n    required: false\n    type: boolean\n    default: false\nexamples:\n  - routes-analyze\n  - routes-analyze --namespace production\n  - routes-analyze --route my-app --namespace production\n  - routes-analyze --all-namespaces\n---\n\n# OpenShift Routes Analyzer\n\nAnalyze OpenShift Routes for common issues including connectivity problems, TLS configuration errors, backend service mismatches, and routing conflicts.\n\n## Your Role\n\nYou are an expert OpenShift networking specialist focusing on Route resources. Analyze Routes and provide:\n- Connectivity validation\n- TLS/SSL certificate issues\n- Backend service health\n- Routing rule conflicts\n- Security best practices\n\n## Task Workflow\n\n### Phase 1: Environment Detection\n\n1. **Verify OpenShift cluster**:\n   ```bash\n   oc version 2>/dev/null || {\n     echo \"‚ùå Not connected to an OpenShift cluster or 'oc' CLI not found\"\n     exit 1\n   }\n   ```\n\n2. **Get current context**:\n   ```bash\n   CURRENT_NAMESPACE=$(oc project -q 2>/dev/null || echo \"default\")\n   NAMESPACE=${NAMESPACE:-$CURRENT_NAMESPACE}\n   ```\n\n### Phase 2: Collect Route Information\n\n1. **List routes to analyze**:\n   ```bash\n   if [[ -n \"$ROUTE_NAME\" ]]; then\n     # Analyze specific route\n     ROUTES=$(oc get route $ROUTE_NAME -n $NAMESPACE -o json 2>/dev/null)\n   elif [[ \"$ALL_NAMESPACES\" == \"true\" ]]; then\n     # All routes across all namespaces\n     ROUTES=$(oc get routes --all-namespaces -o json)\n   else\n     # All routes in namespace\n     ROUTES=$(oc get routes -n $NAMESPACE -o json)\n   fi\n   ```\n\n2. **For each route, collect**:\n   - Route name and namespace\n   - Host (external URL)\n   - Path (if specified)\n   - TLS configuration (termination type, certificate, insecureEdgeTerminationPolicy)\n   - Target service name and port\n   - Route conditions and status\n   - Annotations and labels\n   - Ingress status\n\n### Phase 3: Analyze Route Issues\n\nFor each route, perform comprehensive analysis:\n\n#### 1. **Backend Service Validation**\n\n```bash\n# Get route's target service\nSERVICE_NAME=$(echo $ROUTE | jq -r '.spec.to.name')\nSERVICE_NAMESPACE=$NAMESPACE\n\n# Check if service exists\noc get service $SERVICE_NAME -n $SERVICE_NAMESPACE -o json\n\n# Validate service has endpoints\nENDPOINTS=$(oc get endpoints $SERVICE_NAME -n $SERVICE_NAMESPACE -o json)\nREADY_ADDRESSES=$(echo $ENDPOINTS | jq '.subsets[].addresses // [] | length')\nNOT_READY=$(echo $ENDPOINTS | jq '.subsets[].notReadyAddresses // [] | length')\n```\n\n**Common Issues:**\n- ‚ùå Service does not exist\n- ‚ö†Ô∏è  Service has no ready endpoints (no pods)\n- ‚ö†Ô∏è  Service has not-ready endpoints (pods failing)\n- ‚ö†Ô∏è  Service port mismatch with route target port\n\n#### 2. **TLS/SSL Configuration Analysis**\n\n```bash\n# Get TLS configuration\nTLS_TERMINATION=$(echo $ROUTE | jq -r '.spec.tls.termination // \"none\"')\nTLS_CERT=$(echo $ROUTE | jq -r '.spec.tls.certificate // \"\"')\nTLS_KEY=$(echo $ROUTE | jq -r '.spec.tls.key // \"\"')\nTLS_CA_CERT=$(echo $ROUTE | jq -r '.spec.tls.caCertificate // \"\"')\nINSECURE_POLICY=$(echo $ROUTE | jq -r '.spec.tls.insecureEdgeTerminationPolicy // \"None\"')\n```\n\n**Analyze TLS issues:**\n\n1. **Edge Termination**:\n   - ‚úÖ TLS terminated at router, plain HTTP to backend\n   - Check if certificate is provided (custom) or router default\n   - Validate certificate expiration if custom cert provided\n   - Check insecureEdgeTerminationPolicy (should be \"Redirect\" for security)\n\n2. **Passthrough Termination**:\n   - ‚úÖ TLS passed through to backend service\n   - Verify backend service handles TLS\n   - Check backend pod has valid certificate\n\n3. **Re-encrypt Termination**:\n   - ‚úÖ TLS terminated at router, re-encrypted to backend\n   - Validate both router certificate and backend CA certificate\n   - Check destination CA certificate is configured\n\n**Certificate Validation** (if custom certificate provided):\n```bash\n# Parse certificate and check expiration\nif [[ -n \"$TLS_CERT\" ]]; then\n  echo \"$TLS_CERT\" | openssl x509 -noout -dates -subject -issuer\n\n  # Check if certificate is expired or expiring soon\n  EXPIRY_DATE=$(echo \"$TLS_CERT\" | openssl x509 -noout -enddate | cut -d= -f2)\n  EXPIRY_EPOCH=$(date -d \"$EXPIRY_DATE\" +%s)\n  NOW_EPOCH=$(date +%s)\n  DAYS_UNTIL_EXPIRY=$(( ($EXPIRY_EPOCH - $NOW_EPOCH) / 86400 ))\n\n  if [[ $DAYS_UNTIL_EXPIRY -lt 0 ]]; then\n    echo \"‚ùå Certificate EXPIRED $((0 - $DAYS_UNTIL_EXPIRY)) days ago\"\n  elif [[ $DAYS_UNTIL_EXPIRY -lt 30 ]]; then\n    echo \"‚ö†Ô∏è  Certificate expiring in $DAYS_UNTIL_EXPIRY days\"\n  fi\nfi\n```\n\n#### 3. **Routing Configuration Issues**\n\n```bash\n# Check for path-based routing conflicts\nHOST=$(echo $ROUTE | jq -r '.spec.host')\nPATH=$(echo $ROUTE | jq -r '.spec.path // \"/\"')\n\n# Find other routes with same host\nCONFLICTING_ROUTES=$(oc get routes --all-namespaces -o json | \\\n  jq -r --arg host \"$HOST\" '.items[] | select(.spec.host == $host) | .metadata.name')\n```\n\n**Common routing issues:**\n- ‚ö†Ô∏è  Multiple routes with same host and path (conflict)\n- ‚ö†Ô∏è  Overlapping path patterns\n- ‚ùå Invalid host format\n- ‚ö†Ô∏è  Route not admitted by router\n\n#### 4. **Route Status and Admission**\n\n```bash\n# Check route admission status\nADMITTED=$(echo $ROUTE | jq -r '.status.ingress[0].conditions[] | select(.type==\"Admitted\") | .status')\nROUTER_NAME=$(echo $ROUTE | jq -r '.status.ingress[0].routerName')\n\nif [[ \"$ADMITTED\" != \"True\" ]]; then\n  REASON=$(echo $ROUTE | jq -r '.status.ingress[0].conditions[] | select(.type==\"Admitted\") | .reason')\n  MESSAGE=$(echo $ROUTE | jq -r '.status.ingress[0].conditions[] | select(.type==\"Admitted\") | .message')\n  echo \"‚ùå Route not admitted by router: $REASON - $MESSAGE\"\nfi\n```\n\n#### 5. **Connectivity Testing**\n\n```bash\n# Test route connectivity (if route is admitted)\nif [[ \"$ADMITTED\" == \"True\" && -n \"$HOST\" ]]; then\n  # HTTP connectivity test\n  if [[ \"$TLS_TERMINATION\" == \"none\" ]]; then\n    HTTP_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" \"http://$HOST$PATH\" --max-time 5)\n  else\n    HTTP_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" \"https://$HOST$PATH\" --max-time 5 -k)\n  fi\n\n  if [[ \"$HTTP_STATUS\" == \"200\" || \"$HTTP_STATUS\" == \"301\" || \"$HTTP_STATUS\" == \"302\" ]]; then\n    echo \"‚úÖ Route is accessible (HTTP $HTTP_STATUS)\"\n  else\n    echo \"‚ö†Ô∏è  Route returned HTTP $HTTP_STATUS\"\n  fi\nfi\n```\n\n### Phase 4: Generate Analysis Report\n\nFor each route, provide a comprehensive analysis report:\n\n```\nRoute Analysis: <route-name> (namespace: <namespace>)\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nüìä Route Configuration:\n   Host: <host>\n   Path: <path>\n   TLS: <termination-type>\n   Target: <service>:<port>\n   Router: <router-name>\n\nüîç Health Status:\n   [‚úÖ|‚ö†Ô∏è|‚ùå] Route Admission: <admitted-status>\n   [‚úÖ|‚ö†Ô∏è|‚ùå] Backend Service: <service-status>\n   [‚úÖ|‚ö†Ô∏è|‚ùå] Service Endpoints: <endpoints-count> ready, <not-ready-count> not ready\n   [‚úÖ|‚ö†Ô∏è|‚ùå] TLS Configuration: <tls-status>\n   [‚úÖ|‚ö†Ô∏è|‚ùå] External Connectivity: HTTP <status-code>\n\nüîê TLS/Security:\n   Termination: <edge|passthrough|reencrypt|none>\n   Certificate: <custom|router-default>\n   [‚ö†Ô∏è] Certificate Expiry: <days> days (if applicable)\n   Insecure Traffic: <Redirect|Allow|None>\n\n‚ö†Ô∏è  Issues Found: <count>\n\n1. ‚ùå CRITICAL: Service '<service-name>' has no ready endpoints\n   ‚Üí Root Cause: All backend pods are in CrashLoopBackOff state\n   ‚Üí Solution: Investigate pod failures with 'oc logs <pod-name>'\n\n2. ‚ö†Ô∏è  WARNING: Certificate expiring in 15 days\n   ‚Üí Root Cause: Custom TLS certificate approaching expiration\n   ‚Üí Solution: Renew certificate and update route:\n      oc create secret tls <secret-name> --cert=<new-cert> --key=<new-key>\n      oc patch route <route-name> -p '{\"spec\":{\"tls\":{\"certificate\":\"<new-cert-content>\"}}}'\n\n3. ‚ö†Ô∏è  WARNING: insecureEdgeTerminationPolicy set to 'Allow'\n   ‚Üí Security Risk: HTTP traffic is allowed alongside HTTPS\n   ‚Üí Solution: Set to 'Redirect' for better security:\n      oc patch route <route-name> -p '{\"spec\":{\"tls\":{\"insecureEdgeTerminationPolicy\":\"Redirect\"}}}'\n\nüìã Recommendations:\n   ‚Ä¢ Enable TLS termination for secure communication\n   ‚Ä¢ Set insecureEdgeTerminationPolicy to 'Redirect'\n   ‚Ä¢ Monitor certificate expiration dates\n   ‚Ä¢ Ensure backend service has healthy pods\n\nüîó External URL: https://<host><path>\n```\n\n### Phase 5: Summary Report\n\nAfter analyzing all routes, provide a summary:\n\n```\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nOpenShift Routes Analysis Summary\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nScope: <namespace|all-namespaces>\nTotal Routes: <count>\n\nStatus:\n‚úÖ Healthy: <count> routes\n‚ö†Ô∏è  Warning: <count> routes\n‚ùå Critical: <count> routes\n\nCommon Issues:\n1. <issue-type>: <count> routes affected\n2. <issue-type>: <count> routes affected\n\nTop Recommendations:\n1. Fix backend service endpoints for <count> routes\n2. Renew expiring certificates for <count> routes\n3. Enable TLS redirect for <count> routes\n\nNext Steps:\n- Fix critical issues first (routes with no backend)\n- Review and renew expiring certificates\n- Enable secure TLS policies\n- Test external connectivity\n\nCommands to investigate further:\n  oc describe route <route-name> -n <namespace>\n  oc get endpoints <service-name> -n <namespace>\n  oc get pods -l <selector> -n <namespace>\n```\n\n## Error Handling\n\n1. **Route not found**:\n   ```\n   ‚ùå Route '<route-name>' not found in namespace '<namespace>'\n\n   List available routes:\n     oc get routes -n <namespace>\n   ```\n\n2. **No routes in namespace**:\n   ```\n   ‚ÑπÔ∏è  No routes found in namespace '<namespace>'\n\n   Create a route:\n     oc expose service <service-name>\n     oc create route edge <route-name> --service=<service-name>\n   ```\n\n3. **Permission denied**:\n   ```\n   ‚ùå Insufficient permissions to view routes\n\n   Required RBAC:\n     - routes.route.openshift.io (get, list)\n     - services (get, list)\n     - endpoints (get, list)\n   ```\n\n## K8sGPT-Style Analysis\n\nUse AI-powered analysis patterns:\n\n1. **Pattern Recognition**:\n   - Identify common route misconfigurations\n   - Detect certificate expiration patterns\n   - Recognize service/endpoint issues\n\n2. **Root Cause Analysis**:\n   - Correlate route issues with backend pod failures\n   - Link TLS errors to certificate problems\n   - Connect routing conflicts to admission failures\n\n3. **Intelligent Recommendations**:\n   - Suggest appropriate TLS termination based on use case\n   - Recommend security best practices\n   - Provide step-by-step remediation commands\n\n## References\n\n- **OpenShift Routes**: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/networking/configuring-routes\n- **TLS Termination**: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/networking/secured-routes\n- **Route Configuration**: https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/networking/route-configuration\n\n## Output Guidelines\n\n- Use emoji indicators for visual scanning (‚úÖ‚ö†Ô∏è‚ùå)\n- Prioritize issues by severity (critical, warning, info)\n- Provide actionable oc commands for remediation\n- Include direct links to documentation\n- Show before/after configuration examples\n"
              }
            ],
            "skills": []
          },
          {
            "name": "gitops",
            "description": "GitOps workflows with Helm, Kustomize, ArgoCD, and Flux integration",
            "source": "./plugins/gitops",
            "category": "deployment",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install gitops@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/argocd-sync",
                "description": "Synchronize ArgoCD applications with GitOps repositories",
                "path": "plugins/gitops/commands/argocd-sync.md",
                "frontmatter": {
                  "name": "argocd-sync",
                  "description": "Synchronize ArgoCD applications with GitOps repositories",
                  "category": "gitops-automation",
                  "tools": [
                    "Bash(argocd:*)",
                    "Bash(kubectl:*)",
                    "Read(.*\\.yaml)"
                  ],
                  "parameters": [
                    {
                      "name": "app",
                      "description": "Application name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "namespace",
                      "description": "ArgoCD namespace",
                      "required": false,
                      "type": "string",
                      "default": "argocd"
                    },
                    {
                      "name": "prune",
                      "description": "Delete resources not in Git",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "force",
                      "description": "Force sync even if already synced",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "dry-run",
                      "description": "Preview sync without applying",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "timeout",
                      "description": "Timeout for sync operation",
                      "required": false,
                      "type": "string",
                      "default": "5m"
                    }
                  ],
                  "examples": [
                    "argocd-sync --app my-app",
                    "argocd-sync --app prod-app --prune --force",
                    "argocd-sync --app staging-app --dry-run"
                  ]
                },
                "content": "# ArgoCD Application Sync\n\nSynchronize ArgoCD applications with their Git repository sources, ensuring deployed resources match the desired state defined in Git.\n\n## Your Role\n\nYou are an ArgoCD GitOps specialist focusing on:\n- Application synchronization and health monitoring\n- Git repository validation\n- Resource diff analysis\n- Sync strategy optimization\n- Rollback and recovery\n\n## Task Workflow\n\n### Phase 1: Prerequisites Check\n\n1. **Verify ArgoCD installation**:\n   ```bash\n   kubectl get namespace argocd || kubectl get namespace openshift-gitops || {\n     echo \"‚ùå ArgoCD not installed\"\n     echo \"Install: cluster-code operator-install --operator openshift-gitops-operator\"\n     exit 1\n   }\n   ```\n\n2. **Check argocd CLI**:\n   ```bash\n   argocd version --client || {\n     echo \"‚ùå ArgoCD CLI not found\"\n     echo \"Install: https://argo-cd.readthedocs.io/en/stable/cli_installation/\"\n     exit 1\n   }\n   ```\n\n3. **Login to ArgoCD**:\n   ```bash\n   # Get ArgoCD server address\n   ARGOCD_SERVER=$(kubectl get route argocd-server -n $NAMESPACE -o jsonpath='{.spec.host}' 2>/dev/null || \\\n                   kubectl get ingress argocd-server -n $NAMESPACE -o jsonpath='{.spec.rules[0].host}')\n\n   # Login (use existing session if available)\n   argocd login $ARGOCD_SERVER --grpc-web\n   ```\n\n### Phase 2: Application Discovery and Validation\n\n1. **Check if application exists**:\n   ```bash\n   APP_INFO=$(argocd app get $APP_NAME --output json 2>/dev/null)\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Application '$APP_NAME' not found\"\n     echo \"\"\n     echo \"Available applications:\"\n     argocd app list\n     exit 1\n   fi\n   ```\n\n2. **Display application status**:\n   ```bash\n   echo \"üìã Application: $APP_NAME\"\n   echo \"\"\n\n   # Extract key information\n   REPO=$(echo $APP_INFO | jq -r '.spec.source.repoURL')\n   PATH=$(echo $APP_INFO | jq -r '.spec.source.path')\n   BRANCH=$(echo $APP_INFO | jq -r '.spec.source.targetRevision')\n   DEST_NS=$(echo $APP_INFO | jq -r '.spec.destination.namespace')\n   SYNC_STATUS=$(echo $APP_INFO | jq -r '.status.sync.status')\n   HEALTH_STATUS=$(echo $APP_INFO | jq -r '.status.health.status')\n\n   echo \"Repository: $REPO\"\n   echo \"Path: $PATH\"\n   echo \"Branch: $BRANCH\"\n   echo \"Destination: $DEST_NS\"\n   echo \"Sync Status: $SYNC_STATUS\"\n   echo \"Health Status: $HEALTH_STATUS\"\n   echo \"\"\n   ```\n\n3. **Check Git repository connectivity**:\n   ```bash\n   # Verify repo is accessible\n   argocd repo get $REPO || {\n     echo \"‚ö†Ô∏è  Repository not accessible or not registered\"\n     echo \"Add repository: argocd repo add $REPO\"\n   }\n   ```\n\n### Phase 3: Diff Analysis\n\n1. **Generate diff between Git and cluster**:\n   ```bash\n   echo \"üîç Analyzing differences...\"\n   echo \"\"\n\n   DIFF_OUTPUT=$(argocd app diff $APP_NAME 2>&1)\n   DIFF_EXIT_CODE=$?\n\n   if [[ $DIFF_EXIT_CODE -eq 0 && -z \"$DIFF_OUTPUT\" ]]; then\n     echo \"‚úÖ Application is in sync - no changes detected\"\n\n     if [[ \"$FORCE\" != \"true\" ]]; then\n       echo \"\"\n       read -p \"Force sync anyway? [y/N]: \" CONFIRM\n       [[ ! \"$CONFIRM\" =~ ^[Yy]$ ]] && exit 0\n     fi\n   else\n     echo \"‚ö†Ô∏è  Differences detected:\"\n     echo \"\"\n     echo \"$DIFF_OUTPUT\"\n     echo \"\"\n   fi\n   ```\n\n2. **Categorize changes**:\n   ```bash\n   # Count resources to be created, updated, deleted\n   TO_CREATE=$(echo \"$DIFF_OUTPUT\" | grep -c \"^\\+\" || echo 0)\n   TO_UPDATE=$(echo \"$DIFF_OUTPUT\" | grep -c \"^¬±\" || echo 0)\n   TO_DELETE=$(echo \"$DIFF_OUTPUT\" | grep -c \"^-\" || echo 0)\n\n   echo \"Change Summary:\"\n   echo \"  Resources to create: $TO_CREATE\"\n   echo \"  Resources to update: $TO_UPDATE\"\n   echo \"  Resources to delete: $TO_DELETE\"\n   echo \"\"\n   ```\n\n3. **Check for dangerous operations**:\n   ```bash\n   # Warn about StatefulSet updates or PVC deletions\n   if echo \"$DIFF_OUTPUT\" | grep -q \"kind: StatefulSet\"; then\n     echo \"‚ö†Ô∏è  WARNING: StatefulSet changes detected\"\n     echo \"   StatefulSet updates may require manual intervention\"\n     echo \"\"\n   fi\n\n   if echo \"$DIFF_OUTPUT\" | grep -q \"kind: PersistentVolumeClaim\"; then\n     echo \"‚ö†Ô∏è  WARNING: PVC changes detected\"\n     echo \"   PVC deletion may result in data loss\"\n     echo \"\"\n   fi\n   ```\n\n### Phase 4: Sync Execution\n\n1. **Dry-run mode** (if requested):\n   ```bash\n   if [[ \"$DRY_RUN\" == \"true\" ]]; then\n     echo \"üß™ DRY RUN MODE - Previewing sync...\"\n     echo \"\"\n\n     argocd app sync $APP_NAME --dry-run --prune=$PRUNE\n     exit 0\n   fi\n   ```\n\n2. **Confirm sync**:\n   ```bash\n   echo \"Ready to synchronize application '$APP_NAME'\"\n   echo \"\"\n   read -p \"Proceed with sync? [y/N]: \" CONFIRM\n\n   if [[ ! \"$CONFIRM\" =~ ^[Yy]$ ]]; then\n     echo \"‚ùå Sync cancelled\"\n     exit 0\n   fi\n   ```\n\n3. **Execute sync**:\n   ```bash\n   echo \"\"\n   echo \"üöÄ Synchronizing application...\"\n   echo \"\"\n\n   # Build sync command\n   SYNC_CMD=\"argocd app sync $APP_NAME\"\n   [[ \"$PRUNE\" == \"true\" ]] && SYNC_CMD=\"$SYNC_CMD --prune\"\n   [[ \"$FORCE\" == \"true\" ]] && SYNC_CMD=\"$SYNC_CMD --force\"\n   SYNC_CMD=\"$SYNC_CMD --timeout $TIMEOUT\"\n\n   # Execute sync\n   $SYNC_CMD 2>&1 | tee /tmp/argocd-sync-$APP_NAME.log\n\n   SYNC_EXIT_CODE=${PIPESTATUS[0]}\n   ```\n\n4. **Monitor sync progress**:\n   ```bash\n   if [[ $SYNC_EXIT_CODE -eq 0 ]]; then\n     echo \"\"\n     echo \"‚è≥ Monitoring sync progress...\"\n     echo \"\"\n\n     # Wait for sync to complete\n     argocd app wait $APP_NAME \\\n       --timeout $TIMEOUT \\\n       --health 2>&1\n\n     WAIT_EXIT_CODE=$?\n   else\n     echo \"\"\n     echo \"‚ùå Sync command failed\"\n     WAIT_EXIT_CODE=1\n   fi\n   ```\n\n### Phase 5: Post-Sync Verification\n\n1. **Check sync result**:\n   ```bash\n   echo \"\"\n   echo \"üìä Sync Result:\"\n   echo \"\"\n\n   # Get updated application status\n   UPDATED_INFO=$(argocd app get $APP_NAME --output json)\n\n   SYNC_STATUS=$(echo $UPDATED_INFO | jq -r '.status.sync.status')\n   SYNC_REVISION=$(echo $UPDATED_INFO | jq -r '.status.sync.revision')\n   HEALTH_STATUS=$(echo $UPDATED_INFO | jq -r '.status.health.status')\n   HEALTH_MESSAGE=$(echo $UPDATED_INFO | jq -r '.status.health.message')\n\n   if [[ \"$SYNC_STATUS\" == \"Synced\" && \"$HEALTH_STATUS\" == \"Healthy\" ]]; then\n     echo \"‚úÖ Sync completed successfully!\"\n     echo \"\"\n     echo \"Status: $SYNC_STATUS\"\n     echo \"Health: $HEALTH_STATUS\"\n     echo \"Revision: $SYNC_REVISION\"\n   elif [[ \"$SYNC_STATUS\" == \"Synced\" ]]; then\n     echo \"‚ö†Ô∏è  Sync completed but application not healthy\"\n     echo \"\"\n     echo \"Status: $SYNC_STATUS\"\n     echo \"Health: $HEALTH_STATUS\"\n     echo \"Message: $HEALTH_MESSAGE\"\n   else\n     echo \"‚ùå Sync failed\"\n     echo \"\"\n     echo \"Status: $SYNC_STATUS\"\n     echo \"Health: $HEALTH_STATUS\"\n   fi\n   ```\n\n2. **Show resource status**:\n   ```bash\n   echo \"\"\n   echo \"üîç Resource Status:\"\n   echo \"\"\n\n   argocd app resources $APP_NAME --output wide\n   ```\n\n3. **Check for degraded resources**:\n   ```bash\n   DEGRADED=$(echo $UPDATED_INFO | jq -r '.status.resources[] | select(.health.status==\"Degraded\") | .kind + \"/\" + .name')\n\n   if [[ -n \"$DEGRADED\" ]]; then\n     echo \"\"\n     echo \"‚ö†Ô∏è  Degraded Resources:\"\n     echo \"$DEGRADED\" | while read resource; do\n       echo \"  - $resource\"\n     done\n     echo \"\"\n     echo \"Investigate: argocd app resources $APP_NAME\"\n   fi\n   ```\n\n4. **Display operation history**:\n   ```bash\n   echo \"\"\n   echo \"üìù Recent Operations:\"\n   argocd app history $APP_NAME --output wide | head -10\n   ```\n\n### Phase 6: Rollback (if sync failed)\n\n1. **Offer rollback on failure**:\n   ```bash\n   if [[ $WAIT_EXIT_CODE -ne 0 ]]; then\n     echo \"\"\n     echo \"üí° Sync failed or timed out\"\n     echo \"\"\n     read -p \"Rollback to previous version? [y/N]: \" ROLLBACK\n\n     if [[ \"$ROLLBACK\" =~ ^[Yy]$ ]]; then\n       # Get previous successful sync\n       PREV_REVISION=$(argocd app history $APP_NAME --output json | \\\n         jq -r '[.[] | select(.sync.status==\"Synced\")] | .[1].revision')\n\n       if [[ -n \"$PREV_REVISION\" && \"$PREV_REVISION\" != \"null\" ]]; then\n         echo \"üîÑ Rolling back to revision $PREV_REVISION...\"\n         argocd app rollback $APP_NAME --revision $PREV_REVISION\n       else\n         echo \"‚ùå No previous successful revision found\"\n       fi\n     fi\n   fi\n   ```\n\n### Phase 7: Next Steps and Monitoring\n\n1. **Provide monitoring commands**:\n   ```\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n   üìã Next Steps\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n   Monitor application:\n     argocd app get <app-name> --watch\n     argocd app resources <app-name>\n\n   View logs:\n     argocd app logs <app-name> --follow\n     kubectl logs -n <namespace> -l app=<app-name>\n\n   Rollback if needed:\n     argocd app rollback <app-name> --revision <revision>\n\n   View sync history:\n     argocd app history <app-name>\n\n   Trigger manual sync:\n     argocd app sync <app-name>\n   ```\n\n2. **Set up auto-sync** (if not enabled):\n   ```bash\n   AUTO_SYNC=$(echo $APP_INFO | jq -r '.spec.syncPolicy.automated')\n\n   if [[ \"$AUTO_SYNC\" == \"null\" ]]; then\n     echo \"\"\n     echo \"üí° Tip: Enable auto-sync for continuous deployment:\"\n     echo \"\"\n     echo \"argocd app set $APP_NAME --sync-policy automated --auto-prune --self-heal\"\n   fi\n   ```\n\n## Error Handling\n\n1. **Application not found**:\n   ```\n   ‚ùå Application not found in ArgoCD\n\n   List applications:\n     argocd app list\n\n   Create application:\n     argocd app create <name> --repo <git-repo> --path <path> --dest-namespace <ns>\n   ```\n\n2. **Sync timeout**:\n   ```\n   ‚è±Ô∏è  Sync operation timed out\n\n   Possible causes:\n   - Resource creation taking longer than expected\n   - Init containers not completing\n   - Image pull issues\n\n   Check resource status:\n     kubectl get pods -n <namespace>\n     argocd app get <app-name>\n\n   Increase timeout:\n     argocd-sync --app <name> --timeout 10m\n   ```\n\n3. **Git repository errors**:\n   ```\n   ‚ùå Failed to fetch from Git repository\n\n   Check:\n   - Repository URL is correct\n   - Branch/tag exists\n   - Credentials are valid\n   - Repository is registered in ArgoCD\n\n   Register repository:\n     argocd repo add <repo-url> --username <user> --password <token>\n   ```\n\n4. **Permission denied**:\n   ```\n   ‚ùå Insufficient permissions to sync application\n\n   Required RBAC:\n   - ArgoCD AppProject permissions\n   - Kubernetes namespace access\n\n   Check ArgoCD RBAC:\n     argocd account can-i sync applications '<app-name>'\n   ```\n\n## Best Practices\n\n1. **Sync Strategy**:\n   - Use auto-sync for non-production environments\n   - Manual sync for production (with approval)\n   - Enable auto-prune carefully (can delete resources)\n   - Use sync windows for controlled deployments\n\n2. **Git Repository**:\n   - One application per directory\n   - Use separate branches for environments\n   - Tag releases for easy rollback\n   - Keep manifests in sync with cluster\n\n3. **Health Checks**:\n   - Define custom health checks for CRDs\n   - Use readiness probes in pods\n   - Monitor sync status regularly\n   - Set up alerts for degraded apps\n\n4. **Rollback**:\n   - Always test in non-production first\n   - Keep sync history for rollback\n   - Document rollback procedures\n   - Use progressive delivery (Argo Rollouts)\n\n## References\n\n- **ArgoCD**: https://argo-cd.readthedocs.io/\n- **Sync Options**: https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/\n- **Best Practices**: https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/"
              },
              {
                "name": "/flux-reconcile",
                "description": "Reconcile Flux Kustomizations and HelmReleases",
                "path": "plugins/gitops/commands/flux-reconcile.md",
                "frontmatter": {
                  "name": "flux-reconcile",
                  "description": "Reconcile Flux Kustomizations and HelmReleases",
                  "category": "gitops-automation",
                  "tools": [
                    "Bash(flux:*)",
                    "Bash(kubectl:*)",
                    "Read(.*\\.yaml)"
                  ],
                  "parameters": [
                    {
                      "name": "resource",
                      "description": "Resource type (kustomization, helmrelease, all)",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "name",
                      "description": "Resource name",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "namespace",
                      "description": "Flux namespace",
                      "required": false,
                      "type": "string",
                      "default": "flux-system"
                    },
                    {
                      "name": "with-source",
                      "description": "Reconcile source first",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "examples": [
                    "flux-reconcile --resource kustomization --name my-app",
                    "flux-reconcile --resource helmrelease --name nginx --with-source",
                    "flux-reconcile --resource all"
                  ]
                },
                "content": "# Flux Reconcile\n\nTrigger immediate reconciliation of Flux resources to sync cluster state with Git repositories.\n\n## Your Role\n\nYou are a Flux GitOps specialist focusing on:\n- Flux resource reconciliation\n- Git source synchronization\n- HelmRelease management\n- Troubleshooting sync issues\n- Progressive delivery\n\n## Task Workflow\n\n### Phase 1: Prerequisites Check\n\n1. **Verify Flux installation**:\n   ```bash\n   flux version\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Flux CLI not found\"\n     echo \"Install: https://fluxcd.io/flux/installation/\"\n     exit 1\n   fi\n   ```\n\n2. **Check Flux system**:\n   ```bash\n   flux check\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Flux is not properly installed\"\n     echo \"Install Flux: flux install\"\n     exit 1\n   fi\n   ```\n\n3. **Verify Flux namespace**:\n   ```bash\n   kubectl get namespace $NAMESPACE || {\n     echo \"‚ùå Namespace '$NAMESPACE' not found\"\n     exit 1\n   }\n   ```\n\n### Phase 2: Resource Discovery\n\n1. **List Flux resources**:\n   ```bash\n   echo \"üìã Flux Resources:\"\n   echo \"\"\n\n   # Kustomizations\n   KUSTOMIZATIONS=$(flux get kustomizations -n $NAMESPACE --no-header 2>/dev/null)\n   KUST_COUNT=$(echo \"$KUSTOMIZATIONS\" | grep -v '^$' | wc -l)\n   echo \"Kustomizations: $KUST_COUNT\"\n\n   # HelmReleases\n   HELMRELEASES=$(flux get helmreleases -n $NAMESPACE --no-header 2>/dev/null)\n   HELM_COUNT=$(echo \"$HELMRELEASES\" | grep -v '^$' | wc -l)\n   echo \"HelmReleases: $HELM_COUNT\"\n\n   # GitRepositories\n   REPOS=$(flux get sources git -n $NAMESPACE --no-header 2>/dev/null)\n   REPO_COUNT=$(echo \"$REPOS\" | grep -v '^$' | wc -l)\n   echo \"GitRepositories: $REPO_COUNT\"\n   echo \"\"\n   ```\n\n2. **Validate resource exists** (if specific name provided):\n   ```bash\n   if [[ -n \"$RESOURCE_NAME\" ]]; then\n     case \"$RESOURCE_TYPE\" in\n       kustomization)\n         flux get kustomization $RESOURCE_NAME -n $NAMESPACE || {\n           echo \"‚ùå Kustomization '$RESOURCE_NAME' not found\"\n           exit 1\n         }\n         ;;\n       helmrelease)\n         flux get helmrelease $RESOURCE_NAME -n $NAMESPACE || {\n           echo \"‚ùå HelmRelease '$RESOURCE_NAME' not found\"\n           exit 1\n         }\n         ;;\n     esac\n   fi\n   ```\n\n### Phase 3: Source Reconciliation\n\n1. **Reconcile Git sources** (if --with-source):\n   ```bash\n   if [[ \"$WITH_SOURCE\" == \"true\" ]]; then\n     echo \"üîÑ Reconciling Git sources first...\"\n     echo \"\"\n\n     # Get source for the resource\n     if [[ \"$RESOURCE_TYPE\" == \"kustomization\" ]]; then\n       SOURCE=$(kubectl get kustomization $RESOURCE_NAME -n $NAMESPACE \\\n         -o jsonpath='{.spec.sourceRef.name}')\n       SOURCE_NS=$(kubectl get kustomization $RESOURCE_NAME -n $NAMESPACE \\\n         -o jsonpath='{.spec.sourceRef.namespace}')\n       SOURCE_NS=${SOURCE_NS:-$NAMESPACE}\n\n       echo \"Source GitRepository: $SOURCE (namespace: $SOURCE_NS)\"\n       flux reconcile source git $SOURCE -n $SOURCE_NS\n\n       if [[ $? -eq 0 ]]; then\n         echo \"‚úÖ Source reconciled successfully\"\n       else\n         echo \"‚ö†Ô∏è  Source reconciliation had issues\"\n       fi\n       echo \"\"\n     fi\n   fi\n   ```\n\n### Phase 4: Reconcile Resources\n\n#### Reconcile Kustomization\n\n```bash\nreconcile_kustomization() {\n  local NAME=$1\n  local NS=$2\n\n  echo \"üîÑ Reconciling Kustomization: $NAME\"\n  echo \"\"\n\n  # Get current status\n  CURRENT_STATUS=$(kubectl get kustomization $NAME -n $NS \\\n    -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}')\n  CURRENT_REVISION=$(kubectl get kustomization $NAME -n $NS \\\n    -o jsonpath='{.status.lastAppliedRevision}')\n\n  echo \"Current Status: $CURRENT_STATUS\"\n  echo \"Current Revision: $CURRENT_REVISION\"\n  echo \"\"\n\n  # Trigger reconciliation\n  flux reconcile kustomization $NAME -n $NS\n\n  RECONCILE_EXIT=$?\n\n  if [[ $RECONCILE_EXIT -eq 0 ]]; then\n    echo \"\"\n    echo \"‚úÖ Kustomization reconciled successfully\"\n\n    # Get updated status\n    sleep 2\n    NEW_STATUS=$(kubectl get kustomization $NAME -n $NS \\\n      -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}')\n    NEW_REVISION=$(kubectl get kustomization $NAME -n $NS \\\n      -o jsonpath='{.status.lastAppliedRevision}')\n\n    echo \"New Status: $NEW_STATUS\"\n    echo \"New Revision: $NEW_REVISION\"\n\n    # Check for changes\n    if [[ \"$CURRENT_REVISION\" != \"$NEW_REVISION\" ]]; then\n      echo \"\"\n      echo \"üÜï Revision changed: $CURRENT_REVISION ‚Üí $NEW_REVISION\"\n    fi\n  else\n    echo \"\"\n    echo \"‚ùå Reconciliation failed\"\n\n    # Show error details\n    kubectl get kustomization $NAME -n $NS -o yaml | \\\n      yq eval '.status.conditions[] | select(.type==\"Ready\")' -\n  fi\n  echo \"\"\n}\n```\n\n#### Reconcile HelmRelease\n\n```bash\nreconcile_helmrelease() {\n  local NAME=$1\n  local NS=$2\n\n  echo \"üîÑ Reconciling HelmRelease: $NAME\"\n  echo \"\"\n\n  # Get current status\n  CURRENT_STATUS=$(kubectl get helmrelease $NAME -n $NS \\\n    -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}')\n  CURRENT_CHART=$(kubectl get helmrelease $NAME -n $NS \\\n    -o jsonpath='{.status.lastAppliedRevision}')\n\n  echo \"Current Status: $CURRENT_STATUS\"\n  echo \"Current Chart: $CURRENT_CHART\"\n  echo \"\"\n\n  # Trigger reconciliation\n  flux reconcile helmrelease $NAME -n $NS\n\n  RECONCILE_EXIT=$?\n\n  if [[ $RECONCILE_EXIT -eq 0 ]]; then\n    echo \"\"\n    echo \"‚úÖ HelmRelease reconciled successfully\"\n\n    # Get updated status\n    sleep 3\n    NEW_STATUS=$(kubectl get helmrelease $NAME -n $NS \\\n      -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}')\n    NEW_CHART=$(kubectl get helmrelease $NAME -n $NS \\\n      -o jsonpath='{.status.lastAppliedRevision}')\n\n    echo \"New Status: $NEW_STATUS\"\n    echo \"New Chart: $NEW_CHART\"\n\n    # Check for changes\n    if [[ \"$CURRENT_CHART\" != \"$NEW_CHART\" ]]; then\n      echo \"\"\n      echo \"üÜï Chart version changed: $CURRENT_CHART ‚Üí $NEW_CHART\"\n    fi\n\n    # Show deployed resources\n    echo \"\"\n    echo \"üì¶ Deployed Resources:\"\n    kubectl get helmrelease $NAME -n $NS \\\n      -o jsonpath='{.status.lastReleaseRevision}' | \\\n      xargs -I {} helm list -n $NS --filter $NAME\n  else\n    echo \"\"\n    echo \"‚ùå Reconciliation failed\"\n\n    # Show error details\n    kubectl get helmrelease $NAME -n $NS -o yaml | \\\n      yq eval '.status.conditions[] | select(.type==\"Ready\")' -\n  fi\n  echo \"\"\n}\n```\n\n#### Reconcile All\n\n```bash\nreconcile_all() {\n  local NS=$1\n\n  echo \"üîÑ Reconciling all Flux resources in namespace: $NS\"\n  echo \"\"\n\n  # Reconcile all GitRepositories first\n  echo \"1Ô∏è‚É£ Reconciling Git sources...\"\n  flux get sources git -n $NS --no-header | while read NAME REVISION AGE STATUS; do\n    if [[ -n \"$NAME\" ]]; then\n      echo \"  üìÇ $NAME\"\n      flux reconcile source git $NAME -n $NS\n    fi\n  done\n  echo \"\"\n\n  # Reconcile all Kustomizations\n  echo \"2Ô∏è‚É£ Reconciling Kustomizations...\"\n  flux get kustomizations -n $NS --no-header | while read NAME REVISION STATUS AGE; do\n    if [[ -n \"$NAME\" ]]; then\n      echo \"  üìã $NAME\"\n      flux reconcile kustomization $NAME -n $NS\n    fi\n  done\n  echo \"\"\n\n  # Reconcile all HelmReleases\n  echo \"3Ô∏è‚É£ Reconciling HelmReleases...\"\n  flux get helmreleases -n $NS --no-header | while read NAME REVISION STATUS AGE; do\n    if [[ -n \"$NAME\" ]]; then\n      echo \"  üì¶ $NAME\"\n      flux reconcile helmrelease $NAME -n $NS\n    fi\n  done\n  echo \"\"\n\n  echo \"‚úÖ All resources reconciled\"\n}\n```\n\n### Phase 5: Execute Reconciliation\n\n```bash\ncase \"$RESOURCE_TYPE\" in\n  kustomization)\n    if [[ -n \"$RESOURCE_NAME\" ]]; then\n      reconcile_kustomization \"$RESOURCE_NAME\" \"$NAMESPACE\"\n    else\n      echo \"‚ùå Resource name required for kustomization\"\n      exit 1\n    fi\n    ;;\n\n  helmrelease)\n    if [[ -n \"$RESOURCE_NAME\" ]]; then\n      reconcile_helmrelease \"$RESOURCE_NAME\" \"$NAMESPACE\"\n    else\n      echo \"‚ùå Resource name required for helmrelease\"\n      exit 1\n    fi\n    ;;\n\n  all)\n    reconcile_all \"$NAMESPACE\"\n    ;;\n\n  *)\n    echo \"‚ùå Unknown resource type: $RESOURCE_TYPE\"\n    echo \"Valid types: kustomization, helmrelease, all\"\n    exit 1\n    ;;\nesac\n```\n\n### Phase 6: Post-Reconciliation Verification\n\n1. **Check resource health**:\n   ```bash\n   echo \"üè• Health Check:\"\n   echo \"\"\n\n   # Get all Flux resources status\n   flux get all -n $NAMESPACE\n   ```\n\n2. **Show recent events**:\n   ```bash\n   echo \"\"\n   echo \"üìÖ Recent Events:\"\n   kubectl get events -n $NAMESPACE \\\n     --sort-by='.lastTimestamp' \\\n     --field-selector involvedObject.kind=Kustomization \\\n     | tail -10\n\n   kubectl get events -n $NAMESPACE \\\n     --sort-by='.lastTimestamp' \\\n     --field-selector involvedObject.kind=HelmRelease \\\n     | tail -10\n   ```\n\n3. **Check for failures**:\n   ```bash\n   FAILED_KUST=$(flux get kustomizations -n $NAMESPACE | grep False | wc -l)\n   FAILED_HELM=$(flux get helmreleases -n $NAMESPACE | grep False | wc -l)\n\n   if [[ $FAILED_KUST -gt 0 || $FAILED_HELM -gt 0 ]]; then\n     echo \"\"\n     echo \"‚ö†Ô∏è  Failed Resources:\"\n     [[ $FAILED_KUST -gt 0 ]] && echo \"  - Kustomizations: $FAILED_KUST\"\n     [[ $FAILED_HELM -gt 0 ]] && echo \"  - HelmReleases: $FAILED_HELM\"\n     echo \"\"\n     echo \"Investigate:\"\n     echo \"  flux logs --level=error\"\n   fi\n   ```\n\n### Phase 7: Monitoring and Next Steps\n\n```\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìã Next Steps\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nMonitor reconciliation:\n  flux get all -n <namespace> --watch\n\nView logs:\n  flux logs --follow\n  flux logs --kind=Kustomization --name=<name>\n  flux logs --kind=HelmRelease --name=<name>\n\nSuspend reconciliation:\n  flux suspend kustomization <name>\n  flux suspend helmrelease <name>\n\nResume reconciliation:\n  flux resume kustomization <name>\n  flux resume helmrelease <name>\n\nExport resources:\n  flux export kustomization <name> > kust.yaml\n  flux export helmrelease <name> > helm.yaml\n```\n\n## Error Handling\n\n1. **Reconciliation timeout**:\n   ```\n   ‚è±Ô∏è  Reconciliation timed out\n\n   Check resource status:\n     flux get kustomization <name>\n     kubectl describe kustomization <name>\n\n   View logs:\n     flux logs --kind=Kustomization --name=<name>\n   ```\n\n2. **Source not ready**:\n   ```\n   ‚ùå Git source not ready\n\n   Check source:\n     flux get sources git\n     kubectl describe gitrepository <source-name>\n\n   Reconcile source:\n     flux reconcile source git <source-name>\n\n   Check credentials:\n     kubectl get secret -n flux-system\n   ```\n\n3. **HelmRelease install failed**:\n   ```\n   ‚ùå Helm install/upgrade failed\n\n   Check Helm status:\n     helm list -n <namespace>\n     helm history <release-name> -n <namespace>\n\n   View HelmRelease status:\n     kubectl get helmrelease <name> -o yaml\n\n   Rollback:\n     helm rollback <release-name> -n <namespace>\n   ```\n\n## Best Practices\n\n1. **Reconciliation Strategy**:\n   - Use automatic reconciliation for dev/staging\n   - Manual reconciliation for production\n   - Set appropriate intervals (default: 5m)\n   - Use webhooks for faster updates\n\n2. **Git Repository**:\n   - Use dedicated branches per environment\n   - Tag releases for rollback\n   - Keep kustomization manifests in Git\n   - Use secrets management (Sealed Secrets, SOPS)\n\n3. **Monitoring**:\n   - Set up alerts for failed reconciliations\n   - Monitor Flux controller metrics\n   - Use Grafana dashboards\n   - Check notification controllers\n\n4. **Progressive Delivery**:\n   - Use Flagger for canary deployments\n   - Implement health checks\n   - Define rollback policies\n   - Monitor deployment metrics\n\n## References\n\n- **Flux**: https://fluxcd.io/\n- **Reconciliation**: https://fluxcd.io/flux/components/kustomize/kustomization/\n- **HelmRelease**: https://fluxcd.io/flux/components/helm/helmreleases/\n- **Best Practices**: https://fluxcd.io/flux/guides/"
              },
              {
                "name": "/helm-deploy",
                "description": "Deploy applications using Helm charts with intelligent validation and rollback",
                "path": "plugins/gitops/commands/helm-deploy.md",
                "frontmatter": {
                  "name": "helm-deploy",
                  "description": "Deploy applications using Helm charts with intelligent validation and rollback",
                  "category": "gitops-deployment",
                  "tools": [
                    "Bash(helm:*)",
                    "Bash(kubectl:*)",
                    "Read(.*\\.yaml)",
                    "Read(.*\\.yml)",
                    "Read(.*/Chart.yaml)",
                    "Read(.*/values.yaml)",
                    "Write(.*\\.yaml)"
                  ],
                  "parameters": [
                    {
                      "name": "chart",
                      "description": "Helm chart name or path",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "release",
                      "description": "Release name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "namespace",
                      "description": "Target namespace",
                      "required": false,
                      "type": "string",
                      "default": "default"
                    },
                    {
                      "name": "values",
                      "description": "Values file path",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "set",
                      "description": "Set values on command line (key=value)",
                      "required": false,
                      "type": "array"
                    },
                    {
                      "name": "create-namespace",
                      "description": "Create namespace if it doesn't exist",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "dry-run",
                      "description": "Simulate deployment without applying",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "wait",
                      "description": "Wait for resources to be ready",
                      "required": false,
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "timeout",
                      "description": "Timeout for waiting (e.g., 5m, 10m)",
                      "required": false,
                      "type": "string",
                      "default": "5m"
                    }
                  ],
                  "examples": [
                    "helm-deploy --chart nginx --release my-nginx --namespace web",
                    "helm-deploy --chart ./my-chart --release my-app --values prod-values.yaml",
                    "helm-deploy --chart bitnami/postgresql --release db --set postgresqlPassword=secret123",
                    "helm-deploy --chart ./app --release my-app --dry-run"
                  ]
                },
                "content": "# Helm Chart Deployment\n\nDeploy applications to Kubernetes using Helm charts with comprehensive validation, health checks, and intelligent rollback capabilities.\n\n## Your Role\n\nYou are a Helm deployment specialist focused on:\n- Chart validation and best practices\n- Values configuration and templating\n- Deployment health monitoring\n- Rollback strategies\n- Resource dependency management\n\n## Task Workflow\n\n### Phase 1: Prerequisites Check\n\n1. **Verify Helm installation**:\n   ```bash\n   helm version --short || {\n     echo \"‚ùå Helm not installed\"\n     echo \"Install: https://helm.sh/docs/intro/install/\"\n     exit 1\n   }\n   ```\n\n2. **Verify kubectl connectivity**:\n   ```bash\n   kubectl cluster-info || {\n     echo \"‚ùå Not connected to a Kubernetes cluster\"\n     exit 1\n   }\n   ```\n\n3. **Check namespace**:\n   ```bash\n   if ! kubectl get namespace $NAMESPACE &>/dev/null; then\n     if [[ \"$CREATE_NAMESPACE\" == \"true\" ]]; then\n       kubectl create namespace $NAMESPACE\n       echo \"‚úÖ Created namespace: $NAMESPACE\"\n     else\n       echo \"‚ùå Namespace '$NAMESPACE' does not exist\"\n       echo \"Use --create-namespace to create it automatically\"\n       exit 1\n     fi\n   fi\n   ```\n\n### Phase 2: Chart Resolution\n\n1. **Determine chart type**:\n\n   **Local chart** (path starts with . or /):\n   ```bash\n   if [[ -f \"$CHART/Chart.yaml\" ]]; then\n     CHART_TYPE=\"local\"\n     CHART_PATH=\"$CHART\"\n   else\n     echo \"‚ùå Chart.yaml not found in $CHART\"\n     exit 1\n   fi\n   ```\n\n   **Repository chart** (format: repo/chart):\n   ```bash\n   if [[ \"$CHART\" == *\"/\"* ]]; then\n     CHART_TYPE=\"repository\"\n     REPO_NAME=\"${CHART%%/*}\"\n     CHART_NAME=\"${CHART##*/}\"\n\n     # Update repository\n     helm repo update $REPO_NAME 2>/dev/null || {\n       echo \"‚ö†Ô∏è  Repository '$REPO_NAME' not found\"\n       echo \"Add repository first: helm repo add <name> <url>\"\n       exit 1\n     }\n   fi\n   ```\n\n   **Chart name only** (search in added repos):\n   ```bash\n   if [[ \"$CHART\" != *\"/\"* && ! -d \"$CHART\" ]]; then\n     CHART_TYPE=\"search\"\n\n     # Search for chart in repositories\n     SEARCH_RESULTS=$(helm search repo $CHART --output json)\n\n     if [[ $(echo \"$SEARCH_RESULTS\" | jq 'length') -eq 0 ]]; then\n       echo \"‚ùå Chart '$CHART' not found in any repository\"\n       echo \"Search all repositories: helm search repo $CHART\"\n       exit 1\n     fi\n\n     # Use first result\n     CHART_FULL=$(echo \"$SEARCH_RESULTS\" | jq -r '.[0].name')\n     echo \"üì¶ Found chart: $CHART_FULL\"\n   fi\n   ```\n\n2. **Display chart information**:\n   ```bash\n   helm show chart $CHART_PATH 2>/dev/null || helm show chart $CHART_FULL\n\n   # Extract key information\n   CHART_VERSION=$(helm show chart $CHART | yq eval '.version' -)\n   APP_VERSION=$(helm show chart $CHART | yq eval '.appVersion' -)\n   DESCRIPTION=$(helm show chart $CHART | yq eval '.description' -)\n\n   echo \"\"\n   echo \"üìã Chart Details:\"\n   echo \"  Name: $CHART\"\n   echo \"  Version: $CHART_VERSION\"\n   echo \"  App Version: $APP_VERSION\"\n   echo \"  Description: $DESCRIPTION\"\n   ```\n\n### Phase 3: Values Configuration\n\n1. **Load values files**:\n   ```bash\n   VALUES_ARGS=\"\"\n\n   if [[ -n \"$VALUES_FILE\" ]]; then\n     if [[ ! -f \"$VALUES_FILE\" ]]; then\n       echo \"‚ùå Values file not found: $VALUES_FILE\"\n       exit 1\n     fi\n\n     VALUES_ARGS=\"-f $VALUES_FILE\"\n     echo \"üìÑ Using values file: $VALUES_FILE\"\n   fi\n   ```\n\n2. **Process --set arguments**:\n   ```bash\n   SET_ARGS=\"\"\n   if [[ ${#SET_VALUES[@]} -gt 0 ]]; then\n     for kv in \"${SET_VALUES[@]}\"; do\n       SET_ARGS=\"$SET_ARGS --set $kv\"\n     done\n     echo \"‚öôÔ∏è  Overriding values: ${SET_VALUES[*]}\"\n   fi\n   ```\n\n3. **Show merged values** (if requested):\n   ```bash\n   if [[ \"$SHOW_VALUES\" == \"true\" ]]; then\n     echo \"\"\n     echo \"üìã Final Values (after merging):\"\n     helm template $RELEASE $CHART $VALUES_ARGS $SET_ARGS --namespace $NAMESPACE | \\\n       helm get values $RELEASE --all 2>/dev/null || \\\n       helm show values $CHART\n   fi\n   ```\n\n### Phase 4: Pre-Deployment Validation\n\n1. **Template validation** (dry-run):\n   ```bash\n   echo \"\"\n   echo \"üîç Validating chart templates...\"\n\n   TEMPLATE_OUTPUT=$(helm template $RELEASE $CHART \\\n     $VALUES_ARGS $SET_ARGS \\\n     --namespace $NAMESPACE \\\n     --validate 2>&1)\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Template validation failed:\"\n     echo \"$TEMPLATE_OUTPUT\"\n     exit 1\n   fi\n\n   echo \"‚úÖ Chart templates are valid\"\n   ```\n\n2. **Resource validation**:\n   ```bash\n   # Count resources to be created\n   RESOURCE_COUNT=$(echo \"$TEMPLATE_OUTPUT\" | grep -c \"^kind:\")\n\n   echo \"üìä Resources to be deployed: $RESOURCE_COUNT\"\n   echo \"\"\n   echo \"Resource Types:\"\n   echo \"$TEMPLATE_OUTPUT\" | grep \"^kind:\" | sort | uniq -c | \\\n     awk '{printf \"  - %s: %d\\n\", $2, $1}'\n   ```\n\n3. **Check for existing release**:\n   ```bash\n   if helm list -n $NAMESPACE | grep -q \"^$RELEASE\"; then\n     EXISTING_VERSION=$(helm list -n $NAMESPACE -o json | \\\n       jq -r \".[] | select(.name==\\\"$RELEASE\\\") | .chart\")\n\n     echo \"\"\n     echo \"‚ö†Ô∏è  Release '$RELEASE' already exists (version: $EXISTING_VERSION)\"\n     echo \"\"\n     read -p \"Upgrade existing release? [y/N]: \" UPGRADE\n\n     if [[ ! \"$UPGRADE\" =~ ^[Yy]$ ]]; then\n       echo \"‚ùå Deployment cancelled\"\n       echo \"Use helm-upgrade command to upgrade existing releases\"\n       exit 0\n     fi\n\n     OPERATION=\"upgrade\"\n   else\n     OPERATION=\"install\"\n   fi\n   ```\n\n### Phase 5: Deployment\n\n1. **Execute Helm deployment**:\n\n   **For new installation**:\n   ```bash\n   if [[ \"$DRY_RUN\" == \"true\" ]]; then\n     echo \"üß™ DRY RUN MODE - No changes will be applied\"\n     echo \"\"\n\n     helm install $RELEASE $CHART \\\n       --namespace $NAMESPACE \\\n       $VALUES_ARGS $SET_ARGS \\\n       --dry-run --debug\n   else\n     echo \"üöÄ Deploying release '$RELEASE'...\"\n     echo \"\"\n\n     helm install $RELEASE $CHART \\\n       --namespace $NAMESPACE \\\n       $VALUES_ARGS $SET_ARGS \\\n       ${CREATE_NAMESPACE:+--create-namespace} \\\n       ${WAIT:+--wait --timeout $TIMEOUT} \\\n       --output json | tee /tmp/helm-install-$RELEASE.json\n\n     if [[ $? -eq 0 ]]; then\n       echo \"\"\n       echo \"‚úÖ Release '$RELEASE' deployed successfully!\"\n     else\n       echo \"\"\n       echo \"‚ùå Deployment failed\"\n       exit 1\n     fi\n   fi\n   ```\n\n   **For upgrade**:\n   ```bash\n   if [[ \"$DRY_RUN\" == \"true\" ]]; then\n     echo \"üß™ DRY RUN MODE - No changes will be applied\"\n     echo \"\"\n\n     helm upgrade $RELEASE $CHART \\\n       --namespace $NAMESPACE \\\n       $VALUES_ARGS $SET_ARGS \\\n       --dry-run --debug\n   else\n     echo \"‚¨ÜÔ∏è  Upgrading release '$RELEASE'...\"\n     echo \"\"\n\n     helm upgrade $RELEASE $CHART \\\n       --namespace $NAMESPACE \\\n       $VALUES_ARGS $SET_ARGS \\\n       ${WAIT:+--wait --timeout $TIMEOUT} \\\n       --output json | tee /tmp/helm-upgrade-$RELEASE.json\n\n     if [[ $? -eq 0 ]]; then\n       echo \"\"\n       echo \"‚úÖ Release '$RELEASE' upgraded successfully!\"\n     else\n       echo \"\"\n       echo \"‚ùå Upgrade failed\"\n       echo \"Rollback: helm rollback $RELEASE -n $NAMESPACE\"\n       exit 1\n     fi\n   fi\n   ```\n\n### Phase 6: Post-Deployment Verification\n\n1. **Show release status**:\n   ```bash\n   echo \"\"\n   echo \"üìä Release Status:\"\n   helm status $RELEASE -n $NAMESPACE\n   ```\n\n2. **Check deployed resources**:\n   ```bash\n   echo \"\"\n   echo \"üîç Deployed Resources:\"\n\n   # Get all resources from release\n   helm get manifest $RELEASE -n $NAMESPACE | \\\n     kubectl get -f - --show-kind --show-labels -o wide 2>/dev/null || \\\n     echo \"‚ö†Ô∏è  Some resources may not be ready yet\"\n   ```\n\n3. **Health checks**:\n   ```bash\n   echo \"\"\n   echo \"üè• Health Checks:\"\n\n   # Check pods\n   PODS=$(kubectl get pods -n $NAMESPACE -l \"app.kubernetes.io/instance=$RELEASE\" \\\n     --no-headers 2>/dev/null)\n\n   if [[ -n \"$PODS\" ]]; then\n     TOTAL_PODS=$(echo \"$PODS\" | wc -l)\n     READY_PODS=$(echo \"$PODS\" | grep \"Running\" | grep -E \"([0-9]+)/\\1\" | wc -l)\n\n     echo \"  Pods: $READY_PODS/$TOTAL_PODS ready\"\n\n     if [[ $READY_PODS -lt $TOTAL_PODS ]]; then\n       echo \"\"\n       echo \"‚ö†Ô∏è  Not all pods are ready:\"\n       echo \"$PODS\" | while read POD STATUS READY AGE; do\n         if [[ \"$STATUS\" != \"Running\" ]] || [[ ! \"$READY\" =~ ([0-9]+)/\\1 ]]; then\n           echo \"    - $POD: $STATUS (Ready: $READY)\"\n         fi\n       done\n     fi\n   fi\n\n   # Check services\n   SERVICES=$(kubectl get services -n $NAMESPACE -l \"app.kubernetes.io/instance=$RELEASE\" \\\n     --no-headers 2>/dev/null)\n\n   if [[ -n \"$SERVICES\" ]]; then\n     SERVICE_COUNT=$(echo \"$SERVICES\" | wc -l)\n     echo \"  Services: $SERVICE_COUNT deployed\"\n   fi\n\n   # Check ingresses/routes\n   INGRESSES=$(kubectl get ingress -n $NAMESPACE -l \"app.kubernetes.io/instance=$RELEASE\" \\\n     --no-headers 2>/dev/null)\n\n   if [[ -n \"$INGRESSES\" ]]; then\n     INGRESS_COUNT=$(echo \"$INGRESSES\" | wc -l)\n     echo \"  Ingresses: $INGRESS_COUNT deployed\"\n\n     echo \"\"\n     echo \"üåê External Access:\"\n     echo \"$INGRESSES\" | while read NAME CLASS HOSTS ADDRESS PORTS AGE; do\n       echo \"    - https://$HOSTS\"\n     done\n   fi\n   ```\n\n4. **Show notes**:\n   ```bash\n   echo \"\"\n   echo \"üìù Release Notes:\"\n   helm get notes $RELEASE -n $NAMESPACE\n   ```\n\n### Phase 7: Monitoring and Next Steps\n\n1. **Provide monitoring commands**:\n   ```\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n   üìã Next Steps\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n   Monitor deployment:\n     kubectl get pods -n <namespace> -l app.kubernetes.io/instance=<release> --watch\n\n   View logs:\n     kubectl logs -n <namespace> -l app.kubernetes.io/instance=<release> --tail=100 -f\n\n   Check release history:\n     helm history <release> -n <namespace>\n\n   Upgrade release:\n     helm-upgrade --release <release> --namespace <namespace>\n\n   Rollback if needed:\n     helm rollback <release> -n <namespace>\n\n   Uninstall release:\n     helm uninstall <release> -n <namespace>\n   ```\n\n2. **Set up monitoring** (if monitoring enabled):\n   ```bash\n   # Create ServiceMonitor for Prometheus (if Prometheus Operator is installed)\n   if kubectl get crd servicemonitors.monitoring.coreos.com &>/dev/null; then\n     echo \"\"\n     echo \"üí° Prometheus Operator detected. You can create a ServiceMonitor:\"\n     cat <<EOF\n   apiVersion: monitoring.coreos.com/v1\n   kind: ServiceMonitor\n   metadata:\n     name: $RELEASE\n     namespace: $NAMESPACE\n   spec:\n     selector:\n       matchLabels:\n         app.kubernetes.io/instance: $RELEASE\n     endpoints:\n     - port: metrics\n       interval: 30s\n   EOF\n   fi\n   ```\n\n## Error Handling\n\n1. **Chart not found**:\n   ```\n   ‚ùå Chart '<chart>' not found\n\n   Search for charts:\n     helm search repo <keyword>\n     helm search hub <keyword>\n\n   Add a repository:\n     helm repo add bitnami https://charts.bitnami.com/bitnami\n     helm repo add stable https://charts.helm.sh/stable\n   ```\n\n2. **Template rendering errors**:\n   ```\n   ‚ùå Failed to render chart templates\n\n   Common causes:\n   - Invalid values in values.yaml\n   - Missing required values\n   - Syntax errors in templates\n\n   Debug:\n     helm template <release> <chart> --debug\n     helm lint <chart>\n   ```\n\n3. **Deployment timeout**:\n   ```\n   ‚è±Ô∏è  Deployment timed out after <timeout>\n\n   Check pod status:\n     kubectl get pods -n <namespace> -l app.kubernetes.io/instance=<release>\n\n   View pod logs:\n     kubectl logs -n <namespace> <pod-name>\n\n   Increase timeout:\n     helm-deploy --chart <chart> --release <release> --timeout 10m\n   ```\n\n4. **Resource conflicts**:\n   ```\n   ‚ùå Resource already exists\n\n   This usually happens when:\n   - A previous installation wasn't fully removed\n   - Resources are managed by multiple Helm releases\n\n   Solutions:\n   1. Delete conflicting resources manually\n   2. Use different release name\n   3. Change namespace\n   ```\n\n## Best Practices\n\n1. **Version Control**:\n   - Store values files in Git\n   - Use semantic versioning for charts\n   - Tag releases in Git to match Helm releases\n\n2. **Values Management**:\n   - Separate values files per environment (dev, staging, prod)\n   - Use --set sparingly, prefer values files\n   - Document all custom values\n\n3. **Security**:\n   - Don't store secrets in values files\n   - Use Kubernetes secrets or external secret managers\n   - Review chart templates before deployment\n   - Enable RBAC for Helm\n\n4. **Deployment Strategy**:\n   - Always test with --dry-run first\n   - Use --wait to ensure successful deployment\n   - Keep deployment history (helm history)\n   - Plan rollback strategy before upgrading\n\n## References\n\n- **Helm Docs**: https://helm.sh/docs/\n- **Chart Best Practices**: https://helm.sh/docs/chart_best_practices/\n- **Helm Hub**: https://artifacthub.io/\n- **Bitnami Charts**: https://github.com/bitnami/charts"
              },
              {
                "name": "/kustomize-apply",
                "description": "Apply Kustomize overlays with validation and intelligent resource management",
                "path": "plugins/gitops/commands/kustomize-apply.md",
                "frontmatter": {
                  "name": "kustomize-apply",
                  "description": "Apply Kustomize overlays with validation and intelligent resource management",
                  "category": "gitops-deployment",
                  "tools": [
                    "Bash(kubectl:*)",
                    "Bash(kustomize:*)",
                    "Read(.*\\.yaml)",
                    "Read(.*\\.yml)",
                    "Read(.*/kustomization.yaml)"
                  ],
                  "parameters": [
                    {
                      "name": "path",
                      "description": "Path to kustomization directory",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "namespace",
                      "description": "Override namespace",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "dry-run",
                      "description": "Show what would be applied without making changes",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "prune",
                      "description": "Prune resources not in current configuration",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "wait",
                      "description": "Wait for resources to be ready",
                      "required": false,
                      "type": "boolean",
                      "default": true
                    }
                  ],
                  "examples": [
                    "kustomize-apply --path ./overlays/production",
                    "kustomize-apply --path ./base --namespace my-app --dry-run",
                    "kustomize-apply --path ./overlays/dev --prune --wait"
                  ]
                },
                "content": "# Kustomize Apply\n\nApply Kubernetes manifests using Kustomize overlays with comprehensive validation, diff analysis, and intelligent resource management.\n\n## Your Role\n\nYou are a Kustomize deployment specialist focusing on:\n- Overlay validation and best practices\n- Resource patching and transformations\n- Configuration management across environments\n- Intelligent resource pruning\n- GitOps workflows\n\n## Task Workflow\n\n### Phase 1: Prerequisites Check\n\n1. **Verify kubectl with kustomize**:\n   ```bash\n   kubectl version --client | grep -q \"Kustomize\" || {\n     echo \"‚ùå kubectl with Kustomize support not found\"\n     echo \"Install: https://kubernetes.io/docs/tasks/tools/\"\n     exit 1\n   }\n   ```\n\n2. **Check kustomization file**:\n   ```bash\n   if [[ ! -f \"$PATH/kustomization.yaml\" && ! -f \"$PATH/kustomization.yml\" ]]; then\n     echo \"‚ùå No kustomization.yaml found in $PATH\"\n     exit 1\n   fi\n\n   echo \"‚úÖ Found kustomization file\"\n   ```\n\n3. **Verify cluster connectivity**:\n   ```bash\n   kubectl cluster-info || {\n     echo \"‚ùå Not connected to a Kubernetes cluster\"\n     exit 1\n   }\n   ```\n\n### Phase 2: Kustomization Analysis\n\n1. **Parse kustomization file**:\n   ```bash\n   KUST_FILE=\"$PATH/kustomization.yaml\"\n   [[ ! -f \"$KUST_FILE\" ]] && KUST_FILE=\"$PATH/kustomization.yml\"\n\n   echo \"üìã Kustomization Configuration:\"\n   echo \"\"\n\n   # Extract key information\n   NAMESPACE=$(yq eval '.namespace // \"default\"' $KUST_FILE)\n   NAME_PREFIX=$(yq eval '.namePrefix // \"\"' $KUST_FILE)\n   NAME_SUFFIX=$(yq eval '.nameSuffix // \"\"' $KUST_FILE)\n   COMMON_LABELS=$(yq eval '.commonLabels // {}' $KUST_FILE)\n\n   echo \"  Namespace: ${NAMESPACE_OVERRIDE:-$NAMESPACE}\"\n   [[ -n \"$NAME_PREFIX\" ]] && echo \"  Name Prefix: $NAME_PREFIX\"\n   [[ -n \"$NAME_SUFFIX\" ]] && echo \"  Name Suffix: $NAME_SUFFIX\"\n   ```\n\n2. **List resources**:\n   ```bash\n   echo \"\"\n   echo \"üì¶ Base Resources:\"\n   yq eval '.resources[]' $KUST_FILE | while read resource; do\n     echo \"  - $resource\"\n   done\n\n   # Check for patches\n   PATCHES=$(yq eval '.patches // [] | length' $KUST_FILE)\n   if [[ $PATCHES -gt 0 ]]; then\n     echo \"\"\n     echo \"üîß Patches: $PATCHES\"\n     yq eval '.patches[]' $KUST_FILE | while read patch; do\n       echo \"  - $patch\"\n     done\n   fi\n\n   # Check for strategic merge patches\n   STRATEGIC_MERGE=$(yq eval '.patchesStrategicMerge // [] | length' $KUST_FILE)\n   if [[ $STRATEGIC_MERGE -gt 0 ]]; then\n     echo \"\"\n     echo \"üîÄ Strategic Merge Patches: $STRATEGIC_MERGE\"\n   fi\n\n   # Check for JSON 6902 patches\n   JSON_PATCHES=$(yq eval '.patchesJson6902 // [] | length' $KUST_FILE)\n   if [[ $JSON_PATCHES -gt 0 ]]; then\n     echo \"\"\n     echo \"üîß JSON 6902 Patches: $JSON_PATCHES\"\n   fi\n   ```\n\n3. **Check for ConfigMap and Secret generators**:\n   ```bash\n   CONFIGMAP_GEN=$(yq eval '.configMapGenerator // [] | length' $KUST_FILE)\n   SECRET_GEN=$(yq eval '.secretGenerator // [] | length' $KUST_FILE)\n\n   if [[ $CONFIGMAP_GEN -gt 0 ]]; then\n     echo \"\"\n     echo \"‚öôÔ∏è  ConfigMap Generators: $CONFIGMAP_GEN\"\n   fi\n\n   if [[ $SECRET_GEN -gt 0 ]]; then\n     echo \"\"\n     echo \"üîê Secret Generators: $SECRET_GEN\"\n   fi\n   ```\n\n### Phase 3: Build and Validate\n\n1. **Build Kustomization**:\n   ```bash\n   echo \"\"\n   echo \"üî® Building Kustomization...\"\n\n   KUST_OUTPUT=$(kubectl kustomize $PATH ${NAMESPACE_OVERRIDE:+--namespace=$NAMESPACE_OVERRIDE} 2>&1)\n\n   if [[ $? -ne 0 ]]; then\n     echo \"‚ùå Failed to build kustomization:\"\n     echo \"$KUST_OUTPUT\"\n     exit 1\n   fi\n\n   echo \"‚úÖ Kustomization built successfully\"\n   ```\n\n2. **Validate resources**:\n   ```bash\n   echo \"\"\n   echo \"üîç Validating resources...\"\n\n   # Count resources\n   RESOURCE_COUNT=$(echo \"$KUST_OUTPUT\" | grep -c \"^kind:\")\n\n   echo \"üìä Total Resources: $RESOURCE_COUNT\"\n   echo \"\"\n\n   # Group by kind\n   echo \"Resource Types:\"\n   echo \"$KUST_OUTPUT\" | grep \"^kind:\" | sort | uniq -c | \\\n     awk '{printf \"  - %s: %d\\n\", $2, $1}'\n\n   # Validate YAML syntax\n   echo \"$KUST_OUTPUT\" | kubectl apply --dry-run=client -f - &>/dev/null\n\n   if [[ $? -eq 0 ]]; then\n     echo \"\"\n     echo \"‚úÖ All resources are valid\"\n   else\n     echo \"\"\n     echo \"‚ùå Resource validation failed\"\n     exit 1\n   fi\n   ```\n\n3. **Extract resource names**:\n   ```bash\n   # Parse all resource names for tracking\n   RESOURCES=$(echo \"$KUST_OUTPUT\" | yq eval -N '\n     .kind + \"/\" + .metadata.name +\n     (if .metadata.namespace then \" -n \" + .metadata.namespace else \"\" end)\n   ' -)\n   ```\n\n### Phase 4: Diff Analysis\n\n1. **Compare with cluster state**:\n   ```bash\n   echo \"\"\n   echo \"üìä Analyzing changes...\"\n\n   # Get server-side diff\n   DIFF_OUTPUT=$(echo \"$KUST_OUTPUT\" | kubectl diff -f - 2>&1)\n   DIFF_EXIT_CODE=$?\n\n   if [[ $DIFF_EXIT_CODE -eq 0 ]]; then\n     echo \"‚ÑπÔ∏è  No changes detected - cluster is up to date\"\n   elif [[ $DIFF_EXIT_CODE -eq 1 ]]; then\n     echo \"‚ö†Ô∏è  Changes detected:\"\n     echo \"\"\n     echo \"$DIFF_OUTPUT\"\n   else\n     echo \"‚ö†Ô∏è  Could not generate diff (some resources may not exist yet)\"\n   fi\n   ```\n\n2. **Categorize changes**:\n   ```bash\n   # Count new, modified, and deleted resources\n   NEW_RESOURCES=$(echo \"$RESOURCES\" | while read res; do\n     kubectl get $res &>/dev/null || echo \"$res\"\n   done | wc -l)\n\n   MODIFIED_RESOURCES=$(echo \"$DIFF_OUTPUT\" | grep -c \"^¬±\" || echo \"0\")\n\n   echo \"\"\n   echo \"Change Summary:\"\n   echo \"  New Resources: $NEW_RESOURCES\"\n   echo \"  Modified Resources: $MODIFIED_RESOURCES\"\n\n   if [[ \"$PRUNE\" == \"true\" ]]; then\n     # Calculate resources to be deleted (simplified)\n     echo \"  ‚ö†Ô∏è  Prune enabled - orphaned resources will be removed\"\n   fi\n   ```\n\n### Phase 5: Apply Configuration\n\n1. **Apply or dry-run**:\n\n   **Dry Run Mode**:\n   ```bash\n   if [[ \"$DRY_RUN\" == \"true\" ]]; then\n     echo \"\"\n     echo \"üß™ DRY RUN MODE - No changes will be applied\"\n     echo \"\"\n     echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n     echo \"Rendered Manifests:\"\n     echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n     echo \"$KUST_OUTPUT\"\n     exit 0\n   fi\n   ```\n\n   **Apply Mode**:\n   ```bash\n   echo \"\"\n   read -p \"Proceed with deployment? [y/N]: \" CONFIRM\n\n   if [[ ! \"$CONFIRM\" =~ ^[Yy]$ ]]; then\n     echo \"‚ùå Deployment cancelled\"\n     exit 0\n   fi\n\n   echo \"\"\n   echo \"üöÄ Applying kustomization...\"\n\n   # Build apply command\n   APPLY_CMD=\"kubectl apply -k $PATH\"\n   [[ -n \"$NAMESPACE_OVERRIDE\" ]] && APPLY_CMD=\"$APPLY_CMD --namespace=$NAMESPACE_OVERRIDE\"\n   [[ \"$PRUNE\" == \"true\" ]] && APPLY_CMD=\"$APPLY_CMD --prune\"\n   [[ \"$WAIT\" == \"true\" ]] && APPLY_CMD=\"$APPLY_CMD --wait\"\n\n   # Apply\n   APPLY_OUTPUT=$($APPLY_CMD 2>&1)\n   APPLY_EXIT_CODE=$?\n\n   if [[ $APPLY_EXIT_CODE -eq 0 ]]; then\n     echo \"\"\n     echo \"‚úÖ Kustomization applied successfully!\"\n     echo \"\"\n     echo \"$APPLY_OUTPUT\"\n   else\n     echo \"\"\n     echo \"‚ùå Failed to apply kustomization:\"\n     echo \"$APPLY_OUTPUT\"\n     exit 1\n   fi\n   ```\n\n### Phase 6: Post-Deployment Verification\n\n1. **Verify resource status**:\n   ```bash\n   echo \"\"\n   echo \"üîç Verifying deployed resources...\"\n   echo \"\"\n\n   # Check each resource\n   echo \"$RESOURCES\" | while read resource; do\n     if kubectl get $resource &>/dev/null; then\n       STATUS=$(kubectl get $resource -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}' 2>/dev/null)\n\n       if [[ \"$STATUS\" == \"True\" ]]; then\n         echo \"  ‚úÖ $resource\"\n       else\n         echo \"  ‚è≥ $resource (not ready yet)\"\n       fi\n     else\n       echo \"  ‚ùå $resource (not found)\"\n     fi\n   done\n   ```\n\n2. **Check pod health**:\n   ```bash\n   echo \"\"\n   echo \"üè• Pod Health Check:\"\n\n   # Get all pods in the namespace\n   PODS=$(kubectl get pods -n ${NAMESPACE_OVERRIDE:-$NAMESPACE} --no-headers 2>/dev/null)\n\n   if [[ -n \"$PODS\" ]]; then\n     TOTAL_PODS=$(echo \"$PODS\" | wc -l)\n     RUNNING_PODS=$(echo \"$PODS\" | grep \"Running\" | wc -l)\n     READY_PODS=$(echo \"$PODS\" | grep \"Running\" | grep -E \"([0-9]+)/\\1\" | wc -l)\n\n     echo \"  Total Pods: $TOTAL_PODS\"\n     echo \"  Running: $RUNNING_PODS\"\n     echo \"  Ready: $READY_PODS\"\n\n     # Show pods with issues\n     if [[ $READY_PODS -lt $RUNNING_PODS ]]; then\n       echo \"\"\n       echo \"‚ö†Ô∏è  Pods with issues:\"\n       echo \"$PODS\" | while read POD READY STATUS RESTARTS AGE; do\n         if [[ \"$STATUS\" != \"Running\" ]] || [[ ! \"$READY\" =~ ([0-9]+)/\\1 ]]; then\n           echo \"    - $POD: $STATUS (Ready: $READY, Restarts: $RESTARTS)\"\n         fi\n       done\n     fi\n   fi\n   ```\n\n3. **Check services and endpoints**:\n   ```bash\n   echo \"\"\n   echo \"üåê Services:\"\n\n   SERVICES=$(kubectl get services -n ${NAMESPACE_OVERRIDE:-$NAMESPACE} --no-headers 2>/dev/null)\n\n   if [[ -n \"$SERVICES\" ]]; then\n     echo \"$SERVICES\" | while read SVC TYPE CLUSTER_IP EXTERNAL_IP PORT AGE; do\n       # Check endpoints\n       ENDPOINTS=$(kubectl get endpoints $SVC -n ${NAMESPACE_OVERRIDE:-$NAMESPACE} \\\n         -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null)\n\n       if [[ -n \"$ENDPOINTS\" ]]; then\n         ENDPOINT_COUNT=$(echo \"$ENDPOINTS\" | wc -w)\n         echo \"  ‚úÖ $SVC ($TYPE) - $ENDPOINT_COUNT endpoints\"\n       else\n         echo \"  ‚ö†Ô∏è  $SVC ($TYPE) - no endpoints\"\n       fi\n     done\n   fi\n   ```\n\n4. **Generate deployment report**:\n   ```bash\n   echo \"\"\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"Deployment Report\"\n   echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n   echo \"\"\n   echo \"Kustomization: $PATH\"\n   echo \"Namespace: ${NAMESPACE_OVERRIDE:-$NAMESPACE}\"\n   echo \"Resources Deployed: $RESOURCE_COUNT\"\n   echo \"Status: $(if [[ $READY_PODS -eq $TOTAL_PODS ]]; then echo \"‚úÖ Healthy\"; else echo \"‚ö†Ô∏è  Partially Ready\"; fi)\"\n   echo \"\"\n   ```\n\n### Phase 7: Monitoring and Next Steps\n\n1. **Provide monitoring commands**:\n   ```\n   üìã Next Steps:\n\n   Monitor resources:\n     kubectl get all -n <namespace>\n\n   Watch pod status:\n     kubectl get pods -n <namespace> --watch\n\n   View logs:\n     kubectl logs -n <namespace> -l <label-selector> --tail=100 -f\n\n   Update deployment:\n     # Edit your kustomization files, then:\n     kustomize-apply --path <path>\n\n   Rollback:\n     kubectl rollout undo deployment/<deployment-name> -n <namespace>\n\n   View applied manifests:\n     kubectl kustomize <path>\n   ```\n\n2. **GitOps recommendations**:\n   ```\n   üí° GitOps Best Practices:\n\n   1. Store kustomization files in Git\n   2. Use separate overlays for each environment\n   3. Automate deployments with CI/CD\n   4. Use ArgoCD or Flux for continuous sync\n\n   Set up ArgoCD:\n     cluster-code argocd-sync --path <path>\n\n   Set up Flux:\n     flux create kustomization <name> --source=<git-repo> --path=<path>\n   ```\n\n## Error Handling\n\n1. **Kustomization build errors**:\n   ```\n   ‚ùå Failed to build kustomization\n\n   Common causes:\n   - Invalid YAML syntax in patches\n   - Missing base resources\n   - Incorrect file paths in resources[]\n   - Invalid transformers\n\n   Debug:\n     kubectl kustomize <path> --enable-helm\n     kustomize build <path> --load-restrictor LoadRestrictionsNone\n   ```\n\n2. **Resource conflicts**:\n   ```\n   ‚ùå Resource already exists and is not managed by kustomize\n\n   Solutions:\n   1. Add management labels to existing resources\n   2. Delete existing resources manually\n   3. Use different resource names\n   4. Adopt resources with kubectl label\n   ```\n\n3. **Namespace issues**:\n   ```\n   ‚ö†Ô∏è  Namespace mismatch\n\n   Kustomization namespace: <namespace1>\n   Override namespace: <namespace2>\n\n   Note: --namespace flag overrides kustomization.yaml namespace setting\n   ```\n\n## Best Practices\n\n1. **Directory Structure**:\n   ```\n   my-app/\n   ‚îú‚îÄ‚îÄ base/\n   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml\n   ‚îÇ   ‚îú‚îÄ‚îÄ service.yaml\n   ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml\n   ‚îî‚îÄ‚îÄ overlays/\n       ‚îú‚îÄ‚îÄ dev/\n       ‚îÇ   ‚îú‚îÄ‚îÄ kustomization.yaml\n       ‚îÇ   ‚îî‚îÄ‚îÄ patches/\n       ‚îú‚îÄ‚îÄ staging/\n       ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml\n       ‚îî‚îÄ‚îÄ production/\n           ‚îú‚îÄ‚îÄ kustomization.yaml\n           ‚îî‚îÄ‚îÄ patches/\n   ```\n\n2. **Naming Convention**:\n   - Use namePrefix for environment distinction\n   - Use nameSuffix for versioning\n   - Apply commonLabels for resource grouping\n\n3. **Patch Strategy**:\n   - Use strategic merge for simple changes\n   - Use JSON 6902 for precise modifications\n   - Document patch rationale in comments\n\n4. **Secret Management**:\n   - Never commit secrets to Git\n   - Use secretGenerator with external files\n   - Consider sealed-secrets or external-secrets\n\n## References\n\n- **Kustomize Docs**: https://kustomize.io/\n- **Kubectl Kustomize**: https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/\n- **Best Practices**: https://kubectl.docs.kubernetes.io/guides/config_management/"
              }
            ],
            "skills": []
          },
          {
            "name": "cloud-aws",
            "description": "AWS cloud provider integration for EKS and ROSA cluster management with MCP server",
            "source": "./plugins/cloud-aws",
            "category": "cloud-provider",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install cloud-aws@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/aws-cluster-create",
                "description": "Create a new EKS or ROSA cluster on AWS",
                "path": "plugins/cloud-aws/commands/aws-cluster-create.md",
                "frontmatter": {
                  "name": "aws-cluster-create",
                  "description": "Create a new EKS or ROSA cluster on AWS",
                  "category": "cloud-provisioning",
                  "tools": [
                    "Bash(aws:*)",
                    "Bash(rosa:*)",
                    "Bash(kubectl:*)",
                    "Bash(oc:*)",
                    "Write(.*\\.yaml)",
                    "Write(.*\\.json)",
                    "Write(.*\\.tf)"
                  ],
                  "parameters": [
                    {
                      "name": "type",
                      "description": "Cluster type (eks or rosa)",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "name",
                      "description": "Cluster name",
                      "required": true,
                      "type": "string"
                    },
                    {
                      "name": "region",
                      "description": "AWS region",
                      "required": false,
                      "type": "string",
                      "default": "us-east-1"
                    },
                    {
                      "name": "version",
                      "description": "Kubernetes/OpenShift version",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "nodes",
                      "description": "Number of worker nodes",
                      "required": false,
                      "type": "integer",
                      "default": 2
                    },
                    {
                      "name": "instance-type",
                      "description": "EC2 instance type for nodes",
                      "required": false,
                      "type": "string"
                    },
                    {
                      "name": "multi-az",
                      "description": "Deploy across multiple availability zones",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "private",
                      "description": "Create private cluster",
                      "required": false,
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "output",
                      "description": "Output format (terraform, json, yaml)",
                      "required": false,
                      "type": "string",
                      "default": "json"
                    }
                  ],
                  "examples": [
                    "aws-cluster-create --type eks --name my-eks-cluster --region us-east-1",
                    "aws-cluster-create --type rosa --name my-rosa-cluster --region us-east-2 --nodes 3 --multi-az",
                    "aws-cluster-create --type eks --name prod-cluster --region us-west-2 --output terraform"
                  ]
                },
                "content": "# AWS Cluster Creation\n\nYou are a specialized agent for creating EKS (Elastic Kubernetes Service) and ROSA (Red Hat OpenShift Service on AWS) clusters on Amazon Web Services.\n\n## Your Role\n\nGuide users through creating production-ready clusters with best practices for:\n- Network architecture and security\n- IAM roles and policies\n- High availability and disaster recovery\n- Cost optimization\n- Compliance and governance\n\n## Task Workflow\n\n### Phase 1: Validation & Prerequisites\n\n1. **Check AWS CLI authentication**:\n   ```bash\n   aws sts get-caller-identity\n   ```\n   - Verify correct AWS account\n   - Check IAM permissions\n   - If not authenticated, instruct: `aws configure`\n\n2. **For ROSA clusters**, check ROSA CLI:\n   ```bash\n   rosa version\n   rosa verify permissions\n   rosa verify quota --region <region>\n   ```\n   - Install if missing: https://console.redhat.com/openshift/downloads\n   - Verify Red Hat account is linked\n   - Check AWS service quotas\n\n3. **Validate parameters**:\n   - Cluster type must be 'eks' or 'rosa'\n   - Cluster name: 1-100 chars, alphanumeric and hyphens\n   - Region must be valid AWS region\n   - Version must be available in region\n\n4. **Check available versions**:\n   - For EKS: `aws eks describe-addon-versions --region <region>`\n   - For ROSA: `rosa list versions`\n\n### Phase 2: Network Infrastructure Setup\n\n#### For EKS Clusters:\n\n1. **Check for existing VPC** or create new:\n   ```bash\n   # List existing VPCs\n   aws ec2 describe-vpcs --region <region>\n\n   # Create VPC if needed (using eksctl)\n   eksctl create cluster --name <name> \\\n     --region <region> \\\n     --version <version> \\\n     --vpc-cidr 10.0.0.0/16 \\\n     --dry-run\n\n   # Or manually create VPC\n   aws ec2 create-vpc --cidr-block 10.0.0.0/16 --region <region>\n   ```\n\n2. **Create subnets** (minimum 2, preferably across 3 AZs):\n   ```bash\n   # Public subnets (for load balancers)\n   aws ec2 create-subnet \\\n     --vpc-id <vpc-id> \\\n     --cidr-block 10.0.1.0/24 \\\n     --availability-zone <az-1>\n\n   aws ec2 create-subnet \\\n     --vpc-id <vpc-id> \\\n     --cidr-block 10.0.2.0/24 \\\n     --availability-zone <az-2>\n\n   # Private subnets (for worker nodes)\n   aws ec2 create-subnet \\\n     --vpc-id <vpc-id> \\\n     --cidr-block 10.0.101.0/24 \\\n     --availability-zone <az-1>\n\n   aws ec2 create-subnet \\\n     --vpc-id <vpc-id> \\\n     --cidr-block 10.0.102.0/24 \\\n     --availability-zone <az-2>\n   ```\n\n3. **Configure internet gateway and NAT**:\n   ```bash\n   # Internet Gateway for public subnets\n   aws ec2 create-internet-gateway\n   aws ec2 attach-internet-gateway \\\n     --vpc-id <vpc-id> \\\n     --internet-gateway-id <igw-id>\n\n   # NAT Gateway for private subnets\n   aws ec2 create-nat-gateway \\\n     --subnet-id <public-subnet-id> \\\n     --allocation-id <eip-allocation-id>\n   ```\n\n#### For ROSA Clusters:\n\n1. **Use existing VPC or let ROSA create**:\n   ```bash\n   # List VPCs\n   aws ec2 describe-vpcs --region <region>\n\n   # ROSA can create VPC automatically\n   # Or specify existing VPC with --subnet-ids\n   ```\n\n2. **Ensure VPC requirements**:\n   - CIDR block: /16 or larger\n   - DNS hostnames enabled\n   - DNS resolution enabled\n   - Subnets in at least 2 AZs\n\n### Phase 3: IAM Setup\n\n#### For EKS:\n\n1. **Create cluster IAM role**:\n   ```bash\n   # Create trust policy\n   cat > eks-cluster-role-trust-policy.json <<EOF\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Service\": \"eks.amazonaws.com\"\n         },\n         \"Action\": \"sts:AssumeRole\"\n       }\n     ]\n   }\n   EOF\n\n   # Create role\n   aws iam create-role \\\n     --role-name eksClusterRole-<cluster-name> \\\n     --assume-role-policy-document file://eks-cluster-role-trust-policy.json\n\n   # Attach policies\n   aws iam attach-role-policy \\\n     --role-name eksClusterRole-<cluster-name> \\\n     --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\n   ```\n\n2. **Create node IAM role**:\n   ```bash\n   # Node trust policy\n   cat > eks-node-role-trust-policy.json <<EOF\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Service\": \"ec2.amazonaws.com\"\n         },\n         \"Action\": \"sts:AssumeRole\"\n       }\n     ]\n   }\n   EOF\n\n   # Create role\n   aws iam create-role \\\n     --role-name eksNodeRole-<cluster-name> \\\n     --assume-role-policy-document file://eks-node-role-trust-policy.json\n\n   # Attach required policies\n   aws iam attach-role-policy \\\n     --role-name eksNodeRole-<cluster-name> \\\n     --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n\n   aws iam attach-role-policy \\\n     --role-name eksNodeRole-<cluster-name> \\\n     --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n\n   aws iam attach-role-policy \\\n     --role-name eksNodeRole-<cluster-name> \\\n     --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n   ```\n\n#### For ROSA:\n\nROSA handles IAM automatically with STS (recommended) or classic mode.\n\n### Phase 4: Cluster Creation\n\n#### For EKS Clusters:\n\n**Using eksctl (recommended)**:\n```bash\neksctl create cluster \\\n  --name <cluster-name> \\\n  --region <region> \\\n  --version <version> \\\n  --nodegroup-name standard-workers \\\n  --node-type <instance-type> \\\n  --nodes <node-count> \\\n  --nodes-min 1 \\\n  --nodes-max 10 \\\n  --managed \\\n  --with-oidc \\\n  --alb-ingress-access \\\n  --full-ecr-access \\\n  --tags \"ManagedBy=cluster-code,Environment=production\"\n```\n\n**Using AWS CLI (manual)**:\n```bash\n# Create control plane\naws eks create-cluster \\\n  --region <region> \\\n  --name <cluster-name> \\\n  --kubernetes-version <version> \\\n  --role-arn <cluster-role-arn> \\\n  --resources-vpc-config subnetIds=<subnet-ids>,securityGroupIds=<sg-ids> \\\n  --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' \\\n  --tags ManagedBy=cluster-code,Environment=production\n\n# Wait for cluster to be ACTIVE\naws eks wait cluster-active \\\n  --region <region> \\\n  --name <cluster-name>\n\n# Create node group\naws eks create-nodegroup \\\n  --cluster-name <cluster-name> \\\n  --nodegroup-name standard-workers \\\n  --scaling-config minSize=1,maxSize=10,desiredSize=<node-count> \\\n  --subnets <subnet-ids> \\\n  --instance-types <instance-type> \\\n  --node-role <node-role-arn> \\\n  --region <region>\n```\n\n#### For ROSA Clusters:\n\n**Hosted Control Plane (HyperShift) - Recommended**:\n```bash\nrosa create cluster \\\n  --cluster-name <cluster-name> \\\n  --region <region> \\\n  --version <version> \\\n  --compute-nodes <node-count> \\\n  --compute-machine-type <instance-type> \\\n  --hosted-cp \\\n  --sts \\\n  --mode auto \\\n  --yes\n```\n\n**Classic ROSA**:\n```bash\nrosa create cluster \\\n  --cluster-name <cluster-name> \\\n  --region <region> \\\n  --version <version> \\\n  --compute-nodes <node-count> \\\n  --compute-machine-type <instance-type> \\\n  --multi-az \\\n  --sts \\\n  --mode auto \\\n  --yes\n```\n\n### Phase 5: Monitor Creation Progress\n\n1. **Show creation status**:\n   ```\n   üöÄ Creating <type> cluster '<cluster-name>' in region '<region>'...\n\n   Typical creation times:\n   - EKS: 10-15 minutes\n   - ROSA (Hosted CP): 10-15 minutes\n   - ROSA (Classic): 30-40 minutes\n\n   Monitor progress:\n   - EKS: aws eks describe-cluster --name <cluster-name> --region <region>\n   - ROSA: rosa describe cluster --cluster <cluster-name>\n   ```\n\n2. **Wait for completion**:\n   - Poll cluster status every 30 seconds\n   - Show progress indicator\n   - Report errors immediately\n\n### Phase 6: Post-Creation Configuration\n\n1. **Get cluster credentials**:\n\n   **For EKS**:\n   ```bash\n   aws eks update-kubeconfig \\\n     --region <region> \\\n     --name <cluster-name>\n   ```\n\n   **For ROSA**:\n   ```bash\n   # Get admin credentials\n   rosa describe admin --cluster <cluster-name>\n\n   # Login with oc\n   rosa login --cluster <cluster-name>\n   ```\n\n2. **Verify cluster connectivity**:\n   ```bash\n   kubectl cluster-info\n   kubectl get nodes\n   kubectl get pods --all-namespaces\n   ```\n\n3. **Install essential add-ons** (EKS):\n   ```bash\n   # AWS Load Balancer Controller\n   kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\n\n   # EBS CSI Driver\n   kubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.25\"\n\n   # Cluster Autoscaler\n   kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml\n   ```\n\n4. **Install operators** (ROSA):\n   ```bash\n   # OpenShift GitOps\n   rosa install addon --cluster <cluster-name> openshift-gitops-operator\n\n   # OpenShift Pipelines\n   rosa install addon --cluster <cluster-name> openshift-pipelines-operator\n   ```\n\n5. **Display cluster information**:\n   ```\n   ‚úÖ Cluster created successfully!\n\n   Cluster Details:\n   - Name: <cluster-name>\n   - Type: <EKS/ROSA>\n   - Region: <region>\n   - Version: <version>\n   - Node Count: <nodes>\n   - API Endpoint: <api-endpoint>\n   - Console: <console-url> (ROSA only)\n\n   Next Steps:\n   1. Initialize cluster-code: cluster-code init --context <cluster-name>\n   2. Run diagnostics: cluster-code diagnose\n   3. Deploy applications: cluster-code helm-deploy / kustomize-apply\n   ```\n\n### Phase 7: Infrastructure as Code Output\n\nIf `--output terraform`, generate Terraform configuration:\n\n```hcl\n# terraform/main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"<region>\"\n}\n\n# EKS Module\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 19.0\"\n\n  cluster_name    = \"<cluster-name>\"\n  cluster_version = \"<version>\"\n\n  cluster_endpoint_public_access = true\n\n  vpc_id     = \"<vpc-id>\"\n  subnet_ids = [\"<subnet-ids>\"]\n\n  eks_managed_node_groups = {\n    main = {\n      min_size     = 1\n      max_size     = 10\n      desired_size = <node-count>\n\n      instance_types = [\"<instance-type>\"]\n    }\n  }\n\n  tags = {\n    ManagedBy = \"cluster-code\"\n  }\n}\n```\n\n## Error Handling\n\n### Common Errors & Solutions:\n\n1. **InsufficientPermissions**:\n   ```\n   ‚ùå IAM user/role lacks required permissions\n\n   EKS requires:\n   - eks:CreateCluster\n   - iam:CreateRole\n   - ec2:CreateVpc (if creating VPC)\n\n   ROSA requires:\n   - Full admin access or specific ROSA permissions\n   - Link Red Hat account: rosa login\n   ```\n\n2. **QuotaExceeded**:\n   ```\n   ‚ùå AWS service limits exceeded\n\n   Check limits:\n     aws service-quotas list-service-quotas --service-code eks\n     rosa verify quota --region <region>\n\n   Request increase via AWS Console or Support\n   ```\n\n3. **UnsupportedAvailabilityZone**:\n   ```\n   ‚ùå EKS not available in all AZs\n\n   List available AZs:\n     aws eks describe-addon-versions --region <region>\n\n   Select AZs that support EKS\n   ```\n\n4. **VPCLimitExceeded**:\n   ```\n   ‚ùå VPC limit reached (default: 5 per region)\n\n   Solutions:\n   1. Use existing VPC\n   2. Delete unused VPCs\n   3. Request limit increase\n   ```\n\n## Best Practices\n\n### Production Clusters:\n\n1. **High Availability**:\n   - Deploy across multiple AZs (minimum 3)\n   - Use managed node groups (EKS) or machine pools (ROSA)\n   - Enable cluster autoscaling\n\n2. **Security**:\n   - Private API endpoint for production\n   - Use IAM roles for service accounts (IRSA)\n   - Enable control plane logging\n   - Use STS mode for ROSA\n   - Enable encryption at rest\n\n3. **Networking**:\n   - Separate public and private subnets\n   - Use AWS VPC CNI (EKS) or OVN (ROSA)\n   - Configure security groups properly\n   - Use AWS PrivateLink for private clusters\n\n4. **Cost Optimization**:\n   - Use Spot instances for non-critical workloads\n   - Enable cluster autoscaler\n   - Right-size instance types\n   - Use Savings Plans or Reserved Instances\n   - ROSA Hosted CP reduces costs\n\n5. **Monitoring & Logging**:\n   - Enable CloudWatch Container Insights\n   - Configure control plane logging\n   - Set up Prometheus/Grafana\n   - Use AWS X-Ray for tracing\n\n## References\n\n- **EKS**: https://docs.aws.amazon.com/eks/\n- **ROSA**: https://docs.openshift.com/rosa/\n- **eksctl**: https://eksctl.io/\n- **ROSA CLI**: https://console.redhat.com/openshift/downloads\n- **Best Practices**: https://aws.github.io/aws-eks-best-practices/"
              },
              {
                "name": "/aws-cluster-delete",
                "description": "Safely delete an EKS or ROSA cluster with validation and backup",
                "path": "plugins/cloud-aws/commands/aws-cluster-delete.md",
                "frontmatter": {
                  "name": "aws-cluster-delete",
                  "description": "Safely delete an EKS or ROSA cluster with validation and backup",
                  "category": "cloud-provisioning",
                  "parameters": [
                    {
                      "name": "cluster-name",
                      "description": "Name of the cluster to delete",
                      "required": true
                    },
                    {
                      "name": "type",
                      "description": "Cluster type (eks or rosa)",
                      "required": true,
                      "default": "eks"
                    },
                    {
                      "name": "region",
                      "description": "AWS region where cluster is located",
                      "required": true
                    },
                    {
                      "name": "skip-backup",
                      "description": "Skip automatic backup before deletion",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "force",
                      "description": "Skip all confirmation prompts (dangerous)",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "delete-vpc",
                      "description": "Delete associated VPC and networking resources",
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "tags": [
                    "aws",
                    "eks",
                    "rosa",
                    "cluster-lifecycle",
                    "deletion",
                    "safety"
                  ]
                },
                "content": "# AWS Cluster Delete\n\nSafely delete an EKS or ROSA cluster with comprehensive validation, automatic backup, and resource cleanup.\n\n## Overview\n\nThis command provides a safe way to delete AWS-managed Kubernetes clusters (EKS or ROSA) with multiple safety checks, automatic backups, and proper resource cleanup.\n\n**‚ö†Ô∏è WARNING: This is a destructive operation that cannot be undone. All cluster data will be permanently deleted.**\n\n## Prerequisites\n\n- AWS CLI installed and configured (`aws --version`)\n- For EKS: `eksctl` CLI installed\n- For ROSA: `rosa` CLI installed and authenticated\n- `kubectl` configured with cluster access\n- Appropriate IAM permissions:\n  - EKS: `eks:DeleteCluster`, `eks:DescribeCluster`, `ec2:DeleteVpc`, etc.\n  - ROSA: Cluster admin or organization admin role\n\n## Safety Features\n\nThis command implements multiple layers of safety:\n\n1. **Production Detection**: Warns if cluster has production tags\n2. **Resource Analysis**: Lists all resources that will be deleted\n3. **Persistent Volume Warning**: Alerts about PV data loss\n4. **Automatic Backup**: Creates backup before deletion (unless skipped)\n5. **Confirmation Prompts**: Requires explicit confirmation\n6. **Grace Period**: Provides time to cancel operation\n\n## Workflow\n\n### Phase 1: Pre-deletion Validation\n\n#### 1.1 Verify Cluster Exists\n\n```bash\nCLUSTER_NAME=\"${CLUSTER_NAME}\"\nCLUSTER_TYPE=\"${CLUSTER_TYPE:-eks}\"\nAWS_REGION=\"${AWS_REGION}\"\n\necho \"üîç Verifying cluster: $CLUSTER_NAME in region $AWS_REGION\"\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    # Check EKS cluster exists\n    if ! aws eks describe-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" &>/dev/null; then\n        echo \"‚ùå ERROR: EKS cluster '$CLUSTER_NAME' not found in region $AWS_REGION\"\n        exit 1\n    fi\n\n    # Get cluster details\n    CLUSTER_INFO=$(aws eks describe-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" --output json)\n    CLUSTER_STATUS=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.status')\n    CLUSTER_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.version')\n    CLUSTER_VPC=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.resourcesVpcConfig.vpcId')\n    CLUSTER_ARN=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.arn')\n\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    # Check ROSA cluster exists\n    if ! rosa describe cluster -c \"$CLUSTER_NAME\" &>/dev/null; then\n        echo \"‚ùå ERROR: ROSA cluster '$CLUSTER_NAME' not found\"\n        exit 1\n    fi\n\n    # Get cluster details\n    CLUSTER_INFO=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json)\n    CLUSTER_STATUS=$(echo \"$CLUSTER_INFO\" | jq -r '.status.state')\n    CLUSTER_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.version.raw_id')\n    CLUSTER_ID=$(echo \"$CLUSTER_INFO\" | jq -r '.id')\nfi\n\necho \"‚úÖ Cluster found: $CLUSTER_NAME\"\necho \"   Type: $CLUSTER_TYPE\"\necho \"   Status: $CLUSTER_STATUS\"\necho \"   Version: $CLUSTER_VERSION\"\n```\n\n#### 1.2 Analyze Cluster Resources\n\n```bash\necho \"\"\necho \"üìä Analyzing cluster resources...\"\n\n# Get kubeconfig\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    aws eks update-kubeconfig --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" --kubeconfig /tmp/kubeconfig-$CLUSTER_NAME\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    rosa create admin -c \"$CLUSTER_NAME\" > /tmp/rosa-admin-$CLUSTER_NAME.txt\n    # Extract credentials and configure kubeconfig\n    ADMIN_USER=$(grep \"oc login\" /tmp/rosa-admin-$CLUSTER_NAME.txt | awk '{print $5}')\n    ADMIN_PASS=$(grep \"oc login\" /tmp/rosa-admin-$CLUSTER_NAME.txt | awk '{print $7}')\nfi\n\nexport KUBECONFIG=/tmp/kubeconfig-$CLUSTER_NAME\n\n# Count resources\nNAMESPACE_COUNT=$(kubectl get namespaces --no-headers 2>/dev/null | wc -l)\nPOD_COUNT=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | wc -l)\nSERVICE_COUNT=$(kubectl get services --all-namespaces --no-headers 2>/dev/null | wc -l)\nPV_COUNT=$(kubectl get pv --no-headers 2>/dev/null | wc -l)\nPVC_COUNT=$(kubectl get pvc --all-namespaces --no-headers 2>/dev/null | wc -l)\nDEPLOYMENT_COUNT=$(kubectl get deployments --all-namespaces --no-headers 2>/dev/null | wc -l)\n\necho \"   Namespaces: $NAMESPACE_COUNT\"\necho \"   Pods: $POD_COUNT\"\necho \"   Services: $SERVICE_COUNT\"\necho \"   Deployments: $DEPLOYMENT_COUNT\"\necho \"   Persistent Volumes: $PV_COUNT\"\necho \"   Persistent Volume Claims: $PVC_COUNT\"\n```\n\n#### 1.3 Safety Checks and Warnings\n\n```bash\necho \"\"\necho \"‚ö†Ô∏è  SAFETY CHECKS\"\necho \"=================\"\n\nWARNINGS=0\n\n# Check for production tags/labels\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    TAGS=$(aws eks list-tags-for-resource --resource-arn \"$CLUSTER_ARN\" --region \"$AWS_REGION\" --output json)\n    ENV_TAG=$(echo \"$TAGS\" | jq -r '.tags.Environment // .tags.environment // .tags.env // \"\"')\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    ENV_TAG=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json | jq -r '.tags.environment // \"\"')\nfi\n\nif [[ \"$ENV_TAG\" =~ ^(prod|production|prd)$ ]]; then\n    echo \"‚ö†Ô∏è  WARNING: Cluster is tagged as PRODUCTION\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check for persistent volumes\nif [[ $PV_COUNT -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $PV_COUNT Persistent Volumes will be deleted\"\n    echo \"   This may result in DATA LOSS if volumes contain important data\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check for LoadBalancer services\nLB_COUNT=$(kubectl get services --all-namespaces -o json 2>/dev/null | jq '[.items[] | select(.spec.type==\"LoadBalancer\")] | length')\nif [[ $LB_COUNT -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $LB_COUNT LoadBalancer services found\"\n    echo \"   Associated AWS Load Balancers and security groups will be deleted\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check cluster age\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    CREATED_AT=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.createdAt')\n    DAYS_OLD=$(( ($(date +%s) - $(date -d \"$CREATED_AT\" +%s)) / 86400 ))\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    CREATED_AT=$(echo \"$CLUSTER_INFO\" | jq -r '.creation_timestamp')\n    DAYS_OLD=$(( ($(date +%s) - $(date -d \"$CREATED_AT\" +%s)) / 86400 ))\nfi\n\nif [[ $DAYS_OLD -gt 30 ]]; then\n    echo \"‚ÑπÔ∏è  INFO: Cluster is $DAYS_OLD days old\"\nfi\n\necho \"\"\necho \"Total warnings: $WARNINGS\"\n```\n\n### Phase 2: Backup (Optional)\n\n```bash\nif [[ \"${SKIP_BACKUP}\" != \"true\" ]]; then\n    echo \"\"\n    echo \"üíæ Creating backup before deletion...\"\n\n    BACKUP_DIR=\"./cluster-backup-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n    mkdir -p \"$BACKUP_DIR\"\n\n    echo \"   Backup directory: $BACKUP_DIR\"\n\n    # Backup all resources\n    echo \"   Backing up all Kubernetes resources...\"\n    kubectl get all --all-namespaces -o yaml > \"$BACKUP_DIR/all-resources.yaml\" 2>/dev/null\n\n    # Backup persistent volumes and claims\n    echo \"   Backing up PV/PVC definitions...\"\n    kubectl get pv -o yaml > \"$BACKUP_DIR/persistent-volumes.yaml\" 2>/dev/null\n    kubectl get pvc --all-namespaces -o yaml > \"$BACKUP_DIR/persistent-volume-claims.yaml\" 2>/dev/null\n\n    # Backup ConfigMaps and Secrets\n    echo \"   Backing up ConfigMaps and Secrets...\"\n    kubectl get configmaps --all-namespaces -o yaml > \"$BACKUP_DIR/configmaps.yaml\" 2>/dev/null\n    kubectl get secrets --all-namespaces -o yaml > \"$BACKUP_DIR/secrets.yaml\" 2>/dev/null\n\n    # Backup CRDs and custom resources\n    echo \"   Backing up CRDs...\"\n    kubectl get crds -o yaml > \"$BACKUP_DIR/crds.yaml\" 2>/dev/null\n\n    # Backup cluster-specific info\n    if [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n        aws eks describe-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" > \"$BACKUP_DIR/cluster-info.json\"\n        eksctl get nodegroup --cluster \"$CLUSTER_NAME\" --region \"$AWS_REGION\" -o json > \"$BACKUP_DIR/nodegroups.json\" 2>/dev/null\n    elif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n        rosa describe cluster -c \"$CLUSTER_NAME\" -o json > \"$BACKUP_DIR/cluster-info.json\"\n    fi\n\n    echo \"‚úÖ Backup completed: $BACKUP_DIR\"\nelse\n    echo \"‚ö†Ô∏è  Skipping backup (--skip-backup flag set)\"\nfi\n```\n\n### Phase 3: Confirmation\n\n```bash\necho \"\"\necho \"üö® FINAL CONFIRMATION REQUIRED\"\necho \"==============================\"\necho \"\"\necho \"You are about to DELETE the following cluster:\"\necho \"\"\necho \"  Cluster Name: $CLUSTER_NAME\"\necho \"  Cluster Type: $CLUSTER_TYPE\"\necho \"  Region: $AWS_REGION\"\necho \"  Resources: $NAMESPACE_COUNT namespaces, $POD_COUNT pods, $PV_COUNT PVs\"\necho \"\"\necho \"This action will:\"\necho \"  ‚ùå Delete the entire Kubernetes cluster\"\necho \"  ‚ùå Delete all workloads and configurations\"\necho \"  ‚ùå Delete all persistent volumes and data\"\necho \"  ‚ùå Delete associated AWS resources (node groups, security groups, etc.)\"\nif [[ \"${DELETE_VPC}\" == \"true\" ]]; then\n    echo \"  ‚ùå Delete the VPC and networking resources\"\nfi\necho \"\"\necho \"‚ö†Ô∏è  THIS CANNOT BE UNDONE!\"\necho \"\"\n\nif [[ \"${FORCE}\" != \"true\" ]]; then\n    # First confirmation: type cluster name\n    echo \"To confirm, please type the cluster name: $CLUSTER_NAME\"\n    read -r CONFIRM_NAME\n\n    if [[ \"$CONFIRM_NAME\" != \"$CLUSTER_NAME\" ]]; then\n        echo \"‚ùå Cluster name does not match. Aborting.\"\n        exit 1\n    fi\n\n    # Second confirmation: yes/no\n    echo \"\"\n    echo \"Are you absolutely sure you want to delete this cluster? (yes/no)\"\n    read -r CONFIRM_DELETE\n\n    if [[ \"$CONFIRM_DELETE\" != \"yes\" ]]; then\n        echo \"‚ùå Deletion cancelled.\"\n        exit 0\n    fi\n\n    # Grace period\n    echo \"\"\n    echo \"‚è≥ Starting deletion in 10 seconds... (Ctrl+C to cancel)\"\n    sleep 10\nelse\n    echo \"‚ö†Ô∏è  FORCE mode enabled - skipping confirmations\"\nfi\n```\n\n### Phase 4: Deletion\n\n```bash\necho \"\"\necho \"üóëÔ∏è  Deleting cluster...\"\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    # Delete EKS cluster using eksctl (handles nodegroups, etc.)\n    echo \"   Deleting EKS cluster and node groups...\"\n\n    if eksctl delete cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" --wait; then\n        echo \"‚úÖ EKS cluster deleted successfully\"\n    else\n        echo \"‚ö†Ô∏è  eksctl delete failed or timed out, attempting manual cleanup...\"\n\n        # Manual cleanup of node groups\n        echo \"   Deleting node groups...\"\n        NODEGROUPS=$(aws eks list-nodegroups --cluster-name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" --output text --query 'nodegroups[]' 2>/dev/null)\n        for NG in $NODEGROUPS; do\n            echo \"   - Deleting nodegroup: $NG\"\n            aws eks delete-nodegroup --cluster-name \"$CLUSTER_NAME\" --nodegroup-name \"$NG\" --region \"$AWS_REGION\"\n        done\n\n        # Wait for nodegroups to delete\n        echo \"   Waiting for node groups to delete...\"\n        sleep 30\n\n        # Delete cluster\n        echo \"   Deleting cluster control plane...\"\n        aws eks delete-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\"\n    fi\n\n    # Clean up VPC if requested\n    if [[ \"${DELETE_VPC}\" == \"true\" && -n \"$CLUSTER_VPC\" ]]; then\n        echo \"\"\n        echo \"   Deleting VPC and networking resources...\"\n\n        # Delete NAT Gateways\n        NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter \"Name=vpc-id,Values=$CLUSTER_VPC\" --region \"$AWS_REGION\" --query 'NatGateways[].NatGatewayId' --output text)\n        for NAT in $NAT_GATEWAYS; do\n            echo \"   - Deleting NAT Gateway: $NAT\"\n            aws ec2 delete-nat-gateway --nat-gateway-id \"$NAT\" --region \"$AWS_REGION\"\n        done\n\n        sleep 30\n\n        # Delete subnets\n        SUBNETS=$(aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$CLUSTER_VPC\" --region \"$AWS_REGION\" --query 'Subnets[].SubnetId' --output text)\n        for SUBNET in $SUBNETS; do\n            echo \"   - Deleting subnet: $SUBNET\"\n            aws ec2 delete-subnet --subnet-id \"$SUBNET\" --region \"$AWS_REGION\" 2>/dev/null || true\n        done\n\n        # Delete route tables\n        ROUTE_TABLES=$(aws ec2 describe-route-tables --filters \"Name=vpc-id,Values=$CLUSTER_VPC\" --region \"$AWS_REGION\" --query 'RouteTables[?Associations[0].Main==`false`].RouteTableId' --output text)\n        for RT in $ROUTE_TABLES; do\n            echo \"   - Deleting route table: $RT\"\n            aws ec2 delete-route-table --route-table-id \"$RT\" --region \"$AWS_REGION\" 2>/dev/null || true\n        done\n\n        # Delete internet gateway\n        IGW=$(aws ec2 describe-internet-gateways --filters \"Name=attachment.vpc-id,Values=$CLUSTER_VPC\" --region \"$AWS_REGION\" --query 'InternetGateways[0].InternetGatewayId' --output text)\n        if [[ \"$IGW\" != \"None\" && -n \"$IGW\" ]]; then\n            echo \"   - Detaching and deleting Internet Gateway: $IGW\"\n            aws ec2 detach-internet-gateway --internet-gateway-id \"$IGW\" --vpc-id \"$CLUSTER_VPC\" --region \"$AWS_REGION\"\n            aws ec2 delete-internet-gateway --internet-gateway-id \"$IGW\" --region \"$AWS_REGION\"\n        fi\n\n        # Delete security groups (except default)\n        SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=$CLUSTER_VPC\" --region \"$AWS_REGION\" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text)\n        for SG in $SECURITY_GROUPS; do\n            echo \"   - Deleting security group: $SG\"\n            aws ec2 delete-security-group --group-id \"$SG\" --region \"$AWS_REGION\" 2>/dev/null || true\n        done\n\n        # Finally delete VPC\n        echo \"   - Deleting VPC: $CLUSTER_VPC\"\n        aws ec2 delete-vpc --vpc-id \"$CLUSTER_VPC\" --region \"$AWS_REGION\"\n\n        echo \"‚úÖ VPC and networking resources deleted\"\n    fi\n\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    # Delete ROSA cluster\n    echo \"   Deleting ROSA cluster...\"\n\n    if rosa delete cluster --cluster \"$CLUSTER_NAME\" --yes; then\n        echo \"   Waiting for cluster deletion to complete...\"\n\n        # Monitor deletion progress\n        while rosa describe cluster -c \"$CLUSTER_NAME\" &>/dev/null; do\n            echo \"   Cluster still deleting...\"\n            sleep 30\n        done\n\n        echo \"‚úÖ ROSA cluster deleted successfully\"\n    else\n        echo \"‚ùå ROSA cluster deletion failed\"\n        exit 1\n    fi\nfi\n```\n\n### Phase 5: Post-Deletion Cleanup\n\n```bash\necho \"\"\necho \"üßπ Cleaning up local resources...\"\n\n# Remove kubeconfig entry\nkubectl config delete-context \"$CLUSTER_NAME\" 2>/dev/null || true\nkubectl config delete-cluster \"$CLUSTER_NAME\" 2>/dev/null || true\n\n# Clean up temporary files\nrm -f /tmp/kubeconfig-$CLUSTER_NAME\nrm -f /tmp/rosa-admin-$CLUSTER_NAME.txt\n\necho \"‚úÖ Local cleanup completed\"\n```\n\n### Phase 6: Summary\n\n```bash\necho \"\"\necho \"‚úÖ CLUSTER DELETION COMPLETE\"\necho \"============================\"\necho \"\"\necho \"Cluster '$CLUSTER_NAME' has been successfully deleted.\"\necho \"\"\n\nif [[ \"${SKIP_BACKUP}\" != \"true\" ]]; then\n    echo \"Backup saved to: $BACKUP_DIR\"\n    echo \"\"\nfi\n\necho \"Next steps:\"\necho \"  ‚Ä¢ Verify AWS resources are fully cleaned up in the console\"\necho \"  ‚Ä¢ Check for any orphaned resources (EBS volumes, EIPs, etc.)\"\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    echo \"  ‚Ä¢ Review CloudFormation stacks for eksctl-* stacks\"\nfi\necho \"  ‚Ä¢ Update documentation and inventory\"\necho \"\"\n```\n\n## Safety Best Practices\n\n1. **Always create backups** unless absolutely certain\n2. **Review the resource list** before confirming\n3. **Test deletion in non-production** first\n4. **Check for orphaned resources** after deletion\n5. **Document the deletion** for audit purposes\n6. **Verify IAM cleanup** if cluster-specific roles were created\n\n## Common Issues\n\n### Issue: Deletion stuck or timing out\n\n**Solution**: Check for resources blocking deletion:\n\n```bash\n# Check for lingering node groups\naws eks list-nodegroups --cluster-name $CLUSTER_NAME --region $AWS_REGION\n\n# Check for persistent volumes\nkubectl get pv\n\n# Check for LoadBalancer services\nkubectl get svc --all-namespaces -o wide | grep LoadBalancer\n```\n\n### Issue: VPC deletion fails\n\n**Solution**: Manually identify and delete dependencies:\n\n```bash\n# Find ENIs\naws ec2 describe-network-interfaces --filters \"Name=vpc-id,Values=$VPC_ID\" --region $AWS_REGION\n\n# Find security group dependencies\naws ec2 describe-security-groups --filters \"Name=vpc-id,Values=$VPC_ID\" --region $AWS_REGION\n```\n\n### Issue: ROSA cluster deletion requires manual steps\n\n**Solution**: Use ROSA CLI to check status:\n\n```bash\nrosa logs uninstall -c $CLUSTER_NAME\n```\n\n## Examples\n\n### Example 1: Safe EKS deletion with backup\n\n```bash\n# Production-safe deletion with all safety features\ncluster-code aws-cluster-delete \\\n  --cluster-name my-prod-cluster \\\n  --type eks \\\n  --region us-west-2\n```\n\n### Example 2: Quick deletion for development\n\n```bash\n# Skip backup for temporary dev cluster\ncluster-code aws-cluster-delete \\\n  --cluster-name dev-test-123 \\\n  --type eks \\\n  --region us-east-1 \\\n  --skip-backup\n```\n\n### Example 3: Complete cleanup including VPC\n\n```bash\n# Delete cluster and all networking\ncluster-code aws-cluster-delete \\\n  --cluster-name old-cluster \\\n  --type eks \\\n  --region eu-west-1 \\\n  --delete-vpc\n```\n\n### Example 4: Delete ROSA cluster\n\n```bash\n# Delete Red Hat OpenShift Service on AWS cluster\ncluster-code aws-cluster-delete \\\n  --cluster-name rosa-prod \\\n  --type rosa \\\n  --region us-east-1\n```\n\n## Related Commands\n\n- `aws-cluster-create`: Create new EKS/ROSA clusters\n- `aws-cluster-upgrade`: Upgrade cluster version\n- `backup-cluster`: Create comprehensive cluster backup\n- `cluster-diagnose`: Run diagnostics before deletion"
              },
              {
                "name": "/aws-cluster-upgrade",
                "description": "Upgrade EKS or ROSA cluster to a new Kubernetes version",
                "path": "plugins/cloud-aws/commands/aws-cluster-upgrade.md",
                "frontmatter": {
                  "name": "aws-cluster-upgrade",
                  "description": "Upgrade EKS or ROSA cluster to a new Kubernetes version",
                  "category": "cloud-provisioning",
                  "parameters": [
                    {
                      "name": "cluster-name",
                      "description": "Name of the cluster to upgrade",
                      "required": true
                    },
                    {
                      "name": "type",
                      "description": "Cluster type (eks or rosa)",
                      "required": true,
                      "default": "eks"
                    },
                    {
                      "name": "region",
                      "description": "AWS region where cluster is located",
                      "required": true
                    },
                    {
                      "name": "version",
                      "description": "Target Kubernetes version (e.g., 1.29)",
                      "required": true
                    },
                    {
                      "name": "upgrade-addons",
                      "description": "Automatically upgrade add-ons after cluster upgrade",
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "upgrade-nodegroups",
                      "description": "Automatically upgrade node groups after control plane",
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "skip-backup",
                      "description": "Skip pre-upgrade backup",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "dry-run",
                      "description": "Show what would be upgraded without making changes",
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "tags": [
                    "aws",
                    "eks",
                    "rosa",
                    "cluster-lifecycle",
                    "upgrade",
                    "maintenance"
                  ]
                },
                "content": "# AWS Cluster Upgrade\n\nSafely upgrade an EKS or ROSA cluster to a new Kubernetes version with automated validation, backup, and verification.\n\n## Overview\n\nThis command provides a comprehensive cluster upgrade workflow with:\n\n- **Version Validation**: Ensures target version is available and upgrade path is valid\n- **Pre-upgrade Backup**: Automatic backup before making changes\n- **Staged Upgrade**: Control plane first, then add-ons, then node groups\n- **Health Monitoring**: Continuous health checks throughout upgrade\n- **Rollback Support**: Documented procedures for rolling back if needed\n\n## Prerequisites\n\n- AWS CLI installed and configured (`aws --version` >= 2.13.0)\n- For EKS: `eksctl` CLI installed (>= 0.165.0)\n- For ROSA: `rosa` CLI installed and authenticated\n- `kubectl` configured with cluster admin access\n- Appropriate IAM permissions:\n  - EKS: `eks:UpdateClusterVersion`, `eks:UpdateNodegroupVersion`\n  - ROSA: Cluster admin or organization admin role\n\n## Important Considerations\n\n### Kubernetes Version Upgrade Policy\n\n- **EKS**: Can only upgrade one minor version at a time (e.g., 1.28 ‚Üí 1.29)\n- **ROSA**: Can upgrade one minor version at a time\n- **Patch versions**: Can skip directly to latest patch (e.g., 1.28.2 ‚Üí 1.28.7)\n- **Downgrade**: Not supported - must redeploy cluster\n\n### Upgrade Impact\n\n- **Control Plane**: Brief API server disruption during upgrade (~5-10 min)\n- **Workloads**: Continue running during control plane upgrade\n- **Node Groups**: Rolling replacement when upgraded (workload disruption possible)\n- **Add-ons**: May require update or reinstallation\n\n## Workflow\n\n### Phase 1: Pre-upgrade Validation\n\n#### 1.1 Check Current Version and Status\n\n```bash\nCLUSTER_NAME=\"${CLUSTER_NAME}\"\nCLUSTER_TYPE=\"${CLUSTER_TYPE:-eks}\"\nAWS_REGION=\"${AWS_REGION}\"\nTARGET_VERSION=\"${TARGET_VERSION}\"\n\necho \"üîç Validating cluster upgrade: $CLUSTER_NAME\"\necho \"\"\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    # Get current EKS version\n    CLUSTER_INFO=$(aws eks describe-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" --output json)\n    CURRENT_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.version')\n    CLUSTER_STATUS=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.status')\n    PLATFORM_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.cluster.platformVersion')\n\n    echo \"Current cluster version: $CURRENT_VERSION\"\n    echo \"Platform version: $PLATFORM_VERSION\"\n    echo \"Cluster status: $CLUSTER_STATUS\"\n\n    # Check cluster is in ACTIVE state\n    if [[ \"$CLUSTER_STATUS\" != \"ACTIVE\" ]]; then\n        echo \"‚ùå ERROR: Cluster must be in ACTIVE state for upgrade\"\n        echo \"   Current state: $CLUSTER_STATUS\"\n        exit 1\n    fi\n\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    # Get current ROSA version\n    CLUSTER_INFO=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json)\n    CURRENT_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.version.raw_id' | cut -d. -f1-2)\n    CLUSTER_STATE=$(echo \"$CLUSTER_INFO\" | jq -r '.status.state')\n\n    echo \"Current cluster version: $CURRENT_VERSION\"\n    echo \"Cluster state: $CLUSTER_STATE\"\n\n    if [[ \"$CLUSTER_STATE\" != \"ready\" ]]; then\n        echo \"‚ùå ERROR: Cluster must be in ready state for upgrade\"\n        exit 1\n    fi\nfi\n\necho \"Target version: $TARGET_VERSION\"\n```\n\n#### 1.2 Validate Upgrade Path\n\n```bash\necho \"\"\necho \"‚úÖ Validating upgrade path...\"\n\n# Parse versions\nCURRENT_MAJOR=$(echo \"$CURRENT_VERSION\" | cut -d. -f1)\nCURRENT_MINOR=$(echo \"$CURRENT_VERSION\" | cut -d. -f2)\nTARGET_MAJOR=$(echo \"$TARGET_VERSION\" | cut -d. -f1)\nTARGET_MINOR=$(echo \"$TARGET_VERSION\" | cut -d. -f2)\n\n# Check major version\nif [[ \"$CURRENT_MAJOR\" != \"$TARGET_MAJOR\" ]]; then\n    echo \"‚ùå ERROR: Major version upgrades are not supported\"\n    echo \"   Current: $CURRENT_MAJOR.x, Target: $TARGET_MAJOR.x\"\n    exit 1\nfi\n\n# Check minor version increment\nMINOR_DIFF=$((TARGET_MINOR - CURRENT_MINOR))\n\nif [[ $MINOR_DIFF -lt 0 ]]; then\n    echo \"‚ùå ERROR: Downgrading is not supported\"\n    echo \"   Current: $CURRENT_VERSION, Target: $TARGET_VERSION\"\n    exit 1\nelif [[ $MINOR_DIFF -eq 0 ]]; then\n    echo \"‚ÑπÔ∏è  Same minor version - will upgrade to latest patch version\"\nelif [[ $MINOR_DIFF -gt 1 ]]; then\n    echo \"‚ùå ERROR: Can only upgrade one minor version at a time\"\n    echo \"   Current: $CURRENT_VERSION, Target: $TARGET_VERSION\"\n    echo \"   Please upgrade to $CURRENT_MAJOR.$((CURRENT_MINOR + 1)) first\"\n    exit 1\nelse\n    echo \"‚úÖ Valid upgrade path: $CURRENT_VERSION ‚Üí $TARGET_VERSION\"\nfi\n```\n\n#### 1.3 Check Available Versions\n\n```bash\necho \"\"\necho \"üìã Checking available versions...\"\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    # List available EKS versions\n    AVAILABLE_VERSIONS=$(aws eks describe-addon-versions --region \"$AWS_REGION\" --query 'addons[0].addonVersions[0].compatibilities[].clusterVersion' --output text | sort -V | uniq)\n\n    echo \"Available EKS versions:\"\n    echo \"$AVAILABLE_VERSIONS\" | tr '\\t' '\\n' | sed 's/^/  - /'\n\n    # Check if target version is available\n    if ! echo \"$AVAILABLE_VERSIONS\" | grep -q \"$TARGET_VERSION\"; then\n        echo \"‚ùå ERROR: Target version $TARGET_VERSION is not available in region $AWS_REGION\"\n        exit 1\n    fi\n\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    # List available ROSA versions\n    AVAILABLE_VERSIONS=$(rosa list versions -o json | jq -r '.[].raw_id' | sort -V)\n\n    echo \"Available ROSA versions:\"\n    echo \"$AVAILABLE_VERSIONS\" | sed 's/^/  - /'\n\n    # Check if target version is available\n    if ! echo \"$AVAILABLE_VERSIONS\" | grep -q \"^$TARGET_VERSION\"; then\n        echo \"‚ùå ERROR: Target version $TARGET_VERSION is not available\"\n        exit 1\n    fi\nfi\n\necho \"‚úÖ Target version $TARGET_VERSION is available\"\n```\n\n#### 1.4 Health Check\n\n```bash\necho \"\"\necho \"üè• Running pre-upgrade health check...\"\n\n# Update kubeconfig\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    aws eks update-kubeconfig --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\"\nfi\n\n# Check node status\necho \"   Checking node health...\"\nUNHEALTHY_NODES=$(kubectl get nodes --no-headers | grep -v \" Ready\" | wc -l)\nif [[ $UNHEALTHY_NODES -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $UNHEALTHY_NODES nodes are not in Ready state\"\n    kubectl get nodes | grep -v \" Ready\"\nfi\n\n# Check pod health\necho \"   Checking pod health...\"\nFAILING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers 2>/dev/null | wc -l)\nif [[ $FAILING_PODS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $FAILING_PODS pods are not in Running/Succeeded state\"\nfi\n\n# Check for deprecated APIs\necho \"   Checking for deprecated APIs in target version...\"\nif command -v pluto &>/dev/null; then\n    pluto detect-all-in-cluster --target-versions k8s=v$TARGET_VERSION\nelse\n    echo \"‚ÑπÔ∏è  Install 'pluto' for deprecated API detection: brew install FairwindsOps/tap/pluto\"\nfi\n\n# Check PodDisruptionBudgets\necho \"   Checking PodDisruptionBudgets...\"\nPDB_COUNT=$(kubectl get pdb --all-namespaces --no-headers 2>/dev/null | wc -l)\nif [[ $PDB_COUNT -gt 0 ]]; then\n    echo \"‚ÑπÔ∏è  Found $PDB_COUNT PodDisruptionBudgets (may affect node upgrade)\"\nfi\n\necho \"‚úÖ Pre-upgrade health check completed\"\n```\n\n### Phase 2: Backup and Preparation\n\n```bash\nif [[ \"${SKIP_BACKUP}\" != \"true\" && \"${DRY_RUN}\" != \"true\" ]]; then\n    echo \"\"\n    echo \"üíæ Creating pre-upgrade backup...\"\n\n    BACKUP_DIR=\"./cluster-backup-upgrade-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n    mkdir -p \"$BACKUP_DIR\"\n\n    # Backup cluster configuration\n    if [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n        aws eks describe-cluster --name \"$CLUSTER_NAME\" --region \"$AWS_REGION\" > \"$BACKUP_DIR/cluster-config.json\"\n        eksctl get nodegroup --cluster \"$CLUSTER_NAME\" --region \"$AWS_REGION\" -o json > \"$BACKUP_DIR/nodegroups.json\" 2>/dev/null\n    elif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n        rosa describe cluster -c \"$CLUSTER_NAME\" -o json > \"$BACKUP_DIR/cluster-config.json\"\n    fi\n\n    # Backup Kubernetes resources\n    kubectl get all --all-namespaces -o yaml > \"$BACKUP_DIR/all-resources.yaml\"\n    kubectl get pv,pvc --all-namespaces -o yaml > \"$BACKUP_DIR/volumes.yaml\"\n    kubectl get configmap,secret --all-namespaces -o yaml > \"$BACKUP_DIR/configs-secrets.yaml\"\n\n    echo \"‚úÖ Backup saved to: $BACKUP_DIR\"\nfi\n```\n\n### Phase 3: Upgrade Execution\n\n#### 3.1 Upgrade Control Plane\n\n```bash\necho \"\"\necho \"üöÄ UPGRADE EXECUTION\"\necho \"====================\"\n\nif [[ \"${DRY_RUN}\" == \"true\" ]]; then\n    echo \"üß™ DRY RUN MODE - No changes will be made\"\n    echo \"\"\n    echo \"Would upgrade:\"\n    echo \"  Cluster: $CLUSTER_NAME\"\n    echo \"  From: $CURRENT_VERSION\"\n    echo \"  To: $TARGET_VERSION\"\n    echo \"  Region: $AWS_REGION\"\n    exit 0\nfi\n\necho \"\"\necho \"üìù Upgrade Summary:\"\necho \"  Cluster: $CLUSTER_NAME\"\necho \"  Type: $CLUSTER_TYPE\"\necho \"  Current Version: $CURRENT_VERSION\"\necho \"  Target Version: $TARGET_VERSION\"\necho \"  Region: $AWS_REGION\"\necho \"\"\necho \"Upgrade phases:\"\necho \"  1. Control plane upgrade (~10-15 minutes)\"\nif [[ \"${UPGRADE_ADDONS}\" == \"true\" ]]; then\n    echo \"  2. Add-on upgrades (~5-10 minutes)\"\nfi\nif [[ \"${UPGRADE_NODEGROUPS}\" == \"true\" ]]; then\n    echo \"  3. Node group upgrades (~15-30 minutes per group)\"\nfi\necho \"\"\necho \"‚ö†Ô∏è  API server will be briefly unavailable during control plane upgrade\"\necho \"\"\n\nread -p \"Continue with upgrade? (yes/no): \" CONFIRM\nif [[ \"$CONFIRM\" != \"yes\" ]]; then\n    echo \"‚ùå Upgrade cancelled\"\n    exit 0\nfi\n\necho \"\"\necho \"Step 1/3: Upgrading control plane to $TARGET_VERSION...\"\necho \"‚è≥ This may take 10-15 minutes...\"\n\nUPGRADE_START=$(date +%s)\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    # Upgrade EKS control plane\n    UPDATE_ID=$(aws eks update-cluster-version \\\n        --name \"$CLUSTER_NAME\" \\\n        --kubernetes-version \"$TARGET_VERSION\" \\\n        --region \"$AWS_REGION\" \\\n        --output text \\\n        --query 'update.id')\n\n    echo \"   Update ID: $UPDATE_ID\"\n    echo \"   Monitoring upgrade progress...\"\n\n    # Wait for upgrade to complete\n    while true; do\n        UPDATE_STATUS=$(aws eks describe-update \\\n            --name \"$CLUSTER_NAME\" \\\n            --update-id \"$UPDATE_ID\" \\\n            --region \"$AWS_REGION\" \\\n            --output text \\\n            --query 'update.status')\n\n        case \"$UPDATE_STATUS\" in\n            \"InProgress\")\n                echo \"   Status: In Progress ($(( ($(date +%s) - UPGRADE_START) / 60 )) minutes elapsed)\"\n                sleep 30\n                ;;\n            \"Successful\")\n                echo \"‚úÖ Control plane upgraded successfully\"\n                break\n                ;;\n            \"Failed\"|\"Cancelled\")\n                echo \"‚ùå Control plane upgrade $UPDATE_STATUS\"\n                aws eks describe-update --name \"$CLUSTER_NAME\" --update-id \"$UPDATE_ID\" --region \"$AWS_REGION\"\n                exit 1\n                ;;\n        esac\n    done\n\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    # Upgrade ROSA cluster\n    echo \"   Initiating ROSA upgrade...\"\n\n    rosa upgrade cluster \\\n        --cluster \"$CLUSTER_NAME\" \\\n        --version \"$TARGET_VERSION\" \\\n        --mode auto \\\n        --yes\n\n    echo \"   Monitoring upgrade progress...\"\n\n    # Monitor upgrade status\n    while true; do\n        UPGRADE_STATE=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json | jq -r '.status.upgrade.state // \"completed\"')\n\n        if [[ \"$UPGRADE_STATE\" == \"completed\" || \"$UPGRADE_STATE\" == \"null\" ]]; then\n            # Check if version matches target\n            NEW_VERSION=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json | jq -r '.version.raw_id' | cut -d. -f1-2)\n            if [[ \"$NEW_VERSION\" == \"$TARGET_VERSION\" ]]; then\n                echo \"‚úÖ Control plane upgraded successfully\"\n                break\n            fi\n        elif [[ \"$UPGRADE_STATE\" == \"failed\" ]]; then\n            echo \"‚ùå Control plane upgrade failed\"\n            rosa logs install -c \"$CLUSTER_NAME\"\n            exit 1\n        else\n            echo \"   Status: $UPGRADE_STATE ($(( ($(date +%s) - UPGRADE_START) / 60 )) minutes elapsed)\"\n            sleep 30\n        fi\n    done\nfi\n```\n\n#### 3.2 Upgrade Add-ons (EKS only)\n\n```bash\nif [[ \"$CLUSTER_TYPE\" == \"eks\" && \"${UPGRADE_ADDONS}\" == \"true\" ]]; then\n    echo \"\"\n    echo \"Step 2/3: Upgrading EKS add-ons...\"\n\n    # Core add-ons to upgrade\n    ADDONS=(\"vpc-cni\" \"coredns\" \"kube-proxy\" \"aws-ebs-csi-driver\")\n\n    for ADDON in \"${ADDONS[@]}\"; do\n        # Check if addon is installed\n        if aws eks describe-addon \\\n            --cluster-name \"$CLUSTER_NAME\" \\\n            --addon-name \"$ADDON\" \\\n            --region \"$AWS_REGION\" &>/dev/null; then\n\n            CURRENT_ADDON_VERSION=$(aws eks describe-addon \\\n                --cluster-name \"$CLUSTER_NAME\" \\\n                --addon-name \"$ADDON\" \\\n                --region \"$AWS_REGION\" \\\n                --query 'addon.addonVersion' \\\n                --output text)\n\n            # Get latest compatible version\n            LATEST_ADDON_VERSION=$(aws eks describe-addon-versions \\\n                --addon-name \"$ADDON\" \\\n                --kubernetes-version \"$TARGET_VERSION\" \\\n                --region \"$AWS_REGION\" \\\n                --query 'addons[0].addonVersions[0].addonVersion' \\\n                --output text)\n\n            if [[ \"$CURRENT_ADDON_VERSION\" != \"$LATEST_ADDON_VERSION\" ]]; then\n                echo \"   Upgrading $ADDON: $CURRENT_ADDON_VERSION ‚Üí $LATEST_ADDON_VERSION\"\n\n                aws eks update-addon \\\n                    --cluster-name \"$CLUSTER_NAME\" \\\n                    --addon-name \"$ADDON\" \\\n                    --addon-version \"$LATEST_ADDON_VERSION\" \\\n                    --resolve-conflicts OVERWRITE \\\n                    --region \"$AWS_REGION\"\n\n                # Wait for addon update\n                aws eks wait addon-active \\\n                    --cluster-name \"$CLUSTER_NAME\" \\\n                    --addon-name \"$ADDON\" \\\n                    --region \"$AWS_REGION\"\n\n                echo \"   ‚úÖ $ADDON upgraded\"\n            else\n                echo \"   ‚úÖ $ADDON already at latest version\"\n            fi\n        else\n            echo \"   ‚ÑπÔ∏è  $ADDON not installed, skipping\"\n        fi\n    done\n\n    echo \"‚úÖ Add-ons upgrade completed\"\nfi\n```\n\n#### 3.3 Upgrade Node Groups\n\n```bash\nif [[ \"${UPGRADE_NODEGROUPS}\" == \"true\" ]]; then\n    echo \"\"\n    echo \"Step 3/3: Upgrading node groups...\"\n\n    if [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n        # Get all node groups\n        NODEGROUPS=$(aws eks list-nodegroups \\\n            --cluster-name \"$CLUSTER_NAME\" \\\n            --region \"$AWS_REGION\" \\\n            --output text \\\n            --query 'nodegroups[]')\n\n        if [[ -z \"$NODEGROUPS\" ]]; then\n            echo \"‚ÑπÔ∏è  No managed node groups found\"\n        else\n            for NG in $NODEGROUPS; do\n                echo \"\"\n                echo \"   Upgrading node group: $NG\"\n\n                # Get current node group version\n                NG_VERSION=$(aws eks describe-nodegroup \\\n                    --cluster-name \"$CLUSTER_NAME\" \\\n                    --nodegroup-name \"$NG\" \\\n                    --region \"$AWS_REGION\" \\\n                    --query 'nodegroup.version' \\\n                    --output text)\n\n                if [[ \"$NG_VERSION\" == \"$TARGET_VERSION\" ]]; then\n                    echo \"   ‚úÖ Already at version $TARGET_VERSION\"\n                    continue\n                fi\n\n                echo \"   Current version: $NG_VERSION\"\n                echo \"   Target version: $TARGET_VERSION\"\n                echo \"   ‚è≥ Upgrading (this will perform rolling node replacement)...\"\n\n                # Initiate node group upgrade\n                UPDATE_ID=$(aws eks update-nodegroup-version \\\n                    --cluster-name \"$CLUSTER_NAME\" \\\n                    --nodegroup-name \"$NG\" \\\n                    --region \"$AWS_REGION\" \\\n                    --output text \\\n                    --query 'update.id')\n\n                # Monitor upgrade\n                while true; do\n                    UPDATE_STATUS=$(aws eks describe-update \\\n                        --name \"$CLUSTER_NAME\" \\\n                        --update-id \"$UPDATE_ID\" \\\n                        --nodegroup-name \"$NG\" \\\n                        --region \"$AWS_REGION\" \\\n                        --output text \\\n                        --query 'update.status')\n\n                    case \"$UPDATE_STATUS\" in\n                        \"InProgress\")\n                            echo \"   Status: In Progress...\"\n                            sleep 30\n                            ;;\n                        \"Successful\")\n                            echo \"   ‚úÖ $NG upgraded successfully\"\n                            break\n                            ;;\n                        \"Failed\"|\"Cancelled\")\n                            echo \"   ‚ùå $NG upgrade $UPDATE_STATUS\"\n                            exit 1\n                            ;;\n                    esac\n                done\n            done\n        fi\n\n    elif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n        echo \"   Upgrading ROSA machine pools...\"\n\n        # List machine pools\n        MACHINE_POOLS=$(rosa list machinepools -c \"$CLUSTER_NAME\" -o json | jq -r '.[].id')\n\n        for MP in $MACHINE_POOLS; do\n            echo \"   Upgrading machine pool: $MP\"\n\n            rosa upgrade machinepool \\\n                --cluster \"$CLUSTER_NAME\" \\\n                --machinepool \"$MP\" \\\n                --version \"$TARGET_VERSION\" \\\n                --yes\n\n            echo \"   ‚úÖ $MP upgrade initiated\"\n        done\n    fi\n\n    echo \"‚úÖ Node groups upgrade completed\"\nelse\n    echo \"\"\n    echo \"‚ö†Ô∏è  Skipping node group upgrade (--upgrade-nodegroups=false)\"\n    echo \"   Node groups are still running version $CURRENT_VERSION\"\n    echo \"   Upgrade manually with: aws eks update-nodegroup-version\"\nfi\n```\n\n### Phase 4: Post-Upgrade Verification\n\n```bash\necho \"\"\necho \"üîç POST-UPGRADE VERIFICATION\"\necho \"============================\"\n\n# Verify control plane version\necho \"\"\necho \"Verifying cluster version...\"\n\nif [[ \"$CLUSTER_TYPE\" == \"eks\" ]]; then\n    NEW_VERSION=$(aws eks describe-cluster \\\n        --name \"$CLUSTER_NAME\" \\\n        --region \"$AWS_REGION\" \\\n        --query 'cluster.version' \\\n        --output text)\nelif [[ \"$CLUSTER_TYPE\" == \"rosa\" ]]; then\n    NEW_VERSION=$(rosa describe cluster -c \"$CLUSTER_NAME\" -o json | jq -r '.version.raw_id' | cut -d. -f1-2)\nfi\n\nif [[ \"$NEW_VERSION\" == \"$TARGET_VERSION\" ]]; then\n    echo \"‚úÖ Cluster version: $NEW_VERSION\"\nelse\n    echo \"‚ùå Version mismatch! Expected: $TARGET_VERSION, Got: $NEW_VERSION\"\nfi\n\n# Check node versions\necho \"\"\necho \"Checking node versions...\"\nkubectl get nodes -o custom-columns=NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion\n\n# Check pod health\necho \"\"\necho \"Checking pod health...\"\nFAILING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers 2>/dev/null | wc -l)\n\nif [[ $FAILING_PODS -eq 0 ]]; then\n    echo \"‚úÖ All pods healthy\"\nelse\n    echo \"‚ö†Ô∏è  $FAILING_PODS pods not in Running/Succeeded state:\"\n    kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded\nfi\n\n# Run diagnostics\necho \"\"\necho \"Running cluster diagnostics...\"\nif command -v k8sgpt &>/dev/null; then\n    k8sgpt analyze --explain --filter=Node,Pod\nelse\n    echo \"‚ÑπÔ∏è  Install k8sgpt for AI-powered diagnostics\"\nfi\n\nUPGRADE_END=$(date +%s)\nTOTAL_TIME=$(( (UPGRADE_END - UPGRADE_START) / 60 ))\n\necho \"\"\necho \"‚úÖ UPGRADE COMPLETED SUCCESSFULLY\"\necho \"==================================\"\necho \"\"\necho \"Cluster: $CLUSTER_NAME\"\necho \"Old version: $CURRENT_VERSION\"\necho \"New version: $NEW_VERSION\"\necho \"Total time: $TOTAL_TIME minutes\"\necho \"\"\necho \"Next steps:\"\necho \"  ‚Ä¢ Monitor applications for any issues\"\necho \"  ‚Ä¢ Update CI/CD pipelines to use new version\"\necho \"  ‚Ä¢ Review Kubernetes $TARGET_VERSION changelog for new features\"\necho \"  ‚Ä¢ Update documentation and runbooks\"\necho \"\"\n```\n\n## Rollback Procedures\n\nIf the upgrade fails or causes issues:\n\n### EKS Rollback\n\n1. **Control plane cannot be rolled back** - must deploy new cluster with old version\n2. **Node groups can be rolled back**:\n\n```bash\n# Deploy old version node group\neksctl create nodegroup \\\n  --cluster $CLUSTER_NAME \\\n  --version $OLD_VERSION \\\n  --name rollback-ng\n\n# Drain and delete new nodes\nkubectl drain <node> --ignore-daemonsets\neksctl delete nodegroup --cluster $CLUSTER_NAME --name <new-ng>\n```\n\n### ROSA Rollback\n\nROSA does not support rollback. Recovery options:\n1. Restore from backup to new cluster\n2. Contact Red Hat support for assistance\n\n## Best Practices\n\n1. **Test in non-production first** - Always upgrade dev/staging before production\n2. **Review release notes** - Check Kubernetes and EKS/ROSA release notes\n3. **Schedule maintenance window** - Plan for API server downtime\n4. **Monitor during upgrade** - Watch for pod evictions and node replacements\n5. **Keep backups** - Maintain backups for at least 30 days post-upgrade\n6. **Update workloads** - Ensure apps are compatible with new Kubernetes version\n\n## Related Commands\n\n- `aws-cluster-delete`: Delete clusters\n- `cluster-diagnose`: Run comprehensive diagnostics\n- `backup-cluster`: Create cluster backup\n- `node-drain`: Manually drain nodes before upgrade"
              }
            ],
            "skills": []
          },
          {
            "name": "cloud-gcp",
            "description": "GCP cloud provider integration for GKE cluster management with MCP server",
            "source": "./plugins/cloud-gcp",
            "category": "cloud-provider",
            "version": "1.0.0",
            "author": {
              "name": "Cluster Code Team",
              "email": "support@cluster-code.io"
            },
            "install_commands": [
              "/plugin marketplace add kcns008/cluster-code",
              "/plugin install cloud-gcp@cluster-code-plugins"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2026-01-09T20:48:20Z",
              "created_at": "2025-10-27T03:14:59Z",
              "license": null
            },
            "commands": [
              {
                "name": "/gcp-cluster-create",
                "description": "Create a new GKE Standard or Autopilot cluster on Google Cloud",
                "path": "plugins/cloud-gcp/commands/gcp-cluster-create.md",
                "frontmatter": {
                  "name": "gcp-cluster-create",
                  "description": "Create a new GKE Standard or Autopilot cluster on Google Cloud",
                  "category": "cloud-provisioning",
                  "parameters": [
                    {
                      "name": "cluster-name",
                      "description": "Name of the cluster to create",
                      "required": true
                    },
                    {
                      "name": "mode",
                      "description": "Cluster mode (standard or autopilot)",
                      "required": true,
                      "default": "standard"
                    },
                    {
                      "name": "project",
                      "description": "GCP project ID",
                      "required": true
                    },
                    {
                      "name": "region",
                      "description": "GCP region (e.g., us-central1)",
                      "required": false
                    },
                    {
                      "name": "zone",
                      "description": "GCP zone for zonal cluster (e.g., us-central1-a)",
                      "required": false
                    },
                    {
                      "name": "version",
                      "description": "Kubernetes version (e.g., 1.29)",
                      "required": false
                    },
                    {
                      "name": "num-nodes",
                      "description": "Number of nodes per zone (Standard mode only)",
                      "type": "number",
                      "default": 3
                    },
                    {
                      "name": "machine-type",
                      "description": "Machine type for nodes (Standard mode only)",
                      "default": "e2-medium"
                    },
                    {
                      "name": "disk-size",
                      "description": "Boot disk size in GB",
                      "type": "number",
                      "default": 100
                    },
                    {
                      "name": "enable-autoscaling",
                      "description": "Enable cluster autoscaler",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "min-nodes",
                      "description": "Minimum nodes per zone (if autoscaling enabled)",
                      "type": "number",
                      "default": 1
                    },
                    {
                      "name": "max-nodes",
                      "description": "Maximum nodes per zone (if autoscaling enabled)",
                      "type": "number",
                      "default": 10
                    },
                    {
                      "name": "network",
                      "description": "VPC network name",
                      "required": false
                    },
                    {
                      "name": "subnetwork",
                      "description": "Subnetwork name",
                      "required": false
                    },
                    {
                      "name": "release-channel",
                      "description": "Release channel (rapid, regular, stable, None)",
                      "default": "regular"
                    }
                  ],
                  "tags": [
                    "gcp",
                    "gke",
                    "cluster-lifecycle",
                    "provisioning"
                  ]
                },
                "content": "# GCP Cluster Create\n\nCreate a new Google Kubernetes Engine (GKE) cluster with support for both Standard and Autopilot modes.\n\n## Overview\n\nThis command creates production-ready GKE clusters with:\n\n- **Two modes**: Standard (full control) or Autopilot (managed infrastructure)\n- **High Availability**: Regional or zonal deployment\n- **Auto-scaling**: Cluster and pod autoscaling options\n- **Security**: Workload Identity, GKE security features\n- **Monitoring**: Cloud Operations integration\n- **Networking**: VPC-native clusters with custom networking\n\n## Prerequisites\n\n- Google Cloud SDK installed (`gcloud --version` >= 450.0.0)\n- `kubectl` installed (>= 1.28.0)\n- Authenticated to GCP: `gcloud auth login`\n- Project configured: `gcloud config set project PROJECT_ID`\n- Required APIs enabled:\n  - `container.googleapis.com` (GKE)\n  - `compute.googleapis.com` (Compute Engine)\n  - `cloudresourcemanager.googleapis.com`\n- IAM permissions:\n  - `container.clusters.create`\n  - `compute.networks.create` (if creating network)\n  - `iam.serviceAccounts.create`\n\n## Cluster Modes\n\n### Standard Mode\n\n- **Full control** over node configuration and management\n- **Customizable** machine types, node pools, networking\n- **Pay per node** - charged for all provisioned nodes\n- **Best for**: Workloads requiring specific configurations, GPU workloads, Windows nodes\n\n### Autopilot Mode\n\n- **Fully managed** - Google manages nodes, scaling, security\n- **Pod-based pricing** - pay only for pod resource requests\n- **Optimized** for cost and operations\n- **Best for**: Standard workloads, teams wanting less operational overhead\n\n## Workflow\n\n### Phase 1: Pre-creation Validation\n\n#### 1.1 Validate Prerequisites\n\n```bash\nCLUSTER_NAME=\"${CLUSTER_NAME}\"\nCLUSTER_MODE=\"${CLUSTER_MODE:-standard}\"\nGCP_PROJECT=\"${GCP_PROJECT}\"\nGCP_REGION=\"${GCP_REGION}\"\nGCP_ZONE=\"${GCP_ZONE}\"\nK8S_VERSION=\"${K8S_VERSION}\"\nRELEASE_CHANNEL=\"${RELEASE_CHANNEL:-regular}\"\n\necho \"üîç Validating prerequisites for GKE cluster creation\"\necho \"\"\necho \"Cluster configuration:\"\necho \"  Name: $CLUSTER_NAME\"\necho \"  Mode: $CLUSTER_MODE\"\necho \"  Project: $GCP_PROJECT\"\nif [[ -n \"$GCP_REGION\" ]]; then\n    echo \"  Region: $GCP_REGION (regional cluster)\"\n    LOCATION=\"$GCP_REGION\"\n    LOCATION_TYPE=\"region\"\nelif [[ -n \"$GCP_ZONE\" ]]; then\n    echo \"  Zone: $GCP_ZONE (zonal cluster)\"\n    LOCATION=\"$GCP_ZONE\"\n    LOCATION_TYPE=\"zone\"\nelse\n    echo \"‚ùå ERROR: Either --region or --zone must be specified\"\n    exit 1\nfi\n\n# Set project\ngcloud config set project \"$GCP_PROJECT\"\n\n# Check if gcloud is authenticated\nif ! gcloud auth list --filter=status:ACTIVE --format=\"value(account)\" &>/dev/null; then\n    echo \"‚ùå ERROR: Not authenticated to GCP\"\n    echo \"   Run: gcloud auth login\"\n    exit 1\nfi\n\necho \"‚úÖ Authentication verified\"\n\n# Check if APIs are enabled\necho \"\"\necho \"Checking required APIs...\"\n\nREQUIRED_APIS=(\"container.googleapis.com\" \"compute.googleapis.com\")\nfor API in \"${REQUIRED_APIS[@]}\"; do\n    if gcloud services list --enabled --filter=\"name:$API\" --format=\"value(name)\" | grep -q \"$API\"; then\n        echo \"  ‚úÖ $API enabled\"\n    else\n        echo \"  ‚ö†Ô∏è  $API not enabled. Enabling...\"\n        gcloud services enable \"$API\"\n    fi\ndone\n```\n\n#### 1.2 Check Cluster Name Availability\n\n```bash\necho \"\"\necho \"Checking cluster name availability...\"\n\nif gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" &>/dev/null; then\n    echo \"‚ùå ERROR: Cluster '$CLUSTER_NAME' already exists in $LOCATION\"\n    exit 1\nfi\n\necho \"‚úÖ Cluster name available\"\n```\n\n#### 1.3 Determine Kubernetes Version\n\n```bash\nif [[ -z \"$K8S_VERSION\" ]]; then\n    echo \"\"\n    echo \"üìã Fetching available Kubernetes versions...\"\n\n    if [[ \"$RELEASE_CHANNEL\" == \"None\" ]]; then\n        # Get default version for static version clusters\n        K8S_VERSION=$(gcloud container get-server-config \\\n            --$LOCATION_TYPE=\"$LOCATION\" \\\n            --format=\"value(defaultClusterVersion)\")\n        echo \"   Using default version: $K8S_VERSION\"\n    else\n        # Get default version for the release channel\n        K8S_VERSION=$(gcloud container get-server-config \\\n            --$LOCATION_TYPE=\"$LOCATION\" \\\n            --format=\"value(channels.$RELEASE_CHANNEL.defaultVersion)\")\n        echo \"   Using $RELEASE_CHANNEL channel version: $K8S_VERSION\"\n    fi\nelse\n    echo \"   Using specified version: $K8S_VERSION\"\nfi\n```\n\n### Phase 2: Network Configuration\n\n#### 2.1 Setup VPC Network\n\n```bash\nNETWORK_NAME=\"${NETWORK:-gke-network-$CLUSTER_NAME}\"\nSUBNET_NAME=\"${SUBNETWORK:-gke-subnet-$CLUSTER_NAME}\"\n\necho \"\"\necho \"üåê Configuring network...\"\n\n# Check if network exists\nif gcloud compute networks describe \"$NETWORK_NAME\" &>/dev/null; then\n    echo \"   Using existing network: $NETWORK_NAME\"\nelse\n    echo \"   Creating VPC network: $NETWORK_NAME\"\n    gcloud compute networks create \"$NETWORK_NAME\" \\\n        --subnet-mode=custom \\\n        --bgp-routing-mode=regional\nfi\n\n# Check if subnet exists\nif gcloud compute networks subnets describe \"$SUBNET_NAME\" --region=\"$GCP_REGION\" &>/dev/null 2>&1; then\n    echo \"   Using existing subnet: $SUBNET_NAME\"\nelse\n    echo \"   Creating subnet: $SUBNET_NAME\"\n\n    # Determine region from zone if needed\n    if [[ \"$LOCATION_TYPE\" == \"zone\" ]]; then\n        SUBNET_REGION=$(echo \"$GCP_ZONE\" | sed 's/-[a-z]$//')\n    else\n        SUBNET_REGION=\"$GCP_REGION\"\n    fi\n\n    gcloud compute networks subnets create \"$SUBNET_NAME\" \\\n        --network=\"$NETWORK_NAME\" \\\n        --region=\"$SUBNET_REGION\" \\\n        --range=10.0.0.0/20 \\\n        --secondary-range pods=10.4.0.0/14 \\\n        --secondary-range services=10.0.16.0/20 \\\n        --enable-private-ip-google-access \\\n        --enable-flow-logs\nfi\n\necho \"‚úÖ Network configuration completed\"\n```\n\n#### 2.2 Configure Firewall Rules\n\n```bash\necho \"\"\necho \"üî• Configuring firewall rules...\"\n\n# Allow internal communication\nif ! gcloud compute firewall-rules describe \"gke-$CLUSTER_NAME-internal\" &>/dev/null; then\n    gcloud compute firewall-rules create \"gke-$CLUSTER_NAME-internal\" \\\n        --network=\"$NETWORK_NAME\" \\\n        --allow=tcp,udp,icmp \\\n        --source-ranges=10.0.0.0/8\nfi\n\n# Allow SSH from IAP\nif ! gcloud compute firewall-rules describe \"gke-$CLUSTER_NAME-ssh-iap\" &>/dev/null; then\n    gcloud compute firewall-rules create \"gke-$CLUSTER_NAME-ssh-iap\" \\\n        --network=\"$NETWORK_NAME\" \\\n        --allow=tcp:22 \\\n        --source-ranges=35.235.240.0/20\nfi\n\necho \"‚úÖ Firewall rules configured\"\n```\n\n### Phase 3: Cluster Creation\n\n#### 3.1 Build Cluster Creation Command\n\n```bash\necho \"\"\necho \"üöÄ Creating GKE cluster...\"\necho \"\"\n\n# Base command\nif [[ \"$CLUSTER_MODE\" == \"autopilot\" ]]; then\n    CREATE_CMD=\"gcloud container clusters create-auto \\\"$CLUSTER_NAME\\\"\"\nelse\n    CREATE_CMD=\"gcloud container clusters create \\\"$CLUSTER_NAME\\\"\"\nfi\n\n# Location\nCREATE_CMD=\"$CREATE_CMD --$LOCATION_TYPE=\\\"$LOCATION\\\"\"\n\n# Network\nCREATE_CMD=\"$CREATE_CMD --network=\\\"$NETWORK_NAME\\\"\"\nCREATE_CMD=\"$CREATE_CMD --subnetwork=\\\"$SUBNET_NAME\\\"\"\nCREATE_CMD=\"$CREATE_CMD --enable-ip-alias\"\nCREATE_CMD=\"$CREATE_CMD --cluster-secondary-range-name=pods\"\nCREATE_CMD=\"$CREATE_CMD --services-secondary-range-name=services\"\n\n# Release channel or version\nif [[ \"$RELEASE_CHANNEL\" != \"None\" ]]; then\n    CREATE_CMD=\"$CREATE_CMD --release-channel=\\\"$RELEASE_CHANNEL\\\"\"\nelse\n    CREATE_CMD=\"$CREATE_CMD --cluster-version=\\\"$K8S_VERSION\\\"\"\n    CREATE_CMD=\"$CREATE_CMD --no-enable-autoupgrade\"\nfi\n\n# Standard mode specific settings\nif [[ \"$CLUSTER_MODE\" == \"standard\" ]]; then\n    NUM_NODES=\"${NUM_NODES:-3}\"\n    MACHINE_TYPE=\"${MACHINE_TYPE:-e2-medium}\"\n    DISK_SIZE=\"${DISK_SIZE:-100}\"\n\n    CREATE_CMD=\"$CREATE_CMD --num-nodes=\\\"$NUM_NODES\\\"\"\n    CREATE_CMD=\"$CREATE_CMD --machine-type=\\\"$MACHINE_TYPE\\\"\"\n    CREATE_CMD=\"$CREATE_CMD --disk-size=\\\"$DISK_SIZE\\\"\"\n    CREATE_CMD=\"$CREATE_CMD --disk-type=pd-standard\"\n    CREATE_CMD=\"$CREATE_CMD --image-type=COS_CONTAINERD\"\n\n    # Autoscaling\n    if [[ \"${ENABLE_AUTOSCALING}\" == \"true\" ]]; then\n        MIN_NODES=\"${MIN_NODES:-1}\"\n        MAX_NODES=\"${MAX_NODES:-10}\"\n        CREATE_CMD=\"$CREATE_CMD --enable-autoscaling\"\n        CREATE_CMD=\"$CREATE_CMD --min-nodes=\\\"$MIN_NODES\\\"\"\n        CREATE_CMD=\"$CREATE_CMD --max-nodes=\\\"$MAX_NODES\\\"\"\n    fi\n\n    # Node pool settings\n    CREATE_CMD=\"$CREATE_CMD --enable-autorepair\"\n    CREATE_CMD=\"$CREATE_CMD --enable-autoupgrade\"\n    CREATE_CMD=\"$CREATE_CMD --max-surge-upgrade=1\"\n    CREATE_CMD=\"$CREATE_CMD --max-unavailable-upgrade=0\"\nfi\n\n# Security features\nCREATE_CMD=\"$CREATE_CMD --enable-shielded-nodes\"\nCREATE_CMD=\"$CREATE_CMD --workload-pool=\\\"$GCP_PROJECT.svc.id.goog\\\"\"\nCREATE_CMD=\"$CREATE_CMD --enable-network-policy\"\n\n# Monitoring and logging\nCREATE_CMD=\"$CREATE_CMD --enable-cloud-logging\"\nCREATE_CMD=\"$CREATE_CMD --enable-cloud-monitoring\"\nCREATE_CMD=\"$CREATE_CMD --logging=SYSTEM,WORKLOAD\"\nCREATE_CMD=\"$CREATE_CMD --monitoring=SYSTEM\"\n\n# Additional features\nCREATE_CMD=\"$CREATE_CMD --addons=HorizontalPodAutoscaling,HttpLoadBalancing,GcePersistentDiskCsiDriver\"\nCREATE_CMD=\"$CREATE_CMD --enable-stackdriver-kubernetes\"\n\n# Maintenance window (02:00-06:00 UTC)\nCREATE_CMD=\"$CREATE_CMD --maintenance-window-start=2024-01-01T02:00:00Z\"\nCREATE_CMD=\"$CREATE_CMD --maintenance-window-duration=4h\"\nCREATE_CMD=\"$CREATE_CMD --maintenance-window-recurrence='FREQ=WEEKLY;BYDAY=SA'\"\n\necho \"Cluster creation command:\"\necho \"$CREATE_CMD\"\necho \"\"\necho \"‚è≥ This may take 5-10 minutes...\"\necho \"\"\n```\n\n#### 3.2 Execute Cluster Creation\n\n```bash\n# Execute the command\neval \"$CREATE_CMD\"\n\nif [[ $? -eq 0 ]]; then\n    echo \"\"\n    echo \"‚úÖ GKE cluster created successfully\"\nelse\n    echo \"\"\n    echo \"‚ùå Cluster creation failed\"\n    exit 1\nfi\n```\n\n### Phase 4: Post-Creation Configuration\n\n#### 4.1 Configure kubectl\n\n```bash\necho \"\"\necho \"‚öôÔ∏è  Configuring kubectl access...\"\n\ngcloud container clusters get-credentials \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --project=\"$GCP_PROJECT\"\n\necho \"‚úÖ kubectl configured\"\n\n# Verify connection\necho \"\"\necho \"Testing cluster connection...\"\nif kubectl cluster-info &>/dev/null; then\n    echo \"‚úÖ Cluster connection verified\"\nelse\n    echo \"‚ùå Failed to connect to cluster\"\n    exit 1\nfi\n```\n\n#### 4.2 Install Essential Components\n\n```bash\necho \"\"\necho \"üì¶ Installing essential components...\"\n\n# Install metrics-server (if not autopilot, which includes it)\nif [[ \"$CLUSTER_MODE\" == \"standard\" ]]; then\n    echo \"   Installing metrics-server...\"\n    kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nfi\n\n# Create monitoring namespace\nkubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -\n\n# Setup Workload Identity for common namespaces\nfor NS in default kube-system monitoring; do\n    if [[ \"$NS\" != \"kube-system\" ]]; then\n        kubectl create namespace \"$NS\" --dry-run=client -o yaml | kubectl apply -f -\n    fi\n\n    # Annotate service account for Workload Identity\n    kubectl annotate serviceaccount default \\\n        -n \"$NS\" \\\n        iam.gke.io/gcp-service-account=\"${CLUSTER_NAME}-sa@${GCP_PROJECT}.iam.gserviceaccount.com\" \\\n        --overwrite 2>/dev/null || true\ndone\n\necho \"‚úÖ Essential components installed\"\n```\n\n#### 4.3 Apply Security Policies\n\n```bash\necho \"\"\necho \"üîí Applying security policies...\"\n\n# Create Pod Security Standards\nkubectl create namespace prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl label namespace prod \\\n    pod-security.kubernetes.io/enforce=restricted \\\n    pod-security.kubernetes.io/audit=restricted \\\n    pod-security.kubernetes.io/warn=restricted\n\nkubectl label namespace default \\\n    pod-security.kubernetes.io/enforce=baseline \\\n    pod-security.kubernetes.io/audit=baseline \\\n    pod-security.kubernetes.io/warn=baseline\n\n# Create default NetworkPolicy (deny all ingress)\ncat <<EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: default\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\nEOF\n\necho \"‚úÖ Security policies applied\"\n```\n\n#### 4.4 Cluster Information and Diagnostics\n\n```bash\necho \"\"\necho \"üìä Gathering cluster information...\"\n\n# Cluster details\necho \"\"\necho \"Cluster Details:\"\necho \"================\"\ngcloud container clusters describe \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --format=\"table(\n        name,\n        location,\n        currentMasterVersion,\n        currentNodeCount,\n        status\n    )\"\n\n# Node information\necho \"\"\necho \"Nodes:\"\necho \"======\"\nkubectl get nodes -o wide\n\n# Check cluster health\necho \"\"\necho \"Cluster Health:\"\necho \"===============\"\n\nREADY_NODES=$(kubectl get nodes --no-headers | grep -c \" Ready \")\nTOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)\n\necho \"Ready Nodes: $READY_NODES / $TOTAL_NODES\"\n\nif [[ \"$READY_NODES\" -eq \"$TOTAL_NODES\" ]]; then\n    echo \"‚úÖ All nodes ready\"\nelse\n    echo \"‚ö†Ô∏è  Some nodes not ready\"\nfi\n\n# System pods\necho \"\"\necho \"System Pods:\"\necho \"============\"\nkubectl get pods -n kube-system\n\n# Get cluster endpoint\nCLUSTER_ENDPOINT=$(gcloud container clusters describe \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --format=\"value(endpoint)\")\n\necho \"\"\necho \"Cluster Endpoint: https://$CLUSTER_ENDPOINT\"\n```\n\n### Phase 5: Cost Estimation\n\n```bash\necho \"\"\necho \"üí∞ COST ESTIMATION\"\necho \"==================\"\n\nif [[ \"$CLUSTER_MODE\" == \"autopilot\" ]]; then\n    echo \"\"\n    echo \"Autopilot Mode - Pay per pod resource request\"\n    echo \"\"\n    echo \"Estimated costs (approximate):\"\n    echo \"  ‚Ä¢ Base: \\$0.10 per vCPU per hour\"\n    echo \"  ‚Ä¢ Base: \\$0.011 per GB memory per hour\"\n    echo \"  ‚Ä¢ No charge for cluster management\"\n    echo \"\"\n    echo \"Example: 10 pods @ 0.5 vCPU, 1GB RAM each\"\n    echo \"  ‚Ä¢ Monthly: ~\\$182 (vCPU) + ~\\$40 (memory) = ~\\$222\"\n\nelse\n    HOURLY_COST=$(echo \"scale=2; $NUM_NODES * 0.05\" | bc)  # Rough estimate for e2-medium\n    MONTHLY_COST=$(echo \"scale=2; $HOURLY_COST * 730\" | bc)\n\n    echo \"\"\n    echo \"Standard Mode - Pay per node\"\n    echo \"\"\n    echo \"Current configuration:\"\n    echo \"  ‚Ä¢ Machine Type: $MACHINE_TYPE\"\n    echo \"  ‚Ä¢ Number of Nodes: $NUM_NODES\"\n    if [[ \"${ENABLE_AUTOSCALING}\" == \"true\" ]]; then\n        echo \"  ‚Ä¢ Autoscaling: $MIN_NODES - $MAX_NODES nodes\"\n    fi\n    echo \"\"\n    echo \"Estimated costs (very approximate):\"\n    echo \"  ‚Ä¢ Per node per hour: ~\\$0.05\"\n    echo \"  ‚Ä¢ Total per hour: ~\\$$HOURLY_COST\"\n    echo \"  ‚Ä¢ Total per month: ~\\$$MONTHLY_COST\"\n    echo \"\"\n    echo \"Note: Actual costs vary by machine type, region, and usage\"\nfi\n\necho \"\"\necho \"For accurate pricing, use: https://cloud.google.com/products/calculator\"\n```\n\n### Phase 6: Summary and Next Steps\n\n```bash\necho \"\"\necho \"‚úÖ CLUSTER CREATION COMPLETE\"\necho \"============================\"\necho \"\"\necho \"Cluster: $CLUSTER_NAME\"\necho \"Mode: $CLUSTER_MODE\"\necho \"Location: $LOCATION\"\necho \"Version: $K8S_VERSION\"\nif [[ \"$CLUSTER_MODE\" == \"standard\" ]]; then\n    echo \"Nodes: $NUM_NODES x $MACHINE_TYPE\"\nfi\necho \"\"\necho \"Next steps:\"\necho \"\"\necho \"1. Deploy your first application:\"\necho \"   kubectl create deployment hello --image=gcr.io/google-samples/hello-app:1.0\"\necho \"   kubectl expose deployment hello --type=LoadBalancer --port=80 --target-port=8080\"\necho \"\"\necho \"2. Enable additional features:\"\necho \"   ‚Ä¢ Install Ingress controller: gcloud container clusters update $CLUSTER_NAME --update-addons=HttpLoadBalancing=ENABLED\"\necho \"   ‚Ä¢ Enable Binary Authorization for image security\"\necho \"   ‚Ä¢ Configure Cloud Armor for DDoS protection\"\necho \"\"\necho \"3. Setup monitoring:\"\necho \"   ‚Ä¢ View metrics in Cloud Console: https://console.cloud.google.com/kubernetes/clusters/$LOCATION/$CLUSTER_NAME\"\necho \"   ‚Ä¢ Install GKE Dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\"\necho \"\"\necho \"4. Configure CI/CD:\"\necho \"   ‚Ä¢ Setup Cloud Build triggers\"\necho \"   ‚Ä¢ Configure Artifact Registry\"\necho \"   ‚Ä¢ Implement GitOps with Config Sync\"\necho \"\"\necho \"5. Review security:\"\necho \"   ‚Ä¢ Enable Binary Authorization\"\necho \"   ‚Ä¢ Configure Workload Identity for all workloads\"\necho \"   ‚Ä¢ Review GKE Security Posture dashboard\"\necho \"\"\necho \"Documentation: https://cloud.google.com/kubernetes-engine/docs\"\necho \"\"\n```\n\n## Cluster Modes Comparison\n\n| Feature | Standard | Autopilot |\n|---------|----------|-----------|\n| **Node Management** | Manual | Fully managed |\n| **Pricing** | Per node | Per pod request |\n| **Customization** | Full control | Limited |\n| **Operations** | More overhead | Less overhead |\n| **GPU Support** | Yes | Yes |\n| **Windows Nodes** | Yes | No |\n| **Best For** | Custom configs | Standard workloads |\n\n## Common Issues\n\n### Issue: Insufficient quota\n\n**Error**: `Quota exceeded for quota metric 'compute.googleapis.com/cpus'`\n\n**Solution**: Request quota increase:\n```bash\ngcloud compute project-info describe --project=$PROJECT_ID\n# Request increase in GCP Console > IAM & Admin > Quotas\n```\n\n### Issue: VPC network errors\n\n**Solution**: Ensure VPC has sufficient IP ranges:\n```bash\n# Check subnet ranges\ngcloud compute networks subnets describe $SUBNET_NAME --region=$REGION\n\n# Expand if needed\ngcloud compute networks subnets expand-ip-range $SUBNET_NAME \\\n    --region=$REGION \\\n    --prefix-length=16\n```\n\n### Issue: Autopilot pod scheduling failures\n\n**Solution**: Check pod resource requests match Autopilot constraints:\n- vCPU: 0.25-110 in increments of 0.25\n- Memory: Must match CPU (1:4 ratio typical)\n\n## Examples\n\n### Example 1: Create regional Autopilot cluster (recommended for production)\n\n```bash\ncluster-code gcp-cluster-create \\\n  --cluster-name prod-cluster \\\n  --mode autopilot \\\n  --project my-project \\\n  --region us-central1 \\\n  --release-channel regular\n```\n\n### Example 2: Create Standard cluster with autoscaling\n\n```bash\ncluster-code gcp-cluster-create \\\n  --cluster-name dev-cluster \\\n  --mode standard \\\n  --project my-project \\\n  --zone us-central1-a \\\n  --machine-type n2-standard-4 \\\n  --enable-autoscaling \\\n  --min-nodes 2 \\\n  --max-nodes 20\n```\n\n### Example 3: Create high-performance Standard cluster\n\n```bash\ncluster-code gcp-cluster-create \\\n  --cluster-name ml-cluster \\\n  --mode standard \\\n  --project my-ml-project \\\n  --region us-central1 \\\n  --machine-type n2-highmem-8 \\\n  --disk-size 200 \\\n  --num-nodes 5\n```\n\n## Related Commands\n\n- `gcp-cluster-delete`: Delete GKE clusters\n- `gcp-cluster-upgrade`: Upgrade cluster version\n- `cluster-diagnose`: Run diagnostics\n- `backup-cluster`: Create cluster backup"
              },
              {
                "name": "/gcp-cluster-delete",
                "description": "Safely delete a GKE cluster with validation and backup",
                "path": "plugins/cloud-gcp/commands/gcp-cluster-delete.md",
                "frontmatter": {
                  "name": "gcp-cluster-delete",
                  "description": "Safely delete a GKE cluster with validation and backup",
                  "category": "cloud-provisioning",
                  "parameters": [
                    {
                      "name": "cluster-name",
                      "description": "Name of the cluster to delete",
                      "required": true
                    },
                    {
                      "name": "project",
                      "description": "GCP project ID",
                      "required": true
                    },
                    {
                      "name": "region",
                      "description": "GCP region where cluster is located",
                      "required": false
                    },
                    {
                      "name": "zone",
                      "description": "GCP zone where cluster is located",
                      "required": false
                    },
                    {
                      "name": "skip-backup",
                      "description": "Skip automatic backup before deletion",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "force",
                      "description": "Skip all confirmation prompts (dangerous)",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "delete-network",
                      "description": "Delete associated VPC network and subnetwork",
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "tags": [
                    "gcp",
                    "gke",
                    "cluster-lifecycle",
                    "deletion",
                    "safety"
                  ]
                },
                "content": "# GCP Cluster Delete\n\nSafely delete a GKE cluster with comprehensive validation, automatic backup, and resource cleanup.\n\n## Overview\n\nThis command provides a safe way to delete GKE clusters (both Standard and Autopilot modes) with:\n\n- **Production Detection**: Warns if cluster has production labels\n- **Resource Analysis**: Lists all resources that will be deleted\n- **Automatic Backup**: Creates backup before deletion (unless skipped)\n- **Confirmation Prompts**: Requires explicit confirmation\n- **Complete Cleanup**: Removes cluster and optionally networking resources\n\n**‚ö†Ô∏è WARNING: This is a destructive operation that cannot be undone. All cluster data will be permanently deleted.**\n\n## Prerequisites\n\n- Google Cloud SDK installed (`gcloud --version`)\n- `kubectl` configured with cluster access\n- Authenticated to GCP (`gcloud auth list`)\n- IAM permissions:\n  - `container.clusters.delete`\n  - `container.clusters.get`\n  - `compute.networks.delete` (if deleting network)\n  - `compute.subnetworks.delete` (if deleting network)\n\n## Safety Features\n\n1. **Production Detection**: Warns if cluster has production labels\n2. **Resource Counting**: Shows number of workloads, PVs, services\n3. **Persistent Volume Warning**: Alerts about data loss\n4. **LoadBalancer Detection**: Warns about external load balancers\n5. **Automatic Backup**: Creates pre-deletion backup\n6. **Dual Confirmation**: Requires cluster name + yes/no confirmation\n7. **Grace Period**: 10-second cancellation window\n\n## Workflow\n\n### Phase 1: Pre-deletion Validation\n\n#### 1.1 Verify Cluster Exists\n\n```bash\nCLUSTER_NAME=\"${CLUSTER_NAME}\"\nGCP_PROJECT=\"${GCP_PROJECT}\"\nGCP_REGION=\"${GCP_REGION}\"\nGCP_ZONE=\"${GCP_ZONE}\"\n\n# Determine location type\nif [[ -n \"$GCP_REGION\" ]]; then\n    LOCATION=\"$GCP_REGION\"\n    LOCATION_TYPE=\"region\"\nelif [[ -n \"$GCP_ZONE\" ]]; then\n    LOCATION=\"$GCP_ZONE\"\n    LOCATION_TYPE=\"zone\"\nelse\n    echo \"‚ùå ERROR: Either --region or --zone must be specified\"\n    exit 1\nfi\n\necho \"üîç Verifying cluster: $CLUSTER_NAME in $LOCATION\"\n\n# Set project\ngcloud config set project \"$GCP_PROJECT\"\n\n# Check if cluster exists\nif ! gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" &>/dev/null; then\n    echo \"‚ùå ERROR: Cluster '$CLUSTER_NAME' not found in $LOCATION\"\n    echo \"\"\n    echo \"Available clusters:\"\n    gcloud container clusters list --format=\"table(name,location,status)\"\n    exit 1\nfi\n\n# Get cluster details\nCLUSTER_INFO=$(gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" --format=json)\nCLUSTER_STATUS=$(echo \"$CLUSTER_INFO\" | jq -r '.status')\nCLUSTER_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.currentMasterVersion')\nCLUSTER_MODE=$(echo \"$CLUSTER_INFO\" | jq -r '.autopilot.enabled // false')\nNODE_COUNT=$(echo \"$CLUSTER_INFO\" | jq -r '.currentNodeCount // 0')\nNETWORK=$(echo \"$CLUSTER_INFO\" | jq -r '.network')\nSUBNETWORK=$(echo \"$CLUSTER_INFO\" | jq -r '.subnetwork')\n\nif [[ \"$CLUSTER_MODE\" == \"true\" ]]; then\n    MODE=\"Autopilot\"\nelse\n    MODE=\"Standard\"\nfi\n\necho \"‚úÖ Cluster found: $CLUSTER_NAME\"\necho \"   Mode: $MODE\"\necho \"   Status: $CLUSTER_STATUS\"\necho \"   Version: $CLUSTER_VERSION\"\necho \"   Nodes: $NODE_COUNT\"\necho \"   Network: $NETWORK\"\necho \"   Subnetwork: $SUBNETWORK\"\n```\n\n#### 1.2 Analyze Cluster Resources\n\n```bash\necho \"\"\necho \"üìä Analyzing cluster resources...\"\n\n# Get kubeconfig\ngcloud container clusters get-credentials \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --project=\"$GCP_PROJECT\"\n\n# Count resources\nNAMESPACE_COUNT=$(kubectl get namespaces --no-headers 2>/dev/null | wc -l)\nPOD_COUNT=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | wc -l)\nSERVICE_COUNT=$(kubectl get services --all-namespaces --no-headers 2>/dev/null | wc -l)\nPV_COUNT=$(kubectl get pv --no-headers 2>/dev/null | wc -l)\nPVC_COUNT=$(kubectl get pvc --all-namespaces --no-headers 2>/dev/null | wc -l)\nDEPLOYMENT_COUNT=$(kubectl get deployments --all-namespaces --no-headers 2>/dev/null | wc -l)\nINGRESS_COUNT=$(kubectl get ingress --all-namespaces --no-headers 2>/dev/null | wc -l)\n\necho \"   Namespaces: $NAMESPACE_COUNT\"\necho \"   Pods: $POD_COUNT\"\necho \"   Services: $SERVICE_COUNT\"\necho \"   Deployments: $DEPLOYMENT_COUNT\"\necho \"   Ingresses: $INGRESS_COUNT\"\necho \"   Persistent Volumes: $PV_COUNT\"\necho \"   Persistent Volume Claims: $PVC_COUNT\"\n\n# Get GCP-specific resources\nLB_COUNT=$(kubectl get services --all-namespaces -o json 2>/dev/null | jq '[.items[] | select(.spec.type==\"LoadBalancer\")] | length')\necho \"   LoadBalancer Services: $LB_COUNT\"\n```\n\n#### 1.3 Safety Checks and Warnings\n\n```bash\necho \"\"\necho \"‚ö†Ô∏è  SAFETY CHECKS\"\necho \"=================\"\n\nWARNINGS=0\n\n# Check for production labels\nLABELS=$(echo \"$CLUSTER_INFO\" | jq -r '.resourceLabels // {}')\nENV_LABEL=$(echo \"$LABELS\" | jq -r '.environment // .env // \"\"')\n\nif [[ \"$ENV_LABEL\" =~ ^(prod|production|prd)$ ]]; then\n    echo \"‚ö†Ô∏è  WARNING: Cluster is labeled as PRODUCTION\"\n    echo \"   Label: environment=$ENV_LABEL\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check for GKE release channel\nRELEASE_CHANNEL=$(echo \"$CLUSTER_INFO\" | jq -r '.releaseChannel.channel // \"UNSPECIFIED\"')\nif [[ \"$RELEASE_CHANNEL\" == \"STABLE\" ]]; then\n    echo \"‚ö†Ô∏è  WARNING: Cluster uses STABLE release channel (typical for production)\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check for persistent volumes\nif [[ $PV_COUNT -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $PV_COUNT Persistent Volumes will be deleted\"\n    echo \"   This may result in DATA LOSS if volumes contain important data\"\n    WARNINGS=$((WARNINGS + 1))\n\n    # List PV types\n    echo \"\"\n    echo \"   Persistent Volume Types:\"\n    kubectl get pv -o custom-columns=NAME:.metadata.name,TYPE:.spec.gcePersistentDisk.pdName,SIZE:.spec.capacity.storage 2>/dev/null | head -10\nfi\n\n# Check for LoadBalancer services (creates GCP load balancers)\nif [[ $LB_COUNT -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $LB_COUNT LoadBalancer services found\"\n    echo \"   Associated GCP Load Balancers will be deleted\"\n    WARNINGS=$((WARNINGS + 1))\n\n    # List LoadBalancer services\n    echo \"\"\n    echo \"   LoadBalancer Services:\"\n    kubectl get services --all-namespaces -o json 2>/dev/null | \\\n        jq -r '.items[] | select(.spec.type==\"LoadBalancer\") | \"   - \\(.metadata.namespace)/\\(.metadata.name)\"' | head -10\nfi\n\n# Check for Ingress resources (creates GCP L7 load balancers)\nif [[ $INGRESS_COUNT -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $INGRESS_COUNT Ingress resources found\"\n    echo \"   Associated GCP HTTP(S) Load Balancers will be deleted\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Check cluster age\nCREATED_AT=$(echo \"$CLUSTER_INFO\" | jq -r '.createTime')\nif [[ \"$CREATED_AT\" != \"null\" ]]; then\n    CREATED_TIMESTAMP=$(date -d \"$CREATED_AT\" +%s 2>/dev/null || date -j -f \"%Y-%m-%dT%H:%M:%S\" \"$CREATED_AT\" +%s 2>/dev/null || echo \"0\")\n    if [[ $CREATED_TIMESTAMP -gt 0 ]]; then\n        DAYS_OLD=$(( ($(date +%s) - CREATED_TIMESTAMP) / 86400 ))\n        if [[ $DAYS_OLD -gt 30 ]]; then\n            echo \"‚ÑπÔ∏è  INFO: Cluster is $DAYS_OLD days old\"\n        fi\n    fi\nfi\n\n# Check for GKE managed services\nMANAGED_ADDONS=$(echo \"$CLUSTER_INFO\" | jq -r '.addonsConfig | to_entries[] | select(.value.disabled == false) | .key' 2>/dev/null)\nif [[ -n \"$MANAGED_ADDONS\" ]]; then\n    echo \"‚ÑπÔ∏è  INFO: Cluster has managed add-ons enabled\"\nfi\n\necho \"\"\necho \"Total warnings: $WARNINGS\"\n```\n\n### Phase 2: Backup (Optional)\n\n```bash\nif [[ \"${SKIP_BACKUP}\" != \"true\" ]]; then\n    echo \"\"\n    echo \"üíæ Creating backup before deletion...\"\n\n    BACKUP_DIR=\"./cluster-backup-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n    mkdir -p \"$BACKUP_DIR\"\n\n    echo \"   Backup directory: $BACKUP_DIR\"\n\n    # Backup all Kubernetes resources\n    echo \"   Backing up Kubernetes resources...\"\n    kubectl get all --all-namespaces -o yaml > \"$BACKUP_DIR/all-resources.yaml\" 2>/dev/null\n\n    # Backup persistent volumes and claims\n    echo \"   Backing up PV/PVC definitions...\"\n    kubectl get pv -o yaml > \"$BACKUP_DIR/persistent-volumes.yaml\" 2>/dev/null\n    kubectl get pvc --all-namespaces -o yaml > \"$BACKUP_DIR/persistent-volume-claims.yaml\" 2>/dev/null\n\n    # Backup ConfigMaps and Secrets\n    echo \"   Backing up ConfigMaps and Secrets...\"\n    kubectl get configmaps --all-namespaces -o yaml > \"$BACKUP_DIR/configmaps.yaml\" 2>/dev/null\n    kubectl get secrets --all-namespaces -o yaml > \"$BACKUP_DIR/secrets.yaml\" 2>/dev/null\n\n    # Backup CRDs and custom resources\n    echo \"   Backing up CRDs...\"\n    kubectl get crds -o yaml > \"$BACKUP_DIR/crds.yaml\" 2>/dev/null\n\n    # Backup Ingress and Services\n    echo \"   Backing up Ingress and Services...\"\n    kubectl get ingress --all-namespaces -o yaml > \"$BACKUP_DIR/ingress.yaml\" 2>/dev/null\n    kubectl get services --all-namespaces -o yaml > \"$BACKUP_DIR/services.yaml\" 2>/dev/null\n\n    # Backup GKE cluster configuration\n    echo \"   Backing up cluster configuration...\"\n    gcloud container clusters describe \"$CLUSTER_NAME\" \\\n        --$LOCATION_TYPE=\"$LOCATION\" \\\n        --format=json > \"$BACKUP_DIR/cluster-config.json\"\n\n    # Export node pool configurations (Standard mode only)\n    if [[ \"$MODE\" == \"Standard\" ]]; then\n        gcloud container node-pools list \\\n            --cluster=\"$CLUSTER_NAME\" \\\n            --$LOCATION_TYPE=\"$LOCATION\" \\\n            --format=json > \"$BACKUP_DIR/node-pools.json\" 2>/dev/null\n    fi\n\n    # Create README with restore instructions\n    cat > \"$BACKUP_DIR/README.md\" <<EOF\n# Cluster Backup: $CLUSTER_NAME\n\n**Backup Date**: $(date -u +\"%Y-%m-%d %H:%M:%S UTC\")\n**Cluster**: $CLUSTER_NAME\n**Project**: $GCP_PROJECT\n**Location**: $LOCATION ($LOCATION_TYPE)\n**Mode**: $MODE\n**Version**: $CLUSTER_VERSION\n\n## Contents\n\n- \\`cluster-config.json\\`: GKE cluster configuration\n- \\`all-resources.yaml\\`: All Kubernetes resources\n- \\`persistent-volumes.yaml\\`: PV definitions\n- \\`persistent-volume-claims.yaml\\`: PVC definitions\n- \\`configmaps.yaml\\`: ConfigMaps\n- \\`secrets.yaml\\`: Secrets (base64 encoded)\n- \\`crds.yaml\\`: Custom Resource Definitions\n- \\`ingress.yaml\\`: Ingress resources\n- \\`services.yaml\\`: Service definitions\n- \\`node-pools.json\\`: Node pool configurations (Standard mode)\n\n## Restore Instructions\n\n1. Create new cluster with similar configuration:\n   \\`\\`\\`bash\n   # Review cluster-config.json for settings\n   cluster-code gcp-cluster-create \\\\\n     --cluster-name $CLUSTER_NAME-restored \\\\\n     --project $GCP_PROJECT \\\\\n     --region/zone $LOCATION\n   \\`\\`\\`\n\n2. Restore CRDs first:\n   \\`\\`\\`bash\n   kubectl apply -f crds.yaml\n   \\`\\`\\`\n\n3. Restore ConfigMaps and Secrets:\n   \\`\\`\\`bash\n   kubectl apply -f configmaps.yaml\n   kubectl apply -f secrets.yaml\n   \\`\\`\\`\n\n4. Restore PVCs (will create new volumes):\n   \\`\\`\\`bash\n   kubectl apply -f persistent-volume-claims.yaml\n   \\`\\`\\`\n\n5. Restore workloads:\n   \\`\\`\\`bash\n   kubectl apply -f all-resources.yaml\n   \\`\\`\\`\n\n## Notes\n\n- PersistentVolume data is NOT backed up, only definitions\n- For data backup, use Velero or GCP snapshots\n- LoadBalancer services will get new external IPs\n- Ingress resources will get new GCP load balancers\n\nEOF\n\n    echo \"‚úÖ Backup completed: $BACKUP_DIR\"\nelse\n    echo \"‚ö†Ô∏è  Skipping backup (--skip-backup flag set)\"\nfi\n```\n\n### Phase 3: Confirmation\n\n```bash\necho \"\"\necho \"üö® FINAL CONFIRMATION REQUIRED\"\necho \"==============================\"\necho \"\"\necho \"You are about to DELETE the following cluster:\"\necho \"\"\necho \"  Cluster Name: $CLUSTER_NAME\"\necho \"  Mode: $MODE\"\necho \"  Project: $GCP_PROJECT\"\necho \"  Location: $LOCATION\"\necho \"  Resources: $NAMESPACE_COUNT namespaces, $POD_COUNT pods, $PV_COUNT PVs\"\necho \"\"\necho \"This action will:\"\necho \"  ‚ùå Delete the entire GKE cluster\"\necho \"  ‚ùå Delete all workloads and configurations\"\necho \"  ‚ùå Delete all persistent volumes and data\"\necho \"  ‚ùå Delete all GCP load balancers created by the cluster\"\nif [[ \"${DELETE_NETWORK}\" == \"true\" ]]; then\n    echo \"  ‚ùå Delete the VPC network and subnetwork\"\nfi\necho \"\"\necho \"‚ö†Ô∏è  THIS CANNOT BE UNDONE!\"\necho \"\"\n\nif [[ \"${FORCE}\" != \"true\" ]]; then\n    # First confirmation: type cluster name\n    echo \"To confirm, please type the cluster name: $CLUSTER_NAME\"\n    read -r CONFIRM_NAME\n\n    if [[ \"$CONFIRM_NAME\" != \"$CLUSTER_NAME\" ]]; then\n        echo \"‚ùå Cluster name does not match. Aborting.\"\n        exit 1\n    fi\n\n    # Second confirmation: yes/no\n    echo \"\"\n    echo \"Are you absolutely sure you want to delete this cluster? (yes/no)\"\n    read -r CONFIRM_DELETE\n\n    if [[ \"$CONFIRM_DELETE\" != \"yes\" ]]; then\n        echo \"‚ùå Deletion cancelled.\"\n        exit 0\n    fi\n\n    # Grace period\n    echo \"\"\n    echo \"‚è≥ Starting deletion in 10 seconds... (Ctrl+C to cancel)\"\n    sleep 10\nelse\n    echo \"‚ö†Ô∏è  FORCE mode enabled - skipping confirmations\"\nfi\n```\n\n### Phase 4: Deletion\n\n```bash\necho \"\"\necho \"üóëÔ∏è  Deleting cluster...\"\necho \"\"\n\n# Delete the cluster\necho \"‚è≥ Deleting GKE cluster (this may take 5-10 minutes)...\"\n\nDELETE_START=$(date +%s)\n\nif gcloud container clusters delete \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --quiet; then\n\n    DELETE_END=$(date +%s)\n    DELETE_TIME=$(( (DELETE_END - DELETE_START) / 60 ))\n\n    echo \"‚úÖ GKE cluster deleted successfully (took $DELETE_TIME minutes)\"\nelse\n    echo \"‚ùå Cluster deletion failed\"\n    exit 1\nfi\n```\n\n### Phase 5: Network Cleanup (Optional)\n\n```bash\nif [[ \"${DELETE_NETWORK}\" == \"true\" ]]; then\n    echo \"\"\n    echo \"üåê Cleaning up network resources...\"\n\n    # Extract network and subnet names from full paths\n    NETWORK_NAME=$(basename \"$NETWORK\")\n    SUBNET_NAME=$(basename \"$SUBNETWORK\")\n\n    # Determine region for subnet\n    if [[ \"$LOCATION_TYPE\" == \"zone\" ]]; then\n        SUBNET_REGION=$(echo \"$LOCATION\" | sed 's/-[a-z]$//')\n    else\n        SUBNET_REGION=\"$LOCATION\"\n    fi\n\n    # Check if network is used by other resources\n    echo \"   Checking if network is in use...\"\n    INSTANCES_IN_NETWORK=$(gcloud compute instances list \\\n        --filter=\"networkInterfaces.network:$NETWORK_NAME\" \\\n        --format=\"value(name)\" 2>/dev/null | wc -l)\n\n    OTHER_CLUSTERS=$(gcloud container clusters list \\\n        --filter=\"network:$NETWORK_NAME AND name!=$CLUSTER_NAME\" \\\n        --format=\"value(name)\" 2>/dev/null | wc -l)\n\n    if [[ $INSTANCES_IN_NETWORK -gt 0 || $OTHER_CLUSTERS -gt 0 ]]; then\n        echo \"‚ö†Ô∏è  WARNING: Network '$NETWORK_NAME' is still in use\"\n        echo \"   Instances: $INSTANCES_IN_NETWORK\"\n        echo \"   Other clusters: $OTHER_CLUSTERS\"\n        echo \"   Skipping network deletion for safety\"\n    else\n        # Delete firewall rules first\n        echo \"   Deleting firewall rules...\"\n        FIREWALL_RULES=$(gcloud compute firewall-rules list \\\n            --filter=\"network:$NETWORK_NAME\" \\\n            --format=\"value(name)\" 2>/dev/null)\n\n        for RULE in $FIREWALL_RULES; do\n            echo \"   - Deleting firewall rule: $RULE\"\n            gcloud compute firewall-rules delete \"$RULE\" --quiet 2>/dev/null || true\n        done\n\n        # Delete subnet\n        if [[ -n \"$SUBNET_NAME\" ]]; then\n            echo \"   Deleting subnet: $SUBNET_NAME\"\n            gcloud compute networks subnets delete \"$SUBNET_NAME\" \\\n                --region=\"$SUBNET_REGION\" \\\n                --quiet 2>/dev/null || echo \"   Failed to delete subnet (may not exist or in use)\"\n        fi\n\n        # Delete network\n        echo \"   Deleting network: $NETWORK_NAME\"\n        if gcloud compute networks delete \"$NETWORK_NAME\" --quiet 2>/dev/null; then\n            echo \"‚úÖ Network resources deleted\"\n        else\n            echo \"‚ö†Ô∏è  Failed to delete network (may have remaining dependencies)\"\n        fi\n    fi\nelse\n    echo \"\"\n    echo \"‚ÑπÔ∏è  Network resources retained (use --delete-network to remove)\"\nfi\n```\n\n### Phase 6: Post-Deletion Cleanup\n\n```bash\necho \"\"\necho \"üßπ Cleaning up local resources...\"\n\n# Remove kubectl context\nCONTEXT_NAME=\"gke_${GCP_PROJECT}_${LOCATION}_${CLUSTER_NAME}\"\nkubectl config delete-context \"$CONTEXT_NAME\" 2>/dev/null || true\nkubectl config delete-cluster \"$CONTEXT_NAME\" 2>/dev/null || true\nkubectl config unset users.\"$CONTEXT_NAME\" 2>/dev/null || true\n\necho \"‚úÖ Local cleanup completed\"\n```\n\n### Phase 7: Verification and Summary\n\n```bash\necho \"\"\necho \"üîç Verifying deletion...\"\n\n# Verify cluster is gone\nif gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" &>/dev/null; then\n    echo \"‚ö†Ô∏è  WARNING: Cluster still exists (deletion may be in progress)\"\nelse\n    echo \"‚úÖ Cluster deletion verified\"\nfi\n\n# Check for orphaned resources\necho \"\"\necho \"Checking for potential orphaned resources...\"\n\n# Check for disks that may have been from PVs\nORPHANED_DISKS=$(gcloud compute disks list \\\n    --filter=\"name~gke-$CLUSTER_NAME\" \\\n    --format=\"value(name)\" 2>/dev/null | wc -l)\n\nif [[ $ORPHANED_DISKS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  Found $ORPHANED_DISKS potentially orphaned disks:\"\n    gcloud compute disks list --filter=\"name~gke-$CLUSTER_NAME\" --format=\"table(name,zone,sizeGb,status)\"\n    echo \"\"\n    echo \"To delete orphaned disks:\"\n    echo \"  gcloud compute disks list --filter='name~gke-$CLUSTER_NAME' --format='value(name,zone)' | while read name zone; do\"\n    echo \"    gcloud compute disks delete \\$name --zone=\\$zone --quiet\"\n    echo \"  done\"\nfi\n\necho \"\"\necho \"‚úÖ CLUSTER DELETION COMPLETE\"\necho \"============================\"\necho \"\"\necho \"Cluster '$CLUSTER_NAME' has been successfully deleted.\"\necho \"\"\n\nif [[ \"${SKIP_BACKUP}\" != \"true\" ]]; then\n    echo \"Backup saved to: $BACKUP_DIR\"\n    echo \"\"\nfi\n\necho \"Next steps:\"\necho \"  ‚Ä¢ Verify GCP resources are fully cleaned up in Cloud Console\"\necho \"  ‚Ä¢ Check for orphaned disks, load balancers, and static IPs\"\necho \"  ‚Ä¢ Review firewall rules if custom rules were created\"\necho \"  ‚Ä¢ Update documentation and inventory\"\nif [[ \"${DELETE_NETWORK}\" != \"true\" ]]; then\n    echo \"  ‚Ä¢ Optionally clean up VPC network: $NETWORK\"\nfi\necho \"\"\necho \"GCP Console: https://console.cloud.google.com/kubernetes/list?project=$GCP_PROJECT\"\necho \"\"\n```\n\n## Common Issues\n\n### Issue: Cluster deletion stuck\n\n**Solution**: Check for resources blocking deletion:\n\n```bash\n# Check cluster status\ngcloud container clusters describe $CLUSTER_NAME --zone=$ZONE\n\n# Check for lingering node pools\ngcloud container node-pools list --cluster=$CLUSTER_NAME --zone=$ZONE\n\n# Force delete if necessary (use with caution)\ngcloud container clusters delete $CLUSTER_NAME --zone=$ZONE --async\n```\n\n### Issue: Load balancers not deleted\n\n**Solution**: Manually clean up GCP load balancers:\n\n```bash\n# List forwarding rules\ngcloud compute forwarding-rules list --filter=\"description~$CLUSTER_NAME\"\n\n# Delete forwarding rules\ngcloud compute forwarding-rules delete <NAME> --region=$REGION\n```\n\n### Issue: Orphaned persistent disks\n\n**Solution**: Identify and delete orphaned disks:\n\n```bash\n# List disks created by GKE\ngcloud compute disks list --filter=\"name~gke-$CLUSTER_NAME\"\n\n# Delete specific disk\ngcloud compute disks delete <DISK_NAME> --zone=$ZONE\n```\n\n## Examples\n\n### Example 1: Safe deletion with backup (recommended)\n\n```bash\ncluster-code gcp-cluster-delete \\\n  --cluster-name my-prod-cluster \\\n  --project my-project \\\n  --region us-central1\n```\n\n### Example 2: Quick deletion for dev cluster\n\n```bash\ncluster-code gcp-cluster-delete \\\n  --cluster-name dev-test-123 \\\n  --project my-dev-project \\\n  --zone us-central1-a \\\n  --skip-backup\n```\n\n### Example 3: Complete cleanup including network\n\n```bash\ncluster-code gcp-cluster-delete \\\n  --cluster-name old-cluster \\\n  --project my-project \\\n  --region europe-west1 \\\n  --delete-network\n```\n\n### Example 4: Zonal cluster deletion\n\n```bash\ncluster-code gcp-cluster-delete \\\n  --cluster-name staging-cluster \\\n  --project my-project \\\n  --zone asia-southeast1-a\n```\n\n## Related Commands\n\n- `gcp-cluster-create`: Create new GKE clusters\n- `gcp-cluster-upgrade`: Upgrade cluster version\n- `backup-cluster`: Create comprehensive cluster backup\n- `cluster-diagnose`: Run diagnostics before deletion"
              },
              {
                "name": "/gcp-cluster-upgrade",
                "description": "Upgrade a GKE cluster to a new Kubernetes version",
                "path": "plugins/cloud-gcp/commands/gcp-cluster-upgrade.md",
                "frontmatter": {
                  "name": "gcp-cluster-upgrade",
                  "description": "Upgrade a GKE cluster to a new Kubernetes version",
                  "category": "cloud-provisioning",
                  "parameters": [
                    {
                      "name": "cluster-name",
                      "description": "Name of the cluster to upgrade",
                      "required": true
                    },
                    {
                      "name": "project",
                      "description": "GCP project ID",
                      "required": true
                    },
                    {
                      "name": "region",
                      "description": "GCP region where cluster is located",
                      "required": false
                    },
                    {
                      "name": "zone",
                      "description": "GCP zone where cluster is located",
                      "required": false
                    },
                    {
                      "name": "version",
                      "description": "Target Kubernetes version (e.g., 1.29.1-gke.1000)",
                      "required": false
                    },
                    {
                      "name": "upgrade-nodes",
                      "description": "Automatically upgrade node pools after control plane",
                      "type": "boolean",
                      "default": true
                    },
                    {
                      "name": "skip-backup",
                      "description": "Skip pre-upgrade backup",
                      "type": "boolean",
                      "default": false
                    },
                    {
                      "name": "dry-run",
                      "description": "Show what would be upgraded without making changes",
                      "type": "boolean",
                      "default": false
                    }
                  ],
                  "tags": [
                    "gcp",
                    "gke",
                    "cluster-lifecycle",
                    "upgrade",
                    "maintenance"
                  ]
                },
                "content": "# GCP Cluster Upgrade\n\nSafely upgrade a GKE cluster (Standard or Autopilot) to a new Kubernetes version with automated validation and monitoring.\n\n## Overview\n\nThis command provides a comprehensive GKE cluster upgrade workflow with:\n\n- **Automatic Version Selection**: Uses release channel default if no version specified\n- **Pre-upgrade Validation**: Checks upgrade path and cluster health\n- **Backup**: Optional pre-upgrade backup\n- **Staged Upgrade**: Control plane first, then node pools\n- **Health Monitoring**: Continuous health checks throughout upgrade\n- **Rollback Guidance**: Instructions for rollback if issues occur\n\n## Prerequisites\n\n- Google Cloud SDK installed (`gcloud --version` >= 450.0.0)\n- `kubectl` configured with cluster admin access\n- Authenticated to GCP (`gcloud auth list`)\n- IAM permissions:\n  - `container.clusters.update`\n  - `container.clusters.get`\n  - `container.operations.get`\n\n## GKE Upgrade Features\n\n### Release Channels\n\nGKE offers three release channels that automatically manage upgrades:\n\n- **Rapid**: Latest features, weekly updates\n- **Regular**: Balanced updates, 2-3 months behind Rapid\n- **Stable**: Production-ready, 2-3 months behind Regular\n\nClusters in release channels auto-upgrade during maintenance windows.\n\n### Manual Version Selection\n\nClusters not enrolled in a release channel can specify exact versions but require manual upgrade management.\n\n## Workflow\n\n### Phase 1: Pre-upgrade Validation\n\n#### 1.1 Verify Cluster and Current Version\n\n```bash\nCLUSTER_NAME=\"${CLUSTER_NAME}\"\nGCP_PROJECT=\"${GCP_PROJECT}\"\nGCP_REGION=\"${GCP_REGION}\"\nGCP_ZONE=\"${GCP_ZONE}\"\nTARGET_VERSION=\"${TARGET_VERSION}\"\n\n# Determine location type\nif [[ -n \"$GCP_REGION\" ]]; then\n    LOCATION=\"$GCP_REGION\"\n    LOCATION_TYPE=\"region\"\nelif [[ -n \"$GCP_ZONE\" ]]; then\n    LOCATION=\"$GCP_ZONE\"\n    LOCATION_TYPE=\"zone\"\nelse\n    echo \"‚ùå ERROR: Either --region or --zone must be specified\"\n    exit 1\nfi\n\necho \"üîç Validating cluster upgrade: $CLUSTER_NAME\"\necho \"\"\n\n# Set project\ngcloud config set project \"$GCP_PROJECT\"\n\n# Check if cluster exists\nif ! gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" &>/dev/null; then\n    echo \"‚ùå ERROR: Cluster '$CLUSTER_NAME' not found in $LOCATION\"\n    exit 1\nfi\n\n# Get cluster details\nCLUSTER_INFO=$(gcloud container clusters describe \"$CLUSTER_NAME\" --$LOCATION_TYPE=\"$LOCATION\" --format=json)\nCURRENT_VERSION=$(echo \"$CLUSTER_INFO\" | jq -r '.currentMasterVersion')\nCLUSTER_STATUS=$(echo \"$CLUSTER_INFO\" | jq -r '.status')\nCLUSTER_MODE=$(echo \"$CLUSTER_INFO\" | jq -r '.autopilot.enabled // false')\nRELEASE_CHANNEL=$(echo \"$CLUSTER_INFO\" | jq -r '.releaseChannel.channel // \"UNSPECIFIED\"')\nNODE_COUNT=$(echo \"$CLUSTER_INFO\" | jq -r '.currentNodeCount // 0')\n\nif [[ \"$CLUSTER_MODE\" == \"true\" ]]; then\n    MODE=\"Autopilot\"\nelse\n    MODE=\"Standard\"\nfi\n\necho \"Cluster: $CLUSTER_NAME\"\necho \"Mode: $MODE\"\necho \"Current version: $CURRENT_VERSION\"\necho \"Status: $CLUSTER_STATUS\"\necho \"Release channel: $RELEASE_CHANNEL\"\necho \"Nodes: $NODE_COUNT\"\n\n# Verify cluster is in RUNNING state\nif [[ \"$CLUSTER_STATUS\" != \"RUNNING\" ]]; then\n    echo \"‚ùå ERROR: Cluster must be in RUNNING state for upgrade\"\n    echo \"   Current state: $CLUSTER_STATUS\"\n    exit 1\nfi\n\necho \"‚úÖ Cluster is ready for upgrade\"\n```\n\n#### 1.2 Determine Target Version\n\n```bash\necho \"\"\necho \"üìã Determining target version...\"\n\n# Get available versions and upgrade information\nSERVER_CONFIG=$(gcloud container get-server-config --$LOCATION_TYPE=\"$LOCATION\" --format=json)\n\nif [[ -z \"$TARGET_VERSION\" ]]; then\n    # Auto-select target version based on release channel or current version\n    if [[ \"$RELEASE_CHANNEL\" == \"UNSPECIFIED\" || \"$RELEASE_CHANNEL\" == \"null\" ]]; then\n        # No release channel - get next available version\n        VALID_VERSIONS=$(echo \"$SERVER_CONFIG\" | jq -r '.validMasterVersions[]')\n\n        # Find next minor version\n        CURRENT_MINOR=$(echo \"$CURRENT_VERSION\" | grep -oP '^\\d+\\.\\d+' || echo \"$CURRENT_VERSION\" | cut -d. -f1-2)\n        TARGET_VERSION=$(echo \"$VALID_VERSIONS\" | grep \"^$CURRENT_MINOR\" | head -1)\n\n        if [[ -z \"$TARGET_VERSION\" ]]; then\n            # Try next minor version\n            NEXT_MINOR=$(echo \"$CURRENT_MINOR\" | awk -F. '{print $1\".\"$2+1}')\n            TARGET_VERSION=$(echo \"$VALID_VERSIONS\" | grep \"^$NEXT_MINOR\" | head -1)\n        fi\n\n        if [[ -z \"$TARGET_VERSION\" ]]; then\n            echo \"‚ùå ERROR: Could not determine target version\"\n            echo \"   Available versions:\"\n            echo \"$VALID_VERSIONS\" | sed 's/^/   - /'\n            exit 1\n        fi\n\n        echo \"Auto-selected version: $TARGET_VERSION (latest compatible)\"\n    else\n        # Use release channel default\n        TARGET_VERSION=$(echo \"$SERVER_CONFIG\" | jq -r \".channels.${RELEASE_CHANNEL}.defaultVersion\")\n\n        if [[ \"$TARGET_VERSION\" == \"null\" || -z \"$TARGET_VERSION\" ]]; then\n            echo \"‚ùå ERROR: Could not determine default version for $RELEASE_CHANNEL channel\"\n            exit 1\n        fi\n\n        echo \"Release channel ($RELEASE_CHANNEL) version: $TARGET_VERSION\"\n    fi\nelse\n    echo \"Using specified version: $TARGET_VERSION\"\n\n    # Verify version is available\n    VALID_VERSIONS=$(echo \"$SERVER_CONFIG\" | jq -r '.validMasterVersions[]')\n    if ! echo \"$VALID_VERSIONS\" | grep -q \"^$TARGET_VERSION$\"; then\n        echo \"‚ùå ERROR: Version $TARGET_VERSION is not available in $LOCATION\"\n        echo \"\"\n        echo \"Available versions:\"\n        echo \"$VALID_VERSIONS\" | sed 's/^/  - /'\n        exit 1\n    fi\nfi\n\necho \"Target version: $TARGET_VERSION\"\n```\n\n#### 1.3 Validate Upgrade Path\n\n```bash\necho \"\"\necho \"‚úÖ Validating upgrade path...\"\n\n# Parse version components\nCURRENT_MAJOR=$(echo \"$CURRENT_VERSION\" | cut -d. -f1)\nCURRENT_MINOR=$(echo \"$CURRENT_VERSION\" | cut -d. -f2)\nCURRENT_PATCH=$(echo \"$CURRENT_VERSION\" | cut -d. -f3 | cut -d- -f1)\n\nTARGET_MAJOR=$(echo \"$TARGET_VERSION\" | cut -d. -f1)\nTARGET_MINOR=$(echo \"$TARGET_VERSION\" | cut -d. -f2)\nTARGET_PATCH=$(echo \"$TARGET_VERSION\" | cut -d. -f3 | cut -d- -f1)\n\n# Check for downgrades\nif [[ \"$TARGET_MAJOR\" -lt \"$CURRENT_MAJOR\" ]] || \\\n   [[ \"$TARGET_MAJOR\" -eq \"$CURRENT_MAJOR\" && \"$TARGET_MINOR\" -lt \"$CURRENT_MINOR\" ]]; then\n    echo \"‚ùå ERROR: Downgrading is not supported\"\n    echo \"   Current: $CURRENT_VERSION\"\n    echo \"   Target: $TARGET_VERSION\"\n    exit 1\nfi\n\n# Check for same version\nif [[ \"$CURRENT_VERSION\" == \"$TARGET_VERSION\" ]]; then\n    echo \"‚ÑπÔ∏è  Cluster is already at version $TARGET_VERSION\"\n    echo \"   No upgrade needed\"\n    exit 0\nfi\n\n# Check minor version skip\nMINOR_DIFF=$((TARGET_MINOR - CURRENT_MINOR))\nif [[ $MINOR_DIFF -gt 1 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: Skipping minor versions (from $CURRENT_MINOR to $TARGET_MINOR)\"\n    echo \"   GKE allows this, but it's recommended to upgrade one minor version at a time\"\nfi\n\necho \"‚úÖ Valid upgrade path: $CURRENT_VERSION ‚Üí $TARGET_VERSION\"\n```\n\n#### 1.4 Pre-upgrade Health Check\n\n```bash\necho \"\"\necho \"üè• Running pre-upgrade health check...\"\n\n# Get kubectl credentials\ngcloud container clusters get-credentials \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --project=\"$GCP_PROJECT\"\n\n# Check node health\necho \"   Checking node health...\"\nUNHEALTHY_NODES=$(kubectl get nodes --no-headers | grep -v \" Ready\" | wc -l)\nTOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)\n\nif [[ $UNHEALTHY_NODES -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $UNHEALTHY_NODES of $TOTAL_NODES nodes are not Ready\"\n    kubectl get nodes | grep -v \" Ready\"\nelse\n    echo \"‚úÖ All $TOTAL_NODES nodes are Ready\"\nfi\n\n# Check pod health\necho \"   Checking pod health...\"\nFAILING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers 2>/dev/null | wc -l)\n\nif [[ $FAILING_PODS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  WARNING: $FAILING_PODS pods are not Running/Succeeded\"\n    echo \"   Review: kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded\"\nelse\n    echo \"‚úÖ All pods are healthy\"\nfi\n\n# Check for deprecated APIs\necho \"   Checking for deprecated APIs...\"\nif command -v pluto &>/dev/null; then\n    pluto detect-all-in-cluster --target-versions k8s=v$TARGET_MAJOR.$TARGET_MINOR.0\nelse\n    echo \"‚ÑπÔ∏è  Install 'pluto' for deprecated API detection:\"\n    echo \"   brew install FairwindsOps/tap/pluto\"\nfi\n\n# Check PodDisruptionBudgets\nPDB_COUNT=$(kubectl get pdb --all-namespaces --no-headers 2>/dev/null | wc -l)\nif [[ $PDB_COUNT -gt 0 ]]; then\n    echo \"‚ÑπÔ∏è  Found $PDB_COUNT PodDisruptionBudgets (may affect node upgrade timing)\"\nfi\n\necho \"‚úÖ Pre-upgrade health check completed\"\n```\n\n### Phase 2: Backup (Optional)\n\n```bash\nif [[ \"${SKIP_BACKUP}\" != \"true\" && \"${DRY_RUN}\" != \"true\" ]]; then\n    echo \"\"\n    echo \"üíæ Creating pre-upgrade backup...\"\n\n    BACKUP_DIR=\"./cluster-backup-upgrade-$CLUSTER_NAME-$(date +%Y%m%d-%H%M%S)\"\n    mkdir -p \"$BACKUP_DIR\"\n\n    # Backup cluster configuration\n    gcloud container clusters describe \"$CLUSTER_NAME\" \\\n        --$LOCATION_TYPE=\"$LOCATION\" \\\n        --format=json > \"$BACKUP_DIR/cluster-config.json\"\n\n    # Backup node pool configurations (Standard mode only)\n    if [[ \"$MODE\" == \"Standard\" ]]; then\n        gcloud container node-pools list \\\n            --cluster=\"$CLUSTER_NAME\" \\\n            --$LOCATION_TYPE=\"$LOCATION\" \\\n            --format=json > \"$BACKUP_DIR/node-pools.json\" 2>/dev/null\n    fi\n\n    # Backup Kubernetes resources\n    kubectl get all --all-namespaces -o yaml > \"$BACKUP_DIR/all-resources.yaml\"\n    kubectl get pv,pvc --all-namespaces -o yaml > \"$BACKUP_DIR/volumes.yaml\"\n    kubectl get configmap,secret --all-namespaces -o yaml > \"$BACKUP_DIR/configs-secrets.yaml\"\n    kubectl get crds -o yaml > \"$BACKUP_DIR/crds.yaml\" 2>/dev/null\n\n    echo \"‚úÖ Backup saved to: $BACKUP_DIR\"\nfi\n```\n\n### Phase 3: Upgrade Execution\n\n#### 3.1 Upgrade Summary and Confirmation\n\n```bash\necho \"\"\necho \"üöÄ UPGRADE EXECUTION\"\necho \"====================\"\n\nif [[ \"${DRY_RUN}\" == \"true\" ]]; then\n    echo \"\"\n    echo \"üß™ DRY RUN MODE - No changes will be made\"\n    echo \"\"\n    echo \"Would upgrade:\"\n    echo \"  Cluster: $CLUSTER_NAME\"\n    echo \"  Mode: $MODE\"\n    echo \"  From: $CURRENT_VERSION\"\n    echo \"  To: $TARGET_VERSION\"\n    echo \"  Location: $LOCATION\"\n    echo \"  Node pools: ${UPGRADE_NODES}\"\n    exit 0\nfi\n\necho \"\"\necho \"üìù Upgrade Summary:\"\necho \"  Cluster: $CLUSTER_NAME\"\necho \"  Mode: $MODE\"\necho \"  Current Version: $CURRENT_VERSION\"\necho \"  Target Version: $TARGET_VERSION\"\necho \"  Location: $LOCATION\"\necho \"  Release Channel: $RELEASE_CHANNEL\"\necho \"\"\necho \"Upgrade phases:\"\necho \"  1. Control plane upgrade (~5-10 minutes)\"\nif [[ \"${UPGRADE_NODES}\" == \"true\" && \"$MODE\" == \"Standard\" ]]; then\n    echo \"  2. Node pool upgrades (~10-20 minutes per pool)\"\nelif [[ \"$MODE\" == \"Autopilot\" ]]; then\n    echo \"  2. Node upgrade (automatic, managed by GKE)\"\nfi\necho \"\"\necho \"Impact:\"\necho \"  ‚Ä¢ Control plane: Brief API unavailability during upgrade\"\necho \"  ‚Ä¢ Workloads: Continue running during control plane upgrade\"\nif [[ \"${UPGRADE_NODES}\" == \"true\" ]]; then\n    echo \"  ‚Ä¢ Nodes: Rolling replacement (pods will be rescheduled)\"\nfi\necho \"\"\n\nread -p \"Continue with upgrade? (yes/no): \" CONFIRM\nif [[ \"$CONFIRM\" != \"yes\" ]]; then\n    echo \"‚ùå Upgrade cancelled\"\n    exit 0\nfi\n```\n\n#### 3.2 Upgrade Control Plane\n\n```bash\necho \"\"\necho \"Step 1: Upgrading control plane to $TARGET_VERSION...\"\necho \"‚è≥ This may take 5-10 minutes...\"\necho \"\"\n\nUPGRADE_START=$(date +%s)\n\n# Initiate control plane upgrade\ngcloud container clusters upgrade \"$CLUSTER_NAME\" \\\n    --master \\\n    --cluster-version=\"$TARGET_VERSION\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --quiet\n\nif [[ $? -eq 0 ]]; then\n    echo \"‚úÖ Control plane upgraded successfully\"\nelse\n    echo \"‚ùå Control plane upgrade failed\"\n    exit 1\nfi\n\n# Verify control plane version\necho \"\"\necho \"Verifying control plane version...\"\nNEW_MASTER_VERSION=$(gcloud container clusters describe \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --format=\"value(currentMasterVersion)\")\n\nif [[ \"$NEW_MASTER_VERSION\" == \"$TARGET_VERSION\" ]]; then\n    echo \"‚úÖ Control plane verified at version $NEW_MASTER_VERSION\"\nelse\n    echo \"‚ö†Ô∏è  Control plane version mismatch\"\n    echo \"   Expected: $TARGET_VERSION\"\n    echo \"   Actual: $NEW_MASTER_VERSION\"\nfi\n```\n\n#### 3.3 Upgrade Node Pools (Standard mode)\n\n```bash\nif [[ \"${UPGRADE_NODES}\" == \"true\" ]]; then\n    if [[ \"$MODE\" == \"Standard\" ]]; then\n        echo \"\"\n        echo \"Step 2: Upgrading node pools...\"\n\n        # Get all node pools\n        NODE_POOLS=$(gcloud container node-pools list \\\n            --cluster=\"$CLUSTER_NAME\" \\\n            --$LOCATION_TYPE=\"$LOCATION\" \\\n            --format=\"value(name)\")\n\n        if [[ -z \"$NODE_POOLS\" ]]; then\n            echo \"‚ÑπÔ∏è  No node pools found\"\n        else\n            for POOL in $NODE_POOLS; do\n                echo \"\"\n                echo \"   Upgrading node pool: $POOL\"\n\n                # Get current pool version\n                POOL_VERSION=$(gcloud container node-pools describe \"$POOL\" \\\n                    --cluster=\"$CLUSTER_NAME\" \\\n                    --$LOCATION_TYPE=\"$LOCATION\" \\\n                    --format=\"value(version)\")\n\n                echo \"   Current version: $POOL_VERSION\"\n                echo \"   Target version: $TARGET_VERSION\"\n\n                if [[ \"$POOL_VERSION\" == \"$TARGET_VERSION\" ]]; then\n                    echo \"   ‚úÖ Already at target version\"\n                    continue\n                fi\n\n                echo \"   ‚è≥ Upgrading (rolling node replacement)...\"\n\n                # Upgrade node pool\n                gcloud container clusters upgrade \"$CLUSTER_NAME\" \\\n                    --node-pool=\"$POOL\" \\\n                    --cluster-version=\"$TARGET_VERSION\" \\\n                    --$LOCATION_TYPE=\"$LOCATION\" \\\n                    --quiet\n\n                if [[ $? -eq 0 ]]; then\n                    echo \"   ‚úÖ Node pool $POOL upgraded successfully\"\n                else\n                    echo \"   ‚ùå Node pool $POOL upgrade failed\"\n                    exit 1\n                fi\n\n                # Brief pause between node pool upgrades\n                sleep 5\n            done\n\n            echo \"\"\n            echo \"‚úÖ All node pools upgraded\"\n        fi\n\n    elif [[ \"$MODE\" == \"Autopilot\" ]]; then\n        echo \"\"\n        echo \"Step 2: Node upgrade (Autopilot mode)\"\n        echo \"   Autopilot clusters automatically upgrade nodes\"\n        echo \"   Node upgrade will happen gradually over the next few hours\"\n        echo \"   Monitor progress: gcloud container operations list --filter='type=UPGRADE_NODES'\"\n    fi\nelse\n    echo \"\"\n    echo \"‚ö†Ô∏è  Skipping node upgrade (--upgrade-nodes=false)\"\n    echo \"   Nodes are still running version $CURRENT_VERSION\"\n    echo \"\"\n    echo \"To upgrade nodes manually:\"\n    if [[ \"$MODE\" == \"Standard\" ]]; then\n        echo \"  gcloud container clusters upgrade $CLUSTER_NAME \\\\\"\n        echo \"    --node-pool=<POOL_NAME> \\\\\"\n        echo \"    --cluster-version=$TARGET_VERSION \\\\\"\n        echo \"    --$LOCATION_TYPE=$LOCATION\"\n    else\n        echo \"  Autopilot nodes will auto-upgrade during maintenance window\"\n    fi\nfi\n```\n\n### Phase 4: Post-Upgrade Verification\n\n```bash\necho \"\"\necho \"üîç POST-UPGRADE VERIFICATION\"\necho \"============================\"\n\n# Verify control plane version\necho \"\"\necho \"Control Plane Version:\"\nFINAL_MASTER_VERSION=$(gcloud container clusters describe \"$CLUSTER_NAME\" \\\n    --$LOCATION_TYPE=\"$LOCATION\" \\\n    --format=\"value(currentMasterVersion)\")\n\nif [[ \"$FINAL_MASTER_VERSION\" == \"$TARGET_VERSION\" ]]; then\n    echo \"‚úÖ $FINAL_MASTER_VERSION\"\nelse\n    echo \"‚ö†Ô∏è  $FINAL_MASTER_VERSION (expected: $TARGET_VERSION)\"\nfi\n\n# Check node versions\necho \"\"\necho \"Node Versions:\"\nkubectl get nodes -o custom-columns=\\\nNAME:.metadata.name,\\\nVERSION:.status.nodeInfo.kubeletVersion,\\\nSTATUS:.status.conditions[-1].type\n\n# Count nodes by version\necho \"\"\necho \"Node Version Distribution:\"\nkubectl get nodes -o jsonpath='{range .items[*]}{.status.nodeInfo.kubeletVersion}{\"\\n\"}{end}' | \\\n    sort | uniq -c | sed 's/^/  /'\n\n# Check node health\necho \"\"\necho \"Node Health:\"\nREADY_NODES=$(kubectl get nodes --no-headers | grep \" Ready\" | wc -l)\nTOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)\n\nif [[ $READY_NODES -eq $TOTAL_NODES ]]; then\n    echo \"‚úÖ All $TOTAL_NODES nodes are Ready\"\nelse\n    echo \"‚ö†Ô∏è  Only $READY_NODES of $TOTAL_NODES nodes are Ready\"\nfi\n\n# Check pod health\necho \"\"\necho \"Pod Health:\"\nRUNNING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)\nTOTAL_PODS=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | wc -l)\n\necho \"Running Pods: $RUNNING_PODS / $TOTAL_PODS\"\n\nFAILING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers 2>/dev/null | wc -l)\nif [[ $FAILING_PODS -gt 0 ]]; then\n    echo \"‚ö†Ô∏è  $FAILING_PODS pods not in Running/Succeeded state\"\n    echo \"\"\n    echo \"Review failing pods:\"\n    kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded\nelse\n    echo \"‚úÖ All pods healthy\"\nfi\n\n# Run cluster diagnostics\necho \"\"\necho \"Running cluster diagnostics...\"\nif command -v k8sgpt &>/dev/null; then\n    k8sgpt analyze --explain --filter=Node,Pod | head -20\nelse\n    echo \"‚ÑπÔ∏è  Install k8sgpt for AI-powered diagnostics\"\nfi\n\n# Calculate upgrade time\nUPGRADE_END=$(date +%s)\nTOTAL_TIME=$(( (UPGRADE_END - UPGRADE_START) / 60 ))\n\necho \"\"\necho \"‚úÖ UPGRADE COMPLETED SUCCESSFULLY\"\necho \"==================================\"\necho \"\"\necho \"Cluster: $CLUSTER_NAME\"\necho \"Mode: $MODE\"\necho \"Old version: $CURRENT_VERSION\"\necho \"New version: $FINAL_MASTER_VERSION\"\necho \"Total time: $TOTAL_TIME minutes\"\necho \"\"\necho \"Next steps:\"\necho \"  1. Monitor workloads for any compatibility issues\"\necho \"  2. Test critical application flows\"\necho \"  3. Review Kubernetes $TARGET_VERSION release notes\"\necho \"  4. Update documentation and runbooks\"\nif [[ \"${UPGRADE_NODES}\" != \"true\" ]]; then\n    echo \"  5. Schedule node pool upgrades\"\nfi\nif [[ \"$MODE\" == \"Autopilot\" ]]; then\n    echo \"  5. Monitor automatic node upgrades over next few hours\"\nfi\necho \"\"\necho \"GKE Console: https://console.cloud.google.com/kubernetes/clusters/$LOCATION_TYPE/$LOCATION/$CLUSTER_NAME?project=$GCP_PROJECT\"\necho \"\"\n```\n\n## Rollback Procedures\n\nGKE does not support direct rollback. If upgrade causes issues:\n\n### Option 1: Restore from Backup\n\n1. Create new cluster with old version\n2. Restore workloads from backup\n3. Redirect traffic to new cluster\n\n### Option 2: Fix Forward\n\n1. Identify compatibility issues\n2. Update workloads to be compatible\n3. Deploy fixes to upgraded cluster\n\n### Option 3: Contact Support\n\nFor critical issues, contact Google Cloud Support with:\n- Cluster name and project\n- Upgrade operation ID\n- Error messages and logs\n\n## Best Practices\n\n1. **Test in non-production** - Always upgrade dev/staging first\n2. **Review release notes** - Check Kubernetes and GKE release notes for breaking changes\n3. **Use release channels** - Simplifies upgrade management for most clusters\n4. **Schedule maintenance** - Upgrade during low-traffic periods\n5. **Monitor during upgrade** - Watch for pod evictions and failures\n6. **Gradual rollout** - Upgrade one cluster at a time in production\n7. **Keep workloads updated** - Ensure apps are compatible with new K8s versions\n\n## Autopilot vs Standard Upgrades\n\n| Aspect | Autopilot | Standard |\n|--------|-----------|----------|\n| **Control Plane** | Auto-upgrades in channel | Auto or manual |\n| **Nodes** | Auto-upgrades gradually | Manual or auto |\n| **Timing** | Maintenance window | On-demand or scheduled |\n| **Control** | Limited (channel-based) | Full control |\n| **Disruption** | Minimized by GKE | Depends on configuration |\n\n## Common Issues\n\n### Issue: API server timeout during upgrade\n\n**Solution**: This is expected. Wait 5-10 minutes for upgrade to complete.\n\n### Issue: Nodes stuck in NotReady after upgrade\n\n**Solution**: Check node logs and events:\n```bash\nkubectl describe node <NODE_NAME>\ngcloud logging read \"resource.type=k8s_node AND resource.labels.node_name=<NODE_NAME>\"\n```\n\n### Issue: Workload compatibility errors\n\n**Solution**: Check for deprecated API usage:\n```bash\nkubectl api-resources --verbs=list --namespaced -o name | \\\n  xargs -n 1 kubectl get --show-kind --ignore-not-found -A\n```\n\n## Related Commands\n\n- `gcp-cluster-delete`: Delete clusters\n- `gcp-cluster-create`: Create new clusters\n- `cluster-diagnose`: Run comprehensive diagnostics\n- `backup-cluster`: Create cluster backup\n- `node-drain`: Manually drain nodes"
              }
            ],
            "skills": []
          }
        ]
      }
    }
  ]
}