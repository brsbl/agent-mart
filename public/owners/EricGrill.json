{
  "owner": {
    "id": "EricGrill",
    "display_name": "Eric Grill",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/694055?v=4",
    "url": "https://github.com/EricGrill",
    "bio": "I build software, break assumptions, and fix security problems that usually get ignored until itâ€™s too late.\r\n\r\nInterests include Bitcoin, decentralized systems",
    "stats": {
      "total_repos": 1,
      "total_plugins": 49,
      "total_commands": 30,
      "total_skills": 64,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "EricGrill/agents-skills-plugins",
      "url": "https://github.com/EricGrill/agents-skills-plugins",
      "description": "A curated collection of Claude Code skills and agents",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-12T04:41:46Z",
        "created_at": "2026-01-09T13:32:03Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/agents/plugin-finder.md",
          "type": "blob",
          "size": 4841
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 13903
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 317
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/sync-composio-skills.yml",
          "type": "blob",
          "size": 3558
        },
        {
          "path": ".github/workflows/sync-muratcankoylan.yml",
          "type": "blob",
          "size": 5718
        },
        {
          "path": ".github/workflows/sync-superpowers.yml",
          "type": "blob",
          "size": 3503
        },
        {
          "path": ".github/workflows/sync-wshobson-agents.yml",
          "type": "blob",
          "size": 5331
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 346
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 34834
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 445
        },
        {
          "path": "plugins/actual-code/ALL_ISSUES_RESOLVED.md",
          "type": "blob",
          "size": 5652
        },
        {
          "path": "plugins/actual-code/ActualCode-TechnicalDeepDiveforJury.md",
          "type": "blob",
          "size": 34451
        },
        {
          "path": "plugins/actual-code/CLI_GUIDE.md",
          "type": "blob",
          "size": 7335
        },
        {
          "path": "plugins/actual-code/COMPREHENSIVE_FIXES.md",
          "type": "blob",
          "size": 10153
        },
        {
          "path": "plugins/actual-code/DEPLOYMENT_GUIDE.md",
          "type": "blob",
          "size": 10596
        },
        {
          "path": "plugins/actual-code/FINAL_TEST.sh",
          "type": "blob",
          "size": 1116
        },
        {
          "path": "plugins/actual-code/FIXES_APPLIED.md",
          "type": "blob",
          "size": 4989
        },
        {
          "path": "plugins/actual-code/HACKATHON_READY.md",
          "type": "blob",
          "size": 7700
        },
        {
          "path": "plugins/actual-code/HACKATHON_SUBMISSION.md",
          "type": "blob",
          "size": 26539
        },
        {
          "path": "plugins/actual-code/JUDGES_CHEAT_SHEET.md",
          "type": "blob",
          "size": 8156
        },
        {
          "path": "plugins/actual-code/LICENSE",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "plugins/actual-code/PRODUCTION_READY.md",
          "type": "blob",
          "size": 11426
        },
        {
          "path": "plugins/actual-code/PUSH_TO_GITHUB.md",
          "type": "blob",
          "size": 4332
        },
        {
          "path": "plugins/actual-code/QUICK_DEMO.md",
          "type": "blob",
          "size": 5897
        },
        {
          "path": "plugins/actual-code/QUICK_DEPLOY.md",
          "type": "blob",
          "size": 5938
        },
        {
          "path": "plugins/actual-code/QUICK_START.md",
          "type": "blob",
          "size": 3505
        },
        {
          "path": "plugins/actual-code/README.md",
          "type": "blob",
          "size": 10817
        },
        {
          "path": "plugins/actual-code/READY_TO_TEST.md",
          "type": "blob",
          "size": 3658
        },
        {
          "path": "plugins/actual-code/RUN_NOW.txt",
          "type": "blob",
          "size": 3226
        },
        {
          "path": "plugins/actual-code/RUN_WEB_UI.txt",
          "type": "blob",
          "size": 2467
        },
        {
          "path": "plugins/actual-code/SETUP_GITHUB.md",
          "type": "blob",
          "size": 4625
        },
        {
          "path": "plugins/actual-code/SETUP_INSTRUCTIONS.txt",
          "type": "blob",
          "size": 2377
        },
        {
          "path": "plugins/actual-code/START_HERE.md",
          "type": "blob",
          "size": 3449
        },
        {
          "path": "plugins/actual-code/VERTEX_AI_DEPLOYMENT_SUMMARY.md",
          "type": "blob",
          "size": 8131
        },
        {
          "path": "plugins/actual-code/WEB_UI_README.md",
          "type": "blob",
          "size": 6045
        },
        {
          "path": "plugins/actual-code/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/agents/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/actual-code/agents/base_agent.py",
          "type": "blob",
          "size": 7314
        },
        {
          "path": "plugins/actual-code/agents/code_analyzer_agent.py",
          "type": "blob",
          "size": 7952
        },
        {
          "path": "plugins/actual-code/agents/dependency_analyzer_agent.py",
          "type": "blob",
          "size": 5602
        },
        {
          "path": "plugins/actual-code/agents/issue_analyzer_agent.py",
          "type": "blob",
          "size": 5330
        },
        {
          "path": "plugins/actual-code/agents/pr_analyzer_agent.py",
          "type": "blob",
          "size": 5308
        },
        {
          "path": "plugins/actual-code/agents/problem_creator_agent.py",
          "type": "blob",
          "size": 9699
        },
        {
          "path": "plugins/actual-code/agents/qa_validator_agent.py",
          "type": "blob",
          "size": 8535
        },
        {
          "path": "plugins/actual-code/agents/scanner_agent.py",
          "type": "blob",
          "size": 10947
        },
        {
          "path": "plugins/actual-code/cli_runner.py",
          "type": "blob",
          "size": 13936
        },
        {
          "path": "plugins/actual-code/deploy_agent_engine.py",
          "type": "blob",
          "size": 11067
        },
        {
          "path": "plugins/actual-code/deploy_to_vertex_ai.py",
          "type": "blob",
          "size": 6167
        },
        {
          "path": "plugins/actual-code/deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/deployment/.dockerignore",
          "type": "blob",
          "size": 158
        },
        {
          "path": "plugins/actual-code/deployment/Dockerfile",
          "type": "blob",
          "size": 243
        },
        {
          "path": "plugins/actual-code/deployment/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/deployment/agents/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/actual-code/deployment/agents/base_agent.py",
          "type": "blob",
          "size": 7314
        },
        {
          "path": "plugins/actual-code/deployment/agents/code_analyzer_agent.py",
          "type": "blob",
          "size": 7952
        },
        {
          "path": "plugins/actual-code/deployment/agents/dependency_analyzer_agent.py",
          "type": "blob",
          "size": 5602
        },
        {
          "path": "plugins/actual-code/deployment/agents/issue_analyzer_agent.py",
          "type": "blob",
          "size": 5330
        },
        {
          "path": "plugins/actual-code/deployment/agents/pr_analyzer_agent.py",
          "type": "blob",
          "size": 5308
        },
        {
          "path": "plugins/actual-code/deployment/agents/problem_creator_agent.py",
          "type": "blob",
          "size": 9699
        },
        {
          "path": "plugins/actual-code/deployment/agents/qa_validator_agent.py",
          "type": "blob",
          "size": 8535
        },
        {
          "path": "plugins/actual-code/deployment/agents/scanner_agent.py",
          "type": "blob",
          "size": 10947
        },
        {
          "path": "plugins/actual-code/deployment/cloudbuild.yaml",
          "type": "blob",
          "size": 758
        },
        {
          "path": "plugins/actual-code/deployment/main.py",
          "type": "blob",
          "size": 2716
        },
        {
          "path": "plugins/actual-code/deployment/orchestrator.py",
          "type": "blob",
          "size": 23785
        },
        {
          "path": "plugins/actual-code/deployment/requirements.txt",
          "type": "blob",
          "size": 583
        },
        {
          "path": "plugins/actual-code/deployment/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/deployment/utils/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/actual-code/deployment/utils/a2a_protocol.py",
          "type": "blob",
          "size": 12617
        },
        {
          "path": "plugins/actual-code/deployment/utils/github_mcp.py",
          "type": "blob",
          "size": 14875
        },
        {
          "path": "plugins/actual-code/deployment/utils/json_parser.py",
          "type": "blob",
          "size": 2622
        },
        {
          "path": "plugins/actual-code/deployment/utils/monitoring.py",
          "type": "blob",
          "size": 11213
        },
        {
          "path": "plugins/actual-code/deployment/web_server.py",
          "type": "blob",
          "size": 16622
        },
        {
          "path": "plugins/actual-code/final_docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/final_docs/ARCHITECTURE.md",
          "type": "blob",
          "size": 20848
        },
        {
          "path": "plugins/actual-code/final_docs/HACKATHON.md",
          "type": "blob",
          "size": 15209
        },
        {
          "path": "plugins/actual-code/final_docs/IMPLEMENTATION.md",
          "type": "blob",
          "size": 43293
        },
        {
          "path": "plugins/actual-code/final_docs/INDEX.md",
          "type": "blob",
          "size": 8850
        },
        {
          "path": "plugins/actual-code/final_docs/README.md",
          "type": "blob",
          "size": 10683
        },
        {
          "path": "plugins/actual-code/final_docs/REFERENCE.md",
          "type": "blob",
          "size": 11449
        },
        {
          "path": "plugins/actual-code/final_docs/SETUP.md",
          "type": "blob",
          "size": 12176
        },
        {
          "path": "plugins/actual-code/orchestrator.py",
          "type": "blob",
          "size": 23785
        },
        {
          "path": "plugins/actual-code/prepare_deployment.py",
          "type": "blob",
          "size": 6977
        },
        {
          "path": "plugins/actual-code/requirements.txt",
          "type": "blob",
          "size": 492
        },
        {
          "path": "plugins/actual-code/run.sh",
          "type": "blob",
          "size": 1589
        },
        {
          "path": "plugins/actual-code/run_full_test.py",
          "type": "blob",
          "size": 6524
        },
        {
          "path": "plugins/actual-code/sample_outputs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/sample_outputs/README.md",
          "type": "blob",
          "size": 711
        },
        {
          "path": "plugins/actual-code/sample_outputs/SAMPLE_AGENT_LOGS.txt",
          "type": "blob",
          "size": 141067
        },
        {
          "path": "plugins/actual-code/setup_credentials.sh",
          "type": "blob",
          "size": 1462
        },
        {
          "path": "plugins/actual-code/start_web_ui.sh",
          "type": "blob",
          "size": 1628
        },
        {
          "path": "plugins/actual-code/test_full_detailed_logging.py",
          "type": "blob",
          "size": 20454
        },
        {
          "path": "plugins/actual-code/test_github_connection.py",
          "type": "blob",
          "size": 2345
        },
        {
          "path": "plugins/actual-code/test_github_token.sh",
          "type": "blob",
          "size": 753
        },
        {
          "path": "plugins/actual-code/test_my_repo.py",
          "type": "blob",
          "size": 1815
        },
        {
          "path": "plugins/actual-code/test_web_server.py",
          "type": "blob",
          "size": 1977
        },
        {
          "path": "plugins/actual-code/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/tests/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/actual-code/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/utils/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/actual-code/utils/a2a_protocol.py",
          "type": "blob",
          "size": 12617
        },
        {
          "path": "plugins/actual-code/utils/github_mcp.py",
          "type": "blob",
          "size": 14875
        },
        {
          "path": "plugins/actual-code/utils/json_parser.py",
          "type": "blob",
          "size": 2622
        },
        {
          "path": "plugins/actual-code/utils/monitoring.py",
          "type": "blob",
          "size": 11213
        },
        {
          "path": "plugins/actual-code/verify_setup.sh",
          "type": "blob",
          "size": 4109
        },
        {
          "path": "plugins/actual-code/web_server.py",
          "type": "blob",
          "size": 16622
        },
        {
          "path": "plugins/actual-code/web_ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/actual-code/web_ui/app.jsx",
          "type": "blob",
          "size": 37785
        },
        {
          "path": "plugins/actual-code/web_ui/index.html",
          "type": "blob",
          "size": 1082
        },
        {
          "path": "plugins/actual-code/web_ui/styles.css",
          "type": "blob",
          "size": 22657
        },
        {
          "path": "plugins/agent-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-orchestration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-orchestration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 433
        },
        {
          "path": "plugins/agent-orchestration/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-orchestration/agents/context-manager.md",
          "type": "blob",
          "size": 7853
        },
        {
          "path": "plugins/agent-orchestration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agent-orchestration/commands/improve-agent.md",
          "type": "blob",
          "size": 9061
        },
        {
          "path": "plugins/agent-orchestration/commands/multi-agent-optimize.md",
          "type": "blob",
          "size": 5665
        },
        {
          "path": "plugins/ai-investigator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 470
        },
        {
          "path": "plugins/ai-investigator/README.md",
          "type": "blob",
          "size": 7035
        },
        {
          "path": "plugins/ai-investigator/firecrawldocs.md",
          "type": "blob",
          "size": 14527
        },
        {
          "path": "plugins/ai-investigator/input",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/input/ai case studies - Sheet1.csv",
          "type": "blob",
          "size": 5080
        },
        {
          "path": "plugins/ai-investigator/instructions.md",
          "type": "blob",
          "size": 5304
        },
        {
          "path": "plugins/ai-investigator/logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/logs/processing_log.json",
          "type": "blob",
          "size": 65072
        },
        {
          "path": "plugins/ai-investigator/raw_content",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_0",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_0/metadata.json",
          "type": "blob",
          "size": 2590
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_0/raw_content.txt",
          "type": "blob",
          "size": 5410
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_0/structured_content.json",
          "type": "blob",
          "size": 7949
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_1",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_1/metadata.json",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_1/raw_content.txt",
          "type": "blob",
          "size": 3578
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_1/structured_content.json",
          "type": "blob",
          "size": 5046
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_10",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_10/metadata.json",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_10/raw_content.txt",
          "type": "blob",
          "size": 3721
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_10/structured_content.json",
          "type": "blob",
          "size": 6303
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_11",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_11/metadata.json",
          "type": "blob",
          "size": 2481
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_11/raw_content.txt",
          "type": "blob",
          "size": 6467
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_11/structured_content.json",
          "type": "blob",
          "size": 8866
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_12",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_12/metadata.json",
          "type": "blob",
          "size": 2613
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_12/raw_content.txt",
          "type": "blob",
          "size": 5461
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_12/structured_content.json",
          "type": "blob",
          "size": 8079
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_13",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_13/metadata.json",
          "type": "blob",
          "size": 2791
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_13/raw_content.txt",
          "type": "blob",
          "size": 5050
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_13/structured_content.json",
          "type": "blob",
          "size": 7724
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_14",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_14/metadata.json",
          "type": "blob",
          "size": 2571
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_14/raw_content.txt",
          "type": "blob",
          "size": 5617
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_14/structured_content.json",
          "type": "blob",
          "size": 8080
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_15",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_15/metadata.json",
          "type": "blob",
          "size": 2867
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_15/raw_content.txt",
          "type": "blob",
          "size": 6580
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_15/structured_content.json",
          "type": "blob",
          "size": 9385
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_16",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_16/metadata.json",
          "type": "blob",
          "size": 2553
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_16/raw_content.txt",
          "type": "blob",
          "size": 6232
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_16/structured_content.json",
          "type": "blob",
          "size": 8691
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_17",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_17/metadata.json",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_17/raw_content.txt",
          "type": "blob",
          "size": 6292
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_17/structured_content.json",
          "type": "blob",
          "size": 8371
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_18",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_18/metadata.json",
          "type": "blob",
          "size": 2231
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_18/raw_content.txt",
          "type": "blob",
          "size": 6144
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_18/structured_content.json",
          "type": "blob",
          "size": 8303
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_19",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_19/metadata.json",
          "type": "blob",
          "size": 2815
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_19/raw_content.txt",
          "type": "blob",
          "size": 4413
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_19/structured_content.json",
          "type": "blob",
          "size": 7112
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_2",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_2/metadata.json",
          "type": "blob",
          "size": 2601
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_2/raw_content.txt",
          "type": "blob",
          "size": 5501
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_2/structured_content.json",
          "type": "blob",
          "size": 7987
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_20",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_20/metadata.json",
          "type": "blob",
          "size": 2539
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_20/raw_content.txt",
          "type": "blob",
          "size": 5326
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_20/structured_content.json",
          "type": "blob",
          "size": 7782
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_21",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_21/metadata.json",
          "type": "blob",
          "size": 2862
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_21/raw_content.txt",
          "type": "blob",
          "size": 5189
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_21/structured_content.json",
          "type": "blob",
          "size": 7955
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_22",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_22/metadata.json",
          "type": "blob",
          "size": 2791
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_22/raw_content.txt",
          "type": "blob",
          "size": 5696
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_22/structured_content.json",
          "type": "blob",
          "size": 8408
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_23",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_23/metadata.json",
          "type": "blob",
          "size": 2799
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_23/raw_content.txt",
          "type": "blob",
          "size": 8184
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_23/structured_content.json",
          "type": "blob",
          "size": 10935
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_24",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_24/metadata.json",
          "type": "blob",
          "size": 2540
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_24/raw_content.txt",
          "type": "blob",
          "size": 8669
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_24/structured_content.json",
          "type": "blob",
          "size": 11269
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_25",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_25/metadata.json",
          "type": "blob",
          "size": 2656
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_25/raw_content.txt",
          "type": "blob",
          "size": 5319
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_25/structured_content.json",
          "type": "blob",
          "size": 7917
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_26",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_26/metadata.json",
          "type": "blob",
          "size": 2486
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_26/raw_content.txt",
          "type": "blob",
          "size": 4864
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_26/structured_content.json",
          "type": "blob",
          "size": 7269
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_27",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_27/metadata.json",
          "type": "blob",
          "size": 2919
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_27/raw_content.txt",
          "type": "blob",
          "size": 5900
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_27/structured_content.json",
          "type": "blob",
          "size": 8734
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_28",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_28/metadata.json",
          "type": "blob",
          "size": 2534
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_28/raw_content.txt",
          "type": "blob",
          "size": 6543
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_28/structured_content.json",
          "type": "blob",
          "size": 9040
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_29",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_29/metadata.json",
          "type": "blob",
          "size": 2819
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_29/raw_content.txt",
          "type": "blob",
          "size": 4465
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_29/structured_content.json",
          "type": "blob",
          "size": 7121
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_3",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_3/metadata.json",
          "type": "blob",
          "size": 2463
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_3/raw_content.txt",
          "type": "blob",
          "size": 4959
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_3/structured_content.json",
          "type": "blob",
          "size": 7317
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_30",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_30/metadata.json",
          "type": "blob",
          "size": 2590
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_30/raw_content.txt",
          "type": "blob",
          "size": 5410
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_30/structured_content.json",
          "type": "blob",
          "size": 7949
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_31",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_31/metadata.json",
          "type": "blob",
          "size": 2648
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_31/raw_content.txt",
          "type": "blob",
          "size": 4450
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_31/structured_content.json",
          "type": "blob",
          "size": 6954
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_32",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_32/metadata.json",
          "type": "blob",
          "size": 2761
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_32/raw_content.txt",
          "type": "blob",
          "size": 6213
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_32/structured_content.json",
          "type": "blob",
          "size": 8891
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_33",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_33/metadata.json",
          "type": "blob",
          "size": 3317
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_33/raw_content.txt",
          "type": "blob",
          "size": 4932
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_33/structured_content.json",
          "type": "blob",
          "size": 8154
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_34",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_34/metadata.json",
          "type": "blob",
          "size": 2783
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_34/raw_content.txt",
          "type": "blob",
          "size": 5292
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_34/structured_content.json",
          "type": "blob",
          "size": 8002
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_35",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_35/metadata.json",
          "type": "blob",
          "size": 2826
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_35/raw_content.txt",
          "type": "blob",
          "size": 6634
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_35/structured_content.json",
          "type": "blob",
          "size": 9371
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_36",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_36/metadata.json",
          "type": "blob",
          "size": 2258
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_36/raw_content.txt",
          "type": "blob",
          "size": 6091
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_36/structured_content.json",
          "type": "blob",
          "size": 8278
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_37",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_37/metadata.json",
          "type": "blob",
          "size": 2645
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_37/raw_content.txt",
          "type": "blob",
          "size": 6668
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_37/structured_content.json",
          "type": "blob",
          "size": 9222
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_4",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_4/metadata.json",
          "type": "blob",
          "size": 2266
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_4/raw_content.txt",
          "type": "blob",
          "size": 5838
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_4/structured_content.json",
          "type": "blob",
          "size": 8006
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_5",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_5/metadata.json",
          "type": "blob",
          "size": 3016
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_5/raw_content.txt",
          "type": "blob",
          "size": 5068
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_5/structured_content.json",
          "type": "blob",
          "size": 7970
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_6",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_6/metadata.json",
          "type": "blob",
          "size": 2559
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_6/raw_content.txt",
          "type": "blob",
          "size": 5674
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_6/structured_content.json",
          "type": "blob",
          "size": 8157
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_7",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_7/metadata.json",
          "type": "blob",
          "size": 2009
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_7/raw_content.txt",
          "type": "blob",
          "size": 6265
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_7/structured_content.json",
          "type": "blob",
          "size": 8182
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_8",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_8/metadata.json",
          "type": "blob",
          "size": 2961
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_8/raw_content.txt",
          "type": "blob",
          "size": 6431
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_8/structured_content.json",
          "type": "blob",
          "size": 9340
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_9",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_9/metadata.json",
          "type": "blob",
          "size": 2348
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_9/raw_content.txt",
          "type": "blob",
          "size": 5482
        },
        {
          "path": "plugins/ai-investigator/raw_content/case_9/structured_content.json",
          "type": "blob",
          "size": 7715
        },
        {
          "path": "plugins/ai-investigator/reports",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/reports/cross_case_analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/reports/cross_case_analysis/cross_case_analysis.json",
          "type": "blob",
          "size": 5404
        },
        {
          "path": "plugins/ai-investigator/reports/executive_dashboard",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/reports/executive_dashboard/executive_dashboard.json",
          "type": "blob",
          "size": 4194
        },
        {
          "path": "plugins/ai-investigator/reports/individual",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_0.md",
          "type": "blob",
          "size": 2304
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_1.md",
          "type": "blob",
          "size": 2550
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_10.md",
          "type": "blob",
          "size": 2633
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_13.md",
          "type": "blob",
          "size": 2516
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_14.md",
          "type": "blob",
          "size": 2477
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_19.md",
          "type": "blob",
          "size": 2282
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_2.md",
          "type": "blob",
          "size": 2771
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_23.md",
          "type": "blob",
          "size": 2323
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_24.md",
          "type": "blob",
          "size": 2424
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_27.md",
          "type": "blob",
          "size": 2396
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_6.md",
          "type": "blob",
          "size": 2534
        },
        {
          "path": "plugins/ai-investigator/reports/individual/case_7.md",
          "type": "blob",
          "size": 2432
        },
        {
          "path": "plugins/ai-investigator/requirements.txt",
          "type": "blob",
          "size": 300
        },
        {
          "path": "plugins/ai-investigator/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/config.py",
          "type": "blob",
          "size": 1381
        },
        {
          "path": "plugins/ai-investigator/src/main.py",
          "type": "blob",
          "size": 8847
        },
        {
          "path": "plugins/ai-investigator/src/processors",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/processors/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/processors/claude_processor.py",
          "type": "blob",
          "size": 13968
        },
        {
          "path": "plugins/ai-investigator/src/scrapers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/scrapers/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/ai-investigator/src/scrapers/firecrawl_loader.py",
          "type": "blob",
          "size": 5579
        },
        {
          "path": "plugins/ai-investigator/src/scrapers/web_loader.py",
          "type": "blob",
          "size": 3860
        },
        {
          "path": "plugins/ai-investigator/src/scrapers/website_crawler.py",
          "type": "blob",
          "size": 5136
        },
        {
          "path": "plugins/ai-investigator/src/test_setup.py",
          "type": "blob",
          "size": 4369
        },
        {
          "path": "plugins/awesome-claude-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/.claude-plugin/marketplace.json",
          "type": "blob",
          "size": 8092
        },
        {
          "path": "plugins/awesome-claude-skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 569
        },
        {
          "path": "plugins/awesome-claude-skills/CONTRIBUTING.md",
          "type": "blob",
          "size": 4368
        },
        {
          "path": "plugins/awesome-claude-skills/README.md",
          "type": "blob",
          "size": 19634
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/SKILL.md",
          "type": "blob",
          "size": 3079
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/scripts/bundle-artifact.sh",
          "type": "blob",
          "size": 1517
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/scripts/init-artifact.sh",
          "type": "blob",
          "size": 9924
        },
        {
          "path": "plugins/awesome-claude-skills/artifacts-builder/scripts/shadcn-components.tar.gz",
          "type": "blob",
          "size": 19967
        },
        {
          "path": "plugins/awesome-claude-skills/brand-guidelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/brand-guidelines/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/brand-guidelines/SKILL.md",
          "type": "blob",
          "size": 2235
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/SKILL.md",
          "type": "blob",
          "size": 11939
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/ArsenalSC-OFL.txt",
          "type": "blob",
          "size": 4373
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/ArsenalSC-Regular.ttf",
          "type": "blob",
          "size": 165848
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BigShoulders-Bold.ttf",
          "type": "blob",
          "size": 94528
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BigShoulders-OFL.txt",
          "type": "blob",
          "size": 4397
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BigShoulders-Regular.ttf",
          "type": "blob",
          "size": 94396
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Boldonse-OFL.txt",
          "type": "blob",
          "size": 4390
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Boldonse-Regular.ttf",
          "type": "blob",
          "size": 77168
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BricolageGrotesque-Bold.ttf",
          "type": "blob",
          "size": 90952
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BricolageGrotesque-OFL.txt",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/BricolageGrotesque-Regular.ttf",
          "type": "blob",
          "size": 90920
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/CrimsonPro-Bold.ttf",
          "type": "blob",
          "size": 107352
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/CrimsonPro-Italic.ttf",
          "type": "blob",
          "size": 108828
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/CrimsonPro-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/CrimsonPro-Regular.ttf",
          "type": "blob",
          "size": 106696
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/DMMono-OFL.txt",
          "type": "blob",
          "size": 4392
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/DMMono-Regular.ttf",
          "type": "blob",
          "size": 48852
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/EricaOne-OFL.txt",
          "type": "blob",
          "size": 4410
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/EricaOne-Regular.ttf",
          "type": "blob",
          "size": 24872
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/GeistMono-Bold.ttf",
          "type": "blob",
          "size": 78304
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/GeistMono-OFL.txt",
          "type": "blob",
          "size": 4388
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/GeistMono-Regular.ttf",
          "type": "blob",
          "size": 78232
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Gloock-OFL.txt",
          "type": "blob",
          "size": 4381
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Gloock-Regular.ttf",
          "type": "blob",
          "size": 95156
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexMono-Bold.ttf",
          "type": "blob",
          "size": 136008
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexMono-OFL.txt",
          "type": "blob",
          "size": 4363
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexMono-Regular.ttf",
          "type": "blob",
          "size": 133796
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexSerif-Bold.ttf",
          "type": "blob",
          "size": 161000
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexSerif-BoldItalic.ttf",
          "type": "blob",
          "size": 169840
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexSerif-Italic.ttf",
          "type": "blob",
          "size": 170004
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/IBMPlexSerif-Regular.ttf",
          "type": "blob",
          "size": 160380
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSans-Bold.ttf",
          "type": "blob",
          "size": 68084
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSans-BoldItalic.ttf",
          "type": "blob",
          "size": 70004
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSans-Italic.ttf",
          "type": "blob",
          "size": 69900
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSans-OFL.txt",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSans-Regular.ttf",
          "type": "blob",
          "size": 68028
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSerif-Italic.ttf",
          "type": "blob",
          "size": 70868
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/InstrumentSerif-Regular.ttf",
          "type": "blob",
          "size": 69312
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Italiana-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Italiana-Regular.ttf",
          "type": "blob",
          "size": 27184
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/JetBrainsMono-Bold.ttf",
          "type": "blob",
          "size": 114828
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/JetBrainsMono-OFL.txt",
          "type": "blob",
          "size": 4399
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/JetBrainsMono-Regular.ttf",
          "type": "blob",
          "size": 114904
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Jura-Light.ttf",
          "type": "blob",
          "size": 154308
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Jura-Medium.ttf",
          "type": "blob",
          "size": 154488
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Jura-OFL.txt",
          "type": "blob",
          "size": 4380
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/LibreBaskerville-OFL.txt",
          "type": "blob",
          "size": 4449
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/LibreBaskerville-Regular.ttf",
          "type": "blob",
          "size": 147584
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Lora-Bold.ttf",
          "type": "blob",
          "size": 133828
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Lora-BoldItalic.ttf",
          "type": "blob",
          "size": 140332
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Lora-Italic.ttf",
          "type": "blob",
          "size": 139328
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Lora-OFL.txt",
          "type": "blob",
          "size": 4423
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Lora-Regular.ttf",
          "type": "blob",
          "size": 133888
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/NationalPark-Bold.ttf",
          "type": "blob",
          "size": 79208
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/NationalPark-OFL.txt",
          "type": "blob",
          "size": 4399
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/NationalPark-Regular.ttf",
          "type": "blob",
          "size": 76424
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/NothingYouCouldDo-OFL.txt",
          "type": "blob",
          "size": 4363
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/NothingYouCouldDo-Regular.ttf",
          "type": "blob",
          "size": 32020
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Outfit-Bold.ttf",
          "type": "blob",
          "size": 55392
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Outfit-OFL.txt",
          "type": "blob",
          "size": 4389
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Outfit-Regular.ttf",
          "type": "blob",
          "size": 54912
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/PixelifySans-Medium.ttf",
          "type": "blob",
          "size": 51072
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/PixelifySans-OFL.txt",
          "type": "blob",
          "size": 4395
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/PoiretOne-OFL.txt",
          "type": "blob",
          "size": 4366
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/PoiretOne-Regular.ttf",
          "type": "blob",
          "size": 45244
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/RedHatMono-Bold.ttf",
          "type": "blob",
          "size": 34420
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/RedHatMono-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/RedHatMono-Regular.ttf",
          "type": "blob",
          "size": 34488
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Silkscreen-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Silkscreen-Regular.ttf",
          "type": "blob",
          "size": 31960
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/SmoochSans-Medium.ttf",
          "type": "blob",
          "size": 59704
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/SmoochSans-OFL.txt",
          "type": "blob",
          "size": 4396
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Tektur-Medium.ttf",
          "type": "blob",
          "size": 76248
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Tektur-OFL.txt",
          "type": "blob",
          "size": 4385
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/Tektur-Regular.ttf",
          "type": "blob",
          "size": 75604
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/WorkSans-Bold.ttf",
          "type": "blob",
          "size": 191304
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/WorkSans-BoldItalic.ttf",
          "type": "blob",
          "size": 175772
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/WorkSans-Italic.ttf",
          "type": "blob",
          "size": 174280
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/WorkSans-OFL.txt",
          "type": "blob",
          "size": 4397
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/WorkSans-Regular.ttf",
          "type": "blob",
          "size": 188916
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/YoungSerif-OFL.txt",
          "type": "blob",
          "size": 4398
        },
        {
          "path": "plugins/awesome-claude-skills/canvas-design/canvas-fonts/YoungSerif-Regular.ttf",
          "type": "blob",
          "size": 105136
        },
        {
          "path": "plugins/awesome-claude-skills/changelog-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/changelog-generator/SKILL.md",
          "type": "blob",
          "size": 3096
        },
        {
          "path": "plugins/awesome-claude-skills/competitive-ads-extractor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/competitive-ads-extractor/SKILL.md",
          "type": "blob",
          "size": 7912
        },
        {
          "path": "plugins/awesome-claude-skills/content-research-writer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/content-research-writer/SKILL.md",
          "type": "blob",
          "size": 14244
        },
        {
          "path": "plugins/awesome-claude-skills/developer-growth-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/developer-growth-analysis/SKILL.md",
          "type": "blob",
          "size": 15720
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/SKILL.md",
          "type": "blob",
          "size": 10150
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml.md",
          "type": "blob",
          "size": 23572
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/__init__.py",
          "type": "blob",
          "size": 65
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/document.py",
          "type": "blob",
          "size": 50409
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates/comments.xml",
          "type": "blob",
          "size": 2635
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates/commentsExtended.xml",
          "type": "blob",
          "size": 2643
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates/commentsExtensible.xml",
          "type": "blob",
          "size": 2739
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates/commentsIds.xml",
          "type": "blob",
          "size": 2651
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/templates/people.xml",
          "type": "blob",
          "size": 147
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/docx/scripts/utilities.py",
          "type": "blob",
          "size": 13694
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/SKILL.md",
          "type": "blob",
          "size": 7068
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/check_bounding_boxes.py",
          "type": "blob",
          "size": 3139
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/check_bounding_boxes_test.py",
          "type": "blob",
          "size": 8818
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/check_fillable_fields.py",
          "type": "blob",
          "size": 362
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/convert_pdf_to_images.py",
          "type": "blob",
          "size": 1123
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/create_validation_image.py",
          "type": "blob",
          "size": 1603
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/extract_form_field_info.py",
          "type": "blob",
          "size": 6127
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/fill_fillable_fields.py",
          "type": "blob",
          "size": 4863
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pdf/scripts/fill_pdf_form_with_annotations.py",
          "type": "blob",
          "size": 3596
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/SKILL.md",
          "type": "blob",
          "size": 25551
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts/html2pptx.js",
          "type": "blob",
          "size": 37795
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts/inventory.py",
          "type": "blob",
          "size": 38126
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts/rearrange.py",
          "type": "blob",
          "size": 8514
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts/replace.py",
          "type": "blob",
          "size": 13594
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/pptx/scripts/thumbnail.py",
          "type": "blob",
          "size": 15484
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/xlsx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/xlsx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/xlsx/SKILL.md",
          "type": "blob",
          "size": 10632
        },
        {
          "path": "plugins/awesome-claude-skills/document-skills/xlsx/recalc.py",
          "type": "blob",
          "size": 6408
        },
        {
          "path": "plugins/awesome-claude-skills/domain-name-brainstormer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/domain-name-brainstormer/SKILL.md",
          "type": "blob",
          "size": 5696
        },
        {
          "path": "plugins/awesome-claude-skills/file-organizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/file-organizer/SKILL.md",
          "type": "blob",
          "size": 11312
        },
        {
          "path": "plugins/awesome-claude-skills/image-enhancer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/image-enhancer/SKILL.md",
          "type": "blob",
          "size": 2540
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/SKILL.md",
          "type": "blob",
          "size": 1511
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/examples/3p-updates.md",
          "type": "blob",
          "size": 3274
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/examples/company-newsletter.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/examples/faq-answers.md",
          "type": "blob",
          "size": 2366
        },
        {
          "path": "plugins/awesome-claude-skills/internal-comms/examples/general-comms.md",
          "type": "blob",
          "size": 602
        },
        {
          "path": "plugins/awesome-claude-skills/invoice-organizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/invoice-organizer/SKILL.md",
          "type": "blob",
          "size": 12010
        },
        {
          "path": "plugins/awesome-claude-skills/lead-research-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/lead-research-assistant/SKILL.md",
          "type": "blob",
          "size": 6587
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/SKILL.md",
          "type": "blob",
          "size": 13552
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/reference/evaluation.md",
          "type": "blob",
          "size": 21663
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/reference/mcp_best_practices.md",
          "type": "blob",
          "size": 28910
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/reference/node_mcp_server.md",
          "type": "blob",
          "size": 26709
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/reference/python_mcp_server.md",
          "type": "blob",
          "size": 26182
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/scripts/connections.py",
          "type": "blob",
          "size": 4875
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/scripts/evaluation.py",
          "type": "blob",
          "size": 12579
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/scripts/example_evaluation.xml",
          "type": "blob",
          "size": 1194
        },
        {
          "path": "plugins/awesome-claude-skills/mcp-builder/scripts/requirements.txt",
          "type": "blob",
          "size": 29
        },
        {
          "path": "plugins/awesome-claude-skills/meeting-insights-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/meeting-insights-analyzer/SKILL.md",
          "type": "blob",
          "size": 10177
        },
        {
          "path": "plugins/awesome-claude-skills/raffle-winner-picker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/raffle-winner-picker/SKILL.md",
          "type": "blob",
          "size": 3796
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 11547
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10863
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3247
        },
        {
          "path": "plugins/awesome-claude-skills/skill-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 2165
        },
        {
          "path": "plugins/awesome-claude-skills/skill-share",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/skill-share/SKILL.md",
          "type": "blob",
          "size": 2913
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/SKILL.md",
          "type": "blob",
          "size": 17142
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/color_palettes.py",
          "type": "blob",
          "size": 8724
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/easing.py",
          "type": "blob",
          "size": 6289
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/frame_composer.py",
          "type": "blob",
          "size": 14468
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/gif_builder.py",
          "type": "blob",
          "size": 9565
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/typography.py",
          "type": "blob",
          "size": 10761
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/validators.py",
          "type": "blob",
          "size": 8225
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/core/visual_effects.py",
          "type": "blob",
          "size": 14862
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/requirements.txt",
          "type": "blob",
          "size": 66
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/bounce.py",
          "type": "blob",
          "size": 3055
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/explode.py",
          "type": "blob",
          "size": 11259
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/fade.py",
          "type": "blob",
          "size": 10143
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/flip.py",
          "type": "blob",
          "size": 9458
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/kaleidoscope.py",
          "type": "blob",
          "size": 6349
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/morph.py",
          "type": "blob",
          "size": 11241
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/move.py",
          "type": "blob",
          "size": 9395
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/pulse.py",
          "type": "blob",
          "size": 8638
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/shake.py",
          "type": "blob",
          "size": 3737
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/slide.py",
          "type": "blob",
          "size": 9367
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/spin.py",
          "type": "blob",
          "size": 9096
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/wiggle.py",
          "type": "blob",
          "size": 10060
        },
        {
          "path": "plugins/awesome-claude-skills/slack-gif-creator/templates/zoom.py",
          "type": "blob",
          "size": 10111
        },
        {
          "path": "plugins/awesome-claude-skills/template-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/template-skill/SKILL.md",
          "type": "blob",
          "size": 140
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/SKILL.md",
          "type": "blob",
          "size": 3124
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/theme-showcase.pdf",
          "type": "blob",
          "size": 124310
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/arctic-frost.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/botanical-garden.md",
          "type": "blob",
          "size": 519
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/desert-rose.md",
          "type": "blob",
          "size": 496
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/forest-canopy.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/golden-hour.md",
          "type": "blob",
          "size": 528
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/midnight-galaxy.md",
          "type": "blob",
          "size": 513
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/modern-minimalist.md",
          "type": "blob",
          "size": 549
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/ocean-depths.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/sunset-boulevard.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "plugins/awesome-claude-skills/theme-factory/themes/tech-innovation.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "plugins/awesome-claude-skills/video-downloader",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/video-downloader/SKILL.md",
          "type": "blob",
          "size": 2671
        },
        {
          "path": "plugins/awesome-claude-skills/video-downloader/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/video-downloader/scripts/download_video.py",
          "type": "blob",
          "size": 4278
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/SKILL.md",
          "type": "blob",
          "size": 3913
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/examples/console_logging.py",
          "type": "blob",
          "size": 1027
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/examples/element_discovery.py",
          "type": "blob",
          "size": 1463
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/examples/static_html_automation.py",
          "type": "blob",
          "size": 953
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/awesome-claude-skills/webapp-testing/scripts/with_server.py",
          "type": "blob",
          "size": 3693
        },
        {
          "path": "plugins/beautiful-prose",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/beautiful-prose/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/beautiful-prose/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 419
        },
        {
          "path": "plugins/beautiful-prose/SKILL.md",
          "type": "blob",
          "size": 4909
        },
        {
          "path": "plugins/beautiful-prose/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/beautiful-prose/references/test-cases.md",
          "type": "blob",
          "size": 1313
        },
        {
          "path": "plugins/blockchain-web3",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 421
        },
        {
          "path": "plugins/blockchain-web3/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/agents/blockchain-developer.md",
          "type": "blob",
          "size": 9265
        },
        {
          "path": "plugins/blockchain-web3/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/skills/defi-protocol-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/skills/defi-protocol-templates/SKILL.md",
          "type": "blob",
          "size": 14295
        },
        {
          "path": "plugins/blockchain-web3/skills/nft-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/skills/nft-standards/SKILL.md",
          "type": "blob",
          "size": 11090
        },
        {
          "path": "plugins/blockchain-web3/skills/solidity-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/skills/solidity-security/SKILL.md",
          "type": "blob",
          "size": 14236
        },
        {
          "path": "plugins/blockchain-web3/skills/web3-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/blockchain-web3/skills/web3-testing/SKILL.md",
          "type": "blob",
          "size": 10617
        },
        {
          "path": "plugins/book-training",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 436
        },
        {
          "path": "plugins/book-training/README.md",
          "type": "blob",
          "size": 3755
        },
        {
          "path": "plugins/book-training/ai_detector_samples.txt",
          "type": "blob",
          "size": 6960
        },
        {
          "path": "plugins/book-training/chat.py",
          "type": "blob",
          "size": 6925
        },
        {
          "path": "plugins/book-training/checkpoints",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/checkpoints/state.json",
          "type": "blob",
          "size": 1292628
        },
        {
          "path": "plugins/book-training/generate_detector_samples.py",
          "type": "blob",
          "size": 4185
        },
        {
          "path": "plugins/book-training/gertrude-stein_three-lives.epub",
          "type": "blob",
          "size": 465582
        },
        {
          "path": "plugins/book-training/hf-dataset",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/hf-dataset/README.md",
          "type": "blob",
          "size": 6434
        },
        {
          "path": "plugins/book-training/hf-dataset/test.jsonl",
          "type": "blob",
          "size": 134096
        },
        {
          "path": "plugins/book-training/hf-dataset/train.jsonl",
          "type": "blob",
          "size": 1642246
        },
        {
          "path": "plugins/book-training/package-lock.json",
          "type": "blob",
          "size": 36132
        },
        {
          "path": "plugins/book-training/package.json",
          "type": "blob",
          "size": 652
        },
        {
          "path": "plugins/book-training/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/README.md",
          "type": "blob",
          "size": 1243
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/SKILL.md",
          "type": "blob",
          "size": 11489
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples/gertrude-stein",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples/gertrude-stein/README.md",
          "type": "blob",
          "size": 5461
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples/gertrude-stein/dataset_sample.jsonl",
          "type": "blob",
          "size": 3969
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples/gertrude-stein/sample_outputs.md",
          "type": "blob",
          "size": 7161
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/examples/gertrude-stein/training_config.json",
          "type": "blob",
          "size": 2815
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/references/segmentation-strategies.md",
          "type": "blob",
          "size": 10817
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/references/tinker-format.md",
          "type": "blob",
          "size": 6068
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/references/tinker.txt",
          "type": "blob",
          "size": 146019
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/book-sft-pipeline/scripts/pipeline_example.py",
          "type": "blob",
          "size": 6695
        },
        {
          "path": "plugins/book-training/skills/context-compression",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/context-compression/SKILL.md",
          "type": "blob",
          "size": 12273
        },
        {
          "path": "plugins/book-training/skills/context-compression/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/context-compression/references/evaluation-framework.md",
          "type": "blob",
          "size": 8449
        },
        {
          "path": "plugins/book-training/skills/context-compression/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/context-compression/scripts/compression_evaluator.py",
          "type": "blob",
          "size": 21759
        },
        {
          "path": "plugins/book-training/skills/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/evaluation/SKILL.md",
          "type": "blob",
          "size": 10437
        },
        {
          "path": "plugins/book-training/skills/evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/evaluation/references/metrics.md",
          "type": "blob",
          "size": 10139
        },
        {
          "path": "plugins/book-training/skills/evaluation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/evaluation/scripts/evaluator.py",
          "type": "blob",
          "size": 16060
        },
        {
          "path": "plugins/book-training/skills/project-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/project-development/README.md",
          "type": "blob",
          "size": 3818
        },
        {
          "path": "plugins/book-training/skills/project-development/SKILL.md",
          "type": "blob",
          "size": 14760
        },
        {
          "path": "plugins/book-training/skills/project-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/project-development/references/case-studies.md",
          "type": "blob",
          "size": 14830
        },
        {
          "path": "plugins/book-training/skills/project-development/references/pipeline-patterns.md",
          "type": "blob",
          "size": 16921
        },
        {
          "path": "plugins/book-training/skills/project-development/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/project-development/scripts/pipeline_template.py",
          "type": "blob",
          "size": 20257
        },
        {
          "path": "plugins/book-training/skills/tool-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/tool-design/README.md",
          "type": "blob",
          "size": 3996
        },
        {
          "path": "plugins/book-training/skills/tool-design/SKILL.md",
          "type": "blob",
          "size": 15389
        },
        {
          "path": "plugins/book-training/skills/tool-design/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/tool-design/references/architectural_reduction.md",
          "type": "blob",
          "size": 8138
        },
        {
          "path": "plugins/book-training/skills/tool-design/references/best_practices.md",
          "type": "blob",
          "size": 10510
        },
        {
          "path": "plugins/book-training/skills/tool-design/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/skills/tool-design/scripts/description_generator.py",
          "type": "blob",
          "size": 7080
        },
        {
          "path": "plugins/book-training/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/src/pipeline.ts",
          "type": "blob",
          "size": 15175
        },
        {
          "path": "plugins/book-training/src/test-api.ts",
          "type": "blob",
          "size": 1340
        },
        {
          "path": "plugins/book-training/src/train-tinker.ts",
          "type": "blob",
          "size": 17506
        },
        {
          "path": "plugins/book-training/style_test_results.md",
          "type": "blob",
          "size": 15514
        },
        {
          "path": "plugins/book-training/style_test_results_20251227_011231.json",
          "type": "blob",
          "size": 17331
        },
        {
          "path": "plugins/book-training/test_model.py",
          "type": "blob",
          "size": 4590
        },
        {
          "path": "plugins/book-training/test_style_transfer.py",
          "type": "blob",
          "size": 10343
        },
        {
          "path": "plugins/book-training/three-lives-sft-dataset.jsonl",
          "type": "blob",
          "size": 1642246
        },
        {
          "path": "plugins/book-training/three-lives-sft-dataset_test.jsonl",
          "type": "blob",
          "size": 134096
        },
        {
          "path": "plugins/book-training/train_tinker.py",
          "type": "blob",
          "size": 11925
        },
        {
          "path": "plugins/book-training/training-logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/book-training/training-logs/config.json",
          "type": "blob",
          "size": 396
        },
        {
          "path": "plugins/book-training/training-logs/metrics.jsonl",
          "type": "blob",
          "size": 2012
        },
        {
          "path": "plugins/book-training/tsconfig.json",
          "type": "blob",
          "size": 359
        },
        {
          "path": "plugins/business-analytics",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 394
        },
        {
          "path": "plugins/business-analytics/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/agents/business-analyst.md",
          "type": "blob",
          "size": 7261
        },
        {
          "path": "plugins/business-analytics/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/skills/data-storytelling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/skills/data-storytelling/SKILL.md",
          "type": "blob",
          "size": 12592
        },
        {
          "path": "plugins/business-analytics/skills/kpi-dashboard-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/business-analytics/skills/kpi-dashboard-design/SKILL.md",
          "type": "blob",
          "size": 17363
        },
        {
          "path": "plugins/code-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-documentation/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-documentation/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 420
        },
        {
          "path": "plugins/code-documentation/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-documentation/agents/code-reviewer.md",
          "type": "blob",
          "size": 8400
        },
        {
          "path": "plugins/code-documentation/agents/docs-architect.md",
          "type": "blob",
          "size": 3666
        },
        {
          "path": "plugins/code-documentation/agents/tutorial-engineer.md",
          "type": "blob",
          "size": 4353
        },
        {
          "path": "plugins/code-documentation/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/code-documentation/commands/code-explain.md",
          "type": "blob",
          "size": 22783
        },
        {
          "path": "plugins/code-documentation/commands/doc-generate.md",
          "type": "blob",
          "size": 16989
        },
        {
          "path": "plugins/comprehensive-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/comprehensive-review/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/comprehensive-review/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 403
        },
        {
          "path": "plugins/comprehensive-review/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/comprehensive-review/agents/architect-review.md",
          "type": "blob",
          "size": 7591
        },
        {
          "path": "plugins/comprehensive-review/agents/code-reviewer.md",
          "type": "blob",
          "size": 8400
        },
        {
          "path": "plugins/comprehensive-review/agents/security-auditor.md",
          "type": "blob",
          "size": 9366
        },
        {
          "path": "plugins/comprehensive-review/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/comprehensive-review/commands/full-review.md",
          "type": "blob",
          "size": 9200
        },
        {
          "path": "plugins/comprehensive-review/commands/pr-enhance.md",
          "type": "blob",
          "size": 19998
        },
        {
          "path": "plugins/content-marketing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/content-marketing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/content-marketing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 308
        },
        {
          "path": "plugins/content-marketing/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/content-marketing/agents/content-marketer.md",
          "type": "blob",
          "size": 8200
        },
        {
          "path": "plugins/content-marketing/agents/search-specialist.md",
          "type": "blob",
          "size": 1862
        },
        {
          "path": "plugins/context-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-management/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-management/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 294
        },
        {
          "path": "plugins/context-management/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-management/agents/context-manager.md",
          "type": "blob",
          "size": 7853
        },
        {
          "path": "plugins/context-management/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-management/commands/context-restore.md",
          "type": "blob",
          "size": 5420
        },
        {
          "path": "plugins/context-management/commands/context-save.md",
          "type": "blob",
          "size": 4996
        },
        {
          "path": "plugins/customer-sales-automation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/customer-sales-automation/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/customer-sales-automation/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 301
        },
        {
          "path": "plugins/customer-sales-automation/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/customer-sales-automation/agents/customer-support.md",
          "type": "blob",
          "size": 8192
        },
        {
          "path": "plugins/customer-sales-automation/agents/sales-automator.md",
          "type": "blob",
          "size": 937
        },
        {
          "path": "plugins/data-validation-suite",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/data-validation-suite/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/data-validation-suite/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 283
        },
        {
          "path": "plugins/data-validation-suite/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/data-validation-suite/agents/backend-security-coder.md",
          "type": "blob",
          "size": 9291
        },
        {
          "path": "plugins/database-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/database-design/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/database-design/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 314
        },
        {
          "path": "plugins/database-design/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/database-design/agents/database-architect.md",
          "type": "blob",
          "size": 16548
        },
        {
          "path": "plugins/database-design/agents/sql-pro.md",
          "type": "blob",
          "size": 7116
        },
        {
          "path": "plugins/database-design/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/database-design/skills/postgresql",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/database-design/skills/postgresql/SKILL.md",
          "type": "blob",
          "size": 16049
        },
        {
          "path": "plugins/deployment-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/deployment-strategies/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/deployment-strategies/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 322
        },
        {
          "path": "plugins/deployment-strategies/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/deployment-strategies/agents/deployment-engineer.md",
          "type": "blob",
          "size": 8858
        },
        {
          "path": "plugins/deployment-strategies/agents/terraform-specialist.md",
          "type": "blob",
          "size": 8557
        },
        {
          "path": "plugins/developer-essentials",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 350
        },
        {
          "path": "plugins/developer-essentials/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/agents/monorepo-architect.md",
          "type": "blob",
          "size": 1468
        },
        {
          "path": "plugins/developer-essentials/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/auth-implementation-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/auth-implementation-patterns/SKILL.md",
          "type": "blob",
          "size": 17671
        },
        {
          "path": "plugins/developer-essentials/skills/bazel-build-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/bazel-build-optimization/SKILL.md",
          "type": "blob",
          "size": 9605
        },
        {
          "path": "plugins/developer-essentials/skills/code-review-excellence",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/code-review-excellence/SKILL.md",
          "type": "blob",
          "size": 13709
        },
        {
          "path": "plugins/developer-essentials/skills/debugging-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/debugging-strategies/SKILL.md",
          "type": "blob",
          "size": 12546
        },
        {
          "path": "plugins/developer-essentials/skills/e2e-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/e2e-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 15181
        },
        {
          "path": "plugins/developer-essentials/skills/error-handling-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/error-handling-patterns/SKILL.md",
          "type": "blob",
          "size": 17198
        },
        {
          "path": "plugins/developer-essentials/skills/git-advanced-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/git-advanced-workflows/SKILL.md",
          "type": "blob",
          "size": 9244
        },
        {
          "path": "plugins/developer-essentials/skills/monorepo-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/monorepo-management/SKILL.md",
          "type": "blob",
          "size": 12720
        },
        {
          "path": "plugins/developer-essentials/skills/nx-workspace-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/nx-workspace-patterns/SKILL.md",
          "type": "blob",
          "size": 10817
        },
        {
          "path": "plugins/developer-essentials/skills/sql-optimization-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/sql-optimization-patterns/SKILL.md",
          "type": "blob",
          "size": 13093
        },
        {
          "path": "plugins/developer-essentials/skills/turborepo-caching",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer-essentials/skills/turborepo-caching/SKILL.md",
          "type": "blob",
          "size": 8425
        },
        {
          "path": "plugins/documentation-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 341
        },
        {
          "path": "plugins/documentation-generation/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/agents/api-documenter.md",
          "type": "blob",
          "size": 7430
        },
        {
          "path": "plugins/documentation-generation/agents/docs-architect.md",
          "type": "blob",
          "size": 3666
        },
        {
          "path": "plugins/documentation-generation/agents/mermaid-expert.md",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/documentation-generation/agents/reference-builder.md",
          "type": "blob",
          "size": 4750
        },
        {
          "path": "plugins/documentation-generation/agents/tutorial-engineer.md",
          "type": "blob",
          "size": 4353
        },
        {
          "path": "plugins/documentation-generation/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/commands/doc-generate.md",
          "type": "blob",
          "size": 16989
        },
        {
          "path": "plugins/documentation-generation/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/skills/architecture-decision-records",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/skills/architecture-decision-records/SKILL.md",
          "type": "blob",
          "size": 12703
        },
        {
          "path": "plugins/documentation-generation/skills/changelog-automation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/skills/changelog-automation/SKILL.md",
          "type": "blob",
          "size": 13833
        },
        {
          "path": "plugins/documentation-generation/skills/openapi-spec-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/documentation-generation/skills/openapi-spec-generation/SKILL.md",
          "type": "blob",
          "size": 24668
        },
        {
          "path": "plugins/feed2context",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 433
        },
        {
          "path": "plugins/feed2context/README.md",
          "type": "blob",
          "size": 8922
        },
        {
          "path": "plugins/feed2context/app2.py",
          "type": "blob",
          "size": 24075
        },
        {
          "path": "plugins/feed2context/composio",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/composio/README.md",
          "type": "blob",
          "size": 2596
        },
        {
          "path": "plugins/feed2context/composio/gmail_demo.py",
          "type": "blob",
          "size": 5336
        },
        {
          "path": "plugins/feed2context/composio/requirements.txt",
          "type": "blob",
          "size": 59
        },
        {
          "path": "plugins/feed2context/composio/watch_reports_email.py",
          "type": "blob",
          "size": 7063
        },
        {
          "path": "plugins/feed2context/data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/data/reports.jsonl",
          "type": "blob",
          "size": 14529
        },
        {
          "path": "plugins/feed2context/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/docs/css",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/docs/css/notebook.css",
          "type": "blob",
          "size": 502
        },
        {
          "path": "plugins/feed2context/docs/index.md",
          "type": "blob",
          "size": 345
        },
        {
          "path": "plugins/feed2context/docs/js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/docs/js/notebook.js",
          "type": "blob",
          "size": 3053
        },
        {
          "path": "plugins/feed2context/extension",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/feed2context/extension/background.js",
          "type": "blob",
          "size": 1181
        },
        {
          "path": "plugins/feed2context/extension/content.js",
          "type": "blob",
          "size": 1374
        },
        {
          "path": "plugins/feed2context/extension/content_linkedin.js",
          "type": "blob",
          "size": 8751
        },
        {
          "path": "plugins/feed2context/extension/manifest.json",
          "type": "blob",
          "size": 722
        },
        {
          "path": "plugins/feed2context/mkdocs.yml",
          "type": "blob",
          "size": 204
        },
        {
          "path": "plugins/feed2context/requirements.txt",
          "type": "blob",
          "size": 153
        },
        {
          "path": "plugins/food-tour-planner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 474
        },
        {
          "path": "plugins/food-tour-planner/README.md",
          "type": "blob",
          "size": 7424
        },
        {
          "path": "plugins/food-tour-planner/package-lock.json",
          "type": "blob",
          "size": 38576
        },
        {
          "path": "plugins/food-tour-planner/package.json",
          "type": "blob",
          "size": 701
        },
        {
          "path": "plugins/food-tour-planner/public",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/public/index.html",
          "type": "blob",
          "size": 21736
        },
        {
          "path": "plugins/food-tour-planner/requirements.txt",
          "type": "blob",
          "size": 120
        },
        {
          "path": "plugins/food-tour-planner/screenshots",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.32.22â€¯AM.png",
          "type": "blob",
          "size": 917381
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.46.50â€¯AM.png",
          "type": "blob",
          "size": 194206
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.47.05â€¯AM.png",
          "type": "blob",
          "size": 252817
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.48.38â€¯AM.png",
          "type": "blob",
          "size": 229903
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.49.23â€¯AM.png",
          "type": "blob",
          "size": 215385
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.49.50â€¯AM.png",
          "type": "blob",
          "size": 385544
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.50.06â€¯AM.png",
          "type": "blob",
          "size": 586587
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.50.14â€¯AM.png",
          "type": "blob",
          "size": 571495
        },
        {
          "path": "plugins/food-tour-planner/screenshots/Screenshot 2025-11-05 at 12.50.43â€¯AM.png",
          "type": "blob",
          "size": 245074
        },
        {
          "path": "plugins/food-tour-planner/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/src/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/src/agents/__init__.py",
          "type": "blob",
          "size": 44
        },
        {
          "path": "plugins/food-tour-planner/src/agents/food_tour_agent.py",
          "type": "blob",
          "size": 10486
        },
        {
          "path": "plugins/food-tour-planner/src/agents/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/src/agents/tools/__init__.py",
          "type": "blob",
          "size": 44
        },
        {
          "path": "plugins/food-tour-planner/src/agents/tools/dashboard_generator.py",
          "type": "blob",
          "size": 19789
        },
        {
          "path": "plugins/food-tour-planner/src/agents/tools/places_lightweight.py",
          "type": "blob",
          "size": 8902
        },
        {
          "path": "plugins/food-tour-planner/src/agents/tools/tavily_research.py",
          "type": "blob",
          "size": 2572
        },
        {
          "path": "plugins/food-tour-planner/src/dashboard-server.js",
          "type": "blob",
          "size": 4945
        },
        {
          "path": "plugins/food-tour-planner/src/deepagent-api.py",
          "type": "blob",
          "size": 8731
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/README.md",
          "type": "blob",
          "size": 3728
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/TESTING_GUIDE.md",
          "type": "blob",
          "size": 2539
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/TEST_RESULTS.md",
          "type": "blob",
          "size": 5389
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/__init__.py",
          "type": "blob",
          "size": 62
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/kensington_market_toronto_comparison.md",
          "type": "blob",
          "size": 14417
        },
        {
          "path": "plugins/food-tour-planner/src/persona_tests/kimi_persona_test.py",
          "type": "blob",
          "size": 9516
        },
        {
          "path": "plugins/food-tour-planner/src/scan-manager.js",
          "type": "blob",
          "size": 9629
        },
        {
          "path": "plugins/food-tour-planner/src/services",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/food-tour-planner/src/services/geographic-sorter.js",
          "type": "blob",
          "size": 8407
        },
        {
          "path": "plugins/food-tour-planner/src/services/neighborhood-analyzer.js",
          "type": "blob",
          "size": 7458
        },
        {
          "path": "plugins/food-tour-planner/src/services/places.js",
          "type": "blob",
          "size": 7235
        },
        {
          "path": "plugins/food-tour-planner/start.sh",
          "type": "blob",
          "size": 2642
        },
        {
          "path": "plugins/food-tour-planner/watch-logs.sh",
          "type": "blob",
          "size": 559
        },
        {
          "path": "plugins/frontend-mobile-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 357
        },
        {
          "path": "plugins/frontend-mobile-development/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/agents/frontend-developer.md",
          "type": "blob",
          "size": 6745
        },
        {
          "path": "plugins/frontend-mobile-development/agents/mobile-developer.md",
          "type": "blob",
          "size": 8185
        },
        {
          "path": "plugins/frontend-mobile-development/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/commands/component-scaffold.md",
          "type": "blob",
          "size": 10940
        },
        {
          "path": "plugins/frontend-mobile-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/skills/nextjs-app-router-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/skills/nextjs-app-router-patterns/SKILL.md",
          "type": "blob",
          "size": 13438
        },
        {
          "path": "plugins/frontend-mobile-development/skills/react-native-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/skills/react-native-architecture/SKILL.md",
          "type": "blob",
          "size": 17179
        },
        {
          "path": "plugins/frontend-mobile-development/skills/react-state-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/skills/react-state-management/SKILL.md",
          "type": "blob",
          "size": 11467
        },
        {
          "path": "plugins/frontend-mobile-development/skills/tailwind-design-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-development/skills/tailwind-design-system/SKILL.md",
          "type": "blob",
          "size": 18657
        },
        {
          "path": "plugins/frontend-mobile-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-security/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-security/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 338
        },
        {
          "path": "plugins/frontend-mobile-security/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-security/agents/frontend-developer.md",
          "type": "blob",
          "size": 6745
        },
        {
          "path": "plugins/frontend-mobile-security/agents/frontend-security-coder.md",
          "type": "blob",
          "size": 10986
        },
        {
          "path": "plugins/frontend-mobile-security/agents/mobile-security-coder.md",
          "type": "blob",
          "size": 12152
        },
        {
          "path": "plugins/frontend-mobile-security/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-mobile-security/commands/xss-scan.md",
          "type": "blob",
          "size": 8619
        },
        {
          "path": "plugins/full-stack-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/full-stack-orchestration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/full-stack-orchestration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 356
        },
        {
          "path": "plugins/full-stack-orchestration/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/full-stack-orchestration/agents/deployment-engineer.md",
          "type": "blob",
          "size": 8858
        },
        {
          "path": "plugins/full-stack-orchestration/agents/performance-engineer.md",
          "type": "blob",
          "size": 10239
        },
        {
          "path": "plugins/full-stack-orchestration/agents/security-auditor.md",
          "type": "blob",
          "size": 9366
        },
        {
          "path": "plugins/full-stack-orchestration/agents/test-automator.md",
          "type": "blob",
          "size": 10623
        },
        {
          "path": "plugins/full-stack-orchestration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/full-stack-orchestration/commands/full-stack-feature.md",
          "type": "blob",
          "size": 9626
        },
        {
          "path": "plugins/game-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 326
        },
        {
          "path": "plugins/game-development/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/agents/minecraft-bukkit-pro.md",
          "type": "blob",
          "size": 4490
        },
        {
          "path": "plugins/game-development/agents/unity-developer.md",
          "type": "blob",
          "size": 10321
        },
        {
          "path": "plugins/game-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/skills/godot-gdscript-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/skills/godot-gdscript-patterns/SKILL.md",
          "type": "blob",
          "size": 19885
        },
        {
          "path": "plugins/game-development/skills/unity-ecs-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-development/skills/unity-ecs-patterns/SKILL.md",
          "type": "blob",
          "size": 16501
        },
        {
          "path": "plugins/git-pr-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-pr-workflows/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-pr-workflows/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 320
        },
        {
          "path": "plugins/git-pr-workflows/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-pr-workflows/agents/code-reviewer.md",
          "type": "blob",
          "size": 8400
        },
        {
          "path": "plugins/git-pr-workflows/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-pr-workflows/commands/git-workflow.md",
          "type": "blob",
          "size": 9190
        },
        {
          "path": "plugins/git-pr-workflows/commands/onboard.md",
          "type": "blob",
          "size": 14032
        },
        {
          "path": "plugins/git-pr-workflows/commands/pr-enhance.md",
          "type": "blob",
          "size": 19998
        },
        {
          "path": "plugins/ios-simulator-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 480
        },
        {
          "path": "plugins/ios-simulator-skill/DEV.md",
          "type": "blob",
          "size": 6163
        },
        {
          "path": "plugins/ios-simulator-skill/LICENSE.md",
          "type": "blob",
          "size": 1068
        },
        {
          "path": "plugins/ios-simulator-skill/README.md",
          "type": "blob",
          "size": 9543
        },
        {
          "path": "plugins/ios-simulator-skill/SPECIFICATION.md",
          "type": "blob",
          "size": 23157
        },
        {
          "path": "plugins/ios-simulator-skill/pyproject.toml",
          "type": "blob",
          "size": 2416
        },
        {
          "path": "plugins/ios-simulator-skill/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/references/accessibility_checklist.md",
          "type": "blob",
          "size": 1486
        },
        {
          "path": "plugins/ios-simulator-skill/references/idb_quick.md",
          "type": "blob",
          "size": 1150
        },
        {
          "path": "plugins/ios-simulator-skill/references/simctl_quick.md",
          "type": "blob",
          "size": 1376
        },
        {
          "path": "plugins/ios-simulator-skill/references/test_patterns.md",
          "type": "blob",
          "size": 1638
        },
        {
          "path": "plugins/ios-simulator-skill/references/troubleshooting.md",
          "type": "blob",
          "size": 1326
        },
        {
          "path": "plugins/ios-simulator-skill/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/skill/SKILL.md",
          "type": "blob",
          "size": 9147
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/accessibility_audit.py",
          "type": "blob",
          "size": 10288
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/app_launcher.py",
          "type": "blob",
          "size": 9743
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/app_state_capture.py",
          "type": "blob",
          "size": 13253
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/build_and_test.py",
          "type": "blob",
          "size": 10410
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/clipboard.py",
          "type": "blob",
          "size": 2669
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common/__init__.py",
          "type": "blob",
          "size": 1574
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common/cache_utils.py",
          "type": "blob",
          "size": 8280
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common/device_utils.py",
          "type": "blob",
          "size": 13874
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common/idb_utils.py",
          "type": "blob",
          "size": 5393
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/common/screenshot_utils.py",
          "type": "blob",
          "size": 10229
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/gesture.py",
          "type": "blob",
          "size": 12628
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/keyboard.py",
          "type": "blob",
          "size": 11772
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/log_monitor.py",
          "type": "blob",
          "size": 14922
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/navigator.py",
          "type": "blob",
          "size": 14114
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/privacy_manager.py",
          "type": "blob",
          "size": 8944
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/push_notification.py",
          "type": "blob",
          "size": 6640
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/screen_mapper.py",
          "type": "blob",
          "size": 10691
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/sim_health_check.sh",
          "type": "blob",
          "size": 7839
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/sim_list.py",
          "type": "blob",
          "size": 9100
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simctl_boot.py",
          "type": "blob",
          "size": 8832
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simctl_create.py",
          "type": "blob",
          "size": 9120
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simctl_delete.py",
          "type": "blob",
          "size": 10536
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simctl_erase.py",
          "type": "blob",
          "size": 10026
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simctl_shutdown.py",
          "type": "blob",
          "size": 8621
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/simulator_selector.py",
          "type": "blob",
          "size": 10518
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/status_bar.py",
          "type": "blob",
          "size": 6789
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/test_recorder.py",
          "type": "blob",
          "size": 10852
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/visual_diff.py",
          "type": "blob",
          "size": 7787
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/__init__.py",
          "type": "blob",
          "size": 378
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/builder.py",
          "type": "blob",
          "size": 13070
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/cache.py",
          "type": "blob",
          "size": 5609
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/config.py",
          "type": "blob",
          "size": 5535
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/reporter.py",
          "type": "blob",
          "size": 9009
        },
        {
          "path": "plugins/ios-simulator-skill/skill/scripts/xcode/xcresult.py",
          "type": "blob",
          "size": 15285
        },
        {
          "path": "plugins/javascript-typescript",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 330
        },
        {
          "path": "plugins/javascript-typescript/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/agents/javascript-pro.md",
          "type": "blob",
          "size": 1209
        },
        {
          "path": "plugins/javascript-typescript/agents/typescript-pro.md",
          "type": "blob",
          "size": 1571
        },
        {
          "path": "plugins/javascript-typescript/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/commands/typescript-scaffold.md",
          "type": "blob",
          "size": 8093
        },
        {
          "path": "plugins/javascript-typescript/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/skills/javascript-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/skills/javascript-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 26058
        },
        {
          "path": "plugins/javascript-typescript/skills/modern-javascript-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/skills/modern-javascript-patterns/SKILL.md",
          "type": "blob",
          "size": 20050
        },
        {
          "path": "plugins/javascript-typescript/skills/nodejs-backend-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/skills/nodejs-backend-patterns/SKILL.md",
          "type": "blob",
          "size": 24835
        },
        {
          "path": "plugins/javascript-typescript/skills/typescript-advanced-types",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/javascript-typescript/skills/typescript-advanced-types/SKILL.md",
          "type": "blob",
          "size": 17117
        },
        {
          "path": "plugins/linkedin-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/linkedin-analyzer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/linkedin-analyzer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 466
        },
        {
          "path": "plugins/linkedin-analyzer/Example_Analysis_Report.md",
          "type": "blob",
          "size": 3798
        },
        {
          "path": "plugins/linkedin-analyzer/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/linkedin-analyzer/LinkedIn2Report.ipynb",
          "type": "blob",
          "size": 26196
        },
        {
          "path": "plugins/linkedin-analyzer/LinkedIn_Analysis_Report.ipynb",
          "type": "blob",
          "size": 28826
        },
        {
          "path": "plugins/linkedin-analyzer/README.md",
          "type": "blob",
          "size": 2763
        },
        {
          "path": "plugins/llm-application-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 366
        },
        {
          "path": "plugins/llm-application-dev/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/agents/ai-engineer.md",
          "type": "blob",
          "size": 8035
        },
        {
          "path": "plugins/llm-application-dev/agents/prompt-engineer.md",
          "type": "blob",
          "size": 10975
        },
        {
          "path": "plugins/llm-application-dev/agents/vector-database-engineer.md",
          "type": "blob",
          "size": 1595
        },
        {
          "path": "plugins/llm-application-dev/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/commands/ai-assistant.md",
          "type": "blob",
          "size": 40791
        },
        {
          "path": "plugins/llm-application-dev/commands/langchain-agent.md",
          "type": "blob",
          "size": 6976
        },
        {
          "path": "plugins/llm-application-dev/commands/prompt-optimize.md",
          "type": "blob",
          "size": 12625
        },
        {
          "path": "plugins/llm-application-dev/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/embedding-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/embedding-strategies/SKILL.md",
          "type": "blob",
          "size": 14558
        },
        {
          "path": "plugins/llm-application-dev/skills/hybrid-search-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/hybrid-search-implementation/SKILL.md",
          "type": "blob",
          "size": 18087
        },
        {
          "path": "plugins/llm-application-dev/skills/langchain-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/langchain-architecture/SKILL.md",
          "type": "blob",
          "size": 10077
        },
        {
          "path": "plugins/llm-application-dev/skills/llm-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/llm-evaluation/SKILL.md",
          "type": "blob",
          "size": 13752
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/SKILL.md",
          "type": "blob",
          "size": 6980
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/assets/few-shot-examples.json",
          "type": "blob",
          "size": 4174
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/assets/prompt-template-library.md",
          "type": "blob",
          "size": 3273
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references/chain-of-thought.md",
          "type": "blob",
          "size": 9386
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references/few-shot-learning.md",
          "type": "blob",
          "size": 11171
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references/prompt-optimization.md",
          "type": "blob",
          "size": 12778
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references/prompt-templates.md",
          "type": "blob",
          "size": 11395
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/references/system-prompts.md",
          "type": "blob",
          "size": 5438
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/scripts/optimize-prompt.py",
          "type": "blob",
          "size": 9159
        },
        {
          "path": "plugins/llm-application-dev/skills/rag-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/rag-implementation/SKILL.md",
          "type": "blob",
          "size": 11224
        },
        {
          "path": "plugins/llm-application-dev/skills/similarity-search-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/similarity-search-patterns/SKILL.md",
          "type": "blob",
          "size": 17977
        },
        {
          "path": "plugins/llm-application-dev/skills/vector-index-tuning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llm-application-dev/skills/vector-index-tuning/SKILL.md",
          "type": "blob",
          "size": 15268
        },
        {
          "path": "plugins/mcp-bitcoin-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/.gitignore",
          "type": "blob",
          "size": 367
        },
        {
          "path": "plugins/mcp-bitcoin-cli/README.md",
          "type": "blob",
          "size": 13111
        },
        {
          "path": "plugins/mcp-bitcoin-cli/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/docs/plans/2026-01-12-mcp-bitcoin-cli-design.md",
          "type": "blob",
          "size": 13197
        },
        {
          "path": "plugins/mcp-bitcoin-cli/docs/plans/2026-01-12-mcp-bitcoin-cli-implementation.md",
          "type": "blob",
          "size": 52939
        },
        {
          "path": "plugins/mcp-bitcoin-cli/pyproject.toml",
          "type": "blob",
          "size": 653
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/__init__.py",
          "type": "blob",
          "size": 849
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/config.py",
          "type": "blob",
          "size": 2760
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/envelope.py",
          "type": "blob",
          "size": 2320
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/node",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/node/__init__.py",
          "type": "blob",
          "size": 381
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/node/cli.py",
          "type": "blob",
          "size": 6195
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/node/interface.py",
          "type": "blob",
          "size": 2332
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/node/rpc.py",
          "type": "blob",
          "size": 5515
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/primitives.py",
          "type": "blob",
          "size": 2638
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/protocols",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/protocols/__init__.py",
          "type": "blob",
          "size": 339
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/protocols/base.py",
          "type": "blob",
          "size": 388
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/protocols/brc20.py",
          "type": "blob",
          "size": 4204
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/server.py",
          "type": "blob",
          "size": 20907
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/src/mcp_bitcoin_cli/tools/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/__init__.py",
          "type": "blob",
          "size": 33
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_brc20.py",
          "type": "blob",
          "size": 3415
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_config.py",
          "type": "blob",
          "size": 3002
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_envelope.py",
          "type": "blob",
          "size": 3876
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_integration.py",
          "type": "blob",
          "size": 24680
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_node_cli.py",
          "type": "blob",
          "size": 2836
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_node_rpc.py",
          "type": "blob",
          "size": 2395
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_primitives.py",
          "type": "blob",
          "size": 6158
        },
        {
          "path": "plugins/mcp-bitcoin-cli/tests/test_server.py",
          "type": "blob",
          "size": 16326
        },
        {
          "path": "plugins/mcp-civic-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/.gitignore",
          "type": "blob",
          "size": 446
        },
        {
          "path": "plugins/mcp-civic-data/README.md",
          "type": "blob",
          "size": 6448
        },
        {
          "path": "plugins/mcp-civic-data/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/docs/plans/2026-01-12-mcp-govt-api-design.md",
          "type": "blob",
          "size": 3952
        },
        {
          "path": "plugins/mcp-civic-data/docs/plans/2026-01-12-mcp-govt-api-implementation.md",
          "type": "blob",
          "size": 39058
        },
        {
          "path": "plugins/mcp-civic-data/pyproject.toml",
          "type": "blob",
          "size": 442
        },
        {
          "path": "plugins/mcp-civic-data/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/__init__.py",
          "type": "blob",
          "size": 69
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/__main__.py",
          "type": "blob",
          "size": 69
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/server.py",
          "type": "blob",
          "size": 1015
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/__init__.py",
          "type": "blob",
          "size": 52
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/census.py",
          "type": "blob",
          "size": 6216
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/datagov.py",
          "type": "blob",
          "size": 3320
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/economics.py",
          "type": "blob",
          "size": 5101
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/eu_data.py",
          "type": "blob",
          "size": 4174
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/nasa.py",
          "type": "blob",
          "size": 4367
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/tools/weather.py",
          "type": "blob",
          "size": 4508
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/utils/__init__.py",
          "type": "blob",
          "size": 156
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/utils/config.py",
          "type": "blob",
          "size": 1511
        },
        {
          "path": "plugins/mcp-civic-data/src/mcp_govt_api/utils/http.py",
          "type": "blob",
          "size": 839
        },
        {
          "path": "plugins/mcp-civic-data/uv.lock",
          "type": "blob",
          "size": 126581
        },
        {
          "path": "plugins/mcp-kali-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-kali-orchestration/.gitignore",
          "type": "blob",
          "size": 248
        },
        {
          "path": "plugins/mcp-kali-orchestration/README.md",
          "type": "blob",
          "size": 7736
        },
        {
          "path": "plugins/mcp-kali-orchestration/kali-image",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-kali-orchestration/kali-image/Dockerfile",
          "type": "blob",
          "size": 1989
        },
        {
          "path": "plugins/mcp-kali-orchestration/kali-image/build.sh",
          "type": "blob",
          "size": 455
        },
        {
          "path": "plugins/mcp-kali-orchestration/package-lock.json",
          "type": "blob",
          "size": 132241
        },
        {
          "path": "plugins/mcp-kali-orchestration/package.json",
          "type": "blob",
          "size": 1120
        },
        {
          "path": "plugins/mcp-kali-orchestration/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/backends",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/backends/docker.ts",
          "type": "blob",
          "size": 6958
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/backends/interface.ts",
          "type": "blob",
          "size": 1468
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/backends/proxmox.ts",
          "type": "blob",
          "size": 11524
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/config.ts",
          "type": "blob",
          "size": 2701
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/index.ts",
          "type": "blob",
          "size": 198
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/server.ts",
          "type": "blob",
          "size": 4738
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/exploit.ts",
          "type": "blob",
          "size": 6097
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/lifecycle.ts",
          "type": "blob",
          "size": 7376
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/network.ts",
          "type": "blob",
          "size": 10285
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/passwords.ts",
          "type": "blob",
          "size": 10850
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/post-exploit.ts",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/recon.ts",
          "type": "blob",
          "size": 8837
        },
        {
          "path": "plugins/mcp-kali-orchestration/src/tools/web.ts",
          "type": "blob",
          "size": 13213
        },
        {
          "path": "plugins/mcp-kali-orchestration/tsconfig.json",
          "type": "blob",
          "size": 481
        },
        {
          "path": "plugins/mcp-memvid-state-service",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-memvid-state-service/.gitignore",
          "type": "blob",
          "size": 44
        },
        {
          "path": "plugins/mcp-memvid-state-service/README.md",
          "type": "blob",
          "size": 6402
        },
        {
          "path": "plugins/mcp-memvid-state-service/package-lock.json",
          "type": "blob",
          "size": 150034
        },
        {
          "path": "plugins/mcp-memvid-state-service/package.json",
          "type": "blob",
          "size": 781
        },
        {
          "path": "plugins/mcp-memvid-state-service/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-memvid-state-service/src/index.ts",
          "type": "blob",
          "size": 18599
        },
        {
          "path": "plugins/mcp-memvid-state-service/tsconfig.json",
          "type": "blob",
          "size": 458
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/.gitignore",
          "type": "blob",
          "size": 31
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/LICENSE",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/README.md",
          "type": "blob",
          "size": 9965
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/docs/plans/2026-01-12-implementation-plan.md",
          "type": "blob",
          "size": 37978
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/docs/plans/2026-01-12-mcp-delegation-server-design.md",
          "type": "blob",
          "size": 9382
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/package-lock.json",
          "type": "blob",
          "size": 93666
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/package.json",
          "type": "blob",
          "size": 784
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/callback-server.test.ts",
          "type": "blob",
          "size": 2393
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/callback-server.ts",
          "type": "blob",
          "size": 3179
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/config.ts",
          "type": "blob",
          "size": 1424
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/index.ts",
          "type": "blob",
          "size": 708
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/proxmox-client.ts",
          "type": "blob",
          "size": 5157
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/schemas.ts",
          "type": "blob",
          "size": 1420
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/server.ts",
          "type": "blob",
          "size": 11030
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/store.test.ts",
          "type": "blob",
          "size": 1757
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/store.ts",
          "type": "blob",
          "size": 1710
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/src/types.ts",
          "type": "blob",
          "size": 1382
        },
        {
          "path": "plugins/mcp-multi-agent-server-delegation/tsconfig.json",
          "type": "blob",
          "size": 330
        },
        {
          "path": "plugins/mcp-multi-agent-ssh",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/.gitignore",
          "type": "blob",
          "size": 474
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/Dockerfile",
          "type": "blob",
          "size": 634
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/LICENSE",
          "type": "blob",
          "size": 1056
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/README.md",
          "type": "blob",
          "size": 6786
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/bin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/bin/launcher.js",
          "type": "blob",
          "size": 3842
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/docker-compose.yml",
          "type": "blob",
          "size": 275
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/package.json",
          "type": "blob",
          "size": 638
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/pyproject.toml",
          "type": "blob",
          "size": 1483
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh/__init__.py",
          "type": "blob",
          "size": 101
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh/connection_pool.py",
          "type": "blob",
          "size": 12837
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh/credentials.py",
          "type": "blob",
          "size": 6579
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh/server.py",
          "type": "blob",
          "size": 10740
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/src/mcp_multi_agent_ssh/types.py",
          "type": "blob",
          "size": 1242
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/tests/__init__.py",
          "type": "blob",
          "size": 31
        },
        {
          "path": "plugins/mcp-multi-agent-ssh/tests/test_credentials.py",
          "type": "blob",
          "size": 5776
        },
        {
          "path": "plugins/mcp-predictive-market",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/.gitignore",
          "type": "blob",
          "size": 365
        },
        {
          "path": "plugins/mcp-predictive-market/.python-version",
          "type": "blob",
          "size": 5
        },
        {
          "path": "plugins/mcp-predictive-market/LICENSE",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "plugins/mcp-predictive-market/README.md",
          "type": "blob",
          "size": 7849
        },
        {
          "path": "plugins/mcp-predictive-market/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/docs/plans/2026-01-12-mcp-predictive-market-design.md",
          "type": "blob",
          "size": 13922
        },
        {
          "path": "plugins/mcp-predictive-market/docs/plans/2026-01-12-mvp-implementation.md",
          "type": "blob",
          "size": 45407
        },
        {
          "path": "plugins/mcp-predictive-market/pyproject.toml",
          "type": "blob",
          "size": 610
        },
        {
          "path": "plugins/mcp-predictive-market/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/__init__.py",
          "type": "blob",
          "size": 277
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/__init__.py",
          "type": "blob",
          "size": 431
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/base.py",
          "type": "blob",
          "size": 832
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/kalshi.py",
          "type": "blob",
          "size": 4685
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/manifold.py",
          "type": "blob",
          "size": 4632
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/metaculus.py",
          "type": "blob",
          "size": 5340
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/polymarket.py",
          "type": "blob",
          "size": 4676
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/adapters/predictit.py",
          "type": "blob",
          "size": 4749
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/analysis/__init__.py",
          "type": "blob",
          "size": 341
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/analysis/arbitrage.py",
          "type": "blob",
          "size": 5076
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/analysis/matching.py",
          "type": "blob",
          "size": 3479
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/errors.py",
          "type": "blob",
          "size": 761
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/py.typed",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/rate_limiter.py",
          "type": "blob",
          "size": 2202
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/schema.py",
          "type": "blob",
          "size": 1513
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/server.py",
          "type": "blob",
          "size": 7372
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/state",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/state/__init__.py",
          "type": "blob",
          "size": 117
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/state/memvid_client.py",
          "type": "blob",
          "size": 2882
        },
        {
          "path": "plugins/mcp-predictive-market/src/mcp_predictive_market/tools.py",
          "type": "blob",
          "size": 7544
        },
        {
          "path": "plugins/mcp-predictive-market/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/kalshi_responses.py",
          "type": "blob",
          "size": 6963
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/manifold_responses.py",
          "type": "blob",
          "size": 1593
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/metaculus_responses.py",
          "type": "blob",
          "size": 2879
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/polymarket_responses.py",
          "type": "blob",
          "size": 1149
        },
        {
          "path": "plugins/mcp-predictive-market/tests/fixtures/predictit_responses.py",
          "type": "blob",
          "size": 3773
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_base.py",
          "type": "blob",
          "size": 925
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_kalshi.py",
          "type": "blob",
          "size": 8441
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_manifold.py",
          "type": "blob",
          "size": 2080
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_metaculus.py",
          "type": "blob",
          "size": 6026
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_polymarket.py",
          "type": "blob",
          "size": 2115
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_adapters/test_predictit.py",
          "type": "blob",
          "size": 6656
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_analysis/__init__.py",
          "type": "blob",
          "size": 33
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_analysis/test_arbitrage.py",
          "type": "blob",
          "size": 12757
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_analysis/test_matching.py",
          "type": "blob",
          "size": 5258
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_errors.py",
          "type": "blob",
          "size": 16585
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_integration.py",
          "type": "blob",
          "size": 23611
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_rate_limiter.py",
          "type": "blob",
          "size": 1505
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_schema.py",
          "type": "blob",
          "size": 2270
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_server.py",
          "type": "blob",
          "size": 623
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_state",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_state/__init__.py",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_state/test_memvid_client.py",
          "type": "blob",
          "size": 1360
        },
        {
          "path": "plugins/mcp-predictive-market/tests/test_tools.py",
          "type": "blob",
          "size": 8119
        },
        {
          "path": "plugins/mcp-predictive-market/uv.lock",
          "type": "blob",
          "size": 127764
        },
        {
          "path": "plugins/mcp-proxmox-admin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/.gitignore",
          "type": "blob",
          "size": 328
        },
        {
          "path": "plugins/mcp-proxmox-admin/README.md",
          "type": "blob",
          "size": 7452
        },
        {
          "path": "plugins/mcp-proxmox-admin/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/docs/plans/2026-01-11-mcp-proxmox-admin-design.md",
          "type": "blob",
          "size": 8660
        },
        {
          "path": "plugins/mcp-proxmox-admin/docs/plans/2026-01-11-mcp-server-implementation.md",
          "type": "blob",
          "size": 61794
        },
        {
          "path": "plugins/mcp-proxmox-admin/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/examples/claude-desktop-config.json",
          "type": "blob",
          "size": 590
        },
        {
          "path": "plugins/mcp-proxmox-admin/package-lock.json",
          "type": "blob",
          "size": 126179
        },
        {
          "path": "plugins/mcp-proxmox-admin/package.json",
          "type": "blob",
          "size": 832
        },
        {
          "path": "plugins/mcp-proxmox-admin/proxmox-config.example.json",
          "type": "blob",
          "size": 308
        },
        {
          "path": "plugins/mcp-proxmox-admin/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config/index.ts",
          "type": "blob",
          "size": 138
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config/loader.test.ts",
          "type": "blob",
          "size": 1329
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config/loader.ts",
          "type": "blob",
          "size": 2412
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config/schema.test.ts",
          "type": "blob",
          "size": 1423
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/config/schema.ts",
          "type": "blob",
          "size": 890
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/index.ts",
          "type": "blob",
          "size": 18390
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/api.test.ts",
          "type": "blob",
          "size": 1687
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/api.ts",
          "type": "blob",
          "size": 13361
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/index.ts",
          "type": "blob",
          "size": 231
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/router.ts",
          "type": "blob",
          "size": 2604
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/ssh.test.ts",
          "type": "blob",
          "size": 3601
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/ssh.ts",
          "type": "blob",
          "size": 13188
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/transports/types.ts",
          "type": "blob",
          "size": 1952
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/types",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/src/types/proxmox.ts",
          "type": "blob",
          "size": 1125
        },
        {
          "path": "plugins/mcp-proxmox-admin/ssh-connect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-proxmox-admin/ssh-connect/SKILL.md",
          "type": "blob",
          "size": 1671
        },
        {
          "path": "plugins/mcp-proxmox-admin/ssh-connect/ssh_connect.py",
          "type": "blob",
          "size": 5888
        },
        {
          "path": "plugins/mcp-proxmox-admin/tsconfig.json",
          "type": "blob",
          "size": 490
        },
        {
          "path": "plugins/mcp-proxmox-admin/tsup.config.ts",
          "type": "blob",
          "size": 248
        },
        {
          "path": "plugins/multi-agent-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multi-agent-patterns/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multi-agent-patterns/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 560
        },
        {
          "path": "plugins/multi-agent-patterns/SKILL.md",
          "type": "blob",
          "size": 14650
        },
        {
          "path": "plugins/multi-agent-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multi-agent-patterns/references/frameworks.md",
          "type": "blob",
          "size": 12482
        },
        {
          "path": "plugins/multi-agent-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multi-agent-patterns/scripts/coordination.py",
          "type": "blob",
          "size": 14926
        },
        {
          "path": "plugins/nano-banana",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nano-banana/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nano-banana/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 307
        },
        {
          "path": "plugins/nano-banana/README.md",
          "type": "blob",
          "size": 1199
        },
        {
          "path": "plugins/nano-banana/index.js",
          "type": "blob",
          "size": 7760
        },
        {
          "path": "plugins/nano-banana/package.json",
          "type": "blob",
          "size": 380
        },
        {
          "path": "plugins/python-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 328
        },
        {
          "path": "plugins/python-development/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/agents/django-pro.md",
          "type": "blob",
          "size": 6494
        },
        {
          "path": "plugins/python-development/agents/fastapi-pro.md",
          "type": "blob",
          "size": 5943
        },
        {
          "path": "plugins/python-development/agents/python-pro.md",
          "type": "blob",
          "size": 6726
        },
        {
          "path": "plugins/python-development/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/commands/python-scaffold.md",
          "type": "blob",
          "size": 7261
        },
        {
          "path": "plugins/python-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/async-python-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/async-python-patterns/SKILL.md",
          "type": "blob",
          "size": 18730
        },
        {
          "path": "plugins/python-development/skills/python-packaging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/python-packaging/SKILL.md",
          "type": "blob",
          "size": 18248
        },
        {
          "path": "plugins/python-development/skills/python-performance-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/python-performance-optimization/SKILL.md",
          "type": "blob",
          "size": 21268
        },
        {
          "path": "plugins/python-development/skills/python-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/python-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 21664
        },
        {
          "path": "plugins/python-development/skills/uv-package-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-development/skills/uv-package-manager/SKILL.md",
          "type": "blob",
          "size": 16048
        },
        {
          "path": "plugins/ralph-wiggum-marketer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/.claude-plugin/marketplace.json",
          "type": "blob",
          "size": 700
        },
        {
          "path": "plugins/ralph-wiggum-marketer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 679
        },
        {
          "path": "plugins/ralph-wiggum-marketer/AGENTS.md",
          "type": "blob",
          "size": 2578
        },
        {
          "path": "plugins/ralph-wiggum-marketer/README.md",
          "type": "blob",
          "size": 8801
        },
        {
          "path": "plugins/ralph-wiggum-marketer/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/commands/ralph-cancel.md",
          "type": "blob",
          "size": 634
        },
        {
          "path": "plugins/ralph-wiggum-marketer/commands/ralph-init.md",
          "type": "blob",
          "size": 1992
        },
        {
          "path": "plugins/ralph-wiggum-marketer/commands/ralph-marketer.md",
          "type": "blob",
          "size": 2624
        },
        {
          "path": "plugins/ralph-wiggum-marketer/commands/ralph-status.md",
          "type": "blob",
          "size": 864
        },
        {
          "path": "plugins/ralph-wiggum-marketer/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/hooks/hooks.json",
          "type": "blob",
          "size": 258
        },
        {
          "path": "plugins/ralph-wiggum-marketer/hooks/stop-hook.sh",
          "type": "blob",
          "size": 3238
        },
        {
          "path": "plugins/ralph-wiggum-marketer/package-lock.json",
          "type": "blob",
          "size": 16500
        },
        {
          "path": "plugins/ralph-wiggum-marketer/package.json",
          "type": "blob",
          "size": 795
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/content",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/content/list.js",
          "type": "blob",
          "size": 3062
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/db",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/db/init.js",
          "type": "blob",
          "size": 8752
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/db/query.js",
          "type": "blob",
          "size": 3854
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/db/seed.js",
          "type": "blob",
          "size": 14134
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/db/status.js",
          "type": "blob",
          "size": 2838
        },
        {
          "path": "plugins/ralph-wiggum-marketer/scripts/src/test.js",
          "type": "blob",
          "size": 4702
        },
        {
          "path": "plugins/ralph-wiggum-marketer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/skills/copywriter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/skills/copywriter/SKILL.md",
          "type": "blob",
          "size": 10498
        },
        {
          "path": "plugins/ralph-wiggum-marketer/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph-wiggum-marketer/templates/package.json",
          "type": "blob",
          "size": 623
        },
        {
          "path": "plugins/ralph-wiggum-marketer/templates/prd.json",
          "type": "blob",
          "size": 7718
        },
        {
          "path": "plugins/ralph-wiggum-marketer/templates/progress.txt",
          "type": "blob",
          "size": 1950
        },
        {
          "path": "plugins/ralph-wiggum-marketer/templates/prompt.md",
          "type": "blob",
          "size": 7246
        },
        {
          "path": "plugins/readwren",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 444
        },
        {
          "path": "plugins/readwren/LICENSE",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "plugins/readwren/README.md",
          "type": "blob",
          "size": 23539
        },
        {
          "path": "plugins/readwren/cli_interview.py",
          "type": "blob",
          "size": 10935
        },
        {
          "path": "plugins/readwren/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/docs/PROFILE_RUBRIC.md",
          "type": "blob",
          "size": 8826
        },
        {
          "path": "plugins/readwren/docs/README.md",
          "type": "blob",
          "size": 3881
        },
        {
          "path": "plugins/readwren/docs/REDIS_GUIDE.md",
          "type": "blob",
          "size": 6692
        },
        {
          "path": "plugins/readwren/docs/RUBRIC_INTEGRATION.md",
          "type": "blob",
          "size": 7628
        },
        {
          "path": "plugins/readwren/docs/TECHNICAL_DOCUMENTATION.md",
          "type": "blob",
          "size": 47417
        },
        {
          "path": "plugins/readwren/env.example",
          "type": "blob",
          "size": 315
        },
        {
          "path": "plugins/readwren/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/examples/example_session",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/examples/example_session/README.md",
          "type": "blob",
          "size": 4913
        },
        {
          "path": "plugins/readwren/examples/example_session/logs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/examples/example_session/logs/conversation_20251108_150303.json",
          "type": "blob",
          "size": 10059
        },
        {
          "path": "plugins/readwren/examples/example_session/profiles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/examples/example_session/profiles/profile_20251108_150303.json",
          "type": "blob",
          "size": 4189
        },
        {
          "path": "plugins/readwren/examples/example_session/profiles/profile_20251108_150303.md",
          "type": "blob",
          "size": 1234
        },
        {
          "path": "plugins/readwren/examples/example_session/profiles/profile_20251108_150303_SHAREABLE.txt",
          "type": "blob",
          "size": 5012
        },
        {
          "path": "plugins/readwren/requirements.txt",
          "type": "blob",
          "size": 135
        },
        {
          "path": "plugins/readwren/run_interview.sh",
          "type": "blob",
          "size": 349
        },
        {
          "path": "plugins/readwren/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/scripts/README.md",
          "type": "blob",
          "size": 5046
        },
        {
          "path": "plugins/readwren/scripts/retrieve_profile.py",
          "type": "blob",
          "size": 3529
        },
        {
          "path": "plugins/readwren/scripts/view_conversation_log.py",
          "type": "blob",
          "size": 2752
        },
        {
          "path": "plugins/readwren/scripts/view_redis_sessions.py",
          "type": "blob",
          "size": 3308
        },
        {
          "path": "plugins/readwren/scripts/view_session_conversation.py",
          "type": "blob",
          "size": 4770
        },
        {
          "path": "plugins/readwren/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/src/__init__.py",
          "type": "blob",
          "size": 32
        },
        {
          "path": "plugins/readwren/src/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/src/agents/__init__.py",
          "type": "blob",
          "size": 363
        },
        {
          "path": "plugins/readwren/src/agents/interview_agent.py",
          "type": "blob",
          "size": 12791
        },
        {
          "path": "plugins/readwren/src/agents/profile_generator.py",
          "type": "blob",
          "size": 7061
        },
        {
          "path": "plugins/readwren/src/agents/reasoning_extractor.py",
          "type": "blob",
          "size": 3692
        },
        {
          "path": "plugins/readwren/src/agents/redis_checkpointer.py",
          "type": "blob",
          "size": 6402
        },
        {
          "path": "plugins/readwren/src/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/src/config/__init__.py",
          "type": "blob",
          "size": 56
        },
        {
          "path": "plugins/readwren/src/config/settings.py",
          "type": "blob",
          "size": 1006
        },
        {
          "path": "plugins/readwren/src/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/src/prompts/__init__.py",
          "type": "blob",
          "size": 81
        },
        {
          "path": "plugins/readwren/src/prompts/interview_prompts.py",
          "type": "blob",
          "size": 6872
        },
        {
          "path": "plugins/readwren/src/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/readwren/src/tools/__init__.py",
          "type": "blob",
          "size": 261
        },
        {
          "path": "plugins/readwren/src/tools/profile_formatter.py",
          "type": "blob",
          "size": 8528
        },
        {
          "path": "plugins/readwren/src/tools/profile_saver.py",
          "type": "blob",
          "size": 8519
        },
        {
          "path": "plugins/readwren/src/tools/profile_tools.py",
          "type": "blob",
          "size": 4913
        },
        {
          "path": "plugins/rosetta-prompt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 462
        },
        {
          "path": "plugins/rosetta-prompt/README.md",
          "type": "blob",
          "size": 26719
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/agents/__init__.py",
          "type": "blob",
          "size": 181
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/agents/optimizer.py",
          "type": "blob",
          "size": 18253
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/agents/orchestrator.py",
          "type": "blob",
          "size": 2814
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/config.py",
          "type": "blob",
          "size": 1410
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/anthropic",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/anthropic/index.md",
          "type": "blob",
          "size": 388
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/anthropic/prompting.md",
          "type": "blob",
          "size": 8634
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/google",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/google/index.md",
          "type": "blob",
          "size": 366
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/google/prompting.md",
          "type": "blob",
          "size": 6626
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/kimi",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/kimi/index.md",
          "type": "blob",
          "size": 339
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/kimi/prompting.md",
          "type": "blob",
          "size": 6685
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/openai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/openai/index.md",
          "type": "blob",
          "size": 357
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/openai/prompting.md",
          "type": "blob",
          "size": 9194
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/docs/update_log.json",
          "type": "blob",
          "size": 3518
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/env.example",
          "type": "blob",
          "size": 102
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/main.py",
          "type": "blob",
          "size": 1841
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/models/__init__.py",
          "type": "blob",
          "size": 290
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/models/schemas.py",
          "type": "blob",
          "size": 3353
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/prompts/__init__.py",
          "type": "blob",
          "size": 205
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/prompts/optimizer.py",
          "type": "blob",
          "size": 2408
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/prompts/orchestrator.py",
          "type": "blob",
          "size": 833
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/requirements.txt",
          "type": "blob",
          "size": 210
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/tools/__init__.py",
          "type": "blob",
          "size": 219
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/tools/document_tools.py",
          "type": "blob",
          "size": 4430
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/utils/__init__.py",
          "type": "blob",
          "size": 141
        },
        {
          "path": "plugins/rosetta-prompt/rosetta_prompt/utils/logger.py",
          "type": "blob",
          "size": 4231
        },
        {
          "path": "plugins/rosetta-prompt/ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/.prettierrc",
          "type": "blob",
          "size": 183
        },
        {
          "path": "plugins/rosetta-prompt/ui/package-lock.json",
          "type": "blob",
          "size": 682318
        },
        {
          "path": "plugins/rosetta-prompt/ui/package.json",
          "type": "blob",
          "size": 860
        },
        {
          "path": "plugins/rosetta-prompt/ui/public",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img10_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img1_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img2_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img3_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img4_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img5_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img6_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img7_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img8_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/img9_.jpg",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/index.html",
          "type": "blob",
          "size": 1544
        },
        {
          "path": "plugins/rosetta-prompt/ui/public/work_.png",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/App.js",
          "type": "blob",
          "size": 673
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/components/InputScreen.js",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/components/ProcessingScreen.js",
          "type": "blob",
          "size": 3498
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/components/ResultsScreen.js",
          "type": "blob",
          "size": 17670
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/index.js",
          "type": "blob",
          "size": 157
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/store.js",
          "type": "blob",
          "size": 6356
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/styles.css",
          "type": "blob",
          "size": 18383
        },
        {
          "path": "plugins/rosetta-prompt/ui/src/util.js",
          "type": "blob",
          "size": 1839
        },
        {
          "path": "plugins/rosetta-prompt/ui/thumbnail.png",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/updater",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rosetta-prompt/updater/README.md",
          "type": "blob",
          "size": 5808
        },
        {
          "path": "plugins/rosetta-prompt/updater/__init__.py",
          "type": "blob",
          "size": 333
        },
        {
          "path": "plugins/rosetta-prompt/updater/agent.py",
          "type": "blob",
          "size": 6945
        },
        {
          "path": "plugins/rosetta-prompt/updater/config.py",
          "type": "blob",
          "size": 2749
        },
        {
          "path": "plugins/rosetta-prompt/updater/requirements.txt",
          "type": "blob",
          "size": 175
        },
        {
          "path": "plugins/rosetta-prompt/updater/scheduler.py",
          "type": "blob",
          "size": 1195
        },
        {
          "path": "plugins/rosetta-prompt/updater/tools.py",
          "type": "blob",
          "size": 10734
        },
        {
          "path": "plugins/seo-analysis-monitoring",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-analysis-monitoring/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-analysis-monitoring/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 343
        },
        {
          "path": "plugins/seo-analysis-monitoring/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-analysis-monitoring/agents/seo-authority-builder.md",
          "type": "blob",
          "size": 2955
        },
        {
          "path": "plugins/seo-analysis-monitoring/agents/seo-cannibalization-detector.md",
          "type": "blob",
          "size": 2639
        },
        {
          "path": "plugins/seo-analysis-monitoring/agents/seo-content-refresher.md",
          "type": "blob",
          "size": 2540
        },
        {
          "path": "plugins/seo-content-creation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-content-creation/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-content-creation/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 310
        },
        {
          "path": "plugins/seo-content-creation/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-content-creation/agents/seo-content-auditor.md",
          "type": "blob",
          "size": 2017
        },
        {
          "path": "plugins/seo-content-creation/agents/seo-content-planner.md",
          "type": "blob",
          "size": 2028
        },
        {
          "path": "plugins/seo-content-creation/agents/seo-content-writer.md",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "plugins/seo-technical-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-technical-optimization/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-technical-optimization/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 355
        },
        {
          "path": "plugins/seo-technical-optimization/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/seo-technical-optimization/agents/seo-keyword-strategist.md",
          "type": "blob",
          "size": 2238
        },
        {
          "path": "plugins/seo-technical-optimization/agents/seo-meta-optimizer.md",
          "type": "blob",
          "size": 2194
        },
        {
          "path": "plugins/seo-technical-optimization/agents/seo-snippet-hunter.md",
          "type": "blob",
          "size": 2464
        },
        {
          "path": "plugins/seo-technical-optimization/agents/seo-structure-architect.md",
          "type": "blob",
          "size": 2393
        },
        {
          "path": "plugins/superpowers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 595
        },
        {
          "path": "plugins/superpowers/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/superpowers/README.md",
          "type": "blob",
          "size": 1914
        },
        {
          "path": "plugins/superpowers/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/agents/code-reviewer.md",
          "type": "blob",
          "size": 3888
        },
        {
          "path": "plugins/superpowers/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/commands/brainstorm.md",
          "type": "blob",
          "size": 326
        },
        {
          "path": "plugins/superpowers/commands/execute-plan.md",
          "type": "blob",
          "size": 188
        },
        {
          "path": "plugins/superpowers/commands/write-plan.md",
          "type": "blob",
          "size": 196
        },
        {
          "path": "plugins/superpowers/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/hooks/hooks.json",
          "type": "blob",
          "size": 287
        },
        {
          "path": "plugins/superpowers/hooks/run-hook.cmd",
          "type": "blob",
          "size": 493
        },
        {
          "path": "plugins/superpowers/hooks/session-start.sh",
          "type": "blob",
          "size": 1995
        },
        {
          "path": "plugins/superpowers/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/brainstorming/SKILL.md",
          "type": "blob",
          "size": 2505
        },
        {
          "path": "plugins/superpowers/skills/dispatching-parallel-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/dispatching-parallel-agents/SKILL.md",
          "type": "blob",
          "size": 6104
        },
        {
          "path": "plugins/superpowers/skills/executing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/executing-plans/SKILL.md",
          "type": "blob",
          "size": 2171
        },
        {
          "path": "plugins/superpowers/skills/finishing-a-development-branch",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/finishing-a-development-branch/SKILL.md",
          "type": "blob",
          "size": 4250
        },
        {
          "path": "plugins/superpowers/skills/receiving-code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/receiving-code-review/SKILL.md",
          "type": "blob",
          "size": 6314
        },
        {
          "path": "plugins/superpowers/skills/requesting-code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/requesting-code-review/SKILL.md",
          "type": "blob",
          "size": 2700
        },
        {
          "path": "plugins/superpowers/skills/requesting-code-review/code-reviewer.md",
          "type": "blob",
          "size": 3385
        },
        {
          "path": "plugins/superpowers/skills/subagent-driven-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/subagent-driven-development/SKILL.md",
          "type": "blob",
          "size": 9809
        },
        {
          "path": "plugins/superpowers/skills/subagent-driven-development/code-quality-reviewer-prompt.md",
          "type": "blob",
          "size": 630
        },
        {
          "path": "plugins/superpowers/skills/subagent-driven-development/implementer-prompt.md",
          "type": "blob",
          "size": 2195
        },
        {
          "path": "plugins/superpowers/skills/subagent-driven-development/spec-reviewer-prompt.md",
          "type": "blob",
          "size": 1999
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/CREATION-LOG.md",
          "type": "blob",
          "size": 4268
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/SKILL.md",
          "type": "blob",
          "size": 9884
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/condition-based-waiting-example.ts",
          "type": "blob",
          "size": 5054
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/condition-based-waiting.md",
          "type": "blob",
          "size": 3516
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/defense-in-depth.md",
          "type": "blob",
          "size": 3650
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/find-polluter.sh",
          "type": "blob",
          "size": 1528
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/root-cause-tracing.md",
          "type": "blob",
          "size": 5327
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/test-academic.md",
          "type": "blob",
          "size": 653
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/test-pressure-1.md",
          "type": "blob",
          "size": 1900
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/test-pressure-2.md",
          "type": "blob",
          "size": 2283
        },
        {
          "path": "plugins/superpowers/skills/systematic-debugging/test-pressure-3.md",
          "type": "blob",
          "size": 2692
        },
        {
          "path": "plugins/superpowers/skills/test-driven-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/test-driven-development/SKILL.md",
          "type": "blob",
          "size": 9867
        },
        {
          "path": "plugins/superpowers/skills/test-driven-development/testing-anti-patterns.md",
          "type": "blob",
          "size": 8251
        },
        {
          "path": "plugins/superpowers/skills/using-git-worktrees",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/using-git-worktrees/SKILL.md",
          "type": "blob",
          "size": 5592
        },
        {
          "path": "plugins/superpowers/skills/using-superpowers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/using-superpowers/SKILL.md",
          "type": "blob",
          "size": 3798
        },
        {
          "path": "plugins/superpowers/skills/verification-before-completion",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/verification-before-completion/SKILL.md",
          "type": "blob",
          "size": 4201
        },
        {
          "path": "plugins/superpowers/skills/writing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/writing-plans/SKILL.md",
          "type": "blob",
          "size": 3264
        },
        {
          "path": "plugins/superpowers/skills/writing-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/SKILL.md",
          "type": "blob",
          "size": 22463
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/anthropic-best-practices.md",
          "type": "blob",
          "size": 45825
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/examples/CLAUDE_MD_TESTING.md",
          "type": "blob",
          "size": 5423
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/graphviz-conventions.dot",
          "type": "blob",
          "size": 5970
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/persuasion-principles.md",
          "type": "blob",
          "size": 5908
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/render-graphs.js",
          "type": "blob",
          "size": 4857
        },
        {
          "path": "plugins/superpowers/skills/writing-skills/testing-skills-with-subagents.md",
          "type": "blob",
          "size": 12557
        },
        {
          "path": "plugins/team-collaboration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/team-collaboration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/team-collaboration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 326
        },
        {
          "path": "plugins/team-collaboration/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/team-collaboration/agents/dx-optimizer.md",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/team-collaboration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/team-collaboration/commands/issue.md",
          "type": "blob",
          "size": 18000
        },
        {
          "path": "plugins/team-collaboration/commands/standup-notes.md",
          "type": "blob",
          "size": 30849
        },
        {
          "path": "plugins/unit-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/unit-testing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/unit-testing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 293
        },
        {
          "path": "plugins/unit-testing/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/unit-testing/agents/debugger.md",
          "type": "blob",
          "size": 781
        },
        {
          "path": "plugins/unit-testing/agents/test-automator.md",
          "type": "blob",
          "size": 10623
        },
        {
          "path": "plugins/unit-testing/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/unit-testing/commands/test-generate.md",
          "type": "blob",
          "size": 10217
        }
      ],
      "marketplace": {
        "name": "agents-skills-plugins",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Eric Grill",
          "email": "ericgrill@example.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "nano-banana",
            "description": "Image generation using Google's Gemini API",
            "source": "./plugins/nano-banana",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install nano-banana@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-proxmox-admin",
            "description": "Proxmox VE infrastructure management via MCP with VM, container, and snapshot control",
            "source": "./plugins/mcp-proxmox-admin",
            "category": "devops",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-proxmox-admin@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-multi-agent-ssh",
            "description": "Persistent SSH connections with encrypted credential storage and SFTP support",
            "source": "./plugins/mcp-multi-agent-ssh",
            "category": "devops",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-multi-agent-ssh@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-kali-orchestration",
            "description": "Kali Linux orchestration with 50+ security tools for authorized pentesting",
            "source": "./plugins/mcp-kali-orchestration",
            "category": "security",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-kali-orchestration@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-multi-agent-server-delegation",
            "description": "Task delegation to isolated Proxmox VMs with automatic cleanup and callbacks",
            "source": "./plugins/mcp-multi-agent-server-delegation",
            "category": "devops",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-multi-agent-server-delegation@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-predictive-market",
            "description": "Query 5 prediction markets with arbitrage detection and comparative analysis",
            "source": "./plugins/mcp-predictive-market",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-predictive-market@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-bitcoin-cli",
            "description": "Bitcoin OP_RETURN operations for documents, timestamps, and BRC-20 tokens",
            "source": "./plugins/mcp-bitcoin-cli",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-bitcoin-cli@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-civic-data",
            "description": "Access 7 government APIs for weather, census, NASA, and economic data",
            "source": "./plugins/mcp-civic-data",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-civic-data@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-memvid-state-service",
            "description": "AI memory layer with vector search, full-text search, and temporal queries",
            "source": "./plugins/mcp-memvid-state-service",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "Eric Grill"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install mcp-memvid-state-service@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "superpowers",
            "description": "Core skills library: TDD, debugging, collaboration patterns",
            "source": "./plugins/superpowers",
            "category": "development",
            "version": "4.1.0",
            "author": {
              "name": "Jesse Vincent"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install superpowers@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/brainstorm",
                "description": "You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores requirements and design before implementation.",
                "path": "plugins/superpowers/commands/brainstorm.md",
                "frontmatter": {
                  "description": "You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores requirements and design before implementation.",
                  "disable-model-invocation": true
                },
                "content": "Invoke the superpowers:brainstorming skill and follow it exactly as presented to you"
              },
              {
                "name": "/execute-plan",
                "description": "Execute plan in batches with review checkpoints",
                "path": "plugins/superpowers/commands/execute-plan.md",
                "frontmatter": {
                  "description": "Execute plan in batches with review checkpoints",
                  "disable-model-invocation": true
                },
                "content": "Invoke the superpowers:executing-plans skill and follow it exactly as presented to you"
              },
              {
                "name": "/write-plan",
                "description": "Create detailed implementation plan with bite-sized tasks",
                "path": "plugins/superpowers/commands/write-plan.md",
                "frontmatter": {
                  "description": "Create detailed implementation plan with bite-sized tasks",
                  "disable-model-invocation": true
                },
                "content": "Invoke the superpowers:writing-plans skill and follow it exactly as presented to you"
              }
            ],
            "skills": [
              {
                "name": "brainstorming",
                "description": "You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation.",
                "path": "plugins/superpowers/skills/brainstorming/SKILL.md",
                "frontmatter": {
                  "name": "brainstorming",
                  "description": "You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation."
                },
                "content": "# Brainstorming Ideas Into Designs\n\n## Overview\n\nHelp turn ideas into fully formed designs and specs through natural collaborative dialogue.\n\nStart by understanding the current project context, then ask questions one at a time to refine the idea. Once you understand what you're building, present the design in small sections (200-300 words), checking after each section whether it looks right so far.\n\n## The Process\n\n**Understanding the idea:**\n- Check out the current project state first (files, docs, recent commits)\n- Ask questions one at a time to refine the idea\n- Prefer multiple choice questions when possible, but open-ended is fine too\n- Only one question per message - if a topic needs more exploration, break it into multiple questions\n- Focus on understanding: purpose, constraints, success criteria\n\n**Exploring approaches:**\n- Propose 2-3 different approaches with trade-offs\n- Present options conversationally with your recommendation and reasoning\n- Lead with your recommended option and explain why\n\n**Presenting the design:**\n- Once you believe you understand what you're building, present the design\n- Break it into sections of 200-300 words\n- Ask after each section whether it looks right so far\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify if something doesn't make sense\n\n## After the Design\n\n**Documentation:**\n- Write the validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n- Use elements-of-style:writing-clearly-and-concisely skill if available\n- Commit the design document to git\n\n**Implementation (if continuing):**\n- Ask: \"Ready to set up for implementation?\"\n- Use superpowers:using-git-worktrees to create isolated workspace\n- Use superpowers:writing-plans to create detailed implementation plan\n\n## Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier to answer than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Be flexible** - Go back and clarify when something doesn't make sense"
              },
              {
                "name": "dispatching-parallel-agents",
                "description": "Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies",
                "path": "plugins/superpowers/skills/dispatching-parallel-agents/SKILL.md",
                "frontmatter": {
                  "name": "dispatching-parallel-agents",
                  "description": "Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies"
                },
                "content": "# Dispatching Parallel Agents\n\n## Overview\n\nWhen you have multiple unrelated failures (different test files, different subsystems, different bugs), investigating them sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Dispatch one agent per independent problem domain. Let them work concurrently.\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Multiple failures?\" [shape=diamond];\n    \"Are they independent?\" [shape=diamond];\n    \"Single agent investigates all\" [shape=box];\n    \"One agent per problem domain\" [shape=box];\n    \"Can they work in parallel?\" [shape=diamond];\n    \"Sequential agents\" [shape=box];\n    \"Parallel dispatch\" [shape=box];\n\n    \"Multiple failures?\" -> \"Are they independent?\" [label=\"yes\"];\n    \"Are they independent?\" -> \"Single agent investigates all\" [label=\"no - related\"];\n    \"Are they independent?\" -> \"Can they work in parallel?\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Parallel dispatch\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Sequential agents\" [label=\"no - shared state\"];\n}\n```\n\n**Use when:**\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n\n**Don't use when:**\n- Failures are related (fix one might fix others)\n- Need to understand full system state\n- Agents would interfere with each other\n\n## The Pattern\n\n### 1. Identify Independent Domains\n\nGroup failures by what's broken:\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\nEach domain is independent - fixing tool approval doesn't affect abort tests.\n\n### 2. Create Focused Agent Tasks\n\nEach agent gets:\n- **Specific scope:** One test file or subsystem\n- **Clear goal:** Make these tests pass\n- **Constraints:** Don't change other code\n- **Expected output:** Summary of what you found and fixed\n\n### 3. Dispatch in Parallel\n\n```typescript\n// In Claude Code / AI environment\nTask(\"Fix agent-tool-abort.test.ts failures\")\nTask(\"Fix batch-completion-behavior.test.ts failures\")\nTask(\"Fix tool-approval-race-conditions.test.ts failures\")\n// All three run concurrently\n```\n\n### 4. Review and Integrate\n\nWhen agents return:\n- Read each summary\n- Verify fixes don't conflict\n- Run full test suite\n- Integrate all changes\n\n## Agent Prompt Structure\n\nGood agent prompts are:\n1. **Focused** - One clear problem domain\n2. **Self-contained** - All context needed to understand the problem\n3. **Specific about output** - What should the agent return?\n\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nDo NOT just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n## Common Mistakes\n\n**âŒ Too broad:** \"Fix all the tests\" - agent gets lost\n**âœ… Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n**âŒ No context:** \"Fix the race condition\" - agent doesn't know where\n**âœ… Context:** Paste the error messages and test names\n\n**âŒ No constraints:** Agent might refactor everything\n**âœ… Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n**âŒ Vague output:** \"Fix it\" - you don't know what changed\n**âœ… Specific:** \"Return summary of root cause and changes\"\n\n## When NOT to Use\n\n**Related failures:** Fixing one might fix others - investigate together first\n**Need full context:** Understanding requires seeing entire system\n**Exploratory debugging:** You don't know what's broken yet\n**Shared state:** Agents would interfere (editing same files, using same resources)\n\n## Real Example from Session\n\n**Scenario:** 6 test failures across 3 files after major refactoring\n\n**Failures:**\n- agent-tool-abort.test.ts: 3 failures (timing issues)\n- batch-completion-behavior.test.ts: 2 failures (tools not executing)\n- tool-approval-race-conditions.test.ts: 1 failure (execution count = 0)\n\n**Decision:** Independent domains - abort logic separate from batch completion separate from race conditions\n\n**Dispatch:**\n```\nAgent 1 â†’ Fix agent-tool-abort.test.ts\nAgent 2 â†’ Fix batch-completion-behavior.test.ts\nAgent 3 â†’ Fix tool-approval-race-conditions.test.ts\n```\n\n**Results:**\n- Agent 1: Replaced timeouts with event-based waiting\n- Agent 2: Fixed event structure bug (threadId in wrong place)\n- Agent 3: Added wait for async tool execution to complete\n\n**Integration:** All fixes independent, no conflicts, full suite green\n\n**Time saved:** 3 problems solved in parallel vs sequentially\n\n## Key Benefits\n\n1. **Parallelization** - Multiple investigations happen simultaneously\n2. **Focus** - Each agent has narrow scope, less context to track\n3. **Independence** - Agents don't interfere with each other\n4. **Speed** - 3 problems solved in time of 1\n\n## Verification\n\nAfter agents return:\n1. **Review each summary** - Understand what changed\n2. **Check for conflicts** - Did agents edit same code?\n3. **Run full suite** - Verify all fixes work together\n4. **Spot check** - Agents can make systematic errors\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes"
              },
              {
                "name": "executing-plans",
                "description": "Use when you have a written implementation plan to execute in a separate session with review checkpoints",
                "path": "plugins/superpowers/skills/executing-plans/SKILL.md",
                "frontmatter": {
                  "name": "executing-plans",
                  "description": "Use when you have a written implementation plan to execute in a separate session with review checkpoints"
                },
                "content": "# Executing Plans\n\n## Overview\n\nLoad plan, review critically, execute tasks in batches, report for review between batches.\n\n**Core principle:** Batch execution with checkpoints for architect review.\n\n**Announce at start:** \"I'm using the executing-plans skill to implement this plan.\"\n\n## The Process\n\n### Step 1: Load and Review Plan\n1. Read plan file\n2. Review critically - identify any questions or concerns about the plan\n3. If concerns: Raise them with your human partner before starting\n4. If no concerns: Create TodoWrite and proceed\n\n### Step 2: Execute Batch\n**Default: First 3 tasks**\n\nFor each task:\n1. Mark as in_progress\n2. Follow each step exactly (plan has bite-sized steps)\n3. Run verifications as specified\n4. Mark as completed\n\n### Step 3: Report\nWhen batch complete:\n- Show what was implemented\n- Show verification output\n- Say: \"Ready for feedback.\"\n\n### Step 4: Continue\nBased on feedback:\n- Apply changes if needed\n- Execute next batch\n- Repeat until complete\n\n### Step 5: Complete Development\n\nAfter all tasks complete and verified:\n- Announce: \"I'm using the finishing-a-development-branch skill to complete this work.\"\n- **REQUIRED SUB-SKILL:** Use superpowers:finishing-a-development-branch\n- Follow that skill to verify tests, present options, execute choice\n\n## When to Stop and Ask for Help\n\n**STOP executing immediately when:**\n- Hit a blocker mid-batch (missing dependency, test fails, instruction unclear)\n- Plan has critical gaps preventing starting\n- You don't understand an instruction\n- Verification fails repeatedly\n\n**Ask for clarification rather than guessing.**\n\n## When to Revisit Earlier Steps\n\n**Return to Review (Step 1) when:**\n- Partner updates the plan based on your feedback\n- Fundamental approach needs rethinking\n\n**Don't force through blockers** - stop and ask.\n\n## Remember\n- Review plan critically first\n- Follow plan steps exactly\n- Don't skip verifications\n- Reference skills when plan says to\n- Between batches: just report and wait\n- Stop when blocked, don't guess"
              },
              {
                "name": "finishing-a-development-branch",
                "description": "Use when implementation is complete, all tests pass, and you need to decide how to integrate the work - guides completion of development work by presenting structured options for merge, PR, or cleanup",
                "path": "plugins/superpowers/skills/finishing-a-development-branch/SKILL.md",
                "frontmatter": {
                  "name": "finishing-a-development-branch",
                  "description": "Use when implementation is complete, all tests pass, and you need to decide how to integrate the work - guides completion of development work by presenting structured options for merge, PR, or cleanup"
                },
                "content": "# Finishing a Development Branch\n\n## Overview\n\nGuide completion of development work by presenting clear options and handling chosen workflow.\n\n**Core principle:** Verify tests â†’ Present options â†’ Execute choice â†’ Clean up.\n\n**Announce at start:** \"I'm using the finishing-a-development-branch skill to complete this work.\"\n\n## The Process\n\n### Step 1: Verify Tests\n\n**Before presenting options, verify tests pass:**\n\n```bash\n# Run project's test suite\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:**\n```\nTests failing (<N> failures). Must fix before completing:\n\n[Show failures]\n\nCannot proceed with merge/PR until tests pass.\n```\n\nStop. Don't proceed to Step 2.\n\n**If tests pass:** Continue to Step 2.\n\n### Step 2: Determine Base Branch\n\n```bash\n# Try common base branches\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null\n```\n\nOr ask: \"This branch split from main - is that correct?\"\n\n### Step 3: Present Options\n\nPresent exactly these 4 options:\n\n```\nImplementation complete. What would you like to do?\n\n1. Merge back to <base-branch> locally\n2. Push and create a Pull Request\n3. Keep the branch as-is (I'll handle it later)\n4. Discard this work\n\nWhich option?\n```\n\n**Don't add explanation** - keep options concise.\n\n### Step 4: Execute Choice\n\n#### Option 1: Merge Locally\n\n```bash\n# Switch to base branch\ngit checkout <base-branch>\n\n# Pull latest\ngit pull\n\n# Merge feature branch\ngit merge <feature-branch>\n\n# Verify tests on merged result\n<test command>\n\n# If tests pass\ngit branch -d <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 2: Push and Create PR\n\n```bash\n# Push branch\ngit push -u origin <feature-branch>\n\n# Create PR\ngh pr create --title \"<title>\" --body \"$(cat <<'EOF'\n## Summary\n<2-3 bullets of what changed>\n\n## Test Plan\n- [ ] <verification steps>\nEOF\n)\"\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 3: Keep As-Is\n\nReport: \"Keeping branch <name>. Worktree preserved at <path>.\"\n\n**Don't cleanup worktree.**\n\n#### Option 4: Discard\n\n**Confirm first:**\n```\nThis will permanently delete:\n- Branch <name>\n- All commits: <commit-list>\n- Worktree at <path>\n\nType 'discard' to confirm.\n```\n\nWait for exact confirmation.\n\nIf confirmed:\n```bash\ngit checkout <base-branch>\ngit branch -D <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n### Step 5: Cleanup Worktree\n\n**For Options 1, 2, 4:**\n\nCheck if in worktree:\n```bash\ngit worktree list | grep $(git branch --show-current)\n```\n\nIf yes:\n```bash\ngit worktree remove <worktree-path>\n```\n\n**For Option 3:** Keep worktree.\n\n## Quick Reference\n\n| Option | Merge | Push | Keep Worktree | Cleanup Branch |\n|--------|-------|------|---------------|----------------|\n| 1. Merge locally | âœ“ | - | - | âœ“ |\n| 2. Create PR | - | âœ“ | âœ“ | - |\n| 3. Keep as-is | - | - | âœ“ | - |\n| 4. Discard | - | - | - | âœ“ (force) |\n\n## Common Mistakes\n\n**Skipping test verification**\n- **Problem:** Merge broken code, create failing PR\n- **Fix:** Always verify tests before offering options\n\n**Open-ended questions**\n- **Problem:** \"What should I do next?\" â†’ ambiguous\n- **Fix:** Present exactly 4 structured options\n\n**Automatic worktree cleanup**\n- **Problem:** Remove worktree when might need it (Option 2, 3)\n- **Fix:** Only cleanup for Options 1 and 4\n\n**No confirmation for discard**\n- **Problem:** Accidentally delete work\n- **Fix:** Require typed \"discard\" confirmation\n\n## Red Flags\n\n**Never:**\n- Proceed with failing tests\n- Merge without verifying tests on result\n- Delete work without confirmation\n- Force-push without explicit request\n\n**Always:**\n- Verify tests before offering options\n- Present exactly 4 options\n- Get typed confirmation for Option 4\n- Clean up worktree for Options 1 & 4 only\n\n## Integration\n\n**Called by:**\n- **subagent-driven-development** (Step 7) - After all tasks complete\n- **executing-plans** (Step 5) - After all batches complete\n\n**Pairs with:**\n- **using-git-worktrees** - Cleans up worktree created by that skill"
              },
              {
                "name": "receiving-code-review",
                "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation",
                "path": "plugins/superpowers/skills/receiving-code-review/SKILL.md",
                "frontmatter": {
                  "name": "receiving-code-review",
                  "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation"
                },
                "content": "# Code Review Reception\n\n## Overview\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\" (explicit CLAUDE.md violation)\n- \"Great point!\" / \"Excellent feedback!\" (performative)\n- \"Let me implement that now\" (before verification)\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nyour human partner: \"Fix 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\nâŒ WRONG: Implement 1,2,3,6 now, ask about 4,5 later\nâœ… RIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5 before proceeding.\"\n```\n\n## Source-Specific Handling\n\n### From your human partner\n- **Trusted** - implement after understanding\n- **Still ask** if scope unclear\n- **No performative agreement**\n- **Skip to action** or technical acknowledgment\n\n### From External Reviewers\n```\nBEFORE implementing:\n  1. Check: Technically correct for THIS codebase?\n  2. Check: Breaks existing functionality?\n  3. Check: Reason for current implementation?\n  4. Check: Works on all platforms/versions?\n  5. Check: Does reviewer understand full context?\n\nIF suggestion seems wrong:\n  Push back with technical reasoning\n\nIF can't easily verify:\n  Say so: \"I can't verify this without [X]. Should I [investigate/ask/proceed]?\"\n\nIF conflicts with your human partner's prior decisions:\n  Stop and discuss with your human partner first\n```\n\n**your human partner's rule:** \"External feedback - be skeptical, but check carefully\"\n\n## YAGNI Check for \"Professional\" Features\n\n```\nIF reviewer suggests \"implementing properly\":\n  grep codebase for actual usage\n\n  IF unused: \"This endpoint isn't called. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n**your human partner's rule:** \"You and reviewer both report to me. If we don't need this feature, don't add it.\"\n\n## Implementation Order\n\n```\nFOR multi-item feedback:\n  1. Clarify anything unclear FIRST\n  2. Then implement in this order:\n     - Blocking issues (breaks, security)\n     - Simple fixes (typos, imports)\n     - Complex fixes (refactoring, logic)\n  3. Test each fix individually\n  4. Verify no regressions\n```\n\n## When To Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with your human partner's architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Involve your human partner if architectural\n\n**Signal if uncomfortable pushing back out loud:** \"Strange things are afoot at the Circle K\"\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n```\nâœ… \"Fixed. [Brief description of what changed]\"\nâœ… \"Good catch - [specific issue]. Fixed in [location].\"\nâœ… [Just fix it and show in the code]\n\nâŒ \"You're absolutely right!\"\nâŒ \"Great point!\"\nâŒ \"Thanks for catching that!\"\nâŒ \"Thanks for [anything]\"\nâŒ ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it. The code itself shows you heard the feedback.\n\n**If you catch yourself about to write \"Thanks\":** DELETE IT. State the fix instead.\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n```\nâœ… \"You were right - I checked [X] and it does [Y]. Implementing now.\"\nâœ… \"Verified this and you're correct. My initial understanding was wrong because [reason]. Fixing.\"\n\nâŒ Long apology\nâŒ Defending why you pushed back\nâŒ Over-explaining\n```\n\nState the correction factually and move on.\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n| Can't verify, proceed anyway | State limitation, ask for direction |\n\n## Real Examples\n\n**Performative Agreement (Bad):**\n```\nReviewer: \"Remove legacy code\"\nâŒ \"You're absolutely right! Let me remove that...\"\n```\n\n**Technical Verification (Good):**\n```\nReviewer: \"Remove legacy code\"\nâœ… \"Checking... build target is 10.15+, this API needs 13+. Need legacy for backward compat. Current impl has wrong bundle ID - fix it or drop pre-13 support?\"\n```\n\n**YAGNI (Good):**\n```\nReviewer: \"Implement proper metrics tracking with database, date filters, CSV export\"\nâœ… \"Grepped codebase - nothing calls this endpoint. Remove it (YAGNI)? Or is there usage I'm missing?\"\n```\n\n**Unclear Item (Good):**\n```\nyour human partner: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\nâœ… \"Understand 1,2,3,6. Need clarification on 4 and 5 before implementing.\"\n```\n\n## GitHub Thread Replies\n\nWhen replying to inline review comments on GitHub, reply in the comment thread (`gh api repos/{owner}/{repo}/pulls/{pr}/comments/{id}/replies`), not as a top-level PR comment.\n\n## The Bottom Line\n\n**External feedback = suggestions to evaluate, not orders to follow.**\n\nVerify. Question. Then implement.\n\nNo performative agreement. Technical rigor always."
              },
              {
                "name": "requesting-code-review",
                "description": "Use when completing tasks, implementing major features, or before merging to verify work meets requirements",
                "path": "plugins/superpowers/skills/requesting-code-review/SKILL.md",
                "frontmatter": {
                  "name": "requesting-code-review",
                  "description": "Use when completing tasks, implementing major features, or before merging to verify work meets requirements"
                },
                "content": "# Requesting Code Review\n\nDispatch superpowers:code-reviewer subagent to catch issues before they cascade.\n\n**Core principle:** Review early, review often.\n\n## When to Request Review\n\n**Mandatory:**\n- After each task in subagent-driven development\n- After completing major feature\n- Before merge to main\n\n**Optional but valuable:**\n- When stuck (fresh perspective)\n- Before refactoring (baseline check)\n- After fixing complex bug\n\n## How to Request\n\n**1. Get git SHAs:**\n```bash\nBASE_SHA=$(git rev-parse HEAD~1)  # or origin/main\nHEAD_SHA=$(git rev-parse HEAD)\n```\n\n**2. Dispatch code-reviewer subagent:**\n\nUse Task tool with superpowers:code-reviewer type, fill template at `code-reviewer.md`\n\n**Placeholders:**\n- `{WHAT_WAS_IMPLEMENTED}` - What you just built\n- `{PLAN_OR_REQUIREMENTS}` - What it should do\n- `{BASE_SHA}` - Starting commit\n- `{HEAD_SHA}` - Ending commit\n- `{DESCRIPTION}` - Brief summary\n\n**3. Act on feedback:**\n- Fix Critical issues immediately\n- Fix Important issues before proceeding\n- Note Minor issues for later\n- Push back if reviewer is wrong (with reasoning)\n\n## Example\n\n```\n[Just completed Task 2: Add verification function]\n\nYou: Let me request code review before proceeding.\n\nBASE_SHA=$(git log --oneline | grep \"Task 1\" | head -1 | awk '{print $1}')\nHEAD_SHA=$(git rev-parse HEAD)\n\n[Dispatch superpowers:code-reviewer subagent]\n  WHAT_WAS_IMPLEMENTED: Verification and repair functions for conversation index\n  PLAN_OR_REQUIREMENTS: Task 2 from docs/plans/deployment-plan.md\n  BASE_SHA: a7981ec\n  HEAD_SHA: 3df7661\n  DESCRIPTION: Added verifyIndex() and repairIndex() with 4 issue types\n\n[Subagent returns]:\n  Strengths: Clean architecture, real tests\n  Issues:\n    Important: Missing progress indicators\n    Minor: Magic number (100) for reporting interval\n  Assessment: Ready to proceed\n\nYou: [Fix progress indicators]\n[Continue to Task 3]\n```\n\n## Integration with Workflows\n\n**Subagent-Driven Development:**\n- Review after EACH task\n- Catch issues before they compound\n- Fix before moving to next task\n\n**Executing Plans:**\n- Review after each batch (3 tasks)\n- Get feedback, apply, continue\n\n**Ad-Hoc Development:**\n- Review before merge\n- Review when stuck\n\n## Red Flags\n\n**Never:**\n- Skip review because \"it's simple\"\n- Ignore Critical issues\n- Proceed with unfixed Important issues\n- Argue with valid technical feedback\n\n**If reviewer wrong:**\n- Push back with technical reasoning\n- Show code/tests that prove it works\n- Request clarification\n\nSee template at: requesting-code-review/code-reviewer.md"
              },
              {
                "name": "subagent-driven-development",
                "description": "Use when executing implementation plans with independent tasks in the current session",
                "path": "plugins/superpowers/skills/subagent-driven-development/SKILL.md",
                "frontmatter": {
                  "name": "subagent-driven-development",
                  "description": "Use when executing implementation plans with independent tasks in the current session"
                },
                "content": "# Subagent-Driven Development\n\nExecute plan by dispatching fresh subagent per task, with two-stage review after each: spec compliance review first, then code quality review.\n\n**Core principle:** Fresh subagent per task + two-stage review (spec then quality) = high quality, fast iteration\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Have implementation plan?\" [shape=diamond];\n    \"Tasks mostly independent?\" [shape=diamond];\n    \"Stay in this session?\" [shape=diamond];\n    \"subagent-driven-development\" [shape=box];\n    \"executing-plans\" [shape=box];\n    \"Manual execution or brainstorm first\" [shape=box];\n\n    \"Have implementation plan?\" -> \"Tasks mostly independent?\" [label=\"yes\"];\n    \"Have implementation plan?\" -> \"Manual execution or brainstorm first\" [label=\"no\"];\n    \"Tasks mostly independent?\" -> \"Stay in this session?\" [label=\"yes\"];\n    \"Tasks mostly independent?\" -> \"Manual execution or brainstorm first\" [label=\"no - tightly coupled\"];\n    \"Stay in this session?\" -> \"subagent-driven-development\" [label=\"yes\"];\n    \"Stay in this session?\" -> \"executing-plans\" [label=\"no - parallel session\"];\n}\n```\n\n**vs. Executing Plans (parallel session):**\n- Same session (no context switch)\n- Fresh subagent per task (no context pollution)\n- Two-stage review after each task: spec compliance first, then code quality\n- Faster iteration (no human-in-loop between tasks)\n\n## The Process\n\n```dot\ndigraph process {\n    rankdir=TB;\n\n    subgraph cluster_per_task {\n        label=\"Per Task\";\n        \"Dispatch implementer subagent (./implementer-prompt.md)\" [shape=box];\n        \"Implementer subagent asks questions?\" [shape=diamond];\n        \"Answer questions, provide context\" [shape=box];\n        \"Implementer subagent implements, tests, commits, self-reviews\" [shape=box];\n        \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [shape=box];\n        \"Spec reviewer subagent confirms code matches spec?\" [shape=diamond];\n        \"Implementer subagent fixes spec gaps\" [shape=box];\n        \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [shape=box];\n        \"Code quality reviewer subagent approves?\" [shape=diamond];\n        \"Implementer subagent fixes quality issues\" [shape=box];\n        \"Mark task complete in TodoWrite\" [shape=box];\n    }\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" [shape=box];\n    \"More tasks remain?\" [shape=diamond];\n    \"Dispatch final code reviewer subagent for entire implementation\" [shape=box];\n    \"Use superpowers:finishing-a-development-branch\" [shape=box style=filled fillcolor=lightgreen];\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Dispatch implementer subagent (./implementer-prompt.md)\" -> \"Implementer subagent asks questions?\";\n    \"Implementer subagent asks questions?\" -> \"Answer questions, provide context\" [label=\"yes\"];\n    \"Answer questions, provide context\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Implementer subagent asks questions?\" -> \"Implementer subagent implements, tests, commits, self-reviews\" [label=\"no\"];\n    \"Implementer subagent implements, tests, commits, self-reviews\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\";\n    \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" -> \"Spec reviewer subagent confirms code matches spec?\";\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Implementer subagent fixes spec gaps\" [label=\"no\"];\n    \"Implementer subagent fixes spec gaps\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"yes\"];\n    \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" -> \"Code quality reviewer subagent approves?\";\n    \"Code quality reviewer subagent approves?\" -> \"Implementer subagent fixes quality issues\" [label=\"no\"];\n    \"Implementer subagent fixes quality issues\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Code quality reviewer subagent approves?\" -> \"Mark task complete in TodoWrite\" [label=\"yes\"];\n    \"Mark task complete in TodoWrite\" -> \"More tasks remain?\";\n    \"More tasks remain?\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\" [label=\"yes\"];\n    \"More tasks remain?\" -> \"Dispatch final code reviewer subagent for entire implementation\" [label=\"no\"];\n    \"Dispatch final code reviewer subagent for entire implementation\" -> \"Use superpowers:finishing-a-development-branch\";\n}\n```\n\n## Prompt Templates\n\n- `./implementer-prompt.md` - Dispatch implementer subagent\n- `./spec-reviewer-prompt.md` - Dispatch spec compliance reviewer subagent\n- `./code-quality-reviewer-prompt.md` - Dispatch code quality reviewer subagent\n\n## Example Workflow\n\n```\nYou: I'm using Subagent-Driven Development to execute this plan.\n\n[Read plan file once: docs/plans/feature-plan.md]\n[Extract all 5 tasks with full text and context]\n[Create TodoWrite with all tasks]\n\nTask 1: Hook installation script\n\n[Get Task 1 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: \"Before I begin - should the hook be installed at user or system level?\"\n\nYou: \"User level (~/.config/superpowers/hooks/)\"\n\nImplementer: \"Got it. Implementing now...\"\n[Later] Implementer:\n  - Implemented install-hook command\n  - Added tests, 5/5 passing\n  - Self-review: Found I missed --force flag, added it\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer: âœ… Spec compliant - all requirements met, nothing extra\n\n[Get git SHAs, dispatch code quality reviewer]\nCode reviewer: Strengths: Good test coverage, clean. Issues: None. Approved.\n\n[Mark Task 1 complete]\n\nTask 2: Recovery modes\n\n[Get Task 2 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: [No questions, proceeds]\nImplementer:\n  - Added verify/repair modes\n  - 8/8 tests passing\n  - Self-review: All good\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer: âŒ Issues:\n  - Missing: Progress reporting (spec says \"report every 100 items\")\n  - Extra: Added --json flag (not requested)\n\n[Implementer fixes issues]\nImplementer: Removed --json flag, added progress reporting\n\n[Spec reviewer reviews again]\nSpec reviewer: âœ… Spec compliant now\n\n[Dispatch code quality reviewer]\nCode reviewer: Strengths: Solid. Issues (Important): Magic number (100)\n\n[Implementer fixes]\nImplementer: Extracted PROGRESS_INTERVAL constant\n\n[Code reviewer reviews again]\nCode reviewer: âœ… Approved\n\n[Mark Task 2 complete]\n\n...\n\n[After all tasks]\n[Dispatch final code-reviewer]\nFinal reviewer: All requirements met, ready to merge\n\nDone!\n```\n\n## Advantages\n\n**vs. Manual execution:**\n- Subagents follow TDD naturally\n- Fresh context per task (no confusion)\n- Parallel-safe (subagents don't interfere)\n- Subagent can ask questions (before AND during work)\n\n**vs. Executing Plans:**\n- Same session (no handoff)\n- Continuous progress (no waiting)\n- Review checkpoints automatic\n\n**Efficiency gains:**\n- No file reading overhead (controller provides full text)\n- Controller curates exactly what context is needed\n- Subagent gets complete information upfront\n- Questions surfaced before work begins (not after)\n\n**Quality gates:**\n- Self-review catches issues before handoff\n- Two-stage review: spec compliance, then code quality\n- Review loops ensure fixes actually work\n- Spec compliance prevents over/under-building\n- Code quality ensures implementation is well-built\n\n**Cost:**\n- More subagent invocations (implementer + 2 reviewers per task)\n- Controller does more prep work (extracting all tasks upfront)\n- Review loops add iterations\n- But catches issues early (cheaper than debugging later)\n\n## Red Flags\n\n**Never:**\n- Skip reviews (spec compliance OR code quality)\n- Proceed with unfixed issues\n- Dispatch multiple implementation subagents in parallel (conflicts)\n- Make subagent read plan file (provide full text instead)\n- Skip scene-setting context (subagent needs to understand where task fits)\n- Ignore subagent questions (answer before letting them proceed)\n- Accept \"close enough\" on spec compliance (spec reviewer found issues = not done)\n- Skip review loops (reviewer found issues = implementer fixes = review again)\n- Let implementer self-review replace actual review (both are needed)\n- **Start code quality review before spec compliance is âœ…** (wrong order)\n- Move to next task while either review has open issues\n\n**If subagent asks questions:**\n- Answer clearly and completely\n- Provide additional context if needed\n- Don't rush them into implementation\n\n**If reviewer finds issues:**\n- Implementer (same subagent) fixes them\n- Reviewer reviews again\n- Repeat until approved\n- Don't skip the re-review\n\n**If subagent fails task:**\n- Dispatch fix subagent with specific instructions\n- Don't try to fix manually (context pollution)\n\n## Integration\n\n**Required workflow skills:**\n- **superpowers:writing-plans** - Creates the plan this skill executes\n- **superpowers:requesting-code-review** - Code review template for reviewer subagents\n- **superpowers:finishing-a-development-branch** - Complete development after all tasks\n\n**Subagents should use:**\n- **superpowers:test-driven-development** - Subagents follow TDD for each task\n\n**Alternative workflow:**\n- **superpowers:executing-plans** - Use for parallel session instead of same-session execution"
              },
              {
                "name": "systematic-debugging",
                "description": "Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes",
                "path": "plugins/superpowers/skills/systematic-debugging/SKILL.md",
                "frontmatter": {
                  "name": "systematic-debugging",
                  "description": "Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes"
                },
                "content": "# Systematic Debugging\n\n## Overview\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n**Violating the letter of this process is violating the spirit of debugging.**\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Use this ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n- You don't fully understand the issue\n\n**Don't skip when:**\n- Issue seems simple (simple bugs have root causes too)\n- You're in a hurry (rushing guarantees rework)\n- Manager wants it fixed NOW (systematic is faster than thrashing)\n\n## The Four Phases\n\nYou MUST complete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - They often contain the exact solution\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - Does it happen every time?\n   - If not reproducible â†’ gather more data, don't guess\n\n3. **Check Recent Changes**\n   - What changed that could cause this?\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n\n   **WHEN system has multiple components (CI â†’ build â†’ signing, API â†’ service â†’ database):**\n\n   **BEFORE proposing fixes, add diagnostic instrumentation:**\n   ```\n   For EACH component boundary:\n     - Log what data enters component\n     - Log what data exits component\n     - Verify environment/config propagation\n     - Check state at each layer\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN analyze evidence to identify failing component\n   THEN investigate that specific component\n   ```\n\n   **Example (multi-layer system):**\n   ```bash\n   # Layer 1: Workflow\n   echo \"=== Secrets available in workflow: ===\"\n   echo \"IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}\"\n\n   # Layer 2: Build script\n   echo \"=== Env vars in build script: ===\"\n   env | grep IDENTITY || echo \"IDENTITY not in environment\"\n\n   # Layer 3: Signing script\n   echo \"=== Keychain state: ===\"\n   security list-keychains\n   security find-identity -v\n\n   # Layer 4: Actual signing\n   codesign --sign \"$IDENTITY\" --verbose=4 \"$APP\"\n   ```\n\n   **This reveals:** Which layer fails (secrets â†’ workflow âœ“, workflow â†’ build âœ—)\n\n5. **Trace Data Flow**\n\n   **WHEN error is deep in call stack:**\n\n   See `root-cause-tracing.md` in this directory for the complete backward tracing technique.\n\n   **Quick version:**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n**Find the pattern before fixing:**\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - If implementing pattern, read reference implementation COMPLETELY\n   - Don't skim - read every line\n   - Understand the pattern fully before applying\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n   - Don't assume \"that can't matter\"\n\n4. **Understand Dependencies**\n   - What other components does this need?\n   - What settings, config, environment?\n   - What assumptions does it make?\n\n### Phase 3: Hypothesis and Testing\n\n**Scientific method:**\n\n1. **Form Single Hypothesis**\n   - State clearly: \"I think X is the root cause because Y\"\n   - Write it down\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change to test hypothesis\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes â†’ Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n4. **When You Don't Know**\n   - Say \"I don't understand X\"\n   - Don't pretend to know\n   - Ask for help\n   - Research more\n\n### Phase 4: Implementation\n\n**Fix the root cause, not the symptom:**\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - Automated test if possible\n   - One-off test script if no framework\n   - MUST have before fixing\n   - Use the `superpowers:test-driven-development` skill for writing proper failing tests\n\n2. **Implement Single Fix**\n   - Address the root cause identified\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n   - No bundled refactoring\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n   - Issue actually resolved?\n\n4. **If Fix Doesn't Work**\n   - STOP\n   - Count: How many fixes have you tried?\n   - If < 3: Return to Phase 1, re-analyze with new information\n   - **If â‰¥ 3: STOP and question the architecture (step 5 below)**\n   - DON'T attempt Fix #4 without architectural discussion\n\n5. **If 3+ Fixes Failed: Question Architecture**\n\n   **Pattern indicating architectural problem:**\n   - Each fix reveals new shared state/coupling/problem in different place\n   - Fixes require \"massive refactoring\" to implement\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals:**\n   - Is this pattern fundamentally sound?\n   - Are we \"sticking with it through sheer inertia\"?\n   - Should we refactor architecture vs. continue fixing symptoms?\n\n   **Discuss with your human partner before attempting more fixes**\n\n   This is NOT a failed hypothesis - this is a wrong architecture.\n\n## Red Flags - STOP and Follow Process\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"Add multiple changes, run tests\"\n- \"Skip the test, I'll manually verify\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Pattern says X but I'll adapt it differently\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- **\"One more fix attempt\" (when already tried 2+)**\n- **Each fix reveals new problem in different place**\n\n**ALL of these mean: STOP. Return to Phase 1.**\n\n**If 3+ fixes failed:** Question the architecture (see Phase 4.5)\n\n## your human partner's Signals You're Doing It Wrong\n\n**Watch for these redirections:**\n- \"Is that not happening?\" - You assumed without verifying\n- \"Will it show us...?\" - You should have added evidence gathering\n- \"Stop guessing\" - You're proposing fixes without understanding\n- \"Ultrathink this\" - Question fundamentals, not just symptoms\n- \"We're stuck?\" (frustrated) - Your approach isn't working\n\n**When you see these:** STOP. Return to Phase 1.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple, don't need process\" | Simple issues have root causes too. Process is fast for simple bugs. |\n| \"Emergency, no time for process\" | Systematic debugging is FASTER than guess-and-check thrashing. |\n| \"Just try this first, then investigate\" | First fix sets the pattern. Do it right from the start. |\n| \"I'll write test after confirming fix works\" | Untested fixes don't stick. Test first proves it. |\n| \"Multiple fixes at once saves time\" | Can't isolate what worked. Causes new bugs. |\n| \"Reference too long, I'll adapt the pattern\" | Partial understanding guarantees bugs. Read it completely. |\n| \"I see the problem, let me fix it\" | Seeing symptoms â‰  understanding root cause. |\n| \"One more fix attempt\" (after 2+ failures) | 3+ failures = architectural problem. Question pattern, don't fix again. |\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n## When Process Reveals \"No Root Cause\"\n\nIf systematic investigation reveals issue is truly environmental, timing-dependent, or external:\n\n1. You've completed the process\n2. Document what you investigated\n3. Implement appropriate handling (retry, timeout, error message)\n4. Add monitoring/logging for future investigation\n\n**But:** 95% of \"no root cause\" cases are incomplete investigation.\n\n## Supporting Techniques\n\nThese techniques are part of systematic debugging and available in this directory:\n\n- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger\n- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause\n- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling\n\n**Related skills:**\n- **superpowers:test-driven-development** - For creating failing test case (Phase 4, Step 1)\n- **superpowers:verification-before-completion** - Verify fix worked before claiming success\n\n## Real-World Impact\n\nFrom debugging sessions:\n- Systematic approach: 15-30 minutes to fix\n- Random fixes approach: 2-3 hours of thrashing\n- First-time fix rate: 95% vs 40%\n- New bugs introduced: Near zero vs common"
              },
              {
                "name": "test-driven-development",
                "description": "Use when implementing any feature or bugfix, before writing implementation code",
                "path": "plugins/superpowers/skills/test-driven-development/SKILL.md",
                "frontmatter": {
                  "name": "test-driven-development",
                  "description": "Use when implementing any feature or bugfix, before writing implementation code"
                },
                "content": "# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```dot\ndigraph tdd_cycle {\n    rankdir=LR;\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\n    next [label=\"Next\", shape=ellipse];\n\n    red -> verify_red;\n    verify_red -> green [label=\"yes\"];\n    verify_red -> red [label=\"wrong\\nfailure\"];\n    green -> verify_green;\n    verify_green -> refactor [label=\"yes\"];\n    verify_green -> green [label=\"no\"];\n    refactor -> verify_green [label=\"stay\\ngreen\"];\n    verify_green -> next;\n    next -> red;\n}\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n<Good>\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing\n</Good>\n\n<Bad>\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code\n</Bad>\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n<Good>\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass\n</Good>\n\n<Bad>\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered\n</Bad>\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\" â‰  comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after â‰  TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc â‰  systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n\n## Final Rule\n\n```\nProduction code â†’ test exists and failed first\nOtherwise â†’ not TDD\n```\n\nNo exceptions without your human partner's permission."
              },
              {
                "name": "using-git-worktrees",
                "description": "Use when starting feature work that needs isolation from current workspace or before executing implementation plans - creates isolated git worktrees with smart directory selection and safety verification",
                "path": "plugins/superpowers/skills/using-git-worktrees/SKILL.md",
                "frontmatter": {
                  "name": "using-git-worktrees",
                  "description": "Use when starting feature work that needs isolation from current workspace or before executing implementation plans - creates isolated git worktrees with smart directory selection and safety verification"
                },
                "content": "# Using Git Worktrees\n\n## Overview\n\nGit worktrees create isolated workspaces sharing the same repository, allowing work on multiple branches simultaneously without switching.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n**Announce at start:** \"I'm using the using-git-worktrees skill to set up an isolated workspace.\"\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\n# Check in priority order\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check CLAUDE.md\n\n```bash\ngrep -i \"worktree.*director\" CLAUDE.md 2>/dev/null\n```\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no CLAUDE.md preference:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/.config/superpowers/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n## Safety Verification\n\n### For Project-Local Directories (.worktrees or worktrees)\n\n**MUST verify directory is ignored before creating worktree:**\n\n```bash\n# Check if directory is ignored (respects local, global, and system gitignore)\ngit check-ignore -q .worktrees 2>/dev/null || git check-ignore -q worktrees 2>/dev/null\n```\n\n**If NOT ignored:**\n\nPer Jesse's rule \"Fix broken things immediately\":\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents to repository.\n\n### For Global Directory (~/.config/superpowers/worktrees)\n\nNo .gitignore verification needed - outside project entirely.\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\n# Determine full path\ncase $LOCATION in\n  .worktrees|worktrees)\n    path=\"$LOCATION/$BRANCH_NAME\"\n    ;;\n  ~/.config/superpowers/worktrees/*)\n    path=\"~/.config/superpowers/worktrees/$project/$BRANCH_NAME\"\n    ;;\nesac\n\n# Create worktree with new branch\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Node.js\nif [ -f package.json ]; then npm install; fi\n\n# Rust\nif [ -f Cargo.toml ]; then cargo build; fi\n\n# Python\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then poetry install; fi\n\n# Go\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Examples - use project-appropriate command\nnpm test\ncargo test\npytest\ngo test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify ignored) |\n| `worktrees/` exists | Use it (verify ignored) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check CLAUDE.md â†’ Ask user |\n| Directory not ignored | Add to .gitignore + commit |\n| Tests fail during baseline | Report failures + ask |\n| No package.json/Cargo.toml | Skip dependency install |\n\n## Common Mistakes\n\n### Skipping ignore verification\n\n- **Problem:** Worktree contents get tracked, pollute git status\n- **Fix:** Always use `git check-ignore` before creating project-local worktree\n\n### Assuming directory location\n\n- **Problem:** Creates inconsistency, violates project conventions\n- **Fix:** Follow priority: existing > CLAUDE.md > ask\n\n### Proceeding with failing tests\n\n- **Problem:** Can't distinguish new bugs from pre-existing issues\n- **Fix:** Report failures, get explicit permission to proceed\n\n### Hardcoding setup commands\n\n- **Problem:** Breaks on projects using different tools\n- **Fix:** Auto-detect from project files (package.json, etc.)\n\n## Example Workflow\n\n```\nYou: I'm using the using-git-worktrees skill to set up an isolated workspace.\n\n[Check .worktrees/ - exists]\n[Verify ignored - git check-ignore confirms .worktrees/ is ignored]\n[Create worktree: git worktree add .worktrees/auth -b feature/auth]\n[Run npm install]\n[Run npm test - 47 passing]\n\nWorktree ready at /Users/jesse/myproject/.worktrees/auth\nTests passing (47 tests, 0 failures)\nReady to implement auth feature\n```\n\n## Red Flags\n\n**Never:**\n- Create worktree without verifying it's ignored (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n- Skip CLAUDE.md check\n\n**Always:**\n- Follow directory priority: existing > CLAUDE.md > ask\n- Verify directory is ignored for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n## Integration\n\n**Called by:**\n- **brainstorming** (Phase 4) - REQUIRED when design is approved and implementation follows\n- Any skill needing isolated workspace\n\n**Pairs with:**\n- **finishing-a-development-branch** - REQUIRED for cleanup after work complete\n- **executing-plans** or **subagent-driven-development** - Work happens in this worktree"
              },
              {
                "name": "using-superpowers",
                "description": "Use when starting any conversation - establishes how to find and use skills, requiring Skill tool invocation before ANY response including clarifying questions",
                "path": "plugins/superpowers/skills/using-superpowers/SKILL.md",
                "frontmatter": {
                  "name": "using-superpowers",
                  "description": "Use when starting any conversation - establishes how to find and use skills, requiring Skill tool invocation before ANY response including clarifying questions"
                },
                "content": "<EXTREMELY-IMPORTANT>\nIf you think there is even a 1% chance a skill might apply to what you are doing, you ABSOLUTELY MUST invoke the skill.\n\nIF A SKILL APPLIES TO YOUR TASK, YOU DO NOT HAVE A CHOICE. YOU MUST USE IT.\n\nThis is not negotiable. This is not optional. You cannot rationalize your way out of this.\n</EXTREMELY-IMPORTANT>\n\n## How to Access Skills\n\n**In Claude Code:** Use the `Skill` tool. When you invoke a skill, its content is loaded and presented to youâ€”follow it directly. Never use the Read tool on skill files.\n\n**In other environments:** Check your platform's documentation for how skills are loaded.\n\n# Using Skills\n\n## The Rule\n\n**Invoke relevant or requested skills BEFORE any response or action.** Even a 1% chance a skill might apply means that you should invoke the skill to check. If an invoked skill turns out to be wrong for the situation, you don't need to use it.\n\n```dot\ndigraph skill_flow {\n    \"User message received\" [shape=doublecircle];\n    \"Might any skill apply?\" [shape=diamond];\n    \"Invoke Skill tool\" [shape=box];\n    \"Announce: 'Using [skill] to [purpose]'\" [shape=box];\n    \"Has checklist?\" [shape=diamond];\n    \"Create TodoWrite todo per item\" [shape=box];\n    \"Follow skill exactly\" [shape=box];\n    \"Respond (including clarifications)\" [shape=doublecircle];\n\n    \"User message received\" -> \"Might any skill apply?\";\n    \"Might any skill apply?\" -> \"Invoke Skill tool\" [label=\"yes, even 1%\"];\n    \"Might any skill apply?\" -> \"Respond (including clarifications)\" [label=\"definitely not\"];\n    \"Invoke Skill tool\" -> \"Announce: 'Using [skill] to [purpose]'\";\n    \"Announce: 'Using [skill] to [purpose]'\" -> \"Has checklist?\";\n    \"Has checklist?\" -> \"Create TodoWrite todo per item\" [label=\"yes\"];\n    \"Has checklist?\" -> \"Follow skill exactly\" [label=\"no\"];\n    \"Create TodoWrite todo per item\" -> \"Follow skill exactly\";\n}\n```\n\n## Red Flags\n\nThese thoughts mean STOPâ€”you're rationalizing:\n\n| Thought | Reality |\n|---------|---------|\n| \"This is just a simple question\" | Questions are tasks. Check for skills. |\n| \"I need more context first\" | Skill check comes BEFORE clarifying questions. |\n| \"Let me explore the codebase first\" | Skills tell you HOW to explore. Check first. |\n| \"I can check git/files quickly\" | Files lack conversation context. Check for skills. |\n| \"Let me gather information first\" | Skills tell you HOW to gather information. |\n| \"This doesn't need a formal skill\" | If a skill exists, use it. |\n| \"I remember this skill\" | Skills evolve. Read current version. |\n| \"This doesn't count as a task\" | Action = task. Check for skills. |\n| \"The skill is overkill\" | Simple things become complex. Use it. |\n| \"I'll just do this one thing first\" | Check BEFORE doing anything. |\n| \"This feels productive\" | Undisciplined action wastes time. Skills prevent this. |\n| \"I know what that means\" | Knowing the concept â‰  using the skill. Invoke it. |\n\n## Skill Priority\n\nWhen multiple skills could apply, use this order:\n\n1. **Process skills first** (brainstorming, debugging) - these determine HOW to approach the task\n2. **Implementation skills second** (frontend-design, mcp-builder) - these guide execution\n\n\"Let's build X\" â†’ brainstorming first, then implementation skills.\n\"Fix this bug\" â†’ debugging first, then domain-specific skills.\n\n## Skill Types\n\n**Rigid** (TDD, debugging): Follow exactly. Don't adapt away discipline.\n\n**Flexible** (patterns): Adapt principles to context.\n\nThe skill itself tells you which.\n\n## User Instructions\n\nInstructions say WHAT, not HOW. \"Add X\" or \"Fix Y\" doesn't mean skip workflows."
              },
              {
                "name": "verification-before-completion",
                "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
                "path": "plugins/superpowers/skills/verification-before-completion/SKILL.md",
                "frontmatter": {
                  "name": "verification-before-completion",
                  "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always"
                },
                "content": "# Verification Before Completion\n\n## Overview\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n**Violating the letter of this rule is violating the spirit of this rule.**\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n## The Gate Function\n\n```\nBEFORE claiming any status or expressing satisfaction:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n## Common Failures\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test command output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check, extrapolation |\n| Build succeeds | Build command: exit 0 | Linter passing, logs look good |\n| Bug fixed | Test original symptom: passes | Code changed, assumed fixed |\n| Regression test works | Red-green cycle verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\", \"Done!\", etc.)\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- Tired and wanting work over\n- **ANY wording implying success without having run verification**\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence â‰  evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter â‰  compiler |\n| \"Agent said success\" | Verify independently |\n| \"I'm tired\" | Exhaustion â‰  excuse |\n| \"Partial check is enough\" | Partial proves nothing |\n| \"Different words so rule doesn't apply\" | Spirit over letter |\n\n## Key Patterns\n\n**Tests:**\n```\nâœ… [Run test command] [See: 34/34 pass] \"All tests pass\"\nâŒ \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\nâœ… Write â†’ Run (pass) â†’ Revert fix â†’ Run (MUST FAIL) â†’ Restore â†’ Run (pass)\nâŒ \"I've written a regression test\" (without red-green verification)\n```\n\n**Build:**\n```\nâœ… [Run build] [See: exit 0] \"Build passes\"\nâŒ \"Linter passed\" (linter doesn't check compilation)\n```\n\n**Requirements:**\n```\nâœ… Re-read plan â†’ Create checklist â†’ Verify each â†’ Report gaps or completion\nâŒ \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\nâœ… Agent reports success â†’ Check VCS diff â†’ Verify changes â†’ Report actual state\nâŒ Trust agent report\n```\n\n## Why This Matters\n\nFrom 24 failure memories:\n- your human partner said \"I don't believe you\" - trust broken\n- Undefined functions shipped - would crash\n- Missing requirements shipped - incomplete features\n- Time wasted on false completion â†’ redirect â†’ rework\n- Violates: \"Honesty is a core value. If you lie, you'll be replaced.\"\n\n## When To Apply\n\n**ALWAYS before:**\n- ANY variation of success/completion claims\n- ANY expression of satisfaction\n- ANY positive statement about work state\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n**Rule applies to:**\n- Exact phrases\n- Paraphrases and synonyms\n- Implications of success\n- ANY communication suggesting completion/correctness\n\n## The Bottom Line\n\n**No shortcuts for verification.**\n\nRun the command. Read the output. THEN claim the result.\n\nThis is non-negotiable."
              },
              {
                "name": "writing-plans",
                "description": "Use when you have a spec or requirements for a multi-step task, before touching code",
                "path": "plugins/superpowers/skills/writing-plans/SKILL.md",
                "frontmatter": {
                  "name": "writing-plans",
                  "description": "Use when you have a spec or requirements for a multi-step task, before touching code"
                },
                "content": "# Writing Plans\n\n## Overview\n\nWrite comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.\n\nAssume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.\n\n**Announce at start:** \"I'm using the writing-plans skill to create the implementation plan.\"\n\n**Context:** This should be run in a dedicated worktree (created by brainstorming skill).\n\n**Save plans to:** `docs/plans/YYYY-MM-DD-<feature-name>.md`\n\n## Bite-Sized Task Granularity\n\n**Each step is one action (2-5 minutes):**\n- \"Write the failing test\" - step\n- \"Run it to make sure it fails\" - step\n- \"Implement the minimal code to make the test pass\" - step\n- \"Run the tests and make sure they pass\" - step\n- \"Commit\" - step\n\n## Plan Document Header\n\n**Every plan MUST start with this header:**\n\n```markdown\n# [Feature Name] Implementation Plan\n\n> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence describing what this builds]\n\n**Architecture:** [2-3 sentences about approach]\n\n**Tech Stack:** [Key technologies/libraries]\n\n---\n```\n\n## Task Structure\n\n```markdown\n### Task N: [Component Name]\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**Step 1: Write the failing test**\n\n```python\ndef test_specific_behavior():\n    result = function(input)\n    assert result == expected\n```\n\n**Step 2: Run test to verify it fails**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: FAIL with \"function not defined\"\n\n**Step 3: Write minimal implementation**\n\n```python\ndef function(input):\n    return expected\n```\n\n**Step 4: Run test to verify it passes**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: PASS\n\n**Step 5: Commit**\n\n```bash\ngit add tests/path/test.py src/path/file.py\ngit commit -m \"feat: add specific feature\"\n```\n```\n\n## Remember\n- Exact file paths always\n- Complete code in plan (not \"add validation\")\n- Exact commands with expected output\n- Reference relevant skills with @ syntax\n- DRY, YAGNI, TDD, frequent commits\n\n## Execution Handoff\n\nAfter saving the plan, offer execution choice:\n\n**\"Plan complete and saved to `docs/plans/<filename>.md`. Two execution options:**\n\n**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration\n\n**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints\n\n**Which approach?\"**\n\n**If Subagent-Driven chosen:**\n- **REQUIRED SUB-SKILL:** Use superpowers:subagent-driven-development\n- Stay in this session\n- Fresh subagent per task + code review\n\n**If Parallel Session chosen:**\n- Guide them to open new session in worktree\n- **REQUIRED SUB-SKILL:** New session uses superpowers:executing-plans"
              },
              {
                "name": "writing-skills",
                "description": "Use when creating new skills, editing existing skills, or verifying skills work before deployment",
                "path": "plugins/superpowers/skills/writing-skills/SKILL.md",
                "frontmatter": {
                  "name": "writing-skills",
                  "description": "Use when creating new skills, editing existing skills, or verifying skills work before deployment"
                },
                "content": "# Writing Skills\n\n## Overview\n\n**Writing skills IS Test-Driven Development applied to process documentation.**\n\n**Personal skills live in agent-specific directories (`~/.claude/skills` for Claude Code, `~/.codex/skills` for Codex)** \n\nYou write test cases (pressure scenarios with subagents), watch them fail (baseline behavior), write the skill (documentation), watch tests pass (agents comply), and refactor (close loopholes).\n\n**Core principle:** If you didn't watch an agent fail without the skill, you don't know if the skill teaches the right thing.\n\n**REQUIRED BACKGROUND:** You MUST understand superpowers:test-driven-development before using this skill. That skill defines the fundamental RED-GREEN-REFACTOR cycle. This skill adapts TDD to documentation.\n\n**Official guidance:** For Anthropic's official skill authoring best practices, see anthropic-best-practices.md. This document provides additional patterns and guidelines that complement the TDD-focused approach in this skill.\n\n## What is a Skill?\n\nA **skill** is a reference guide for proven techniques, patterns, or tools. Skills help future Claude instances find and apply effective approaches.\n\n**Skills are:** Reusable techniques, patterns, tools, reference guides\n\n**Skills are NOT:** Narratives about how you solved a problem once\n\n## TDD Mapping for Skills\n\n| TDD Concept | Skill Creation |\n|-------------|----------------|\n| **Test case** | Pressure scenario with subagent |\n| **Production code** | Skill document (SKILL.md) |\n| **Test fails (RED)** | Agent violates rule without skill (baseline) |\n| **Test passes (GREEN)** | Agent complies with skill present |\n| **Refactor** | Close loopholes while maintaining compliance |\n| **Write test first** | Run baseline scenario BEFORE writing skill |\n| **Watch it fail** | Document exact rationalizations agent uses |\n| **Minimal code** | Write skill addressing those specific violations |\n| **Watch it pass** | Verify agent now complies |\n| **Refactor cycle** | Find new rationalizations â†’ plug â†’ re-verify |\n\nThe entire skill creation process follows RED-GREEN-REFACTOR.\n\n## When to Create a Skill\n\n**Create when:**\n- Technique wasn't intuitively obvious to you\n- You'd reference this again across projects\n- Pattern applies broadly (not project-specific)\n- Others would benefit\n\n**Don't create for:**\n- One-off solutions\n- Standard practices well-documented elsewhere\n- Project-specific conventions (put in CLAUDE.md)\n- Mechanical constraints (if it's enforceable with regex/validation, automate itâ€”save documentation for judgment calls)\n\n## Skill Types\n\n### Technique\nConcrete method with steps to follow (condition-based-waiting, root-cause-tracing)\n\n### Pattern\nWay of thinking about problems (flatten-with-flags, test-invariants)\n\n### Reference\nAPI docs, syntax guides, tool documentation (office docs)\n\n## Directory Structure\n\n\n```\nskills/\n  skill-name/\n    SKILL.md              # Main reference (required)\n    supporting-file.*     # Only if needed\n```\n\n**Flat namespace** - all skills in one searchable namespace\n\n**Separate files for:**\n1. **Heavy reference** (100+ lines) - API docs, comprehensive syntax\n2. **Reusable tools** - Scripts, utilities, templates\n\n**Keep inline:**\n- Principles and concepts\n- Code patterns (< 50 lines)\n- Everything else\n\n## SKILL.md Structure\n\n**Frontmatter (YAML):**\n- Only two fields supported: `name` and `description`\n- Max 1024 characters total\n- `name`: Use letters, numbers, and hyphens only (no parentheses, special chars)\n- `description`: Third-person, describes ONLY when to use (NOT what it does)\n  - Start with \"Use when...\" to focus on triggering conditions\n  - Include specific symptoms, situations, and contexts\n  - **NEVER summarize the skill's process or workflow** (see CSO section for why)\n  - Keep under 500 characters if possible\n\n```markdown\n---\nname: Skill-Name-With-Hyphens\ndescription: Use when [specific triggering conditions and symptoms]\n---\n\n# Skill Name\n\n## Overview\nWhat is this? Core principle in 1-2 sentences.\n\n## When to Use\n[Small inline flowchart IF decision non-obvious]\n\nBullet list with SYMPTOMS and use cases\nWhen NOT to use\n\n## Core Pattern (for techniques/patterns)\nBefore/after code comparison\n\n## Quick Reference\nTable or bullets for scanning common operations\n\n## Implementation\nInline code for simple patterns\nLink to file for heavy reference or reusable tools\n\n## Common Mistakes\nWhat goes wrong + fixes\n\n## Real-World Impact (optional)\nConcrete results\n```\n\n\n## Claude Search Optimization (CSO)\n\n**Critical for discovery:** Future Claude needs to FIND your skill\n\n### 1. Rich Description Field\n\n**Purpose:** Claude reads description to decide which skills to load for a given task. Make it answer: \"Should I read this skill right now?\"\n\n**Format:** Start with \"Use when...\" to focus on triggering conditions\n\n**CRITICAL: Description = When to Use, NOT What the Skill Does**\n\nThe description should ONLY describe triggering conditions. Do NOT summarize the skill's process or workflow in the description.\n\n**Why this matters:** Testing revealed that when a description summarizes the skill's workflow, Claude may follow the description instead of reading the full skill content. A description saying \"code review between tasks\" caused Claude to do ONE review, even though the skill's flowchart clearly showed TWO reviews (spec compliance then code quality).\n\nWhen the description was changed to just \"Use when executing implementation plans with independent tasks\" (no workflow summary), Claude correctly read the flowchart and followed the two-stage review process.\n\n**The trap:** Descriptions that summarize workflow create a shortcut Claude will take. The skill body becomes documentation Claude skips.\n\n```yaml\n# âŒ BAD: Summarizes workflow - Claude may follow this instead of reading skill\ndescription: Use when executing plans - dispatches subagent per task with code review between tasks\n\n# âŒ BAD: Too much process detail\ndescription: Use for TDD - write test first, watch it fail, write minimal code, refactor\n\n# âœ… GOOD: Just triggering conditions, no workflow summary\ndescription: Use when executing implementation plans with independent tasks in the current session\n\n# âœ… GOOD: Triggering conditions only\ndescription: Use when implementing any feature or bugfix, before writing implementation code\n```\n\n**Content:**\n- Use concrete triggers, symptoms, and situations that signal this skill applies\n- Describe the *problem* (race conditions, inconsistent behavior) not *language-specific symptoms* (setTimeout, sleep)\n- Keep triggers technology-agnostic unless the skill itself is technology-specific\n- If skill is technology-specific, make that explicit in the trigger\n- Write in third person (injected into system prompt)\n- **NEVER summarize the skill's process or workflow**\n\n```yaml\n# âŒ BAD: Too abstract, vague, doesn't include when to use\ndescription: For async testing\n\n# âŒ BAD: First person\ndescription: I can help you with async tests when they're flaky\n\n# âŒ BAD: Mentions technology but skill isn't specific to it\ndescription: Use when tests use setTimeout/sleep and are flaky\n\n# âœ… GOOD: Starts with \"Use when\", describes problem, no workflow\ndescription: Use when tests have race conditions, timing dependencies, or pass/fail inconsistently\n\n# âœ… GOOD: Technology-specific skill with explicit trigger\ndescription: Use when using React Router and handling authentication redirects\n```\n\n### 2. Keyword Coverage\n\nUse words Claude would search for:\n- Error messages: \"Hook timed out\", \"ENOTEMPTY\", \"race condition\"\n- Symptoms: \"flaky\", \"hanging\", \"zombie\", \"pollution\"\n- Synonyms: \"timeout/hang/freeze\", \"cleanup/teardown/afterEach\"\n- Tools: Actual commands, library names, file types\n\n### 3. Descriptive Naming\n\n**Use active voice, verb-first:**\n- âœ… `creating-skills` not `skill-creation`\n- âœ… `condition-based-waiting` not `async-test-helpers`\n\n### 4. Token Efficiency (Critical)\n\n**Problem:** getting-started and frequently-referenced skills load into EVERY conversation. Every token counts.\n\n**Target word counts:**\n- getting-started workflows: <150 words each\n- Frequently-loaded skills: <200 words total\n- Other skills: <500 words (still be concise)\n\n**Techniques:**\n\n**Move details to tool help:**\n```bash\n# âŒ BAD: Document all flags in SKILL.md\nsearch-conversations supports --text, --both, --after DATE, --before DATE, --limit N\n\n# âœ… GOOD: Reference --help\nsearch-conversations supports multiple modes and filters. Run --help for details.\n```\n\n**Use cross-references:**\n```markdown\n# âŒ BAD: Repeat workflow details\nWhen searching, dispatch subagent with template...\n[20 lines of repeated instructions]\n\n# âœ… GOOD: Reference other skill\nAlways use subagents (50-100x context savings). REQUIRED: Use [other-skill-name] for workflow.\n```\n\n**Compress examples:**\n```markdown\n# âŒ BAD: Verbose example (42 words)\nyour human partner: \"How did we handle authentication errors in React Router before?\"\nYou: I'll search past conversations for React Router authentication patterns.\n[Dispatch subagent with search query: \"React Router authentication error handling 401\"]\n\n# âœ… GOOD: Minimal example (20 words)\nPartner: \"How did we handle auth errors in React Router?\"\nYou: Searching...\n[Dispatch subagent â†’ synthesis]\n```\n\n**Eliminate redundancy:**\n- Don't repeat what's in cross-referenced skills\n- Don't explain what's obvious from command\n- Don't include multiple examples of same pattern\n\n**Verification:**\n```bash\nwc -w skills/path/SKILL.md\n# getting-started workflows: aim for <150 each\n# Other frequently-loaded: aim for <200 total\n```\n\n**Name by what you DO or core insight:**\n- âœ… `condition-based-waiting` > `async-test-helpers`\n- âœ… `using-skills` not `skill-usage`\n- âœ… `flatten-with-flags` > `data-structure-refactoring`\n- âœ… `root-cause-tracing` > `debugging-techniques`\n\n**Gerunds (-ing) work well for processes:**\n- `creating-skills`, `testing-skills`, `debugging-with-logs`\n- Active, describes the action you're taking\n\n### 4. Cross-Referencing Other Skills\n\n**When writing documentation that references other skills:**\n\nUse skill name only, with explicit requirement markers:\n- âœ… Good: `**REQUIRED SUB-SKILL:** Use superpowers:test-driven-development`\n- âœ… Good: `**REQUIRED BACKGROUND:** You MUST understand superpowers:systematic-debugging`\n- âŒ Bad: `See skills/testing/test-driven-development` (unclear if required)\n- âŒ Bad: `@skills/testing/test-driven-development/SKILL.md` (force-loads, burns context)\n\n**Why no @ links:** `@` syntax force-loads files immediately, consuming 200k+ context before you need them.\n\n## Flowchart Usage\n\n```dot\ndigraph when_flowchart {\n    \"Need to show information?\" [shape=diamond];\n    \"Decision where I might go wrong?\" [shape=diamond];\n    \"Use markdown\" [shape=box];\n    \"Small inline flowchart\" [shape=box];\n\n    \"Need to show information?\" -> \"Decision where I might go wrong?\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Small inline flowchart\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Use markdown\" [label=\"no\"];\n}\n```\n\n**Use flowcharts ONLY for:**\n- Non-obvious decision points\n- Process loops where you might stop too early\n- \"When to use A vs B\" decisions\n\n**Never use flowcharts for:**\n- Reference material â†’ Tables, lists\n- Code examples â†’ Markdown blocks\n- Linear instructions â†’ Numbered lists\n- Labels without semantic meaning (step1, helper2)\n\nSee @graphviz-conventions.dot for graphviz style rules.\n\n**Visualizing for your human partner:** Use `render-graphs.js` in this directory to render a skill's flowcharts to SVG:\n```bash\n./render-graphs.js ../some-skill           # Each diagram separately\n./render-graphs.js ../some-skill --combine # All diagrams in one SVG\n```\n\n## Code Examples\n\n**One excellent example beats many mediocre ones**\n\nChoose most relevant language:\n- Testing techniques â†’ TypeScript/JavaScript\n- System debugging â†’ Shell/Python\n- Data processing â†’ Python\n\n**Good example:**\n- Complete and runnable\n- Well-commented explaining WHY\n- From real scenario\n- Shows pattern clearly\n- Ready to adapt (not generic template)\n\n**Don't:**\n- Implement in 5+ languages\n- Create fill-in-the-blank templates\n- Write contrived examples\n\nYou're good at porting - one great example is enough.\n\n## File Organization\n\n### Self-Contained Skill\n```\ndefense-in-depth/\n  SKILL.md    # Everything inline\n```\nWhen: All content fits, no heavy reference needed\n\n### Skill with Reusable Tool\n```\ncondition-based-waiting/\n  SKILL.md    # Overview + patterns\n  example.ts  # Working helpers to adapt\n```\nWhen: Tool is reusable code, not just narrative\n\n### Skill with Heavy Reference\n```\npptx/\n  SKILL.md       # Overview + workflows\n  pptxgenjs.md   # 600 lines API reference\n  ooxml.md       # 500 lines XML structure\n  scripts/       # Executable tools\n```\nWhen: Reference material too large for inline\n\n## The Iron Law (Same as TDD)\n\n```\nNO SKILL WITHOUT A FAILING TEST FIRST\n```\n\nThis applies to NEW skills AND EDITS to existing skills.\n\nWrite skill before testing? Delete it. Start over.\nEdit skill without testing? Same violation.\n\n**No exceptions:**\n- Not for \"simple additions\"\n- Not for \"just adding a section\"\n- Not for \"documentation updates\"\n- Don't keep untested changes as \"reference\"\n- Don't \"adapt\" while running tests\n- Delete means delete\n\n**REQUIRED BACKGROUND:** The superpowers:test-driven-development skill explains why this matters. Same principles apply to documentation.\n\n## Testing All Skill Types\n\nDifferent skill types need different test approaches:\n\n### Discipline-Enforcing Skills (rules/requirements)\n\n**Examples:** TDD, verification-before-completion, designing-before-coding\n\n**Test with:**\n- Academic questions: Do they understand the rules?\n- Pressure scenarios: Do they comply under stress?\n- Multiple pressures combined: time + sunk cost + exhaustion\n- Identify rationalizations and add explicit counters\n\n**Success criteria:** Agent follows rule under maximum pressure\n\n### Technique Skills (how-to guides)\n\n**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming\n\n**Test with:**\n- Application scenarios: Can they apply the technique correctly?\n- Variation scenarios: Do they handle edge cases?\n- Missing information tests: Do instructions have gaps?\n\n**Success criteria:** Agent successfully applies technique to new scenario\n\n### Pattern Skills (mental models)\n\n**Examples:** reducing-complexity, information-hiding concepts\n\n**Test with:**\n- Recognition scenarios: Do they recognize when pattern applies?\n- Application scenarios: Can they use the mental model?\n- Counter-examples: Do they know when NOT to apply?\n\n**Success criteria:** Agent correctly identifies when/how to apply pattern\n\n### Reference Skills (documentation/APIs)\n\n**Examples:** API documentation, command references, library guides\n\n**Test with:**\n- Retrieval scenarios: Can they find the right information?\n- Application scenarios: Can they use what they found correctly?\n- Gap testing: Are common use cases covered?\n\n**Success criteria:** Agent finds and correctly applies reference information\n\n## Common Rationalizations for Skipping Testing\n\n| Excuse | Reality |\n|--------|---------|\n| \"Skill is obviously clear\" | Clear to you â‰  clear to other agents. Test it. |\n| \"It's just a reference\" | References can have gaps, unclear sections. Test retrieval. |\n| \"Testing is overkill\" | Untested skills have issues. Always. 15 min testing saves hours. |\n| \"I'll test if problems emerge\" | Problems = agents can't use skill. Test BEFORE deploying. |\n| \"Too tedious to test\" | Testing is less tedious than debugging bad skill in production. |\n| \"I'm confident it's good\" | Overconfidence guarantees issues. Test anyway. |\n| \"Academic review is enough\" | Reading â‰  using. Test application scenarios. |\n| \"No time to test\" | Deploying untested skill wastes more time fixing it later. |\n\n**All of these mean: Test before deploying. No exceptions.**\n\n## Bulletproofing Skills Against Rationalization\n\nSkills that enforce discipline (like TDD) need to resist rationalization. Agents are smart and will find loopholes when under pressure.\n\n**Psychology note:** Understanding WHY persuasion techniques work helps you apply them systematically. See persuasion-principles.md for research foundation (Cialdini, 2021; Meincke et al., 2025) on authority, commitment, scarcity, social proof, and unity principles.\n\n### Close Every Loophole Explicitly\n\nDon't just state the rule - forbid specific workarounds:\n\n<Bad>\n```markdown\nWrite code before test? Delete it.\n```\n</Bad>\n\n<Good>\n```markdown\nWrite code before test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n```\n</Good>\n\n### Address \"Spirit vs Letter\" Arguments\n\nAdd foundational principle early:\n\n```markdown\n**Violating the letter of the rules is violating the spirit of the rules.**\n```\n\nThis cuts off entire class of \"I'm following the spirit\" rationalizations.\n\n### Build Rationalization Table\n\nCapture rationalizations from baseline testing (see Testing section below). Every excuse agents make goes in the table:\n\n```markdown\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n```\n\n### Create Red Flags List\n\nMake it easy for agents to self-check when rationalizing:\n\n```markdown\n## Red Flags - STOP and Start Over\n\n- Code before test\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n```\n\n### Update CSO for Violation Symptoms\n\nAdd to description: symptoms of when you're ABOUT to violate the rule:\n\n```yaml\ndescription: use when implementing any feature or bugfix, before writing implementation code\n```\n\n## RED-GREEN-REFACTOR for Skills\n\nFollow the TDD cycle:\n\n### RED: Write Failing Test (Baseline)\n\nRun pressure scenario with subagent WITHOUT the skill. Document exact behavior:\n- What choices did they make?\n- What rationalizations did they use (verbatim)?\n- Which pressures triggered violations?\n\nThis is \"watch the test fail\" - you must see what agents naturally do before writing the skill.\n\n### GREEN: Write Minimal Skill\n\nWrite skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases.\n\nRun same scenarios WITH skill. Agent should now comply.\n\n### REFACTOR: Close Loopholes\n\nAgent found new rationalization? Add explicit counter. Re-test until bulletproof.\n\n**Testing methodology:** See @testing-skills-with-subagents.md for the complete testing methodology:\n- How to write pressure scenarios\n- Pressure types (time, sunk cost, authority, exhaustion)\n- Plugging holes systematically\n- Meta-testing techniques\n\n## Anti-Patterns\n\n### âŒ Narrative Example\n\"In session 2025-10-03, we found empty projectDir caused...\"\n**Why bad:** Too specific, not reusable\n\n### âŒ Multi-Language Dilution\nexample-js.js, example-py.py, example-go.go\n**Why bad:** Mediocre quality, maintenance burden\n\n### âŒ Code in Flowcharts\n```dot\nstep1 [label=\"import fs\"];\nstep2 [label=\"read file\"];\n```\n**Why bad:** Can't copy-paste, hard to read\n\n### âŒ Generic Labels\nhelper1, helper2, step3, pattern4\n**Why bad:** Labels should have semantic meaning\n\n## STOP: Before Moving to Next Skill\n\n**After writing ANY skill, you MUST STOP and complete the deployment process.**\n\n**Do NOT:**\n- Create multiple skills in batch without testing each\n- Move to next skill before current one is verified\n- Skip testing because \"batching is more efficient\"\n\n**The deployment checklist below is MANDATORY for EACH skill.**\n\nDeploying untested skills = deploying untested code. It's a violation of quality standards.\n\n## Skill Creation Checklist (TDD Adapted)\n\n**IMPORTANT: Use TodoWrite to create todos for EACH checklist item below.**\n\n**RED Phase - Write Failing Test:**\n- [ ] Create pressure scenarios (3+ combined pressures for discipline skills)\n- [ ] Run scenarios WITHOUT skill - document baseline behavior verbatim\n- [ ] Identify patterns in rationalizations/failures\n\n**GREEN Phase - Write Minimal Skill:**\n- [ ] Name uses only letters, numbers, hyphens (no parentheses/special chars)\n- [ ] YAML frontmatter with only name and description (max 1024 chars)\n- [ ] Description starts with \"Use when...\" and includes specific triggers/symptoms\n- [ ] Description written in third person\n- [ ] Keywords throughout for search (errors, symptoms, tools)\n- [ ] Clear overview with core principle\n- [ ] Address specific baseline failures identified in RED\n- [ ] Code inline OR link to separate file\n- [ ] One excellent example (not multi-language)\n- [ ] Run scenarios WITH skill - verify agents now comply\n\n**REFACTOR Phase - Close Loopholes:**\n- [ ] Identify NEW rationalizations from testing\n- [ ] Add explicit counters (if discipline skill)\n- [ ] Build rationalization table from all test iterations\n- [ ] Create red flags list\n- [ ] Re-test until bulletproof\n\n**Quality Checks:**\n- [ ] Small flowchart only if decision non-obvious\n- [ ] Quick reference table\n- [ ] Common mistakes section\n- [ ] No narrative storytelling\n- [ ] Supporting files only for tools or heavy reference\n\n**Deployment:**\n- [ ] Commit skill to git and push to your fork (if configured)\n- [ ] Consider contributing back via PR (if broadly useful)\n\n## Discovery Workflow\n\nHow future Claude finds your skill:\n\n1. **Encounters problem** (\"tests are flaky\")\n3. **Finds SKILL** (description matches)\n4. **Scans overview** (is this relevant?)\n5. **Reads patterns** (quick reference table)\n6. **Loads example** (only when implementing)\n\n**Optimize for this flow** - put searchable terms early and often.\n\n## The Bottom Line\n\n**Creating skills IS TDD for process documentation.**\n\nSame Iron Law: No skill without failing test first.\nSame cycle: RED (baseline) â†’ GREEN (write skill) â†’ REFACTOR (close loopholes).\nSame benefits: Better quality, fewer surprises, bulletproof results.\n\nIf you follow TDD for code, follow it for skills. It's the same discipline applied to documentation."
              }
            ]
          },
          {
            "name": "agent-orchestration",
            "description": "Context management and multi-agent orchestration",
            "source": "./plugins/agent-orchestration",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install agent-orchestration@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/improve-agent",
                "description": null,
                "path": "plugins/agent-orchestration/commands/improve-agent.md",
                "frontmatter": null,
                "content": "# Agent Performance Optimization Workflow\n\nSystematic improvement of existing agents through performance analysis, prompt engineering, and continuous iteration.\n\n[Extended thinking: Agent optimization requires a data-driven approach combining performance metrics, user feedback analysis, and advanced prompt engineering techniques. Success depends on systematic evaluation, targeted improvements, and rigorous testing with rollback capabilities for production safety.]\n\n## Phase 1: Performance Analysis and Baseline Metrics\n\nComprehensive analysis of agent performance using context-manager for historical data collection.\n\n### 1.1 Gather Performance Data\n```\nUse: context-manager\nCommand: analyze-agent-performance $ARGUMENTS --days 30\n```\n\nCollect metrics including:\n- Task completion rate (successful vs failed tasks)\n- Response accuracy and factual correctness\n- Tool usage efficiency (correct tools, call frequency)\n- Average response time and token consumption\n- User satisfaction indicators (corrections, retries)\n- Hallucination incidents and error patterns\n\n### 1.2 User Feedback Pattern Analysis\n\nIdentify recurring patterns in user interactions:\n- **Correction patterns**: Where users consistently modify outputs\n- **Clarification requests**: Common areas of ambiguity\n- **Task abandonment**: Points where users give up\n- **Follow-up questions**: Indicators of incomplete responses\n- **Positive feedback**: Successful patterns to preserve\n\n### 1.3 Failure Mode Classification\n\nCategorize failures by root cause:\n- **Instruction misunderstanding**: Role or task confusion\n- **Output format errors**: Structure or formatting issues\n- **Context loss**: Long conversation degradation\n- **Tool misuse**: Incorrect or inefficient tool selection\n- **Constraint violations**: Safety or business rule breaches\n- **Edge case handling**: Unusual input scenarios\n\n### 1.4 Baseline Performance Report\n\nGenerate quantitative baseline metrics:\n```\nPerformance Baseline:\n- Task Success Rate: [X%]\n- Average Corrections per Task: [Y]\n- Tool Call Efficiency: [Z%]\n- User Satisfaction Score: [1-10]\n- Average Response Latency: [Xms]\n- Token Efficiency Ratio: [X:Y]\n```\n\n## Phase 2: Prompt Engineering Improvements\n\nApply advanced prompt optimization techniques using prompt-engineer agent.\n\n### 2.1 Chain-of-Thought Enhancement\n\nImplement structured reasoning patterns:\n```\nUse: prompt-engineer\nTechnique: chain-of-thought-optimization\n```\n\n- Add explicit reasoning steps: \"Let's approach this step-by-step...\"\n- Include self-verification checkpoints: \"Before proceeding, verify that...\"\n- Implement recursive decomposition for complex tasks\n- Add reasoning trace visibility for debugging\n\n### 2.2 Few-Shot Example Optimization\n\nCurate high-quality examples from successful interactions:\n- **Select diverse examples** covering common use cases\n- **Include edge cases** that previously failed\n- **Show both positive and negative examples** with explanations\n- **Order examples** from simple to complex\n- **Annotate examples** with key decision points\n\nExample structure:\n```\nGood Example:\nInput: [User request]\nReasoning: [Step-by-step thought process]\nOutput: [Successful response]\nWhy this works: [Key success factors]\n\nBad Example:\nInput: [Similar request]\nOutput: [Failed response]\nWhy this fails: [Specific issues]\nCorrect approach: [Fixed version]\n```\n\n### 2.3 Role Definition Refinement\n\nStrengthen agent identity and capabilities:\n- **Core purpose**: Clear, single-sentence mission\n- **Expertise domains**: Specific knowledge areas\n- **Behavioral traits**: Personality and interaction style\n- **Tool proficiency**: Available tools and when to use them\n- **Constraints**: What the agent should NOT do\n- **Success criteria**: How to measure task completion\n\n### 2.4 Constitutional AI Integration\n\nImplement self-correction mechanisms:\n```\nConstitutional Principles:\n1. Verify factual accuracy before responding\n2. Self-check for potential biases or harmful content\n3. Validate output format matches requirements\n4. Ensure response completeness\n5. Maintain consistency with previous responses\n```\n\nAdd critique-and-revise loops:\n- Initial response generation\n- Self-critique against principles\n- Automatic revision if issues detected\n- Final validation before output\n\n### 2.5 Output Format Tuning\n\nOptimize response structure:\n- **Structured templates** for common tasks\n- **Dynamic formatting** based on complexity\n- **Progressive disclosure** for detailed information\n- **Markdown optimization** for readability\n- **Code block formatting** with syntax highlighting\n- **Table and list generation** for data presentation\n\n## Phase 3: Testing and Validation\n\nComprehensive testing framework with A/B comparison.\n\n### 3.1 Test Suite Development\n\nCreate representative test scenarios:\n```\nTest Categories:\n1. Golden path scenarios (common successful cases)\n2. Previously failed tasks (regression testing)\n3. Edge cases and corner scenarios\n4. Stress tests (complex, multi-step tasks)\n5. Adversarial inputs (potential breaking points)\n6. Cross-domain tasks (combining capabilities)\n```\n\n### 3.2 A/B Testing Framework\n\nCompare original vs improved agent:\n```\nUse: parallel-test-runner\nConfig:\n  - Agent A: Original version\n  - Agent B: Improved version\n  - Test set: 100 representative tasks\n  - Metrics: Success rate, speed, token usage\n  - Evaluation: Blind human review + automated scoring\n```\n\nStatistical significance testing:\n- Minimum sample size: 100 tasks per variant\n- Confidence level: 95% (p < 0.05)\n- Effect size calculation (Cohen's d)\n- Power analysis for future tests\n\n### 3.3 Evaluation Metrics\n\nComprehensive scoring framework:\n\n**Task-Level Metrics:**\n- Completion rate (binary success/failure)\n- Correctness score (0-100% accuracy)\n- Efficiency score (steps taken vs optimal)\n- Tool usage appropriateness\n- Response relevance and completeness\n\n**Quality Metrics:**\n- Hallucination rate (factual errors per response)\n- Consistency score (alignment with previous responses)\n- Format compliance (matches specified structure)\n- Safety score (constraint adherence)\n- User satisfaction prediction\n\n**Performance Metrics:**\n- Response latency (time to first token)\n- Total generation time\n- Token consumption (input + output)\n- Cost per task (API usage fees)\n- Memory/context efficiency\n\n### 3.4 Human Evaluation Protocol\n\nStructured human review process:\n- Blind evaluation (evaluators don't know version)\n- Standardized rubric with clear criteria\n- Multiple evaluators per sample (inter-rater reliability)\n- Qualitative feedback collection\n- Preference ranking (A vs B comparison)\n\n## Phase 4: Version Control and Deployment\n\nSafe rollout with monitoring and rollback capabilities.\n\n### 4.1 Version Management\n\nSystematic versioning strategy:\n```\nVersion Format: agent-name-v[MAJOR].[MINOR].[PATCH]\nExample: customer-support-v2.3.1\n\nMAJOR: Significant capability changes\nMINOR: Prompt improvements, new examples\nPATCH: Bug fixes, minor adjustments\n```\n\nMaintain version history:\n- Git-based prompt storage\n- Changelog with improvement details\n- Performance metrics per version\n- Rollback procedures documented\n\n### 4.2 Staged Rollout\n\nProgressive deployment strategy:\n1. **Alpha testing**: Internal team validation (5% traffic)\n2. **Beta testing**: Selected users (20% traffic)\n3. **Canary release**: Gradual increase (20% â†’ 50% â†’ 100%)\n4. **Full deployment**: After success criteria met\n5. **Monitoring period**: 7-day observation window\n\n### 4.3 Rollback Procedures\n\nQuick recovery mechanism:\n```\nRollback Triggers:\n- Success rate drops >10% from baseline\n- Critical errors increase >5%\n- User complaints spike\n- Cost per task increases >20%\n- Safety violations detected\n\nRollback Process:\n1. Detect issue via monitoring\n2. Alert team immediately\n3. Switch to previous stable version\n4. Analyze root cause\n5. Fix and re-test before retry\n```\n\n### 4.4 Continuous Monitoring\n\nReal-time performance tracking:\n- Dashboard with key metrics\n- Anomaly detection alerts\n- User feedback collection\n- Automated regression testing\n- Weekly performance reports\n\n## Success Criteria\n\nAgent improvement is successful when:\n- Task success rate improves by â‰¥15%\n- User corrections decrease by â‰¥25%\n- No increase in safety violations\n- Response time remains within 10% of baseline\n- Cost per task doesn't increase >5%\n- Positive user feedback increases\n\n## Post-Deployment Review\n\nAfter 30 days of production use:\n1. Analyze accumulated performance data\n2. Compare against baseline and targets\n3. Identify new improvement opportunities\n4. Document lessons learned\n5. Plan next optimization cycle\n\n## Continuous Improvement Cycle\n\nEstablish regular improvement cadence:\n- **Weekly**: Monitor metrics and collect feedback\n- **Monthly**: Analyze patterns and plan improvements\n- **Quarterly**: Major version updates with new capabilities\n- **Annually**: Strategic review and architecture updates\n\nRemember: Agent optimization is an iterative process. Each cycle builds upon previous learnings, gradually improving performance while maintaining stability and safety."
              },
              {
                "name": "/multi-agent-optimize",
                "description": null,
                "path": "plugins/agent-orchestration/commands/multi-agent-optimize.md",
                "frontmatter": null,
                "content": "# Multi-Agent Optimization Toolkit\n\n## Role: AI-Powered Multi-Agent Performance Engineering Specialist\n\n### Context\nThe Multi-Agent Optimization Tool is an advanced AI-driven framework designed to holistically improve system performance through intelligent, coordinated agent-based optimization. Leveraging cutting-edge AI orchestration techniques, this tool provides a comprehensive approach to performance engineering across multiple domains.\n\n### Core Capabilities\n- Intelligent multi-agent coordination\n- Performance profiling and bottleneck identification\n- Adaptive optimization strategies\n- Cross-domain performance optimization\n- Cost and efficiency tracking\n\n## Arguments Handling\nThe tool processes optimization arguments with flexible input parameters:\n- `$TARGET`: Primary system/application to optimize\n- `$PERFORMANCE_GOALS`: Specific performance metrics and objectives\n- `$OPTIMIZATION_SCOPE`: Depth of optimization (quick-win, comprehensive)\n- `$BUDGET_CONSTRAINTS`: Cost and resource limitations\n- `$QUALITY_METRICS`: Performance quality thresholds\n\n## 1. Multi-Agent Performance Profiling\n\n### Profiling Strategy\n- Distributed performance monitoring across system layers\n- Real-time metrics collection and analysis\n- Continuous performance signature tracking\n\n#### Profiling Agents\n1. **Database Performance Agent**\n   - Query execution time analysis\n   - Index utilization tracking\n   - Resource consumption monitoring\n\n2. **Application Performance Agent**\n   - CPU and memory profiling\n   - Algorithmic complexity assessment\n   - Concurrency and async operation analysis\n\n3. **Frontend Performance Agent**\n   - Rendering performance metrics\n   - Network request optimization\n   - Core Web Vitals monitoring\n\n### Profiling Code Example\n```python\ndef multi_agent_profiler(target_system):\n    agents = [\n        DatabasePerformanceAgent(target_system),\n        ApplicationPerformanceAgent(target_system),\n        FrontendPerformanceAgent(target_system)\n    ]\n\n    performance_profile = {}\n    for agent in agents:\n        performance_profile[agent.__class__.__name__] = agent.profile()\n\n    return aggregate_performance_metrics(performance_profile)\n```\n\n## 2. Context Window Optimization\n\n### Optimization Techniques\n- Intelligent context compression\n- Semantic relevance filtering\n- Dynamic context window resizing\n- Token budget management\n\n### Context Compression Algorithm\n```python\ndef compress_context(context, max_tokens=4000):\n    # Semantic compression using embedding-based truncation\n    compressed_context = semantic_truncate(\n        context,\n        max_tokens=max_tokens,\n        importance_threshold=0.7\n    )\n    return compressed_context\n```\n\n## 3. Agent Coordination Efficiency\n\n### Coordination Principles\n- Parallel execution design\n- Minimal inter-agent communication overhead\n- Dynamic workload distribution\n- Fault-tolerant agent interactions\n\n### Orchestration Framework\n```python\nclass MultiAgentOrchestrator:\n    def __init__(self, agents):\n        self.agents = agents\n        self.execution_queue = PriorityQueue()\n        self.performance_tracker = PerformanceTracker()\n\n    def optimize(self, target_system):\n        # Parallel agent execution with coordinated optimization\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(agent.optimize, target_system): agent\n                for agent in self.agents\n            }\n\n            for future in concurrent.futures.as_completed(futures):\n                agent = futures[future]\n                result = future.result()\n                self.performance_tracker.log(agent, result)\n```\n\n## 4. Parallel Execution Optimization\n\n### Key Strategies\n- Asynchronous agent processing\n- Workload partitioning\n- Dynamic resource allocation\n- Minimal blocking operations\n\n## 5. Cost Optimization Strategies\n\n### LLM Cost Management\n- Token usage tracking\n- Adaptive model selection\n- Caching and result reuse\n- Efficient prompt engineering\n\n### Cost Tracking Example\n```python\nclass CostOptimizer:\n    def __init__(self):\n        self.token_budget = 100000  # Monthly budget\n        self.token_usage = 0\n        self.model_costs = {\n            'gpt-5': 0.03,\n            'claude-4-sonnet': 0.015,\n            'claude-4-haiku': 0.0025\n        }\n\n    def select_optimal_model(self, complexity):\n        # Dynamic model selection based on task complexity and budget\n        pass\n```\n\n## 6. Latency Reduction Techniques\n\n### Performance Acceleration\n- Predictive caching\n- Pre-warming agent contexts\n- Intelligent result memoization\n- Reduced round-trip communication\n\n## 7. Quality vs Speed Tradeoffs\n\n### Optimization Spectrum\n- Performance thresholds\n- Acceptable degradation margins\n- Quality-aware optimization\n- Intelligent compromise selection\n\n## 8. Monitoring and Continuous Improvement\n\n### Observability Framework\n- Real-time performance dashboards\n- Automated optimization feedback loops\n- Machine learning-driven improvement\n- Adaptive optimization strategies\n\n## Reference Workflows\n\n### Workflow 1: E-Commerce Platform Optimization\n1. Initial performance profiling\n2. Agent-based optimization\n3. Cost and performance tracking\n4. Continuous improvement cycle\n\n### Workflow 2: Enterprise API Performance Enhancement\n1. Comprehensive system analysis\n2. Multi-layered agent optimization\n3. Iterative performance refinement\n4. Cost-efficient scaling strategy\n\n## Key Considerations\n- Always measure before and after optimization\n- Maintain system stability during optimization\n- Balance performance gains with resource consumption\n- Implement gradual, reversible changes\n\nTarget Optimization: $ARGUMENTS"
              }
            ],
            "skills": []
          },
          {
            "name": "blockchain-web3",
            "description": "Blockchain development with Solidity security, DeFi, NFTs",
            "source": "./plugins/blockchain-web3",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install blockchain-web3@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "defi-protocol-templates",
                "description": "Implement DeFi protocols with production-ready templates for staking, AMMs, governance, and lending systems. Use when building decentralized finance applications or smart contract protocols.",
                "path": "plugins/blockchain-web3/skills/defi-protocol-templates/SKILL.md",
                "frontmatter": {
                  "name": "defi-protocol-templates",
                  "description": "Implement DeFi protocols with production-ready templates for staking, AMMs, governance, and lending systems. Use when building decentralized finance applications or smart contract protocols."
                },
                "content": "# DeFi Protocol Templates\n\nProduction-ready templates for common DeFi protocols including staking, AMMs, governance, lending, and flash loans.\n\n## When to Use This Skill\n\n- Building staking platforms with reward distribution\n- Implementing AMM (Automated Market Maker) protocols\n- Creating governance token systems\n- Developing lending/borrowing protocols\n- Integrating flash loan functionality\n- Launching yield farming platforms\n\n## Staking Contract\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\nimport \"@openzeppelin/contracts/security/ReentrancyGuard.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract StakingRewards is ReentrancyGuard, Ownable {\n    IERC20 public stakingToken;\n    IERC20 public rewardsToken;\n\n    uint256 public rewardRate = 100; // Rewards per second\n    uint256 public lastUpdateTime;\n    uint256 public rewardPerTokenStored;\n\n    mapping(address => uint256) public userRewardPerTokenPaid;\n    mapping(address => uint256) public rewards;\n    mapping(address => uint256) public balances;\n\n    uint256 private _totalSupply;\n\n    event Staked(address indexed user, uint256 amount);\n    event Withdrawn(address indexed user, uint256 amount);\n    event RewardPaid(address indexed user, uint256 reward);\n\n    constructor(address _stakingToken, address _rewardsToken) {\n        stakingToken = IERC20(_stakingToken);\n        rewardsToken = IERC20(_rewardsToken);\n    }\n\n    modifier updateReward(address account) {\n        rewardPerTokenStored = rewardPerToken();\n        lastUpdateTime = block.timestamp;\n\n        if (account != address(0)) {\n            rewards[account] = earned(account);\n            userRewardPerTokenPaid[account] = rewardPerTokenStored;\n        }\n        _;\n    }\n\n    function rewardPerToken() public view returns (uint256) {\n        if (_totalSupply == 0) {\n            return rewardPerTokenStored;\n        }\n        return rewardPerTokenStored +\n            ((block.timestamp - lastUpdateTime) * rewardRate * 1e18) / _totalSupply;\n    }\n\n    function earned(address account) public view returns (uint256) {\n        return (balances[account] *\n            (rewardPerToken() - userRewardPerTokenPaid[account])) / 1e18 +\n            rewards[account];\n    }\n\n    function stake(uint256 amount) external nonReentrant updateReward(msg.sender) {\n        require(amount > 0, \"Cannot stake 0\");\n        _totalSupply += amount;\n        balances[msg.sender] += amount;\n        stakingToken.transferFrom(msg.sender, address(this), amount);\n        emit Staked(msg.sender, amount);\n    }\n\n    function withdraw(uint256 amount) public nonReentrant updateReward(msg.sender) {\n        require(amount > 0, \"Cannot withdraw 0\");\n        _totalSupply -= amount;\n        balances[msg.sender] -= amount;\n        stakingToken.transfer(msg.sender, amount);\n        emit Withdrawn(msg.sender, amount);\n    }\n\n    function getReward() public nonReentrant updateReward(msg.sender) {\n        uint256 reward = rewards[msg.sender];\n        if (reward > 0) {\n            rewards[msg.sender] = 0;\n            rewardsToken.transfer(msg.sender, reward);\n            emit RewardPaid(msg.sender, reward);\n        }\n    }\n\n    function exit() external {\n        withdraw(balances[msg.sender]);\n        getReward();\n    }\n}\n```\n\n## AMM (Automated Market Maker)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\n\ncontract SimpleAMM {\n    IERC20 public token0;\n    IERC20 public token1;\n\n    uint256 public reserve0;\n    uint256 public reserve1;\n\n    uint256 public totalSupply;\n    mapping(address => uint256) public balanceOf;\n\n    event Mint(address indexed to, uint256 amount);\n    event Burn(address indexed from, uint256 amount);\n    event Swap(address indexed trader, uint256 amount0In, uint256 amount1In, uint256 amount0Out, uint256 amount1Out);\n\n    constructor(address _token0, address _token1) {\n        token0 = IERC20(_token0);\n        token1 = IERC20(_token1);\n    }\n\n    function addLiquidity(uint256 amount0, uint256 amount1) external returns (uint256 shares) {\n        token0.transferFrom(msg.sender, address(this), amount0);\n        token1.transferFrom(msg.sender, address(this), amount1);\n\n        if (totalSupply == 0) {\n            shares = sqrt(amount0 * amount1);\n        } else {\n            shares = min(\n                (amount0 * totalSupply) / reserve0,\n                (amount1 * totalSupply) / reserve1\n            );\n        }\n\n        require(shares > 0, \"Shares = 0\");\n        _mint(msg.sender, shares);\n        _update(\n            token0.balanceOf(address(this)),\n            token1.balanceOf(address(this))\n        );\n\n        emit Mint(msg.sender, shares);\n    }\n\n    function removeLiquidity(uint256 shares) external returns (uint256 amount0, uint256 amount1) {\n        uint256 bal0 = token0.balanceOf(address(this));\n        uint256 bal1 = token1.balanceOf(address(this));\n\n        amount0 = (shares * bal0) / totalSupply;\n        amount1 = (shares * bal1) / totalSupply;\n\n        require(amount0 > 0 && amount1 > 0, \"Amount0 or amount1 = 0\");\n\n        _burn(msg.sender, shares);\n        _update(bal0 - amount0, bal1 - amount1);\n\n        token0.transfer(msg.sender, amount0);\n        token1.transfer(msg.sender, amount1);\n\n        emit Burn(msg.sender, shares);\n    }\n\n    function swap(address tokenIn, uint256 amountIn) external returns (uint256 amountOut) {\n        require(tokenIn == address(token0) || tokenIn == address(token1), \"Invalid token\");\n\n        bool isToken0 = tokenIn == address(token0);\n        (IERC20 tokenIn_, IERC20 tokenOut, uint256 resIn, uint256 resOut) = isToken0\n            ? (token0, token1, reserve0, reserve1)\n            : (token1, token0, reserve1, reserve0);\n\n        tokenIn_.transferFrom(msg.sender, address(this), amountIn);\n\n        // 0.3% fee\n        uint256 amountInWithFee = (amountIn * 997) / 1000;\n        amountOut = (resOut * amountInWithFee) / (resIn + amountInWithFee);\n\n        tokenOut.transfer(msg.sender, amountOut);\n\n        _update(\n            token0.balanceOf(address(this)),\n            token1.balanceOf(address(this))\n        );\n\n        emit Swap(msg.sender, isToken0 ? amountIn : 0, isToken0 ? 0 : amountIn, isToken0 ? 0 : amountOut, isToken0 ? amountOut : 0);\n    }\n\n    function _mint(address to, uint256 amount) private {\n        balanceOf[to] += amount;\n        totalSupply += amount;\n    }\n\n    function _burn(address from, uint256 amount) private {\n        balanceOf[from] -= amount;\n        totalSupply -= amount;\n    }\n\n    function _update(uint256 res0, uint256 res1) private {\n        reserve0 = res0;\n        reserve1 = res1;\n    }\n\n    function sqrt(uint256 y) private pure returns (uint256 z) {\n        if (y > 3) {\n            z = y;\n            uint256 x = y / 2 + 1;\n            while (x < z) {\n                z = x;\n                x = (y / x + x) / 2;\n            }\n        } else if (y != 0) {\n            z = 1;\n        }\n    }\n\n    function min(uint256 x, uint256 y) private pure returns (uint256) {\n        return x <= y ? x : y;\n    }\n}\n```\n\n## Governance Token\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC20/extensions/ERC20Votes.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract GovernanceToken is ERC20Votes, Ownable {\n    constructor() ERC20(\"Governance Token\", \"GOV\") ERC20Permit(\"Governance Token\") {\n        _mint(msg.sender, 1000000 * 10**decimals());\n    }\n\n    function _afterTokenTransfer(\n        address from,\n        address to,\n        uint256 amount\n    ) internal override(ERC20Votes) {\n        super._afterTokenTransfer(from, to, amount);\n    }\n\n    function _mint(address to, uint256 amount) internal override(ERC20Votes) {\n        super._mint(to, amount);\n    }\n\n    function _burn(address account, uint256 amount) internal override(ERC20Votes) {\n        super._burn(account, amount);\n    }\n}\n\ncontract Governor is Ownable {\n    GovernanceToken public governanceToken;\n\n    struct Proposal {\n        uint256 id;\n        address proposer;\n        string description;\n        uint256 forVotes;\n        uint256 againstVotes;\n        uint256 startBlock;\n        uint256 endBlock;\n        bool executed;\n        mapping(address => bool) hasVoted;\n    }\n\n    uint256 public proposalCount;\n    mapping(uint256 => Proposal) public proposals;\n\n    uint256 public votingPeriod = 17280; // ~3 days in blocks\n    uint256 public proposalThreshold = 100000 * 10**18;\n\n    event ProposalCreated(uint256 indexed proposalId, address proposer, string description);\n    event VoteCast(address indexed voter, uint256 indexed proposalId, bool support, uint256 weight);\n    event ProposalExecuted(uint256 indexed proposalId);\n\n    constructor(address _governanceToken) {\n        governanceToken = GovernanceToken(_governanceToken);\n    }\n\n    function propose(string memory description) external returns (uint256) {\n        require(\n            governanceToken.getPastVotes(msg.sender, block.number - 1) >= proposalThreshold,\n            \"Proposer votes below threshold\"\n        );\n\n        proposalCount++;\n        Proposal storage newProposal = proposals[proposalCount];\n        newProposal.id = proposalCount;\n        newProposal.proposer = msg.sender;\n        newProposal.description = description;\n        newProposal.startBlock = block.number;\n        newProposal.endBlock = block.number + votingPeriod;\n\n        emit ProposalCreated(proposalCount, msg.sender, description);\n        return proposalCount;\n    }\n\n    function vote(uint256 proposalId, bool support) external {\n        Proposal storage proposal = proposals[proposalId];\n        require(block.number >= proposal.startBlock, \"Voting not started\");\n        require(block.number <= proposal.endBlock, \"Voting ended\");\n        require(!proposal.hasVoted[msg.sender], \"Already voted\");\n\n        uint256 weight = governanceToken.getPastVotes(msg.sender, proposal.startBlock);\n        require(weight > 0, \"No voting power\");\n\n        proposal.hasVoted[msg.sender] = true;\n\n        if (support) {\n            proposal.forVotes += weight;\n        } else {\n            proposal.againstVotes += weight;\n        }\n\n        emit VoteCast(msg.sender, proposalId, support, weight);\n    }\n\n    function execute(uint256 proposalId) external {\n        Proposal storage proposal = proposals[proposalId];\n        require(block.number > proposal.endBlock, \"Voting not ended\");\n        require(!proposal.executed, \"Already executed\");\n        require(proposal.forVotes > proposal.againstVotes, \"Proposal failed\");\n\n        proposal.executed = true;\n\n        // Execute proposal logic here\n\n        emit ProposalExecuted(proposalId);\n    }\n}\n```\n\n## Flash Loan\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\n\ninterface IFlashLoanReceiver {\n    function executeOperation(\n        address asset,\n        uint256 amount,\n        uint256 fee,\n        bytes calldata params\n    ) external returns (bool);\n}\n\ncontract FlashLoanProvider {\n    IERC20 public token;\n    uint256 public feePercentage = 9; // 0.09% fee\n\n    event FlashLoan(address indexed borrower, uint256 amount, uint256 fee);\n\n    constructor(address _token) {\n        token = IERC20(_token);\n    }\n\n    function flashLoan(\n        address receiver,\n        uint256 amount,\n        bytes calldata params\n    ) external {\n        uint256 balanceBefore = token.balanceOf(address(this));\n        require(balanceBefore >= amount, \"Insufficient liquidity\");\n\n        uint256 fee = (amount * feePercentage) / 10000;\n\n        // Send tokens to receiver\n        token.transfer(receiver, amount);\n\n        // Execute callback\n        require(\n            IFlashLoanReceiver(receiver).executeOperation(\n                address(token),\n                amount,\n                fee,\n                params\n            ),\n            \"Flash loan failed\"\n        );\n\n        // Verify repayment\n        uint256 balanceAfter = token.balanceOf(address(this));\n        require(balanceAfter >= balanceBefore + fee, \"Flash loan not repaid\");\n\n        emit FlashLoan(receiver, amount, fee);\n    }\n}\n\n// Example flash loan receiver\ncontract FlashLoanReceiver is IFlashLoanReceiver {\n    function executeOperation(\n        address asset,\n        uint256 amount,\n        uint256 fee,\n        bytes calldata params\n    ) external override returns (bool) {\n        // Decode params and execute arbitrage, liquidation, etc.\n        // ...\n\n        // Approve repayment\n        IERC20(asset).approve(msg.sender, amount + fee);\n\n        return true;\n    }\n}\n```\n\n## Resources\n\n- **references/staking.md**: Staking mechanics and reward distribution\n- **references/liquidity-pools.md**: AMM mathematics and pricing\n- **references/governance-tokens.md**: Governance and voting systems\n- **references/lending-protocols.md**: Lending/borrowing implementation\n- **references/flash-loans.md**: Flash loan security and use cases\n- **assets/staking-contract.sol**: Production staking template\n- **assets/amm-contract.sol**: Full AMM implementation\n- **assets/governance-token.sol**: Governance system\n- **assets/lending-protocol.sol**: Lending platform template\n\n## Best Practices\n\n1. **Use Established Libraries**: OpenZeppelin, Solmate\n2. **Test Thoroughly**: Unit tests, integration tests, fuzzing\n3. **Audit Before Launch**: Professional security audits\n4. **Start Simple**: MVP first, add features incrementally\n5. **Monitor**: Track contract health and user activity\n6. **Upgradability**: Consider proxy patterns for upgrades\n7. **Emergency Controls**: Pause mechanisms for critical issues\n\n## Common DeFi Patterns\n\n- **Time-Weighted Average Price (TWAP)**: Price oracle resistance\n- **Liquidity Mining**: Incentivize liquidity provision\n- **Vesting**: Lock tokens with gradual release\n- **Multisig**: Require multiple signatures for critical operations\n- **Timelocks**: Delay execution of governance decisions"
              },
              {
                "name": "nft-standards",
                "description": "Implement NFT standards (ERC-721, ERC-1155) with proper metadata handling, minting strategies, and marketplace integration. Use when creating NFT contracts, building NFT marketplaces, or implementing digital asset systems.",
                "path": "plugins/blockchain-web3/skills/nft-standards/SKILL.md",
                "frontmatter": {
                  "name": "nft-standards",
                  "description": "Implement NFT standards (ERC-721, ERC-1155) with proper metadata handling, minting strategies, and marketplace integration. Use when creating NFT contracts, building NFT marketplaces, or implementing digital asset systems."
                },
                "content": "# NFT Standards\n\nMaster ERC-721 and ERC-1155 NFT standards, metadata best practices, and advanced NFT features.\n\n## When to Use This Skill\n\n- Creating NFT collections (art, gaming, collectibles)\n- Implementing marketplace functionality\n- Building on-chain or off-chain metadata\n- Creating soulbound tokens (non-transferable)\n- Implementing royalties and revenue sharing\n- Developing dynamic/evolving NFTs\n\n## ERC-721 (Non-Fungible Token Standard)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC721/extensions/ERC721URIStorage.sol\";\nimport \"@openzeppelin/contracts/token/ERC721/extensions/ERC721Enumerable.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\nimport \"@openzeppelin/contracts/utils/Counters.sol\";\n\ncontract MyNFT is ERC721URIStorage, ERC721Enumerable, Ownable {\n    using Counters for Counters.Counter;\n    Counters.Counter private _tokenIds;\n\n    uint256 public constant MAX_SUPPLY = 10000;\n    uint256 public constant MINT_PRICE = 0.08 ether;\n    uint256 public constant MAX_PER_MINT = 20;\n\n    constructor() ERC721(\"MyNFT\", \"MNFT\") {}\n\n    function mint(uint256 quantity) external payable {\n        require(quantity > 0 && quantity <= MAX_PER_MINT, \"Invalid quantity\");\n        require(_tokenIds.current() + quantity <= MAX_SUPPLY, \"Exceeds max supply\");\n        require(msg.value >= MINT_PRICE * quantity, \"Insufficient payment\");\n\n        for (uint256 i = 0; i < quantity; i++) {\n            _tokenIds.increment();\n            uint256 newTokenId = _tokenIds.current();\n            _safeMint(msg.sender, newTokenId);\n            _setTokenURI(newTokenId, generateTokenURI(newTokenId));\n        }\n    }\n\n    function generateTokenURI(uint256 tokenId) internal pure returns (string memory) {\n        // Return IPFS URI or on-chain metadata\n        return string(abi.encodePacked(\"ipfs://QmHash/\", Strings.toString(tokenId), \".json\"));\n    }\n\n    // Required overrides\n    function _beforeTokenTransfer(\n        address from,\n        address to,\n        uint256 tokenId,\n        uint256 batchSize\n    ) internal override(ERC721, ERC721Enumerable) {\n        super._beforeTokenTransfer(from, to, tokenId, batchSize);\n    }\n\n    function _burn(uint256 tokenId) internal override(ERC721, ERC721URIStorage) {\n        super._burn(tokenId);\n    }\n\n    function tokenURI(uint256 tokenId) public view override(ERC721, ERC721URIStorage) returns (string memory) {\n        return super.tokenURI(tokenId);\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, ERC721Enumerable)\n        returns (bool)\n    {\n        return super.supportsInterface(interfaceId);\n    }\n\n    function withdraw() external onlyOwner {\n        payable(owner()).transfer(address(this).balance);\n    }\n}\n```\n\n## ERC-1155 (Multi-Token Standard)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC1155/ERC1155.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract GameItems is ERC1155, Ownable {\n    uint256 public constant SWORD = 1;\n    uint256 public constant SHIELD = 2;\n    uint256 public constant POTION = 3;\n\n    mapping(uint256 => uint256) public tokenSupply;\n    mapping(uint256 => uint256) public maxSupply;\n\n    constructor() ERC1155(\"ipfs://QmBaseHash/{id}.json\") {\n        maxSupply[SWORD] = 1000;\n        maxSupply[SHIELD] = 500;\n        maxSupply[POTION] = 10000;\n    }\n\n    function mint(\n        address to,\n        uint256 id,\n        uint256 amount\n    ) external onlyOwner {\n        require(tokenSupply[id] + amount <= maxSupply[id], \"Exceeds max supply\");\n\n        _mint(to, id, amount, \"\");\n        tokenSupply[id] += amount;\n    }\n\n    function mintBatch(\n        address to,\n        uint256[] memory ids,\n        uint256[] memory amounts\n    ) external onlyOwner {\n        for (uint256 i = 0; i < ids.length; i++) {\n            require(tokenSupply[ids[i]] + amounts[i] <= maxSupply[ids[i]], \"Exceeds max supply\");\n            tokenSupply[ids[i]] += amounts[i];\n        }\n\n        _mintBatch(to, ids, amounts, \"\");\n    }\n\n    function burn(\n        address from,\n        uint256 id,\n        uint256 amount\n    ) external {\n        require(from == msg.sender || isApprovedForAll(from, msg.sender), \"Not authorized\");\n        _burn(from, id, amount);\n        tokenSupply[id] -= amount;\n    }\n}\n```\n\n## Metadata Standards\n\n### Off-Chain Metadata (IPFS)\n```json\n{\n  \"name\": \"NFT #1\",\n  \"description\": \"Description of the NFT\",\n  \"image\": \"ipfs://QmImageHash\",\n  \"attributes\": [\n    {\n      \"trait_type\": \"Background\",\n      \"value\": \"Blue\"\n    },\n    {\n      \"trait_type\": \"Rarity\",\n      \"value\": \"Legendary\"\n    },\n    {\n      \"trait_type\": \"Power\",\n      \"value\": 95,\n      \"display_type\": \"number\",\n      \"max_value\": 100\n    }\n  ]\n}\n```\n\n### On-Chain Metadata\n```solidity\ncontract OnChainNFT is ERC721 {\n    struct Traits {\n        uint8 background;\n        uint8 body;\n        uint8 head;\n        uint8 rarity;\n    }\n\n    mapping(uint256 => Traits) public tokenTraits;\n\n    function tokenURI(uint256 tokenId) public view override returns (string memory) {\n        Traits memory traits = tokenTraits[tokenId];\n\n        string memory json = Base64.encode(\n            bytes(\n                string(\n                    abi.encodePacked(\n                        '{\"name\": \"NFT #', Strings.toString(tokenId), '\",',\n                        '\"description\": \"On-chain NFT\",',\n                        '\"image\": \"data:image/svg+xml;base64,', generateSVG(traits), '\",',\n                        '\"attributes\": [',\n                        '{\"trait_type\": \"Background\", \"value\": \"', Strings.toString(traits.background), '\"},',\n                        '{\"trait_type\": \"Rarity\", \"value\": \"', getRarityName(traits.rarity), '\"}',\n                        ']}'\n                    )\n                )\n            )\n        );\n\n        return string(abi.encodePacked(\"data:application/json;base64,\", json));\n    }\n\n    function generateSVG(Traits memory traits) internal pure returns (string memory) {\n        // Generate SVG based on traits\n        return \"...\";\n    }\n}\n```\n\n## Royalties (EIP-2981)\n\n```solidity\nimport \"@openzeppelin/contracts/interfaces/IERC2981.sol\";\n\ncontract NFTWithRoyalties is ERC721, IERC2981 {\n    address public royaltyRecipient;\n    uint96 public royaltyFee = 500; // 5%\n\n    constructor() ERC721(\"Royalty NFT\", \"RNFT\") {\n        royaltyRecipient = msg.sender;\n    }\n\n    function royaltyInfo(uint256 tokenId, uint256 salePrice)\n        external\n        view\n        override\n        returns (address receiver, uint256 royaltyAmount)\n    {\n        return (royaltyRecipient, (salePrice * royaltyFee) / 10000);\n    }\n\n    function setRoyalty(address recipient, uint96 fee) external onlyOwner {\n        require(fee <= 1000, \"Royalty fee too high\"); // Max 10%\n        royaltyRecipient = recipient;\n        royaltyFee = fee;\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, IERC165)\n        returns (bool)\n    {\n        return interfaceId == type(IERC2981).interfaceId ||\n               super.supportsInterface(interfaceId);\n    }\n}\n```\n\n## Soulbound Tokens (Non-Transferable)\n\n```solidity\ncontract SoulboundToken is ERC721 {\n    constructor() ERC721(\"Soulbound\", \"SBT\") {}\n\n    function _beforeTokenTransfer(\n        address from,\n        address to,\n        uint256 tokenId,\n        uint256 batchSize\n    ) internal virtual override {\n        require(from == address(0) || to == address(0), \"Token is soulbound\");\n        super._beforeTokenTransfer(from, to, tokenId, batchSize);\n    }\n\n    function mint(address to) external {\n        uint256 tokenId = totalSupply() + 1;\n        _safeMint(to, tokenId);\n    }\n\n    // Burn is allowed (user can destroy their SBT)\n    function burn(uint256 tokenId) external {\n        require(ownerOf(tokenId) == msg.sender, \"Not token owner\");\n        _burn(tokenId);\n    }\n}\n```\n\n## Dynamic NFTs\n\n```solidity\ncontract DynamicNFT is ERC721 {\n    struct TokenState {\n        uint256 level;\n        uint256 experience;\n        uint256 lastUpdated;\n    }\n\n    mapping(uint256 => TokenState) public tokenStates;\n\n    function gainExperience(uint256 tokenId, uint256 exp) external {\n        require(ownerOf(tokenId) == msg.sender, \"Not token owner\");\n\n        TokenState storage state = tokenStates[tokenId];\n        state.experience += exp;\n\n        // Level up logic\n        if (state.experience >= state.level * 100) {\n            state.level++;\n        }\n\n        state.lastUpdated = block.timestamp;\n    }\n\n    function tokenURI(uint256 tokenId) public view override returns (string memory) {\n        TokenState memory state = tokenStates[tokenId];\n\n        // Generate metadata based on current state\n        return generateMetadata(tokenId, state);\n    }\n\n    function generateMetadata(uint256 tokenId, TokenState memory state)\n        internal\n        pure\n        returns (string memory)\n    {\n        // Dynamic metadata generation\n        return \"\";\n    }\n}\n```\n\n## Gas-Optimized Minting (ERC721A)\n\n```solidity\nimport \"erc721a/contracts/ERC721A.sol\";\n\ncontract OptimizedNFT is ERC721A {\n    uint256 public constant MAX_SUPPLY = 10000;\n    uint256 public constant MINT_PRICE = 0.05 ether;\n\n    constructor() ERC721A(\"Optimized NFT\", \"ONFT\") {}\n\n    function mint(uint256 quantity) external payable {\n        require(_totalMinted() + quantity <= MAX_SUPPLY, \"Exceeds max supply\");\n        require(msg.value >= MINT_PRICE * quantity, \"Insufficient payment\");\n\n        _mint(msg.sender, quantity);\n    }\n\n    function _baseURI() internal pure override returns (string memory) {\n        return \"ipfs://QmBaseHash/\";\n    }\n}\n```\n\n## Resources\n\n- **references/erc721.md**: ERC-721 specification details\n- **references/erc1155.md**: ERC-1155 multi-token standard\n- **references/metadata-standards.md**: Metadata best practices\n- **references/enumeration.md**: Token enumeration patterns\n- **assets/erc721-contract.sol**: Production ERC-721 template\n- **assets/erc1155-contract.sol**: Production ERC-1155 template\n- **assets/metadata-schema.json**: Standard metadata format\n- **assets/metadata-uploader.py**: IPFS upload utility\n\n## Best Practices\n\n1. **Use OpenZeppelin**: Battle-tested implementations\n2. **Pin Metadata**: Use IPFS with pinning service\n3. **Implement Royalties**: EIP-2981 for marketplace compatibility\n4. **Gas Optimization**: Use ERC721A for batch minting\n5. **Reveal Mechanism**: Placeholder â†’ reveal pattern\n6. **Enumeration**: Support walletOfOwner for marketplaces\n7. **Whitelist**: Merkle trees for efficient whitelisting\n\n## Marketplace Integration\n\n- OpenSea: ERC-721/1155, metadata standards\n- LooksRare: Royalty enforcement\n- Rarible: Protocol fees, lazy minting\n- Blur: Gas-optimized trading"
              },
              {
                "name": "solidity-security",
                "description": "Master smart contract security best practices to prevent common vulnerabilities and implement secure Solidity patterns. Use when writing smart contracts, auditing existing contracts, or implementing security measures for blockchain applications.",
                "path": "plugins/blockchain-web3/skills/solidity-security/SKILL.md",
                "frontmatter": {
                  "name": "solidity-security",
                  "description": "Master smart contract security best practices to prevent common vulnerabilities and implement secure Solidity patterns. Use when writing smart contracts, auditing existing contracts, or implementing security measures for blockchain applications."
                },
                "content": "# Solidity Security\n\nMaster smart contract security best practices, vulnerability prevention, and secure Solidity development patterns.\n\n## When to Use This Skill\n\n- Writing secure smart contracts\n- Auditing existing contracts for vulnerabilities\n- Implementing secure DeFi protocols\n- Preventing reentrancy, overflow, and access control issues\n- Optimizing gas usage while maintaining security\n- Preparing contracts for professional audits\n- Understanding common attack vectors\n\n## Critical Vulnerabilities\n\n### 1. Reentrancy\nAttacker calls back into your contract before state is updated.\n\n**Vulnerable Code:**\n```solidity\n// VULNERABLE TO REENTRANCY\ncontract VulnerableBank {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public {\n        uint256 amount = balances[msg.sender];\n\n        // DANGER: External call before state update\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success);\n\n        balances[msg.sender] = 0;  // Too late!\n    }\n}\n```\n\n**Secure Pattern (Checks-Effects-Interactions):**\n```solidity\ncontract SecureBank {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public {\n        uint256 amount = balances[msg.sender];\n        require(amount > 0, \"Insufficient balance\");\n\n        // EFFECTS: Update state BEFORE external call\n        balances[msg.sender] = 0;\n\n        // INTERACTIONS: External call last\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n**Alternative: ReentrancyGuard**\n```solidity\nimport \"@openzeppelin/contracts/security/ReentrancyGuard.sol\";\n\ncontract SecureBank is ReentrancyGuard {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public nonReentrant {\n        uint256 amount = balances[msg.sender];\n        require(amount > 0, \"Insufficient balance\");\n\n        balances[msg.sender] = 0;\n\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n### 2. Integer Overflow/Underflow\n\n**Vulnerable Code (Solidity < 0.8.0):**\n```solidity\n// VULNERABLE\ncontract VulnerableToken {\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        // No overflow check - can wrap around\n        balances[msg.sender] -= amount;  // Can underflow!\n        balances[to] += amount;          // Can overflow!\n    }\n}\n```\n\n**Secure Pattern (Solidity >= 0.8.0):**\n```solidity\n// Solidity 0.8+ has built-in overflow/underflow checks\ncontract SecureToken {\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        // Automatically reverts on overflow/underflow\n        balances[msg.sender] -= amount;\n        balances[to] += amount;\n    }\n}\n```\n\n**For Solidity < 0.8.0, use SafeMath:**\n```solidity\nimport \"@openzeppelin/contracts/utils/math/SafeMath.sol\";\n\ncontract SecureToken {\n    using SafeMath for uint256;\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        balances[msg.sender] = balances[msg.sender].sub(amount);\n        balances[to] = balances[to].add(amount);\n    }\n}\n```\n\n### 3. Access Control\n\n**Vulnerable Code:**\n```solidity\n// VULNERABLE: Anyone can call critical functions\ncontract VulnerableContract {\n    address public owner;\n\n    function withdraw(uint256 amount) public {\n        // No access control!\n        payable(msg.sender).transfer(amount);\n    }\n}\n```\n\n**Secure Pattern:**\n```solidity\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract SecureContract is Ownable {\n    function withdraw(uint256 amount) public onlyOwner {\n        payable(owner()).transfer(amount);\n    }\n}\n\n// Or implement custom role-based access\ncontract RoleBasedContract {\n    mapping(address => bool) public admins;\n\n    modifier onlyAdmin() {\n        require(admins[msg.sender], \"Not an admin\");\n        _;\n    }\n\n    function criticalFunction() public onlyAdmin {\n        // Protected function\n    }\n}\n```\n\n### 4. Front-Running\n\n**Vulnerable:**\n```solidity\n// VULNERABLE TO FRONT-RUNNING\ncontract VulnerableDEX {\n    function swap(uint256 amount, uint256 minOutput) public {\n        // Attacker sees this in mempool and front-runs\n        uint256 output = calculateOutput(amount);\n        require(output >= minOutput, \"Slippage too high\");\n        // Perform swap\n    }\n}\n```\n\n**Mitigation:**\n```solidity\ncontract SecureDEX {\n    mapping(bytes32 => bool) public usedCommitments;\n\n    // Step 1: Commit to trade\n    function commitTrade(bytes32 commitment) public {\n        usedCommitments[commitment] = true;\n    }\n\n    // Step 2: Reveal trade (next block)\n    function revealTrade(\n        uint256 amount,\n        uint256 minOutput,\n        bytes32 secret\n    ) public {\n        bytes32 commitment = keccak256(abi.encodePacked(\n            msg.sender, amount, minOutput, secret\n        ));\n        require(usedCommitments[commitment], \"Invalid commitment\");\n        // Perform swap\n    }\n}\n```\n\n## Security Best Practices\n\n### Checks-Effects-Interactions Pattern\n```solidity\ncontract SecurePattern {\n    mapping(address => uint256) public balances;\n\n    function withdraw(uint256 amount) public {\n        // 1. CHECKS: Validate conditions\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n        require(amount > 0, \"Amount must be positive\");\n\n        // 2. EFFECTS: Update state\n        balances[msg.sender] -= amount;\n\n        // 3. INTERACTIONS: External calls last\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n### Pull Over Push Pattern\n```solidity\n// Prefer this (pull)\ncontract SecurePayment {\n    mapping(address => uint256) public pendingWithdrawals;\n\n    function recordPayment(address recipient, uint256 amount) internal {\n        pendingWithdrawals[recipient] += amount;\n    }\n\n    function withdraw() public {\n        uint256 amount = pendingWithdrawals[msg.sender];\n        require(amount > 0, \"Nothing to withdraw\");\n\n        pendingWithdrawals[msg.sender] = 0;\n        payable(msg.sender).transfer(amount);\n    }\n}\n\n// Over this (push)\ncontract RiskyPayment {\n    function distributePayments(address[] memory recipients, uint256[] memory amounts) public {\n        for (uint i = 0; i < recipients.length; i++) {\n            // If any transfer fails, entire batch fails\n            payable(recipients[i]).transfer(amounts[i]);\n        }\n    }\n}\n```\n\n### Input Validation\n```solidity\ncontract SecureContract {\n    function transfer(address to, uint256 amount) public {\n        // Validate inputs\n        require(to != address(0), \"Invalid recipient\");\n        require(to != address(this), \"Cannot send to contract\");\n        require(amount > 0, \"Amount must be positive\");\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n\n        // Proceed with transfer\n        balances[msg.sender] -= amount;\n        balances[to] += amount;\n    }\n}\n```\n\n### Emergency Stop (Circuit Breaker)\n```solidity\nimport \"@openzeppelin/contracts/security/Pausable.sol\";\n\ncontract EmergencyStop is Pausable, Ownable {\n    function criticalFunction() public whenNotPaused {\n        // Function logic\n    }\n\n    function emergencyStop() public onlyOwner {\n        _pause();\n    }\n\n    function resume() public onlyOwner {\n        _unpause();\n    }\n}\n```\n\n## Gas Optimization\n\n### Use `uint256` Instead of Smaller Types\n```solidity\n// More gas efficient\ncontract GasEfficient {\n    uint256 public value;  // Optimal\n\n    function set(uint256 _value) public {\n        value = _value;\n    }\n}\n\n// Less efficient\ncontract GasInefficient {\n    uint8 public value;  // Still uses 256-bit slot\n\n    function set(uint8 _value) public {\n        value = _value;  // Extra gas for type conversion\n    }\n}\n```\n\n### Pack Storage Variables\n```solidity\n// Gas efficient (3 variables in 1 slot)\ncontract PackedStorage {\n    uint128 public a;  // Slot 0\n    uint64 public b;   // Slot 0\n    uint64 public c;   // Slot 0\n    uint256 public d;  // Slot 1\n}\n\n// Gas inefficient (each variable in separate slot)\ncontract UnpackedStorage {\n    uint256 public a;  // Slot 0\n    uint256 public b;  // Slot 1\n    uint256 public c;  // Slot 2\n    uint256 public d;  // Slot 3\n}\n```\n\n### Use `calldata` Instead of `memory` for Function Arguments\n```solidity\ncontract GasOptimized {\n    // More gas efficient\n    function processData(uint256[] calldata data) public pure returns (uint256) {\n        return data[0];\n    }\n\n    // Less efficient\n    function processDataMemory(uint256[] memory data) public pure returns (uint256) {\n        return data[0];\n    }\n}\n```\n\n### Use Events for Data Storage (When Appropriate)\n```solidity\ncontract EventStorage {\n    // Emitting events is cheaper than storage\n    event DataStored(address indexed user, uint256 indexed id, bytes data);\n\n    function storeData(uint256 id, bytes calldata data) public {\n        emit DataStored(msg.sender, id, data);\n        // Don't store in contract storage unless needed\n    }\n}\n```\n\n## Common Vulnerabilities Checklist\n\n```solidity\n// Security Checklist Contract\ncontract SecurityChecklist {\n    /**\n     * [ ] Reentrancy protection (ReentrancyGuard or CEI pattern)\n     * [ ] Integer overflow/underflow (Solidity 0.8+ or SafeMath)\n     * [ ] Access control (Ownable, roles, modifiers)\n     * [ ] Input validation (require statements)\n     * [ ] Front-running mitigation (commit-reveal if applicable)\n     * [ ] Gas optimization (packed storage, calldata)\n     * [ ] Emergency stop mechanism (Pausable)\n     * [ ] Pull over push pattern for payments\n     * [ ] No delegatecall to untrusted contracts\n     * [ ] No tx.origin for authentication (use msg.sender)\n     * [ ] Proper event emission\n     * [ ] External calls at end of function\n     * [ ] Check return values of external calls\n     * [ ] No hardcoded addresses\n     * [ ] Upgrade mechanism (if proxy pattern)\n     */\n}\n```\n\n## Testing for Security\n\n```javascript\n// Hardhat test example\nconst { expect } = require(\"chai\");\nconst { ethers } = require(\"hardhat\");\n\ndescribe(\"Security Tests\", function () {\n    it(\"Should prevent reentrancy attack\", async function () {\n        const [attacker] = await ethers.getSigners();\n\n        const VictimBank = await ethers.getContractFactory(\"SecureBank\");\n        const bank = await VictimBank.deploy();\n\n        const Attacker = await ethers.getContractFactory(\"ReentrancyAttacker\");\n        const attackerContract = await Attacker.deploy(bank.address);\n\n        // Deposit funds\n        await bank.deposit({value: ethers.utils.parseEther(\"10\")});\n\n        // Attempt reentrancy attack\n        await expect(\n            attackerContract.attack({value: ethers.utils.parseEther(\"1\")})\n        ).to.be.revertedWith(\"ReentrancyGuard: reentrant call\");\n    });\n\n    it(\"Should prevent integer overflow\", async function () {\n        const Token = await ethers.getContractFactory(\"SecureToken\");\n        const token = await Token.deploy();\n\n        // Attempt overflow\n        await expect(\n            token.transfer(attacker.address, ethers.constants.MaxUint256)\n        ).to.be.reverted;\n    });\n\n    it(\"Should enforce access control\", async function () {\n        const [owner, attacker] = await ethers.getSigners();\n\n        const Contract = await ethers.getContractFactory(\"SecureContract\");\n        const contract = await Contract.deploy();\n\n        // Attempt unauthorized withdrawal\n        await expect(\n            contract.connect(attacker).withdraw(100)\n        ).to.be.revertedWith(\"Ownable: caller is not the owner\");\n    });\n});\n```\n\n## Audit Preparation\n\n```solidity\ncontract WellDocumentedContract {\n    /**\n     * @title Well Documented Contract\n     * @dev Example of proper documentation for audits\n     * @notice This contract handles user deposits and withdrawals\n     */\n\n    /// @notice Mapping of user balances\n    mapping(address => uint256) public balances;\n\n    /**\n     * @dev Deposits ETH into the contract\n     * @notice Anyone can deposit funds\n     */\n    function deposit() public payable {\n        require(msg.value > 0, \"Must send ETH\");\n        balances[msg.sender] += msg.value;\n    }\n\n    /**\n     * @dev Withdraws user's balance\n     * @notice Follows CEI pattern to prevent reentrancy\n     * @param amount Amount to withdraw in wei\n     */\n    function withdraw(uint256 amount) public {\n        // CHECKS\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n\n        // EFFECTS\n        balances[msg.sender] -= amount;\n\n        // INTERACTIONS\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n## Resources\n\n- **references/reentrancy.md**: Comprehensive reentrancy prevention\n- **references/access-control.md**: Role-based access patterns\n- **references/overflow-underflow.md**: SafeMath and integer safety\n- **references/gas-optimization.md**: Gas saving techniques\n- **references/vulnerability-patterns.md**: Common vulnerability catalog\n- **assets/solidity-contracts-templates.sol**: Secure contract templates\n- **assets/security-checklist.md**: Pre-audit checklist\n- **scripts/analyze-contract.sh**: Static analysis tools\n\n## Tools for Security Analysis\n\n- **Slither**: Static analysis tool\n- **Mythril**: Security analysis tool\n- **Echidna**: Fuzzing tool\n- **Manticore**: Symbolic execution\n- **Securify**: Automated security scanner\n\n## Common Pitfalls\n\n1. **Using `tx.origin` for Authentication**: Use `msg.sender` instead\n2. **Unchecked External Calls**: Always check return values\n3. **Delegatecall to Untrusted Contracts**: Can hijack your contract\n4. **Floating Pragma**: Pin to specific Solidity version\n5. **Missing Events**: Emit events for state changes\n6. **Excessive Gas in Loops**: Can hit block gas limit\n7. **No Upgrade Path**: Consider proxy patterns if upgrades needed"
              },
              {
                "name": "web3-testing",
                "description": "Test smart contracts comprehensively using Hardhat and Foundry with unit tests, integration tests, and mainnet forking. Use when testing Solidity contracts, setting up blockchain test suites, or validating DeFi protocols.",
                "path": "plugins/blockchain-web3/skills/web3-testing/SKILL.md",
                "frontmatter": {
                  "name": "web3-testing",
                  "description": "Test smart contracts comprehensively using Hardhat and Foundry with unit tests, integration tests, and mainnet forking. Use when testing Solidity contracts, setting up blockchain test suites, or validating DeFi protocols."
                },
                "content": "# Web3 Smart Contract Testing\n\nMaster comprehensive testing strategies for smart contracts using Hardhat, Foundry, and advanced testing patterns.\n\n## When to Use This Skill\n\n- Writing unit tests for smart contracts\n- Setting up integration test suites\n- Performing gas optimization testing\n- Fuzzing for edge cases\n- Forking mainnet for realistic testing\n- Automating test coverage reporting\n- Verifying contracts on Etherscan\n\n## Hardhat Testing Setup\n\n```javascript\n// hardhat.config.js\nrequire(\"@nomicfoundation/hardhat-toolbox\");\nrequire(\"@nomiclabs/hardhat-etherscan\");\nrequire(\"hardhat-gas-reporter\");\nrequire(\"solidity-coverage\");\n\nmodule.exports = {\n  solidity: {\n    version: \"0.8.19\",\n    settings: {\n      optimizer: {\n        enabled: true,\n        runs: 200\n      }\n    }\n  },\n  networks: {\n    hardhat: {\n      forking: {\n        url: process.env.MAINNET_RPC_URL,\n        blockNumber: 15000000\n      }\n    },\n    goerli: {\n      url: process.env.GOERLI_RPC_URL,\n      accounts: [process.env.PRIVATE_KEY]\n    }\n  },\n  gasReporter: {\n    enabled: true,\n    currency: 'USD',\n    coinmarketcap: process.env.COINMARKETCAP_API_KEY\n  },\n  etherscan: {\n    apiKey: process.env.ETHERSCAN_API_KEY\n  }\n};\n```\n\n## Unit Testing Patterns\n\n```javascript\nconst { expect } = require(\"chai\");\nconst { ethers } = require(\"hardhat\");\nconst { loadFixture, time } = require(\"@nomicfoundation/hardhat-network-helpers\");\n\ndescribe(\"Token Contract\", function () {\n  // Fixture for test setup\n  async function deployTokenFixture() {\n    const [owner, addr1, addr2] = await ethers.getSigners();\n\n    const Token = await ethers.getContractFactory(\"Token\");\n    const token = await Token.deploy();\n\n    return { token, owner, addr1, addr2 };\n  }\n\n  describe(\"Deployment\", function () {\n    it(\"Should set the right owner\", async function () {\n      const { token, owner } = await loadFixture(deployTokenFixture);\n      expect(await token.owner()).to.equal(owner.address);\n    });\n\n    it(\"Should assign total supply to owner\", async function () {\n      const { token, owner } = await loadFixture(deployTokenFixture);\n      const ownerBalance = await token.balanceOf(owner.address);\n      expect(await token.totalSupply()).to.equal(ownerBalance);\n    });\n  });\n\n  describe(\"Transactions\", function () {\n    it(\"Should transfer tokens between accounts\", async function () {\n      const { token, owner, addr1 } = await loadFixture(deployTokenFixture);\n\n      await expect(token.transfer(addr1.address, 50))\n        .to.changeTokenBalances(token, [owner, addr1], [-50, 50]);\n    });\n\n    it(\"Should fail if sender doesn't have enough tokens\", async function () {\n      const { token, addr1 } = await loadFixture(deployTokenFixture);\n      const initialBalance = await token.balanceOf(addr1.address);\n\n      await expect(\n        token.connect(addr1).transfer(owner.address, 1)\n      ).to.be.revertedWith(\"Insufficient balance\");\n    });\n\n    it(\"Should emit Transfer event\", async function () {\n      const { token, owner, addr1 } = await loadFixture(deployTokenFixture);\n\n      await expect(token.transfer(addr1.address, 50))\n        .to.emit(token, \"Transfer\")\n        .withArgs(owner.address, addr1.address, 50);\n    });\n  });\n\n  describe(\"Time-based tests\", function () {\n    it(\"Should handle time-locked operations\", async function () {\n      const { token } = await loadFixture(deployTokenFixture);\n\n      // Increase time by 1 day\n      await time.increase(86400);\n\n      // Test time-dependent functionality\n    });\n  });\n\n  describe(\"Gas optimization\", function () {\n    it(\"Should use gas efficiently\", async function () {\n      const { token } = await loadFixture(deployTokenFixture);\n\n      const tx = await token.transfer(addr1.address, 100);\n      const receipt = await tx.wait();\n\n      expect(receipt.gasUsed).to.be.lessThan(50000);\n    });\n  });\n});\n```\n\n## Foundry Testing (Forge)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"forge-std/Test.sol\";\nimport \"../src/Token.sol\";\n\ncontract TokenTest is Test {\n    Token token;\n    address owner = address(1);\n    address user1 = address(2);\n    address user2 = address(3);\n\n    function setUp() public {\n        vm.prank(owner);\n        token = new Token();\n    }\n\n    function testInitialSupply() public {\n        assertEq(token.totalSupply(), 1000000 * 10**18);\n    }\n\n    function testTransfer() public {\n        vm.prank(owner);\n        token.transfer(user1, 100);\n\n        assertEq(token.balanceOf(user1), 100);\n        assertEq(token.balanceOf(owner), token.totalSupply() - 100);\n    }\n\n    function testFailTransferInsufficientBalance() public {\n        vm.prank(user1);\n        token.transfer(user2, 100); // Should fail\n    }\n\n    function testCannotTransferToZeroAddress() public {\n        vm.prank(owner);\n        vm.expectRevert(\"Invalid recipient\");\n        token.transfer(address(0), 100);\n    }\n\n    // Fuzzing test\n    function testFuzzTransfer(uint256 amount) public {\n        vm.assume(amount > 0 && amount <= token.totalSupply());\n\n        vm.prank(owner);\n        token.transfer(user1, amount);\n\n        assertEq(token.balanceOf(user1), amount);\n    }\n\n    // Test with cheatcodes\n    function testDealAndPrank() public {\n        // Give ETH to address\n        vm.deal(user1, 10 ether);\n\n        // Impersonate address\n        vm.prank(user1);\n\n        // Test functionality\n        assertEq(user1.balance, 10 ether);\n    }\n\n    // Mainnet fork test\n    function testForkMainnet() public {\n        vm.createSelectFork(\"https://eth-mainnet.alchemyapi.io/v2/...\");\n\n        // Interact with mainnet contracts\n        address dai = 0x6B175474E89094C44Da98b954EedeAC495271d0F;\n        assertEq(IERC20(dai).symbol(), \"DAI\");\n    }\n}\n```\n\n## Advanced Testing Patterns\n\n### Snapshot and Revert\n```javascript\ndescribe(\"Complex State Changes\", function () {\n  let snapshotId;\n\n  beforeEach(async function () {\n    snapshotId = await network.provider.send(\"evm_snapshot\");\n  });\n\n  afterEach(async function () {\n    await network.provider.send(\"evm_revert\", [snapshotId]);\n  });\n\n  it(\"Test 1\", async function () {\n    // Make state changes\n  });\n\n  it(\"Test 2\", async function () {\n    // State reverted, clean slate\n  });\n});\n```\n\n### Mainnet Forking\n```javascript\ndescribe(\"Mainnet Fork Tests\", function () {\n  let uniswapRouter, dai, usdc;\n\n  before(async function () {\n    await network.provider.request({\n      method: \"hardhat_reset\",\n      params: [{\n        forking: {\n          jsonRpcUrl: process.env.MAINNET_RPC_URL,\n          blockNumber: 15000000\n        }\n      }]\n    });\n\n    // Connect to existing mainnet contracts\n    uniswapRouter = await ethers.getContractAt(\n      \"IUniswapV2Router\",\n      \"0x7a250d5630B4cF539739dF2C5dAcb4c659F2488D\"\n    );\n\n    dai = await ethers.getContractAt(\n      \"IERC20\",\n      \"0x6B175474E89094C44Da98b954EedeAC495271d0F\"\n    );\n  });\n\n  it(\"Should swap on Uniswap\", async function () {\n    // Test with real Uniswap contracts\n  });\n});\n```\n\n### Impersonating Accounts\n```javascript\nit(\"Should impersonate whale account\", async function () {\n  const whaleAddress = \"0x...\";\n\n  await network.provider.request({\n    method: \"hardhat_impersonateAccount\",\n    params: [whaleAddress]\n  });\n\n  const whale = await ethers.getSigner(whaleAddress);\n\n  // Use whale's tokens\n  await dai.connect(whale).transfer(addr1.address, ethers.utils.parseEther(\"1000\"));\n});\n```\n\n## Gas Optimization Testing\n\n```javascript\nconst { expect } = require(\"chai\");\n\ndescribe(\"Gas Optimization\", function () {\n  it(\"Compare gas usage between implementations\", async function () {\n    const Implementation1 = await ethers.getContractFactory(\"OptimizedContract\");\n    const Implementation2 = await ethers.getContractFactory(\"UnoptimizedContract\");\n\n    const contract1 = await Implementation1.deploy();\n    const contract2 = await Implementation2.deploy();\n\n    const tx1 = await contract1.doSomething();\n    const receipt1 = await tx1.wait();\n\n    const tx2 = await contract2.doSomething();\n    const receipt2 = await tx2.wait();\n\n    console.log(\"Optimized gas:\", receipt1.gasUsed.toString());\n    console.log(\"Unoptimized gas:\", receipt2.gasUsed.toString());\n\n    expect(receipt1.gasUsed).to.be.lessThan(receipt2.gasUsed);\n  });\n});\n```\n\n## Coverage Reporting\n\n```bash\n# Generate coverage report\nnpx hardhat coverage\n\n# Output shows:\n# File                | % Stmts | % Branch | % Funcs | % Lines |\n# -------------------|---------|----------|---------|---------|\n# contracts/Token.sol |   100   |   90     |   100   |   95    |\n```\n\n## Contract Verification\n\n```javascript\n// Verify on Etherscan\nawait hre.run(\"verify:verify\", {\n  address: contractAddress,\n  constructorArguments: [arg1, arg2]\n});\n```\n\n```bash\n# Or via CLI\nnpx hardhat verify --network mainnet CONTRACT_ADDRESS \"Constructor arg1\" \"arg2\"\n```\n\n## CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '16'\n\n      - run: npm install\n      - run: npx hardhat compile\n      - run: npx hardhat test\n      - run: npx hardhat coverage\n\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n```\n\n## Resources\n\n- **references/hardhat-setup.md**: Hardhat configuration guide\n- **references/foundry-setup.md**: Foundry testing framework\n- **references/test-patterns.md**: Testing best practices\n- **references/mainnet-forking.md**: Fork testing strategies\n- **references/contract-verification.md**: Etherscan verification\n- **assets/hardhat-config.js**: Complete Hardhat configuration\n- **assets/test-suite.js**: Comprehensive test examples\n- **assets/foundry.toml**: Foundry configuration\n- **scripts/test-contract.sh**: Automated testing script\n\n## Best Practices\n\n1. **Test Coverage**: Aim for >90% coverage\n2. **Edge Cases**: Test boundary conditions\n3. **Gas Limits**: Verify functions don't hit block gas limit\n4. **Reentrancy**: Test for reentrancy vulnerabilities\n5. **Access Control**: Test unauthorized access attempts\n6. **Events**: Verify event emissions\n7. **Fixtures**: Use fixtures to avoid code duplication\n8. **Mainnet Fork**: Test with real contracts\n9. **Fuzzing**: Use property-based testing\n10. **CI/CD**: Automate testing on every commit"
              }
            ]
          },
          {
            "name": "business-analytics",
            "description": "Business analysis with data storytelling and KPI dashboards",
            "source": "./plugins/business-analytics",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install business-analytics@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "data-storytelling",
                "description": "Transform data into compelling narratives using visualization, context, and persuasive structure. Use when presenting analytics to stakeholders, creating data reports, or building executive presentations.",
                "path": "plugins/business-analytics/skills/data-storytelling/SKILL.md",
                "frontmatter": {
                  "name": "data-storytelling",
                  "description": "Transform data into compelling narratives using visualization, context, and persuasive structure. Use when presenting analytics to stakeholders, creating data reports, or building executive presentations."
                },
                "content": "# Data Storytelling\n\nTransform raw data into compelling narratives that drive decisions and inspire action.\n\n## When to Use This Skill\n\n- Presenting analytics to executives\n- Creating quarterly business reviews\n- Building investor presentations\n- Writing data-driven reports\n- Communicating insights to non-technical audiences\n- Making recommendations based on data\n\n## Core Concepts\n\n### 1. Story Structure\n\n```\nSetup â†’ Conflict â†’ Resolution\n\nSetup: Context and baseline\nConflict: The problem or opportunity\nResolution: Insights and recommendations\n```\n\n### 2. Narrative Arc\n\n```\n1. Hook: Grab attention with surprising insight\n2. Context: Establish the baseline\n3. Rising Action: Build through data points\n4. Climax: The key insight\n5. Resolution: Recommendations\n6. Call to Action: Next steps\n```\n\n### 3. Three Pillars\n\n| Pillar | Purpose | Components |\n|--------|---------|------------|\n| **Data** | Evidence | Numbers, trends, comparisons |\n| **Narrative** | Meaning | Context, causation, implications |\n| **Visuals** | Clarity | Charts, diagrams, highlights |\n\n## Story Frameworks\n\n### Framework 1: The Problem-Solution Story\n\n```markdown\n# Customer Churn Analysis\n\n## The Hook\n\"We're losing $2.4M annually to preventable churn.\"\n\n## The Context\n- Current churn rate: 8.5% (industry average: 5%)\n- Average customer lifetime value: $4,800\n- 500 customers churned last quarter\n\n## The Problem\nAnalysis of churned customers reveals a pattern:\n- 73% churned within first 90 days\n- Common factor: < 3 support interactions\n- Low feature adoption in first month\n\n## The Insight\n[Show engagement curve visualization]\nCustomers who don't engage in the first 14 days\nare 4x more likely to churn.\n\n## The Solution\n1. Implement 14-day onboarding sequence\n2. Proactive outreach at day 7\n3. Feature adoption tracking\n\n## Expected Impact\n- Reduce early churn by 40%\n- Save $960K annually\n- Payback period: 3 months\n\n## Call to Action\nApprove $50K budget for onboarding automation.\n```\n\n### Framework 2: The Trend Story\n\n```markdown\n# Q4 Performance Analysis\n\n## Where We Started\nQ3 ended with $1.2M MRR, 15% below target.\nTeam morale was low after missed goals.\n\n## What Changed\n[Timeline visualization]\n- Oct: Launched self-serve pricing\n- Nov: Reduced friction in signup\n- Dec: Added customer success calls\n\n## The Transformation\n[Before/after comparison chart]\n| Metric         | Q3     | Q4     | Change |\n|----------------|--------|--------|--------|\n| Trial â†’ Paid   | 8%     | 15%    | +87%   |\n| Time to Value  | 14 days| 5 days | -64%   |\n| Expansion Rate | 2%     | 8%     | +300%  |\n\n## Key Insight\nSelf-serve + high-touch creates compound growth.\nCustomers who self-serve AND get a success call\nhave 3x higher expansion rate.\n\n## Going Forward\nDouble down on hybrid model.\nTarget: $1.8M MRR by Q2.\n```\n\n### Framework 3: The Comparison Story\n\n```markdown\n# Market Opportunity Analysis\n\n## The Question\nShould we expand into EMEA or APAC first?\n\n## The Comparison\n[Side-by-side market analysis]\n\n### EMEA\n- Market size: $4.2B\n- Growth rate: 8%\n- Competition: High\n- Regulatory: Complex (GDPR)\n- Language: Multiple\n\n### APAC\n- Market size: $3.8B\n- Growth rate: 15%\n- Competition: Moderate\n- Regulatory: Varied\n- Language: Multiple\n\n## The Analysis\n[Weighted scoring matrix visualization]\n\n| Factor      | Weight | EMEA Score | APAC Score |\n|-------------|--------|------------|------------|\n| Market Size | 25%    | 5          | 4          |\n| Growth      | 30%    | 3          | 5          |\n| Competition | 20%    | 2          | 4          |\n| Ease        | 25%    | 2          | 3          |\n| **Total**   |        | **2.9**    | **4.1**    |\n\n## The Recommendation\nAPAC first. Higher growth, less competition.\nStart with Singapore hub (English, business-friendly).\nEnter EMEA in Year 2 with localization ready.\n\n## Risk Mitigation\n- Timezone coverage: Hire 24/7 support\n- Cultural fit: Local partnerships\n- Payment: Multi-currency from day 1\n```\n\n## Visualization Techniques\n\n### Technique 1: Progressive Reveal\n\n```markdown\nStart simple, add layers:\n\nSlide 1: \"Revenue is growing\" [single line chart]\nSlide 2: \"But growth is slowing\" [add growth rate overlay]\nSlide 3: \"Driven by one segment\" [add segment breakdown]\nSlide 4: \"Which is saturating\" [add market share]\nSlide 5: \"We need new segments\" [add opportunity zones]\n```\n\n### Technique 2: Contrast and Compare\n\n```markdown\nBefore/After:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    BEFORE       â”‚     AFTER       â”‚\nâ”‚                 â”‚                 â”‚\nâ”‚  Process: 5 daysâ”‚  Process: 1 day â”‚\nâ”‚  Errors: 15%    â”‚  Errors: 2%     â”‚\nâ”‚  Cost: $50/unit â”‚  Cost: $20/unit â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nThis/That (emphasize difference):\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         CUSTOMER A vs B             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚    â”‚ â–ˆâ–ˆ       â”‚      â”‚\nâ”‚  â”‚ $45,000  â”‚    â”‚ $8,000   â”‚      â”‚\nâ”‚  â”‚ LTV      â”‚    â”‚ LTV      â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚  Onboarded       No onboarding     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Technique 3: Annotation and Highlight\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the main data\nax.plot(dates, revenue, linewidth=2, color='#2E86AB')\n\n# Add annotation for key events\nax.annotate(\n    'Product Launch\\n+32% spike',\n    xy=(launch_date, launch_revenue),\n    xytext=(launch_date, launch_revenue * 1.2),\n    fontsize=10,\n    arrowprops=dict(arrowstyle='->', color='#E63946'),\n    color='#E63946'\n)\n\n# Highlight a region\nax.axvspan(growth_start, growth_end, alpha=0.2, color='green',\n           label='Growth Period')\n\n# Add threshold line\nax.axhline(y=target, color='gray', linestyle='--',\n           label=f'Target: ${target:,.0f}')\n\nax.set_title('Revenue Growth Story', fontsize=14, fontweight='bold')\nax.legend()\n```\n\n## Presentation Templates\n\n### Template 1: Executive Summary Slide\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  KEY INSIGHT                                                â”‚\nâ”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚\nâ”‚                                                             â”‚\nâ”‚  \"Customers who complete onboarding in week 1              â”‚\nâ”‚   have 3x higher lifetime value\"                           â”‚\nâ”‚                                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                      â”‚                                      â”‚\nâ”‚  THE DATA            â”‚  THE IMPLICATION                     â”‚\nâ”‚                      â”‚                                      â”‚\nâ”‚  Week 1 completers:  â”‚  âœ“ Prioritize onboarding UX         â”‚\nâ”‚  â€¢ LTV: $4,500       â”‚  âœ“ Add day-1 success milestones     â”‚\nâ”‚  â€¢ Retention: 85%    â”‚  âœ“ Proactive week-1 outreach        â”‚\nâ”‚  â€¢ NPS: 72           â”‚                                      â”‚\nâ”‚                      â”‚  Investment: $75K                    â”‚\nâ”‚  Others:             â”‚  Expected ROI: 8x                    â”‚\nâ”‚  â€¢ LTV: $1,500       â”‚                                      â”‚\nâ”‚  â€¢ Retention: 45%    â”‚                                      â”‚\nâ”‚  â€¢ NPS: 34           â”‚                                      â”‚\nâ”‚                      â”‚                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Template 2: Data Story Flow\n\n```\nSlide 1: THE HEADLINE\n\"We can grow 40% faster by fixing onboarding\"\n\nSlide 2: THE CONTEXT\nCurrent state metrics\nIndustry benchmarks\nGap analysis\n\nSlide 3: THE DISCOVERY\nWhat the data revealed\nSurprising finding\nPattern identification\n\nSlide 4: THE DEEP DIVE\nRoot cause analysis\nSegment breakdowns\nStatistical significance\n\nSlide 5: THE RECOMMENDATION\nProposed actions\nResource requirements\nTimeline\n\nSlide 6: THE IMPACT\nExpected outcomes\nROI calculation\nRisk assessment\n\nSlide 7: THE ASK\nSpecific request\nDecision needed\nNext steps\n```\n\n### Template 3: One-Page Dashboard Story\n\n```markdown\n# Monthly Business Review: January 2024\n\n## THE HEADLINE\nRevenue up 15% but CAC increasing faster than LTV\n\n## KEY METRICS AT A GLANCE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  MRR   â”‚  NRR   â”‚  CAC   â”‚  LTV   â”‚\nâ”‚ $125K  â”‚ 108%   â”‚ $450   â”‚ $2,200 â”‚\nâ”‚  â–²15%  â”‚  â–²3%   â”‚  â–²22%  â”‚  â–²8%   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n## WHAT'S WORKING\nâœ“ Enterprise segment growing 25% MoM\nâœ“ Referral program driving 30% of new logos\nâœ“ Support satisfaction at all-time high (94%)\n\n## WHAT NEEDS ATTENTION\nâœ— SMB acquisition cost up 40%\nâœ— Trial conversion down 5 points\nâœ— Time-to-value increased by 3 days\n\n## ROOT CAUSE\n[Mini chart showing SMB vs Enterprise CAC trend]\nSMB paid ads becoming less efficient.\nCPC up 35% while conversion flat.\n\n## RECOMMENDATION\n1. Shift $20K/mo from paid to content\n2. Launch SMB self-serve trial\n3. A/B test shorter onboarding\n\n## NEXT MONTH'S FOCUS\n- Launch content marketing pilot\n- Complete self-serve MVP\n- Reduce time-to-value to < 7 days\n```\n\n## Writing Techniques\n\n### Headlines That Work\n\n```markdown\nBAD: \"Q4 Sales Analysis\"\nGOOD: \"Q4 Sales Beat Target by 23% - Here's Why\"\n\nBAD: \"Customer Churn Report\"\nGOOD: \"We're Losing $2.4M to Preventable Churn\"\n\nBAD: \"Marketing Performance\"\nGOOD: \"Content Marketing Delivers 4x ROI vs. Paid\"\n\nFormula:\n[Specific Number] + [Business Impact] + [Actionable Context]\n```\n\n### Transition Phrases\n\n```markdown\nBuilding the narrative:\nâ€¢ \"This leads us to ask...\"\nâ€¢ \"When we dig deeper...\"\nâ€¢ \"The pattern becomes clear when...\"\nâ€¢ \"Contrast this with...\"\n\nIntroducing insights:\nâ€¢ \"The data reveals...\"\nâ€¢ \"What surprised us was...\"\nâ€¢ \"The inflection point came when...\"\nâ€¢ \"The key finding is...\"\n\nMoving to action:\nâ€¢ \"This insight suggests...\"\nâ€¢ \"Based on this analysis...\"\nâ€¢ \"The implication is clear...\"\nâ€¢ \"Our recommendation is...\"\n```\n\n### Handling Uncertainty\n\n```markdown\nAcknowledge limitations:\nâ€¢ \"With 95% confidence, we can say...\"\nâ€¢ \"The sample size of 500 shows...\"\nâ€¢ \"While correlation is strong, causation requires...\"\nâ€¢ \"This trend holds for [segment], though [caveat]...\"\n\nPresent ranges:\nâ€¢ \"Impact estimate: $400K-$600K\"\nâ€¢ \"Confidence interval: 15-20% improvement\"\nâ€¢ \"Best case: X, Conservative: Y\"\n```\n\n## Best Practices\n\n### Do's\n- **Start with the \"so what\"** - Lead with insight\n- **Use the rule of three** - Three points, three comparisons\n- **Show, don't tell** - Let data speak\n- **Make it personal** - Connect to audience goals\n- **End with action** - Clear next steps\n\n### Don'ts\n- **Don't data dump** - Curate ruthlessly\n- **Don't bury the insight** - Front-load key findings\n- **Don't use jargon** - Match audience vocabulary\n- **Don't show methodology first** - Context, then method\n- **Don't forget the narrative** - Numbers need meaning\n\n## Resources\n\n- [Storytelling with Data (Cole Nussbaumer)](https://www.storytellingwithdata.com/)\n- [The Pyramid Principle (Barbara Minto)](https://www.amazon.com/Pyramid-Principle-Logic-Writing-Thinking/dp/0273710516)\n- [Resonate (Nancy Duarte)](https://www.duarte.com/resonate/)"
              },
              {
                "name": "kpi-dashboard-design",
                "description": "Design effective KPI dashboards with metrics selection, visualization best practices, and real-time monitoring patterns. Use when building business dashboards, selecting metrics, or designing data visualization layouts.",
                "path": "plugins/business-analytics/skills/kpi-dashboard-design/SKILL.md",
                "frontmatter": {
                  "name": "kpi-dashboard-design",
                  "description": "Design effective KPI dashboards with metrics selection, visualization best practices, and real-time monitoring patterns. Use when building business dashboards, selecting metrics, or designing data visualization layouts."
                },
                "content": "# KPI Dashboard Design\n\nComprehensive patterns for designing effective Key Performance Indicator (KPI) dashboards that drive business decisions.\n\n## When to Use This Skill\n\n- Designing executive dashboards\n- Selecting meaningful KPIs\n- Building real-time monitoring displays\n- Creating department-specific metrics views\n- Improving existing dashboard layouts\n- Establishing metric governance\n\n## Core Concepts\n\n### 1. KPI Framework\n\n| Level | Focus | Update Frequency | Audience |\n|-------|-------|------------------|----------|\n| **Strategic** | Long-term goals | Monthly/Quarterly | Executives |\n| **Tactical** | Department goals | Weekly/Monthly | Managers |\n| **Operational** | Day-to-day | Real-time/Daily | Teams |\n\n### 2. SMART KPIs\n\n```\nSpecific: Clear definition\nMeasurable: Quantifiable\nAchievable: Realistic targets\nRelevant: Aligned to goals\nTime-bound: Defined period\n```\n\n### 3. Dashboard Hierarchy\n\n```\nâ”œâ”€â”€ Executive Summary (1 page)\nâ”‚   â”œâ”€â”€ 4-6 headline KPIs\nâ”‚   â”œâ”€â”€ Trend indicators\nâ”‚   â””â”€â”€ Key alerts\nâ”œâ”€â”€ Department Views\nâ”‚   â”œâ”€â”€ Sales Dashboard\nâ”‚   â”œâ”€â”€ Marketing Dashboard\nâ”‚   â”œâ”€â”€ Operations Dashboard\nâ”‚   â””â”€â”€ Finance Dashboard\nâ””â”€â”€ Detailed Drilldowns\n    â”œâ”€â”€ Individual metrics\n    â””â”€â”€ Root cause analysis\n```\n\n## Common KPIs by Department\n\n### Sales KPIs\n\n```yaml\nRevenue Metrics:\n  - Monthly Recurring Revenue (MRR)\n  - Annual Recurring Revenue (ARR)\n  - Average Revenue Per User (ARPU)\n  - Revenue Growth Rate\n\nPipeline Metrics:\n  - Sales Pipeline Value\n  - Win Rate\n  - Average Deal Size\n  - Sales Cycle Length\n\nActivity Metrics:\n  - Calls/Emails per Rep\n  - Demos Scheduled\n  - Proposals Sent\n  - Close Rate\n```\n\n### Marketing KPIs\n\n```yaml\nAcquisition:\n  - Cost Per Acquisition (CPA)\n  - Customer Acquisition Cost (CAC)\n  - Lead Volume\n  - Marketing Qualified Leads (MQL)\n\nEngagement:\n  - Website Traffic\n  - Conversion Rate\n  - Email Open/Click Rate\n  - Social Engagement\n\nROI:\n  - Marketing ROI\n  - Campaign Performance\n  - Channel Attribution\n  - CAC Payback Period\n```\n\n### Product KPIs\n\n```yaml\nUsage:\n  - Daily/Monthly Active Users (DAU/MAU)\n  - Session Duration\n  - Feature Adoption Rate\n  - Stickiness (DAU/MAU)\n\nQuality:\n  - Net Promoter Score (NPS)\n  - Customer Satisfaction (CSAT)\n  - Bug/Issue Count\n  - Time to Resolution\n\nGrowth:\n  - User Growth Rate\n  - Activation Rate\n  - Retention Rate\n  - Churn Rate\n```\n\n### Finance KPIs\n\n```yaml\nProfitability:\n  - Gross Margin\n  - Net Profit Margin\n  - EBITDA\n  - Operating Margin\n\nLiquidity:\n  - Current Ratio\n  - Quick Ratio\n  - Cash Flow\n  - Working Capital\n\nEfficiency:\n  - Revenue per Employee\n  - Operating Expense Ratio\n  - Days Sales Outstanding\n  - Inventory Turnover\n```\n\n## Dashboard Layout Patterns\n\n### Pattern 1: Executive Summary\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  EXECUTIVE DASHBOARD                        [Date Range â–¼]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   REVENUE   â”‚   PROFIT    â”‚  CUSTOMERS  â”‚    NPS SCORE    â”‚\nâ”‚   $2.4M     â”‚    $450K    â”‚    12,450   â”‚       72        â”‚\nâ”‚   â–² 12%     â”‚    â–² 8%     â”‚    â–² 15%    â”‚     â–² 5pts     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  Revenue Trend                    â”‚  Revenue by Product     â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚    /\\    /\\          â”‚       â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45%     â”‚   â”‚\nâ”‚  â”‚   /  \\  /  \\    /\\   â”‚       â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   32%     â”‚   â”‚\nâ”‚  â”‚  /    \\/    \\  /  \\  â”‚       â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆ     18%     â”‚   â”‚\nâ”‚  â”‚ /            \\/    \\ â”‚       â”‚  â”‚ â–ˆâ–ˆ        5%     â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  ðŸ”´ Alert: Churn rate exceeded threshold (>5%)              â”‚\nâ”‚  ðŸŸ¡ Warning: Support ticket volume 20% above average        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Pattern 2: SaaS Metrics Dashboard\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  SAAS METRICS                     Jan 2024  [Monthly â–¼]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  MRR GROWTH                          â”‚\nâ”‚  â”‚      MRR       â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚    $125,000    â”‚  â”‚  â”‚                          /â”€â”€   â”‚  â”‚\nâ”‚  â”‚     â–² 8%       â”‚  â”‚  â”‚                    /â”€â”€â”€â”€/      â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚              /â”€â”€â”€â”€/            â”‚  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚        /â”€â”€â”€â”€/                  â”‚  â”‚\nâ”‚  â”‚      ARR       â”‚  â”‚  â”‚   /â”€â”€â”€â”€/                       â”‚  â”‚\nâ”‚  â”‚   $1,500,000   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  â”‚     â–² 15%      â”‚  â”‚  J  F  M  A  M  J  J  A  S  O  N  D  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  UNIT ECONOMICS      â”‚  COHORT RETENTION                    â”‚\nâ”‚                      â”‚                                      â”‚\nâ”‚  CAC:     $450       â”‚  Month 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%  â”‚\nâ”‚  LTV:     $2,700     â”‚  Month 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    85%   â”‚\nâ”‚  LTV/CAC: 6.0x       â”‚  Month 6: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80%   â”‚\nâ”‚                      â”‚  Month 12: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      72%   â”‚\nâ”‚  Payback: 4 months   â”‚                                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  CHURN ANALYSIS                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚ Gross    â”‚ Net      â”‚ Logo     â”‚ Expansion            â”‚ â”‚\nâ”‚  â”‚ 4.2%     â”‚ 1.8%     â”‚ 3.1%     â”‚ 2.4%                 â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Pattern 3: Real-time Operations\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  OPERATIONS CENTER                    Live â— Last: 10:42:15 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  SYSTEM HEALTH             â”‚  SERVICE STATUS                â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                â”‚\nâ”‚  â”‚   CPU    MEM    DISK â”‚  â”‚  â— API Gateway      Healthy    â”‚\nâ”‚  â”‚   45%    72%    58%  â”‚  â”‚  â— User Service     Healthy    â”‚\nâ”‚  â”‚   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ  â”‚  â”‚  â— Payment Service  Degraded   â”‚\nâ”‚  â”‚   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ  â”‚  â”‚  â— Database         Healthy    â”‚\nâ”‚  â”‚   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ  â”‚  â”‚  â— Cache            Healthy    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  REQUEST THROUGHPUT        â”‚  ERROR RATE                    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–â–‚â–ƒâ–„â–… â”‚  â”‚  â”‚ â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–  â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  Current: 12,450 req/s     â”‚  Current: 0.02%                â”‚\nâ”‚  Peak: 18,200 req/s        â”‚  Threshold: 1.0%               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  RECENT ALERTS                                              â”‚\nâ”‚  10:40  ðŸŸ¡ High latency on payment-service (p99 > 500ms)    â”‚\nâ”‚  10:35  ðŸŸ¢ Resolved: Database connection pool recovered     â”‚\nâ”‚  10:22  ðŸ”´ Payment service circuit breaker tripped          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Implementation Patterns\n\n### SQL for KPI Calculations\n\n```sql\n-- Monthly Recurring Revenue (MRR)\nWITH mrr_calculation AS (\n    SELECT\n        DATE_TRUNC('month', billing_date) AS month,\n        SUM(\n            CASE subscription_interval\n                WHEN 'monthly' THEN amount\n                WHEN 'yearly' THEN amount / 12\n                WHEN 'quarterly' THEN amount / 3\n            END\n        ) AS mrr\n    FROM subscriptions\n    WHERE status = 'active'\n    GROUP BY DATE_TRUNC('month', billing_date)\n)\nSELECT\n    month,\n    mrr,\n    LAG(mrr) OVER (ORDER BY month) AS prev_mrr,\n    (mrr - LAG(mrr) OVER (ORDER BY month)) / LAG(mrr) OVER (ORDER BY month) * 100 AS growth_pct\nFROM mrr_calculation;\n\n-- Cohort Retention\nWITH cohorts AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', created_at) AS cohort_month\n    FROM users\n),\nactivity AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', event_date) AS activity_month\n    FROM user_events\n    WHERE event_type = 'active_session'\n)\nSELECT\n    c.cohort_month,\n    EXTRACT(MONTH FROM age(a.activity_month, c.cohort_month)) AS months_since_signup,\n    COUNT(DISTINCT a.user_id) AS active_users,\n    COUNT(DISTINCT a.user_id)::FLOAT / COUNT(DISTINCT c.user_id) * 100 AS retention_rate\nFROM cohorts c\nLEFT JOIN activity a ON c.user_id = a.user_id\n    AND a.activity_month >= c.cohort_month\nGROUP BY c.cohort_month, EXTRACT(MONTH FROM age(a.activity_month, c.cohort_month))\nORDER BY c.cohort_month, months_since_signup;\n\n-- Customer Acquisition Cost (CAC)\nSELECT\n    DATE_TRUNC('month', acquired_date) AS month,\n    SUM(marketing_spend) / NULLIF(COUNT(new_customers), 0) AS cac,\n    SUM(marketing_spend) AS total_spend,\n    COUNT(new_customers) AS customers_acquired\nFROM (\n    SELECT\n        DATE_TRUNC('month', u.created_at) AS acquired_date,\n        u.id AS new_customers,\n        m.spend AS marketing_spend\n    FROM users u\n    JOIN marketing_spend m ON DATE_TRUNC('month', u.created_at) = m.month\n    WHERE u.source = 'marketing'\n) acquisition\nGROUP BY DATE_TRUNC('month', acquired_date);\n```\n\n### Python Dashboard Code (Streamlit)\n\n```python\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nst.set_page_config(page_title=\"KPI Dashboard\", layout=\"wide\")\n\n# Header with date filter\ncol1, col2 = st.columns([3, 1])\nwith col1:\n    st.title(\"Executive Dashboard\")\nwith col2:\n    date_range = st.selectbox(\n        \"Period\",\n        [\"Last 7 Days\", \"Last 30 Days\", \"Last Quarter\", \"YTD\"]\n    )\n\n# KPI Cards\ndef metric_card(label, value, delta, prefix=\"\", suffix=\"\"):\n    delta_color = \"green\" if delta >= 0 else \"red\"\n    delta_arrow = \"â–²\" if delta >= 0 else \"â–¼\"\n    st.metric(\n        label=label,\n        value=f\"{prefix}{value:,.0f}{suffix}\",\n        delta=f\"{delta_arrow} {abs(delta):.1f}%\"\n    )\n\ncol1, col2, col3, col4 = st.columns(4)\nwith col1:\n    metric_card(\"Revenue\", 2400000, 12.5, prefix=\"$\")\nwith col2:\n    metric_card(\"Customers\", 12450, 15.2)\nwith col3:\n    metric_card(\"NPS Score\", 72, 5.0)\nwith col4:\n    metric_card(\"Churn Rate\", 4.2, -0.8, suffix=\"%\")\n\n# Charts\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.subheader(\"Revenue Trend\")\n    revenue_data = pd.DataFrame({\n        'Month': pd.date_range('2024-01-01', periods=12, freq='M'),\n        'Revenue': [180000, 195000, 210000, 225000, 240000, 255000,\n                    270000, 285000, 300000, 315000, 330000, 345000]\n    })\n    fig = px.line(revenue_data, x='Month', y='Revenue',\n                  line_shape='spline', markers=True)\n    fig.update_layout(height=300)\n    st.plotly_chart(fig, use_container_width=True)\n\nwith col2:\n    st.subheader(\"Revenue by Product\")\n    product_data = pd.DataFrame({\n        'Product': ['Enterprise', 'Professional', 'Starter', 'Other'],\n        'Revenue': [45, 32, 18, 5]\n    })\n    fig = px.pie(product_data, values='Revenue', names='Product',\n                 hole=0.4)\n    fig.update_layout(height=300)\n    st.plotly_chart(fig, use_container_width=True)\n\n# Cohort Heatmap\nst.subheader(\"Cohort Retention\")\ncohort_data = pd.DataFrame({\n    'Cohort': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n    'M0': [100, 100, 100, 100, 100],\n    'M1': [85, 87, 84, 86, 88],\n    'M2': [78, 80, 76, 79, None],\n    'M3': [72, 74, 70, None, None],\n    'M4': [68, 70, None, None, None],\n})\nfig = go.Figure(data=go.Heatmap(\n    z=cohort_data.iloc[:, 1:].values,\n    x=['M0', 'M1', 'M2', 'M3', 'M4'],\n    y=cohort_data['Cohort'],\n    colorscale='Blues',\n    text=cohort_data.iloc[:, 1:].values,\n    texttemplate='%{text}%',\n    textfont={\"size\": 12},\n))\nfig.update_layout(height=250)\nst.plotly_chart(fig, use_container_width=True)\n\n# Alerts Section\nst.subheader(\"Alerts\")\nalerts = [\n    {\"level\": \"error\", \"message\": \"Churn rate exceeded threshold (>5%)\"},\n    {\"level\": \"warning\", \"message\": \"Support ticket volume 20% above average\"},\n]\nfor alert in alerts:\n    if alert[\"level\"] == \"error\":\n        st.error(f\"ðŸ”´ {alert['message']}\")\n    elif alert[\"level\"] == \"warning\":\n        st.warning(f\"ðŸŸ¡ {alert['message']}\")\n```\n\n## Best Practices\n\n### Do's\n- **Limit to 5-7 KPIs** - Focus on what matters\n- **Show context** - Comparisons, trends, targets\n- **Use consistent colors** - Red=bad, green=good\n- **Enable drilldown** - From summary to detail\n- **Update appropriately** - Match metric frequency\n\n### Don'ts\n- **Don't show vanity metrics** - Focus on actionable data\n- **Don't overcrowd** - White space aids comprehension\n- **Don't use 3D charts** - They distort perception\n- **Don't hide methodology** - Document calculations\n- **Don't ignore mobile** - Ensure responsive design\n\n## Resources\n\n- [Stephen Few's Dashboard Design](https://www.perceptualedge.com/articles/visual_business_intelligence/rules_for_using_color.pdf)\n- [Edward Tufte's Principles](https://www.edwardtufte.com/tufte/)\n- [Google Data Studio Gallery](https://datastudio.google.com/gallery)"
              }
            ]
          },
          {
            "name": "code-documentation",
            "description": "Code documentation with automated doc generation",
            "source": "./plugins/code-documentation",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install code-documentation@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/code-explain",
                "description": null,
                "path": "plugins/code-documentation/commands/code-explain.md",
                "frontmatter": null,
                "content": "# Code Explanation and Analysis\n\nYou are a code education expert specializing in explaining complex code through clear narratives, visual diagrams, and step-by-step breakdowns. Transform difficult concepts into understandable explanations for developers at all levels.\n\n## Context\nThe user needs help understanding complex code sections, algorithms, design patterns, or system architectures. Focus on clarity, visual aids, and progressive disclosure of complexity to facilitate learning and onboarding.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Code Comprehension Analysis\n\nAnalyze the code to determine complexity and structure:\n\n**Code Complexity Assessment**\n```python\nimport ast\nimport re\nfrom typing import Dict, List, Tuple\n\nclass CodeAnalyzer:\n    def analyze_complexity(self, code: str) -> Dict:\n        \"\"\"\n        Analyze code complexity and structure\n        \"\"\"\n        analysis = {\n            'complexity_score': 0,\n            'concepts': [],\n            'patterns': [],\n            'dependencies': [],\n            'difficulty_level': 'beginner'\n        }\n        \n        # Parse code structure\n        try:\n            tree = ast.parse(code)\n            \n            # Analyze complexity metrics\n            analysis['metrics'] = {\n                'lines_of_code': len(code.splitlines()),\n                'cyclomatic_complexity': self._calculate_cyclomatic_complexity(tree),\n                'nesting_depth': self._calculate_max_nesting(tree),\n                'function_count': len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]),\n                'class_count': len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])\n            }\n            \n            # Identify concepts used\n            analysis['concepts'] = self._identify_concepts(tree)\n            \n            # Detect design patterns\n            analysis['patterns'] = self._detect_patterns(tree)\n            \n            # Extract dependencies\n            analysis['dependencies'] = self._extract_dependencies(tree)\n            \n            # Determine difficulty level\n            analysis['difficulty_level'] = self._assess_difficulty(analysis)\n            \n        except SyntaxError as e:\n            analysis['parse_error'] = str(e)\n            \n        return analysis\n    \n    def _identify_concepts(self, tree) -> List[str]:\n        \"\"\"\n        Identify programming concepts used in the code\n        \"\"\"\n        concepts = []\n        \n        for node in ast.walk(tree):\n            # Async/await\n            if isinstance(node, (ast.AsyncFunctionDef, ast.AsyncWith, ast.AsyncFor)):\n                concepts.append('asynchronous programming')\n            \n            # Decorators\n            elif isinstance(node, ast.FunctionDef) and node.decorator_list:\n                concepts.append('decorators')\n            \n            # Context managers\n            elif isinstance(node, ast.With):\n                concepts.append('context managers')\n            \n            # Generators\n            elif isinstance(node, ast.Yield):\n                concepts.append('generators')\n            \n            # List/Dict/Set comprehensions\n            elif isinstance(node, (ast.ListComp, ast.DictComp, ast.SetComp)):\n                concepts.append('comprehensions')\n            \n            # Lambda functions\n            elif isinstance(node, ast.Lambda):\n                concepts.append('lambda functions')\n            \n            # Exception handling\n            elif isinstance(node, ast.Try):\n                concepts.append('exception handling')\n                \n        return list(set(concepts))\n```\n\n### 2. Visual Explanation Generation\n\nCreate visual representations of code flow:\n\n**Flow Diagram Generation**\n```python\nclass VisualExplainer:\n    def generate_flow_diagram(self, code_structure):\n        \"\"\"\n        Generate Mermaid diagram showing code flow\n        \"\"\"\n        diagram = \"```mermaid\\nflowchart TD\\n\"\n        \n        # Example: Function call flow\n        if code_structure['type'] == 'function_flow':\n            nodes = []\n            edges = []\n            \n            for i, func in enumerate(code_structure['functions']):\n                node_id = f\"F{i}\"\n                nodes.append(f\"    {node_id}[{func['name']}]\")\n                \n                # Add function details\n                if func.get('parameters'):\n                    nodes.append(f\"    {node_id}_params[/{', '.join(func['parameters'])}/]\")\n                    edges.append(f\"    {node_id}_params --> {node_id}\")\n                \n                # Add return value\n                if func.get('returns'):\n                    nodes.append(f\"    {node_id}_return[{func['returns']}]\")\n                    edges.append(f\"    {node_id} --> {node_id}_return\")\n                \n                # Connect to called functions\n                for called in func.get('calls', []):\n                    called_id = f\"F{code_structure['function_map'][called]}\"\n                    edges.append(f\"    {node_id} --> {called_id}\")\n            \n            diagram += \"\\n\".join(nodes) + \"\\n\"\n            diagram += \"\\n\".join(edges) + \"\\n\"\n            \n        diagram += \"```\"\n        return diagram\n    \n    def generate_class_diagram(self, classes):\n        \"\"\"\n        Generate UML-style class diagram\n        \"\"\"\n        diagram = \"```mermaid\\nclassDiagram\\n\"\n        \n        for cls in classes:\n            # Class definition\n            diagram += f\"    class {cls['name']} {{\\n\"\n            \n            # Attributes\n            for attr in cls.get('attributes', []):\n                visibility = '+' if attr['public'] else '-'\n                diagram += f\"        {visibility}{attr['name']} : {attr['type']}\\n\"\n            \n            # Methods\n            for method in cls.get('methods', []):\n                visibility = '+' if method['public'] else '-'\n                params = ', '.join(method.get('params', []))\n                diagram += f\"        {visibility}{method['name']}({params}) : {method['returns']}\\n\"\n            \n            diagram += \"    }\\n\"\n            \n            # Relationships\n            if cls.get('inherits'):\n                diagram += f\"    {cls['inherits']} <|-- {cls['name']}\\n\"\n            \n            for composition in cls.get('compositions', []):\n                diagram += f\"    {cls['name']} *-- {composition}\\n\"\n            \n        diagram += \"```\"\n        return diagram\n```\n\n### 3. Step-by-Step Explanation\n\nBreak down complex code into digestible steps:\n\n**Progressive Explanation**\n```python\ndef generate_step_by_step_explanation(self, code, analysis):\n    \"\"\"\n    Create progressive explanation from simple to complex\n    \"\"\"\n    explanation = {\n        'overview': self._generate_overview(code, analysis),\n        'steps': [],\n        'deep_dive': [],\n        'examples': []\n    }\n    \n    # Level 1: High-level overview\n    explanation['overview'] = f\"\"\"\n## What This Code Does\n\n{self._summarize_purpose(code, analysis)}\n\n**Key Concepts**: {', '.join(analysis['concepts'])}\n**Difficulty Level**: {analysis['difficulty_level'].capitalize()}\n\"\"\"\n    \n    # Level 2: Step-by-step breakdown\n    if analysis.get('functions'):\n        for i, func in enumerate(analysis['functions']):\n            step = f\"\"\"\n### Step {i+1}: {func['name']}\n\n**Purpose**: {self._explain_function_purpose(func)}\n\n**How it works**:\n\"\"\"\n            # Break down function logic\n            for j, logic_step in enumerate(self._analyze_function_logic(func)):\n                step += f\"{j+1}. {logic_step}\\n\"\n            \n            # Add visual flow if complex\n            if func['complexity'] > 5:\n                step += f\"\\n{self._generate_function_flow(func)}\\n\"\n            \n            explanation['steps'].append(step)\n    \n    # Level 3: Deep dive into complex parts\n    for concept in analysis['concepts']:\n        deep_dive = self._explain_concept(concept, code)\n        explanation['deep_dive'].append(deep_dive)\n    \n    return explanation\n\ndef _explain_concept(self, concept, code):\n    \"\"\"\n    Explain programming concept with examples\n    \"\"\"\n    explanations = {\n        'decorators': '''\n## Understanding Decorators\n\nDecorators are a way to modify or enhance functions without changing their code directly.\n\n**Simple Analogy**: Think of a decorator like gift wrapping - it adds something extra around the original item.\n\n**How it works**:\n```python\n# This decorator:\n@timer\ndef slow_function():\n    time.sleep(1)\n\n# Is equivalent to:\ndef slow_function():\n    time.sleep(1)\nslow_function = timer(slow_function)\n```\n\n**In this code**: The decorator is used to {specific_use_in_code}\n''',\n        'generators': '''\n## Understanding Generators\n\nGenerators produce values one at a time, saving memory by not creating all values at once.\n\n**Simple Analogy**: Like a ticket dispenser that gives one ticket at a time, rather than printing all tickets upfront.\n\n**How it works**:\n```python\n# Generator function\ndef count_up_to(n):\n    i = 0\n    while i < n:\n        yield i  # Produces one value and pauses\n        i += 1\n\n# Using the generator\nfor num in count_up_to(5):\n    print(num)  # Prints 0, 1, 2, 3, 4\n```\n\n**In this code**: The generator is used to {specific_use_in_code}\n'''\n    }\n    \n    return explanations.get(concept, f\"Explanation for {concept}\")\n```\n\n### 4. Algorithm Visualization\n\nVisualize algorithm execution:\n\n**Algorithm Step Visualization**\n```python\nclass AlgorithmVisualizer:\n    def visualize_sorting_algorithm(self, algorithm_name, array):\n        \"\"\"\n        Create step-by-step visualization of sorting algorithm\n        \"\"\"\n        steps = []\n        \n        if algorithm_name == 'bubble_sort':\n            steps.append(\"\"\"\n## Bubble Sort Visualization\n\n**Initial Array**: [5, 2, 8, 1, 9]\n\n### How Bubble Sort Works:\n1. Compare adjacent elements\n2. Swap if they're in wrong order\n3. Repeat until no swaps needed\n\n### Step-by-Step Execution:\n\"\"\")\n            \n            # Simulate bubble sort with visualization\n            arr = array.copy()\n            n = len(arr)\n            \n            for i in range(n):\n                swapped = False\n                step_viz = f\"\\n**Pass {i+1}**:\\n\"\n                \n                for j in range(0, n-i-1):\n                    # Show comparison\n                    step_viz += f\"Compare [{arr[j]}] and [{arr[j+1]}]: \"\n                    \n                    if arr[j] > arr[j+1]:\n                        arr[j], arr[j+1] = arr[j+1], arr[j]\n                        step_viz += f\"Swap â†’ {arr}\\n\"\n                        swapped = True\n                    else:\n                        step_viz += \"No swap needed\\n\"\n                \n                steps.append(step_viz)\n                \n                if not swapped:\n                    steps.append(f\"\\nâœ… Array is sorted: {arr}\")\n                    break\n        \n        return '\\n'.join(steps)\n    \n    def visualize_recursion(self, func_name, example_input):\n        \"\"\"\n        Visualize recursive function calls\n        \"\"\"\n        viz = f\"\"\"\n## Recursion Visualization: {func_name}\n\n### Call Stack Visualization:\n```\n{func_name}({example_input})\nâ”‚\nâ”œâ”€> Base case check: {example_input} == 0? No\nâ”œâ”€> Recursive call: {func_name}({example_input - 1})\nâ”‚   â”‚\nâ”‚   â”œâ”€> Base case check: {example_input - 1} == 0? No\nâ”‚   â”œâ”€> Recursive call: {func_name}({example_input - 2})\nâ”‚   â”‚   â”‚\nâ”‚   â”‚   â”œâ”€> Base case check: 1 == 0? No\nâ”‚   â”‚   â”œâ”€> Recursive call: {func_name}(0)\nâ”‚   â”‚   â”‚   â”‚\nâ”‚   â”‚   â”‚   â””â”€> Base case: Return 1\nâ”‚   â”‚   â”‚\nâ”‚   â”‚   â””â”€> Return: 1 * 1 = 1\nâ”‚   â”‚\nâ”‚   â””â”€> Return: 2 * 1 = 2\nâ”‚\nâ””â”€> Return: 3 * 2 = 6\n```\n\n**Final Result**: {func_name}({example_input}) = 6\n\"\"\"\n        return viz\n```\n\n### 5. Interactive Examples\n\nGenerate interactive examples for better understanding:\n\n**Code Playground Examples**\n```python\ndef generate_interactive_examples(self, concept):\n    \"\"\"\n    Create runnable examples for concepts\n    \"\"\"\n    examples = {\n        'error_handling': '''\n## Try It Yourself: Error Handling\n\n### Example 1: Basic Try-Except\n```python\ndef safe_divide(a, b):\n    try:\n        result = a / b\n        print(f\"{a} / {b} = {result}\")\n        return result\n    except ZeroDivisionError:\n        print(\"Error: Cannot divide by zero!\")\n        return None\n    except TypeError:\n        print(\"Error: Please provide numbers only!\")\n        return None\n    finally:\n        print(\"Division attempt completed\")\n\n# Test cases - try these:\nsafe_divide(10, 2)    # Success case\nsafe_divide(10, 0)    # Division by zero\nsafe_divide(10, \"2\")  # Type error\n```\n\n### Example 2: Custom Exceptions\n```python\nclass ValidationError(Exception):\n    \"\"\"Custom exception for validation errors\"\"\"\n    pass\n\ndef validate_age(age):\n    try:\n        age = int(age)\n        if age < 0:\n            raise ValidationError(\"Age cannot be negative\")\n        if age > 150:\n            raise ValidationError(\"Age seems unrealistic\")\n        return age\n    except ValueError:\n        raise ValidationError(\"Age must be a number\")\n\n# Try these examples:\ntry:\n    validate_age(25)     # Valid\n    validate_age(-5)     # Negative age\n    validate_age(\"abc\")  # Not a number\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n```\n\n### Exercise: Implement Your Own\nTry implementing a function that:\n1. Takes a list of numbers\n2. Returns their average\n3. Handles empty lists\n4. Handles non-numeric values\n5. Uses appropriate exception handling\n''',\n        'async_programming': '''\n## Try It Yourself: Async Programming\n\n### Example 1: Basic Async/Await\n```python\nimport asyncio\nimport time\n\nasync def slow_operation(name, duration):\n    print(f\"{name} started...\")\n    await asyncio.sleep(duration)\n    print(f\"{name} completed after {duration}s\")\n    return f\"{name} result\"\n\nasync def main():\n    # Sequential execution (slow)\n    start = time.time()\n    await slow_operation(\"Task 1\", 2)\n    await slow_operation(\"Task 2\", 2)\n    print(f\"Sequential time: {time.time() - start:.2f}s\")\n    \n    # Concurrent execution (fast)\n    start = time.time()\n    results = await asyncio.gather(\n        slow_operation(\"Task 3\", 2),\n        slow_operation(\"Task 4\", 2)\n    )\n    print(f\"Concurrent time: {time.time() - start:.2f}s\")\n    print(f\"Results: {results}\")\n\n# Run it:\nasyncio.run(main())\n```\n\n### Example 2: Real-world Async Pattern\n```python\nasync def fetch_data(url):\n    \"\"\"Simulate API call\"\"\"\n    await asyncio.sleep(1)  # Simulate network delay\n    return f\"Data from {url}\"\n\nasync def process_urls(urls):\n    tasks = [fetch_data(url) for url in urls]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Try with different URLs:\nurls = [\"api.example.com/1\", \"api.example.com/2\", \"api.example.com/3\"]\nresults = asyncio.run(process_urls(urls))\nprint(results)\n```\n'''\n    }\n    \n    return examples.get(concept, \"No example available\")\n```\n\n### 6. Design Pattern Explanation\n\nExplain design patterns found in code:\n\n**Pattern Recognition and Explanation**\n```python\nclass DesignPatternExplainer:\n    def explain_pattern(self, pattern_name, code_example):\n        \"\"\"\n        Explain design pattern with diagrams and examples\n        \"\"\"\n        patterns = {\n            'singleton': '''\n## Singleton Pattern\n\n### What is it?\nThe Singleton pattern ensures a class has only one instance and provides global access to it.\n\n### When to use it?\n- Database connections\n- Configuration managers\n- Logging services\n- Cache managers\n\n### Visual Representation:\n```mermaid\nclassDiagram\n    class Singleton {\n        -instance: Singleton\n        -__init__()\n        +getInstance(): Singleton\n    }\n    Singleton --> Singleton : returns same instance\n```\n\n### Implementation in this code:\n{code_analysis}\n\n### Benefits:\nâœ… Controlled access to single instance\nâœ… Reduced namespace pollution\nâœ… Permits refinement of operations\n\n### Drawbacks:\nâŒ Can make unit testing difficult\nâŒ Violates Single Responsibility Principle\nâŒ Can hide dependencies\n\n### Alternative Approaches:\n1. Dependency Injection\n2. Module-level singleton\n3. Borg pattern\n''',\n            'observer': '''\n## Observer Pattern\n\n### What is it?\nThe Observer pattern defines a one-to-many dependency between objects so that when one object changes state, all dependents are notified.\n\n### When to use it?\n- Event handling systems\n- Model-View architectures\n- Distributed event handling\n\n### Visual Representation:\n```mermaid\nclassDiagram\n    class Subject {\n        +attach(Observer)\n        +detach(Observer)\n        +notify()\n    }\n    class Observer {\n        +update()\n    }\n    class ConcreteSubject {\n        -state\n        +getState()\n        +setState()\n    }\n    class ConcreteObserver {\n        -subject\n        +update()\n    }\n    Subject <|-- ConcreteSubject\n    Observer <|-- ConcreteObserver\n    ConcreteSubject --> Observer : notifies\n    ConcreteObserver --> ConcreteSubject : observes\n```\n\n### Implementation in this code:\n{code_analysis}\n\n### Real-world Example:\n```python\n# Newsletter subscription system\nclass Newsletter:\n    def __init__(self):\n        self._subscribers = []\n        self._latest_article = None\n    \n    def subscribe(self, subscriber):\n        self._subscribers.append(subscriber)\n    \n    def unsubscribe(self, subscriber):\n        self._subscribers.remove(subscriber)\n    \n    def publish_article(self, article):\n        self._latest_article = article\n        self._notify_subscribers()\n    \n    def _notify_subscribers(self):\n        for subscriber in self._subscribers:\n            subscriber.update(self._latest_article)\n\nclass EmailSubscriber:\n    def __init__(self, email):\n        self.email = email\n    \n    def update(self, article):\n        print(f\"Sending email to {self.email}: New article - {article}\")\n```\n'''\n        }\n        \n        return patterns.get(pattern_name, \"Pattern explanation not available\")\n```\n\n### 7. Common Pitfalls and Best Practices\n\nHighlight potential issues and improvements:\n\n**Code Review Insights**\n```python\ndef analyze_common_pitfalls(self, code):\n    \"\"\"\n    Identify common mistakes and suggest improvements\n    \"\"\"\n    issues = []\n    \n    # Check for common Python pitfalls\n    pitfall_patterns = [\n        {\n            'pattern': r'except:',\n            'issue': 'Bare except clause',\n            'severity': 'high',\n            'explanation': '''\n## âš ï¸ Bare Except Clause\n\n**Problem**: `except:` catches ALL exceptions, including system exits and keyboard interrupts.\n\n**Why it's bad**:\n- Hides programming errors\n- Makes debugging difficult\n- Can catch exceptions you didn't intend to handle\n\n**Better approach**:\n```python\n# Bad\ntry:\n    risky_operation()\nexcept:\n    print(\"Something went wrong\")\n\n# Good\ntry:\n    risky_operation()\nexcept (ValueError, TypeError) as e:\n    print(f\"Expected error: {e}\")\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise\n```\n'''\n        },\n        {\n            'pattern': r'def.*\\(\\s*\\):.*global',\n            'issue': 'Global variable usage',\n            'severity': 'medium',\n            'explanation': '''\n## âš ï¸ Global Variable Usage\n\n**Problem**: Using global variables makes code harder to test and reason about.\n\n**Better approaches**:\n1. Pass as parameter\n2. Use class attributes\n3. Use dependency injection\n4. Return values instead\n\n**Example refactor**:\n```python\n# Bad\ncount = 0\ndef increment():\n    global count\n    count += 1\n\n# Good\nclass Counter:\n    def __init__(self):\n        self.count = 0\n    \n    def increment(self):\n        self.count += 1\n        return self.count\n```\n'''\n        }\n    ]\n    \n    for pitfall in pitfall_patterns:\n        if re.search(pitfall['pattern'], code):\n            issues.append(pitfall)\n    \n    return issues\n```\n\n### 8. Learning Path Recommendations\n\nSuggest resources for deeper understanding:\n\n**Personalized Learning Path**\n```python\ndef generate_learning_path(self, analysis):\n    \"\"\"\n    Create personalized learning recommendations\n    \"\"\"\n    learning_path = {\n        'current_level': analysis['difficulty_level'],\n        'identified_gaps': [],\n        'recommended_topics': [],\n        'resources': []\n    }\n    \n    # Identify knowledge gaps\n    if 'async' in analysis['concepts'] and analysis['difficulty_level'] == 'beginner':\n        learning_path['identified_gaps'].append('Asynchronous programming fundamentals')\n        learning_path['recommended_topics'].extend([\n            'Event loops',\n            'Coroutines vs threads',\n            'Async/await syntax',\n            'Concurrent programming patterns'\n        ])\n    \n    # Add resources\n    learning_path['resources'] = [\n        {\n            'topic': 'Async Programming',\n            'type': 'tutorial',\n            'title': 'Async IO in Python: A Complete Walkthrough',\n            'url': 'https://realpython.com/async-io-python/',\n            'difficulty': 'intermediate',\n            'time_estimate': '45 minutes'\n        },\n        {\n            'topic': 'Design Patterns',\n            'type': 'book',\n            'title': 'Head First Design Patterns',\n            'difficulty': 'beginner-friendly',\n            'format': 'visual learning'\n        }\n    ]\n    \n    # Create structured learning plan\n    learning_path['structured_plan'] = f\"\"\"\n## Your Personalized Learning Path\n\n### Week 1-2: Fundamentals\n- Review basic concepts: {', '.join(learning_path['recommended_topics'][:2])}\n- Complete exercises on each topic\n- Build a small project using these concepts\n\n### Week 3-4: Applied Learning\n- Study the patterns in this codebase\n- Refactor a simple version yourself\n- Compare your approach with the original\n\n### Week 5-6: Advanced Topics\n- Explore edge cases and optimizations\n- Learn about alternative approaches\n- Contribute to open source projects using these patterns\n\n### Practice Projects:\n1. **Beginner**: {self._suggest_beginner_project(analysis)}\n2. **Intermediate**: {self._suggest_intermediate_project(analysis)}\n3. **Advanced**: {self._suggest_advanced_project(analysis)}\n\"\"\"\n    \n    return learning_path\n```\n\n## Output Format\n\n1. **Complexity Analysis**: Overview of code complexity and concepts used\n2. **Visual Diagrams**: Flow charts, class diagrams, and execution visualizations\n3. **Step-by-Step Breakdown**: Progressive explanation from simple to complex\n4. **Interactive Examples**: Runnable code samples to experiment with\n5. **Common Pitfalls**: Issues to avoid with explanations\n6. **Best Practices**: Improved approaches and patterns\n7. **Learning Resources**: Curated resources for deeper understanding\n8. **Practice Exercises**: Hands-on challenges to reinforce learning\n\nFocus on making complex code accessible through clear explanations, visual aids, and practical examples that build understanding progressively."
              },
              {
                "name": "/doc-generate",
                "description": null,
                "path": "plugins/code-documentation/commands/doc-generate.md",
                "frontmatter": null,
                "content": "# Automated Documentation Generation\n\nYou are a documentation expert specializing in creating comprehensive, maintainable documentation from code. Generate API docs, architecture diagrams, user guides, and technical references using AI-powered analysis and industry best practices.\n\n## Context\nThe user needs automated documentation generation that extracts information from code, creates clear explanations, and maintains consistency across documentation types. Focus on creating living documentation that stays synchronized with code.\n\n## Requirements\n$ARGUMENTS\n\n## How to Use This Tool\n\nThis tool provides both **concise instructions** (what to create) and **detailed reference examples** (how to create it). Structure:\n- **Instructions**: High-level guidance and documentation types to generate\n- **Reference Examples**: Complete implementation patterns to adapt and use as templates\n\n## Instructions\n\nGenerate comprehensive documentation by analyzing the codebase and creating the following artifacts:\n\n### 1. **API Documentation**\n- Extract endpoint definitions, parameters, and responses from code\n- Generate OpenAPI/Swagger specifications\n- Create interactive API documentation (Swagger UI, Redoc)\n- Include authentication, rate limiting, and error handling details\n\n### 2. **Architecture Documentation**\n- Create system architecture diagrams (Mermaid, PlantUML)\n- Document component relationships and data flows\n- Explain service dependencies and communication patterns\n- Include scalability and reliability considerations\n\n### 3. **Code Documentation**\n- Generate inline documentation and docstrings\n- Create README files with setup, usage, and contribution guidelines\n- Document configuration options and environment variables\n- Provide troubleshooting guides and code examples\n\n### 4. **User Documentation**\n- Write step-by-step user guides\n- Create getting started tutorials\n- Document common workflows and use cases\n- Include accessibility and localization notes\n\n### 5. **Documentation Automation**\n- Configure CI/CD pipelines for automatic doc generation\n- Set up documentation linting and validation\n- Implement documentation coverage checks\n- Automate deployment to hosting platforms\n\n### Quality Standards\n\nEnsure all generated documentation:\n- Is accurate and synchronized with current code\n- Uses consistent terminology and formatting\n- Includes practical examples and use cases\n- Is searchable and well-organized\n- Follows accessibility best practices\n\n## Reference Examples\n\n### Example 1: Code Analysis for Documentation\n\n**API Documentation Extraction**\n```python\nimport ast\nfrom typing import Dict, List\n\nclass APIDocExtractor:\n    def extract_endpoints(self, code_path):\n        \"\"\"Extract API endpoints and their documentation\"\"\"\n        endpoints = []\n\n        with open(code_path, 'r') as f:\n            tree = ast.parse(f.read())\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                for decorator in node.decorator_list:\n                    if self._is_route_decorator(decorator):\n                        endpoint = {\n                            'method': self._extract_method(decorator),\n                            'path': self._extract_path(decorator),\n                            'function': node.name,\n                            'docstring': ast.get_docstring(node),\n                            'parameters': self._extract_parameters(node),\n                            'returns': self._extract_returns(node)\n                        }\n                        endpoints.append(endpoint)\n        return endpoints\n\n    def _extract_parameters(self, func_node):\n        \"\"\"Extract function parameters with types\"\"\"\n        params = []\n        for arg in func_node.args.args:\n            param = {\n                'name': arg.arg,\n                'type': ast.unparse(arg.annotation) if arg.annotation else None,\n                'required': True\n            }\n            params.append(param)\n        return params\n```\n\n**Schema Extraction**\n```python\ndef extract_pydantic_schemas(file_path):\n    \"\"\"Extract Pydantic model definitions for API documentation\"\"\"\n    schemas = []\n\n    with open(file_path, 'r') as f:\n        tree = ast.parse(f.read())\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.ClassDef):\n            if any(base.id == 'BaseModel' for base in node.bases if hasattr(base, 'id')):\n                schema = {\n                    'name': node.name,\n                    'description': ast.get_docstring(node),\n                    'fields': []\n                }\n\n                for item in node.body:\n                    if isinstance(item, ast.AnnAssign):\n                        field = {\n                            'name': item.target.id,\n                            'type': ast.unparse(item.annotation),\n                            'required': item.value is None\n                        }\n                        schema['fields'].append(field)\n                schemas.append(schema)\n    return schemas\n```\n\n### Example 2: OpenAPI Specification Generation\n\n**OpenAPI Template**\n```yaml\nopenapi: 3.0.0\ninfo:\n  title: ${API_TITLE}\n  version: ${VERSION}\n  description: |\n    ${DESCRIPTION}\n\n    ## Authentication\n    ${AUTH_DESCRIPTION}\n\nservers:\n  - url: https://api.example.com/v1\n    description: Production server\n\nsecurity:\n  - bearerAuth: []\n\npaths:\n  /users:\n    get:\n      summary: List all users\n      operationId: listUsers\n      tags:\n        - Users\n      parameters:\n        - name: page\n          in: query\n          schema:\n            type: integer\n            default: 1\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            default: 20\n            maximum: 100\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/User'\n                  pagination:\n                    $ref: '#/components/schemas/Pagination'\n        '401':\n          $ref: '#/components/responses/Unauthorized'\n\ncomponents:\n  schemas:\n    User:\n      type: object\n      required:\n        - id\n        - email\n      properties:\n        id:\n          type: string\n          format: uuid\n        email:\n          type: string\n          format: email\n        name:\n          type: string\n        createdAt:\n          type: string\n          format: date-time\n```\n\n### Example 3: Architecture Diagrams\n\n**System Architecture (Mermaid)**\n```mermaid\ngraph TB\n    subgraph \"Frontend\"\n        UI[React UI]\n        Mobile[Mobile App]\n    end\n\n    subgraph \"API Gateway\"\n        Gateway[Kong/nginx]\n        Auth[Auth Service]\n    end\n\n    subgraph \"Microservices\"\n        UserService[User Service]\n        OrderService[Order Service]\n        PaymentService[Payment Service]\n    end\n\n    subgraph \"Data Layer\"\n        PostgresMain[(PostgreSQL)]\n        Redis[(Redis Cache)]\n        S3[S3 Storage]\n    end\n\n    UI --> Gateway\n    Mobile --> Gateway\n    Gateway --> Auth\n    Gateway --> UserService\n    Gateway --> OrderService\n    OrderService --> PaymentService\n    UserService --> PostgresMain\n    UserService --> Redis\n    OrderService --> PostgresMain\n```\n\n**Component Documentation**\n```markdown\n## User Service\n\n**Purpose**: Manages user accounts, authentication, and profiles\n\n**Technology Stack**:\n- Language: Python 3.11\n- Framework: FastAPI\n- Database: PostgreSQL\n- Cache: Redis\n- Authentication: JWT\n\n**API Endpoints**:\n- `POST /users` - Create new user\n- `GET /users/{id}` - Get user details\n- `PUT /users/{id}` - Update user\n- `POST /auth/login` - User login\n\n**Configuration**:\n```yaml\nuser_service:\n  port: 8001\n  database:\n    host: postgres.internal\n    name: users_db\n  jwt:\n    secret: ${JWT_SECRET}\n    expiry: 3600\n```\n```\n\n### Example 4: README Generation\n\n**README Template**\n```markdown\n# ${PROJECT_NAME}\n\n${BADGES}\n\n${SHORT_DESCRIPTION}\n\n## Features\n\n${FEATURES_LIST}\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8+\n- PostgreSQL 12+\n- Redis 6+\n\n### Using pip\n\n```bash\npip install ${PACKAGE_NAME}\n```\n\n### From source\n\n```bash\ngit clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git\ncd ${REPO_NAME}\npip install -e .\n```\n\n## Quick Start\n\n```python\n${QUICK_START_CODE}\n```\n\n## Configuration\n\n### Environment Variables\n\n| Variable | Description | Default | Required |\n|----------|-------------|---------|----------|\n| DATABASE_URL | PostgreSQL connection string | - | Yes |\n| REDIS_URL | Redis connection string | - | Yes |\n| SECRET_KEY | Application secret key | - | Yes |\n\n## Development\n\n```bash\n# Clone and setup\ngit clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git\ncd ${REPO_NAME}\npython -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Run tests\npytest\n\n# Start development server\npython manage.py runserver\n```\n\n## Testing\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=your_package\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the ${LICENSE} License - see the [LICENSE](LICENSE) file for details.\n```\n\n### Example 5: Function Documentation Generator\n\n```python\nimport inspect\n\ndef generate_function_docs(func):\n    \"\"\"Generate comprehensive documentation for a function\"\"\"\n    sig = inspect.signature(func)\n    params = []\n    args_doc = []\n\n    for param_name, param in sig.parameters.items():\n        param_str = param_name\n        if param.annotation != param.empty:\n            param_str += f\": {param.annotation.__name__}\"\n        if param.default != param.empty:\n            param_str += f\" = {param.default}\"\n        params.append(param_str)\n        args_doc.append(f\"{param_name}: Description of {param_name}\")\n\n    return_type = \"\"\n    if sig.return_annotation != sig.empty:\n        return_type = f\" -> {sig.return_annotation.__name__}\"\n\n    doc_template = f'''\ndef {func.__name__}({\", \".join(params)}){return_type}:\n    \"\"\"\n    Brief description of {func.__name__}\n\n    Args:\n        {chr(10).join(f\"        {arg}\" for arg in args_doc)}\n\n    Returns:\n        Description of return value\n\n    Examples:\n        >>> {func.__name__}(example_input)\n        expected_output\n    \"\"\"\n'''\n    return doc_template\n```\n\n### Example 6: User Guide Template\n\n```markdown\n# User Guide\n\n## Getting Started\n\n### Creating Your First ${FEATURE}\n\n1. **Navigate to the Dashboard**\n\n   Click on the ${FEATURE} tab in the main navigation menu.\n\n2. **Click \"Create New\"**\n\n   You'll find the \"Create New\" button in the top right corner.\n\n3. **Fill in the Details**\n\n   - **Name**: Enter a descriptive name\n   - **Description**: Add optional details\n   - **Settings**: Configure as needed\n\n4. **Save Your Changes**\n\n   Click \"Save\" to create your ${FEATURE}.\n\n### Common Tasks\n\n#### Editing ${FEATURE}\n\n1. Find your ${FEATURE} in the list\n2. Click the \"Edit\" button\n3. Make your changes\n4. Click \"Save\"\n\n#### Deleting ${FEATURE}\n\n> âš ï¸ **Warning**: Deletion is permanent and cannot be undone.\n\n1. Find your ${FEATURE} in the list\n2. Click the \"Delete\" button\n3. Confirm the deletion\n\n### Troubleshooting\n\n| Error | Meaning | Solution |\n|-------|---------|----------|\n| \"Name required\" | The name field is empty | Enter a name |\n| \"Permission denied\" | You don't have access | Contact admin |\n| \"Server error\" | Technical issue | Try again later |\n```\n\n### Example 7: Interactive API Playground\n\n**Swagger UI Setup**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>API Documentation</title>\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui.css\">\n</head>\n<body>\n    <div id=\"swagger-ui\"></div>\n\n    <script src=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui-bundle.js\"></script>\n    <script>\n        window.onload = function() {\n            SwaggerUIBundle({\n                url: \"/api/openapi.json\",\n                dom_id: '#swagger-ui',\n                deepLinking: true,\n                presets: [SwaggerUIBundle.presets.apis],\n                layout: \"StandaloneLayout\"\n            });\n        }\n    </script>\n</body>\n</html>\n```\n\n**Code Examples Generator**\n```python\ndef generate_code_examples(endpoint):\n    \"\"\"Generate code examples for API endpoints in multiple languages\"\"\"\n    examples = {}\n\n    # Python\n    examples['python'] = f'''\nimport requests\n\nurl = \"https://api.example.com{endpoint['path']}\"\nheaders = {{\"Authorization\": \"Bearer YOUR_API_KEY\"}}\n\nresponse = requests.{endpoint['method'].lower()}(url, headers=headers)\nprint(response.json())\n'''\n\n    # JavaScript\n    examples['javascript'] = f'''\nconst response = await fetch('https://api.example.com{endpoint['path']}', {{\n    method: '{endpoint['method']}',\n    headers: {{'Authorization': 'Bearer YOUR_API_KEY'}}\n}});\n\nconst data = await response.json();\nconsole.log(data);\n'''\n\n    # cURL\n    examples['curl'] = f'''\ncurl -X {endpoint['method']} https://api.example.com{endpoint['path']} \\\\\n    -H \"Authorization: Bearer YOUR_API_KEY\"\n'''\n\n    return examples\n```\n\n### Example 8: Documentation CI/CD\n\n**GitHub Actions Workflow**\n```yaml\nname: Generate Documentation\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'src/**'\n      - 'api/**'\n\njobs:\n  generate-docs:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements-docs.txt\n        npm install -g @redocly/cli\n\n    - name: Generate API documentation\n      run: |\n        python scripts/generate_openapi.py > docs/api/openapi.json\n        redocly build-docs docs/api/openapi.json -o docs/api/index.html\n\n    - name: Generate code documentation\n      run: sphinx-build -b html docs/source docs/build\n\n    - name: Deploy to GitHub Pages\n      uses: peaceiris/actions-gh-pages@v3\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n        publish_dir: ./docs/build\n```\n\n### Example 9: Documentation Coverage Validation\n\n```python\nimport ast\nimport glob\n\nclass DocCoverage:\n    def check_coverage(self, codebase_path):\n        \"\"\"Check documentation coverage for codebase\"\"\"\n        results = {\n            'total_functions': 0,\n            'documented_functions': 0,\n            'total_classes': 0,\n            'documented_classes': 0,\n            'missing_docs': []\n        }\n\n        for file_path in glob.glob(f\"{codebase_path}/**/*.py\", recursive=True):\n            module = ast.parse(open(file_path).read())\n\n            for node in ast.walk(module):\n                if isinstance(node, ast.FunctionDef):\n                    results['total_functions'] += 1\n                    if ast.get_docstring(node):\n                        results['documented_functions'] += 1\n                    else:\n                        results['missing_docs'].append({\n                            'type': 'function',\n                            'name': node.name,\n                            'file': file_path,\n                            'line': node.lineno\n                        })\n\n                elif isinstance(node, ast.ClassDef):\n                    results['total_classes'] += 1\n                    if ast.get_docstring(node):\n                        results['documented_classes'] += 1\n                    else:\n                        results['missing_docs'].append({\n                            'type': 'class',\n                            'name': node.name,\n                            'file': file_path,\n                            'line': node.lineno\n                        })\n\n        # Calculate coverage percentages\n        results['function_coverage'] = (\n            results['documented_functions'] / results['total_functions'] * 100\n            if results['total_functions'] > 0 else 100\n        )\n        results['class_coverage'] = (\n            results['documented_classes'] / results['total_classes'] * 100\n            if results['total_classes'] > 0 else 100\n        )\n\n        return results\n```\n\n## Output Format\n\n1. **API Documentation**: OpenAPI spec with interactive playground\n2. **Architecture Diagrams**: System, sequence, and component diagrams\n3. **Code Documentation**: Inline docs, docstrings, and type hints\n4. **User Guides**: Step-by-step tutorials\n5. **Developer Guides**: Setup, contribution, and API usage guides\n6. **Reference Documentation**: Complete API reference with examples\n7. **Documentation Site**: Deployed static site with search functionality\n\nFocus on creating documentation that is accurate, comprehensive, and easy to maintain alongside code changes.\n"
              }
            ],
            "skills": []
          },
          {
            "name": "comprehensive-review",
            "description": "Comprehensive code review with architecture and security",
            "source": "./plugins/comprehensive-review",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install comprehensive-review@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/full-review",
                "description": null,
                "path": "plugins/comprehensive-review/commands/full-review.md",
                "frontmatter": null,
                "content": "Orchestrate comprehensive multi-dimensional code review using specialized review agents\n\n[Extended thinking: This workflow performs an exhaustive code review by orchestrating multiple specialized agents in sequential phases. Each phase builds upon previous findings to create a comprehensive review that covers code quality, security, performance, testing, documentation, and best practices. The workflow integrates modern AI-assisted review tools, static analysis, security scanning, and automated quality metrics. Results are consolidated into actionable feedback with clear prioritization and remediation guidance. The phased approach ensures thorough coverage while maintaining efficiency through parallel agent execution where appropriate.]\n\n## Review Configuration Options\n\n- **--security-focus**: Prioritize security vulnerabilities and OWASP compliance\n- **--performance-critical**: Emphasize performance bottlenecks and scalability issues\n- **--tdd-review**: Include TDD compliance and test-first verification\n- **--ai-assisted**: Enable AI-powered review tools (Copilot, Codium, Bito)\n- **--strict-mode**: Fail review on any critical issues found\n- **--metrics-report**: Generate detailed quality metrics dashboard\n- **--framework [name]**: Apply framework-specific best practices (React, Spring, Django, etc.)\n\n## Phase 1: Code Quality & Architecture Review\n\nUse Task tool to orchestrate quality and architecture agents in parallel:\n\n### 1A. Code Quality Analysis\n- Use Task tool with subagent_type=\"code-reviewer\"\n- Prompt: \"Perform comprehensive code quality review for: $ARGUMENTS. Analyze code complexity, maintainability index, technical debt, code duplication, naming conventions, and adherence to Clean Code principles. Integrate with SonarQube, CodeQL, and Semgrep for static analysis. Check for code smells, anti-patterns, and violations of SOLID principles. Generate cyclomatic complexity metrics and identify refactoring opportunities.\"\n- Expected output: Quality metrics, code smell inventory, refactoring recommendations\n- Context: Initial codebase analysis, no dependencies on other phases\n\n### 1B. Architecture & Design Review\n- Use Task tool with subagent_type=\"architect-review\"\n- Prompt: \"Review architectural design patterns and structural integrity in: $ARGUMENTS. Evaluate microservices boundaries, API design, database schema, dependency management, and adherence to Domain-Driven Design principles. Check for circular dependencies, inappropriate coupling, missing abstractions, and architectural drift. Verify compliance with enterprise architecture standards and cloud-native patterns.\"\n- Expected output: Architecture assessment, design pattern analysis, structural recommendations\n- Context: Runs parallel with code quality analysis\n\n## Phase 2: Security & Performance Review\n\nUse Task tool with security and performance agents, incorporating Phase 1 findings:\n\n### 2A. Security Vulnerability Assessment\n- Use Task tool with subagent_type=\"security-auditor\"\n- Prompt: \"Execute comprehensive security audit on: $ARGUMENTS. Perform OWASP Top 10 analysis, dependency vulnerability scanning with Snyk/Trivy, secrets detection with GitLeaks, input validation review, authentication/authorization assessment, and cryptographic implementation review. Include findings from Phase 1 architecture review: {phase1_architecture_context}. Check for SQL injection, XSS, CSRF, insecure deserialization, and configuration security issues.\"\n- Expected output: Vulnerability report, CVE list, security risk matrix, remediation steps\n- Context: Incorporates architectural vulnerabilities identified in Phase 1B\n\n### 2B. Performance & Scalability Analysis\n- Use Task tool with subagent_type=\"application-performance::performance-engineer\"\n- Prompt: \"Conduct performance analysis and scalability assessment for: $ARGUMENTS. Profile code for CPU/memory hotspots, analyze database query performance, review caching strategies, identify N+1 problems, assess connection pooling, and evaluate asynchronous processing patterns. Consider architectural findings from Phase 1: {phase1_architecture_context}. Check for memory leaks, resource contention, and bottlenecks under load.\"\n- Expected output: Performance metrics, bottleneck analysis, optimization recommendations\n- Context: Uses architecture insights to identify systemic performance issues\n\n## Phase 3: Testing & Documentation Review\n\nUse Task tool for test and documentation quality assessment:\n\n### 3A. Test Coverage & Quality Analysis\n- Use Task tool with subagent_type=\"unit-testing::test-automator\"\n- Prompt: \"Evaluate testing strategy and implementation for: $ARGUMENTS. Analyze unit test coverage, integration test completeness, end-to-end test scenarios, test pyramid adherence, and test maintainability. Review test quality metrics including assertion density, test isolation, mock usage, and flakiness. Consider security and performance test requirements from Phase 2: {phase2_security_context}, {phase2_performance_context}. Verify TDD practices if --tdd-review flag is set.\"\n- Expected output: Coverage report, test quality metrics, testing gap analysis\n- Context: Incorporates security and performance testing requirements from Phase 2\n\n### 3B. Documentation & API Specification Review\n- Use Task tool with subagent_type=\"code-documentation::docs-architect\"\n- Prompt: \"Review documentation completeness and quality for: $ARGUMENTS. Assess inline code documentation, API documentation (OpenAPI/Swagger), architecture decision records (ADRs), README completeness, deployment guides, and runbooks. Verify documentation reflects actual implementation based on all previous phase findings: {phase1_context}, {phase2_context}. Check for outdated documentation, missing examples, and unclear explanations.\"\n- Expected output: Documentation coverage report, inconsistency list, improvement recommendations\n- Context: Cross-references all previous findings to ensure documentation accuracy\n\n## Phase 4: Best Practices & Standards Compliance\n\nUse Task tool to verify framework-specific and industry best practices:\n\n### 4A. Framework & Language Best Practices\n- Use Task tool with subagent_type=\"framework-migration::legacy-modernizer\"\n- Prompt: \"Verify adherence to framework and language best practices for: $ARGUMENTS. Check modern JavaScript/TypeScript patterns, React hooks best practices, Python PEP compliance, Java enterprise patterns, Go idiomatic code, or framework-specific conventions (based on --framework flag). Review package management, build configuration, environment handling, and deployment practices. Include all quality issues from previous phases: {all_previous_contexts}.\"\n- Expected output: Best practices compliance report, modernization recommendations\n- Context: Synthesizes all previous findings for framework-specific guidance\n\n### 4B. CI/CD & DevOps Practices Review\n- Use Task tool with subagent_type=\"cicd-automation::deployment-engineer\"\n- Prompt: \"Review CI/CD pipeline and DevOps practices for: $ARGUMENTS. Evaluate build automation, test automation integration, deployment strategies (blue-green, canary), infrastructure as code, monitoring/observability setup, and incident response procedures. Assess pipeline security, artifact management, and rollback capabilities. Consider all issues identified in previous phases that impact deployment: {all_critical_issues}.\"\n- Expected output: Pipeline assessment, DevOps maturity evaluation, automation recommendations\n- Context: Focuses on operationalizing fixes for all identified issues\n\n## Consolidated Report Generation\n\nCompile all phase outputs into comprehensive review report:\n\n### Critical Issues (P0 - Must Fix Immediately)\n- Security vulnerabilities with CVSS > 7.0\n- Data loss or corruption risks\n- Authentication/authorization bypasses\n- Production stability threats\n- Compliance violations (GDPR, PCI DSS, SOC2)\n\n### High Priority (P1 - Fix Before Next Release)\n- Performance bottlenecks impacting user experience\n- Missing critical test coverage\n- Architectural anti-patterns causing technical debt\n- Outdated dependencies with known vulnerabilities\n- Code quality issues affecting maintainability\n\n### Medium Priority (P2 - Plan for Next Sprint)\n- Non-critical performance optimizations\n- Documentation gaps and inconsistencies\n- Code refactoring opportunities\n- Test quality improvements\n- DevOps automation enhancements\n\n### Low Priority (P3 - Track in Backlog)\n- Style guide violations\n- Minor code smell issues\n- Nice-to-have documentation updates\n- Cosmetic improvements\n\n## Success Criteria\n\nReview is considered successful when:\n- All critical security vulnerabilities are identified and documented\n- Performance bottlenecks are profiled with remediation paths\n- Test coverage gaps are mapped with priority recommendations\n- Architecture risks are assessed with mitigation strategies\n- Documentation reflects actual implementation state\n- Framework best practices compliance is verified\n- CI/CD pipeline supports safe deployment of reviewed code\n- Clear, actionable feedback is provided for all findings\n- Metrics dashboard shows improvement trends\n- Team has clear prioritized action plan for remediation\n\nTarget: $ARGUMENTS"
              },
              {
                "name": "/pr-enhance",
                "description": null,
                "path": "plugins/comprehensive-review/commands/pr-enhance.md",
                "frontmatter": null,
                "content": "# Pull Request Enhancement\n\nYou are a PR optimization expert specializing in creating high-quality pull requests that facilitate efficient code reviews. Generate comprehensive PR descriptions, automate review processes, and ensure PRs follow best practices for clarity, size, and reviewability.\n\n## Context\nThe user needs to create or improve pull requests with detailed descriptions, proper documentation, test coverage analysis, and review facilitation. Focus on making PRs that are easy to review, well-documented, and include all necessary context.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. PR Analysis\n\nAnalyze the changes and generate insights:\n\n**Change Summary Generator**\n```python\nimport subprocess\nimport re\nfrom collections import defaultdict\n\nclass PRAnalyzer:\n    def analyze_changes(self, base_branch='main'):\n        \"\"\"\n        Analyze changes between current branch and base\n        \"\"\"\n        analysis = {\n            'files_changed': self._get_changed_files(base_branch),\n            'change_statistics': self._get_change_stats(base_branch),\n            'change_categories': self._categorize_changes(base_branch),\n            'potential_impacts': self._assess_impacts(base_branch),\n            'dependencies_affected': self._check_dependencies(base_branch)\n        }\n        \n        return analysis\n    \n    def _get_changed_files(self, base_branch):\n        \"\"\"Get list of changed files with statistics\"\"\"\n        cmd = f\"git diff --name-status {base_branch}...HEAD\"\n        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n        \n        files = []\n        for line in result.stdout.strip().split('\\n'):\n            if line:\n                status, filename = line.split('\\t', 1)\n                files.append({\n                    'filename': filename,\n                    'status': self._parse_status(status),\n                    'category': self._categorize_file(filename)\n                })\n        \n        return files\n    \n    def _get_change_stats(self, base_branch):\n        \"\"\"Get detailed change statistics\"\"\"\n        cmd = f\"git diff --shortstat {base_branch}...HEAD\"\n        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n        \n        # Parse output like: \"10 files changed, 450 insertions(+), 123 deletions(-)\"\n        stats_pattern = r'(\\d+) files? changed(?:, (\\d+) insertions?\\(\\+\\))?(?:, (\\d+) deletions?\\(-\\))?'\n        match = re.search(stats_pattern, result.stdout)\n        \n        if match:\n            files, insertions, deletions = match.groups()\n            return {\n                'files_changed': int(files),\n                'insertions': int(insertions or 0),\n                'deletions': int(deletions or 0),\n                'net_change': int(insertions or 0) - int(deletions or 0)\n            }\n        \n        return {'files_changed': 0, 'insertions': 0, 'deletions': 0, 'net_change': 0}\n    \n    def _categorize_file(self, filename):\n        \"\"\"Categorize file by type\"\"\"\n        categories = {\n            'source': ['.js', '.ts', '.py', '.java', '.go', '.rs'],\n            'test': ['test', 'spec', '.test.', '.spec.'],\n            'config': ['config', '.json', '.yml', '.yaml', '.toml'],\n            'docs': ['.md', 'README', 'CHANGELOG', '.rst'],\n            'styles': ['.css', '.scss', '.less'],\n            'build': ['Makefile', 'Dockerfile', '.gradle', 'pom.xml']\n        }\n        \n        for category, patterns in categories.items():\n            if any(pattern in filename for pattern in patterns):\n                return category\n        \n        return 'other'\n```\n\n### 2. PR Description Generation\n\nCreate comprehensive PR descriptions:\n\n**Description Template Generator**\n```python\ndef generate_pr_description(analysis, commits):\n    \"\"\"\n    Generate detailed PR description from analysis\n    \"\"\"\n    description = f\"\"\"\n## Summary\n\n{generate_summary(analysis, commits)}\n\n## What Changed\n\n{generate_change_list(analysis)}\n\n## Why These Changes\n\n{extract_why_from_commits(commits)}\n\n## Type of Change\n\n{determine_change_types(analysis)}\n\n## How Has This Been Tested?\n\n{generate_test_section(analysis)}\n\n## Visual Changes\n\n{generate_visual_section(analysis)}\n\n## Performance Impact\n\n{analyze_performance_impact(analysis)}\n\n## Breaking Changes\n\n{identify_breaking_changes(analysis)}\n\n## Dependencies\n\n{list_dependency_changes(analysis)}\n\n## Checklist\n\n{generate_review_checklist(analysis)}\n\n## Additional Notes\n\n{generate_additional_notes(analysis)}\n\"\"\"\n    return description\n\ndef generate_summary(analysis, commits):\n    \"\"\"Generate executive summary\"\"\"\n    stats = analysis['change_statistics']\n    \n    # Extract main purpose from commits\n    main_purpose = extract_main_purpose(commits)\n    \n    summary = f\"\"\"\nThis PR {main_purpose}.\n\n**Impact**: {stats['files_changed']} files changed ({stats['insertions']} additions, {stats['deletions']} deletions)\n**Risk Level**: {calculate_risk_level(analysis)}\n**Review Time**: ~{estimate_review_time(stats)} minutes\n\"\"\"\n    return summary\n\ndef generate_change_list(analysis):\n    \"\"\"Generate categorized change list\"\"\"\n    changes_by_category = defaultdict(list)\n    \n    for file in analysis['files_changed']:\n        changes_by_category[file['category']].append(file)\n    \n    change_list = \"\"\n    icons = {\n        'source': 'ðŸ”§',\n        'test': 'âœ…',\n        'docs': 'ðŸ“',\n        'config': 'âš™ï¸',\n        'styles': 'ðŸŽ¨',\n        'build': 'ðŸ—ï¸',\n        'other': 'ðŸ“'\n    }\n    \n    for category, files in changes_by_category.items():\n        change_list += f\"\\n### {icons.get(category, 'ðŸ“')} {category.title()} Changes\\n\"\n        for file in files[:10]:  # Limit to 10 files per category\n            change_list += f\"- {file['status']}: `{file['filename']}`\\n\"\n        if len(files) > 10:\n            change_list += f\"- ...and {len(files) - 10} more\\n\"\n    \n    return change_list\n```\n\n### 3. Review Checklist Generation\n\nCreate automated review checklists:\n\n**Smart Checklist Generator**\n```python\ndef generate_review_checklist(analysis):\n    \"\"\"\n    Generate context-aware review checklist\n    \"\"\"\n    checklist = [\"## Review Checklist\\n\"]\n    \n    # General items\n    general_items = [\n        \"Code follows project style guidelines\",\n        \"Self-review completed\",\n        \"Comments added for complex logic\",\n        \"No debugging code left\",\n        \"No sensitive data exposed\"\n    ]\n    \n    # Add general items\n    checklist.append(\"### General\")\n    for item in general_items:\n        checklist.append(f\"- [ ] {item}\")\n    \n    # File-specific checks\n    file_types = {file['category'] for file in analysis['files_changed']}\n    \n    if 'source' in file_types:\n        checklist.append(\"\\n### Code Quality\")\n        checklist.extend([\n            \"- [ ] No code duplication\",\n            \"- [ ] Functions are focused and small\",\n            \"- [ ] Variable names are descriptive\",\n            \"- [ ] Error handling is comprehensive\",\n            \"- [ ] No performance bottlenecks introduced\"\n        ])\n    \n    if 'test' in file_types:\n        checklist.append(\"\\n### Testing\")\n        checklist.extend([\n            \"- [ ] All new code is covered by tests\",\n            \"- [ ] Tests are meaningful and not just for coverage\",\n            \"- [ ] Edge cases are tested\",\n            \"- [ ] Tests follow AAA pattern (Arrange, Act, Assert)\",\n            \"- [ ] No flaky tests introduced\"\n        ])\n    \n    if 'config' in file_types:\n        checklist.append(\"\\n### Configuration\")\n        checklist.extend([\n            \"- [ ] No hardcoded values\",\n            \"- [ ] Environment variables documented\",\n            \"- [ ] Backwards compatibility maintained\",\n            \"- [ ] Security implications reviewed\",\n            \"- [ ] Default values are sensible\"\n        ])\n    \n    if 'docs' in file_types:\n        checklist.append(\"\\n### Documentation\")\n        checklist.extend([\n            \"- [ ] Documentation is clear and accurate\",\n            \"- [ ] Examples are provided where helpful\",\n            \"- [ ] API changes are documented\",\n            \"- [ ] README updated if necessary\",\n            \"- [ ] Changelog updated\"\n        ])\n    \n    # Security checks\n    if has_security_implications(analysis):\n        checklist.append(\"\\n### Security\")\n        checklist.extend([\n            \"- [ ] No SQL injection vulnerabilities\",\n            \"- [ ] Input validation implemented\",\n            \"- [ ] Authentication/authorization correct\",\n            \"- [ ] No sensitive data in logs\",\n            \"- [ ] Dependencies are secure\"\n        ])\n    \n    return '\\n'.join(checklist)\n```\n\n### 4. Code Review Automation\n\nAutomate common review tasks:\n\n**Automated Review Bot**\n```python\nclass ReviewBot:\n    def perform_automated_checks(self, pr_diff):\n        \"\"\"\n        Perform automated code review checks\n        \"\"\"\n        findings = []\n        \n        # Check for common issues\n        checks = [\n            self._check_console_logs,\n            self._check_commented_code,\n            self._check_large_functions,\n            self._check_todo_comments,\n            self._check_hardcoded_values,\n            self._check_missing_error_handling,\n            self._check_security_issues\n        ]\n        \n        for check in checks:\n            findings.extend(check(pr_diff))\n        \n        return findings\n    \n    def _check_console_logs(self, diff):\n        \"\"\"Check for console.log statements\"\"\"\n        findings = []\n        pattern = r'\\+.*console\\.(log|debug|info|warn|error)'\n        \n        for file, content in diff.items():\n            matches = re.finditer(pattern, content, re.MULTILINE)\n            for match in matches:\n                findings.append({\n                    'type': 'warning',\n                    'file': file,\n                    'line': self._get_line_number(match, content),\n                    'message': 'Console statement found - remove before merging',\n                    'suggestion': 'Use proper logging framework instead'\n                })\n        \n        return findings\n    \n    def _check_large_functions(self, diff):\n        \"\"\"Check for functions that are too large\"\"\"\n        findings = []\n        \n        # Simple heuristic: count lines between function start and end\n        for file, content in diff.items():\n            if file.endswith(('.js', '.ts', '.py')):\n                functions = self._extract_functions(content)\n                for func in functions:\n                    if func['lines'] > 50:\n                        findings.append({\n                            'type': 'suggestion',\n                            'file': file,\n                            'line': func['start_line'],\n                            'message': f\"Function '{func['name']}' is {func['lines']} lines long\",\n                            'suggestion': 'Consider breaking into smaller functions'\n                        })\n        \n        return findings\n```\n\n### 5. PR Size Optimization\n\nHelp split large PRs:\n\n**PR Splitter Suggestions**\n```python\ndef suggest_pr_splits(analysis):\n    \"\"\"\n    Suggest how to split large PRs\n    \"\"\"\n    stats = analysis['change_statistics']\n    \n    # Check if PR is too large\n    if stats['files_changed'] > 20 or stats['insertions'] + stats['deletions'] > 1000:\n        suggestions = analyze_split_opportunities(analysis)\n        \n        return f\"\"\"\n## âš ï¸ Large PR Detected\n\nThis PR changes {stats['files_changed']} files with {stats['insertions'] + stats['deletions']} total changes.\nLarge PRs are harder to review and more likely to introduce bugs.\n\n### Suggested Splits:\n\n{format_split_suggestions(suggestions)}\n\n### How to Split:\n\n1. Create feature branch from current branch\n2. Cherry-pick commits for first logical unit\n3. Create PR for first unit\n4. Repeat for remaining units\n\n```bash\n# Example split workflow\ngit checkout -b feature/part-1\ngit cherry-pick <commit-hashes-for-part-1>\ngit push origin feature/part-1\n# Create PR for part 1\n\ngit checkout -b feature/part-2\ngit cherry-pick <commit-hashes-for-part-2>\ngit push origin feature/part-2\n# Create PR for part 2\n```\n\"\"\"\n    \n    return \"\"\n\ndef analyze_split_opportunities(analysis):\n    \"\"\"Find logical units for splitting\"\"\"\n    suggestions = []\n    \n    # Group by feature areas\n    feature_groups = defaultdict(list)\n    for file in analysis['files_changed']:\n        feature = extract_feature_area(file['filename'])\n        feature_groups[feature].append(file)\n    \n    # Suggest splits\n    for feature, files in feature_groups.items():\n        if len(files) >= 5:\n            suggestions.append({\n                'name': f\"{feature} changes\",\n                'files': files,\n                'reason': f\"Isolated changes to {feature} feature\"\n            })\n    \n    return suggestions\n```\n\n### 6. Visual Diff Enhancement\n\nGenerate visual representations:\n\n**Mermaid Diagram Generator**\n```python\ndef generate_architecture_diff(analysis):\n    \"\"\"\n    Generate diagram showing architectural changes\n    \"\"\"\n    if has_architectural_changes(analysis):\n        return f\"\"\"\n## Architecture Changes\n\n```mermaid\ngraph LR\n    subgraph \"Before\"\n        A1[Component A] --> B1[Component B]\n        B1 --> C1[Database]\n    end\n    \n    subgraph \"After\"\n        A2[Component A] --> B2[Component B]\n        B2 --> C2[Database]\n        B2 --> D2[New Cache Layer]\n        A2 --> E2[New API Gateway]\n    end\n    \n    style D2 fill:#90EE90\n    style E2 fill:#90EE90\n```\n\n### Key Changes:\n1. Added caching layer for performance\n2. Introduced API gateway for better routing\n3. Refactored component communication\n\"\"\"\n    return \"\"\n```\n\n### 7. Test Coverage Report\n\nInclude test coverage analysis:\n\n**Coverage Report Generator**\n```python\ndef generate_coverage_report(base_branch='main'):\n    \"\"\"\n    Generate test coverage comparison\n    \"\"\"\n    # Get coverage before and after\n    before_coverage = get_coverage_for_branch(base_branch)\n    after_coverage = get_coverage_for_branch('HEAD')\n    \n    coverage_diff = after_coverage - before_coverage\n    \n    report = f\"\"\"\n## Test Coverage\n\n| Metric | Before | After | Change |\n|--------|--------|-------|--------|\n| Lines | {before_coverage['lines']:.1f}% | {after_coverage['lines']:.1f}% | {format_diff(coverage_diff['lines'])} |\n| Functions | {before_coverage['functions']:.1f}% | {after_coverage['functions']:.1f}% | {format_diff(coverage_diff['functions'])} |\n| Branches | {before_coverage['branches']:.1f}% | {after_coverage['branches']:.1f}% | {format_diff(coverage_diff['branches'])} |\n\n### Uncovered Files\n\"\"\"\n    \n    # List files with low coverage\n    for file in get_low_coverage_files():\n        report += f\"- `{file['name']}`: {file['coverage']:.1f}% coverage\\n\"\n    \n    return report\n\ndef format_diff(value):\n    \"\"\"Format coverage difference\"\"\"\n    if value > 0:\n        return f\"<span style='color: green'>+{value:.1f}%</span> âœ…\"\n    elif value < 0:\n        return f\"<span style='color: red'>{value:.1f}%</span> âš ï¸\"\n    else:\n        return \"No change\"\n```\n\n### 8. Risk Assessment\n\nEvaluate PR risk:\n\n**Risk Calculator**\n```python\ndef calculate_pr_risk(analysis):\n    \"\"\"\n    Calculate risk score for PR\n    \"\"\"\n    risk_factors = {\n        'size': calculate_size_risk(analysis),\n        'complexity': calculate_complexity_risk(analysis),\n        'test_coverage': calculate_test_risk(analysis),\n        'dependencies': calculate_dependency_risk(analysis),\n        'security': calculate_security_risk(analysis)\n    }\n    \n    overall_risk = sum(risk_factors.values()) / len(risk_factors)\n    \n    risk_report = f\"\"\"\n## Risk Assessment\n\n**Overall Risk Level**: {get_risk_level(overall_risk)} ({overall_risk:.1f}/10)\n\n### Risk Factors\n\n| Factor | Score | Details |\n|--------|-------|---------|\n| Size | {risk_factors['size']:.1f}/10 | {get_size_details(analysis)} |\n| Complexity | {risk_factors['complexity']:.1f}/10 | {get_complexity_details(analysis)} |\n| Test Coverage | {risk_factors['test_coverage']:.1f}/10 | {get_test_details(analysis)} |\n| Dependencies | {risk_factors['dependencies']:.1f}/10 | {get_dependency_details(analysis)} |\n| Security | {risk_factors['security']:.1f}/10 | {get_security_details(analysis)} |\n\n### Mitigation Strategies\n\n{generate_mitigation_strategies(risk_factors)}\n\"\"\"\n    \n    return risk_report\n\ndef get_risk_level(score):\n    \"\"\"Convert score to risk level\"\"\"\n    if score < 3:\n        return \"ðŸŸ¢ Low\"\n    elif score < 6:\n        return \"ðŸŸ¡ Medium\"\n    elif score < 8:\n        return \"ðŸŸ  High\"\n    else:\n        return \"ðŸ”´ Critical\"\n```\n\n### 9. PR Templates\n\nGenerate context-specific templates:\n\n```python\ndef generate_pr_template(pr_type, analysis):\n    \"\"\"\n    Generate PR template based on type\n    \"\"\"\n    templates = {\n        'feature': f\"\"\"\n## Feature: {extract_feature_name(analysis)}\n\n### Description\n{generate_feature_description(analysis)}\n\n### User Story\nAs a [user type]\nI want [feature]\nSo that [benefit]\n\n### Acceptance Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n- [ ] Criterion 3\n\n### Demo\n[Link to demo or screenshots]\n\n### Technical Implementation\n{generate_technical_summary(analysis)}\n\n### Testing Strategy\n{generate_test_strategy(analysis)}\n\"\"\",\n        'bugfix': f\"\"\"\n## Bug Fix: {extract_bug_description(analysis)}\n\n### Issue\n- **Reported in**: #[issue-number]\n- **Severity**: {determine_severity(analysis)}\n- **Affected versions**: {get_affected_versions(analysis)}\n\n### Root Cause\n{analyze_root_cause(analysis)}\n\n### Solution\n{describe_solution(analysis)}\n\n### Testing\n- [ ] Bug is reproducible before fix\n- [ ] Bug is resolved after fix\n- [ ] No regressions introduced\n- [ ] Edge cases tested\n\n### Verification Steps\n1. Step to reproduce original issue\n2. Apply this fix\n3. Verify issue is resolved\n\"\"\",\n        'refactor': f\"\"\"\n## Refactoring: {extract_refactor_scope(analysis)}\n\n### Motivation\n{describe_refactor_motivation(analysis)}\n\n### Changes Made\n{list_refactor_changes(analysis)}\n\n### Benefits\n- Improved {list_improvements(analysis)}\n- Reduced {list_reductions(analysis)}\n\n### Compatibility\n- [ ] No breaking changes\n- [ ] API remains unchanged\n- [ ] Performance maintained or improved\n\n### Metrics\n| Metric | Before | After |\n|--------|--------|-------|\n| Complexity | X | Y |\n| Test Coverage | X% | Y% |\n| Performance | Xms | Yms |\n\"\"\"\n    }\n    \n    return templates.get(pr_type, templates['feature'])\n```\n\n### 10. Review Response Templates\n\nHelp with review responses:\n\n```python\nreview_response_templates = {\n    'acknowledge_feedback': \"\"\"\nThank you for the thorough review! I'll address these points.\n\"\"\",\n    \n    'explain_decision': \"\"\"\nGreat question! I chose this approach because:\n1. [Reason 1]\n2. [Reason 2]\n\nAlternative approaches considered:\n- [Alternative 1]: [Why not chosen]\n- [Alternative 2]: [Why not chosen]\n\nHappy to discuss further if you have concerns.\n\"\"\",\n    \n    'request_clarification': \"\"\"\nThanks for the feedback. Could you clarify what you mean by [specific point]?\nI want to make sure I understand your concern correctly before making changes.\n\"\"\",\n    \n    'disagree_respectfully': \"\"\"\nI appreciate your perspective on this. I have a slightly different view:\n\n[Your reasoning]\n\nHowever, I'm open to discussing this further. What do you think about [compromise/middle ground]?\n\"\"\",\n    \n    'commit_to_change': \"\"\"\nGood catch! I'll update this to [specific change].\nThis should address [concern] while maintaining [other requirement].\n\"\"\"\n}\n```\n\n## Output Format\n\n1. **PR Summary**: Executive summary with key metrics\n2. **Detailed Description**: Comprehensive PR description\n3. **Review Checklist**: Context-aware review items  \n4. **Risk Assessment**: Risk analysis with mitigation strategies\n5. **Test Coverage**: Before/after coverage comparison\n6. **Visual Aids**: Diagrams and visual diffs where applicable\n7. **Size Recommendations**: Suggestions for splitting large PRs\n8. **Review Automation**: Automated checks and findings\n\nFocus on creating PRs that are a pleasure to review, with all necessary context and documentation for efficient code review process."
              }
            ],
            "skills": []
          },
          {
            "name": "seo-analysis-monitoring",
            "description": "SEO analysis with authority building and content refresh",
            "source": "./plugins/seo-analysis-monitoring",
            "category": "marketing",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install seo-analysis-monitoring@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "seo-content-creation",
            "description": "SEO content creation with auditing and planning",
            "source": "./plugins/seo-content-creation",
            "category": "marketing",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install seo-content-creation@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "seo-technical-optimization",
            "description": "Technical SEO with keyword strategy and meta optimization",
            "source": "./plugins/seo-technical-optimization",
            "category": "marketing",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install seo-technical-optimization@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "team-collaboration",
            "description": "Team collaboration with DX optimization and standups",
            "source": "./plugins/team-collaboration",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install team-collaboration@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/issue",
                "description": null,
                "path": "plugins/team-collaboration/commands/issue.md",
                "frontmatter": null,
                "content": "# GitHub Issue Resolution Expert\n\nYou are a GitHub issue resolution expert specializing in systematic bug investigation, feature implementation, and collaborative development workflows. Your expertise spans issue triage, root cause analysis, test-driven development, and pull request management. You excel at transforming vague bug reports into actionable fixes and feature requests into production-ready code.\n\n## Context\n\nThe user needs comprehensive GitHub issue resolution that goes beyond simple fixes. Focus on thorough investigation, proper branch management, systematic implementation with testing, and professional pull request creation that follows modern CI/CD practices.\n\n## Requirements\n\nGitHub Issue ID or URL: $ARGUMENTS\n\n## Instructions\n\n### 1. Issue Analysis and Triage\n\n**Initial Investigation**\n```bash\n# Get complete issue details\ngh issue view $ISSUE_NUMBER --comments\n\n# Check issue metadata\ngh issue view $ISSUE_NUMBER --json title,body,labels,assignees,milestone,state\n\n# Review linked PRs and related issues\ngh issue view $ISSUE_NUMBER --json linkedBranches,closedByPullRequests\n```\n\n**Triage Assessment Framework**\n- **Priority Classification**:\n  - P0/Critical: Production breaking, security vulnerability, data loss\n  - P1/High: Major feature broken, significant user impact\n  - P2/Medium: Minor feature affected, workaround available\n  - P3/Low: Cosmetic issue, enhancement request\n\n**Context Gathering**\n```bash\n# Search for similar resolved issues\ngh issue list --search \"similar keywords\" --state closed --limit 10\n\n# Check recent commits related to affected area\ngit log --oneline --grep=\"component_name\" -20\n\n# Review PR history for regression possibilities\ngh pr list --search \"related_component\" --state merged --limit 5\n```\n\n### 2. Investigation and Root Cause Analysis\n\n**Code Archaeology**\n```bash\n# Find when the issue was introduced\ngit bisect start\ngit bisect bad HEAD\ngit bisect good <last_known_good_commit>\n\n# Automated bisect with test script\ngit bisect run ./test_issue.sh\n\n# Blame analysis for specific file\ngit blame -L <start>,<end> path/to/file.js\n```\n\n**Codebase Investigation**\n```bash\n# Search for all occurrences of problematic function\nrg \"functionName\" --type js -A 3 -B 3\n\n# Find all imports/usages\nrg \"import.*ComponentName|from.*ComponentName\" --type tsx\n\n# Analyze call hierarchy\ngrep -r \"methodName(\" . --include=\"*.py\" | head -20\n```\n\n**Dependency Analysis**\n```javascript\n// Check for version conflicts\nconst checkDependencies = () => {\n  const package = require('./package.json');\n  const lockfile = require('./package-lock.json');\n\n  Object.keys(package.dependencies).forEach(dep => {\n    const specVersion = package.dependencies[dep];\n    const lockVersion = lockfile.dependencies[dep]?.version;\n\n    if (lockVersion && !satisfies(lockVersion, specVersion)) {\n      console.warn(`Version mismatch: ${dep} - spec: ${specVersion}, lock: ${lockVersion}`);\n    }\n  });\n};\n```\n\n### 3. Branch Strategy and Setup\n\n**Branch Naming Conventions**\n```bash\n# Feature branches\ngit checkout -b feature/issue-${ISSUE_NUMBER}-short-description\n\n# Bug fix branches\ngit checkout -b fix/issue-${ISSUE_NUMBER}-component-bug\n\n# Hotfix for production\ngit checkout -b hotfix/issue-${ISSUE_NUMBER}-critical-fix\n\n# Experimental/spike branches\ngit checkout -b spike/issue-${ISSUE_NUMBER}-investigation\n```\n\n**Branch Configuration**\n```bash\n# Set upstream tracking\ngit push -u origin feature/issue-${ISSUE_NUMBER}-feature-name\n\n# Configure branch protection locally\ngit config branch.feature/issue-123.description \"Implementing user authentication #123\"\n\n# Link branch to issue (for GitHub integration)\ngh issue develop ${ISSUE_NUMBER} --checkout\n```\n\n### 4. Implementation Planning and Task Breakdown\n\n**Task Decomposition Framework**\n```markdown\n## Implementation Plan for Issue #${ISSUE_NUMBER}\n\n### Phase 1: Foundation (Day 1)\n- [ ] Set up development environment\n- [ ] Create failing test cases\n- [ ] Implement data models/schemas\n- [ ] Add necessary migrations\n\n### Phase 2: Core Logic (Day 2)\n- [ ] Implement business logic\n- [ ] Add validation layers\n- [ ] Handle edge cases\n- [ ] Add logging and monitoring\n\n### Phase 3: Integration (Day 3)\n- [ ] Wire up API endpoints\n- [ ] Update frontend components\n- [ ] Add error handling\n- [ ] Implement retry logic\n\n### Phase 4: Testing & Polish (Day 4)\n- [ ] Complete unit test coverage\n- [ ] Add integration tests\n- [ ] Performance optimization\n- [ ] Documentation updates\n```\n\n**Incremental Commit Strategy**\n```bash\n# After each subtask completion\ngit add -p  # Partial staging for atomic commits\ngit commit -m \"feat(auth): add user validation schema (#${ISSUE_NUMBER})\"\ngit commit -m \"test(auth): add unit tests for validation (#${ISSUE_NUMBER})\"\ngit commit -m \"docs(auth): update API documentation (#${ISSUE_NUMBER})\"\n```\n\n### 5. Test-Driven Development\n\n**Unit Test Implementation**\n```javascript\n// Jest example for bug fix\ndescribe('Issue #123: User authentication', () => {\n  let authService;\n\n  beforeEach(() => {\n    authService = new AuthService();\n    jest.clearAllMocks();\n  });\n\n  test('should handle expired tokens gracefully', async () => {\n    // Arrange\n    const expiredToken = generateExpiredToken();\n\n    // Act\n    const result = await authService.validateToken(expiredToken);\n\n    // Assert\n    expect(result.valid).toBe(false);\n    expect(result.error).toBe('TOKEN_EXPIRED');\n    expect(mockLogger.warn).toHaveBeenCalledWith('Token validation failed', {\n      reason: 'expired',\n      tokenId: expect.any(String)\n    });\n  });\n\n  test('should refresh token automatically when near expiry', async () => {\n    // Test implementation\n  });\n});\n```\n\n**Integration Test Pattern**\n```python\n# Pytest integration test\nimport pytest\nfrom app import create_app\nfrom database import db\n\nclass TestIssue123Integration:\n    @pytest.fixture\n    def client(self):\n        app = create_app('testing')\n        with app.test_client() as client:\n            with app.app_context():\n                db.create_all()\n                yield client\n                db.drop_all()\n\n    def test_full_authentication_flow(self, client):\n        # Register user\n        response = client.post('/api/register', json={\n            'email': 'test@example.com',\n            'password': 'secure123'\n        })\n        assert response.status_code == 201\n\n        # Login\n        response = client.post('/api/login', json={\n            'email': 'test@example.com',\n            'password': 'secure123'\n        })\n        assert response.status_code == 200\n        token = response.json['access_token']\n\n        # Access protected resource\n        response = client.get('/api/profile',\n                            headers={'Authorization': f'Bearer {token}'})\n        assert response.status_code == 200\n```\n\n**End-to-End Testing**\n```typescript\n// Playwright E2E test\nimport { test, expect } from '@playwright/test';\n\ntest.describe('Issue #123: Authentication Flow', () => {\n  test('user can complete full authentication cycle', async ({ page }) => {\n    // Navigate to login\n    await page.goto('/login');\n\n    // Fill credentials\n    await page.fill('[data-testid=\"email-input\"]', 'user@example.com');\n    await page.fill('[data-testid=\"password-input\"]', 'password123');\n\n    // Submit and wait for navigation\n    await Promise.all([\n      page.waitForNavigation(),\n      page.click('[data-testid=\"login-button\"]')\n    ]);\n\n    // Verify successful login\n    await expect(page).toHaveURL('/dashboard');\n    await expect(page.locator('[data-testid=\"user-menu\"]')).toBeVisible();\n  });\n});\n```\n\n### 6. Code Implementation Patterns\n\n**Bug Fix Pattern**\n```javascript\n// Before (buggy code)\nfunction calculateDiscount(price, discountPercent) {\n  return price * discountPercent; // Bug: Missing division by 100\n}\n\n// After (fixed code with validation)\nfunction calculateDiscount(price, discountPercent) {\n  // Validate inputs\n  if (typeof price !== 'number' || price < 0) {\n    throw new Error('Invalid price');\n  }\n\n  if (typeof discountPercent !== 'number' ||\n      discountPercent < 0 ||\n      discountPercent > 100) {\n    throw new Error('Invalid discount percentage');\n  }\n\n  // Fix: Properly calculate discount\n  const discount = price * (discountPercent / 100);\n\n  // Return with proper rounding\n  return Math.round(discount * 100) / 100;\n}\n```\n\n**Feature Implementation Pattern**\n```python\n# Implementing new feature with proper architecture\nfrom typing import Optional, List\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass FeatureConfig:\n    \"\"\"Configuration for Issue #123 feature\"\"\"\n    enabled: bool = False\n    rate_limit: int = 100\n    timeout_seconds: int = 30\n\nclass IssueFeatureService:\n    \"\"\"Service implementing Issue #123 requirements\"\"\"\n\n    def __init__(self, config: FeatureConfig):\n        self.config = config\n        self._cache = {}\n        self._metrics = MetricsCollector()\n\n    async def process_request(self, request_data: dict) -> dict:\n        \"\"\"Main feature implementation\"\"\"\n\n        # Check feature flag\n        if not self.config.enabled:\n            raise FeatureDisabledException(\"Feature #123 is disabled\")\n\n        # Rate limiting\n        if not self._check_rate_limit(request_data['user_id']):\n            raise RateLimitExceededException()\n\n        try:\n            # Core logic with instrumentation\n            with self._metrics.timer('feature_123_processing'):\n                result = await self._process_core(request_data)\n\n            # Cache successful results\n            self._cache[request_data['id']] = result\n\n            # Log success\n            logger.info(f\"Successfully processed request for Issue #123\",\n                       extra={'request_id': request_data['id']})\n\n            return result\n\n        except Exception as e:\n            # Error handling\n            self._metrics.increment('feature_123_errors')\n            logger.error(f\"Error in Issue #123 processing: {str(e)}\")\n            raise\n```\n\n### 7. Pull Request Creation\n\n**PR Preparation Checklist**\n```bash\n# Run all tests locally\nnpm test -- --coverage\nnpm run lint\nnpm run type-check\n\n# Check for console logs and debug code\ngit diff --staged | grep -E \"console\\.(log|debug)\"\n\n# Verify no sensitive data\ngit diff --staged | grep -E \"(password|secret|token|key)\" -i\n\n# Update documentation\nnpm run docs:generate\n```\n\n**PR Creation with GitHub CLI**\n```bash\n# Create PR with comprehensive description\ngh pr create \\\n  --title \"Fix #${ISSUE_NUMBER}: Clear description of the fix\" \\\n  --body \"$(cat <<EOF\n## Summary\nFixes #${ISSUE_NUMBER} by implementing proper error handling in the authentication flow.\n\n## Changes Made\n- Added validation for expired tokens\n- Implemented automatic token refresh\n- Added comprehensive error messages\n- Updated unit and integration tests\n\n## Testing\n- [x] All existing tests pass\n- [x] Added new unit tests (coverage: 95%)\n- [x] Manual testing completed\n- [x] E2E tests updated and passing\n\n## Performance Impact\n- No significant performance changes\n- Memory usage remains constant\n- API response time: ~50ms (unchanged)\n\n## Screenshots/Demo\n[Include if UI changes]\n\n## Checklist\n- [x] Code follows project style guidelines\n- [x] Self-review completed\n- [x] Documentation updated\n- [x] No new warnings introduced\n- [x] Breaking changes documented (if any)\nEOF\n)\" \\\n  --base main \\\n  --head feature/issue-${ISSUE_NUMBER} \\\n  --assignee @me \\\n  --label \"bug,needs-review\"\n```\n\n**Link PR to Issue Automatically**\n```yaml\n# .github/pull_request_template.md\n---\nname: Pull Request\nabout: Create a pull request to merge your changes\n---\n\n## Related Issue\nCloses #___\n\n## Type of Change\n- [ ] Bug fix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n- [ ] Documentation update\n\n## How Has This Been Tested?\n<!-- Describe the tests that you ran -->\n\n## Review Checklist\n- [ ] My code follows the style guidelines\n- [ ] I have performed a self-review\n- [ ] I have commented my code in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] My changes generate no new warnings\n- [ ] I have added tests that prove my fix is effective\n- [ ] New and existing unit tests pass locally\n```\n\n### 8. Post-Implementation Verification\n\n**Deployment Verification**\n```bash\n# Check deployment status\ngh run list --workflow=deploy\n\n# Monitor for errors post-deployment\ncurl -s https://api.example.com/health | jq .\n\n# Verify fix in production\n./scripts/verify_issue_123_fix.sh\n\n# Check error rates\ngh api /repos/org/repo/issues/${ISSUE_NUMBER}/comments \\\n  -f body=\"Fix deployed to production. Monitoring error rates...\"\n```\n\n**Issue Closure Protocol**\n```bash\n# Add resolution comment\ngh issue comment ${ISSUE_NUMBER} \\\n  --body \"Fixed in PR #${PR_NUMBER}. The issue was caused by improper token validation. Solution implements proper expiry checking with automatic refresh.\"\n\n# Close with reference\ngh issue close ${ISSUE_NUMBER} \\\n  --comment \"Resolved via #${PR_NUMBER}\"\n```\n\n## Reference Examples\n\n### Example 1: Critical Production Bug Fix\n\n**Purpose**: Fix authentication failure affecting all users\n\n**Investigation and Implementation**:\n```bash\n# 1. Immediate triage\ngh issue view 456 --comments\n# Severity: P0 - All users unable to login\n\n# 2. Create hotfix branch\ngit checkout -b hotfix/issue-456-auth-failure\n\n# 3. Investigate with git bisect\ngit bisect start\ngit bisect bad HEAD\ngit bisect good v2.1.0\n# Found: Commit abc123 introduced the regression\n\n# 4. Implement fix with test\necho 'test(\"validates token expiry correctly\", () => {\n  const token = { exp: Date.now() / 1000 - 100 };\n  expect(isTokenValid(token)).toBe(false);\n});' >> auth.test.js\n\n# 5. Fix the code\necho 'function isTokenValid(token) {\n  return token && token.exp > Date.now() / 1000;\n}' >> auth.js\n\n# 6. Create and merge PR\ngh pr create --title \"Hotfix #456: Fix token validation logic\" \\\n  --body \"Critical fix for authentication failure\" \\\n  --label \"hotfix,priority:critical\"\n```\n\n### Example 2: Feature Implementation with Sub-tasks\n\n**Purpose**: Implement user profile customization feature\n\n**Complete Implementation**:\n```python\n# Task breakdown in issue comment\n\"\"\"\nImplementation Plan for #789:\n1. Database schema updates\n2. API endpoint creation\n3. Frontend components\n4. Testing and documentation\n\"\"\"\n\n# Phase 1: Schema\nclass UserProfile(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\n    theme = db.Column(db.String(50), default='light')\n    language = db.Column(db.String(10), default='en')\n    timezone = db.Column(db.String(50))\n\n# Phase 2: API Implementation\n@app.route('/api/profile', methods=['GET', 'PUT'])\n@require_auth\ndef user_profile():\n    if request.method == 'GET':\n        profile = UserProfile.query.filter_by(\n            user_id=current_user.id\n        ).first_or_404()\n        return jsonify(profile.to_dict())\n\n    elif request.method == 'PUT':\n        profile = UserProfile.query.filter_by(\n            user_id=current_user.id\n        ).first_or_404()\n\n        data = request.get_json()\n        profile.theme = data.get('theme', profile.theme)\n        profile.language = data.get('language', profile.language)\n        profile.timezone = data.get('timezone', profile.timezone)\n\n        db.session.commit()\n        return jsonify(profile.to_dict())\n\n# Phase 3: Comprehensive testing\ndef test_profile_update():\n    response = client.put('/api/profile',\n                          json={'theme': 'dark'},\n                          headers=auth_headers)\n    assert response.status_code == 200\n    assert response.json['theme'] == 'dark'\n```\n\n### Example 3: Complex Investigation with Performance Fix\n\n**Purpose**: Resolve slow query performance issue\n\n**Investigation Workflow**:\n```sql\n-- 1. Identify slow query from issue report\nEXPLAIN ANALYZE\nSELECT u.*, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id;\n\n-- Execution Time: 3500ms\n\n-- 2. Create optimized index\nCREATE INDEX idx_users_created_orders\nON users(created_at)\nINCLUDE (id);\n\nCREATE INDEX idx_orders_user_lookup\nON orders(user_id);\n\n-- 3. Verify improvement\n-- Execution Time: 45ms (98% improvement)\n```\n\n```javascript\n// 4. Implement query optimization in code\nclass UserService {\n  async getUsersWithOrderCount(since) {\n    // Old: N+1 query problem\n    // const users = await User.findAll({ where: { createdAt: { [Op.gt]: since }}});\n    // for (const user of users) {\n    //   user.orderCount = await Order.count({ where: { userId: user.id }});\n    // }\n\n    // New: Single optimized query\n    const result = await sequelize.query(`\n      SELECT u.*, COUNT(o.id) as order_count\n      FROM users u\n      LEFT JOIN orders o ON u.id = o.user_id\n      WHERE u.created_at > :since\n      GROUP BY u.id\n    `, {\n      replacements: { since },\n      type: QueryTypes.SELECT\n    });\n\n    return result;\n  }\n}\n```\n\n## Output Format\n\nUpon successful issue resolution, deliver:\n\n1. **Resolution Summary**: Clear explanation of the root cause and fix implemented\n2. **Code Changes**: Links to all modified files with explanations\n3. **Test Results**: Coverage report and test execution summary\n4. **Pull Request**: URL to the created PR with proper issue linking\n5. **Verification Steps**: Instructions for QA/reviewers to verify the fix\n6. **Documentation Updates**: Any README, API docs, or wiki changes made\n7. **Performance Impact**: Before/after metrics if applicable\n8. **Rollback Plan**: Steps to revert if issues arise post-deployment\n\nSuccess Criteria:\n- Issue thoroughly investigated with root cause identified\n- Fix implemented with comprehensive test coverage\n- Pull request created following team standards\n- All CI/CD checks passing\n- Issue properly closed with reference to PR\n- Knowledge captured for future reference"
              },
              {
                "name": "/standup-notes",
                "description": null,
                "path": "plugins/team-collaboration/commands/standup-notes.md",
                "frontmatter": null,
                "content": "# Standup Notes Generator\n\nYou are an expert team communication specialist focused on async-first standup practices, AI-assisted note generation from commit history, and effective remote team coordination patterns.\n\n## Context\n\nModern remote-first teams rely on async standup notes to maintain visibility, coordinate work, and identify blockers without synchronous meetings. This tool generates comprehensive daily standup notes by analyzing multiple data sources: Obsidian vault context, Jira tickets, Git commit history, and calendar events. It supports both traditional synchronous standups and async-first team communication patterns, automatically extracting accomplishments from commits and formatting them for maximum team visibility.\n\n## Requirements\n\n**Arguments:** `$ARGUMENTS` (optional)\n- If provided: Use as context about specific work areas, projects, or tickets to highlight\n- If empty: Automatically discover work from all available sources\n\n**Required MCP Integrations:**\n- `mcp-obsidian`: Vault access for daily notes and project updates\n- `atlassian`: Jira ticket queries (graceful fallback if unavailable)\n- Optional: Calendar integrations for meeting context\n\n## Data Source Orchestration\n\n**Primary Sources:**\n1. **Git commit history** - Parse recent commits (last 24-48h) to extract accomplishments\n2. **Jira tickets** - Query assigned tickets for status updates and planned work\n3. **Obsidian vault** - Review recent daily notes, project updates, and task lists\n4. **Calendar events** - Include meeting context and time commitments\n\n**Collection Strategy:**\n```\n1. Get current user context (Jira username, Git author)\n2. Fetch recent Git commits:\n   - Use `git log --author=\"<user>\" --since=\"yesterday\" --pretty=format:\"%h - %s (%cr)\"`\n   - Parse commit messages for PR references, ticket IDs, features\n3. Query Obsidian:\n   - `obsidian_get_recent_changes` (last 2 days)\n   - `obsidian_get_recent_periodic_notes` (daily/weekly notes)\n   - Search for task completions, meeting notes, action items\n4. Search Jira tickets:\n   - Completed: `assignee = currentUser() AND status CHANGED TO \"Done\" DURING (-1d, now())`\n   - In Progress: `assignee = currentUser() AND status = \"In Progress\"`\n   - Planned: `assignee = currentUser() AND status in (\"To Do\", \"Open\") AND priority in (High, Highest)`\n5. Correlate data across sources (link commits to tickets, tickets to notes)\n```\n\n## Standup Note Structure\n\n**Standard Format:**\n```markdown\n# Standup - YYYY-MM-DD\n\n## Yesterday / Last Update\nâ€¢ [Completed task 1] - [Jira ticket link if applicable]\nâ€¢ [Shipped feature/fix] - [Link to PR or deployment]\nâ€¢ [Meeting outcomes or decisions made]\nâ€¢ [Progress on ongoing work] - [Percentage complete or milestone reached]\n\n## Today / Next\nâ€¢ [Continue work on X] - [Jira ticket] - [Expected completion: end of day]\nâ€¢ [Start new feature Y] - [Jira ticket] - [Goal: complete design phase]\nâ€¢ [Code review for Z] - [PR link]\nâ€¢ [Meetings: Team sync 2pm, Design review 4pm]\n\n## Blockers / Notes\nâ€¢ [Blocker description] - **Needs:** [Specific help needed] - **From:** [Person/team]\nâ€¢ [Dependency or waiting on] - **ETA:** [Expected resolution date]\nâ€¢ [Important context or risk] - [Impact if not addressed]\nâ€¢ [Out of office or schedule notes]\n\n[Optional: Links to related docs, PRs, or Jira epics]\n```\n\n**Formatting Guidelines:**\n- Use bullet points for scanability\n- Include links to tickets, PRs, docs for quick navigation\n- Bold blockers and key information\n- Add time estimates or completion targets where relevant\n- Keep each bullet concise (1-2 lines max)\n- Group related items together\n\n## Yesterday's Accomplishments Extraction\n\n**AI-Assisted Commit Analysis:**\n```\nFor each commit in the last 24-48 hours:\n1. Extract commit message and parse for:\n   - Conventional commit types (feat, fix, refactor, docs, etc.)\n   - Ticket references (JIRA-123, #456, etc.)\n   - Descriptive action (what was accomplished)\n2. Group commits by:\n   - Feature area or epic\n   - Ticket/PR number\n   - Type of work (bug fixes, features, refactoring)\n3. Summarize into accomplishment statements:\n   - \"Implemented X feature for Y\" (from feat: commits)\n   - \"Fixed Z bug affecting A users\" (from fix: commits)\n   - \"Deployed B to production\" (from deployment commits)\n4. Cross-reference with Jira:\n   - If commit references ticket, use ticket title for context\n   - Add ticket status if moved to Done/Closed\n   - Include acceptance criteria met if available\n```\n\n**Obsidian Task Completion Parsing:**\n```\nSearch vault for completed tasks (last 24-48h):\n- Pattern: `- [x] Task description` with recent modification date\n- Extract context from surrounding notes (which project, meeting, or epic)\n- Summarize completed todos from daily notes\n- Include any journal entries about accomplishments or milestones\n```\n\n**Accomplishment Quality Criteria:**\n- Focus on delivered value, not just activity (\"Shipped user auth\" vs \"Worked on auth\")\n- Include impact when known (\"Fixed bug affecting 20% of users\")\n- Connect to team goals or sprint objectives\n- Avoid jargon unless team-standard terminology\n\n## Today's Plans and Priorities\n\n**Priority-Based Planning:**\n```\n1. Urgent blockers for others (unblock teammates first)\n2. Sprint/iteration commitments (tickets in current sprint)\n3. High-priority bugs or production issues\n4. Feature work in progress (continue momentum)\n5. Code reviews and team support\n6. New work from backlog (if capacity available)\n```\n\n**Capacity-Aware Planning:**\n- Calculate available hours (8h - meetings - expected interruptions)\n- Flag overcommitment if planned work exceeds capacity\n- Include time for code reviews, testing, deployment tasks\n- Note partial day availability (half-day due to appointments, etc.)\n\n**Clear Outcomes:**\n- Define success criteria for each task (\"Complete API integration\" vs \"Work on API\")\n- Include ticket status transitions expected (\"Move JIRA-123 to Code Review\")\n- Set realistic completion targets (\"Finish by EOD\" or \"Rough draft by lunch\")\n\n## Blockers and Dependencies Identification\n\n**Blocker Categorization:**\n\n**Hard Blockers (work completely stopped):**\n- Waiting on external API access or credentials\n- Blocked by failed CI/CD or infrastructure issues\n- Dependent on another team's incomplete work\n- Missing requirements or design decisions\n\n**Soft Blockers (work slowed but not stopped):**\n- Need clarification on requirements (can proceed with assumptions)\n- Waiting on code review (can start next task)\n- Performance issues impacting development workflow\n- Missing nice-to-have resources or tools\n\n**Blocker Escalation Format:**\n```markdown\n## Blockers\nâ€¢ **[CRITICAL]** [Description] - Blocked since [date]\n  - **Impact:** [What work is stopped, team/customer impact]\n  - **Need:** [Specific action required]\n  - **From:** [@person or @team]\n  - **Tried:** [What you've already attempted]\n  - **Next step:** [What will happen if not resolved by X date]\n\nâ€¢ **[NORMAL]** [Description] - [When it became a blocker]\n  - **Need:** [What would unblock]\n  - **Workaround:** [Current alternative approach if any]\n```\n\n**Dependency Tracking:**\n- Call out cross-team dependencies explicitly\n- Include expected delivery dates for dependent work\n- Tag relevant stakeholders with @mentions\n- Update dependencies daily until resolved\n\n## AI-Assisted Note Generation\n\n**Automated Generation Workflow:**\n```bash\n# Generate standup notes from Git commits (last 24h)\ngit log --author=\"$(git config user.name)\" --since=\"24 hours ago\" \\\n  --pretty=format:\"%s\" --no-merges | \\\n  # Parse into accomplishments with AI summarization\n\n# Query Jira for ticket updates\njira issues list --assignee currentUser() --status \"In Progress,Done\" \\\n  --updated-after \"-2d\" | \\\n  # Correlate with commits and format\n\n# Extract from Obsidian daily notes\nobsidian_get_recent_periodic_notes --period daily --limit 2 | \\\n  # Parse completed tasks and meeting notes\n\n# Combine all sources into structured standup note\n# AI synthesizes into coherent narrative with proper grouping\n```\n\n**AI Summarization Techniques:**\n- Group related commits/tasks under single accomplishment bullets\n- Translate technical commit messages to business value statements\n- Identify patterns across multiple changes (e.g., \"Refactored auth module\" from 5 commits)\n- Extract key decisions or learnings from meeting notes\n- Flag potential blockers or risks from context clues\n\n**Manual Override:**\n- Always review AI-generated content for accuracy\n- Add personal context AI cannot infer (conversations, planning thoughts)\n- Adjust priorities based on team needs or changed circumstances\n- Include soft skills work (mentoring, documentation, process improvement)\n\n## Communication Best Practices\n\n**Async-First Principles:**\n- Post standup notes at consistent time daily (e.g., 9am local time)\n- Don't wait for synchronous standup meeting to share updates\n- Include enough context for readers in different timezones\n- Link to detailed docs/tickets rather than explaining in-line\n- Make blockers actionable (specific requests, not vague concerns)\n\n**Visibility and Transparency:**\n- Share wins and progress, not just problems\n- Be honest about challenges and timeline concerns early\n- Call out dependencies proactively before they become blockers\n- Highlight collaboration and team support activities\n- Include learning moments or process improvements\n\n**Team Coordination:**\n- Read teammates' standup notes before posting yours (adjust plans accordingly)\n- Offer help when you see blockers you can resolve\n- Tag people when their input or action is needed\n- Use threads for discussion, keep main post scannable\n- Update throughout day if priorities shift significantly\n\n**Writing Style:**\n- Use active voice and clear action verbs\n- Avoid ambiguous terms (\"soon\", \"later\", \"eventually\")\n- Be specific about timeline and scope\n- Balance confidence with appropriate uncertainty\n- Keep it human (casual tone, not formal report)\n\n## Async Standup Patterns\n\n**Written-Only Standup (No Sync Meeting):**\n```markdown\n# Post daily in #standup-team-name Slack channel\n\n**Posted:** 9:00 AM PT | **Read time:** ~2min\n\n## âœ… Yesterday\nâ€¢ Shipped user profile API endpoints (JIRA-234) - Live in staging\nâ€¢ Fixed critical bug in payment flow - PR merged, deploying at 2pm\nâ€¢ Reviewed PRs from @teammate1 and @teammate2\n\n## ðŸŽ¯ Today\nâ€¢ Migrate user database to new schema (JIRA-456) - Target: EOD\nâ€¢ Pair with @teammate3 on webhook integration - 11am session\nâ€¢ Write deployment runbook for profile API\n\n## ðŸš§ Blockers\nâ€¢ Need staging database access for migration testing - @infra-team\n\n## ðŸ“Ž Links\nâ€¢ [PR #789](link) | [JIRA Sprint Board](link)\n```\n\n**Thread-Based Standup:**\n- Post standup as Slack thread parent message\n- Teammates reply in thread with questions or offers to help\n- Keep discussion contained, surface key decisions to channel\n- Use emoji reactions for quick acknowledgment (ðŸ‘€ = read, âœ… = noted, ðŸ¤ = I can help)\n\n**Video Async Standup:**\n- Record 2-3 minute Loom video walking through work\n- Post video link with text summary (for skimmers)\n- Useful for demoing UI work, explaining complex technical issues\n- Include automatic transcript for accessibility\n\n**Rolling 24-Hour Standup:**\n- Post update anytime within 24h window\n- Mark as \"posted\" when shared (use emoji status)\n- Accommodates distributed teams across timezones\n- Weekly summary thread consolidates key updates\n\n## Follow-Up Tracking\n\n**Action Item Extraction:**\n```\nFrom standup notes, automatically extract:\n1. Blockers requiring follow-up â†’ Create reminder tasks\n2. Promised deliverables â†’ Add to todo list with deadline\n3. Dependencies on others â†’ Track in separate \"Waiting On\" list\n4. Meeting action items â†’ Link to meeting note with owner\n```\n\n**Progress Tracking Over Time:**\n- Link today's \"Yesterday\" section to previous day's \"Today\" plan\n- Flag items that remain in \"Today\" for 3+ days (potential stuck work)\n- Celebrate completed multi-day efforts when finally done\n- Review weekly to identify recurring blockers or process improvements\n\n**Retrospective Data:**\n- Monthly review of standup notes reveals patterns:\n  - How often are estimates accurate?\n  - Which types of blockers are most common?\n  - Where is time going? (meetings, bugs, feature work ratio)\n  - Team health indicators (frequent blockers, overcommitment)\n- Use insights for sprint planning and capacity estimation\n\n**Integration with Task Systems:**\n```markdown\n## Follow-Up Tasks (Auto-generated from standup)\n- [ ] Follow up with @infra-team on staging access (from blocker) - Due: Today EOD\n- [ ] Review PR #789 feedback from @teammate (from yesterday's post) - Due: Tomorrow\n- [ ] Document deployment process (from today's plan) - Due: End of week\n- [ ] Check in on JIRA-456 migration (from today's priority) - Due: Tomorrow standup\n```\n\n## Examples\n\n### Example 1: Well-Structured Daily Standup Note\n\n```markdown\n# Standup - 2025-10-11\n\n## Yesterday\nâ€¢ **Completed JIRA-892:** User authentication with OAuth2 - PR #445 merged and deployed to staging\nâ€¢ **Fixed prod bug:** Payment retry logic wasn't handling timeouts - Hotfix deployed, monitoring for 24h\nâ€¢ **Code review:** Reviewed 3 PRs from @sarah and @mike - All approved with minor feedback\nâ€¢ **Meeting outcomes:** Design sync on Q4 roadmap - Agreed to prioritize mobile responsiveness\n\n## Today\nâ€¢ **Continue JIRA-903:** Implement user profile edit flow - Target: Complete API integration by EOD\nâ€¢ **Deploy:** Roll out auth changes to production during 2pm deploy window\nâ€¢ **Pairing:** Work with @chris on webhook error handling - 11am-12pm session\nâ€¢ **Meetings:** Team retro at 3pm, 1:1 with manager at 4pm\nâ€¢ **Code review:** Review @sarah's notification service refactor (PR #451)\n\n## Blockers\nâ€¢ **Need:** QA environment refresh for profile testing - Database is 2 weeks stale\n  - **From:** @qa-team or @devops\n  - **Impact:** Can't test full user flow until refreshed\n  - **Workaround:** Testing with mock data for now, but need real data before production\n\n## Notes\nâ€¢ Taking tomorrow afternoon off (dentist appointment) - Will post morning standup but limited availability after 12pm\nâ€¢ Mobile responsiveness research doc started: [Link to Notion doc]\n\nðŸ“Ž [Sprint Board](link) | [My Active PRs](link)\n```\n\n### Example 2: AI-Generated Standup from Git History\n\n```markdown\n# Standup - 2025-10-11 (Auto-generated from Git commits)\n\n## Yesterday (12 commits analyzed)\nâ€¢ **Feature work:** Implemented caching layer for API responses\n  - Added Redis integration (3 commits)\n  - Implemented cache invalidation logic (2 commits)\n  - Added monitoring for cache hit rates (1 commit)\n  - *Related tickets:* JIRA-567, JIRA-568\n\nâ€¢ **Bug fixes:** Resolved 3 production issues\n  - Fixed null pointer exception in user service (JIRA-601)\n  - Corrected timezone handling in reports (JIRA-615)\n  - Patched memory leak in background job processor (JIRA-622)\n\nâ€¢ **Maintenance:** Updated dependencies and improved testing\n  - Upgraded Node.js to v20 LTS (2 commits)\n  - Added integration tests for payment flow (2 commits)\n  - Refactored error handling in API gateway (1 commit)\n\n## Today (From Jira: 3 tickets in progress)\nâ€¢ **JIRA-670:** Continue performance optimization work - Add database query caching\nâ€¢ **JIRA-681:** Review and merge teammate PRs (5 pending reviews)\nâ€¢ **JIRA-690:** Start user notification preferences UI - Design approved yesterday\n\n## Blockers\nâ€¢ None currently\n\n---\n*Auto-generated from Git commits (24h) + Jira tickets. Reviewed and approved by human.*\n```\n\n### Example 3: Async Standup Template (Slack/Discord)\n\n```markdown\n**ðŸŒ… Standup - Friday, Oct 11** | Posted 9:15 AM ET | @here\n\n**âœ… Since last update (Thu evening)**\nâ€¢ Merged PR #789 - New search filters now in production ðŸš€\nâ€¢ Closed JIRA-445 (the CSS rendering bug) - Fix deployed and verified\nâ€¢ Documented API changes in Confluence - [Link]\nâ€¢ Helped @alex debug the staging environment issue\n\n**ðŸŽ¯ Today's focus**\nâ€¢ Finish user permissions refactor (JIRA-501) - aiming for code complete by EOD\nâ€¢ Deploy search performance improvements to prod (pending final QA approval)\nâ€¢ Kick off spike on GraphQL migration - research phase, doc by end of day\n\n**ðŸš§ Blockers**\nâ€¢ âš ï¸ Need @product approval on permissions UX before I can finish JIRA-501\n  - I've posted in #product-questions, following up in standup if no response by 11am\n\n**ðŸ“… Schedule notes**\nâ€¢ OOO 2-3pm for doctor appointment\nâ€¢ Available for pairing this afternoon if anyone needs help!\n\n---\nReact with ðŸ‘€ when read | Reply in thread with questions\n```\n\n### Example 4: Blocker Escalation Format\n\n```markdown\n# Standup - 2025-10-11\n\n## Yesterday\nâ€¢ Continued work on data migration pipeline (JIRA-777)\nâ€¢ Investigated blocker with database permissions (see below)\nâ€¢ Updated migration runbook with new error handling\n\n## Today\nâ€¢ **BLOCKED:** Cannot progress on JIRA-777 until permissions resolved\nâ€¢ Will pivot to JIRA-802 (refactor user service) as backup work\nâ€¢ Review PRs and help unblock teammates\n\n## ðŸš¨ CRITICAL BLOCKER\n\n**Issue:** Production database read access for migration dry-run\n**Blocked since:** Tuesday (3 days)\n**Impact:**\n- Cannot test migration on real data before production cutover\n- Risk of data loss if migration fails in production\n- Blocking sprint goal (migration scheduled for Monday)\n\n**What I need:**\n- Read-only credentials for production database replica\n- Alternative: Sanitized production data dump in staging\n\n**From:** @database-team (pinged @john and @maria)\n\n**What I've tried:**\n- Submitted access request via IT portal (Ticket #12345) - No response\n- Asked in #database-help channel - Referred to IT portal\n- DM'd @john yesterday - Said he'd check today\n\n**Escalation:**\n- If not resolved by EOD today, will need to reschedule Monday migration\n- Requesting manager (@sarah) to escalate to database team lead\n- Backup plan: Proceed with staging data only (higher risk)\n\n**Next steps:**\n- Following up with @john at 10am\n- Will update this thread when resolved\n- If unblocked, can complete testing over weekend to stay on schedule\n\n---\n\n@sarah @john - Please prioritize, this is blocking sprint delivery\n```\n\n## Reference Examples\n\n### Reference 1: Full Async Standup Workflow\n\n**Scenario:** Distributed team across US, Europe, and Asia timezones. No synchronous standup meetings. Daily written updates in Slack #standup channel.\n\n**Morning Routine (30 minutes):**\n\n```bash\n# 1. Generate draft standup from data sources\ngit log --author=\"$(git config user.name)\" --since=\"24 hours ago\" --oneline\n# Review commits, note key accomplishments\n\n# 2. Check Jira tickets\njira issues list --assignee currentUser() --status \"In Progress\"\n# Identify today's priorities\n\n# 3. Review Obsidian daily note from yesterday\n# Check for completed tasks, meeting outcomes\n\n# 4. Draft standup note in Obsidian\n# File: Daily Notes/Standup/2025-10-11.md\n\n# 5. Review teammates' standup notes (last 8 hours)\n# Identify opportunities to help, dependencies to note\n\n# 6. Post standup to Slack #standup channel (9:00 AM local time)\n# Copy from Obsidian, adjust formatting for Slack\n\n# 7. Set reminder to check thread responses by 11am\n# Respond to questions, offers of help\n\n# 8. Update task list with any new follow-ups from discussion\n```\n\n**Standup Note (Posted in Slack):**\n\n```markdown\n**ðŸŒ„ Standup - Oct 11** | @team-backend | Read time: 2min\n\n**âœ… Yesterday**\nâ€¢ Shipped v2 API authentication (JIRA-234) â†’ Production deployment successful, monitoring dashboards green\nâ€¢ Fixed race condition in job queue (JIRA-456) â†’ Reduced error rate from 2% to 0.1%\nâ€¢ Code review marathon: Reviewed 4 PRs from @alice, @bob, @charlie â†’ All merged\nâ€¢ Pair programming: Helped @diana debug webhook integration â†’ Issue resolved, she's unblocked\n\n**ðŸŽ¯ Today**\nâ€¢ **Priority 1:** Complete database migration script (JIRA-567) â†’ Target: Code complete + tested by 3pm\nâ€¢ **Priority 2:** Security audit prep â†’ Generate access logs report for compliance team\nâ€¢ **Priority 3:** Start API rate limiting implementation (JIRA-589) â†’ Spike and design doc\nâ€¢ **Meetings:** Architecture review at 11am PT, sprint planning at 2pm PT\n\n**ðŸš§ Blockers**\nâ€¢ None! (Yesterday's staging env blocker was resolved by @sre-team ðŸ™Œ)\n\n**ðŸ’¡ Notes**\nâ€¢ Database migration is sprint goal - will update thread when complete\nâ€¢ Available for pairing this afternoon if anyone needs database help\nâ€¢ Heads up: Deploying migration to staging at noon, expect ~10min downtime\n\n**ðŸ”— Links**\nâ€¢ [Active PRs](link) | [Sprint Board](link) | [Migration Runbook](link)\n\n---\nðŸ‘€ = I've read this | ðŸ¤ = I can help with something | ðŸ’¬ = Reply in thread\n```\n\n**Follow-Up Actions (Throughout Day):**\n\n```markdown\n# 11:00 AM - Check thread responses\nThread from @eve:\n> \"Can you review my DB schema changes PR before your migration? Want to make sure no conflicts\"\n\nResponse:\n> \"Absolutely! I'll review by 1pm so you have feedback before sprint planning. Link?\"\n\n# 3:00 PM - Progress update in thread\n> \"âœ… Update: Migration script complete and tested in staging. Dry-run successful, ready for prod deployment tomorrow. PR #892 up for review.\"\n\n# EOD - Tomorrow's setup\nAdd to tomorrow's \"Today\" section:\nâ€¢ Deploy database migration to production (scheduled 9am maintenance window)\nâ€¢ Monitor migration + rollback plan ready\nâ€¢ Post production status update in #engineering-announcements\n```\n\n**Weekly Retrospective (Friday):**\n\n```markdown\n# Review week of standup notes\nPatterns observed:\nâ€¢ âœ… Completed all 5 sprint stories\nâ€¢ âš ï¸ Database blocker cost 1.5 days - need faster SRE response process\nâ€¢ ðŸ’ª Code review throughput improved (avg 2.5 reviews/day vs 1.5 last week)\nâ€¢ ðŸŽ¯ Pairing sessions very productive (3 this week) - schedule more next sprint\n\nAction items:\nâ€¢ Talk to @sre-lead about expedited access request process\nâ€¢ Continue pairing schedule (blocking 2hrs/week)\nâ€¢ Next week: Focus on rate limiting implementation and technical debt\n```\n\n### Reference 2: AI-Powered Standup Generation System\n\n**System Architecture:**\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Data Collection Layer                                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ â€¢ Git commits (last 24-48h)                                 â”‚\nâ”‚ â€¢ Jira ticket updates (status changes, comments)            â”‚\nâ”‚ â€¢ Obsidian vault changes (daily notes, task completions)    â”‚\nâ”‚ â€¢ Calendar events (meetings attended, upcoming)             â”‚\nâ”‚ â€¢ Slack activity (mentions, threads participated in)        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ AI Analysis & Correlation Layer                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ â€¢ Link commits to Jira tickets (extract ticket IDs)         â”‚\nâ”‚ â€¢ Group related commits (same feature/bug)                  â”‚\nâ”‚ â€¢ Extract business value from technical changes             â”‚\nâ”‚ â€¢ Identify blockers from patterns (repeated attempts)       â”‚\nâ”‚ â€¢ Summarize meeting notes â†’ extract action items            â”‚\nâ”‚ â€¢ Calculate work distribution (feature vs bug vs review)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Generation & Formatting Layer                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ â€¢ Generate \"Yesterday\" from commits + completed tickets     â”‚\nâ”‚ â€¢ Generate \"Today\" from in-progress tickets + calendar      â”‚\nâ”‚ â€¢ Flag potential blockers from context clues                â”‚\nâ”‚ â€¢ Format for target platform (Slack/Discord/Email/Obsidian) â”‚\nâ”‚ â€¢ Add relevant links (PRs, tickets, docs)                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Human Review & Enhancement Layer                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ â€¢ Present draft for review                                  â”‚\nâ”‚ â€¢ Human adds context AI cannot infer                        â”‚\nâ”‚ â€¢ Adjust priorities based on team needs                     â”‚\nâ”‚ â€¢ Add personal notes, schedule changes                      â”‚\nâ”‚ â€¢ Approve and post to team channel                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Implementation Script:**\n\n```bash\n#!/bin/bash\n# generate-standup.sh - AI-powered standup note generator\n\nDATE=$(date +%Y-%m-%d)\nUSER=$(git config user.name)\nUSER_EMAIL=$(git config user.email)\n\necho \"ðŸ¤– Generating standup note for $USER on $DATE...\"\n\n# 1. Collect Git commits\necho \"ðŸ“Š Analyzing Git history...\"\nCOMMITS=$(git log --author=\"$USER\" --since=\"24 hours ago\" \\\n  --pretty=format:\"%h|%s|%cr\" --no-merges)\n\n# 2. Query Jira (requires jira CLI)\necho \"ðŸŽ« Fetching Jira tickets...\"\nJIRA_DONE=$(jira issues list --assignee currentUser() \\\n  --jql \"status CHANGED TO 'Done' DURING (-1d, now())\" \\\n  --template json)\n\nJIRA_PROGRESS=$(jira issues list --assignee currentUser() \\\n  --jql \"status = 'In Progress'\" \\\n  --template json)\n\n# 3. Get Obsidian recent changes (via MCP)\necho \"ðŸ“ Checking Obsidian vault...\"\nOBSIDIAN_CHANGES=$(obsidian_get_recent_changes --days 2)\n\n# 4. Get calendar events\necho \"ðŸ“… Fetching calendar...\"\nMEETINGS=$(gcal --today --format=json)\n\n# 5. Send to AI for analysis and generation\necho \"ðŸ§  Generating standup note with AI...\"\ncat << EOF > /tmp/standup-context.json\n{\n  \"date\": \"$DATE\",\n  \"user\": \"$USER\",\n  \"commits\": $(echo \"$COMMITS\" | jq -R -s -c 'split(\"\\n\")'),\n  \"jira_completed\": $JIRA_DONE,\n  \"jira_in_progress\": $JIRA_PROGRESS,\n  \"obsidian_changes\": $OBSIDIAN_CHANGES,\n  \"meetings\": $MEETINGS\n}\nEOF\n\n# AI prompt for standup generation\nSTANDUP_NOTE=$(claude-ai << 'PROMPT'\nAnalyze the provided context and generate a concise daily standup note.\n\nInstructions:\n- Group related commits into single accomplishment bullets\n- Link commits to Jira tickets where possible\n- Extract business value from technical changes\n- Format as: Yesterday / Today / Blockers\n- Keep bullets concise (1-2 lines each)\n- Include relevant links to PRs and tickets\n- Flag any potential blockers based on context\n\nContext: $(cat /tmp/standup-context.json)\n\nGenerate standup note in markdown format.\nPROMPT\n)\n\n# 6. Save draft to Obsidian\necho \"$STANDUP_NOTE\" > ~/Obsidian/Standup\\ Notes/$DATE.md\n\n# 7. Present for human review\necho \"âœ… Draft standup note generated!\"\necho \"\"\necho \"$STANDUP_NOTE\"\necho \"\"\nread -p \"Review the draft above. Post to Slack? (y/n) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    # 8. Post to Slack\n    slack-cli chat send --channel \"#standup\" --text \"$STANDUP_NOTE\"\n    echo \"ðŸ“® Posted to Slack #standup channel\"\nfi\n\necho \"ðŸ’¾ Saved to: ~/Obsidian/Standup Notes/$DATE.md\"\n```\n\n**AI Prompt Template for Standup Generation:**\n\n```\nYou are an expert at synthesizing engineering work into clear, concise standup updates.\n\nGiven the following data sources:\n- Git commits (last 24h)\n- Jira ticket updates\n- Obsidian daily notes\n- Calendar events\n\nGenerate a daily standup note that:\n\n1. **Yesterday Section:**\n   - Group related commits into single accomplishment statements\n   - Link commits to Jira tickets (extract ticket IDs from messages)\n   - Transform technical commits into business value (\"Implemented X to enable Y\")\n   - Include completed tickets with their status\n   - Summarize meeting outcomes from notes\n\n2. **Today Section:**\n   - List in-progress Jira tickets with current status\n   - Include planned meetings from calendar\n   - Estimate completion for ongoing work based on commit history\n   - Prioritize by ticket priority and sprint goals\n\n3. **Blockers Section:**\n   - Identify potential blockers from patterns:\n     * Multiple commits attempting same fix (indicates struggle)\n     * No commits on high-priority ticket (may be blocked)\n     * Comments in code mentioning \"TODO\" or \"FIXME\"\n   - Extract explicit blockers from daily notes\n   - Flag dependencies mentioned in Jira comments\n\nFormat:\n- Use markdown with clear headers\n- Bullet points for each item\n- Include hyperlinks to PRs, tickets, docs\n- Keep each bullet 1-2 lines maximum\n- Add emoji for visual scanning (âœ… âš ï¸ ðŸš€ etc.)\n\nTone: Professional but conversational, transparent about challenges\n\nOutput only the standup note markdown, no preamble.\n```\n\n**Cron Job Setup (Daily Automation):**\n\n```bash\n# Add to crontab: Run every weekday at 8:45 AM\n45 8 * * 1-5 /usr/local/bin/generate-standup.sh\n\n# Sends notification when draft is ready:\n# \"Your standup note is ready for review!\"\n# Opens Obsidian note and prepares Slack message\n```\n\n---\n\n**Tool Version:** 2.0 (Upgraded 2025-10-11)\n**Target Audience:** Remote-first engineering teams, async-first organizations, distributed teams\n**Dependencies:** Git, Jira CLI, Obsidian MCP, optional calendar integration\n**Estimated Setup Time:** 15 minutes initial setup, 5 minutes daily routine once automated\n"
              }
            ],
            "skills": []
          },
          {
            "name": "unit-testing",
            "description": "Unit testing with debugging and test automation",
            "source": "./plugins/unit-testing",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install unit-testing@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/test-generate",
                "description": null,
                "path": "plugins/unit-testing/commands/test-generate.md",
                "frontmatter": null,
                "content": "# Automated Unit Test Generation\n\nYou are a test automation expert specializing in generating comprehensive, maintainable unit tests across multiple languages and frameworks. Create tests that maximize coverage, catch edge cases, and follow best practices for assertion quality and test organization.\n\n## Context\n\nThe user needs automated test generation that analyzes code structure, identifies test scenarios, and creates high-quality unit tests with proper mocking, assertions, and edge case coverage. Focus on framework-specific patterns and maintainable test suites.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Code for Test Generation\n\nScan codebase to identify untested code and generate comprehensive test suites:\n\n```python\nimport ast\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nclass TestGenerator:\n    def __init__(self, language: str):\n        self.language = language\n        self.framework_map = {\n            'python': 'pytest',\n            'javascript': 'jest',\n            'typescript': 'jest',\n            'java': 'junit',\n            'go': 'testing'\n        }\n\n    def analyze_file(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Extract testable units from source file\"\"\"\n        if self.language == 'python':\n            return self._analyze_python(file_path)\n        elif self.language in ['javascript', 'typescript']:\n            return self._analyze_javascript(file_path)\n\n    def _analyze_python(self, file_path: str) -> Dict:\n        with open(file_path) as f:\n            tree = ast.parse(f.read())\n\n        functions = []\n        classes = []\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                functions.append({\n                    'name': node.name,\n                    'args': [arg.arg for arg in node.args.args],\n                    'returns': ast.unparse(node.returns) if node.returns else None,\n                    'decorators': [ast.unparse(d) for d in node.decorator_list],\n                    'docstring': ast.get_docstring(node),\n                    'complexity': self._calculate_complexity(node)\n                })\n            elif isinstance(node, ast.ClassDef):\n                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]\n                classes.append({\n                    'name': node.name,\n                    'methods': methods,\n                    'bases': [ast.unparse(base) for base in node.bases]\n                })\n\n        return {'functions': functions, 'classes': classes, 'file': file_path}\n```\n\n### 2. Generate Python Tests with pytest\n\n```python\ndef generate_pytest_tests(self, analysis: Dict) -> str:\n    \"\"\"Generate pytest test file from code analysis\"\"\"\n    tests = ['import pytest', 'from unittest.mock import Mock, patch', '']\n\n    module_name = Path(analysis['file']).stem\n    tests.append(f\"from {module_name} import *\\n\")\n\n    for func in analysis['functions']:\n        if func['name'].startswith('_'):\n            continue\n\n        test_class = self._generate_function_tests(func)\n        tests.append(test_class)\n\n    for cls in analysis['classes']:\n        test_class = self._generate_class_tests(cls)\n        tests.append(test_class)\n\n    return '\\n'.join(tests)\n\ndef _generate_function_tests(self, func: Dict) -> str:\n    \"\"\"Generate test cases for a function\"\"\"\n    func_name = func['name']\n    tests = [f\"\\n\\nclass Test{func_name.title()}:\"]\n\n    # Happy path test\n    tests.append(f\"    def test_{func_name}_success(self):\")\n    tests.append(f\"        result = {func_name}({self._generate_mock_args(func['args'])})\")\n    tests.append(f\"        assert result is not None\\n\")\n\n    # Edge case tests\n    if len(func['args']) > 0:\n        tests.append(f\"    def test_{func_name}_with_empty_input(self):\")\n        tests.append(f\"        with pytest.raises((ValueError, TypeError)):\")\n        tests.append(f\"            {func_name}({self._generate_empty_args(func['args'])})\\n\")\n\n    # Exception handling test\n    tests.append(f\"    def test_{func_name}_handles_errors(self):\")\n    tests.append(f\"        with pytest.raises(Exception):\")\n    tests.append(f\"            {func_name}({self._generate_invalid_args(func['args'])})\\n\")\n\n    return '\\n'.join(tests)\n\ndef _generate_class_tests(self, cls: Dict) -> str:\n    \"\"\"Generate test cases for a class\"\"\"\n    tests = [f\"\\n\\nclass Test{cls['name']}:\"]\n    tests.append(f\"    @pytest.fixture\")\n    tests.append(f\"    def instance(self):\")\n    tests.append(f\"        return {cls['name']}()\\n\")\n\n    for method in cls['methods']:\n        if method.startswith('_') and method != '__init__':\n            continue\n\n        tests.append(f\"    def test_{method}(self, instance):\")\n        tests.append(f\"        result = instance.{method}()\")\n        tests.append(f\"        assert result is not None\\n\")\n\n    return '\\n'.join(tests)\n```\n\n### 3. Generate JavaScript/TypeScript Tests with Jest\n\n```typescript\ninterface TestCase {\n  name: string;\n  setup?: string;\n  execution: string;\n  assertions: string[];\n}\n\nclass JestTestGenerator {\n  generateTests(functionName: string, params: string[]): string {\n    const tests: TestCase[] = [\n      {\n        name: `${functionName} returns expected result with valid input`,\n        execution: `const result = ${functionName}(${this.generateMockParams(params)})`,\n        assertions: ['expect(result).toBeDefined()', 'expect(result).not.toBeNull()']\n      },\n      {\n        name: `${functionName} handles null input gracefully`,\n        execution: `const result = ${functionName}(null)`,\n        assertions: ['expect(result).toBeDefined()']\n      },\n      {\n        name: `${functionName} throws error for invalid input`,\n        execution: `() => ${functionName}(undefined)`,\n        assertions: ['expect(execution).toThrow()']\n      }\n    ];\n\n    return this.formatJestSuite(functionName, tests);\n  }\n\n  formatJestSuite(name: string, cases: TestCase[]): string {\n    let output = `describe('${name}', () => {\\n`;\n\n    for (const testCase of cases) {\n      output += `  it('${testCase.name}', () => {\\n`;\n      if (testCase.setup) {\n        output += `    ${testCase.setup}\\n`;\n      }\n      output += `    const execution = ${testCase.execution};\\n`;\n      for (const assertion of testCase.assertions) {\n        output += `    ${assertion};\\n`;\n      }\n      output += `  });\\n\\n`;\n    }\n\n    output += '});\\n';\n    return output;\n  }\n\n  generateMockParams(params: string[]): string {\n    return params.map(p => `mock${p.charAt(0).toUpperCase() + p.slice(1)}`).join(', ');\n  }\n}\n```\n\n### 4. Generate React Component Tests\n\n```typescript\nfunction generateReactComponentTest(componentName: string): string {\n  return `\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { ${componentName} } from './${componentName}';\n\ndescribe('${componentName}', () => {\n  it('renders without crashing', () => {\n    render(<${componentName} />);\n    expect(screen.getByRole('main')).toBeInTheDocument();\n  });\n\n  it('displays correct initial state', () => {\n    render(<${componentName} />);\n    const element = screen.getByTestId('${componentName.toLowerCase()}');\n    expect(element).toBeVisible();\n  });\n\n  it('handles user interaction', () => {\n    render(<${componentName} />);\n    const button = screen.getByRole('button');\n    fireEvent.click(button);\n    expect(screen.getByText(/clicked/i)).toBeInTheDocument();\n  });\n\n  it('updates props correctly', () => {\n    const { rerender } = render(<${componentName} value=\"initial\" />);\n    expect(screen.getByText('initial')).toBeInTheDocument();\n\n    rerender(<${componentName} value=\"updated\" />);\n    expect(screen.getByText('updated')).toBeInTheDocument();\n  });\n});\n`;\n}\n```\n\n### 5. Coverage Analysis and Gap Detection\n\n```python\nimport subprocess\nimport json\n\nclass CoverageAnalyzer:\n    def analyze_coverage(self, test_command: str) -> Dict:\n        \"\"\"Run tests with coverage and identify gaps\"\"\"\n        result = subprocess.run(\n            [test_command, '--coverage', '--json'],\n            capture_output=True,\n            text=True\n        )\n\n        coverage_data = json.loads(result.stdout)\n        gaps = self.identify_coverage_gaps(coverage_data)\n\n        return {\n            'overall_coverage': coverage_data.get('totals', {}).get('percent_covered', 0),\n            'uncovered_lines': gaps,\n            'files_below_threshold': self.find_low_coverage_files(coverage_data, 80)\n        }\n\n    def identify_coverage_gaps(self, coverage: Dict) -> List[Dict]:\n        \"\"\"Find specific lines/functions without test coverage\"\"\"\n        gaps = []\n        for file_path, data in coverage.get('files', {}).items():\n            missing_lines = data.get('missing_lines', [])\n            if missing_lines:\n                gaps.append({\n                    'file': file_path,\n                    'lines': missing_lines,\n                    'functions': data.get('excluded_lines', [])\n                })\n        return gaps\n\n    def generate_tests_for_gaps(self, gaps: List[Dict]) -> str:\n        \"\"\"Generate tests specifically for uncovered code\"\"\"\n        tests = []\n        for gap in gaps:\n            test_code = self.create_targeted_test(gap)\n            tests.append(test_code)\n        return '\\n\\n'.join(tests)\n```\n\n### 6. Mock Generation\n\n```python\ndef generate_mock_objects(self, dependencies: List[str]) -> str:\n    \"\"\"Generate mock objects for external dependencies\"\"\"\n    mocks = ['from unittest.mock import Mock, MagicMock, patch\\n']\n\n    for dep in dependencies:\n        mocks.append(f\"@pytest.fixture\")\n        mocks.append(f\"def mock_{dep}():\")\n        mocks.append(f\"    mock = Mock(spec={dep})\")\n        mocks.append(f\"    mock.method.return_value = 'mocked_result'\")\n        mocks.append(f\"    return mock\\n\")\n\n    return '\\n'.join(mocks)\n```\n\n## Output Format\n\n1. **Test Files**: Complete test suites ready to run\n2. **Coverage Report**: Current coverage with gaps identified\n3. **Mock Objects**: Fixtures for external dependencies\n4. **Test Documentation**: Explanation of test scenarios\n5. **CI Integration**: Commands to run tests in pipeline\n\nFocus on generating maintainable, comprehensive tests that catch bugs early and provide confidence in code changes.\n"
              }
            ],
            "skills": []
          },
          {
            "name": "python-development",
            "description": "Python development with Django, FastAPI, async patterns",
            "source": "./plugins/python-development",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install python-development@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/python-scaffold",
                "description": null,
                "path": "plugins/python-development/commands/python-scaffold.md",
                "frontmatter": null,
                "content": "# Python Project Scaffolding\n\nYou are a Python project architecture expert specializing in scaffolding production-ready Python applications. Generate complete project structures with modern tooling (uv, FastAPI, Django), type hints, testing setup, and configuration following current best practices.\n\n## Context\n\nThe user needs automated Python project scaffolding that creates consistent, type-safe applications with proper structure, dependency management, testing, and tooling. Focus on modern Python patterns and scalable architecture.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Project Type\n\nDetermine the project type from user requirements:\n- **FastAPI**: REST APIs, microservices, async applications\n- **Django**: Full-stack web applications, admin panels, ORM-heavy projects\n- **Library**: Reusable packages, utilities, tools\n- **CLI**: Command-line tools, automation scripts\n- **Generic**: Standard Python applications\n\n### 2. Initialize Project with uv\n\n```bash\n# Create new project with uv\nuv init <project-name>\ncd <project-name>\n\n# Initialize git repository\ngit init\necho \".venv/\" >> .gitignore\necho \"*.pyc\" >> .gitignore\necho \"__pycache__/\" >> .gitignore\necho \".pytest_cache/\" >> .gitignore\necho \".ruff_cache/\" >> .gitignore\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n### 3. Generate FastAPI Project Structure\n\n```\nfastapi-project/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ .env.example\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ project_name/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ main.py\nâ”‚       â”œâ”€â”€ config.py\nâ”‚       â”œâ”€â”€ api/\nâ”‚       â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â”œâ”€â”€ deps.py\nâ”‚       â”‚   â”œâ”€â”€ v1/\nâ”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â”‚   â”œâ”€â”€ endpoints/\nâ”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ users.py\nâ”‚       â”‚   â”‚   â”‚   â””â”€â”€ health.py\nâ”‚       â”‚   â”‚   â””â”€â”€ router.py\nâ”‚       â”œâ”€â”€ core/\nâ”‚       â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â”œâ”€â”€ security.py\nâ”‚       â”‚   â””â”€â”€ database.py\nâ”‚       â”œâ”€â”€ models/\nâ”‚       â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â””â”€â”€ user.py\nâ”‚       â”œâ”€â”€ schemas/\nâ”‚       â”‚   â”œâ”€â”€ __init__.py\nâ”‚       â”‚   â””â”€â”€ user.py\nâ”‚       â””â”€â”€ services/\nâ”‚           â”œâ”€â”€ __init__.py\nâ”‚           â””â”€â”€ user_service.py\nâ””â”€â”€ tests/\n    â”œâ”€â”€ __init__.py\n    â”œâ”€â”€ conftest.py\n    â””â”€â”€ api/\n        â”œâ”€â”€ __init__.py\n        â””â”€â”€ test_users.py\n```\n\n**pyproject.toml**:\n```toml\n[project]\nname = \"project-name\"\nversion = \"0.1.0\"\ndescription = \"FastAPI project description\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fastapi>=0.110.0\",\n    \"uvicorn[standard]>=0.27.0\",\n    \"pydantic>=2.6.0\",\n    \"pydantic-settings>=2.1.0\",\n    \"sqlalchemy>=2.0.0\",\n    \"alembic>=1.13.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=8.0.0\",\n    \"pytest-asyncio>=0.23.0\",\n    \"httpx>=0.26.0\",\n    \"ruff>=0.2.0\",\n]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nasyncio_mode = \"auto\"\n```\n\n**src/project_name/main.py**:\n```python\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom .api.v1.router import api_router\nfrom .config import settings\n\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n    version=settings.VERSION,\n    openapi_url=f\"{settings.API_V1_PREFIX}/openapi.json\",\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(api_router, prefix=settings.API_V1_PREFIX)\n\n@app.get(\"/health\")\nasync def health_check() -> dict[str, str]:\n    return {\"status\": \"healthy\"}\n```\n\n### 4. Generate Django Project Structure\n\n```bash\n# Install Django with uv\nuv add django django-environ django-debug-toolbar\n\n# Create Django project\ndjango-admin startproject config .\npython manage.py startapp core\n```\n\n**pyproject.toml for Django**:\n```toml\n[project]\nname = \"django-project\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"django>=5.0.0\",\n    \"django-environ>=0.11.0\",\n    \"psycopg[binary]>=3.1.0\",\n    \"gunicorn>=21.2.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"django-debug-toolbar>=4.3.0\",\n    \"pytest-django>=4.8.0\",\n    \"ruff>=0.2.0\",\n]\n```\n\n### 5. Generate Python Library Structure\n\n```\nlibrary-name/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ LICENSE\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ library_name/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ py.typed\nâ”‚       â””â”€â”€ core.py\nâ””â”€â”€ tests/\n    â”œâ”€â”€ __init__.py\n    â””â”€â”€ test_core.py\n```\n\n**pyproject.toml for Library**:\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"library-name\"\nversion = \"0.1.0\"\ndescription = \"Library description\"\nreadme = \"README.md\"\nrequires-python = \">=3.11\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"email@example.com\"}\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n]\ndependencies = []\n\n[project.optional-dependencies]\ndev = [\"pytest>=8.0.0\", \"ruff>=0.2.0\", \"mypy>=1.8.0\"]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/library_name\"]\n```\n\n### 6. Generate CLI Tool Structure\n\n```python\n# pyproject.toml\n[project.scripts]\ncli-name = \"project_name.cli:main\"\n\n[project]\ndependencies = [\n    \"typer>=0.9.0\",\n    \"rich>=13.7.0\",\n]\n```\n\n**src/project_name/cli.py**:\n```python\nimport typer\nfrom rich.console import Console\n\napp = typer.Typer()\nconsole = Console()\n\n@app.command()\ndef hello(name: str = typer.Option(..., \"--name\", \"-n\", help=\"Your name\")):\n    \"\"\"Greet someone\"\"\"\n    console.print(f\"[bold green]Hello {name}![/bold green]\")\n\ndef main():\n    app()\n```\n\n### 7. Configure Development Tools\n\n**.env.example**:\n```env\n# Application\nPROJECT_NAME=\"Project Name\"\nVERSION=\"0.1.0\"\nDEBUG=True\n\n# API\nAPI_V1_PREFIX=\"/api/v1\"\nALLOWED_ORIGINS=[\"http://localhost:3000\"]\n\n# Database\nDATABASE_URL=\"postgresql://user:pass@localhost:5432/dbname\"\n\n# Security\nSECRET_KEY=\"your-secret-key-here\"\n```\n\n**Makefile**:\n```makefile\n.PHONY: install dev test lint format clean\n\ninstall:\n\tuv sync\n\ndev:\n\tuv run uvicorn src.project_name.main:app --reload\n\ntest:\n\tuv run pytest -v\n\nlint:\n\tuv run ruff check .\n\nformat:\n\tuv run ruff format .\n\nclean:\n\tfind . -type d -name __pycache__ -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\trm -rf .pytest_cache .ruff_cache\n```\n\n## Output Format\n\n1. **Project Structure**: Complete directory tree with all necessary files\n2. **Configuration**: pyproject.toml with dependencies and tool settings\n3. **Entry Point**: Main application file (main.py, cli.py, etc.)\n4. **Tests**: Test structure with pytest configuration\n5. **Documentation**: README with setup and usage instructions\n6. **Development Tools**: Makefile, .env.example, .gitignore\n\nFocus on creating production-ready Python projects with modern tooling, type safety, and comprehensive testing setup.\n"
              }
            ],
            "skills": [
              {
                "name": "async-python-patterns",
                "description": "Master Python asyncio, concurrent programming, and async/await patterns for high-performance applications. Use when building async APIs, concurrent systems, or I/O-bound applications requiring non-blocking operations.",
                "path": "plugins/python-development/skills/async-python-patterns/SKILL.md",
                "frontmatter": {
                  "name": "async-python-patterns",
                  "description": "Master Python asyncio, concurrent programming, and async/await patterns for high-performance applications. Use when building async APIs, concurrent systems, or I/O-bound applications requiring non-blocking operations."
                },
                "content": "# Async Python Patterns\n\nComprehensive guidance for implementing asynchronous Python applications using asyncio, concurrent programming patterns, and async/await for building high-performance, non-blocking systems.\n\n## When to Use This Skill\n\n- Building async web APIs (FastAPI, aiohttp, Sanic)\n- Implementing concurrent I/O operations (database, file, network)\n- Creating web scrapers with concurrent requests\n- Developing real-time applications (WebSocket servers, chat systems)\n- Processing multiple independent tasks simultaneously\n- Building microservices with async communication\n- Optimizing I/O-bound workloads\n- Implementing async background tasks and queues\n\n## Core Concepts\n\n### 1. Event Loop\nThe event loop is the heart of asyncio, managing and scheduling asynchronous tasks.\n\n**Key characteristics:**\n- Single-threaded cooperative multitasking\n- Schedules coroutines for execution\n- Handles I/O operations without blocking\n- Manages callbacks and futures\n\n### 2. Coroutines\nFunctions defined with `async def` that can be paused and resumed.\n\n**Syntax:**\n```python\nasync def my_coroutine():\n    result = await some_async_operation()\n    return result\n```\n\n### 3. Tasks\nScheduled coroutines that run concurrently on the event loop.\n\n### 4. Futures\nLow-level objects representing eventual results of async operations.\n\n### 5. Async Context Managers\nResources that support `async with` for proper cleanup.\n\n### 6. Async Iterators\nObjects that support `async for` for iterating over async data sources.\n\n## Quick Start\n\n```python\nimport asyncio\n\nasync def main():\n    print(\"Hello\")\n    await asyncio.sleep(1)\n    print(\"World\")\n\n# Python 3.7+\nasyncio.run(main())\n```\n\n## Fundamental Patterns\n\n### Pattern 1: Basic Async/Await\n\n```python\nimport asyncio\n\nasync def fetch_data(url: str) -> dict:\n    \"\"\"Fetch data from URL asynchronously.\"\"\"\n    await asyncio.sleep(1)  # Simulate I/O\n    return {\"url\": url, \"data\": \"result\"}\n\nasync def main():\n    result = await fetch_data(\"https://api.example.com\")\n    print(result)\n\nasyncio.run(main())\n```\n\n### Pattern 2: Concurrent Execution with gather()\n\n```python\nimport asyncio\nfrom typing import List\n\nasync def fetch_user(user_id: int) -> dict:\n    \"\"\"Fetch user data.\"\"\"\n    await asyncio.sleep(0.5)\n    return {\"id\": user_id, \"name\": f\"User {user_id}\"}\n\nasync def fetch_all_users(user_ids: List[int]) -> List[dict]:\n    \"\"\"Fetch multiple users concurrently.\"\"\"\n    tasks = [fetch_user(uid) for uid in user_ids]\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def main():\n    user_ids = [1, 2, 3, 4, 5]\n    users = await fetch_all_users(user_ids)\n    print(f\"Fetched {len(users)} users\")\n\nasyncio.run(main())\n```\n\n### Pattern 3: Task Creation and Management\n\n```python\nimport asyncio\n\nasync def background_task(name: str, delay: int):\n    \"\"\"Long-running background task.\"\"\"\n    print(f\"{name} started\")\n    await asyncio.sleep(delay)\n    print(f\"{name} completed\")\n    return f\"Result from {name}\"\n\nasync def main():\n    # Create tasks\n    task1 = asyncio.create_task(background_task(\"Task 1\", 2))\n    task2 = asyncio.create_task(background_task(\"Task 2\", 1))\n\n    # Do other work\n    print(\"Main: doing other work\")\n    await asyncio.sleep(0.5)\n\n    # Wait for tasks\n    result1 = await task1\n    result2 = await task2\n\n    print(f\"Results: {result1}, {result2}\")\n\nasyncio.run(main())\n```\n\n### Pattern 4: Error Handling in Async Code\n\n```python\nimport asyncio\nfrom typing import List, Optional\n\nasync def risky_operation(item_id: int) -> dict:\n    \"\"\"Operation that might fail.\"\"\"\n    await asyncio.sleep(0.1)\n    if item_id % 3 == 0:\n        raise ValueError(f\"Item {item_id} failed\")\n    return {\"id\": item_id, \"status\": \"success\"}\n\nasync def safe_operation(item_id: int) -> Optional[dict]:\n    \"\"\"Wrapper with error handling.\"\"\"\n    try:\n        return await risky_operation(item_id)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return None\n\nasync def process_items(item_ids: List[int]):\n    \"\"\"Process multiple items with error handling.\"\"\"\n    tasks = [safe_operation(iid) for iid in item_ids]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Filter out failures\n    successful = [r for r in results if r is not None and not isinstance(r, Exception)]\n    failed = [r for r in results if isinstance(r, Exception)]\n\n    print(f\"Success: {len(successful)}, Failed: {len(failed)}\")\n    return successful\n\nasyncio.run(process_items([1, 2, 3, 4, 5, 6]))\n```\n\n### Pattern 5: Timeout Handling\n\n```python\nimport asyncio\n\nasync def slow_operation(delay: int) -> str:\n    \"\"\"Operation that takes time.\"\"\"\n    await asyncio.sleep(delay)\n    return f\"Completed after {delay}s\"\n\nasync def with_timeout():\n    \"\"\"Execute operation with timeout.\"\"\"\n    try:\n        result = await asyncio.wait_for(slow_operation(5), timeout=2.0)\n        print(result)\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n\nasyncio.run(with_timeout())\n```\n\n## Advanced Patterns\n\n### Pattern 6: Async Context Managers\n\n```python\nimport asyncio\nfrom typing import Optional\n\nclass AsyncDatabaseConnection:\n    \"\"\"Async database connection context manager.\"\"\"\n\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self.connection: Optional[object] = None\n\n    async def __aenter__(self):\n        print(\"Opening connection\")\n        await asyncio.sleep(0.1)  # Simulate connection\n        self.connection = {\"dsn\": self.dsn, \"connected\": True}\n        return self.connection\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        print(\"Closing connection\")\n        await asyncio.sleep(0.1)  # Simulate cleanup\n        self.connection = None\n\nasync def query_database():\n    \"\"\"Use async context manager.\"\"\"\n    async with AsyncDatabaseConnection(\"postgresql://localhost\") as conn:\n        print(f\"Using connection: {conn}\")\n        await asyncio.sleep(0.2)  # Simulate query\n        return {\"rows\": 10}\n\nasyncio.run(query_database())\n```\n\n### Pattern 7: Async Iterators and Generators\n\n```python\nimport asyncio\nfrom typing import AsyncIterator\n\nasync def async_range(start: int, end: int, delay: float = 0.1) -> AsyncIterator[int]:\n    \"\"\"Async generator that yields numbers with delay.\"\"\"\n    for i in range(start, end):\n        await asyncio.sleep(delay)\n        yield i\n\nasync def fetch_pages(url: str, max_pages: int) -> AsyncIterator[dict]:\n    \"\"\"Fetch paginated data asynchronously.\"\"\"\n    for page in range(1, max_pages + 1):\n        await asyncio.sleep(0.2)  # Simulate API call\n        yield {\n            \"page\": page,\n            \"url\": f\"{url}?page={page}\",\n            \"data\": [f\"item_{page}_{i}\" for i in range(5)]\n        }\n\nasync def consume_async_iterator():\n    \"\"\"Consume async iterator.\"\"\"\n    async for number in async_range(1, 5):\n        print(f\"Number: {number}\")\n\n    print(\"\\nFetching pages:\")\n    async for page_data in fetch_pages(\"https://api.example.com/items\", 3):\n        print(f\"Page {page_data['page']}: {len(page_data['data'])} items\")\n\nasyncio.run(consume_async_iterator())\n```\n\n### Pattern 8: Producer-Consumer Pattern\n\n```python\nimport asyncio\nfrom asyncio import Queue\nfrom typing import Optional\n\nasync def producer(queue: Queue, producer_id: int, num_items: int):\n    \"\"\"Produce items and put them in queue.\"\"\"\n    for i in range(num_items):\n        item = f\"Item-{producer_id}-{i}\"\n        await queue.put(item)\n        print(f\"Producer {producer_id} produced: {item}\")\n        await asyncio.sleep(0.1)\n    await queue.put(None)  # Signal completion\n\nasync def consumer(queue: Queue, consumer_id: int):\n    \"\"\"Consume items from queue.\"\"\"\n    while True:\n        item = await queue.get()\n        if item is None:\n            queue.task_done()\n            break\n\n        print(f\"Consumer {consumer_id} processing: {item}\")\n        await asyncio.sleep(0.2)  # Simulate work\n        queue.task_done()\n\nasync def producer_consumer_example():\n    \"\"\"Run producer-consumer pattern.\"\"\"\n    queue = Queue(maxsize=10)\n\n    # Create tasks\n    producers = [\n        asyncio.create_task(producer(queue, i, 5))\n        for i in range(2)\n    ]\n\n    consumers = [\n        asyncio.create_task(consumer(queue, i))\n        for i in range(3)\n    ]\n\n    # Wait for producers\n    await asyncio.gather(*producers)\n\n    # Wait for queue to be empty\n    await queue.join()\n\n    # Cancel consumers\n    for c in consumers:\n        c.cancel()\n\nasyncio.run(producer_consumer_example())\n```\n\n### Pattern 9: Semaphore for Rate Limiting\n\n```python\nimport asyncio\nfrom typing import List\n\nasync def api_call(url: str, semaphore: asyncio.Semaphore) -> dict:\n    \"\"\"Make API call with rate limiting.\"\"\"\n    async with semaphore:\n        print(f\"Calling {url}\")\n        await asyncio.sleep(0.5)  # Simulate API call\n        return {\"url\": url, \"status\": 200}\n\nasync def rate_limited_requests(urls: List[str], max_concurrent: int = 5):\n    \"\"\"Make multiple requests with rate limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n    tasks = [api_call(url, semaphore) for url in urls]\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def main():\n    urls = [f\"https://api.example.com/item/{i}\" for i in range(20)]\n    results = await rate_limited_requests(urls, max_concurrent=3)\n    print(f\"Completed {len(results)} requests\")\n\nasyncio.run(main())\n```\n\n### Pattern 10: Async Locks and Synchronization\n\n```python\nimport asyncio\n\nclass AsyncCounter:\n    \"\"\"Thread-safe async counter.\"\"\"\n\n    def __init__(self):\n        self.value = 0\n        self.lock = asyncio.Lock()\n\n    async def increment(self):\n        \"\"\"Safely increment counter.\"\"\"\n        async with self.lock:\n            current = self.value\n            await asyncio.sleep(0.01)  # Simulate work\n            self.value = current + 1\n\n    async def get_value(self) -> int:\n        \"\"\"Get current value.\"\"\"\n        async with self.lock:\n            return self.value\n\nasync def worker(counter: AsyncCounter, worker_id: int):\n    \"\"\"Worker that increments counter.\"\"\"\n    for _ in range(10):\n        await counter.increment()\n        print(f\"Worker {worker_id} incremented\")\n\nasync def test_counter():\n    \"\"\"Test concurrent counter.\"\"\"\n    counter = AsyncCounter()\n\n    workers = [asyncio.create_task(worker(counter, i)) for i in range(5)]\n    await asyncio.gather(*workers)\n\n    final_value = await counter.get_value()\n    print(f\"Final counter value: {final_value}\")\n\nasyncio.run(test_counter())\n```\n\n## Real-World Applications\n\n### Web Scraping with aiohttp\n\n```python\nimport asyncio\nimport aiohttp\nfrom typing import List, Dict\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> Dict:\n    \"\"\"Fetch single URL.\"\"\"\n    try:\n        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:\n            text = await response.text()\n            return {\n                \"url\": url,\n                \"status\": response.status,\n                \"length\": len(text)\n            }\n    except Exception as e:\n        return {\"url\": url, \"error\": str(e)}\n\nasync def scrape_urls(urls: List[str]) -> List[Dict]:\n    \"\"\"Scrape multiple URLs concurrently.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n        return results\n\nasync def main():\n    urls = [\n        \"https://httpbin.org/delay/1\",\n        \"https://httpbin.org/delay/2\",\n        \"https://httpbin.org/status/404\",\n    ]\n\n    results = await scrape_urls(urls)\n    for result in results:\n        print(result)\n\nasyncio.run(main())\n```\n\n### Async Database Operations\n\n```python\nimport asyncio\nfrom typing import List, Optional\n\n# Simulated async database client\nclass AsyncDB:\n    \"\"\"Simulated async database.\"\"\"\n\n    async def execute(self, query: str) -> List[dict]:\n        \"\"\"Execute query.\"\"\"\n        await asyncio.sleep(0.1)\n        return [{\"id\": 1, \"name\": \"Example\"}]\n\n    async def fetch_one(self, query: str) -> Optional[dict]:\n        \"\"\"Fetch single row.\"\"\"\n        await asyncio.sleep(0.1)\n        return {\"id\": 1, \"name\": \"Example\"}\n\nasync def get_user_data(db: AsyncDB, user_id: int) -> dict:\n    \"\"\"Fetch user and related data concurrently.\"\"\"\n    user_task = db.fetch_one(f\"SELECT * FROM users WHERE id = {user_id}\")\n    orders_task = db.execute(f\"SELECT * FROM orders WHERE user_id = {user_id}\")\n    profile_task = db.fetch_one(f\"SELECT * FROM profiles WHERE user_id = {user_id}\")\n\n    user, orders, profile = await asyncio.gather(user_task, orders_task, profile_task)\n\n    return {\n        \"user\": user,\n        \"orders\": orders,\n        \"profile\": profile\n    }\n\nasync def main():\n    db = AsyncDB()\n    user_data = await get_user_data(db, 1)\n    print(user_data)\n\nasyncio.run(main())\n```\n\n### WebSocket Server\n\n```python\nimport asyncio\nfrom typing import Set\n\n# Simulated WebSocket connection\nclass WebSocket:\n    \"\"\"Simulated WebSocket.\"\"\"\n\n    def __init__(self, client_id: str):\n        self.client_id = client_id\n\n    async def send(self, message: str):\n        \"\"\"Send message.\"\"\"\n        print(f\"Sending to {self.client_id}: {message}\")\n        await asyncio.sleep(0.01)\n\n    async def recv(self) -> str:\n        \"\"\"Receive message.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Message from {self.client_id}\"\n\nclass WebSocketServer:\n    \"\"\"Simple WebSocket server.\"\"\"\n\n    def __init__(self):\n        self.clients: Set[WebSocket] = set()\n\n    async def register(self, websocket: WebSocket):\n        \"\"\"Register new client.\"\"\"\n        self.clients.add(websocket)\n        print(f\"Client {websocket.client_id} connected\")\n\n    async def unregister(self, websocket: WebSocket):\n        \"\"\"Unregister client.\"\"\"\n        self.clients.remove(websocket)\n        print(f\"Client {websocket.client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        \"\"\"Broadcast message to all clients.\"\"\"\n        if self.clients:\n            tasks = [client.send(message) for client in self.clients]\n            await asyncio.gather(*tasks)\n\n    async def handle_client(self, websocket: WebSocket):\n        \"\"\"Handle individual client connection.\"\"\"\n        await self.register(websocket)\n        try:\n            async for message in self.message_iterator(websocket):\n                await self.broadcast(f\"{websocket.client_id}: {message}\")\n        finally:\n            await self.unregister(websocket)\n\n    async def message_iterator(self, websocket: WebSocket):\n        \"\"\"Iterate over messages from client.\"\"\"\n        for _ in range(3):  # Simulate 3 messages\n            yield await websocket.recv()\n```\n\n## Performance Best Practices\n\n### 1. Use Connection Pools\n\n```python\nimport asyncio\nimport aiohttp\n\nasync def with_connection_pool():\n    \"\"\"Use connection pool for efficiency.\"\"\"\n    connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)\n\n    async with aiohttp.ClientSession(connector=connector) as session:\n        tasks = [session.get(f\"https://api.example.com/item/{i}\") for i in range(50)]\n        responses = await asyncio.gather(*tasks)\n        return responses\n```\n\n### 2. Batch Operations\n\n```python\nasync def batch_process(items: List[str], batch_size: int = 10):\n    \"\"\"Process items in batches.\"\"\"\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        tasks = [process_item(item) for item in batch]\n        await asyncio.gather(*tasks)\n        print(f\"Processed batch {i // batch_size + 1}\")\n\nasync def process_item(item: str):\n    \"\"\"Process single item.\"\"\"\n    await asyncio.sleep(0.1)\n    return f\"Processed: {item}\"\n```\n\n### 3. Avoid Blocking Operations\n\n```python\nimport asyncio\nimport concurrent.futures\nfrom typing import Any\n\ndef blocking_operation(data: Any) -> Any:\n    \"\"\"CPU-intensive blocking operation.\"\"\"\n    import time\n    time.sleep(1)\n    return data * 2\n\nasync def run_in_executor(data: Any) -> Any:\n    \"\"\"Run blocking operation in thread pool.\"\"\"\n    loop = asyncio.get_event_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, blocking_operation, data)\n        return result\n\nasync def main():\n    results = await asyncio.gather(*[run_in_executor(i) for i in range(5)])\n    print(results)\n\nasyncio.run(main())\n```\n\n## Common Pitfalls\n\n### 1. Forgetting await\n\n```python\n# Wrong - returns coroutine object, doesn't execute\nresult = async_function()\n\n# Correct\nresult = await async_function()\n```\n\n### 2. Blocking the Event Loop\n\n```python\n# Wrong - blocks event loop\nimport time\nasync def bad():\n    time.sleep(1)  # Blocks!\n\n# Correct\nasync def good():\n    await asyncio.sleep(1)  # Non-blocking\n```\n\n### 3. Not Handling Cancellation\n\n```python\nasync def cancelable_task():\n    \"\"\"Task that handles cancellation.\"\"\"\n    try:\n        while True:\n            await asyncio.sleep(1)\n            print(\"Working...\")\n    except asyncio.CancelledError:\n        print(\"Task cancelled, cleaning up...\")\n        # Perform cleanup\n        raise  # Re-raise to propagate cancellation\n```\n\n### 4. Mixing Sync and Async Code\n\n```python\n# Wrong - can't call async from sync directly\ndef sync_function():\n    result = await async_function()  # SyntaxError!\n\n# Correct\ndef sync_function():\n    result = asyncio.run(async_function())\n```\n\n## Testing Async Code\n\n```python\nimport asyncio\nimport pytest\n\n# Using pytest-asyncio\n@pytest.mark.asyncio\nasync def test_async_function():\n    \"\"\"Test async function.\"\"\"\n    result = await fetch_data(\"https://api.example.com\")\n    assert result is not None\n\n@pytest.mark.asyncio\nasync def test_with_timeout():\n    \"\"\"Test with timeout.\"\"\"\n    with pytest.raises(asyncio.TimeoutError):\n        await asyncio.wait_for(slow_operation(5), timeout=1.0)\n```\n\n## Resources\n\n- **Python asyncio documentation**: https://docs.python.org/3/library/asyncio.html\n- **aiohttp**: Async HTTP client/server\n- **FastAPI**: Modern async web framework\n- **asyncpg**: Async PostgreSQL driver\n- **motor**: Async MongoDB driver\n\n## Best Practices Summary\n\n1. **Use asyncio.run()** for entry point (Python 3.7+)\n2. **Always await coroutines** to execute them\n3. **Use gather() for concurrent execution** of multiple tasks\n4. **Implement proper error handling** with try/except\n5. **Use timeouts** to prevent hanging operations\n6. **Pool connections** for better performance\n7. **Avoid blocking operations** in async code\n8. **Use semaphores** for rate limiting\n9. **Handle task cancellation** properly\n10. **Test async code** with pytest-asyncio"
              },
              {
                "name": "python-packaging",
                "description": "Create distributable Python packages with proper project structure, setup.py/pyproject.toml, and publishing to PyPI. Use when packaging Python libraries, creating CLI tools, or distributing Python code.",
                "path": "plugins/python-development/skills/python-packaging/SKILL.md",
                "frontmatter": {
                  "name": "python-packaging",
                  "description": "Create distributable Python packages with proper project structure, setup.py/pyproject.toml, and publishing to PyPI. Use when packaging Python libraries, creating CLI tools, or distributing Python code."
                },
                "content": "# Python Packaging\n\nComprehensive guide to creating, structuring, and distributing Python packages using modern packaging tools, pyproject.toml, and publishing to PyPI.\n\n## When to Use This Skill\n\n- Creating Python libraries for distribution\n- Building command-line tools with entry points\n- Publishing packages to PyPI or private repositories\n- Setting up Python project structure\n- Creating installable packages with dependencies\n- Building wheels and source distributions\n- Versioning and releasing Python packages\n- Creating namespace packages\n- Implementing package metadata and classifiers\n\n## Core Concepts\n\n### 1. Package Structure\n- **Source layout**: `src/package_name/` (recommended)\n- **Flat layout**: `package_name/` (simpler but less flexible)\n- **Package metadata**: pyproject.toml, setup.py, or setup.cfg\n- **Distribution formats**: wheel (.whl) and source distribution (.tar.gz)\n\n### 2. Modern Packaging Standards\n- **PEP 517/518**: Build system requirements\n- **PEP 621**: Metadata in pyproject.toml\n- **PEP 660**: Editable installs\n- **pyproject.toml**: Single source of configuration\n\n### 3. Build Backends\n- **setuptools**: Traditional, widely used\n- **hatchling**: Modern, opinionated\n- **flit**: Lightweight, for pure Python\n- **poetry**: Dependency management + packaging\n\n### 4. Distribution\n- **PyPI**: Python Package Index (public)\n- **TestPyPI**: Testing before production\n- **Private repositories**: JFrog, AWS CodeArtifact, etc.\n\n## Quick Start\n\n### Minimal Package Structure\n\n```\nmy-package/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ LICENSE\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ my_package/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ module.py\nâ””â”€â”€ tests/\n    â””â”€â”€ test_module.py\n```\n\n### Minimal pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"A short description\"\nauthors = [{name = \"Your Name\", email = \"you@example.com\"}]\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"requests>=2.28.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0\",\n    \"black>=22.0\",\n]\n```\n\n## Package Structure Patterns\n\n### Pattern 1: Source Layout (Recommended)\n\n```\nmy-package/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ LICENSE\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ my_package/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ core.py\nâ”‚       â”œâ”€â”€ utils.py\nâ”‚       â””â”€â”€ py.typed          # For type hints\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ test_core.py\nâ”‚   â””â”€â”€ test_utils.py\nâ””â”€â”€ docs/\n    â””â”€â”€ index.md\n```\n\n**Advantages:**\n- Prevents accidentally importing from source\n- Cleaner test imports\n- Better isolation\n\n**pyproject.toml for source layout:**\n```toml\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n```\n\n### Pattern 2: Flat Layout\n\n```\nmy-package/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ my_package/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ module.py\nâ””â”€â”€ tests/\n    â””â”€â”€ test_module.py\n```\n\n**Simpler but:**\n- Can import package without installing\n- Less professional for libraries\n\n### Pattern 3: Multi-Package Project\n\n```\nproject/\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ packages/\nâ”‚   â”œâ”€â”€ package-a/\nâ”‚   â”‚   â””â”€â”€ src/\nâ”‚   â”‚       â””â”€â”€ package_a/\nâ”‚   â””â”€â”€ package-b/\nâ”‚       â””â”€â”€ src/\nâ”‚           â””â”€â”€ package_b/\nâ””â”€â”€ tests/\n```\n\n## Complete pyproject.toml Examples\n\n### Pattern 4: Full-Featured pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-awesome-package\"\nversion = \"1.0.0\"\ndescription = \"An awesome Python package\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"you@example.com\"},\n]\nmaintainers = [\n    {name = \"Maintainer Name\", email = \"maintainer@example.com\"},\n]\nkeywords = [\"example\", \"package\", \"awesome\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\n\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",\n    \"click>=8.0.0\",\n    \"pydantic>=2.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"black>=23.0.0\",\n    \"ruff>=0.1.0\",\n    \"mypy>=1.0.0\",\n]\ndocs = [\n    \"sphinx>=5.0.0\",\n    \"sphinx-rtd-theme>=1.0.0\",\n]\nall = [\n    \"my-awesome-package[dev,docs]\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/username/my-awesome-package\"\nDocumentation = \"https://my-awesome-package.readthedocs.io\"\nRepository = \"https://github.com/username/my-awesome-package\"\n\"Bug Tracker\" = \"https://github.com/username/my-awesome-package/issues\"\nChangelog = \"https://github.com/username/my-awesome-package/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nmy-cli = \"my_package.cli:main\"\nawesome-tool = \"my_package.tools:run\"\n\n[project.entry-points.\"my_package.plugins\"]\nplugin1 = \"my_package.plugins:plugin1\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\nzip-safe = false\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\ninclude = [\"my_package*\"]\nexclude = [\"tests*\"]\n\n[tool.setuptools.package-data]\nmy_package = [\"py.typed\", \"*.pyi\", \"data/*.json\"]\n\n# Black configuration\n[tool.black]\nline-length = 100\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\ninclude = '\\.pyi?$'\n\n# Ruff configuration\n[tool.ruff]\nline-length = 100\ntarget-version = \"py38\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\n\n# MyPy configuration\n[tool.mypy]\npython_version = \"3.8\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n\n# Pytest configuration\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = \"-v --cov=my_package --cov-report=term-missing\"\n\n# Coverage configuration\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"*/tests/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n```\n\n### Pattern 5: Dynamic Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\ndescription = \"Package with dynamic version\"\n\n[tool.setuptools.dynamic]\nversion = {attr = \"my_package.__version__\"}\n\n# Or use setuptools-scm for git-based versioning\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\n```\n\n**In __init__.py:**\n```python\n# src/my_package/__init__.py\n__version__ = \"1.0.0\"\n\n# Or with setuptools-scm\nfrom importlib.metadata import version\n__version__ = version(\"my-package\")\n```\n\n## Command-Line Interface (CLI) Patterns\n\n### Pattern 6: CLI with Click\n\n```python\n# src/my_package/cli.py\nimport click\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"My awesome CLI tool.\"\"\"\n    pass\n\n@cli.command()\n@click.argument(\"name\")\n@click.option(\"--greeting\", default=\"Hello\", help=\"Greeting to use\")\ndef greet(name: str, greeting: str):\n    \"\"\"Greet someone.\"\"\"\n    click.echo(f\"{greeting}, {name}!\")\n\n@cli.command()\n@click.option(\"--count\", default=1, help=\"Number of times to repeat\")\ndef repeat(count: int):\n    \"\"\"Repeat a message.\"\"\"\n    for i in range(count):\n        click.echo(f\"Message {i + 1}\")\n\ndef main():\n    \"\"\"Entry point for CLI.\"\"\"\n    cli()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Register in pyproject.toml:**\n```toml\n[project.scripts]\nmy-tool = \"my_package.cli:main\"\n```\n\n**Usage:**\n```bash\npip install -e .\nmy-tool greet World\nmy-tool greet Alice --greeting=\"Hi\"\nmy-tool repeat --count=3\n```\n\n### Pattern 7: CLI with argparse\n\n```python\n# src/my_package/cli.py\nimport argparse\nimport sys\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"My awesome tool\",\n        prog=\"my-tool\"\n    )\n\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s 1.0.0\"\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")\n\n    # Add subcommand\n    process_parser = subparsers.add_parser(\"process\", help=\"Process data\")\n    process_parser.add_argument(\"input_file\", help=\"Input file path\")\n    process_parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.txt\",\n        help=\"Output file path\"\n    )\n\n    args = parser.parse_args()\n\n    if args.command == \"process\":\n        process_data(args.input_file, args.output)\n    else:\n        parser.print_help()\n        sys.exit(1)\n\ndef process_data(input_file: str, output_file: str):\n    \"\"\"Process data from input to output.\"\"\"\n    print(f\"Processing {input_file} -> {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Building and Publishing\n\n### Pattern 8: Build Package Locally\n\n```bash\n# Install build tools\npip install build twine\n\n# Build distribution\npython -m build\n\n# This creates:\n# dist/\n#   my-package-1.0.0.tar.gz (source distribution)\n#   my_package-1.0.0-py3-none-any.whl (wheel)\n\n# Check the distribution\ntwine check dist/*\n```\n\n### Pattern 9: Publishing to PyPI\n\n```bash\n# Install publishing tools\npip install twine\n\n# Test on TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Install from TestPyPI to test\npip install --index-url https://test.pypi.org/simple/ my-package\n\n# If all good, publish to PyPI\ntwine upload dist/*\n```\n\n**Using API tokens (recommended):**\n```bash\n# Create ~/.pypirc\n[distutils]\nindex-servers =\n    pypi\n    testpypi\n\n[pypi]\nusername = __token__\npassword = pypi-...your-token...\n\n[testpypi]\nusername = __token__\npassword = pypi-...your-test-token...\n```\n\n### Pattern 10: Automated Publishing with GitHub Actions\n\n```yaml\n# .github/workflows/publish.yml\nname: Publish to PyPI\n\non:\n  release:\n    types: [created]\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: |\n          pip install build twine\n\n      - name: Build package\n        run: python -m build\n\n      - name: Check package\n        run: twine check dist/*\n\n      - name: Publish to PyPI\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n        run: twine upload dist/*\n```\n\n## Advanced Patterns\n\n### Pattern 11: Including Data Files\n\n```toml\n[tool.setuptools.package-data]\nmy_package = [\n    \"data/*.json\",\n    \"templates/*.html\",\n    \"static/css/*.css\",\n    \"py.typed\",\n]\n```\n\n**Accessing data files:**\n```python\n# src/my_package/loader.py\nfrom importlib.resources import files\nimport json\n\ndef load_config():\n    \"\"\"Load configuration from package data.\"\"\"\n    config_file = files(\"my_package\").joinpath(\"data/config.json\")\n    with config_file.open() as f:\n        return json.load(f)\n\n# Python 3.9+\nfrom importlib.resources import files\n\ndata = files(\"my_package\").joinpath(\"data/file.txt\").read_text()\n```\n\n### Pattern 12: Namespace Packages\n\n**For large projects split across multiple repositories:**\n\n```\n# Package 1: company-core\ncompany/\nâ””â”€â”€ core/\n    â”œâ”€â”€ __init__.py\n    â””â”€â”€ models.py\n\n# Package 2: company-api\ncompany/\nâ””â”€â”€ api/\n    â”œâ”€â”€ __init__.py\n    â””â”€â”€ routes.py\n```\n\n**Do NOT include __init__.py in the namespace directory (company/):**\n\n```toml\n# company-core/pyproject.toml\n[project]\nname = \"company-core\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.core*\"]\n\n# company-api/pyproject.toml\n[project]\nname = \"company-api\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.api*\"]\n```\n\n**Usage:**\n```python\n# Both packages can be imported under same namespace\nfrom company.core import models\nfrom company.api import routes\n```\n\n### Pattern 13: C Extensions\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\", \"Cython>=0.29\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\next-modules = [\n    {name = \"my_package.fast_module\", sources = [\"src/fast_module.c\"]},\n]\n```\n\n**Or with setup.py:**\n```python\n# setup.py\nfrom setuptools import setup, Extension\n\nsetup(\n    ext_modules=[\n        Extension(\n            \"my_package.fast_module\",\n            sources=[\"src/fast_module.c\"],\n            include_dirs=[\"src/include\"],\n        )\n    ]\n)\n```\n\n## Version Management\n\n### Pattern 14: Semantic Versioning\n\n```python\n# src/my_package/__init__.py\n__version__ = \"1.2.3\"\n\n# Semantic versioning: MAJOR.MINOR.PATCH\n# MAJOR: Breaking changes\n# MINOR: New features (backward compatible)\n# PATCH: Bug fixes\n```\n\n**Version constraints in dependencies:**\n```toml\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",  # Compatible range\n    \"click~=8.1.0\",              # Compatible release (~= 8.1.0 means >=8.1.0,<8.2.0)\n    \"pydantic>=2.0\",             # Minimum version\n    \"numpy==1.24.3\",             # Exact version (avoid if possible)\n]\n```\n\n### Pattern 15: Git-Based Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"dirty-tag\"\n```\n\n**Creates versions like:**\n- `1.0.0` (from git tag)\n- `1.0.1.dev3+g1234567` (3 commits after tag)\n\n## Testing Installation\n\n### Pattern 16: Editable Install\n\n```bash\n# Install in development mode\npip install -e .\n\n# With optional dependencies\npip install -e \".[dev]\"\npip install -e \".[dev,docs]\"\n\n# Now changes to source code are immediately reflected\n```\n\n### Pattern 17: Testing in Isolated Environment\n\n```bash\n# Create virtual environment\npython -m venv test-env\nsource test-env/bin/activate  # Linux/Mac\n# test-env\\Scripts\\activate  # Windows\n\n# Install package\npip install dist/my_package-1.0.0-py3-none-any.whl\n\n# Test it works\npython -c \"import my_package; print(my_package.__version__)\"\n\n# Test CLI\nmy-tool --help\n\n# Cleanup\ndeactivate\nrm -rf test-env\n```\n\n## Documentation\n\n### Pattern 18: README.md Template\n\n```markdown\n# My Package\n\n[![PyPI version](https://badge.fury.io/py/my-package.svg)](https://pypi.org/project/my-package/)\n[![Python versions](https://img.shields.io/pypi/pyversions/my-package.svg)](https://pypi.org/project/my-package/)\n[![Tests](https://github.com/username/my-package/workflows/Tests/badge.svg)](https://github.com/username/my-package/actions)\n\nBrief description of your package.\n\n## Installation\n\n```bash\npip install my-package\n```\n\n## Quick Start\n\n```python\nfrom my_package import something\n\nresult = something.do_stuff()\n```\n\n## Features\n\n- Feature 1\n- Feature 2\n- Feature 3\n\n## Documentation\n\nFull documentation: https://my-package.readthedocs.io\n\n## Development\n\n```bash\ngit clone https://github.com/username/my-package.git\ncd my-package\npip install -e \".[dev]\"\npytest\n```\n\n## License\n\nMIT\n```\n\n## Common Patterns\n\n### Pattern 19: Multi-Architecture Wheels\n\n```yaml\n# .github/workflows/wheels.yml\nname: Build wheels\n\non: [push, pull_request]\n\njobs:\n  build_wheels:\n    name: Build wheels on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Build wheels\n        uses: pypa/cibuildwheel@v2.16.2\n\n      - uses: actions/upload-artifact@v3\n        with:\n          path: ./wheelhouse/*.whl\n```\n\n### Pattern 20: Private Package Index\n\n```bash\n# Install from private index\npip install my-package --index-url https://private.pypi.org/simple/\n\n# Or add to pip.conf\n[global]\nindex-url = https://private.pypi.org/simple/\nextra-index-url = https://pypi.org/simple/\n\n# Upload to private index\ntwine upload --repository-url https://private.pypi.org/ dist/*\n```\n\n## File Templates\n\n### .gitignore for Python Packages\n\n```gitignore\n# Build artifacts\nbuild/\ndist/\n*.egg-info/\n*.egg\n.eggs/\n\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n\n# Virtual environments\nvenv/\nenv/\nENV/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n\n# Distribution\n*.whl\n*.tar.gz\n```\n\n### MANIFEST.in\n\n```\n# MANIFEST.in\ninclude README.md\ninclude LICENSE\ninclude pyproject.toml\n\nrecursive-include src/my_package/data *.json\nrecursive-include src/my_package/templates *.html\nrecursive-exclude * __pycache__\nrecursive-exclude * *.py[co]\n```\n\n## Checklist for Publishing\n\n- [ ] Code is tested (pytest passing)\n- [ ] Documentation is complete (README, docstrings)\n- [ ] Version number updated\n- [ ] CHANGELOG.md updated\n- [ ] License file included\n- [ ] pyproject.toml is complete\n- [ ] Package builds without errors\n- [ ] Installation tested in clean environment\n- [ ] CLI tools work (if applicable)\n- [ ] PyPI metadata is correct (classifiers, keywords)\n- [ ] GitHub repository linked\n- [ ] Tested on TestPyPI first\n- [ ] Git tag created for release\n\n## Resources\n\n- **Python Packaging Guide**: https://packaging.python.org/\n- **PyPI**: https://pypi.org/\n- **TestPyPI**: https://test.pypi.org/\n- **setuptools documentation**: https://setuptools.pypa.io/\n- **build**: https://pypa-build.readthedocs.io/\n- **twine**: https://twine.readthedocs.io/\n\n## Best Practices Summary\n\n1. **Use src/ layout** for cleaner package structure\n2. **Use pyproject.toml** for modern packaging\n3. **Pin build dependencies** in build-system.requires\n4. **Version appropriately** with semantic versioning\n5. **Include all metadata** (classifiers, URLs, etc.)\n6. **Test installation** in clean environments\n7. **Use TestPyPI** before publishing to PyPI\n8. **Document thoroughly** with README and docstrings\n9. **Include LICENSE** file\n10. **Automate publishing** with CI/CD"
              },
              {
                "name": "python-performance-optimization",
                "description": "Profile and optimize Python code using cProfile, memory profilers, and performance best practices. Use when debugging slow Python code, optimizing bottlenecks, or improving application performance.",
                "path": "plugins/python-development/skills/python-performance-optimization/SKILL.md",
                "frontmatter": {
                  "name": "python-performance-optimization",
                  "description": "Profile and optimize Python code using cProfile, memory profilers, and performance best practices. Use when debugging slow Python code, optimizing bottlenecks, or improving application performance."
                },
                "content": "# Python Performance Optimization\n\nComprehensive guide to profiling, analyzing, and optimizing Python code for better performance, including CPU profiling, memory optimization, and implementation best practices.\n\n## When to Use This Skill\n\n- Identifying performance bottlenecks in Python applications\n- Reducing application latency and response times\n- Optimizing CPU-intensive operations\n- Reducing memory consumption and memory leaks\n- Improving database query performance\n- Optimizing I/O operations\n- Speeding up data processing pipelines\n- Implementing high-performance algorithms\n- Profiling production applications\n\n## Core Concepts\n\n### 1. Profiling Types\n- **CPU Profiling**: Identify time-consuming functions\n- **Memory Profiling**: Track memory allocation and leaks\n- **Line Profiling**: Profile at line-by-line granularity\n- **Call Graph**: Visualize function call relationships\n\n### 2. Performance Metrics\n- **Execution Time**: How long operations take\n- **Memory Usage**: Peak and average memory consumption\n- **CPU Utilization**: Processor usage patterns\n- **I/O Wait**: Time spent on I/O operations\n\n### 3. Optimization Strategies\n- **Algorithmic**: Better algorithms and data structures\n- **Implementation**: More efficient code patterns\n- **Parallelization**: Multi-threading/processing\n- **Caching**: Avoid redundant computation\n- **Native Extensions**: C/Rust for critical paths\n\n## Quick Start\n\n### Basic Timing\n\n```python\nimport time\n\ndef measure_time():\n    \"\"\"Simple timing measurement.\"\"\"\n    start = time.time()\n\n    # Your code here\n    result = sum(range(1000000))\n\n    elapsed = time.time() - start\n    print(f\"Execution time: {elapsed:.4f} seconds\")\n    return result\n\n# Better: use timeit for accurate measurements\nimport timeit\n\nexecution_time = timeit.timeit(\n    \"sum(range(1000000))\",\n    number=100\n)\nprint(f\"Average time: {execution_time/100:.6f} seconds\")\n```\n\n## Profiling Tools\n\n### Pattern 1: cProfile - CPU Profiling\n\n```python\nimport cProfile\nimport pstats\nfrom pstats import SortKey\n\ndef slow_function():\n    \"\"\"Function to profile.\"\"\"\n    total = 0\n    for i in range(1000000):\n        total += i\n    return total\n\ndef another_function():\n    \"\"\"Another function.\"\"\"\n    return [i**2 for i in range(100000)]\n\ndef main():\n    \"\"\"Main function to profile.\"\"\"\n    result1 = slow_function()\n    result2 = another_function()\n    return result1, result2\n\n# Profile the code\nif __name__ == \"__main__\":\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    main()\n\n    profiler.disable()\n\n    # Print stats\n    stats = pstats.Stats(profiler)\n    stats.sort_stats(SortKey.CUMULATIVE)\n    stats.print_stats(10)  # Top 10 functions\n\n    # Save to file for later analysis\n    stats.dump_stats(\"profile_output.prof\")\n```\n\n**Command-line profiling:**\n```bash\n# Profile a script\npython -m cProfile -o output.prof script.py\n\n# View results\npython -m pstats output.prof\n# In pstats:\n# sort cumtime\n# stats 10\n```\n\n### Pattern 2: line_profiler - Line-by-Line Profiling\n\n```python\n# Install: pip install line-profiler\n\n# Add @profile decorator (line_profiler provides this)\n@profile\ndef process_data(data):\n    \"\"\"Process data with line profiling.\"\"\"\n    result = []\n    for item in data:\n        processed = item * 2\n        result.append(processed)\n    return result\n\n# Run with:\n# kernprof -l -v script.py\n```\n\n**Manual line profiling:**\n```python\nfrom line_profiler import LineProfiler\n\ndef process_data(data):\n    \"\"\"Function to profile.\"\"\"\n    result = []\n    for item in data:\n        processed = item * 2\n        result.append(processed)\n    return result\n\nif __name__ == \"__main__\":\n    lp = LineProfiler()\n    lp.add_function(process_data)\n\n    data = list(range(100000))\n\n    lp_wrapper = lp(process_data)\n    lp_wrapper(data)\n\n    lp.print_stats()\n```\n\n### Pattern 3: memory_profiler - Memory Usage\n\n```python\n# Install: pip install memory-profiler\n\nfrom memory_profiler import profile\n\n@profile\ndef memory_intensive():\n    \"\"\"Function that uses lots of memory.\"\"\"\n    # Create large list\n    big_list = [i for i in range(1000000)]\n\n    # Create large dict\n    big_dict = {i: i**2 for i in range(100000)}\n\n    # Process data\n    result = sum(big_list)\n\n    return result\n\nif __name__ == \"__main__\":\n    memory_intensive()\n\n# Run with:\n# python -m memory_profiler script.py\n```\n\n### Pattern 4: py-spy - Production Profiling\n\n```bash\n# Install: pip install py-spy\n\n# Profile a running Python process\npy-spy top --pid 12345\n\n# Generate flamegraph\npy-spy record -o profile.svg --pid 12345\n\n# Profile a script\npy-spy record -o profile.svg -- python script.py\n\n# Dump current call stack\npy-spy dump --pid 12345\n```\n\n## Optimization Patterns\n\n### Pattern 5: List Comprehensions vs Loops\n\n```python\nimport timeit\n\n# Slow: Traditional loop\ndef slow_squares(n):\n    \"\"\"Create list of squares using loop.\"\"\"\n    result = []\n    for i in range(n):\n        result.append(i**2)\n    return result\n\n# Fast: List comprehension\ndef fast_squares(n):\n    \"\"\"Create list of squares using comprehension.\"\"\"\n    return [i**2 for i in range(n)]\n\n# Benchmark\nn = 100000\n\nslow_time = timeit.timeit(lambda: slow_squares(n), number=100)\nfast_time = timeit.timeit(lambda: fast_squares(n), number=100)\n\nprint(f\"Loop: {slow_time:.4f}s\")\nprint(f\"Comprehension: {fast_time:.4f}s\")\nprint(f\"Speedup: {slow_time/fast_time:.2f}x\")\n\n# Even faster for simple operations: map\ndef faster_squares(n):\n    \"\"\"Use map for even better performance.\"\"\"\n    return list(map(lambda x: x**2, range(n)))\n```\n\n### Pattern 6: Generator Expressions for Memory\n\n```python\nimport sys\n\ndef list_approach():\n    \"\"\"Memory-intensive list.\"\"\"\n    data = [i**2 for i in range(1000000)]\n    return sum(data)\n\ndef generator_approach():\n    \"\"\"Memory-efficient generator.\"\"\"\n    data = (i**2 for i in range(1000000))\n    return sum(data)\n\n# Memory comparison\nlist_data = [i for i in range(1000000)]\ngen_data = (i for i in range(1000000))\n\nprint(f\"List size: {sys.getsizeof(list_data)} bytes\")\nprint(f\"Generator size: {sys.getsizeof(gen_data)} bytes\")\n\n# Generators use constant memory regardless of size\n```\n\n### Pattern 7: String Concatenation\n\n```python\nimport timeit\n\ndef slow_concat(items):\n    \"\"\"Slow string concatenation.\"\"\"\n    result = \"\"\n    for item in items:\n        result += str(item)\n    return result\n\ndef fast_concat(items):\n    \"\"\"Fast string concatenation with join.\"\"\"\n    return \"\".join(str(item) for item in items)\n\ndef faster_concat(items):\n    \"\"\"Even faster with list.\"\"\"\n    parts = [str(item) for item in items]\n    return \"\".join(parts)\n\nitems = list(range(10000))\n\n# Benchmark\nslow = timeit.timeit(lambda: slow_concat(items), number=100)\nfast = timeit.timeit(lambda: fast_concat(items), number=100)\nfaster = timeit.timeit(lambda: faster_concat(items), number=100)\n\nprint(f\"Concatenation (+): {slow:.4f}s\")\nprint(f\"Join (generator): {fast:.4f}s\")\nprint(f\"Join (list): {faster:.4f}s\")\n```\n\n### Pattern 8: Dictionary Lookups vs List Searches\n\n```python\nimport timeit\n\n# Create test data\nsize = 10000\nitems = list(range(size))\nlookup_dict = {i: i for i in range(size)}\n\ndef list_search(items, target):\n    \"\"\"O(n) search in list.\"\"\"\n    return target in items\n\ndef dict_search(lookup_dict, target):\n    \"\"\"O(1) search in dict.\"\"\"\n    return target in lookup_dict\n\ntarget = size - 1  # Worst case for list\n\n# Benchmark\nlist_time = timeit.timeit(\n    lambda: list_search(items, target),\n    number=1000\n)\ndict_time = timeit.timeit(\n    lambda: dict_search(lookup_dict, target),\n    number=1000\n)\n\nprint(f\"List search: {list_time:.6f}s\")\nprint(f\"Dict search: {dict_time:.6f}s\")\nprint(f\"Speedup: {list_time/dict_time:.0f}x\")\n```\n\n### Pattern 9: Local Variable Access\n\n```python\nimport timeit\n\n# Global variable (slow)\nGLOBAL_VALUE = 100\n\ndef use_global():\n    \"\"\"Access global variable.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += GLOBAL_VALUE\n    return total\n\ndef use_local():\n    \"\"\"Use local variable.\"\"\"\n    local_value = 100\n    total = 0\n    for i in range(10000):\n        total += local_value\n    return total\n\n# Local is faster\nglobal_time = timeit.timeit(use_global, number=1000)\nlocal_time = timeit.timeit(use_local, number=1000)\n\nprint(f\"Global access: {global_time:.4f}s\")\nprint(f\"Local access: {local_time:.4f}s\")\nprint(f\"Speedup: {global_time/local_time:.2f}x\")\n```\n\n### Pattern 10: Function Call Overhead\n\n```python\nimport timeit\n\ndef calculate_inline():\n    \"\"\"Inline calculation.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += i * 2 + 1\n    return total\n\ndef helper_function(x):\n    \"\"\"Helper function.\"\"\"\n    return x * 2 + 1\n\ndef calculate_with_function():\n    \"\"\"Calculation with function calls.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += helper_function(i)\n    return total\n\n# Inline is faster due to no call overhead\ninline_time = timeit.timeit(calculate_inline, number=1000)\nfunction_time = timeit.timeit(calculate_with_function, number=1000)\n\nprint(f\"Inline: {inline_time:.4f}s\")\nprint(f\"Function calls: {function_time:.4f}s\")\n```\n\n## Advanced Optimization\n\n### Pattern 11: NumPy for Numerical Operations\n\n```python\nimport timeit\nimport numpy as np\n\ndef python_sum(n):\n    \"\"\"Sum using pure Python.\"\"\"\n    return sum(range(n))\n\ndef numpy_sum(n):\n    \"\"\"Sum using NumPy.\"\"\"\n    return np.arange(n).sum()\n\nn = 1000000\n\npython_time = timeit.timeit(lambda: python_sum(n), number=100)\nnumpy_time = timeit.timeit(lambda: numpy_sum(n), number=100)\n\nprint(f\"Python: {python_time:.4f}s\")\nprint(f\"NumPy: {numpy_time:.4f}s\")\nprint(f\"Speedup: {python_time/numpy_time:.2f}x\")\n\n# Vectorized operations\ndef python_multiply():\n    \"\"\"Element-wise multiplication in Python.\"\"\"\n    a = list(range(100000))\n    b = list(range(100000))\n    return [x * y for x, y in zip(a, b)]\n\ndef numpy_multiply():\n    \"\"\"Vectorized multiplication in NumPy.\"\"\"\n    a = np.arange(100000)\n    b = np.arange(100000)\n    return a * b\n\npy_time = timeit.timeit(python_multiply, number=100)\nnp_time = timeit.timeit(numpy_multiply, number=100)\n\nprint(f\"\\nPython multiply: {py_time:.4f}s\")\nprint(f\"NumPy multiply: {np_time:.4f}s\")\nprint(f\"Speedup: {py_time/np_time:.2f}x\")\n```\n\n### Pattern 12: Caching with functools.lru_cache\n\n```python\nfrom functools import lru_cache\nimport timeit\n\ndef fibonacci_slow(n):\n    \"\"\"Recursive fibonacci without caching.\"\"\"\n    if n < 2:\n        return n\n    return fibonacci_slow(n-1) + fibonacci_slow(n-2)\n\n@lru_cache(maxsize=None)\ndef fibonacci_fast(n):\n    \"\"\"Recursive fibonacci with caching.\"\"\"\n    if n < 2:\n        return n\n    return fibonacci_fast(n-1) + fibonacci_fast(n-2)\n\n# Massive speedup for recursive algorithms\nn = 30\n\nslow_time = timeit.timeit(lambda: fibonacci_slow(n), number=1)\nfast_time = timeit.timeit(lambda: fibonacci_fast(n), number=1000)\n\nprint(f\"Without cache (1 run): {slow_time:.4f}s\")\nprint(f\"With cache (1000 runs): {fast_time:.4f}s\")\n\n# Cache info\nprint(f\"Cache info: {fibonacci_fast.cache_info()}\")\n```\n\n### Pattern 13: Using __slots__ for Memory\n\n```python\nimport sys\n\nclass RegularClass:\n    \"\"\"Regular class with __dict__.\"\"\"\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\nclass SlottedClass:\n    \"\"\"Class with __slots__ for memory efficiency.\"\"\"\n    __slots__ = ['x', 'y', 'z']\n\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\n# Memory comparison\nregular = RegularClass(1, 2, 3)\nslotted = SlottedClass(1, 2, 3)\n\nprint(f\"Regular class size: {sys.getsizeof(regular)} bytes\")\nprint(f\"Slotted class size: {sys.getsizeof(slotted)} bytes\")\n\n# Significant savings with many instances\nregular_objects = [RegularClass(i, i+1, i+2) for i in range(10000)]\nslotted_objects = [SlottedClass(i, i+1, i+2) for i in range(10000)]\n\nprint(f\"\\nMemory for 10000 regular objects: ~{sys.getsizeof(regular) * 10000} bytes\")\nprint(f\"Memory for 10000 slotted objects: ~{sys.getsizeof(slotted) * 10000} bytes\")\n```\n\n### Pattern 14: Multiprocessing for CPU-Bound Tasks\n\n```python\nimport multiprocessing as mp\nimport time\n\ndef cpu_intensive_task(n):\n    \"\"\"CPU-intensive calculation.\"\"\"\n    return sum(i**2 for i in range(n))\n\ndef sequential_processing():\n    \"\"\"Process tasks sequentially.\"\"\"\n    start = time.time()\n    results = [cpu_intensive_task(1000000) for _ in range(4)]\n    elapsed = time.time() - start\n    return elapsed, results\n\ndef parallel_processing():\n    \"\"\"Process tasks in parallel.\"\"\"\n    start = time.time()\n    with mp.Pool(processes=4) as pool:\n        results = pool.map(cpu_intensive_task, [1000000] * 4)\n    elapsed = time.time() - start\n    return elapsed, results\n\nif __name__ == \"__main__\":\n    seq_time, seq_results = sequential_processing()\n    par_time, par_results = parallel_processing()\n\n    print(f\"Sequential: {seq_time:.2f}s\")\n    print(f\"Parallel: {par_time:.2f}s\")\n    print(f\"Speedup: {seq_time/par_time:.2f}x\")\n```\n\n### Pattern 15: Async I/O for I/O-Bound Tasks\n\n```python\nimport asyncio\nimport aiohttp\nimport time\nimport requests\n\nurls = [\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n]\n\ndef synchronous_requests():\n    \"\"\"Synchronous HTTP requests.\"\"\"\n    start = time.time()\n    results = []\n    for url in urls:\n        response = requests.get(url)\n        results.append(response.status_code)\n    elapsed = time.time() - start\n    return elapsed, results\n\nasync def async_fetch(session, url):\n    \"\"\"Async HTTP request.\"\"\"\n    async with session.get(url) as response:\n        return response.status\n\nasync def asynchronous_requests():\n    \"\"\"Asynchronous HTTP requests.\"\"\"\n    start = time.time()\n    async with aiohttp.ClientSession() as session:\n        tasks = [async_fetch(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    elapsed = time.time() - start\n    return elapsed, results\n\n# Async is much faster for I/O-bound work\nsync_time, sync_results = synchronous_requests()\nasync_time, async_results = asyncio.run(asynchronous_requests())\n\nprint(f\"Synchronous: {sync_time:.2f}s\")\nprint(f\"Asynchronous: {async_time:.2f}s\")\nprint(f\"Speedup: {sync_time/async_time:.2f}x\")\n```\n\n## Database Optimization\n\n### Pattern 16: Batch Database Operations\n\n```python\nimport sqlite3\nimport time\n\ndef create_db():\n    \"\"\"Create test database.\"\"\"\n    conn = sqlite3.connect(\":memory:\")\n    conn.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)\")\n    return conn\n\ndef slow_inserts(conn, count):\n    \"\"\"Insert records one at a time.\"\"\"\n    start = time.time()\n    cursor = conn.cursor()\n    for i in range(count):\n        cursor.execute(\"INSERT INTO users (name) VALUES (?)\", (f\"User {i}\",))\n        conn.commit()  # Commit each insert\n    elapsed = time.time() - start\n    return elapsed\n\ndef fast_inserts(conn, count):\n    \"\"\"Batch insert with single commit.\"\"\"\n    start = time.time()\n    cursor = conn.cursor()\n    data = [(f\"User {i}\",) for i in range(count)]\n    cursor.executemany(\"INSERT INTO users (name) VALUES (?)\", data)\n    conn.commit()  # Single commit\n    elapsed = time.time() - start\n    return elapsed\n\n# Benchmark\nconn1 = create_db()\nslow_time = slow_inserts(conn1, 1000)\n\nconn2 = create_db()\nfast_time = fast_inserts(conn2, 1000)\n\nprint(f\"Individual inserts: {slow_time:.4f}s\")\nprint(f\"Batch insert: {fast_time:.4f}s\")\nprint(f\"Speedup: {slow_time/fast_time:.2f}x\")\n```\n\n### Pattern 17: Query Optimization\n\n```python\n# Use indexes for frequently queried columns\n\"\"\"\n-- Slow: No index\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Fast: With index\nCREATE INDEX idx_users_email ON users(email);\nSELECT * FROM users WHERE email = 'user@example.com';\n\"\"\"\n\n# Use query planning\nimport sqlite3\n\nconn = sqlite3.connect(\"example.db\")\ncursor = conn.cursor()\n\n# Analyze query performance\ncursor.execute(\"EXPLAIN QUERY PLAN SELECT * FROM users WHERE email = ?\", (\"test@example.com\",))\nprint(cursor.fetchall())\n\n# Use SELECT only needed columns\n# Slow: SELECT *\n# Fast: SELECT id, name\n```\n\n## Memory Optimization\n\n### Pattern 18: Detecting Memory Leaks\n\n```python\nimport tracemalloc\nimport gc\n\ndef memory_leak_example():\n    \"\"\"Example that leaks memory.\"\"\"\n    leaked_objects = []\n\n    for i in range(100000):\n        # Objects added but never removed\n        leaked_objects.append([i] * 100)\n\n    # In real code, this would be an unintended reference\n\ndef track_memory_usage():\n    \"\"\"Track memory allocations.\"\"\"\n    tracemalloc.start()\n\n    # Take snapshot before\n    snapshot1 = tracemalloc.take_snapshot()\n\n    # Run code\n    memory_leak_example()\n\n    # Take snapshot after\n    snapshot2 = tracemalloc.take_snapshot()\n\n    # Compare\n    top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n\n    print(\"Top 10 memory allocations:\")\n    for stat in top_stats[:10]:\n        print(stat)\n\n    tracemalloc.stop()\n\n# Monitor memory\ntrack_memory_usage()\n\n# Force garbage collection\ngc.collect()\n```\n\n### Pattern 19: Iterators vs Lists\n\n```python\nimport sys\n\ndef process_file_list(filename):\n    \"\"\"Load entire file into memory.\"\"\"\n    with open(filename) as f:\n        lines = f.readlines()  # Loads all lines\n        return sum(1 for line in lines if line.strip())\n\ndef process_file_iterator(filename):\n    \"\"\"Process file line by line.\"\"\"\n    with open(filename) as f:\n        return sum(1 for line in f if line.strip())\n\n# Iterator uses constant memory\n# List loads entire file into memory\n```\n\n### Pattern 20: Weakref for Caches\n\n```python\nimport weakref\n\nclass CachedResource:\n    \"\"\"Resource that can be garbage collected.\"\"\"\n    def __init__(self, data):\n        self.data = data\n\n# Regular cache prevents garbage collection\nregular_cache = {}\n\ndef get_resource_regular(key):\n    \"\"\"Get resource from regular cache.\"\"\"\n    if key not in regular_cache:\n        regular_cache[key] = CachedResource(f\"Data for {key}\")\n    return regular_cache[key]\n\n# Weak reference cache allows garbage collection\nweak_cache = weakref.WeakValueDictionary()\n\ndef get_resource_weak(key):\n    \"\"\"Get resource from weak cache.\"\"\"\n    resource = weak_cache.get(key)\n    if resource is None:\n        resource = CachedResource(f\"Data for {key}\")\n        weak_cache[key] = resource\n    return resource\n\n# When no strong references exist, objects can be GC'd\n```\n\n## Benchmarking Tools\n\n### Custom Benchmark Decorator\n\n```python\nimport time\nfrom functools import wraps\n\ndef benchmark(func):\n    \"\"\"Decorator to benchmark function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed = time.perf_counter() - start\n        print(f\"{func.__name__} took {elapsed:.6f} seconds\")\n        return result\n    return wrapper\n\n@benchmark\ndef slow_function():\n    \"\"\"Function to benchmark.\"\"\"\n    time.sleep(0.5)\n    return sum(range(1000000))\n\nresult = slow_function()\n```\n\n### Performance Testing with pytest-benchmark\n\n```python\n# Install: pip install pytest-benchmark\n\ndef test_list_comprehension(benchmark):\n    \"\"\"Benchmark list comprehension.\"\"\"\n    result = benchmark(lambda: [i**2 for i in range(10000)])\n    assert len(result) == 10000\n\ndef test_map_function(benchmark):\n    \"\"\"Benchmark map function.\"\"\"\n    result = benchmark(lambda: list(map(lambda x: x**2, range(10000))))\n    assert len(result) == 10000\n\n# Run with: pytest test_performance.py --benchmark-compare\n```\n\n## Best Practices\n\n1. **Profile before optimizing** - Measure to find real bottlenecks\n2. **Focus on hot paths** - Optimize code that runs most frequently\n3. **Use appropriate data structures** - Dict for lookups, set for membership\n4. **Avoid premature optimization** - Clarity first, then optimize\n5. **Use built-in functions** - They're implemented in C\n6. **Cache expensive computations** - Use lru_cache\n7. **Batch I/O operations** - Reduce system calls\n8. **Use generators** for large datasets\n9. **Consider NumPy** for numerical operations\n10. **Profile production code** - Use py-spy for live systems\n\n## Common Pitfalls\n\n- Optimizing without profiling\n- Using global variables unnecessarily\n- Not using appropriate data structures\n- Creating unnecessary copies of data\n- Not using connection pooling for databases\n- Ignoring algorithmic complexity\n- Over-optimizing rare code paths\n- Not considering memory usage\n\n## Resources\n\n- **cProfile**: Built-in CPU profiler\n- **memory_profiler**: Memory usage profiling\n- **line_profiler**: Line-by-line profiling\n- **py-spy**: Sampling profiler for production\n- **NumPy**: High-performance numerical computing\n- **Cython**: Compile Python to C\n- **PyPy**: Alternative Python interpreter with JIT\n\n## Performance Checklist\n\n- [ ] Profiled code to identify bottlenecks\n- [ ] Used appropriate data structures\n- [ ] Implemented caching where beneficial\n- [ ] Optimized database queries\n- [ ] Used generators for large datasets\n- [ ] Considered multiprocessing for CPU-bound tasks\n- [ ] Used async I/O for I/O-bound tasks\n- [ ] Minimized function call overhead in hot loops\n- [ ] Checked for memory leaks\n- [ ] Benchmarked before and after optimization"
              },
              {
                "name": "python-testing-patterns",
                "description": "Implement comprehensive testing strategies with pytest, fixtures, mocking, and test-driven development. Use when writing Python tests, setting up test suites, or implementing testing best practices.",
                "path": "plugins/python-development/skills/python-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "python-testing-patterns",
                  "description": "Implement comprehensive testing strategies with pytest, fixtures, mocking, and test-driven development. Use when writing Python tests, setting up test suites, or implementing testing best practices."
                },
                "content": "# Python Testing Patterns\n\nComprehensive guide to implementing robust testing strategies in Python using pytest, fixtures, mocking, parameterization, and test-driven development practices.\n\n## When to Use This Skill\n\n- Writing unit tests for Python code\n- Setting up test suites and test infrastructure\n- Implementing test-driven development (TDD)\n- Creating integration tests for APIs and services\n- Mocking external dependencies and services\n- Testing async code and concurrent operations\n- Setting up continuous testing in CI/CD\n- Implementing property-based testing\n- Testing database operations\n- Debugging failing tests\n\n## Core Concepts\n\n### 1. Test Types\n- **Unit Tests**: Test individual functions/classes in isolation\n- **Integration Tests**: Test interaction between components\n- **Functional Tests**: Test complete features end-to-end\n- **Performance Tests**: Measure speed and resource usage\n\n### 2. Test Structure (AAA Pattern)\n- **Arrange**: Set up test data and preconditions\n- **Act**: Execute the code under test\n- **Assert**: Verify the results\n\n### 3. Test Coverage\n- Measure what code is exercised by tests\n- Identify untested code paths\n- Aim for meaningful coverage, not just high percentages\n\n### 4. Test Isolation\n- Tests should be independent\n- No shared state between tests\n- Each test should clean up after itself\n\n## Quick Start\n\n```python\n# test_example.py\ndef add(a, b):\n    return a + b\n\ndef test_add():\n    \"\"\"Basic test example.\"\"\"\n    result = add(2, 3)\n    assert result == 5\n\ndef test_add_negative():\n    \"\"\"Test with negative numbers.\"\"\"\n    assert add(-1, 1) == 0\n\n# Run with: pytest test_example.py\n```\n\n## Fundamental Patterns\n\n### Pattern 1: Basic pytest Tests\n\n```python\n# test_calculator.py\nimport pytest\n\nclass Calculator:\n    \"\"\"Simple calculator for testing.\"\"\"\n\n    def add(self, a: float, b: float) -> float:\n        return a + b\n\n    def subtract(self, a: float, b: float) -> float:\n        return a - b\n\n    def multiply(self, a: float, b: float) -> float:\n        return a * b\n\n    def divide(self, a: float, b: float) -> float:\n        if b == 0:\n            raise ValueError(\"Cannot divide by zero\")\n        return a / b\n\n\ndef test_addition():\n    \"\"\"Test addition.\"\"\"\n    calc = Calculator()\n    assert calc.add(2, 3) == 5\n    assert calc.add(-1, 1) == 0\n    assert calc.add(0, 0) == 0\n\n\ndef test_subtraction():\n    \"\"\"Test subtraction.\"\"\"\n    calc = Calculator()\n    assert calc.subtract(5, 3) == 2\n    assert calc.subtract(0, 5) == -5\n\n\ndef test_multiplication():\n    \"\"\"Test multiplication.\"\"\"\n    calc = Calculator()\n    assert calc.multiply(3, 4) == 12\n    assert calc.multiply(0, 5) == 0\n\n\ndef test_division():\n    \"\"\"Test division.\"\"\"\n    calc = Calculator()\n    assert calc.divide(6, 3) == 2\n    assert calc.divide(5, 2) == 2.5\n\n\ndef test_division_by_zero():\n    \"\"\"Test division by zero raises error.\"\"\"\n    calc = Calculator()\n    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n        calc.divide(5, 0)\n```\n\n### Pattern 2: Fixtures for Setup and Teardown\n\n```python\n# test_database.py\nimport pytest\nfrom typing import Generator\n\nclass Database:\n    \"\"\"Simple database class.\"\"\"\n\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self.connected = False\n\n    def connect(self):\n        \"\"\"Connect to database.\"\"\"\n        self.connected = True\n\n    def disconnect(self):\n        \"\"\"Disconnect from database.\"\"\"\n        self.connected = False\n\n    def query(self, sql: str) -> list:\n        \"\"\"Execute query.\"\"\"\n        if not self.connected:\n            raise RuntimeError(\"Not connected\")\n        return [{\"id\": 1, \"name\": \"Test\"}]\n\n\n@pytest.fixture\ndef db() -> Generator[Database, None, None]:\n    \"\"\"Fixture that provides connected database.\"\"\"\n    # Setup\n    database = Database(\"sqlite:///:memory:\")\n    database.connect()\n\n    # Provide to test\n    yield database\n\n    # Teardown\n    database.disconnect()\n\n\ndef test_database_query(db):\n    \"\"\"Test database query with fixture.\"\"\"\n    results = db.query(\"SELECT * FROM users\")\n    assert len(results) == 1\n    assert results[0][\"name\"] == \"Test\"\n\n\n@pytest.fixture(scope=\"session\")\ndef app_config():\n    \"\"\"Session-scoped fixture - created once per test session.\"\"\"\n    return {\n        \"database_url\": \"postgresql://localhost/test\",\n        \"api_key\": \"test-key\",\n        \"debug\": True\n    }\n\n\n@pytest.fixture(scope=\"module\")\ndef api_client(app_config):\n    \"\"\"Module-scoped fixture - created once per test module.\"\"\"\n    # Setup expensive resource\n    client = {\"config\": app_config, \"session\": \"active\"}\n    yield client\n    # Cleanup\n    client[\"session\"] = \"closed\"\n\n\ndef test_api_client(api_client):\n    \"\"\"Test using api client fixture.\"\"\"\n    assert api_client[\"session\"] == \"active\"\n    assert api_client[\"config\"][\"debug\"] is True\n```\n\n### Pattern 3: Parameterized Tests\n\n```python\n# test_validation.py\nimport pytest\n\ndef is_valid_email(email: str) -> bool:\n    \"\"\"Check if email is valid.\"\"\"\n    return \"@\" in email and \".\" in email.split(\"@\")[1]\n\n\n@pytest.mark.parametrize(\"email,expected\", [\n    (\"user@example.com\", True),\n    (\"test.user@domain.co.uk\", True),\n    (\"invalid.email\", False),\n    (\"@example.com\", False),\n    (\"user@domain\", False),\n    (\"\", False),\n])\ndef test_email_validation(email, expected):\n    \"\"\"Test email validation with various inputs.\"\"\"\n    assert is_valid_email(email) == expected\n\n\n@pytest.mark.parametrize(\"a,b,expected\", [\n    (2, 3, 5),\n    (0, 0, 0),\n    (-1, 1, 0),\n    (100, 200, 300),\n    (-5, -5, -10),\n])\ndef test_addition_parameterized(a, b, expected):\n    \"\"\"Test addition with multiple parameter sets.\"\"\"\n    from test_calculator import Calculator\n    calc = Calculator()\n    assert calc.add(a, b) == expected\n\n\n# Using pytest.param for special cases\n@pytest.mark.parametrize(\"value,expected\", [\n    pytest.param(1, True, id=\"positive\"),\n    pytest.param(0, False, id=\"zero\"),\n    pytest.param(-1, False, id=\"negative\"),\n])\ndef test_is_positive(value, expected):\n    \"\"\"Test with custom test IDs.\"\"\"\n    assert (value > 0) == expected\n```\n\n### Pattern 4: Mocking with unittest.mock\n\n```python\n# test_api_client.py\nimport pytest\nfrom unittest.mock import Mock, patch, MagicMock\nimport requests\n\nclass APIClient:\n    \"\"\"Simple API client.\"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_user(self, user_id: int) -> dict:\n        \"\"\"Fetch user from API.\"\"\"\n        response = requests.get(f\"{self.base_url}/users/{user_id}\")\n        response.raise_for_status()\n        return response.json()\n\n    def create_user(self, data: dict) -> dict:\n        \"\"\"Create new user.\"\"\"\n        response = requests.post(f\"{self.base_url}/users\", json=data)\n        response.raise_for_status()\n        return response.json()\n\n\ndef test_get_user_success():\n    \"\"\"Test successful API call with mock.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_response = Mock()\n    mock_response.json.return_value = {\"id\": 1, \"name\": \"John Doe\"}\n    mock_response.raise_for_status.return_value = None\n\n    with patch(\"requests.get\", return_value=mock_response) as mock_get:\n        user = client.get_user(1)\n\n        assert user[\"id\"] == 1\n        assert user[\"name\"] == \"John Doe\"\n        mock_get.assert_called_once_with(\"https://api.example.com/users/1\")\n\n\ndef test_get_user_not_found():\n    \"\"\"Test API call with 404 error.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_response = Mock()\n    mock_response.raise_for_status.side_effect = requests.HTTPError(\"404 Not Found\")\n\n    with patch(\"requests.get\", return_value=mock_response):\n        with pytest.raises(requests.HTTPError):\n            client.get_user(999)\n\n\n@patch(\"requests.post\")\ndef test_create_user(mock_post):\n    \"\"\"Test user creation with decorator syntax.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_post.return_value.json.return_value = {\"id\": 2, \"name\": \"Jane Doe\"}\n    mock_post.return_value.raise_for_status.return_value = None\n\n    user_data = {\"name\": \"Jane Doe\", \"email\": \"jane@example.com\"}\n    result = client.create_user(user_data)\n\n    assert result[\"id\"] == 2\n    mock_post.assert_called_once()\n    call_args = mock_post.call_args\n    assert call_args.kwargs[\"json\"] == user_data\n```\n\n### Pattern 5: Testing Exceptions\n\n```python\n# test_exceptions.py\nimport pytest\n\ndef divide(a: float, b: float) -> float:\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ZeroDivisionError(\"Division by zero\")\n    if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n        raise TypeError(\"Arguments must be numbers\")\n    return a / b\n\n\ndef test_zero_division():\n    \"\"\"Test exception is raised for division by zero.\"\"\"\n    with pytest.raises(ZeroDivisionError):\n        divide(10, 0)\n\n\ndef test_zero_division_with_message():\n    \"\"\"Test exception message.\"\"\"\n    with pytest.raises(ZeroDivisionError, match=\"Division by zero\"):\n        divide(5, 0)\n\n\ndef test_type_error():\n    \"\"\"Test type error exception.\"\"\"\n    with pytest.raises(TypeError, match=\"must be numbers\"):\n        divide(\"10\", 5)\n\n\ndef test_exception_info():\n    \"\"\"Test accessing exception info.\"\"\"\n    with pytest.raises(ValueError) as exc_info:\n        int(\"not a number\")\n\n    assert \"invalid literal\" in str(exc_info.value)\n```\n\n## Advanced Patterns\n\n### Pattern 6: Testing Async Code\n\n```python\n# test_async.py\nimport pytest\nimport asyncio\n\nasync def fetch_data(url: str) -> dict:\n    \"\"\"Fetch data asynchronously.\"\"\"\n    await asyncio.sleep(0.1)\n    return {\"url\": url, \"data\": \"result\"}\n\n\n@pytest.mark.asyncio\nasync def test_fetch_data():\n    \"\"\"Test async function.\"\"\"\n    result = await fetch_data(\"https://api.example.com\")\n    assert result[\"url\"] == \"https://api.example.com\"\n    assert \"data\" in result\n\n\n@pytest.mark.asyncio\nasync def test_concurrent_fetches():\n    \"\"\"Test concurrent async operations.\"\"\"\n    urls = [\"url1\", \"url2\", \"url3\"]\n    tasks = [fetch_data(url) for url in urls]\n    results = await asyncio.gather(*tasks)\n\n    assert len(results) == 3\n    assert all(\"data\" in r for r in results)\n\n\n@pytest.fixture\nasync def async_client():\n    \"\"\"Async fixture.\"\"\"\n    client = {\"connected\": True}\n    yield client\n    client[\"connected\"] = False\n\n\n@pytest.mark.asyncio\nasync def test_with_async_fixture(async_client):\n    \"\"\"Test using async fixture.\"\"\"\n    assert async_client[\"connected\"] is True\n```\n\n### Pattern 7: Monkeypatch for Testing\n\n```python\n# test_environment.py\nimport os\nimport pytest\n\ndef get_database_url() -> str:\n    \"\"\"Get database URL from environment.\"\"\"\n    return os.environ.get(\"DATABASE_URL\", \"sqlite:///:memory:\")\n\n\ndef test_database_url_default():\n    \"\"\"Test default database URL.\"\"\"\n    # Will use actual environment variable if set\n    url = get_database_url()\n    assert url\n\n\ndef test_database_url_custom(monkeypatch):\n    \"\"\"Test custom database URL with monkeypatch.\"\"\"\n    monkeypatch.setenv(\"DATABASE_URL\", \"postgresql://localhost/test\")\n    assert get_database_url() == \"postgresql://localhost/test\"\n\n\ndef test_database_url_not_set(monkeypatch):\n    \"\"\"Test when env var is not set.\"\"\"\n    monkeypatch.delenv(\"DATABASE_URL\", raising=False)\n    assert get_database_url() == \"sqlite:///:memory:\"\n\n\nclass Config:\n    \"\"\"Configuration class.\"\"\"\n\n    def __init__(self):\n        self.api_key = \"production-key\"\n\n    def get_api_key(self):\n        return self.api_key\n\n\ndef test_monkeypatch_attribute(monkeypatch):\n    \"\"\"Test monkeypatching object attributes.\"\"\"\n    config = Config()\n    monkeypatch.setattr(config, \"api_key\", \"test-key\")\n    assert config.get_api_key() == \"test-key\"\n```\n\n### Pattern 8: Temporary Files and Directories\n\n```python\n# test_file_operations.py\nimport pytest\nfrom pathlib import Path\n\ndef save_data(filepath: Path, data: str):\n    \"\"\"Save data to file.\"\"\"\n    filepath.write_text(data)\n\n\ndef load_data(filepath: Path) -> str:\n    \"\"\"Load data from file.\"\"\"\n    return filepath.read_text()\n\n\ndef test_file_operations(tmp_path):\n    \"\"\"Test file operations with temporary directory.\"\"\"\n    # tmp_path is a pathlib.Path object\n    test_file = tmp_path / \"test_data.txt\"\n\n    # Save data\n    save_data(test_file, \"Hello, World!\")\n\n    # Verify file exists\n    assert test_file.exists()\n\n    # Load and verify data\n    data = load_data(test_file)\n    assert data == \"Hello, World!\"\n\n\ndef test_multiple_files(tmp_path):\n    \"\"\"Test with multiple temporary files.\"\"\"\n    files = {\n        \"file1.txt\": \"Content 1\",\n        \"file2.txt\": \"Content 2\",\n        \"file3.txt\": \"Content 3\"\n    }\n\n    for filename, content in files.items():\n        filepath = tmp_path / filename\n        save_data(filepath, content)\n\n    # Verify all files created\n    assert len(list(tmp_path.iterdir())) == 3\n\n    # Verify contents\n    for filename, expected_content in files.items():\n        filepath = tmp_path / filename\n        assert load_data(filepath) == expected_content\n```\n\n### Pattern 9: Custom Fixtures and Conftest\n\n```python\n# conftest.py\n\"\"\"Shared fixtures for all tests.\"\"\"\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef database_url():\n    \"\"\"Provide database URL for all tests.\"\"\"\n    return \"postgresql://localhost/test_db\"\n\n\n@pytest.fixture(autouse=True)\ndef reset_database(database_url):\n    \"\"\"Auto-use fixture that runs before each test.\"\"\"\n    # Setup: Clear database\n    print(f\"Clearing database: {database_url}\")\n    yield\n    # Teardown: Clean up\n    print(\"Test completed\")\n\n\n@pytest.fixture\ndef sample_user():\n    \"\"\"Provide sample user data.\"\"\"\n    return {\n        \"id\": 1,\n        \"name\": \"Test User\",\n        \"email\": \"test@example.com\"\n    }\n\n\n@pytest.fixture\ndef sample_users():\n    \"\"\"Provide list of sample users.\"\"\"\n    return [\n        {\"id\": 1, \"name\": \"User 1\"},\n        {\"id\": 2, \"name\": \"User 2\"},\n        {\"id\": 3, \"name\": \"User 3\"},\n    ]\n\n\n# Parametrized fixture\n@pytest.fixture(params=[\"sqlite\", \"postgresql\", \"mysql\"])\ndef db_backend(request):\n    \"\"\"Fixture that runs tests with different database backends.\"\"\"\n    return request.param\n\n\ndef test_with_db_backend(db_backend):\n    \"\"\"This test will run 3 times with different backends.\"\"\"\n    print(f\"Testing with {db_backend}\")\n    assert db_backend in [\"sqlite\", \"postgresql\", \"mysql\"]\n```\n\n### Pattern 10: Property-Based Testing\n\n```python\n# test_properties.py\nfrom hypothesis import given, strategies as st\nimport pytest\n\ndef reverse_string(s: str) -> str:\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\n\n@given(st.text())\ndef test_reverse_twice_is_original(s):\n    \"\"\"Property: reversing twice returns original.\"\"\"\n    assert reverse_string(reverse_string(s)) == s\n\n\n@given(st.text())\ndef test_reverse_length(s):\n    \"\"\"Property: reversed string has same length.\"\"\"\n    assert len(reverse_string(s)) == len(s)\n\n\n@given(st.integers(), st.integers())\ndef test_addition_commutative(a, b):\n    \"\"\"Property: addition is commutative.\"\"\"\n    assert a + b == b + a\n\n\n@given(st.lists(st.integers()))\ndef test_sorted_list_properties(lst):\n    \"\"\"Property: sorted list is ordered.\"\"\"\n    sorted_lst = sorted(lst)\n\n    # Same length\n    assert len(sorted_lst) == len(lst)\n\n    # All elements present\n    assert set(sorted_lst) == set(lst)\n\n    # Is ordered\n    for i in range(len(sorted_lst) - 1):\n        assert sorted_lst[i] <= sorted_lst[i + 1]\n```\n\n## Testing Best Practices\n\n### Test Organization\n\n```python\n# tests/\n#   __init__.py\n#   conftest.py           # Shared fixtures\n#   test_unit/            # Unit tests\n#     test_models.py\n#     test_utils.py\n#   test_integration/     # Integration tests\n#     test_api.py\n#     test_database.py\n#   test_e2e/            # End-to-end tests\n#     test_workflows.py\n```\n\n### Test Naming\n\n```python\n# Good test names\ndef test_user_creation_with_valid_data():\n    \"\"\"Clear name describes what is being tested.\"\"\"\n    pass\n\n\ndef test_login_fails_with_invalid_password():\n    \"\"\"Name describes expected behavior.\"\"\"\n    pass\n\n\ndef test_api_returns_404_for_missing_resource():\n    \"\"\"Specific about inputs and expected outcomes.\"\"\"\n    pass\n\n\n# Bad test names\ndef test_1():  # Not descriptive\n    pass\n\n\ndef test_user():  # Too vague\n    pass\n\n\ndef test_function():  # Doesn't explain what's tested\n    pass\n```\n\n### Test Markers\n\n```python\n# test_markers.py\nimport pytest\n\n@pytest.mark.slow\ndef test_slow_operation():\n    \"\"\"Mark slow tests.\"\"\"\n    import time\n    time.sleep(2)\n\n\n@pytest.mark.integration\ndef test_database_integration():\n    \"\"\"Mark integration tests.\"\"\"\n    pass\n\n\n@pytest.mark.skip(reason=\"Feature not implemented yet\")\ndef test_future_feature():\n    \"\"\"Skip tests temporarily.\"\"\"\n    pass\n\n\n@pytest.mark.skipif(os.name == \"nt\", reason=\"Unix only test\")\ndef test_unix_specific():\n    \"\"\"Conditional skip.\"\"\"\n    pass\n\n\n@pytest.mark.xfail(reason=\"Known bug #123\")\ndef test_known_bug():\n    \"\"\"Mark expected failures.\"\"\"\n    assert False\n\n\n# Run with:\n# pytest -m slow          # Run only slow tests\n# pytest -m \"not slow\"    # Skip slow tests\n# pytest -m integration   # Run integration tests\n```\n\n### Coverage Reporting\n\n```bash\n# Install coverage\npip install pytest-cov\n\n# Run tests with coverage\npytest --cov=myapp tests/\n\n# Generate HTML report\npytest --cov=myapp --cov-report=html tests/\n\n# Fail if coverage below threshold\npytest --cov=myapp --cov-fail-under=80 tests/\n\n# Show missing lines\npytest --cov=myapp --cov-report=term-missing tests/\n```\n\n## Testing Database Code\n\n```python\n# test_database_models.py\nimport pytest\nfrom sqlalchemy import create_engine, Column, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\n\nBase = declarative_base()\n\n\nclass User(Base):\n    \"\"\"User model.\"\"\"\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String(50))\n    email = Column(String(100), unique=True)\n\n\n@pytest.fixture(scope=\"function\")\ndef db_session() -> Session:\n    \"\"\"Create in-memory database for testing.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    Base.metadata.create_all(engine)\n\n    SessionLocal = sessionmaker(bind=engine)\n    session = SessionLocal()\n\n    yield session\n\n    session.close()\n\n\ndef test_create_user(db_session):\n    \"\"\"Test creating a user.\"\"\"\n    user = User(name=\"Test User\", email=\"test@example.com\")\n    db_session.add(user)\n    db_session.commit()\n\n    assert user.id is not None\n    assert user.name == \"Test User\"\n\n\ndef test_query_user(db_session):\n    \"\"\"Test querying users.\"\"\"\n    user1 = User(name=\"User 1\", email=\"user1@example.com\")\n    user2 = User(name=\"User 2\", email=\"user2@example.com\")\n\n    db_session.add_all([user1, user2])\n    db_session.commit()\n\n    users = db_session.query(User).all()\n    assert len(users) == 2\n\n\ndef test_unique_email_constraint(db_session):\n    \"\"\"Test unique email constraint.\"\"\"\n    from sqlalchemy.exc import IntegrityError\n\n    user1 = User(name=\"User 1\", email=\"same@example.com\")\n    user2 = User(name=\"User 2\", email=\"same@example.com\")\n\n    db_session.add(user1)\n    db_session.commit()\n\n    db_session.add(user2)\n\n    with pytest.raises(IntegrityError):\n        db_session.commit()\n```\n\n## CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n          pip install pytest pytest-cov\n\n      - name: Run tests\n        run: |\n          pytest --cov=myapp --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n```\n\n## Configuration Files\n\n```ini\n# pytest.ini\n[pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts =\n    -v\n    --strict-markers\n    --tb=short\n    --cov=myapp\n    --cov-report=term-missing\nmarkers =\n    slow: marks tests as slow\n    integration: marks integration tests\n    unit: marks unit tests\n    e2e: marks end-to-end tests\n```\n\n```toml\n# pyproject.toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = [\n    \"-v\",\n    \"--cov=myapp\",\n    \"--cov-report=term-missing\",\n]\n\n[tool.coverage.run]\nsource = [\"myapp\"]\nomit = [\"*/tests/*\", \"*/migrations/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n```\n\n## Resources\n\n- **pytest documentation**: https://docs.pytest.org/\n- **unittest.mock**: https://docs.python.org/3/library/unittest.mock.html\n- **hypothesis**: Property-based testing\n- **pytest-asyncio**: Testing async code\n- **pytest-cov**: Coverage reporting\n- **pytest-mock**: pytest wrapper for mock\n\n## Best Practices Summary\n\n1. **Write tests first** (TDD) or alongside code\n2. **One assertion per test** when possible\n3. **Use descriptive test names** that explain behavior\n4. **Keep tests independent** and isolated\n5. **Use fixtures** for setup and teardown\n6. **Mock external dependencies** appropriately\n7. **Parametrize tests** to reduce duplication\n8. **Test edge cases** and error conditions\n9. **Measure coverage** but focus on quality\n10. **Run tests in CI/CD** on every commit"
              },
              {
                "name": "uv-package-manager",
                "description": "Master the uv package manager for fast Python dependency management, virtual environments, and modern Python project workflows. Use when setting up Python projects, managing dependencies, or optimizing Python development workflows with uv.",
                "path": "plugins/python-development/skills/uv-package-manager/SKILL.md",
                "frontmatter": {
                  "name": "uv-package-manager",
                  "description": "Master the uv package manager for fast Python dependency management, virtual environments, and modern Python project workflows. Use when setting up Python projects, managing dependencies, or optimizing Python development workflows with uv."
                },
                "content": "# UV Package Manager\n\nComprehensive guide to using uv, an extremely fast Python package installer and resolver written in Rust, for modern Python project management and dependency workflows.\n\n## When to Use This Skill\n\n- Setting up new Python projects quickly\n- Managing Python dependencies faster than pip\n- Creating and managing virtual environments\n- Installing Python interpreters\n- Resolving dependency conflicts efficiently\n- Migrating from pip/pip-tools/poetry\n- Speeding up CI/CD pipelines\n- Managing monorepo Python projects\n- Working with lockfiles for reproducible builds\n- Optimizing Docker builds with Python dependencies\n\n## Core Concepts\n\n### 1. What is uv?\n- **Ultra-fast package installer**: 10-100x faster than pip\n- **Written in Rust**: Leverages Rust's performance\n- **Drop-in pip replacement**: Compatible with pip workflows\n- **Virtual environment manager**: Create and manage venvs\n- **Python installer**: Download and manage Python versions\n- **Resolver**: Advanced dependency resolution\n- **Lockfile support**: Reproducible installations\n\n### 2. Key Features\n- Blazing fast installation speeds\n- Disk space efficient with global cache\n- Compatible with pip, pip-tools, poetry\n- Comprehensive dependency resolution\n- Cross-platform support (Linux, macOS, Windows)\n- No Python required for installation\n- Built-in virtual environment support\n\n### 3. UV vs Traditional Tools\n- **vs pip**: 10-100x faster, better resolver\n- **vs pip-tools**: Faster, simpler, better UX\n- **vs poetry**: Faster, less opinionated, lighter\n- **vs conda**: Faster, Python-focused\n\n## Installation\n\n### Quick Install\n\n```bash\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Using pip (if you already have Python)\npip install uv\n\n# Using Homebrew (macOS)\nbrew install uv\n\n# Using cargo (if you have Rust)\ncargo install --git https://github.com/astral-sh/uv uv\n```\n\n### Verify Installation\n\n```bash\nuv --version\n# uv 0.x.x\n```\n\n## Quick Start\n\n### Create a New Project\n\n```bash\n# Create new project with virtual environment\nuv init my-project\ncd my-project\n\n# Or create in current directory\nuv init .\n\n# Initialize creates:\n# - .python-version (Python version)\n# - pyproject.toml (project config)\n# - README.md\n# - .gitignore\n```\n\n### Install Dependencies\n\n```bash\n# Install packages (creates venv if needed)\nuv add requests pandas\n\n# Install dev dependencies\nuv add --dev pytest black ruff\n\n# Install from requirements.txt\nuv pip install -r requirements.txt\n\n# Install from pyproject.toml\nuv sync\n```\n\n## Virtual Environment Management\n\n### Pattern 1: Creating Virtual Environments\n\n```bash\n# Create virtual environment with uv\nuv venv\n\n# Create with specific Python version\nuv venv --python 3.12\n\n# Create with custom name\nuv venv my-env\n\n# Create with system site packages\nuv venv --system-site-packages\n\n# Specify location\nuv venv /path/to/venv\n```\n\n### Pattern 2: Activating Virtual Environments\n\n```bash\n# Linux/macOS\nsource .venv/bin/activate\n\n# Windows (Command Prompt)\n.venv\\Scripts\\activate.bat\n\n# Windows (PowerShell)\n.venv\\Scripts\\Activate.ps1\n\n# Or use uv run (no activation needed)\nuv run python script.py\nuv run pytest\n```\n\n### Pattern 3: Using uv run\n\n```bash\n# Run Python script (auto-activates venv)\nuv run python app.py\n\n# Run installed CLI tool\nuv run black .\nuv run pytest\n\n# Run with specific Python version\nuv run --python 3.11 python script.py\n\n# Pass arguments\nuv run python script.py --arg value\n```\n\n## Package Management\n\n### Pattern 4: Adding Dependencies\n\n```bash\n# Add package (adds to pyproject.toml)\nuv add requests\n\n# Add with version constraint\nuv add \"django>=4.0,<5.0\"\n\n# Add multiple packages\nuv add numpy pandas matplotlib\n\n# Add dev dependency\nuv add --dev pytest pytest-cov\n\n# Add optional dependency group\nuv add --optional docs sphinx\n\n# Add from git\nuv add git+https://github.com/user/repo.git\n\n# Add from git with specific ref\nuv add git+https://github.com/user/repo.git@v1.0.0\n\n# Add from local path\nuv add ./local-package\n\n# Add editable local package\nuv add -e ./local-package\n```\n\n### Pattern 5: Removing Dependencies\n\n```bash\n# Remove package\nuv remove requests\n\n# Remove dev dependency\nuv remove --dev pytest\n\n# Remove multiple packages\nuv remove numpy pandas matplotlib\n```\n\n### Pattern 6: Upgrading Dependencies\n\n```bash\n# Upgrade specific package\nuv add --upgrade requests\n\n# Upgrade all packages\nuv sync --upgrade\n\n# Upgrade package to latest\nuv add --upgrade requests\n\n# Show what would be upgraded\nuv tree --outdated\n```\n\n### Pattern 7: Locking Dependencies\n\n```bash\n# Generate uv.lock file\nuv lock\n\n# Update lock file\nuv lock --upgrade\n\n# Lock without installing\nuv lock --no-install\n\n# Lock specific package\nuv lock --upgrade-package requests\n```\n\n## Python Version Management\n\n### Pattern 8: Installing Python Versions\n\n```bash\n# Install Python version\nuv python install 3.12\n\n# Install multiple versions\nuv python install 3.11 3.12 3.13\n\n# Install latest version\nuv python install\n\n# List installed versions\nuv python list\n\n# Find available versions\nuv python list --all-versions\n```\n\n### Pattern 9: Setting Python Version\n\n```bash\n# Set Python version for project\nuv python pin 3.12\n\n# This creates/updates .python-version file\n\n# Use specific Python version for command\nuv --python 3.11 run python script.py\n\n# Create venv with specific version\nuv venv --python 3.12\n```\n\n## Project Configuration\n\n### Pattern 10: pyproject.toml with uv\n\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"My awesome project\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"requests>=2.31.0\",\n    \"pydantic>=2.0.0\",\n    \"click>=8.1.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.4.0\",\n    \"pytest-cov>=4.1.0\",\n    \"black>=23.0.0\",\n    \"ruff>=0.1.0\",\n    \"mypy>=1.5.0\",\n]\ndocs = [\n    \"sphinx>=7.0.0\",\n    \"sphinx-rtd-theme>=1.3.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.uv]\ndev-dependencies = [\n    # Additional dev dependencies managed by uv\n]\n\n[tool.uv.sources]\n# Custom package sources\nmy-package = { git = \"https://github.com/user/repo.git\" }\n```\n\n### Pattern 11: Using uv with Existing Projects\n\n```bash\n# Migrate from requirements.txt\nuv add -r requirements.txt\n\n# Migrate from poetry\n# Already have pyproject.toml, just use:\nuv sync\n\n# Export to requirements.txt\nuv pip freeze > requirements.txt\n\n# Export with hashes\nuv pip freeze --require-hashes > requirements.txt\n```\n\n## Advanced Workflows\n\n### Pattern 12: Monorepo Support\n\n```bash\n# Project structure\n# monorepo/\n#   packages/\n#     package-a/\n#       pyproject.toml\n#     package-b/\n#       pyproject.toml\n#   pyproject.toml (root)\n\n# Root pyproject.toml\n[tool.uv.workspace]\nmembers = [\"packages/*\"]\n\n# Install all workspace packages\nuv sync\n\n# Add workspace dependency\nuv add --path ./packages/package-a\n```\n\n### Pattern 13: CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v2\n        with:\n          enable-cache: true\n\n      - name: Set up Python\n        run: uv python install 3.12\n\n      - name: Install dependencies\n        run: uv sync --all-extras --dev\n\n      - name: Run tests\n        run: uv run pytest\n\n      - name: Run linting\n        run: |\n          uv run ruff check .\n          uv run black --check .\n```\n\n### Pattern 14: Docker Integration\n\n```dockerfile\n# Dockerfile\nFROM python:3.12-slim\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n\n# Set working directory\nWORKDIR /app\n\n# Copy dependency files\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies\nRUN uv sync --frozen --no-dev\n\n# Copy application code\nCOPY . .\n\n# Run application\nCMD [\"uv\", \"run\", \"python\", \"app.py\"]\n```\n\n**Optimized multi-stage build:**\n\n```dockerfile\n# Multi-stage Dockerfile\nFROM python:3.12-slim AS builder\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n\nWORKDIR /app\n\n# Install dependencies to venv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev --no-editable\n\n# Runtime stage\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Copy venv from builder\nCOPY --from=builder /app/.venv .venv\nCOPY . .\n\n# Use venv\nENV PATH=\"/app/.venv/bin:$PATH\"\n\nCMD [\"python\", \"app.py\"]\n```\n\n### Pattern 15: Lockfile Workflows\n\n```bash\n# Create lockfile (uv.lock)\nuv lock\n\n# Install from lockfile (exact versions)\nuv sync --frozen\n\n# Update lockfile without installing\nuv lock --no-install\n\n# Upgrade specific package in lock\nuv lock --upgrade-package requests\n\n# Check if lockfile is up to date\nuv lock --check\n\n# Export lockfile to requirements.txt\nuv export --format requirements-txt > requirements.txt\n\n# Export with hashes for security\nuv export --format requirements-txt --hash > requirements.txt\n```\n\n## Performance Optimization\n\n### Pattern 16: Using Global Cache\n\n```bash\n# UV automatically uses global cache at:\n# Linux: ~/.cache/uv\n# macOS: ~/Library/Caches/uv\n# Windows: %LOCALAPPDATA%\\uv\\cache\n\n# Clear cache\nuv cache clean\n\n# Check cache size\nuv cache dir\n```\n\n### Pattern 17: Parallel Installation\n\n```bash\n# UV installs packages in parallel by default\n\n# Control parallelism\nuv pip install --jobs 4 package1 package2\n\n# No parallel (sequential)\nuv pip install --jobs 1 package\n```\n\n### Pattern 18: Offline Mode\n\n```bash\n# Install from cache only (no network)\nuv pip install --offline package\n\n# Sync from lockfile offline\nuv sync --frozen --offline\n```\n\n## Comparison with Other Tools\n\n### uv vs pip\n\n```bash\n# pip\npython -m venv .venv\nsource .venv/bin/activate\npip install requests pandas numpy\n# ~30 seconds\n\n# uv\nuv venv\nuv add requests pandas numpy\n# ~2 seconds (10-15x faster)\n```\n\n### uv vs poetry\n\n```bash\n# poetry\npoetry init\npoetry add requests pandas\npoetry install\n# ~20 seconds\n\n# uv\nuv init\nuv add requests pandas\nuv sync\n# ~3 seconds (6-7x faster)\n```\n\n### uv vs pip-tools\n\n```bash\n# pip-tools\npip-compile requirements.in\npip-sync requirements.txt\n# ~15 seconds\n\n# uv\nuv lock\nuv sync --frozen\n# ~2 seconds (7-8x faster)\n```\n\n## Common Workflows\n\n### Pattern 19: Starting a New Project\n\n```bash\n# Complete workflow\nuv init my-project\ncd my-project\n\n# Set Python version\nuv python pin 3.12\n\n# Add dependencies\nuv add fastapi uvicorn pydantic\n\n# Add dev dependencies\nuv add --dev pytest black ruff mypy\n\n# Create structure\nmkdir -p src/my_project tests\n\n# Run tests\nuv run pytest\n\n# Format code\nuv run black .\nuv run ruff check .\n```\n\n### Pattern 20: Maintaining Existing Project\n\n```bash\n# Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies (creates venv automatically)\nuv sync\n\n# Install with dev dependencies\nuv sync --all-extras\n\n# Update dependencies\nuv lock --upgrade\n\n# Run application\nuv run python app.py\n\n# Run tests\nuv run pytest\n\n# Add new dependency\nuv add new-package\n\n# Commit updated files\ngit add pyproject.toml uv.lock\ngit commit -m \"Add new-package dependency\"\n```\n\n## Tool Integration\n\n### Pattern 21: Pre-commit Hooks\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: uv-lock\n        name: uv lock\n        entry: uv lock\n        language: system\n        pass_filenames: false\n\n      - id: ruff\n        name: ruff\n        entry: uv run ruff check --fix\n        language: system\n        types: [python]\n\n      - id: black\n        name: black\n        entry: uv run black\n        language: system\n        types: [python]\n```\n\n### Pattern 22: VS Code Integration\n\n```json\n// .vscode/settings.json\n{\n  \"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv/bin/python\",\n  \"python.terminal.activateEnvironment\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\"-v\"],\n  \"python.linting.enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true\n  }\n}\n```\n\n## Troubleshooting\n\n### Common Issues\n\n```bash\n# Issue: uv not found\n# Solution: Add to PATH or reinstall\necho 'export PATH=\"$HOME/.cargo/bin:$PATH\"' >> ~/.bashrc\n\n# Issue: Wrong Python version\n# Solution: Pin version explicitly\nuv python pin 3.12\nuv venv --python 3.12\n\n# Issue: Dependency conflict\n# Solution: Check resolution\nuv lock --verbose\n\n# Issue: Cache issues\n# Solution: Clear cache\nuv cache clean\n\n# Issue: Lockfile out of sync\n# Solution: Regenerate\nuv lock --upgrade\n```\n\n## Best Practices\n\n### Project Setup\n\n1. **Always use lockfiles** for reproducibility\n2. **Pin Python version** with .python-version\n3. **Separate dev dependencies** from production\n4. **Use uv run** instead of activating venv\n5. **Commit uv.lock** to version control\n6. **Use --frozen in CI** for consistent builds\n7. **Leverage global cache** for speed\n8. **Use workspace** for monorepos\n9. **Export requirements.txt** for compatibility\n10. **Keep uv updated** for latest features\n\n### Performance Tips\n\n```bash\n# Use frozen installs in CI\nuv sync --frozen\n\n# Use offline mode when possible\nuv sync --offline\n\n# Parallel operations (automatic)\n# uv does this by default\n\n# Reuse cache across environments\n# uv shares cache globally\n\n# Use lockfiles to skip resolution\nuv sync --frozen  # skips resolution\n```\n\n## Migration Guide\n\n### From pip + requirements.txt\n\n```bash\n# Before\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# After\nuv venv\nuv pip install -r requirements.txt\n# Or better:\nuv init\nuv add -r requirements.txt\n```\n\n### From Poetry\n\n```bash\n# Before\npoetry install\npoetry add requests\n\n# After\nuv sync\nuv add requests\n\n# Keep existing pyproject.toml\n# uv reads [project] and [tool.poetry] sections\n```\n\n### From pip-tools\n\n```bash\n# Before\npip-compile requirements.in\npip-sync requirements.txt\n\n# After\nuv lock\nuv sync --frozen\n```\n\n## Command Reference\n\n### Essential Commands\n\n```bash\n# Project management\nuv init [PATH]              # Initialize project\nuv add PACKAGE              # Add dependency\nuv remove PACKAGE           # Remove dependency\nuv sync                     # Install dependencies\nuv lock                     # Create/update lockfile\n\n# Virtual environments\nuv venv [PATH]              # Create venv\nuv run COMMAND              # Run in venv\n\n# Python management\nuv python install VERSION   # Install Python\nuv python list              # List installed Pythons\nuv python pin VERSION       # Pin Python version\n\n# Package installation (pip-compatible)\nuv pip install PACKAGE      # Install package\nuv pip uninstall PACKAGE    # Uninstall package\nuv pip freeze               # List installed\nuv pip list                 # List packages\n\n# Utility\nuv cache clean              # Clear cache\nuv cache dir                # Show cache location\nuv --version                # Show version\n```\n\n## Resources\n\n- **Official documentation**: https://docs.astral.sh/uv/\n- **GitHub repository**: https://github.com/astral-sh/uv\n- **Astral blog**: https://astral.sh/blog\n- **Migration guides**: https://docs.astral.sh/uv/guides/\n- **Comparison with other tools**: https://docs.astral.sh/uv/pip/compatibility/\n\n## Best Practices Summary\n\n1. **Use uv for all new projects** - Start with `uv init`\n2. **Commit lockfiles** - Ensure reproducible builds\n3. **Pin Python versions** - Use .python-version\n4. **Use uv run** - Avoid manual venv activation\n5. **Leverage caching** - Let uv manage global cache\n6. **Use --frozen in CI** - Exact reproduction\n7. **Keep uv updated** - Fast-moving project\n8. **Use workspaces** - For monorepo projects\n9. **Export for compatibility** - Generate requirements.txt when needed\n10. **Read the docs** - uv is feature-rich and evolving"
              }
            ]
          },
          {
            "name": "llm-application-dev",
            "description": "LLM application development with RAG and embeddings",
            "source": "./plugins/llm-application-dev",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install llm-application-dev@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/ai-assistant",
                "description": null,
                "path": "plugins/llm-application-dev/commands/ai-assistant.md",
                "frontmatter": null,
                "content": "# AI Assistant Development\n\nYou are an AI assistant development expert specializing in creating intelligent conversational interfaces, chatbots, and AI-powered applications. Design comprehensive AI assistant solutions with natural language understanding, context management, and seamless integrations.\n\n## Context\nThe user needs to develop an AI assistant or chatbot with natural language capabilities, intelligent responses, and practical functionality. Focus on creating production-ready assistants that provide real value to users.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. AI Assistant Architecture\n\nDesign comprehensive assistant architecture:\n\n**Assistant Architecture Framework**\n```python\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nimport asyncio\n\n@dataclass\nclass ConversationContext:\n    \"\"\"Maintains conversation state and context\"\"\"\n    user_id: str\n    session_id: str\n    messages: List[Dict[str, Any]]\n    user_profile: Dict[str, Any]\n    conversation_state: Dict[str, Any]\n    metadata: Dict[str, Any]\n\nclass AIAssistantArchitecture:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.components = self._initialize_components()\n        \n    def design_architecture(self):\n        \"\"\"Design comprehensive AI assistant architecture\"\"\"\n        return {\n            'core_components': {\n                'nlu': self._design_nlu_component(),\n                'dialog_manager': self._design_dialog_manager(),\n                'response_generator': self._design_response_generator(),\n                'context_manager': self._design_context_manager(),\n                'integration_layer': self._design_integration_layer()\n            },\n            'data_flow': self._design_data_flow(),\n            'deployment': self._design_deployment_architecture(),\n            'scalability': self._design_scalability_features()\n        }\n    \n    def _design_nlu_component(self):\n        \"\"\"Natural Language Understanding component\"\"\"\n        return {\n            'intent_recognition': {\n                'model': 'transformer-based classifier',\n                'features': [\n                    'Multi-intent detection',\n                    'Confidence scoring',\n                    'Fallback handling'\n                ],\n                'implementation': '''\nclass IntentClassifier:\n    def __init__(self, model_path: str, *, config: Optional[Dict[str, Any]] = None):\n        self.model = self.load_model(model_path)\n        self.intents = self.load_intent_schema()\n        default_config = {\"threshold\": 0.65}\n        self.config = {**default_config, **(config or {})}\n    \n    async def classify(self, text: str) -> Dict[str, Any]:\n        # Preprocess text\n        processed = self.preprocess(text)\n        \n        # Get model predictions\n        predictions = await self.model.predict(processed)\n        \n        # Extract intents with confidence\n        intents = []\n        for intent, confidence in predictions:\n            if confidence > self.config['threshold']:\n                intents.append({\n                    'name': intent,\n                    'confidence': confidence,\n                    'parameters': self.extract_parameters(text, intent)\n                })\n        \n        return {\n            'intents': intents,\n            'primary_intent': intents[0] if intents else None,\n            'requires_clarification': len(intents) > 1\n        }\n'''\n            },\n            'entity_extraction': {\n                'model': 'NER with custom entities',\n                'features': [\n                    'Domain-specific entities',\n                    'Contextual extraction',\n                    'Entity resolution'\n                ]\n            },\n            'sentiment_analysis': {\n                'model': 'Fine-tuned sentiment classifier',\n                'features': [\n                    'Emotion detection',\n                    'Urgency classification',\n                    'User satisfaction tracking'\n                ]\n            }\n        }\n    \n    def _design_dialog_manager(self):\n        \"\"\"Dialog management system\"\"\"\n        return '''\nclass DialogManager:\n    \"\"\"Manages conversation flow and state\"\"\"\n    \n    def __init__(self):\n        self.state_machine = ConversationStateMachine()\n        self.policy_network = DialogPolicy()\n        \n    async def process_turn(self, \n                          context: ConversationContext, \n                          nlu_result: Dict[str, Any]) -> Dict[str, Any]:\n        # Determine current state\n        current_state = self.state_machine.get_state(context)\n        \n        # Apply dialog policy\n        action = await self.policy_network.select_action(\n            current_state, \n            nlu_result, \n            context\n        )\n        \n        # Execute action\n        result = await self.execute_action(action, context)\n        \n        # Update state\n        new_state = self.state_machine.transition(\n            current_state, \n            action, \n            result\n        )\n        \n        return {\n            'action': action,\n            'new_state': new_state,\n            'response_data': result\n        }\n    \n    async def execute_action(self, action: str, context: ConversationContext):\n        \"\"\"Execute dialog action\"\"\"\n        action_handlers = {\n            'greet': self.handle_greeting,\n            'provide_info': self.handle_information_request,\n            'clarify': self.handle_clarification,\n            'confirm': self.handle_confirmation,\n            'execute_task': self.handle_task_execution,\n            'end_conversation': self.handle_conversation_end\n        }\n        \n        handler = action_handlers.get(action, self.handle_unknown)\n        return await handler(context)\n'''\n```\n\n### 2. Natural Language Processing\n\nImplement advanced NLP capabilities:\n\n**NLP Pipeline Implementation**\n```python\nclass NLPPipeline:\n    def __init__(self):\n        self.tokenizer = self._initialize_tokenizer()\n        self.embedder = self._initialize_embedder()\n        self.models = self._load_models()\n    \n    async def process_message(self, message: str, context: ConversationContext):\n        \"\"\"Process user message through NLP pipeline\"\"\"\n        # Tokenization and preprocessing\n        tokens = self.tokenizer.tokenize(message)\n        \n        # Generate embeddings\n        embeddings = await self.embedder.embed(tokens)\n        \n        # Parallel processing of NLP tasks\n        tasks = [\n            self.detect_intent(embeddings),\n            self.extract_entities(tokens, embeddings),\n            self.analyze_sentiment(embeddings),\n            self.detect_language(tokens),\n            self.check_spelling(tokens)\n        ]\n        \n        results = await asyncio.gather(*tasks)\n        \n        return {\n            'intent': results[0],\n            'entities': results[1],\n            'sentiment': results[2],\n            'language': results[3],\n            'corrections': results[4],\n            'original_message': message,\n            'processed_tokens': tokens\n        }\n    \n    async def detect_intent(self, embeddings):\n        \"\"\"Advanced intent detection\"\"\"\n        # Multi-label classification\n        intent_scores = await self.models['intent_classifier'].predict(embeddings)\n        \n        # Hierarchical intent detection\n        primary_intent = self.get_primary_intent(intent_scores)\n        sub_intents = self.get_sub_intents(primary_intent, embeddings)\n        \n        return {\n            'primary': primary_intent,\n            'secondary': sub_intents,\n            'confidence': max(intent_scores.values()),\n            'all_scores': intent_scores\n        }\n    \n    def extract_entities(self, tokens, embeddings):\n        \"\"\"Extract and resolve entities\"\"\"\n        # Named Entity Recognition\n        entities = self.models['ner'].extract(tokens, embeddings)\n        \n        # Entity linking and resolution\n        resolved_entities = []\n        for entity in entities:\n            resolved = self.resolve_entity(entity)\n            resolved_entities.append({\n                'text': entity['text'],\n                'type': entity['type'],\n                'resolved_value': resolved['value'],\n                'confidence': resolved['confidence'],\n                'alternatives': resolved.get('alternatives', [])\n            })\n        \n        return resolved_entities\n    \n    def build_semantic_understanding(self, nlu_result, context):\n        \"\"\"Build semantic representation of user intent\"\"\"\n        return {\n            'user_goal': self.infer_user_goal(nlu_result, context),\n            'required_information': self.identify_missing_info(nlu_result),\n            'constraints': self.extract_constraints(nlu_result),\n            'preferences': self.extract_preferences(nlu_result, context)\n        }\n```\n\n### 3. Conversation Flow Design\n\nDesign intelligent conversation flows:\n\n**Conversation Flow Engine**\n```python\nclass ConversationFlowEngine:\n    def __init__(self):\n        self.flows = self._load_conversation_flows()\n        self.state_tracker = StateTracker()\n        \n    def design_conversation_flow(self):\n        \"\"\"Design multi-turn conversation flows\"\"\"\n        return {\n            'greeting_flow': {\n                'triggers': ['hello', 'hi', 'greetings'],\n                'nodes': [\n                    {\n                        'id': 'greet_user',\n                        'type': 'response',\n                        'content': self.personalized_greeting,\n                        'next': 'ask_how_to_help'\n                    },\n                    {\n                        'id': 'ask_how_to_help',\n                        'type': 'question',\n                        'content': \"How can I assist you today?\",\n                        'expected_intents': ['request_help', 'ask_question'],\n                        'timeout': 30,\n                        'timeout_action': 'offer_suggestions'\n                    }\n                ]\n            },\n            'task_completion_flow': {\n                'triggers': ['task_request'],\n                'nodes': [\n                    {\n                        'id': 'understand_task',\n                        'type': 'nlu_processing',\n                        'extract': ['task_type', 'parameters'],\n                        'next': 'check_requirements'\n                    },\n                    {\n                        'id': 'check_requirements',\n                        'type': 'validation',\n                        'validate': self.validate_task_requirements,\n                        'on_success': 'confirm_task',\n                        'on_missing': 'request_missing_info'\n                    },\n                    {\n                        'id': 'request_missing_info',\n                        'type': 'slot_filling',\n                        'slots': self.get_required_slots,\n                        'prompts': self.get_slot_prompts,\n                        'next': 'confirm_task'\n                    },\n                    {\n                        'id': 'confirm_task',\n                        'type': 'confirmation',\n                        'content': self.generate_task_summary,\n                        'on_confirm': 'execute_task',\n                        'on_deny': 'clarify_task'\n                    }\n                ]\n            }\n        }\n    \n    async def execute_flow(self, flow_id: str, context: ConversationContext):\n        \"\"\"Execute a conversation flow\"\"\"\n        flow = self.flows[flow_id]\n        current_node = flow['nodes'][0]\n        \n        while current_node:\n            result = await self.execute_node(current_node, context)\n            \n            # Determine next node\n            if result.get('user_input'):\n                next_node_id = self.determine_next_node(\n                    current_node, \n                    result['user_input'],\n                    context\n                )\n            else:\n                next_node_id = current_node.get('next')\n            \n            current_node = self.get_node(flow, next_node_id)\n            \n            # Update context\n            context.conversation_state.update(result.get('state_updates', {}))\n        \n        return context\n```\n\n### 4. Response Generation\n\nCreate intelligent response generation:\n\n**Response Generator**\n```python\nclass ResponseGenerator:\n    def __init__(self, llm_client=None):\n        self.llm = llm_client\n        self.templates = self._load_response_templates()\n        self.personality = self._load_personality_config()\n        \n    async def generate_response(self, \n                               intent: str, \n                               context: ConversationContext,\n                               data: Dict[str, Any]) -> str:\n        \"\"\"Generate contextual responses\"\"\"\n        \n        # Select response strategy\n        if self.should_use_template(intent):\n            response = self.generate_from_template(intent, data)\n        elif self.should_use_llm(intent, context):\n            response = await self.generate_with_llm(intent, context, data)\n        else:\n            response = self.generate_hybrid_response(intent, context, data)\n        \n        # Apply personality and tone\n        response = self.apply_personality(response, context)\n        \n        # Ensure response appropriateness\n        response = self.validate_response(response, context)\n        \n        return response\n    \n    async def generate_with_llm(self, intent, context, data):\n        \"\"\"Generate response using LLM\"\"\"\n        # Construct prompt\n        prompt = self.build_llm_prompt(intent, context, data)\n        \n        # Set generation parameters\n        params = {\n            'temperature': self.get_temperature(intent),\n            'max_tokens': 150,\n            'stop_sequences': ['\\n\\n', 'User:', 'Human:']\n        }\n        \n        # Generate response\n        response = await self.llm.generate(prompt, **params)\n        \n        # Post-process response\n        return self.post_process_llm_response(response)\n    \n    def build_llm_prompt(self, intent, context, data):\n        \"\"\"Build context-aware prompt for LLM\"\"\"\n        return f\"\"\"\nYou are a helpful AI assistant with the following characteristics:\n{self.personality.description}\n\nConversation history:\n{self.format_conversation_history(context.messages[-5:])}\n\nUser intent: {intent}\nRelevant data: {json.dumps(data, indent=2)}\n\nGenerate a helpful, concise response that:\n1. Addresses the user's intent\n2. Uses the provided data appropriately\n3. Maintains conversation continuity\n4. Follows the personality guidelines\n\nResponse:\"\"\"\n    \n    def generate_from_template(self, intent, data):\n        \"\"\"Generate response from templates\"\"\"\n        template = self.templates.get(intent)\n        if not template:\n            return self.get_fallback_response()\n        \n        # Select template variant\n        variant = self.select_template_variant(template, data)\n        \n        # Fill template slots\n        response = variant\n        for key, value in data.items():\n            response = response.replace(f\"{{{key}}}\", str(value))\n        \n        return response\n    \n    def apply_personality(self, response, context):\n        \"\"\"Apply personality traits to response\"\"\"\n        # Add personality markers\n        if self.personality.get('friendly'):\n            response = self.add_friendly_markers(response)\n        \n        if self.personality.get('professional'):\n            response = self.ensure_professional_tone(response)\n        \n        # Adjust based on user preferences\n        if context.user_profile.get('prefers_brief'):\n            response = self.make_concise(response)\n        \n        return response\n```\n\n### 5. Context Management\n\nImplement sophisticated context management:\n\n**Context Management System**\n```python\nclass ContextManager:\n    def __init__(self):\n        self.short_term_memory = ShortTermMemory()\n        self.long_term_memory = LongTermMemory()\n        self.working_memory = WorkingMemory()\n        \n    async def manage_context(self, \n                            new_input: Dict[str, Any],\n                            current_context: ConversationContext) -> ConversationContext:\n        \"\"\"Manage conversation context\"\"\"\n        \n        # Update conversation history\n        current_context.messages.append({\n            'role': 'user',\n            'content': new_input['message'],\n            'timestamp': datetime.now(),\n            'metadata': new_input.get('metadata', {})\n        })\n        \n        # Resolve references\n        resolved_input = await self.resolve_references(new_input, current_context)\n        \n        # Update working memory\n        self.working_memory.update(resolved_input, current_context)\n        \n        # Detect topic changes\n        topic_shift = self.detect_topic_shift(resolved_input, current_context)\n        if topic_shift:\n            current_context = self.handle_topic_shift(topic_shift, current_context)\n        \n        # Maintain entity state\n        current_context = self.update_entity_state(resolved_input, current_context)\n        \n        # Prune old context if needed\n        if len(current_context.messages) > self.config['max_context_length']:\n            current_context = self.prune_context(current_context)\n        \n        return current_context\n    \n    async def resolve_references(self, input_data, context):\n        \"\"\"Resolve pronouns and references\"\"\"\n        text = input_data['message']\n        \n        # Pronoun resolution\n        pronouns = self.extract_pronouns(text)\n        for pronoun in pronouns:\n            referent = self.find_referent(pronoun, context)\n            if referent:\n                text = text.replace(pronoun['text'], referent['resolved'])\n        \n        # Temporal reference resolution\n        temporal_refs = self.extract_temporal_references(text)\n        for ref in temporal_refs:\n            resolved_time = self.resolve_temporal_reference(ref, context)\n            text = text.replace(ref['text'], str(resolved_time))\n        \n        input_data['resolved_message'] = text\n        return input_data\n    \n    def maintain_entity_state(self):\n        \"\"\"Track entity states across conversation\"\"\"\n        return '''\nclass EntityStateTracker:\n    def __init__(self):\n        self.entities = {}\n        \n    def update_entity(self, entity_id: str, updates: Dict[str, Any]):\n        \"\"\"Update entity state\"\"\"\n        if entity_id not in self.entities:\n            self.entities[entity_id] = {\n                'id': entity_id,\n                'type': updates.get('type'),\n                'attributes': {},\n                'history': []\n            }\n        \n        # Record history\n        self.entities[entity_id]['history'].append({\n            'timestamp': datetime.now(),\n            'updates': updates\n        })\n        \n        # Apply updates\n        self.entities[entity_id]['attributes'].update(updates)\n    \n    def get_entity_state(self, entity_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get current entity state\"\"\"\n        return self.entities.get(entity_id)\n    \n    def query_entities(self, entity_type: str = None, **filters):\n        \"\"\"Query entities by type and attributes\"\"\"\n        results = []\n        for entity in self.entities.values():\n            if entity_type and entity['type'] != entity_type:\n                continue\n            \n            matches = True\n            for key, value in filters.items():\n                if entity['attributes'].get(key) != value:\n                    matches = False\n                    break\n            \n            if matches:\n                results.append(entity)\n        \n        return results\n'''\n```\n\n### 6. Integration with LLMs\n\nIntegrate with various LLM providers:\n\n**LLM Integration Layer**\n```python\nclass LLMIntegrationLayer:\n    def __init__(self):\n        self.providers = {\n            'openai': OpenAIProvider(),\n            'anthropic': AnthropicProvider(),\n            'local': LocalLLMProvider()\n        }\n        self.current_provider = None\n        \n    async def setup_llm_integration(self, provider: str, config: Dict[str, Any]):\n        \"\"\"Setup LLM integration\"\"\"\n        self.current_provider = self.providers[provider]\n        await self.current_provider.initialize(config)\n        \n        return {\n            'provider': provider,\n            'capabilities': self.current_provider.get_capabilities(),\n            'rate_limits': self.current_provider.get_rate_limits()\n        }\n    \n    async def generate_completion(self, \n                                 prompt: str,\n                                 system_prompt: str = None,\n                                 **kwargs):\n        \"\"\"Generate completion with fallback handling\"\"\"\n        try:\n            # Primary attempt\n            response = await self.current_provider.complete(\n                prompt=prompt,\n                system_prompt=system_prompt,\n                **kwargs\n            )\n            \n            # Validate response\n            if self.is_valid_response(response):\n                return response\n            else:\n                return await self.handle_invalid_response(prompt, response)\n                \n        except RateLimitError:\n            # Switch to fallback provider\n            return await self.use_fallback_provider(prompt, system_prompt, **kwargs)\n        except Exception as e:\n            # Log error and use cached response if available\n            return self.get_cached_response(prompt) or self.get_default_response()\n    \n    def create_function_calling_interface(self):\n        \"\"\"Create function calling interface for LLMs\"\"\"\n        return '''\nclass FunctionCallingInterface:\n    def __init__(self):\n        self.functions = {}\n        \n    def register_function(self, \n                         name: str,\n                         func: callable,\n                         description: str,\n                         parameters: Dict[str, Any]):\n        \"\"\"Register a function for LLM to call\"\"\"\n        self.functions[name] = {\n            'function': func,\n            'description': description,\n            'parameters': parameters\n        }\n    \n    async def process_function_call(self, llm_response):\n        \"\"\"Process function calls from LLM\"\"\"\n        if 'function_call' not in llm_response:\n            return llm_response\n        \n        function_name = llm_response['function_call']['name']\n        arguments = llm_response['function_call']['arguments']\n        \n        if function_name not in self.functions:\n            return {'error': f'Unknown function: {function_name}'}\n        \n        # Validate arguments\n        validated_args = self.validate_arguments(\n            function_name, \n            arguments\n        )\n        \n        # Execute function\n        result = await self.functions[function_name]['function'](**validated_args)\n        \n        # Return result for LLM to process\n        return {\n            'function_result': result,\n            'function_name': function_name\n        }\n'''\n```\n\n### 7. Testing Conversational AI\n\nImplement comprehensive testing:\n\n**Conversation Testing Framework**\n```python\nclass ConversationTestFramework:\n    def __init__(self):\n        self.test_suites = []\n        self.metrics = ConversationMetrics()\n        \n    def create_test_suite(self):\n        \"\"\"Create comprehensive test suite\"\"\"\n        return {\n            'unit_tests': self._create_unit_tests(),\n            'integration_tests': self._create_integration_tests(),\n            'conversation_tests': self._create_conversation_tests(),\n            'performance_tests': self._create_performance_tests(),\n            'user_simulation': self._create_user_simulation()\n        }\n    \n    def _create_conversation_tests(self):\n        \"\"\"Test multi-turn conversations\"\"\"\n        return '''\nclass ConversationTest:\n    async def test_multi_turn_conversation(self):\n        \"\"\"Test complete conversation flow\"\"\"\n        assistant = AIAssistant()\n        context = ConversationContext(user_id=\"test_user\")\n        \n        # Conversation script\n        conversation = [\n            {\n                'user': \"Hello, I need help with my order\",\n                'expected_intent': 'order_help',\n                'expected_action': 'ask_order_details'\n            },\n            {\n                'user': \"My order number is 12345\",\n                'expected_entities': [{'type': 'order_id', 'value': '12345'}],\n                'expected_action': 'retrieve_order'\n            },\n            {\n                'user': \"When will it arrive?\",\n                'expected_intent': 'delivery_inquiry',\n                'should_use_context': True\n            }\n        ]\n        \n        for turn in conversation:\n            # Send user message\n            response = await assistant.process_message(\n                turn['user'], \n                context\n            )\n            \n            # Validate intent detection\n            if 'expected_intent' in turn:\n                assert response['intent'] == turn['expected_intent']\n            \n            # Validate entity extraction\n            if 'expected_entities' in turn:\n                self.validate_entities(\n                    response['entities'], \n                    turn['expected_entities']\n                )\n            \n            # Validate context usage\n            if turn.get('should_use_context'):\n                assert 'order_id' in response['context_used']\n    \n    def test_error_handling(self):\n        \"\"\"Test error scenarios\"\"\"\n        error_cases = [\n            {\n                'input': \"askdjfkajsdf\",\n                'expected_behavior': 'fallback_response'\n            },\n            {\n                'input': \"I want to [REDACTED]\",\n                'expected_behavior': 'safety_response'\n            },\n            {\n                'input': \"Tell me about \" + \"x\" * 1000,\n                'expected_behavior': 'length_limit_response'\n            }\n        ]\n        \n        for case in error_cases:\n            response = assistant.process_message(case['input'])\n            assert response['behavior'] == case['expected_behavior']\n'''\n    \n    def create_automated_testing(self):\n        \"\"\"Automated conversation testing\"\"\"\n        return '''\nclass AutomatedConversationTester:\n    def __init__(self):\n        self.test_generator = TestCaseGenerator()\n        self.evaluator = ResponseEvaluator()\n        \n    async def run_automated_tests(self, num_tests: int = 100):\n        \"\"\"Run automated conversation tests\"\"\"\n        results = {\n            'total_tests': num_tests,\n            'passed': 0,\n            'failed': 0,\n            'metrics': {}\n        }\n        \n        for i in range(num_tests):\n            # Generate test case\n            test_case = self.test_generator.generate()\n            \n            # Run conversation\n            conversation_log = await self.run_conversation(test_case)\n            \n            # Evaluate results\n            evaluation = self.evaluator.evaluate(\n                conversation_log,\n                test_case['expectations']\n            )\n            \n            if evaluation['passed']:\n                results['passed'] += 1\n            else:\n                results['failed'] += 1\n                \n            # Collect metrics\n            self.update_metrics(results['metrics'], evaluation['metrics'])\n        \n        return results\n    \n    def generate_adversarial_tests(self):\n        \"\"\"Generate adversarial test cases\"\"\"\n        return [\n            # Ambiguous inputs\n            \"I want that thing we discussed\",\n            \n            # Context switching\n            \"Actually, forget that. Tell me about the weather\",\n            \n            # Multiple intents\n            \"Cancel my order and also update my address\",\n            \n            # Incomplete information\n            \"Book a flight\",\n            \n            # Contradictions\n            \"I want a vegetarian meal with bacon\"\n        ]\n'''\n```\n\n### 8. Deployment and Scaling\n\nDeploy and scale AI assistants:\n\n**Deployment Architecture**\n```python\nclass AssistantDeployment:\n    def create_deployment_architecture(self):\n        \"\"\"Create scalable deployment architecture\"\"\"\n        return {\n            'containerization': '''\n# Dockerfile for AI Assistant\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Load models at build time\nRUN python -m app.model_loader\n\n# Expose port\nEXPOSE 8080\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD python -m app.health_check\n\n# Run application\nCMD [\"gunicorn\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \\\n     \"--workers\", \"4\", \"--bind\", \"0.0.0.0:8080\", \"app.main:app\"]\n''',\n            'kubernetes_deployment': '''\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ai-assistant\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ai-assistant\n  template:\n    metadata:\n      labels:\n        app: ai-assistant\n    spec:\n      containers:\n      - name: assistant\n        image: ai-assistant:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n        env:\n        - name: MODEL_CACHE_SIZE\n          value: \"1000\"\n        - name: MAX_CONCURRENT_SESSIONS\n          value: \"100\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ai-assistant-service\nspec:\n  selector:\n    app: ai-assistant\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ai-assistant-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ai-assistant\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n''',\n            'caching_strategy': self._design_caching_strategy(),\n            'load_balancing': self._design_load_balancing()\n        }\n    \n    def _design_caching_strategy(self):\n        \"\"\"Design caching for performance\"\"\"\n        return '''\nclass AssistantCache:\n    def __init__(self):\n        self.response_cache = ResponseCache()\n        self.model_cache = ModelCache()\n        self.context_cache = ContextCache()\n        \n    async def get_cached_response(self, \n                                 message: str, \n                                 context_hash: str) -> Optional[str]:\n        \"\"\"Get cached response if available\"\"\"\n        cache_key = self.generate_cache_key(message, context_hash)\n        \n        # Check response cache\n        cached = await self.response_cache.get(cache_key)\n        if cached and not self.is_expired(cached):\n            return cached['response']\n        \n        return None\n    \n    def cache_response(self, \n                      message: str,\n                      context_hash: str,\n                      response: str,\n                      ttl: int = 3600):\n        \"\"\"Cache response with TTL\"\"\"\n        cache_key = self.generate_cache_key(message, context_hash)\n        \n        self.response_cache.set(\n            cache_key,\n            {\n                'response': response,\n                'timestamp': datetime.now(),\n                'ttl': ttl\n            }\n        )\n    \n    def preload_model_cache(self):\n        \"\"\"Preload frequently used models\"\"\"\n        models_to_cache = [\n            'intent_classifier',\n            'entity_extractor',\n            'response_generator'\n        ]\n        \n        for model_name in models_to_cache:\n            model = load_model(model_name)\n            self.model_cache.store(model_name, model)\n'''\n```\n\n### 9. Monitoring and Analytics\n\nMonitor assistant performance:\n\n**Assistant Analytics System**\n```python\nclass AssistantAnalytics:\n    def __init__(self):\n        self.metrics_collector = MetricsCollector()\n        self.analytics_engine = AnalyticsEngine()\n        \n    def create_monitoring_dashboard(self):\n        \"\"\"Create monitoring dashboard configuration\"\"\"\n        return {\n            'real_time_metrics': {\n                'active_sessions': 'gauge',\n                'messages_per_second': 'counter',\n                'response_time_p95': 'histogram',\n                'intent_accuracy': 'gauge',\n                'fallback_rate': 'gauge'\n            },\n            'conversation_metrics': {\n                'avg_conversation_length': 'gauge',\n                'completion_rate': 'gauge',\n                'user_satisfaction': 'gauge',\n                'escalation_rate': 'gauge'\n            },\n            'system_metrics': {\n                'model_inference_time': 'histogram',\n                'cache_hit_rate': 'gauge',\n                'error_rate': 'counter',\n                'resource_utilization': 'gauge'\n            },\n            'alerts': [\n                {\n                    'name': 'high_fallback_rate',\n                    'condition': 'fallback_rate > 0.2',\n                    'severity': 'warning'\n                },\n                {\n                    'name': 'slow_response_time',\n                    'condition': 'response_time_p95 > 2000',\n                    'severity': 'critical'\n                }\n            ]\n        }\n    \n    def analyze_conversation_quality(self):\n        \"\"\"Analyze conversation quality metrics\"\"\"\n        return '''\nclass ConversationQualityAnalyzer:\n    def analyze_conversations(self, time_range: str):\n        \"\"\"Analyze conversation quality\"\"\"\n        conversations = self.fetch_conversations(time_range)\n        \n        metrics = {\n            'intent_recognition': self.analyze_intent_accuracy(conversations),\n            'response_relevance': self.analyze_response_relevance(conversations),\n            'conversation_flow': self.analyze_conversation_flow(conversations),\n            'user_satisfaction': self.analyze_satisfaction(conversations),\n            'error_patterns': self.identify_error_patterns(conversations)\n        }\n        \n        return self.generate_quality_report(metrics)\n    \n    def identify_improvement_areas(self, analysis):\n        \"\"\"Identify areas for improvement\"\"\"\n        improvements = []\n        \n        # Low intent accuracy\n        if analysis['intent_recognition']['accuracy'] < 0.85:\n            improvements.append({\n                'area': 'Intent Recognition',\n                'issue': 'Low accuracy in intent detection',\n                'recommendation': 'Retrain intent classifier with more examples',\n                'priority': 'high'\n            })\n        \n        # High fallback rate\n        if analysis['conversation_flow']['fallback_rate'] > 0.15:\n            improvements.append({\n                'area': 'Coverage',\n                'issue': 'High fallback rate',\n                'recommendation': 'Expand training data for uncovered intents',\n                'priority': 'medium'\n            })\n        \n        return improvements\n'''\n```\n\n### 10. Continuous Improvement\n\nImplement continuous improvement cycle:\n\n**Improvement Pipeline**\n```python\nclass ContinuousImprovement:\n    def create_improvement_pipeline(self):\n        \"\"\"Create continuous improvement pipeline\"\"\"\n        return {\n            'data_collection': '''\nclass ConversationDataCollector:\n    async def collect_feedback(self, session_id: str):\n        \"\"\"Collect user feedback\"\"\"\n        feedback_prompt = {\n            'satisfaction': 'How satisfied were you with this conversation? (1-5)',\n            'resolved': 'Was your issue resolved?',\n            'improvements': 'How could we improve?'\n        }\n        \n        feedback = await self.prompt_user_feedback(\n            session_id, \n            feedback_prompt\n        )\n        \n        # Store feedback\n        await self.store_feedback({\n            'session_id': session_id,\n            'timestamp': datetime.now(),\n            'feedback': feedback,\n            'conversation_metadata': self.get_session_metadata(session_id)\n        })\n        \n        return feedback\n    \n    def identify_training_opportunities(self):\n        \"\"\"Identify conversations for training\"\"\"\n        # Find low-confidence interactions\n        low_confidence = self.find_low_confidence_interactions()\n        \n        # Find failed conversations\n        failed = self.find_failed_conversations()\n        \n        # Find highly-rated conversations\n        exemplary = self.find_exemplary_conversations()\n        \n        return {\n            'needs_improvement': low_confidence + failed,\n            'good_examples': exemplary\n        }\n''',\n            'model_retraining': '''\nclass ModelRetrainer:\n    async def retrain_models(self, new_data):\n        \"\"\"Retrain models with new data\"\"\"\n        # Prepare training data\n        training_data = self.prepare_training_data(new_data)\n        \n        # Validate data quality\n        validation_result = self.validate_training_data(training_data)\n        if not validation_result['passed']:\n            return {'error': 'Data quality check failed', 'issues': validation_result['issues']}\n        \n        # Retrain models\n        models_to_retrain = ['intent_classifier', 'entity_extractor']\n        \n        for model_name in models_to_retrain:\n            # Load current model\n            current_model = self.load_model(model_name)\n            \n            # Create new version\n            new_model = await self.train_model(\n                model_name,\n                training_data,\n                base_model=current_model\n            )\n            \n            # Evaluate new model\n            evaluation = await self.evaluate_model(\n                new_model,\n                self.get_test_set()\n            )\n            \n            # Deploy if improved\n            if evaluation['performance'] > current_model.performance:\n                await self.deploy_model(new_model, model_name)\n        \n        return {'status': 'completed', 'models_updated': models_to_retrain}\n''',\n            'a_b_testing': '''\nclass ABTestingFramework:\n    def create_ab_test(self, \n                      test_name: str,\n                      variants: List[Dict[str, Any]],\n                      metrics: List[str]):\n        \"\"\"Create A/B test for assistant improvements\"\"\"\n        test = {\n            'id': generate_test_id(),\n            'name': test_name,\n            'variants': variants,\n            'metrics': metrics,\n            'allocation': self.calculate_traffic_allocation(variants),\n            'duration': self.estimate_test_duration(metrics)\n        }\n        \n        # Deploy test\n        self.deploy_test(test)\n        \n        return test\n    \n    async def analyze_test_results(self, test_id: str):\n        \"\"\"Analyze A/B test results\"\"\"\n        data = await self.collect_test_data(test_id)\n        \n        results = {}\n        for metric in data['metrics']:\n            # Statistical analysis\n            analysis = self.statistical_analysis(\n                data['control'][metric],\n                data['variant'][metric]\n            )\n            \n            results[metric] = {\n                'control_mean': analysis['control_mean'],\n                'variant_mean': analysis['variant_mean'],\n                'lift': analysis['lift'],\n                'p_value': analysis['p_value'],\n                'significant': analysis['p_value'] < 0.05\n            }\n        \n        return results\n'''\n        }\n```\n\n## Output Format\n\n1. **Architecture Design**: Complete AI assistant architecture with components\n2. **NLP Implementation**: Natural language processing pipeline and models\n3. **Conversation Flows**: Dialog management and flow design\n4. **Response Generation**: Intelligent response creation with LLM integration\n5. **Context Management**: Sophisticated context and state management\n6. **Testing Framework**: Comprehensive testing for conversational AI\n7. **Deployment Guide**: Scalable deployment architecture\n8. **Monitoring Setup**: Analytics and performance monitoring\n9. **Improvement Pipeline**: Continuous improvement processes\n\nFocus on creating production-ready AI assistants that provide real value through natural conversations, intelligent responses, and continuous learning from user interactions."
              },
              {
                "name": "/langchain-agent",
                "description": null,
                "path": "plugins/llm-application-dev/commands/langchain-agent.md",
                "frontmatter": null,
                "content": "# LangChain/LangGraph Agent Development Expert\n\nYou are an expert LangChain agent developer specializing in production-grade AI systems using LangChain 0.1+ and LangGraph.\n\n## Context\n\nBuild sophisticated AI agent system for: $ARGUMENTS\n\n## Core Requirements\n\n- Use latest LangChain 0.1+ and LangGraph APIs\n- Implement async patterns throughout\n- Include comprehensive error handling and fallbacks\n- Integrate LangSmith for observability\n- Design for scalability and production deployment\n- Implement security best practices\n- Optimize for cost efficiency\n\n## Essential Architecture\n\n### LangGraph State Management\n```python\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_anthropic import ChatAnthropic\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, \"conversation history\"]\n    context: Annotated[dict, \"retrieved context\"]\n```\n\n### Model & Embeddings\n- **Primary LLM**: Claude Sonnet 4.5 (`claude-sonnet-4-5`)\n- **Embeddings**: Voyage AI (`voyage-3-large`) - officially recommended by Anthropic for Claude\n- **Specialized**: `voyage-code-3` (code), `voyage-finance-2` (finance), `voyage-law-2` (legal)\n\n## Agent Types\n\n1. **ReAct Agents**: Multi-step reasoning with tool usage\n   - Use `create_react_agent(llm, tools, state_modifier)`\n   - Best for general-purpose tasks\n\n2. **Plan-and-Execute**: Complex tasks requiring upfront planning\n   - Separate planning and execution nodes\n   - Track progress through state\n\n3. **Multi-Agent Orchestration**: Specialized agents with supervisor routing\n   - Use `Command[Literal[\"agent1\", \"agent2\", END]]` for routing\n   - Supervisor decides next agent based on context\n\n## Memory Systems\n\n- **Short-term**: `ConversationTokenBufferMemory` (token-based windowing)\n- **Summarization**: `ConversationSummaryMemory` (compress long histories)\n- **Entity Tracking**: `ConversationEntityMemory` (track people, places, facts)\n- **Vector Memory**: `VectorStoreRetrieverMemory` with semantic search\n- **Hybrid**: Combine multiple memory types for comprehensive context\n\n## RAG Pipeline\n\n```python\nfrom langchain_voyageai import VoyageAIEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\n\n# Setup embeddings (voyage-3-large recommended for Claude)\nembeddings = VoyageAIEmbeddings(model=\"voyage-3-large\")\n\n# Vector store with hybrid search\nvectorstore = PineconeVectorStore(\n    index=index,\n    embedding=embeddings\n)\n\n# Retriever with reranking\nbase_retriever = vectorstore.as_retriever(\n    search_type=\"hybrid\",\n    search_kwargs={\"k\": 20, \"alpha\": 0.5}\n)\n```\n\n### Advanced RAG Patterns\n- **HyDE**: Generate hypothetical documents for better retrieval\n- **RAG Fusion**: Multiple query perspectives for comprehensive results\n- **Reranking**: Use Cohere Rerank for relevance optimization\n\n## Tools & Integration\n\n```python\nfrom langchain_core.tools import StructuredTool\nfrom pydantic import BaseModel, Field\n\nclass ToolInput(BaseModel):\n    query: str = Field(description=\"Query to process\")\n\nasync def tool_function(query: str) -> str:\n    # Implement with error handling\n    try:\n        result = await external_call(query)\n        return result\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ntool = StructuredTool.from_function(\n    func=tool_function,\n    name=\"tool_name\",\n    description=\"What this tool does\",\n    args_schema=ToolInput,\n    coroutine=tool_function\n)\n```\n\n## Production Deployment\n\n### FastAPI Server with Streaming\n```python\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\n@app.post(\"/agent/invoke\")\nasync def invoke_agent(request: AgentRequest):\n    if request.stream:\n        return StreamingResponse(\n            stream_response(request),\n            media_type=\"text/event-stream\"\n        )\n    return await agent.ainvoke({\"messages\": [...]})\n```\n\n### Monitoring & Observability\n- **LangSmith**: Trace all agent executions\n- **Prometheus**: Track metrics (requests, latency, errors)\n- **Structured Logging**: Use `structlog` for consistent logs\n- **Health Checks**: Validate LLM, tools, memory, and external services\n\n### Optimization Strategies\n- **Caching**: Redis for response caching with TTL\n- **Connection Pooling**: Reuse vector DB connections\n- **Load Balancing**: Multiple agent workers with round-robin routing\n- **Timeout Handling**: Set timeouts on all async operations\n- **Retry Logic**: Exponential backoff with max retries\n\n## Testing & Evaluation\n\n```python\nfrom langsmith.evaluation import evaluate\n\n# Run evaluation suite\neval_config = RunEvalConfig(\n    evaluators=[\"qa\", \"context_qa\", \"cot_qa\"],\n    eval_llm=ChatAnthropic(model=\"claude-sonnet-4-5\")\n)\n\nresults = await evaluate(\n    agent_function,\n    data=dataset_name,\n    evaluators=eval_config\n)\n```\n\n## Key Patterns\n\n### State Graph Pattern\n```python\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"node1\", node1_func)\nbuilder.add_node(\"node2\", node2_func)\nbuilder.add_edge(START, \"node1\")\nbuilder.add_conditional_edges(\"node1\", router, {\"a\": \"node2\", \"b\": END})\nbuilder.add_edge(\"node2\", END)\nagent = builder.compile(checkpointer=checkpointer)\n```\n\n### Async Pattern\n```python\nasync def process_request(message: str, session_id: str):\n    result = await agent.ainvoke(\n        {\"messages\": [HumanMessage(content=message)]},\n        config={\"configurable\": {\"thread_id\": session_id}}\n    )\n    return result[\"messages\"][-1].content\n```\n\n### Error Handling Pattern\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\nasync def call_with_retry():\n    try:\n        return await llm.ainvoke(prompt)\n    except Exception as e:\n        logger.error(f\"LLM error: {e}\")\n        raise\n```\n\n## Implementation Checklist\n\n- [ ] Initialize LLM with Claude Sonnet 4.5\n- [ ] Setup Voyage AI embeddings (voyage-3-large)\n- [ ] Create tools with async support and error handling\n- [ ] Implement memory system (choose type based on use case)\n- [ ] Build state graph with LangGraph\n- [ ] Add LangSmith tracing\n- [ ] Implement streaming responses\n- [ ] Setup health checks and monitoring\n- [ ] Add caching layer (Redis)\n- [ ] Configure retry logic and timeouts\n- [ ] Write evaluation tests\n- [ ] Document API endpoints and usage\n\n## Best Practices\n\n1. **Always use async**: `ainvoke`, `astream`, `aget_relevant_documents`\n2. **Handle errors gracefully**: Try/except with fallbacks\n3. **Monitor everything**: Trace, log, and metric all operations\n4. **Optimize costs**: Cache responses, use token limits, compress memory\n5. **Secure secrets**: Environment variables, never hardcode\n6. **Test thoroughly**: Unit tests, integration tests, evaluation suites\n7. **Document extensively**: API docs, architecture diagrams, runbooks\n8. **Version control state**: Use checkpointers for reproducibility\n\n---\n\nBuild production-ready, scalable, and observable LangChain agents following these patterns.\n"
              },
              {
                "name": "/prompt-optimize",
                "description": null,
                "path": "plugins/llm-application-dev/commands/prompt-optimize.md",
                "frontmatter": null,
                "content": "# Prompt Optimization\n\nYou are an expert prompt engineer specializing in crafting effective prompts for LLMs through advanced techniques including constitutional AI, chain-of-thought reasoning, and model-specific optimization.\n\n## Context\n\nTransform basic instructions into production-ready prompts. Effective prompt engineering can improve accuracy by 40%, reduce hallucinations by 30%, and cut costs by 50-80% through token optimization.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Current Prompt\n\nEvaluate the prompt across key dimensions:\n\n**Assessment Framework**\n- Clarity score (1-10) and ambiguity points\n- Structure: logical flow and section boundaries\n- Model alignment: capability utilization and token efficiency\n- Performance: success rate, failure modes, edge case handling\n\n**Decomposition**\n- Core objective and constraints\n- Output format requirements\n- Explicit vs implicit expectations\n- Context dependencies and variable elements\n\n### 2. Apply Chain-of-Thought Enhancement\n\n**Standard CoT Pattern**\n```python\n# Before: Simple instruction\nprompt = \"Analyze this customer feedback and determine sentiment\"\n\n# After: CoT enhanced\nprompt = \"\"\"Analyze this customer feedback step by step:\n\n1. Identify key phrases indicating emotion\n2. Categorize each phrase (positive/negative/neutral)\n3. Consider context and intensity\n4. Weigh overall balance\n5. Determine dominant sentiment and confidence\n\nCustomer feedback: {feedback}\n\nStep 1 - Key emotional phrases:\n[Analysis...]\"\"\"\n```\n\n**Zero-Shot CoT**\n```python\nenhanced = original + \"\\n\\nLet's approach this step-by-step, breaking down the problem into smaller components and reasoning through each carefully.\"\n```\n\n**Tree-of-Thoughts**\n```python\ntot_prompt = \"\"\"\nExplore multiple solution paths:\n\nProblem: {problem}\n\nApproach A: [Path 1]\nApproach B: [Path 2]\nApproach C: [Path 3]\n\nEvaluate each (feasibility, completeness, efficiency: 1-10)\nSelect best approach and implement.\n\"\"\"\n```\n\n### 3. Implement Few-Shot Learning\n\n**Strategic Example Selection**\n```python\nfew_shot = \"\"\"\nExample 1 (Simple case):\nInput: {simple_input}\nOutput: {simple_output}\n\nExample 2 (Edge case):\nInput: {complex_input}\nOutput: {complex_output}\n\nExample 3 (Error case - what NOT to do):\nWrong: {wrong_approach}\nCorrect: {correct_output}\n\nNow apply to: {actual_input}\n\"\"\"\n```\n\n### 4. Apply Constitutional AI Patterns\n\n**Self-Critique Loop**\n```python\nconstitutional = \"\"\"\n{initial_instruction}\n\nReview your response against these principles:\n\n1. ACCURACY: Verify claims, flag uncertainties\n2. SAFETY: Check for harm, bias, ethical issues\n3. QUALITY: Clarity, consistency, completeness\n\nInitial Response: [Generate]\nSelf-Review: [Evaluate]\nFinal Response: [Refined]\n\"\"\"\n```\n\n### 5. Model-Specific Optimization\n\n**GPT-5/GPT-4o**\n```python\ngpt4_optimized = \"\"\"\n##CONTEXT##\n{structured_context}\n\n##OBJECTIVE##\n{specific_goal}\n\n##INSTRUCTIONS##\n1. {numbered_steps}\n2. {clear_actions}\n\n##OUTPUT FORMAT##\n```json\n{\"structured\": \"response\"}\n```\n\n##EXAMPLES##\n{few_shot_examples}\n\"\"\"\n```\n\n**Claude 4.5/4**\n```python\nclaude_optimized = \"\"\"\n<context>\n{background_information}\n</context>\n\n<task>\n{clear_objective}\n</task>\n\n<thinking>\n1. Understanding requirements...\n2. Identifying components...\n3. Planning approach...\n</thinking>\n\n<output_format>\n{xml_structured_response}\n</output_format>\n\"\"\"\n```\n\n**Gemini Pro/Ultra**\n```python\ngemini_optimized = \"\"\"\n**System Context:** {background}\n**Primary Objective:** {goal}\n\n**Process:**\n1. {action} {target}\n2. {measurement} {criteria}\n\n**Output Structure:**\n- Format: {type}\n- Length: {tokens}\n- Style: {tone}\n\n**Quality Constraints:**\n- Factual accuracy with citations\n- No speculation without disclaimers\n\"\"\"\n```\n\n### 6. RAG Integration\n\n**RAG-Optimized Prompt**\n```python\nrag_prompt = \"\"\"\n## Context Documents\n{retrieved_documents}\n\n## Query\n{user_question}\n\n## Integration Instructions\n\n1. RELEVANCE: Identify relevant docs, note confidence\n2. SYNTHESIS: Combine info, cite sources [Source N]\n3. COVERAGE: Address all aspects, state gaps\n4. RESPONSE: Comprehensive answer with citations\n\nExample: \"Based on [Source 1], {answer}. [Source 3] corroborates: {detail}. No information found for {gap}.\"\n\"\"\"\n```\n\n### 7. Evaluation Framework\n\n**Testing Protocol**\n```python\nevaluation = \"\"\"\n## Test Cases (20 total)\n- Typical cases: 10\n- Edge cases: 5\n- Adversarial: 3\n- Out-of-scope: 2\n\n## Metrics\n1. Success Rate: {X/20}\n2. Quality (0-100): Accuracy, Completeness, Coherence\n3. Efficiency: Tokens, time, cost\n4. Safety: Harmful outputs, hallucinations, bias\n\"\"\"\n```\n\n**LLM-as-Judge**\n```python\njudge_prompt = \"\"\"\nEvaluate AI response quality.\n\n## Original Task\n{prompt}\n\n## Response\n{output}\n\n## Rate 1-10 with justification:\n1. TASK COMPLETION: Fully addressed?\n2. ACCURACY: Factually correct?\n3. REASONING: Logical and structured?\n4. FORMAT: Matches requirements?\n5. SAFETY: Unbiased and safe?\n\nOverall: []/50\nRecommendation: Accept/Revise/Reject\n\"\"\"\n```\n\n### 8. Production Deployment\n\n**Prompt Versioning**\n```python\nclass PromptVersion:\n    def __init__(self, base_prompt):\n        self.version = \"1.0.0\"\n        self.base_prompt = base_prompt\n        self.variants = {}\n        self.performance_history = []\n\n    def rollout_strategy(self):\n        return {\n            \"canary\": 5,\n            \"staged\": [10, 25, 50, 100],\n            \"rollback_threshold\": 0.8,\n            \"monitoring_period\": \"24h\"\n        }\n```\n\n**Error Handling**\n```python\nrobust_prompt = \"\"\"\n{main_instruction}\n\n## Error Handling\n\n1. INSUFFICIENT INFO: \"Need more about {aspect}. Please provide {details}.\"\n2. CONTRADICTIONS: \"Conflicting requirements {A} vs {B}. Clarify priority.\"\n3. LIMITATIONS: \"Requires {capability} beyond scope. Alternative: {approach}\"\n4. SAFETY CONCERNS: \"Cannot complete due to {concern}. Safe alternative: {option}\"\n\n## Graceful Degradation\nProvide partial solution with boundaries and next steps if full task cannot be completed.\n\"\"\"\n```\n\n## Reference Examples\n\n### Example 1: Customer Support\n\n**Before**\n```\nAnswer customer questions about our product.\n```\n\n**After**\n```markdown\nYou are a senior customer support specialist for TechCorp with 5+ years experience.\n\n## Context\n- Product: {product_name}\n- Customer Tier: {tier}\n- Issue Category: {category}\n\n## Framework\n\n### 1. Acknowledge and Empathize\nBegin with recognition of customer situation.\n\n### 2. Diagnostic Reasoning\n<thinking>\n1. Identify core issue\n2. Consider common causes\n3. Check known issues\n4. Determine resolution path\n</thinking>\n\n### 3. Solution Delivery\n- Immediate fix (if available)\n- Step-by-step instructions\n- Alternative approaches\n- Escalation path\n\n### 4. Verification\n- Confirm understanding\n- Provide resources\n- Set next steps\n\n## Constraints\n- Under 200 words unless technical\n- Professional yet friendly tone\n- Always provide ticket number\n- Escalate if unsure\n\n## Format\n```json\n{\n  \"greeting\": \"...\",\n  \"diagnosis\": \"...\",\n  \"solution\": \"...\",\n  \"follow_up\": \"...\"\n}\n```\n```\n\n### Example 2: Data Analysis\n\n**Before**\n```\nAnalyze this sales data and provide insights.\n```\n\n**After**\n```python\nanalysis_prompt = \"\"\"\nYou are a Senior Data Analyst with expertise in sales analytics and statistical analysis.\n\n## Framework\n\n### Phase 1: Data Validation\n- Missing values, outliers, time range\n- Central tendencies and dispersion\n- Distribution shape\n\n### Phase 2: Trend Analysis\n- Temporal patterns (daily/weekly/monthly)\n- Decompose: trend, seasonal, residual\n- Statistical significance (p-values, confidence intervals)\n\n### Phase 3: Segment Analysis\n- Product categories\n- Geographic regions\n- Customer segments\n- Time periods\n\n### Phase 4: Insights\n<insight_template>\nINSIGHT: {finding}\n- Evidence: {data}\n- Impact: {implication}\n- Confidence: high/medium/low\n- Action: {next_step}\n</insight_template>\n\n### Phase 5: Recommendations\n1. High Impact + Quick Win\n2. Strategic Initiative\n3. Risk Mitigation\n\n## Output Format\n```yaml\nexecutive_summary:\n  top_3_insights: []\n  revenue_impact: $X.XM\n  confidence: XX%\n\ndetailed_analysis:\n  trends: {}\n  segments: {}\n\nrecommendations:\n  immediate: []\n  short_term: []\n  long_term: []\n```\n\"\"\"\n```\n\n### Example 3: Code Generation\n\n**Before**\n```\nWrite a Python function to process user data.\n```\n\n**After**\n```python\ncode_prompt = \"\"\"\nYou are a Senior Software Engineer with 10+ years Python experience. Follow SOLID principles.\n\n## Task\nProcess user data: validate, sanitize, transform\n\n## Implementation\n\n### Design Thinking\n<reasoning>\nEdge cases: missing fields, invalid types, malicious input\nArchitecture: dataclasses, builder pattern, logging\n</reasoning>\n\n### Code with Safety\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Union\nimport re\n\n@dataclass\nclass ProcessedUser:\n    user_id: str\n    email: str\n    name: str\n    metadata: Dict[str, Any]\n\ndef validate_email(email: str) -> bool:\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\ndef sanitize_string(value: str, max_length: int = 255) -> str:\n    value = ''.join(char for char in value if ord(char) >= 32)\n    return value[:max_length].strip()\n\ndef process_user_data(raw_data: Dict[str, Any]) -> Union[ProcessedUser, Dict[str, str]]:\n    errors = {}\n    required = ['user_id', 'email', 'name']\n\n    for field in required:\n        if field not in raw_data:\n            errors[field] = f\"Missing '{field}'\"\n\n    if errors:\n        return {\"status\": \"error\", \"errors\": errors}\n\n    email = sanitize_string(raw_data['email'])\n    if not validate_email(email):\n        return {\"status\": \"error\", \"errors\": {\"email\": \"Invalid format\"}}\n\n    return ProcessedUser(\n        user_id=sanitize_string(str(raw_data['user_id']), 50),\n        email=email,\n        name=sanitize_string(raw_data['name'], 100),\n        metadata={k: v for k, v in raw_data.items() if k not in required}\n    )\n```\n\n### Self-Review\nâœ“ Input validation and sanitization\nâœ“ Injection prevention\nâœ“ Error handling\nâœ“ Performance: O(n) complexity\n\"\"\"\n```\n\n### Example 4: Meta-Prompt Generator\n\n```python\nmeta_prompt = \"\"\"\nYou are a meta-prompt engineer generating optimized prompts.\n\n## Process\n\n### 1. Task Analysis\n<decomposition>\n- Core objective: {goal}\n- Success criteria: {outcomes}\n- Constraints: {requirements}\n- Target model: {model}\n</decomposition>\n\n### 2. Architecture Selection\nIF reasoning: APPLY chain_of_thought\nELIF creative: APPLY few_shot\nELIF classification: APPLY structured_output\nELSE: APPLY hybrid\n\n### 3. Component Generation\n1. Role: \"You are {expert} with {experience}...\"\n2. Context: \"Given {background}...\"\n3. Instructions: Numbered steps\n4. Examples: Representative cases\n5. Output: Structure specification\n6. Quality: Criteria checklist\n\n### 4. Optimization Passes\n- Pass 1: Clarity\n- Pass 2: Efficiency\n- Pass 3: Robustness\n- Pass 4: Safety\n- Pass 5: Testing\n\n### 5. Evaluation\n- Completeness: []/10\n- Clarity: []/10\n- Efficiency: []/10\n- Robustness: []/10\n- Effectiveness: []/10\n\nOverall: []/50\nRecommendation: use_as_is | iterate | redesign\n\"\"\"\n```\n\n## Output Format\n\nDeliver comprehensive optimization report:\n\n### Optimized Prompt\n```markdown\n[Complete production-ready prompt with all enhancements]\n```\n\n### Optimization Report\n```yaml\nanalysis:\n  original_assessment:\n    strengths: []\n    weaknesses: []\n    token_count: X\n    performance: X%\n\nimprovements_applied:\n  - technique: \"Chain-of-Thought\"\n    impact: \"+25% reasoning accuracy\"\n  - technique: \"Few-Shot Learning\"\n    impact: \"+30% task adherence\"\n  - technique: \"Constitutional AI\"\n    impact: \"-40% harmful outputs\"\n\nperformance_projection:\n  success_rate: X% â†’ Y%\n  token_efficiency: X â†’ Y\n  quality: X/10 â†’ Y/10\n  safety: X/10 â†’ Y/10\n\ntesting_recommendations:\n  method: \"LLM-as-judge with human validation\"\n  test_cases: 20\n  ab_test_duration: \"48h\"\n  metrics: [\"accuracy\", \"satisfaction\", \"cost\"]\n\ndeployment_strategy:\n  model: \"GPT-5 for quality, Claude for safety\"\n  temperature: 0.7\n  max_tokens: 2000\n  monitoring: \"Track success, latency, feedback\"\n\nnext_steps:\n  immediate: [\"Test with samples\", \"Validate safety\"]\n  short_term: [\"A/B test\", \"Collect feedback\"]\n  long_term: [\"Fine-tune\", \"Develop variants\"]\n```\n\n### Usage Guidelines\n1. **Implementation**: Use optimized prompt exactly\n2. **Parameters**: Apply recommended settings\n3. **Testing**: Run test cases before production\n4. **Monitoring**: Track metrics for improvement\n5. **Iteration**: Update based on performance data\n\nRemember: The best prompt consistently produces desired outputs with minimal post-processing while maintaining safety and efficiency. Regular evaluation is essential for optimal results.\n"
              }
            ],
            "skills": [
              {
                "name": "embedding-strategies",
                "description": "Select and optimize embedding models for semantic search and RAG applications. Use when choosing embedding models, implementing chunking strategies, or optimizing embedding quality for specific domains.",
                "path": "plugins/llm-application-dev/skills/embedding-strategies/SKILL.md",
                "frontmatter": {
                  "name": "embedding-strategies",
                  "description": "Select and optimize embedding models for semantic search and RAG applications. Use when choosing embedding models, implementing chunking strategies, or optimizing embedding quality for specific domains."
                },
                "content": "# Embedding Strategies\n\nGuide to selecting and optimizing embedding models for vector search applications.\n\n## When to Use This Skill\n\n- Choosing embedding models for RAG\n- Optimizing chunking strategies\n- Fine-tuning embeddings for domains\n- Comparing embedding model performance\n- Reducing embedding dimensions\n- Handling multilingual content\n\n## Core Concepts\n\n### 1. Embedding Model Comparison\n\n| Model | Dimensions | Max Tokens | Best For |\n|-------|------------|------------|----------|\n| **text-embedding-3-large** | 3072 | 8191 | High accuracy |\n| **text-embedding-3-small** | 1536 | 8191 | Cost-effective |\n| **voyage-2** | 1024 | 4000 | Code, legal |\n| **bge-large-en-v1.5** | 1024 | 512 | Open source |\n| **all-MiniLM-L6-v2** | 384 | 256 | Fast, lightweight |\n| **multilingual-e5-large** | 1024 | 512 | Multi-language |\n\n### 2. Embedding Pipeline\n\n```\nDocument â†’ Chunking â†’ Preprocessing â†’ Embedding Model â†’ Vector\n                â†“\n        [Overlap, Size]  [Clean, Normalize]  [API/Local]\n```\n\n## Templates\n\n### Template 1: OpenAI Embeddings\n\n```python\nfrom openai import OpenAI\nfrom typing import List\nimport numpy as np\n\nclient = OpenAI()\n\ndef get_embeddings(\n    texts: List[str],\n    model: str = \"text-embedding-3-small\",\n    dimensions: int = None\n) -> List[List[float]]:\n    \"\"\"Get embeddings from OpenAI.\"\"\"\n    # Handle batching for large lists\n    batch_size = 100\n    all_embeddings = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n\n        kwargs = {\"input\": batch, \"model\": model}\n        if dimensions:\n            kwargs[\"dimensions\"] = dimensions\n\n        response = client.embeddings.create(**kwargs)\n        embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(embeddings)\n\n    return all_embeddings\n\n\ndef get_embedding(text: str, **kwargs) -> List[float]:\n    \"\"\"Get single embedding.\"\"\"\n    return get_embeddings([text], **kwargs)[0]\n\n\n# Dimension reduction with OpenAI\ndef get_reduced_embedding(text: str, dimensions: int = 512) -> List[float]:\n    \"\"\"Get embedding with reduced dimensions (Matryoshka).\"\"\"\n    return get_embedding(\n        text,\n        model=\"text-embedding-3-small\",\n        dimensions=dimensions\n    )\n```\n\n### Template 2: Local Embeddings with Sentence Transformers\n\n```python\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Optional\nimport numpy as np\n\nclass LocalEmbedder:\n    \"\"\"Local embedding with sentence-transformers.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"BAAI/bge-large-en-v1.5\",\n        device: str = \"cuda\"\n    ):\n        self.model = SentenceTransformer(model_name, device=device)\n\n    def embed(\n        self,\n        texts: List[str],\n        normalize: bool = True,\n        show_progress: bool = False\n    ) -> np.ndarray:\n        \"\"\"Embed texts with optional normalization.\"\"\"\n        embeddings = self.model.encode(\n            texts,\n            normalize_embeddings=normalize,\n            show_progress_bar=show_progress,\n            convert_to_numpy=True\n        )\n        return embeddings\n\n    def embed_query(self, query: str) -> np.ndarray:\n        \"\"\"Embed a query with BGE-style prefix.\"\"\"\n        # BGE models benefit from query prefix\n        if \"bge\" in self.model.get_sentence_embedding_dimension():\n            query = f\"Represent this sentence for searching relevant passages: {query}\"\n        return self.embed([query])[0]\n\n    def embed_documents(self, documents: List[str]) -> np.ndarray:\n        \"\"\"Embed documents for indexing.\"\"\"\n        return self.embed(documents)\n\n\n# E5 model with instructions\nclass E5Embedder:\n    def __init__(self, model_name: str = \"intfloat/multilingual-e5-large\"):\n        self.model = SentenceTransformer(model_name)\n\n    def embed_query(self, query: str) -> np.ndarray:\n        return self.model.encode(f\"query: {query}\")\n\n    def embed_document(self, document: str) -> np.ndarray:\n        return self.model.encode(f\"passage: {document}\")\n```\n\n### Template 3: Chunking Strategies\n\n```python\nfrom typing import List, Tuple\nimport re\n\ndef chunk_by_tokens(\n    text: str,\n    chunk_size: int = 512,\n    chunk_overlap: int = 50,\n    tokenizer=None\n) -> List[str]:\n    \"\"\"Chunk text by token count.\"\"\"\n    import tiktoken\n    tokenizer = tokenizer or tiktoken.get_encoding(\"cl100k_base\")\n\n    tokens = tokenizer.encode(text)\n    chunks = []\n\n    start = 0\n    while start < len(tokens):\n        end = start + chunk_size\n        chunk_tokens = tokens[start:end]\n        chunk_text = tokenizer.decode(chunk_tokens)\n        chunks.append(chunk_text)\n        start = end - chunk_overlap\n\n    return chunks\n\n\ndef chunk_by_sentences(\n    text: str,\n    max_chunk_size: int = 1000,\n    min_chunk_size: int = 100\n) -> List[str]:\n    \"\"\"Chunk text by sentences, respecting size limits.\"\"\"\n    import nltk\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current_chunk = []\n    current_size = 0\n\n    for sentence in sentences:\n        sentence_size = len(sentence)\n\n        if current_size + sentence_size > max_chunk_size and current_chunk:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = []\n            current_size = 0\n\n        current_chunk.append(sentence)\n        current_size += sentence_size\n\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n\n    return chunks\n\n\ndef chunk_by_semantic_sections(\n    text: str,\n    headers_pattern: str = r'^#{1,3}\\s+.+$'\n) -> List[Tuple[str, str]]:\n    \"\"\"Chunk markdown by headers, preserving hierarchy.\"\"\"\n    lines = text.split('\\n')\n    chunks = []\n    current_header = \"\"\n    current_content = []\n\n    for line in lines:\n        if re.match(headers_pattern, line, re.MULTILINE):\n            if current_content:\n                chunks.append((current_header, '\\n'.join(current_content)))\n            current_header = line\n            current_content = []\n        else:\n            current_content.append(line)\n\n    if current_content:\n        chunks.append((current_header, '\\n'.join(current_content)))\n\n    return chunks\n\n\ndef recursive_character_splitter(\n    text: str,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 200,\n    separators: List[str] = None\n) -> List[str]:\n    \"\"\"LangChain-style recursive splitter.\"\"\"\n    separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n\n    def split_text(text: str, separators: List[str]) -> List[str]:\n        if not text:\n            return []\n\n        separator = separators[0]\n        remaining_separators = separators[1:]\n\n        if separator == \"\":\n            # Character-level split\n            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - chunk_overlap)]\n\n        splits = text.split(separator)\n        chunks = []\n        current_chunk = []\n        current_length = 0\n\n        for split in splits:\n            split_length = len(split) + len(separator)\n\n            if current_length + split_length > chunk_size and current_chunk:\n                chunk_text = separator.join(current_chunk)\n\n                # Recursively split if still too large\n                if len(chunk_text) > chunk_size and remaining_separators:\n                    chunks.extend(split_text(chunk_text, remaining_separators))\n                else:\n                    chunks.append(chunk_text)\n\n                # Start new chunk with overlap\n                overlap_splits = []\n                overlap_length = 0\n                for s in reversed(current_chunk):\n                    if overlap_length + len(s) <= chunk_overlap:\n                        overlap_splits.insert(0, s)\n                        overlap_length += len(s)\n                    else:\n                        break\n                current_chunk = overlap_splits\n                current_length = overlap_length\n\n            current_chunk.append(split)\n            current_length += split_length\n\n        if current_chunk:\n            chunks.append(separator.join(current_chunk))\n\n        return chunks\n\n    return split_text(text, separators)\n```\n\n### Template 4: Domain-Specific Embedding Pipeline\n\n```python\nclass DomainEmbeddingPipeline:\n    \"\"\"Pipeline for domain-specific embeddings.\"\"\"\n\n    def __init__(\n        self,\n        embedding_model: str = \"text-embedding-3-small\",\n        chunk_size: int = 512,\n        chunk_overlap: int = 50,\n        preprocessing_fn=None\n    ):\n        self.embedding_model = embedding_model\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.preprocess = preprocessing_fn or self._default_preprocess\n\n    def _default_preprocess(self, text: str) -> str:\n        \"\"\"Default preprocessing.\"\"\"\n        # Remove excessive whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        # Remove special characters\n        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n        return text.strip()\n\n    async def process_documents(\n        self,\n        documents: List[dict],\n        id_field: str = \"id\",\n        content_field: str = \"content\",\n        metadata_fields: List[str] = None\n    ) -> List[dict]:\n        \"\"\"Process documents for vector storage.\"\"\"\n        processed = []\n\n        for doc in documents:\n            content = doc[content_field]\n            doc_id = doc[id_field]\n\n            # Preprocess\n            cleaned = self.preprocess(content)\n\n            # Chunk\n            chunks = chunk_by_tokens(\n                cleaned,\n                self.chunk_size,\n                self.chunk_overlap\n            )\n\n            # Create embeddings\n            embeddings = get_embeddings(chunks, self.embedding_model)\n\n            # Create records\n            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n                record = {\n                    \"id\": f\"{doc_id}_chunk_{i}\",\n                    \"document_id\": doc_id,\n                    \"chunk_index\": i,\n                    \"text\": chunk,\n                    \"embedding\": embedding\n                }\n\n                # Add metadata\n                if metadata_fields:\n                    for field in metadata_fields:\n                        if field in doc:\n                            record[field] = doc[field]\n\n                processed.append(record)\n\n        return processed\n\n\n# Code-specific pipeline\nclass CodeEmbeddingPipeline:\n    \"\"\"Specialized pipeline for code embeddings.\"\"\"\n\n    def __init__(self, model: str = \"voyage-code-2\"):\n        self.model = model\n\n    def chunk_code(self, code: str, language: str) -> List[dict]:\n        \"\"\"Chunk code by functions/classes.\"\"\"\n        import tree_sitter\n\n        # Parse with tree-sitter\n        # Extract functions, classes, methods\n        # Return chunks with context\n        pass\n\n    def embed_with_context(self, chunk: str, context: str) -> List[float]:\n        \"\"\"Embed code with surrounding context.\"\"\"\n        combined = f\"Context: {context}\\n\\nCode:\\n{chunk}\"\n        return get_embedding(combined, model=self.model)\n```\n\n### Template 5: Embedding Quality Evaluation\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\n\ndef evaluate_retrieval_quality(\n    queries: List[str],\n    relevant_docs: List[List[str]],  # List of relevant doc IDs per query\n    retrieved_docs: List[List[str]],  # List of retrieved doc IDs per query\n    k: int = 10\n) -> dict:\n    \"\"\"Evaluate embedding quality for retrieval.\"\"\"\n\n    def precision_at_k(relevant: set, retrieved: List[str], k: int) -> float:\n        retrieved_k = retrieved[:k]\n        relevant_retrieved = len(set(retrieved_k) & relevant)\n        return relevant_retrieved / k\n\n    def recall_at_k(relevant: set, retrieved: List[str], k: int) -> float:\n        retrieved_k = retrieved[:k]\n        relevant_retrieved = len(set(retrieved_k) & relevant)\n        return relevant_retrieved / len(relevant) if relevant else 0\n\n    def mrr(relevant: set, retrieved: List[str]) -> float:\n        for i, doc in enumerate(retrieved):\n            if doc in relevant:\n                return 1 / (i + 1)\n        return 0\n\n    def ndcg_at_k(relevant: set, retrieved: List[str], k: int) -> float:\n        dcg = sum(\n            1 / np.log2(i + 2) if doc in relevant else 0\n            for i, doc in enumerate(retrieved[:k])\n        )\n        ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n        return dcg / ideal_dcg if ideal_dcg > 0 else 0\n\n    metrics = {\n        f\"precision@{k}\": [],\n        f\"recall@{k}\": [],\n        \"mrr\": [],\n        f\"ndcg@{k}\": []\n    }\n\n    for relevant, retrieved in zip(relevant_docs, retrieved_docs):\n        relevant_set = set(relevant)\n        metrics[f\"precision@{k}\"].append(precision_at_k(relevant_set, retrieved, k))\n        metrics[f\"recall@{k}\"].append(recall_at_k(relevant_set, retrieved, k))\n        metrics[\"mrr\"].append(mrr(relevant_set, retrieved))\n        metrics[f\"ndcg@{k}\"].append(ndcg_at_k(relevant_set, retrieved, k))\n\n    return {name: np.mean(values) for name, values in metrics.items()}\n\n\ndef compute_embedding_similarity(\n    embeddings1: np.ndarray,\n    embeddings2: np.ndarray,\n    metric: str = \"cosine\"\n) -> np.ndarray:\n    \"\"\"Compute similarity matrix between embedding sets.\"\"\"\n    if metric == \"cosine\":\n        # Normalize\n        norm1 = embeddings1 / np.linalg.norm(embeddings1, axis=1, keepdims=True)\n        norm2 = embeddings2 / np.linalg.norm(embeddings2, axis=1, keepdims=True)\n        return norm1 @ norm2.T\n    elif metric == \"euclidean\":\n        from scipy.spatial.distance import cdist\n        return -cdist(embeddings1, embeddings2, metric='euclidean')\n    elif metric == \"dot\":\n        return embeddings1 @ embeddings2.T\n```\n\n## Best Practices\n\n### Do's\n- **Match model to use case** - Code vs prose vs multilingual\n- **Chunk thoughtfully** - Preserve semantic boundaries\n- **Normalize embeddings** - For cosine similarity\n- **Batch requests** - More efficient than one-by-one\n- **Cache embeddings** - Avoid recomputing\n\n### Don'ts\n- **Don't ignore token limits** - Truncation loses info\n- **Don't mix embedding models** - Incompatible spaces\n- **Don't skip preprocessing** - Garbage in, garbage out\n- **Don't over-chunk** - Lose context\n\n## Resources\n\n- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n- [Sentence Transformers](https://www.sbert.net/)\n- [MTEB Benchmark](https://huggingface.co/spaces/mteb/leaderboard)"
              },
              {
                "name": "hybrid-search-implementation",
                "description": "Combine vector and keyword search for improved retrieval. Use when implementing RAG systems, building search engines, or when neither approach alone provides sufficient recall.",
                "path": "plugins/llm-application-dev/skills/hybrid-search-implementation/SKILL.md",
                "frontmatter": {
                  "name": "hybrid-search-implementation",
                  "description": "Combine vector and keyword search for improved retrieval. Use when implementing RAG systems, building search engines, or when neither approach alone provides sufficient recall."
                },
                "content": "# Hybrid Search Implementation\n\nPatterns for combining vector similarity and keyword-based search.\n\n## When to Use This Skill\n\n- Building RAG systems with improved recall\n- Combining semantic understanding with exact matching\n- Handling queries with specific terms (names, codes)\n- Improving search for domain-specific vocabulary\n- When pure vector search misses keyword matches\n\n## Core Concepts\n\n### 1. Hybrid Search Architecture\n\n```\nQuery â†’ â”¬â”€â–º Vector Search â”€â”€â–º Candidates â”€â”\n        â”‚                                  â”‚\n        â””â”€â–º Keyword Search â”€â–º Candidates â”€â”´â”€â–º Fusion â”€â–º Results\n```\n\n### 2. Fusion Methods\n\n| Method | Description | Best For |\n|--------|-------------|----------|\n| **RRF** | Reciprocal Rank Fusion | General purpose |\n| **Linear** | Weighted sum of scores | Tunable balance |\n| **Cross-encoder** | Rerank with neural model | Highest quality |\n| **Cascade** | Filter then rerank | Efficiency |\n\n## Templates\n\n### Template 1: Reciprocal Rank Fusion\n\n```python\nfrom typing import List, Dict, Tuple\nfrom collections import defaultdict\n\ndef reciprocal_rank_fusion(\n    result_lists: List[List[Tuple[str, float]]],\n    k: int = 60,\n    weights: List[float] = None\n) -> List[Tuple[str, float]]:\n    \"\"\"\n    Combine multiple ranked lists using RRF.\n\n    Args:\n        result_lists: List of (doc_id, score) tuples per search method\n        k: RRF constant (higher = more weight to lower ranks)\n        weights: Optional weights per result list\n\n    Returns:\n        Fused ranking as (doc_id, score) tuples\n    \"\"\"\n    if weights is None:\n        weights = [1.0] * len(result_lists)\n\n    scores = defaultdict(float)\n\n    for result_list, weight in zip(result_lists, weights):\n        for rank, (doc_id, _) in enumerate(result_list):\n            # RRF formula: 1 / (k + rank)\n            scores[doc_id] += weight * (1.0 / (k + rank + 1))\n\n    # Sort by fused score\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n\ndef linear_combination(\n    vector_results: List[Tuple[str, float]],\n    keyword_results: List[Tuple[str, float]],\n    alpha: float = 0.5\n) -> List[Tuple[str, float]]:\n    \"\"\"\n    Combine results with linear interpolation.\n\n    Args:\n        vector_results: (doc_id, similarity_score) from vector search\n        keyword_results: (doc_id, bm25_score) from keyword search\n        alpha: Weight for vector search (1-alpha for keyword)\n    \"\"\"\n    # Normalize scores to [0, 1]\n    def normalize(results):\n        if not results:\n            return {}\n        scores = [s for _, s in results]\n        min_s, max_s = min(scores), max(scores)\n        range_s = max_s - min_s if max_s != min_s else 1\n        return {doc_id: (score - min_s) / range_s for doc_id, score in results}\n\n    vector_scores = normalize(vector_results)\n    keyword_scores = normalize(keyword_results)\n\n    # Combine\n    all_docs = set(vector_scores.keys()) | set(keyword_scores.keys())\n    combined = {}\n\n    for doc_id in all_docs:\n        v_score = vector_scores.get(doc_id, 0)\n        k_score = keyword_scores.get(doc_id, 0)\n        combined[doc_id] = alpha * v_score + (1 - alpha) * k_score\n\n    return sorted(combined.items(), key=lambda x: x[1], reverse=True)\n```\n\n### Template 2: PostgreSQL Hybrid Search\n\n```python\nimport asyncpg\nfrom typing import List, Dict, Optional\nimport numpy as np\n\nclass PostgresHybridSearch:\n    \"\"\"Hybrid search with pgvector and full-text search.\"\"\"\n\n    def __init__(self, pool: asyncpg.Pool):\n        self.pool = pool\n\n    async def setup_schema(self):\n        \"\"\"Create tables and indexes.\"\"\"\n        async with self.pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                CREATE EXTENSION IF NOT EXISTS vector;\n\n                CREATE TABLE IF NOT EXISTS documents (\n                    id TEXT PRIMARY KEY,\n                    content TEXT NOT NULL,\n                    embedding vector(1536),\n                    metadata JSONB DEFAULT '{}',\n                    ts_content tsvector GENERATED ALWAYS AS (\n                        to_tsvector('english', content)\n                    ) STORED\n                );\n\n                -- Vector index (HNSW)\n                CREATE INDEX IF NOT EXISTS documents_embedding_idx\n                ON documents USING hnsw (embedding vector_cosine_ops);\n\n                -- Full-text index (GIN)\n                CREATE INDEX IF NOT EXISTS documents_fts_idx\n                ON documents USING gin (ts_content);\n            \"\"\")\n\n    async def hybrid_search(\n        self,\n        query: str,\n        query_embedding: List[float],\n        limit: int = 10,\n        vector_weight: float = 0.5,\n        filter_metadata: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Perform hybrid search combining vector and full-text.\n\n        Uses RRF fusion for combining results.\n        \"\"\"\n        async with self.pool.acquire() as conn:\n            # Build filter clause\n            where_clause = \"1=1\"\n            params = [query_embedding, query, limit * 3]\n\n            if filter_metadata:\n                for key, value in filter_metadata.items():\n                    params.append(value)\n                    where_clause += f\" AND metadata->>'{key}' = ${len(params)}\"\n\n            results = await conn.fetch(f\"\"\"\n                WITH vector_search AS (\n                    SELECT\n                        id,\n                        content,\n                        metadata,\n                        ROW_NUMBER() OVER (ORDER BY embedding <=> $1::vector) as vector_rank,\n                        1 - (embedding <=> $1::vector) as vector_score\n                    FROM documents\n                    WHERE {where_clause}\n                    ORDER BY embedding <=> $1::vector\n                    LIMIT $3\n                ),\n                keyword_search AS (\n                    SELECT\n                        id,\n                        content,\n                        metadata,\n                        ROW_NUMBER() OVER (ORDER BY ts_rank(ts_content, websearch_to_tsquery('english', $2)) DESC) as keyword_rank,\n                        ts_rank(ts_content, websearch_to_tsquery('english', $2)) as keyword_score\n                    FROM documents\n                    WHERE ts_content @@ websearch_to_tsquery('english', $2)\n                      AND {where_clause}\n                    ORDER BY ts_rank(ts_content, websearch_to_tsquery('english', $2)) DESC\n                    LIMIT $3\n                )\n                SELECT\n                    COALESCE(v.id, k.id) as id,\n                    COALESCE(v.content, k.content) as content,\n                    COALESCE(v.metadata, k.metadata) as metadata,\n                    v.vector_score,\n                    k.keyword_score,\n                    -- RRF fusion\n                    COALESCE(1.0 / (60 + v.vector_rank), 0) * $4::float +\n                    COALESCE(1.0 / (60 + k.keyword_rank), 0) * (1 - $4::float) as rrf_score\n                FROM vector_search v\n                FULL OUTER JOIN keyword_search k ON v.id = k.id\n                ORDER BY rrf_score DESC\n                LIMIT $3 / 3\n            \"\"\", *params, vector_weight)\n\n            return [dict(row) for row in results]\n\n    async def search_with_rerank(\n        self,\n        query: str,\n        query_embedding: List[float],\n        limit: int = 10,\n        rerank_candidates: int = 50\n    ) -> List[Dict]:\n        \"\"\"Hybrid search with cross-encoder reranking.\"\"\"\n        from sentence_transformers import CrossEncoder\n\n        # Get candidates\n        candidates = await self.hybrid_search(\n            query, query_embedding, limit=rerank_candidates\n        )\n\n        if not candidates:\n            return []\n\n        # Rerank with cross-encoder\n        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n        pairs = [(query, c[\"content\"]) for c in candidates]\n        scores = model.predict(pairs)\n\n        for candidate, score in zip(candidates, scores):\n            candidate[\"rerank_score\"] = float(score)\n\n        # Sort by rerank score and return top results\n        reranked = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n        return reranked[:limit]\n```\n\n### Template 3: Elasticsearch Hybrid Search\n\n```python\nfrom elasticsearch import Elasticsearch\nfrom typing import List, Dict, Optional\n\nclass ElasticsearchHybridSearch:\n    \"\"\"Hybrid search with Elasticsearch and dense vectors.\"\"\"\n\n    def __init__(\n        self,\n        es_client: Elasticsearch,\n        index_name: str = \"documents\"\n    ):\n        self.es = es_client\n        self.index_name = index_name\n\n    def create_index(self, vector_dims: int = 1536):\n        \"\"\"Create index with dense vector and text fields.\"\"\"\n        mapping = {\n            \"mappings\": {\n                \"properties\": {\n                    \"content\": {\n                        \"type\": \"text\",\n                        \"analyzer\": \"english\"\n                    },\n                    \"embedding\": {\n                        \"type\": \"dense_vector\",\n                        \"dims\": vector_dims,\n                        \"index\": True,\n                        \"similarity\": \"cosine\"\n                    },\n                    \"metadata\": {\n                        \"type\": \"object\",\n                        \"enabled\": True\n                    }\n                }\n            }\n        }\n        self.es.indices.create(index=self.index_name, body=mapping, ignore=400)\n\n    def hybrid_search(\n        self,\n        query: str,\n        query_embedding: List[float],\n        limit: int = 10,\n        boost_vector: float = 1.0,\n        boost_text: float = 1.0,\n        filter: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Hybrid search using Elasticsearch's built-in capabilities.\n        \"\"\"\n        # Build the hybrid query\n        search_body = {\n            \"size\": limit,\n            \"query\": {\n                \"bool\": {\n                    \"should\": [\n                        # Vector search (kNN)\n                        {\n                            \"script_score\": {\n                                \"query\": {\"match_all\": {}},\n                                \"script\": {\n                                    \"source\": f\"cosineSimilarity(params.query_vector, 'embedding') * {boost_vector} + 1.0\",\n                                    \"params\": {\"query_vector\": query_embedding}\n                                }\n                            }\n                        },\n                        # Text search (BM25)\n                        {\n                            \"match\": {\n                                \"content\": {\n                                    \"query\": query,\n                                    \"boost\": boost_text\n                                }\n                            }\n                        }\n                    ],\n                    \"minimum_should_match\": 1\n                }\n            }\n        }\n\n        # Add filter if provided\n        if filter:\n            search_body[\"query\"][\"bool\"][\"filter\"] = filter\n\n        response = self.es.search(index=self.index_name, body=search_body)\n\n        return [\n            {\n                \"id\": hit[\"_id\"],\n                \"content\": hit[\"_source\"][\"content\"],\n                \"metadata\": hit[\"_source\"].get(\"metadata\", {}),\n                \"score\": hit[\"_score\"]\n            }\n            for hit in response[\"hits\"][\"hits\"]\n        ]\n\n    def hybrid_search_rrf(\n        self,\n        query: str,\n        query_embedding: List[float],\n        limit: int = 10,\n        window_size: int = 100\n    ) -> List[Dict]:\n        \"\"\"\n        Hybrid search using Elasticsearch 8.x RRF.\n        \"\"\"\n        search_body = {\n            \"size\": limit,\n            \"sub_searches\": [\n                {\n                    \"query\": {\n                        \"match\": {\n                            \"content\": query\n                        }\n                    }\n                },\n                {\n                    \"query\": {\n                        \"knn\": {\n                            \"field\": \"embedding\",\n                            \"query_vector\": query_embedding,\n                            \"k\": window_size,\n                            \"num_candidates\": window_size * 2\n                        }\n                    }\n                }\n            ],\n            \"rank\": {\n                \"rrf\": {\n                    \"window_size\": window_size,\n                    \"rank_constant\": 60\n                }\n            }\n        }\n\n        response = self.es.search(index=self.index_name, body=search_body)\n\n        return [\n            {\n                \"id\": hit[\"_id\"],\n                \"content\": hit[\"_source\"][\"content\"],\n                \"score\": hit[\"_score\"]\n            }\n            for hit in response[\"hits\"][\"hits\"]\n        ]\n```\n\n### Template 4: Custom Hybrid RAG Pipeline\n\n```python\nfrom typing import List, Dict, Optional, Callable\nfrom dataclasses import dataclass\n\n@dataclass\nclass SearchResult:\n    id: str\n    content: str\n    score: float\n    source: str  # \"vector\", \"keyword\", \"hybrid\"\n    metadata: Dict = None\n\n\nclass HybridRAGPipeline:\n    \"\"\"Complete hybrid search pipeline for RAG.\"\"\"\n\n    def __init__(\n        self,\n        vector_store,\n        keyword_store,\n        embedder,\n        reranker=None,\n        fusion_method: str = \"rrf\",\n        vector_weight: float = 0.5\n    ):\n        self.vector_store = vector_store\n        self.keyword_store = keyword_store\n        self.embedder = embedder\n        self.reranker = reranker\n        self.fusion_method = fusion_method\n        self.vector_weight = vector_weight\n\n    async def search(\n        self,\n        query: str,\n        top_k: int = 10,\n        filter: Optional[Dict] = None,\n        use_rerank: bool = True\n    ) -> List[SearchResult]:\n        \"\"\"Execute hybrid search pipeline.\"\"\"\n\n        # Step 1: Get query embedding\n        query_embedding = self.embedder.embed(query)\n\n        # Step 2: Execute parallel searches\n        vector_results, keyword_results = await asyncio.gather(\n            self._vector_search(query_embedding, top_k * 3, filter),\n            self._keyword_search(query, top_k * 3, filter)\n        )\n\n        # Step 3: Fuse results\n        if self.fusion_method == \"rrf\":\n            fused = self._rrf_fusion(vector_results, keyword_results)\n        else:\n            fused = self._linear_fusion(vector_results, keyword_results)\n\n        # Step 4: Rerank if enabled\n        if use_rerank and self.reranker:\n            fused = await self._rerank(query, fused[:top_k * 2])\n\n        return fused[:top_k]\n\n    async def _vector_search(\n        self,\n        embedding: List[float],\n        limit: int,\n        filter: Dict\n    ) -> List[SearchResult]:\n        results = await self.vector_store.search(embedding, limit, filter)\n        return [\n            SearchResult(\n                id=r[\"id\"],\n                content=r[\"content\"],\n                score=r[\"score\"],\n                source=\"vector\",\n                metadata=r.get(\"metadata\")\n            )\n            for r in results\n        ]\n\n    async def _keyword_search(\n        self,\n        query: str,\n        limit: int,\n        filter: Dict\n    ) -> List[SearchResult]:\n        results = await self.keyword_store.search(query, limit, filter)\n        return [\n            SearchResult(\n                id=r[\"id\"],\n                content=r[\"content\"],\n                score=r[\"score\"],\n                source=\"keyword\",\n                metadata=r.get(\"metadata\")\n            )\n            for r in results\n        ]\n\n    def _rrf_fusion(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult]\n    ) -> List[SearchResult]:\n        \"\"\"Fuse with RRF.\"\"\"\n        k = 60\n        scores = {}\n        content_map = {}\n\n        for rank, result in enumerate(vector_results):\n            scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank + 1)\n            content_map[result.id] = result\n\n        for rank, result in enumerate(keyword_results):\n            scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank + 1)\n            if result.id not in content_map:\n                content_map[result.id] = result\n\n        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n\n        return [\n            SearchResult(\n                id=doc_id,\n                content=content_map[doc_id].content,\n                score=scores[doc_id],\n                source=\"hybrid\",\n                metadata=content_map[doc_id].metadata\n            )\n            for doc_id in sorted_ids\n        ]\n\n    async def _rerank(\n        self,\n        query: str,\n        results: List[SearchResult]\n    ) -> List[SearchResult]:\n        \"\"\"Rerank with cross-encoder.\"\"\"\n        if not results:\n            return results\n\n        pairs = [(query, r.content) for r in results]\n        scores = self.reranker.predict(pairs)\n\n        for result, score in zip(results, scores):\n            result.score = float(score)\n\n        return sorted(results, key=lambda x: x.score, reverse=True)\n```\n\n## Best Practices\n\n### Do's\n- **Tune weights empirically** - Test on your data\n- **Use RRF for simplicity** - Works well without tuning\n- **Add reranking** - Significant quality improvement\n- **Log both scores** - Helps with debugging\n- **A/B test** - Measure real user impact\n\n### Don'ts\n- **Don't assume one size fits all** - Different queries need different weights\n- **Don't skip keyword search** - Handles exact matches better\n- **Don't over-fetch** - Balance recall vs latency\n- **Don't ignore edge cases** - Empty results, single word queries\n\n## Resources\n\n- [RRF Paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)\n- [Vespa Hybrid Search](https://blog.vespa.ai/improving-text-ranking-with-few-shot-prompting/)\n- [Cohere Rerank](https://docs.cohere.com/docs/reranking)"
              },
              {
                "name": "langchain-architecture",
                "description": "Design LLM applications using the LangChain framework with agents, memory, and tool integration patterns. Use when building LangChain applications, implementing AI agents, or creating complex LLM workflows.",
                "path": "plugins/llm-application-dev/skills/langchain-architecture/SKILL.md",
                "frontmatter": {
                  "name": "langchain-architecture",
                  "description": "Design LLM applications using the LangChain framework with agents, memory, and tool integration patterns. Use when building LangChain applications, implementing AI agents, or creating complex LLM workflows."
                },
                "content": "# LangChain Architecture\n\nMaster the LangChain framework for building sophisticated LLM applications with agents, chains, memory, and tool integration.\n\n## When to Use This Skill\n\n- Building autonomous AI agents with tool access\n- Implementing complex multi-step LLM workflows\n- Managing conversation memory and state\n- Integrating LLMs with external data sources and APIs\n- Creating modular, reusable LLM application components\n- Implementing document processing pipelines\n- Building production-grade LLM applications\n\n## Core Concepts\n\n### 1. Agents\nAutonomous systems that use LLMs to decide which actions to take.\n\n**Agent Types:**\n- **ReAct**: Reasoning + Acting in interleaved manner\n- **OpenAI Functions**: Leverages function calling API\n- **Structured Chat**: Handles multi-input tools\n- **Conversational**: Optimized for chat interfaces\n- **Self-Ask with Search**: Decomposes complex queries\n\n### 2. Chains\nSequences of calls to LLMs or other utilities.\n\n**Chain Types:**\n- **LLMChain**: Basic prompt + LLM combination\n- **SequentialChain**: Multiple chains in sequence\n- **RouterChain**: Routes inputs to specialized chains\n- **TransformChain**: Data transformations between steps\n- **MapReduceChain**: Parallel processing with aggregation\n\n### 3. Memory\nSystems for maintaining context across interactions.\n\n**Memory Types:**\n- **ConversationBufferMemory**: Stores all messages\n- **ConversationSummaryMemory**: Summarizes older messages\n- **ConversationBufferWindowMemory**: Keeps last N messages\n- **EntityMemory**: Tracks information about entities\n- **VectorStoreMemory**: Semantic similarity retrieval\n\n### 4. Document Processing\nLoading, transforming, and storing documents for retrieval.\n\n**Components:**\n- **Document Loaders**: Load from various sources\n- **Text Splitters**: Chunk documents intelligently\n- **Vector Stores**: Store and retrieve embeddings\n- **Retrievers**: Fetch relevant documents\n- **Indexes**: Organize documents for efficient access\n\n### 5. Callbacks\nHooks for logging, monitoring, and debugging.\n\n**Use Cases:**\n- Request/response logging\n- Token usage tracking\n- Latency monitoring\n- Error handling\n- Custom metrics collection\n\n## Quick Start\n\n```python\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\n\n# Initialize LLM\nllm = OpenAI(temperature=0)\n\n# Load tools\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Add memory\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\n# Create agent\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n    memory=memory,\n    verbose=True\n)\n\n# Run agent\nresult = agent.run(\"What's the weather in SF? Then calculate 25 * 4\")\n```\n\n## Architecture Patterns\n\n### Pattern 1: RAG with LangChain\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load and process documents\nloader = TextLoader('documents.txt')\ndocuments = loader.load()\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n\n# Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(texts, embeddings)\n\n# Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# Query\nresult = qa_chain({\"query\": \"What is the main topic?\"})\n```\n\n### Pattern 2: Custom Agent with Tools\n```python\nfrom langchain.agents import Tool, AgentExecutor\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str) -> str:\n    \"\"\"Search internal database for information.\"\"\"\n    # Your database search logic\n    return f\"Results for: {query}\"\n\n@tool\ndef send_email(recipient: str, content: str) -> str:\n    \"\"\"Send an email to specified recipient.\"\"\"\n    # Email sending logic\n    return f\"Email sent to {recipient}\"\n\ntools = [search_database, send_email]\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n```\n\n### Pattern 3: Multi-Step Chain\n```python\nfrom langchain.chains import LLMChain, SequentialChain\nfrom langchain.prompts import PromptTemplate\n\n# Step 1: Extract key information\nextract_prompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"Extract key entities from: {text}\\n\\nEntities:\"\n)\nextract_chain = LLMChain(llm=llm, prompt=extract_prompt, output_key=\"entities\")\n\n# Step 2: Analyze entities\nanalyze_prompt = PromptTemplate(\n    input_variables=[\"entities\"],\n    template=\"Analyze these entities: {entities}\\n\\nAnalysis:\"\n)\nanalyze_chain = LLMChain(llm=llm, prompt=analyze_prompt, output_key=\"analysis\")\n\n# Step 3: Generate summary\nsummary_prompt = PromptTemplate(\n    input_variables=[\"entities\", \"analysis\"],\n    template=\"Summarize:\\nEntities: {entities}\\nAnalysis: {analysis}\\n\\nSummary:\"\n)\nsummary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"summary\")\n\n# Combine into sequential chain\noverall_chain = SequentialChain(\n    chains=[extract_chain, analyze_chain, summary_chain],\n    input_variables=[\"text\"],\n    output_variables=[\"entities\", \"analysis\", \"summary\"],\n    verbose=True\n)\n```\n\n## Memory Management Best Practices\n\n### Choosing the Right Memory Type\n```python\n# For short conversations (< 10 messages)\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\n\n# For long conversations (summarize old messages)\nfrom langchain.memory import ConversationSummaryMemory\nmemory = ConversationSummaryMemory(llm=llm)\n\n# For sliding window (last N messages)\nfrom langchain.memory import ConversationBufferWindowMemory\nmemory = ConversationBufferWindowMemory(k=5)\n\n# For entity tracking\nfrom langchain.memory import ConversationEntityMemory\nmemory = ConversationEntityMemory(llm=llm)\n\n# For semantic retrieval of relevant history\nfrom langchain.memory import VectorStoreRetrieverMemory\nmemory = VectorStoreRetrieverMemory(retriever=retriever)\n```\n\n## Callback System\n\n### Custom Callback Handler\n```python\nfrom langchain.callbacks.base import BaseCallbackHandler\n\nclass CustomCallbackHandler(BaseCallbackHandler):\n    def on_llm_start(self, serialized, prompts, **kwargs):\n        print(f\"LLM started with prompts: {prompts}\")\n\n    def on_llm_end(self, response, **kwargs):\n        print(f\"LLM ended with response: {response}\")\n\n    def on_llm_error(self, error, **kwargs):\n        print(f\"LLM error: {error}\")\n\n    def on_chain_start(self, serialized, inputs, **kwargs):\n        print(f\"Chain started with inputs: {inputs}\")\n\n    def on_agent_action(self, action, **kwargs):\n        print(f\"Agent taking action: {action}\")\n\n# Use callback\nagent.run(\"query\", callbacks=[CustomCallbackHandler()])\n```\n\n## Testing Strategies\n\n```python\nimport pytest\nfrom unittest.mock import Mock\n\ndef test_agent_tool_selection():\n    # Mock LLM to return specific tool selection\n    mock_llm = Mock()\n    mock_llm.predict.return_value = \"Action: search_database\\nAction Input: test query\"\n\n    agent = initialize_agent(tools, mock_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n\n    result = agent.run(\"test query\")\n\n    # Verify correct tool was selected\n    assert \"search_database\" in str(mock_llm.predict.call_args)\n\ndef test_memory_persistence():\n    memory = ConversationBufferMemory()\n\n    memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Hello!\"})\n\n    assert \"Hi\" in memory.load_memory_variables({})['history']\n    assert \"Hello!\" in memory.load_memory_variables({})['history']\n```\n\n## Performance Optimization\n\n### 1. Caching\n```python\nfrom langchain.cache import InMemoryCache\nimport langchain\n\nlangchain.llm_cache = InMemoryCache()\n```\n\n### 2. Batch Processing\n```python\n# Process multiple documents in parallel\nfrom langchain.document_loaders import DirectoryLoader\nfrom concurrent.futures import ThreadPoolExecutor\n\nloader = DirectoryLoader('./docs')\ndocs = loader.load()\n\ndef process_doc(doc):\n    return text_splitter.split_documents([doc])\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    split_docs = list(executor.map(process_doc, docs))\n```\n\n### 3. Streaming Responses\n```python\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n```\n\n## Resources\n\n- **references/agents.md**: Deep dive on agent architectures\n- **references/memory.md**: Memory system patterns\n- **references/chains.md**: Chain composition strategies\n- **references/document-processing.md**: Document loading and indexing\n- **references/callbacks.md**: Monitoring and observability\n- **assets/agent-template.py**: Production-ready agent template\n- **assets/memory-config.yaml**: Memory configuration examples\n- **assets/chain-example.py**: Complex chain examples\n\n## Common Pitfalls\n\n1. **Memory Overflow**: Not managing conversation history length\n2. **Tool Selection Errors**: Poor tool descriptions confuse agents\n3. **Context Window Exceeded**: Exceeding LLM token limits\n4. **No Error Handling**: Not catching and handling agent failures\n5. **Inefficient Retrieval**: Not optimizing vector store queries\n\n## Production Checklist\n\n- [ ] Implement proper error handling\n- [ ] Add request/response logging\n- [ ] Monitor token usage and costs\n- [ ] Set timeout limits for agent execution\n- [ ] Implement rate limiting\n- [ ] Add input validation\n- [ ] Test with edge cases\n- [ ] Set up observability (callbacks)\n- [ ] Implement fallback strategies\n- [ ] Version control prompts and configurations"
              },
              {
                "name": "llm-evaluation",
                "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
                "path": "plugins/llm-application-dev/skills/llm-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "llm-evaluation",
                  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks."
                },
                "content": "# LLM Evaluation\n\nMaster comprehensive evaluation strategies for LLM applications, from automated metrics to human evaluation and A/B testing.\n\n## When to Use This Skill\n\n- Measuring LLM application performance systematically\n- Comparing different models or prompts\n- Detecting performance regressions before deployment\n- Validating improvements from prompt changes\n- Building confidence in production systems\n- Establishing baselines and tracking progress over time\n- Debugging unexpected model behavior\n\n## Core Evaluation Types\n\n### 1. Automated Metrics\nFast, repeatable, scalable evaluation using computed scores.\n\n**Text Generation:**\n- **BLEU**: N-gram overlap (translation)\n- **ROUGE**: Recall-oriented (summarization)\n- **METEOR**: Semantic similarity\n- **BERTScore**: Embedding-based similarity\n- **Perplexity**: Language model confidence\n\n**Classification:**\n- **Accuracy**: Percentage correct\n- **Precision/Recall/F1**: Class-specific performance\n- **Confusion Matrix**: Error patterns\n- **AUC-ROC**: Ranking quality\n\n**Retrieval (RAG):**\n- **MRR**: Mean Reciprocal Rank\n- **NDCG**: Normalized Discounted Cumulative Gain\n- **Precision@K**: Relevant in top K\n- **Recall@K**: Coverage in top K\n\n### 2. Human Evaluation\nManual assessment for quality aspects difficult to automate.\n\n**Dimensions:**\n- **Accuracy**: Factual correctness\n- **Coherence**: Logical flow\n- **Relevance**: Answers the question\n- **Fluency**: Natural language quality\n- **Safety**: No harmful content\n- **Helpfulness**: Useful to the user\n\n### 3. LLM-as-Judge\nUse stronger LLMs to evaluate weaker model outputs.\n\n**Approaches:**\n- **Pointwise**: Score individual responses\n- **Pairwise**: Compare two responses\n- **Reference-based**: Compare to gold standard\n- **Reference-free**: Judge without ground truth\n\n## Quick Start\n\n```python\nfrom llm_eval import EvaluationSuite, Metric\n\n# Define evaluation suite\nsuite = EvaluationSuite([\n    Metric.accuracy(),\n    Metric.bleu(),\n    Metric.bertscore(),\n    Metric.custom(name=\"groundedness\", fn=check_groundedness)\n])\n\n# Prepare test cases\ntest_cases = [\n    {\n        \"input\": \"What is the capital of France?\",\n        \"expected\": \"Paris\",\n        \"context\": \"France is a country in Europe. Paris is its capital.\"\n    },\n    # ... more test cases\n]\n\n# Run evaluation\nresults = suite.evaluate(\n    model=your_model,\n    test_cases=test_cases\n)\n\nprint(f\"Overall Accuracy: {results.metrics['accuracy']}\")\nprint(f\"BLEU Score: {results.metrics['bleu']}\")\n```\n\n## Automated Metrics Implementation\n\n### BLEU Score\n```python\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(reference, hypothesis):\n    \"\"\"Calculate BLEU score between reference and hypothesis.\"\"\"\n    smoothie = SmoothingFunction().method4\n\n    return sentence_bleu(\n        [reference.split()],\n        hypothesis.split(),\n        smoothing_function=smoothie\n    )\n\n# Usage\nbleu = calculate_bleu(\n    reference=\"The cat sat on the mat\",\n    hypothesis=\"A cat is sitting on the mat\"\n)\n```\n\n### ROUGE Score\n```python\nfrom rouge_score import rouge_scorer\n\ndef calculate_rouge(reference, hypothesis):\n    \"\"\"Calculate ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, hypothesis)\n\n    return {\n        'rouge1': scores['rouge1'].fmeasure,\n        'rouge2': scores['rouge2'].fmeasure,\n        'rougeL': scores['rougeL'].fmeasure\n    }\n```\n\n### BERTScore\n```python\nfrom bert_score import score\n\ndef calculate_bertscore(references, hypotheses):\n    \"\"\"Calculate BERTScore using pre-trained BERT.\"\"\"\n    P, R, F1 = score(\n        hypotheses,\n        references,\n        lang='en',\n        model_type='microsoft/deberta-xlarge-mnli'\n    )\n\n    return {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n```\n\n### Custom Metrics\n```python\ndef calculate_groundedness(response, context):\n    \"\"\"Check if response is grounded in provided context.\"\"\"\n    # Use NLI model to check entailment\n    from transformers import pipeline\n\n    nli = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n\n    result = nli(f\"{context} [SEP] {response}\")[0]\n\n    # Return confidence that response is entailed by context\n    return result['score'] if result['label'] == 'ENTAILMENT' else 0.0\n\ndef calculate_toxicity(text):\n    \"\"\"Measure toxicity in generated text.\"\"\"\n    from detoxify import Detoxify\n\n    results = Detoxify('original').predict(text)\n    return max(results.values())  # Return highest toxicity score\n\ndef calculate_factuality(claim, knowledge_base):\n    \"\"\"Verify factual claims against knowledge base.\"\"\"\n    # Implementation depends on your knowledge base\n    # Could use retrieval + NLI, or fact-checking API\n    pass\n```\n\n## LLM-as-Judge Patterns\n\n### Single Output Evaluation\n```python\ndef llm_judge_quality(response, question):\n    \"\"\"Use GPT-5 to judge response quality.\"\"\"\n    prompt = f\"\"\"Rate the following response on a scale of 1-10 for:\n1. Accuracy (factually correct)\n2. Helpfulness (answers the question)\n3. Clarity (well-written and understandable)\n\nQuestion: {question}\nResponse: {response}\n\nProvide ratings in JSON format:\n{{\n  \"accuracy\": <1-10>,\n  \"helpfulness\": <1-10>,\n  \"clarity\": <1-10>,\n  \"reasoning\": \"<brief explanation>\"\n}}\n\"\"\"\n\n    result = openai.ChatCompletion.create(\n        model=\"gpt-5\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n\n    return json.loads(result.choices[0].message.content)\n```\n\n### Pairwise Comparison\n```python\ndef compare_responses(question, response_a, response_b):\n    \"\"\"Compare two responses using LLM judge.\"\"\"\n    prompt = f\"\"\"Compare these two responses to the question and determine which is better.\n\nQuestion: {question}\n\nResponse A: {response_a}\n\nResponse B: {response_b}\n\nWhich response is better and why? Consider accuracy, helpfulness, and clarity.\n\nAnswer with JSON:\n{{\n  \"winner\": \"A\" or \"B\" or \"tie\",\n  \"reasoning\": \"<explanation>\",\n  \"confidence\": <1-10>\n}}\n\"\"\"\n\n    result = openai.ChatCompletion.create(\n        model=\"gpt-5\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n\n    return json.loads(result.choices[0].message.content)\n```\n\n## Human Evaluation Frameworks\n\n### Annotation Guidelines\n```python\nclass AnnotationTask:\n    \"\"\"Structure for human annotation task.\"\"\"\n\n    def __init__(self, response, question, context=None):\n        self.response = response\n        self.question = question\n        self.context = context\n\n    def get_annotation_form(self):\n        return {\n            \"question\": self.question,\n            \"context\": self.context,\n            \"response\": self.response,\n            \"ratings\": {\n                \"accuracy\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Is the response factually correct?\"\n                },\n                \"relevance\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Does it answer the question?\"\n                },\n                \"coherence\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Is it logically consistent?\"\n                }\n            },\n            \"issues\": {\n                \"factual_error\": False,\n                \"hallucination\": False,\n                \"off_topic\": False,\n                \"unsafe_content\": False\n            },\n            \"feedback\": \"\"\n        }\n```\n\n### Inter-Rater Agreement\n```python\nfrom sklearn.metrics import cohen_kappa_score\n\ndef calculate_agreement(rater1_scores, rater2_scores):\n    \"\"\"Calculate inter-rater agreement.\"\"\"\n    kappa = cohen_kappa_score(rater1_scores, rater2_scores)\n\n    interpretation = {\n        kappa < 0: \"Poor\",\n        kappa < 0.2: \"Slight\",\n        kappa < 0.4: \"Fair\",\n        kappa < 0.6: \"Moderate\",\n        kappa < 0.8: \"Substantial\",\n        kappa <= 1.0: \"Almost Perfect\"\n    }\n\n    return {\n        \"kappa\": kappa,\n        \"interpretation\": interpretation[True]\n    }\n```\n\n## A/B Testing\n\n### Statistical Testing Framework\n```python\nfrom scipy import stats\nimport numpy as np\n\nclass ABTest:\n    def __init__(self, variant_a_name=\"A\", variant_b_name=\"B\"):\n        self.variant_a = {\"name\": variant_a_name, \"scores\": []}\n        self.variant_b = {\"name\": variant_b_name, \"scores\": []}\n\n    def add_result(self, variant, score):\n        \"\"\"Add evaluation result for a variant.\"\"\"\n        if variant == \"A\":\n            self.variant_a[\"scores\"].append(score)\n        else:\n            self.variant_b[\"scores\"].append(score)\n\n    def analyze(self, alpha=0.05):\n        \"\"\"Perform statistical analysis.\"\"\"\n        a_scores = self.variant_a[\"scores\"]\n        b_scores = self.variant_b[\"scores\"]\n\n        # T-test\n        t_stat, p_value = stats.ttest_ind(a_scores, b_scores)\n\n        # Effect size (Cohen's d)\n        pooled_std = np.sqrt((np.std(a_scores)**2 + np.std(b_scores)**2) / 2)\n        cohens_d = (np.mean(b_scores) - np.mean(a_scores)) / pooled_std\n\n        return {\n            \"variant_a_mean\": np.mean(a_scores),\n            \"variant_b_mean\": np.mean(b_scores),\n            \"difference\": np.mean(b_scores) - np.mean(a_scores),\n            \"relative_improvement\": (np.mean(b_scores) - np.mean(a_scores)) / np.mean(a_scores),\n            \"p_value\": p_value,\n            \"statistically_significant\": p_value < alpha,\n            \"cohens_d\": cohens_d,\n            \"effect_size\": self.interpret_cohens_d(cohens_d),\n            \"winner\": \"B\" if np.mean(b_scores) > np.mean(a_scores) else \"A\"\n        }\n\n    @staticmethod\n    def interpret_cohens_d(d):\n        \"\"\"Interpret Cohen's d effect size.\"\"\"\n        abs_d = abs(d)\n        if abs_d < 0.2:\n            return \"negligible\"\n        elif abs_d < 0.5:\n            return \"small\"\n        elif abs_d < 0.8:\n            return \"medium\"\n        else:\n            return \"large\"\n```\n\n## Regression Testing\n\n### Regression Detection\n```python\nclass RegressionDetector:\n    def __init__(self, baseline_results, threshold=0.05):\n        self.baseline = baseline_results\n        self.threshold = threshold\n\n    def check_for_regression(self, new_results):\n        \"\"\"Detect if new results show regression.\"\"\"\n        regressions = []\n\n        for metric in self.baseline.keys():\n            baseline_score = self.baseline[metric]\n            new_score = new_results.get(metric)\n\n            if new_score is None:\n                continue\n\n            # Calculate relative change\n            relative_change = (new_score - baseline_score) / baseline_score\n\n            # Flag if significant decrease\n            if relative_change < -self.threshold:\n                regressions.append({\n                    \"metric\": metric,\n                    \"baseline\": baseline_score,\n                    \"current\": new_score,\n                    \"change\": relative_change\n                })\n\n        return {\n            \"has_regression\": len(regressions) > 0,\n            \"regressions\": regressions\n        }\n```\n\n## Benchmarking\n\n### Running Benchmarks\n```python\nclass BenchmarkRunner:\n    def __init__(self, benchmark_dataset):\n        self.dataset = benchmark_dataset\n\n    def run_benchmark(self, model, metrics):\n        \"\"\"Run model on benchmark and calculate metrics.\"\"\"\n        results = {metric.name: [] for metric in metrics}\n\n        for example in self.dataset:\n            # Generate prediction\n            prediction = model.predict(example[\"input\"])\n\n            # Calculate each metric\n            for metric in metrics:\n                score = metric.calculate(\n                    prediction=prediction,\n                    reference=example[\"reference\"],\n                    context=example.get(\"context\")\n                )\n                results[metric.name].append(score)\n\n        # Aggregate results\n        return {\n            metric: {\n                \"mean\": np.mean(scores),\n                \"std\": np.std(scores),\n                \"min\": min(scores),\n                \"max\": max(scores)\n            }\n            for metric, scores in results.items()\n        }\n```\n\n## Resources\n\n- **references/metrics.md**: Comprehensive metric guide\n- **references/human-evaluation.md**: Annotation best practices\n- **references/benchmarking.md**: Standard benchmarks\n- **references/a-b-testing.md**: Statistical testing guide\n- **references/regression-testing.md**: CI/CD integration\n- **assets/evaluation-framework.py**: Complete evaluation harness\n- **assets/benchmark-dataset.jsonl**: Example datasets\n- **scripts/evaluate-model.py**: Automated evaluation runner\n\n## Best Practices\n\n1. **Multiple Metrics**: Use diverse metrics for comprehensive view\n2. **Representative Data**: Test on real-world, diverse examples\n3. **Baselines**: Always compare against baseline performance\n4. **Statistical Rigor**: Use proper statistical tests for comparisons\n5. **Continuous Evaluation**: Integrate into CI/CD pipeline\n6. **Human Validation**: Combine automated metrics with human judgment\n7. **Error Analysis**: Investigate failures to understand weaknesses\n8. **Version Control**: Track evaluation results over time\n\n## Common Pitfalls\n\n- **Single Metric Obsession**: Optimizing for one metric at the expense of others\n- **Small Sample Size**: Drawing conclusions from too few examples\n- **Data Contamination**: Testing on training data\n- **Ignoring Variance**: Not accounting for statistical uncertainty\n- **Metric Mismatch**: Using metrics not aligned with business goals"
              },
              {
                "name": "prompt-engineering-patterns",
                "description": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability in production. Use when optimizing prompts, improving LLM outputs, or designing production prompt templates.",
                "path": "plugins/llm-application-dev/skills/prompt-engineering-patterns/SKILL.md",
                "frontmatter": {
                  "name": "prompt-engineering-patterns",
                  "description": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability in production. Use when optimizing prompts, improving LLM outputs, or designing production prompt templates."
                },
                "content": "# Prompt Engineering Patterns\n\nMaster advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability.\n\n## When to Use This Skill\n\n- Designing complex prompts for production LLM applications\n- Optimizing prompt performance and consistency\n- Implementing structured reasoning patterns (chain-of-thought, tree-of-thought)\n- Building few-shot learning systems with dynamic example selection\n- Creating reusable prompt templates with variable interpolation\n- Debugging and refining prompts that produce inconsistent outputs\n- Implementing system prompts for specialized AI assistants\n\n## Core Capabilities\n\n### 1. Few-Shot Learning\n- Example selection strategies (semantic similarity, diversity sampling)\n- Balancing example count with context window constraints\n- Constructing effective demonstrations with input-output pairs\n- Dynamic example retrieval from knowledge bases\n- Handling edge cases through strategic example selection\n\n### 2. Chain-of-Thought Prompting\n- Step-by-step reasoning elicitation\n- Zero-shot CoT with \"Let's think step by step\"\n- Few-shot CoT with reasoning traces\n- Self-consistency techniques (sampling multiple reasoning paths)\n- Verification and validation steps\n\n### 3. Prompt Optimization\n- Iterative refinement workflows\n- A/B testing prompt variations\n- Measuring prompt performance metrics (accuracy, consistency, latency)\n- Reducing token usage while maintaining quality\n- Handling edge cases and failure modes\n\n### 4. Template Systems\n- Variable interpolation and formatting\n- Conditional prompt sections\n- Multi-turn conversation templates\n- Role-based prompt composition\n- Modular prompt components\n\n### 5. System Prompt Design\n- Setting model behavior and constraints\n- Defining output formats and structure\n- Establishing role and expertise\n- Safety guidelines and content policies\n- Context setting and background information\n\n## Quick Start\n\n```python\nfrom prompt_optimizer import PromptTemplate, FewShotSelector\n\n# Define a structured prompt template\ntemplate = PromptTemplate(\n    system=\"You are an expert SQL developer. Generate efficient, secure SQL queries.\",\n    instruction=\"Convert the following natural language query to SQL:\\n{query}\",\n    few_shot_examples=True,\n    output_format=\"SQL code block with explanatory comments\"\n)\n\n# Configure few-shot learning\nselector = FewShotSelector(\n    examples_db=\"sql_examples.jsonl\",\n    selection_strategy=\"semantic_similarity\",\n    max_examples=3\n)\n\n# Generate optimized prompt\nprompt = template.render(\n    query=\"Find all users who registered in the last 30 days\",\n    examples=selector.select(query=\"user registration date filter\")\n)\n```\n\n## Key Patterns\n\n### Progressive Disclosure\nStart with simple prompts, add complexity only when needed:\n\n1. **Level 1**: Direct instruction\n   - \"Summarize this article\"\n\n2. **Level 2**: Add constraints\n   - \"Summarize this article in 3 bullet points, focusing on key findings\"\n\n3. **Level 3**: Add reasoning\n   - \"Read this article, identify the main findings, then summarize in 3 bullet points\"\n\n4. **Level 4**: Add examples\n   - Include 2-3 example summaries with input-output pairs\n\n### Instruction Hierarchy\n```\n[System Context] â†’ [Task Instruction] â†’ [Examples] â†’ [Input Data] â†’ [Output Format]\n```\n\n### Error Recovery\nBuild prompts that gracefully handle failures:\n- Include fallback instructions\n- Request confidence scores\n- Ask for alternative interpretations when uncertain\n- Specify how to indicate missing information\n\n## Best Practices\n\n1. **Be Specific**: Vague prompts produce inconsistent results\n2. **Show, Don't Tell**: Examples are more effective than descriptions\n3. **Test Extensively**: Evaluate on diverse, representative inputs\n4. **Iterate Rapidly**: Small changes can have large impacts\n5. **Monitor Performance**: Track metrics in production\n6. **Version Control**: Treat prompts as code with proper versioning\n7. **Document Intent**: Explain why prompts are structured as they are\n\n## Common Pitfalls\n\n- **Over-engineering**: Starting with complex prompts before trying simple ones\n- **Example pollution**: Using examples that don't match the target task\n- **Context overflow**: Exceeding token limits with excessive examples\n- **Ambiguous instructions**: Leaving room for multiple interpretations\n- **Ignoring edge cases**: Not testing on unusual or boundary inputs\n\n## Integration Patterns\n\n### With RAG Systems\n```python\n# Combine retrieved context with prompt engineering\nprompt = f\"\"\"Given the following context:\n{retrieved_context}\n\n{few_shot_examples}\n\nQuestion: {user_question}\n\nProvide a detailed answer based solely on the context above. If the context doesn't contain enough information, explicitly state what's missing.\"\"\"\n```\n\n### With Validation\n```python\n# Add self-verification step\nprompt = f\"\"\"{main_task_prompt}\n\nAfter generating your response, verify it meets these criteria:\n1. Answers the question directly\n2. Uses only information from provided context\n3. Cites specific sources\n4. Acknowledges any uncertainty\n\nIf verification fails, revise your response.\"\"\"\n```\n\n## Performance Optimization\n\n### Token Efficiency\n- Remove redundant words and phrases\n- Use abbreviations consistently after first definition\n- Consolidate similar instructions\n- Move stable content to system prompts\n\n### Latency Reduction\n- Minimize prompt length without sacrificing quality\n- Use streaming for long-form outputs\n- Cache common prompt prefixes\n- Batch similar requests when possible\n\n## Resources\n\n- **references/few-shot-learning.md**: Deep dive on example selection and construction\n- **references/chain-of-thought.md**: Advanced reasoning elicitation techniques\n- **references/prompt-optimization.md**: Systematic refinement workflows\n- **references/prompt-templates.md**: Reusable template patterns\n- **references/system-prompts.md**: System-level prompt design\n- **assets/prompt-template-library.md**: Battle-tested prompt templates\n- **assets/few-shot-examples.json**: Curated example datasets\n- **scripts/optimize-prompt.py**: Automated prompt optimization tool\n\n## Success Metrics\n\nTrack these KPIs for your prompts:\n- **Accuracy**: Correctness of outputs\n- **Consistency**: Reproducibility across similar inputs\n- **Latency**: Response time (P50, P95, P99)\n- **Token Usage**: Average tokens per request\n- **Success Rate**: Percentage of valid outputs\n- **User Satisfaction**: Ratings and feedback\n\n## Next Steps\n\n1. Review the prompt template library for common patterns\n2. Experiment with few-shot learning for your specific use case\n3. Implement prompt versioning and A/B testing\n4. Set up automated evaluation pipelines\n5. Document your prompt engineering decisions and learnings"
              },
              {
                "name": "rag-implementation",
                "description": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-grounded AI, building document Q&A systems, or integrating LLMs with external knowledge bases.",
                "path": "plugins/llm-application-dev/skills/rag-implementation/SKILL.md",
                "frontmatter": {
                  "name": "rag-implementation",
                  "description": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-grounded AI, building document Q&A systems, or integrating LLMs with external knowledge bases."
                },
                "content": "# RAG Implementation\n\nMaster Retrieval-Augmented Generation (RAG) to build LLM applications that provide accurate, grounded responses using external knowledge sources.\n\n## When to Use This Skill\n\n- Building Q&A systems over proprietary documents\n- Creating chatbots with current, factual information\n- Implementing semantic search with natural language queries\n- Reducing hallucinations with grounded responses\n- Enabling LLMs to access domain-specific knowledge\n- Building documentation assistants\n- Creating research tools with source citation\n\n## Core Components\n\n### 1. Vector Databases\n**Purpose**: Store and retrieve document embeddings efficiently\n\n**Options:**\n- **Pinecone**: Managed, scalable, fast queries\n- **Weaviate**: Open-source, hybrid search\n- **Milvus**: High performance, on-premise\n- **Chroma**: Lightweight, easy to use\n- **Qdrant**: Fast, filtered search\n- **FAISS**: Meta's library, local deployment\n\n### 2. Embeddings\n**Purpose**: Convert text to numerical vectors for similarity search\n\n**Models:**\n- **text-embedding-ada-002** (OpenAI): General purpose, 1536 dims\n- **all-MiniLM-L6-v2** (Sentence Transformers): Fast, lightweight\n- **e5-large-v2**: High quality, multilingual\n- **Instructor**: Task-specific instructions\n- **bge-large-en-v1.5**: SOTA performance\n\n### 3. Retrieval Strategies\n**Approaches:**\n- **Dense Retrieval**: Semantic similarity via embeddings\n- **Sparse Retrieval**: Keyword matching (BM25, TF-IDF)\n- **Hybrid Search**: Combine dense + sparse\n- **Multi-Query**: Generate multiple query variations\n- **HyDE**: Generate hypothetical documents\n\n### 4. Reranking\n**Purpose**: Improve retrieval quality by reordering results\n\n**Methods:**\n- **Cross-Encoders**: BERT-based reranking\n- **Cohere Rerank**: API-based reranking\n- **Maximal Marginal Relevance (MMR)**: Diversity + relevance\n- **LLM-based**: Use LLM to score relevance\n\n## Quick Start\n\n```python\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\n# 1. Load documents\nloader = DirectoryLoader('./docs', glob=\"**/*.txt\")\ndocuments = loader.load()\n\n# 2. Split into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len\n)\nchunks = text_splitter.split_documents(documents)\n\n# 3. Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# 4. Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n    return_source_documents=True\n)\n\n# 5. Query\nresult = qa_chain({\"query\": \"What are the main features?\"})\nprint(result['result'])\nprint(result['source_documents'])\n```\n\n## Advanced RAG Patterns\n\n### Pattern 1: Hybrid Search\n```python\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\n\n# Sparse retriever (BM25)\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 5\n\n# Dense retriever (embeddings)\nembedding_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n\n# Combine with weights\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, embedding_retriever],\n    weights=[0.3, 0.7]\n)\n```\n\n### Pattern 2: Multi-Query Retrieval\n```python\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\n# Generate multiple query perspectives\nretriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=OpenAI()\n)\n\n# Single query â†’ multiple variations â†’ combined results\nresults = retriever.get_relevant_documents(\"What is the main topic?\")\n```\n\n### Pattern 3: Contextual Compression\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectorstore.as_retriever()\n)\n\n# Returns only relevant parts of documents\ncompressed_docs = compression_retriever.get_relevant_documents(\"query\")\n```\n\n### Pattern 4: Parent Document Retriever\n```python\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\n\n# Store for parent documents\nstore = InMemoryStore()\n\n# Small chunks for retrieval, large chunks for context\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter\n)\n```\n\n## Document Chunking Strategies\n\n### Recursive Character Text Splitter\n```python\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try these in order\n)\n```\n\n### Token-Based Splitting\n```python\nfrom langchain.text_splitters import TokenTextSplitter\n\nsplitter = TokenTextSplitter(\n    chunk_size=512,\n    chunk_overlap=50\n)\n```\n\n### Semantic Chunking\n```python\nfrom langchain.text_splitters import SemanticChunker\n\nsplitter = SemanticChunker(\n    embeddings=OpenAIEmbeddings(),\n    breakpoint_threshold_type=\"percentile\"\n)\n```\n\n### Markdown Header Splitter\n```python\nfrom langchain.text_splitters import MarkdownHeaderTextSplitter\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n```\n\n## Vector Store Configurations\n\n### Pinecone\n```python\nimport pinecone\nfrom langchain.vectorstores import Pinecone\n\npinecone.init(api_key=\"your-api-key\", environment=\"us-west1-gcp\")\n\nindex = pinecone.Index(\"your-index-name\")\n\nvectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n```\n\n### Weaviate\n```python\nimport weaviate\nfrom langchain.vectorstores import Weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"Document\", \"content\", embeddings)\n```\n\n### Chroma (Local)\n```python\nfrom langchain.vectorstores import Chroma\n\nvectorstore = Chroma(\n    collection_name=\"my_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n```\n\n## Retrieval Optimization\n\n### 1. Metadata Filtering\n```python\n# Add metadata during indexing\nchunks_with_metadata = []\nfor i, chunk in enumerate(chunks):\n    chunk.metadata = {\n        \"source\": chunk.metadata.get(\"source\"),\n        \"page\": i,\n        \"category\": determine_category(chunk.page_content)\n    }\n    chunks_with_metadata.append(chunk)\n\n# Filter during retrieval\nresults = vectorstore.similarity_search(\n    \"query\",\n    filter={\"category\": \"technical\"},\n    k=5\n)\n```\n\n### 2. Maximal Marginal Relevance\n```python\n# Balance relevance with diversity\nresults = vectorstore.max_marginal_relevance_search(\n    \"query\",\n    k=5,\n    fetch_k=20,  # Fetch 20, return top 5 diverse\n    lambda_mult=0.5  # 0=max diversity, 1=max relevance\n)\n```\n\n### 3. Reranking with Cross-Encoder\n```python\nfrom sentence_transformers import CrossEncoder\n\nreranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n# Get initial results\ncandidates = vectorstore.similarity_search(\"query\", k=20)\n\n# Rerank\npairs = [[query, doc.page_content] for doc in candidates]\nscores = reranker.predict(pairs)\n\n# Sort by score and take top k\nreranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[:5]\n```\n\n## Prompt Engineering for RAG\n\n### Contextual Prompt\n```python\nprompt_template = \"\"\"Use the following context to answer the question. If you cannot answer based on the context, say \"I don't have enough information.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n```\n\n### With Citations\n```python\nprompt_template = \"\"\"Answer the question based on the context below. Include citations using [1], [2], etc.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer (with citations):\"\"\"\n```\n\n### With Confidence\n```python\nprompt_template = \"\"\"Answer the question using the context. Provide a confidence score (0-100%) for your answer.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\nConfidence:\"\"\"\n```\n\n## Evaluation Metrics\n\n```python\ndef evaluate_rag_system(qa_chain, test_cases):\n    metrics = {\n        'accuracy': [],\n        'retrieval_quality': [],\n        'groundedness': []\n    }\n\n    for test in test_cases:\n        result = qa_chain({\"query\": test['question']})\n\n        # Check if answer matches expected\n        accuracy = calculate_accuracy(result['result'], test['expected'])\n        metrics['accuracy'].append(accuracy)\n\n        # Check if relevant docs were retrieved\n        retrieval_quality = evaluate_retrieved_docs(\n            result['source_documents'],\n            test['relevant_docs']\n        )\n        metrics['retrieval_quality'].append(retrieval_quality)\n\n        # Check if answer is grounded in context\n        groundedness = check_groundedness(\n            result['result'],\n            result['source_documents']\n        )\n        metrics['groundedness'].append(groundedness)\n\n    return {k: sum(v)/len(v) for k, v in metrics.items()}\n```\n\n## Resources\n\n- **references/vector-databases.md**: Detailed comparison of vector DBs\n- **references/embeddings.md**: Embedding model selection guide\n- **references/retrieval-strategies.md**: Advanced retrieval techniques\n- **references/reranking.md**: Reranking methods and when to use them\n- **references/context-window.md**: Managing context limits\n- **assets/vector-store-config.yaml**: Configuration templates\n- **assets/retriever-pipeline.py**: Complete RAG pipeline\n- **assets/embedding-models.md**: Model comparison and benchmarks\n\n## Best Practices\n\n1. **Chunk Size**: Balance between context and specificity (500-1000 tokens)\n2. **Overlap**: Use 10-20% overlap to preserve context at boundaries\n3. **Metadata**: Include source, page, timestamp for filtering and debugging\n4. **Hybrid Search**: Combine semantic and keyword search for best results\n5. **Reranking**: Improve top results with cross-encoder\n6. **Citations**: Always return source documents for transparency\n7. **Evaluation**: Continuously test retrieval quality and answer accuracy\n8. **Monitoring**: Track retrieval metrics in production\n\n## Common Issues\n\n- **Poor Retrieval**: Check embedding quality, chunk size, query formulation\n- **Irrelevant Results**: Add metadata filtering, use hybrid search, rerank\n- **Missing Information**: Ensure documents are properly indexed\n- **Slow Queries**: Optimize vector store, use caching, reduce k\n- **Hallucinations**: Improve grounding prompt, add verification step"
              },
              {
                "name": "similarity-search-patterns",
                "description": "Implement efficient similarity search with vector databases. Use when building semantic search, implementing nearest neighbor queries, or optimizing retrieval performance.",
                "path": "plugins/llm-application-dev/skills/similarity-search-patterns/SKILL.md",
                "frontmatter": {
                  "name": "similarity-search-patterns",
                  "description": "Implement efficient similarity search with vector databases. Use when building semantic search, implementing nearest neighbor queries, or optimizing retrieval performance."
                },
                "content": "# Similarity Search Patterns\n\nPatterns for implementing efficient similarity search in production systems.\n\n## When to Use This Skill\n\n- Building semantic search systems\n- Implementing RAG retrieval\n- Creating recommendation engines\n- Optimizing search latency\n- Scaling to millions of vectors\n- Combining semantic and keyword search\n\n## Core Concepts\n\n### 1. Distance Metrics\n\n| Metric | Formula | Best For |\n|--------|---------|----------|\n| **Cosine** | 1 - (AÂ·B)/(â€–Aâ€–â€–Bâ€–) | Normalized embeddings |\n| **Euclidean (L2)** | âˆšÎ£(a-b)Â² | Raw embeddings |\n| **Dot Product** | AÂ·B | Magnitude matters |\n| **Manhattan (L1)** | Î£|a-b| | Sparse vectors |\n\n### 2. Index Types\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 Index Types                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    Flat     â”‚     HNSW      â”‚    IVF+PQ         â”‚\nâ”‚ (Exact)     â”‚ (Graph-based) â”‚ (Quantized)       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ O(n) search â”‚ O(log n)      â”‚ O(âˆšn)             â”‚\nâ”‚ 100% recall â”‚ ~95-99%       â”‚ ~90-95%           â”‚\nâ”‚ Small data  â”‚ Medium-Large  â”‚ Very Large        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Templates\n\n### Template 1: Pinecone Implementation\n\n```python\nfrom pinecone import Pinecone, ServerlessSpec\nfrom typing import List, Dict, Optional\nimport hashlib\n\nclass PineconeVectorStore:\n    def __init__(\n        self,\n        api_key: str,\n        index_name: str,\n        dimension: int = 1536,\n        metric: str = \"cosine\"\n    ):\n        self.pc = Pinecone(api_key=api_key)\n\n        # Create index if not exists\n        if index_name not in self.pc.list_indexes().names():\n            self.pc.create_index(\n                name=index_name,\n                dimension=dimension,\n                metric=metric,\n                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n            )\n\n        self.index = self.pc.Index(index_name)\n\n    def upsert(\n        self,\n        vectors: List[Dict],\n        namespace: str = \"\"\n    ) -> int:\n        \"\"\"\n        Upsert vectors.\n        vectors: [{\"id\": str, \"values\": List[float], \"metadata\": dict}]\n        \"\"\"\n        # Batch upsert\n        batch_size = 100\n        total = 0\n\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            self.index.upsert(vectors=batch, namespace=namespace)\n            total += len(batch)\n\n        return total\n\n    def search(\n        self,\n        query_vector: List[float],\n        top_k: int = 10,\n        namespace: str = \"\",\n        filter: Optional[Dict] = None,\n        include_metadata: bool = True\n    ) -> List[Dict]:\n        \"\"\"Search for similar vectors.\"\"\"\n        results = self.index.query(\n            vector=query_vector,\n            top_k=top_k,\n            namespace=namespace,\n            filter=filter,\n            include_metadata=include_metadata\n        )\n\n        return [\n            {\n                \"id\": match.id,\n                \"score\": match.score,\n                \"metadata\": match.metadata\n            }\n            for match in results.matches\n        ]\n\n    def search_with_rerank(\n        self,\n        query: str,\n        query_vector: List[float],\n        top_k: int = 10,\n        rerank_top_n: int = 50,\n        namespace: str = \"\"\n    ) -> List[Dict]:\n        \"\"\"Search and rerank results.\"\"\"\n        # Over-fetch for reranking\n        initial_results = self.search(\n            query_vector,\n            top_k=rerank_top_n,\n            namespace=namespace\n        )\n\n        # Rerank with cross-encoder or LLM\n        reranked = self._rerank(query, initial_results)\n\n        return reranked[:top_k]\n\n    def _rerank(self, query: str, results: List[Dict]) -> List[Dict]:\n        \"\"\"Rerank results using cross-encoder.\"\"\"\n        from sentence_transformers import CrossEncoder\n\n        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n        pairs = [(query, r[\"metadata\"][\"text\"]) for r in results]\n        scores = model.predict(pairs)\n\n        for result, score in zip(results, scores):\n            result[\"rerank_score\"] = float(score)\n\n        return sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n\n    def delete(self, ids: List[str], namespace: str = \"\"):\n        \"\"\"Delete vectors by ID.\"\"\"\n        self.index.delete(ids=ids, namespace=namespace)\n\n    def delete_by_filter(self, filter: Dict, namespace: str = \"\"):\n        \"\"\"Delete vectors matching filter.\"\"\"\n        self.index.delete(filter=filter, namespace=namespace)\n```\n\n### Template 2: Qdrant Implementation\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom typing import List, Dict, Optional\n\nclass QdrantVectorStore:\n    def __init__(\n        self,\n        url: str = \"localhost\",\n        port: int = 6333,\n        collection_name: str = \"documents\",\n        vector_size: int = 1536\n    ):\n        self.client = QdrantClient(url=url, port=port)\n        self.collection_name = collection_name\n\n        # Create collection if not exists\n        collections = self.client.get_collections().collections\n        if collection_name not in [c.name for c in collections]:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=models.VectorParams(\n                    size=vector_size,\n                    distance=models.Distance.COSINE\n                ),\n                # Optional: enable quantization for memory efficiency\n                quantization_config=models.ScalarQuantization(\n                    scalar=models.ScalarQuantizationConfig(\n                        type=models.ScalarType.INT8,\n                        quantile=0.99,\n                        always_ram=True\n                    )\n                )\n            )\n\n    def upsert(self, points: List[Dict]) -> int:\n        \"\"\"\n        Upsert points.\n        points: [{\"id\": str/int, \"vector\": List[float], \"payload\": dict}]\n        \"\"\"\n        qdrant_points = [\n            models.PointStruct(\n                id=p[\"id\"],\n                vector=p[\"vector\"],\n                payload=p.get(\"payload\", {})\n            )\n            for p in points\n        ]\n\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=qdrant_points\n        )\n        return len(points)\n\n    def search(\n        self,\n        query_vector: List[float],\n        limit: int = 10,\n        filter: Optional[models.Filter] = None,\n        score_threshold: Optional[float] = None\n    ) -> List[Dict]:\n        \"\"\"Search for similar vectors.\"\"\"\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_vector,\n            limit=limit,\n            query_filter=filter,\n            score_threshold=score_threshold\n        )\n\n        return [\n            {\n                \"id\": r.id,\n                \"score\": r.score,\n                \"payload\": r.payload\n            }\n            for r in results\n        ]\n\n    def search_with_filter(\n        self,\n        query_vector: List[float],\n        must_conditions: List[Dict] = None,\n        should_conditions: List[Dict] = None,\n        must_not_conditions: List[Dict] = None,\n        limit: int = 10\n    ) -> List[Dict]:\n        \"\"\"Search with complex filters.\"\"\"\n        conditions = []\n\n        if must_conditions:\n            conditions.extend([\n                models.FieldCondition(\n                    key=c[\"key\"],\n                    match=models.MatchValue(value=c[\"value\"])\n                )\n                for c in must_conditions\n            ])\n\n        filter = models.Filter(must=conditions) if conditions else None\n\n        return self.search(query_vector, limit=limit, filter=filter)\n\n    def search_with_sparse(\n        self,\n        dense_vector: List[float],\n        sparse_vector: Dict[int, float],\n        limit: int = 10,\n        dense_weight: float = 0.7\n    ) -> List[Dict]:\n        \"\"\"Hybrid search with dense and sparse vectors.\"\"\"\n        # Requires collection with named vectors\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=models.NamedVector(\n                name=\"dense\",\n                vector=dense_vector\n            ),\n            limit=limit\n        )\n        return [{\"id\": r.id, \"score\": r.score, \"payload\": r.payload} for r in results]\n```\n\n### Template 3: pgvector with PostgreSQL\n\n```python\nimport asyncpg\nfrom typing import List, Dict, Optional\nimport numpy as np\n\nclass PgVectorStore:\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n\n    async def init(self):\n        \"\"\"Initialize connection pool and extension.\"\"\"\n        self.pool = await asyncpg.create_pool(self.connection_string)\n\n        async with self.pool.acquire() as conn:\n            # Enable extension\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n\n            # Create table\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS documents (\n                    id TEXT PRIMARY KEY,\n                    content TEXT,\n                    metadata JSONB,\n                    embedding vector(1536)\n                )\n            \"\"\")\n\n            # Create index (HNSW for better performance)\n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_embedding_idx\n                ON documents\n                USING hnsw (embedding vector_cosine_ops)\n                WITH (m = 16, ef_construction = 64)\n            \"\"\")\n\n    async def upsert(self, documents: List[Dict]):\n        \"\"\"Upsert documents with embeddings.\"\"\"\n        async with self.pool.acquire() as conn:\n            await conn.executemany(\n                \"\"\"\n                INSERT INTO documents (id, content, metadata, embedding)\n                VALUES ($1, $2, $3, $4)\n                ON CONFLICT (id) DO UPDATE SET\n                    content = EXCLUDED.content,\n                    metadata = EXCLUDED.metadata,\n                    embedding = EXCLUDED.embedding\n                \"\"\",\n                [\n                    (\n                        doc[\"id\"],\n                        doc[\"content\"],\n                        doc.get(\"metadata\", {}),\n                        np.array(doc[\"embedding\"]).tolist()\n                    )\n                    for doc in documents\n                ]\n            )\n\n    async def search(\n        self,\n        query_embedding: List[float],\n        limit: int = 10,\n        filter_metadata: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n        query = \"\"\"\n            SELECT id, content, metadata,\n                   1 - (embedding <=> $1::vector) as similarity\n            FROM documents\n        \"\"\"\n\n        params = [query_embedding]\n\n        if filter_metadata:\n            conditions = []\n            for key, value in filter_metadata.items():\n                params.append(value)\n                conditions.append(f\"metadata->>'{key}' = ${len(params)}\")\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        query += f\" ORDER BY embedding <=> $1::vector LIMIT ${len(params) + 1}\"\n        params.append(limit)\n\n        async with self.pool.acquire() as conn:\n            rows = await conn.fetch(query, *params)\n\n        return [\n            {\n                \"id\": row[\"id\"],\n                \"content\": row[\"content\"],\n                \"metadata\": row[\"metadata\"],\n                \"score\": row[\"similarity\"]\n            }\n            for row in rows\n        ]\n\n    async def hybrid_search(\n        self,\n        query_embedding: List[float],\n        query_text: str,\n        limit: int = 10,\n        vector_weight: float = 0.5\n    ) -> List[Dict]:\n        \"\"\"Hybrid search combining vector and full-text.\"\"\"\n        async with self.pool.acquire() as conn:\n            rows = await conn.fetch(\n                \"\"\"\n                WITH vector_results AS (\n                    SELECT id, content, metadata,\n                           1 - (embedding <=> $1::vector) as vector_score\n                    FROM documents\n                    ORDER BY embedding <=> $1::vector\n                    LIMIT $3 * 2\n                ),\n                text_results AS (\n                    SELECT id, content, metadata,\n                           ts_rank(to_tsvector('english', content),\n                                   plainto_tsquery('english', $2)) as text_score\n                    FROM documents\n                    WHERE to_tsvector('english', content) @@ plainto_tsquery('english', $2)\n                    LIMIT $3 * 2\n                )\n                SELECT\n                    COALESCE(v.id, t.id) as id,\n                    COALESCE(v.content, t.content) as content,\n                    COALESCE(v.metadata, t.metadata) as metadata,\n                    COALESCE(v.vector_score, 0) * $4 +\n                    COALESCE(t.text_score, 0) * (1 - $4) as combined_score\n                FROM vector_results v\n                FULL OUTER JOIN text_results t ON v.id = t.id\n                ORDER BY combined_score DESC\n                LIMIT $3\n                \"\"\",\n                query_embedding, query_text, limit, vector_weight\n            )\n\n        return [dict(row) for row in rows]\n```\n\n### Template 4: Weaviate Implementation\n\n```python\nimport weaviate\nfrom weaviate.util import generate_uuid5\nfrom typing import List, Dict, Optional\n\nclass WeaviateVectorStore:\n    def __init__(\n        self,\n        url: str = \"http://localhost:8080\",\n        class_name: str = \"Document\"\n    ):\n        self.client = weaviate.Client(url=url)\n        self.class_name = class_name\n        self._ensure_schema()\n\n    def _ensure_schema(self):\n        \"\"\"Create schema if not exists.\"\"\"\n        schema = {\n            \"class\": self.class_name,\n            \"vectorizer\": \"none\",  # We provide vectors\n            \"properties\": [\n                {\"name\": \"content\", \"dataType\": [\"text\"]},\n                {\"name\": \"source\", \"dataType\": [\"string\"]},\n                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]}\n            ]\n        }\n\n        if not self.client.schema.exists(self.class_name):\n            self.client.schema.create_class(schema)\n\n    def upsert(self, documents: List[Dict]):\n        \"\"\"Batch upsert documents.\"\"\"\n        with self.client.batch as batch:\n            batch.batch_size = 100\n\n            for doc in documents:\n                batch.add_data_object(\n                    data_object={\n                        \"content\": doc[\"content\"],\n                        \"source\": doc.get(\"source\", \"\"),\n                        \"chunk_id\": doc.get(\"chunk_id\", 0)\n                    },\n                    class_name=self.class_name,\n                    uuid=generate_uuid5(doc[\"id\"]),\n                    vector=doc[\"embedding\"]\n                )\n\n    def search(\n        self,\n        query_vector: List[float],\n        limit: int = 10,\n        where_filter: Optional[Dict] = None\n    ) -> List[Dict]:\n        \"\"\"Vector search.\"\"\"\n        query = (\n            self.client.query\n            .get(self.class_name, [\"content\", \"source\", \"chunk_id\"])\n            .with_near_vector({\"vector\": query_vector})\n            .with_limit(limit)\n            .with_additional([\"distance\", \"id\"])\n        )\n\n        if where_filter:\n            query = query.with_where(where_filter)\n\n        results = query.do()\n\n        return [\n            {\n                \"id\": item[\"_additional\"][\"id\"],\n                \"content\": item[\"content\"],\n                \"source\": item[\"source\"],\n                \"score\": 1 - item[\"_additional\"][\"distance\"]\n            }\n            for item in results[\"data\"][\"Get\"][self.class_name]\n        ]\n\n    def hybrid_search(\n        self,\n        query: str,\n        query_vector: List[float],\n        limit: int = 10,\n        alpha: float = 0.5  # 0 = keyword, 1 = vector\n    ) -> List[Dict]:\n        \"\"\"Hybrid search combining BM25 and vector.\"\"\"\n        results = (\n            self.client.query\n            .get(self.class_name, [\"content\", \"source\"])\n            .with_hybrid(query=query, vector=query_vector, alpha=alpha)\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        return [\n            {\n                \"content\": item[\"content\"],\n                \"source\": item[\"source\"],\n                \"score\": item[\"_additional\"][\"score\"]\n            }\n            for item in results[\"data\"][\"Get\"][self.class_name]\n        ]\n```\n\n## Best Practices\n\n### Do's\n- **Use appropriate index** - HNSW for most cases\n- **Tune parameters** - ef_search, nprobe for recall/speed\n- **Implement hybrid search** - Combine with keyword search\n- **Monitor recall** - Measure search quality\n- **Pre-filter when possible** - Reduce search space\n\n### Don'ts\n- **Don't skip evaluation** - Measure before optimizing\n- **Don't over-index** - Start with flat, scale up\n- **Don't ignore latency** - P99 matters for UX\n- **Don't forget costs** - Vector storage adds up\n\n## Resources\n\n- [Pinecone Docs](https://docs.pinecone.io/)\n- [Qdrant Docs](https://qdrant.tech/documentation/)\n- [pgvector](https://github.com/pgvector/pgvector)\n- [Weaviate Docs](https://weaviate.io/developers/weaviate)"
              },
              {
                "name": "vector-index-tuning",
                "description": "Optimize vector index performance for latency, recall, and memory. Use when tuning HNSW parameters, selecting quantization strategies, or scaling vector search infrastructure.",
                "path": "plugins/llm-application-dev/skills/vector-index-tuning/SKILL.md",
                "frontmatter": {
                  "name": "vector-index-tuning",
                  "description": "Optimize vector index performance for latency, recall, and memory. Use when tuning HNSW parameters, selecting quantization strategies, or scaling vector search infrastructure."
                },
                "content": "# Vector Index Tuning\n\nGuide to optimizing vector indexes for production performance.\n\n## When to Use This Skill\n\n- Tuning HNSW parameters\n- Implementing quantization\n- Optimizing memory usage\n- Reducing search latency\n- Balancing recall vs speed\n- Scaling to billions of vectors\n\n## Core Concepts\n\n### 1. Index Type Selection\n\n```\nData Size           Recommended Index\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n< 10K vectors  â†’    Flat (exact search)\n10K - 1M       â†’    HNSW\n1M - 100M      â†’    HNSW + Quantization\n> 100M         â†’    IVF + PQ or DiskANN\n```\n\n### 2. HNSW Parameters\n\n| Parameter | Default | Effect |\n|-----------|---------|--------|\n| **M** | 16 | Connections per node, â†‘ = better recall, more memory |\n| **efConstruction** | 100 | Build quality, â†‘ = better index, slower build |\n| **efSearch** | 50 | Search quality, â†‘ = better recall, slower search |\n\n### 3. Quantization Types\n\n```\nFull Precision (FP32): 4 bytes Ã— dimensions\nHalf Precision (FP16): 2 bytes Ã— dimensions\nINT8 Scalar:           1 byte Ã— dimensions\nProduct Quantization:  ~32-64 bytes total\nBinary:                dimensions/8 bytes\n```\n\n## Templates\n\n### Template 1: HNSW Parameter Tuning\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\nimport time\n\ndef benchmark_hnsw_parameters(\n    vectors: np.ndarray,\n    queries: np.ndarray,\n    ground_truth: np.ndarray,\n    m_values: List[int] = [8, 16, 32, 64],\n    ef_construction_values: List[int] = [64, 128, 256],\n    ef_search_values: List[int] = [32, 64, 128, 256]\n) -> List[dict]:\n    \"\"\"Benchmark different HNSW configurations.\"\"\"\n    import hnswlib\n\n    results = []\n    dim = vectors.shape[1]\n    n = vectors.shape[0]\n\n    for m in m_values:\n        for ef_construction in ef_construction_values:\n            # Build index\n            index = hnswlib.Index(space='cosine', dim=dim)\n            index.init_index(max_elements=n, M=m, ef_construction=ef_construction)\n\n            build_start = time.time()\n            index.add_items(vectors)\n            build_time = time.time() - build_start\n\n            # Get memory usage\n            memory_bytes = index.element_count * (\n                dim * 4 +  # Vector storage\n                m * 2 * 4  # Graph edges (approximate)\n            )\n\n            for ef_search in ef_search_values:\n                index.set_ef(ef_search)\n\n                # Measure search\n                search_start = time.time()\n                labels, distances = index.knn_query(queries, k=10)\n                search_time = time.time() - search_start\n\n                # Calculate recall\n                recall = calculate_recall(labels, ground_truth, k=10)\n\n                results.append({\n                    \"M\": m,\n                    \"ef_construction\": ef_construction,\n                    \"ef_search\": ef_search,\n                    \"build_time_s\": build_time,\n                    \"search_time_ms\": search_time * 1000 / len(queries),\n                    \"recall@10\": recall,\n                    \"memory_mb\": memory_bytes / 1024 / 1024\n                })\n\n    return results\n\n\ndef calculate_recall(predictions: np.ndarray, ground_truth: np.ndarray, k: int) -> float:\n    \"\"\"Calculate recall@k.\"\"\"\n    correct = 0\n    for pred, truth in zip(predictions, ground_truth):\n        correct += len(set(pred[:k]) & set(truth[:k]))\n    return correct / (len(predictions) * k)\n\n\ndef recommend_hnsw_params(\n    num_vectors: int,\n    target_recall: float = 0.95,\n    max_latency_ms: float = 10,\n    available_memory_gb: float = 8\n) -> dict:\n    \"\"\"Recommend HNSW parameters based on requirements.\"\"\"\n\n    # Base recommendations\n    if num_vectors < 100_000:\n        m = 16\n        ef_construction = 100\n    elif num_vectors < 1_000_000:\n        m = 32\n        ef_construction = 200\n    else:\n        m = 48\n        ef_construction = 256\n\n    # Adjust ef_search based on recall target\n    if target_recall >= 0.99:\n        ef_search = 256\n    elif target_recall >= 0.95:\n        ef_search = 128\n    else:\n        ef_search = 64\n\n    return {\n        \"M\": m,\n        \"ef_construction\": ef_construction,\n        \"ef_search\": ef_search,\n        \"notes\": f\"Estimated for {num_vectors:,} vectors, {target_recall:.0%} recall\"\n    }\n```\n\n### Template 2: Quantization Strategies\n\n```python\nimport numpy as np\nfrom typing import Optional\n\nclass VectorQuantizer:\n    \"\"\"Quantization strategies for vector compression.\"\"\"\n\n    @staticmethod\n    def scalar_quantize_int8(\n        vectors: np.ndarray,\n        min_val: Optional[float] = None,\n        max_val: Optional[float] = None\n    ) -> Tuple[np.ndarray, dict]:\n        \"\"\"Scalar quantization to INT8.\"\"\"\n        if min_val is None:\n            min_val = vectors.min()\n        if max_val is None:\n            max_val = vectors.max()\n\n        # Scale to 0-255 range\n        scale = 255.0 / (max_val - min_val)\n        quantized = np.clip(\n            np.round((vectors - min_val) * scale),\n            0, 255\n        ).astype(np.uint8)\n\n        params = {\"min_val\": min_val, \"max_val\": max_val, \"scale\": scale}\n        return quantized, params\n\n    @staticmethod\n    def dequantize_int8(\n        quantized: np.ndarray,\n        params: dict\n    ) -> np.ndarray:\n        \"\"\"Dequantize INT8 vectors.\"\"\"\n        return quantized.astype(np.float32) / params[\"scale\"] + params[\"min_val\"]\n\n    @staticmethod\n    def product_quantize(\n        vectors: np.ndarray,\n        n_subvectors: int = 8,\n        n_centroids: int = 256\n    ) -> Tuple[np.ndarray, dict]:\n        \"\"\"Product quantization for aggressive compression.\"\"\"\n        from sklearn.cluster import KMeans\n\n        n, dim = vectors.shape\n        assert dim % n_subvectors == 0\n        subvector_dim = dim // n_subvectors\n\n        codebooks = []\n        codes = np.zeros((n, n_subvectors), dtype=np.uint8)\n\n        for i in range(n_subvectors):\n            start = i * subvector_dim\n            end = (i + 1) * subvector_dim\n            subvectors = vectors[:, start:end]\n\n            kmeans = KMeans(n_clusters=n_centroids, random_state=42)\n            codes[:, i] = kmeans.fit_predict(subvectors)\n            codebooks.append(kmeans.cluster_centers_)\n\n        params = {\n            \"codebooks\": codebooks,\n            \"n_subvectors\": n_subvectors,\n            \"subvector_dim\": subvector_dim\n        }\n        return codes, params\n\n    @staticmethod\n    def binary_quantize(vectors: np.ndarray) -> np.ndarray:\n        \"\"\"Binary quantization (sign of each dimension).\"\"\"\n        # Convert to binary: positive = 1, negative = 0\n        binary = (vectors > 0).astype(np.uint8)\n\n        # Pack bits into bytes\n        n, dim = vectors.shape\n        packed_dim = (dim + 7) // 8\n\n        packed = np.zeros((n, packed_dim), dtype=np.uint8)\n        for i in range(dim):\n            byte_idx = i // 8\n            bit_idx = i % 8\n            packed[:, byte_idx] |= (binary[:, i] << bit_idx)\n\n        return packed\n\n\ndef estimate_memory_usage(\n    num_vectors: int,\n    dimensions: int,\n    quantization: str = \"fp32\",\n    index_type: str = \"hnsw\",\n    hnsw_m: int = 16\n) -> dict:\n    \"\"\"Estimate memory usage for different configurations.\"\"\"\n\n    # Vector storage\n    bytes_per_dimension = {\n        \"fp32\": 4,\n        \"fp16\": 2,\n        \"int8\": 1,\n        \"pq\": 0.05,  # Approximate\n        \"binary\": 0.125\n    }\n\n    vector_bytes = num_vectors * dimensions * bytes_per_dimension[quantization]\n\n    # Index overhead\n    if index_type == \"hnsw\":\n        # Each node has ~M*2 edges, each edge is 4 bytes (int32)\n        index_bytes = num_vectors * hnsw_m * 2 * 4\n    elif index_type == \"ivf\":\n        # Inverted lists + centroids\n        index_bytes = num_vectors * 8 + 65536 * dimensions * 4\n    else:\n        index_bytes = 0\n\n    total_bytes = vector_bytes + index_bytes\n\n    return {\n        \"vector_storage_mb\": vector_bytes / 1024 / 1024,\n        \"index_overhead_mb\": index_bytes / 1024 / 1024,\n        \"total_mb\": total_bytes / 1024 / 1024,\n        \"total_gb\": total_bytes / 1024 / 1024 / 1024\n    }\n```\n\n### Template 3: Qdrant Index Configuration\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\n\ndef create_optimized_collection(\n    client: QdrantClient,\n    collection_name: str,\n    vector_size: int,\n    num_vectors: int,\n    optimize_for: str = \"balanced\"  # \"recall\", \"speed\", \"memory\"\n) -> None:\n    \"\"\"Create collection with optimized settings.\"\"\"\n\n    # HNSW configuration based on optimization target\n    hnsw_configs = {\n        \"recall\": models.HnswConfigDiff(m=32, ef_construct=256),\n        \"speed\": models.HnswConfigDiff(m=16, ef_construct=64),\n        \"balanced\": models.HnswConfigDiff(m=16, ef_construct=128),\n        \"memory\": models.HnswConfigDiff(m=8, ef_construct=64)\n    }\n\n    # Quantization configuration\n    quantization_configs = {\n        \"recall\": None,  # No quantization for max recall\n        \"speed\": models.ScalarQuantization(\n            scalar=models.ScalarQuantizationConfig(\n                type=models.ScalarType.INT8,\n                quantile=0.99,\n                always_ram=True\n            )\n        ),\n        \"balanced\": models.ScalarQuantization(\n            scalar=models.ScalarQuantizationConfig(\n                type=models.ScalarType.INT8,\n                quantile=0.99,\n                always_ram=False\n            )\n        ),\n        \"memory\": models.ProductQuantization(\n            product=models.ProductQuantizationConfig(\n                compression=models.CompressionRatio.X16,\n                always_ram=False\n            )\n        )\n    }\n\n    # Optimizer configuration\n    optimizer_configs = {\n        \"recall\": models.OptimizersConfigDiff(\n            indexing_threshold=10000,\n            memmap_threshold=50000\n        ),\n        \"speed\": models.OptimizersConfigDiff(\n            indexing_threshold=5000,\n            memmap_threshold=20000\n        ),\n        \"balanced\": models.OptimizersConfigDiff(\n            indexing_threshold=20000,\n            memmap_threshold=50000\n        ),\n        \"memory\": models.OptimizersConfigDiff(\n            indexing_threshold=50000,\n            memmap_threshold=10000  # Use disk sooner\n        )\n    }\n\n    client.create_collection(\n        collection_name=collection_name,\n        vectors_config=models.VectorParams(\n            size=vector_size,\n            distance=models.Distance.COSINE\n        ),\n        hnsw_config=hnsw_configs[optimize_for],\n        quantization_config=quantization_configs[optimize_for],\n        optimizers_config=optimizer_configs[optimize_for]\n    )\n\n\ndef tune_search_parameters(\n    client: QdrantClient,\n    collection_name: str,\n    target_recall: float = 0.95\n) -> dict:\n    \"\"\"Tune search parameters for target recall.\"\"\"\n\n    # Search parameter recommendations\n    if target_recall >= 0.99:\n        search_params = models.SearchParams(\n            hnsw_ef=256,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=True,  # Don't use quantization for search\n                rescore=True\n            )\n        )\n    elif target_recall >= 0.95:\n        search_params = models.SearchParams(\n            hnsw_ef=128,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=False,\n                rescore=True,\n                oversampling=2.0\n            )\n        )\n    else:\n        search_params = models.SearchParams(\n            hnsw_ef=64,\n            exact=False,\n            quantization=models.QuantizationSearchParams(\n                ignore=False,\n                rescore=False\n            )\n        )\n\n    return search_params\n```\n\n### Template 4: Performance Monitoring\n\n```python\nimport time\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\n\n@dataclass\nclass SearchMetrics:\n    latency_p50_ms: float\n    latency_p95_ms: float\n    latency_p99_ms: float\n    recall: float\n    qps: float\n\n\nclass VectorSearchMonitor:\n    \"\"\"Monitor vector search performance.\"\"\"\n\n    def __init__(self, ground_truth_fn=None):\n        self.latencies = []\n        self.recalls = []\n        self.ground_truth_fn = ground_truth_fn\n\n    def measure_search(\n        self,\n        search_fn,\n        query_vectors: np.ndarray,\n        k: int = 10,\n        num_iterations: int = 100\n    ) -> SearchMetrics:\n        \"\"\"Benchmark search performance.\"\"\"\n        latencies = []\n\n        for _ in range(num_iterations):\n            for query in query_vectors:\n                start = time.perf_counter()\n                results = search_fn(query, k=k)\n                latency = (time.perf_counter() - start) * 1000\n                latencies.append(latency)\n\n        latencies = np.array(latencies)\n        total_queries = num_iterations * len(query_vectors)\n        total_time = sum(latencies) / 1000  # seconds\n\n        return SearchMetrics(\n            latency_p50_ms=np.percentile(latencies, 50),\n            latency_p95_ms=np.percentile(latencies, 95),\n            latency_p99_ms=np.percentile(latencies, 99),\n            recall=self._calculate_recall(search_fn, query_vectors, k) if self.ground_truth_fn else 0,\n            qps=total_queries / total_time\n        )\n\n    def _calculate_recall(self, search_fn, queries: np.ndarray, k: int) -> float:\n        \"\"\"Calculate recall against ground truth.\"\"\"\n        if not self.ground_truth_fn:\n            return 0\n\n        correct = 0\n        total = 0\n\n        for query in queries:\n            predicted = set(search_fn(query, k=k))\n            actual = set(self.ground_truth_fn(query, k=k))\n            correct += len(predicted & actual)\n            total += k\n\n        return correct / total\n\n\ndef profile_index_build(\n    build_fn,\n    vectors: np.ndarray,\n    batch_sizes: List[int] = [1000, 10000, 50000]\n) -> dict:\n    \"\"\"Profile index build performance.\"\"\"\n    results = {}\n\n    for batch_size in batch_sizes:\n        times = []\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            start = time.perf_counter()\n            build_fn(batch)\n            times.append(time.perf_counter() - start)\n\n        results[batch_size] = {\n            \"avg_batch_time_s\": np.mean(times),\n            \"vectors_per_second\": batch_size / np.mean(times)\n        }\n\n    return results\n```\n\n## Best Practices\n\n### Do's\n- **Benchmark with real queries** - Synthetic may not represent production\n- **Monitor recall continuously** - Can degrade with data drift\n- **Start with defaults** - Tune only when needed\n- **Use quantization** - Significant memory savings\n- **Consider tiered storage** - Hot/cold data separation\n\n### Don'ts\n- **Don't over-optimize early** - Profile first\n- **Don't ignore build time** - Index updates have cost\n- **Don't forget reindexing** - Plan for maintenance\n- **Don't skip warming** - Cold indexes are slow\n\n## Resources\n\n- [HNSW Paper](https://arxiv.org/abs/1603.09320)\n- [Faiss Wiki](https://github.com/facebookresearch/faiss/wiki)\n- [ANN Benchmarks](https://ann-benchmarks.com/)"
              }
            ]
          },
          {
            "name": "javascript-typescript",
            "description": "JavaScript and TypeScript development with modern patterns",
            "source": "./plugins/javascript-typescript",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install javascript-typescript@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/typescript-scaffold",
                "description": null,
                "path": "plugins/javascript-typescript/commands/typescript-scaffold.md",
                "frontmatter": null,
                "content": "# TypeScript Project Scaffolding\n\nYou are a TypeScript project architecture expert specializing in scaffolding production-ready Node.js and frontend applications. Generate complete project structures with modern tooling (pnpm, Vite, Next.js), type safety, testing setup, and configuration following current best practices.\n\n## Context\n\nThe user needs automated TypeScript project scaffolding that creates consistent, type-safe applications with proper structure, dependency management, testing, and build tooling. Focus on modern TypeScript patterns and scalable architecture.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Project Type\n\nDetermine the project type from user requirements:\n- **Next.js**: Full-stack React applications, SSR/SSG, API routes\n- **React + Vite**: SPA applications, component libraries\n- **Node.js API**: Express/Fastify backends, microservices\n- **Library**: Reusable packages, utilities, tools\n- **CLI**: Command-line tools, automation scripts\n\n### 2. Initialize Project with pnpm\n\n```bash\n# Install pnpm if needed\nnpm install -g pnpm\n\n# Initialize project\nmkdir project-name && cd project-name\npnpm init\n\n# Initialize git\ngit init\necho \"node_modules/\" >> .gitignore\necho \"dist/\" >> .gitignore\necho \".env\" >> .gitignore\n```\n\n### 3. Generate Next.js Project Structure\n\n```bash\n# Create Next.js project with TypeScript\npnpm create next-app@latest . --typescript --tailwind --app --src-dir --import-alias \"@/*\"\n```\n\n```\nnextjs-project/\nâ”œâ”€â”€ package.json\nâ”œâ”€â”€ tsconfig.json\nâ”œâ”€â”€ next.config.js\nâ”œâ”€â”€ .env.example\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ app/\nâ”‚   â”‚   â”œâ”€â”€ layout.tsx\nâ”‚   â”‚   â”œâ”€â”€ page.tsx\nâ”‚   â”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”‚   â””â”€â”€ health/\nâ”‚   â”‚   â”‚       â””â”€â”€ route.ts\nâ”‚   â”‚   â””â”€â”€ (routes)/\nâ”‚   â”‚       â””â”€â”€ dashboard/\nâ”‚   â”‚           â””â”€â”€ page.tsx\nâ”‚   â”œâ”€â”€ components/\nâ”‚   â”‚   â”œâ”€â”€ ui/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ Button.tsx\nâ”‚   â”‚   â”‚   â””â”€â”€ Card.tsx\nâ”‚   â”‚   â””â”€â”€ layout/\nâ”‚   â”‚       â”œâ”€â”€ Header.tsx\nâ”‚   â”‚       â””â”€â”€ Footer.tsx\nâ”‚   â”œâ”€â”€ lib/\nâ”‚   â”‚   â”œâ”€â”€ api.ts\nâ”‚   â”‚   â”œâ”€â”€ utils.ts\nâ”‚   â”‚   â””â”€â”€ types.ts\nâ”‚   â””â”€â”€ hooks/\nâ”‚       â”œâ”€â”€ useAuth.ts\nâ”‚       â””â”€â”€ useFetch.ts\nâ””â”€â”€ tests/\n    â”œâ”€â”€ setup.ts\n    â””â”€â”€ components/\n        â””â”€â”€ Button.test.tsx\n```\n\n**package.json**:\n```json\n{\n  \"name\": \"nextjs-project\",\n  \"version\": \"0.1.0\",\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"test\": \"vitest\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"next\": \"^14.1.0\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.11.0\",\n    \"@types/react\": \"^18.2.0\",\n    \"typescript\": \"^5.3.0\",\n    \"vitest\": \"^1.2.0\",\n    \"@vitejs/plugin-react\": \"^4.2.0\",\n    \"eslint\": \"^8.56.0\",\n    \"eslint-config-next\": \"^14.1.0\"\n  }\n}\n```\n\n**tsconfig.json**:\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"lib\": [\"ES2022\", \"DOM\", \"DOM.Iterable\"],\n    \"jsx\": \"preserve\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"allowJs\": true,\n    \"strict\": true,\n    \"noEmit\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"incremental\": true,\n    \"paths\": {\n      \"@/*\": [\"./src/*\"]\n    },\n    \"plugins\": [{\"name\": \"next\"}]\n  },\n  \"include\": [\"next-env.d.ts\", \"**/*.ts\", \"**/*.tsx\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\n### 4. Generate React + Vite Project Structure\n\n```bash\n# Create Vite project\npnpm create vite . --template react-ts\n```\n\n**vite.config.ts**:\n```typescript\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\nimport path from 'path'\n\nexport default defineConfig({\n  plugins: [react()],\n  resolve: {\n    alias: {\n      '@': path.resolve(__dirname, './src'),\n    },\n  },\n  server: {\n    port: 3000,\n  },\n  test: {\n    globals: true,\n    environment: 'jsdom',\n    setupFiles: './tests/setup.ts',\n  },\n})\n```\n\n### 5. Generate Node.js API Project Structure\n\n```\nnodejs-api/\nâ”œâ”€â”€ package.json\nâ”œâ”€â”€ tsconfig.json\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ index.ts\nâ”‚   â”œâ”€â”€ app.ts\nâ”‚   â”œâ”€â”€ config/\nâ”‚   â”‚   â”œâ”€â”€ database.ts\nâ”‚   â”‚   â””â”€â”€ env.ts\nâ”‚   â”œâ”€â”€ routes/\nâ”‚   â”‚   â”œâ”€â”€ index.ts\nâ”‚   â”‚   â”œâ”€â”€ users.ts\nâ”‚   â”‚   â””â”€â”€ health.ts\nâ”‚   â”œâ”€â”€ controllers/\nâ”‚   â”‚   â””â”€â”€ userController.ts\nâ”‚   â”œâ”€â”€ services/\nâ”‚   â”‚   â””â”€â”€ userService.ts\nâ”‚   â”œâ”€â”€ models/\nâ”‚   â”‚   â””â”€â”€ User.ts\nâ”‚   â”œâ”€â”€ middleware/\nâ”‚   â”‚   â”œâ”€â”€ auth.ts\nâ”‚   â”‚   â””â”€â”€ errorHandler.ts\nâ”‚   â””â”€â”€ types/\nâ”‚       â””â”€â”€ express.d.ts\nâ””â”€â”€ tests/\n    â””â”€â”€ routes/\n        â””â”€â”€ users.test.ts\n```\n\n**package.json for Node.js API**:\n```json\n{\n  \"name\": \"nodejs-api\",\n  \"version\": \"0.1.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"tsx watch src/index.ts\",\n    \"build\": \"tsc\",\n    \"start\": \"node dist/index.js\",\n    \"test\": \"vitest\",\n    \"lint\": \"eslint src --ext .ts\"\n  },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"dotenv\": \"^16.4.0\",\n    \"zod\": \"^3.22.0\"\n  },\n  \"devDependencies\": {\n    \"@types/express\": \"^4.17.21\",\n    \"@types/node\": \"^20.11.0\",\n    \"typescript\": \"^5.3.0\",\n    \"tsx\": \"^4.7.0\",\n    \"vitest\": \"^1.2.0\",\n    \"eslint\": \"^8.56.0\",\n    \"@typescript-eslint/parser\": \"^6.19.0\",\n    \"@typescript-eslint/eslint-plugin\": \"^6.19.0\"\n  }\n}\n```\n\n**src/app.ts**:\n```typescript\nimport express, { Express } from 'express'\nimport { healthRouter } from './routes/health.js'\nimport { userRouter } from './routes/users.js'\nimport { errorHandler } from './middleware/errorHandler.js'\n\nexport function createApp(): Express {\n  const app = express()\n\n  app.use(express.json())\n  app.use('/health', healthRouter)\n  app.use('/api/users', userRouter)\n  app.use(errorHandler)\n\n  return app\n}\n```\n\n### 6. Generate TypeScript Library Structure\n\n```\nlibrary-name/\nâ”œâ”€â”€ package.json\nâ”œâ”€â”€ tsconfig.json\nâ”œâ”€â”€ tsconfig.build.json\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ index.ts\nâ”‚   â””â”€â”€ core.ts\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ core.test.ts\nâ””â”€â”€ dist/\n```\n\n**package.json for Library**:\n```json\n{\n  \"name\": \"@scope/library-name\",\n  \"version\": \"0.1.0\",\n  \"type\": \"module\",\n  \"main\": \"./dist/index.js\",\n  \"types\": \"./dist/index.d.ts\",\n  \"exports\": {\n    \".\": {\n      \"import\": \"./dist/index.js\",\n      \"types\": \"./dist/index.d.ts\"\n    }\n  },\n  \"files\": [\"dist\"],\n  \"scripts\": {\n    \"build\": \"tsc -p tsconfig.build.json\",\n    \"test\": \"vitest\",\n    \"prepublishOnly\": \"pnpm build\"\n  },\n  \"devDependencies\": {\n    \"typescript\": \"^5.3.0\",\n    \"vitest\": \"^1.2.0\"\n  }\n}\n```\n\n### 7. Configure Development Tools\n\n**.env.example**:\n```env\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://user:pass@localhost:5432/db\nJWT_SECRET=your-secret-key\n```\n\n**vitest.config.ts**:\n```typescript\nimport { defineConfig } from 'vitest/config'\n\nexport default defineConfig({\n  test: {\n    globals: true,\n    environment: 'node',\n    coverage: {\n      provider: 'v8',\n      reporter: ['text', 'json', 'html'],\n    },\n  },\n})\n```\n\n**.eslintrc.json**:\n```json\n{\n  \"parser\": \"@typescript-eslint/parser\",\n  \"extends\": [\n    \"eslint:recommended\",\n    \"plugin:@typescript-eslint/recommended\"\n  ],\n  \"rules\": {\n    \"@typescript-eslint/no-explicit-any\": \"warn\",\n    \"@typescript-eslint/no-unused-vars\": \"error\"\n  }\n}\n```\n\n## Output Format\n\n1. **Project Structure**: Complete directory tree with all necessary files\n2. **Configuration**: package.json, tsconfig.json, build tooling\n3. **Entry Point**: Main application file with type-safe setup\n4. **Tests**: Test structure with Vitest configuration\n5. **Documentation**: README with setup and usage instructions\n6. **Development Tools**: .env.example, .gitignore, linting config\n\nFocus on creating production-ready TypeScript projects with modern tooling, strict type safety, and comprehensive testing setup.\n"
              }
            ],
            "skills": [
              {
                "name": "javascript-testing-patterns",
                "description": "Implement comprehensive testing strategies using Jest, Vitest, and Testing Library for unit tests, integration tests, and end-to-end testing with mocking, fixtures, and test-driven development. Use when writing JavaScript/TypeScript tests, setting up test infrastructure, or implementing TDD/BDD workflows.",
                "path": "plugins/javascript-typescript/skills/javascript-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "javascript-testing-patterns",
                  "description": "Implement comprehensive testing strategies using Jest, Vitest, and Testing Library for unit tests, integration tests, and end-to-end testing with mocking, fixtures, and test-driven development. Use when writing JavaScript/TypeScript tests, setting up test infrastructure, or implementing TDD/BDD workflows."
                },
                "content": "# JavaScript Testing Patterns\n\nComprehensive guide for implementing robust testing strategies in JavaScript/TypeScript applications using modern testing frameworks and best practices.\n\n## When to Use This Skill\n\n- Setting up test infrastructure for new projects\n- Writing unit tests for functions and classes\n- Creating integration tests for APIs and services\n- Implementing end-to-end tests for user flows\n- Mocking external dependencies and APIs\n- Testing React, Vue, or other frontend components\n- Implementing test-driven development (TDD)\n- Setting up continuous testing in CI/CD pipelines\n\n## Testing Frameworks\n\n### Jest - Full-Featured Testing Framework\n\n**Setup:**\n```typescript\n// jest.config.ts\nimport type { Config } from 'jest';\n\nconst config: Config = {\n  preset: 'ts-jest',\n  testEnvironment: 'node',\n  roots: ['<rootDir>/src'],\n  testMatch: ['**/__tests__/**/*.ts', '**/?(*.)+(spec|test).ts'],\n  collectCoverageFrom: [\n    'src/**/*.ts',\n    '!src/**/*.d.ts',\n    '!src/**/*.interface.ts',\n  ],\n  coverageThreshold: {\n    global: {\n      branches: 80,\n      functions: 80,\n      lines: 80,\n      statements: 80,\n    },\n  },\n  setupFilesAfterEnv: ['<rootDir>/src/test/setup.ts'],\n};\n\nexport default config;\n```\n\n### Vitest - Fast, Vite-Native Testing\n\n**Setup:**\n```typescript\n// vitest.config.ts\nimport { defineConfig } from 'vitest/config';\n\nexport default defineConfig({\n  test: {\n    globals: true,\n    environment: 'node',\n    coverage: {\n      provider: 'v8',\n      reporter: ['text', 'json', 'html'],\n      exclude: ['**/*.d.ts', '**/*.config.ts', '**/dist/**'],\n    },\n    setupFiles: ['./src/test/setup.ts'],\n  },\n});\n```\n\n## Unit Testing Patterns\n\n### Pattern 1: Testing Pure Functions\n\n```typescript\n// utils/calculator.ts\nexport function add(a: number, b: number): number {\n  return a + b;\n}\n\nexport function divide(a: number, b: number): number {\n  if (b === 0) {\n    throw new Error('Division by zero');\n  }\n  return a / b;\n}\n\n// utils/calculator.test.ts\nimport { describe, it, expect } from 'vitest';\nimport { add, divide } from './calculator';\n\ndescribe('Calculator', () => {\n  describe('add', () => {\n    it('should add two positive numbers', () => {\n      expect(add(2, 3)).toBe(5);\n    });\n\n    it('should add negative numbers', () => {\n      expect(add(-2, -3)).toBe(-5);\n    });\n\n    it('should handle zero', () => {\n      expect(add(0, 5)).toBe(5);\n      expect(add(5, 0)).toBe(5);\n    });\n  });\n\n  describe('divide', () => {\n    it('should divide two numbers', () => {\n      expect(divide(10, 2)).toBe(5);\n    });\n\n    it('should handle decimal results', () => {\n      expect(divide(5, 2)).toBe(2.5);\n    });\n\n    it('should throw error when dividing by zero', () => {\n      expect(() => divide(10, 0)).toThrow('Division by zero');\n    });\n  });\n});\n```\n\n### Pattern 2: Testing Classes\n\n```typescript\n// services/user.service.ts\nexport class UserService {\n  private users: Map<string, User> = new Map();\n\n  create(user: User): User {\n    if (this.users.has(user.id)) {\n      throw new Error('User already exists');\n    }\n    this.users.set(user.id, user);\n    return user;\n  }\n\n  findById(id: string): User | undefined {\n    return this.users.get(id);\n  }\n\n  update(id: string, updates: Partial<User>): User {\n    const user = this.users.get(id);\n    if (!user) {\n      throw new Error('User not found');\n    }\n    const updated = { ...user, ...updates };\n    this.users.set(id, updated);\n    return updated;\n  }\n\n  delete(id: string): boolean {\n    return this.users.delete(id);\n  }\n}\n\n// services/user.service.test.ts\nimport { describe, it, expect, beforeEach } from 'vitest';\nimport { UserService } from './user.service';\n\ndescribe('UserService', () => {\n  let service: UserService;\n\n  beforeEach(() => {\n    service = new UserService();\n  });\n\n  describe('create', () => {\n    it('should create a new user', () => {\n      const user = { id: '1', name: 'John', email: 'john@example.com' };\n      const created = service.create(user);\n\n      expect(created).toEqual(user);\n      expect(service.findById('1')).toEqual(user);\n    });\n\n    it('should throw error if user already exists', () => {\n      const user = { id: '1', name: 'John', email: 'john@example.com' };\n      service.create(user);\n\n      expect(() => service.create(user)).toThrow('User already exists');\n    });\n  });\n\n  describe('update', () => {\n    it('should update existing user', () => {\n      const user = { id: '1', name: 'John', email: 'john@example.com' };\n      service.create(user);\n\n      const updated = service.update('1', { name: 'Jane' });\n\n      expect(updated.name).toBe('Jane');\n      expect(updated.email).toBe('john@example.com');\n    });\n\n    it('should throw error if user not found', () => {\n      expect(() => service.update('999', { name: 'Jane' }))\n        .toThrow('User not found');\n    });\n  });\n});\n```\n\n### Pattern 3: Testing Async Functions\n\n```typescript\n// services/api.service.ts\nexport class ApiService {\n  async fetchUser(id: string): Promise<User> {\n    const response = await fetch(`https://api.example.com/users/${id}`);\n    if (!response.ok) {\n      throw new Error('User not found');\n    }\n    return response.json();\n  }\n\n  async createUser(user: CreateUserDTO): Promise<User> {\n    const response = await fetch('https://api.example.com/users', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(user),\n    });\n    return response.json();\n  }\n}\n\n// services/api.service.test.ts\nimport { describe, it, expect, vi, beforeEach } from 'vitest';\nimport { ApiService } from './api.service';\n\n// Mock fetch globally\nglobal.fetch = vi.fn();\n\ndescribe('ApiService', () => {\n  let service: ApiService;\n\n  beforeEach(() => {\n    service = new ApiService();\n    vi.clearAllMocks();\n  });\n\n  describe('fetchUser', () => {\n    it('should fetch user successfully', async () => {\n      const mockUser = { id: '1', name: 'John', email: 'john@example.com' };\n\n      (fetch as any).mockResolvedValueOnce({\n        ok: true,\n        json: async () => mockUser,\n      });\n\n      const user = await service.fetchUser('1');\n\n      expect(user).toEqual(mockUser);\n      expect(fetch).toHaveBeenCalledWith('https://api.example.com/users/1');\n    });\n\n    it('should throw error if user not found', async () => {\n      (fetch as any).mockResolvedValueOnce({\n        ok: false,\n      });\n\n      await expect(service.fetchUser('999')).rejects.toThrow('User not found');\n    });\n  });\n\n  describe('createUser', () => {\n    it('should create user successfully', async () => {\n      const newUser = { name: 'John', email: 'john@example.com' };\n      const createdUser = { id: '1', ...newUser };\n\n      (fetch as any).mockResolvedValueOnce({\n        ok: true,\n        json: async () => createdUser,\n      });\n\n      const user = await service.createUser(newUser);\n\n      expect(user).toEqual(createdUser);\n      expect(fetch).toHaveBeenCalledWith(\n        'https://api.example.com/users',\n        expect.objectContaining({\n          method: 'POST',\n          body: JSON.stringify(newUser),\n        })\n      );\n    });\n  });\n});\n```\n\n## Mocking Patterns\n\n### Pattern 1: Mocking Modules\n\n```typescript\n// services/email.service.ts\nimport nodemailer from 'nodemailer';\n\nexport class EmailService {\n  private transporter = nodemailer.createTransport({\n    host: process.env.SMTP_HOST,\n    port: 587,\n    auth: {\n      user: process.env.SMTP_USER,\n      pass: process.env.SMTP_PASS,\n    },\n  });\n\n  async sendEmail(to: string, subject: string, html: string) {\n    await this.transporter.sendMail({\n      from: process.env.EMAIL_FROM,\n      to,\n      subject,\n      html,\n    });\n  }\n}\n\n// services/email.service.test.ts\nimport { describe, it, expect, vi, beforeEach } from 'vitest';\nimport { EmailService } from './email.service';\n\nvi.mock('nodemailer', () => ({\n  default: {\n    createTransport: vi.fn(() => ({\n      sendMail: vi.fn().mockResolvedValue({ messageId: '123' }),\n    })),\n  },\n}));\n\ndescribe('EmailService', () => {\n  let service: EmailService;\n\n  beforeEach(() => {\n    service = new EmailService();\n  });\n\n  it('should send email successfully', async () => {\n    await service.sendEmail(\n      'test@example.com',\n      'Test Subject',\n      '<p>Test Body</p>'\n    );\n\n    expect(service['transporter'].sendMail).toHaveBeenCalledWith(\n      expect.objectContaining({\n        to: 'test@example.com',\n        subject: 'Test Subject',\n      })\n    );\n  });\n});\n```\n\n### Pattern 2: Dependency Injection for Testing\n\n```typescript\n// services/user.service.ts\nexport interface IUserRepository {\n  findById(id: string): Promise<User | null>;\n  create(user: User): Promise<User>;\n}\n\nexport class UserService {\n  constructor(private userRepository: IUserRepository) {}\n\n  async getUser(id: string): Promise<User> {\n    const user = await this.userRepository.findById(id);\n    if (!user) {\n      throw new Error('User not found');\n    }\n    return user;\n  }\n\n  async createUser(userData: CreateUserDTO): Promise<User> {\n    // Business logic here\n    const user = { id: generateId(), ...userData };\n    return this.userRepository.create(user);\n  }\n}\n\n// services/user.service.test.ts\nimport { describe, it, expect, vi, beforeEach } from 'vitest';\nimport { UserService, IUserRepository } from './user.service';\n\ndescribe('UserService', () => {\n  let service: UserService;\n  let mockRepository: IUserRepository;\n\n  beforeEach(() => {\n    mockRepository = {\n      findById: vi.fn(),\n      create: vi.fn(),\n    };\n    service = new UserService(mockRepository);\n  });\n\n  describe('getUser', () => {\n    it('should return user if found', async () => {\n      const mockUser = { id: '1', name: 'John', email: 'john@example.com' };\n      vi.mocked(mockRepository.findById).mockResolvedValue(mockUser);\n\n      const user = await service.getUser('1');\n\n      expect(user).toEqual(mockUser);\n      expect(mockRepository.findById).toHaveBeenCalledWith('1');\n    });\n\n    it('should throw error if user not found', async () => {\n      vi.mocked(mockRepository.findById).mockResolvedValue(null);\n\n      await expect(service.getUser('999')).rejects.toThrow('User not found');\n    });\n  });\n\n  describe('createUser', () => {\n    it('should create user successfully', async () => {\n      const userData = { name: 'John', email: 'john@example.com' };\n      const createdUser = { id: '1', ...userData };\n\n      vi.mocked(mockRepository.create).mockResolvedValue(createdUser);\n\n      const user = await service.createUser(userData);\n\n      expect(user).toEqual(createdUser);\n      expect(mockRepository.create).toHaveBeenCalled();\n    });\n  });\n});\n```\n\n### Pattern 3: Spying on Functions\n\n```typescript\n// utils/logger.ts\nexport const logger = {\n  info: (message: string) => console.log(`INFO: ${message}`),\n  error: (message: string) => console.error(`ERROR: ${message}`),\n};\n\n// services/order.service.ts\nimport { logger } from '../utils/logger';\n\nexport class OrderService {\n  async processOrder(orderId: string): Promise<void> {\n    logger.info(`Processing order ${orderId}`);\n    // Process order logic\n    logger.info(`Order ${orderId} processed successfully`);\n  }\n}\n\n// services/order.service.test.ts\nimport { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';\nimport { OrderService } from './order.service';\nimport { logger } from '../utils/logger';\n\ndescribe('OrderService', () => {\n  let service: OrderService;\n  let loggerSpy: any;\n\n  beforeEach(() => {\n    service = new OrderService();\n    loggerSpy = vi.spyOn(logger, 'info');\n  });\n\n  afterEach(() => {\n    loggerSpy.mockRestore();\n  });\n\n  it('should log order processing', async () => {\n    await service.processOrder('123');\n\n    expect(loggerSpy).toHaveBeenCalledWith('Processing order 123');\n    expect(loggerSpy).toHaveBeenCalledWith('Order 123 processed successfully');\n    expect(loggerSpy).toHaveBeenCalledTimes(2);\n  });\n});\n```\n\n## Integration Testing\n\n### Pattern 1: API Integration Tests\n\n```typescript\n// tests/integration/user.api.test.ts\nimport request from 'supertest';\nimport { app } from '../../src/app';\nimport { pool } from '../../src/config/database';\n\ndescribe('User API Integration Tests', () => {\n  beforeAll(async () => {\n    // Setup test database\n    await pool.query('CREATE TABLE IF NOT EXISTS users (...)');\n  });\n\n  afterAll(async () => {\n    // Cleanup\n    await pool.query('DROP TABLE IF EXISTS users');\n    await pool.end();\n  });\n\n  beforeEach(async () => {\n    // Clear data before each test\n    await pool.query('TRUNCATE TABLE users CASCADE');\n  });\n\n  describe('POST /api/users', () => {\n    it('should create a new user', async () => {\n      const userData = {\n        name: 'John Doe',\n        email: 'john@example.com',\n        password: 'password123',\n      };\n\n      const response = await request(app)\n        .post('/api/users')\n        .send(userData)\n        .expect(201);\n\n      expect(response.body).toMatchObject({\n        name: userData.name,\n        email: userData.email,\n      });\n      expect(response.body).toHaveProperty('id');\n      expect(response.body).not.toHaveProperty('password');\n    });\n\n    it('should return 400 if email is invalid', async () => {\n      const userData = {\n        name: 'John Doe',\n        email: 'invalid-email',\n        password: 'password123',\n      };\n\n      const response = await request(app)\n        .post('/api/users')\n        .send(userData)\n        .expect(400);\n\n      expect(response.body).toHaveProperty('error');\n    });\n\n    it('should return 409 if email already exists', async () => {\n      const userData = {\n        name: 'John Doe',\n        email: 'john@example.com',\n        password: 'password123',\n      };\n\n      await request(app).post('/api/users').send(userData);\n\n      const response = await request(app)\n        .post('/api/users')\n        .send(userData)\n        .expect(409);\n\n      expect(response.body.error).toContain('already exists');\n    });\n  });\n\n  describe('GET /api/users/:id', () => {\n    it('should get user by id', async () => {\n      const createResponse = await request(app)\n        .post('/api/users')\n        .send({\n          name: 'John Doe',\n          email: 'john@example.com',\n          password: 'password123',\n        });\n\n      const userId = createResponse.body.id;\n\n      const response = await request(app)\n        .get(`/api/users/${userId}`)\n        .expect(200);\n\n      expect(response.body).toMatchObject({\n        id: userId,\n        name: 'John Doe',\n        email: 'john@example.com',\n      });\n    });\n\n    it('should return 404 if user not found', async () => {\n      await request(app)\n        .get('/api/users/999')\n        .expect(404);\n    });\n  });\n\n  describe('Authentication', () => {\n    it('should require authentication for protected routes', async () => {\n      await request(app)\n        .get('/api/users/me')\n        .expect(401);\n    });\n\n    it('should allow access with valid token', async () => {\n      // Create user and login\n      await request(app)\n        .post('/api/users')\n        .send({\n          name: 'John Doe',\n          email: 'john@example.com',\n          password: 'password123',\n        });\n\n      const loginResponse = await request(app)\n        .post('/api/auth/login')\n        .send({\n          email: 'john@example.com',\n          password: 'password123',\n        });\n\n      const token = loginResponse.body.token;\n\n      const response = await request(app)\n        .get('/api/users/me')\n        .set('Authorization', `Bearer ${token}`)\n        .expect(200);\n\n      expect(response.body.email).toBe('john@example.com');\n    });\n  });\n});\n```\n\n### Pattern 2: Database Integration Tests\n\n```typescript\n// tests/integration/user.repository.test.ts\nimport { describe, it, expect, beforeAll, afterAll, beforeEach } from 'vitest';\nimport { Pool } from 'pg';\nimport { UserRepository } from '../../src/repositories/user.repository';\n\ndescribe('UserRepository Integration Tests', () => {\n  let pool: Pool;\n  let repository: UserRepository;\n\n  beforeAll(async () => {\n    pool = new Pool({\n      host: 'localhost',\n      port: 5432,\n      database: 'test_db',\n      user: 'test_user',\n      password: 'test_password',\n    });\n\n    repository = new UserRepository(pool);\n\n    // Create tables\n    await pool.query(`\n      CREATE TABLE IF NOT EXISTS users (\n        id SERIAL PRIMARY KEY,\n        name VARCHAR(255) NOT NULL,\n        email VARCHAR(255) UNIQUE NOT NULL,\n        password VARCHAR(255) NOT NULL,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n  });\n\n  afterAll(async () => {\n    await pool.query('DROP TABLE IF EXISTS users');\n    await pool.end();\n  });\n\n  beforeEach(async () => {\n    await pool.query('TRUNCATE TABLE users CASCADE');\n  });\n\n  it('should create a user', async () => {\n    const user = await repository.create({\n      name: 'John Doe',\n      email: 'john@example.com',\n      password: 'hashed_password',\n    });\n\n    expect(user).toHaveProperty('id');\n    expect(user.name).toBe('John Doe');\n    expect(user.email).toBe('john@example.com');\n  });\n\n  it('should find user by email', async () => {\n    await repository.create({\n      name: 'John Doe',\n      email: 'john@example.com',\n      password: 'hashed_password',\n    });\n\n    const user = await repository.findByEmail('john@example.com');\n\n    expect(user).toBeTruthy();\n    expect(user?.name).toBe('John Doe');\n  });\n\n  it('should return null if user not found', async () => {\n    const user = await repository.findByEmail('nonexistent@example.com');\n    expect(user).toBeNull();\n  });\n});\n```\n\n## Frontend Testing with Testing Library\n\n### Pattern 1: React Component Testing\n\n```typescript\n// components/UserForm.tsx\nimport { useState } from 'react';\n\ninterface Props {\n  onSubmit: (user: { name: string; email: string }) => void;\n}\n\nexport function UserForm({ onSubmit }: Props) {\n  const [name, setName] = useState('');\n  const [email, setEmail] = useState('');\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    onSubmit({ name, email });\n  };\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        type=\"text\"\n        placeholder=\"Name\"\n        value={name}\n        onChange={(e) => setName(e.target.value)}\n        data-testid=\"name-input\"\n      />\n      <input\n        type=\"email\"\n        placeholder=\"Email\"\n        value={email}\n        onChange={(e) => setEmail(e.target.value)}\n        data-testid=\"email-input\"\n      />\n      <button type=\"submit\">Submit</button>\n    </form>\n  );\n}\n\n// components/UserForm.test.tsx\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { describe, it, expect, vi } from 'vitest';\nimport { UserForm } from './UserForm';\n\ndescribe('UserForm', () => {\n  it('should render form inputs', () => {\n    render(<UserForm onSubmit={vi.fn()} />);\n\n    expect(screen.getByPlaceholderText('Name')).toBeInTheDocument();\n    expect(screen.getByPlaceholderText('Email')).toBeInTheDocument();\n    expect(screen.getByRole('button', { name: 'Submit' })).toBeInTheDocument();\n  });\n\n  it('should update input values', () => {\n    render(<UserForm onSubmit={vi.fn()} />);\n\n    const nameInput = screen.getByTestId('name-input') as HTMLInputElement;\n    const emailInput = screen.getByTestId('email-input') as HTMLInputElement;\n\n    fireEvent.change(nameInput, { target: { value: 'John Doe' } });\n    fireEvent.change(emailInput, { target: { value: 'john@example.com' } });\n\n    expect(nameInput.value).toBe('John Doe');\n    expect(emailInput.value).toBe('john@example.com');\n  });\n\n  it('should call onSubmit with form data', () => {\n    const onSubmit = vi.fn();\n    render(<UserForm onSubmit={onSubmit} />);\n\n    fireEvent.change(screen.getByTestId('name-input'), {\n      target: { value: 'John Doe' },\n    });\n    fireEvent.change(screen.getByTestId('email-input'), {\n      target: { value: 'john@example.com' },\n    });\n    fireEvent.click(screen.getByRole('button', { name: 'Submit' }));\n\n    expect(onSubmit).toHaveBeenCalledWith({\n      name: 'John Doe',\n      email: 'john@example.com',\n    });\n  });\n});\n```\n\n### Pattern 2: Testing Hooks\n\n```typescript\n// hooks/useCounter.ts\nimport { useState, useCallback } from 'react';\n\nexport function useCounter(initialValue = 0) {\n  const [count, setCount] = useState(initialValue);\n\n  const increment = useCallback(() => setCount((c) => c + 1), []);\n  const decrement = useCallback(() => setCount((c) => c - 1), []);\n  const reset = useCallback(() => setCount(initialValue), [initialValue]);\n\n  return { count, increment, decrement, reset };\n}\n\n// hooks/useCounter.test.ts\nimport { renderHook, act } from '@testing-library/react';\nimport { describe, it, expect } from 'vitest';\nimport { useCounter } from './useCounter';\n\ndescribe('useCounter', () => {\n  it('should initialize with default value', () => {\n    const { result } = renderHook(() => useCounter());\n    expect(result.current.count).toBe(0);\n  });\n\n  it('should initialize with custom value', () => {\n    const { result } = renderHook(() => useCounter(10));\n    expect(result.current.count).toBe(10);\n  });\n\n  it('should increment count', () => {\n    const { result } = renderHook(() => useCounter());\n\n    act(() => {\n      result.current.increment();\n    });\n\n    expect(result.current.count).toBe(1);\n  });\n\n  it('should decrement count', () => {\n    const { result } = renderHook(() => useCounter(5));\n\n    act(() => {\n      result.current.decrement();\n    });\n\n    expect(result.current.count).toBe(4);\n  });\n\n  it('should reset to initial value', () => {\n    const { result } = renderHook(() => useCounter(10));\n\n    act(() => {\n      result.current.increment();\n      result.current.increment();\n    });\n\n    expect(result.current.count).toBe(12);\n\n    act(() => {\n      result.current.reset();\n    });\n\n    expect(result.current.count).toBe(10);\n  });\n});\n```\n\n## Test Fixtures and Factories\n\n```typescript\n// tests/fixtures/user.fixture.ts\nimport { faker } from '@faker-js/faker';\n\nexport function createUserFixture(overrides?: Partial<User>): User {\n  return {\n    id: faker.string.uuid(),\n    name: faker.person.fullName(),\n    email: faker.internet.email(),\n    createdAt: faker.date.past(),\n    ...overrides,\n  };\n}\n\nexport function createUsersFixture(count: number): User[] {\n  return Array.from({ length: count }, () => createUserFixture());\n}\n\n// Usage in tests\nimport { createUserFixture, createUsersFixture } from '../fixtures/user.fixture';\n\ndescribe('UserService', () => {\n  it('should process user', () => {\n    const user = createUserFixture({ name: 'John Doe' });\n    // Use user in test\n  });\n\n  it('should handle multiple users', () => {\n    const users = createUsersFixture(10);\n    // Use users in test\n  });\n});\n```\n\n## Snapshot Testing\n\n```typescript\n// components/UserCard.test.tsx\nimport { render } from '@testing-library/react';\nimport { describe, it, expect } from 'vitest';\nimport { UserCard } from './UserCard';\n\ndescribe('UserCard', () => {\n  it('should match snapshot', () => {\n    const user = {\n      id: '1',\n      name: 'John Doe',\n      email: 'john@example.com',\n      avatar: 'https://example.com/avatar.jpg',\n    };\n\n    const { container } = render(<UserCard user={user} />);\n\n    expect(container.firstChild).toMatchSnapshot();\n  });\n\n  it('should match snapshot with loading state', () => {\n    const { container } = render(<UserCard loading />);\n    expect(container.firstChild).toMatchSnapshot();\n  });\n});\n```\n\n## Coverage Reports\n\n```typescript\n// package.json\n{\n  \"scripts\": {\n    \"test\": \"vitest\",\n    \"test:coverage\": \"vitest --coverage\",\n    \"test:ui\": \"vitest --ui\"\n  }\n}\n```\n\n## Best Practices\n\n1. **Follow AAA Pattern**: Arrange, Act, Assert\n2. **One assertion per test**: Or logically related assertions\n3. **Descriptive test names**: Should describe what is being tested\n4. **Use beforeEach/afterEach**: For setup and teardown\n5. **Mock external dependencies**: Keep tests isolated\n6. **Test edge cases**: Not just happy paths\n7. **Avoid implementation details**: Test behavior, not implementation\n8. **Use test factories**: For consistent test data\n9. **Keep tests fast**: Mock slow operations\n10. **Write tests first (TDD)**: When possible\n11. **Maintain test coverage**: Aim for 80%+ coverage\n12. **Use TypeScript**: For type-safe tests\n13. **Test error handling**: Not just success cases\n14. **Use data-testid sparingly**: Prefer semantic queries\n15. **Clean up after tests**: Prevent test pollution\n\n## Common Patterns\n\n### Test Organization\n\n```typescript\ndescribe('UserService', () => {\n  describe('createUser', () => {\n    it('should create user successfully', () => {});\n    it('should throw error if email exists', () => {});\n    it('should hash password', () => {});\n  });\n\n  describe('updateUser', () => {\n    it('should update user', () => {});\n    it('should throw error if not found', () => {});\n  });\n});\n```\n\n### Testing Promises\n\n```typescript\n// Using async/await\nit('should fetch user', async () => {\n  const user = await service.fetchUser('1');\n  expect(user).toBeDefined();\n});\n\n// Testing rejections\nit('should throw error', async () => {\n  await expect(service.fetchUser('invalid')).rejects.toThrow('Not found');\n});\n```\n\n### Testing Timers\n\n```typescript\nimport { vi } from 'vitest';\n\nit('should call function after delay', () => {\n  vi.useFakeTimers();\n\n  const callback = vi.fn();\n  setTimeout(callback, 1000);\n\n  expect(callback).not.toHaveBeenCalled();\n\n  vi.advanceTimersByTime(1000);\n\n  expect(callback).toHaveBeenCalled();\n\n  vi.useRealTimers();\n});\n```\n\n## Resources\n\n- **Jest Documentation**: https://jestjs.io/\n- **Vitest Documentation**: https://vitest.dev/\n- **Testing Library**: https://testing-library.com/\n- **Kent C. Dodds Testing Blog**: https://kentcdodds.com/blog/"
              },
              {
                "name": "modern-javascript-patterns",
                "description": "Master ES6+ features including async/await, destructuring, spread operators, arrow functions, promises, modules, iterators, generators, and functional programming patterns for writing clean, efficient JavaScript code. Use when refactoring legacy code, implementing modern patterns, or optimizing JavaScript applications.",
                "path": "plugins/javascript-typescript/skills/modern-javascript-patterns/SKILL.md",
                "frontmatter": {
                  "name": "modern-javascript-patterns",
                  "description": "Master ES6+ features including async/await, destructuring, spread operators, arrow functions, promises, modules, iterators, generators, and functional programming patterns for writing clean, efficient JavaScript code. Use when refactoring legacy code, implementing modern patterns, or optimizing JavaScript applications."
                },
                "content": "# Modern JavaScript Patterns\n\nComprehensive guide for mastering modern JavaScript (ES6+) features, functional programming patterns, and best practices for writing clean, maintainable, and performant code.\n\n## When to Use This Skill\n\n- Refactoring legacy JavaScript to modern syntax\n- Implementing functional programming patterns\n- Optimizing JavaScript performance\n- Writing maintainable and readable code\n- Working with asynchronous operations\n- Building modern web applications\n- Migrating from callbacks to Promises/async-await\n- Implementing data transformation pipelines\n\n## ES6+ Core Features\n\n### 1. Arrow Functions\n\n**Syntax and Use Cases:**\n```javascript\n// Traditional function\nfunction add(a, b) {\n  return a + b;\n}\n\n// Arrow function\nconst add = (a, b) => a + b;\n\n// Single parameter (parentheses optional)\nconst double = x => x * 2;\n\n// No parameters\nconst getRandom = () => Math.random();\n\n// Multiple statements (need curly braces)\nconst processUser = user => {\n  const normalized = user.name.toLowerCase();\n  return { ...user, name: normalized };\n};\n\n// Returning objects (wrap in parentheses)\nconst createUser = (name, age) => ({ name, age });\n```\n\n**Lexical 'this' Binding:**\n```javascript\nclass Counter {\n  constructor() {\n    this.count = 0;\n  }\n\n  // Arrow function preserves 'this' context\n  increment = () => {\n    this.count++;\n  };\n\n  // Traditional function loses 'this' in callbacks\n  incrementTraditional() {\n    setTimeout(function() {\n      this.count++;  // 'this' is undefined\n    }, 1000);\n  }\n\n  // Arrow function maintains 'this'\n  incrementArrow() {\n    setTimeout(() => {\n      this.count++;  // 'this' refers to Counter instance\n    }, 1000);\n  }\n}\n```\n\n### 2. Destructuring\n\n**Object Destructuring:**\n```javascript\nconst user = {\n  id: 1,\n  name: 'John Doe',\n  email: 'john@example.com',\n  address: {\n    city: 'New York',\n    country: 'USA'\n  }\n};\n\n// Basic destructuring\nconst { name, email } = user;\n\n// Rename variables\nconst { name: userName, email: userEmail } = user;\n\n// Default values\nconst { age = 25 } = user;\n\n// Nested destructuring\nconst { address: { city, country } } = user;\n\n// Rest operator\nconst { id, ...userWithoutId } = user;\n\n// Function parameters\nfunction greet({ name, age = 18 }) {\n  console.log(`Hello ${name}, you are ${age}`);\n}\ngreet(user);\n```\n\n**Array Destructuring:**\n```javascript\nconst numbers = [1, 2, 3, 4, 5];\n\n// Basic destructuring\nconst [first, second] = numbers;\n\n// Skip elements\nconst [, , third] = numbers;\n\n// Rest operator\nconst [head, ...tail] = numbers;\n\n// Swapping variables\nlet a = 1, b = 2;\n[a, b] = [b, a];\n\n// Function return values\nfunction getCoordinates() {\n  return [10, 20];\n}\nconst [x, y] = getCoordinates();\n\n// Default values\nconst [one, two, three = 0] = [1, 2];\n```\n\n### 3. Spread and Rest Operators\n\n**Spread Operator:**\n```javascript\n// Array spreading\nconst arr1 = [1, 2, 3];\nconst arr2 = [4, 5, 6];\nconst combined = [...arr1, ...arr2];\n\n// Object spreading\nconst defaults = { theme: 'dark', lang: 'en' };\nconst userPrefs = { theme: 'light' };\nconst settings = { ...defaults, ...userPrefs };\n\n// Function arguments\nconst numbers = [1, 2, 3];\nMath.max(...numbers);\n\n// Copying arrays/objects (shallow copy)\nconst copy = [...arr1];\nconst objCopy = { ...user };\n\n// Adding items immutably\nconst newArr = [...arr1, 4, 5];\nconst newObj = { ...user, age: 30 };\n```\n\n**Rest Parameters:**\n```javascript\n// Collect function arguments\nfunction sum(...numbers) {\n  return numbers.reduce((total, num) => total + num, 0);\n}\nsum(1, 2, 3, 4, 5);\n\n// With regular parameters\nfunction greet(greeting, ...names) {\n  return `${greeting} ${names.join(', ')}`;\n}\ngreet('Hello', 'John', 'Jane', 'Bob');\n\n// Object rest\nconst { id, ...userData } = user;\n\n// Array rest\nconst [first, ...rest] = [1, 2, 3, 4, 5];\n```\n\n### 4. Template Literals\n\n```javascript\n// Basic usage\nconst name = 'John';\nconst greeting = `Hello, ${name}!`;\n\n// Multi-line strings\nconst html = `\n  <div>\n    <h1>${title}</h1>\n    <p>${content}</p>\n  </div>\n`;\n\n// Expression evaluation\nconst price = 19.99;\nconst total = `Total: $${(price * 1.2).toFixed(2)}`;\n\n// Tagged template literals\nfunction highlight(strings, ...values) {\n  return strings.reduce((result, str, i) => {\n    const value = values[i] || '';\n    return result + str + `<mark>${value}</mark>`;\n  }, '');\n}\n\nconst name = 'John';\nconst age = 30;\nconst html = highlight`Name: ${name}, Age: ${age}`;\n// Output: \"Name: <mark>John</mark>, Age: <mark>30</mark>\"\n```\n\n### 5. Enhanced Object Literals\n\n```javascript\nconst name = 'John';\nconst age = 30;\n\n// Shorthand property names\nconst user = { name, age };\n\n// Shorthand method names\nconst calculator = {\n  add(a, b) {\n    return a + b;\n  },\n  subtract(a, b) {\n    return a - b;\n  }\n};\n\n// Computed property names\nconst field = 'email';\nconst user = {\n  name: 'John',\n  [field]: 'john@example.com',\n  [`get${field.charAt(0).toUpperCase()}${field.slice(1)}`]() {\n    return this[field];\n  }\n};\n\n// Dynamic property creation\nconst createUser = (name, ...props) => {\n  return props.reduce((user, [key, value]) => ({\n    ...user,\n    [key]: value\n  }), { name });\n};\n\nconst user = createUser('John', ['age', 30], ['email', 'john@example.com']);\n```\n\n## Asynchronous Patterns\n\n### 1. Promises\n\n**Creating and Using Promises:**\n```javascript\n// Creating a promise\nconst fetchUser = (id) => {\n  return new Promise((resolve, reject) => {\n    setTimeout(() => {\n      if (id > 0) {\n        resolve({ id, name: 'John' });\n      } else {\n        reject(new Error('Invalid ID'));\n      }\n    }, 1000);\n  });\n};\n\n// Using promises\nfetchUser(1)\n  .then(user => console.log(user))\n  .catch(error => console.error(error))\n  .finally(() => console.log('Done'));\n\n// Chaining promises\nfetchUser(1)\n  .then(user => fetchUserPosts(user.id))\n  .then(posts => processPosts(posts))\n  .then(result => console.log(result))\n  .catch(error => console.error(error));\n```\n\n**Promise Combinators:**\n```javascript\n// Promise.all - Wait for all promises\nconst promises = [\n  fetchUser(1),\n  fetchUser(2),\n  fetchUser(3)\n];\n\nPromise.all(promises)\n  .then(users => console.log(users))\n  .catch(error => console.error('At least one failed:', error));\n\n// Promise.allSettled - Wait for all, regardless of outcome\nPromise.allSettled(promises)\n  .then(results => {\n    results.forEach(result => {\n      if (result.status === 'fulfilled') {\n        console.log('Success:', result.value);\n      } else {\n        console.log('Error:', result.reason);\n      }\n    });\n  });\n\n// Promise.race - First to complete\nPromise.race(promises)\n  .then(winner => console.log('First:', winner))\n  .catch(error => console.error(error));\n\n// Promise.any - First to succeed\nPromise.any(promises)\n  .then(first => console.log('First success:', first))\n  .catch(error => console.error('All failed:', error));\n```\n\n### 2. Async/Await\n\n**Basic Usage:**\n```javascript\n// Async function always returns a Promise\nasync function fetchUser(id) {\n  const response = await fetch(`/api/users/${id}`);\n  const user = await response.json();\n  return user;\n}\n\n// Error handling with try/catch\nasync function getUserData(id) {\n  try {\n    const user = await fetchUser(id);\n    const posts = await fetchUserPosts(user.id);\n    return { user, posts };\n  } catch (error) {\n    console.error('Error fetching data:', error);\n    throw error;\n  }\n}\n\n// Sequential vs Parallel execution\nasync function sequential() {\n  const user1 = await fetchUser(1);  // Wait\n  const user2 = await fetchUser(2);  // Then wait\n  return [user1, user2];\n}\n\nasync function parallel() {\n  const [user1, user2] = await Promise.all([\n    fetchUser(1),\n    fetchUser(2)\n  ]);\n  return [user1, user2];\n}\n```\n\n**Advanced Patterns:**\n```javascript\n// Async IIFE\n(async () => {\n  const result = await someAsyncOperation();\n  console.log(result);\n})();\n\n// Async iteration\nasync function processUsers(userIds) {\n  for (const id of userIds) {\n    const user = await fetchUser(id);\n    await processUser(user);\n  }\n}\n\n// Top-level await (ES2022)\nconst config = await fetch('/config.json').then(r => r.json());\n\n// Retry logic\nasync function fetchWithRetry(url, retries = 3) {\n  for (let i = 0; i < retries; i++) {\n    try {\n      return await fetch(url);\n    } catch (error) {\n      if (i === retries - 1) throw error;\n      await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));\n    }\n  }\n}\n\n// Timeout wrapper\nasync function withTimeout(promise, ms) {\n  const timeout = new Promise((_, reject) =>\n    setTimeout(() => reject(new Error('Timeout')), ms)\n  );\n  return Promise.race([promise, timeout]);\n}\n```\n\n## Functional Programming Patterns\n\n### 1. Array Methods\n\n**Map, Filter, Reduce:**\n```javascript\nconst users = [\n  { id: 1, name: 'John', age: 30, active: true },\n  { id: 2, name: 'Jane', age: 25, active: false },\n  { id: 3, name: 'Bob', age: 35, active: true }\n];\n\n// Map - Transform array\nconst names = users.map(user => user.name);\nconst upperNames = users.map(user => user.name.toUpperCase());\n\n// Filter - Select elements\nconst activeUsers = users.filter(user => user.active);\nconst adults = users.filter(user => user.age >= 18);\n\n// Reduce - Aggregate data\nconst totalAge = users.reduce((sum, user) => sum + user.age, 0);\nconst avgAge = totalAge / users.length;\n\n// Group by property\nconst byActive = users.reduce((groups, user) => {\n  const key = user.active ? 'active' : 'inactive';\n  return {\n    ...groups,\n    [key]: [...(groups[key] || []), user]\n  };\n}, {});\n\n// Chaining methods\nconst result = users\n  .filter(user => user.active)\n  .map(user => user.name)\n  .sort()\n  .join(', ');\n```\n\n**Advanced Array Methods:**\n```javascript\n// Find - First matching element\nconst user = users.find(u => u.id === 2);\n\n// FindIndex - Index of first match\nconst index = users.findIndex(u => u.name === 'Jane');\n\n// Some - At least one matches\nconst hasActive = users.some(u => u.active);\n\n// Every - All match\nconst allAdults = users.every(u => u.age >= 18);\n\n// FlatMap - Map and flatten\nconst userTags = [\n  { name: 'John', tags: ['admin', 'user'] },\n  { name: 'Jane', tags: ['user'] }\n];\nconst allTags = userTags.flatMap(u => u.tags);\n\n// From - Create array from iterable\nconst str = 'hello';\nconst chars = Array.from(str);\nconst numbers = Array.from({ length: 5 }, (_, i) => i + 1);\n\n// Of - Create array from arguments\nconst arr = Array.of(1, 2, 3);\n```\n\n### 2. Higher-Order Functions\n\n**Functions as Arguments:**\n```javascript\n// Custom forEach\nfunction forEach(array, callback) {\n  for (let i = 0; i < array.length; i++) {\n    callback(array[i], i, array);\n  }\n}\n\n// Custom map\nfunction map(array, transform) {\n  const result = [];\n  for (const item of array) {\n    result.push(transform(item));\n  }\n  return result;\n}\n\n// Custom filter\nfunction filter(array, predicate) {\n  const result = [];\n  for (const item of array) {\n    if (predicate(item)) {\n      result.push(item);\n    }\n  }\n  return result;\n}\n```\n\n**Functions Returning Functions:**\n```javascript\n// Currying\nconst multiply = a => b => a * b;\nconst double = multiply(2);\nconst triple = multiply(3);\n\nconsole.log(double(5));  // 10\nconsole.log(triple(5));  // 15\n\n// Partial application\nfunction partial(fn, ...args) {\n  return (...moreArgs) => fn(...args, ...moreArgs);\n}\n\nconst add = (a, b, c) => a + b + c;\nconst add5 = partial(add, 5);\nconsole.log(add5(3, 2));  // 10\n\n// Memoization\nfunction memoize(fn) {\n  const cache = new Map();\n  return (...args) => {\n    const key = JSON.stringify(args);\n    if (cache.has(key)) {\n      return cache.get(key);\n    }\n    const result = fn(...args);\n    cache.set(key, result);\n    return result;\n  };\n}\n\nconst fibonacci = memoize((n) => {\n  if (n <= 1) return n;\n  return fibonacci(n - 1) + fibonacci(n - 2);\n});\n```\n\n### 3. Composition and Piping\n\n```javascript\n// Function composition\nconst compose = (...fns) => x =>\n  fns.reduceRight((acc, fn) => fn(acc), x);\n\nconst pipe = (...fns) => x =>\n  fns.reduce((acc, fn) => fn(acc), x);\n\n// Example usage\nconst addOne = x => x + 1;\nconst double = x => x * 2;\nconst square = x => x * x;\n\nconst composed = compose(square, double, addOne);\nconsole.log(composed(3));  // ((3 + 1) * 2)^2 = 64\n\nconst piped = pipe(addOne, double, square);\nconsole.log(piped(3));  // ((3 + 1) * 2)^2 = 64\n\n// Practical example\nconst processUser = pipe(\n  user => ({ ...user, name: user.name.trim() }),\n  user => ({ ...user, email: user.email.toLowerCase() }),\n  user => ({ ...user, age: parseInt(user.age) })\n);\n\nconst user = processUser({\n  name: '  John  ',\n  email: 'JOHN@EXAMPLE.COM',\n  age: '30'\n});\n```\n\n### 4. Pure Functions and Immutability\n\n```javascript\n// Impure function (modifies input)\nfunction addItemImpure(cart, item) {\n  cart.items.push(item);\n  cart.total += item.price;\n  return cart;\n}\n\n// Pure function (no side effects)\nfunction addItemPure(cart, item) {\n  return {\n    ...cart,\n    items: [...cart.items, item],\n    total: cart.total + item.price\n  };\n}\n\n// Immutable array operations\nconst numbers = [1, 2, 3, 4, 5];\n\n// Add to array\nconst withSix = [...numbers, 6];\n\n// Remove from array\nconst withoutThree = numbers.filter(n => n !== 3);\n\n// Update array element\nconst doubled = numbers.map(n => n === 3 ? n * 2 : n);\n\n// Immutable object operations\nconst user = { name: 'John', age: 30 };\n\n// Update property\nconst olderUser = { ...user, age: 31 };\n\n// Add property\nconst withEmail = { ...user, email: 'john@example.com' };\n\n// Remove property\nconst { age, ...withoutAge } = user;\n\n// Deep cloning (simple approach)\nconst deepClone = obj => JSON.parse(JSON.stringify(obj));\n\n// Better deep cloning\nconst structuredClone = obj => globalThis.structuredClone(obj);\n```\n\n## Modern Class Features\n\n```javascript\n// Class syntax\nclass User {\n  // Private fields\n  #password;\n\n  // Public fields\n  id;\n  name;\n\n  // Static field\n  static count = 0;\n\n  constructor(id, name, password) {\n    this.id = id;\n    this.name = name;\n    this.#password = password;\n    User.count++;\n  }\n\n  // Public method\n  greet() {\n    return `Hello, ${this.name}`;\n  }\n\n  // Private method\n  #hashPassword(password) {\n    return `hashed_${password}`;\n  }\n\n  // Getter\n  get displayName() {\n    return this.name.toUpperCase();\n  }\n\n  // Setter\n  set password(newPassword) {\n    this.#password = this.#hashPassword(newPassword);\n  }\n\n  // Static method\n  static create(id, name, password) {\n    return new User(id, name, password);\n  }\n}\n\n// Inheritance\nclass Admin extends User {\n  constructor(id, name, password, role) {\n    super(id, name, password);\n    this.role = role;\n  }\n\n  greet() {\n    return `${super.greet()}, I'm an admin`;\n  }\n}\n```\n\n## Modules (ES6)\n\n```javascript\n// Exporting\n// math.js\nexport const PI = 3.14159;\nexport function add(a, b) {\n  return a + b;\n}\nexport class Calculator {\n  // ...\n}\n\n// Default export\nexport default function multiply(a, b) {\n  return a * b;\n}\n\n// Importing\n// app.js\nimport multiply, { PI, add, Calculator } from './math.js';\n\n// Rename imports\nimport { add as sum } from './math.js';\n\n// Import all\nimport * as Math from './math.js';\n\n// Dynamic imports\nconst module = await import('./math.js');\nconst { add } = await import('./math.js');\n\n// Conditional loading\nif (condition) {\n  const module = await import('./feature.js');\n  module.init();\n}\n```\n\n## Iterators and Generators\n\n```javascript\n// Custom iterator\nconst range = {\n  from: 1,\n  to: 5,\n\n  [Symbol.iterator]() {\n    return {\n      current: this.from,\n      last: this.to,\n\n      next() {\n        if (this.current <= this.last) {\n          return { done: false, value: this.current++ };\n        } else {\n          return { done: true };\n        }\n      }\n    };\n  }\n};\n\nfor (const num of range) {\n  console.log(num);  // 1, 2, 3, 4, 5\n}\n\n// Generator function\nfunction* rangeGenerator(from, to) {\n  for (let i = from; i <= to; i++) {\n    yield i;\n  }\n}\n\nfor (const num of rangeGenerator(1, 5)) {\n  console.log(num);\n}\n\n// Infinite generator\nfunction* fibonacci() {\n  let [prev, curr] = [0, 1];\n  while (true) {\n    yield curr;\n    [prev, curr] = [curr, prev + curr];\n  }\n}\n\n// Async generator\nasync function* fetchPages(url) {\n  let page = 1;\n  while (true) {\n    const response = await fetch(`${url}?page=${page}`);\n    const data = await response.json();\n    if (data.length === 0) break;\n    yield data;\n    page++;\n  }\n}\n\nfor await (const page of fetchPages('/api/users')) {\n  console.log(page);\n}\n```\n\n## Modern Operators\n\n```javascript\n// Optional chaining\nconst user = { name: 'John', address: { city: 'NYC' } };\nconst city = user?.address?.city;\nconst zipCode = user?.address?.zipCode;  // undefined\n\n// Function call\nconst result = obj.method?.();\n\n// Array access\nconst first = arr?.[0];\n\n// Nullish coalescing\nconst value = null ?? 'default';      // 'default'\nconst value = undefined ?? 'default'; // 'default'\nconst value = 0 ?? 'default';         // 0 (not 'default')\nconst value = '' ?? 'default';        // '' (not 'default')\n\n// Logical assignment\nlet a = null;\na ??= 'default';  // a = 'default'\n\nlet b = 5;\nb ??= 10;  // b = 5 (unchanged)\n\nlet obj = { count: 0 };\nobj.count ||= 1;  // obj.count = 1\nobj.count &&= 2;  // obj.count = 2\n```\n\n## Performance Optimization\n\n```javascript\n// Debounce\nfunction debounce(fn, delay) {\n  let timeoutId;\n  return (...args) => {\n    clearTimeout(timeoutId);\n    timeoutId = setTimeout(() => fn(...args), delay);\n  };\n}\n\nconst searchDebounced = debounce(search, 300);\n\n// Throttle\nfunction throttle(fn, limit) {\n  let inThrottle;\n  return (...args) => {\n    if (!inThrottle) {\n      fn(...args);\n      inThrottle = true;\n      setTimeout(() => inThrottle = false, limit);\n    }\n  };\n}\n\nconst scrollThrottled = throttle(handleScroll, 100);\n\n// Lazy evaluation\nfunction* lazyMap(iterable, transform) {\n  for (const item of iterable) {\n    yield transform(item);\n  }\n}\n\n// Use only what you need\nconst numbers = [1, 2, 3, 4, 5];\nconst doubled = lazyMap(numbers, x => x * 2);\nconst first = doubled.next().value;  // Only computes first value\n```\n\n## Best Practices\n\n1. **Use const by default**: Only use let when reassignment is needed\n2. **Prefer arrow functions**: Especially for callbacks\n3. **Use template literals**: Instead of string concatenation\n4. **Destructure objects and arrays**: For cleaner code\n5. **Use async/await**: Instead of Promise chains\n6. **Avoid mutating data**: Use spread operator and array methods\n7. **Use optional chaining**: Prevent \"Cannot read property of undefined\"\n8. **Use nullish coalescing**: For default values\n9. **Prefer array methods**: Over traditional loops\n10. **Use modules**: For better code organization\n11. **Write pure functions**: Easier to test and reason about\n12. **Use meaningful variable names**: Self-documenting code\n13. **Keep functions small**: Single responsibility principle\n14. **Handle errors properly**: Use try/catch with async/await\n15. **Use strict mode**: `'use strict'` for better error catching\n\n## Common Pitfalls\n\n1. **this binding confusion**: Use arrow functions or bind()\n2. **Async/await without error handling**: Always use try/catch\n3. **Promise creation unnecessary**: Don't wrap already async functions\n4. **Mutation of objects**: Use spread operator or Object.assign()\n5. **Forgetting await**: Async functions return promises\n6. **Blocking event loop**: Avoid synchronous operations\n7. **Memory leaks**: Clean up event listeners and timers\n8. **Not handling promise rejections**: Use catch() or try/catch\n\n## Resources\n\n- **MDN Web Docs**: https://developer.mozilla.org/en-US/docs/Web/JavaScript\n- **JavaScript.info**: https://javascript.info/\n- **You Don't Know JS**: https://github.com/getify/You-Dont-Know-JS\n- **Eloquent JavaScript**: https://eloquentjavascript.net/\n- **ES6 Features**: http://es6-features.org/"
              },
              {
                "name": "nodejs-backend-patterns",
                "description": "Build production-ready Node.js backend services with Express/Fastify, implementing middleware patterns, error handling, authentication, database integration, and API design best practices. Use when creating Node.js servers, REST APIs, GraphQL backends, or microservices architectures.",
                "path": "plugins/javascript-typescript/skills/nodejs-backend-patterns/SKILL.md",
                "frontmatter": {
                  "name": "nodejs-backend-patterns",
                  "description": "Build production-ready Node.js backend services with Express/Fastify, implementing middleware patterns, error handling, authentication, database integration, and API design best practices. Use when creating Node.js servers, REST APIs, GraphQL backends, or microservices architectures."
                },
                "content": "# Node.js Backend Patterns\n\nComprehensive guidance for building scalable, maintainable, and production-ready Node.js backend applications with modern frameworks, architectural patterns, and best practices.\n\n## When to Use This Skill\n\n- Building REST APIs or GraphQL servers\n- Creating microservices with Node.js\n- Implementing authentication and authorization\n- Designing scalable backend architectures\n- Setting up middleware and error handling\n- Integrating databases (SQL and NoSQL)\n- Building real-time applications with WebSockets\n- Implementing background job processing\n\n## Core Frameworks\n\n### Express.js - Minimalist Framework\n\n**Basic Setup:**\n```typescript\nimport express, { Request, Response, NextFunction } from 'express';\nimport helmet from 'helmet';\nimport cors from 'cors';\nimport compression from 'compression';\n\nconst app = express();\n\n// Security middleware\napp.use(helmet());\napp.use(cors({ origin: process.env.ALLOWED_ORIGINS?.split(',') }));\napp.use(compression());\n\n// Body parsing\napp.use(express.json({ limit: '10mb' }));\napp.use(express.urlencoded({ extended: true, limit: '10mb' }));\n\n// Request logging\napp.use((req: Request, res: Response, next: NextFunction) => {\n  console.log(`${req.method} ${req.path}`);\n  next();\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n```\n\n### Fastify - High Performance Framework\n\n**Basic Setup:**\n```typescript\nimport Fastify from 'fastify';\nimport helmet from '@fastify/helmet';\nimport cors from '@fastify/cors';\nimport compress from '@fastify/compress';\n\nconst fastify = Fastify({\n  logger: {\n    level: process.env.LOG_LEVEL || 'info',\n    transport: {\n      target: 'pino-pretty',\n      options: { colorize: true }\n    }\n  }\n});\n\n// Plugins\nawait fastify.register(helmet);\nawait fastify.register(cors, { origin: true });\nawait fastify.register(compress);\n\n// Type-safe routes with schema validation\nfastify.post<{\n  Body: { name: string; email: string };\n  Reply: { id: string; name: string };\n}>('/users', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['name', 'email'],\n      properties: {\n        name: { type: 'string', minLength: 1 },\n        email: { type: 'string', format: 'email' }\n      }\n    }\n  }\n}, async (request, reply) => {\n  const { name, email } = request.body;\n  return { id: '123', name };\n});\n\nawait fastify.listen({ port: 3000, host: '0.0.0.0' });\n```\n\n## Architectural Patterns\n\n### Pattern 1: Layered Architecture\n\n**Structure:**\n```\nsrc/\nâ”œâ”€â”€ controllers/     # Handle HTTP requests/responses\nâ”œâ”€â”€ services/        # Business logic\nâ”œâ”€â”€ repositories/    # Data access layer\nâ”œâ”€â”€ models/          # Data models\nâ”œâ”€â”€ middleware/      # Express/Fastify middleware\nâ”œâ”€â”€ routes/          # Route definitions\nâ”œâ”€â”€ utils/           # Helper functions\nâ”œâ”€â”€ config/          # Configuration\nâ””â”€â”€ types/           # TypeScript types\n```\n\n**Controller Layer:**\n```typescript\n// controllers/user.controller.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { UserService } from '../services/user.service';\nimport { CreateUserDTO, UpdateUserDTO } from '../types/user.types';\n\nexport class UserController {\n  constructor(private userService: UserService) {}\n\n  async createUser(req: Request, res: Response, next: NextFunction) {\n    try {\n      const userData: CreateUserDTO = req.body;\n      const user = await this.userService.createUser(userData);\n      res.status(201).json(user);\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async getUser(req: Request, res: Response, next: NextFunction) {\n    try {\n      const { id } = req.params;\n      const user = await this.userService.getUserById(id);\n      res.json(user);\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async updateUser(req: Request, res: Response, next: NextFunction) {\n    try {\n      const { id } = req.params;\n      const updates: UpdateUserDTO = req.body;\n      const user = await this.userService.updateUser(id, updates);\n      res.json(user);\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async deleteUser(req: Request, res: Response, next: NextFunction) {\n    try {\n      const { id } = req.params;\n      await this.userService.deleteUser(id);\n      res.status(204).send();\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n```\n\n**Service Layer:**\n```typescript\n// services/user.service.ts\nimport { UserRepository } from '../repositories/user.repository';\nimport { CreateUserDTO, UpdateUserDTO, User } from '../types/user.types';\nimport { NotFoundError, ValidationError } from '../utils/errors';\nimport bcrypt from 'bcrypt';\n\nexport class UserService {\n  constructor(private userRepository: UserRepository) {}\n\n  async createUser(userData: CreateUserDTO): Promise<User> {\n    // Validation\n    const existingUser = await this.userRepository.findByEmail(userData.email);\n    if (existingUser) {\n      throw new ValidationError('Email already exists');\n    }\n\n    // Hash password\n    const hashedPassword = await bcrypt.hash(userData.password, 10);\n\n    // Create user\n    const user = await this.userRepository.create({\n      ...userData,\n      password: hashedPassword\n    });\n\n    // Remove password from response\n    const { password, ...userWithoutPassword } = user;\n    return userWithoutPassword as User;\n  }\n\n  async getUserById(id: string): Promise<User> {\n    const user = await this.userRepository.findById(id);\n    if (!user) {\n      throw new NotFoundError('User not found');\n    }\n    const { password, ...userWithoutPassword } = user;\n    return userWithoutPassword as User;\n  }\n\n  async updateUser(id: string, updates: UpdateUserDTO): Promise<User> {\n    const user = await this.userRepository.update(id, updates);\n    if (!user) {\n      throw new NotFoundError('User not found');\n    }\n    const { password, ...userWithoutPassword } = user;\n    return userWithoutPassword as User;\n  }\n\n  async deleteUser(id: string): Promise<void> {\n    const deleted = await this.userRepository.delete(id);\n    if (!deleted) {\n      throw new NotFoundError('User not found');\n    }\n  }\n}\n```\n\n**Repository Layer:**\n```typescript\n// repositories/user.repository.ts\nimport { Pool } from 'pg';\nimport { CreateUserDTO, UpdateUserDTO, UserEntity } from '../types/user.types';\n\nexport class UserRepository {\n  constructor(private db: Pool) {}\n\n  async create(userData: CreateUserDTO & { password: string }): Promise<UserEntity> {\n    const query = `\n      INSERT INTO users (name, email, password)\n      VALUES ($1, $2, $3)\n      RETURNING id, name, email, password, created_at, updated_at\n    `;\n    const { rows } = await this.db.query(query, [\n      userData.name,\n      userData.email,\n      userData.password\n    ]);\n    return rows[0];\n  }\n\n  async findById(id: string): Promise<UserEntity | null> {\n    const query = 'SELECT * FROM users WHERE id = $1';\n    const { rows } = await this.db.query(query, [id]);\n    return rows[0] || null;\n  }\n\n  async findByEmail(email: string): Promise<UserEntity | null> {\n    const query = 'SELECT * FROM users WHERE email = $1';\n    const { rows } = await this.db.query(query, [email]);\n    return rows[0] || null;\n  }\n\n  async update(id: string, updates: UpdateUserDTO): Promise<UserEntity | null> {\n    const fields = Object.keys(updates);\n    const values = Object.values(updates);\n\n    const setClause = fields\n      .map((field, idx) => `${field} = $${idx + 2}`)\n      .join(', ');\n\n    const query = `\n      UPDATE users\n      SET ${setClause}, updated_at = CURRENT_TIMESTAMP\n      WHERE id = $1\n      RETURNING *\n    `;\n\n    const { rows } = await this.db.query(query, [id, ...values]);\n    return rows[0] || null;\n  }\n\n  async delete(id: string): Promise<boolean> {\n    const query = 'DELETE FROM users WHERE id = $1';\n    const { rowCount } = await this.db.query(query, [id]);\n    return rowCount > 0;\n  }\n}\n```\n\n### Pattern 2: Dependency Injection\n\n**DI Container:**\n```typescript\n// di-container.ts\nimport { Pool } from 'pg';\nimport { UserRepository } from './repositories/user.repository';\nimport { UserService } from './services/user.service';\nimport { UserController } from './controllers/user.controller';\nimport { AuthService } from './services/auth.service';\n\nclass Container {\n  private instances = new Map<string, any>();\n\n  register<T>(key: string, factory: () => T): void {\n    this.instances.set(key, factory);\n  }\n\n  resolve<T>(key: string): T {\n    const factory = this.instances.get(key);\n    if (!factory) {\n      throw new Error(`No factory registered for ${key}`);\n    }\n    return factory();\n  }\n\n  singleton<T>(key: string, factory: () => T): void {\n    let instance: T;\n    this.instances.set(key, () => {\n      if (!instance) {\n        instance = factory();\n      }\n      return instance;\n    });\n  }\n}\n\nexport const container = new Container();\n\n// Register dependencies\ncontainer.singleton('db', () => new Pool({\n  host: process.env.DB_HOST,\n  port: parseInt(process.env.DB_PORT || '5432'),\n  database: process.env.DB_NAME,\n  user: process.env.DB_USER,\n  password: process.env.DB_PASSWORD,\n  max: 20,\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 2000,\n}));\n\ncontainer.singleton('userRepository', () =>\n  new UserRepository(container.resolve('db'))\n);\n\ncontainer.singleton('userService', () =>\n  new UserService(container.resolve('userRepository'))\n);\n\ncontainer.register('userController', () =>\n  new UserController(container.resolve('userService'))\n);\n\ncontainer.singleton('authService', () =>\n  new AuthService(container.resolve('userRepository'))\n);\n```\n\n## Middleware Patterns\n\n### Authentication Middleware\n\n```typescript\n// middleware/auth.middleware.ts\nimport { Request, Response, NextFunction } from 'express';\nimport jwt from 'jsonwebtoken';\nimport { UnauthorizedError } from '../utils/errors';\n\ninterface JWTPayload {\n  userId: string;\n  email: string;\n}\n\ndeclare global {\n  namespace Express {\n    interface Request {\n      user?: JWTPayload;\n    }\n  }\n}\n\nexport const authenticate = async (\n  req: Request,\n  res: Response,\n  next: NextFunction\n) => {\n  try {\n    const token = req.headers.authorization?.replace('Bearer ', '');\n\n    if (!token) {\n      throw new UnauthorizedError('No token provided');\n    }\n\n    const payload = jwt.verify(\n      token,\n      process.env.JWT_SECRET!\n    ) as JWTPayload;\n\n    req.user = payload;\n    next();\n  } catch (error) {\n    next(new UnauthorizedError('Invalid token'));\n  }\n};\n\nexport const authorize = (...roles: string[]) => {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    if (!req.user) {\n      return next(new UnauthorizedError('Not authenticated'));\n    }\n\n    // Check if user has required role\n    const hasRole = roles.some(role =>\n      req.user?.roles?.includes(role)\n    );\n\n    if (!hasRole) {\n      return next(new UnauthorizedError('Insufficient permissions'));\n    }\n\n    next();\n  };\n};\n```\n\n### Validation Middleware\n\n```typescript\n// middleware/validation.middleware.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { AnyZodObject, ZodError } from 'zod';\nimport { ValidationError } from '../utils/errors';\n\nexport const validate = (schema: AnyZodObject) => {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    try {\n      await schema.parseAsync({\n        body: req.body,\n        query: req.query,\n        params: req.params\n      });\n      next();\n    } catch (error) {\n      if (error instanceof ZodError) {\n        const errors = error.errors.map(err => ({\n          field: err.path.join('.'),\n          message: err.message\n        }));\n        next(new ValidationError('Validation failed', errors));\n      } else {\n        next(error);\n      }\n    }\n  };\n};\n\n// Usage with Zod\nimport { z } from 'zod';\n\nconst createUserSchema = z.object({\n  body: z.object({\n    name: z.string().min(1),\n    email: z.string().email(),\n    password: z.string().min(8)\n  })\n});\n\nrouter.post('/users', validate(createUserSchema), userController.createUser);\n```\n\n### Rate Limiting Middleware\n\n```typescript\n// middleware/rate-limit.middleware.ts\nimport rateLimit from 'express-rate-limit';\nimport RedisStore from 'rate-limit-redis';\nimport Redis from 'ioredis';\n\nconst redis = new Redis({\n  host: process.env.REDIS_HOST,\n  port: parseInt(process.env.REDIS_PORT || '6379')\n});\n\nexport const apiLimiter = rateLimit({\n  store: new RedisStore({\n    client: redis,\n    prefix: 'rl:',\n  }),\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // Limit each IP to 100 requests per windowMs\n  message: 'Too many requests from this IP, please try again later',\n  standardHeaders: true,\n  legacyHeaders: false,\n});\n\nexport const authLimiter = rateLimit({\n  store: new RedisStore({\n    client: redis,\n    prefix: 'rl:auth:',\n  }),\n  windowMs: 15 * 60 * 1000,\n  max: 5, // Stricter limit for auth endpoints\n  skipSuccessfulRequests: true,\n});\n```\n\n### Request Logging Middleware\n\n```typescript\n// middleware/logger.middleware.ts\nimport { Request, Response, NextFunction } from 'express';\nimport pino from 'pino';\n\nconst logger = pino({\n  level: process.env.LOG_LEVEL || 'info',\n  transport: {\n    target: 'pino-pretty',\n    options: { colorize: true }\n  }\n});\n\nexport const requestLogger = (\n  req: Request,\n  res: Response,\n  next: NextFunction\n) => {\n  const start = Date.now();\n\n  // Log response when finished\n  res.on('finish', () => {\n    const duration = Date.now() - start;\n    logger.info({\n      method: req.method,\n      url: req.url,\n      status: res.statusCode,\n      duration: `${duration}ms`,\n      userAgent: req.headers['user-agent'],\n      ip: req.ip\n    });\n  });\n\n  next();\n};\n\nexport { logger };\n```\n\n## Error Handling\n\n### Custom Error Classes\n\n```typescript\n// utils/errors.ts\nexport class AppError extends Error {\n  constructor(\n    public message: string,\n    public statusCode: number = 500,\n    public isOperational: boolean = true\n  ) {\n    super(message);\n    Object.setPrototypeOf(this, AppError.prototype);\n    Error.captureStackTrace(this, this.constructor);\n  }\n}\n\nexport class ValidationError extends AppError {\n  constructor(message: string, public errors?: any[]) {\n    super(message, 400);\n  }\n}\n\nexport class NotFoundError extends AppError {\n  constructor(message: string = 'Resource not found') {\n    super(message, 404);\n  }\n}\n\nexport class UnauthorizedError extends AppError {\n  constructor(message: string = 'Unauthorized') {\n    super(message, 401);\n  }\n}\n\nexport class ForbiddenError extends AppError {\n  constructor(message: string = 'Forbidden') {\n    super(message, 403);\n  }\n}\n\nexport class ConflictError extends AppError {\n  constructor(message: string) {\n    super(message, 409);\n  }\n}\n```\n\n### Global Error Handler\n\n```typescript\n// middleware/error-handler.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { AppError } from '../utils/errors';\nimport { logger } from './logger.middleware';\n\nexport const errorHandler = (\n  err: Error,\n  req: Request,\n  res: Response,\n  next: NextFunction\n) => {\n  if (err instanceof AppError) {\n    return res.status(err.statusCode).json({\n      status: 'error',\n      message: err.message,\n      ...(err instanceof ValidationError && { errors: err.errors })\n    });\n  }\n\n  // Log unexpected errors\n  logger.error({\n    error: err.message,\n    stack: err.stack,\n    url: req.url,\n    method: req.method\n  });\n\n  // Don't leak error details in production\n  const message = process.env.NODE_ENV === 'production'\n    ? 'Internal server error'\n    : err.message;\n\n  res.status(500).json({\n    status: 'error',\n    message\n  });\n};\n\n// Async error wrapper\nexport const asyncHandler = (\n  fn: (req: Request, res: Response, next: NextFunction) => Promise<any>\n) => {\n  return (req: Request, res: Response, next: NextFunction) => {\n    Promise.resolve(fn(req, res, next)).catch(next);\n  };\n};\n```\n\n## Database Patterns\n\n### PostgreSQL with Connection Pool\n\n```typescript\n// config/database.ts\nimport { Pool, PoolConfig } from 'pg';\n\nconst poolConfig: PoolConfig = {\n  host: process.env.DB_HOST,\n  port: parseInt(process.env.DB_PORT || '5432'),\n  database: process.env.DB_NAME,\n  user: process.env.DB_USER,\n  password: process.env.DB_PASSWORD,\n  max: 20,\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 2000,\n};\n\nexport const pool = new Pool(poolConfig);\n\n// Test connection\npool.on('connect', () => {\n  console.log('Database connected');\n});\n\npool.on('error', (err) => {\n  console.error('Unexpected database error', err);\n  process.exit(-1);\n});\n\n// Graceful shutdown\nexport const closeDatabase = async () => {\n  await pool.end();\n  console.log('Database connection closed');\n};\n```\n\n### MongoDB with Mongoose\n\n```typescript\n// config/mongoose.ts\nimport mongoose from 'mongoose';\n\nconst connectDB = async () => {\n  try {\n    await mongoose.connect(process.env.MONGODB_URI!, {\n      maxPoolSize: 10,\n      serverSelectionTimeoutMS: 5000,\n      socketTimeoutMS: 45000,\n    });\n\n    console.log('MongoDB connected');\n  } catch (error) {\n    console.error('MongoDB connection error:', error);\n    process.exit(1);\n  }\n};\n\nmongoose.connection.on('disconnected', () => {\n  console.log('MongoDB disconnected');\n});\n\nmongoose.connection.on('error', (err) => {\n  console.error('MongoDB error:', err);\n});\n\nexport { connectDB };\n\n// Model example\nimport { Schema, model, Document } from 'mongoose';\n\ninterface IUser extends Document {\n  name: string;\n  email: string;\n  password: string;\n  createdAt: Date;\n  updatedAt: Date;\n}\n\nconst userSchema = new Schema<IUser>({\n  name: { type: String, required: true },\n  email: { type: String, required: true, unique: true },\n  password: { type: String, required: true },\n}, {\n  timestamps: true\n});\n\n// Indexes\nuserSchema.index({ email: 1 });\n\nexport const User = model<IUser>('User', userSchema);\n```\n\n### Transaction Pattern\n\n```typescript\n// services/order.service.ts\nimport { Pool } from 'pg';\n\nexport class OrderService {\n  constructor(private db: Pool) {}\n\n  async createOrder(userId: string, items: any[]) {\n    const client = await this.db.connect();\n\n    try {\n      await client.query('BEGIN');\n\n      // Create order\n      const orderResult = await client.query(\n        'INSERT INTO orders (user_id, total) VALUES ($1, $2) RETURNING id',\n        [userId, calculateTotal(items)]\n      );\n      const orderId = orderResult.rows[0].id;\n\n      // Create order items\n      for (const item of items) {\n        await client.query(\n          'INSERT INTO order_items (order_id, product_id, quantity, price) VALUES ($1, $2, $3, $4)',\n          [orderId, item.productId, item.quantity, item.price]\n        );\n\n        // Update inventory\n        await client.query(\n          'UPDATE products SET stock = stock - $1 WHERE id = $2',\n          [item.quantity, item.productId]\n        );\n      }\n\n      await client.query('COMMIT');\n      return orderId;\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n}\n```\n\n## Authentication & Authorization\n\n### JWT Authentication\n\n```typescript\n// services/auth.service.ts\nimport jwt from 'jsonwebtoken';\nimport bcrypt from 'bcrypt';\nimport { UserRepository } from '../repositories/user.repository';\nimport { UnauthorizedError } from '../utils/errors';\n\nexport class AuthService {\n  constructor(private userRepository: UserRepository) {}\n\n  async login(email: string, password: string) {\n    const user = await this.userRepository.findByEmail(email);\n\n    if (!user) {\n      throw new UnauthorizedError('Invalid credentials');\n    }\n\n    const isValid = await bcrypt.compare(password, user.password);\n\n    if (!isValid) {\n      throw new UnauthorizedError('Invalid credentials');\n    }\n\n    const token = this.generateToken({\n      userId: user.id,\n      email: user.email\n    });\n\n    const refreshToken = this.generateRefreshToken({\n      userId: user.id\n    });\n\n    return {\n      token,\n      refreshToken,\n      user: {\n        id: user.id,\n        name: user.name,\n        email: user.email\n      }\n    };\n  }\n\n  async refreshToken(refreshToken: string) {\n    try {\n      const payload = jwt.verify(\n        refreshToken,\n        process.env.REFRESH_TOKEN_SECRET!\n      ) as { userId: string };\n\n      const user = await this.userRepository.findById(payload.userId);\n\n      if (!user) {\n        throw new UnauthorizedError('User not found');\n      }\n\n      const token = this.generateToken({\n        userId: user.id,\n        email: user.email\n      });\n\n      return { token };\n    } catch (error) {\n      throw new UnauthorizedError('Invalid refresh token');\n    }\n  }\n\n  private generateToken(payload: any): string {\n    return jwt.sign(payload, process.env.JWT_SECRET!, {\n      expiresIn: '15m'\n    });\n  }\n\n  private generateRefreshToken(payload: any): string {\n    return jwt.sign(payload, process.env.REFRESH_TOKEN_SECRET!, {\n      expiresIn: '7d'\n    });\n  }\n}\n```\n\n## Caching Strategies\n\n```typescript\n// utils/cache.ts\nimport Redis from 'ioredis';\n\nconst redis = new Redis({\n  host: process.env.REDIS_HOST,\n  port: parseInt(process.env.REDIS_PORT || '6379'),\n  retryStrategy: (times) => {\n    const delay = Math.min(times * 50, 2000);\n    return delay;\n  }\n});\n\nexport class CacheService {\n  async get<T>(key: string): Promise<T | null> {\n    const data = await redis.get(key);\n    return data ? JSON.parse(data) : null;\n  }\n\n  async set(key: string, value: any, ttl?: number): Promise<void> {\n    const serialized = JSON.stringify(value);\n    if (ttl) {\n      await redis.setex(key, ttl, serialized);\n    } else {\n      await redis.set(key, serialized);\n    }\n  }\n\n  async delete(key: string): Promise<void> {\n    await redis.del(key);\n  }\n\n  async invalidatePattern(pattern: string): Promise<void> {\n    const keys = await redis.keys(pattern);\n    if (keys.length > 0) {\n      await redis.del(...keys);\n    }\n  }\n}\n\n// Cache decorator\nexport function Cacheable(ttl: number = 300) {\n  return function (\n    target: any,\n    propertyKey: string,\n    descriptor: PropertyDescriptor\n  ) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      const cache = new CacheService();\n      const cacheKey = `${propertyKey}:${JSON.stringify(args)}`;\n\n      const cached = await cache.get(cacheKey);\n      if (cached) {\n        return cached;\n      }\n\n      const result = await originalMethod.apply(this, args);\n      await cache.set(cacheKey, result, ttl);\n\n      return result;\n    };\n\n    return descriptor;\n  };\n}\n```\n\n## API Response Format\n\n```typescript\n// utils/response.ts\nimport { Response } from 'express';\n\nexport class ApiResponse {\n  static success<T>(res: Response, data: T, message?: string, statusCode = 200) {\n    return res.status(statusCode).json({\n      status: 'success',\n      message,\n      data\n    });\n  }\n\n  static error(res: Response, message: string, statusCode = 500, errors?: any) {\n    return res.status(statusCode).json({\n      status: 'error',\n      message,\n      ...(errors && { errors })\n    });\n  }\n\n  static paginated<T>(\n    res: Response,\n    data: T[],\n    page: number,\n    limit: number,\n    total: number\n  ) {\n    return res.json({\n      status: 'success',\n      data,\n      pagination: {\n        page,\n        limit,\n        total,\n        pages: Math.ceil(total / limit)\n      }\n    });\n  }\n}\n```\n\n## Best Practices\n\n1. **Use TypeScript**: Type safety prevents runtime errors\n2. **Implement proper error handling**: Use custom error classes\n3. **Validate input**: Use libraries like Zod or Joi\n4. **Use environment variables**: Never hardcode secrets\n5. **Implement logging**: Use structured logging (Pino, Winston)\n6. **Add rate limiting**: Prevent abuse\n7. **Use HTTPS**: Always in production\n8. **Implement CORS properly**: Don't use `*` in production\n9. **Use dependency injection**: Easier testing and maintenance\n10. **Write tests**: Unit, integration, and E2E tests\n11. **Handle graceful shutdown**: Clean up resources\n12. **Use connection pooling**: For databases\n13. **Implement health checks**: For monitoring\n14. **Use compression**: Reduce response size\n15. **Monitor performance**: Use APM tools\n\n## Testing Patterns\n\nSee `javascript-testing-patterns` skill for comprehensive testing guidance.\n\n## Resources\n\n- **Node.js Best Practices**: https://github.com/goldbergyoni/nodebestpractices\n- **Express.js Guide**: https://expressjs.com/en/guide/\n- **Fastify Documentation**: https://www.fastify.io/docs/\n- **TypeScript Node Starter**: https://github.com/microsoft/TypeScript-Node-Starter"
              },
              {
                "name": "typescript-advanced-types",
                "description": "Master TypeScript's advanced type system including generics, conditional types, mapped types, template literals, and utility types for building type-safe applications. Use when implementing complex type logic, creating reusable type utilities, or ensuring compile-time type safety in TypeScript projects.",
                "path": "plugins/javascript-typescript/skills/typescript-advanced-types/SKILL.md",
                "frontmatter": {
                  "name": "typescript-advanced-types",
                  "description": "Master TypeScript's advanced type system including generics, conditional types, mapped types, template literals, and utility types for building type-safe applications. Use when implementing complex type logic, creating reusable type utilities, or ensuring compile-time type safety in TypeScript projects."
                },
                "content": "# TypeScript Advanced Types\n\nComprehensive guidance for mastering TypeScript's advanced type system including generics, conditional types, mapped types, template literal types, and utility types for building robust, type-safe applications.\n\n## When to Use This Skill\n\n- Building type-safe libraries or frameworks\n- Creating reusable generic components\n- Implementing complex type inference logic\n- Designing type-safe API clients\n- Building form validation systems\n- Creating strongly-typed configuration objects\n- Implementing type-safe state management\n- Migrating JavaScript codebases to TypeScript\n\n## Core Concepts\n\n### 1. Generics\n\n**Purpose:** Create reusable, type-flexible components while maintaining type safety.\n\n**Basic Generic Function:**\n```typescript\nfunction identity<T>(value: T): T {\n  return value;\n}\n\nconst num = identity<number>(42);        // Type: number\nconst str = identity<string>(\"hello\");    // Type: string\nconst auto = identity(true);              // Type inferred: boolean\n```\n\n**Generic Constraints:**\n```typescript\ninterface HasLength {\n  length: number;\n}\n\nfunction logLength<T extends HasLength>(item: T): T {\n  console.log(item.length);\n  return item;\n}\n\nlogLength(\"hello\");           // OK: string has length\nlogLength([1, 2, 3]);         // OK: array has length\nlogLength({ length: 10 });    // OK: object has length\n// logLength(42);             // Error: number has no length\n```\n\n**Multiple Type Parameters:**\n```typescript\nfunction merge<T, U>(obj1: T, obj2: U): T & U {\n  return { ...obj1, ...obj2 };\n}\n\nconst merged = merge(\n  { name: \"John\" },\n  { age: 30 }\n);\n// Type: { name: string } & { age: number }\n```\n\n### 2. Conditional Types\n\n**Purpose:** Create types that depend on conditions, enabling sophisticated type logic.\n\n**Basic Conditional Type:**\n```typescript\ntype IsString<T> = T extends string ? true : false;\n\ntype A = IsString<string>;    // true\ntype B = IsString<number>;    // false\n```\n\n**Extracting Return Types:**\n```typescript\ntype ReturnType<T> = T extends (...args: any[]) => infer R ? R : never;\n\nfunction getUser() {\n  return { id: 1, name: \"John\" };\n}\n\ntype User = ReturnType<typeof getUser>;\n// Type: { id: number; name: string; }\n```\n\n**Distributive Conditional Types:**\n```typescript\ntype ToArray<T> = T extends any ? T[] : never;\n\ntype StrOrNumArray = ToArray<string | number>;\n// Type: string[] | number[]\n```\n\n**Nested Conditions:**\n```typescript\ntype TypeName<T> =\n  T extends string ? \"string\" :\n  T extends number ? \"number\" :\n  T extends boolean ? \"boolean\" :\n  T extends undefined ? \"undefined\" :\n  T extends Function ? \"function\" :\n  \"object\";\n\ntype T1 = TypeName<string>;     // \"string\"\ntype T2 = TypeName<() => void>; // \"function\"\n```\n\n### 3. Mapped Types\n\n**Purpose:** Transform existing types by iterating over their properties.\n\n**Basic Mapped Type:**\n```typescript\ntype Readonly<T> = {\n  readonly [P in keyof T]: T[P];\n};\n\ninterface User {\n  id: number;\n  name: string;\n}\n\ntype ReadonlyUser = Readonly<User>;\n// Type: { readonly id: number; readonly name: string; }\n```\n\n**Optional Properties:**\n```typescript\ntype Partial<T> = {\n  [P in keyof T]?: T[P];\n};\n\ntype PartialUser = Partial<User>;\n// Type: { id?: number; name?: string; }\n```\n\n**Key Remapping:**\n```typescript\ntype Getters<T> = {\n  [K in keyof T as `get${Capitalize<string & K>}`]: () => T[K]\n};\n\ninterface Person {\n  name: string;\n  age: number;\n}\n\ntype PersonGetters = Getters<Person>;\n// Type: { getName: () => string; getAge: () => number; }\n```\n\n**Filtering Properties:**\n```typescript\ntype PickByType<T, U> = {\n  [K in keyof T as T[K] extends U ? K : never]: T[K]\n};\n\ninterface Mixed {\n  id: number;\n  name: string;\n  age: number;\n  active: boolean;\n}\n\ntype OnlyNumbers = PickByType<Mixed, number>;\n// Type: { id: number; age: number; }\n```\n\n### 4. Template Literal Types\n\n**Purpose:** Create string-based types with pattern matching and transformation.\n\n**Basic Template Literal:**\n```typescript\ntype EventName = \"click\" | \"focus\" | \"blur\";\ntype EventHandler = `on${Capitalize<EventName>}`;\n// Type: \"onClick\" | \"onFocus\" | \"onBlur\"\n```\n\n**String Manipulation:**\n```typescript\ntype UppercaseGreeting = Uppercase<\"hello\">;  // \"HELLO\"\ntype LowercaseGreeting = Lowercase<\"HELLO\">;  // \"hello\"\ntype CapitalizedName = Capitalize<\"john\">;    // \"John\"\ntype UncapitalizedName = Uncapitalize<\"John\">; // \"john\"\n```\n\n**Path Building:**\n```typescript\ntype Path<T> = T extends object\n  ? { [K in keyof T]: K extends string\n      ? `${K}` | `${K}.${Path<T[K]>}`\n      : never\n    }[keyof T]\n  : never;\n\ninterface Config {\n  server: {\n    host: string;\n    port: number;\n  };\n  database: {\n    url: string;\n  };\n}\n\ntype ConfigPath = Path<Config>;\n// Type: \"server\" | \"database\" | \"server.host\" | \"server.port\" | \"database.url\"\n```\n\n### 5. Utility Types\n\n**Built-in Utility Types:**\n\n```typescript\n// Partial<T> - Make all properties optional\ntype PartialUser = Partial<User>;\n\n// Required<T> - Make all properties required\ntype RequiredUser = Required<PartialUser>;\n\n// Readonly<T> - Make all properties readonly\ntype ReadonlyUser = Readonly<User>;\n\n// Pick<T, K> - Select specific properties\ntype UserName = Pick<User, \"name\" | \"email\">;\n\n// Omit<T, K> - Remove specific properties\ntype UserWithoutPassword = Omit<User, \"password\">;\n\n// Exclude<T, U> - Exclude types from union\ntype T1 = Exclude<\"a\" | \"b\" | \"c\", \"a\">;  // \"b\" | \"c\"\n\n// Extract<T, U> - Extract types from union\ntype T2 = Extract<\"a\" | \"b\" | \"c\", \"a\" | \"b\">;  // \"a\" | \"b\"\n\n// NonNullable<T> - Exclude null and undefined\ntype T3 = NonNullable<string | null | undefined>;  // string\n\n// Record<K, T> - Create object type with keys K and values T\ntype PageInfo = Record<\"home\" | \"about\", { title: string }>;\n```\n\n## Advanced Patterns\n\n### Pattern 1: Type-Safe Event Emitter\n\n```typescript\ntype EventMap = {\n  \"user:created\": { id: string; name: string };\n  \"user:updated\": { id: string };\n  \"user:deleted\": { id: string };\n};\n\nclass TypedEventEmitter<T extends Record<string, any>> {\n  private listeners: {\n    [K in keyof T]?: Array<(data: T[K]) => void>;\n  } = {};\n\n  on<K extends keyof T>(event: K, callback: (data: T[K]) => void): void {\n    if (!this.listeners[event]) {\n      this.listeners[event] = [];\n    }\n    this.listeners[event]!.push(callback);\n  }\n\n  emit<K extends keyof T>(event: K, data: T[K]): void {\n    const callbacks = this.listeners[event];\n    if (callbacks) {\n      callbacks.forEach(callback => callback(data));\n    }\n  }\n}\n\nconst emitter = new TypedEventEmitter<EventMap>();\n\nemitter.on(\"user:created\", (data) => {\n  console.log(data.id, data.name);  // Type-safe!\n});\n\nemitter.emit(\"user:created\", { id: \"1\", name: \"John\" });\n// emitter.emit(\"user:created\", { id: \"1\" });  // Error: missing 'name'\n```\n\n### Pattern 2: Type-Safe API Client\n\n```typescript\ntype HTTPMethod = \"GET\" | \"POST\" | \"PUT\" | \"DELETE\";\n\ntype EndpointConfig = {\n  \"/users\": {\n    GET: { response: User[] };\n    POST: { body: { name: string; email: string }; response: User };\n  };\n  \"/users/:id\": {\n    GET: { params: { id: string }; response: User };\n    PUT: { params: { id: string }; body: Partial<User>; response: User };\n    DELETE: { params: { id: string }; response: void };\n  };\n};\n\ntype ExtractParams<T> = T extends { params: infer P } ? P : never;\ntype ExtractBody<T> = T extends { body: infer B } ? B : never;\ntype ExtractResponse<T> = T extends { response: infer R } ? R : never;\n\nclass APIClient<Config extends Record<string, Record<HTTPMethod, any>>> {\n  async request<\n    Path extends keyof Config,\n    Method extends keyof Config[Path]\n  >(\n    path: Path,\n    method: Method,\n    ...[options]: ExtractParams<Config[Path][Method]> extends never\n      ? ExtractBody<Config[Path][Method]> extends never\n        ? []\n        : [{ body: ExtractBody<Config[Path][Method]> }]\n      : [{\n          params: ExtractParams<Config[Path][Method]>;\n          body?: ExtractBody<Config[Path][Method]>;\n        }]\n  ): Promise<ExtractResponse<Config[Path][Method]>> {\n    // Implementation here\n    return {} as any;\n  }\n}\n\nconst api = new APIClient<EndpointConfig>();\n\n// Type-safe API calls\nconst users = await api.request(\"/users\", \"GET\");\n// Type: User[]\n\nconst newUser = await api.request(\"/users\", \"POST\", {\n  body: { name: \"John\", email: \"john@example.com\" }\n});\n// Type: User\n\nconst user = await api.request(\"/users/:id\", \"GET\", {\n  params: { id: \"123\" }\n});\n// Type: User\n```\n\n### Pattern 3: Builder Pattern with Type Safety\n\n```typescript\ntype BuilderState<T> = {\n  [K in keyof T]: T[K] | undefined;\n};\n\ntype RequiredKeys<T> = {\n  [K in keyof T]-?: {} extends Pick<T, K> ? never : K;\n}[keyof T];\n\ntype OptionalKeys<T> = {\n  [K in keyof T]-?: {} extends Pick<T, K> ? K : never;\n}[keyof T];\n\ntype IsComplete<T, S> =\n  RequiredKeys<T> extends keyof S\n    ? S[RequiredKeys<T>] extends undefined\n      ? false\n      : true\n    : false;\n\nclass Builder<T, S extends BuilderState<T> = {}> {\n  private state: S = {} as S;\n\n  set<K extends keyof T>(\n    key: K,\n    value: T[K]\n  ): Builder<T, S & Record<K, T[K]>> {\n    this.state[key] = value;\n    return this as any;\n  }\n\n  build(\n    this: IsComplete<T, S> extends true ? this : never\n  ): T {\n    return this.state as T;\n  }\n}\n\ninterface User {\n  id: string;\n  name: string;\n  email: string;\n  age?: number;\n}\n\nconst builder = new Builder<User>();\n\nconst user = builder\n  .set(\"id\", \"1\")\n  .set(\"name\", \"John\")\n  .set(\"email\", \"john@example.com\")\n  .build();  // OK: all required fields set\n\n// const incomplete = builder\n//   .set(\"id\", \"1\")\n//   .build();  // Error: missing required fields\n```\n\n### Pattern 4: Deep Readonly/Partial\n\n```typescript\ntype DeepReadonly<T> = {\n  readonly [P in keyof T]: T[P] extends object\n    ? T[P] extends Function\n      ? T[P]\n      : DeepReadonly<T[P]>\n    : T[P];\n};\n\ntype DeepPartial<T> = {\n  [P in keyof T]?: T[P] extends object\n    ? T[P] extends Array<infer U>\n      ? Array<DeepPartial<U>>\n      : DeepPartial<T[P]>\n    : T[P];\n};\n\ninterface Config {\n  server: {\n    host: string;\n    port: number;\n    ssl: {\n      enabled: boolean;\n      cert: string;\n    };\n  };\n  database: {\n    url: string;\n    pool: {\n      min: number;\n      max: number;\n    };\n  };\n}\n\ntype ReadonlyConfig = DeepReadonly<Config>;\n// All nested properties are readonly\n\ntype PartialConfig = DeepPartial<Config>;\n// All nested properties are optional\n```\n\n### Pattern 5: Type-Safe Form Validation\n\n```typescript\ntype ValidationRule<T> = {\n  validate: (value: T) => boolean;\n  message: string;\n};\n\ntype FieldValidation<T> = {\n  [K in keyof T]?: ValidationRule<T[K]>[];\n};\n\ntype ValidationErrors<T> = {\n  [K in keyof T]?: string[];\n};\n\nclass FormValidator<T extends Record<string, any>> {\n  constructor(private rules: FieldValidation<T>) {}\n\n  validate(data: T): ValidationErrors<T> | null {\n    const errors: ValidationErrors<T> = {};\n    let hasErrors = false;\n\n    for (const key in this.rules) {\n      const fieldRules = this.rules[key];\n      const value = data[key];\n\n      if (fieldRules) {\n        const fieldErrors: string[] = [];\n\n        for (const rule of fieldRules) {\n          if (!rule.validate(value)) {\n            fieldErrors.push(rule.message);\n          }\n        }\n\n        if (fieldErrors.length > 0) {\n          errors[key] = fieldErrors;\n          hasErrors = true;\n        }\n      }\n    }\n\n    return hasErrors ? errors : null;\n  }\n}\n\ninterface LoginForm {\n  email: string;\n  password: string;\n}\n\nconst validator = new FormValidator<LoginForm>({\n  email: [\n    {\n      validate: (v) => v.includes(\"@\"),\n      message: \"Email must contain @\"\n    },\n    {\n      validate: (v) => v.length > 0,\n      message: \"Email is required\"\n    }\n  ],\n  password: [\n    {\n      validate: (v) => v.length >= 8,\n      message: \"Password must be at least 8 characters\"\n    }\n  ]\n});\n\nconst errors = validator.validate({\n  email: \"invalid\",\n  password: \"short\"\n});\n// Type: { email?: string[]; password?: string[]; } | null\n```\n\n### Pattern 6: Discriminated Unions\n\n```typescript\ntype Success<T> = {\n  status: \"success\";\n  data: T;\n};\n\ntype Error = {\n  status: \"error\";\n  error: string;\n};\n\ntype Loading = {\n  status: \"loading\";\n};\n\ntype AsyncState<T> = Success<T> | Error | Loading;\n\nfunction handleState<T>(state: AsyncState<T>): void {\n  switch (state.status) {\n    case \"success\":\n      console.log(state.data);  // Type: T\n      break;\n    case \"error\":\n      console.log(state.error);  // Type: string\n      break;\n    case \"loading\":\n      console.log(\"Loading...\");\n      break;\n  }\n}\n\n// Type-safe state machine\ntype State =\n  | { type: \"idle\" }\n  | { type: \"fetching\"; requestId: string }\n  | { type: \"success\"; data: any }\n  | { type: \"error\"; error: Error };\n\ntype Event =\n  | { type: \"FETCH\"; requestId: string }\n  | { type: \"SUCCESS\"; data: any }\n  | { type: \"ERROR\"; error: Error }\n  | { type: \"RESET\" };\n\nfunction reducer(state: State, event: Event): State {\n  switch (state.type) {\n    case \"idle\":\n      return event.type === \"FETCH\"\n        ? { type: \"fetching\", requestId: event.requestId }\n        : state;\n    case \"fetching\":\n      if (event.type === \"SUCCESS\") {\n        return { type: \"success\", data: event.data };\n      }\n      if (event.type === \"ERROR\") {\n        return { type: \"error\", error: event.error };\n      }\n      return state;\n    case \"success\":\n    case \"error\":\n      return event.type === \"RESET\" ? { type: \"idle\" } : state;\n  }\n}\n```\n\n## Type Inference Techniques\n\n### 1. Infer Keyword\n\n```typescript\n// Extract array element type\ntype ElementType<T> = T extends (infer U)[] ? U : never;\n\ntype NumArray = number[];\ntype Num = ElementType<NumArray>;  // number\n\n// Extract promise type\ntype PromiseType<T> = T extends Promise<infer U> ? U : never;\n\ntype AsyncNum = PromiseType<Promise<number>>;  // number\n\n// Extract function parameters\ntype Parameters<T> = T extends (...args: infer P) => any ? P : never;\n\nfunction foo(a: string, b: number) {}\ntype FooParams = Parameters<typeof foo>;  // [string, number]\n```\n\n### 2. Type Guards\n\n```typescript\nfunction isString(value: unknown): value is string {\n  return typeof value === \"string\";\n}\n\nfunction isArrayOf<T>(\n  value: unknown,\n  guard: (item: unknown) => item is T\n): value is T[] {\n  return Array.isArray(value) && value.every(guard);\n}\n\nconst data: unknown = [\"a\", \"b\", \"c\"];\n\nif (isArrayOf(data, isString)) {\n  data.forEach(s => s.toUpperCase());  // Type: string[]\n}\n```\n\n### 3. Assertion Functions\n\n```typescript\nfunction assertIsString(value: unknown): asserts value is string {\n  if (typeof value !== \"string\") {\n    throw new Error(\"Not a string\");\n  }\n}\n\nfunction processValue(value: unknown) {\n  assertIsString(value);\n  // value is now typed as string\n  console.log(value.toUpperCase());\n}\n```\n\n## Best Practices\n\n1. **Use `unknown` over `any`**: Enforce type checking\n2. **Prefer `interface` for object shapes**: Better error messages\n3. **Use `type` for unions and complex types**: More flexible\n4. **Leverage type inference**: Let TypeScript infer when possible\n5. **Create helper types**: Build reusable type utilities\n6. **Use const assertions**: Preserve literal types\n7. **Avoid type assertions**: Use type guards instead\n8. **Document complex types**: Add JSDoc comments\n9. **Use strict mode**: Enable all strict compiler options\n10. **Test your types**: Use type tests to verify type behavior\n\n## Type Testing\n\n```typescript\n// Type assertion tests\ntype AssertEqual<T, U> =\n  [T] extends [U]\n    ? [U] extends [T]\n      ? true\n      : false\n    : false;\n\ntype Test1 = AssertEqual<string, string>;        // true\ntype Test2 = AssertEqual<string, number>;        // false\ntype Test3 = AssertEqual<string | number, string>; // false\n\n// Expect error helper\ntype ExpectError<T extends never> = T;\n\n// Example usage\ntype ShouldError = ExpectError<AssertEqual<string, number>>;\n```\n\n## Common Pitfalls\n\n1. **Over-using `any`**: Defeats the purpose of TypeScript\n2. **Ignoring strict null checks**: Can lead to runtime errors\n3. **Too complex types**: Can slow down compilation\n4. **Not using discriminated unions**: Misses type narrowing opportunities\n5. **Forgetting readonly modifiers**: Allows unintended mutations\n6. **Circular type references**: Can cause compiler errors\n7. **Not handling edge cases**: Like empty arrays or null values\n\n## Performance Considerations\n\n- Avoid deeply nested conditional types\n- Use simple types when possible\n- Cache complex type computations\n- Limit recursion depth in recursive types\n- Use build tools to skip type checking in production\n\n## Resources\n\n- **TypeScript Handbook**: https://www.typescriptlang.org/docs/handbook/\n- **Type Challenges**: https://github.com/type-challenges/type-challenges\n- **TypeScript Deep Dive**: https://basarat.gitbook.io/typescript/\n- **Effective TypeScript**: Book by Dan Vanderkam"
              }
            ]
          },
          {
            "name": "git-pr-workflows",
            "description": "Git and PR workflows with code review and onboarding",
            "source": "./plugins/git-pr-workflows",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install git-pr-workflows@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/git-workflow",
                "description": null,
                "path": "plugins/git-pr-workflows/commands/git-workflow.md",
                "frontmatter": null,
                "content": "# Complete Git Workflow with Multi-Agent Orchestration\n\nOrchestrate a comprehensive git workflow from code review through PR creation, leveraging specialized agents for quality assurance, testing, and deployment readiness. This workflow implements modern git best practices including Conventional Commits, automated testing, and structured PR creation.\n\n[Extended thinking: This workflow coordinates multiple specialized agents to ensure code quality before commits are made. The code-reviewer agent performs initial quality checks, test-automator ensures all tests pass, and deployment-engineer verifies production readiness. By orchestrating these agents sequentially with context passing, we prevent broken code from entering the repository while maintaining high velocity. The workflow supports both trunk-based and feature-branch strategies with configurable options for different team needs.]\n\n## Configuration\n\n**Target branch**: $ARGUMENTS (defaults to 'main' if not specified)\n\n**Supported flags**:\n- `--skip-tests`: Skip automated test execution (use with caution)\n- `--draft-pr`: Create PR as draft for work-in-progress\n- `--no-push`: Perform all checks but don't push to remote\n- `--squash`: Squash commits before pushing\n- `--conventional`: Enforce Conventional Commits format strictly\n- `--trunk-based`: Use trunk-based development workflow\n- `--feature-branch`: Use feature branch workflow (default)\n\n## Phase 1: Pre-Commit Review and Analysis\n\n### 1. Code Quality Assessment\n- Use Task tool with subagent_type=\"code-reviewer\"\n- Prompt: \"Review all uncommitted changes for code quality issues. Check for: 1) Code style violations, 2) Security vulnerabilities, 3) Performance concerns, 4) Missing error handling, 5) Incomplete implementations. Generate a detailed report with severity levels (critical/high/medium/low) and provide specific line-by-line feedback. Output format: JSON with {issues: [], summary: {critical: 0, high: 0, medium: 0, low: 0}, recommendations: []}\"\n- Expected output: Structured code review report for next phase\n\n### 2. Dependency and Breaking Change Analysis\n- Use Task tool with subagent_type=\"code-reviewer\"\n- Prompt: \"Analyze the changes for: 1) New dependencies or version changes, 2) Breaking API changes, 3) Database schema modifications, 4) Configuration changes, 5) Backward compatibility issues. Context from previous review: [insert issues summary]. Identify any changes that require migration scripts or documentation updates.\"\n- Context from previous: Code quality issues that might indicate breaking changes\n- Expected output: Breaking change assessment and migration requirements\n\n## Phase 2: Testing and Validation\n\n### 1. Test Execution and Coverage\n- Use Task tool with subagent_type=\"unit-testing::test-automator\"\n- Prompt: \"Execute all test suites for the modified code. Run: 1) Unit tests, 2) Integration tests, 3) End-to-end tests if applicable. Generate coverage report and identify any untested code paths. Based on review issues: [insert critical/high issues], ensure tests cover the problem areas. Provide test results in format: {passed: [], failed: [], skipped: [], coverage: {statements: %, branches: %, functions: %, lines: %}, untested_critical_paths: []}\"\n- Context from previous: Critical code review issues that need test coverage\n- Expected output: Complete test results and coverage metrics\n\n### 2. Test Recommendations and Gap Analysis\n- Use Task tool with subagent_type=\"unit-testing::test-automator\"\n- Prompt: \"Based on test results [insert summary] and code changes, identify: 1) Missing test scenarios, 2) Edge cases not covered, 3) Integration points needing verification, 4) Performance benchmarks needed. Generate test implementation recommendations prioritized by risk. Consider the breaking changes identified: [insert breaking changes].\"\n- Context from previous: Test results, breaking changes, untested paths\n- Expected output: Prioritized list of additional tests needed\n\n## Phase 3: Commit Message Generation\n\n### 1. Change Analysis and Categorization\n- Use Task tool with subagent_type=\"code-reviewer\"\n- Prompt: \"Analyze all changes and categorize them according to Conventional Commits specification. Identify the primary change type (feat/fix/docs/style/refactor/perf/test/build/ci/chore/revert) and scope. For changes: [insert file list and summary], determine if this should be a single commit or multiple atomic commits. Consider test results: [insert test summary].\"\n- Context from previous: Test results, code review summary\n- Expected output: Commit structure recommendation\n\n### 2. Conventional Commit Message Creation\n- Use Task tool with subagent_type=\"llm-application-dev::prompt-engineer\"\n- Prompt: \"Create Conventional Commits format message(s) based on categorization: [insert categorization]. Format: <type>(<scope>): <subject> with blank line then <body> explaining what and why (not how), then <footer> with BREAKING CHANGE: if applicable. Include: 1) Clear subject line (50 chars max), 2) Detailed body explaining rationale, 3) References to issues/tickets, 4) Co-authors if applicable. Consider the impact: [insert breaking changes if any].\"\n- Context from previous: Change categorization, breaking changes\n- Expected output: Properly formatted commit message(s)\n\n## Phase 4: Branch Strategy and Push Preparation\n\n### 1. Branch Management\n- Use Task tool with subagent_type=\"cicd-automation::deployment-engineer\"\n- Prompt: \"Based on workflow type [--trunk-based or --feature-branch], prepare branch strategy. For feature branch: ensure branch name follows pattern (feature|bugfix|hotfix)/<ticket>-<description>. For trunk-based: prepare for direct main push with feature flag strategy if needed. Current branch: [insert branch], target: [insert target branch]. Verify no conflicts with target branch.\"\n- Expected output: Branch preparation commands and conflict status\n\n### 2. Pre-Push Validation\n- Use Task tool with subagent_type=\"cicd-automation::deployment-engineer\"\n- Prompt: \"Perform final pre-push checks: 1) Verify all CI checks will pass, 2) Confirm no sensitive data in commits, 3) Validate commit signatures if required, 4) Check branch protection rules, 5) Ensure all review comments addressed. Test summary: [insert test results]. Review status: [insert review summary].\"\n- Context from previous: All previous validation results\n- Expected output: Push readiness confirmation or blocking issues\n\n## Phase 5: Pull Request Creation\n\n### 1. PR Description Generation\n- Use Task tool with subagent_type=\"documentation-generation::docs-architect\"\n- Prompt: \"Create comprehensive PR description including: 1) Summary of changes (what and why), 2) Type of change checklist, 3) Testing performed summary from [insert test results], 4) Screenshots/recordings if UI changes, 5) Deployment notes from [insert deployment considerations], 6) Related issues/tickets, 7) Breaking changes section if applicable: [insert breaking changes], 8) Reviewer checklist. Format as GitHub-flavored Markdown.\"\n- Context from previous: All validation results, test outcomes, breaking changes\n- Expected output: Complete PR description in Markdown\n\n### 2. PR Metadata and Automation Setup\n- Use Task tool with subagent_type=\"cicd-automation::deployment-engineer\"\n- Prompt: \"Configure PR metadata: 1) Assign appropriate reviewers based on CODEOWNERS, 2) Add labels (type, priority, component), 3) Link related issues, 4) Set milestone if applicable, 5) Configure merge strategy (squash/merge/rebase), 6) Set up auto-merge if all checks pass. Consider draft status: [--draft-pr flag]. Include test status: [insert test summary].\"\n- Context from previous: PR description, test results, review status\n- Expected output: PR configuration commands and automation rules\n\n## Success Criteria\n\n- âœ… All critical and high-severity code issues resolved\n- âœ… Test coverage maintained or improved (target: >80%)\n- âœ… All tests passing (unit, integration, e2e)\n- âœ… Commit messages follow Conventional Commits format\n- âœ… No merge conflicts with target branch\n- âœ… PR description complete with all required sections\n- âœ… Branch protection rules satisfied\n- âœ… Security scanning completed with no critical vulnerabilities\n- âœ… Performance benchmarks within acceptable thresholds\n- âœ… Documentation updated for any API changes\n\n## Rollback Procedures\n\nIn case of issues after merge:\n\n1. **Immediate Revert**: Create revert PR with `git revert <commit-hash>`\n2. **Feature Flag Disable**: If using feature flags, disable immediately\n3. **Hotfix Branch**: For critical issues, create hotfix branch from main\n4. **Communication**: Notify team via designated channels\n5. **Root Cause Analysis**: Document issue in postmortem template\n\n## Best Practices Reference\n\n- **Commit Frequency**: Commit early and often, but ensure each commit is atomic\n- **Branch Naming**: `(feature|bugfix|hotfix|docs|chore)/<ticket-id>-<brief-description>`\n- **PR Size**: Keep PRs under 400 lines for effective review\n- **Review Response**: Address review comments within 24 hours\n- **Merge Strategy**: Squash for feature branches, merge for release branches\n- **Sign-Off**: Require at least 2 approvals for main branch changes"
              },
              {
                "name": "/onboard",
                "description": null,
                "path": "plugins/git-pr-workflows/commands/onboard.md",
                "frontmatter": null,
                "content": "# Onboard\n\nYou are an **expert onboarding specialist and knowledge transfer architect** with deep experience in remote-first organizations, technical team integration, and accelerated learning methodologies. Your role is to ensure smooth, comprehensive onboarding that transforms new team members into productive contributors while preserving institutional knowledge.\n\n## Context\n\nThis tool orchestrates the complete onboarding experience for new team members, from pre-arrival preparation through their first 90 days. It creates customized onboarding plans based on role, seniority, location, and team structure, ensuring both technical proficiency and cultural integration. The tool emphasizes documentation, mentorship, and measurable milestones to track onboarding success.\n\n## Requirements\n\nYou are given the following context:\n$ARGUMENTS\n\nParse the arguments to understand:\n- **Role details**: Position title, level, team, reporting structure\n- **Start date**: When the new hire begins\n- **Location**: Remote, hybrid, or on-site specifics\n- **Technical requirements**: Languages, frameworks, tools needed\n- **Team context**: Size, distribution, working patterns\n- **Special considerations**: Fast-track needs, domain expertise required\n\n## Pre-Onboarding Preparation\n\nBefore the new hire's first day, ensure complete readiness:\n\n1. **Access and Accounts Setup**\n   - Create all necessary accounts (email, Slack, GitHub, AWS, etc.)\n   - Configure SSO and 2FA requirements\n   - Prepare hardware (laptop, monitors, peripherals) with shipping tracking\n   - Generate temporary credentials and password manager setup guide\n   - Schedule IT support session for Day 1\n\n2. **Documentation Preparation**\n   - Compile role-specific documentation package\n   - Update team roster and org charts\n   - Prepare personalized onboarding checklist\n   - Create welcome packet with company handbook, benefits guide\n   - Record welcome videos from team members\n\n3. **Workspace Configuration**\n   - For remote: Verify home office setup requirements and stipend\n   - For on-site: Assign desk, access badges, parking\n   - Order business cards and nameplate\n   - Configure calendar with initial meetings\n\n## Day 1 Orientation and Setup\n\nFirst day focus on warmth, clarity, and essential setup:\n\n1. **Welcome and Orientation (Morning)**\n   - Manager 1:1 welcome (30 min)\n   - Company mission, values, and culture overview (45 min)\n   - Team introductions and virtual coffee chats\n   - Role expectations and success criteria discussion\n   - Review of first-week schedule\n\n2. **Technical Setup (Afternoon)**\n   - IT-guided laptop configuration\n   - Development environment initial setup\n   - Password manager and security tools\n   - Communication tools (Slack workspaces, channels)\n   - Calendar and meeting tools configuration\n\n3. **Administrative Completion**\n   - HR paperwork and benefits enrollment\n   - Emergency contact information\n   - Photo for directory and badge\n   - Expense and timesheet system training\n\n## Week 1 Codebase Immersion\n\nSystematic introduction to technical landscape:\n\n1. **Repository Orientation**\n   - Architecture overview and system diagrams\n   - Main repositories walkthrough with tech lead\n   - Development workflow and branching strategy\n   - Code style guides and conventions\n   - Testing philosophy and coverage requirements\n\n2. **Development Practices**\n   - Pull request process and review culture\n   - CI/CD pipeline introduction\n   - Deployment procedures and environments\n   - Monitoring and logging systems tour\n   - Incident response procedures\n\n3. **First Code Contributions**\n   - Identify \"good first issues\" labeled tasks\n   - Pair programming session on simple fix\n   - Submit first PR with buddy guidance\n   - Participate in first code review\n\n## Development Environment Setup\n\nComplete configuration for productive development:\n\n1. **Local Environment**\n   ```\n   - IDE/Editor setup (VSCode, IntelliJ, Vim)\n   - Extensions and plugins installation\n   - Linters, formatters, and code quality tools\n   - Debugger configuration\n   - Git configuration and SSH keys\n   ```\n\n2. **Service Access**\n   - Database connections and read-only access\n   - API keys and service credentials (via secrets manager)\n   - Staging and development environment access\n   - Monitoring dashboard permissions\n   - Documentation wiki edit rights\n\n3. **Toolchain Mastery**\n   - Build tool configuration (npm, gradle, make)\n   - Container setup (Docker, Kubernetes access)\n   - Testing framework familiarization\n   - Performance profiling tools\n   - Security scanning integration\n\n## Team Integration and Culture\n\nBuilding relationships and understanding team dynamics:\n\n1. **Buddy System Implementation**\n   - Assign dedicated onboarding buddy for 30 days\n   - Daily check-ins for first week (15 min)\n   - Weekly sync meetings thereafter\n   - Buddy responsibility checklist and training\n   - Feedback channel for concerns\n\n2. **Team Immersion Activities**\n   - Shadow team ceremonies (standups, retros, planning)\n   - 1:1 meetings with each team member (30 min each)\n   - Cross-functional introductions (Product, Design, QA)\n   - Virtual lunch sessions or coffee chats\n   - Team traditions and social channels participation\n\n3. **Communication Norms**\n   - Slack etiquette and channel purposes\n   - Meeting culture and documentation practices\n   - Async communication expectations\n   - Time zone considerations and core hours\n   - Escalation paths and decision-making process\n\n## Learning Resources and Documentation\n\nCurated learning paths for role proficiency:\n\n1. **Technical Learning Path**\n   - Domain-specific courses and certifications\n   - Internal tech talks and brown bags library\n   - Recommended books and articles\n   - Conference talk recordings\n   - Hands-on labs and sandboxes\n\n2. **Product Knowledge**\n   - Product demos and user journey walkthroughs\n   - Customer personas and use cases\n   - Competitive landscape overview\n   - Roadmap and vision presentations\n   - Feature flag experiments participation\n\n3. **Knowledge Management**\n   - Documentation contribution guidelines\n   - Wiki navigation and search tips\n   - Runbook creation and maintenance\n   - ADR (Architecture Decision Records) process\n   - Knowledge sharing expectations\n\n## Milestone Tracking and Check-ins\n\nStructured progress monitoring and feedback:\n\n1. **30-Day Milestone**\n   - Complete all mandatory training\n   - Merge at least 3 pull requests\n   - Document one process or system\n   - Present learnings to team (10 min)\n   - Manager feedback session and adjustment\n\n2. **60-Day Milestone**\n   - Own a small feature end-to-end\n   - Participate in on-call rotation shadow\n   - Contribute to technical design discussion\n   - Establish working relationships across teams\n   - Self-assessment and goal setting\n\n3. **90-Day Milestone**\n   - Independent feature delivery\n   - Active code review participation\n   - Mentor a newer team member\n   - Propose process improvement\n   - Performance review and permanent role confirmation\n\n## Feedback Loops and Continuous Improvement\n\nEnsuring onboarding effectiveness and iteration:\n\n1. **Feedback Collection**\n   - Weekly pulse surveys (5 questions)\n   - Buddy feedback forms\n   - Manager 1:1 structured questions\n   - Anonymous feedback channel option\n   - Exit interviews for onboarding gaps\n\n2. **Onboarding Metrics**\n   - Time to first commit\n   - Time to first production deploy\n   - Ramp-up velocity tracking\n   - Knowledge retention assessments\n   - Team integration satisfaction scores\n\n3. **Program Refinement**\n   - Quarterly onboarding retrospectives\n   - Success story documentation\n   - Failure pattern analysis\n   - Onboarding handbook updates\n   - Buddy program training improvements\n\n## Example Plans\n\n### Software Engineer Onboarding (30/60/90 Day Plan)\n\n**Pre-Start (1 week before)**\n- [ ] Laptop shipped with tracking confirmation\n- [ ] Accounts created: GitHub, Slack, Jira, AWS\n- [ ] Welcome email with Day 1 agenda sent\n- [ ] Buddy assigned and introduced via email\n- [ ] Manager prep: role doc, first tasks identified\n\n**Day 1-7: Foundation**\n- [ ] IT setup and security training (Day 1)\n- [ ] Team introductions and role overview (Day 1)\n- [ ] Development environment setup (Day 2-3)\n- [ ] First PR merged (good first issue) (Day 4-5)\n- [ ] Architecture overview sessions (Day 5-7)\n- [ ] Daily buddy check-ins (15 min)\n\n**Week 2-4: Immersion**\n- [ ] Complete 5+ PR reviews as observer\n- [ ] Shadow senior engineer for 1 full day\n- [ ] Attend all team ceremonies\n- [ ] Complete product deep-dive sessions\n- [ ] Document one unclear process\n- [ ] Set up local development for all services\n\n**Day 30 Checkpoint:**\n- 10+ commits merged\n- All onboarding modules complete\n- Team relationships established\n- Development environment fully functional\n- First bug fix deployed to production\n\n**Day 31-60: Contribution**\n- [ ] Own first small feature (2-3 day effort)\n- [ ] Participate in technical design review\n- [ ] Shadow on-call engineer for 1 shift\n- [ ] Present tech talk on previous experience\n- [ ] Pair program with 3+ team members\n- [ ] Contribute to team documentation\n\n**Day 60 Checkpoint:**\n- First feature shipped to production\n- Active in code reviews (giving feedback)\n- On-call ready (shadowing complete)\n- Technical documentation contributed\n- Cross-team relationships building\n\n**Day 61-90: Integration**\n- [ ] Lead a small project independently\n- [ ] Participate in planning and estimation\n- [ ] Handle on-call issues with supervision\n- [ ] Mentor newer team member\n- [ ] Propose one process improvement\n- [ ] Build relationship with product/design\n\n**Day 90 Final Review:**\n- Fully autonomous on team tasks\n- Actively contributing to team culture\n- On-call rotation ready\n- Mentoring capabilities demonstrated\n- Process improvements identified\n\n### Remote Employee Onboarding (Distributed Team)\n\n**Week 0: Pre-Boarding**\n- [ ] Home office stipend processed ($1,500)\n- [ ] Equipment ordered: laptop, monitor, desk accessories\n- [ ] Welcome package sent: swag, notebook, coffee\n- [ ] Virtual team lunch scheduled for Day 1\n- [ ] Time zone preferences documented\n\n**Week 1: Virtual Integration**\n- [ ] Day 1: Virtual welcome breakfast with team\n- [ ] Timezone-friendly meeting schedule created\n- [ ] Slack presence hours established\n- [ ] Virtual office tour and tool walkthrough\n- [ ] Async communication norms training\n- [ ] Daily \"coffee chats\" with different team members\n\n**Week 2-4: Remote Collaboration**\n- [ ] Pair programming sessions across timezones\n- [ ] Async code review participation\n- [ ] Documentation of working hours and availability\n- [ ] Virtual whiteboarding session participation\n- [ ] Recording of important sessions for replay\n- [ ] Contribution to team wiki and runbooks\n\n**Ongoing Remote Success:**\n- Weekly 1:1 video calls with manager\n- Monthly virtual team social events\n- Quarterly in-person team gathering (if possible)\n- Clear async communication protocols\n- Documented decision-making process\n- Regular feedback on remote experience\n\n### Senior/Lead Engineer Onboarding (Accelerated)\n\n**Week 1: Rapid Immersion**\n- [ ] Day 1: Leadership team introductions\n- [ ] Day 2: Full system architecture deep-dive\n- [ ] Day 3: Current challenges and priorities briefing\n- [ ] Day 4: Codebase archaeology with principal engineer\n- [ ] Day 5: Stakeholder meetings (Product, Design, QA)\n- [ ] End of week: Initial observations documented\n\n**Week 2-3: Assessment and Planning**\n- [ ] Review last quarter's postmortems\n- [ ] Analyze technical debt backlog\n- [ ] Audit current team processes\n- [ ] Identify quick wins (1-week improvements)\n- [ ] Begin relationship building with other teams\n- [ ] Propose initial technical improvements\n\n**Week 4: Taking Ownership**\n- [ ] Lead first team ceremony (retro or planning)\n- [ ] Own critical technical decision\n- [ ] Establish 1:1 cadence with team members\n- [ ] Define technical vision alignment\n- [ ] Start mentoring program participation\n- [ ] Submit first major architectural proposal\n\n**30-Day Deliverables:**\n- Technical assessment document\n- Team process improvement plan\n- Relationship map established\n- First major PR merged\n- Technical roadmap contribution\n\n## Reference Examples\n\n### Complete Day 1 Checklist\n\n**Morning (9:00 AM - 12:00 PM)**\n```checklist\n- [ ] Manager welcome and agenda review (30 min)\n- [ ] HR benefits and paperwork (45 min)\n- [ ] Company culture presentation (30 min)\n- [ ] Team standup observation (15 min)\n- [ ] Break and informal chat (30 min)\n- [ ] Security training and 2FA setup (30 min)\n```\n\n**Afternoon (1:00 PM - 5:00 PM)**\n```checklist\n- [ ] Lunch with buddy and team (60 min)\n- [ ] Laptop setup with IT support (90 min)\n- [ ] Slack and communication tools (30 min)\n- [ ] First Git commit ceremony (30 min)\n- [ ] Team happy hour or social (30 min)\n- [ ] Day 1 feedback survey (10 min)\n```\n\n### Buddy Responsibility Matrix\n\n| Week | Frequency | Activities | Time Commitment |\n|------|-----------|------------|----------------|\n| 1 | Daily | Morning check-in, pair programming, question answering | 2 hours/day |\n| 2-3 | 3x/week | Code review together, architecture discussions, social lunch | 1 hour/day |\n| 4 | 2x/week | Project collaboration, introduction facilitation | 30 min/day |\n| 5-8 | Weekly | Progress check-in, career development chat | 1 hour/week |\n| 9-12 | Bi-weekly | Mentorship transition, success celebration | 30 min/week |\n\n## Execution Guidelines\n\n1. **Customize based on context**: Adapt the plan based on role, seniority, and team needs\n2. **Document everything**: Create artifacts that can be reused for future onboarding\n3. **Measure success**: Track metrics and gather feedback continuously\n4. **Iterate rapidly**: Adjust the plan based on what's working\n5. **Prioritize connection**: Technical skills matter, but team integration is crucial\n6. **Maintain momentum**: Keep the new hire engaged and progressing daily\n\nRemember: Great onboarding reduces time-to-productivity from months to weeks while building lasting engagement and retention."
              },
              {
                "name": "/pr-enhance",
                "description": null,
                "path": "plugins/git-pr-workflows/commands/pr-enhance.md",
                "frontmatter": null,
                "content": "# Pull Request Enhancement\n\nYou are a PR optimization expert specializing in creating high-quality pull requests that facilitate efficient code reviews. Generate comprehensive PR descriptions, automate review processes, and ensure PRs follow best practices for clarity, size, and reviewability.\n\n## Context\nThe user needs to create or improve pull requests with detailed descriptions, proper documentation, test coverage analysis, and review facilitation. Focus on making PRs that are easy to review, well-documented, and include all necessary context.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. PR Analysis\n\nAnalyze the changes and generate insights:\n\n**Change Summary Generator**\n```python\nimport subprocess\nimport re\nfrom collections import defaultdict\n\nclass PRAnalyzer:\n    def analyze_changes(self, base_branch='main'):\n        \"\"\"\n        Analyze changes between current branch and base\n        \"\"\"\n        analysis = {\n            'files_changed': self._get_changed_files(base_branch),\n            'change_statistics': self._get_change_stats(base_branch),\n            'change_categories': self._categorize_changes(base_branch),\n            'potential_impacts': self._assess_impacts(base_branch),\n            'dependencies_affected': self._check_dependencies(base_branch)\n        }\n        \n        return analysis\n    \n    def _get_changed_files(self, base_branch):\n        \"\"\"Get list of changed files with statistics\"\"\"\n        cmd = f\"git diff --name-status {base_branch}...HEAD\"\n        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n        \n        files = []\n        for line in result.stdout.strip().split('\\n'):\n            if line:\n                status, filename = line.split('\\t', 1)\n                files.append({\n                    'filename': filename,\n                    'status': self._parse_status(status),\n                    'category': self._categorize_file(filename)\n                })\n        \n        return files\n    \n    def _get_change_stats(self, base_branch):\n        \"\"\"Get detailed change statistics\"\"\"\n        cmd = f\"git diff --shortstat {base_branch}...HEAD\"\n        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n        \n        # Parse output like: \"10 files changed, 450 insertions(+), 123 deletions(-)\"\n        stats_pattern = r'(\\d+) files? changed(?:, (\\d+) insertions?\\(\\+\\))?(?:, (\\d+) deletions?\\(-\\))?'\n        match = re.search(stats_pattern, result.stdout)\n        \n        if match:\n            files, insertions, deletions = match.groups()\n            return {\n                'files_changed': int(files),\n                'insertions': int(insertions or 0),\n                'deletions': int(deletions or 0),\n                'net_change': int(insertions or 0) - int(deletions or 0)\n            }\n        \n        return {'files_changed': 0, 'insertions': 0, 'deletions': 0, 'net_change': 0}\n    \n    def _categorize_file(self, filename):\n        \"\"\"Categorize file by type\"\"\"\n        categories = {\n            'source': ['.js', '.ts', '.py', '.java', '.go', '.rs'],\n            'test': ['test', 'spec', '.test.', '.spec.'],\n            'config': ['config', '.json', '.yml', '.yaml', '.toml'],\n            'docs': ['.md', 'README', 'CHANGELOG', '.rst'],\n            'styles': ['.css', '.scss', '.less'],\n            'build': ['Makefile', 'Dockerfile', '.gradle', 'pom.xml']\n        }\n        \n        for category, patterns in categories.items():\n            if any(pattern in filename for pattern in patterns):\n                return category\n        \n        return 'other'\n```\n\n### 2. PR Description Generation\n\nCreate comprehensive PR descriptions:\n\n**Description Template Generator**\n```python\ndef generate_pr_description(analysis, commits):\n    \"\"\"\n    Generate detailed PR description from analysis\n    \"\"\"\n    description = f\"\"\"\n## Summary\n\n{generate_summary(analysis, commits)}\n\n## What Changed\n\n{generate_change_list(analysis)}\n\n## Why These Changes\n\n{extract_why_from_commits(commits)}\n\n## Type of Change\n\n{determine_change_types(analysis)}\n\n## How Has This Been Tested?\n\n{generate_test_section(analysis)}\n\n## Visual Changes\n\n{generate_visual_section(analysis)}\n\n## Performance Impact\n\n{analyze_performance_impact(analysis)}\n\n## Breaking Changes\n\n{identify_breaking_changes(analysis)}\n\n## Dependencies\n\n{list_dependency_changes(analysis)}\n\n## Checklist\n\n{generate_review_checklist(analysis)}\n\n## Additional Notes\n\n{generate_additional_notes(analysis)}\n\"\"\"\n    return description\n\ndef generate_summary(analysis, commits):\n    \"\"\"Generate executive summary\"\"\"\n    stats = analysis['change_statistics']\n    \n    # Extract main purpose from commits\n    main_purpose = extract_main_purpose(commits)\n    \n    summary = f\"\"\"\nThis PR {main_purpose}.\n\n**Impact**: {stats['files_changed']} files changed ({stats['insertions']} additions, {stats['deletions']} deletions)\n**Risk Level**: {calculate_risk_level(analysis)}\n**Review Time**: ~{estimate_review_time(stats)} minutes\n\"\"\"\n    return summary\n\ndef generate_change_list(analysis):\n    \"\"\"Generate categorized change list\"\"\"\n    changes_by_category = defaultdict(list)\n    \n    for file in analysis['files_changed']:\n        changes_by_category[file['category']].append(file)\n    \n    change_list = \"\"\n    icons = {\n        'source': 'ðŸ”§',\n        'test': 'âœ…',\n        'docs': 'ðŸ“',\n        'config': 'âš™ï¸',\n        'styles': 'ðŸŽ¨',\n        'build': 'ðŸ—ï¸',\n        'other': 'ðŸ“'\n    }\n    \n    for category, files in changes_by_category.items():\n        change_list += f\"\\n### {icons.get(category, 'ðŸ“')} {category.title()} Changes\\n\"\n        for file in files[:10]:  # Limit to 10 files per category\n            change_list += f\"- {file['status']}: `{file['filename']}`\\n\"\n        if len(files) > 10:\n            change_list += f\"- ...and {len(files) - 10} more\\n\"\n    \n    return change_list\n```\n\n### 3. Review Checklist Generation\n\nCreate automated review checklists:\n\n**Smart Checklist Generator**\n```python\ndef generate_review_checklist(analysis):\n    \"\"\"\n    Generate context-aware review checklist\n    \"\"\"\n    checklist = [\"## Review Checklist\\n\"]\n    \n    # General items\n    general_items = [\n        \"Code follows project style guidelines\",\n        \"Self-review completed\",\n        \"Comments added for complex logic\",\n        \"No debugging code left\",\n        \"No sensitive data exposed\"\n    ]\n    \n    # Add general items\n    checklist.append(\"### General\")\n    for item in general_items:\n        checklist.append(f\"- [ ] {item}\")\n    \n    # File-specific checks\n    file_types = {file['category'] for file in analysis['files_changed']}\n    \n    if 'source' in file_types:\n        checklist.append(\"\\n### Code Quality\")\n        checklist.extend([\n            \"- [ ] No code duplication\",\n            \"- [ ] Functions are focused and small\",\n            \"- [ ] Variable names are descriptive\",\n            \"- [ ] Error handling is comprehensive\",\n            \"- [ ] No performance bottlenecks introduced\"\n        ])\n    \n    if 'test' in file_types:\n        checklist.append(\"\\n### Testing\")\n        checklist.extend([\n            \"- [ ] All new code is covered by tests\",\n            \"- [ ] Tests are meaningful and not just for coverage\",\n            \"- [ ] Edge cases are tested\",\n            \"- [ ] Tests follow AAA pattern (Arrange, Act, Assert)\",\n            \"- [ ] No flaky tests introduced\"\n        ])\n    \n    if 'config' in file_types:\n        checklist.append(\"\\n### Configuration\")\n        checklist.extend([\n            \"- [ ] No hardcoded values\",\n            \"- [ ] Environment variables documented\",\n            \"- [ ] Backwards compatibility maintained\",\n            \"- [ ] Security implications reviewed\",\n            \"- [ ] Default values are sensible\"\n        ])\n    \n    if 'docs' in file_types:\n        checklist.append(\"\\n### Documentation\")\n        checklist.extend([\n            \"- [ ] Documentation is clear and accurate\",\n            \"- [ ] Examples are provided where helpful\",\n            \"- [ ] API changes are documented\",\n            \"- [ ] README updated if necessary\",\n            \"- [ ] Changelog updated\"\n        ])\n    \n    # Security checks\n    if has_security_implications(analysis):\n        checklist.append(\"\\n### Security\")\n        checklist.extend([\n            \"- [ ] No SQL injection vulnerabilities\",\n            \"- [ ] Input validation implemented\",\n            \"- [ ] Authentication/authorization correct\",\n            \"- [ ] No sensitive data in logs\",\n            \"- [ ] Dependencies are secure\"\n        ])\n    \n    return '\\n'.join(checklist)\n```\n\n### 4. Code Review Automation\n\nAutomate common review tasks:\n\n**Automated Review Bot**\n```python\nclass ReviewBot:\n    def perform_automated_checks(self, pr_diff):\n        \"\"\"\n        Perform automated code review checks\n        \"\"\"\n        findings = []\n        \n        # Check for common issues\n        checks = [\n            self._check_console_logs,\n            self._check_commented_code,\n            self._check_large_functions,\n            self._check_todo_comments,\n            self._check_hardcoded_values,\n            self._check_missing_error_handling,\n            self._check_security_issues\n        ]\n        \n        for check in checks:\n            findings.extend(check(pr_diff))\n        \n        return findings\n    \n    def _check_console_logs(self, diff):\n        \"\"\"Check for console.log statements\"\"\"\n        findings = []\n        pattern = r'\\+.*console\\.(log|debug|info|warn|error)'\n        \n        for file, content in diff.items():\n            matches = re.finditer(pattern, content, re.MULTILINE)\n            for match in matches:\n                findings.append({\n                    'type': 'warning',\n                    'file': file,\n                    'line': self._get_line_number(match, content),\n                    'message': 'Console statement found - remove before merging',\n                    'suggestion': 'Use proper logging framework instead'\n                })\n        \n        return findings\n    \n    def _check_large_functions(self, diff):\n        \"\"\"Check for functions that are too large\"\"\"\n        findings = []\n        \n        # Simple heuristic: count lines between function start and end\n        for file, content in diff.items():\n            if file.endswith(('.js', '.ts', '.py')):\n                functions = self._extract_functions(content)\n                for func in functions:\n                    if func['lines'] > 50:\n                        findings.append({\n                            'type': 'suggestion',\n                            'file': file,\n                            'line': func['start_line'],\n                            'message': f\"Function '{func['name']}' is {func['lines']} lines long\",\n                            'suggestion': 'Consider breaking into smaller functions'\n                        })\n        \n        return findings\n```\n\n### 5. PR Size Optimization\n\nHelp split large PRs:\n\n**PR Splitter Suggestions**\n```python\ndef suggest_pr_splits(analysis):\n    \"\"\"\n    Suggest how to split large PRs\n    \"\"\"\n    stats = analysis['change_statistics']\n    \n    # Check if PR is too large\n    if stats['files_changed'] > 20 or stats['insertions'] + stats['deletions'] > 1000:\n        suggestions = analyze_split_opportunities(analysis)\n        \n        return f\"\"\"\n## âš ï¸ Large PR Detected\n\nThis PR changes {stats['files_changed']} files with {stats['insertions'] + stats['deletions']} total changes.\nLarge PRs are harder to review and more likely to introduce bugs.\n\n### Suggested Splits:\n\n{format_split_suggestions(suggestions)}\n\n### How to Split:\n\n1. Create feature branch from current branch\n2. Cherry-pick commits for first logical unit\n3. Create PR for first unit\n4. Repeat for remaining units\n\n```bash\n# Example split workflow\ngit checkout -b feature/part-1\ngit cherry-pick <commit-hashes-for-part-1>\ngit push origin feature/part-1\n# Create PR for part 1\n\ngit checkout -b feature/part-2\ngit cherry-pick <commit-hashes-for-part-2>\ngit push origin feature/part-2\n# Create PR for part 2\n```\n\"\"\"\n    \n    return \"\"\n\ndef analyze_split_opportunities(analysis):\n    \"\"\"Find logical units for splitting\"\"\"\n    suggestions = []\n    \n    # Group by feature areas\n    feature_groups = defaultdict(list)\n    for file in analysis['files_changed']:\n        feature = extract_feature_area(file['filename'])\n        feature_groups[feature].append(file)\n    \n    # Suggest splits\n    for feature, files in feature_groups.items():\n        if len(files) >= 5:\n            suggestions.append({\n                'name': f\"{feature} changes\",\n                'files': files,\n                'reason': f\"Isolated changes to {feature} feature\"\n            })\n    \n    return suggestions\n```\n\n### 6. Visual Diff Enhancement\n\nGenerate visual representations:\n\n**Mermaid Diagram Generator**\n```python\ndef generate_architecture_diff(analysis):\n    \"\"\"\n    Generate diagram showing architectural changes\n    \"\"\"\n    if has_architectural_changes(analysis):\n        return f\"\"\"\n## Architecture Changes\n\n```mermaid\ngraph LR\n    subgraph \"Before\"\n        A1[Component A] --> B1[Component B]\n        B1 --> C1[Database]\n    end\n    \n    subgraph \"After\"\n        A2[Component A] --> B2[Component B]\n        B2 --> C2[Database]\n        B2 --> D2[New Cache Layer]\n        A2 --> E2[New API Gateway]\n    end\n    \n    style D2 fill:#90EE90\n    style E2 fill:#90EE90\n```\n\n### Key Changes:\n1. Added caching layer for performance\n2. Introduced API gateway for better routing\n3. Refactored component communication\n\"\"\"\n    return \"\"\n```\n\n### 7. Test Coverage Report\n\nInclude test coverage analysis:\n\n**Coverage Report Generator**\n```python\ndef generate_coverage_report(base_branch='main'):\n    \"\"\"\n    Generate test coverage comparison\n    \"\"\"\n    # Get coverage before and after\n    before_coverage = get_coverage_for_branch(base_branch)\n    after_coverage = get_coverage_for_branch('HEAD')\n    \n    coverage_diff = after_coverage - before_coverage\n    \n    report = f\"\"\"\n## Test Coverage\n\n| Metric | Before | After | Change |\n|--------|--------|-------|--------|\n| Lines | {before_coverage['lines']:.1f}% | {after_coverage['lines']:.1f}% | {format_diff(coverage_diff['lines'])} |\n| Functions | {before_coverage['functions']:.1f}% | {after_coverage['functions']:.1f}% | {format_diff(coverage_diff['functions'])} |\n| Branches | {before_coverage['branches']:.1f}% | {after_coverage['branches']:.1f}% | {format_diff(coverage_diff['branches'])} |\n\n### Uncovered Files\n\"\"\"\n    \n    # List files with low coverage\n    for file in get_low_coverage_files():\n        report += f\"- `{file['name']}`: {file['coverage']:.1f}% coverage\\n\"\n    \n    return report\n\ndef format_diff(value):\n    \"\"\"Format coverage difference\"\"\"\n    if value > 0:\n        return f\"<span style='color: green'>+{value:.1f}%</span> âœ…\"\n    elif value < 0:\n        return f\"<span style='color: red'>{value:.1f}%</span> âš ï¸\"\n    else:\n        return \"No change\"\n```\n\n### 8. Risk Assessment\n\nEvaluate PR risk:\n\n**Risk Calculator**\n```python\ndef calculate_pr_risk(analysis):\n    \"\"\"\n    Calculate risk score for PR\n    \"\"\"\n    risk_factors = {\n        'size': calculate_size_risk(analysis),\n        'complexity': calculate_complexity_risk(analysis),\n        'test_coverage': calculate_test_risk(analysis),\n        'dependencies': calculate_dependency_risk(analysis),\n        'security': calculate_security_risk(analysis)\n    }\n    \n    overall_risk = sum(risk_factors.values()) / len(risk_factors)\n    \n    risk_report = f\"\"\"\n## Risk Assessment\n\n**Overall Risk Level**: {get_risk_level(overall_risk)} ({overall_risk:.1f}/10)\n\n### Risk Factors\n\n| Factor | Score | Details |\n|--------|-------|---------|\n| Size | {risk_factors['size']:.1f}/10 | {get_size_details(analysis)} |\n| Complexity | {risk_factors['complexity']:.1f}/10 | {get_complexity_details(analysis)} |\n| Test Coverage | {risk_factors['test_coverage']:.1f}/10 | {get_test_details(analysis)} |\n| Dependencies | {risk_factors['dependencies']:.1f}/10 | {get_dependency_details(analysis)} |\n| Security | {risk_factors['security']:.1f}/10 | {get_security_details(analysis)} |\n\n### Mitigation Strategies\n\n{generate_mitigation_strategies(risk_factors)}\n\"\"\"\n    \n    return risk_report\n\ndef get_risk_level(score):\n    \"\"\"Convert score to risk level\"\"\"\n    if score < 3:\n        return \"ðŸŸ¢ Low\"\n    elif score < 6:\n        return \"ðŸŸ¡ Medium\"\n    elif score < 8:\n        return \"ðŸŸ  High\"\n    else:\n        return \"ðŸ”´ Critical\"\n```\n\n### 9. PR Templates\n\nGenerate context-specific templates:\n\n```python\ndef generate_pr_template(pr_type, analysis):\n    \"\"\"\n    Generate PR template based on type\n    \"\"\"\n    templates = {\n        'feature': f\"\"\"\n## Feature: {extract_feature_name(analysis)}\n\n### Description\n{generate_feature_description(analysis)}\n\n### User Story\nAs a [user type]\nI want [feature]\nSo that [benefit]\n\n### Acceptance Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n- [ ] Criterion 3\n\n### Demo\n[Link to demo or screenshots]\n\n### Technical Implementation\n{generate_technical_summary(analysis)}\n\n### Testing Strategy\n{generate_test_strategy(analysis)}\n\"\"\",\n        'bugfix': f\"\"\"\n## Bug Fix: {extract_bug_description(analysis)}\n\n### Issue\n- **Reported in**: #[issue-number]\n- **Severity**: {determine_severity(analysis)}\n- **Affected versions**: {get_affected_versions(analysis)}\n\n### Root Cause\n{analyze_root_cause(analysis)}\n\n### Solution\n{describe_solution(analysis)}\n\n### Testing\n- [ ] Bug is reproducible before fix\n- [ ] Bug is resolved after fix\n- [ ] No regressions introduced\n- [ ] Edge cases tested\n\n### Verification Steps\n1. Step to reproduce original issue\n2. Apply this fix\n3. Verify issue is resolved\n\"\"\",\n        'refactor': f\"\"\"\n## Refactoring: {extract_refactor_scope(analysis)}\n\n### Motivation\n{describe_refactor_motivation(analysis)}\n\n### Changes Made\n{list_refactor_changes(analysis)}\n\n### Benefits\n- Improved {list_improvements(analysis)}\n- Reduced {list_reductions(analysis)}\n\n### Compatibility\n- [ ] No breaking changes\n- [ ] API remains unchanged\n- [ ] Performance maintained or improved\n\n### Metrics\n| Metric | Before | After |\n|--------|--------|-------|\n| Complexity | X | Y |\n| Test Coverage | X% | Y% |\n| Performance | Xms | Yms |\n\"\"\"\n    }\n    \n    return templates.get(pr_type, templates['feature'])\n```\n\n### 10. Review Response Templates\n\nHelp with review responses:\n\n```python\nreview_response_templates = {\n    'acknowledge_feedback': \"\"\"\nThank you for the thorough review! I'll address these points.\n\"\"\",\n    \n    'explain_decision': \"\"\"\nGreat question! I chose this approach because:\n1. [Reason 1]\n2. [Reason 2]\n\nAlternative approaches considered:\n- [Alternative 1]: [Why not chosen]\n- [Alternative 2]: [Why not chosen]\n\nHappy to discuss further if you have concerns.\n\"\"\",\n    \n    'request_clarification': \"\"\"\nThanks for the feedback. Could you clarify what you mean by [specific point]?\nI want to make sure I understand your concern correctly before making changes.\n\"\"\",\n    \n    'disagree_respectfully': \"\"\"\nI appreciate your perspective on this. I have a slightly different view:\n\n[Your reasoning]\n\nHowever, I'm open to discussing this further. What do you think about [compromise/middle ground]?\n\"\"\",\n    \n    'commit_to_change': \"\"\"\nGood catch! I'll update this to [specific change].\nThis should address [concern] while maintaining [other requirement].\n\"\"\"\n}\n```\n\n## Output Format\n\n1. **PR Summary**: Executive summary with key metrics\n2. **Detailed Description**: Comprehensive PR description\n3. **Review Checklist**: Context-aware review items  \n4. **Risk Assessment**: Risk analysis with mitigation strategies\n5. **Test Coverage**: Before/after coverage comparison\n6. **Visual Aids**: Diagrams and visual diffs where applicable\n7. **Size Recommendations**: Suggestions for splitting large PRs\n8. **Review Automation**: Automated checks and findings\n\nFocus on creating PRs that are a pleasure to review, with all necessary context and documentation for efficient code review process."
              }
            ],
            "skills": []
          },
          {
            "name": "game-development",
            "description": "Game development with Unity, Godot, and Minecraft",
            "source": "./plugins/game-development",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install game-development@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "godot-gdscript-patterns",
                "description": "Master Godot 4 GDScript patterns including signals, scenes, state machines, and optimization. Use when building Godot games, implementing game systems, or learning GDScript best practices.",
                "path": "plugins/game-development/skills/godot-gdscript-patterns/SKILL.md",
                "frontmatter": {
                  "name": "godot-gdscript-patterns",
                  "description": "Master Godot 4 GDScript patterns including signals, scenes, state machines, and optimization. Use when building Godot games, implementing game systems, or learning GDScript best practices."
                },
                "content": "# Godot GDScript Patterns\n\nProduction patterns for Godot 4.x game development with GDScript, covering architecture, signals, scenes, and optimization.\n\n## When to Use This Skill\n\n- Building games with Godot 4\n- Implementing game systems in GDScript\n- Designing scene architecture\n- Managing game state\n- Optimizing GDScript performance\n- Learning Godot best practices\n\n## Core Concepts\n\n### 1. Godot Architecture\n\n```\nNode: Base building block\nâ”œâ”€â”€ Scene: Reusable node tree (saved as .tscn)\nâ”œâ”€â”€ Resource: Data container (saved as .tres)\nâ”œâ”€â”€ Signal: Event communication\nâ””â”€â”€ Group: Node categorization\n```\n\n### 2. GDScript Basics\n\n```gdscript\nclass_name Player\nextends CharacterBody2D\n\n# Signals\nsignal health_changed(new_health: int)\nsignal died\n\n# Exports (Inspector-editable)\n@export var speed: float = 200.0\n@export var max_health: int = 100\n@export_range(0, 1) var damage_reduction: float = 0.0\n@export_group(\"Combat\")\n@export var attack_damage: int = 10\n@export var attack_cooldown: float = 0.5\n\n# Onready (initialized when ready)\n@onready var sprite: Sprite2D = $Sprite2D\n@onready var animation: AnimationPlayer = $AnimationPlayer\n@onready var hitbox: Area2D = $Hitbox\n\n# Private variables (convention: underscore prefix)\nvar _health: int\nvar _can_attack: bool = true\n\nfunc _ready() -> void:\n    _health = max_health\n\nfunc _physics_process(delta: float) -> void:\n    var direction := Input.get_vector(\"left\", \"right\", \"up\", \"down\")\n    velocity = direction * speed\n    move_and_slide()\n\nfunc take_damage(amount: int) -> void:\n    var actual_damage := int(amount * (1.0 - damage_reduction))\n    _health = max(_health - actual_damage, 0)\n    health_changed.emit(_health)\n\n    if _health <= 0:\n        died.emit()\n```\n\n## Patterns\n\n### Pattern 1: State Machine\n\n```gdscript\n# state_machine.gd\nclass_name StateMachine\nextends Node\n\nsignal state_changed(from_state: StringName, to_state: StringName)\n\n@export var initial_state: State\n\nvar current_state: State\nvar states: Dictionary = {}\n\nfunc _ready() -> void:\n    # Register all State children\n    for child in get_children():\n        if child is State:\n            states[child.name] = child\n            child.state_machine = self\n            child.process_mode = Node.PROCESS_MODE_DISABLED\n\n    # Start initial state\n    if initial_state:\n        current_state = initial_state\n        current_state.process_mode = Node.PROCESS_MODE_INHERIT\n        current_state.enter()\n\nfunc _process(delta: float) -> void:\n    if current_state:\n        current_state.update(delta)\n\nfunc _physics_process(delta: float) -> void:\n    if current_state:\n        current_state.physics_update(delta)\n\nfunc _unhandled_input(event: InputEvent) -> void:\n    if current_state:\n        current_state.handle_input(event)\n\nfunc transition_to(state_name: StringName, msg: Dictionary = {}) -> void:\n    if not states.has(state_name):\n        push_error(\"State '%s' not found\" % state_name)\n        return\n\n    var previous_state := current_state\n    previous_state.exit()\n    previous_state.process_mode = Node.PROCESS_MODE_DISABLED\n\n    current_state = states[state_name]\n    current_state.process_mode = Node.PROCESS_MODE_INHERIT\n    current_state.enter(msg)\n\n    state_changed.emit(previous_state.name, current_state.name)\n```\n\n```gdscript\n# state.gd\nclass_name State\nextends Node\n\nvar state_machine: StateMachine\n\nfunc enter(_msg: Dictionary = {}) -> void:\n    pass\n\nfunc exit() -> void:\n    pass\n\nfunc update(_delta: float) -> void:\n    pass\n\nfunc physics_update(_delta: float) -> void:\n    pass\n\nfunc handle_input(_event: InputEvent) -> void:\n    pass\n```\n\n```gdscript\n# player_idle.gd\nclass_name PlayerIdle\nextends State\n\n@export var player: Player\n\nfunc enter(_msg: Dictionary = {}) -> void:\n    player.animation.play(\"idle\")\n\nfunc physics_update(_delta: float) -> void:\n    var direction := Input.get_vector(\"left\", \"right\", \"up\", \"down\")\n\n    if direction != Vector2.ZERO:\n        state_machine.transition_to(\"Move\")\n\nfunc handle_input(event: InputEvent) -> void:\n    if event.is_action_pressed(\"attack\"):\n        state_machine.transition_to(\"Attack\")\n    elif event.is_action_pressed(\"jump\"):\n        state_machine.transition_to(\"Jump\")\n```\n\n### Pattern 2: Autoload Singletons\n\n```gdscript\n# game_manager.gd (Add to Project Settings > Autoload)\nextends Node\n\nsignal game_started\nsignal game_paused(is_paused: bool)\nsignal game_over(won: bool)\nsignal score_changed(new_score: int)\n\nenum GameState { MENU, PLAYING, PAUSED, GAME_OVER }\n\nvar state: GameState = GameState.MENU\nvar score: int = 0:\n    set(value):\n        score = value\n        score_changed.emit(score)\n\nvar high_score: int = 0\n\nfunc _ready() -> void:\n    process_mode = Node.PROCESS_MODE_ALWAYS\n    _load_high_score()\n\nfunc _input(event: InputEvent) -> void:\n    if event.is_action_pressed(\"pause\") and state == GameState.PLAYING:\n        toggle_pause()\n\nfunc start_game() -> void:\n    score = 0\n    state = GameState.PLAYING\n    game_started.emit()\n\nfunc toggle_pause() -> void:\n    var is_paused := state != GameState.PAUSED\n\n    if is_paused:\n        state = GameState.PAUSED\n        get_tree().paused = true\n    else:\n        state = GameState.PLAYING\n        get_tree().paused = false\n\n    game_paused.emit(is_paused)\n\nfunc end_game(won: bool) -> void:\n    state = GameState.GAME_OVER\n\n    if score > high_score:\n        high_score = score\n        _save_high_score()\n\n    game_over.emit(won)\n\nfunc add_score(points: int) -> void:\n    score += points\n\nfunc _load_high_score() -> void:\n    if FileAccess.file_exists(\"user://high_score.save\"):\n        var file := FileAccess.open(\"user://high_score.save\", FileAccess.READ)\n        high_score = file.get_32()\n\nfunc _save_high_score() -> void:\n    var file := FileAccess.open(\"user://high_score.save\", FileAccess.WRITE)\n    file.store_32(high_score)\n```\n\n```gdscript\n# event_bus.gd (Global signal bus)\nextends Node\n\n# Player events\nsignal player_spawned(player: Node2D)\nsignal player_died(player: Node2D)\nsignal player_health_changed(health: int, max_health: int)\n\n# Enemy events\nsignal enemy_spawned(enemy: Node2D)\nsignal enemy_died(enemy: Node2D, position: Vector2)\n\n# Item events\nsignal item_collected(item_type: StringName, value: int)\nsignal powerup_activated(powerup_type: StringName)\n\n# Level events\nsignal level_started(level_number: int)\nsignal level_completed(level_number: int, time: float)\nsignal checkpoint_reached(checkpoint_id: int)\n```\n\n### Pattern 3: Resource-based Data\n\n```gdscript\n# weapon_data.gd\nclass_name WeaponData\nextends Resource\n\n@export var name: StringName\n@export var damage: int\n@export var attack_speed: float\n@export var range: float\n@export_multiline var description: String\n@export var icon: Texture2D\n@export var projectile_scene: PackedScene\n@export var sound_attack: AudioStream\n```\n\n```gdscript\n# character_stats.gd\nclass_name CharacterStats\nextends Resource\n\nsignal stat_changed(stat_name: StringName, new_value: float)\n\n@export var max_health: float = 100.0\n@export var attack: float = 10.0\n@export var defense: float = 5.0\n@export var speed: float = 200.0\n\n# Runtime values (not saved)\nvar _current_health: float\n\nfunc _init() -> void:\n    _current_health = max_health\n\nfunc get_current_health() -> float:\n    return _current_health\n\nfunc take_damage(amount: float) -> float:\n    var actual_damage := maxf(amount - defense, 1.0)\n    _current_health = maxf(_current_health - actual_damage, 0.0)\n    stat_changed.emit(\"health\", _current_health)\n    return actual_damage\n\nfunc heal(amount: float) -> void:\n    _current_health = minf(_current_health + amount, max_health)\n    stat_changed.emit(\"health\", _current_health)\n\nfunc duplicate_for_runtime() -> CharacterStats:\n    var copy := duplicate() as CharacterStats\n    copy._current_health = copy.max_health\n    return copy\n```\n\n```gdscript\n# Using resources\nclass_name Character\nextends CharacterBody2D\n\n@export var base_stats: CharacterStats\n@export var weapon: WeaponData\n\nvar stats: CharacterStats\n\nfunc _ready() -> void:\n    # Create runtime copy to avoid modifying the resource\n    stats = base_stats.duplicate_for_runtime()\n    stats.stat_changed.connect(_on_stat_changed)\n\nfunc attack() -> void:\n    if weapon:\n        print(\"Attacking with %s for %d damage\" % [weapon.name, weapon.damage])\n\nfunc _on_stat_changed(stat_name: StringName, value: float) -> void:\n    if stat_name == \"health\" and value <= 0:\n        die()\n```\n\n### Pattern 4: Object Pooling\n\n```gdscript\n# object_pool.gd\nclass_name ObjectPool\nextends Node\n\n@export var pooled_scene: PackedScene\n@export var initial_size: int = 10\n@export var can_grow: bool = true\n\nvar _available: Array[Node] = []\nvar _in_use: Array[Node] = []\n\nfunc _ready() -> void:\n    _initialize_pool()\n\nfunc _initialize_pool() -> void:\n    for i in initial_size:\n        _create_instance()\n\nfunc _create_instance() -> Node:\n    var instance := pooled_scene.instantiate()\n    instance.process_mode = Node.PROCESS_MODE_DISABLED\n    instance.visible = false\n    add_child(instance)\n    _available.append(instance)\n\n    # Connect return signal if exists\n    if instance.has_signal(\"returned_to_pool\"):\n        instance.returned_to_pool.connect(_return_to_pool.bind(instance))\n\n    return instance\n\nfunc get_instance() -> Node:\n    var instance: Node\n\n    if _available.is_empty():\n        if can_grow:\n            instance = _create_instance()\n            _available.erase(instance)\n        else:\n            push_warning(\"Pool exhausted and cannot grow\")\n            return null\n    else:\n        instance = _available.pop_back()\n\n    instance.process_mode = Node.PROCESS_MODE_INHERIT\n    instance.visible = true\n    _in_use.append(instance)\n\n    if instance.has_method(\"on_spawn\"):\n        instance.on_spawn()\n\n    return instance\n\nfunc _return_to_pool(instance: Node) -> void:\n    if not instance in _in_use:\n        return\n\n    _in_use.erase(instance)\n\n    if instance.has_method(\"on_despawn\"):\n        instance.on_despawn()\n\n    instance.process_mode = Node.PROCESS_MODE_DISABLED\n    instance.visible = false\n    _available.append(instance)\n\nfunc return_all() -> void:\n    for instance in _in_use.duplicate():\n        _return_to_pool(instance)\n```\n\n```gdscript\n# pooled_bullet.gd\nclass_name PooledBullet\nextends Area2D\n\nsignal returned_to_pool\n\n@export var speed: float = 500.0\n@export var lifetime: float = 5.0\n\nvar direction: Vector2\nvar _timer: float\n\nfunc on_spawn() -> void:\n    _timer = lifetime\n\nfunc on_despawn() -> void:\n    direction = Vector2.ZERO\n\nfunc initialize(pos: Vector2, dir: Vector2) -> void:\n    global_position = pos\n    direction = dir.normalized()\n    rotation = direction.angle()\n\nfunc _physics_process(delta: float) -> void:\n    position += direction * speed * delta\n\n    _timer -= delta\n    if _timer <= 0:\n        returned_to_pool.emit()\n\nfunc _on_body_entered(body: Node2D) -> void:\n    if body.has_method(\"take_damage\"):\n        body.take_damage(10)\n    returned_to_pool.emit()\n```\n\n### Pattern 5: Component System\n\n```gdscript\n# health_component.gd\nclass_name HealthComponent\nextends Node\n\nsignal health_changed(current: int, maximum: int)\nsignal damaged(amount: int, source: Node)\nsignal healed(amount: int)\nsignal died\n\n@export var max_health: int = 100\n@export var invincibility_time: float = 0.0\n\nvar current_health: int:\n    set(value):\n        var old := current_health\n        current_health = clampi(value, 0, max_health)\n        if current_health != old:\n            health_changed.emit(current_health, max_health)\n\nvar _invincible: bool = false\n\nfunc _ready() -> void:\n    current_health = max_health\n\nfunc take_damage(amount: int, source: Node = null) -> int:\n    if _invincible or current_health <= 0:\n        return 0\n\n    var actual := mini(amount, current_health)\n    current_health -= actual\n    damaged.emit(actual, source)\n\n    if current_health <= 0:\n        died.emit()\n    elif invincibility_time > 0:\n        _start_invincibility()\n\n    return actual\n\nfunc heal(amount: int) -> int:\n    var actual := mini(amount, max_health - current_health)\n    current_health += actual\n    if actual > 0:\n        healed.emit(actual)\n    return actual\n\nfunc _start_invincibility() -> void:\n    _invincible = true\n    await get_tree().create_timer(invincibility_time).timeout\n    _invincible = false\n```\n\n```gdscript\n# hitbox_component.gd\nclass_name HitboxComponent\nextends Area2D\n\nsignal hit(hurtbox: HurtboxComponent)\n\n@export var damage: int = 10\n@export var knockback_force: float = 200.0\n\nvar owner_node: Node\n\nfunc _ready() -> void:\n    owner_node = get_parent()\n    area_entered.connect(_on_area_entered)\n\nfunc _on_area_entered(area: Area2D) -> void:\n    if area is HurtboxComponent:\n        var hurtbox := area as HurtboxComponent\n        if hurtbox.owner_node != owner_node:\n            hit.emit(hurtbox)\n            hurtbox.receive_hit(self)\n```\n\n```gdscript\n# hurtbox_component.gd\nclass_name HurtboxComponent\nextends Area2D\n\nsignal hurt(hitbox: HitboxComponent)\n\n@export var health_component: HealthComponent\n\nvar owner_node: Node\n\nfunc _ready() -> void:\n    owner_node = get_parent()\n\nfunc receive_hit(hitbox: HitboxComponent) -> void:\n    hurt.emit(hitbox)\n\n    if health_component:\n        health_component.take_damage(hitbox.damage, hitbox.owner_node)\n```\n\n### Pattern 6: Scene Management\n\n```gdscript\n# scene_manager.gd (Autoload)\nextends Node\n\nsignal scene_loading_started(scene_path: String)\nsignal scene_loading_progress(progress: float)\nsignal scene_loaded(scene: Node)\nsignal transition_started\nsignal transition_finished\n\n@export var transition_scene: PackedScene\n@export var loading_scene: PackedScene\n\nvar _current_scene: Node\nvar _transition: CanvasLayer\nvar _loader: ResourceLoader\n\nfunc _ready() -> void:\n    _current_scene = get_tree().current_scene\n\n    if transition_scene:\n        _transition = transition_scene.instantiate()\n        add_child(_transition)\n        _transition.visible = false\n\nfunc change_scene(scene_path: String, with_transition: bool = true) -> void:\n    if with_transition:\n        await _play_transition_out()\n\n    _load_scene(scene_path)\n\nfunc change_scene_packed(scene: PackedScene, with_transition: bool = true) -> void:\n    if with_transition:\n        await _play_transition_out()\n\n    _swap_scene(scene.instantiate())\n\nfunc _load_scene(path: String) -> void:\n    scene_loading_started.emit(path)\n\n    # Check if already loaded\n    if ResourceLoader.has_cached(path):\n        var scene := load(path) as PackedScene\n        _swap_scene(scene.instantiate())\n        return\n\n    # Async loading\n    ResourceLoader.load_threaded_request(path)\n\n    while true:\n        var progress := []\n        var status := ResourceLoader.load_threaded_get_status(path, progress)\n\n        match status:\n            ResourceLoader.THREAD_LOAD_IN_PROGRESS:\n                scene_loading_progress.emit(progress[0])\n                await get_tree().process_frame\n            ResourceLoader.THREAD_LOAD_LOADED:\n                var scene := ResourceLoader.load_threaded_get(path) as PackedScene\n                _swap_scene(scene.instantiate())\n                return\n            _:\n                push_error(\"Failed to load scene: %s\" % path)\n                return\n\nfunc _swap_scene(new_scene: Node) -> void:\n    if _current_scene:\n        _current_scene.queue_free()\n\n    _current_scene = new_scene\n    get_tree().root.add_child(_current_scene)\n    get_tree().current_scene = _current_scene\n\n    scene_loaded.emit(_current_scene)\n    await _play_transition_in()\n\nfunc _play_transition_out() -> void:\n    if not _transition:\n        return\n\n    transition_started.emit()\n    _transition.visible = true\n\n    if _transition.has_method(\"transition_out\"):\n        await _transition.transition_out()\n    else:\n        await get_tree().create_timer(0.3).timeout\n\nfunc _play_transition_in() -> void:\n    if not _transition:\n        transition_finished.emit()\n        return\n\n    if _transition.has_method(\"transition_in\"):\n        await _transition.transition_in()\n    else:\n        await get_tree().create_timer(0.3).timeout\n\n    _transition.visible = false\n    transition_finished.emit()\n```\n\n### Pattern 7: Save System\n\n```gdscript\n# save_manager.gd (Autoload)\nextends Node\n\nconst SAVE_PATH := \"user://savegame.save\"\nconst ENCRYPTION_KEY := \"your_secret_key_here\"\n\nsignal save_completed\nsignal load_completed\nsignal save_error(message: String)\n\nfunc save_game(data: Dictionary) -> void:\n    var file := FileAccess.open_encrypted_with_pass(\n        SAVE_PATH,\n        FileAccess.WRITE,\n        ENCRYPTION_KEY\n    )\n\n    if file == null:\n        save_error.emit(\"Could not open save file\")\n        return\n\n    var json := JSON.stringify(data)\n    file.store_string(json)\n    file.close()\n\n    save_completed.emit()\n\nfunc load_game() -> Dictionary:\n    if not FileAccess.file_exists(SAVE_PATH):\n        return {}\n\n    var file := FileAccess.open_encrypted_with_pass(\n        SAVE_PATH,\n        FileAccess.READ,\n        ENCRYPTION_KEY\n    )\n\n    if file == null:\n        save_error.emit(\"Could not open save file\")\n        return {}\n\n    var json := file.get_as_text()\n    file.close()\n\n    var parsed := JSON.parse_string(json)\n    if parsed == null:\n        save_error.emit(\"Could not parse save data\")\n        return {}\n\n    load_completed.emit()\n    return parsed\n\nfunc delete_save() -> void:\n    if FileAccess.file_exists(SAVE_PATH):\n        DirAccess.remove_absolute(SAVE_PATH)\n\nfunc has_save() -> bool:\n    return FileAccess.file_exists(SAVE_PATH)\n```\n\n```gdscript\n# saveable.gd (Attach to saveable nodes)\nclass_name Saveable\nextends Node\n\n@export var save_id: String\n\nfunc _ready() -> void:\n    if save_id.is_empty():\n        save_id = str(get_path())\n\nfunc get_save_data() -> Dictionary:\n    var parent := get_parent()\n    var data := {\"id\": save_id}\n\n    if parent is Node2D:\n        data[\"position\"] = {\"x\": parent.position.x, \"y\": parent.position.y}\n\n    if parent.has_method(\"get_custom_save_data\"):\n        data.merge(parent.get_custom_save_data())\n\n    return data\n\nfunc load_save_data(data: Dictionary) -> void:\n    var parent := get_parent()\n\n    if data.has(\"position\") and parent is Node2D:\n        parent.position = Vector2(data.position.x, data.position.y)\n\n    if parent.has_method(\"load_custom_save_data\"):\n        parent.load_custom_save_data(data)\n```\n\n## Performance Tips\n\n```gdscript\n# 1. Cache node references\n@onready var sprite := $Sprite2D  # Good\n# $Sprite2D in _process()  # Bad - repeated lookup\n\n# 2. Use object pooling for frequent spawning\n# See Pattern 4\n\n# 3. Avoid allocations in hot paths\nvar _reusable_array: Array = []\n\nfunc _process(_delta: float) -> void:\n    _reusable_array.clear()  # Reuse instead of creating new\n\n# 4. Use static typing\nfunc calculate(value: float) -> float:  # Good\n    return value * 2.0\n\n# 5. Disable processing when not needed\nfunc _on_off_screen() -> void:\n    set_process(false)\n    set_physics_process(false)\n```\n\n## Best Practices\n\n### Do's\n- **Use signals for decoupling** - Avoid direct references\n- **Type everything** - Static typing catches errors\n- **Use resources for data** - Separate data from logic\n- **Pool frequently spawned objects** - Avoid GC hitches\n- **Use Autoloads sparingly** - Only for truly global systems\n\n### Don'ts\n- **Don't use `get_node()` in loops** - Cache references\n- **Don't couple scenes tightly** - Use signals\n- **Don't put logic in resources** - Keep them data-only\n- **Don't ignore the Profiler** - Monitor performance\n- **Don't fight the scene tree** - Work with Godot's design\n\n## Resources\n\n- [Godot Documentation](https://docs.godotengine.org/en/stable/)\n- [GDQuest Tutorials](https://www.gdquest.com/)\n- [Godot Recipes](https://kidscancode.org/godot_recipes/)"
              },
              {
                "name": "unity-ecs-patterns",
                "description": "Master Unity ECS (Entity Component System) with DOTS, Jobs, and Burst for high-performance game development. Use when building data-oriented games, optimizing performance, or working with large entity counts.",
                "path": "plugins/game-development/skills/unity-ecs-patterns/SKILL.md",
                "frontmatter": {
                  "name": "unity-ecs-patterns",
                  "description": "Master Unity ECS (Entity Component System) with DOTS, Jobs, and Burst for high-performance game development. Use when building data-oriented games, optimizing performance, or working with large entity counts."
                },
                "content": "# Unity ECS Patterns\n\nProduction patterns for Unity's Data-Oriented Technology Stack (DOTS) including Entity Component System, Job System, and Burst Compiler.\n\n## When to Use This Skill\n\n- Building high-performance Unity games\n- Managing thousands of entities efficiently\n- Implementing data-oriented game systems\n- Optimizing CPU-bound game logic\n- Converting OOP game code to ECS\n- Using Jobs and Burst for parallelization\n\n## Core Concepts\n\n### 1. ECS vs OOP\n\n| Aspect | Traditional OOP | ECS/DOTS |\n|--------|-----------------|----------|\n| Data layout | Object-oriented | Data-oriented |\n| Memory | Scattered | Contiguous |\n| Processing | Per-object | Batched |\n| Scaling | Poor with count | Linear scaling |\n| Best for | Complex behaviors | Mass simulation |\n\n### 2. DOTS Components\n\n```\nEntity: Lightweight ID (no data)\nComponent: Pure data (no behavior)\nSystem: Logic that processes components\nWorld: Container for entities\nArchetype: Unique combination of components\nChunk: Memory block for same-archetype entities\n```\n\n## Patterns\n\n### Pattern 1: Basic ECS Setup\n\n```csharp\nusing Unity.Entities;\nusing Unity.Mathematics;\nusing Unity.Transforms;\nusing Unity.Burst;\nusing Unity.Collections;\n\n// Component: Pure data, no methods\npublic struct Speed : IComponentData\n{\n    public float Value;\n}\n\npublic struct Health : IComponentData\n{\n    public float Current;\n    public float Max;\n}\n\npublic struct Target : IComponentData\n{\n    public Entity Value;\n}\n\n// Tag component (zero-size marker)\npublic struct EnemyTag : IComponentData { }\npublic struct PlayerTag : IComponentData { }\n\n// Buffer component (variable-size array)\n[InternalBufferCapacity(8)]\npublic struct InventoryItem : IBufferElementData\n{\n    public int ItemId;\n    public int Quantity;\n}\n\n// Shared component (grouped entities)\npublic struct TeamId : ISharedComponentData\n{\n    public int Value;\n}\n```\n\n### Pattern 2: Systems with ISystem (Recommended)\n\n```csharp\nusing Unity.Entities;\nusing Unity.Transforms;\nusing Unity.Mathematics;\nusing Unity.Burst;\n\n// ISystem: Unmanaged, Burst-compatible, highest performance\n[BurstCompile]\npublic partial struct MovementSystem : ISystem\n{\n    [BurstCompile]\n    public void OnCreate(ref SystemState state)\n    {\n        // Require components before system runs\n        state.RequireForUpdate<Speed>();\n    }\n\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        float deltaTime = SystemAPI.Time.DeltaTime;\n\n        // Simple foreach - auto-generates job\n        foreach (var (transform, speed) in\n            SystemAPI.Query<RefRW<LocalTransform>, RefRO<Speed>>())\n        {\n            transform.ValueRW.Position +=\n                new float3(0, 0, speed.ValueRO.Value * deltaTime);\n        }\n    }\n\n    [BurstCompile]\n    public void OnDestroy(ref SystemState state) { }\n}\n\n// With explicit job for more control\n[BurstCompile]\npublic partial struct MovementJobSystem : ISystem\n{\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        var job = new MoveJob\n        {\n            DeltaTime = SystemAPI.Time.DeltaTime\n        };\n\n        state.Dependency = job.ScheduleParallel(state.Dependency);\n    }\n}\n\n[BurstCompile]\npublic partial struct MoveJob : IJobEntity\n{\n    public float DeltaTime;\n\n    void Execute(ref LocalTransform transform, in Speed speed)\n    {\n        transform.Position += new float3(0, 0, speed.Value * DeltaTime);\n    }\n}\n```\n\n### Pattern 3: Entity Queries\n\n```csharp\n[BurstCompile]\npublic partial struct QueryExamplesSystem : ISystem\n{\n    private EntityQuery _enemyQuery;\n\n    public void OnCreate(ref SystemState state)\n    {\n        // Build query manually for complex cases\n        _enemyQuery = new EntityQueryBuilder(Allocator.Temp)\n            .WithAll<EnemyTag, Health, LocalTransform>()\n            .WithNone<Dead>()\n            .WithOptions(EntityQueryOptions.FilterWriteGroup)\n            .Build(ref state);\n    }\n\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        // SystemAPI.Query - simplest approach\n        foreach (var (health, entity) in\n            SystemAPI.Query<RefRW<Health>>()\n                .WithAll<EnemyTag>()\n                .WithEntityAccess())\n        {\n            if (health.ValueRO.Current <= 0)\n            {\n                // Mark for destruction\n                SystemAPI.GetSingleton<EndSimulationEntityCommandBufferSystem.Singleton>()\n                    .CreateCommandBuffer(state.WorldUnmanaged)\n                    .DestroyEntity(entity);\n            }\n        }\n\n        // Get count\n        int enemyCount = _enemyQuery.CalculateEntityCount();\n\n        // Get all entities\n        var enemies = _enemyQuery.ToEntityArray(Allocator.Temp);\n\n        // Get component arrays\n        var healths = _enemyQuery.ToComponentDataArray<Health>(Allocator.Temp);\n    }\n}\n```\n\n### Pattern 4: Entity Command Buffers (Structural Changes)\n\n```csharp\n// Structural changes (create/destroy/add/remove) require command buffers\n[BurstCompile]\n[UpdateInGroup(typeof(SimulationSystemGroup))]\npublic partial struct SpawnSystem : ISystem\n{\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        var ecbSingleton = SystemAPI.GetSingleton<BeginSimulationEntityCommandBufferSystem.Singleton>();\n        var ecb = ecbSingleton.CreateCommandBuffer(state.WorldUnmanaged);\n\n        foreach (var (spawner, transform) in\n            SystemAPI.Query<RefRW<Spawner>, RefRO<LocalTransform>>())\n        {\n            spawner.ValueRW.Timer -= SystemAPI.Time.DeltaTime;\n\n            if (spawner.ValueRO.Timer <= 0)\n            {\n                spawner.ValueRW.Timer = spawner.ValueRO.Interval;\n\n                // Create entity (deferred until sync point)\n                Entity newEntity = ecb.Instantiate(spawner.ValueRO.Prefab);\n\n                // Set component values\n                ecb.SetComponent(newEntity, new LocalTransform\n                {\n                    Position = transform.ValueRO.Position,\n                    Rotation = quaternion.identity,\n                    Scale = 1f\n                });\n\n                // Add component\n                ecb.AddComponent(newEntity, new Speed { Value = 5f });\n            }\n        }\n    }\n}\n\n// Parallel ECB usage\n[BurstCompile]\npublic partial struct ParallelSpawnJob : IJobEntity\n{\n    public EntityCommandBuffer.ParallelWriter ECB;\n\n    void Execute([EntityIndexInQuery] int index, in Spawner spawner)\n    {\n        Entity e = ECB.Instantiate(index, spawner.Prefab);\n        ECB.AddComponent(index, e, new Speed { Value = 5f });\n    }\n}\n```\n\n### Pattern 5: Aspect (Grouping Components)\n\n```csharp\nusing Unity.Entities;\nusing Unity.Transforms;\nusing Unity.Mathematics;\n\n// Aspect: Groups related components for cleaner code\npublic readonly partial struct CharacterAspect : IAspect\n{\n    public readonly Entity Entity;\n\n    private readonly RefRW<LocalTransform> _transform;\n    private readonly RefRO<Speed> _speed;\n    private readonly RefRW<Health> _health;\n\n    // Optional component\n    [Optional]\n    private readonly RefRO<Shield> _shield;\n\n    // Buffer\n    private readonly DynamicBuffer<InventoryItem> _inventory;\n\n    public float3 Position\n    {\n        get => _transform.ValueRO.Position;\n        set => _transform.ValueRW.Position = value;\n    }\n\n    public float CurrentHealth => _health.ValueRO.Current;\n    public float MaxHealth => _health.ValueRO.Max;\n    public float MoveSpeed => _speed.ValueRO.Value;\n\n    public bool HasShield => _shield.IsValid;\n    public float ShieldAmount => HasShield ? _shield.ValueRO.Amount : 0f;\n\n    public void TakeDamage(float amount)\n    {\n        float remaining = amount;\n\n        if (HasShield && _shield.ValueRO.Amount > 0)\n        {\n            // Shield absorbs damage first\n            remaining = math.max(0, amount - _shield.ValueRO.Amount);\n        }\n\n        _health.ValueRW.Current = math.max(0, _health.ValueRO.Current - remaining);\n    }\n\n    public void Move(float3 direction, float deltaTime)\n    {\n        _transform.ValueRW.Position += direction * _speed.ValueRO.Value * deltaTime;\n    }\n\n    public void AddItem(int itemId, int quantity)\n    {\n        _inventory.Add(new InventoryItem { ItemId = itemId, Quantity = quantity });\n    }\n}\n\n// Using aspect in system\n[BurstCompile]\npublic partial struct CharacterSystem : ISystem\n{\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        float dt = SystemAPI.Time.DeltaTime;\n\n        foreach (var character in SystemAPI.Query<CharacterAspect>())\n        {\n            character.Move(new float3(1, 0, 0), dt);\n\n            if (character.CurrentHealth < character.MaxHealth * 0.5f)\n            {\n                // Low health logic\n            }\n        }\n    }\n}\n```\n\n### Pattern 6: Singleton Components\n\n```csharp\n// Singleton: Exactly one entity with this component\npublic struct GameConfig : IComponentData\n{\n    public float DifficultyMultiplier;\n    public int MaxEnemies;\n    public float SpawnRate;\n}\n\npublic struct GameState : IComponentData\n{\n    public int Score;\n    public int Wave;\n    public float TimeRemaining;\n}\n\n// Create singleton on world creation\npublic partial struct GameInitSystem : ISystem\n{\n    public void OnCreate(ref SystemState state)\n    {\n        var entity = state.EntityManager.CreateEntity();\n        state.EntityManager.AddComponentData(entity, new GameConfig\n        {\n            DifficultyMultiplier = 1.0f,\n            MaxEnemies = 100,\n            SpawnRate = 2.0f\n        });\n        state.EntityManager.AddComponentData(entity, new GameState\n        {\n            Score = 0,\n            Wave = 1,\n            TimeRemaining = 120f\n        });\n    }\n}\n\n// Access singleton in system\n[BurstCompile]\npublic partial struct ScoreSystem : ISystem\n{\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        // Read singleton\n        var config = SystemAPI.GetSingleton<GameConfig>();\n\n        // Write singleton\n        ref var gameState = ref SystemAPI.GetSingletonRW<GameState>().ValueRW;\n        gameState.TimeRemaining -= SystemAPI.Time.DeltaTime;\n\n        // Check exists\n        if (SystemAPI.HasSingleton<GameConfig>())\n        {\n            // ...\n        }\n    }\n}\n```\n\n### Pattern 7: Baking (Converting GameObjects)\n\n```csharp\nusing Unity.Entities;\nusing UnityEngine;\n\n// Authoring component (MonoBehaviour in Editor)\npublic class EnemyAuthoring : MonoBehaviour\n{\n    public float Speed = 5f;\n    public float Health = 100f;\n    public GameObject ProjectilePrefab;\n\n    class Baker : Baker<EnemyAuthoring>\n    {\n        public override void Bake(EnemyAuthoring authoring)\n        {\n            var entity = GetEntity(TransformUsageFlags.Dynamic);\n\n            AddComponent(entity, new Speed { Value = authoring.Speed });\n            AddComponent(entity, new Health\n            {\n                Current = authoring.Health,\n                Max = authoring.Health\n            });\n            AddComponent(entity, new EnemyTag());\n\n            if (authoring.ProjectilePrefab != null)\n            {\n                AddComponent(entity, new ProjectilePrefab\n                {\n                    Value = GetEntity(authoring.ProjectilePrefab, TransformUsageFlags.Dynamic)\n                });\n            }\n        }\n    }\n}\n\n// Complex baking with dependencies\npublic class SpawnerAuthoring : MonoBehaviour\n{\n    public GameObject[] Prefabs;\n    public float Interval = 1f;\n\n    class Baker : Baker<SpawnerAuthoring>\n    {\n        public override void Bake(SpawnerAuthoring authoring)\n        {\n            var entity = GetEntity(TransformUsageFlags.Dynamic);\n\n            AddComponent(entity, new Spawner\n            {\n                Interval = authoring.Interval,\n                Timer = 0f\n            });\n\n            // Bake buffer of prefabs\n            var buffer = AddBuffer<SpawnPrefabElement>(entity);\n            foreach (var prefab in authoring.Prefabs)\n            {\n                buffer.Add(new SpawnPrefabElement\n                {\n                    Prefab = GetEntity(prefab, TransformUsageFlags.Dynamic)\n                });\n            }\n\n            // Declare dependencies\n            DependsOn(authoring.Prefabs);\n        }\n    }\n}\n```\n\n### Pattern 8: Jobs with Native Collections\n\n```csharp\nusing Unity.Jobs;\nusing Unity.Collections;\nusing Unity.Burst;\nusing Unity.Mathematics;\n\n[BurstCompile]\npublic struct SpatialHashJob : IJobParallelFor\n{\n    [ReadOnly]\n    public NativeArray<float3> Positions;\n\n    // Thread-safe write to hash map\n    public NativeParallelMultiHashMap<int, int>.ParallelWriter HashMap;\n\n    public float CellSize;\n\n    public void Execute(int index)\n    {\n        float3 pos = Positions[index];\n        int hash = GetHash(pos);\n        HashMap.Add(hash, index);\n    }\n\n    int GetHash(float3 pos)\n    {\n        int x = (int)math.floor(pos.x / CellSize);\n        int y = (int)math.floor(pos.y / CellSize);\n        int z = (int)math.floor(pos.z / CellSize);\n        return x * 73856093 ^ y * 19349663 ^ z * 83492791;\n    }\n}\n\n[BurstCompile]\npublic partial struct SpatialHashSystem : ISystem\n{\n    private NativeParallelMultiHashMap<int, int> _hashMap;\n\n    public void OnCreate(ref SystemState state)\n    {\n        _hashMap = new NativeParallelMultiHashMap<int, int>(10000, Allocator.Persistent);\n    }\n\n    public void OnDestroy(ref SystemState state)\n    {\n        _hashMap.Dispose();\n    }\n\n    [BurstCompile]\n    public void OnUpdate(ref SystemState state)\n    {\n        var query = SystemAPI.QueryBuilder()\n            .WithAll<LocalTransform>()\n            .Build();\n\n        int count = query.CalculateEntityCount();\n\n        // Resize if needed\n        if (_hashMap.Capacity < count)\n        {\n            _hashMap.Capacity = count * 2;\n        }\n\n        _hashMap.Clear();\n\n        // Get positions\n        var positions = query.ToComponentDataArray<LocalTransform>(Allocator.TempJob);\n        var posFloat3 = new NativeArray<float3>(count, Allocator.TempJob);\n\n        for (int i = 0; i < count; i++)\n        {\n            posFloat3[i] = positions[i].Position;\n        }\n\n        // Build hash map\n        var hashJob = new SpatialHashJob\n        {\n            Positions = posFloat3,\n            HashMap = _hashMap.AsParallelWriter(),\n            CellSize = 10f\n        };\n\n        state.Dependency = hashJob.Schedule(count, 64, state.Dependency);\n\n        // Cleanup\n        positions.Dispose(state.Dependency);\n        posFloat3.Dispose(state.Dependency);\n    }\n}\n```\n\n## Performance Tips\n\n```csharp\n// 1. Use Burst everywhere\n[BurstCompile]\npublic partial struct MySystem : ISystem { }\n\n// 2. Prefer IJobEntity over manual iteration\n[BurstCompile]\npartial struct OptimizedJob : IJobEntity\n{\n    void Execute(ref LocalTransform transform) { }\n}\n\n// 3. Schedule parallel when possible\nstate.Dependency = job.ScheduleParallel(state.Dependency);\n\n// 4. Use ScheduleParallel with chunk iteration\n[BurstCompile]\npartial struct ChunkJob : IJobChunk\n{\n    public ComponentTypeHandle<Health> HealthHandle;\n\n    public void Execute(in ArchetypeChunk chunk, int unfilteredChunkIndex,\n        bool useEnabledMask, in v128 chunkEnabledMask)\n    {\n        var healths = chunk.GetNativeArray(ref HealthHandle);\n        for (int i = 0; i < chunk.Count; i++)\n        {\n            // Process\n        }\n    }\n}\n\n// 5. Avoid structural changes in hot paths\n// Use enableable components instead of add/remove\npublic struct Disabled : IComponentData, IEnableableComponent { }\n```\n\n## Best Practices\n\n### Do's\n- **Use ISystem over SystemBase** - Better performance\n- **Burst compile everything** - Massive speedup\n- **Batch structural changes** - Use ECB\n- **Profile with Profiler** - Identify bottlenecks\n- **Use Aspects** - Clean component grouping\n\n### Don'ts\n- **Don't use managed types** - Breaks Burst\n- **Don't structural change in jobs** - Use ECB\n- **Don't over-architect** - Start simple\n- **Don't ignore chunk utilization** - Group similar entities\n- **Don't forget disposal** - Native collections leak\n\n## Resources\n\n- [Unity DOTS Documentation](https://docs.unity3d.com/Packages/com.unity.entities@latest)\n- [Unity DOTS Samples](https://github.com/Unity-Technologies/EntityComponentSystemSamples)\n- [Burst User Guide](https://docs.unity3d.com/Packages/com.unity.burst@latest)"
              }
            ]
          },
          {
            "name": "full-stack-orchestration",
            "description": "Full-stack orchestration with deployment and security",
            "source": "./plugins/full-stack-orchestration",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install full-stack-orchestration@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/full-stack-feature",
                "description": null,
                "path": "plugins/full-stack-orchestration/commands/full-stack-feature.md",
                "frontmatter": null,
                "content": "Orchestrate full-stack feature development across backend, frontend, and infrastructure layers with modern API-first approach:\n\n[Extended thinking: This workflow coordinates multiple specialized agents to deliver a complete full-stack feature from architecture through deployment. It follows API-first development principles, ensuring contract-driven development where the API specification drives both backend implementation and frontend consumption. Each phase builds upon previous outputs, creating a cohesive system with proper separation of concerns, comprehensive testing, and production-ready deployment. The workflow emphasizes modern practices like component-driven UI development, feature flags, observability, and progressive rollout strategies.]\n\n## Phase 1: Architecture & Design Foundation\n\n### 1. Database Architecture Design\n- Use Task tool with subagent_type=\"database-design::database-architect\"\n- Prompt: \"Design database schema and data models for: $ARGUMENTS. Consider scalability, query patterns, indexing strategy, and data consistency requirements. Include migration strategy if modifying existing schema. Provide both logical and physical data models.\"\n- Expected output: Entity relationship diagrams, table schemas, indexing strategy, migration scripts, data access patterns\n- Context: Initial requirements and business domain model\n\n### 2. Backend Service Architecture\n- Use Task tool with subagent_type=\"backend-development::backend-architect\"\n- Prompt: \"Design backend service architecture for: $ARGUMENTS. Using the database design from previous step, create service boundaries, define API contracts (OpenAPI/GraphQL), design authentication/authorization strategy, and specify inter-service communication patterns. Include resilience patterns (circuit breakers, retries) and caching strategy.\"\n- Expected output: Service architecture diagram, OpenAPI specifications, authentication flows, caching architecture, message queue design (if applicable)\n- Context: Database schema from step 1, non-functional requirements\n\n### 3. Frontend Component Architecture\n- Use Task tool with subagent_type=\"frontend-mobile-development::frontend-developer\"\n- Prompt: \"Design frontend architecture and component structure for: $ARGUMENTS. Based on the API contracts from previous step, design component hierarchy, state management approach (Redux/Zustand/Context), routing structure, and data fetching patterns. Include accessibility requirements and responsive design strategy. Plan for Storybook component documentation.\"\n- Expected output: Component tree diagram, state management design, routing configuration, design system integration plan, accessibility checklist\n- Context: API specifications from step 2, UI/UX requirements\n\n## Phase 2: Parallel Implementation\n\n### 4. Backend Service Implementation\n- Use Task tool with subagent_type=\"python-development::python-pro\" (or \"golang-pro\"/\"nodejs-expert\" based on stack)\n- Prompt: \"Implement backend services for: $ARGUMENTS. Using the architecture and API specs from Phase 1, build RESTful/GraphQL endpoints with proper validation, error handling, and logging. Implement business logic, data access layer, authentication middleware, and integration with external services. Include observability (structured logging, metrics, tracing).\"\n- Expected output: Backend service code, API endpoints, middleware, background jobs, unit tests, integration tests\n- Context: Architecture designs from Phase 1, database schema\n\n### 5. Frontend Implementation\n- Use Task tool with subagent_type=\"frontend-mobile-development::frontend-developer\"\n- Prompt: \"Implement frontend application for: $ARGUMENTS. Build React/Next.js components using the component architecture from Phase 1. Implement state management, API integration with proper error handling and loading states, form validation, and responsive layouts. Create Storybook stories for components. Ensure accessibility (WCAG 2.1 AA compliance).\"\n- Expected output: React components, state management implementation, API client code, Storybook stories, responsive styles, accessibility implementations\n- Context: Component architecture from step 3, API contracts\n\n### 6. Database Implementation & Optimization\n- Use Task tool with subagent_type=\"database-design::sql-pro\"\n- Prompt: \"Implement and optimize database layer for: $ARGUMENTS. Create migration scripts, stored procedures (if needed), optimize queries identified by backend implementation, set up proper indexes, and implement data validation constraints. Include database-level security measures and backup strategies.\"\n- Expected output: Migration scripts, optimized queries, stored procedures, index definitions, database security configuration\n- Context: Database design from step 1, query patterns from backend implementation\n\n## Phase 3: Integration & Testing\n\n### 7. API Contract Testing\n- Use Task tool with subagent_type=\"test-automator\"\n- Prompt: \"Create contract tests for: $ARGUMENTS. Implement Pact/Dredd tests to validate API contracts between backend and frontend. Create integration tests for all API endpoints, test authentication flows, validate error responses, and ensure proper CORS configuration. Include load testing scenarios.\"\n- Expected output: Contract test suites, integration tests, load test scenarios, API documentation validation\n- Context: API implementations from Phase 2\n\n### 8. End-to-End Testing\n- Use Task tool with subagent_type=\"test-automator\"\n- Prompt: \"Implement E2E tests for: $ARGUMENTS. Create Playwright/Cypress tests covering critical user journeys, cross-browser compatibility, mobile responsiveness, and error scenarios. Test feature flags integration, analytics tracking, and performance metrics. Include visual regression tests.\"\n- Expected output: E2E test suites, visual regression baselines, performance benchmarks, test reports\n- Context: Frontend and backend implementations from Phase 2\n\n### 9. Security Audit & Hardening\n- Use Task tool with subagent_type=\"security-auditor\"\n- Prompt: \"Perform security audit for: $ARGUMENTS. Review API security (authentication, authorization, rate limiting), check for OWASP Top 10 vulnerabilities, audit frontend for XSS/CSRF risks, validate input sanitization, and review secrets management. Provide penetration testing results and remediation steps.\"\n- Expected output: Security audit report, vulnerability assessment, remediation recommendations, security headers configuration\n- Context: All implementations from Phase 2\n\n## Phase 4: Deployment & Operations\n\n### 10. Infrastructure & CI/CD Setup\n- Use Task tool with subagent_type=\"deployment-engineer\"\n- Prompt: \"Setup deployment infrastructure for: $ARGUMENTS. Create Docker containers, Kubernetes manifests (or cloud-specific configs), implement CI/CD pipelines with automated testing gates, setup feature flags (LaunchDarkly/Unleash), and configure monitoring/alerting. Include blue-green deployment strategy and rollback procedures.\"\n- Expected output: Dockerfiles, K8s manifests, CI/CD pipeline configs, feature flag setup, IaC templates (Terraform/CloudFormation)\n- Context: All implementations and tests from previous phases\n\n### 11. Observability & Monitoring\n- Use Task tool with subagent_type=\"deployment-engineer\"\n- Prompt: \"Implement observability stack for: $ARGUMENTS. Setup distributed tracing (OpenTelemetry), configure application metrics (Prometheus/DataDog), implement centralized logging (ELK/Splunk), create dashboards for key metrics, and define SLIs/SLOs. Include alerting rules and on-call procedures.\"\n- Expected output: Observability configuration, dashboard definitions, alert rules, runbooks, SLI/SLO definitions\n- Context: Infrastructure setup from step 10\n\n### 12. Performance Optimization\n- Use Task tool with subagent_type=\"performance-engineer\"\n- Prompt: \"Optimize performance across stack for: $ARGUMENTS. Analyze and optimize database queries, implement caching strategies (Redis/CDN), optimize frontend bundle size and loading performance, setup lazy loading and code splitting, and tune backend service performance. Include before/after metrics.\"\n- Expected output: Performance improvements, caching configuration, CDN setup, optimized bundles, performance metrics report\n- Context: Monitoring data from step 11, load test results\n\n## Configuration Options\n- `stack`: Specify technology stack (e.g., \"React/FastAPI/PostgreSQL\", \"Next.js/Django/MongoDB\")\n- `deployment_target`: Cloud platform (AWS/GCP/Azure) or on-premises\n- `feature_flags`: Enable/disable feature flag integration\n- `api_style`: REST or GraphQL\n- `testing_depth`: Comprehensive or essential\n- `compliance`: Specific compliance requirements (GDPR, HIPAA, SOC2)\n\n## Success Criteria\n- All API contracts validated through contract tests\n- Frontend and backend integration tests passing\n- E2E tests covering critical user journeys\n- Security audit passed with no critical vulnerabilities\n- Performance metrics meeting defined SLOs\n- Observability stack capturing all key metrics\n- Feature flags configured for progressive rollout\n- Documentation complete for all components\n- CI/CD pipeline with automated quality gates\n- Zero-downtime deployment capability verified\n\n## Coordination Notes\n- Each phase builds upon outputs from previous phases\n- Parallel tasks in Phase 2 can run simultaneously but must converge for Phase 3\n- Maintain traceability between requirements and implementations\n- Use correlation IDs across all services for distributed tracing\n- Document all architectural decisions in ADRs\n- Ensure consistent error handling and API responses across services\n\nFeature to implement: $ARGUMENTS"
              }
            ],
            "skills": []
          },
          {
            "name": "content-marketing",
            "description": "Content marketing with strategy and search agents",
            "source": "./plugins/content-marketing",
            "category": "marketing",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install content-marketing@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "context-management",
            "description": "Context management with save and restore capabilities",
            "source": "./plugins/context-management",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install context-management@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/context-restore",
                "description": null,
                "path": "plugins/context-management/commands/context-restore.md",
                "frontmatter": null,
                "content": "# Context Restoration: Advanced Semantic Memory Rehydration\n\n## Role Statement\n\nExpert Context Restoration Specialist focused on intelligent, semantic-aware context retrieval and reconstruction across complex multi-agent AI workflows. Specializes in preserving and reconstructing project knowledge with high fidelity and minimal information loss.\n\n## Context Overview\n\nThe Context Restoration tool is a sophisticated memory management system designed to:\n- Recover and reconstruct project context across distributed AI workflows\n- Enable seamless continuity in complex, long-running projects\n- Provide intelligent, semantically-aware context rehydration\n- Maintain historical knowledge integrity and decision traceability\n\n## Core Requirements and Arguments\n\n### Input Parameters\n- `context_source`: Primary context storage location (vector database, file system)\n- `project_identifier`: Unique project namespace\n- `restoration_mode`:\n  - `full`: Complete context restoration\n  - `incremental`: Partial context update\n  - `diff`: Compare and merge context versions\n- `token_budget`: Maximum context tokens to restore (default: 8192)\n- `relevance_threshold`: Semantic similarity cutoff for context components (default: 0.75)\n\n## Advanced Context Retrieval Strategies\n\n### 1. Semantic Vector Search\n- Utilize multi-dimensional embedding models for context retrieval\n- Employ cosine similarity and vector clustering techniques\n- Support multi-modal embedding (text, code, architectural diagrams)\n\n```python\ndef semantic_context_retrieve(project_id, query_vector, top_k=5):\n    \"\"\"Semantically retrieve most relevant context vectors\"\"\"\n    vector_db = VectorDatabase(project_id)\n    matching_contexts = vector_db.search(\n        query_vector,\n        similarity_threshold=0.75,\n        max_results=top_k\n    )\n    return rank_and_filter_contexts(matching_contexts)\n```\n\n### 2. Relevance Filtering and Ranking\n- Implement multi-stage relevance scoring\n- Consider temporal decay, semantic similarity, and historical impact\n- Dynamic weighting of context components\n\n```python\ndef rank_context_components(contexts, current_state):\n    \"\"\"Rank context components based on multiple relevance signals\"\"\"\n    ranked_contexts = []\n    for context in contexts:\n        relevance_score = calculate_composite_score(\n            semantic_similarity=context.semantic_score,\n            temporal_relevance=context.age_factor,\n            historical_impact=context.decision_weight\n        )\n        ranked_contexts.append((context, relevance_score))\n\n    return sorted(ranked_contexts, key=lambda x: x[1], reverse=True)\n```\n\n### 3. Context Rehydration Patterns\n- Implement incremental context loading\n- Support partial and full context reconstruction\n- Manage token budgets dynamically\n\n```python\ndef rehydrate_context(project_context, token_budget=8192):\n    \"\"\"Intelligent context rehydration with token budget management\"\"\"\n    context_components = [\n        'project_overview',\n        'architectural_decisions',\n        'technology_stack',\n        'recent_agent_work',\n        'known_issues'\n    ]\n\n    prioritized_components = prioritize_components(context_components)\n    restored_context = {}\n\n    current_tokens = 0\n    for component in prioritized_components:\n        component_tokens = estimate_tokens(component)\n        if current_tokens + component_tokens <= token_budget:\n            restored_context[component] = load_component(component)\n            current_tokens += component_tokens\n\n    return restored_context\n```\n\n### 4. Session State Reconstruction\n- Reconstruct agent workflow state\n- Preserve decision trails and reasoning contexts\n- Support multi-agent collaboration history\n\n### 5. Context Merging and Conflict Resolution\n- Implement three-way merge strategies\n- Detect and resolve semantic conflicts\n- Maintain provenance and decision traceability\n\n### 6. Incremental Context Loading\n- Support lazy loading of context components\n- Implement context streaming for large projects\n- Enable dynamic context expansion\n\n### 7. Context Validation and Integrity Checks\n- Cryptographic context signatures\n- Semantic consistency verification\n- Version compatibility checks\n\n### 8. Performance Optimization\n- Implement efficient caching mechanisms\n- Use probabilistic data structures for context indexing\n- Optimize vector search algorithms\n\n## Reference Workflows\n\n### Workflow 1: Project Resumption\n1. Retrieve most recent project context\n2. Validate context against current codebase\n3. Selectively restore relevant components\n4. Generate resumption summary\n\n### Workflow 2: Cross-Project Knowledge Transfer\n1. Extract semantic vectors from source project\n2. Map and transfer relevant knowledge\n3. Adapt context to target project's domain\n4. Validate knowledge transferability\n\n## Usage Examples\n\n```bash\n# Full context restoration\ncontext-restore project:ai-assistant --mode full\n\n# Incremental context update\ncontext-restore project:web-platform --mode incremental\n\n# Semantic context query\ncontext-restore project:ml-pipeline --query \"model training strategy\"\n```\n\n## Integration Patterns\n- RAG (Retrieval Augmented Generation) pipelines\n- Multi-agent workflow coordination\n- Continuous learning systems\n- Enterprise knowledge management\n\n## Future Roadmap\n- Enhanced multi-modal embedding support\n- Quantum-inspired vector search algorithms\n- Self-healing context reconstruction\n- Adaptive learning context strategies"
              },
              {
                "name": "/context-save",
                "description": null,
                "path": "plugins/context-management/commands/context-save.md",
                "frontmatter": null,
                "content": "# Context Save Tool: Intelligent Context Management Specialist\n\n## Role and Purpose\nAn elite context engineering specialist focused on comprehensive, semantic, and dynamically adaptable context preservation across AI workflows. This tool orchestrates advanced context capture, serialization, and retrieval strategies to maintain institutional knowledge and enable seamless multi-session collaboration.\n\n## Context Management Overview\nThe Context Save Tool is a sophisticated context engineering solution designed to:\n- Capture comprehensive project state and knowledge\n- Enable semantic context retrieval\n- Support multi-agent workflow coordination\n- Preserve architectural decisions and project evolution\n- Facilitate intelligent knowledge transfer\n\n## Requirements and Argument Handling\n\n### Input Parameters\n- `$PROJECT_ROOT`: Absolute path to project root\n- `$CONTEXT_TYPE`: Granularity of context capture (minimal, standard, comprehensive)\n- `$STORAGE_FORMAT`: Preferred storage format (json, markdown, vector)\n- `$TAGS`: Optional semantic tags for context categorization\n\n## Context Extraction Strategies\n\n### 1. Semantic Information Identification\n- Extract high-level architectural patterns\n- Capture decision-making rationales\n- Identify cross-cutting concerns and dependencies\n- Map implicit knowledge structures\n\n### 2. State Serialization Patterns\n- Use JSON Schema for structured representation\n- Support nested, hierarchical context models\n- Implement type-safe serialization\n- Enable lossless context reconstruction\n\n### 3. Multi-Session Context Management\n- Generate unique context fingerprints\n- Support version control for context artifacts\n- Implement context drift detection\n- Create semantic diff capabilities\n\n### 4. Context Compression Techniques\n- Use advanced compression algorithms\n- Support lossy and lossless compression modes\n- Implement semantic token reduction\n- Optimize storage efficiency\n\n### 5. Vector Database Integration\nSupported Vector Databases:\n- Pinecone\n- Weaviate\n- Qdrant\n\nIntegration Features:\n- Semantic embedding generation\n- Vector index construction\n- Similarity-based context retrieval\n- Multi-dimensional knowledge mapping\n\n### 6. Knowledge Graph Construction\n- Extract relational metadata\n- Create ontological representations\n- Support cross-domain knowledge linking\n- Enable inference-based context expansion\n\n### 7. Storage Format Selection\nSupported Formats:\n- Structured JSON\n- Markdown with frontmatter\n- Protocol Buffers\n- MessagePack\n- YAML with semantic annotations\n\n## Code Examples\n\n### 1. Context Extraction\n```python\ndef extract_project_context(project_root, context_type='standard'):\n    context = {\n        'project_metadata': extract_project_metadata(project_root),\n        'architectural_decisions': analyze_architecture(project_root),\n        'dependency_graph': build_dependency_graph(project_root),\n        'semantic_tags': generate_semantic_tags(project_root)\n    }\n    return context\n```\n\n### 2. State Serialization Schema\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"project_name\": {\"type\": \"string\"},\n    \"version\": {\"type\": \"string\"},\n    \"context_fingerprint\": {\"type\": \"string\"},\n    \"captured_at\": {\"type\": \"string\", \"format\": \"date-time\"},\n    \"architectural_decisions\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"decision_type\": {\"type\": \"string\"},\n          \"rationale\": {\"type\": \"string\"},\n          \"impact_score\": {\"type\": \"number\"}\n        }\n      }\n    }\n  }\n}\n```\n\n### 3. Context Compression Algorithm\n```python\ndef compress_context(context, compression_level='standard'):\n    strategies = {\n        'minimal': remove_redundant_tokens,\n        'standard': semantic_compression,\n        'comprehensive': advanced_vector_compression\n    }\n    compressor = strategies.get(compression_level, semantic_compression)\n    return compressor(context)\n```\n\n## Reference Workflows\n\n### Workflow 1: Project Onboarding Context Capture\n1. Analyze project structure\n2. Extract architectural decisions\n3. Generate semantic embeddings\n4. Store in vector database\n5. Create markdown summary\n\n### Workflow 2: Long-Running Session Context Management\n1. Periodically capture context snapshots\n2. Detect significant architectural changes\n3. Version and archive context\n4. Enable selective context restoration\n\n## Advanced Integration Capabilities\n- Real-time context synchronization\n- Cross-platform context portability\n- Compliance with enterprise knowledge management standards\n- Support for multi-modal context representation\n\n## Limitations and Considerations\n- Sensitive information must be explicitly excluded\n- Context capture has computational overhead\n- Requires careful configuration for optimal performance\n\n## Future Roadmap\n- Improved ML-driven context compression\n- Enhanced cross-domain knowledge transfer\n- Real-time collaborative context editing\n- Predictive context recommendation systems"
              }
            ],
            "skills": []
          },
          {
            "name": "customer-sales-automation",
            "description": "Customer support and sales automation agents",
            "source": "./plugins/customer-sales-automation",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install customer-sales-automation@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "database-design",
            "description": "Database architecture and SQL optimization",
            "source": "./plugins/database-design",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install database-design@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "postgresql-table-design",
                "description": "Design a PostgreSQL-specific schema. Covers best-practices, data types, indexing, constraints, performance patterns, and advanced features",
                "path": "plugins/database-design/skills/postgresql/SKILL.md",
                "frontmatter": {
                  "name": "postgresql-table-design",
                  "description": "Design a PostgreSQL-specific schema. Covers best-practices, data types, indexing, constraints, performance patterns, and advanced features"
                },
                "content": "# PostgreSQL Table Design \n\n## Core Rules\n\n- Define a **PRIMARY KEY** for reference tables (users, orders, etc.). Not always needed for time-series/event/log data. When used, prefer `BIGINT GENERATED ALWAYS AS IDENTITY`; use `UUID` only when global uniqueness/opacity is needed.\n- **Normalize first (to 3NF)** to eliminate data redundancy and update anomalies; denormalize **only** for measured, high-ROI reads where join performance is proven problematic. Premature denormalization creates maintenance burden.\n- Add **NOT NULL** everywhere itâ€™s semantically required; use **DEFAULT**s for common values.\n- Create **indexes for access paths you actually query**: PK/unique (auto), **FK columns (manual!)**, frequent filters/sorts, and join keys.\n- Prefer **TIMESTAMPTZ** for event time; **NUMERIC** for money; **TEXT** for strings; **BIGINT** for integer values, **DOUBLE PRECISION** for floats (or `NUMERIC` for exact decimal arithmetic).\n\n## PostgreSQL â€œGotchasâ€\n\n- **Identifiers**: unquoted â†’ lowercased. Avoid quoted/mixed-case names. Convention: use `snake_case` for table/column names.\n- **Unique + NULLs**: UNIQUE allows multiple NULLs. Use `UNIQUE (...) NULLS NOT DISTINCT` (PG15+) to restrict to one NULL.\n- **FK indexes**: PostgreSQL **does not** auto-index FK columns. Add them.\n- **No silent coercions**: length/precision overflows error out (no truncation). Example: inserting 999 into `NUMERIC(2,0)` fails with error, unlike some databases that silently truncate or round.\n- **Sequences/identity have gaps** (normal; don't \"fix\"). Rollbacks, crashes, and concurrent transactions create gaps in ID sequences (1, 2, 5, 6...). This is expected behaviorâ€”don't try to make IDs consecutive.\n- **Heap storage**: no clustered PK by default (unlike SQL Server/MySQL InnoDB); `CLUSTER` is one-off reorganization, not maintained on subsequent inserts. Row order on disk is insertion order unless explicitly clustered.\n- **MVCC**: updates/deletes leave dead tuples; vacuum handles themâ€”design to avoid hot wide-row churn.\n\n## Data Types\n\n- **IDs**: `BIGINT GENERATED ALWAYS AS IDENTITY` preferred (`GENERATED BY DEFAULT` also fine); `UUID` when merging/federating/used in a distributed system or for opaque IDs. Generate with `uuidv7()` (preferred if using PG18+) or `gen_random_uuid()` (if using an older PG version).\n- **Integers**: prefer `BIGINT` unless storage space is critical; `INTEGER` for smaller ranges; avoid `SMALLINT` unless constrained.\n- **Floats**: prefer `DOUBLE PRECISION` over `REAL` unless storage space is critical. Use `NUMERIC` for exact decimal arithmetic.\n- **Strings**: prefer `TEXT`; if length limits needed, use `CHECK (LENGTH(col) <= n)` instead of `VARCHAR(n)`; avoid `CHAR(n)`. Use `BYTEA` for binary data. Large strings/binary (>2KB default threshold) automatically stored in TOAST with compression. TOAST storage: `PLAIN` (no TOAST), `EXTENDED` (compress + out-of-line), `EXTERNAL` (out-of-line, no compress), `MAIN` (compress, keep in-line if possible). Default `EXTENDED` usually optimal. Control with `ALTER TABLE tbl ALTER COLUMN col SET STORAGE strategy` and `ALTER TABLE tbl SET (toast_tuple_target = 4096)` for threshold. Case-insensitive: for locale/accent handling use non-deterministic collations; for plain ASCII use expression indexes on `LOWER(col)` (preferred unless column needs case-insensitive PK/FK/UNIQUE) or `CITEXT`.\n- **Money**: `NUMERIC(p,s)` (never float).\n- **Time**: `TIMESTAMPTZ` for timestamps; `DATE` for date-only; `INTERVAL` for durations. Avoid `TIMESTAMP` (without timezone). Use `now()` for transaction start time, `clock_timestamp()` for current wall-clock time.\n- **Booleans**: `BOOLEAN` with `NOT NULL` constraint unless tri-state values are required.\n- **Enums**: `CREATE TYPE ... AS ENUM` for small, stable sets (e.g. US states, days of week). For business-logic-driven and evolving values (e.g. order statuses) â†’ use TEXT (or INT) + CHECK or lookup table.\n- **Arrays**: `TEXT[]`, `INTEGER[]`, etc. Use for ordered lists where you query elements. Index with **GIN** for containment (`@>`, `<@`) and overlap (`&&`) queries. Access: `arr[1]` (1-indexed), `arr[1:3]` (slicing). Good for tags, categories; avoid for relationsâ€”use junction tables instead. Literal syntax: `'{val1,val2}'` or `ARRAY[val1,val2]`.\n- **Range types**: `daterange`, `numrange`, `tstzrange` for intervals. Support overlap (`&&`), containment (`@>`), operators. Index with **GiST**. Good for scheduling, versioning, numeric ranges. Pick a bounds scheme and use it consistently; prefer `[)` (inclusive/exclusive) by default.\n- **Network types**: `INET` for IP addresses, `CIDR` for network ranges, `MACADDR` for MAC addresses. Support network operators (`<<`, `>>`, `&&`).\n- **Geometric types**: `POINT`, `LINE`, `POLYGON`, `CIRCLE` for 2D spatial data. Index with **GiST**. Consider **PostGIS** for advanced spatial features.\n- **Text search**: `TSVECTOR` for full-text search documents, `TSQUERY` for search queries. Index `tsvector` with **GIN**. Always specify language: `to_tsvector('english', col)` and `to_tsquery('english', 'query')`. Never use single-argument versions. This applies to both index expressions and queries.\n- **Domain types**: `CREATE DOMAIN email AS TEXT CHECK (VALUE ~ '^[^@]+@[^@]+$')` for reusable custom types with validation. Enforces constraints across tables.\n- **Composite types**: `CREATE TYPE address AS (street TEXT, city TEXT, zip TEXT)` for structured data within columns. Access with `(col).field` syntax.\n- **JSONB**: preferred over JSON; index with **GIN**. Use only for optional/semi-structured attrs. ONLY use JSON if the original ordering of the contents MUST be preserved.\n- **Vector types**: `vector` type by `pgvector` for vector similarity search for embeddings.\n\n\n### Do not use the following data types\n- DO NOT use `timestamp` (without time zone); DO use `timestamptz` instead.\n- DO NOT use `char(n)` or `varchar(n)`; DO use `text` instead.\n- DO NOT use `money` type; DO use `numeric` instead.\n- DO NOT use `timetz` type; DO use `timestamptz` instead.\n- DO NOT use `timestamptz(0)` or any other precision specification; DO use `timestamptz` instead\n- DO NOT use `serial` type; DO use `generated always as identity` instead.\n\n\n## Table Types\n\n- **Regular**: default; fully durable, logged.\n- **TEMPORARY**: session-scoped, auto-dropped, not logged. Faster for scratch work.\n- **UNLOGGED**: persistent but not crash-safe. Faster writes; good for caches/staging.\n\n## Row-Level Security\n\nEnable with `ALTER TABLE tbl ENABLE ROW LEVEL SECURITY`. Create policies: `CREATE POLICY user_access ON orders FOR SELECT TO app_users USING (user_id = current_user_id())`. Built-in user-based access control at the row level.\n\n## Constraints\n\n- **PK**: implicit UNIQUE + NOT NULL; creates a B-tree index.\n- **FK**: specify `ON DELETE/UPDATE` action (`CASCADE`, `RESTRICT`, `SET NULL`, `SET DEFAULT`). Add explicit index on referencing columnâ€”speeds up joins and prevents locking issues on parent deletes/updates. Use `DEFERRABLE INITIALLY DEFERRED` for circular FK dependencies checked at transaction end.\n- **UNIQUE**: creates a B-tree index; allows multiple NULLs unless `NULLS NOT DISTINCT` (PG15+). Standard behavior: `(1, NULL)` and `(1, NULL)` are allowed. With `NULLS NOT DISTINCT`: only one `(1, NULL)` allowed. Prefer `NULLS NOT DISTINCT` unless you specifically need duplicate NULLs.\n- **CHECK**: row-local constraints; NULL values pass the check (three-valued logic). Example: `CHECK (price > 0)` allows NULL prices. Combine with `NOT NULL` to enforce: `price NUMERIC NOT NULL CHECK (price > 0)`.\n- **EXCLUDE**: prevents overlapping values using operators. `EXCLUDE USING gist (room_id WITH =, booking_period WITH &&)` prevents double-booking rooms. Requires appropriate index type (often GiST).\n\n## Indexing\n\n- **B-tree**: default for equality/range queries (`=`, `<`, `>`, `BETWEEN`, `ORDER BY`)\n- **Composite**: order mattersâ€”index used if equality on leftmost prefix (`WHERE a = ? AND b > ?` uses index on `(a,b)`, but `WHERE b = ?` does not). Put most selective/frequently filtered columns first.\n- **Covering**: `CREATE INDEX ON tbl (id) INCLUDE (name, email)` - includes non-key columns for index-only scans without visiting table.\n- **Partial**: for hot subsets (`WHERE status = 'active'` â†’ `CREATE INDEX ON tbl (user_id) WHERE status = 'active'`). Any query with `status = 'active'` can use this index.\n- **Expression**: for computed search keys (`CREATE INDEX ON tbl (LOWER(email))`). Expression must match exactly in WHERE clause: `WHERE LOWER(email) = 'user@example.com'`.\n- **GIN**: JSONB containment/existence, arrays (`@>`, `?`), full-text search (`@@`)\n- **GiST**: ranges, geometry, exclusion constraints\n- **BRIN**: very large, naturally ordered data (time-series)â€”minimal storage overhead. Effective when row order on disk correlates with indexed column (insertion order or after `CLUSTER`).\n\n## Partitioning\n\n- Use for very large tables (>100M rows) where queries consistently filter on partition key (often time/date).\n- Alternate use: use for tables where data maintenance tasks dictates e.g. data pruned or bulk replaced periodically\n- **RANGE**: common for time-series (`PARTITION BY RANGE (created_at)`). Create partitions: `CREATE TABLE logs_2024_01 PARTITION OF logs FOR VALUES FROM ('2024-01-01') TO ('2024-02-01')`. **TimescaleDB** automates time-based or ID-based partitioning with retention policies and compression.\n- **LIST**: for discrete values (`PARTITION BY LIST (region)`). Example: `FOR VALUES IN ('us-east', 'us-west')`.\n- **HASH**: for even distribution when no natural key (`PARTITION BY HASH (user_id)`). Creates N partitions with modulus.\n- **Constraint exclusion**: requires `CHECK` constraints on partitions for query planner to prune. Auto-created for declarative partitioning (PG10+).\n- Prefer declarative partitioning or hypertables. Do NOT use table inheritance.\n- **Limitations**: no global UNIQUE constraintsâ€”include partition key in PK/UNIQUE. FKs from partitioned tables not supported; use triggers.\n\n## Special Considerations\n\n### Update-Heavy Tables\n\n- **Separate hot/cold columns**â€”put frequently updated columns in separate table to minimize bloat.\n- **Use `fillfactor=90`** to leave space for HOT updates that avoid index maintenance.\n- **Avoid updating indexed columns**â€”prevents beneficial HOT updates.\n- **Partition by update patterns**â€”separate frequently updated rows in a different partition from stable data.\n\n### Insert-Heavy Workloads\n\n- **Minimize indexes**â€”only create what you query; every index slows inserts.\n- **Use `COPY` or multi-row `INSERT`** instead of single-row inserts.\n- **UNLOGGED tables** for rebuildable staging dataâ€”much faster writes.\n- **Defer index creation** for bulk loadsâ€”>drop index, load data, recreate indexes.\n- **Partition by time/hash** to distribute load. **TimescaleDB** automates partitioning and compression of insert-heavy data.\n- **Use a natural key for primary key** such as a (timestamp, device_id) if enforcing global uniqueness is important many insert-heavy tables don't need a primary key at all.\n- If you do need a surrogate key, **Prefer `BIGINT GENERATED ALWAYS AS IDENTITY` over `UUID`**.\n\n### Upsert-Friendly Design\n\n- **Requires UNIQUE index** on conflict target columnsâ€”`ON CONFLICT (col1, col2)` needs exact matching unique index (partial indexes don't work).\n- **Use `EXCLUDED.column`** to reference would-be-inserted values; only update columns that actually changed to reduce write overhead.\n- **`DO NOTHING` faster** than `DO UPDATE` when no actual update needed.\n\n### Safe Schema Evolution\n\n- **Transactional DDL**: most DDL operations can run in transactions and be rolled backâ€”`BEGIN; ALTER TABLE...; ROLLBACK;` for safe testing.\n- **Concurrent index creation**: `CREATE INDEX CONCURRENTLY` avoids blocking writes but can't run in transactions.\n- **Volatile defaults cause rewrites**: adding `NOT NULL` columns with volatile defaults (e.g., `now()`, `gen_random_uuid()`) rewrites entire table. Non-volatile defaults are fast.\n- **Drop constraints before columns**: `ALTER TABLE DROP CONSTRAINT` then `DROP COLUMN` to avoid dependency issues.\n- **Function signature changes**: `CREATE OR REPLACE` with different arguments creates overloads, not replacements. DROP old version if no overload desired.\n\n## Generated Columns\n\n- `... GENERATED ALWAYS AS (<expr>) STORED` for computed, indexable fields. PG18+ adds `VIRTUAL` columns (computed on read, not stored).\n\n## Extensions\n\n- **`pgcrypto`**: `crypt()` for password hashing.\n- **`uuid-ossp`**: alternative UUID functions; prefer `pgcrypto` for new projects.\n- **`pg_trgm`**: fuzzy text search with `%` operator, `similarity()` function. Index with GIN for `LIKE '%pattern%'` acceleration.\n- **`citext`**: case-insensitive text type. Prefer expression indexes on `LOWER(col)` unless you need case-insensitive constraints.\n- **`btree_gin`/`btree_gist`**: enable mixed-type indexes (e.g., GIN index on both JSONB and text columns).\n- **`hstore`**: key-value pairs; mostly superseded by JSONB but useful for simple string mappings.\n- **`timescaledb`**: essential for time-seriesâ€”automated partitioning, retention, compression, continuous aggregates.\n- **`postgis`**: comprehensive geospatial support beyond basic geometric typesâ€”essential for location-based applications.\n- **`pgvector`**: vector similarity search for embeddings.\n- **`pgaudit`**: audit logging for all database activity.\n\n## JSONB Guidance\n\n- Prefer `JSONB` with **GIN** index.\n- Default: `CREATE INDEX ON tbl USING GIN (jsonb_col);` â†’ accelerates:\n  - **Containment** `jsonb_col @> '{\"k\":\"v\"}'`\n  - **Key existence** `jsonb_col ? 'k'`, **any/all keys** `?\\|`, `?&`\n  - **Path containment** on nested docs\n  - **Disjunction** `jsonb_col @> ANY(ARRAY['{\"status\":\"active\"}', '{\"status\":\"pending\"}'])`\n- Heavy `@>` workloads: consider opclass `jsonb_path_ops` for smaller/faster containment-only indexes:\n  - `CREATE INDEX ON tbl USING GIN (jsonb_col jsonb_path_ops);`\n  - **Trade-off**: loses support for key existence (`?`, `?|`, `?&`) queriesâ€”only supports containment (`@>`)\n- Equality/range on a specific scalar field: extract and index with B-tree (generated column or expression):\n  - `ALTER TABLE tbl ADD COLUMN price INT GENERATED ALWAYS AS ((jsonb_col->>'price')::INT) STORED;`\n  - `CREATE INDEX ON tbl (price);`\n  - Prefer queries like `WHERE price BETWEEN 100 AND 500` (uses B-tree) over `WHERE (jsonb_col->>'price')::INT BETWEEN 100 AND 500` without index.\n- Arrays inside JSONB: use GIN + `@>` for containment (e.g., tags). Consider `jsonb_path_ops` if only doing containment.\n- Keep core relations in tables; use JSONB for optional/variable attributes.\n- Use constraints to limit allowed JSONB values in a column e.g. `config JSONB NOT NULL CHECK(jsonb_typeof(config) = 'object')`\n\n\n## Examples\n\n### Users\n\n```sql\nCREATE TABLE users (\n  user_id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n  email TEXT NOT NULL UNIQUE,\n  name TEXT NOT NULL,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n);\nCREATE UNIQUE INDEX ON users (LOWER(email));\nCREATE INDEX ON users (created_at);\n```\n\n### Orders\n\n```sql\nCREATE TABLE orders (\n  order_id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n  user_id BIGINT NOT NULL REFERENCES users(user_id),\n  status TEXT NOT NULL DEFAULT 'PENDING' CHECK (status IN ('PENDING','PAID','CANCELED')),\n  total NUMERIC(10,2) NOT NULL CHECK (total > 0),\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n);\nCREATE INDEX ON orders (user_id);\nCREATE INDEX ON orders (created_at);\n```\n\n### JSONB\n\n```sql\nCREATE TABLE profiles (\n  user_id BIGINT PRIMARY KEY REFERENCES users(user_id),\n  attrs JSONB NOT NULL DEFAULT '{}',\n  theme TEXT GENERATED ALWAYS AS (attrs->>'theme') STORED\n);\nCREATE INDEX profiles_attrs_gin ON profiles USING GIN (attrs);\n```"
              }
            ]
          },
          {
            "name": "data-validation-suite",
            "description": "Data validation and backend security coding",
            "source": "./plugins/data-validation-suite",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install data-validation-suite@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "deployment-strategies",
            "description": "Deployment engineering with Terraform and IaC",
            "source": "./plugins/deployment-strategies",
            "category": "devops",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install deployment-strategies@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "developer-essentials",
            "description": "Essential developer skills for monorepos and debugging",
            "source": "./plugins/developer-essentials",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install developer-essentials@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "auth-implementation-patterns",
                "description": "Master authentication and authorization patterns including JWT, OAuth2, session management, and RBAC to build secure, scalable access control systems. Use when implementing auth systems, securing APIs, or debugging security issues.",
                "path": "plugins/developer-essentials/skills/auth-implementation-patterns/SKILL.md",
                "frontmatter": {
                  "name": "auth-implementation-patterns",
                  "description": "Master authentication and authorization patterns including JWT, OAuth2, session management, and RBAC to build secure, scalable access control systems. Use when implementing auth systems, securing APIs, or debugging security issues."
                },
                "content": "# Authentication & Authorization Implementation Patterns\n\nBuild secure, scalable authentication and authorization systems using industry-standard patterns and modern best practices.\n\n## When to Use This Skill\n\n- Implementing user authentication systems\n- Securing REST or GraphQL APIs\n- Adding OAuth2/social login\n- Implementing role-based access control (RBAC)\n- Designing session management\n- Migrating authentication systems\n- Debugging auth issues\n- Implementing SSO or multi-tenancy\n\n## Core Concepts\n\n### 1. Authentication vs Authorization\n\n**Authentication (AuthN)**: Who are you?\n- Verifying identity (username/password, OAuth, biometrics)\n- Issuing credentials (sessions, tokens)\n- Managing login/logout\n\n**Authorization (AuthZ)**: What can you do?\n- Permission checking\n- Role-based access control (RBAC)\n- Resource ownership validation\n- Policy enforcement\n\n### 2. Authentication Strategies\n\n**Session-Based:**\n- Server stores session state\n- Session ID in cookie\n- Traditional, simple, stateful\n\n**Token-Based (JWT):**\n- Stateless, self-contained\n- Scales horizontally\n- Can store claims\n\n**OAuth2/OpenID Connect:**\n- Delegate authentication\n- Social login (Google, GitHub)\n- Enterprise SSO\n\n## JWT Authentication\n\n### Pattern 1: JWT Implementation\n\n```typescript\n// JWT structure: header.payload.signature\nimport jwt from 'jsonwebtoken';\nimport { Request, Response, NextFunction } from 'express';\n\ninterface JWTPayload {\n    userId: string;\n    email: string;\n    role: string;\n    iat: number;\n    exp: number;\n}\n\n// Generate JWT\nfunction generateTokens(userId: string, email: string, role: string) {\n    const accessToken = jwt.sign(\n        { userId, email, role },\n        process.env.JWT_SECRET!,\n        { expiresIn: '15m' }  // Short-lived\n    );\n\n    const refreshToken = jwt.sign(\n        { userId },\n        process.env.JWT_REFRESH_SECRET!,\n        { expiresIn: '7d' }  // Long-lived\n    );\n\n    return { accessToken, refreshToken };\n}\n\n// Verify JWT\nfunction verifyToken(token: string): JWTPayload {\n    try {\n        return jwt.verify(token, process.env.JWT_SECRET!) as JWTPayload;\n    } catch (error) {\n        if (error instanceof jwt.TokenExpiredError) {\n            throw new Error('Token expired');\n        }\n        if (error instanceof jwt.JsonWebTokenError) {\n            throw new Error('Invalid token');\n        }\n        throw error;\n    }\n}\n\n// Middleware\nfunction authenticate(req: Request, res: Response, next: NextFunction) {\n    const authHeader = req.headers.authorization;\n    if (!authHeader?.startsWith('Bearer ')) {\n        return res.status(401).json({ error: 'No token provided' });\n    }\n\n    const token = authHeader.substring(7);\n    try {\n        const payload = verifyToken(token);\n        req.user = payload;  // Attach user to request\n        next();\n    } catch (error) {\n        return res.status(401).json({ error: 'Invalid token' });\n    }\n}\n\n// Usage\napp.get('/api/profile', authenticate, (req, res) => {\n    res.json({ user: req.user });\n});\n```\n\n### Pattern 2: Refresh Token Flow\n\n```typescript\ninterface StoredRefreshToken {\n    token: string;\n    userId: string;\n    expiresAt: Date;\n    createdAt: Date;\n}\n\nclass RefreshTokenService {\n    // Store refresh token in database\n    async storeRefreshToken(userId: string, refreshToken: string) {\n        const expiresAt = new Date(Date.now() + 7 * 24 * 60 * 60 * 1000);\n        await db.refreshTokens.create({\n            token: await hash(refreshToken),  // Hash before storing\n            userId,\n            expiresAt,\n        });\n    }\n\n    // Refresh access token\n    async refreshAccessToken(refreshToken: string) {\n        // Verify refresh token\n        let payload;\n        try {\n            payload = jwt.verify(\n                refreshToken,\n                process.env.JWT_REFRESH_SECRET!\n            ) as { userId: string };\n        } catch {\n            throw new Error('Invalid refresh token');\n        }\n\n        // Check if token exists in database\n        const storedToken = await db.refreshTokens.findOne({\n            where: {\n                token: await hash(refreshToken),\n                userId: payload.userId,\n                expiresAt: { $gt: new Date() },\n            },\n        });\n\n        if (!storedToken) {\n            throw new Error('Refresh token not found or expired');\n        }\n\n        // Get user\n        const user = await db.users.findById(payload.userId);\n        if (!user) {\n            throw new Error('User not found');\n        }\n\n        // Generate new access token\n        const accessToken = jwt.sign(\n            { userId: user.id, email: user.email, role: user.role },\n            process.env.JWT_SECRET!,\n            { expiresIn: '15m' }\n        );\n\n        return { accessToken };\n    }\n\n    // Revoke refresh token (logout)\n    async revokeRefreshToken(refreshToken: string) {\n        await db.refreshTokens.deleteOne({\n            token: await hash(refreshToken),\n        });\n    }\n\n    // Revoke all user tokens (logout all devices)\n    async revokeAllUserTokens(userId: string) {\n        await db.refreshTokens.deleteMany({ userId });\n    }\n}\n\n// API endpoints\napp.post('/api/auth/refresh', async (req, res) => {\n    const { refreshToken } = req.body;\n    try {\n        const { accessToken } = await refreshTokenService\n            .refreshAccessToken(refreshToken);\n        res.json({ accessToken });\n    } catch (error) {\n        res.status(401).json({ error: 'Invalid refresh token' });\n    }\n});\n\napp.post('/api/auth/logout', authenticate, async (req, res) => {\n    const { refreshToken } = req.body;\n    await refreshTokenService.revokeRefreshToken(refreshToken);\n    res.json({ message: 'Logged out successfully' });\n});\n```\n\n## Session-Based Authentication\n\n### Pattern 1: Express Session\n\n```typescript\nimport session from 'express-session';\nimport RedisStore from 'connect-redis';\nimport { createClient } from 'redis';\n\n// Setup Redis for session storage\nconst redisClient = createClient({\n    url: process.env.REDIS_URL,\n});\nawait redisClient.connect();\n\napp.use(\n    session({\n        store: new RedisStore({ client: redisClient }),\n        secret: process.env.SESSION_SECRET!,\n        resave: false,\n        saveUninitialized: false,\n        cookie: {\n            secure: process.env.NODE_ENV === 'production',  // HTTPS only\n            httpOnly: true,  // No JavaScript access\n            maxAge: 24 * 60 * 60 * 1000,  // 24 hours\n            sameSite: 'strict',  // CSRF protection\n        },\n    })\n);\n\n// Login\napp.post('/api/auth/login', async (req, res) => {\n    const { email, password } = req.body;\n\n    const user = await db.users.findOne({ email });\n    if (!user || !(await verifyPassword(password, user.passwordHash))) {\n        return res.status(401).json({ error: 'Invalid credentials' });\n    }\n\n    // Store user in session\n    req.session.userId = user.id;\n    req.session.role = user.role;\n\n    res.json({ user: { id: user.id, email: user.email, role: user.role } });\n});\n\n// Session middleware\nfunction requireAuth(req: Request, res: Response, next: NextFunction) {\n    if (!req.session.userId) {\n        return res.status(401).json({ error: 'Not authenticated' });\n    }\n    next();\n}\n\n// Protected route\napp.get('/api/profile', requireAuth, async (req, res) => {\n    const user = await db.users.findById(req.session.userId);\n    res.json({ user });\n});\n\n// Logout\napp.post('/api/auth/logout', (req, res) => {\n    req.session.destroy((err) => {\n        if (err) {\n            return res.status(500).json({ error: 'Logout failed' });\n        }\n        res.clearCookie('connect.sid');\n        res.json({ message: 'Logged out successfully' });\n    });\n});\n```\n\n## OAuth2 / Social Login\n\n### Pattern 1: OAuth2 with Passport.js\n\n```typescript\nimport passport from 'passport';\nimport { Strategy as GoogleStrategy } from 'passport-google-oauth20';\nimport { Strategy as GitHubStrategy } from 'passport-github2';\n\n// Google OAuth\npassport.use(\n    new GoogleStrategy(\n        {\n            clientID: process.env.GOOGLE_CLIENT_ID!,\n            clientSecret: process.env.GOOGLE_CLIENT_SECRET!,\n            callbackURL: '/api/auth/google/callback',\n        },\n        async (accessToken, refreshToken, profile, done) => {\n            try {\n                // Find or create user\n                let user = await db.users.findOne({\n                    googleId: profile.id,\n                });\n\n                if (!user) {\n                    user = await db.users.create({\n                        googleId: profile.id,\n                        email: profile.emails?.[0]?.value,\n                        name: profile.displayName,\n                        avatar: profile.photos?.[0]?.value,\n                    });\n                }\n\n                return done(null, user);\n            } catch (error) {\n                return done(error, undefined);\n            }\n        }\n    )\n);\n\n// Routes\napp.get('/api/auth/google', passport.authenticate('google', {\n    scope: ['profile', 'email'],\n}));\n\napp.get(\n    '/api/auth/google/callback',\n    passport.authenticate('google', { session: false }),\n    (req, res) => {\n        // Generate JWT\n        const tokens = generateTokens(req.user.id, req.user.email, req.user.role);\n        // Redirect to frontend with token\n        res.redirect(`${process.env.FRONTEND_URL}/auth/callback?token=${tokens.accessToken}`);\n    }\n);\n```\n\n## Authorization Patterns\n\n### Pattern 1: Role-Based Access Control (RBAC)\n\n```typescript\nenum Role {\n    USER = 'user',\n    MODERATOR = 'moderator',\n    ADMIN = 'admin',\n}\n\nconst roleHierarchy: Record<Role, Role[]> = {\n    [Role.ADMIN]: [Role.ADMIN, Role.MODERATOR, Role.USER],\n    [Role.MODERATOR]: [Role.MODERATOR, Role.USER],\n    [Role.USER]: [Role.USER],\n};\n\nfunction hasRole(userRole: Role, requiredRole: Role): boolean {\n    return roleHierarchy[userRole].includes(requiredRole);\n}\n\n// Middleware\nfunction requireRole(...roles: Role[]) {\n    return (req: Request, res: Response, next: NextFunction) => {\n        if (!req.user) {\n            return res.status(401).json({ error: 'Not authenticated' });\n        }\n\n        if (!roles.some(role => hasRole(req.user.role, role))) {\n            return res.status(403).json({ error: 'Insufficient permissions' });\n        }\n\n        next();\n    };\n}\n\n// Usage\napp.delete('/api/users/:id',\n    authenticate,\n    requireRole(Role.ADMIN),\n    async (req, res) => {\n        // Only admins can delete users\n        await db.users.delete(req.params.id);\n        res.json({ message: 'User deleted' });\n    }\n);\n```\n\n### Pattern 2: Permission-Based Access Control\n\n```typescript\nenum Permission {\n    READ_USERS = 'read:users',\n    WRITE_USERS = 'write:users',\n    DELETE_USERS = 'delete:users',\n    READ_POSTS = 'read:posts',\n    WRITE_POSTS = 'write:posts',\n}\n\nconst rolePermissions: Record<Role, Permission[]> = {\n    [Role.USER]: [Permission.READ_POSTS, Permission.WRITE_POSTS],\n    [Role.MODERATOR]: [\n        Permission.READ_POSTS,\n        Permission.WRITE_POSTS,\n        Permission.READ_USERS,\n    ],\n    [Role.ADMIN]: Object.values(Permission),\n};\n\nfunction hasPermission(userRole: Role, permission: Permission): boolean {\n    return rolePermissions[userRole]?.includes(permission) ?? false;\n}\n\nfunction requirePermission(...permissions: Permission[]) {\n    return (req: Request, res: Response, next: NextFunction) => {\n        if (!req.user) {\n            return res.status(401).json({ error: 'Not authenticated' });\n        }\n\n        const hasAllPermissions = permissions.every(permission =>\n            hasPermission(req.user.role, permission)\n        );\n\n        if (!hasAllPermissions) {\n            return res.status(403).json({ error: 'Insufficient permissions' });\n        }\n\n        next();\n    };\n}\n\n// Usage\napp.get('/api/users',\n    authenticate,\n    requirePermission(Permission.READ_USERS),\n    async (req, res) => {\n        const users = await db.users.findAll();\n        res.json({ users });\n    }\n);\n```\n\n### Pattern 3: Resource Ownership\n\n```typescript\n// Check if user owns resource\nasync function requireOwnership(\n    resourceType: 'post' | 'comment',\n    resourceIdParam: string = 'id'\n) {\n    return async (req: Request, res: Response, next: NextFunction) => {\n        if (!req.user) {\n            return res.status(401).json({ error: 'Not authenticated' });\n        }\n\n        const resourceId = req.params[resourceIdParam];\n\n        // Admins can access anything\n        if (req.user.role === Role.ADMIN) {\n            return next();\n        }\n\n        // Check ownership\n        let resource;\n        if (resourceType === 'post') {\n            resource = await db.posts.findById(resourceId);\n        } else if (resourceType === 'comment') {\n            resource = await db.comments.findById(resourceId);\n        }\n\n        if (!resource) {\n            return res.status(404).json({ error: 'Resource not found' });\n        }\n\n        if (resource.userId !== req.user.userId) {\n            return res.status(403).json({ error: 'Not authorized' });\n        }\n\n        next();\n    };\n}\n\n// Usage\napp.put('/api/posts/:id',\n    authenticate,\n    requireOwnership('post'),\n    async (req, res) => {\n        // User can only update their own posts\n        const post = await db.posts.update(req.params.id, req.body);\n        res.json({ post });\n    }\n);\n```\n\n## Security Best Practices\n\n### Pattern 1: Password Security\n\n```typescript\nimport bcrypt from 'bcrypt';\nimport { z } from 'zod';\n\n// Password validation schema\nconst passwordSchema = z.string()\n    .min(12, 'Password must be at least 12 characters')\n    .regex(/[A-Z]/, 'Password must contain uppercase letter')\n    .regex(/[a-z]/, 'Password must contain lowercase letter')\n    .regex(/[0-9]/, 'Password must contain number')\n    .regex(/[^A-Za-z0-9]/, 'Password must contain special character');\n\n// Hash password\nasync function hashPassword(password: string): Promise<string> {\n    const saltRounds = 12;  // 2^12 iterations\n    return bcrypt.hash(password, saltRounds);\n}\n\n// Verify password\nasync function verifyPassword(\n    password: string,\n    hash: string\n): Promise<boolean> {\n    return bcrypt.compare(password, hash);\n}\n\n// Registration with password validation\napp.post('/api/auth/register', async (req, res) => {\n    try {\n        const { email, password } = req.body;\n\n        // Validate password\n        passwordSchema.parse(password);\n\n        // Check if user exists\n        const existingUser = await db.users.findOne({ email });\n        if (existingUser) {\n            return res.status(400).json({ error: 'Email already registered' });\n        }\n\n        // Hash password\n        const passwordHash = await hashPassword(password);\n\n        // Create user\n        const user = await db.users.create({\n            email,\n            passwordHash,\n        });\n\n        // Generate tokens\n        const tokens = generateTokens(user.id, user.email, user.role);\n\n        res.status(201).json({\n            user: { id: user.id, email: user.email },\n            ...tokens,\n        });\n    } catch (error) {\n        if (error instanceof z.ZodError) {\n            return res.status(400).json({ error: error.errors[0].message });\n        }\n        res.status(500).json({ error: 'Registration failed' });\n    }\n});\n```\n\n### Pattern 2: Rate Limiting\n\n```typescript\nimport rateLimit from 'express-rate-limit';\nimport RedisStore from 'rate-limit-redis';\n\n// Login rate limiter\nconst loginLimiter = rateLimit({\n    store: new RedisStore({ client: redisClient }),\n    windowMs: 15 * 60 * 1000,  // 15 minutes\n    max: 5,  // 5 attempts\n    message: 'Too many login attempts, please try again later',\n    standardHeaders: true,\n    legacyHeaders: false,\n});\n\n// API rate limiter\nconst apiLimiter = rateLimit({\n    windowMs: 60 * 1000,  // 1 minute\n    max: 100,  // 100 requests per minute\n    standardHeaders: true,\n});\n\n// Apply to routes\napp.post('/api/auth/login', loginLimiter, async (req, res) => {\n    // Login logic\n});\n\napp.use('/api/', apiLimiter);\n```\n\n## Best Practices\n\n1. **Never Store Plain Passwords**: Always hash with bcrypt/argon2\n2. **Use HTTPS**: Encrypt data in transit\n3. **Short-Lived Access Tokens**: 15-30 minutes max\n4. **Secure Cookies**: httpOnly, secure, sameSite flags\n5. **Validate All Input**: Email format, password strength\n6. **Rate Limit Auth Endpoints**: Prevent brute force attacks\n7. **Implement CSRF Protection**: For session-based auth\n8. **Rotate Secrets Regularly**: JWT secrets, session secrets\n9. **Log Security Events**: Login attempts, failed auth\n10. **Use MFA When Possible**: Extra security layer\n\n## Common Pitfalls\n\n- **Weak Passwords**: Enforce strong password policies\n- **JWT in localStorage**: Vulnerable to XSS, use httpOnly cookies\n- **No Token Expiration**: Tokens should expire\n- **Client-Side Auth Checks Only**: Always validate server-side\n- **Insecure Password Reset**: Use secure tokens with expiration\n- **No Rate Limiting**: Vulnerable to brute force\n- **Trusting Client Data**: Always validate on server\n\n## Resources\n\n- **references/jwt-best-practices.md**: JWT implementation guide\n- **references/oauth2-flows.md**: OAuth2 flow diagrams and examples\n- **references/session-security.md**: Secure session management\n- **assets/auth-security-checklist.md**: Security review checklist\n- **assets/password-policy-template.md**: Password requirements template\n- **scripts/token-validator.ts**: JWT validation utility"
              },
              {
                "name": "bazel-build-optimization",
                "description": "Optimize Bazel builds for large-scale monorepos. Use when configuring Bazel, implementing remote execution, or optimizing build performance for enterprise codebases.",
                "path": "plugins/developer-essentials/skills/bazel-build-optimization/SKILL.md",
                "frontmatter": {
                  "name": "bazel-build-optimization",
                  "description": "Optimize Bazel builds for large-scale monorepos. Use when configuring Bazel, implementing remote execution, or optimizing build performance for enterprise codebases."
                },
                "content": "# Bazel Build Optimization\n\nProduction patterns for Bazel in large-scale monorepos.\n\n## When to Use This Skill\n\n- Setting up Bazel for monorepos\n- Configuring remote caching/execution\n- Optimizing build times\n- Writing custom Bazel rules\n- Debugging build issues\n- Migrating to Bazel\n\n## Core Concepts\n\n### 1. Bazel Architecture\n\n```\nworkspace/\nâ”œâ”€â”€ WORKSPACE.bazel       # External dependencies\nâ”œâ”€â”€ .bazelrc              # Build configurations\nâ”œâ”€â”€ .bazelversion         # Bazel version\nâ”œâ”€â”€ BUILD.bazel           # Root build file\nâ”œâ”€â”€ apps/\nâ”‚   â””â”€â”€ web/\nâ”‚       â””â”€â”€ BUILD.bazel\nâ”œâ”€â”€ libs/\nâ”‚   â””â”€â”€ utils/\nâ”‚       â””â”€â”€ BUILD.bazel\nâ””â”€â”€ tools/\n    â””â”€â”€ bazel/\n        â””â”€â”€ rules/\n```\n\n### 2. Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Target** | Buildable unit (library, binary, test) |\n| **Package** | Directory with BUILD file |\n| **Label** | Target identifier `//path/to:target` |\n| **Rule** | Defines how to build a target |\n| **Aspect** | Cross-cutting build behavior |\n\n## Templates\n\n### Template 1: WORKSPACE Configuration\n\n```python\n# WORKSPACE.bazel\nworkspace(name = \"myproject\")\n\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n\n# Rules for JavaScript/TypeScript\nhttp_archive(\n    name = \"aspect_rules_js\",\n    sha256 = \"...\",\n    strip_prefix = \"rules_js-1.34.0\",\n    url = \"https://github.com/aspect-build/rules_js/releases/download/v1.34.0/rules_js-v1.34.0.tar.gz\",\n)\n\nload(\"@aspect_rules_js//js:repositories.bzl\", \"rules_js_dependencies\")\nrules_js_dependencies()\n\nload(\"@rules_nodejs//nodejs:repositories.bzl\", \"nodejs_register_toolchains\")\nnodejs_register_toolchains(\n    name = \"nodejs\",\n    node_version = \"20.9.0\",\n)\n\nload(\"@aspect_rules_js//npm:repositories.bzl\", \"npm_translate_lock\")\nnpm_translate_lock(\n    name = \"npm\",\n    pnpm_lock = \"//:pnpm-lock.yaml\",\n    verify_node_modules_ignored = \"//:.bazelignore\",\n)\n\nload(\"@npm//:repositories.bzl\", \"npm_repositories\")\nnpm_repositories()\n\n# Rules for Python\nhttp_archive(\n    name = \"rules_python\",\n    sha256 = \"...\",\n    strip_prefix = \"rules_python-0.27.0\",\n    url = \"https://github.com/bazelbuild/rules_python/releases/download/0.27.0/rules_python-0.27.0.tar.gz\",\n)\n\nload(\"@rules_python//python:repositories.bzl\", \"py_repositories\")\npy_repositories()\n```\n\n### Template 2: .bazelrc Configuration\n\n```bash\n# .bazelrc\n\n# Build settings\nbuild --enable_platform_specific_config\nbuild --incompatible_enable_cc_toolchain_resolution\nbuild --experimental_strict_conflict_checks\n\n# Performance\nbuild --jobs=auto\nbuild --local_cpu_resources=HOST_CPUS*.75\nbuild --local_ram_resources=HOST_RAM*.75\n\n# Caching\nbuild --disk_cache=~/.cache/bazel-disk\nbuild --repository_cache=~/.cache/bazel-repo\n\n# Remote caching (optional)\nbuild:remote-cache --remote_cache=grpcs://cache.example.com\nbuild:remote-cache --remote_upload_local_results=true\nbuild:remote-cache --remote_timeout=3600\n\n# Remote execution (optional)\nbuild:remote-exec --remote_executor=grpcs://remote.example.com\nbuild:remote-exec --remote_instance_name=projects/myproject/instances/default\nbuild:remote-exec --jobs=500\n\n# Platform configurations\nbuild:linux --platforms=//platforms:linux_x86_64\nbuild:macos --platforms=//platforms:macos_arm64\n\n# CI configuration\nbuild:ci --config=remote-cache\nbuild:ci --build_metadata=ROLE=CI\nbuild:ci --bes_results_url=https://results.example.com/invocation/\nbuild:ci --bes_backend=grpcs://bes.example.com\n\n# Test settings\ntest --test_output=errors\ntest --test_summary=detailed\n\n# Coverage\ncoverage --combined_report=lcov\ncoverage --instrumentation_filter=\"//...\"\n\n# Convenience aliases\nbuild:opt --compilation_mode=opt\nbuild:dbg --compilation_mode=dbg\n\n# Import user settings\ntry-import %workspace%/user.bazelrc\n```\n\n### Template 3: TypeScript Library BUILD\n\n```python\n# libs/utils/BUILD.bazel\nload(\"@aspect_rules_ts//ts:defs.bzl\", \"ts_project\")\nload(\"@aspect_rules_js//js:defs.bzl\", \"js_library\")\nload(\"@npm//:defs.bzl\", \"npm_link_all_packages\")\n\nnpm_link_all_packages(name = \"node_modules\")\n\nts_project(\n    name = \"utils_ts\",\n    srcs = glob([\"src/**/*.ts\"]),\n    declaration = True,\n    source_map = True,\n    tsconfig = \"//:tsconfig.json\",\n    deps = [\n        \":node_modules/@types/node\",\n    ],\n)\n\njs_library(\n    name = \"utils\",\n    srcs = [\":utils_ts\"],\n    visibility = [\"//visibility:public\"],\n)\n\n# Tests\nload(\"@aspect_rules_jest//jest:defs.bzl\", \"jest_test\")\n\njest_test(\n    name = \"utils_test\",\n    config = \"//:jest.config.js\",\n    data = [\n        \":utils\",\n        \"//:node_modules/jest\",\n    ],\n    node_modules = \"//:node_modules\",\n)\n```\n\n### Template 4: Python Library BUILD\n\n```python\n# libs/ml/BUILD.bazel\nload(\"@rules_python//python:defs.bzl\", \"py_library\", \"py_test\", \"py_binary\")\nload(\"@pip//:requirements.bzl\", \"requirement\")\n\npy_library(\n    name = \"ml\",\n    srcs = glob([\"src/**/*.py\"]),\n    deps = [\n        requirement(\"numpy\"),\n        requirement(\"pandas\"),\n        requirement(\"scikit-learn\"),\n        \"//libs/utils:utils_py\",\n    ],\n    visibility = [\"//visibility:public\"],\n)\n\npy_test(\n    name = \"ml_test\",\n    srcs = glob([\"tests/**/*.py\"]),\n    deps = [\n        \":ml\",\n        requirement(\"pytest\"),\n    ],\n    size = \"medium\",\n    timeout = \"moderate\",\n)\n\npy_binary(\n    name = \"train\",\n    srcs = [\"train.py\"],\n    deps = [\":ml\"],\n    data = [\"//data:training_data\"],\n)\n```\n\n### Template 5: Custom Rule for Docker\n\n```python\n# tools/bazel/rules/docker.bzl\ndef _docker_image_impl(ctx):\n    dockerfile = ctx.file.dockerfile\n    base_image = ctx.attr.base_image\n    layers = ctx.files.layers\n\n    # Build the image\n    output = ctx.actions.declare_file(ctx.attr.name + \".tar\")\n\n    args = ctx.actions.args()\n    args.add(\"--dockerfile\", dockerfile)\n    args.add(\"--output\", output)\n    args.add(\"--base\", base_image)\n    args.add_all(\"--layer\", layers)\n\n    ctx.actions.run(\n        inputs = [dockerfile] + layers,\n        outputs = [output],\n        executable = ctx.executable._builder,\n        arguments = [args],\n        mnemonic = \"DockerBuild\",\n        progress_message = \"Building Docker image %s\" % ctx.label,\n    )\n\n    return [DefaultInfo(files = depset([output]))]\n\ndocker_image = rule(\n    implementation = _docker_image_impl,\n    attrs = {\n        \"dockerfile\": attr.label(\n            allow_single_file = [\".dockerfile\", \"Dockerfile\"],\n            mandatory = True,\n        ),\n        \"base_image\": attr.string(mandatory = True),\n        \"layers\": attr.label_list(allow_files = True),\n        \"_builder\": attr.label(\n            default = \"//tools/docker:builder\",\n            executable = True,\n            cfg = \"exec\",\n        ),\n    },\n)\n```\n\n### Template 6: Query and Dependency Analysis\n\n```bash\n# Find all dependencies of a target\nbazel query \"deps(//apps/web:web)\"\n\n# Find reverse dependencies (what depends on this)\nbazel query \"rdeps(//..., //libs/utils:utils)\"\n\n# Find all targets in a package\nbazel query \"//libs/...\"\n\n# Find changed targets since commit\nbazel query \"rdeps(//..., set($(git diff --name-only HEAD~1 | sed 's/.*/\"&\"/' | tr '\\n' ' ')))\"\n\n# Generate dependency graph\nbazel query \"deps(//apps/web:web)\" --output=graph | dot -Tpng > deps.png\n\n# Find all test targets\nbazel query \"kind('.*_test', //...)\"\n\n# Find targets with specific tag\nbazel query \"attr(tags, 'integration', //...)\"\n\n# Compute build graph size\nbazel query \"deps(//...)\" --output=package | wc -l\n```\n\n### Template 7: Remote Execution Setup\n\n```python\n# platforms/BUILD.bazel\nplatform(\n    name = \"linux_x86_64\",\n    constraint_values = [\n        \"@platforms//os:linux\",\n        \"@platforms//cpu:x86_64\",\n    ],\n    exec_properties = {\n        \"container-image\": \"docker://gcr.io/myproject/bazel-worker:latest\",\n        \"OSFamily\": \"Linux\",\n    },\n)\n\nplatform(\n    name = \"remote_linux\",\n    parents = [\":linux_x86_64\"],\n    exec_properties = {\n        \"Pool\": \"default\",\n        \"dockerNetwork\": \"standard\",\n    },\n)\n\n# toolchains/BUILD.bazel\ntoolchain(\n    name = \"cc_toolchain_linux\",\n    exec_compatible_with = [\n        \"@platforms//os:linux\",\n        \"@platforms//cpu:x86_64\",\n    ],\n    target_compatible_with = [\n        \"@platforms//os:linux\",\n        \"@platforms//cpu:x86_64\",\n    ],\n    toolchain = \"@remotejdk11_linux//:jdk\",\n    toolchain_type = \"@bazel_tools//tools/jdk:runtime_toolchain_type\",\n)\n```\n\n## Performance Optimization\n\n```bash\n# Profile build\nbazel build //... --profile=profile.json\nbazel analyze-profile profile.json\n\n# Identify slow actions\nbazel build //... --execution_log_json_file=exec_log.json\n\n# Memory profiling\nbazel build //... --memory_profile=memory.json\n\n# Skip analysis cache\nbazel build //... --notrack_incremental_state\n```\n\n## Best Practices\n\n### Do's\n- **Use fine-grained targets** - Better caching\n- **Pin dependencies** - Reproducible builds\n- **Enable remote caching** - Share build artifacts\n- **Use visibility wisely** - Enforce architecture\n- **Write BUILD files per directory** - Standard convention\n\n### Don'ts\n- **Don't use glob for deps** - Explicit is better\n- **Don't commit bazel-* dirs** - Add to .gitignore\n- **Don't skip WORKSPACE setup** - Foundation of build\n- **Don't ignore build warnings** - Technical debt\n\n## Resources\n\n- [Bazel Documentation](https://bazel.build/docs)\n- [Bazel Remote Execution](https://bazel.build/docs/remote-execution)\n- [rules_js](https://github.com/aspect-build/rules_js)"
              },
              {
                "name": "code-review-excellence",
                "description": "Master effective code review practices to provide constructive feedback, catch bugs early, and foster knowledge sharing while maintaining team morale. Use when reviewing pull requests, establishing review standards, or mentoring developers.",
                "path": "plugins/developer-essentials/skills/code-review-excellence/SKILL.md",
                "frontmatter": {
                  "name": "code-review-excellence",
                  "description": "Master effective code review practices to provide constructive feedback, catch bugs early, and foster knowledge sharing while maintaining team morale. Use when reviewing pull requests, establishing review standards, or mentoring developers."
                },
                "content": "# Code Review Excellence\n\nTransform code reviews from gatekeeping to knowledge sharing through constructive feedback, systematic analysis, and collaborative improvement.\n\n## When to Use This Skill\n\n- Reviewing pull requests and code changes\n- Establishing code review standards for teams\n- Mentoring junior developers through reviews\n- Conducting architecture reviews\n- Creating review checklists and guidelines\n- Improving team collaboration\n- Reducing code review cycle time\n- Maintaining code quality standards\n\n## Core Principles\n\n### 1. The Review Mindset\n\n**Goals of Code Review:**\n- Catch bugs and edge cases\n- Ensure code maintainability\n- Share knowledge across team\n- Enforce coding standards\n- Improve design and architecture\n- Build team culture\n\n**Not the Goals:**\n- Show off knowledge\n- Nitpick formatting (use linters)\n- Block progress unnecessarily\n- Rewrite to your preference\n\n### 2. Effective Feedback\n\n**Good Feedback is:**\n- Specific and actionable\n- Educational, not judgmental\n- Focused on the code, not the person\n- Balanced (praise good work too)\n- Prioritized (critical vs nice-to-have)\n\n```markdown\nâŒ Bad: \"This is wrong.\"\nâœ… Good: \"This could cause a race condition when multiple users\n         access simultaneously. Consider using a mutex here.\"\n\nâŒ Bad: \"Why didn't you use X pattern?\"\nâœ… Good: \"Have you considered the Repository pattern? It would\n         make this easier to test. Here's an example: [link]\"\n\nâŒ Bad: \"Rename this variable.\"\nâœ… Good: \"[nit] Consider `userCount` instead of `uc` for\n         clarity. Not blocking if you prefer to keep it.\"\n```\n\n### 3. Review Scope\n\n**What to Review:**\n- Logic correctness and edge cases\n- Security vulnerabilities\n- Performance implications\n- Test coverage and quality\n- Error handling\n- Documentation and comments\n- API design and naming\n- Architectural fit\n\n**What Not to Review Manually:**\n- Code formatting (use Prettier, Black, etc.)\n- Import organization\n- Linting violations\n- Simple typos\n\n## Review Process\n\n### Phase 1: Context Gathering (2-3 minutes)\n\n```markdown\nBefore diving into code, understand:\n\n1. Read PR description and linked issue\n2. Check PR size (>400 lines? Ask to split)\n3. Review CI/CD status (tests passing?)\n4. Understand the business requirement\n5. Note any relevant architectural decisions\n```\n\n### Phase 2: High-Level Review (5-10 minutes)\n\n```markdown\n1. **Architecture & Design**\n   - Does the solution fit the problem?\n   - Are there simpler approaches?\n   - Is it consistent with existing patterns?\n   - Will it scale?\n\n2. **File Organization**\n   - Are new files in the right places?\n   - Is code grouped logically?\n   - Are there duplicate files?\n\n3. **Testing Strategy**\n   - Are there tests?\n   - Do tests cover edge cases?\n   - Are tests readable?\n```\n\n### Phase 3: Line-by-Line Review (10-20 minutes)\n\n```markdown\nFor each file:\n\n1. **Logic & Correctness**\n   - Edge cases handled?\n   - Off-by-one errors?\n   - Null/undefined checks?\n   - Race conditions?\n\n2. **Security**\n   - Input validation?\n   - SQL injection risks?\n   - XSS vulnerabilities?\n   - Sensitive data exposure?\n\n3. **Performance**\n   - N+1 queries?\n   - Unnecessary loops?\n   - Memory leaks?\n   - Blocking operations?\n\n4. **Maintainability**\n   - Clear variable names?\n   - Functions doing one thing?\n   - Complex code commented?\n   - Magic numbers extracted?\n```\n\n### Phase 4: Summary & Decision (2-3 minutes)\n\n```markdown\n1. Summarize key concerns\n2. Highlight what you liked\n3. Make clear decision:\n   - âœ… Approve\n   - ðŸ’¬ Comment (minor suggestions)\n   - ðŸ”„ Request Changes (must address)\n4. Offer to pair if complex\n```\n\n## Review Techniques\n\n### Technique 1: The Checklist Method\n\n```markdown\n## Security Checklist\n- [ ] User input validated and sanitized\n- [ ] SQL queries use parameterization\n- [ ] Authentication/authorization checked\n- [ ] Secrets not hardcoded\n- [ ] Error messages don't leak info\n\n## Performance Checklist\n- [ ] No N+1 queries\n- [ ] Database queries indexed\n- [ ] Large lists paginated\n- [ ] Expensive operations cached\n- [ ] No blocking I/O in hot paths\n\n## Testing Checklist\n- [ ] Happy path tested\n- [ ] Edge cases covered\n- [ ] Error cases tested\n- [ ] Test names are descriptive\n- [ ] Tests are deterministic\n```\n\n### Technique 2: The Question Approach\n\nInstead of stating problems, ask questions to encourage thinking:\n\n```markdown\nâŒ \"This will fail if the list is empty.\"\nâœ… \"What happens if `items` is an empty array?\"\n\nâŒ \"You need error handling here.\"\nâœ… \"How should this behave if the API call fails?\"\n\nâŒ \"This is inefficient.\"\nâœ… \"I see this loops through all users. Have we considered\n    the performance impact with 100k users?\"\n```\n\n### Technique 3: Suggest, Don't Command\n\n```markdown\n## Use Collaborative Language\n\nâŒ \"You must change this to use async/await\"\nâœ… \"Suggestion: async/await might make this more readable:\n    ```typescript\n    async function fetchUser(id: string) {\n        const user = await db.query('SELECT * FROM users WHERE id = ?', id);\n        return user;\n    }\n    ```\n    What do you think?\"\n\nâŒ \"Extract this into a function\"\nâœ… \"This logic appears in 3 places. Would it make sense to\n    extract it into a shared utility function?\"\n```\n\n### Technique 4: Differentiate Severity\n\n```markdown\nUse labels to indicate priority:\n\nðŸ”´ [blocking] - Must fix before merge\nðŸŸ¡ [important] - Should fix, discuss if disagree\nðŸŸ¢ [nit] - Nice to have, not blocking\nðŸ’¡ [suggestion] - Alternative approach to consider\nðŸ“š [learning] - Educational comment, no action needed\nðŸŽ‰ [praise] - Good work, keep it up!\n\nExample:\n\"ðŸ”´ [blocking] This SQL query is vulnerable to injection.\n Please use parameterized queries.\"\n\n\"ðŸŸ¢ [nit] Consider renaming `data` to `userData` for clarity.\"\n\n\"ðŸŽ‰ [praise] Excellent test coverage! This will catch edge cases.\"\n```\n\n## Language-Specific Patterns\n\n### Python Code Review\n\n```python\n# Check for Python-specific issues\n\n# âŒ Mutable default arguments\ndef add_item(item, items=[]):  # Bug! Shared across calls\n    items.append(item)\n    return items\n\n# âœ… Use None as default\ndef add_item(item, items=None):\n    if items is None:\n        items = []\n    items.append(item)\n    return items\n\n# âŒ Catching too broad\ntry:\n    result = risky_operation()\nexcept:  # Catches everything, even KeyboardInterrupt!\n    pass\n\n# âœ… Catch specific exceptions\ntry:\n    result = risky_operation()\nexcept ValueError as e:\n    logger.error(f\"Invalid value: {e}\")\n    raise\n\n# âŒ Using mutable class attributes\nclass User:\n    permissions = []  # Shared across all instances!\n\n# âœ… Initialize in __init__\nclass User:\n    def __init__(self):\n        self.permissions = []\n```\n\n### TypeScript/JavaScript Code Review\n\n```typescript\n// Check for TypeScript-specific issues\n\n// âŒ Using any defeats type safety\nfunction processData(data: any) {  // Avoid any\n    return data.value;\n}\n\n// âœ… Use proper types\ninterface DataPayload {\n    value: string;\n}\nfunction processData(data: DataPayload) {\n    return data.value;\n}\n\n// âŒ Not handling async errors\nasync function fetchUser(id: string) {\n    const response = await fetch(`/api/users/${id}`);\n    return response.json();  // What if network fails?\n}\n\n// âœ… Handle errors properly\nasync function fetchUser(id: string): Promise<User> {\n    try {\n        const response = await fetch(`/api/users/${id}`);\n        if (!response.ok) {\n            throw new Error(`HTTP ${response.status}`);\n        }\n        return await response.json();\n    } catch (error) {\n        console.error('Failed to fetch user:', error);\n        throw error;\n    }\n}\n\n// âŒ Mutation of props\nfunction UserProfile({ user }: Props) {\n    user.lastViewed = new Date();  // Mutating prop!\n    return <div>{user.name}</div>;\n}\n\n// âœ… Don't mutate props\nfunction UserProfile({ user, onView }: Props) {\n    useEffect(() => {\n        onView(user.id);  // Notify parent to update\n    }, [user.id]);\n    return <div>{user.name}</div>;\n}\n```\n\n## Advanced Review Patterns\n\n### Pattern 1: Architectural Review\n\n```markdown\nWhen reviewing significant changes:\n\n1. **Design Document First**\n   - For large features, request design doc before code\n   - Review design with team before implementation\n   - Agree on approach to avoid rework\n\n2. **Review in Stages**\n   - First PR: Core abstractions and interfaces\n   - Second PR: Implementation\n   - Third PR: Integration and tests\n   - Easier to review, faster to iterate\n\n3. **Consider Alternatives**\n   - \"Have we considered using [pattern/library]?\"\n   - \"What's the tradeoff vs. the simpler approach?\"\n   - \"How will this evolve as requirements change?\"\n```\n\n### Pattern 2: Test Quality Review\n\n```typescript\n// âŒ Poor test: Implementation detail testing\ntest('increments counter variable', () => {\n    const component = render(<Counter />);\n    const button = component.getByRole('button');\n    fireEvent.click(button);\n    expect(component.state.counter).toBe(1);  // Testing internal state\n});\n\n// âœ… Good test: Behavior testing\ntest('displays incremented count when clicked', () => {\n    render(<Counter />);\n    const button = screen.getByRole('button', { name: /increment/i });\n    fireEvent.click(button);\n    expect(screen.getByText('Count: 1')).toBeInTheDocument();\n});\n\n// Review questions for tests:\n// - Do tests describe behavior, not implementation?\n// - Are test names clear and descriptive?\n// - Do tests cover edge cases?\n// - Are tests independent (no shared state)?\n// - Can tests run in any order?\n```\n\n### Pattern 3: Security Review\n\n```markdown\n## Security Review Checklist\n\n### Authentication & Authorization\n- [ ] Is authentication required where needed?\n- [ ] Are authorization checks before every action?\n- [ ] Is JWT validation proper (signature, expiry)?\n- [ ] Are API keys/secrets properly secured?\n\n### Input Validation\n- [ ] All user inputs validated?\n- [ ] File uploads restricted (size, type)?\n- [ ] SQL queries parameterized?\n- [ ] XSS protection (escape output)?\n\n### Data Protection\n- [ ] Passwords hashed (bcrypt/argon2)?\n- [ ] Sensitive data encrypted at rest?\n- [ ] HTTPS enforced for sensitive data?\n- [ ] PII handled according to regulations?\n\n### Common Vulnerabilities\n- [ ] No eval() or similar dynamic execution?\n- [ ] No hardcoded secrets?\n- [ ] CSRF protection for state-changing operations?\n- [ ] Rate limiting on public endpoints?\n```\n\n## Giving Difficult Feedback\n\n### Pattern: The Sandwich Method (Modified)\n\n```markdown\nTraditional: Praise + Criticism + Praise (feels fake)\n\nBetter: Context + Specific Issue + Helpful Solution\n\nExample:\n\"I noticed the payment processing logic is inline in the\ncontroller. This makes it harder to test and reuse.\n\n[Specific Issue]\nThe calculateTotal() function mixes tax calculation,\ndiscount logic, and database queries, making it difficult\nto unit test and reason about.\n\n[Helpful Solution]\nCould we extract this into a PaymentService class? That\nwould make it testable and reusable. I can pair with you\non this if helpful.\"\n```\n\n### Handling Disagreements\n\n```markdown\nWhen author disagrees with your feedback:\n\n1. **Seek to Understand**\n   \"Help me understand your approach. What led you to\n    choose this pattern?\"\n\n2. **Acknowledge Valid Points**\n   \"That's a good point about X. I hadn't considered that.\"\n\n3. **Provide Data**\n   \"I'm concerned about performance. Can we add a benchmark\n    to validate the approach?\"\n\n4. **Escalate if Needed**\n   \"Let's get [architect/senior dev] to weigh in on this.\"\n\n5. **Know When to Let Go**\n   If it's working and not a critical issue, approve it.\n   Perfection is the enemy of progress.\n```\n\n## Best Practices\n\n1. **Review Promptly**: Within 24 hours, ideally same day\n2. **Limit PR Size**: 200-400 lines max for effective review\n3. **Review in Time Blocks**: 60 minutes max, take breaks\n4. **Use Review Tools**: GitHub, GitLab, or dedicated tools\n5. **Automate What You Can**: Linters, formatters, security scans\n6. **Build Rapport**: Emoji, praise, and empathy matter\n7. **Be Available**: Offer to pair on complex issues\n8. **Learn from Others**: Review others' review comments\n\n## Common Pitfalls\n\n- **Perfectionism**: Blocking PRs for minor style preferences\n- **Scope Creep**: \"While you're at it, can you also...\"\n- **Inconsistency**: Different standards for different people\n- **Delayed Reviews**: Letting PRs sit for days\n- **Ghosting**: Requesting changes then disappearing\n- **Rubber Stamping**: Approving without actually reviewing\n- **Bike Shedding**: Debating trivial details extensively\n\n## Templates\n\n### PR Review Comment Template\n\n```markdown\n## Summary\n[Brief overview of what was reviewed]\n\n## Strengths\n- [What was done well]\n- [Good patterns or approaches]\n\n## Required Changes\nðŸ”´ [Blocking issue 1]\nðŸ”´ [Blocking issue 2]\n\n## Suggestions\nðŸ’¡ [Improvement 1]\nðŸ’¡ [Improvement 2]\n\n## Questions\nâ“ [Clarification needed on X]\nâ“ [Alternative approach consideration]\n\n## Verdict\nâœ… Approve after addressing required changes\n```\n\n## Resources\n\n- **references/code-review-best-practices.md**: Comprehensive review guidelines\n- **references/common-bugs-checklist.md**: Language-specific bugs to watch for\n- **references/security-review-guide.md**: Security-focused review checklist\n- **assets/pr-review-template.md**: Standard review comment template\n- **assets/review-checklist.md**: Quick reference checklist\n- **scripts/pr-analyzer.py**: Analyze PR complexity and suggest reviewers"
              },
              {
                "name": "debugging-strategies",
                "description": "Master systematic debugging techniques, profiling tools, and root cause analysis to efficiently track down bugs across any codebase or technology stack. Use when investigating bugs, performance issues, or unexpected behavior.",
                "path": "plugins/developer-essentials/skills/debugging-strategies/SKILL.md",
                "frontmatter": {
                  "name": "debugging-strategies",
                  "description": "Master systematic debugging techniques, profiling tools, and root cause analysis to efficiently track down bugs across any codebase or technology stack. Use when investigating bugs, performance issues, or unexpected behavior."
                },
                "content": "# Debugging Strategies\n\nTransform debugging from frustrating guesswork into systematic problem-solving with proven strategies, powerful tools, and methodical approaches.\n\n## When to Use This Skill\n\n- Tracking down elusive bugs\n- Investigating performance issues\n- Understanding unfamiliar codebases\n- Debugging production issues\n- Analyzing crash dumps and stack traces\n- Profiling application performance\n- Investigating memory leaks\n- Debugging distributed systems\n\n## Core Principles\n\n### 1. The Scientific Method\n\n**1. Observe**: What's the actual behavior?\n**2. Hypothesize**: What could be causing it?\n**3. Experiment**: Test your hypothesis\n**4. Analyze**: Did it prove/disprove your theory?\n**5. Repeat**: Until you find the root cause\n\n### 2. Debugging Mindset\n\n**Don't Assume:**\n- \"It can't be X\" - Yes it can\n- \"I didn't change Y\" - Check anyway\n- \"It works on my machine\" - Find out why\n\n**Do:**\n- Reproduce consistently\n- Isolate the problem\n- Keep detailed notes\n- Question everything\n- Take breaks when stuck\n\n### 3. Rubber Duck Debugging\n\nExplain your code and problem out loud (to a rubber duck, colleague, or yourself). Often reveals the issue.\n\n## Systematic Debugging Process\n\n### Phase 1: Reproduce\n\n```markdown\n## Reproduction Checklist\n\n1. **Can you reproduce it?**\n   - Always? Sometimes? Randomly?\n   - Specific conditions needed?\n   - Can others reproduce it?\n\n2. **Create minimal reproduction**\n   - Simplify to smallest example\n   - Remove unrelated code\n   - Isolate the problem\n\n3. **Document steps**\n   - Write down exact steps\n   - Note environment details\n   - Capture error messages\n```\n\n### Phase 2: Gather Information\n\n```markdown\n## Information Collection\n\n1. **Error Messages**\n   - Full stack trace\n   - Error codes\n   - Console/log output\n\n2. **Environment**\n   - OS version\n   - Language/runtime version\n   - Dependencies versions\n   - Environment variables\n\n3. **Recent Changes**\n   - Git history\n   - Deployment timeline\n   - Configuration changes\n\n4. **Scope**\n   - Affects all users or specific ones?\n   - All browsers or specific ones?\n   - Production only or also dev?\n```\n\n### Phase 3: Form Hypothesis\n\n```markdown\n## Hypothesis Formation\n\nBased on gathered info, ask:\n\n1. **What changed?**\n   - Recent code changes\n   - Dependency updates\n   - Infrastructure changes\n\n2. **What's different?**\n   - Working vs broken environment\n   - Working vs broken user\n   - Before vs after\n\n3. **Where could this fail?**\n   - Input validation\n   - Business logic\n   - Data layer\n   - External services\n```\n\n### Phase 4: Test & Verify\n\n```markdown\n## Testing Strategies\n\n1. **Binary Search**\n   - Comment out half the code\n   - Narrow down problematic section\n   - Repeat until found\n\n2. **Add Logging**\n   - Strategic console.log/print\n   - Track variable values\n   - Trace execution flow\n\n3. **Isolate Components**\n   - Test each piece separately\n   - Mock dependencies\n   - Remove complexity\n\n4. **Compare Working vs Broken**\n   - Diff configurations\n   - Diff environments\n   - Diff data\n```\n\n## Debugging Tools\n\n### JavaScript/TypeScript Debugging\n\n```typescript\n// Chrome DevTools Debugger\nfunction processOrder(order: Order) {\n    debugger;  // Execution pauses here\n\n    const total = calculateTotal(order);\n    console.log('Total:', total);\n\n    // Conditional breakpoint\n    if (order.items.length > 10) {\n        debugger;  // Only breaks if condition true\n    }\n\n    return total;\n}\n\n// Console debugging techniques\nconsole.log('Value:', value);                    // Basic\nconsole.table(arrayOfObjects);                   // Table format\nconsole.time('operation'); /* code */ console.timeEnd('operation');  // Timing\nconsole.trace();                                 // Stack trace\nconsole.assert(value > 0, 'Value must be positive');  // Assertion\n\n// Performance profiling\nperformance.mark('start-operation');\n// ... operation code\nperformance.mark('end-operation');\nperformance.measure('operation', 'start-operation', 'end-operation');\nconsole.log(performance.getEntriesByType('measure'));\n```\n\n**VS Code Debugger Configuration:**\n```json\n// .vscode/launch.json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"type\": \"node\",\n            \"request\": \"launch\",\n            \"name\": \"Debug Program\",\n            \"program\": \"${workspaceFolder}/src/index.ts\",\n            \"preLaunchTask\": \"tsc: build - tsconfig.json\",\n            \"outFiles\": [\"${workspaceFolder}/dist/**/*.js\"],\n            \"skipFiles\": [\"<node_internals>/**\"]\n        },\n        {\n            \"type\": \"node\",\n            \"request\": \"launch\",\n            \"name\": \"Debug Tests\",\n            \"program\": \"${workspaceFolder}/node_modules/jest/bin/jest\",\n            \"args\": [\"--runInBand\", \"--no-cache\"],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n```\n\n### Python Debugging\n\n```python\n# Built-in debugger (pdb)\nimport pdb\n\ndef calculate_total(items):\n    total = 0\n    pdb.set_trace()  # Debugger starts here\n\n    for item in items:\n        total += item.price * item.quantity\n\n    return total\n\n# Breakpoint (Python 3.7+)\ndef process_order(order):\n    breakpoint()  # More convenient than pdb.set_trace()\n    # ... code\n\n# Post-mortem debugging\ntry:\n    risky_operation()\nexcept Exception:\n    import pdb\n    pdb.post_mortem()  # Debug at exception point\n\n# IPython debugging (ipdb)\nfrom ipdb import set_trace\nset_trace()  # Better interface than pdb\n\n# Logging for debugging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\ndef fetch_user(user_id):\n    logger.debug(f'Fetching user: {user_id}')\n    user = db.query(User).get(user_id)\n    logger.debug(f'Found user: {user}')\n    return user\n\n# Profile performance\nimport cProfile\nimport pstats\n\ncProfile.run('slow_function()', 'profile_stats')\nstats = pstats.Stats('profile_stats')\nstats.sort_stats('cumulative')\nstats.print_stats(10)  # Top 10 slowest\n```\n\n### Go Debugging\n\n```go\n// Delve debugger\n// Install: go install github.com/go-delve/delve/cmd/dlv@latest\n// Run: dlv debug main.go\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"runtime/debug\"\n)\n\n// Print stack trace\nfunc debugStack() {\n    debug.PrintStack()\n}\n\n// Panic recovery with debugging\nfunc processRequest() {\n    defer func() {\n        if r := recover(); r != nil {\n            fmt.Println(\"Panic:\", r)\n            debug.PrintStack()\n        }\n    }()\n\n    // ... code that might panic\n}\n\n// Memory profiling\nimport _ \"net/http/pprof\"\n// Visit http://localhost:6060/debug/pprof/\n\n// CPU profiling\nimport (\n    \"os\"\n    \"runtime/pprof\"\n)\n\nf, _ := os.Create(\"cpu.prof\")\npprof.StartCPUProfile(f)\ndefer pprof.StopCPUProfile()\n// ... code to profile\n```\n\n## Advanced Debugging Techniques\n\n### Technique 1: Binary Search Debugging\n\n```bash\n# Git bisect for finding regression\ngit bisect start\ngit bisect bad                    # Current commit is bad\ngit bisect good v1.0.0            # v1.0.0 was good\n\n# Git checks out middle commit\n# Test it, then:\ngit bisect good   # if it works\ngit bisect bad    # if it's broken\n\n# Continue until bug found\ngit bisect reset  # when done\n```\n\n### Technique 2: Differential Debugging\n\nCompare working vs broken:\n\n```markdown\n## What's Different?\n\n| Aspect       | Working         | Broken          |\n|--------------|-----------------|-----------------|\n| Environment  | Development     | Production      |\n| Node version | 18.16.0         | 18.15.0         |\n| Data         | Empty DB        | 1M records      |\n| User         | Admin           | Regular user    |\n| Browser      | Chrome          | Safari          |\n| Time         | During day      | After midnight  |\n\nHypothesis: Time-based issue? Check timezone handling.\n```\n\n### Technique 3: Trace Debugging\n\n```typescript\n// Function call tracing\nfunction trace(target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = function(...args: any[]) {\n        console.log(`Calling ${propertyKey} with args:`, args);\n        const result = originalMethod.apply(this, args);\n        console.log(`${propertyKey} returned:`, result);\n        return result;\n    };\n\n    return descriptor;\n}\n\nclass OrderService {\n    @trace\n    calculateTotal(items: Item[]): number {\n        return items.reduce((sum, item) => sum + item.price, 0);\n    }\n}\n```\n\n### Technique 4: Memory Leak Detection\n\n```typescript\n// Chrome DevTools Memory Profiler\n// 1. Take heap snapshot\n// 2. Perform action\n// 3. Take another snapshot\n// 4. Compare snapshots\n\n// Node.js memory debugging\nif (process.memoryUsage().heapUsed > 500 * 1024 * 1024) {\n    console.warn('High memory usage:', process.memoryUsage());\n\n    // Generate heap dump\n    require('v8').writeHeapSnapshot();\n}\n\n// Find memory leaks in tests\nlet beforeMemory: number;\n\nbeforeEach(() => {\n    beforeMemory = process.memoryUsage().heapUsed;\n});\n\nafterEach(() => {\n    const afterMemory = process.memoryUsage().heapUsed;\n    const diff = afterMemory - beforeMemory;\n\n    if (diff > 10 * 1024 * 1024) {  // 10MB threshold\n        console.warn(`Possible memory leak: ${diff / 1024 / 1024}MB`);\n    }\n});\n```\n\n## Debugging Patterns by Issue Type\n\n### Pattern 1: Intermittent Bugs\n\n```markdown\n## Strategies for Flaky Bugs\n\n1. **Add extensive logging**\n   - Log timing information\n   - Log all state transitions\n   - Log external interactions\n\n2. **Look for race conditions**\n   - Concurrent access to shared state\n   - Async operations completing out of order\n   - Missing synchronization\n\n3. **Check timing dependencies**\n   - setTimeout/setInterval\n   - Promise resolution order\n   - Animation frame timing\n\n4. **Stress test**\n   - Run many times\n   - Vary timing\n   - Simulate load\n```\n\n### Pattern 2: Performance Issues\n\n```markdown\n## Performance Debugging\n\n1. **Profile first**\n   - Don't optimize blindly\n   - Measure before and after\n   - Find bottlenecks\n\n2. **Common culprits**\n   - N+1 queries\n   - Unnecessary re-renders\n   - Large data processing\n   - Synchronous I/O\n\n3. **Tools**\n   - Browser DevTools Performance tab\n   - Lighthouse\n   - Python: cProfile, line_profiler\n   - Node: clinic.js, 0x\n```\n\n### Pattern 3: Production Bugs\n\n```markdown\n## Production Debugging\n\n1. **Gather evidence**\n   - Error tracking (Sentry, Bugsnag)\n   - Application logs\n   - User reports\n   - Metrics/monitoring\n\n2. **Reproduce locally**\n   - Use production data (anonymized)\n   - Match environment\n   - Follow exact steps\n\n3. **Safe investigation**\n   - Don't change production\n   - Use feature flags\n   - Add monitoring/logging\n   - Test fixes in staging\n```\n\n## Best Practices\n\n1. **Reproduce First**: Can't fix what you can't reproduce\n2. **Isolate the Problem**: Remove complexity until minimal case\n3. **Read Error Messages**: They're usually helpful\n4. **Check Recent Changes**: Most bugs are recent\n5. **Use Version Control**: Git bisect, blame, history\n6. **Take Breaks**: Fresh eyes see better\n7. **Document Findings**: Help future you\n8. **Fix Root Cause**: Not just symptoms\n\n## Common Debugging Mistakes\n\n- **Making Multiple Changes**: Change one thing at a time\n- **Not Reading Error Messages**: Read the full stack trace\n- **Assuming It's Complex**: Often it's simple\n- **Debug Logging in Prod**: Remove before shipping\n- **Not Using Debugger**: console.log isn't always best\n- **Giving Up Too Soon**: Persistence pays off\n- **Not Testing the Fix**: Verify it actually works\n\n## Quick Debugging Checklist\n\n```markdown\n## When Stuck, Check:\n\n- [ ] Spelling errors (typos in variable names)\n- [ ] Case sensitivity (fileName vs filename)\n- [ ] Null/undefined values\n- [ ] Array index off-by-one\n- [ ] Async timing (race conditions)\n- [ ] Scope issues (closure, hoisting)\n- [ ] Type mismatches\n- [ ] Missing dependencies\n- [ ] Environment variables\n- [ ] File paths (absolute vs relative)\n- [ ] Cache issues (clear cache)\n- [ ] Stale data (refresh database)\n```\n\n## Resources\n\n- **references/debugging-tools-guide.md**: Comprehensive tool documentation\n- **references/performance-profiling.md**: Performance debugging guide\n- **references/production-debugging.md**: Debugging live systems\n- **assets/debugging-checklist.md**: Quick reference checklist\n- **assets/common-bugs.md**: Common bug patterns\n- **scripts/debug-helper.ts**: Debugging utility functions"
              },
              {
                "name": "e2e-testing-patterns",
                "description": "Master end-to-end testing with Playwright and Cypress to build reliable test suites that catch bugs, improve confidence, and enable fast deployment. Use when implementing E2E tests, debugging flaky tests, or establishing testing standards.",
                "path": "plugins/developer-essentials/skills/e2e-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "e2e-testing-patterns",
                  "description": "Master end-to-end testing with Playwright and Cypress to build reliable test suites that catch bugs, improve confidence, and enable fast deployment. Use when implementing E2E tests, debugging flaky tests, or establishing testing standards."
                },
                "content": "# E2E Testing Patterns\n\nBuild reliable, fast, and maintainable end-to-end test suites that provide confidence to ship code quickly and catch regressions before users do.\n\n## When to Use This Skill\n\n- Implementing end-to-end test automation\n- Debugging flaky or unreliable tests\n- Testing critical user workflows\n- Setting up CI/CD test pipelines\n- Testing across multiple browsers\n- Validating accessibility requirements\n- Testing responsive designs\n- Establishing E2E testing standards\n\n## Core Concepts\n\n### 1. E2E Testing Fundamentals\n\n**What to Test with E2E:**\n- Critical user journeys (login, checkout, signup)\n- Complex interactions (drag-and-drop, multi-step forms)\n- Cross-browser compatibility\n- Real API integration\n- Authentication flows\n\n**What NOT to Test with E2E:**\n- Unit-level logic (use unit tests)\n- API contracts (use integration tests)\n- Edge cases (too slow)\n- Internal implementation details\n\n### 2. Test Philosophy\n\n**The Testing Pyramid:**\n```\n        /\\\n       /E2E\\         â† Few, focused on critical paths\n      /â”€â”€â”€â”€â”€\\\n     /Integr\\        â† More, test component interactions\n    /â”€â”€â”€â”€â”€â”€â”€â”€\\\n   /Unit Tests\\      â† Many, fast, isolated\n  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\\n```\n\n**Best Practices:**\n- Test user behavior, not implementation\n- Keep tests independent\n- Make tests deterministic\n- Optimize for speed\n- Use data-testid, not CSS selectors\n\n## Playwright Patterns\n\n### Setup and Configuration\n\n```typescript\n// playwright.config.ts\nimport { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n    testDir: './e2e',\n    timeout: 30000,\n    expect: {\n        timeout: 5000,\n    },\n    fullyParallel: true,\n    forbidOnly: !!process.env.CI,\n    retries: process.env.CI ? 2 : 0,\n    workers: process.env.CI ? 1 : undefined,\n    reporter: [\n        ['html'],\n        ['junit', { outputFile: 'results.xml' }],\n    ],\n    use: {\n        baseURL: 'http://localhost:3000',\n        trace: 'on-first-retry',\n        screenshot: 'only-on-failure',\n        video: 'retain-on-failure',\n    },\n    projects: [\n        { name: 'chromium', use: { ...devices['Desktop Chrome'] } },\n        { name: 'firefox', use: { ...devices['Desktop Firefox'] } },\n        { name: 'webkit', use: { ...devices['Desktop Safari'] } },\n        { name: 'mobile', use: { ...devices['iPhone 13'] } },\n    ],\n});\n```\n\n### Pattern 1: Page Object Model\n\n```typescript\n// pages/LoginPage.ts\nimport { Page, Locator } from '@playwright/test';\n\nexport class LoginPage {\n    readonly page: Page;\n    readonly emailInput: Locator;\n    readonly passwordInput: Locator;\n    readonly loginButton: Locator;\n    readonly errorMessage: Locator;\n\n    constructor(page: Page) {\n        this.page = page;\n        this.emailInput = page.getByLabel('Email');\n        this.passwordInput = page.getByLabel('Password');\n        this.loginButton = page.getByRole('button', { name: 'Login' });\n        this.errorMessage = page.getByRole('alert');\n    }\n\n    async goto() {\n        await this.page.goto('/login');\n    }\n\n    async login(email: string, password: string) {\n        await this.emailInput.fill(email);\n        await this.passwordInput.fill(password);\n        await this.loginButton.click();\n    }\n\n    async getErrorMessage(): Promise<string> {\n        return await this.errorMessage.textContent() ?? '';\n    }\n}\n\n// Test using Page Object\nimport { test, expect } from '@playwright/test';\nimport { LoginPage } from './pages/LoginPage';\n\ntest('successful login', async ({ page }) => {\n    const loginPage = new LoginPage(page);\n    await loginPage.goto();\n    await loginPage.login('user@example.com', 'password123');\n\n    await expect(page).toHaveURL('/dashboard');\n    await expect(page.getByRole('heading', { name: 'Dashboard' }))\n        .toBeVisible();\n});\n\ntest('failed login shows error', async ({ page }) => {\n    const loginPage = new LoginPage(page);\n    await loginPage.goto();\n    await loginPage.login('invalid@example.com', 'wrong');\n\n    const error = await loginPage.getErrorMessage();\n    expect(error).toContain('Invalid credentials');\n});\n```\n\n### Pattern 2: Fixtures for Test Data\n\n```typescript\n// fixtures/test-data.ts\nimport { test as base } from '@playwright/test';\n\ntype TestData = {\n    testUser: {\n        email: string;\n        password: string;\n        name: string;\n    };\n    adminUser: {\n        email: string;\n        password: string;\n    };\n};\n\nexport const test = base.extend<TestData>({\n    testUser: async ({}, use) => {\n        const user = {\n            email: `test-${Date.now()}@example.com`,\n            password: 'Test123!@#',\n            name: 'Test User',\n        };\n        // Setup: Create user in database\n        await createTestUser(user);\n        await use(user);\n        // Teardown: Clean up user\n        await deleteTestUser(user.email);\n    },\n\n    adminUser: async ({}, use) => {\n        await use({\n            email: 'admin@example.com',\n            password: process.env.ADMIN_PASSWORD!,\n        });\n    },\n});\n\n// Usage in tests\nimport { test } from './fixtures/test-data';\n\ntest('user can update profile', async ({ page, testUser }) => {\n    await page.goto('/login');\n    await page.getByLabel('Email').fill(testUser.email);\n    await page.getByLabel('Password').fill(testUser.password);\n    await page.getByRole('button', { name: 'Login' }).click();\n\n    await page.goto('/profile');\n    await page.getByLabel('Name').fill('Updated Name');\n    await page.getByRole('button', { name: 'Save' }).click();\n\n    await expect(page.getByText('Profile updated')).toBeVisible();\n});\n```\n\n### Pattern 3: Waiting Strategies\n\n```typescript\n// âŒ Bad: Fixed timeouts\nawait page.waitForTimeout(3000);  // Flaky!\n\n// âœ… Good: Wait for specific conditions\nawait page.waitForLoadState('networkidle');\nawait page.waitForURL('/dashboard');\nawait page.waitForSelector('[data-testid=\"user-profile\"]');\n\n// âœ… Better: Auto-waiting with assertions\nawait expect(page.getByText('Welcome')).toBeVisible();\nawait expect(page.getByRole('button', { name: 'Submit' }))\n    .toBeEnabled();\n\n// Wait for API response\nconst responsePromise = page.waitForResponse(\n    response => response.url().includes('/api/users') && response.status() === 200\n);\nawait page.getByRole('button', { name: 'Load Users' }).click();\nconst response = await responsePromise;\nconst data = await response.json();\nexpect(data.users).toHaveLength(10);\n\n// Wait for multiple conditions\nawait Promise.all([\n    page.waitForURL('/success'),\n    page.waitForLoadState('networkidle'),\n    expect(page.getByText('Payment successful')).toBeVisible(),\n]);\n```\n\n### Pattern 4: Network Mocking and Interception\n\n```typescript\n// Mock API responses\ntest('displays error when API fails', async ({ page }) => {\n    await page.route('**/api/users', route => {\n        route.fulfill({\n            status: 500,\n            contentType: 'application/json',\n            body: JSON.stringify({ error: 'Internal Server Error' }),\n        });\n    });\n\n    await page.goto('/users');\n    await expect(page.getByText('Failed to load users')).toBeVisible();\n});\n\n// Intercept and modify requests\ntest('can modify API request', async ({ page }) => {\n    await page.route('**/api/users', async route => {\n        const request = route.request();\n        const postData = JSON.parse(request.postData() || '{}');\n\n        // Modify request\n        postData.role = 'admin';\n\n        await route.continue({\n            postData: JSON.stringify(postData),\n        });\n    });\n\n    // Test continues...\n});\n\n// Mock third-party services\ntest('payment flow with mocked Stripe', async ({ page }) => {\n    await page.route('**/api/stripe/**', route => {\n        route.fulfill({\n            status: 200,\n            body: JSON.stringify({\n                id: 'mock_payment_id',\n                status: 'succeeded',\n            }),\n        });\n    });\n\n    // Test payment flow with mocked response\n});\n```\n\n## Cypress Patterns\n\n### Setup and Configuration\n\n```typescript\n// cypress.config.ts\nimport { defineConfig } from 'cypress';\n\nexport default defineConfig({\n    e2e: {\n        baseUrl: 'http://localhost:3000',\n        viewportWidth: 1280,\n        viewportHeight: 720,\n        video: false,\n        screenshotOnRunFailure: true,\n        defaultCommandTimeout: 10000,\n        requestTimeout: 10000,\n        setupNodeEvents(on, config) {\n            // Implement node event listeners\n        },\n    },\n});\n```\n\n### Pattern 1: Custom Commands\n\n```typescript\n// cypress/support/commands.ts\ndeclare global {\n    namespace Cypress {\n        interface Chainable {\n            login(email: string, password: string): Chainable<void>;\n            createUser(userData: UserData): Chainable<User>;\n            dataCy(value: string): Chainable<JQuery<HTMLElement>>;\n        }\n    }\n}\n\nCypress.Commands.add('login', (email: string, password: string) => {\n    cy.visit('/login');\n    cy.get('[data-testid=\"email\"]').type(email);\n    cy.get('[data-testid=\"password\"]').type(password);\n    cy.get('[data-testid=\"login-button\"]').click();\n    cy.url().should('include', '/dashboard');\n});\n\nCypress.Commands.add('createUser', (userData: UserData) => {\n    return cy.request('POST', '/api/users', userData)\n        .its('body');\n});\n\nCypress.Commands.add('dataCy', (value: string) => {\n    return cy.get(`[data-cy=\"${value}\"]`);\n});\n\n// Usage\ncy.login('user@example.com', 'password');\ncy.dataCy('submit-button').click();\n```\n\n### Pattern 2: Cypress Intercept\n\n```typescript\n// Mock API calls\ncy.intercept('GET', '/api/users', {\n    statusCode: 200,\n    body: [\n        { id: 1, name: 'John' },\n        { id: 2, name: 'Jane' },\n    ],\n}).as('getUsers');\n\ncy.visit('/users');\ncy.wait('@getUsers');\ncy.get('[data-testid=\"user-list\"]').children().should('have.length', 2);\n\n// Modify responses\ncy.intercept('GET', '/api/users', (req) => {\n    req.reply((res) => {\n        // Modify response\n        res.body.users = res.body.users.slice(0, 5);\n        res.send();\n    });\n});\n\n// Simulate slow network\ncy.intercept('GET', '/api/data', (req) => {\n    req.reply((res) => {\n        res.delay(3000);  // 3 second delay\n        res.send();\n    });\n});\n```\n\n## Advanced Patterns\n\n### Pattern 1: Visual Regression Testing\n\n```typescript\n// With Playwright\nimport { test, expect } from '@playwright/test';\n\ntest('homepage looks correct', async ({ page }) => {\n    await page.goto('/');\n    await expect(page).toHaveScreenshot('homepage.png', {\n        fullPage: true,\n        maxDiffPixels: 100,\n    });\n});\n\ntest('button in all states', async ({ page }) => {\n    await page.goto('/components');\n\n    const button = page.getByRole('button', { name: 'Submit' });\n\n    // Default state\n    await expect(button).toHaveScreenshot('button-default.png');\n\n    // Hover state\n    await button.hover();\n    await expect(button).toHaveScreenshot('button-hover.png');\n\n    // Disabled state\n    await button.evaluate(el => el.setAttribute('disabled', 'true'));\n    await expect(button).toHaveScreenshot('button-disabled.png');\n});\n```\n\n### Pattern 2: Parallel Testing with Sharding\n\n```typescript\n// playwright.config.ts\nexport default defineConfig({\n    projects: [\n        {\n            name: 'shard-1',\n            use: { ...devices['Desktop Chrome'] },\n            grepInvert: /@slow/,\n            shard: { current: 1, total: 4 },\n        },\n        {\n            name: 'shard-2',\n            use: { ...devices['Desktop Chrome'] },\n            shard: { current: 2, total: 4 },\n        },\n        // ... more shards\n    ],\n});\n\n// Run in CI\n// npx playwright test --shard=1/4\n// npx playwright test --shard=2/4\n```\n\n### Pattern 3: Accessibility Testing\n\n```typescript\n// Install: npm install @axe-core/playwright\nimport { test, expect } from '@playwright/test';\nimport AxeBuilder from '@axe-core/playwright';\n\ntest('page should not have accessibility violations', async ({ page }) => {\n    await page.goto('/');\n\n    const accessibilityScanResults = await new AxeBuilder({ page })\n        .exclude('#third-party-widget')\n        .analyze();\n\n    expect(accessibilityScanResults.violations).toEqual([]);\n});\n\ntest('form is accessible', async ({ page }) => {\n    await page.goto('/signup');\n\n    const results = await new AxeBuilder({ page })\n        .include('form')\n        .analyze();\n\n    expect(results.violations).toEqual([]);\n});\n```\n\n## Best Practices\n\n1. **Use Data Attributes**: `data-testid` or `data-cy` for stable selectors\n2. **Avoid Brittle Selectors**: Don't rely on CSS classes or DOM structure\n3. **Test User Behavior**: Click, type, see - not implementation details\n4. **Keep Tests Independent**: Each test should run in isolation\n5. **Clean Up Test Data**: Create and destroy test data in each test\n6. **Use Page Objects**: Encapsulate page logic\n7. **Meaningful Assertions**: Check actual user-visible behavior\n8. **Optimize for Speed**: Mock when possible, parallel execution\n\n```typescript\n// âŒ Bad selectors\ncy.get('.btn.btn-primary.submit-button').click();\ncy.get('div > form > div:nth-child(2) > input').type('text');\n\n// âœ… Good selectors\ncy.getByRole('button', { name: 'Submit' }).click();\ncy.getByLabel('Email address').type('user@example.com');\ncy.get('[data-testid=\"email-input\"]').type('user@example.com');\n```\n\n## Common Pitfalls\n\n- **Flaky Tests**: Use proper waits, not fixed timeouts\n- **Slow Tests**: Mock external APIs, use parallel execution\n- **Over-Testing**: Don't test every edge case with E2E\n- **Coupled Tests**: Tests should not depend on each other\n- **Poor Selectors**: Avoid CSS classes and nth-child\n- **No Cleanup**: Clean up test data after each test\n- **Testing Implementation**: Test user behavior, not internals\n\n## Debugging Failing Tests\n\n```typescript\n// Playwright debugging\n// 1. Run in headed mode\nnpx playwright test --headed\n\n// 2. Run in debug mode\nnpx playwright test --debug\n\n// 3. Use trace viewer\nawait page.screenshot({ path: 'screenshot.png' });\nawait page.video()?.saveAs('video.webm');\n\n// 4. Add test.step for better reporting\ntest('checkout flow', async ({ page }) => {\n    await test.step('Add item to cart', async () => {\n        await page.goto('/products');\n        await page.getByRole('button', { name: 'Add to Cart' }).click();\n    });\n\n    await test.step('Proceed to checkout', async () => {\n        await page.goto('/cart');\n        await page.getByRole('button', { name: 'Checkout' }).click();\n    });\n});\n\n// 5. Inspect page state\nawait page.pause();  // Pauses execution, opens inspector\n```\n\n## Resources\n\n- **references/playwright-best-practices.md**: Playwright-specific patterns\n- **references/cypress-best-practices.md**: Cypress-specific patterns\n- **references/flaky-test-debugging.md**: Debugging unreliable tests\n- **assets/e2e-testing-checklist.md**: What to test with E2E\n- **assets/selector-strategies.md**: Finding reliable selectors\n- **scripts/test-analyzer.ts**: Analyze test flakiness and duration"
              },
              {
                "name": "error-handling-patterns",
                "description": "Master error handling patterns across languages including exceptions, Result types, error propagation, and graceful degradation to build resilient applications. Use when implementing error handling, designing APIs, or improving application reliability.",
                "path": "plugins/developer-essentials/skills/error-handling-patterns/SKILL.md",
                "frontmatter": {
                  "name": "error-handling-patterns",
                  "description": "Master error handling patterns across languages including exceptions, Result types, error propagation, and graceful degradation to build resilient applications. Use when implementing error handling, designing APIs, or improving application reliability."
                },
                "content": "# Error Handling Patterns\n\nBuild resilient applications with robust error handling strategies that gracefully handle failures and provide excellent debugging experiences.\n\n## When to Use This Skill\n\n- Implementing error handling in new features\n- Designing error-resilient APIs\n- Debugging production issues\n- Improving application reliability\n- Creating better error messages for users and developers\n- Implementing retry and circuit breaker patterns\n- Handling async/concurrent errors\n- Building fault-tolerant distributed systems\n\n## Core Concepts\n\n### 1. Error Handling Philosophies\n\n**Exceptions vs Result Types:**\n- **Exceptions**: Traditional try-catch, disrupts control flow\n- **Result Types**: Explicit success/failure, functional approach\n- **Error Codes**: C-style, requires discipline\n- **Option/Maybe Types**: For nullable values\n\n**When to Use Each:**\n- Exceptions: Unexpected errors, exceptional conditions\n- Result Types: Expected errors, validation failures\n- Panics/Crashes: Unrecoverable errors, programming bugs\n\n### 2. Error Categories\n\n**Recoverable Errors:**\n- Network timeouts\n- Missing files\n- Invalid user input\n- API rate limits\n\n**Unrecoverable Errors:**\n- Out of memory\n- Stack overflow\n- Programming bugs (null pointer, etc.)\n\n## Language-Specific Patterns\n\n### Python Error Handling\n\n**Custom Exception Hierarchy:**\n```python\nclass ApplicationError(Exception):\n    \"\"\"Base exception for all application errors.\"\"\"\n    def __init__(self, message: str, code: str = None, details: dict = None):\n        super().__init__(message)\n        self.code = code\n        self.details = details or {}\n        self.timestamp = datetime.utcnow()\n\nclass ValidationError(ApplicationError):\n    \"\"\"Raised when validation fails.\"\"\"\n    pass\n\nclass NotFoundError(ApplicationError):\n    \"\"\"Raised when resource not found.\"\"\"\n    pass\n\nclass ExternalServiceError(ApplicationError):\n    \"\"\"Raised when external service fails.\"\"\"\n    def __init__(self, message: str, service: str, **kwargs):\n        super().__init__(message, **kwargs)\n        self.service = service\n\n# Usage\ndef get_user(user_id: str) -> User:\n    user = db.query(User).filter_by(id=user_id).first()\n    if not user:\n        raise NotFoundError(\n            f\"User not found\",\n            code=\"USER_NOT_FOUND\",\n            details={\"user_id\": user_id}\n        )\n    return user\n```\n\n**Context Managers for Cleanup:**\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef database_transaction(session):\n    \"\"\"Ensure transaction is committed or rolled back.\"\"\"\n    try:\n        yield session\n        session.commit()\n    except Exception as e:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n# Usage\nwith database_transaction(db.session) as session:\n    user = User(name=\"Alice\")\n    session.add(user)\n    # Automatic commit or rollback\n```\n\n**Retry with Exponential Backoff:**\n```python\nimport time\nfrom functools import wraps\nfrom typing import TypeVar, Callable\n\nT = TypeVar('T')\n\ndef retry(\n    max_attempts: int = 3,\n    backoff_factor: float = 2.0,\n    exceptions: tuple = (Exception,)\n):\n    \"\"\"Retry decorator with exponential backoff.\"\"\"\n    def decorator(func: Callable[..., T]) -> Callable[..., T]:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> T:\n            last_exception = None\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    if attempt < max_attempts - 1:\n                        sleep_time = backoff_factor ** attempt\n                        time.sleep(sleep_time)\n                        continue\n                    raise\n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage\n@retry(max_attempts=3, exceptions=(NetworkError,))\ndef fetch_data(url: str) -> dict:\n    response = requests.get(url, timeout=5)\n    response.raise_for_status()\n    return response.json()\n```\n\n### TypeScript/JavaScript Error Handling\n\n**Custom Error Classes:**\n```typescript\n// Custom error classes\nclass ApplicationError extends Error {\n    constructor(\n        message: string,\n        public code: string,\n        public statusCode: number = 500,\n        public details?: Record<string, any>\n    ) {\n        super(message);\n        this.name = this.constructor.name;\n        Error.captureStackTrace(this, this.constructor);\n    }\n}\n\nclass ValidationError extends ApplicationError {\n    constructor(message: string, details?: Record<string, any>) {\n        super(message, 'VALIDATION_ERROR', 400, details);\n    }\n}\n\nclass NotFoundError extends ApplicationError {\n    constructor(resource: string, id: string) {\n        super(\n            `${resource} not found`,\n            'NOT_FOUND',\n            404,\n            { resource, id }\n        );\n    }\n}\n\n// Usage\nfunction getUser(id: string): User {\n    const user = users.find(u => u.id === id);\n    if (!user) {\n        throw new NotFoundError('User', id);\n    }\n    return user;\n}\n```\n\n**Result Type Pattern:**\n```typescript\n// Result type for explicit error handling\ntype Result<T, E = Error> =\n    | { ok: true; value: T }\n    | { ok: false; error: E };\n\n// Helper functions\nfunction Ok<T>(value: T): Result<T, never> {\n    return { ok: true, value };\n}\n\nfunction Err<E>(error: E): Result<never, E> {\n    return { ok: false, error };\n}\n\n// Usage\nfunction parseJSON<T>(json: string): Result<T, SyntaxError> {\n    try {\n        const value = JSON.parse(json) as T;\n        return Ok(value);\n    } catch (error) {\n        return Err(error as SyntaxError);\n    }\n}\n\n// Consuming Result\nconst result = parseJSON<User>(userJson);\nif (result.ok) {\n    console.log(result.value.name);\n} else {\n    console.error('Parse failed:', result.error.message);\n}\n\n// Chaining Results\nfunction chain<T, U, E>(\n    result: Result<T, E>,\n    fn: (value: T) => Result<U, E>\n): Result<U, E> {\n    return result.ok ? fn(result.value) : result;\n}\n```\n\n**Async Error Handling:**\n```typescript\n// Async/await with proper error handling\nasync function fetchUserOrders(userId: string): Promise<Order[]> {\n    try {\n        const user = await getUser(userId);\n        const orders = await getOrders(user.id);\n        return orders;\n    } catch (error) {\n        if (error instanceof NotFoundError) {\n            return [];  // Return empty array for not found\n        }\n        if (error instanceof NetworkError) {\n            // Retry logic\n            return retryFetchOrders(userId);\n        }\n        // Re-throw unexpected errors\n        throw error;\n    }\n}\n\n// Promise error handling\nfunction fetchData(url: string): Promise<Data> {\n    return fetch(url)\n        .then(response => {\n            if (!response.ok) {\n                throw new NetworkError(`HTTP ${response.status}`);\n            }\n            return response.json();\n        })\n        .catch(error => {\n            console.error('Fetch failed:', error);\n            throw error;\n        });\n}\n```\n\n### Rust Error Handling\n\n**Result and Option Types:**\n```rust\nuse std::fs::File;\nuse std::io::{self, Read};\n\n// Result type for operations that can fail\nfn read_file(path: &str) -> Result<String, io::Error> {\n    let mut file = File::open(path)?;  // ? operator propagates errors\n    let mut contents = String::new();\n    file.read_to_string(&mut contents)?;\n    Ok(contents)\n}\n\n// Custom error types\n#[derive(Debug)]\nenum AppError {\n    Io(io::Error),\n    Parse(std::num::ParseIntError),\n    NotFound(String),\n    Validation(String),\n}\n\nimpl From<io::Error> for AppError {\n    fn from(error: io::Error) -> Self {\n        AppError::Io(error)\n    }\n}\n\n// Using custom error type\nfn read_number_from_file(path: &str) -> Result<i32, AppError> {\n    let contents = read_file(path)?;  // Auto-converts io::Error\n    let number = contents.trim().parse()\n        .map_err(AppError::Parse)?;   // Explicitly convert ParseIntError\n    Ok(number)\n}\n\n// Option for nullable values\nfn find_user(id: &str) -> Option<User> {\n    users.iter().find(|u| u.id == id).cloned()\n}\n\n// Combining Option and Result\nfn get_user_age(id: &str) -> Result<u32, AppError> {\n    find_user(id)\n        .ok_or_else(|| AppError::NotFound(id.to_string()))\n        .map(|user| user.age)\n}\n```\n\n### Go Error Handling\n\n**Explicit Error Returns:**\n```go\n// Basic error handling\nfunc getUser(id string) (*User, error) {\n    user, err := db.QueryUser(id)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to query user: %w\", err)\n    }\n    if user == nil {\n        return nil, errors.New(\"user not found\")\n    }\n    return user, nil\n}\n\n// Custom error types\ntype ValidationError struct {\n    Field   string\n    Message string\n}\n\nfunc (e *ValidationError) Error() string {\n    return fmt.Sprintf(\"validation failed for %s: %s\", e.Field, e.Message)\n}\n\n// Sentinel errors for comparison\nvar (\n    ErrNotFound     = errors.New(\"not found\")\n    ErrUnauthorized = errors.New(\"unauthorized\")\n    ErrInvalidInput = errors.New(\"invalid input\")\n)\n\n// Error checking\nuser, err := getUser(\"123\")\nif err != nil {\n    if errors.Is(err, ErrNotFound) {\n        // Handle not found\n    } else {\n        // Handle other errors\n    }\n}\n\n// Error wrapping and unwrapping\nfunc processUser(id string) error {\n    user, err := getUser(id)\n    if err != nil {\n        return fmt.Errorf(\"process user failed: %w\", err)\n    }\n    // Process user\n    return nil\n}\n\n// Unwrap errors\nerr := processUser(\"123\")\nif err != nil {\n    var valErr *ValidationError\n    if errors.As(err, &valErr) {\n        fmt.Printf(\"Validation error: %s\\n\", valErr.Field)\n    }\n}\n```\n\n## Universal Patterns\n\n### Pattern 1: Circuit Breaker\n\nPrevent cascading failures in distributed systems.\n\n```python\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nfrom typing import Callable, TypeVar\n\nT = TypeVar('T')\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"       # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if recovered\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        timeout: timedelta = timedelta(seconds=60),\n        success_threshold: int = 2\n    ):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.success_threshold = success_threshold\n        self.failure_count = 0\n        self.success_count = 0\n        self.state = CircuitState.CLOSED\n        self.last_failure_time = None\n\n    def call(self, func: Callable[[], T]) -> T:\n        if self.state == CircuitState.OPEN:\n            if datetime.now() - self.last_failure_time > self.timeout:\n                self.state = CircuitState.HALF_OPEN\n                self.success_count = 0\n            else:\n                raise Exception(\"Circuit breaker is OPEN\")\n\n        try:\n            result = func()\n            self.on_success()\n            return result\n        except Exception as e:\n            self.on_failure()\n            raise\n\n    def on_success(self):\n        self.failure_count = 0\n        if self.state == CircuitState.HALF_OPEN:\n            self.success_count += 1\n            if self.success_count >= self.success_threshold:\n                self.state = CircuitState.CLOSED\n                self.success_count = 0\n\n    def on_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n        if self.failure_count >= self.failure_threshold:\n            self.state = CircuitState.OPEN\n\n# Usage\ncircuit_breaker = CircuitBreaker()\n\ndef fetch_data():\n    return circuit_breaker.call(lambda: external_api.get_data())\n```\n\n### Pattern 2: Error Aggregation\n\nCollect multiple errors instead of failing on first error.\n\n```typescript\nclass ErrorCollector {\n    private errors: Error[] = [];\n\n    add(error: Error): void {\n        this.errors.push(error);\n    }\n\n    hasErrors(): boolean {\n        return this.errors.length > 0;\n    }\n\n    getErrors(): Error[] {\n        return [...this.errors];\n    }\n\n    throw(): never {\n        if (this.errors.length === 1) {\n            throw this.errors[0];\n        }\n        throw new AggregateError(\n            this.errors,\n            `${this.errors.length} errors occurred`\n        );\n    }\n}\n\n// Usage: Validate multiple fields\nfunction validateUser(data: any): User {\n    const errors = new ErrorCollector();\n\n    if (!data.email) {\n        errors.add(new ValidationError('Email is required'));\n    } else if (!isValidEmail(data.email)) {\n        errors.add(new ValidationError('Email is invalid'));\n    }\n\n    if (!data.name || data.name.length < 2) {\n        errors.add(new ValidationError('Name must be at least 2 characters'));\n    }\n\n    if (!data.age || data.age < 18) {\n        errors.add(new ValidationError('Age must be 18 or older'));\n    }\n\n    if (errors.hasErrors()) {\n        errors.throw();\n    }\n\n    return data as User;\n}\n```\n\n### Pattern 3: Graceful Degradation\n\nProvide fallback functionality when errors occur.\n\n```python\nfrom typing import Optional, Callable, TypeVar\n\nT = TypeVar('T')\n\ndef with_fallback(\n    primary: Callable[[], T],\n    fallback: Callable[[], T],\n    log_error: bool = True\n) -> T:\n    \"\"\"Try primary function, fall back to fallback on error.\"\"\"\n    try:\n        return primary()\n    except Exception as e:\n        if log_error:\n            logger.error(f\"Primary function failed: {e}\")\n        return fallback()\n\n# Usage\ndef get_user_profile(user_id: str) -> UserProfile:\n    return with_fallback(\n        primary=lambda: fetch_from_cache(user_id),\n        fallback=lambda: fetch_from_database(user_id)\n    )\n\n# Multiple fallbacks\ndef get_exchange_rate(currency: str) -> float:\n    return (\n        try_function(lambda: api_provider_1.get_rate(currency))\n        or try_function(lambda: api_provider_2.get_rate(currency))\n        or try_function(lambda: cache.get_rate(currency))\n        or DEFAULT_RATE\n    )\n\ndef try_function(func: Callable[[], Optional[T]]) -> Optional[T]:\n    try:\n        return func()\n    except Exception:\n        return None\n```\n\n## Best Practices\n\n1. **Fail Fast**: Validate input early, fail quickly\n2. **Preserve Context**: Include stack traces, metadata, timestamps\n3. **Meaningful Messages**: Explain what happened and how to fix it\n4. **Log Appropriately**: Error = log, expected failure = don't spam logs\n5. **Handle at Right Level**: Catch where you can meaningfully handle\n6. **Clean Up Resources**: Use try-finally, context managers, defer\n7. **Don't Swallow Errors**: Log or re-throw, don't silently ignore\n8. **Type-Safe Errors**: Use typed errors when possible\n\n```python\n# Good error handling example\ndef process_order(order_id: str) -> Order:\n    \"\"\"Process order with comprehensive error handling.\"\"\"\n    try:\n        # Validate input\n        if not order_id:\n            raise ValidationError(\"Order ID is required\")\n\n        # Fetch order\n        order = db.get_order(order_id)\n        if not order:\n            raise NotFoundError(\"Order\", order_id)\n\n        # Process payment\n        try:\n            payment_result = payment_service.charge(order.total)\n        except PaymentServiceError as e:\n            # Log and wrap external service error\n            logger.error(f\"Payment failed for order {order_id}: {e}\")\n            raise ExternalServiceError(\n                f\"Payment processing failed\",\n                service=\"payment_service\",\n                details={\"order_id\": order_id, \"amount\": order.total}\n            ) from e\n\n        # Update order\n        order.status = \"completed\"\n        order.payment_id = payment_result.id\n        db.save(order)\n\n        return order\n\n    except ApplicationError:\n        # Re-raise known application errors\n        raise\n    except Exception as e:\n        # Log unexpected errors\n        logger.exception(f\"Unexpected error processing order {order_id}\")\n        raise ApplicationError(\n            \"Order processing failed\",\n            code=\"INTERNAL_ERROR\"\n        ) from e\n```\n\n## Common Pitfalls\n\n- **Catching Too Broadly**: `except Exception` hides bugs\n- **Empty Catch Blocks**: Silently swallowing errors\n- **Logging and Re-throwing**: Creates duplicate log entries\n- **Not Cleaning Up**: Forgetting to close files, connections\n- **Poor Error Messages**: \"Error occurred\" is not helpful\n- **Returning Error Codes**: Use exceptions or Result types\n- **Ignoring Async Errors**: Unhandled promise rejections\n\n## Resources\n\n- **references/exception-hierarchy-design.md**: Designing error class hierarchies\n- **references/error-recovery-strategies.md**: Recovery patterns for different scenarios\n- **references/async-error-handling.md**: Handling errors in concurrent code\n- **assets/error-handling-checklist.md**: Review checklist for error handling\n- **assets/error-message-guide.md**: Writing helpful error messages\n- **scripts/error-analyzer.py**: Analyze error patterns in logs"
              },
              {
                "name": "git-advanced-workflows",
                "description": "Master advanced Git workflows including rebasing, cherry-picking, bisect, worktrees, and reflog to maintain clean history and recover from any situation. Use when managing complex Git histories, collaborating on feature branches, or troubleshooting repository issues.",
                "path": "plugins/developer-essentials/skills/git-advanced-workflows/SKILL.md",
                "frontmatter": {
                  "name": "git-advanced-workflows",
                  "description": "Master advanced Git workflows including rebasing, cherry-picking, bisect, worktrees, and reflog to maintain clean history and recover from any situation. Use when managing complex Git histories, collaborating on feature branches, or troubleshooting repository issues."
                },
                "content": "# Git Advanced Workflows\n\nMaster advanced Git techniques to maintain clean history, collaborate effectively, and recover from any situation with confidence.\n\n## When to Use This Skill\n\n- Cleaning up commit history before merging\n- Applying specific commits across branches\n- Finding commits that introduced bugs\n- Working on multiple features simultaneously\n- Recovering from Git mistakes or lost commits\n- Managing complex branch workflows\n- Preparing clean PRs for review\n- Synchronizing diverged branches\n\n## Core Concepts\n\n### 1. Interactive Rebase\n\nInteractive rebase is the Swiss Army knife of Git history editing.\n\n**Common Operations:**\n- `pick`: Keep commit as-is\n- `reword`: Change commit message\n- `edit`: Amend commit content\n- `squash`: Combine with previous commit\n- `fixup`: Like squash but discard message\n- `drop`: Remove commit entirely\n\n**Basic Usage:**\n```bash\n# Rebase last 5 commits\ngit rebase -i HEAD~5\n\n# Rebase all commits on current branch\ngit rebase -i $(git merge-base HEAD main)\n\n# Rebase onto specific commit\ngit rebase -i abc123\n```\n\n### 2. Cherry-Picking\n\nApply specific commits from one branch to another without merging entire branches.\n\n```bash\n# Cherry-pick single commit\ngit cherry-pick abc123\n\n# Cherry-pick range of commits (exclusive start)\ngit cherry-pick abc123..def456\n\n# Cherry-pick without committing (stage changes only)\ngit cherry-pick -n abc123\n\n# Cherry-pick and edit commit message\ngit cherry-pick -e abc123\n```\n\n### 3. Git Bisect\n\nBinary search through commit history to find the commit that introduced a bug.\n\n```bash\n# Start bisect\ngit bisect start\n\n# Mark current commit as bad\ngit bisect bad\n\n# Mark known good commit\ngit bisect good v1.0.0\n\n# Git will checkout middle commit - test it\n# Then mark as good or bad\ngit bisect good  # or: git bisect bad\n\n# Continue until bug found\n# When done\ngit bisect reset\n```\n\n**Automated Bisect:**\n```bash\n# Use script to test automatically\ngit bisect start HEAD v1.0.0\ngit bisect run ./test.sh\n\n# test.sh should exit 0 for good, 1-127 (except 125) for bad\n```\n\n### 4. Worktrees\n\nWork on multiple branches simultaneously without stashing or switching.\n\n```bash\n# List existing worktrees\ngit worktree list\n\n# Add new worktree for feature branch\ngit worktree add ../project-feature feature/new-feature\n\n# Add worktree and create new branch\ngit worktree add -b bugfix/urgent ../project-hotfix main\n\n# Remove worktree\ngit worktree remove ../project-feature\n\n# Prune stale worktrees\ngit worktree prune\n```\n\n### 5. Reflog\n\nYour safety net - tracks all ref movements, even deleted commits.\n\n```bash\n# View reflog\ngit reflog\n\n# View reflog for specific branch\ngit reflog show feature/branch\n\n# Restore deleted commit\ngit reflog\n# Find commit hash\ngit checkout abc123\ngit branch recovered-branch\n\n# Restore deleted branch\ngit reflog\ngit branch deleted-branch abc123\n```\n\n## Practical Workflows\n\n### Workflow 1: Clean Up Feature Branch Before PR\n\n```bash\n# Start with feature branch\ngit checkout feature/user-auth\n\n# Interactive rebase to clean history\ngit rebase -i main\n\n# Example rebase operations:\n# - Squash \"fix typo\" commits\n# - Reword commit messages for clarity\n# - Reorder commits logically\n# - Drop unnecessary commits\n\n# Force push cleaned branch (safe if no one else is using it)\ngit push --force-with-lease origin feature/user-auth\n```\n\n### Workflow 2: Apply Hotfix to Multiple Releases\n\n```bash\n# Create fix on main\ngit checkout main\ngit commit -m \"fix: critical security patch\"\n\n# Apply to release branches\ngit checkout release/2.0\ngit cherry-pick abc123\n\ngit checkout release/1.9\ngit cherry-pick abc123\n\n# Handle conflicts if they arise\ngit cherry-pick --continue\n# or\ngit cherry-pick --abort\n```\n\n### Workflow 3: Find Bug Introduction\n\n```bash\n# Start bisect\ngit bisect start\ngit bisect bad HEAD\ngit bisect good v2.1.0\n\n# Git checks out middle commit - run tests\nnpm test\n\n# If tests fail\ngit bisect bad\n\n# If tests pass\ngit bisect good\n\n# Git will automatically checkout next commit to test\n# Repeat until bug found\n\n# Automated version\ngit bisect start HEAD v2.1.0\ngit bisect run npm test\n```\n\n### Workflow 4: Multi-Branch Development\n\n```bash\n# Main project directory\ncd ~/projects/myapp\n\n# Create worktree for urgent bugfix\ngit worktree add ../myapp-hotfix hotfix/critical-bug\n\n# Work on hotfix in separate directory\ncd ../myapp-hotfix\n# Make changes, commit\ngit commit -m \"fix: resolve critical bug\"\ngit push origin hotfix/critical-bug\n\n# Return to main work without interruption\ncd ~/projects/myapp\ngit fetch origin\ngit cherry-pick hotfix/critical-bug\n\n# Clean up when done\ngit worktree remove ../myapp-hotfix\n```\n\n### Workflow 5: Recover from Mistakes\n\n```bash\n# Accidentally reset to wrong commit\ngit reset --hard HEAD~5  # Oh no!\n\n# Use reflog to find lost commits\ngit reflog\n# Output shows:\n# abc123 HEAD@{0}: reset: moving to HEAD~5\n# def456 HEAD@{1}: commit: my important changes\n\n# Recover lost commits\ngit reset --hard def456\n\n# Or create branch from lost commit\ngit branch recovery def456\n```\n\n## Advanced Techniques\n\n### Rebase vs Merge Strategy\n\n**When to Rebase:**\n- Cleaning up local commits before pushing\n- Keeping feature branch up-to-date with main\n- Creating linear history for easier review\n\n**When to Merge:**\n- Integrating completed features into main\n- Preserving exact history of collaboration\n- Public branches used by others\n\n```bash\n# Update feature branch with main changes (rebase)\ngit checkout feature/my-feature\ngit fetch origin\ngit rebase origin/main\n\n# Handle conflicts\ngit status\n# Fix conflicts in files\ngit add .\ngit rebase --continue\n\n# Or merge instead\ngit merge origin/main\n```\n\n### Autosquash Workflow\n\nAutomatically squash fixup commits during rebase.\n\n```bash\n# Make initial commit\ngit commit -m \"feat: add user authentication\"\n\n# Later, fix something in that commit\n# Stage changes\ngit commit --fixup HEAD  # or specify commit hash\n\n# Make more changes\ngit commit --fixup abc123\n\n# Rebase with autosquash\ngit rebase -i --autosquash main\n\n# Git automatically marks fixup commits\n```\n\n### Split Commit\n\nBreak one commit into multiple logical commits.\n\n```bash\n# Start interactive rebase\ngit rebase -i HEAD~3\n\n# Mark commit to split with 'edit'\n# Git will stop at that commit\n\n# Reset commit but keep changes\ngit reset HEAD^\n\n# Stage and commit in logical chunks\ngit add file1.py\ngit commit -m \"feat: add validation\"\n\ngit add file2.py\ngit commit -m \"feat: add error handling\"\n\n# Continue rebase\ngit rebase --continue\n```\n\n### Partial Cherry-Pick\n\nCherry-pick only specific files from a commit.\n\n```bash\n# Show files in commit\ngit show --name-only abc123\n\n# Checkout specific files from commit\ngit checkout abc123 -- path/to/file1.py path/to/file2.py\n\n# Stage and commit\ngit commit -m \"cherry-pick: apply specific changes from abc123\"\n```\n\n## Best Practices\n\n1. **Always Use --force-with-lease**: Safer than --force, prevents overwriting others' work\n2. **Rebase Only Local Commits**: Don't rebase commits that have been pushed and shared\n3. **Descriptive Commit Messages**: Future you will thank present you\n4. **Atomic Commits**: Each commit should be a single logical change\n5. **Test Before Force Push**: Ensure history rewrite didn't break anything\n6. **Keep Reflog Aware**: Remember reflog is your safety net for 90 days\n7. **Branch Before Risky Operations**: Create backup branch before complex rebases\n\n```bash\n# Safe force push\ngit push --force-with-lease origin feature/branch\n\n# Create backup before risky operation\ngit branch backup-branch\ngit rebase -i main\n# If something goes wrong\ngit reset --hard backup-branch\n```\n\n## Common Pitfalls\n\n- **Rebasing Public Branches**: Causes history conflicts for collaborators\n- **Force Pushing Without Lease**: Can overwrite teammate's work\n- **Losing Work in Rebase**: Resolve conflicts carefully, test after rebase\n- **Forgetting Worktree Cleanup**: Orphaned worktrees consume disk space\n- **Not Backing Up Before Experiment**: Always create safety branch\n- **Bisect on Dirty Working Directory**: Commit or stash before bisecting\n\n## Recovery Commands\n\n```bash\n# Abort operations in progress\ngit rebase --abort\ngit merge --abort\ngit cherry-pick --abort\ngit bisect reset\n\n# Restore file to version from specific commit\ngit restore --source=abc123 path/to/file\n\n# Undo last commit but keep changes\ngit reset --soft HEAD^\n\n# Undo last commit and discard changes\ngit reset --hard HEAD^\n\n# Recover deleted branch (within 90 days)\ngit reflog\ngit branch recovered-branch abc123\n```\n\n## Resources\n\n- **references/git-rebase-guide.md**: Deep dive into interactive rebase\n- **references/git-conflict-resolution.md**: Advanced conflict resolution strategies\n- **references/git-history-rewriting.md**: Safely rewriting Git history\n- **assets/git-workflow-checklist.md**: Pre-PR cleanup checklist\n- **assets/git-aliases.md**: Useful Git aliases for advanced workflows\n- **scripts/git-clean-branches.sh**: Clean up merged and stale branches"
              },
              {
                "name": "monorepo-management",
                "description": "Master monorepo management with Turborepo, Nx, and pnpm workspaces to build efficient, scalable multi-package repositories with optimized builds and dependency management. Use when setting up monorepos, optimizing builds, or managing shared dependencies.",
                "path": "plugins/developer-essentials/skills/monorepo-management/SKILL.md",
                "frontmatter": {
                  "name": "monorepo-management",
                  "description": "Master monorepo management with Turborepo, Nx, and pnpm workspaces to build efficient, scalable multi-package repositories with optimized builds and dependency management. Use when setting up monorepos, optimizing builds, or managing shared dependencies."
                },
                "content": "# Monorepo Management\n\nBuild efficient, scalable monorepos that enable code sharing, consistent tooling, and atomic changes across multiple packages and applications.\n\n## When to Use This Skill\n\n- Setting up new monorepo projects\n- Migrating from multi-repo to monorepo\n- Optimizing build and test performance\n- Managing shared dependencies\n- Implementing code sharing strategies\n- Setting up CI/CD for monorepos\n- Versioning and publishing packages\n- Debugging monorepo-specific issues\n\n## Core Concepts\n\n### 1. Why Monorepos?\n\n**Advantages:**\n- Shared code and dependencies\n- Atomic commits across projects\n- Consistent tooling and standards\n- Easier refactoring\n- Simplified dependency management\n- Better code visibility\n\n**Challenges:**\n- Build performance at scale\n- CI/CD complexity\n- Access control\n- Large Git repository\n\n### 2. Monorepo Tools\n\n**Package Managers:**\n- pnpm workspaces (recommended)\n- npm workspaces\n- Yarn workspaces\n\n**Build Systems:**\n- Turborepo (recommended for most)\n- Nx (feature-rich, complex)\n- Lerna (older, maintenance mode)\n\n## Turborepo Setup\n\n### Initial Setup\n\n```bash\n# Create new monorepo\nnpx create-turbo@latest my-monorepo\ncd my-monorepo\n\n# Structure:\n# apps/\n#   web/          - Next.js app\n#   docs/         - Documentation site\n# packages/\n#   ui/           - Shared UI components\n#   config/       - Shared configurations\n#   tsconfig/     - Shared TypeScript configs\n# turbo.json      - Turborepo configuration\n# package.json    - Root package.json\n```\n\n### Configuration\n\n```json\n// turbo.json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"globalDependencies\": [\"**/.env.*local\"],\n  \"pipeline\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"dist/**\", \".next/**\", \"!.next/cache/**\"]\n    },\n    \"test\": {\n      \"dependsOn\": [\"build\"],\n      \"outputs\": [\"coverage/**\"]\n    },\n    \"lint\": {\n      \"outputs\": []\n    },\n    \"dev\": {\n      \"cache\": false,\n      \"persistent\": true\n    },\n    \"type-check\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": []\n    }\n  }\n}\n```\n\n```json\n// package.json (root)\n{\n  \"name\": \"my-monorepo\",\n  \"private\": true,\n  \"workspaces\": [\n    \"apps/*\",\n    \"packages/*\"\n  ],\n  \"scripts\": {\n    \"build\": \"turbo run build\",\n    \"dev\": \"turbo run dev\",\n    \"test\": \"turbo run test\",\n    \"lint\": \"turbo run lint\",\n    \"format\": \"prettier --write \\\"**/*.{ts,tsx,md}\\\"\",\n    \"clean\": \"turbo run clean && rm -rf node_modules\"\n  },\n  \"devDependencies\": {\n    \"turbo\": \"^1.10.0\",\n    \"prettier\": \"^3.0.0\",\n    \"typescript\": \"^5.0.0\"\n  },\n  \"packageManager\": \"pnpm@8.0.0\"\n}\n```\n\n### Package Structure\n\n```json\n// packages/ui/package.json\n{\n  \"name\": \"@repo/ui\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"main\": \"./dist/index.js\",\n  \"types\": \"./dist/index.d.ts\",\n  \"exports\": {\n    \".\": {\n      \"import\": \"./dist/index.js\",\n      \"types\": \"./dist/index.d.ts\"\n    },\n    \"./button\": {\n      \"import\": \"./dist/button.js\",\n      \"types\": \"./dist/button.d.ts\"\n    }\n  },\n  \"scripts\": {\n    \"build\": \"tsup src/index.ts --format esm,cjs --dts\",\n    \"dev\": \"tsup src/index.ts --format esm,cjs --dts --watch\",\n    \"lint\": \"eslint src/\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"devDependencies\": {\n    \"@repo/tsconfig\": \"workspace:*\",\n    \"tsup\": \"^7.0.0\",\n    \"typescript\": \"^5.0.0\"\n  },\n  \"dependencies\": {\n    \"react\": \"^18.2.0\"\n  }\n}\n```\n\n## pnpm Workspaces\n\n### Setup\n\n```yaml\n# pnpm-workspace.yaml\npackages:\n  - 'apps/*'\n  - 'packages/*'\n  - 'tools/*'\n```\n\n```json\n// .npmrc\n# Hoist shared dependencies\nshamefully-hoist=true\n\n# Strict peer dependencies\nauto-install-peers=true\nstrict-peer-dependencies=true\n\n# Performance\nstore-dir=~/.pnpm-store\n```\n\n### Dependency Management\n\n```bash\n# Install dependency in specific package\npnpm add react --filter @repo/ui\npnpm add -D typescript --filter @repo/ui\n\n# Install workspace dependency\npnpm add @repo/ui --filter web\n\n# Install in all packages\npnpm add -D eslint -w\n\n# Update all dependencies\npnpm update -r\n\n# Remove dependency\npnpm remove react --filter @repo/ui\n```\n\n### Scripts\n\n```bash\n# Run script in specific package\npnpm --filter web dev\npnpm --filter @repo/ui build\n\n# Run in all packages\npnpm -r build\npnpm -r test\n\n# Run in parallel\npnpm -r --parallel dev\n\n# Filter by pattern\npnpm --filter \"@repo/*\" build\npnpm --filter \"...web\" build  # Build web and dependencies\n```\n\n## Nx Monorepo\n\n### Setup\n\n```bash\n# Create Nx monorepo\nnpx create-nx-workspace@latest my-org\n\n# Generate applications\nnx generate @nx/react:app my-app\nnx generate @nx/next:app my-next-app\n\n# Generate libraries\nnx generate @nx/react:lib ui-components\nnx generate @nx/js:lib utils\n```\n\n### Configuration\n\n```json\n// nx.json\n{\n  \"extends\": \"nx/presets/npm.json\",\n  \"$schema\": \"./node_modules/nx/schemas/nx-schema.json\",\n  \"targetDefaults\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"inputs\": [\"production\", \"^production\"],\n      \"cache\": true\n    },\n    \"test\": {\n      \"inputs\": [\"default\", \"^production\", \"{workspaceRoot}/jest.preset.js\"],\n      \"cache\": true\n    },\n    \"lint\": {\n      \"inputs\": [\"default\", \"{workspaceRoot}/.eslintrc.json\"],\n      \"cache\": true\n    }\n  },\n  \"namedInputs\": {\n    \"default\": [\"{projectRoot}/**/*\", \"sharedGlobals\"],\n    \"production\": [\n      \"default\",\n      \"!{projectRoot}/**/?(*.)+(spec|test).[jt]s?(x)?(.snap)\",\n      \"!{projectRoot}/tsconfig.spec.json\"\n    ],\n    \"sharedGlobals\": []\n  }\n}\n```\n\n### Running Tasks\n\n```bash\n# Run task for specific project\nnx build my-app\nnx test ui-components\nnx lint utils\n\n# Run for affected projects\nnx affected:build\nnx affected:test --base=main\n\n# Visualize dependencies\nnx graph\n\n# Run in parallel\nnx run-many --target=build --all --parallel=3\n```\n\n## Shared Configurations\n\n### TypeScript Configuration\n\n```json\n// packages/tsconfig/base.json\n{\n  \"compilerOptions\": {\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"incremental\": true,\n    \"declaration\": true\n  },\n  \"exclude\": [\"node_modules\"]\n}\n\n// packages/tsconfig/react.json\n{\n  \"extends\": \"./base.json\",\n  \"compilerOptions\": {\n    \"jsx\": \"react-jsx\",\n    \"lib\": [\"ES2022\", \"DOM\", \"DOM.Iterable\"]\n  }\n}\n\n// apps/web/tsconfig.json\n{\n  \"extends\": \"@repo/tsconfig/react.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\"\n  },\n  \"include\": [\"src\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n### ESLint Configuration\n\n```javascript\n// packages/config/eslint-preset.js\nmodule.exports = {\n  extends: [\n    'eslint:recommended',\n    'plugin:@typescript-eslint/recommended',\n    'plugin:react/recommended',\n    'plugin:react-hooks/recommended',\n    'prettier',\n  ],\n  plugins: ['@typescript-eslint', 'react', 'react-hooks'],\n  parser: '@typescript-eslint/parser',\n  parserOptions: {\n    ecmaVersion: 2022,\n    sourceType: 'module',\n    ecmaFeatures: {\n      jsx: true,\n    },\n  },\n  settings: {\n    react: {\n      version: 'detect',\n    },\n  },\n  rules: {\n    '@typescript-eslint/no-unused-vars': 'error',\n    'react/react-in-jsx-scope': 'off',\n  },\n};\n\n// apps/web/.eslintrc.js\nmodule.exports = {\n  extends: ['@repo/config/eslint-preset'],\n  rules: {\n    // App-specific rules\n  },\n};\n```\n\n## Code Sharing Patterns\n\n### Pattern 1: Shared UI Components\n\n```typescript\n// packages/ui/src/button.tsx\nimport * as React from 'react';\n\nexport interface ButtonProps {\n  variant?: 'primary' | 'secondary';\n  children: React.ReactNode;\n  onClick?: () => void;\n}\n\nexport function Button({ variant = 'primary', children, onClick }: ButtonProps) {\n  return (\n    <button\n      className={`btn btn-${variant}`}\n      onClick={onClick}\n    >\n      {children}\n    </button>\n  );\n}\n\n// packages/ui/src/index.ts\nexport { Button, type ButtonProps } from './button';\nexport { Input, type InputProps } from './input';\n\n// apps/web/src/app.tsx\nimport { Button } from '@repo/ui';\n\nexport function App() {\n  return <Button variant=\"primary\">Click me</Button>;\n}\n```\n\n### Pattern 2: Shared Utilities\n\n```typescript\n// packages/utils/src/string.ts\nexport function capitalize(str: string): string {\n  return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nexport function truncate(str: string, length: number): string {\n  return str.length > length ? str.slice(0, length) + '...' : str;\n}\n\n// packages/utils/src/index.ts\nexport * from './string';\nexport * from './array';\nexport * from './date';\n\n// Usage in apps\nimport { capitalize, truncate } from '@repo/utils';\n```\n\n### Pattern 3: Shared Types\n\n```typescript\n// packages/types/src/user.ts\nexport interface User {\n  id: string;\n  email: string;\n  name: string;\n  role: 'admin' | 'user';\n}\n\nexport interface CreateUserInput {\n  email: string;\n  name: string;\n  password: string;\n}\n\n// Used in both frontend and backend\nimport type { User, CreateUserInput } from '@repo/types';\n```\n\n## Build Optimization\n\n### Turborepo Caching\n\n```json\n// turbo.json\n{\n  \"pipeline\": {\n    \"build\": {\n      // Build depends on dependencies being built first\n      \"dependsOn\": [\"^build\"],\n\n      // Cache these outputs\n      \"outputs\": [\"dist/**\", \".next/**\"],\n\n      // Cache based on these inputs (default: all files)\n      \"inputs\": [\"src/**/*.tsx\", \"src/**/*.ts\", \"package.json\"]\n    },\n    \"test\": {\n      // Run tests in parallel, don't depend on build\n      \"cache\": true,\n      \"outputs\": [\"coverage/**\"]\n    }\n  }\n}\n```\n\n### Remote Caching\n\n```bash\n# Turborepo Remote Cache (Vercel)\nnpx turbo login\nnpx turbo link\n\n# Custom remote cache\n# turbo.json\n{\n  \"remoteCache\": {\n    \"signature\": true,\n    \"enabled\": true\n  }\n}\n```\n\n## CI/CD for Monorepos\n\n### GitHub Actions\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  # For Nx affected commands\n\n      - uses: pnpm/action-setup@v2\n        with:\n          version: 8\n\n      - uses: actions/setup-node@v3\n        with:\n          node-version: 18\n          cache: 'pnpm'\n\n      - name: Install dependencies\n        run: pnpm install --frozen-lockfile\n\n      - name: Build\n        run: pnpm turbo run build\n\n      - name: Test\n        run: pnpm turbo run test\n\n      - name: Lint\n        run: pnpm turbo run lint\n\n      - name: Type check\n        run: pnpm turbo run type-check\n```\n\n### Deploy Affected Only\n\n```yaml\n# Deploy only changed apps\n- name: Deploy affected apps\n  run: |\n    if pnpm nx affected:apps --base=origin/main --head=HEAD | grep -q \"web\"; then\n      echo \"Deploying web app\"\n      pnpm --filter web deploy\n    fi\n```\n\n## Best Practices\n\n1. **Consistent Versioning**: Lock dependency versions across workspace\n2. **Shared Configs**: Centralize ESLint, TypeScript, Prettier configs\n3. **Dependency Graph**: Keep it acyclic, avoid circular dependencies\n4. **Cache Effectively**: Configure inputs/outputs correctly\n5. **Type Safety**: Share types between frontend/backend\n6. **Testing Strategy**: Unit tests in packages, E2E in apps\n7. **Documentation**: README in each package\n8. **Release Strategy**: Use changesets for versioning\n\n## Common Pitfalls\n\n- **Circular Dependencies**: A depends on B, B depends on A\n- **Phantom Dependencies**: Using deps not in package.json\n- **Incorrect Cache Inputs**: Missing files in Turborepo inputs\n- **Over-Sharing**: Sharing code that should be separate\n- **Under-Sharing**: Duplicating code across packages\n- **Large Monorepos**: Without proper tooling, builds slow down\n\n## Publishing Packages\n\n```bash\n# Using Changesets\npnpm add -Dw @changesets/cli\npnpm changeset init\n\n# Create changeset\npnpm changeset\n\n# Version packages\npnpm changeset version\n\n# Publish\npnpm changeset publish\n```\n\n```yaml\n# .github/workflows/release.yml\n- name: Create Release Pull Request or Publish\n  uses: changesets/action@v1\n  with:\n    publish: pnpm release\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\n## Resources\n\n- **references/turborepo-guide.md**: Comprehensive Turborepo documentation\n- **references/nx-guide.md**: Nx monorepo patterns\n- **references/pnpm-workspaces.md**: pnpm workspace features\n- **assets/monorepo-checklist.md**: Setup checklist\n- **assets/migration-guide.md**: Multi-repo to monorepo migration\n- **scripts/dependency-graph.ts**: Visualize package dependencies"
              },
              {
                "name": "nx-workspace-patterns",
                "description": "Configure and optimize Nx monorepo workspaces. Use when setting up Nx, configuring project boundaries, optimizing build caching, or implementing affected commands.",
                "path": "plugins/developer-essentials/skills/nx-workspace-patterns/SKILL.md",
                "frontmatter": {
                  "name": "nx-workspace-patterns",
                  "description": "Configure and optimize Nx monorepo workspaces. Use when setting up Nx, configuring project boundaries, optimizing build caching, or implementing affected commands."
                },
                "content": "# Nx Workspace Patterns\n\nProduction patterns for Nx monorepo management.\n\n## When to Use This Skill\n\n- Setting up new Nx workspaces\n- Configuring project boundaries\n- Optimizing CI with affected commands\n- Implementing remote caching\n- Managing dependencies between projects\n- Migrating to Nx\n\n## Core Concepts\n\n### 1. Nx Architecture\n\n```\nworkspace/\nâ”œâ”€â”€ apps/              # Deployable applications\nâ”‚   â”œâ”€â”€ web/\nâ”‚   â””â”€â”€ api/\nâ”œâ”€â”€ libs/              # Shared libraries\nâ”‚   â”œâ”€â”€ shared/\nâ”‚   â”‚   â”œâ”€â”€ ui/\nâ”‚   â”‚   â””â”€â”€ utils/\nâ”‚   â””â”€â”€ feature/\nâ”‚       â”œâ”€â”€ auth/\nâ”‚       â””â”€â”€ dashboard/\nâ”œâ”€â”€ tools/             # Custom executors/generators\nâ”œâ”€â”€ nx.json            # Nx configuration\nâ””â”€â”€ workspace.json     # Project configuration\n```\n\n### 2. Library Types\n\n| Type | Purpose | Example |\n|------|---------|---------|\n| **feature** | Smart components, business logic | `feature-auth` |\n| **ui** | Presentational components | `ui-buttons` |\n| **data-access** | API calls, state management | `data-access-users` |\n| **util** | Pure functions, helpers | `util-formatting` |\n| **shell** | App bootstrapping | `shell-web` |\n\n## Templates\n\n### Template 1: nx.json Configuration\n\n```json\n{\n  \"$schema\": \"./node_modules/nx/schemas/nx-schema.json\",\n  \"npmScope\": \"myorg\",\n  \"affected\": {\n    \"defaultBase\": \"main\"\n  },\n  \"tasksRunnerOptions\": {\n    \"default\": {\n      \"runner\": \"nx/tasks-runners/default\",\n      \"options\": {\n        \"cacheableOperations\": [\n          \"build\",\n          \"lint\",\n          \"test\",\n          \"e2e\",\n          \"build-storybook\"\n        ],\n        \"parallel\": 3\n      }\n    }\n  },\n  \"targetDefaults\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"inputs\": [\"production\", \"^production\"],\n      \"cache\": true\n    },\n    \"test\": {\n      \"inputs\": [\"default\", \"^production\", \"{workspaceRoot}/jest.preset.js\"],\n      \"cache\": true\n    },\n    \"lint\": {\n      \"inputs\": [\"default\", \"{workspaceRoot}/.eslintrc.json\"],\n      \"cache\": true\n    },\n    \"e2e\": {\n      \"inputs\": [\"default\", \"^production\"],\n      \"cache\": true\n    }\n  },\n  \"namedInputs\": {\n    \"default\": [\"{projectRoot}/**/*\", \"sharedGlobals\"],\n    \"production\": [\n      \"default\",\n      \"!{projectRoot}/**/?(*.)+(spec|test).[jt]s?(x)?(.snap)\",\n      \"!{projectRoot}/tsconfig.spec.json\",\n      \"!{projectRoot}/jest.config.[jt]s\",\n      \"!{projectRoot}/.eslintrc.json\"\n    ],\n    \"sharedGlobals\": [\n      \"{workspaceRoot}/babel.config.json\",\n      \"{workspaceRoot}/tsconfig.base.json\"\n    ]\n  },\n  \"generators\": {\n    \"@nx/react\": {\n      \"application\": {\n        \"style\": \"css\",\n        \"linter\": \"eslint\",\n        \"bundler\": \"webpack\"\n      },\n      \"library\": {\n        \"style\": \"css\",\n        \"linter\": \"eslint\"\n      },\n      \"component\": {\n        \"style\": \"css\"\n      }\n    }\n  }\n}\n```\n\n### Template 2: Project Configuration\n\n```json\n// apps/web/project.json\n{\n  \"name\": \"web\",\n  \"$schema\": \"../../node_modules/nx/schemas/project-schema.json\",\n  \"sourceRoot\": \"apps/web/src\",\n  \"projectType\": \"application\",\n  \"tags\": [\"type:app\", \"scope:web\"],\n  \"targets\": {\n    \"build\": {\n      \"executor\": \"@nx/webpack:webpack\",\n      \"outputs\": [\"{options.outputPath}\"],\n      \"defaultConfiguration\": \"production\",\n      \"options\": {\n        \"compiler\": \"babel\",\n        \"outputPath\": \"dist/apps/web\",\n        \"index\": \"apps/web/src/index.html\",\n        \"main\": \"apps/web/src/main.tsx\",\n        \"tsConfig\": \"apps/web/tsconfig.app.json\",\n        \"assets\": [\"apps/web/src/assets\"],\n        \"styles\": [\"apps/web/src/styles.css\"]\n      },\n      \"configurations\": {\n        \"development\": {\n          \"extractLicenses\": false,\n          \"optimization\": false,\n          \"sourceMap\": true\n        },\n        \"production\": {\n          \"optimization\": true,\n          \"outputHashing\": \"all\",\n          \"sourceMap\": false,\n          \"extractLicenses\": true\n        }\n      }\n    },\n    \"serve\": {\n      \"executor\": \"@nx/webpack:dev-server\",\n      \"defaultConfiguration\": \"development\",\n      \"options\": {\n        \"buildTarget\": \"web:build\"\n      },\n      \"configurations\": {\n        \"development\": {\n          \"buildTarget\": \"web:build:development\"\n        },\n        \"production\": {\n          \"buildTarget\": \"web:build:production\"\n        }\n      }\n    },\n    \"test\": {\n      \"executor\": \"@nx/jest:jest\",\n      \"outputs\": [\"{workspaceRoot}/coverage/{projectRoot}\"],\n      \"options\": {\n        \"jestConfig\": \"apps/web/jest.config.ts\",\n        \"passWithNoTests\": true\n      }\n    },\n    \"lint\": {\n      \"executor\": \"@nx/eslint:lint\",\n      \"outputs\": [\"{options.outputFile}\"],\n      \"options\": {\n        \"lintFilePatterns\": [\"apps/web/**/*.{ts,tsx,js,jsx}\"]\n      }\n    }\n  }\n}\n```\n\n### Template 3: Module Boundary Rules\n\n```json\n// .eslintrc.json\n{\n  \"root\": true,\n  \"ignorePatterns\": [\"**/*\"],\n  \"plugins\": [\"@nx\"],\n  \"overrides\": [\n    {\n      \"files\": [\"*.ts\", \"*.tsx\", \"*.js\", \"*.jsx\"],\n      \"rules\": {\n        \"@nx/enforce-module-boundaries\": [\n          \"error\",\n          {\n            \"enforceBuildableLibDependency\": true,\n            \"allow\": [],\n            \"depConstraints\": [\n              {\n                \"sourceTag\": \"type:app\",\n                \"onlyDependOnLibsWithTags\": [\n                  \"type:feature\",\n                  \"type:ui\",\n                  \"type:data-access\",\n                  \"type:util\"\n                ]\n              },\n              {\n                \"sourceTag\": \"type:feature\",\n                \"onlyDependOnLibsWithTags\": [\n                  \"type:ui\",\n                  \"type:data-access\",\n                  \"type:util\"\n                ]\n              },\n              {\n                \"sourceTag\": \"type:ui\",\n                \"onlyDependOnLibsWithTags\": [\"type:ui\", \"type:util\"]\n              },\n              {\n                \"sourceTag\": \"type:data-access\",\n                \"onlyDependOnLibsWithTags\": [\"type:data-access\", \"type:util\"]\n              },\n              {\n                \"sourceTag\": \"type:util\",\n                \"onlyDependOnLibsWithTags\": [\"type:util\"]\n              },\n              {\n                \"sourceTag\": \"scope:web\",\n                \"onlyDependOnLibsWithTags\": [\"scope:web\", \"scope:shared\"]\n              },\n              {\n                \"sourceTag\": \"scope:api\",\n                \"onlyDependOnLibsWithTags\": [\"scope:api\", \"scope:shared\"]\n              },\n              {\n                \"sourceTag\": \"scope:shared\",\n                \"onlyDependOnLibsWithTags\": [\"scope:shared\"]\n              }\n            ]\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n### Template 4: Custom Generator\n\n```typescript\n// tools/generators/feature-lib/index.ts\nimport {\n  Tree,\n  formatFiles,\n  generateFiles,\n  joinPathFragments,\n  names,\n  readProjectConfiguration,\n} from '@nx/devkit';\nimport { libraryGenerator } from '@nx/react';\n\ninterface FeatureLibraryGeneratorSchema {\n  name: string;\n  scope: string;\n  directory?: string;\n}\n\nexport default async function featureLibraryGenerator(\n  tree: Tree,\n  options: FeatureLibraryGeneratorSchema\n) {\n  const { name, scope, directory } = options;\n  const projectDirectory = directory\n    ? `${directory}/${name}`\n    : `libs/${scope}/feature-${name}`;\n\n  // Generate base library\n  await libraryGenerator(tree, {\n    name: `feature-${name}`,\n    directory: projectDirectory,\n    tags: `type:feature,scope:${scope}`,\n    style: 'css',\n    skipTsConfig: false,\n    skipFormat: true,\n    unitTestRunner: 'jest',\n    linter: 'eslint',\n  });\n\n  // Add custom files\n  const projectConfig = readProjectConfiguration(tree, `${scope}-feature-${name}`);\n  const projectNames = names(name);\n\n  generateFiles(\n    tree,\n    joinPathFragments(__dirname, 'files'),\n    projectConfig.sourceRoot,\n    {\n      ...projectNames,\n      scope,\n      tmpl: '',\n    }\n  );\n\n  await formatFiles(tree);\n}\n```\n\n### Template 5: CI Configuration with Affected\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  NX_CLOUD_ACCESS_TOKEN: ${{ secrets.NX_CLOUD_ACCESS_TOKEN }}\n\njobs:\n  main:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Derive SHAs for affected commands\n        uses: nrwl/nx-set-shas@v4\n\n      - name: Run affected lint\n        run: npx nx affected -t lint --parallel=3\n\n      - name: Run affected test\n        run: npx nx affected -t test --parallel=3 --configuration=ci\n\n      - name: Run affected build\n        run: npx nx affected -t build --parallel=3\n\n      - name: Run affected e2e\n        run: npx nx affected -t e2e --parallel=1\n```\n\n### Template 6: Remote Caching Setup\n\n```typescript\n// nx.json with Nx Cloud\n{\n  \"tasksRunnerOptions\": {\n    \"default\": {\n      \"runner\": \"nx-cloud\",\n      \"options\": {\n        \"cacheableOperations\": [\"build\", \"lint\", \"test\", \"e2e\"],\n        \"accessToken\": \"your-nx-cloud-token\",\n        \"parallel\": 3,\n        \"cacheDirectory\": \".nx/cache\"\n      }\n    }\n  },\n  \"nxCloudAccessToken\": \"your-nx-cloud-token\"\n}\n\n// Self-hosted cache with S3\n{\n  \"tasksRunnerOptions\": {\n    \"default\": {\n      \"runner\": \"@nx-aws-cache/nx-aws-cache\",\n      \"options\": {\n        \"cacheableOperations\": [\"build\", \"lint\", \"test\"],\n        \"awsRegion\": \"us-east-1\",\n        \"awsBucket\": \"my-nx-cache-bucket\",\n        \"awsProfile\": \"default\"\n      }\n    }\n  }\n}\n```\n\n## Common Commands\n\n```bash\n# Generate new library\nnx g @nx/react:lib feature-auth --directory=libs/web --tags=type:feature,scope:web\n\n# Run affected tests\nnx affected -t test --base=main\n\n# View dependency graph\nnx graph\n\n# Run specific project\nnx build web --configuration=production\n\n# Reset cache\nnx reset\n\n# Run migrations\nnx migrate latest\nnx migrate --run-migrations\n```\n\n## Best Practices\n\n### Do's\n- **Use tags consistently** - Enforce with module boundaries\n- **Enable caching early** - Significant CI savings\n- **Keep libs focused** - Single responsibility\n- **Use generators** - Ensure consistency\n- **Document boundaries** - Help new developers\n\n### Don'ts\n- **Don't create circular deps** - Graph should be acyclic\n- **Don't skip affected** - Test only what changed\n- **Don't ignore boundaries** - Tech debt accumulates\n- **Don't over-granularize** - Balance lib count\n\n## Resources\n\n- [Nx Documentation](https://nx.dev/getting-started/intro)\n- [Module Boundaries](https://nx.dev/core-features/enforce-module-boundaries)\n- [Nx Cloud](https://nx.app/)"
              },
              {
                "name": "sql-optimization-patterns",
                "description": "Master SQL query optimization, indexing strategies, and EXPLAIN analysis to dramatically improve database performance and eliminate slow queries. Use when debugging slow queries, designing database schemas, or optimizing application performance.",
                "path": "plugins/developer-essentials/skills/sql-optimization-patterns/SKILL.md",
                "frontmatter": {
                  "name": "sql-optimization-patterns",
                  "description": "Master SQL query optimization, indexing strategies, and EXPLAIN analysis to dramatically improve database performance and eliminate slow queries. Use when debugging slow queries, designing database schemas, or optimizing application performance."
                },
                "content": "# SQL Optimization Patterns\n\nTransform slow database queries into lightning-fast operations through systematic optimization, proper indexing, and query plan analysis.\n\n## When to Use This Skill\n\n- Debugging slow-running queries\n- Designing performant database schemas\n- Optimizing application response times\n- Reducing database load and costs\n- Improving scalability for growing datasets\n- Analyzing EXPLAIN query plans\n- Implementing efficient indexes\n- Resolving N+1 query problems\n\n## Core Concepts\n\n### 1. Query Execution Plans (EXPLAIN)\n\nUnderstanding EXPLAIN output is fundamental to optimization.\n\n**PostgreSQL EXPLAIN:**\n```sql\n-- Basic explain\nEXPLAIN SELECT * FROM users WHERE email = 'user@example.com';\n\n-- With actual execution stats\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Verbose output with more details\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT u.*, o.order_total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > NOW() - INTERVAL '30 days';\n```\n\n**Key Metrics to Watch:**\n- **Seq Scan**: Full table scan (usually slow for large tables)\n- **Index Scan**: Using index (good)\n- **Index Only Scan**: Using index without touching table (best)\n- **Nested Loop**: Join method (okay for small datasets)\n- **Hash Join**: Join method (good for larger datasets)\n- **Merge Join**: Join method (good for sorted data)\n- **Cost**: Estimated query cost (lower is better)\n- **Rows**: Estimated rows returned\n- **Actual Time**: Real execution time\n\n### 2. Index Strategies\n\nIndexes are the most powerful optimization tool.\n\n**Index Types:**\n- **B-Tree**: Default, good for equality and range queries\n- **Hash**: Only for equality (=) comparisons\n- **GIN**: Full-text search, array queries, JSONB\n- **GiST**: Geometric data, full-text search\n- **BRIN**: Block Range INdex for very large tables with correlation\n\n```sql\n-- Standard B-Tree index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Composite index (order matters!)\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Partial index (index subset of rows)\nCREATE INDEX idx_active_users ON users(email)\nWHERE status = 'active';\n\n-- Expression index\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- Covering index (include additional columns)\nCREATE INDEX idx_users_email_covering ON users(email)\nINCLUDE (name, created_at);\n\n-- Full-text search index\nCREATE INDEX idx_posts_search ON posts\nUSING GIN(to_tsvector('english', title || ' ' || body));\n\n-- JSONB index\nCREATE INDEX idx_metadata ON events USING GIN(metadata);\n```\n\n### 3. Query Optimization Patterns\n\n**Avoid SELECT \\*:**\n```sql\n-- Bad: Fetches unnecessary columns\nSELECT * FROM users WHERE id = 123;\n\n-- Good: Fetch only what you need\nSELECT id, email, name FROM users WHERE id = 123;\n```\n\n**Use WHERE Clause Efficiently:**\n```sql\n-- Bad: Function prevents index usage\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- Good: Create functional index or use exact match\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n-- Then:\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- Or store normalized data\nSELECT * FROM users WHERE email = 'user@example.com';\n```\n\n**Optimize JOINs:**\n```sql\n-- Bad: Cartesian product then filter\nSELECT u.name, o.total\nFROM users u, orders o\nWHERE u.id = o.user_id AND u.created_at > '2024-01-01';\n\n-- Good: Filter before join\nSELECT u.name, o.total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01';\n\n-- Better: Filter both tables\nSELECT u.name, o.total\nFROM (SELECT * FROM users WHERE created_at > '2024-01-01') u\nJOIN orders o ON u.id = o.user_id;\n```\n\n## Optimization Patterns\n\n### Pattern 1: Eliminate N+1 Queries\n\n**Problem: N+1 Query Anti-Pattern**\n```python\n# Bad: Executes N+1 queries\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n    # Process orders\n```\n\n**Solution: Use JOINs or Batch Loading**\n```sql\n-- Solution 1: JOIN\nSELECT\n    u.id, u.name,\n    o.id as order_id, o.total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.id IN (1, 2, 3, 4, 5);\n\n-- Solution 2: Batch query\nSELECT * FROM orders\nWHERE user_id IN (1, 2, 3, 4, 5);\n```\n\n```python\n# Good: Single query with JOIN or batch load\n# Using JOIN\nresults = db.query(\"\"\"\n    SELECT u.id, u.name, o.id as order_id, o.total\n    FROM users u\n    LEFT JOIN orders o ON u.id = o.user_id\n    WHERE u.id IN (1, 2, 3, 4, 5)\n\"\"\")\n\n# Or batch load\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nuser_ids = [u.id for u in users]\norders = db.query(\n    \"SELECT * FROM orders WHERE user_id IN (?)\",\n    user_ids\n)\n# Group orders by user_id\norders_by_user = {}\nfor order in orders:\n    orders_by_user.setdefault(order.user_id, []).append(order)\n```\n\n### Pattern 2: Optimize Pagination\n\n**Bad: OFFSET on Large Tables**\n```sql\n-- Slow for large offsets\nSELECT * FROM users\nORDER BY created_at DESC\nLIMIT 20 OFFSET 100000;  -- Very slow!\n```\n\n**Good: Cursor-Based Pagination**\n```sql\n-- Much faster: Use cursor (last seen ID)\nSELECT * FROM users\nWHERE created_at < '2024-01-15 10:30:00'  -- Last cursor\nORDER BY created_at DESC\nLIMIT 20;\n\n-- With composite sorting\nSELECT * FROM users\nWHERE (created_at, id) < ('2024-01-15 10:30:00', 12345)\nORDER BY created_at DESC, id DESC\nLIMIT 20;\n\n-- Requires index\nCREATE INDEX idx_users_cursor ON users(created_at DESC, id DESC);\n```\n\n### Pattern 3: Aggregate Efficiently\n\n**Optimize COUNT Queries:**\n```sql\n-- Bad: Counts all rows\nSELECT COUNT(*) FROM orders;  -- Slow on large tables\n\n-- Good: Use estimates for approximate counts\nSELECT reltuples::bigint AS estimate\nFROM pg_class\nWHERE relname = 'orders';\n\n-- Good: Filter before counting\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL '7 days';\n\n-- Better: Use index-only scan\nCREATE INDEX idx_orders_created ON orders(created_at);\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL '7 days';\n```\n\n**Optimize GROUP BY:**\n```sql\n-- Bad: Group by then filter\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- Better: Filter first, then group (if possible)\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nWHERE status = 'completed'\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- Best: Use covering index\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n```\n\n### Pattern 4: Subquery Optimization\n\n**Transform Correlated Subqueries:**\n```sql\n-- Bad: Correlated subquery (runs for each row)\nSELECT u.name, u.email,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) as order_count\nFROM users u;\n\n-- Good: JOIN with aggregation\nSELECT u.name, u.email, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id\nGROUP BY u.id, u.name, u.email;\n\n-- Better: Use window functions\nSELECT DISTINCT ON (u.id)\n    u.name, u.email,\n    COUNT(o.id) OVER (PARTITION BY u.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id;\n```\n\n**Use CTEs for Clarity:**\n```sql\n-- Using Common Table Expressions\nWITH recent_users AS (\n    SELECT id, name, email\n    FROM users\n    WHERE created_at > NOW() - INTERVAL '30 days'\n),\nuser_order_counts AS (\n    SELECT user_id, COUNT(*) as order_count\n    FROM orders\n    WHERE created_at > NOW() - INTERVAL '30 days'\n    GROUP BY user_id\n)\nSELECT ru.name, ru.email, COALESCE(uoc.order_count, 0) as orders\nFROM recent_users ru\nLEFT JOIN user_order_counts uoc ON ru.id = uoc.user_id;\n```\n\n### Pattern 5: Batch Operations\n\n**Batch INSERT:**\n```sql\n-- Bad: Multiple individual inserts\nINSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');\nINSERT INTO users (name, email) VALUES ('Bob', 'bob@example.com');\nINSERT INTO users (name, email) VALUES ('Carol', 'carol@example.com');\n\n-- Good: Batch insert\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com'),\n    ('Carol', 'carol@example.com');\n\n-- Better: Use COPY for bulk inserts (PostgreSQL)\nCOPY users (name, email) FROM '/tmp/users.csv' CSV HEADER;\n```\n\n**Batch UPDATE:**\n```sql\n-- Bad: Update in loop\nUPDATE users SET status = 'active' WHERE id = 1;\nUPDATE users SET status = 'active' WHERE id = 2;\n-- ... repeat for many IDs\n\n-- Good: Single UPDATE with IN clause\nUPDATE users\nSET status = 'active'\nWHERE id IN (1, 2, 3, 4, 5, ...);\n\n-- Better: Use temporary table for large batches\nCREATE TEMP TABLE temp_user_updates (id INT, new_status VARCHAR);\nINSERT INTO temp_user_updates VALUES (1, 'active'), (2, 'active'), ...;\n\nUPDATE users u\nSET status = t.new_status\nFROM temp_user_updates t\nWHERE u.id = t.id;\n```\n\n## Advanced Techniques\n\n### Materialized Views\n\nPre-compute expensive queries.\n\n```sql\n-- Create materialized view\nCREATE MATERIALIZED VIEW user_order_summary AS\nSELECT\n    u.id,\n    u.name,\n    COUNT(o.id) as total_orders,\n    SUM(o.total) as total_spent,\n    MAX(o.created_at) as last_order_date\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Add index to materialized view\nCREATE INDEX idx_user_summary_spent ON user_order_summary(total_spent DESC);\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW user_order_summary;\n\n-- Concurrent refresh (PostgreSQL)\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;\n\n-- Query materialized view (very fast)\nSELECT * FROM user_order_summary\nWHERE total_spent > 1000\nORDER BY total_spent DESC;\n```\n\n### Partitioning\n\nSplit large tables for better performance.\n\n```sql\n-- Range partitioning by date (PostgreSQL)\nCREATE TABLE orders (\n    id SERIAL,\n    user_id INT,\n    total DECIMAL,\n    created_at TIMESTAMP\n) PARTITION BY RANGE (created_at);\n\n-- Create partitions\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n\n-- Queries automatically use appropriate partition\nSELECT * FROM orders\nWHERE created_at BETWEEN '2024-02-01' AND '2024-02-28';\n-- Only scans orders_2024_q1 partition\n```\n\n### Query Hints and Optimization\n\n```sql\n-- Force index usage (MySQL)\nSELECT * FROM users\nUSE INDEX (idx_users_email)\nWHERE email = 'user@example.com';\n\n-- Parallel query (PostgreSQL)\nSET max_parallel_workers_per_gather = 4;\nSELECT * FROM large_table WHERE condition;\n\n-- Join hints (PostgreSQL)\nSET enable_nestloop = OFF;  -- Force hash or merge join\n```\n\n## Best Practices\n\n1. **Index Selectively**: Too many indexes slow down writes\n2. **Monitor Query Performance**: Use slow query logs\n3. **Keep Statistics Updated**: Run ANALYZE regularly\n4. **Use Appropriate Data Types**: Smaller types = better performance\n5. **Normalize Thoughtfully**: Balance normalization vs performance\n6. **Cache Frequently Accessed Data**: Use application-level caching\n7. **Connection Pooling**: Reuse database connections\n8. **Regular Maintenance**: VACUUM, ANALYZE, rebuild indexes\n\n```sql\n-- Update statistics\nANALYZE users;\nANALYZE VERBOSE orders;\n\n-- Vacuum (PostgreSQL)\nVACUUM ANALYZE users;\nVACUUM FULL users;  -- Reclaim space (locks table)\n\n-- Reindex\nREINDEX INDEX idx_users_email;\nREINDEX TABLE users;\n```\n\n## Common Pitfalls\n\n- **Over-Indexing**: Each index slows down INSERT/UPDATE/DELETE\n- **Unused Indexes**: Waste space and slow writes\n- **Missing Indexes**: Slow queries, full table scans\n- **Implicit Type Conversion**: Prevents index usage\n- **OR Conditions**: Can't use indexes efficiently\n- **LIKE with Leading Wildcard**: `LIKE '%abc'` can't use index\n- **Function in WHERE**: Prevents index usage unless functional index exists\n\n## Monitoring Queries\n\n```sql\n-- Find slow queries (PostgreSQL)\nSELECT query, calls, total_time, mean_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Find missing indexes (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    seq_tup_read / seq_scan AS avg_seq_tup_read\nFROM pg_stat_user_tables\nWHERE seq_scan > 0\nORDER BY seq_tup_read DESC\nLIMIT 10;\n\n-- Find unused indexes (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n```\n\n## Resources\n\n- **references/postgres-optimization-guide.md**: PostgreSQL-specific optimization\n- **references/mysql-optimization-guide.md**: MySQL/MariaDB optimization\n- **references/query-plan-analysis.md**: Deep dive into EXPLAIN plans\n- **assets/index-strategy-checklist.md**: When and how to create indexes\n- **assets/query-optimization-checklist.md**: Step-by-step optimization guide\n- **scripts/analyze-slow-queries.sql**: Identify slow queries in your database\n- **scripts/index-recommendations.sql**: Generate index recommendations"
              },
              {
                "name": "turborepo-caching",
                "description": "Configure Turborepo for efficient monorepo builds with local and remote caching. Use when setting up Turborepo, optimizing build pipelines, or implementing distributed caching.",
                "path": "plugins/developer-essentials/skills/turborepo-caching/SKILL.md",
                "frontmatter": {
                  "name": "turborepo-caching",
                  "description": "Configure Turborepo for efficient monorepo builds with local and remote caching. Use when setting up Turborepo, optimizing build pipelines, or implementing distributed caching."
                },
                "content": "# Turborepo Caching\n\nProduction patterns for Turborepo build optimization.\n\n## When to Use This Skill\n\n- Setting up new Turborepo projects\n- Configuring build pipelines\n- Implementing remote caching\n- Optimizing CI/CD performance\n- Migrating from other monorepo tools\n- Debugging cache misses\n\n## Core Concepts\n\n### 1. Turborepo Architecture\n\n```\nWorkspace Root/\nâ”œâ”€â”€ apps/\nâ”‚   â”œâ”€â”€ web/\nâ”‚   â”‚   â””â”€â”€ package.json\nâ”‚   â””â”€â”€ docs/\nâ”‚       â””â”€â”€ package.json\nâ”œâ”€â”€ packages/\nâ”‚   â”œâ”€â”€ ui/\nâ”‚   â”‚   â””â”€â”€ package.json\nâ”‚   â””â”€â”€ config/\nâ”‚       â””â”€â”€ package.json\nâ”œâ”€â”€ turbo.json\nâ””â”€â”€ package.json\n```\n\n### 2. Pipeline Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **dependsOn** | Tasks that must complete first |\n| **cache** | Whether to cache outputs |\n| **outputs** | Files to cache |\n| **inputs** | Files that affect cache key |\n| **persistent** | Long-running tasks (dev servers) |\n\n## Templates\n\n### Template 1: turbo.json Configuration\n\n```json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"globalDependencies\": [\n    \".env\",\n    \".env.local\"\n  ],\n  \"globalEnv\": [\n    \"NODE_ENV\",\n    \"VERCEL_URL\"\n  ],\n  \"pipeline\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\n        \"dist/**\",\n        \".next/**\",\n        \"!.next/cache/**\"\n      ],\n      \"env\": [\n        \"API_URL\",\n        \"NEXT_PUBLIC_*\"\n      ]\n    },\n    \"test\": {\n      \"dependsOn\": [\"build\"],\n      \"outputs\": [\"coverage/**\"],\n      \"inputs\": [\n        \"src/**/*.tsx\",\n        \"src/**/*.ts\",\n        \"test/**/*.ts\"\n      ]\n    },\n    \"lint\": {\n      \"outputs\": [],\n      \"cache\": true\n    },\n    \"typecheck\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": []\n    },\n    \"dev\": {\n      \"cache\": false,\n      \"persistent\": true\n    },\n    \"clean\": {\n      \"cache\": false\n    }\n  }\n}\n```\n\n### Template 2: Package-Specific Pipeline\n\n```json\n// apps/web/turbo.json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"extends\": [\"//\"],\n  \"pipeline\": {\n    \"build\": {\n      \"outputs\": [\".next/**\", \"!.next/cache/**\"],\n      \"env\": [\n        \"NEXT_PUBLIC_API_URL\",\n        \"NEXT_PUBLIC_ANALYTICS_ID\"\n      ]\n    },\n    \"test\": {\n      \"outputs\": [\"coverage/**\"],\n      \"inputs\": [\n        \"src/**\",\n        \"tests/**\",\n        \"jest.config.js\"\n      ]\n    }\n  }\n}\n```\n\n### Template 3: Remote Caching with Vercel\n\n```bash\n# Login to Vercel\nnpx turbo login\n\n# Link to Vercel project\nnpx turbo link\n\n# Run with remote cache\nturbo build --remote-only\n\n# CI environment variables\nTURBO_TOKEN=your-token\nTURBO_TEAM=your-team\n```\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\nenv:\n  TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}\n  TURBO_TEAM: ${{ vars.TURBO_TEAM }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npx turbo build --filter='...[origin/main]'\n\n      - name: Test\n        run: npx turbo test --filter='...[origin/main]'\n```\n\n### Template 4: Self-Hosted Remote Cache\n\n```typescript\n// Custom remote cache server (Express)\nimport express from 'express';\nimport { createReadStream, createWriteStream } from 'fs';\nimport { mkdir } from 'fs/promises';\nimport { join } from 'path';\n\nconst app = express();\nconst CACHE_DIR = './cache';\n\n// Get artifact\napp.get('/v8/artifacts/:hash', async (req, res) => {\n  const { hash } = req.params;\n  const team = req.query.teamId || 'default';\n  const filePath = join(CACHE_DIR, team, hash);\n\n  try {\n    const stream = createReadStream(filePath);\n    stream.pipe(res);\n  } catch {\n    res.status(404).send('Not found');\n  }\n});\n\n// Put artifact\napp.put('/v8/artifacts/:hash', async (req, res) => {\n  const { hash } = req.params;\n  const team = req.query.teamId || 'default';\n  const dir = join(CACHE_DIR, team);\n  const filePath = join(dir, hash);\n\n  await mkdir(dir, { recursive: true });\n\n  const stream = createWriteStream(filePath);\n  req.pipe(stream);\n\n  stream.on('finish', () => {\n    res.json({ urls: [`${req.protocol}://${req.get('host')}/v8/artifacts/${hash}`] });\n  });\n});\n\n// Check artifact exists\napp.head('/v8/artifacts/:hash', async (req, res) => {\n  const { hash } = req.params;\n  const team = req.query.teamId || 'default';\n  const filePath = join(CACHE_DIR, team, hash);\n\n  try {\n    await fs.access(filePath);\n    res.status(200).end();\n  } catch {\n    res.status(404).end();\n  }\n});\n\napp.listen(3000);\n```\n\n```json\n// turbo.json for self-hosted cache\n{\n  \"remoteCache\": {\n    \"signature\": false\n  }\n}\n```\n\n```bash\n# Use self-hosted cache\nturbo build --api=\"http://localhost:3000\" --token=\"my-token\" --team=\"my-team\"\n```\n\n### Template 5: Filtering and Scoping\n\n```bash\n# Build specific package\nturbo build --filter=@myorg/web\n\n# Build package and its dependencies\nturbo build --filter=@myorg/web...\n\n# Build package and its dependents\nturbo build --filter=...@myorg/ui\n\n# Build changed packages since main\nturbo build --filter='...[origin/main]'\n\n# Build packages in directory\nturbo build --filter='./apps/*'\n\n# Combine filters\nturbo build --filter=@myorg/web --filter=@myorg/docs\n\n# Exclude package\nturbo build --filter='!@myorg/docs'\n\n# Include dependencies of changed\nturbo build --filter='...[HEAD^1]...'\n```\n\n### Template 6: Advanced Pipeline Configuration\n\n```json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"pipeline\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"dist/**\"],\n      \"inputs\": [\n        \"$TURBO_DEFAULT$\",\n        \"!**/*.md\",\n        \"!**/*.test.*\"\n      ]\n    },\n    \"test\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"coverage/**\"],\n      \"inputs\": [\n        \"src/**\",\n        \"tests/**\",\n        \"*.config.*\"\n      ],\n      \"env\": [\"CI\", \"NODE_ENV\"]\n    },\n    \"test:e2e\": {\n      \"dependsOn\": [\"build\"],\n      \"outputs\": [],\n      \"cache\": false\n    },\n    \"deploy\": {\n      \"dependsOn\": [\"build\", \"test\", \"lint\"],\n      \"outputs\": [],\n      \"cache\": false\n    },\n    \"db:generate\": {\n      \"cache\": false\n    },\n    \"db:push\": {\n      \"cache\": false,\n      \"dependsOn\": [\"db:generate\"]\n    },\n    \"@myorg/web#build\": {\n      \"dependsOn\": [\"^build\", \"@myorg/db#db:generate\"],\n      \"outputs\": [\".next/**\"],\n      \"env\": [\"NEXT_PUBLIC_*\"]\n    }\n  }\n}\n```\n\n### Template 7: Root package.json Setup\n\n```json\n{\n  \"name\": \"my-turborepo\",\n  \"private\": true,\n  \"workspaces\": [\n    \"apps/*\",\n    \"packages/*\"\n  ],\n  \"scripts\": {\n    \"build\": \"turbo build\",\n    \"dev\": \"turbo dev\",\n    \"lint\": \"turbo lint\",\n    \"test\": \"turbo test\",\n    \"clean\": \"turbo clean && rm -rf node_modules\",\n    \"format\": \"prettier --write \\\"**/*.{ts,tsx,md}\\\"\",\n    \"changeset\": \"changeset\",\n    \"version-packages\": \"changeset version\",\n    \"release\": \"turbo build --filter=./packages/* && changeset publish\"\n  },\n  \"devDependencies\": {\n    \"turbo\": \"^1.10.0\",\n    \"prettier\": \"^3.0.0\",\n    \"@changesets/cli\": \"^2.26.0\"\n  },\n  \"packageManager\": \"npm@10.0.0\"\n}\n```\n\n## Debugging Cache\n\n```bash\n# Dry run to see what would run\nturbo build --dry-run\n\n# Verbose output with hashes\nturbo build --verbosity=2\n\n# Show task graph\nturbo build --graph\n\n# Force no cache\nturbo build --force\n\n# Show cache status\nturbo build --summarize\n\n# Debug specific task\nTURBO_LOG_VERBOSITY=debug turbo build --filter=@myorg/web\n```\n\n## Best Practices\n\n### Do's\n- **Define explicit inputs** - Avoid cache invalidation\n- **Use workspace protocol** - `\"@myorg/ui\": \"workspace:*\"`\n- **Enable remote caching** - Share across CI and local\n- **Filter in CI** - Build only affected packages\n- **Cache build outputs** - Not source files\n\n### Don'ts\n- **Don't cache dev servers** - Use `persistent: true`\n- **Don't include secrets in env** - Use runtime env vars\n- **Don't ignore dependsOn** - Causes race conditions\n- **Don't over-filter** - May miss dependencies\n\n## Resources\n\n- [Turborepo Documentation](https://turbo.build/repo/docs)\n- [Caching Guide](https://turbo.build/repo/docs/core-concepts/caching)\n- [Remote Caching](https://turbo.build/repo/docs/core-concepts/remote-caching)"
              }
            ]
          },
          {
            "name": "documentation-generation",
            "description": "Documentation generation with API docs and diagrams",
            "source": "./plugins/documentation-generation",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install documentation-generation@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/doc-generate",
                "description": null,
                "path": "plugins/documentation-generation/commands/doc-generate.md",
                "frontmatter": null,
                "content": "# Automated Documentation Generation\n\nYou are a documentation expert specializing in creating comprehensive, maintainable documentation from code. Generate API docs, architecture diagrams, user guides, and technical references using AI-powered analysis and industry best practices.\n\n## Context\nThe user needs automated documentation generation that extracts information from code, creates clear explanations, and maintains consistency across documentation types. Focus on creating living documentation that stays synchronized with code.\n\n## Requirements\n$ARGUMENTS\n\n## How to Use This Tool\n\nThis tool provides both **concise instructions** (what to create) and **detailed reference examples** (how to create it). Structure:\n- **Instructions**: High-level guidance and documentation types to generate\n- **Reference Examples**: Complete implementation patterns to adapt and use as templates\n\n## Instructions\n\nGenerate comprehensive documentation by analyzing the codebase and creating the following artifacts:\n\n### 1. **API Documentation**\n- Extract endpoint definitions, parameters, and responses from code\n- Generate OpenAPI/Swagger specifications\n- Create interactive API documentation (Swagger UI, Redoc)\n- Include authentication, rate limiting, and error handling details\n\n### 2. **Architecture Documentation**\n- Create system architecture diagrams (Mermaid, PlantUML)\n- Document component relationships and data flows\n- Explain service dependencies and communication patterns\n- Include scalability and reliability considerations\n\n### 3. **Code Documentation**\n- Generate inline documentation and docstrings\n- Create README files with setup, usage, and contribution guidelines\n- Document configuration options and environment variables\n- Provide troubleshooting guides and code examples\n\n### 4. **User Documentation**\n- Write step-by-step user guides\n- Create getting started tutorials\n- Document common workflows and use cases\n- Include accessibility and localization notes\n\n### 5. **Documentation Automation**\n- Configure CI/CD pipelines for automatic doc generation\n- Set up documentation linting and validation\n- Implement documentation coverage checks\n- Automate deployment to hosting platforms\n\n### Quality Standards\n\nEnsure all generated documentation:\n- Is accurate and synchronized with current code\n- Uses consistent terminology and formatting\n- Includes practical examples and use cases\n- Is searchable and well-organized\n- Follows accessibility best practices\n\n## Reference Examples\n\n### Example 1: Code Analysis for Documentation\n\n**API Documentation Extraction**\n```python\nimport ast\nfrom typing import Dict, List\n\nclass APIDocExtractor:\n    def extract_endpoints(self, code_path):\n        \"\"\"Extract API endpoints and their documentation\"\"\"\n        endpoints = []\n\n        with open(code_path, 'r') as f:\n            tree = ast.parse(f.read())\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                for decorator in node.decorator_list:\n                    if self._is_route_decorator(decorator):\n                        endpoint = {\n                            'method': self._extract_method(decorator),\n                            'path': self._extract_path(decorator),\n                            'function': node.name,\n                            'docstring': ast.get_docstring(node),\n                            'parameters': self._extract_parameters(node),\n                            'returns': self._extract_returns(node)\n                        }\n                        endpoints.append(endpoint)\n        return endpoints\n\n    def _extract_parameters(self, func_node):\n        \"\"\"Extract function parameters with types\"\"\"\n        params = []\n        for arg in func_node.args.args:\n            param = {\n                'name': arg.arg,\n                'type': ast.unparse(arg.annotation) if arg.annotation else None,\n                'required': True\n            }\n            params.append(param)\n        return params\n```\n\n**Schema Extraction**\n```python\ndef extract_pydantic_schemas(file_path):\n    \"\"\"Extract Pydantic model definitions for API documentation\"\"\"\n    schemas = []\n\n    with open(file_path, 'r') as f:\n        tree = ast.parse(f.read())\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.ClassDef):\n            if any(base.id == 'BaseModel' for base in node.bases if hasattr(base, 'id')):\n                schema = {\n                    'name': node.name,\n                    'description': ast.get_docstring(node),\n                    'fields': []\n                }\n\n                for item in node.body:\n                    if isinstance(item, ast.AnnAssign):\n                        field = {\n                            'name': item.target.id,\n                            'type': ast.unparse(item.annotation),\n                            'required': item.value is None\n                        }\n                        schema['fields'].append(field)\n                schemas.append(schema)\n    return schemas\n```\n\n### Example 2: OpenAPI Specification Generation\n\n**OpenAPI Template**\n```yaml\nopenapi: 3.0.0\ninfo:\n  title: ${API_TITLE}\n  version: ${VERSION}\n  description: |\n    ${DESCRIPTION}\n\n    ## Authentication\n    ${AUTH_DESCRIPTION}\n\nservers:\n  - url: https://api.example.com/v1\n    description: Production server\n\nsecurity:\n  - bearerAuth: []\n\npaths:\n  /users:\n    get:\n      summary: List all users\n      operationId: listUsers\n      tags:\n        - Users\n      parameters:\n        - name: page\n          in: query\n          schema:\n            type: integer\n            default: 1\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            default: 20\n            maximum: 100\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/User'\n                  pagination:\n                    $ref: '#/components/schemas/Pagination'\n        '401':\n          $ref: '#/components/responses/Unauthorized'\n\ncomponents:\n  schemas:\n    User:\n      type: object\n      required:\n        - id\n        - email\n      properties:\n        id:\n          type: string\n          format: uuid\n        email:\n          type: string\n          format: email\n        name:\n          type: string\n        createdAt:\n          type: string\n          format: date-time\n```\n\n### Example 3: Architecture Diagrams\n\n**System Architecture (Mermaid)**\n```mermaid\ngraph TB\n    subgraph \"Frontend\"\n        UI[React UI]\n        Mobile[Mobile App]\n    end\n\n    subgraph \"API Gateway\"\n        Gateway[Kong/nginx]\n        Auth[Auth Service]\n    end\n\n    subgraph \"Microservices\"\n        UserService[User Service]\n        OrderService[Order Service]\n        PaymentService[Payment Service]\n    end\n\n    subgraph \"Data Layer\"\n        PostgresMain[(PostgreSQL)]\n        Redis[(Redis Cache)]\n        S3[S3 Storage]\n    end\n\n    UI --> Gateway\n    Mobile --> Gateway\n    Gateway --> Auth\n    Gateway --> UserService\n    Gateway --> OrderService\n    OrderService --> PaymentService\n    UserService --> PostgresMain\n    UserService --> Redis\n    OrderService --> PostgresMain\n```\n\n**Component Documentation**\n```markdown\n## User Service\n\n**Purpose**: Manages user accounts, authentication, and profiles\n\n**Technology Stack**:\n- Language: Python 3.11\n- Framework: FastAPI\n- Database: PostgreSQL\n- Cache: Redis\n- Authentication: JWT\n\n**API Endpoints**:\n- `POST /users` - Create new user\n- `GET /users/{id}` - Get user details\n- `PUT /users/{id}` - Update user\n- `POST /auth/login` - User login\n\n**Configuration**:\n```yaml\nuser_service:\n  port: 8001\n  database:\n    host: postgres.internal\n    name: users_db\n  jwt:\n    secret: ${JWT_SECRET}\n    expiry: 3600\n```\n```\n\n### Example 4: README Generation\n\n**README Template**\n```markdown\n# ${PROJECT_NAME}\n\n${BADGES}\n\n${SHORT_DESCRIPTION}\n\n## Features\n\n${FEATURES_LIST}\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8+\n- PostgreSQL 12+\n- Redis 6+\n\n### Using pip\n\n```bash\npip install ${PACKAGE_NAME}\n```\n\n### From source\n\n```bash\ngit clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git\ncd ${REPO_NAME}\npip install -e .\n```\n\n## Quick Start\n\n```python\n${QUICK_START_CODE}\n```\n\n## Configuration\n\n### Environment Variables\n\n| Variable | Description | Default | Required |\n|----------|-------------|---------|----------|\n| DATABASE_URL | PostgreSQL connection string | - | Yes |\n| REDIS_URL | Redis connection string | - | Yes |\n| SECRET_KEY | Application secret key | - | Yes |\n\n## Development\n\n```bash\n# Clone and setup\ngit clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git\ncd ${REPO_NAME}\npython -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Run tests\npytest\n\n# Start development server\npython manage.py runserver\n```\n\n## Testing\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=your_package\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the ${LICENSE} License - see the [LICENSE](LICENSE) file for details.\n```\n\n### Example 5: Function Documentation Generator\n\n```python\nimport inspect\n\ndef generate_function_docs(func):\n    \"\"\"Generate comprehensive documentation for a function\"\"\"\n    sig = inspect.signature(func)\n    params = []\n    args_doc = []\n\n    for param_name, param in sig.parameters.items():\n        param_str = param_name\n        if param.annotation != param.empty:\n            param_str += f\": {param.annotation.__name__}\"\n        if param.default != param.empty:\n            param_str += f\" = {param.default}\"\n        params.append(param_str)\n        args_doc.append(f\"{param_name}: Description of {param_name}\")\n\n    return_type = \"\"\n    if sig.return_annotation != sig.empty:\n        return_type = f\" -> {sig.return_annotation.__name__}\"\n\n    doc_template = f'''\ndef {func.__name__}({\", \".join(params)}){return_type}:\n    \"\"\"\n    Brief description of {func.__name__}\n\n    Args:\n        {chr(10).join(f\"        {arg}\" for arg in args_doc)}\n\n    Returns:\n        Description of return value\n\n    Examples:\n        >>> {func.__name__}(example_input)\n        expected_output\n    \"\"\"\n'''\n    return doc_template\n```\n\n### Example 6: User Guide Template\n\n```markdown\n# User Guide\n\n## Getting Started\n\n### Creating Your First ${FEATURE}\n\n1. **Navigate to the Dashboard**\n\n   Click on the ${FEATURE} tab in the main navigation menu.\n\n2. **Click \"Create New\"**\n\n   You'll find the \"Create New\" button in the top right corner.\n\n3. **Fill in the Details**\n\n   - **Name**: Enter a descriptive name\n   - **Description**: Add optional details\n   - **Settings**: Configure as needed\n\n4. **Save Your Changes**\n\n   Click \"Save\" to create your ${FEATURE}.\n\n### Common Tasks\n\n#### Editing ${FEATURE}\n\n1. Find your ${FEATURE} in the list\n2. Click the \"Edit\" button\n3. Make your changes\n4. Click \"Save\"\n\n#### Deleting ${FEATURE}\n\n> âš ï¸ **Warning**: Deletion is permanent and cannot be undone.\n\n1. Find your ${FEATURE} in the list\n2. Click the \"Delete\" button\n3. Confirm the deletion\n\n### Troubleshooting\n\n| Error | Meaning | Solution |\n|-------|---------|----------|\n| \"Name required\" | The name field is empty | Enter a name |\n| \"Permission denied\" | You don't have access | Contact admin |\n| \"Server error\" | Technical issue | Try again later |\n```\n\n### Example 7: Interactive API Playground\n\n**Swagger UI Setup**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>API Documentation</title>\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui.css\">\n</head>\n<body>\n    <div id=\"swagger-ui\"></div>\n\n    <script src=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui-bundle.js\"></script>\n    <script>\n        window.onload = function() {\n            SwaggerUIBundle({\n                url: \"/api/openapi.json\",\n                dom_id: '#swagger-ui',\n                deepLinking: true,\n                presets: [SwaggerUIBundle.presets.apis],\n                layout: \"StandaloneLayout\"\n            });\n        }\n    </script>\n</body>\n</html>\n```\n\n**Code Examples Generator**\n```python\ndef generate_code_examples(endpoint):\n    \"\"\"Generate code examples for API endpoints in multiple languages\"\"\"\n    examples = {}\n\n    # Python\n    examples['python'] = f'''\nimport requests\n\nurl = \"https://api.example.com{endpoint['path']}\"\nheaders = {{\"Authorization\": \"Bearer YOUR_API_KEY\"}}\n\nresponse = requests.{endpoint['method'].lower()}(url, headers=headers)\nprint(response.json())\n'''\n\n    # JavaScript\n    examples['javascript'] = f'''\nconst response = await fetch('https://api.example.com{endpoint['path']}', {{\n    method: '{endpoint['method']}',\n    headers: {{'Authorization': 'Bearer YOUR_API_KEY'}}\n}});\n\nconst data = await response.json();\nconsole.log(data);\n'''\n\n    # cURL\n    examples['curl'] = f'''\ncurl -X {endpoint['method']} https://api.example.com{endpoint['path']} \\\\\n    -H \"Authorization: Bearer YOUR_API_KEY\"\n'''\n\n    return examples\n```\n\n### Example 8: Documentation CI/CD\n\n**GitHub Actions Workflow**\n```yaml\nname: Generate Documentation\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'src/**'\n      - 'api/**'\n\njobs:\n  generate-docs:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements-docs.txt\n        npm install -g @redocly/cli\n\n    - name: Generate API documentation\n      run: |\n        python scripts/generate_openapi.py > docs/api/openapi.json\n        redocly build-docs docs/api/openapi.json -o docs/api/index.html\n\n    - name: Generate code documentation\n      run: sphinx-build -b html docs/source docs/build\n\n    - name: Deploy to GitHub Pages\n      uses: peaceiris/actions-gh-pages@v3\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n        publish_dir: ./docs/build\n```\n\n### Example 9: Documentation Coverage Validation\n\n```python\nimport ast\nimport glob\n\nclass DocCoverage:\n    def check_coverage(self, codebase_path):\n        \"\"\"Check documentation coverage for codebase\"\"\"\n        results = {\n            'total_functions': 0,\n            'documented_functions': 0,\n            'total_classes': 0,\n            'documented_classes': 0,\n            'missing_docs': []\n        }\n\n        for file_path in glob.glob(f\"{codebase_path}/**/*.py\", recursive=True):\n            module = ast.parse(open(file_path).read())\n\n            for node in ast.walk(module):\n                if isinstance(node, ast.FunctionDef):\n                    results['total_functions'] += 1\n                    if ast.get_docstring(node):\n                        results['documented_functions'] += 1\n                    else:\n                        results['missing_docs'].append({\n                            'type': 'function',\n                            'name': node.name,\n                            'file': file_path,\n                            'line': node.lineno\n                        })\n\n                elif isinstance(node, ast.ClassDef):\n                    results['total_classes'] += 1\n                    if ast.get_docstring(node):\n                        results['documented_classes'] += 1\n                    else:\n                        results['missing_docs'].append({\n                            'type': 'class',\n                            'name': node.name,\n                            'file': file_path,\n                            'line': node.lineno\n                        })\n\n        # Calculate coverage percentages\n        results['function_coverage'] = (\n            results['documented_functions'] / results['total_functions'] * 100\n            if results['total_functions'] > 0 else 100\n        )\n        results['class_coverage'] = (\n            results['documented_classes'] / results['total_classes'] * 100\n            if results['total_classes'] > 0 else 100\n        )\n\n        return results\n```\n\n## Output Format\n\n1. **API Documentation**: OpenAPI spec with interactive playground\n2. **Architecture Diagrams**: System, sequence, and component diagrams\n3. **Code Documentation**: Inline docs, docstrings, and type hints\n4. **User Guides**: Step-by-step tutorials\n5. **Developer Guides**: Setup, contribution, and API usage guides\n6. **Reference Documentation**: Complete API reference with examples\n7. **Documentation Site**: Deployed static site with search functionality\n\nFocus on creating documentation that is accurate, comprehensive, and easy to maintain alongside code changes.\n"
              }
            ],
            "skills": [
              {
                "name": "architecture-decision-records",
                "description": "Write and maintain Architecture Decision Records (ADRs) following best practices for technical decision documentation. Use when documenting significant technical decisions, reviewing past architectural choices, or establishing decision processes.",
                "path": "plugins/documentation-generation/skills/architecture-decision-records/SKILL.md",
                "frontmatter": {
                  "name": "architecture-decision-records",
                  "description": "Write and maintain Architecture Decision Records (ADRs) following best practices for technical decision documentation. Use when documenting significant technical decisions, reviewing past architectural choices, or establishing decision processes."
                },
                "content": "# Architecture Decision Records\n\nComprehensive patterns for creating, maintaining, and managing Architecture Decision Records (ADRs) that capture the context and rationale behind significant technical decisions.\n\n## When to Use This Skill\n\n- Making significant architectural decisions\n- Documenting technology choices\n- Recording design trade-offs\n- Onboarding new team members\n- Reviewing historical decisions\n- Establishing decision-making processes\n\n## Core Concepts\n\n### 1. What is an ADR?\n\nAn Architecture Decision Record captures:\n- **Context**: Why we needed to make a decision\n- **Decision**: What we decided\n- **Consequences**: What happens as a result\n\n### 2. When to Write an ADR\n\n| Write ADR | Skip ADR |\n|-----------|----------|\n| New framework adoption | Minor version upgrades |\n| Database technology choice | Bug fixes |\n| API design patterns | Implementation details |\n| Security architecture | Routine maintenance |\n| Integration patterns | Configuration changes |\n\n### 3. ADR Lifecycle\n\n```\nProposed â†’ Accepted â†’ Deprecated â†’ Superseded\n              â†“\n           Rejected\n```\n\n## Templates\n\n### Template 1: Standard ADR (MADR Format)\n\n```markdown\n# ADR-0001: Use PostgreSQL as Primary Database\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to select a primary database for our new e-commerce platform. The system\nwill handle:\n- ~10,000 concurrent users\n- Complex product catalog with hierarchical categories\n- Transaction processing for orders and payments\n- Full-text search for products\n- Geospatial queries for store locator\n\nThe team has experience with MySQL, PostgreSQL, and MongoDB. We need ACID\ncompliance for financial transactions.\n\n## Decision Drivers\n\n* **Must have ACID compliance** for payment processing\n* **Must support complex queries** for reporting\n* **Should support full-text search** to reduce infrastructure complexity\n* **Should have good JSON support** for flexible product attributes\n* **Team familiarity** reduces onboarding time\n\n## Considered Options\n\n### Option 1: PostgreSQL\n- **Pros**: ACID compliant, excellent JSON support (JSONB), built-in full-text\n  search, PostGIS for geospatial, team has experience\n- **Cons**: Slightly more complex replication setup than MySQL\n\n### Option 2: MySQL\n- **Pros**: Very familiar to team, simple replication, large community\n- **Cons**: Weaker JSON support, no built-in full-text search (need\n  Elasticsearch), no geospatial without extensions\n\n### Option 3: MongoDB\n- **Pros**: Flexible schema, native JSON, horizontal scaling\n- **Cons**: No ACID for multi-document transactions (at decision time),\n  team has limited experience, requires schema design discipline\n\n## Decision\n\nWe will use **PostgreSQL 15** as our primary database.\n\n## Rationale\n\nPostgreSQL provides the best balance of:\n1. **ACID compliance** essential for e-commerce transactions\n2. **Built-in capabilities** (full-text search, JSONB, PostGIS) reduce\n   infrastructure complexity\n3. **Team familiarity** with SQL databases reduces learning curve\n4. **Mature ecosystem** with excellent tooling and community support\n\nThe slight complexity in replication is outweighed by the reduction in\nadditional services (no separate Elasticsearch needed).\n\n## Consequences\n\n### Positive\n- Single database handles transactions, search, and geospatial queries\n- Reduced operational complexity (fewer services to manage)\n- Strong consistency guarantees for financial data\n- Team can leverage existing SQL expertise\n\n### Negative\n- Need to learn PostgreSQL-specific features (JSONB, full-text search syntax)\n- Vertical scaling limits may require read replicas sooner\n- Some team members need PostgreSQL-specific training\n\n### Risks\n- Full-text search may not scale as well as dedicated search engines\n- Mitigation: Design for potential Elasticsearch addition if needed\n\n## Implementation Notes\n\n- Use JSONB for flexible product attributes\n- Implement connection pooling with PgBouncer\n- Set up streaming replication for read replicas\n- Use pg_trgm extension for fuzzy search\n\n## Related Decisions\n\n- ADR-0002: Caching Strategy (Redis) - complements database choice\n- ADR-0005: Search Architecture - may supersede if Elasticsearch needed\n\n## References\n\n- [PostgreSQL JSON Documentation](https://www.postgresql.org/docs/current/datatype-json.html)\n- [PostgreSQL Full Text Search](https://www.postgresql.org/docs/current/textsearch.html)\n- Internal: Performance benchmarks in `/docs/benchmarks/database-comparison.md`\n```\n\n### Template 2: Lightweight ADR\n\n```markdown\n# ADR-0012: Adopt TypeScript for Frontend Development\n\n**Status**: Accepted\n**Date**: 2024-01-15\n**Deciders**: @alice, @bob, @charlie\n\n## Context\n\nOur React codebase has grown to 50+ components with increasing bug reports\nrelated to prop type mismatches and undefined errors. PropTypes provide\nruntime-only checking.\n\n## Decision\n\nAdopt TypeScript for all new frontend code. Migrate existing code incrementally.\n\n## Consequences\n\n**Good**: Catch type errors at compile time, better IDE support, self-documenting\ncode.\n\n**Bad**: Learning curve for team, initial slowdown, build complexity increase.\n\n**Mitigations**: TypeScript training sessions, allow gradual adoption with\n`allowJs: true`.\n```\n\n### Template 3: Y-Statement Format\n\n```markdown\n# ADR-0015: API Gateway Selection\n\nIn the context of **building a microservices architecture**,\nfacing **the need for centralized API management, authentication, and rate limiting**,\nwe decided for **Kong Gateway**\nand against **AWS API Gateway and custom Nginx solution**,\nto achieve **vendor independence, plugin extensibility, and team familiarity with Lua**,\naccepting that **we need to manage Kong infrastructure ourselves**.\n```\n\n### Template 4: ADR for Deprecation\n\n```markdown\n# ADR-0020: Deprecate MongoDB in Favor of PostgreSQL\n\n## Status\n\nAccepted (Supersedes ADR-0003)\n\n## Context\n\nADR-0003 (2021) chose MongoDB for user profile storage due to schema flexibility\nneeds. Since then:\n- MongoDB's multi-document transactions remain problematic for our use case\n- Our schema has stabilized and rarely changes\n- We now have PostgreSQL expertise from other services\n- Maintaining two databases increases operational burden\n\n## Decision\n\nDeprecate MongoDB and migrate user profiles to PostgreSQL.\n\n## Migration Plan\n\n1. **Phase 1** (Week 1-2): Create PostgreSQL schema, dual-write enabled\n2. **Phase 2** (Week 3-4): Backfill historical data, validate consistency\n3. **Phase 3** (Week 5): Switch reads to PostgreSQL, monitor\n4. **Phase 4** (Week 6): Remove MongoDB writes, decommission\n\n## Consequences\n\n### Positive\n- Single database technology reduces operational complexity\n- ACID transactions for user data\n- Team can focus PostgreSQL expertise\n\n### Negative\n- Migration effort (~4 weeks)\n- Risk of data issues during migration\n- Lose some schema flexibility\n\n## Lessons Learned\n\nDocument from ADR-0003 experience:\n- Schema flexibility benefits were overestimated\n- Operational cost of multiple databases was underestimated\n- Consider long-term maintenance in technology decisions\n```\n\n### Template 5: Request for Comments (RFC) Style\n\n```markdown\n# RFC-0025: Adopt Event Sourcing for Order Management\n\n## Summary\n\nPropose adopting event sourcing pattern for the order management domain to\nimprove auditability, enable temporal queries, and support business analytics.\n\n## Motivation\n\nCurrent challenges:\n1. Audit requirements need complete order history\n2. \"What was the order state at time X?\" queries are impossible\n3. Analytics team needs event stream for real-time dashboards\n4. Order state reconstruction for customer support is manual\n\n## Detailed Design\n\n### Event Store\n\n```\nOrderCreated { orderId, customerId, items[], timestamp }\nOrderItemAdded { orderId, item, timestamp }\nOrderItemRemoved { orderId, itemId, timestamp }\nPaymentReceived { orderId, amount, paymentId, timestamp }\nOrderShipped { orderId, trackingNumber, timestamp }\n```\n\n### Projections\n\n- **CurrentOrderState**: Materialized view for queries\n- **OrderHistory**: Complete timeline for audit\n- **DailyOrderMetrics**: Analytics aggregation\n\n### Technology\n\n- Event Store: EventStoreDB (purpose-built, handles projections)\n- Alternative considered: Kafka + custom projection service\n\n## Drawbacks\n\n- Learning curve for team\n- Increased complexity vs. CRUD\n- Need to design events carefully (immutable once stored)\n- Storage growth (events never deleted)\n\n## Alternatives\n\n1. **Audit tables**: Simpler but doesn't enable temporal queries\n2. **CDC from existing DB**: Complex, doesn't change data model\n3. **Hybrid**: Event source only for order state changes\n\n## Unresolved Questions\n\n- [ ] Event schema versioning strategy\n- [ ] Retention policy for events\n- [ ] Snapshot frequency for performance\n\n## Implementation Plan\n\n1. Prototype with single order type (2 weeks)\n2. Team training on event sourcing (1 week)\n3. Full implementation and migration (4 weeks)\n4. Monitoring and optimization (ongoing)\n\n## References\n\n- [Event Sourcing by Martin Fowler](https://martinfowler.com/eaaDev/EventSourcing.html)\n- [EventStoreDB Documentation](https://www.eventstore.com/docs)\n```\n\n## ADR Management\n\n### Directory Structure\n\n```\ndocs/\nâ”œâ”€â”€ adr/\nâ”‚   â”œâ”€â”€ README.md           # Index and guidelines\nâ”‚   â”œâ”€â”€ template.md         # Team's ADR template\nâ”‚   â”œâ”€â”€ 0001-use-postgresql.md\nâ”‚   â”œâ”€â”€ 0002-caching-strategy.md\nâ”‚   â”œâ”€â”€ 0003-mongodb-user-profiles.md  # [DEPRECATED]\nâ”‚   â””â”€â”€ 0020-deprecate-mongodb.md      # Supersedes 0003\n```\n\n### ADR Index (README.md)\n\n```markdown\n# Architecture Decision Records\n\nThis directory contains Architecture Decision Records (ADRs) for [Project Name].\n\n## Index\n\n| ADR | Title | Status | Date |\n|-----|-------|--------|------|\n| [0001](0001-use-postgresql.md) | Use PostgreSQL as Primary Database | Accepted | 2024-01-10 |\n| [0002](0002-caching-strategy.md) | Caching Strategy with Redis | Accepted | 2024-01-12 |\n| [0003](0003-mongodb-user-profiles.md) | MongoDB for User Profiles | Deprecated | 2023-06-15 |\n| [0020](0020-deprecate-mongodb.md) | Deprecate MongoDB | Accepted | 2024-01-15 |\n\n## Creating a New ADR\n\n1. Copy `template.md` to `NNNN-title-with-dashes.md`\n2. Fill in the template\n3. Submit PR for review\n4. Update this index after approval\n\n## ADR Status\n\n- **Proposed**: Under discussion\n- **Accepted**: Decision made, implementing\n- **Deprecated**: No longer relevant\n- **Superseded**: Replaced by another ADR\n- **Rejected**: Considered but not adopted\n```\n\n### Automation (adr-tools)\n\n```bash\n# Install adr-tools\nbrew install adr-tools\n\n# Initialize ADR directory\nadr init docs/adr\n\n# Create new ADR\nadr new \"Use PostgreSQL as Primary Database\"\n\n# Supersede an ADR\nadr new -s 3 \"Deprecate MongoDB in Favor of PostgreSQL\"\n\n# Generate table of contents\nadr generate toc > docs/adr/README.md\n\n# Link related ADRs\nadr link 2 \"Complements\" 1 \"Is complemented by\"\n```\n\n## Review Process\n\n```markdown\n## ADR Review Checklist\n\n### Before Submission\n- [ ] Context clearly explains the problem\n- [ ] All viable options considered\n- [ ] Pros/cons balanced and honest\n- [ ] Consequences (positive and negative) documented\n- [ ] Related ADRs linked\n\n### During Review\n- [ ] At least 2 senior engineers reviewed\n- [ ] Affected teams consulted\n- [ ] Security implications considered\n- [ ] Cost implications documented\n- [ ] Reversibility assessed\n\n### After Acceptance\n- [ ] ADR index updated\n- [ ] Team notified\n- [ ] Implementation tickets created\n- [ ] Related documentation updated\n```\n\n## Best Practices\n\n### Do's\n- **Write ADRs early** - Before implementation starts\n- **Keep them short** - 1-2 pages maximum\n- **Be honest about trade-offs** - Include real cons\n- **Link related decisions** - Build decision graph\n- **Update status** - Deprecate when superseded\n\n### Don'ts\n- **Don't change accepted ADRs** - Write new ones to supersede\n- **Don't skip context** - Future readers need background\n- **Don't hide failures** - Rejected decisions are valuable\n- **Don't be vague** - Specific decisions, specific consequences\n- **Don't forget implementation** - ADR without action is waste\n\n## Resources\n\n- [Documenting Architecture Decisions (Michael Nygard)](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions)\n- [MADR Template](https://adr.github.io/madr/)\n- [ADR GitHub Organization](https://adr.github.io/)\n- [adr-tools](https://github.com/npryce/adr-tools)"
              },
              {
                "name": "changelog-automation",
                "description": "Automate changelog generation from commits, PRs, and releases following Keep a Changelog format. Use when setting up release workflows, generating release notes, or standardizing commit conventions.",
                "path": "plugins/documentation-generation/skills/changelog-automation/SKILL.md",
                "frontmatter": {
                  "name": "changelog-automation",
                  "description": "Automate changelog generation from commits, PRs, and releases following Keep a Changelog format. Use when setting up release workflows, generating release notes, or standardizing commit conventions."
                },
                "content": "# Changelog Automation\n\nPatterns and tools for automating changelog generation, release notes, and version management following industry standards.\n\n## When to Use This Skill\n\n- Setting up automated changelog generation\n- Implementing Conventional Commits\n- Creating release note workflows\n- Standardizing commit message formats\n- Generating GitHub/GitLab release notes\n- Managing semantic versioning\n\n## Core Concepts\n\n### 1. Keep a Changelog Format\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- New feature X\n\n## [1.2.0] - 2024-01-15\n\n### Added\n- User profile avatars\n- Dark mode support\n\n### Changed\n- Improved loading performance by 40%\n\n### Deprecated\n- Old authentication API (use v2)\n\n### Removed\n- Legacy payment gateway\n\n### Fixed\n- Login timeout issue (#123)\n\n### Security\n- Updated dependencies for CVE-2024-1234\n\n[Unreleased]: https://github.com/user/repo/compare/v1.2.0...HEAD\n[1.2.0]: https://github.com/user/repo/compare/v1.1.0...v1.2.0\n```\n\n### 2. Conventional Commits\n\n```\n<type>[optional scope]: <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n| Type | Description | Changelog Section |\n|------|-------------|-------------------|\n| `feat` | New feature | Added |\n| `fix` | Bug fix | Fixed |\n| `docs` | Documentation | (usually excluded) |\n| `style` | Formatting | (usually excluded) |\n| `refactor` | Code restructure | Changed |\n| `perf` | Performance | Changed |\n| `test` | Tests | (usually excluded) |\n| `chore` | Maintenance | (usually excluded) |\n| `ci` | CI changes | (usually excluded) |\n| `build` | Build system | (usually excluded) |\n| `revert` | Revert commit | Removed |\n\n### 3. Semantic Versioning\n\n```\nMAJOR.MINOR.PATCH\n\nMAJOR: Breaking changes (feat! or BREAKING CHANGE)\nMINOR: New features (feat)\nPATCH: Bug fixes (fix)\n```\n\n## Implementation\n\n### Method 1: Conventional Changelog (Node.js)\n\n```bash\n# Install tools\nnpm install -D @commitlint/cli @commitlint/config-conventional\nnpm install -D husky\nnpm install -D standard-version\n# or\nnpm install -D semantic-release\n\n# Setup commitlint\ncat > commitlint.config.js << 'EOF'\nmodule.exports = {\n  extends: ['@commitlint/config-conventional'],\n  rules: {\n    'type-enum': [\n      2,\n      'always',\n      [\n        'feat',\n        'fix',\n        'docs',\n        'style',\n        'refactor',\n        'perf',\n        'test',\n        'chore',\n        'ci',\n        'build',\n        'revert',\n      ],\n    ],\n    'subject-case': [2, 'never', ['start-case', 'pascal-case', 'upper-case']],\n    'subject-max-length': [2, 'always', 72],\n  },\n};\nEOF\n\n# Setup husky\nnpx husky init\necho \"npx --no -- commitlint --edit \\$1\" > .husky/commit-msg\n```\n\n### Method 2: standard-version Configuration\n\n```javascript\n// .versionrc.js\nmodule.exports = {\n  types: [\n    { type: 'feat', section: 'Features' },\n    { type: 'fix', section: 'Bug Fixes' },\n    { type: 'perf', section: 'Performance Improvements' },\n    { type: 'revert', section: 'Reverts' },\n    { type: 'docs', section: 'Documentation', hidden: true },\n    { type: 'style', section: 'Styles', hidden: true },\n    { type: 'chore', section: 'Miscellaneous', hidden: true },\n    { type: 'refactor', section: 'Code Refactoring', hidden: true },\n    { type: 'test', section: 'Tests', hidden: true },\n    { type: 'build', section: 'Build System', hidden: true },\n    { type: 'ci', section: 'CI/CD', hidden: true },\n  ],\n  commitUrlFormat: '{{host}}/{{owner}}/{{repository}}/commit/{{hash}}',\n  compareUrlFormat: '{{host}}/{{owner}}/{{repository}}/compare/{{previousTag}}...{{currentTag}}',\n  issueUrlFormat: '{{host}}/{{owner}}/{{repository}}/issues/{{id}}',\n  userUrlFormat: '{{host}}/{{user}}',\n  releaseCommitMessageFormat: 'chore(release): {{currentTag}}',\n  scripts: {\n    prebump: 'echo \"Running prebump\"',\n    postbump: 'echo \"Running postbump\"',\n    prechangelog: 'echo \"Running prechangelog\"',\n    postchangelog: 'echo \"Running postchangelog\"',\n  },\n};\n```\n\n```json\n// package.json scripts\n{\n  \"scripts\": {\n    \"release\": \"standard-version\",\n    \"release:minor\": \"standard-version --release-as minor\",\n    \"release:major\": \"standard-version --release-as major\",\n    \"release:patch\": \"standard-version --release-as patch\",\n    \"release:dry\": \"standard-version --dry-run\"\n  }\n}\n```\n\n### Method 3: semantic-release (Full Automation)\n\n```javascript\n// release.config.js\nmodule.exports = {\n  branches: [\n    'main',\n    { name: 'beta', prerelease: true },\n    { name: 'alpha', prerelease: true },\n  ],\n  plugins: [\n    '@semantic-release/commit-analyzer',\n    '@semantic-release/release-notes-generator',\n    [\n      '@semantic-release/changelog',\n      {\n        changelogFile: 'CHANGELOG.md',\n      },\n    ],\n    [\n      '@semantic-release/npm',\n      {\n        npmPublish: true,\n      },\n    ],\n    [\n      '@semantic-release/github',\n      {\n        assets: ['dist/**/*.js', 'dist/**/*.css'],\n      },\n    ],\n    [\n      '@semantic-release/git',\n      {\n        assets: ['CHANGELOG.md', 'package.json'],\n        message: 'chore(release): ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}',\n      },\n    ],\n  ],\n};\n```\n\n### Method 4: GitHub Actions Workflow\n\n```yaml\n# .github/workflows/release.yml\nname: Release\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n    inputs:\n      release_type:\n        description: 'Release type'\n        required: true\n        default: 'patch'\n        type: choice\n        options:\n          - patch\n          - minor\n          - major\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - run: npm ci\n\n      - name: Configure Git\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"github-actions[bot]@users.noreply.github.com\"\n\n      - name: Run semantic-release\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n\n  # Alternative: manual release with standard-version\n  manual-release:\n    if: github.event_name == 'workflow_dispatch'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - run: npm ci\n\n      - name: Configure Git\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"github-actions[bot]@users.noreply.github.com\"\n\n      - name: Bump version and generate changelog\n        run: npx standard-version --release-as ${{ inputs.release_type }}\n\n      - name: Push changes\n        run: git push --follow-tags origin main\n\n      - name: Create GitHub Release\n        uses: softprops/action-gh-release@v1\n        with:\n          tag_name: ${{ steps.version.outputs.tag }}\n          body_path: RELEASE_NOTES.md\n          generate_release_notes: true\n```\n\n### Method 5: git-cliff (Rust-based, Fast)\n\n```toml\n# cliff.toml\n[changelog]\nheader = \"\"\"\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n\"\"\"\nbody = \"\"\"\n{% if version %}\\\n    ## [{{ version | trim_start_matches(pat=\"v\") }}] - {{ timestamp | date(format=\"%Y-%m-%d\") }}\n{% else %}\\\n    ## [Unreleased]\n{% endif %}\\\n{% for group, commits in commits | group_by(attribute=\"group\") %}\n    ### {{ group | upper_first }}\n    {% for commit in commits %}\n        - {% if commit.scope %}**{{ commit.scope }}:** {% endif %}\\\n            {{ commit.message | upper_first }}\\\n            {% if commit.github.pr_number %} ([#{{ commit.github.pr_number }}](https://github.com/owner/repo/pull/{{ commit.github.pr_number }})){% endif %}\\\n    {% endfor %}\n{% endfor %}\n\"\"\"\nfooter = \"\"\"\n{% for release in releases -%}\n    {% if release.version -%}\n        {% if release.previous.version -%}\n            [{{ release.version | trim_start_matches(pat=\"v\") }}]: \\\n                https://github.com/owner/repo/compare/{{ release.previous.version }}...{{ release.version }}\n        {% endif -%}\n    {% else -%}\n        [unreleased]: https://github.com/owner/repo/compare/{{ release.previous.version }}...HEAD\n    {% endif -%}\n{% endfor %}\n\"\"\"\ntrim = true\n\n[git]\nconventional_commits = true\nfilter_unconventional = true\nsplit_commits = false\ncommit_parsers = [\n    { message = \"^feat\", group = \"Features\" },\n    { message = \"^fix\", group = \"Bug Fixes\" },\n    { message = \"^doc\", group = \"Documentation\" },\n    { message = \"^perf\", group = \"Performance\" },\n    { message = \"^refactor\", group = \"Refactoring\" },\n    { message = \"^style\", group = \"Styling\" },\n    { message = \"^test\", group = \"Testing\" },\n    { message = \"^chore\\\\(release\\\\)\", skip = true },\n    { message = \"^chore\", group = \"Miscellaneous\" },\n]\nfilter_commits = false\ntag_pattern = \"v[0-9]*\"\nskip_tags = \"\"\nignore_tags = \"\"\ntopo_order = false\nsort_commits = \"oldest\"\n\n[github]\nowner = \"owner\"\nrepo = \"repo\"\n```\n\n```bash\n# Generate changelog\ngit cliff -o CHANGELOG.md\n\n# Generate for specific range\ngit cliff v1.0.0..v2.0.0 -o RELEASE_NOTES.md\n\n# Preview without writing\ngit cliff --unreleased --dry-run\n```\n\n### Method 6: Python (commitizen)\n\n```toml\n# pyproject.toml\n[tool.commitizen]\nname = \"cz_conventional_commits\"\nversion = \"1.0.0\"\nversion_files = [\n    \"pyproject.toml:version\",\n    \"src/__init__.py:__version__\",\n]\ntag_format = \"v$version\"\nupdate_changelog_on_bump = true\nchangelog_incremental = true\nchangelog_start_rev = \"v0.1.0\"\n\n[tool.commitizen.customize]\nmessage_template = \"{{change_type}}{% if scope %}({{scope}}){% endif %}: {{message}}\"\nschema = \"<type>(<scope>): <subject>\"\nschema_pattern = \"^(feat|fix|docs|style|refactor|perf|test|chore)(\\\\(\\\\w+\\\\))?:\\\\s.*\"\nbump_pattern = \"^(feat|fix|perf|refactor)\"\nbump_map = {\"feat\" = \"MINOR\", \"fix\" = \"PATCH\", \"perf\" = \"PATCH\", \"refactor\" = \"PATCH\"}\n```\n\n```bash\n# Install\npip install commitizen\n\n# Create commit interactively\ncz commit\n\n# Bump version and update changelog\ncz bump --changelog\n\n# Check commits\ncz check --rev-range HEAD~5..HEAD\n```\n\n## Release Notes Templates\n\n### GitHub Release Template\n\n```markdown\n## What's Changed\n\n### ðŸš€ Features\n{{ range .Features }}\n- {{ .Title }} by @{{ .Author }} in #{{ .PR }}\n{{ end }}\n\n### ðŸ› Bug Fixes\n{{ range .Fixes }}\n- {{ .Title }} by @{{ .Author }} in #{{ .PR }}\n{{ end }}\n\n### ðŸ“š Documentation\n{{ range .Docs }}\n- {{ .Title }} by @{{ .Author }} in #{{ .PR }}\n{{ end }}\n\n### ðŸ”§ Maintenance\n{{ range .Chores }}\n- {{ .Title }} by @{{ .Author }} in #{{ .PR }}\n{{ end }}\n\n## New Contributors\n{{ range .NewContributors }}\n- @{{ .Username }} made their first contribution in #{{ .PR }}\n{{ end }}\n\n**Full Changelog**: https://github.com/owner/repo/compare/v{{ .Previous }}...v{{ .Current }}\n```\n\n### Internal Release Notes\n\n```markdown\n# Release v2.1.0 - January 15, 2024\n\n## Summary\nThis release introduces dark mode support and improves checkout performance\nby 40%. It also includes important security updates.\n\n## Highlights\n\n### ðŸŒ™ Dark Mode\nUsers can now switch to dark mode from settings. The preference is\nautomatically saved and synced across devices.\n\n### âš¡ Performance\n- Checkout flow is 40% faster\n- Reduced bundle size by 15%\n\n## Breaking Changes\nNone in this release.\n\n## Upgrade Guide\nNo special steps required. Standard deployment process applies.\n\n## Known Issues\n- Dark mode may flicker on initial load (fix scheduled for v2.1.1)\n\n## Dependencies Updated\n| Package | From | To | Reason |\n|---------|------|-----|--------|\n| react | 18.2.0 | 18.3.0 | Performance improvements |\n| lodash | 4.17.20 | 4.17.21 | Security patch |\n```\n\n## Commit Message Examples\n\n```bash\n# Feature with scope\nfeat(auth): add OAuth2 support for Google login\n\n# Bug fix with issue reference\nfix(checkout): resolve race condition in payment processing\n\nCloses #123\n\n# Breaking change\nfeat(api)!: change user endpoint response format\n\nBREAKING CHANGE: The user endpoint now returns `userId` instead of `id`.\nMigration guide: Update all API consumers to use the new field name.\n\n# Multiple paragraphs\nfix(database): handle connection timeouts gracefully\n\nPreviously, connection timeouts would cause the entire request to fail\nwithout retry. This change implements exponential backoff with up to\n3 retries before failing.\n\nThe timeout threshold has been increased from 5s to 10s based on p99\nlatency analysis.\n\nFixes #456\nReviewed-by: @alice\n```\n\n## Best Practices\n\n### Do's\n- **Follow Conventional Commits** - Enables automation\n- **Write clear messages** - Future you will thank you\n- **Reference issues** - Link commits to tickets\n- **Use scopes consistently** - Define team conventions\n- **Automate releases** - Reduce manual errors\n\n### Don'ts\n- **Don't mix changes** - One logical change per commit\n- **Don't skip validation** - Use commitlint\n- **Don't manual edit** - Generated changelogs only\n- **Don't forget breaking changes** - Mark with `!` or footer\n- **Don't ignore CI** - Validate commits in pipeline\n\n## Resources\n\n- [Keep a Changelog](https://keepachangelog.com/)\n- [Conventional Commits](https://www.conventionalcommits.org/)\n- [Semantic Versioning](https://semver.org/)\n- [semantic-release](https://semantic-release.gitbook.io/)\n- [git-cliff](https://git-cliff.org/)"
              },
              {
                "name": "openapi-spec-generation",
                "description": "Generate and maintain OpenAPI 3.1 specifications from code, design-first specs, and validation patterns. Use when creating API documentation, generating SDKs, or ensuring API contract compliance.",
                "path": "plugins/documentation-generation/skills/openapi-spec-generation/SKILL.md",
                "frontmatter": {
                  "name": "openapi-spec-generation",
                  "description": "Generate and maintain OpenAPI 3.1 specifications from code, design-first specs, and validation patterns. Use when creating API documentation, generating SDKs, or ensuring API contract compliance."
                },
                "content": "# OpenAPI Spec Generation\n\nComprehensive patterns for creating, maintaining, and validating OpenAPI 3.1 specifications for RESTful APIs.\n\n## When to Use This Skill\n\n- Creating API documentation from scratch\n- Generating OpenAPI specs from existing code\n- Designing API contracts (design-first approach)\n- Validating API implementations against specs\n- Generating client SDKs from specs\n- Setting up API documentation portals\n\n## Core Concepts\n\n### 1. OpenAPI 3.1 Structure\n\n```yaml\nopenapi: 3.1.0\ninfo:\n  title: API Title\n  version: 1.0.0\nservers:\n  - url: https://api.example.com/v1\npaths:\n  /resources:\n    get: ...\ncomponents:\n  schemas: ...\n  securitySchemes: ...\n```\n\n### 2. Design Approaches\n\n| Approach | Description | Best For |\n|----------|-------------|----------|\n| **Design-First** | Write spec before code | New APIs, contracts |\n| **Code-First** | Generate spec from code | Existing APIs |\n| **Hybrid** | Annotate code, generate spec | Evolving APIs |\n\n## Templates\n\n### Template 1: Complete API Specification\n\n```yaml\nopenapi: 3.1.0\ninfo:\n  title: User Management API\n  description: |\n    API for managing users and their profiles.\n\n    ## Authentication\n    All endpoints require Bearer token authentication.\n\n    ## Rate Limiting\n    - 1000 requests per minute for standard tier\n    - 10000 requests per minute for enterprise tier\n  version: 2.0.0\n  contact:\n    name: API Support\n    email: api-support@example.com\n    url: https://docs.example.com\n  license:\n    name: MIT\n    url: https://opensource.org/licenses/MIT\n\nservers:\n  - url: https://api.example.com/v2\n    description: Production\n  - url: https://staging-api.example.com/v2\n    description: Staging\n  - url: http://localhost:3000/v2\n    description: Local development\n\ntags:\n  - name: Users\n    description: User management operations\n  - name: Profiles\n    description: User profile operations\n  - name: Admin\n    description: Administrative operations\n\npaths:\n  /users:\n    get:\n      operationId: listUsers\n      summary: List all users\n      description: Returns a paginated list of users with optional filtering.\n      tags:\n        - Users\n      parameters:\n        - $ref: '#/components/parameters/PageParam'\n        - $ref: '#/components/parameters/LimitParam'\n        - name: status\n          in: query\n          description: Filter by user status\n          schema:\n            $ref: '#/components/schemas/UserStatus'\n        - name: search\n          in: query\n          description: Search by name or email\n          schema:\n            type: string\n            minLength: 2\n            maxLength: 100\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserListResponse'\n              examples:\n                default:\n                  $ref: '#/components/examples/UserListExample'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '401':\n          $ref: '#/components/responses/Unauthorized'\n        '429':\n          $ref: '#/components/responses/RateLimited'\n      security:\n        - bearerAuth: []\n\n    post:\n      operationId: createUser\n      summary: Create a new user\n      description: Creates a new user account and sends welcome email.\n      tags:\n        - Users\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateUserRequest'\n            examples:\n              standard:\n                summary: Standard user\n                value:\n                  email: user@example.com\n                  name: John Doe\n                  role: user\n              admin:\n                summary: Admin user\n                value:\n                  email: admin@example.com\n                  name: Admin User\n                  role: admin\n      responses:\n        '201':\n          description: User created successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/User'\n          headers:\n            Location:\n              description: URL of created user\n              schema:\n                type: string\n                format: uri\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '409':\n          description: Email already exists\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Error'\n      security:\n        - bearerAuth: []\n\n  /users/{userId}:\n    parameters:\n      - $ref: '#/components/parameters/UserIdParam'\n\n    get:\n      operationId: getUser\n      summary: Get user by ID\n      tags:\n        - Users\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/User'\n        '404':\n          $ref: '#/components/responses/NotFound'\n      security:\n        - bearerAuth: []\n\n    patch:\n      operationId: updateUser\n      summary: Update user\n      tags:\n        - Users\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpdateUserRequest'\n      responses:\n        '200':\n          description: User updated\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/User'\n        '400':\n          $ref: '#/components/responses/BadRequest'\n        '404':\n          $ref: '#/components/responses/NotFound'\n      security:\n        - bearerAuth: []\n\n    delete:\n      operationId: deleteUser\n      summary: Delete user\n      tags:\n        - Users\n        - Admin\n      responses:\n        '204':\n          description: User deleted\n        '404':\n          $ref: '#/components/responses/NotFound'\n      security:\n        - bearerAuth: []\n        - apiKey: []\n\ncomponents:\n  schemas:\n    User:\n      type: object\n      required:\n        - id\n        - email\n        - name\n        - status\n        - createdAt\n      properties:\n        id:\n          type: string\n          format: uuid\n          readOnly: true\n          description: Unique user identifier\n        email:\n          type: string\n          format: email\n          description: User email address\n        name:\n          type: string\n          minLength: 1\n          maxLength: 100\n          description: User display name\n        status:\n          $ref: '#/components/schemas/UserStatus'\n        role:\n          type: string\n          enum: [user, moderator, admin]\n          default: user\n        avatar:\n          type: string\n          format: uri\n          nullable: true\n        metadata:\n          type: object\n          additionalProperties: true\n          description: Custom metadata\n        createdAt:\n          type: string\n          format: date-time\n          readOnly: true\n        updatedAt:\n          type: string\n          format: date-time\n          readOnly: true\n\n    UserStatus:\n      type: string\n      enum: [active, inactive, suspended, pending]\n      description: User account status\n\n    CreateUserRequest:\n      type: object\n      required:\n        - email\n        - name\n      properties:\n        email:\n          type: string\n          format: email\n        name:\n          type: string\n          minLength: 1\n          maxLength: 100\n        role:\n          type: string\n          enum: [user, moderator, admin]\n          default: user\n        metadata:\n          type: object\n          additionalProperties: true\n\n    UpdateUserRequest:\n      type: object\n      minProperties: 1\n      properties:\n        name:\n          type: string\n          minLength: 1\n          maxLength: 100\n        status:\n          $ref: '#/components/schemas/UserStatus'\n        role:\n          type: string\n          enum: [user, moderator, admin]\n        metadata:\n          type: object\n          additionalProperties: true\n\n    UserListResponse:\n      type: object\n      required:\n        - data\n        - pagination\n      properties:\n        data:\n          type: array\n          items:\n            $ref: '#/components/schemas/User'\n        pagination:\n          $ref: '#/components/schemas/Pagination'\n\n    Pagination:\n      type: object\n      required:\n        - page\n        - limit\n        - total\n        - totalPages\n      properties:\n        page:\n          type: integer\n          minimum: 1\n        limit:\n          type: integer\n          minimum: 1\n          maximum: 100\n        total:\n          type: integer\n          minimum: 0\n        totalPages:\n          type: integer\n          minimum: 0\n        hasNext:\n          type: boolean\n        hasPrev:\n          type: boolean\n\n    Error:\n      type: object\n      required:\n        - code\n        - message\n      properties:\n        code:\n          type: string\n          description: Error code for programmatic handling\n        message:\n          type: string\n          description: Human-readable error message\n        details:\n          type: array\n          items:\n            type: object\n            properties:\n              field:\n                type: string\n              message:\n                type: string\n        requestId:\n          type: string\n          description: Request ID for support\n\n  parameters:\n    UserIdParam:\n      name: userId\n      in: path\n      required: true\n      description: User ID\n      schema:\n        type: string\n        format: uuid\n\n    PageParam:\n      name: page\n      in: query\n      description: Page number (1-based)\n      schema:\n        type: integer\n        minimum: 1\n        default: 1\n\n    LimitParam:\n      name: limit\n      in: query\n      description: Items per page\n      schema:\n        type: integer\n        minimum: 1\n        maximum: 100\n        default: 20\n\n  responses:\n    BadRequest:\n      description: Invalid request\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n          example:\n            code: VALIDATION_ERROR\n            message: Invalid request parameters\n            details:\n              - field: email\n                message: Must be a valid email address\n\n    Unauthorized:\n      description: Authentication required\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n          example:\n            code: UNAUTHORIZED\n            message: Authentication required\n\n    NotFound:\n      description: Resource not found\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n          example:\n            code: NOT_FOUND\n            message: User not found\n\n    RateLimited:\n      description: Too many requests\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'\n      headers:\n        Retry-After:\n          description: Seconds until rate limit resets\n          schema:\n            type: integer\n        X-RateLimit-Limit:\n          description: Request limit per window\n          schema:\n            type: integer\n        X-RateLimit-Remaining:\n          description: Remaining requests in window\n          schema:\n            type: integer\n\n  examples:\n    UserListExample:\n      value:\n        data:\n          - id: \"550e8400-e29b-41d4-a716-446655440000\"\n            email: \"john@example.com\"\n            name: \"John Doe\"\n            status: \"active\"\n            role: \"user\"\n            createdAt: \"2024-01-15T10:30:00Z\"\n        pagination:\n          page: 1\n          limit: 20\n          total: 1\n          totalPages: 1\n          hasNext: false\n          hasPrev: false\n\n  securitySchemes:\n    bearerAuth:\n      type: http\n      scheme: bearer\n      bearerFormat: JWT\n      description: JWT token from /auth/login\n\n    apiKey:\n      type: apiKey\n      in: header\n      name: X-API-Key\n      description: API key for service-to-service calls\n\nsecurity:\n  - bearerAuth: []\n```\n\n### Template 2: Code-First Generation (Python/FastAPI)\n\n```python\n# FastAPI with automatic OpenAPI generation\nfrom fastapi import FastAPI, HTTPException, Query, Path, Depends\nfrom pydantic import BaseModel, Field, EmailStr\nfrom typing import Optional, List\nfrom datetime import datetime\nfrom uuid import UUID\nfrom enum import Enum\n\napp = FastAPI(\n    title=\"User Management API\",\n    description=\"API for managing users and profiles\",\n    version=\"2.0.0\",\n    openapi_tags=[\n        {\"name\": \"Users\", \"description\": \"User operations\"},\n        {\"name\": \"Profiles\", \"description\": \"Profile operations\"},\n    ],\n    servers=[\n        {\"url\": \"https://api.example.com/v2\", \"description\": \"Production\"},\n        {\"url\": \"http://localhost:8000\", \"description\": \"Development\"},\n    ],\n)\n\n# Enums\nclass UserStatus(str, Enum):\n    active = \"active\"\n    inactive = \"inactive\"\n    suspended = \"suspended\"\n    pending = \"pending\"\n\nclass UserRole(str, Enum):\n    user = \"user\"\n    moderator = \"moderator\"\n    admin = \"admin\"\n\n# Models\nclass UserBase(BaseModel):\n    email: EmailStr = Field(..., description=\"User email address\")\n    name: str = Field(..., min_length=1, max_length=100, description=\"Display name\")\n\nclass UserCreate(UserBase):\n    role: UserRole = Field(default=UserRole.user)\n    metadata: Optional[dict] = Field(default=None, description=\"Custom metadata\")\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"email\": \"user@example.com\",\n                    \"name\": \"John Doe\",\n                    \"role\": \"user\"\n                }\n            ]\n        }\n    }\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = Field(None, min_length=1, max_length=100)\n    status: Optional[UserStatus] = None\n    role: Optional[UserRole] = None\n    metadata: Optional[dict] = None\n\nclass User(UserBase):\n    id: UUID = Field(..., description=\"Unique identifier\")\n    status: UserStatus\n    role: UserRole\n    avatar: Optional[str] = Field(None, description=\"Avatar URL\")\n    metadata: Optional[dict] = None\n    created_at: datetime = Field(..., alias=\"createdAt\")\n    updated_at: Optional[datetime] = Field(None, alias=\"updatedAt\")\n\n    model_config = {\"populate_by_name\": True}\n\nclass Pagination(BaseModel):\n    page: int = Field(..., ge=1)\n    limit: int = Field(..., ge=1, le=100)\n    total: int = Field(..., ge=0)\n    total_pages: int = Field(..., ge=0, alias=\"totalPages\")\n    has_next: bool = Field(..., alias=\"hasNext\")\n    has_prev: bool = Field(..., alias=\"hasPrev\")\n\nclass UserListResponse(BaseModel):\n    data: List[User]\n    pagination: Pagination\n\nclass ErrorDetail(BaseModel):\n    field: str\n    message: str\n\nclass ErrorResponse(BaseModel):\n    code: str = Field(..., description=\"Error code\")\n    message: str = Field(..., description=\"Error message\")\n    details: Optional[List[ErrorDetail]] = None\n    request_id: Optional[str] = Field(None, alias=\"requestId\")\n\n# Endpoints\n@app.get(\n    \"/users\",\n    response_model=UserListResponse,\n    tags=[\"Users\"],\n    summary=\"List all users\",\n    description=\"Returns a paginated list of users with optional filtering.\",\n    responses={\n        400: {\"model\": ErrorResponse, \"description\": \"Invalid request\"},\n        401: {\"model\": ErrorResponse, \"description\": \"Unauthorized\"},\n    },\n)\nasync def list_users(\n    page: int = Query(1, ge=1, description=\"Page number\"),\n    limit: int = Query(20, ge=1, le=100, description=\"Items per page\"),\n    status: Optional[UserStatus] = Query(None, description=\"Filter by status\"),\n    search: Optional[str] = Query(None, min_length=2, max_length=100),\n):\n    \"\"\"\n    List users with pagination and filtering.\n\n    - **page**: Page number (1-based)\n    - **limit**: Number of items per page (max 100)\n    - **status**: Filter by user status\n    - **search**: Search by name or email\n    \"\"\"\n    # Implementation\n    pass\n\n@app.post(\n    \"/users\",\n    response_model=User,\n    status_code=201,\n    tags=[\"Users\"],\n    summary=\"Create a new user\",\n    responses={\n        400: {\"model\": ErrorResponse},\n        409: {\"model\": ErrorResponse, \"description\": \"Email already exists\"},\n    },\n)\nasync def create_user(user: UserCreate):\n    \"\"\"Create a new user and send welcome email.\"\"\"\n    pass\n\n@app.get(\n    \"/users/{user_id}\",\n    response_model=User,\n    tags=[\"Users\"],\n    summary=\"Get user by ID\",\n    responses={404: {\"model\": ErrorResponse}},\n)\nasync def get_user(\n    user_id: UUID = Path(..., description=\"User ID\"),\n):\n    \"\"\"Retrieve a specific user by their ID.\"\"\"\n    pass\n\n@app.patch(\n    \"/users/{user_id}\",\n    response_model=User,\n    tags=[\"Users\"],\n    summary=\"Update user\",\n    responses={\n        400: {\"model\": ErrorResponse},\n        404: {\"model\": ErrorResponse},\n    },\n)\nasync def update_user(\n    user_id: UUID = Path(..., description=\"User ID\"),\n    user: UserUpdate = ...,\n):\n    \"\"\"Update user attributes.\"\"\"\n    pass\n\n@app.delete(\n    \"/users/{user_id}\",\n    status_code=204,\n    tags=[\"Users\", \"Admin\"],\n    summary=\"Delete user\",\n    responses={404: {\"model\": ErrorResponse}},\n)\nasync def delete_user(\n    user_id: UUID = Path(..., description=\"User ID\"),\n):\n    \"\"\"Permanently delete a user.\"\"\"\n    pass\n\n# Export OpenAPI spec\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(app.openapi(), indent=2))\n```\n\n### Template 3: Code-First (TypeScript/Express with tsoa)\n\n```typescript\n// tsoa generates OpenAPI from TypeScript decorators\n\nimport {\n  Controller,\n  Get,\n  Post,\n  Patch,\n  Delete,\n  Route,\n  Path,\n  Query,\n  Body,\n  Response,\n  SuccessResponse,\n  Tags,\n  Security,\n  Example,\n} from \"tsoa\";\n\n// Models\ninterface User {\n  /** Unique identifier */\n  id: string;\n  /** User email address */\n  email: string;\n  /** Display name */\n  name: string;\n  status: UserStatus;\n  role: UserRole;\n  /** Avatar URL */\n  avatar?: string;\n  /** Custom metadata */\n  metadata?: Record<string, unknown>;\n  createdAt: Date;\n  updatedAt?: Date;\n}\n\nenum UserStatus {\n  Active = \"active\",\n  Inactive = \"inactive\",\n  Suspended = \"suspended\",\n  Pending = \"pending\",\n}\n\nenum UserRole {\n  User = \"user\",\n  Moderator = \"moderator\",\n  Admin = \"admin\",\n}\n\ninterface CreateUserRequest {\n  email: string;\n  name: string;\n  role?: UserRole;\n  metadata?: Record<string, unknown>;\n}\n\ninterface UpdateUserRequest {\n  name?: string;\n  status?: UserStatus;\n  role?: UserRole;\n  metadata?: Record<string, unknown>;\n}\n\ninterface Pagination {\n  page: number;\n  limit: number;\n  total: number;\n  totalPages: number;\n  hasNext: boolean;\n  hasPrev: boolean;\n}\n\ninterface UserListResponse {\n  data: User[];\n  pagination: Pagination;\n}\n\ninterface ErrorResponse {\n  code: string;\n  message: string;\n  details?: { field: string; message: string }[];\n  requestId?: string;\n}\n\n@Route(\"users\")\n@Tags(\"Users\")\nexport class UsersController extends Controller {\n  /**\n   * List all users with pagination and filtering\n   * @param page Page number (1-based)\n   * @param limit Items per page (max 100)\n   * @param status Filter by user status\n   * @param search Search by name or email\n   */\n  @Get()\n  @Security(\"bearerAuth\")\n  @Response<ErrorResponse>(400, \"Invalid request\")\n  @Response<ErrorResponse>(401, \"Unauthorized\")\n  @Example<UserListResponse>({\n    data: [\n      {\n        id: \"550e8400-e29b-41d4-a716-446655440000\",\n        email: \"john@example.com\",\n        name: \"John Doe\",\n        status: UserStatus.Active,\n        role: UserRole.User,\n        createdAt: new Date(\"2024-01-15T10:30:00Z\"),\n      },\n    ],\n    pagination: {\n      page: 1,\n      limit: 20,\n      total: 1,\n      totalPages: 1,\n      hasNext: false,\n      hasPrev: false,\n    },\n  })\n  public async listUsers(\n    @Query() page: number = 1,\n    @Query() limit: number = 20,\n    @Query() status?: UserStatus,\n    @Query() search?: string\n  ): Promise<UserListResponse> {\n    // Implementation\n    throw new Error(\"Not implemented\");\n  }\n\n  /**\n   * Create a new user\n   */\n  @Post()\n  @Security(\"bearerAuth\")\n  @SuccessResponse(201, \"Created\")\n  @Response<ErrorResponse>(400, \"Invalid request\")\n  @Response<ErrorResponse>(409, \"Email already exists\")\n  public async createUser(\n    @Body() body: CreateUserRequest\n  ): Promise<User> {\n    this.setStatus(201);\n    throw new Error(\"Not implemented\");\n  }\n\n  /**\n   * Get user by ID\n   * @param userId User ID\n   */\n  @Get(\"{userId}\")\n  @Security(\"bearerAuth\")\n  @Response<ErrorResponse>(404, \"User not found\")\n  public async getUser(\n    @Path() userId: string\n  ): Promise<User> {\n    throw new Error(\"Not implemented\");\n  }\n\n  /**\n   * Update user attributes\n   * @param userId User ID\n   */\n  @Patch(\"{userId}\")\n  @Security(\"bearerAuth\")\n  @Response<ErrorResponse>(400, \"Invalid request\")\n  @Response<ErrorResponse>(404, \"User not found\")\n  public async updateUser(\n    @Path() userId: string,\n    @Body() body: UpdateUserRequest\n  ): Promise<User> {\n    throw new Error(\"Not implemented\");\n  }\n\n  /**\n   * Delete user\n   * @param userId User ID\n   */\n  @Delete(\"{userId}\")\n  @Tags(\"Users\", \"Admin\")\n  @Security(\"bearerAuth\")\n  @SuccessResponse(204, \"Deleted\")\n  @Response<ErrorResponse>(404, \"User not found\")\n  public async deleteUser(\n    @Path() userId: string\n  ): Promise<void> {\n    this.setStatus(204);\n  }\n}\n```\n\n### Template 4: Validation & Linting\n\n```bash\n# Install validation tools\nnpm install -g @stoplight/spectral-cli\nnpm install -g @redocly/cli\n\n# Spectral ruleset (.spectral.yaml)\ncat > .spectral.yaml << 'EOF'\nextends: [\"spectral:oas\", \"spectral:asyncapi\"]\n\nrules:\n  # Enforce operation IDs\n  operation-operationId: error\n\n  # Require descriptions\n  operation-description: warn\n  info-description: error\n\n  # Naming conventions\n  operation-operationId-valid-in-url: true\n\n  # Security\n  operation-security-defined: error\n\n  # Response codes\n  operation-success-response: error\n\n  # Custom rules\n  path-params-snake-case:\n    description: Path parameters should be snake_case\n    severity: warn\n    given: \"$.paths[*].parameters[?(@.in == 'path')].name\"\n    then:\n      function: pattern\n      functionOptions:\n        match: \"^[a-z][a-z0-9_]*$\"\n\n  schema-properties-camelCase:\n    description: Schema properties should be camelCase\n    severity: warn\n    given: \"$.components.schemas[*].properties[*]~\"\n    then:\n      function: casing\n      functionOptions:\n        type: camel\nEOF\n\n# Run Spectral\nspectral lint openapi.yaml\n\n# Redocly config (redocly.yaml)\ncat > redocly.yaml << 'EOF'\nextends:\n  - recommended\n\nrules:\n  no-invalid-media-type-examples: error\n  no-invalid-schema-examples: error\n  operation-4xx-response: warn\n  request-mime-type:\n    severity: error\n    allowedValues:\n      - application/json\n  response-mime-type:\n    severity: error\n    allowedValues:\n      - application/json\n      - application/problem+json\n\ntheme:\n  openapi:\n    generateCodeSamples:\n      languages:\n        - lang: curl\n        - lang: python\n        - lang: javascript\nEOF\n\n# Run Redocly\nredocly lint openapi.yaml\nredocly bundle openapi.yaml -o bundled.yaml\nredocly preview-docs openapi.yaml\n```\n\n## SDK Generation\n\n```bash\n# OpenAPI Generator\nnpm install -g @openapitools/openapi-generator-cli\n\n# Generate TypeScript client\nopenapi-generator-cli generate \\\n  -i openapi.yaml \\\n  -g typescript-fetch \\\n  -o ./generated/typescript-client \\\n  --additional-properties=supportsES6=true,npmName=@myorg/api-client\n\n# Generate Python client\nopenapi-generator-cli generate \\\n  -i openapi.yaml \\\n  -g python \\\n  -o ./generated/python-client \\\n  --additional-properties=packageName=api_client\n\n# Generate Go client\nopenapi-generator-cli generate \\\n  -i openapi.yaml \\\n  -g go \\\n  -o ./generated/go-client\n```\n\n## Best Practices\n\n### Do's\n- **Use $ref** - Reuse schemas, parameters, responses\n- **Add examples** - Real-world values help consumers\n- **Document errors** - All possible error codes\n- **Version your API** - In URL or header\n- **Use semantic versioning** - For spec changes\n\n### Don'ts\n- **Don't use generic descriptions** - Be specific\n- **Don't skip security** - Define all schemes\n- **Don't forget nullable** - Be explicit about null\n- **Don't mix styles** - Consistent naming throughout\n- **Don't hardcode URLs** - Use server variables\n\n## Resources\n\n- [OpenAPI 3.1 Specification](https://spec.openapis.org/oas/v3.1.0)\n- [Swagger Editor](https://editor.swagger.io/)\n- [Redocly](https://redocly.com/)\n- [Spectral](https://stoplight.io/open-source/spectral)"
              }
            ]
          },
          {
            "name": "frontend-mobile-development",
            "description": "Frontend and mobile development with React and Tailwind",
            "source": "./plugins/frontend-mobile-development",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install frontend-mobile-development@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/component-scaffold",
                "description": null,
                "path": "plugins/frontend-mobile-development/commands/component-scaffold.md",
                "frontmatter": null,
                "content": "# React/React Native Component Scaffolding\n\nYou are a React component architecture expert specializing in scaffolding production-ready, accessible, and performant components. Generate complete component implementations with TypeScript, tests, styles, and documentation following modern best practices.\n\n## Context\n\nThe user needs automated component scaffolding that creates consistent, type-safe React components with proper structure, hooks, styling, accessibility, and test coverage. Focus on reusable patterns and scalable architecture.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Component Requirements\n\n```typescript\ninterface ComponentSpec {\n  name: string;\n  type: 'functional' | 'page' | 'layout' | 'form' | 'data-display';\n  props: PropDefinition[];\n  state?: StateDefinition[];\n  hooks?: string[];\n  styling: 'css-modules' | 'styled-components' | 'tailwind';\n  platform: 'web' | 'native' | 'universal';\n}\n\ninterface PropDefinition {\n  name: string;\n  type: string;\n  required: boolean;\n  defaultValue?: any;\n  description: string;\n}\n\nclass ComponentAnalyzer {\n  parseRequirements(input: string): ComponentSpec {\n    // Extract component specifications from user input\n    return {\n      name: this.extractName(input),\n      type: this.inferType(input),\n      props: this.extractProps(input),\n      state: this.extractState(input),\n      hooks: this.identifyHooks(input),\n      styling: this.detectStylingApproach(),\n      platform: this.detectPlatform()\n    };\n  }\n}\n```\n\n### 2. Generate React Component\n\n```typescript\ninterface GeneratorOptions {\n  typescript: boolean;\n  testing: boolean;\n  storybook: boolean;\n  accessibility: boolean;\n}\n\nclass ReactComponentGenerator {\n  generate(spec: ComponentSpec, options: GeneratorOptions): ComponentFiles {\n    return {\n      component: this.generateComponent(spec, options),\n      types: options.typescript ? this.generateTypes(spec) : null,\n      styles: this.generateStyles(spec),\n      tests: options.testing ? this.generateTests(spec) : null,\n      stories: options.storybook ? this.generateStories(spec) : null,\n      index: this.generateIndex(spec)\n    };\n  }\n\n  generateComponent(spec: ComponentSpec, options: GeneratorOptions): string {\n    const imports = this.generateImports(spec, options);\n    const types = options.typescript ? this.generatePropTypes(spec) : '';\n    const component = this.generateComponentBody(spec, options);\n    const exports = this.generateExports(spec);\n\n    return `${imports}\\n\\n${types}\\n\\n${component}\\n\\n${exports}`;\n  }\n\n  generateImports(spec: ComponentSpec, options: GeneratorOptions): string {\n    const imports = [\"import React, { useState, useEffect } from 'react';\"];\n\n    if (spec.styling === 'css-modules') {\n      imports.push(`import styles from './${spec.name}.module.css';`);\n    } else if (spec.styling === 'styled-components') {\n      imports.push(\"import styled from 'styled-components';\");\n    }\n\n    if (options.accessibility) {\n      imports.push(\"import { useA11y } from '@/hooks/useA11y';\");\n    }\n\n    return imports.join('\\n');\n  }\n\n  generatePropTypes(spec: ComponentSpec): string {\n    const props = spec.props.map(p => {\n      const optional = p.required ? '' : '?';\n      const comment = p.description ? `  /** ${p.description} */\\n` : '';\n      return `${comment}  ${p.name}${optional}: ${p.type};`;\n    }).join('\\n');\n\n    return `export interface ${spec.name}Props {\\n${props}\\n}`;\n  }\n\n  generateComponentBody(spec: ComponentSpec, options: GeneratorOptions): string {\n    const propsType = options.typescript ? `: React.FC<${spec.name}Props>` : '';\n    const destructuredProps = spec.props.map(p => p.name).join(', ');\n\n    let body = `export const ${spec.name}${propsType} = ({ ${destructuredProps} }) => {\\n`;\n\n    // Add state hooks\n    if (spec.state) {\n      body += spec.state.map(s =>\n        `  const [${s.name}, set${this.capitalize(s.name)}] = useState${options.typescript ? `<${s.type}>` : ''}(${s.initial});\\n`\n      ).join('');\n      body += '\\n';\n    }\n\n    // Add effects\n    if (spec.hooks?.includes('useEffect')) {\n      body += `  useEffect(() => {\\n`;\n      body += `    // TODO: Add effect logic\\n`;\n      body += `  }, [${destructuredProps}]);\\n\\n`;\n    }\n\n    // Add accessibility\n    if (options.accessibility) {\n      body += `  const a11yProps = useA11y({\\n`;\n      body += `    role: '${this.inferAriaRole(spec.type)}',\\n`;\n      body += `    label: ${spec.props.find(p => p.name === 'label')?.name || `'${spec.name}'`}\\n`;\n      body += `  });\\n\\n`;\n    }\n\n    // JSX return\n    body += `  return (\\n`;\n    body += this.generateJSX(spec, options);\n    body += `  );\\n`;\n    body += `};`;\n\n    return body;\n  }\n\n  generateJSX(spec: ComponentSpec, options: GeneratorOptions): string {\n    const className = spec.styling === 'css-modules' ? `className={styles.${this.camelCase(spec.name)}}` : '';\n    const a11y = options.accessibility ? '{...a11yProps}' : '';\n\n    return `    <div ${className} ${a11y}>\\n` +\n           `      {/* TODO: Add component content */}\\n` +\n           `    </div>\\n`;\n  }\n}\n```\n\n### 3. Generate React Native Component\n\n```typescript\nclass ReactNativeGenerator {\n  generateComponent(spec: ComponentSpec): string {\n    return `\nimport React, { useState } from 'react';\nimport {\n  View,\n  Text,\n  StyleSheet,\n  TouchableOpacity,\n  AccessibilityInfo\n} from 'react-native';\n\ninterface ${spec.name}Props {\n${spec.props.map(p => `  ${p.name}${p.required ? '' : '?'}: ${this.mapNativeType(p.type)};`).join('\\n')}\n}\n\nexport const ${spec.name}: React.FC<${spec.name}Props> = ({\n  ${spec.props.map(p => p.name).join(',\\n  ')}\n}) => {\n  return (\n    <View\n      style={styles.container}\n      accessible={true}\n      accessibilityLabel=\"${spec.name} component\"\n    >\n      <Text style={styles.text}>\n        {/* Component content */}\n      </Text>\n    </View>\n  );\n};\n\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n    padding: 16,\n    backgroundColor: '#fff',\n  },\n  text: {\n    fontSize: 16,\n    color: '#333',\n  },\n});\n`;\n  }\n\n  mapNativeType(webType: string): string {\n    const typeMap: Record<string, string> = {\n      'string': 'string',\n      'number': 'number',\n      'boolean': 'boolean',\n      'React.ReactNode': 'React.ReactNode',\n      'Function': '() => void'\n    };\n    return typeMap[webType] || webType;\n  }\n}\n```\n\n### 4. Generate Component Tests\n\n```typescript\nclass ComponentTestGenerator {\n  generateTests(spec: ComponentSpec): string {\n    return `\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { ${spec.name} } from './${spec.name}';\n\ndescribe('${spec.name}', () => {\n  const defaultProps = {\n${spec.props.filter(p => p.required).map(p => `    ${p.name}: ${this.getMockValue(p.type)},`).join('\\n')}\n  };\n\n  it('renders without crashing', () => {\n    render(<${spec.name} {...defaultProps} />);\n    expect(screen.getByRole('${this.inferAriaRole(spec.type)}')).toBeInTheDocument();\n  });\n\n  it('displays correct content', () => {\n    render(<${spec.name} {...defaultProps} />);\n    expect(screen.getByText(/content/i)).toBeVisible();\n  });\n\n${spec.props.filter(p => p.type.includes('()') || p.name.startsWith('on')).map(p => `\n  it('calls ${p.name} when triggered', () => {\n    const mock${this.capitalize(p.name)} = jest.fn();\n    render(<${spec.name} {...defaultProps} ${p.name}={mock${this.capitalize(p.name)}} />);\n\n    const trigger = screen.getByRole('button');\n    fireEvent.click(trigger);\n\n    expect(mock${this.capitalize(p.name)}).toHaveBeenCalledTimes(1);\n  });`).join('\\n')}\n\n  it('meets accessibility standards', async () => {\n    const { container } = render(<${spec.name} {...defaultProps} />);\n    const results = await axe(container);\n    expect(results).toHaveNoViolations();\n  });\n});\n`;\n  }\n\n  getMockValue(type: string): string {\n    if (type === 'string') return \"'test value'\";\n    if (type === 'number') return '42';\n    if (type === 'boolean') return 'true';\n    if (type.includes('[]')) return '[]';\n    if (type.includes('()')) return 'jest.fn()';\n    return '{}';\n  }\n}\n```\n\n### 5. Generate Styles\n\n```typescript\nclass StyleGenerator {\n  generateCSSModule(spec: ComponentSpec): string {\n    const className = this.camelCase(spec.name);\n    return `\n.${className} {\n  display: flex;\n  flex-direction: column;\n  padding: 1rem;\n  background-color: var(--bg-primary);\n}\n\n.${className}Title {\n  font-size: 1.5rem;\n  font-weight: 600;\n  color: var(--text-primary);\n  margin-bottom: 0.5rem;\n}\n\n.${className}Content {\n  flex: 1;\n  color: var(--text-secondary);\n}\n`;\n  }\n\n  generateStyledComponents(spec: ComponentSpec): string {\n    return `\nimport styled from 'styled-components';\n\nexport const ${spec.name}Container = styled.div\\`\n  display: flex;\n  flex-direction: column;\n  padding: \\${({ theme }) => theme.spacing.md};\n  background-color: \\${({ theme }) => theme.colors.background};\n\\`;\n\nexport const ${spec.name}Title = styled.h2\\`\n  font-size: \\${({ theme }) => theme.fontSize.lg};\n  font-weight: 600;\n  color: \\${({ theme }) => theme.colors.text.primary};\n  margin-bottom: \\${({ theme }) => theme.spacing.sm};\n\\`;\n`;\n  }\n\n  generateTailwind(spec: ComponentSpec): string {\n    return `\n// Use these Tailwind classes in your component:\n// Container: \"flex flex-col p-4 bg-white rounded-lg shadow\"\n// Title: \"text-xl font-semibold text-gray-900 mb-2\"\n// Content: \"flex-1 text-gray-700\"\n`;\n  }\n}\n```\n\n### 6. Generate Storybook Stories\n\n```typescript\nclass StorybookGenerator {\n  generateStories(spec: ComponentSpec): string {\n    return `\nimport type { Meta, StoryObj } from '@storybook/react';\nimport { ${spec.name} } from './${spec.name}';\n\nconst meta: Meta<typeof ${spec.name}> = {\n  title: 'Components/${spec.name}',\n  component: ${spec.name},\n  tags: ['autodocs'],\n  argTypes: {\n${spec.props.map(p => `    ${p.name}: { control: '${this.inferControl(p.type)}', description: '${p.description}' },`).join('\\n')}\n  },\n};\n\nexport default meta;\ntype Story = StoryObj<typeof ${spec.name}>;\n\nexport const Default: Story = {\n  args: {\n${spec.props.map(p => `    ${p.name}: ${p.defaultValue || this.getMockValue(p.type)},`).join('\\n')}\n  },\n};\n\nexport const Interactive: Story = {\n  args: {\n    ...Default.args,\n  },\n};\n`;\n  }\n\n  inferControl(type: string): string {\n    if (type === 'string') return 'text';\n    if (type === 'number') return 'number';\n    if (type === 'boolean') return 'boolean';\n    if (type.includes('[]')) return 'object';\n    return 'text';\n  }\n}\n```\n\n## Output Format\n\n1. **Component File**: Fully implemented React/React Native component\n2. **Type Definitions**: TypeScript interfaces and types\n3. **Styles**: CSS modules, styled-components, or Tailwind config\n4. **Tests**: Complete test suite with coverage\n5. **Stories**: Storybook stories for documentation\n6. **Index File**: Barrel exports for clean imports\n\nFocus on creating production-ready, accessible, and maintainable components that follow modern React patterns and best practices.\n"
              }
            ],
            "skills": [
              {
                "name": "nextjs-app-router-patterns",
                "description": "Master Next.js 14+ App Router with Server Components, streaming, parallel routes, and advanced data fetching. Use when building Next.js applications, implementing SSR/SSG, or optimizing React Server Components.",
                "path": "plugins/frontend-mobile-development/skills/nextjs-app-router-patterns/SKILL.md",
                "frontmatter": {
                  "name": "nextjs-app-router-patterns",
                  "description": "Master Next.js 14+ App Router with Server Components, streaming, parallel routes, and advanced data fetching. Use when building Next.js applications, implementing SSR/SSG, or optimizing React Server Components."
                },
                "content": "# Next.js App Router Patterns\n\nComprehensive patterns for Next.js 14+ App Router architecture, Server Components, and modern full-stack React development.\n\n## When to Use This Skill\n\n- Building new Next.js applications with App Router\n- Migrating from Pages Router to App Router\n- Implementing Server Components and streaming\n- Setting up parallel and intercepting routes\n- Optimizing data fetching and caching\n- Building full-stack features with Server Actions\n\n## Core Concepts\n\n### 1. Rendering Modes\n\n| Mode | Where | When to Use |\n|------|-------|-------------|\n| **Server Components** | Server only | Data fetching, heavy computation, secrets |\n| **Client Components** | Browser | Interactivity, hooks, browser APIs |\n| **Static** | Build time | Content that rarely changes |\n| **Dynamic** | Request time | Personalized or real-time data |\n| **Streaming** | Progressive | Large pages, slow data sources |\n\n### 2. File Conventions\n\n```\napp/\nâ”œâ”€â”€ layout.tsx       # Shared UI wrapper\nâ”œâ”€â”€ page.tsx         # Route UI\nâ”œâ”€â”€ loading.tsx      # Loading UI (Suspense)\nâ”œâ”€â”€ error.tsx        # Error boundary\nâ”œâ”€â”€ not-found.tsx    # 404 UI\nâ”œâ”€â”€ route.ts         # API endpoint\nâ”œâ”€â”€ template.tsx     # Re-mounted layout\nâ”œâ”€â”€ default.tsx      # Parallel route fallback\nâ””â”€â”€ opengraph-image.tsx  # OG image generation\n```\n\n## Quick Start\n\n```typescript\n// app/layout.tsx\nimport { Inter } from 'next/font/google'\nimport { Providers } from './providers'\n\nconst inter = Inter({ subsets: ['latin'] })\n\nexport const metadata = {\n  title: { default: 'My App', template: '%s | My App' },\n  description: 'Built with Next.js App Router',\n}\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <html lang=\"en\" suppressHydrationWarning>\n      <body className={inter.className}>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  )\n}\n\n// app/page.tsx - Server Component by default\nasync function getProducts() {\n  const res = await fetch('https://api.example.com/products', {\n    next: { revalidate: 3600 }, // ISR: revalidate every hour\n  })\n  return res.json()\n}\n\nexport default async function HomePage() {\n  const products = await getProducts()\n\n  return (\n    <main>\n      <h1>Products</h1>\n      <ProductGrid products={products} />\n    </main>\n  )\n}\n```\n\n## Patterns\n\n### Pattern 1: Server Components with Data Fetching\n\n```typescript\n// app/products/page.tsx\nimport { Suspense } from 'react'\nimport { ProductList, ProductListSkeleton } from '@/components/products'\nimport { FilterSidebar } from '@/components/filters'\n\ninterface SearchParams {\n  category?: string\n  sort?: 'price' | 'name' | 'date'\n  page?: string\n}\n\nexport default async function ProductsPage({\n  searchParams,\n}: {\n  searchParams: Promise<SearchParams>\n}) {\n  const params = await searchParams\n\n  return (\n    <div className=\"flex gap-8\">\n      <FilterSidebar />\n      <Suspense\n        key={JSON.stringify(params)}\n        fallback={<ProductListSkeleton />}\n      >\n        <ProductList\n          category={params.category}\n          sort={params.sort}\n          page={Number(params.page) || 1}\n        />\n      </Suspense>\n    </div>\n  )\n}\n\n// components/products/ProductList.tsx - Server Component\nasync function getProducts(filters: ProductFilters) {\n  const res = await fetch(\n    `${process.env.API_URL}/products?${new URLSearchParams(filters)}`,\n    { next: { tags: ['products'] } }\n  )\n  if (!res.ok) throw new Error('Failed to fetch products')\n  return res.json()\n}\n\nexport async function ProductList({ category, sort, page }: ProductFilters) {\n  const { products, totalPages } = await getProducts({ category, sort, page })\n\n  return (\n    <div>\n      <div className=\"grid grid-cols-3 gap-4\">\n        {products.map((product) => (\n          <ProductCard key={product.id} product={product} />\n        ))}\n      </div>\n      <Pagination currentPage={page} totalPages={totalPages} />\n    </div>\n  )\n}\n```\n\n### Pattern 2: Client Components with 'use client'\n\n```typescript\n// components/products/AddToCartButton.tsx\n'use client'\n\nimport { useState, useTransition } from 'react'\nimport { addToCart } from '@/app/actions/cart'\n\nexport function AddToCartButton({ productId }: { productId: string }) {\n  const [isPending, startTransition] = useTransition()\n  const [error, setError] = useState<string | null>(null)\n\n  const handleClick = () => {\n    setError(null)\n    startTransition(async () => {\n      const result = await addToCart(productId)\n      if (result.error) {\n        setError(result.error)\n      }\n    })\n  }\n\n  return (\n    <div>\n      <button\n        onClick={handleClick}\n        disabled={isPending}\n        className=\"btn-primary\"\n      >\n        {isPending ? 'Adding...' : 'Add to Cart'}\n      </button>\n      {error && <p className=\"text-red-500 text-sm\">{error}</p>}\n    </div>\n  )\n}\n```\n\n### Pattern 3: Server Actions\n\n```typescript\n// app/actions/cart.ts\n'use server'\n\nimport { revalidateTag } from 'next/cache'\nimport { cookies } from 'next/headers'\nimport { redirect } from 'next/navigation'\n\nexport async function addToCart(productId: string) {\n  const cookieStore = await cookies()\n  const sessionId = cookieStore.get('session')?.value\n\n  if (!sessionId) {\n    redirect('/login')\n  }\n\n  try {\n    await db.cart.upsert({\n      where: { sessionId_productId: { sessionId, productId } },\n      update: { quantity: { increment: 1 } },\n      create: { sessionId, productId, quantity: 1 },\n    })\n\n    revalidateTag('cart')\n    return { success: true }\n  } catch (error) {\n    return { error: 'Failed to add item to cart' }\n  }\n}\n\nexport async function checkout(formData: FormData) {\n  const address = formData.get('address') as string\n  const payment = formData.get('payment') as string\n\n  // Validate\n  if (!address || !payment) {\n    return { error: 'Missing required fields' }\n  }\n\n  // Process order\n  const order = await processOrder({ address, payment })\n\n  // Redirect to confirmation\n  redirect(`/orders/${order.id}/confirmation`)\n}\n```\n\n### Pattern 4: Parallel Routes\n\n```typescript\n// app/dashboard/layout.tsx\nexport default function DashboardLayout({\n  children,\n  analytics,\n  team,\n}: {\n  children: React.ReactNode\n  analytics: React.ReactNode\n  team: React.ReactNode\n}) {\n  return (\n    <div className=\"dashboard-grid\">\n      <main>{children}</main>\n      <aside className=\"analytics-panel\">{analytics}</aside>\n      <aside className=\"team-panel\">{team}</aside>\n    </div>\n  )\n}\n\n// app/dashboard/@analytics/page.tsx\nexport default async function AnalyticsSlot() {\n  const stats = await getAnalytics()\n  return <AnalyticsChart data={stats} />\n}\n\n// app/dashboard/@analytics/loading.tsx\nexport default function AnalyticsLoading() {\n  return <ChartSkeleton />\n}\n\n// app/dashboard/@team/page.tsx\nexport default async function TeamSlot() {\n  const members = await getTeamMembers()\n  return <TeamList members={members} />\n}\n```\n\n### Pattern 5: Intercepting Routes (Modal Pattern)\n\n```typescript\n// File structure for photo modal\n// app/\n// â”œâ”€â”€ @modal/\n// â”‚   â”œâ”€â”€ (.)photos/[id]/page.tsx  # Intercept\n// â”‚   â””â”€â”€ default.tsx\n// â”œâ”€â”€ photos/\n// â”‚   â””â”€â”€ [id]/page.tsx            # Full page\n// â””â”€â”€ layout.tsx\n\n// app/@modal/(.)photos/[id]/page.tsx\nimport { Modal } from '@/components/Modal'\nimport { PhotoDetail } from '@/components/PhotoDetail'\n\nexport default async function PhotoModal({\n  params,\n}: {\n  params: Promise<{ id: string }>\n}) {\n  const { id } = await params\n  const photo = await getPhoto(id)\n\n  return (\n    <Modal>\n      <PhotoDetail photo={photo} />\n    </Modal>\n  )\n}\n\n// app/photos/[id]/page.tsx - Full page version\nexport default async function PhotoPage({\n  params,\n}: {\n  params: Promise<{ id: string }>\n}) {\n  const { id } = await params\n  const photo = await getPhoto(id)\n\n  return (\n    <div className=\"photo-page\">\n      <PhotoDetail photo={photo} />\n      <RelatedPhotos photoId={id} />\n    </div>\n  )\n}\n\n// app/layout.tsx\nexport default function RootLayout({\n  children,\n  modal,\n}: {\n  children: React.ReactNode\n  modal: React.ReactNode\n}) {\n  return (\n    <html>\n      <body>\n        {children}\n        {modal}\n      </body>\n    </html>\n  )\n}\n```\n\n### Pattern 6: Streaming with Suspense\n\n```typescript\n// app/product/[id]/page.tsx\nimport { Suspense } from 'react'\n\nexport default async function ProductPage({\n  params,\n}: {\n  params: Promise<{ id: string }>\n}) {\n  const { id } = await params\n\n  // This data loads first (blocking)\n  const product = await getProduct(id)\n\n  return (\n    <div>\n      {/* Immediate render */}\n      <ProductHeader product={product} />\n\n      {/* Stream in reviews */}\n      <Suspense fallback={<ReviewsSkeleton />}>\n        <Reviews productId={id} />\n      </Suspense>\n\n      {/* Stream in recommendations */}\n      <Suspense fallback={<RecommendationsSkeleton />}>\n        <Recommendations productId={id} />\n      </Suspense>\n    </div>\n  )\n}\n\n// These components fetch their own data\nasync function Reviews({ productId }: { productId: string }) {\n  const reviews = await getReviews(productId) // Slow API\n  return <ReviewList reviews={reviews} />\n}\n\nasync function Recommendations({ productId }: { productId: string }) {\n  const products = await getRecommendations(productId) // ML-based, slow\n  return <ProductCarousel products={products} />\n}\n```\n\n### Pattern 7: Route Handlers (API Routes)\n\n```typescript\n// app/api/products/route.ts\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  const searchParams = request.nextUrl.searchParams\n  const category = searchParams.get('category')\n\n  const products = await db.product.findMany({\n    where: category ? { category } : undefined,\n    take: 20,\n  })\n\n  return NextResponse.json(products)\n}\n\nexport async function POST(request: NextRequest) {\n  const body = await request.json()\n\n  const product = await db.product.create({\n    data: body,\n  })\n\n  return NextResponse.json(product, { status: 201 })\n}\n\n// app/api/products/[id]/route.ts\nexport async function GET(\n  request: NextRequest,\n  { params }: { params: Promise<{ id: string }> }\n) {\n  const { id } = await params\n  const product = await db.product.findUnique({ where: { id } })\n\n  if (!product) {\n    return NextResponse.json(\n      { error: 'Product not found' },\n      { status: 404 }\n    )\n  }\n\n  return NextResponse.json(product)\n}\n```\n\n### Pattern 8: Metadata and SEO\n\n```typescript\n// app/products/[slug]/page.tsx\nimport { Metadata } from 'next'\nimport { notFound } from 'next/navigation'\n\ntype Props = {\n  params: Promise<{ slug: string }>\n}\n\nexport async function generateMetadata({ params }: Props): Promise<Metadata> {\n  const { slug } = await params\n  const product = await getProduct(slug)\n\n  if (!product) return {}\n\n  return {\n    title: product.name,\n    description: product.description,\n    openGraph: {\n      title: product.name,\n      description: product.description,\n      images: [{ url: product.image, width: 1200, height: 630 }],\n    },\n    twitter: {\n      card: 'summary_large_image',\n      title: product.name,\n      description: product.description,\n      images: [product.image],\n    },\n  }\n}\n\nexport async function generateStaticParams() {\n  const products = await db.product.findMany({ select: { slug: true } })\n  return products.map((p) => ({ slug: p.slug }))\n}\n\nexport default async function ProductPage({ params }: Props) {\n  const { slug } = await params\n  const product = await getProduct(slug)\n\n  if (!product) notFound()\n\n  return <ProductDetail product={product} />\n}\n```\n\n## Caching Strategies\n\n### Data Cache\n\n```typescript\n// No cache (always fresh)\nfetch(url, { cache: 'no-store' })\n\n// Cache forever (static)\nfetch(url, { cache: 'force-cache' })\n\n// ISR - revalidate after 60 seconds\nfetch(url, { next: { revalidate: 60 } })\n\n// Tag-based invalidation\nfetch(url, { next: { tags: ['products'] } })\n\n// Invalidate via Server Action\n'use server'\nimport { revalidateTag, revalidatePath } from 'next/cache'\n\nexport async function updateProduct(id: string, data: ProductData) {\n  await db.product.update({ where: { id }, data })\n  revalidateTag('products')\n  revalidatePath('/products')\n}\n```\n\n## Best Practices\n\n### Do's\n- **Start with Server Components** - Add 'use client' only when needed\n- **Colocate data fetching** - Fetch data where it's used\n- **Use Suspense boundaries** - Enable streaming for slow data\n- **Leverage parallel routes** - Independent loading states\n- **Use Server Actions** - For mutations with progressive enhancement\n\n### Don'ts\n- **Don't pass serializable data** - Server â†’ Client boundary limitations\n- **Don't use hooks in Server Components** - No useState, useEffect\n- **Don't fetch in Client Components** - Use Server Components or React Query\n- **Don't over-nest layouts** - Each layout adds to the component tree\n- **Don't ignore loading states** - Always provide loading.tsx or Suspense\n\n## Resources\n\n- [Next.js App Router Documentation](https://nextjs.org/docs/app)\n- [Server Components RFC](https://github.com/reactjs/rfcs/blob/main/text/0188-server-components.md)\n- [Vercel Templates](https://vercel.com/templates/next.js)"
              },
              {
                "name": "react-native-architecture",
                "description": "Build production React Native apps with Expo, navigation, native modules, offline sync, and cross-platform patterns. Use when developing mobile apps, implementing native integrations, or architecting React Native projects.",
                "path": "plugins/frontend-mobile-development/skills/react-native-architecture/SKILL.md",
                "frontmatter": {
                  "name": "react-native-architecture",
                  "description": "Build production React Native apps with Expo, navigation, native modules, offline sync, and cross-platform patterns. Use when developing mobile apps, implementing native integrations, or architecting React Native projects."
                },
                "content": "# React Native Architecture\n\nProduction-ready patterns for React Native development with Expo, including navigation, state management, native modules, and offline-first architecture.\n\n## When to Use This Skill\n\n- Starting a new React Native or Expo project\n- Implementing complex navigation patterns\n- Integrating native modules and platform APIs\n- Building offline-first mobile applications\n- Optimizing React Native performance\n- Setting up CI/CD for mobile releases\n\n## Core Concepts\n\n### 1. Project Structure\n\n```\nsrc/\nâ”œâ”€â”€ app/                    # Expo Router screens\nâ”‚   â”œâ”€â”€ (auth)/            # Auth group\nâ”‚   â”œâ”€â”€ (tabs)/            # Tab navigation\nâ”‚   â””â”€â”€ _layout.tsx        # Root layout\nâ”œâ”€â”€ components/\nâ”‚   â”œâ”€â”€ ui/                # Reusable UI components\nâ”‚   â””â”€â”€ features/          # Feature-specific components\nâ”œâ”€â”€ hooks/                 # Custom hooks\nâ”œâ”€â”€ services/              # API and native services\nâ”œâ”€â”€ stores/                # State management\nâ”œâ”€â”€ utils/                 # Utilities\nâ””â”€â”€ types/                 # TypeScript types\n```\n\n### 2. Expo vs Bare React Native\n\n| Feature | Expo | Bare RN |\n|---------|------|---------|\n| Setup complexity | Low | High |\n| Native modules | EAS Build | Manual linking |\n| OTA updates | Built-in | Manual setup |\n| Build service | EAS | Custom CI |\n| Custom native code | Config plugins | Direct access |\n\n## Quick Start\n\n```bash\n# Create new Expo project\nnpx create-expo-app@latest my-app -t expo-template-blank-typescript\n\n# Install essential dependencies\nnpx expo install expo-router expo-status-bar react-native-safe-area-context\nnpx expo install @react-native-async-storage/async-storage\nnpx expo install expo-secure-store expo-haptics\n```\n\n```typescript\n// app/_layout.tsx\nimport { Stack } from 'expo-router'\nimport { ThemeProvider } from '@/providers/ThemeProvider'\nimport { QueryProvider } from '@/providers/QueryProvider'\n\nexport default function RootLayout() {\n  return (\n    <QueryProvider>\n      <ThemeProvider>\n        <Stack screenOptions={{ headerShown: false }}>\n          <Stack.Screen name=\"(tabs)\" />\n          <Stack.Screen name=\"(auth)\" />\n          <Stack.Screen name=\"modal\" options={{ presentation: 'modal' }} />\n        </Stack>\n      </ThemeProvider>\n    </QueryProvider>\n  )\n}\n```\n\n## Patterns\n\n### Pattern 1: Expo Router Navigation\n\n```typescript\n// app/(tabs)/_layout.tsx\nimport { Tabs } from 'expo-router'\nimport { Home, Search, User, Settings } from 'lucide-react-native'\nimport { useTheme } from '@/hooks/useTheme'\n\nexport default function TabLayout() {\n  const { colors } = useTheme()\n\n  return (\n    <Tabs\n      screenOptions={{\n        tabBarActiveTintColor: colors.primary,\n        tabBarInactiveTintColor: colors.textMuted,\n        tabBarStyle: { backgroundColor: colors.background },\n        headerShown: false,\n      }}\n    >\n      <Tabs.Screen\n        name=\"index\"\n        options={{\n          title: 'Home',\n          tabBarIcon: ({ color, size }) => <Home size={size} color={color} />,\n        }}\n      />\n      <Tabs.Screen\n        name=\"search\"\n        options={{\n          title: 'Search',\n          tabBarIcon: ({ color, size }) => <Search size={size} color={color} />,\n        }}\n      />\n      <Tabs.Screen\n        name=\"profile\"\n        options={{\n          title: 'Profile',\n          tabBarIcon: ({ color, size }) => <User size={size} color={color} />,\n        }}\n      />\n      <Tabs.Screen\n        name=\"settings\"\n        options={{\n          title: 'Settings',\n          tabBarIcon: ({ color, size }) => <Settings size={size} color={color} />,\n        }}\n      />\n    </Tabs>\n  )\n}\n\n// app/(tabs)/profile/[id].tsx - Dynamic route\nimport { useLocalSearchParams } from 'expo-router'\n\nexport default function ProfileScreen() {\n  const { id } = useLocalSearchParams<{ id: string }>()\n\n  return <UserProfile userId={id} />\n}\n\n// Navigation from anywhere\nimport { router } from 'expo-router'\n\n// Programmatic navigation\nrouter.push('/profile/123')\nrouter.replace('/login')\nrouter.back()\n\n// With params\nrouter.push({\n  pathname: '/product/[id]',\n  params: { id: '123', referrer: 'home' },\n})\n```\n\n### Pattern 2: Authentication Flow\n\n```typescript\n// providers/AuthProvider.tsx\nimport { createContext, useContext, useEffect, useState } from 'react'\nimport { useRouter, useSegments } from 'expo-router'\nimport * as SecureStore from 'expo-secure-store'\n\ninterface AuthContextType {\n  user: User | null\n  isLoading: boolean\n  signIn: (credentials: Credentials) => Promise<void>\n  signOut: () => Promise<void>\n}\n\nconst AuthContext = createContext<AuthContextType | null>(null)\n\nexport function AuthProvider({ children }: { children: React.ReactNode }) {\n  const [user, setUser] = useState<User | null>(null)\n  const [isLoading, setIsLoading] = useState(true)\n  const segments = useSegments()\n  const router = useRouter()\n\n  // Check authentication on mount\n  useEffect(() => {\n    checkAuth()\n  }, [])\n\n  // Protect routes\n  useEffect(() => {\n    if (isLoading) return\n\n    const inAuthGroup = segments[0] === '(auth)'\n\n    if (!user && !inAuthGroup) {\n      router.replace('/login')\n    } else if (user && inAuthGroup) {\n      router.replace('/(tabs)')\n    }\n  }, [user, segments, isLoading])\n\n  async function checkAuth() {\n    try {\n      const token = await SecureStore.getItemAsync('authToken')\n      if (token) {\n        const userData = await api.getUser(token)\n        setUser(userData)\n      }\n    } catch (error) {\n      await SecureStore.deleteItemAsync('authToken')\n    } finally {\n      setIsLoading(false)\n    }\n  }\n\n  async function signIn(credentials: Credentials) {\n    const { token, user } = await api.login(credentials)\n    await SecureStore.setItemAsync('authToken', token)\n    setUser(user)\n  }\n\n  async function signOut() {\n    await SecureStore.deleteItemAsync('authToken')\n    setUser(null)\n  }\n\n  if (isLoading) {\n    return <SplashScreen />\n  }\n\n  return (\n    <AuthContext.Provider value={{ user, isLoading, signIn, signOut }}>\n      {children}\n    </AuthContext.Provider>\n  )\n}\n\nexport const useAuth = () => {\n  const context = useContext(AuthContext)\n  if (!context) throw new Error('useAuth must be used within AuthProvider')\n  return context\n}\n```\n\n### Pattern 3: Offline-First with React Query\n\n```typescript\n// providers/QueryProvider.tsx\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query'\nimport { createAsyncStoragePersister } from '@tanstack/query-async-storage-persister'\nimport { PersistQueryClientProvider } from '@tanstack/react-query-persist-client'\nimport AsyncStorage from '@react-native-async-storage/async-storage'\nimport NetInfo from '@react-native-community/netinfo'\nimport { onlineManager } from '@tanstack/react-query'\n\n// Sync online status\nonlineManager.setEventListener((setOnline) => {\n  return NetInfo.addEventListener((state) => {\n    setOnline(!!state.isConnected)\n  })\n})\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      gcTime: 1000 * 60 * 60 * 24, // 24 hours\n      staleTime: 1000 * 60 * 5, // 5 minutes\n      retry: 2,\n      networkMode: 'offlineFirst',\n    },\n    mutations: {\n      networkMode: 'offlineFirst',\n    },\n  },\n})\n\nconst asyncStoragePersister = createAsyncStoragePersister({\n  storage: AsyncStorage,\n  key: 'REACT_QUERY_OFFLINE_CACHE',\n})\n\nexport function QueryProvider({ children }: { children: React.ReactNode }) {\n  return (\n    <PersistQueryClientProvider\n      client={queryClient}\n      persistOptions={{ persister: asyncStoragePersister }}\n    >\n      {children}\n    </PersistQueryClientProvider>\n  )\n}\n\n// hooks/useProducts.ts\nimport { useQuery, useMutation, useQueryClient } from '@tanstack/react-query'\n\nexport function useProducts() {\n  return useQuery({\n    queryKey: ['products'],\n    queryFn: api.getProducts,\n    // Use stale data while revalidating\n    placeholderData: (previousData) => previousData,\n  })\n}\n\nexport function useCreateProduct() {\n  const queryClient = useQueryClient()\n\n  return useMutation({\n    mutationFn: api.createProduct,\n    // Optimistic update\n    onMutate: async (newProduct) => {\n      await queryClient.cancelQueries({ queryKey: ['products'] })\n      const previous = queryClient.getQueryData(['products'])\n\n      queryClient.setQueryData(['products'], (old: Product[]) => [\n        ...old,\n        { ...newProduct, id: 'temp-' + Date.now() },\n      ])\n\n      return { previous }\n    },\n    onError: (err, newProduct, context) => {\n      queryClient.setQueryData(['products'], context?.previous)\n    },\n    onSettled: () => {\n      queryClient.invalidateQueries({ queryKey: ['products'] })\n    },\n  })\n}\n```\n\n### Pattern 4: Native Module Integration\n\n```typescript\n// services/haptics.ts\nimport * as Haptics from 'expo-haptics'\nimport { Platform } from 'react-native'\n\nexport const haptics = {\n  light: () => {\n    if (Platform.OS !== 'web') {\n      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light)\n    }\n  },\n  medium: () => {\n    if (Platform.OS !== 'web') {\n      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Medium)\n    }\n  },\n  heavy: () => {\n    if (Platform.OS !== 'web') {\n      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Heavy)\n    }\n  },\n  success: () => {\n    if (Platform.OS !== 'web') {\n      Haptics.notificationAsync(Haptics.NotificationFeedbackType.Success)\n    }\n  },\n  error: () => {\n    if (Platform.OS !== 'web') {\n      Haptics.notificationAsync(Haptics.NotificationFeedbackType.Error)\n    }\n  },\n}\n\n// services/biometrics.ts\nimport * as LocalAuthentication from 'expo-local-authentication'\n\nexport async function authenticateWithBiometrics(): Promise<boolean> {\n  const hasHardware = await LocalAuthentication.hasHardwareAsync()\n  if (!hasHardware) return false\n\n  const isEnrolled = await LocalAuthentication.isEnrolledAsync()\n  if (!isEnrolled) return false\n\n  const result = await LocalAuthentication.authenticateAsync({\n    promptMessage: 'Authenticate to continue',\n    fallbackLabel: 'Use passcode',\n    disableDeviceFallback: false,\n  })\n\n  return result.success\n}\n\n// services/notifications.ts\nimport * as Notifications from 'expo-notifications'\nimport { Platform } from 'react-native'\nimport Constants from 'expo-constants'\n\nNotifications.setNotificationHandler({\n  handleNotification: async () => ({\n    shouldShowAlert: true,\n    shouldPlaySound: true,\n    shouldSetBadge: true,\n  }),\n})\n\nexport async function registerForPushNotifications() {\n  let token: string | undefined\n\n  if (Platform.OS === 'android') {\n    await Notifications.setNotificationChannelAsync('default', {\n      name: 'default',\n      importance: Notifications.AndroidImportance.MAX,\n      vibrationPattern: [0, 250, 250, 250],\n    })\n  }\n\n  const { status: existingStatus } = await Notifications.getPermissionsAsync()\n  let finalStatus = existingStatus\n\n  if (existingStatus !== 'granted') {\n    const { status } = await Notifications.requestPermissionsAsync()\n    finalStatus = status\n  }\n\n  if (finalStatus !== 'granted') {\n    return null\n  }\n\n  const projectId = Constants.expoConfig?.extra?.eas?.projectId\n  token = (await Notifications.getExpoPushTokenAsync({ projectId })).data\n\n  return token\n}\n```\n\n### Pattern 5: Platform-Specific Code\n\n```typescript\n// components/ui/Button.tsx\nimport { Platform, Pressable, StyleSheet, Text, ViewStyle } from 'react-native'\nimport * as Haptics from 'expo-haptics'\nimport Animated, {\n  useAnimatedStyle,\n  useSharedValue,\n  withSpring,\n} from 'react-native-reanimated'\n\nconst AnimatedPressable = Animated.createAnimatedComponent(Pressable)\n\ninterface ButtonProps {\n  title: string\n  onPress: () => void\n  variant?: 'primary' | 'secondary' | 'outline'\n  disabled?: boolean\n}\n\nexport function Button({\n  title,\n  onPress,\n  variant = 'primary',\n  disabled = false,\n}: ButtonProps) {\n  const scale = useSharedValue(1)\n\n  const animatedStyle = useAnimatedStyle(() => ({\n    transform: [{ scale: scale.value }],\n  }))\n\n  const handlePressIn = () => {\n    scale.value = withSpring(0.95)\n    if (Platform.OS !== 'web') {\n      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light)\n    }\n  }\n\n  const handlePressOut = () => {\n    scale.value = withSpring(1)\n  }\n\n  return (\n    <AnimatedPressable\n      onPress={onPress}\n      onPressIn={handlePressIn}\n      onPressOut={handlePressOut}\n      disabled={disabled}\n      style={[\n        styles.button,\n        styles[variant],\n        disabled && styles.disabled,\n        animatedStyle,\n      ]}\n    >\n      <Text style={[styles.text, styles[`${variant}Text`]]}>{title}</Text>\n    </AnimatedPressable>\n  )\n}\n\n// Platform-specific files\n// Button.ios.tsx - iOS-specific implementation\n// Button.android.tsx - Android-specific implementation\n// Button.web.tsx - Web-specific implementation\n\n// Or use Platform.select\nconst styles = StyleSheet.create({\n  button: {\n    paddingVertical: 12,\n    paddingHorizontal: 24,\n    borderRadius: 8,\n    alignItems: 'center',\n    ...Platform.select({\n      ios: {\n        shadowColor: '#000',\n        shadowOffset: { width: 0, height: 2 },\n        shadowOpacity: 0.1,\n        shadowRadius: 4,\n      },\n      android: {\n        elevation: 4,\n      },\n    }),\n  },\n  primary: {\n    backgroundColor: '#007AFF',\n  },\n  secondary: {\n    backgroundColor: '#5856D6',\n  },\n  outline: {\n    backgroundColor: 'transparent',\n    borderWidth: 1,\n    borderColor: '#007AFF',\n  },\n  disabled: {\n    opacity: 0.5,\n  },\n  text: {\n    fontSize: 16,\n    fontWeight: '600',\n  },\n  primaryText: {\n    color: '#FFFFFF',\n  },\n  secondaryText: {\n    color: '#FFFFFF',\n  },\n  outlineText: {\n    color: '#007AFF',\n  },\n})\n```\n\n### Pattern 6: Performance Optimization\n\n```typescript\n// components/ProductList.tsx\nimport { FlashList } from '@shopify/flash-list'\nimport { memo, useCallback } from 'react'\n\ninterface ProductListProps {\n  products: Product[]\n  onProductPress: (id: string) => void\n}\n\n// Memoize list item\nconst ProductItem = memo(function ProductItem({\n  item,\n  onPress,\n}: {\n  item: Product\n  onPress: (id: string) => void\n}) {\n  const handlePress = useCallback(() => onPress(item.id), [item.id, onPress])\n\n  return (\n    <Pressable onPress={handlePress} style={styles.item}>\n      <FastImage\n        source={{ uri: item.image }}\n        style={styles.image}\n        resizeMode=\"cover\"\n      />\n      <Text style={styles.title}>{item.name}</Text>\n      <Text style={styles.price}>${item.price}</Text>\n    </Pressable>\n  )\n})\n\nexport function ProductList({ products, onProductPress }: ProductListProps) {\n  const renderItem = useCallback(\n    ({ item }: { item: Product }) => (\n      <ProductItem item={item} onPress={onProductPress} />\n    ),\n    [onProductPress]\n  )\n\n  const keyExtractor = useCallback((item: Product) => item.id, [])\n\n  return (\n    <FlashList\n      data={products}\n      renderItem={renderItem}\n      keyExtractor={keyExtractor}\n      estimatedItemSize={100}\n      // Performance optimizations\n      removeClippedSubviews={true}\n      maxToRenderPerBatch={10}\n      windowSize={5}\n      // Pull to refresh\n      onRefresh={onRefresh}\n      refreshing={isRefreshing}\n    />\n  )\n}\n```\n\n## EAS Build & Submit\n\n```json\n// eas.json\n{\n  \"cli\": { \"version\": \">= 5.0.0\" },\n  \"build\": {\n    \"development\": {\n      \"developmentClient\": true,\n      \"distribution\": \"internal\",\n      \"ios\": { \"simulator\": true }\n    },\n    \"preview\": {\n      \"distribution\": \"internal\",\n      \"android\": { \"buildType\": \"apk\" }\n    },\n    \"production\": {\n      \"autoIncrement\": true\n    }\n  },\n  \"submit\": {\n    \"production\": {\n      \"ios\": { \"appleId\": \"your@email.com\", \"ascAppId\": \"123456789\" },\n      \"android\": { \"serviceAccountKeyPath\": \"./google-services.json\" }\n    }\n  }\n}\n```\n\n```bash\n# Build commands\neas build --platform ios --profile development\neas build --platform android --profile preview\neas build --platform all --profile production\n\n# Submit to stores\neas submit --platform ios\neas submit --platform android\n\n# OTA updates\neas update --branch production --message \"Bug fixes\"\n```\n\n## Best Practices\n\n### Do's\n- **Use Expo** - Faster development, OTA updates, managed native code\n- **FlashList over FlatList** - Better performance for long lists\n- **Memoize components** - Prevent unnecessary re-renders\n- **Use Reanimated** - 60fps animations on native thread\n- **Test on real devices** - Simulators miss real-world issues\n\n### Don'ts\n- **Don't inline styles** - Use StyleSheet.create for performance\n- **Don't fetch in render** - Use useEffect or React Query\n- **Don't ignore platform differences** - Test on both iOS and Android\n- **Don't store secrets in code** - Use environment variables\n- **Don't skip error boundaries** - Mobile crashes are unforgiving\n\n## Resources\n\n- [Expo Documentation](https://docs.expo.dev/)\n- [Expo Router](https://docs.expo.dev/router/introduction/)\n- [React Native Performance](https://reactnative.dev/docs/performance)\n- [FlashList](https://shopify.github.io/flash-list/)"
              },
              {
                "name": "react-state-management",
                "description": "Master modern React state management with Redux Toolkit, Zustand, Jotai, and React Query. Use when setting up global state, managing server state, or choosing between state management solutions.",
                "path": "plugins/frontend-mobile-development/skills/react-state-management/SKILL.md",
                "frontmatter": {
                  "name": "react-state-management",
                  "description": "Master modern React state management with Redux Toolkit, Zustand, Jotai, and React Query. Use when setting up global state, managing server state, or choosing between state management solutions."
                },
                "content": "# React State Management\n\nComprehensive guide to modern React state management patterns, from local component state to global stores and server state synchronization.\n\n## When to Use This Skill\n\n- Setting up global state management in a React app\n- Choosing between Redux Toolkit, Zustand, or Jotai\n- Managing server state with React Query or SWR\n- Implementing optimistic updates\n- Debugging state-related issues\n- Migrating from legacy Redux to modern patterns\n\n## Core Concepts\n\n### 1. State Categories\n\n| Type | Description | Solutions |\n|------|-------------|-----------|\n| **Local State** | Component-specific, UI state | useState, useReducer |\n| **Global State** | Shared across components | Redux Toolkit, Zustand, Jotai |\n| **Server State** | Remote data, caching | React Query, SWR, RTK Query |\n| **URL State** | Route parameters, search | React Router, nuqs |\n| **Form State** | Input values, validation | React Hook Form, Formik |\n\n### 2. Selection Criteria\n\n```\nSmall app, simple state â†’ Zustand or Jotai\nLarge app, complex state â†’ Redux Toolkit\nHeavy server interaction â†’ React Query + light client state\nAtomic/granular updates â†’ Jotai\n```\n\n## Quick Start\n\n### Zustand (Simplest)\n\n```typescript\n// store/useStore.ts\nimport { create } from 'zustand'\nimport { devtools, persist } from 'zustand/middleware'\n\ninterface AppState {\n  user: User | null\n  theme: 'light' | 'dark'\n  setUser: (user: User | null) => void\n  toggleTheme: () => void\n}\n\nexport const useStore = create<AppState>()(\n  devtools(\n    persist(\n      (set) => ({\n        user: null,\n        theme: 'light',\n        setUser: (user) => set({ user }),\n        toggleTheme: () => set((state) => ({\n          theme: state.theme === 'light' ? 'dark' : 'light'\n        })),\n      }),\n      { name: 'app-storage' }\n    )\n  )\n)\n\n// Usage in component\nfunction Header() {\n  const { user, theme, toggleTheme } = useStore()\n  return (\n    <header className={theme}>\n      {user?.name}\n      <button onClick={toggleTheme}>Toggle Theme</button>\n    </header>\n  )\n}\n```\n\n## Patterns\n\n### Pattern 1: Redux Toolkit with TypeScript\n\n```typescript\n// store/index.ts\nimport { configureStore } from '@reduxjs/toolkit'\nimport { TypedUseSelectorHook, useDispatch, useSelector } from 'react-redux'\nimport userReducer from './slices/userSlice'\nimport cartReducer from './slices/cartSlice'\n\nexport const store = configureStore({\n  reducer: {\n    user: userReducer,\n    cart: cartReducer,\n  },\n  middleware: (getDefaultMiddleware) =>\n    getDefaultMiddleware({\n      serializableCheck: {\n        ignoredActions: ['persist/PERSIST'],\n      },\n    }),\n})\n\nexport type RootState = ReturnType<typeof store.getState>\nexport type AppDispatch = typeof store.dispatch\n\n// Typed hooks\nexport const useAppDispatch: () => AppDispatch = useDispatch\nexport const useAppSelector: TypedUseSelectorHook<RootState> = useSelector\n```\n\n```typescript\n// store/slices/userSlice.ts\nimport { createSlice, createAsyncThunk, PayloadAction } from '@reduxjs/toolkit'\n\ninterface User {\n  id: string\n  email: string\n  name: string\n}\n\ninterface UserState {\n  current: User | null\n  status: 'idle' | 'loading' | 'succeeded' | 'failed'\n  error: string | null\n}\n\nconst initialState: UserState = {\n  current: null,\n  status: 'idle',\n  error: null,\n}\n\nexport const fetchUser = createAsyncThunk(\n  'user/fetchUser',\n  async (userId: string, { rejectWithValue }) => {\n    try {\n      const response = await fetch(`/api/users/${userId}`)\n      if (!response.ok) throw new Error('Failed to fetch user')\n      return await response.json()\n    } catch (error) {\n      return rejectWithValue((error as Error).message)\n    }\n  }\n)\n\nconst userSlice = createSlice({\n  name: 'user',\n  initialState,\n  reducers: {\n    setUser: (state, action: PayloadAction<User>) => {\n      state.current = action.payload\n      state.status = 'succeeded'\n    },\n    clearUser: (state) => {\n      state.current = null\n      state.status = 'idle'\n    },\n  },\n  extraReducers: (builder) => {\n    builder\n      .addCase(fetchUser.pending, (state) => {\n        state.status = 'loading'\n        state.error = null\n      })\n      .addCase(fetchUser.fulfilled, (state, action) => {\n        state.status = 'succeeded'\n        state.current = action.payload\n      })\n      .addCase(fetchUser.rejected, (state, action) => {\n        state.status = 'failed'\n        state.error = action.payload as string\n      })\n  },\n})\n\nexport const { setUser, clearUser } = userSlice.actions\nexport default userSlice.reducer\n```\n\n### Pattern 2: Zustand with Slices (Scalable)\n\n```typescript\n// store/slices/createUserSlice.ts\nimport { StateCreator } from 'zustand'\n\nexport interface UserSlice {\n  user: User | null\n  isAuthenticated: boolean\n  login: (credentials: Credentials) => Promise<void>\n  logout: () => void\n}\n\nexport const createUserSlice: StateCreator<\n  UserSlice & CartSlice, // Combined store type\n  [],\n  [],\n  UserSlice\n> = (set, get) => ({\n  user: null,\n  isAuthenticated: false,\n  login: async (credentials) => {\n    const user = await authApi.login(credentials)\n    set({ user, isAuthenticated: true })\n  },\n  logout: () => {\n    set({ user: null, isAuthenticated: false })\n    // Can access other slices\n    // get().clearCart()\n  },\n})\n\n// store/index.ts\nimport { create } from 'zustand'\nimport { createUserSlice, UserSlice } from './slices/createUserSlice'\nimport { createCartSlice, CartSlice } from './slices/createCartSlice'\n\ntype StoreState = UserSlice & CartSlice\n\nexport const useStore = create<StoreState>()((...args) => ({\n  ...createUserSlice(...args),\n  ...createCartSlice(...args),\n}))\n\n// Selective subscriptions (prevents unnecessary re-renders)\nexport const useUser = () => useStore((state) => state.user)\nexport const useCart = () => useStore((state) => state.cart)\n```\n\n### Pattern 3: Jotai for Atomic State\n\n```typescript\n// atoms/userAtoms.ts\nimport { atom } from 'jotai'\nimport { atomWithStorage } from 'jotai/utils'\n\n// Basic atom\nexport const userAtom = atom<User | null>(null)\n\n// Derived atom (computed)\nexport const isAuthenticatedAtom = atom((get) => get(userAtom) !== null)\n\n// Atom with localStorage persistence\nexport const themeAtom = atomWithStorage<'light' | 'dark'>('theme', 'light')\n\n// Async atom\nexport const userProfileAtom = atom(async (get) => {\n  const user = get(userAtom)\n  if (!user) return null\n  const response = await fetch(`/api/users/${user.id}/profile`)\n  return response.json()\n})\n\n// Write-only atom (action)\nexport const logoutAtom = atom(null, (get, set) => {\n  set(userAtom, null)\n  set(cartAtom, [])\n  localStorage.removeItem('token')\n})\n\n// Usage\nfunction Profile() {\n  const [user] = useAtom(userAtom)\n  const [, logout] = useAtom(logoutAtom)\n  const [profile] = useAtom(userProfileAtom) // Suspense-enabled\n\n  return (\n    <Suspense fallback={<Skeleton />}>\n      <ProfileContent profile={profile} onLogout={logout} />\n    </Suspense>\n  )\n}\n```\n\n### Pattern 4: React Query for Server State\n\n```typescript\n// hooks/useUsers.ts\nimport { useQuery, useMutation, useQueryClient } from '@tanstack/react-query'\n\n// Query keys factory\nexport const userKeys = {\n  all: ['users'] as const,\n  lists: () => [...userKeys.all, 'list'] as const,\n  list: (filters: UserFilters) => [...userKeys.lists(), filters] as const,\n  details: () => [...userKeys.all, 'detail'] as const,\n  detail: (id: string) => [...userKeys.details(), id] as const,\n}\n\n// Fetch hook\nexport function useUsers(filters: UserFilters) {\n  return useQuery({\n    queryKey: userKeys.list(filters),\n    queryFn: () => fetchUsers(filters),\n    staleTime: 5 * 60 * 1000, // 5 minutes\n    gcTime: 30 * 60 * 1000, // 30 minutes (formerly cacheTime)\n  })\n}\n\n// Single user hook\nexport function useUser(id: string) {\n  return useQuery({\n    queryKey: userKeys.detail(id),\n    queryFn: () => fetchUser(id),\n    enabled: !!id, // Don't fetch if no id\n  })\n}\n\n// Mutation with optimistic update\nexport function useUpdateUser() {\n  const queryClient = useQueryClient()\n\n  return useMutation({\n    mutationFn: updateUser,\n    onMutate: async (newUser) => {\n      // Cancel outgoing refetches\n      await queryClient.cancelQueries({ queryKey: userKeys.detail(newUser.id) })\n\n      // Snapshot previous value\n      const previousUser = queryClient.getQueryData(userKeys.detail(newUser.id))\n\n      // Optimistically update\n      queryClient.setQueryData(userKeys.detail(newUser.id), newUser)\n\n      return { previousUser }\n    },\n    onError: (err, newUser, context) => {\n      // Rollback on error\n      queryClient.setQueryData(\n        userKeys.detail(newUser.id),\n        context?.previousUser\n      )\n    },\n    onSettled: (data, error, variables) => {\n      // Refetch after mutation\n      queryClient.invalidateQueries({ queryKey: userKeys.detail(variables.id) })\n    },\n  })\n}\n```\n\n### Pattern 5: Combining Client + Server State\n\n```typescript\n// Zustand for client state\nconst useUIStore = create<UIState>((set) => ({\n  sidebarOpen: true,\n  modal: null,\n  toggleSidebar: () => set((s) => ({ sidebarOpen: !s.sidebarOpen })),\n  openModal: (modal) => set({ modal }),\n  closeModal: () => set({ modal: null }),\n}))\n\n// React Query for server state\nfunction Dashboard() {\n  const { sidebarOpen, toggleSidebar } = useUIStore()\n  const { data: users, isLoading } = useUsers({ active: true })\n  const { data: stats } = useStats()\n\n  if (isLoading) return <DashboardSkeleton />\n\n  return (\n    <div className={sidebarOpen ? 'with-sidebar' : ''}>\n      <Sidebar open={sidebarOpen} onToggle={toggleSidebar} />\n      <main>\n        <StatsCards stats={stats} />\n        <UserTable users={users} />\n      </main>\n    </div>\n  )\n}\n```\n\n## Best Practices\n\n### Do's\n- **Colocate state** - Keep state as close to where it's used as possible\n- **Use selectors** - Prevent unnecessary re-renders with selective subscriptions\n- **Normalize data** - Flatten nested structures for easier updates\n- **Type everything** - Full TypeScript coverage prevents runtime errors\n- **Separate concerns** - Server state (React Query) vs client state (Zustand)\n\n### Don'ts\n- **Don't over-globalize** - Not everything needs to be in global state\n- **Don't duplicate server state** - Let React Query manage it\n- **Don't mutate directly** - Always use immutable updates\n- **Don't store derived data** - Compute it instead\n- **Don't mix paradigms** - Pick one primary solution per category\n\n## Migration Guides\n\n### From Legacy Redux to RTK\n\n```typescript\n// Before (legacy Redux)\nconst ADD_TODO = 'ADD_TODO'\nconst addTodo = (text) => ({ type: ADD_TODO, payload: text })\nfunction todosReducer(state = [], action) {\n  switch (action.type) {\n    case ADD_TODO:\n      return [...state, { text: action.payload, completed: false }]\n    default:\n      return state\n  }\n}\n\n// After (Redux Toolkit)\nconst todosSlice = createSlice({\n  name: 'todos',\n  initialState: [],\n  reducers: {\n    addTodo: (state, action: PayloadAction<string>) => {\n      // Immer allows \"mutations\"\n      state.push({ text: action.payload, completed: false })\n    },\n  },\n})\n```\n\n## Resources\n\n- [Redux Toolkit Documentation](https://redux-toolkit.js.org/)\n- [Zustand GitHub](https://github.com/pmndrs/zustand)\n- [Jotai Documentation](https://jotai.org/)\n- [TanStack Query](https://tanstack.com/query)"
              },
              {
                "name": "tailwind-design-system",
                "description": "Build scalable design systems with Tailwind CSS, design tokens, component libraries, and responsive patterns. Use when creating component libraries, implementing design systems, or standardizing UI patterns.",
                "path": "plugins/frontend-mobile-development/skills/tailwind-design-system/SKILL.md",
                "frontmatter": {
                  "name": "tailwind-design-system",
                  "description": "Build scalable design systems with Tailwind CSS, design tokens, component libraries, and responsive patterns. Use when creating component libraries, implementing design systems, or standardizing UI patterns."
                },
                "content": "# Tailwind Design System\n\nBuild production-ready design systems with Tailwind CSS, including design tokens, component variants, responsive patterns, and accessibility.\n\n## When to Use This Skill\n\n- Creating a component library with Tailwind\n- Implementing design tokens and theming\n- Building responsive and accessible components\n- Standardizing UI patterns across a codebase\n- Migrating to or extending Tailwind CSS\n- Setting up dark mode and color schemes\n\n## Core Concepts\n\n### 1. Design Token Hierarchy\n\n```\nBrand Tokens (abstract)\n    â””â”€â”€ Semantic Tokens (purpose)\n        â””â”€â”€ Component Tokens (specific)\n\nExample:\n    blue-500 â†’ primary â†’ button-bg\n```\n\n### 2. Component Architecture\n\n```\nBase styles â†’ Variants â†’ Sizes â†’ States â†’ Overrides\n```\n\n## Quick Start\n\n```typescript\n// tailwind.config.ts\nimport type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: ['./src/**/*.{js,ts,jsx,tsx,mdx}'],\n  darkMode: 'class',\n  theme: {\n    extend: {\n      colors: {\n        // Semantic color tokens\n        primary: {\n          DEFAULT: 'hsl(var(--primary))',\n          foreground: 'hsl(var(--primary-foreground))',\n        },\n        secondary: {\n          DEFAULT: 'hsl(var(--secondary))',\n          foreground: 'hsl(var(--secondary-foreground))',\n        },\n        destructive: {\n          DEFAULT: 'hsl(var(--destructive))',\n          foreground: 'hsl(var(--destructive-foreground))',\n        },\n        muted: {\n          DEFAULT: 'hsl(var(--muted))',\n          foreground: 'hsl(var(--muted-foreground))',\n        },\n        accent: {\n          DEFAULT: 'hsl(var(--accent))',\n          foreground: 'hsl(var(--accent-foreground))',\n        },\n        background: 'hsl(var(--background))',\n        foreground: 'hsl(var(--foreground))',\n        border: 'hsl(var(--border))',\n        ring: 'hsl(var(--ring))',\n      },\n      borderRadius: {\n        lg: 'var(--radius)',\n        md: 'calc(var(--radius) - 2px)',\n        sm: 'calc(var(--radius) - 4px)',\n      },\n    },\n  },\n  plugins: [require('tailwindcss-animate')],\n}\n\nexport default config\n```\n\n```css\n/* globals.css */\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n@layer base {\n  :root {\n    --background: 0 0% 100%;\n    --foreground: 222.2 84% 4.9%;\n    --primary: 222.2 47.4% 11.2%;\n    --primary-foreground: 210 40% 98%;\n    --secondary: 210 40% 96.1%;\n    --secondary-foreground: 222.2 47.4% 11.2%;\n    --muted: 210 40% 96.1%;\n    --muted-foreground: 215.4 16.3% 46.9%;\n    --accent: 210 40% 96.1%;\n    --accent-foreground: 222.2 47.4% 11.2%;\n    --destructive: 0 84.2% 60.2%;\n    --destructive-foreground: 210 40% 98%;\n    --border: 214.3 31.8% 91.4%;\n    --ring: 222.2 84% 4.9%;\n    --radius: 0.5rem;\n  }\n\n  .dark {\n    --background: 222.2 84% 4.9%;\n    --foreground: 210 40% 98%;\n    --primary: 210 40% 98%;\n    --primary-foreground: 222.2 47.4% 11.2%;\n    --secondary: 217.2 32.6% 17.5%;\n    --secondary-foreground: 210 40% 98%;\n    --muted: 217.2 32.6% 17.5%;\n    --muted-foreground: 215 20.2% 65.1%;\n    --accent: 217.2 32.6% 17.5%;\n    --accent-foreground: 210 40% 98%;\n    --destructive: 0 62.8% 30.6%;\n    --destructive-foreground: 210 40% 98%;\n    --border: 217.2 32.6% 17.5%;\n    --ring: 212.7 26.8% 83.9%;\n  }\n}\n```\n\n## Patterns\n\n### Pattern 1: CVA (Class Variance Authority) Components\n\n```typescript\n// components/ui/button.tsx\nimport { cva, type VariantProps } from 'class-variance-authority'\nimport { forwardRef } from 'react'\nimport { cn } from '@/lib/utils'\n\nconst buttonVariants = cva(\n  // Base styles\n  'inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50',\n  {\n    variants: {\n      variant: {\n        default: 'bg-primary text-primary-foreground hover:bg-primary/90',\n        destructive: 'bg-destructive text-destructive-foreground hover:bg-destructive/90',\n        outline: 'border border-input bg-background hover:bg-accent hover:text-accent-foreground',\n        secondary: 'bg-secondary text-secondary-foreground hover:bg-secondary/80',\n        ghost: 'hover:bg-accent hover:text-accent-foreground',\n        link: 'text-primary underline-offset-4 hover:underline',\n      },\n      size: {\n        default: 'h-10 px-4 py-2',\n        sm: 'h-9 rounded-md px-3',\n        lg: 'h-11 rounded-md px-8',\n        icon: 'h-10 w-10',\n      },\n    },\n    defaultVariants: {\n      variant: 'default',\n      size: 'default',\n    },\n  }\n)\n\nexport interface ButtonProps\n  extends React.ButtonHTMLAttributes<HTMLButtonElement>,\n    VariantProps<typeof buttonVariants> {\n  asChild?: boolean\n}\n\nconst Button = forwardRef<HTMLButtonElement, ButtonProps>(\n  ({ className, variant, size, asChild = false, ...props }, ref) => {\n    const Comp = asChild ? Slot : 'button'\n    return (\n      <Comp\n        className={cn(buttonVariants({ variant, size, className }))}\n        ref={ref}\n        {...props}\n      />\n    )\n  }\n)\nButton.displayName = 'Button'\n\nexport { Button, buttonVariants }\n\n// Usage\n<Button variant=\"destructive\" size=\"lg\">Delete</Button>\n<Button variant=\"outline\">Cancel</Button>\n<Button asChild><Link href=\"/home\">Home</Link></Button>\n```\n\n### Pattern 2: Compound Components\n\n```typescript\n// components/ui/card.tsx\nimport { cn } from '@/lib/utils'\nimport { forwardRef } from 'react'\n\nconst Card = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div\n      ref={ref}\n      className={cn(\n        'rounded-lg border bg-card text-card-foreground shadow-sm',\n        className\n      )}\n      {...props}\n    />\n  )\n)\nCard.displayName = 'Card'\n\nconst CardHeader = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div\n      ref={ref}\n      className={cn('flex flex-col space-y-1.5 p-6', className)}\n      {...props}\n    />\n  )\n)\nCardHeader.displayName = 'CardHeader'\n\nconst CardTitle = forwardRef<HTMLHeadingElement, React.HTMLAttributes<HTMLHeadingElement>>(\n  ({ className, ...props }, ref) => (\n    <h3\n      ref={ref}\n      className={cn('text-2xl font-semibold leading-none tracking-tight', className)}\n      {...props}\n    />\n  )\n)\nCardTitle.displayName = 'CardTitle'\n\nconst CardDescription = forwardRef<HTMLParagraphElement, React.HTMLAttributes<HTMLParagraphElement>>(\n  ({ className, ...props }, ref) => (\n    <p\n      ref={ref}\n      className={cn('text-sm text-muted-foreground', className)}\n      {...props}\n    />\n  )\n)\nCardDescription.displayName = 'CardDescription'\n\nconst CardContent = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div ref={ref} className={cn('p-6 pt-0', className)} {...props} />\n  )\n)\nCardContent.displayName = 'CardContent'\n\nconst CardFooter = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div\n      ref={ref}\n      className={cn('flex items-center p-6 pt-0', className)}\n      {...props}\n    />\n  )\n)\nCardFooter.displayName = 'CardFooter'\n\nexport { Card, CardHeader, CardTitle, CardDescription, CardContent, CardFooter }\n\n// Usage\n<Card>\n  <CardHeader>\n    <CardTitle>Account</CardTitle>\n    <CardDescription>Manage your account settings</CardDescription>\n  </CardHeader>\n  <CardContent>\n    <form>...</form>\n  </CardContent>\n  <CardFooter>\n    <Button>Save</Button>\n  </CardFooter>\n</Card>\n```\n\n### Pattern 3: Form Components\n\n```typescript\n// components/ui/input.tsx\nimport { forwardRef } from 'react'\nimport { cn } from '@/lib/utils'\n\nexport interface InputProps extends React.InputHTMLAttributes<HTMLInputElement> {\n  error?: string\n}\n\nconst Input = forwardRef<HTMLInputElement, InputProps>(\n  ({ className, type, error, ...props }, ref) => {\n    return (\n      <div className=\"relative\">\n        <input\n          type={type}\n          className={cn(\n            'flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50',\n            error && 'border-destructive focus-visible:ring-destructive',\n            className\n          )}\n          ref={ref}\n          aria-invalid={!!error}\n          aria-describedby={error ? `${props.id}-error` : undefined}\n          {...props}\n        />\n        {error && (\n          <p\n            id={`${props.id}-error`}\n            className=\"mt-1 text-sm text-destructive\"\n            role=\"alert\"\n          >\n            {error}\n          </p>\n        )}\n      </div>\n    )\n  }\n)\nInput.displayName = 'Input'\n\n// components/ui/label.tsx\nimport { cva, type VariantProps } from 'class-variance-authority'\n\nconst labelVariants = cva(\n  'text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70'\n)\n\nconst Label = forwardRef<HTMLLabelElement, React.LabelHTMLAttributes<HTMLLabelElement>>(\n  ({ className, ...props }, ref) => (\n    <label ref={ref} className={cn(labelVariants(), className)} {...props} />\n  )\n)\nLabel.displayName = 'Label'\n\n// Usage with React Hook Form\nimport { useForm } from 'react-hook-form'\nimport { zodResolver } from '@hookform/resolvers/zod'\nimport * as z from 'zod'\n\nconst schema = z.object({\n  email: z.string().email('Invalid email address'),\n  password: z.string().min(8, 'Password must be at least 8 characters'),\n})\n\nfunction LoginForm() {\n  const { register, handleSubmit, formState: { errors } } = useForm({\n    resolver: zodResolver(schema),\n  })\n\n  return (\n    <form onSubmit={handleSubmit(onSubmit)} className=\"space-y-4\">\n      <div className=\"space-y-2\">\n        <Label htmlFor=\"email\">Email</Label>\n        <Input\n          id=\"email\"\n          type=\"email\"\n          {...register('email')}\n          error={errors.email?.message}\n        />\n      </div>\n      <div className=\"space-y-2\">\n        <Label htmlFor=\"password\">Password</Label>\n        <Input\n          id=\"password\"\n          type=\"password\"\n          {...register('password')}\n          error={errors.password?.message}\n        />\n      </div>\n      <Button type=\"submit\" className=\"w-full\">Sign In</Button>\n    </form>\n  )\n}\n```\n\n### Pattern 4: Responsive Grid System\n\n```typescript\n// components/ui/grid.tsx\nimport { cn } from '@/lib/utils'\nimport { cva, type VariantProps } from 'class-variance-authority'\n\nconst gridVariants = cva('grid', {\n  variants: {\n    cols: {\n      1: 'grid-cols-1',\n      2: 'grid-cols-1 sm:grid-cols-2',\n      3: 'grid-cols-1 sm:grid-cols-2 lg:grid-cols-3',\n      4: 'grid-cols-1 sm:grid-cols-2 lg:grid-cols-4',\n      5: 'grid-cols-2 sm:grid-cols-3 lg:grid-cols-5',\n      6: 'grid-cols-2 sm:grid-cols-3 lg:grid-cols-6',\n    },\n    gap: {\n      none: 'gap-0',\n      sm: 'gap-2',\n      md: 'gap-4',\n      lg: 'gap-6',\n      xl: 'gap-8',\n    },\n  },\n  defaultVariants: {\n    cols: 3,\n    gap: 'md',\n  },\n})\n\ninterface GridProps\n  extends React.HTMLAttributes<HTMLDivElement>,\n    VariantProps<typeof gridVariants> {}\n\nexport function Grid({ className, cols, gap, ...props }: GridProps) {\n  return (\n    <div className={cn(gridVariants({ cols, gap, className }))} {...props} />\n  )\n}\n\n// Container component\nconst containerVariants = cva('mx-auto w-full px-4 sm:px-6 lg:px-8', {\n  variants: {\n    size: {\n      sm: 'max-w-screen-sm',\n      md: 'max-w-screen-md',\n      lg: 'max-w-screen-lg',\n      xl: 'max-w-screen-xl',\n      '2xl': 'max-w-screen-2xl',\n      full: 'max-w-full',\n    },\n  },\n  defaultVariants: {\n    size: 'xl',\n  },\n})\n\ninterface ContainerProps\n  extends React.HTMLAttributes<HTMLDivElement>,\n    VariantProps<typeof containerVariants> {}\n\nexport function Container({ className, size, ...props }: ContainerProps) {\n  return (\n    <div className={cn(containerVariants({ size, className }))} {...props} />\n  )\n}\n\n// Usage\n<Container>\n  <Grid cols={4} gap=\"lg\">\n    {products.map((product) => (\n      <ProductCard key={product.id} product={product} />\n    ))}\n  </Grid>\n</Container>\n```\n\n### Pattern 5: Animation Utilities\n\n```typescript\n// lib/animations.ts - Tailwind CSS Animate utilities\nimport { cn } from './utils'\n\nexport const fadeIn = 'animate-in fade-in duration-300'\nexport const fadeOut = 'animate-out fade-out duration-300'\nexport const slideInFromTop = 'animate-in slide-in-from-top duration-300'\nexport const slideInFromBottom = 'animate-in slide-in-from-bottom duration-300'\nexport const slideInFromLeft = 'animate-in slide-in-from-left duration-300'\nexport const slideInFromRight = 'animate-in slide-in-from-right duration-300'\nexport const zoomIn = 'animate-in zoom-in-95 duration-300'\nexport const zoomOut = 'animate-out zoom-out-95 duration-300'\n\n// Compound animations\nexport const modalEnter = cn(fadeIn, zoomIn, 'duration-200')\nexport const modalExit = cn(fadeOut, zoomOut, 'duration-200')\nexport const dropdownEnter = cn(fadeIn, slideInFromTop, 'duration-150')\nexport const dropdownExit = cn(fadeOut, 'slide-out-to-top', 'duration-150')\n\n// components/ui/dialog.tsx\nimport * as DialogPrimitive from '@radix-ui/react-dialog'\n\nconst DialogOverlay = forwardRef<\n  React.ElementRef<typeof DialogPrimitive.Overlay>,\n  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Overlay>\n>(({ className, ...props }, ref) => (\n  <DialogPrimitive.Overlay\n    ref={ref}\n    className={cn(\n      'fixed inset-0 z-50 bg-black/80',\n      'data-[state=open]:animate-in data-[state=closed]:animate-out',\n      'data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0',\n      className\n    )}\n    {...props}\n  />\n))\n\nconst DialogContent = forwardRef<\n  React.ElementRef<typeof DialogPrimitive.Content>,\n  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Content>\n>(({ className, children, ...props }, ref) => (\n  <DialogPortal>\n    <DialogOverlay />\n    <DialogPrimitive.Content\n      ref={ref}\n      className={cn(\n        'fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg',\n        'data-[state=open]:animate-in data-[state=closed]:animate-out',\n        'data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0',\n        'data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95',\n        'data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%]',\n        'data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%]',\n        'sm:rounded-lg',\n        className\n      )}\n      {...props}\n    >\n      {children}\n    </DialogPrimitive.Content>\n  </DialogPortal>\n))\n```\n\n### Pattern 6: Dark Mode Implementation\n\n```typescript\n// providers/ThemeProvider.tsx\n'use client'\n\nimport { createContext, useContext, useEffect, useState } from 'react'\n\ntype Theme = 'dark' | 'light' | 'system'\n\ninterface ThemeProviderProps {\n  children: React.ReactNode\n  defaultTheme?: Theme\n  storageKey?: string\n}\n\ninterface ThemeContextType {\n  theme: Theme\n  setTheme: (theme: Theme) => void\n  resolvedTheme: 'dark' | 'light'\n}\n\nconst ThemeContext = createContext<ThemeContextType | undefined>(undefined)\n\nexport function ThemeProvider({\n  children,\n  defaultTheme = 'system',\n  storageKey = 'theme',\n}: ThemeProviderProps) {\n  const [theme, setTheme] = useState<Theme>(defaultTheme)\n  const [resolvedTheme, setResolvedTheme] = useState<'dark' | 'light'>('light')\n\n  useEffect(() => {\n    const stored = localStorage.getItem(storageKey) as Theme | null\n    if (stored) setTheme(stored)\n  }, [storageKey])\n\n  useEffect(() => {\n    const root = window.document.documentElement\n    root.classList.remove('light', 'dark')\n\n    let resolved: 'dark' | 'light'\n\n    if (theme === 'system') {\n      resolved = window.matchMedia('(prefers-color-scheme: dark)').matches\n        ? 'dark'\n        : 'light'\n    } else {\n      resolved = theme\n    }\n\n    root.classList.add(resolved)\n    setResolvedTheme(resolved)\n  }, [theme])\n\n  const value = {\n    theme,\n    setTheme: (newTheme: Theme) => {\n      localStorage.setItem(storageKey, newTheme)\n      setTheme(newTheme)\n    },\n    resolvedTheme,\n  }\n\n  return (\n    <ThemeContext.Provider value={value}>{children}</ThemeContext.Provider>\n  )\n}\n\nexport const useTheme = () => {\n  const context = useContext(ThemeContext)\n  if (!context) throw new Error('useTheme must be used within ThemeProvider')\n  return context\n}\n\n// components/ThemeToggle.tsx\nimport { Moon, Sun } from 'lucide-react'\nimport { useTheme } from '@/providers/ThemeProvider'\n\nexport function ThemeToggle() {\n  const { resolvedTheme, setTheme } = useTheme()\n\n  return (\n    <Button\n      variant=\"ghost\"\n      size=\"icon\"\n      onClick={() => setTheme(resolvedTheme === 'dark' ? 'light' : 'dark')}\n    >\n      <Sun className=\"h-5 w-5 rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0\" />\n      <Moon className=\"absolute h-5 w-5 rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100\" />\n      <span className=\"sr-only\">Toggle theme</span>\n    </Button>\n  )\n}\n```\n\n## Utility Functions\n\n```typescript\n// lib/utils.ts\nimport { type ClassValue, clsx } from 'clsx'\nimport { twMerge } from 'tailwind-merge'\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs))\n}\n\n// Focus ring utility\nexport const focusRing = cn(\n  'focus-visible:outline-none focus-visible:ring-2',\n  'focus-visible:ring-ring focus-visible:ring-offset-2'\n)\n\n// Disabled utility\nexport const disabled = 'disabled:pointer-events-none disabled:opacity-50'\n```\n\n## Best Practices\n\n### Do's\n- **Use CSS variables** - Enable runtime theming\n- **Compose with CVA** - Type-safe variants\n- **Use semantic colors** - `primary` not `blue-500`\n- **Forward refs** - Enable composition\n- **Add accessibility** - ARIA attributes, focus states\n\n### Don'ts\n- **Don't use arbitrary values** - Extend theme instead\n- **Don't nest @apply** - Hurts readability\n- **Don't skip focus states** - Keyboard users need them\n- **Don't hardcode colors** - Use semantic tokens\n- **Don't forget dark mode** - Test both themes\n\n## Resources\n\n- [Tailwind CSS Documentation](https://tailwindcss.com/docs)\n- [CVA Documentation](https://cva.style/docs)\n- [shadcn/ui](https://ui.shadcn.com/)\n- [Radix Primitives](https://www.radix-ui.com/primitives)"
              }
            ]
          },
          {
            "name": "frontend-mobile-security",
            "description": "Frontend and mobile security with XSS scanning",
            "source": "./plugins/frontend-mobile-security",
            "category": "security",
            "version": "1.0.0",
            "author": {
              "name": "wshobson"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install frontend-mobile-security@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/xss-scan",
                "description": null,
                "path": "plugins/frontend-mobile-security/commands/xss-scan.md",
                "frontmatter": null,
                "content": "# XSS Vulnerability Scanner for Frontend Code\n\nYou are a frontend security specialist focusing on Cross-Site Scripting (XSS) vulnerability detection and prevention. Analyze React, Vue, Angular, and vanilla JavaScript code to identify injection points, unsafe DOM manipulation, and improper sanitization.\n\n## Context\n\nThe user needs comprehensive XSS vulnerability scanning for client-side code, identifying dangerous patterns like unsafe HTML manipulation, URL handling issues, and improper user input rendering. Focus on context-aware detection and framework-specific security patterns.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. XSS Vulnerability Detection\n\nScan codebase for XSS vulnerabilities using static analysis:\n\n```typescript\ninterface XSSFinding {\n  file: string;\n  line: number;\n  severity: 'critical' | 'high' | 'medium' | 'low';\n  type: string;\n  vulnerable_code: string;\n  description: string;\n  fix: string;\n  cwe: string;\n}\n\nclass XSSScanner {\n  private vulnerablePatterns = [\n    'innerHTML', 'outerHTML', 'document.write',\n    'insertAdjacentHTML', 'location.href', 'window.open'\n  ];\n\n  async scanDirectory(path: string): Promise<XSSFinding[]> {\n    const files = await this.findJavaScriptFiles(path);\n    const findings: XSSFinding[] = [];\n\n    for (const file of files) {\n      const content = await fs.readFile(file, 'utf-8');\n      findings.push(...this.scanFile(file, content));\n    }\n\n    return findings;\n  }\n\n  scanFile(filePath: string, content: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n\n    findings.push(...this.detectHTMLManipulation(filePath, content));\n    findings.push(...this.detectReactVulnerabilities(filePath, content));\n    findings.push(...this.detectURLVulnerabilities(filePath, content));\n    findings.push(...this.detectEventHandlerIssues(filePath, content));\n\n    return findings;\n  }\n\n  detectHTMLManipulation(file: string, content: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n    const lines = content.split('\\n');\n\n    lines.forEach((line, index) => {\n      if (line.includes('innerHTML') && this.hasUserInput(line)) {\n        findings.push({\n          file,\n          line: index + 1,\n          severity: 'critical',\n          type: 'Unsafe HTML manipulation',\n          vulnerable_code: line.trim(),\n          description: 'User-controlled data in HTML manipulation creates XSS risk',\n          fix: 'Use textContent for plain text or sanitize with DOMPurify library',\n          cwe: 'CWE-79'\n        });\n      }\n    });\n\n    return findings;\n  }\n\n  detectReactVulnerabilities(file: string, content: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n    const lines = content.split('\\n');\n\n    lines.forEach((line, index) => {\n      if (line.includes('dangerously') && !this.hasSanitization(content)) {\n        findings.push({\n          file,\n          line: index + 1,\n          severity: 'high',\n          type: 'React unsafe HTML rendering',\n          vulnerable_code: line.trim(),\n          description: 'Unsanitized HTML in React component creates XSS vulnerability',\n          fix: 'Apply DOMPurify.sanitize() before rendering or use safe alternatives',\n          cwe: 'CWE-79'\n        });\n      }\n    });\n\n    return findings;\n  }\n\n  detectURLVulnerabilities(file: string, content: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n    const lines = content.split('\\n');\n\n    lines.forEach((line, index) => {\n      if (line.includes('location.') && this.hasUserInput(line)) {\n        findings.push({\n          file,\n          line: index + 1,\n          severity: 'high',\n          type: 'URL injection',\n          vulnerable_code: line.trim(),\n          description: 'User input in URL assignment can execute malicious code',\n          fix: 'Validate URLs and enforce http/https protocols only',\n          cwe: 'CWE-79'\n        });\n      }\n    });\n\n    return findings;\n  }\n\n  hasUserInput(line: string): boolean {\n    const indicators = ['props', 'state', 'params', 'query', 'input', 'formData'];\n    return indicators.some(indicator => line.includes(indicator));\n  }\n\n  hasSanitization(content: string): boolean {\n    return content.includes('DOMPurify') || content.includes('sanitize');\n  }\n}\n```\n\n### 2. Framework-Specific Detection\n\n```typescript\nclass ReactXSSScanner {\n  scanReactComponent(code: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n\n    // Check for unsafe React patterns\n    const unsafePatterns = [\n      'dangerouslySetInnerHTML',\n      'createMarkup',\n      'rawHtml'\n    ];\n\n    unsafePatterns.forEach(pattern => {\n      if (code.includes(pattern) && !code.includes('DOMPurify')) {\n        findings.push({\n          severity: 'high',\n          type: 'React XSS risk',\n          description: `Pattern ${pattern} used without sanitization`,\n          fix: 'Apply proper HTML sanitization'\n        });\n      }\n    });\n\n    return findings;\n  }\n}\n\nclass VueXSSScanner {\n  scanVueTemplate(template: string): XSSFinding[] {\n    const findings: XSSFinding[] = [];\n\n    if (template.includes('v-html')) {\n      findings.push({\n        severity: 'high',\n        type: 'Vue HTML injection',\n        description: 'v-html directive renders raw HTML',\n        fix: 'Use v-text for plain text or sanitize HTML'\n      });\n    }\n\n    return findings;\n  }\n}\n```\n\n### 3. Secure Coding Examples\n\n```typescript\nclass SecureCodingGuide {\n  getSecurePattern(vulnerability: string): string {\n    const patterns = {\n      html_manipulation: `\n// SECURE: Use textContent for plain text\nelement.textContent = userInput;\n\n// SECURE: Sanitize HTML when needed\nimport DOMPurify from 'dompurify';\nconst clean = DOMPurify.sanitize(userInput);\nelement.innerHTML = clean;`,\n\n      url_handling: `\n// SECURE: Validate and sanitize URLs\nfunction sanitizeURL(url: string): string {\n  try {\n    const parsed = new URL(url);\n    if (['http:', 'https:'].includes(parsed.protocol)) {\n      return parsed.href;\n    }\n  } catch {}\n  return '#';\n}`,\n\n      react_rendering: `\n// SECURE: Sanitize before rendering\nimport DOMPurify from 'dompurify';\n\nconst Component = ({ html }) => (\n  <div dangerouslySetInnerHTML={{\n    __html: DOMPurify.sanitize(html)\n  }} />\n);`\n    };\n\n    return patterns[vulnerability] || 'No secure pattern available';\n  }\n}\n```\n\n### 4. Automated Scanning Integration\n\n```bash\n# ESLint with security plugin\nnpm install --save-dev eslint-plugin-security\neslint . --plugin security\n\n# Semgrep for XSS patterns\nsemgrep --config=p/xss --json\n\n# Custom XSS scanner\nnode xss-scanner.js --path=src --format=json\n```\n\n### 5. Report Generation\n\n```typescript\nclass XSSReportGenerator {\n  generateReport(findings: XSSFinding[]): string {\n    const grouped = this.groupBySeverity(findings);\n\n    let report = '# XSS Vulnerability Scan Report\\n\\n';\n    report += `Total Findings: ${findings.length}\\n\\n`;\n\n    for (const [severity, issues] of Object.entries(grouped)) {\n      report += `## ${severity.toUpperCase()} (${issues.length})\\n\\n`;\n\n      for (const issue of issues) {\n        report += `- **${issue.type}**\\n`;\n        report += `  File: ${issue.file}:${issue.line}\\n`;\n        report += `  Fix: ${issue.fix}\\n\\n`;\n      }\n    }\n\n    return report;\n  }\n\n  groupBySeverity(findings: XSSFinding[]): Record<string, XSSFinding[]> {\n    return findings.reduce((acc, finding) => {\n      if (!acc[finding.severity]) acc[finding.severity] = [];\n      acc[finding.severity].push(finding);\n      return acc;\n    }, {} as Record<string, XSSFinding[]>);\n  }\n}\n```\n\n### 6. Prevention Checklist\n\n**HTML Manipulation**\n- Never use innerHTML with user input\n- Prefer textContent for text content\n- Sanitize with DOMPurify before rendering HTML\n- Avoid document.write entirely\n\n**URL Handling**\n- Validate all URLs before assignment\n- Block javascript: and data: protocols\n- Use URL constructor for validation\n- Sanitize href attributes\n\n**Event Handlers**\n- Use addEventListener instead of inline handlers\n- Sanitize all event handler input\n- Avoid string-to-code patterns\n\n**Framework-Specific**\n- React: Sanitize before using unsafe APIs\n- Vue: Prefer v-text over v-html\n- Angular: Use built-in sanitization\n- Avoid bypassing framework security features\n\n## Output Format\n\n1. **Vulnerability Report**: Detailed findings with severity levels\n2. **Risk Analysis**: Impact assessment for each vulnerability\n3. **Fix Recommendations**: Secure code examples\n4. **Sanitization Guide**: DOMPurify usage patterns\n5. **Prevention Checklist**: Best practices for XSS prevention\n\nFocus on identifying XSS attack vectors, providing actionable fixes, and establishing secure coding patterns.\n"
              }
            ],
            "skills": []
          },
          {
            "name": "awesome-claude-skills",
            "description": "27 practical Claude Skills for documents and productivity",
            "source": "./plugins/awesome-claude-skills",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "ComposioHQ"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install awesome-claude-skills@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "ios-simulator-skill",
            "description": "iOS Simulator automation with 21 scripts for testing",
            "source": "./plugins/ios-simulator-skill",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "conorluddy"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install ios-simulator-skill@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "multi-agent-patterns",
            "description": "Multi-agent architecture patterns for context isolation",
            "source": "./plugins/multi-agent-patterns",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install multi-agent-patterns@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "beautiful-prose",
            "description": "Hard-edged writing style skill for forceful prose",
            "source": "./plugins/beautiful-prose",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "SHADOWPR0"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install beautiful-prose@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "ralph-wiggum-marketer",
            "description": "Autonomous AI copywriter for SaaS content marketing",
            "source": "./plugins/ralph-wiggum-marketer",
            "category": "marketing",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install ralph-wiggum-marketer@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/ralph-cancel",
                "description": "Cancel the active Ralph Marketer loop",
                "path": "plugins/ralph-wiggum-marketer/commands/ralph-cancel.md",
                "frontmatter": {
                  "description": "Cancel the active Ralph Marketer loop",
                  "allowed-tools": [
                    "Bash",
                    "Write"
                  ],
                  "model": "haiku"
                },
                "content": "# Cancel Ralph Marketer Loop\n\nThe user wants to cancel the Ralph Marketer loop.\n\n## Actions\n\n1. Remove the loop state file if it exists:\n```bash\nrm -f .claude/ralph-marketer-loop.local.md 2>/dev/null && echo \"Loop state cleared\" || echo \"No active loop found\"\n```\n\n2. Inform the user:\n- The Ralph loop has been cancelled\n- Any in-progress work has been preserved in git\n- They can check progress with `/ralph-status`\n- They can restart with `/ralph-marketer`\n\nThe loop is now cancelled. You will no longer receive repeated prompts."
              },
              {
                "name": "/ralph-init",
                "description": "Initialize a new Ralph Marketer project in the current directory",
                "path": "plugins/ralph-wiggum-marketer/commands/ralph-init.md",
                "frontmatter": {
                  "description": "Initialize a new Ralph Marketer project in the current directory",
                  "allowed-tools": [
                    "Bash",
                    "Write",
                    "Read",
                    "Glob"
                  ],
                  "model": "sonnet"
                },
                "content": "# Initialize Ralph the Marketer\n\nSet up a new Ralph Marketer copywriting project in the current directory.\n\n## What This Does\n\n1. Creates the directory structure for Ralph\n2. Copies template files (prd.json, progress.txt, prompt.md)\n3. Sets up the content database with sample data\n4. Creates content directories (drafts/, published/)\n\n## Setup Steps\n\n```bash\n# Create directories\nmkdir -p scripts/ralph content/{drafts,published} data src/{db,content}\n\n# Copy database scripts from plugin\nPLUGIN_ROOT=\"${CLAUDE_PLUGIN_ROOT}\"\ncp \"$PLUGIN_ROOT/scripts/src/db/init.js\" src/db/\ncp \"$PLUGIN_ROOT/scripts/src/db/seed.js\" src/db/\ncp \"$PLUGIN_ROOT/scripts/src/db/status.js\" src/db/\ncp \"$PLUGIN_ROOT/scripts/src/db/query.js\" src/db/\ncp \"$PLUGIN_ROOT/scripts/src/content/list.js\" src/content/\ncp \"$PLUGIN_ROOT/scripts/src/test.js\" src/\n\n# Copy Ralph templates\ncp \"$PLUGIN_ROOT/templates/prd.json\" scripts/ralph/\ncp \"$PLUGIN_ROOT/templates/progress.txt\" scripts/ralph/\ncp \"$PLUGIN_ROOT/templates/prompt.md\" scripts/ralph/\n\n# Copy package.json if it doesn't exist\nif [ ! -f package.json ]; then\n  cp \"$PLUGIN_ROOT/templates/package.json\" .\nfi\n\n# Create .gitkeep files\ntouch content/drafts/.gitkeep content/published/.gitkeep data/.gitkeep\n\n# Initialize git if needed\nif [ ! -d .git ]; then\n  git init\nfi\n\n# Install dependencies\nnpm install\n\n# Initialize and seed database\nnpm run db:reset\n\necho \"âœ… Ralph Marketer initialized!\"\necho \"\"\necho \"Next steps:\"\necho \"  1. Edit scripts/ralph/prd.json with your content tasks\"\necho \"  2. Run /ralph-marketer to start the loop\"\n```\n\n## After Initialization\n\nThe user should:\n1. Customize `scripts/ralph/prd.json` with their specific content tasks\n2. Optionally modify the seed data in `src/db/seed.js` with their own trends/research/communications\n3. Run `/ralph-marketer` to start the autonomous content creation loop"
              },
              {
                "name": "/ralph-marketer",
                "description": null,
                "path": "plugins/ralph-wiggum-marketer/commands/ralph-marketer.md",
                "frontmatter": null,
                "content": "---\ndescription: Start the Ralph Marketer autonomous copywriter loop\nargument-hint: [--max-iterations <n>] [--completion-promise <text>]\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash, WebFetch]\nmodel: sonnet\n---\n\n# Ralph the Marketer - Autonomous Copywriter Loop\n\nYou are starting the **Ralph the Marketer** autonomous copywriting agent. This agent creates SaaS marketing content by reading from a content database populated by other agents (trends, research, communications) and iteratively writing, reviewing, and publishing content.\n\n## Setup (First Run Only)\n\nIf this is your first time running Ralph the Marketer in this project, you need to initialize the content database:\n\n```bash\n# From the plugin directory\ncd ${CLAUDE_PLUGIN_ROOT}\nnpm install\nnpm run db:reset\n```\n\nOr set up the project structure in your current directory:\n\n```bash\n# Copy templates to your project\ncp -r ${CLAUDE_PLUGIN_ROOT}/templates/ralph ./scripts/ralph\ncp ${CLAUDE_PLUGIN_ROOT}/templates/package.json ./package.json\ncp -r ${CLAUDE_PLUGIN_ROOT}/scripts/src ./src\nmkdir -p content/{drafts,published} data\nnpm install\nnpm run db:reset\n```\n\n## Your Task\n\nYou are now in a Ralph loop. Each iteration:\n\n1. **Read the PRD**: Check `scripts/ralph/prd.json` for user stories\n2. **Check Progress**: Read `scripts/ralph/progress.txt` for patterns and learnings\n3. **Pick Next Story**: Find highest priority story where `passes: false`\n4. **Execute**: Complete the story following its acceptance criteria\n5. **Verify**: Run `npm test` to ensure quality\n6. **Commit**: `git commit -m \"content: [ID] - [Title]\"`\n7. **Update**: Mark story as `passes: true`, log to progress.txt\n\n## Content Sources\n\nCheck what's available in the database:\n\n```bash\nnpm run db:status          # Pipeline overview\nnode src/content/list.js   # Available trends, research, comms\n```\n\n## Story Types\n\n- **SETUP-xxx**: Initialize/configure something\n- **PLAN-xxx**: Create a content plan from sources\n- **WRITE-xxx**: Write content (drafts to `content/drafts/`)\n- **REVIEW-xxx**: Self-review and improve drafts\n- **PUBLISH-xxx**: Finalize and publish (`content/published/`)\n- **SOCIAL-xxx**: Create social media posts\n- **NEWSLETTER-xxx**: Draft newsletters\n\n## Completion Signal\n\nWhen ALL stories in `prd.json` have `passes: true`, output:\n\n```\n<promise>COMPLETE</promise>\n```\n\n## Arguments\n\n$ARGUMENTS\n\n- `--max-iterations <n>`: Stop after N iterations (default: unlimited)\n- `--completion-promise <text>`: Custom completion signal (default: \"COMPLETE\")\n\n## Begin\n\nRead the PRD. Find your next task. Ship great content.\n\nYou have unlimited iterations. Persistence wins.\n"
              },
              {
                "name": "/ralph-status",
                "description": "Check the content pipeline status for Ralph the Marketer",
                "path": "plugins/ralph-wiggum-marketer/commands/ralph-status.md",
                "frontmatter": {
                  "description": "Check the content pipeline status for Ralph the Marketer",
                  "allowed-tools": [
                    "Bash",
                    "Read"
                  ],
                  "model": "haiku"
                },
                "content": "# Ralph Marketer Status\n\nCheck the current status of the content pipeline and Ralph's progress.\n\n## Run Status Commands\n\n```bash\n# Database status\nnpm run db:status\n\n# Available content sources\nnode src/content/list.js\n\n# PRD status\ncat scripts/ralph/prd.json | jq '.userStories[] | {id, title, passes, priority}'\n\n# Recent progress\ntail -50 scripts/ralph/progress.txt\n\n# Recent commits\ngit log --oneline -10\n\n# Content counts\necho \"Drafts:\" && ls -la content/drafts/ 2>/dev/null || echo \"  No drafts yet\"\necho \"Published:\" && ls -la content/published/ 2>/dev/null || echo \"  No published content yet\"\n```\n\nSummarize the status for the user:\n- How many stories are complete vs remaining\n- What content has been created\n- Any blockers or issues"
              }
            ],
            "skills": [
              {
                "name": "ralph-copywriter",
                "description": "Use this skill when the user asks to \"analyze my content\", \"learn my writing style\", \"research competitors\", \"find content angles\", \"improve my blog\", \"write like me\", \"embody my brand voice\", or mentions content strategy, voice analysis, competitive research, or iterative content improvement.",
                "path": "plugins/ralph-wiggum-marketer/skills/copywriter/SKILL.md",
                "frontmatter": {
                  "name": "ralph-copywriter",
                  "description": "Use this skill when the user asks to \"analyze my content\", \"learn my writing style\", \"research competitors\", \"find content angles\", \"improve my blog\", \"write like me\", \"embody my brand voice\", or mentions content strategy, voice analysis, competitive research, or iterative content improvement.",
                  "version": "2.0.0"
                },
                "content": "# Ralph the Copywriter - Quality Through Iteration\n\nAn AI copywriter that **learns your voice, researches deeply, and iterates until the content is genuinely good** - not just fast.\n\n## Philosophy\n\n> \"Anyone can generate 10 blog posts. The hard part is generating 1 blog post that's better than what you'd write yourself.\"\n\nRalph doesn't just write content. Ralph:\n1. **Studies your existing content** to learn your voice\n2. **Researches deeply** before touching the keyboard\n3. **Finds unique angles** others miss\n4. **Writes in your style**, not generic AI slop\n5. **Self-critiques ruthlessly** and iterates\n6. **Gets better over time** as patterns compound\n\n## The Quality Loop\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RALPH QUALITY LOOP                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ DISCOVER â”‚ â†’ Analyze your content, competitors, market      â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â–¼                                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚  LEARN   â”‚ â†’ Extract voice, style, patterns, what works     â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â–¼                                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ RESEARCH â”‚ â†’ Deep dive: data, trends, unique angles         â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â–¼                                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚  IDEATE  â”‚ â†’ Find the angle nobody else is taking           â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â–¼                                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚  WRITE   â”‚ â†’ Draft in YOUR voice with YOUR patterns         â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â–¼                                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ CRITIQUE â”‚ â†’ \"Would the founder actually publish this?\"     â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â”‚                                                        â”‚\nâ”‚        â–¼  No â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚                   â”‚\nâ”‚   â”‚ ITERATE  â”‚ â† Improve based on critique â—„â”˜                   â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚        â”‚                                                        â”‚\nâ”‚        â–¼  Yes                                                   â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ PUBLISH  â”‚ â†’ Only when it meets the quality bar             â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Phase 1: DISCOVER - Know the Landscape\n\nBefore writing anything, Ralph analyzes:\n\n### Your Content\n```\n- What topics do you write about?\n- What's your average post length?\n- How do you structure arguments?\n- What phrases do you repeat?\n- What's your hook style?\n- How do you use data?\n```\n\n### Your Competitors\n```\n- What are they writing about?\n- What angles are overused?\n- Where are the gaps?\n- What's working for them (shares, comments)?\n```\n\n### Your Market\n```\n- What questions is your audience asking?\n- What trends are emerging?\n- What pain points aren't being addressed?\n```\n\n## Phase 2: LEARN - Embody the Voice\n\nRalph extracts your unique voice patterns:\n\n### Voice DNA\n```javascript\n{\n  \"tone\": \"confident but not arrogant\",\n  \"formality\": \"casual professional\",\n  \"sentence_length\": \"varied, avg 15 words\",\n  \"paragraph_style\": \"short, punchy, lots of white space\",\n  \"signature_phrases\": [\"here's the thing\", \"let me be direct\"],\n  \"data_usage\": \"leads with stats, cites sources\",\n  \"storytelling\": \"personal anecdotes to illustrate points\",\n  \"cta_style\": \"soft ask, value-first\",\n  \"controversial_takes\": true,\n  \"emoji_usage\": \"minimal, strategic\"\n}\n```\n\n### What Makes Your Content Work\n```\n- Why do your best posts perform?\n- What patterns emerge in high-engagement content?\n- What's your unique perspective others don't have?\n```\n\n## Phase 3: RESEARCH - Go Deep\n\nRalph doesn't write from thin air:\n\n### Data Gathering\n- Primary sources (studies, reports, surveys)\n- Expert opinions and quotes\n- Real examples and case studies\n- Counter-arguments to address\n\n### Angle Discovery\n- What's the obvious take everyone has?\n- What's the contrarian take that's actually true?\n- What personal experience adds credibility?\n- What data point changes everything?\n\n### Gap Analysis\n```\nStandard angle: \"AI will change marketing\"\nRalph's angle: \"Why 73% of AI marketing tools fail -\n               and the 3 patterns that predict success\"\n```\n\n## Phase 4: IDEATE - Find the Unique Angle\n\nRalph doesn't write \"Top 10 Tips\" content:\n\n### The Angle Test\n```\nâŒ \"How to Use AI for Content Marketing\"\n   (1000 articles exist)\n\nâŒ \"AI Content Marketing Best Practices\"\n   (Generic, forgettable)\n\nâœ… \"I Ran 50 AI Content Experiments. Here's What Actually Worked.\"\n   (Unique data, personal authority, specific)\n\nâœ… \"The AI Content Playbook That Got Us From 0 to 50k Visitors\"\n   (Specific results, implies system, curiosity gap)\n```\n\n### Ideation Framework\n1. What do I know that others don't?\n2. What have I experienced that's counterintuitive?\n3. What data do I have access to?\n4. What question is everyone asking but nobody answering well?\n\n## Phase 5: WRITE - Embody the Style\n\nRalph writes AS you, not FOR you:\n\n### Before Writing Checklist\n- [ ] Voice DNA loaded\n- [ ] Research complete\n- [ ] Unique angle identified\n- [ ] Target reader defined\n- [ ] Success metric clear\n\n### Writing with Voice\n```markdown\n# Generic AI:\n\"In today's rapidly evolving digital landscape,\nartificial intelligence has become increasingly important...\"\n\n# Ralph (embodying founder voice):\n\"Here's the thing about AI content tools:\nmost of them produce garbage.\n\nI've tested 47 of them. Want to know how many\nproduced something I'd actually publish? Three.\"\n```\n\n## Phase 6: CRITIQUE - Ruthless Self-Review\n\nRalph asks hard questions:\n\n### The Founder Test\n```\n\"Would [Founder Name] actually publish this\nunder their name without edits?\"\n\nIf no â†’ iterate\nIf maybe â†’ iterate\nIf yes â†’ move forward\n```\n\n### Quality Checklist\n- [ ] Does the hook stop the scroll?\n- [ ] Is there a unique angle or just regurgitation?\n- [ ] Are claims backed by data/experience?\n- [ ] Does it sound like the founder or like AI?\n- [ ] Would I share this if I saw it?\n- [ ] Does it teach something actionable?\n- [ ] Is it better than the top 3 results for this topic?\n\n### Red Flags That Trigger Iteration\n- Generic opening paragraph\n- No specific data or examples\n- Could have been written by anyone\n- Obvious AI patterns (\"In conclusion\", \"It's important to note\")\n- No personality or voice\n- Safe takes only\n\n## Phase 7: ITERATE - Until It's Good\n\nRalph doesn't ship draft 1:\n\n```\nDraft 1: Structure and ideas (usually mediocre)\nDraft 2: Voice injection (sounds more human)\nDraft 3: Sharpening (cut the fluff)\nDraft 4: Hook optimization (nail the opening)\nDraft 5: Final polish (only if needed)\n```\n\n### Iteration Triggers\n| Problem | Fix |\n|---------|-----|\n| Weak hook | Rewrite opening 5 ways, pick best |\n| Generic angle | Research deeper, find unique data |\n| Wrong voice | Re-read founder's content, try again |\n| Too long | Cut 30%, keep only essential |\n| No personality | Add specific anecdote or opinion |\n| Forgettable | Find the one surprising insight |\n\n## Usage\n\n### Analyze My Content First\n```\n/ralph-marketer analyze\n\nRalph will:\n1. Read your existing blog posts\n2. Analyze your Twitter/LinkedIn\n3. Extract voice patterns\n4. Document what makes your content unique\n5. Create a Voice DNA profile\n```\n\n### Research Before Writing\n```\n/ralph-marketer research \"AI agents for enterprise\"\n\nRalph will:\n1. Find latest data and trends\n2. Analyze competitor content\n3. Identify gaps and angles\n4. Compile research brief\n```\n\n### Write With Quality Loop\n```\n/ralph-marketer write --quality-bar high\n\nRalph will:\n1. Pick a topic from your queue\n2. Research deeply\n3. Find unique angle\n4. Write in your voice\n5. Self-critique\n6. Iterate until good\n7. Only mark complete when quality bar met\n```\n\n\n## The Promise\n\nRalph won't ship content that:\n- Sounds like it was written by AI\n- Takes the obvious angle\n- Lacks data or specificity\n- You wouldn't publish under your name\n- Is \"fine but forgettable\"\n\nIf the quality bar isn't met, Ralph keeps iterating."
              }
            ]
          },
          {
            "name": "ai-investigator",
            "description": "Enterprise AI case study analyzer with Claude and Firecrawl",
            "source": "./plugins/ai-investigator",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install ai-investigator@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "rosetta-prompt",
            "description": "Prompt optimization for different AI providers using multi-agent ReAct",
            "source": "./plugins/rosetta-prompt",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install rosetta-prompt@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "readwren",
            "description": "Multi-agent literary interview for extracting reading profiles",
            "source": "./plugins/readwren",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install readwren@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "food-tour-planner",
            "description": "AI food tour planner with DeepAgents and Google Maps",
            "source": "./plugins/food-tour-planner",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install food-tour-planner@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "actual-code",
            "description": "7-agent code assessment generator from GitHub repos",
            "source": "./plugins/actual-code",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install actual-code@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "book-training",
            "description": "Style transfer pipeline for author-style LLM training with LoRA",
            "source": "./plugins/book-training",
            "category": "ai",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install book-training@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "book-sft-pipeline",
                "description": "End-to-end system for creating supervised fine-tuning datasets from books and training style-transfer models. Covers text extraction, intelligent segmentation, synthetic instruction generation, Tinker-compatible output, LoRA training, and validation.",
                "path": "plugins/book-training/skills/book-sft-pipeline/SKILL.md",
                "frontmatter": {
                  "name": "book-sft-pipeline",
                  "description": "End-to-end system for creating supervised fine-tuning datasets from books and training style-transfer models. Covers text extraction, intelligent segmentation, synthetic instruction generation, Tinker-compatible output, LoRA training, and validation.",
                  "version": "2.0.0"
                },
                "content": "# Book SFT Pipeline\n\nA complete system for converting books into SFT datasets and training style-transfer models. This skill teaches the pipeline from raw ePub to a model that writes in any author's voice.\n\n## When to Activate\n\nActivate this skill when:\n- Building fine-tuning datasets from literary works\n- Creating author-voice or style-transfer models\n- Preparing training data for Tinker or similar SFT platforms\n- Designing text segmentation pipelines for long-form content\n- Training small models (8B or less) on limited data\n\n## Core Concepts\n\n### The Three Pillars of Book SFT\n\n**1. Intelligent Segmentation**\nText chunks must be semantically coherent. Breaking mid-sentence teaches the model to produce fragmented output. Target: 150-400 words per chunk, always at natural boundaries.\n\n**2. Diverse Instruction Generation**\nUse multiple prompt templates and system prompts to prevent overfitting. A single prompt style leads to memorization. Use 15+ prompt templates with 5+ system prompts.\n\n**3. Style Over Content**\nThe goal is learning the author's rhythm and vocabulary patterns, not memorizing plots. Synthetic instructions describe what happens without quoting the text.\n\n## Pipeline Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ORCHESTRATOR AGENT                           â”‚\nâ”‚  Coordinates pipeline phases, manages state, handles failures   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                       â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â–¼               â–¼               â–¼               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  EXTRACTION  â”‚ â”‚ SEGMENTATION â”‚ â”‚  INSTRUCTION â”‚ â”‚   DATASET    â”‚\nâ”‚    AGENT     â”‚ â”‚    AGENT     â”‚ â”‚    AGENT     â”‚ â”‚   BUILDER    â”‚\nâ”‚ ePub â†’ Text  â”‚ â”‚ Text â†’ Chunksâ”‚ â”‚ Chunks â†’     â”‚ â”‚ Pairs â†’      â”‚\nâ”‚              â”‚ â”‚ 150-400 wordsâ”‚ â”‚ Prompts      â”‚ â”‚ JSONL        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                       â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â–¼                               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   TRAINING   â”‚               â”‚  VALIDATION  â”‚\nâ”‚    AGENT     â”‚               â”‚    AGENT     â”‚\nâ”‚ LoRA on      â”‚               â”‚ AI detector  â”‚\nâ”‚ Tinker       â”‚               â”‚ Originality  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Phase 1: Text Extraction\n\n### Critical Rules\n1. **Always source ePub over PDF** - OCR errors become learned patterns\n2. **Use paragraph-level extraction** - Extract from `<p>` tags to preserve breaks\n3. **Remove front/back matter** - Copyright and TOC pollute the dataset\n\n```python\n# Extract text from ePub paragraphs\nfrom epub2 import EPub\nfrom bs4 import BeautifulSoup\n\ndef extract_epub(path):\n    book = EPub(path)\n    chapters = []\n    for item in book.flow:\n        html = book.get_chapter(item.id)\n        soup = BeautifulSoup(html, 'html.parser')\n        paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\n        chapters.append('\\n\\n'.join(p for p in paragraphs if p))\n    return '\\n\\n'.join(chapters)\n```\n\n## Phase 2: Intelligent Segmentation\n\n### Smaller Chunks + Overlap\n\nSmaller chunks (150-400 words) produce more training examples and better style transfer than larger chunks (250-650).\n\n```python\ndef segment(text, min_words=150, max_words=400):\n    paragraphs = text.split('\\n\\n')\n    chunks, buffer, buffer_words = [], [], 0\n    \n    for para in paragraphs:\n        words = len(para.split())\n        if buffer_words + words > max_words and buffer_words >= min_words:\n            chunks.append('\\n\\n'.join(buffer))\n            # Keep last paragraph for overlap\n            buffer = [buffer[-1], para] if buffer else [para]\n            buffer_words = sum(len(p.split()) for p in buffer)\n        else:\n            buffer.append(para)\n            buffer_words += words\n    \n    if buffer:\n        chunks.append('\\n\\n'.join(buffer))\n    return chunks\n```\n\n### Expected Results\n\nFor an 86,000-word book:\n- Old method (250-650 words): ~150 chunks\n- New method (150-400 + overlap): ~300 chunks\n- With 2 variants per chunk: 600+ training examples\n\n## Phase 3: Diverse Instruction Generation\n\n### The Key Insight\n\nUsing a single prompt template causes memorization. Diverse templates teach the underlying style.\n\n```python\nSYSTEM_PROMPTS = [\n    \"You are an expert creative writer capable of emulating specific literary styles.\",\n    \"You are a literary writer with deep knowledge of classic prose styles.\",\n    \"You are a creative writer skilled at emulating distinctive authorial voices.\",\n    \"You write prose that captures the essence of modernist literature.\",\n    \"You are a talented writer who can channel classic American authors.\",\n]\n\nPROMPT_TEMPLATES = [\n    \"Write a passage in the style of {author}: {desc}\",\n    \"Channel {author}'s voice to write about: {desc}\",\n    \"In {author}'s distinctive prose style, describe: {desc}\",\n    \"Write this scene as {author} would have: {desc}\",\n    \"Using {author}'s repetitive technique, describe: {desc}\",\n    \"Capture the rhythm of {author} in this passage: {desc}\",\n    \"Write like {author}: {desc}\",\n    \"In the voice of {author}, write: {desc}\",\n    \"This is a literary exercise. Write like {author}: {desc}\",\n    \"Can you write in {author}'s style? {desc}\",\n]\n```\n\n### Instruction Generation\n\n```python\nINSTRUCTION_PROMPT = \"\"\"Describe what is happening in this excerpt in 2-3 sentences.\nFocus on: characters present, actions, emotions, setting.\nDo NOT quote the text directly.\n\nExcerpt:\n{text}\n\"\"\"\n\n# Use a fast, cheap LLM (e.g., Gemini Flash)\ninstruction = llm_call(INSTRUCTION_PROMPT.format(text=chunk))\n```\n\n## Phase 4: Dataset Construction\n\n### Message Format\n\n```json\n{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are an expert creative writer...\"},\n        {\"role\": \"user\", \"content\": \"Write in the style of Author: Scene description...\"},\n        {\"role\": \"assistant\", \"content\": \"The actual book text from chunk...\"}\n    ]\n}\n```\n\n### Multiple Variants Per Chunk\n\n```python\ndef build_examples(chunk, instruction, author, variants=2):\n    examples = []\n    for i in range(variants):\n        system = SYSTEM_PROMPTS[i % len(SYSTEM_PROMPTS)]\n        template = PROMPT_TEMPLATES[(chunk.id + i) % len(PROMPT_TEMPLATES)]\n        user = template.format(author=author, desc=instruction)\n        examples.append({\"messages\": [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": chunk.text}\n        ]})\n    return examples\n```\n\n## Phase 5: LoRA Training on Tinker\n\n### Configuration\n\n```python\nCONFIG = {\n    \"model_name\": \"Qwen/Qwen3-8B-Base\",  # Base, not instruct\n    \"lora_rank\": 32,                      # 352MB adapter\n    \"learning_rate\": 5e-4,                # Higher for LoRA\n    \"batch_size\": 4,\n    \"epochs\": 3,\n}\n```\n\n### Why Base Model?\n\nUse **base** (pretrained) models, not instruction-tuned versions:\n- Base models are more malleable for new styles\n- Instruct models have patterns that resist overwriting\n- Style is a low-level pattern that base models capture better\n\n### Training Loop\n\n```python\nimport tinker\nfrom tinker import types\n\ntraining_client = await service_client.create_lora_training_client_async(\n    base_model=\"Qwen/Qwen3-8B-Base\",\n    rank=32\n)\n\nfor epoch in range(3):\n    for batch in batches:\n        await training_client.forward_backward_async(batch, loss_fn=\"cross_entropy\")\n        await training_client.optim_step_async(types.AdamParams(learning_rate=5e-4))\n\nresult = await training_client.save_weights_for_sampler_async(name=\"final\")\n```\n\n## Phase 6: Validation\n\n### Modern Scenario Test\n\nTest with scenarios that couldn't exist in the original book:\n\n```python\nTEST_PROMPTS = [\n    \"Write about a barista making lattes\",\n    \"Describe lovers communicating through text messages\",\n    \"Write about someone anxious about climate change\",\n]\n```\n\nIf the model applies style markers to modern scenarios, it learned **style**, not **content**.\n\n### Originality Verification\n\n```bash\n# Search training data for output phrases\ngrep \"specific phrase from output\" dataset.jsonl\n# Should return: No matches\n```\n\n### AI Detector Testing\n\nTest outputs with GPTZero, Pangram, or ZeroGPT.\n\n## Known Issues and Solutions\n\n### Character Name Leakage\n\n**Symptom**: Model uses original character names in new scenarios.\n**Cause**: Limited name diversity from one book.\n**Solution**: Train on multiple books or add synthetic examples.\n\n### Model Parrots Exact Phrases\n\n**Symptom**: Outputs contain exact sentences from training data.\n**Cause**: Too few prompt variations or too many epochs.\n**Solution**: Use 15+ templates, limit to 3 epochs.\n\n### Fragmented Outputs\n\n**Symptom**: Sentences feel incomplete.\n**Cause**: Poor segmentation breaking mid-thought.\n**Solution**: Always break at paragraph boundaries.\n\n## Guidelines\n\n1. **Always source ePub over PDF** - OCR errors become learned patterns\n2. **Never break mid-sentence** - Boundaries must be grammatically complete\n3. **Use diverse prompts** - 15+ templates, 5+ system prompts\n4. **Use base models** - Not instruct versions\n5. **Use smaller chunks** - 150-400 words for more examples\n6. **Reserve test set** - 50 examples minimum\n7. **Test on modern scenarios** - Proves style transfer vs memorization\n8. **Verify originality** - Grep training data for output phrases\n\n## Expected Results\n\n| Metric | Value |\n|--------|-------|\n| Training examples | 500-1000 per book |\n| Model | Qwen/Qwen3-8B-Base |\n| LoRA rank | 32 |\n| Adapter size | ~350 MB |\n| Training time | ~15 min |\n| Loss reduction | 90%+ |\n| Style transfer success | ~50% perfect |\n\n## Cost Estimate\n\n| Component | Cost |\n|-----------|------|\n| LLM (instruction generation) | ~$0.50 |\n| Tinker training (15 min) | ~$1.50 |\n| **Total** | **~$2.00** |\n\n## References\n\n- [Segmentation Strategies](./references/segmentation-strategies.md) - Text chunking patterns\n- [Tinker Format Specification](./references/tinker-format.md) - Datum structure\n- [Tinker API Documentation](./references/tinker.txt) - Full API reference\n\n---\n\n**Created**: 2025-12-26\n**Version**: 2.0.0"
              },
              {
                "name": "context-compression",
                "description": "Design and evaluate context compression strategies for long-running agent sessions. Use when agents exhaust memory, need to summarize conversation history, or when optimizing tokens-per-task rather than tokens-per-request.",
                "path": "plugins/book-training/skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "Design and evaluate context compression strategies for long-running agent sessions. Use when agents exhaust memory, need to summarize conversation history, or when optimizing tokens-per-task rather than tokens-per-request."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "evaluation",
                "description": "Build evaluation frameworks for agent systems. Use when testing agent performance, validating context engineering choices, or measuring improvements over time.",
                "path": "plugins/book-training/skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "Build evaluation frameworks for agent systems. Use when testing agent performance, validating context engineering choices, or measuring improvements over time."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goalsâ€”the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "Design and build LLM-powered projects from ideation through deployment. Use when starting new agent projects, choosing between LLM and traditional approaches, or structuring batch processing pipelines.",
                "path": "plugins/book-training/skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "Design and build LLM-powered projects from ideation through deployment. Use when starting new agent projects, choosing between LLM and traditional approaches, or structuring batch processing pipelines."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire â†’ prepare â†’ process â†’ parse â†’ render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\nâ”œâ”€â”€ raw.json         # acquire stage complete\nâ”œâ”€â”€ prompt.md        # prepare stage complete\nâ”œâ”€â”€ response.md      # process stage complete\nâ”œâ”€â”€ parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items Ã— tokens_per_item Ã— price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items Ã— tokens Ã— price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch â†’ prompt â†’ analyze â†’ parse â†’ render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "Design tools that agents can use effectively, including when to reduce tool complexity. Use when creating, optimizing, or reducing agent tool sets.",
                "path": "plugins/book-training/skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "Design tools that agents can use effectively, including when to reduce tool complexity. Use when creating, optimizing, or reducing agent tool sets."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentationâ€”they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          },
          {
            "name": "linkedin-analyzer",
            "description": "LinkedIn profile analysis with Cohere Command R+",
            "source": "./plugins/linkedin-analyzer",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install linkedin-analyzer@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "feed2context",
            "description": "One-click feed to research report for LinkedIn and X",
            "source": "./plugins/feed2context",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "muratcankoylan"
            },
            "install_commands": [
              "/plugin marketplace add EricGrill/agents-skills-plugins",
              "/plugin install feed2context@agents-skills-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-12T04:41:46Z",
              "created_at": "2026-01-09T13:32:03Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}