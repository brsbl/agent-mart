{
  "owner": {
    "id": "yonatangross",
    "display_name": "Yonatan",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/19869074?u=e72f2ffbbac18498714cf1b02f74743837bdc509&v=4",
    "url": "https://github.com/yonatangross",
    "bio": "Building Agents.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 89,
      "total_stars": 17,
      "total_forks": 3
    }
  },
  "repos": [
    {
      "full_name": "yonatangross/skillforge-claude-plugin",
      "url": "https://github.com/yonatangross/skillforge-claude-plugin",
      "description": "The Complete AI Development Toolkit for Claude Code - 78 skills, 20 agents, 12 commands, 90 hooks, 7 modules. RAG, LangGraph, React 19, security, testing patterns.",
      "homepage": "https://github.com/yonatangross/skillforge-claude-plugin#readme",
      "signals": {
        "stars": 17,
        "forks": 3,
        "pushed_at": "2026-01-12T07:32:48Z",
        "created_at": "2025-12-31T05:58:52Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 755
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 346
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/agent-registry.json.archived",
          "type": "blob",
          "size": 53839
        },
        {
          "path": ".claude/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/agents/backend-system-architect.md",
          "type": "blob",
          "size": 8474
        },
        {
          "path": ".claude/agents/business-case-builder.md",
          "type": "blob",
          "size": 8847
        },
        {
          "path": ".claude/agents/code-quality-reviewer.md",
          "type": "blob",
          "size": 12599
        },
        {
          "path": ".claude/agents/data-pipeline-engineer.md",
          "type": "blob",
          "size": 6092
        },
        {
          "path": ".claude/agents/database-engineer.md",
          "type": "blob",
          "size": 5980
        },
        {
          "path": ".claude/agents/debug-investigator.md",
          "type": "blob",
          "size": 7511
        },
        {
          "path": ".claude/agents/frontend-ui-developer.md",
          "type": "blob",
          "size": 12924
        },
        {
          "path": ".claude/agents/llm-integrator.md",
          "type": "blob",
          "size": 6937
        },
        {
          "path": ".claude/agents/market-intelligence.md",
          "type": "blob",
          "size": 6724
        },
        {
          "path": ".claude/agents/metrics-architect.md",
          "type": "blob",
          "size": 11448
        },
        {
          "path": ".claude/agents/prioritization-analyst.md",
          "type": "blob",
          "size": 8625
        },
        {
          "path": ".claude/agents/product-strategist.md",
          "type": "blob",
          "size": 9175
        },
        {
          "path": ".claude/agents/rapid-ui-designer.md",
          "type": "blob",
          "size": 9464
        },
        {
          "path": ".claude/agents/requirements-translator.md",
          "type": "blob",
          "size": 11188
        },
        {
          "path": ".claude/agents/security-auditor.md",
          "type": "blob",
          "size": 6279
        },
        {
          "path": ".claude/agents/security-layer-auditor.md",
          "type": "blob",
          "size": 11812
        },
        {
          "path": ".claude/agents/system-design-reviewer.md",
          "type": "blob",
          "size": 9027
        },
        {
          "path": ".claude/agents/test-generator.md",
          "type": "blob",
          "size": 7759
        },
        {
          "path": ".claude/agents/ux-researcher.md",
          "type": "blob",
          "size": 8137
        },
        {
          "path": ".claude/agents/workflow-architect.md",
          "type": "blob",
          "size": 9809
        },
        {
          "path": ".claude/context",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/agents/backend-system-architect.json",
          "type": "blob",
          "size": 1469
        },
        {
          "path": ".claude/context/agents/code-quality-reviewer.json",
          "type": "blob",
          "size": 1312
        },
        {
          "path": ".claude/context/agents/frontend-ui-developer.json",
          "type": "blob",
          "size": 1420
        },
        {
          "path": ".claude/context/agents/llm-integrator.json",
          "type": "blob",
          "size": 1442
        },
        {
          "path": ".claude/context/agents/security-auditor.json",
          "type": "blob",
          "size": 1231
        },
        {
          "path": ".claude/context/agents/test-generator.json",
          "type": "blob",
          "size": 1217
        },
        {
          "path": ".claude/context/agents/workflow-architect.json",
          "type": "blob",
          "size": 1230
        },
        {
          "path": ".claude/context/archive",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/archive/sessions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/archive/sessions/session-20260110-legacy.json",
          "type": "blob",
          "size": 1647
        },
        {
          "path": ".claude/context/identity.json",
          "type": "blob",
          "size": 1766
        },
        {
          "path": ".claude/context/knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/knowledge/blockers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/knowledge/blockers/current.json",
          "type": "blob",
          "size": 698
        },
        {
          "path": ".claude/context/knowledge/decisions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/knowledge/decisions/active.json",
          "type": "blob",
          "size": 2645
        },
        {
          "path": ".claude/context/knowledge/index.json",
          "type": "blob",
          "size": 2100
        },
        {
          "path": ".claude/context/knowledge/patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/knowledge/patterns/established.json",
          "type": "blob",
          "size": 2724
        },
        {
          "path": ".claude/context/knowledge/patterns/pattern-registry.json",
          "type": "blob",
          "size": 11704
        },
        {
          "path": ".claude/context/session",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/context/session/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": ".claude/coordination",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/coordination/bin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/coordination/bin/coord-decisions",
          "type": "blob",
          "size": 4794
        },
        {
          "path": ".claude/coordination/bin/coord-lock",
          "type": "blob",
          "size": 3588
        },
        {
          "path": ".claude/coordination/bin/coord-status",
          "type": "blob",
          "size": 3281
        },
        {
          "path": ".claude/coordination/config.json",
          "type": "blob",
          "size": 3786
        },
        {
          "path": ".claude/coordination/decision-log.json",
          "type": "blob",
          "size": 22601
        },
        {
          "path": ".claude/coordination/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/coordination/examples/conflict-detection-demo.sh",
          "type": "blob",
          "size": 3082
        },
        {
          "path": ".claude/coordination/examples/multi-instance-demo.sh",
          "type": "blob",
          "size": 3848
        },
        {
          "path": ".claude/coordination/examples/stale-instance-demo.sh",
          "type": "blob",
          "size": 2855
        },
        {
          "path": ".claude/coordination/lib",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/coordination/lib/coordination.sh",
          "type": "blob",
          "size": 20623
        },
        {
          "path": ".claude/coordination/registry.json",
          "type": "blob",
          "size": 322
        },
        {
          "path": ".claude/coordination/schema.sql",
          "type": "blob",
          "size": 14536
        },
        {
          "path": ".claude/coordination/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/coordination/schemas/decision-log.schema.json",
          "type": "blob",
          "size": 3373
        },
        {
          "path": ".claude/coordination/schemas/file-lock.schema.json",
          "type": "blob",
          "size": 1638
        },
        {
          "path": ".claude/coordination/schemas/heartbeat.schema.json",
          "type": "blob",
          "size": 847
        },
        {
          "path": ".claude/coordination/schemas/work-registry.schema.json",
          "type": "blob",
          "size": 3370
        },
        {
          "path": ".claude/coordination/work-registry.json",
          "type": "blob",
          "size": null
        },
        {
          "path": ".claude/defaults",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/defaults/config.json",
          "type": "blob",
          "size": 569
        },
        {
          "path": ".claude/defaults/presets",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/defaults/presets/hooks-only.json",
          "type": "blob",
          "size": 582
        },
        {
          "path": ".claude/defaults/presets/lite.json",
          "type": "blob",
          "size": 650
        },
        {
          "path": ".claude/defaults/presets/standard.json",
          "type": "blob",
          "size": 571
        },
        {
          "path": ".claude/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/docs/release-channels.md",
          "type": "blob",
          "size": 2655
        },
        {
          "path": ".claude/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/_lib",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/_lib/common.sh",
          "type": "blob",
          "size": 16182
        },
        {
          "path": ".claude/hooks/_lib/coordination.sh",
          "type": "blob",
          "size": 9476
        },
        {
          "path": ".claude/hooks/agent",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/agent/auto-spawn-quality.sh",
          "type": "blob",
          "size": 9493
        },
        {
          "path": ".claude/hooks/agent/context-publisher.sh",
          "type": "blob",
          "size": 3384
        },
        {
          "path": ".claude/hooks/agent/feedback-loop.sh",
          "type": "blob",
          "size": 9417
        },
        {
          "path": ".claude/hooks/agent/handoff-preparer.sh",
          "type": "blob",
          "size": 6498
        },
        {
          "path": ".claude/hooks/agent/multi-claude-verifier.sh",
          "type": "blob",
          "size": 5793
        },
        {
          "path": ".claude/hooks/agent/output-validator.sh",
          "type": "blob",
          "size": 2551
        },
        {
          "path": ".claude/hooks/hooks.json",
          "type": "blob",
          "size": 8223
        },
        {
          "path": ".claude/hooks/lifecycle",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/lifecycle/context-loader.sh",
          "type": "blob",
          "size": 5126
        },
        {
          "path": ".claude/hooks/lifecycle/coordination-cleanup.sh",
          "type": "blob",
          "size": 1695
        },
        {
          "path": ".claude/hooks/lifecycle/coordination-init.sh",
          "type": "blob",
          "size": 2400
        },
        {
          "path": ".claude/hooks/lifecycle/instance-heartbeat.sh",
          "type": "blob",
          "size": 1979
        },
        {
          "path": ".claude/hooks/lifecycle/multi-instance-init.sh",
          "type": "blob",
          "size": 9272
        },
        {
          "path": ".claude/hooks/lifecycle/session-cleanup.sh",
          "type": "blob",
          "size": 1461
        },
        {
          "path": ".claude/hooks/lifecycle/session-context-loader.sh",
          "type": "blob",
          "size": 2950
        },
        {
          "path": ".claude/hooks/lifecycle/session-env-setup.sh",
          "type": "blob",
          "size": 2247
        },
        {
          "path": ".claude/hooks/lifecycle/session-metrics-summary.sh",
          "type": "blob",
          "size": 815
        },
        {
          "path": ".claude/hooks/lifecycle/startup-dispatcher.sh",
          "type": "blob",
          "size": 2397
        },
        {
          "path": ".claude/hooks/notification",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/notification/desktop.sh",
          "type": "blob",
          "size": 1478
        },
        {
          "path": ".claude/hooks/notification/sound.sh",
          "type": "blob",
          "size": 987
        },
        {
          "path": ".claude/hooks/permission",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/permission/auto-approve-project-writes.sh",
          "type": "blob",
          "size": 1350
        },
        {
          "path": ".claude/hooks/permission/auto-approve-readonly.sh",
          "type": "blob",
          "size": 498
        },
        {
          "path": ".claude/hooks/permission/auto-approve-safe-bash.sh",
          "type": "blob",
          "size": 1526
        },
        {
          "path": ".claude/hooks/posttool",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/posttool/Write",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/posttool/Write/coverage-predictor.sh",
          "type": "blob",
          "size": 1777
        },
        {
          "path": ".claude/hooks/posttool/Write/release-lock-on-commit.sh",
          "type": "blob",
          "size": 755
        },
        {
          "path": ".claude/hooks/posttool/audit-logger.sh",
          "type": "blob",
          "size": 1373
        },
        {
          "path": ".claude/hooks/posttool/context-budget-monitor.sh",
          "type": "blob",
          "size": 4612
        },
        {
          "path": ".claude/hooks/posttool/coordination-heartbeat.sh",
          "type": "blob",
          "size": 970
        },
        {
          "path": ".claude/hooks/posttool/dispatcher.sh",
          "type": "blob",
          "size": 2930
        },
        {
          "path": ".claude/hooks/posttool/error-collector.sh",
          "type": "blob",
          "size": 3877
        },
        {
          "path": ".claude/hooks/posttool/error-tracker.sh",
          "type": "blob",
          "size": 1189
        },
        {
          "path": ".claude/hooks/posttool/session-metrics.sh",
          "type": "blob",
          "size": 1390
        },
        {
          "path": ".claude/hooks/posttool/write-edit",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/posttool/write-edit/file-lock-release.sh",
          "type": "blob",
          "size": 1930
        },
        {
          "path": ".claude/hooks/pretool",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/Edit",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/Edit/file-lock-check.sh",
          "type": "blob",
          "size": 2121
        },
        {
          "path": ".claude/hooks/pretool/Write",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/Write/architecture-change-detector.sh",
          "type": "blob",
          "size": 1600
        },
        {
          "path": ".claude/hooks/pretool/Write/file-lock-check.sh",
          "type": "blob",
          "size": 2126
        },
        {
          "path": ".claude/hooks/pretool/Write/security-pattern-validator.sh",
          "type": "blob",
          "size": 2376
        },
        {
          "path": ".claude/hooks/pretool/bash-dispatcher.sh",
          "type": "blob",
          "size": 2764
        },
        {
          "path": ".claude/hooks/pretool/bash",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/bash/ci-simulation.sh",
          "type": "blob",
          "size": 1723
        },
        {
          "path": ".claude/hooks/pretool/bash/conflict-predictor.sh",
          "type": "blob",
          "size": 3272
        },
        {
          "path": ".claude/hooks/pretool/bash/error-pattern-warner.sh",
          "type": "blob",
          "size": 2962
        },
        {
          "path": ".claude/hooks/pretool/bash/git-branch-protection.sh",
          "type": "blob",
          "size": 1722
        },
        {
          "path": ".claude/hooks/pretool/bash/issue-docs-requirement.sh",
          "type": "blob",
          "size": 2243
        },
        {
          "path": ".claude/hooks/pretool/bash/multi-instance-quality-gate.sh",
          "type": "blob",
          "size": 9195
        },
        {
          "path": ".claude/hooks/pretool/dispatcher.sh",
          "type": "blob",
          "size": 2945
        },
        {
          "path": ".claude/hooks/pretool/input-mod",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/input-mod/bash-defaults.sh",
          "type": "blob",
          "size": 2176
        },
        {
          "path": ".claude/hooks/pretool/input-mod/path-normalizer.sh",
          "type": "blob",
          "size": 2469
        },
        {
          "path": ".claude/hooks/pretool/input-mod/write-headers.sh",
          "type": "blob",
          "size": 3462
        },
        {
          "path": ".claude/hooks/pretool/mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/mcp/context7-tracker.sh",
          "type": "blob",
          "size": 1121
        },
        {
          "path": ".claude/hooks/pretool/mcp/memory-validator.sh",
          "type": "blob",
          "size": 902
        },
        {
          "path": ".claude/hooks/pretool/mcp/playwright-safety.sh",
          "type": "blob",
          "size": 996
        },
        {
          "path": ".claude/hooks/pretool/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/skill/skill-tracker.sh",
          "type": "blob",
          "size": 832
        },
        {
          "path": ".claude/hooks/pretool/task",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/task/context-gate.sh",
          "type": "blob",
          "size": 5837
        },
        {
          "path": ".claude/hooks/pretool/task/subagent-validator.sh",
          "type": "blob",
          "size": 2946
        },
        {
          "path": ".claude/hooks/pretool/write-dispatcher.sh",
          "type": "blob",
          "size": 3224
        },
        {
          "path": ".claude/hooks/pretool/write-edit",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/pretool/write-edit/file-guard.sh",
          "type": "blob",
          "size": 2312
        },
        {
          "path": ".claude/hooks/pretool/write-edit/file-lock-check.sh",
          "type": "blob",
          "size": 3157
        },
        {
          "path": ".claude/hooks/pretool/write-edit/multi-instance-lock.sh",
          "type": "blob",
          "size": 6516
        },
        {
          "path": ".claude/hooks/prompt",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/prompt/context-injector.sh",
          "type": "blob",
          "size": 1296
        },
        {
          "path": ".claude/hooks/prompt/prompt-dispatcher.sh",
          "type": "blob",
          "size": 1475
        },
        {
          "path": ".claude/hooks/prompt/todo-enforcer.sh",
          "type": "blob",
          "size": 1065
        },
        {
          "path": ".claude/hooks/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/skill/backend-file-naming.sh",
          "type": "blob",
          "size": 6606
        },
        {
          "path": ".claude/hooks/skill/backend-layer-validator.sh",
          "type": "blob",
          "size": 10084
        },
        {
          "path": ".claude/hooks/skill/coverage-check.sh",
          "type": "blob",
          "size": 1203
        },
        {
          "path": ".claude/hooks/skill/coverage-threshold-gate.sh",
          "type": "blob",
          "size": 4399
        },
        {
          "path": ".claude/hooks/skill/cross-instance-test-validator.sh",
          "type": "blob",
          "size": 11912
        },
        {
          "path": ".claude/hooks/skill/design-decision-saver.sh",
          "type": "blob",
          "size": 786
        },
        {
          "path": ".claude/hooks/skill/di-pattern-enforcer.sh",
          "type": "blob",
          "size": 7926
        },
        {
          "path": ".claude/hooks/skill/duplicate-code-detector.sh",
          "type": "blob",
          "size": 10649
        },
        {
          "path": ".claude/hooks/skill/eval-metrics-collector.sh",
          "type": "blob",
          "size": 1029
        },
        {
          "path": ".claude/hooks/skill/evidence-collector.sh",
          "type": "blob",
          "size": 1237
        },
        {
          "path": ".claude/hooks/skill/import-direction-enforcer.sh",
          "type": "blob",
          "size": 8484
        },
        {
          "path": ".claude/hooks/skill/merge-conflict-predictor.sh",
          "type": "blob",
          "size": 12196
        },
        {
          "path": ".claude/hooks/skill/merge-readiness-checker.sh",
          "type": "blob",
          "size": 11945
        },
        {
          "path": ".claude/hooks/skill/migration-validator.sh",
          "type": "blob",
          "size": 1079
        },
        {
          "path": ".claude/hooks/skill/pattern-consistency-enforcer.sh",
          "type": "blob",
          "size": 13371
        },
        {
          "path": ".claude/hooks/skill/redact-secrets.sh",
          "type": "blob",
          "size": 841
        },
        {
          "path": ".claude/hooks/skill/review-summary-generator.sh",
          "type": "blob",
          "size": 905
        },
        {
          "path": ".claude/hooks/skill/security-summary.sh",
          "type": "blob",
          "size": 762
        },
        {
          "path": ".claude/hooks/skill/structure-location-validator.sh",
          "type": "blob",
          "size": 8742
        },
        {
          "path": ".claude/hooks/skill/test-location-validator.sh",
          "type": "blob",
          "size": 5259
        },
        {
          "path": ".claude/hooks/skill/test-pattern-validator.sh",
          "type": "blob",
          "size": 9165
        },
        {
          "path": ".claude/hooks/skill/test-runner.sh",
          "type": "blob",
          "size": 1331
        },
        {
          "path": ".claude/hooks/stop",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/stop/auto-save-context.sh",
          "type": "blob",
          "size": 2162
        },
        {
          "path": ".claude/hooks/stop/cleanup-instance.sh",
          "type": "blob",
          "size": 1717
        },
        {
          "path": ".claude/hooks/stop/context-compressor.sh",
          "type": "blob",
          "size": 6185
        },
        {
          "path": ".claude/hooks/stop/full-test-suite.sh",
          "type": "blob",
          "size": 3534
        },
        {
          "path": ".claude/hooks/stop/llm-code-review.sh",
          "type": "blob",
          "size": 4611
        },
        {
          "path": ".claude/hooks/stop/multi-instance-cleanup.sh",
          "type": "blob",
          "size": 6093
        },
        {
          "path": ".claude/hooks/stop/security-scan-aggregator.sh",
          "type": "blob",
          "size": 5380
        },
        {
          "path": ".claude/hooks/stop/stop-dispatcher.sh",
          "type": "blob",
          "size": 1319
        },
        {
          "path": ".claude/hooks/stop/task-completion-check.sh",
          "type": "blob",
          "size": 738
        },
        {
          "path": ".claude/hooks/subagent-start",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/subagent-start/agent-context-loader.sh",
          "type": "blob",
          "size": 4203
        },
        {
          "path": ".claude/hooks/subagent-start/model-enforcer.sh",
          "type": "blob",
          "size": 3956
        },
        {
          "path": ".claude/hooks/subagent-start/subagent-context-stager.sh",
          "type": "blob",
          "size": 4221
        },
        {
          "path": ".claude/hooks/subagent-start/subagent-resource-allocator.sh",
          "type": "blob",
          "size": 2860
        },
        {
          "path": ".claude/hooks/subagent-stop",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/hooks/subagent-stop/subagent-completion-tracker.sh",
          "type": "blob",
          "size": 717
        },
        {
          "path": ".claude/hooks/subagent-stop/subagent-quality-gate.sh",
          "type": "blob",
          "size": 1067
        },
        {
          "path": ".claude/instructions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/instructions/context-initialization.md",
          "type": "blob",
          "size": 5627
        },
        {
          "path": ".claude/permissions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/permissions/profiles",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/permissions/profiles/README.md",
          "type": "blob",
          "size": 1597
        },
        {
          "path": ".claude/permissions/profiles/enterprise.json",
          "type": "blob",
          "size": 1347
        },
        {
          "path": ".claude/permissions/profiles/secure.json",
          "type": "blob",
          "size": 929
        },
        {
          "path": ".claude/permissions/profiles/team.json",
          "type": "blob",
          "size": 1580
        },
        {
          "path": ".claude/policies",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/policies/agent-output-policy.md",
          "type": "blob",
          "size": 9964
        },
        {
          "path": ".claude/rules",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/rules/error_rules.json",
          "type": "blob",
          "size": 1875
        },
        {
          "path": ".claude/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/schemas/agent-registry.schema.json",
          "type": "blob",
          "size": 6358
        },
        {
          "path": ".claude/schemas/config.schema.json",
          "type": "blob",
          "size": 4401
        },
        {
          "path": ".claude/schemas/coordination-registry.schema.json",
          "type": "blob",
          "size": 3035
        },
        {
          "path": ".claude/schemas/hook-output.schema.json",
          "type": "blob",
          "size": 1330
        },
        {
          "path": ".claude/schemas/marketplace.schema.json",
          "type": "blob",
          "size": 2684
        },
        {
          "path": ".claude/schemas/skill-capabilities.schema.json",
          "type": "blob",
          "size": 3062
        },
        {
          "path": ".claude/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/scripts/analyze_errors.py",
          "type": "blob",
          "size": 9315
        },
        {
          "path": ".claude/scripts/config-loader.sh",
          "type": "blob",
          "size": 12589
        },
        {
          "path": ".claude/settings.json",
          "type": "blob",
          "size": 8808
        },
        {
          "path": ".claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/add-golden",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/add-golden/SKILL.md",
          "type": "blob",
          "size": 2483
        },
        {
          "path": ".claude/skills/add-golden/capabilities.json",
          "type": "blob",
          "size": 1075
        },
        {
          "path": ".claude/skills/add-golden/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/add-golden/references/quality-scoring.md",
          "type": "blob",
          "size": 2350
        },
        {
          "path": ".claude/skills/agent-loops",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/agent-loops/SKILL.md",
          "type": "blob",
          "size": 5675
        },
        {
          "path": ".claude/skills/agent-loops/capabilities.json",
          "type": "blob",
          "size": 565
        },
        {
          "path": ".claude/skills/agent-loops/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/agent-loops/templates/agent-workflow-template.ts",
          "type": "blob",
          "size": 11244
        },
        {
          "path": ".claude/skills/api-design-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-design-framework/SKILL.md",
          "type": "blob",
          "size": 22525
        },
        {
          "path": ".claude/skills/api-design-framework/capabilities.json",
          "type": "blob",
          "size": 827
        },
        {
          "path": ".claude/skills/api-design-framework/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-design-framework/checklists/api-design-checklist.md",
          "type": "blob",
          "size": 11117
        },
        {
          "path": ".claude/skills/api-design-framework/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-design-framework/examples/skillforge-api-design.md",
          "type": "blob",
          "size": 14658
        },
        {
          "path": ".claude/skills/api-design-framework/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-design-framework/references/rest-patterns.md",
          "type": "blob",
          "size": 14636
        },
        {
          "path": ".claude/skills/api-design-framework/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-design-framework/templates/asyncapi-template.yaml",
          "type": "blob",
          "size": 14838
        },
        {
          "path": ".claude/skills/api-design-framework/templates/openapi-template.yaml",
          "type": "blob",
          "size": 14208
        },
        {
          "path": ".claude/skills/api-versioning",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-versioning/SKILL.md",
          "type": "blob",
          "size": 10058
        },
        {
          "path": ".claude/skills/api-versioning/capabilities.json",
          "type": "blob",
          "size": 660
        },
        {
          "path": ".claude/skills/api-versioning/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-versioning/checklists/versioning-checklist.md",
          "type": "blob",
          "size": 5048
        },
        {
          "path": ".claude/skills/api-versioning/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-versioning/examples/fastapi-versioning.md",
          "type": "blob",
          "size": 10284
        },
        {
          "path": ".claude/skills/api-versioning/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-versioning/references/versioning-strategies.md",
          "type": "blob",
          "size": 8576
        },
        {
          "path": ".claude/skills/api-versioning/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/api-versioning/templates/fastapi-versioned-router.py",
          "type": "blob",
          "size": 10881
        },
        {
          "path": ".claude/skills/architecture-decision-record",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/architecture-decision-record/SKILL.md",
          "type": "blob",
          "size": 7570
        },
        {
          "path": ".claude/skills/architecture-decision-record/capabilities.json",
          "type": "blob",
          "size": 664
        },
        {
          "path": ".claude/skills/architecture-decision-record/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/architecture-decision-record/checklists/adr-review-checklist.md",
          "type": "blob",
          "size": 11002
        },
        {
          "path": ".claude/skills/architecture-decision-record/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/architecture-decision-record/examples/adr-0001-adopt-microservices.md",
          "type": "blob",
          "size": 13657
        },
        {
          "path": ".claude/skills/architecture-decision-record/examples/adr-0002-choose-postgresql.md",
          "type": "blob",
          "size": 8409
        },
        {
          "path": ".claude/skills/architecture-decision-record/examples/adr-0003-api-versioning-strategy.md",
          "type": "blob",
          "size": 10772
        },
        {
          "path": ".claude/skills/architecture-decision-record/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/architecture-decision-record/references/adr-best-practices.md",
          "type": "blob",
          "size": 38344
        },
        {
          "path": ".claude/skills/architecture-decision-record/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/architecture-decision-record/templates/adr-frontmatter.yaml",
          "type": "blob",
          "size": 2305
        },
        {
          "path": ".claude/skills/architecture-decision-record/templates/adr-template.md",
          "type": "blob",
          "size": 2646
        },
        {
          "path": ".claude/skills/ascii-visualizer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ascii-visualizer/SKILL.md",
          "type": "blob",
          "size": 2805
        },
        {
          "path": ".claude/skills/ascii-visualizer/capabilities.json",
          "type": "blob",
          "size": 772
        },
        {
          "path": ".claude/skills/ascii-visualizer/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ascii-visualizer/checklists/diagram-creation-checklist.md",
          "type": "blob",
          "size": 14167
        },
        {
          "path": ".claude/skills/ascii-visualizer/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ascii-visualizer/examples/skillforge-architecture-diagrams.md",
          "type": "blob",
          "size": 65829
        },
        {
          "path": ".claude/skills/ascii-visualizer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ascii-visualizer/references/architecture-diagrams.md",
          "type": "blob",
          "size": 1809
        },
        {
          "path": ".claude/skills/ascii-visualizer/references/comparisons.md",
          "type": "blob",
          "size": 1497
        },
        {
          "path": ".claude/skills/ascii-visualizer/references/file-trees.md",
          "type": "blob",
          "size": 1146
        },
        {
          "path": ".claude/skills/ascii-visualizer/references/workflows.md",
          "type": "blob",
          "size": 1186
        },
        {
          "path": ".claude/skills/ascii-visualizer/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ascii-visualizer/templates/diagram-templates.md",
          "type": "blob",
          "size": 41236
        },
        {
          "path": ".claude/skills/auth-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/auth-patterns/SKILL.md",
          "type": "blob",
          "size": 4943
        },
        {
          "path": ".claude/skills/auth-patterns/capabilities.json",
          "type": "blob",
          "size": 742
        },
        {
          "path": ".claude/skills/auth-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/auth-patterns/checklists/auth-checklist.md",
          "type": "blob",
          "size": 3058
        },
        {
          "path": ".claude/skills/auth-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/auth-patterns/examples/auth-implementations.md",
          "type": "blob",
          "size": 7983
        },
        {
          "path": ".claude/skills/auth-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/auth-patterns/references/oauth-2.1-passkeys.md",
          "type": "blob",
          "size": 7710
        },
        {
          "path": ".claude/skills/auth-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/auth-patterns/templates/auth-middleware-template.py",
          "type": "blob",
          "size": 7217
        },
        {
          "path": ".claude/skills/backend-architecture-enforcer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/backend-architecture-enforcer/SKILL.md",
          "type": "blob",
          "size": 15541
        },
        {
          "path": ".claude/skills/backend-architecture-enforcer/capabilities.json",
          "type": "blob",
          "size": 713
        },
        {
          "path": ".claude/skills/backend-architecture-enforcer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/backend-architecture-enforcer/references/violation-examples.md",
          "type": "blob",
          "size": 12991
        },
        {
          "path": ".claude/skills/background-jobs",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/background-jobs/SKILL.md",
          "type": "blob",
          "size": 9379
        },
        {
          "path": ".claude/skills/background-jobs/capabilities.json",
          "type": "blob",
          "size": 680
        },
        {
          "path": ".claude/skills/background-jobs/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/background-jobs/checklists/background-jobs-checklist.md",
          "type": "blob",
          "size": 5756
        },
        {
          "path": ".claude/skills/background-jobs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/background-jobs/examples/arq-fastapi.md",
          "type": "blob",
          "size": 10704
        },
        {
          "path": ".claude/skills/background-jobs/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/background-jobs/references/task-queue-patterns.md",
          "type": "blob",
          "size": 6334
        },
        {
          "path": ".claude/skills/background-jobs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/background-jobs/templates/arq-worker-template.py",
          "type": "blob",
          "size": 8884
        },
        {
          "path": ".claude/skills/brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/brainstorming/SKILL.md",
          "type": "blob",
          "size": 12667
        },
        {
          "path": ".claude/skills/brainstorming/capabilities.json",
          "type": "blob",
          "size": 965
        },
        {
          "path": ".claude/skills/brainstorming/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/brainstorming/checklists/brainstorm-session-checklist.md",
          "type": "blob",
          "size": 9112
        },
        {
          "path": ".claude/skills/brainstorming/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/brainstorming/examples/skillforge-feature-brainstorm.md",
          "type": "blob",
          "size": 9405
        },
        {
          "path": ".claude/skills/brainstorming/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/brainstorming/references/example-session-auth.md",
          "type": "blob",
          "size": 5860
        },
        {
          "path": ".claude/skills/brainstorming/references/example-session-dashboard.md",
          "type": "blob",
          "size": 6814
        },
        {
          "path": ".claude/skills/brainstorming/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/brainstorming/templates/decision-matrix-template.md",
          "type": "blob",
          "size": 3439
        },
        {
          "path": ".claude/skills/brainstorming/templates/design-doc-template.md",
          "type": "blob",
          "size": 3344
        },
        {
          "path": ".claude/skills/browser-content-capture",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/browser-content-capture/SKILL.md",
          "type": "blob",
          "size": 8790
        },
        {
          "path": ".claude/skills/browser-content-capture/capabilities.json",
          "type": "blob",
          "size": 760
        },
        {
          "path": ".claude/skills/browser-content-capture/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/browser-content-capture/checklists/browser-capture-checklist.md",
          "type": "blob",
          "size": 4297
        },
        {
          "path": ".claude/skills/browser-content-capture/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/browser-content-capture/examples/capture-react-docs.md",
          "type": "blob",
          "size": 5817
        },
        {
          "path": ".claude/skills/browser-content-capture/examples/skillforge-integration.md",
          "type": "blob",
          "size": 8215
        },
        {
          "path": ".claude/skills/browser-content-capture/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/browser-content-capture/references/auth-handling.md",
          "type": "blob",
          "size": 8095
        },
        {
          "path": ".claude/skills/browser-content-capture/references/mcp-tools.md",
          "type": "blob",
          "size": 7921
        },
        {
          "path": ".claude/skills/browser-content-capture/references/multi-page-crawl.md",
          "type": "blob",
          "size": 12239
        },
        {
          "path": ".claude/skills/browser-content-capture/references/spa-extraction.md",
          "type": "blob",
          "size": 8212
        },
        {
          "path": ".claude/skills/browser-content-capture/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/browser-content-capture/templates/capture-workflow.py",
          "type": "blob",
          "size": 5830
        },
        {
          "path": ".claude/skills/browser-content-capture/templates/extract-article.js",
          "type": "blob",
          "size": 5461
        },
        {
          "path": ".claude/skills/browser-content-capture/templates/queue-to-skillforge.py",
          "type": "blob",
          "size": 7693
        },
        {
          "path": ".claude/skills/cache-cost-tracking",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/cache-cost-tracking/SKILL.md",
          "type": "blob",
          "size": 5820
        },
        {
          "path": ".claude/skills/cache-cost-tracking/capabilities.json",
          "type": "blob",
          "size": 638
        },
        {
          "path": ".claude/skills/cache-cost-tracking/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/cache-cost-tracking/checklists/cache-cost-checklist.md",
          "type": "blob",
          "size": 610
        },
        {
          "path": ".claude/skills/caching-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/caching-strategies/SKILL.md",
          "type": "blob",
          "size": 10728
        },
        {
          "path": ".claude/skills/caching-strategies/capabilities.json",
          "type": "blob",
          "size": 749
        },
        {
          "path": ".claude/skills/caching-strategies/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/caching-strategies/checklists/caching-checklist.md",
          "type": "blob",
          "size": 4793
        },
        {
          "path": ".claude/skills/caching-strategies/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/caching-strategies/examples/redis-caching-fastapi.md",
          "type": "blob",
          "size": 12799
        },
        {
          "path": ".claude/skills/caching-strategies/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/caching-strategies/references/cache-patterns.md",
          "type": "blob",
          "size": 10247
        },
        {
          "path": ".claude/skills/caching-strategies/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/caching-strategies/templates/redis-cache-service.py",
          "type": "blob",
          "size": 13096
        },
        {
          "path": ".claude/skills/clean-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/clean-architecture/SKILL.md",
          "type": "blob",
          "size": 11699
        },
        {
          "path": ".claude/skills/clean-architecture/capabilities.json",
          "type": "blob",
          "size": 718
        },
        {
          "path": ".claude/skills/clean-architecture/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/clean-architecture/checklists/solid-checklist.md",
          "type": "blob",
          "size": 4461
        },
        {
          "path": ".claude/skills/clean-architecture/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/clean-architecture/examples/fastapi-clean-architecture.md",
          "type": "blob",
          "size": 12818
        },
        {
          "path": ".claude/skills/clean-architecture/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/clean-architecture/references/hexagonal-architecture.md",
          "type": "blob",
          "size": 10018
        },
        {
          "path": ".claude/skills/clean-architecture/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/clean-architecture/templates/domain-entity-template.py",
          "type": "blob",
          "size": 9431
        },
        {
          "path": ".claude/skills/code-review-playbook",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/code-review-playbook/SKILL.md",
          "type": "blob",
          "size": 9159
        },
        {
          "path": ".claude/skills/code-review-playbook/capabilities.json",
          "type": "blob",
          "size": 733
        },
        {
          "path": ".claude/skills/code-review-playbook/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/code-review-playbook/checklists/code-review-checklist.md",
          "type": "blob",
          "size": 13292
        },
        {
          "path": ".claude/skills/code-review-playbook/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/code-review-playbook/examples/conventional-comments.md",
          "type": "blob",
          "size": 2784
        },
        {
          "path": ".claude/skills/code-review-playbook/examples/pr-review-walkthrough.md",
          "type": "blob",
          "size": 2445
        },
        {
          "path": ".claude/skills/code-review-playbook/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/code-review-playbook/references/review-patterns.md",
          "type": "blob",
          "size": 8053
        },
        {
          "path": ".claude/skills/code-review-playbook/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/code-review-playbook/templates/pr-template.md",
          "type": "blob",
          "size": 7189
        },
        {
          "path": ".claude/skills/code-review-playbook/templates/review-feedback-template.md",
          "type": "blob",
          "size": 7944
        },
        {
          "path": ".claude/skills/commit",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/commit/SKILL.md",
          "type": "blob",
          "size": 2384
        },
        {
          "path": ".claude/skills/commit/capabilities.json",
          "type": "blob",
          "size": 1067
        },
        {
          "path": ".claude/skills/commit/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/commit/references/conventional-commits.md",
          "type": "blob",
          "size": 1266
        },
        {
          "path": ".claude/skills/commit/references/recovery.md",
          "type": "blob",
          "size": 819
        },
        {
          "path": ".claude/skills/configure",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/configure/SKILL.md",
          "type": "blob",
          "size": 2542
        },
        {
          "path": ".claude/skills/configure/capabilities.json",
          "type": "blob",
          "size": 1028
        },
        {
          "path": ".claude/skills/configure/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/configure/references/mcp-config.md",
          "type": "blob",
          "size": 2325
        },
        {
          "path": ".claude/skills/configure/references/presets.md",
          "type": "blob",
          "size": 2527
        },
        {
          "path": ".claude/skills/context-compression",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-compression/SKILL.md",
          "type": "blob",
          "size": 17872
        },
        {
          "path": ".claude/skills/context-compression/capabilities.json",
          "type": "blob",
          "size": 688
        },
        {
          "path": ".claude/skills/context-compression/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-compression/checklists/compression-checklist.md",
          "type": "blob",
          "size": 6185
        },
        {
          "path": ".claude/skills/context-compression/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-compression/references/compression-strategies.md",
          "type": "blob",
          "size": 9303
        },
        {
          "path": ".claude/skills/context-compression/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-compression/templates/anchored-summary-template.md",
          "type": "blob",
          "size": 7421
        },
        {
          "path": ".claude/skills/context-compression/templates/compression-probes-template.md",
          "type": "blob",
          "size": 9650
        },
        {
          "path": ".claude/skills/context-engineering",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-engineering/SKILL.md",
          "type": "blob",
          "size": 11203
        },
        {
          "path": ".claude/skills/context-engineering/capabilities.json",
          "type": "blob",
          "size": 695
        },
        {
          "path": ".claude/skills/context-engineering/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-engineering/checklists/context-optimization-checklist.md",
          "type": "blob",
          "size": 5668
        },
        {
          "path": ".claude/skills/context-engineering/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-engineering/examples/attention-aware-prompt.md",
          "type": "blob",
          "size": 5223
        },
        {
          "path": ".claude/skills/context-engineering/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/context-engineering/references/attention-mechanics.md",
          "type": "blob",
          "size": 4760
        },
        {
          "path": ".claude/skills/context-engineering/references/context-layers.md",
          "type": "blob",
          "size": 8693
        },
        {
          "path": ".claude/skills/contextual-retrieval",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/contextual-retrieval/SKILL.md",
          "type": "blob",
          "size": 10081
        },
        {
          "path": ".claude/skills/contextual-retrieval/capabilities.json",
          "type": "blob",
          "size": 734
        },
        {
          "path": ".claude/skills/create-pr",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/create-pr/SKILL.md",
          "type": "blob",
          "size": 2479
        },
        {
          "path": ".claude/skills/create-pr/capabilities.json",
          "type": "blob",
          "size": 976
        },
        {
          "path": ".claude/skills/create-pr/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/create-pr/references/pr-template.md",
          "type": "blob",
          "size": 1205
        },
        {
          "path": ".claude/skills/database-schema-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/database-schema-designer/SKILL.md",
          "type": "blob",
          "size": 6714
        },
        {
          "path": ".claude/skills/database-schema-designer/capabilities.json",
          "type": "blob",
          "size": 637
        },
        {
          "path": ".claude/skills/database-schema-designer/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/database-schema-designer/checklists/schema-design-checklist.md",
          "type": "blob",
          "size": 6784
        },
        {
          "path": ".claude/skills/database-schema-designer/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/database-schema-designer/examples/skillforge-database-schema.md",
          "type": "blob",
          "size": 16685
        },
        {
          "path": ".claude/skills/database-schema-designer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/database-schema-designer/references/migration-patterns.md",
          "type": "blob",
          "size": 13650
        },
        {
          "path": ".claude/skills/database-schema-designer/references/normalization-patterns.md",
          "type": "blob",
          "size": 10894
        },
        {
          "path": ".claude/skills/defense-in-depth",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/defense-in-depth/SKILL.md",
          "type": "blob",
          "size": 8916
        },
        {
          "path": ".claude/skills/defense-in-depth/capabilities.json",
          "type": "blob",
          "size": 698
        },
        {
          "path": ".claude/skills/defense-in-depth/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/defense-in-depth/checklists/pre-deployment-security.md",
          "type": "blob",
          "size": 2533
        },
        {
          "path": ".claude/skills/defense-in-depth/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/defense-in-depth/references/audit-logging.md",
          "type": "blob",
          "size": 6194
        },
        {
          "path": ".claude/skills/defense-in-depth/references/request-context-pattern.md",
          "type": "blob",
          "size": 4413
        },
        {
          "path": ".claude/skills/defense-in-depth/references/tenant-isolation.md",
          "type": "blob",
          "size": 7118
        },
        {
          "path": ".claude/skills/design-system-starter",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/design-system-starter/SKILL.md",
          "type": "blob",
          "size": 16771
        },
        {
          "path": ".claude/skills/design-system-starter/capabilities.json",
          "type": "blob",
          "size": 647
        },
        {
          "path": ".claude/skills/design-system-starter/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/design-system-starter/checklists/design-system-checklist.md",
          "type": "blob",
          "size": 12943
        },
        {
          "path": ".claude/skills/design-system-starter/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/design-system-starter/examples/skillforge-design-system.md",
          "type": "blob",
          "size": 13355
        },
        {
          "path": ".claude/skills/design-system-starter/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/design-system-starter/references/component-examples.md",
          "type": "blob",
          "size": 13552
        },
        {
          "path": ".claude/skills/design-system-starter/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/design-system-starter/templates/component-template.tsx",
          "type": "blob",
          "size": 6911
        },
        {
          "path": ".claude/skills/design-system-starter/templates/design-tokens-template.json",
          "type": "blob",
          "size": 17128
        },
        {
          "path": ".claude/skills/devops-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/devops-deployment/SKILL.md",
          "type": "blob",
          "size": 23579
        },
        {
          "path": ".claude/skills/devops-deployment/capabilities.json",
          "type": "blob",
          "size": 848
        },
        {
          "path": ".claude/skills/devops-deployment/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/devops-deployment/checklists/production-readiness.md",
          "type": "blob",
          "size": 2749
        },
        {
          "path": ".claude/skills/devops-deployment/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/devops-deployment/examples/github-actions-cicd.md",
          "type": "blob",
          "size": 5005
        },
        {
          "path": ".claude/skills/devops-deployment/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/devops-deployment/references/ci-cd-pipelines.md",
          "type": "blob",
          "size": 844
        },
        {
          "path": ".claude/skills/devops-deployment/references/containerization.md",
          "type": "blob",
          "size": 999
        },
        {
          "path": ".claude/skills/devops-deployment/references/deployment-strategies.md",
          "type": "blob",
          "size": 1291
        },
        {
          "path": ".claude/skills/devops-deployment/references/infrastructure-as-code.md",
          "type": "blob",
          "size": 845
        },
        {
          "path": ".claude/skills/devops-deployment/references/kubernetes.md",
          "type": "blob",
          "size": 1135
        },
        {
          "path": ".claude/skills/devops-deployment/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/devops-deployment/templates/Dockerfile",
          "type": "blob",
          "size": 1084
        },
        {
          "path": ".claude/skills/devops-deployment/templates/argocd-application.yaml",
          "type": "blob",
          "size": 576
        },
        {
          "path": ".claude/skills/devops-deployment/templates/docker-compose.yml",
          "type": "blob",
          "size": 992
        },
        {
          "path": ".claude/skills/devops-deployment/templates/external-secrets.yaml",
          "type": "blob",
          "size": 517
        },
        {
          "path": ".claude/skills/devops-deployment/templates/github-actions-pipeline.yml",
          "type": "blob",
          "size": 3319
        },
        {
          "path": ".claude/skills/devops-deployment/templates/helm-values.yaml",
          "type": "blob",
          "size": 754
        },
        {
          "path": ".claude/skills/devops-deployment/templates/k8s-manifests.yaml",
          "type": "blob",
          "size": 2241
        },
        {
          "path": ".claude/skills/devops-deployment/templates/terraform-aws.tf",
          "type": "blob",
          "size": 1955
        },
        {
          "path": ".claude/skills/doctor",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/doctor/SKILL.md",
          "type": "blob",
          "size": 5068
        },
        {
          "path": ".claude/skills/doctor/capabilities.json",
          "type": "blob",
          "size": 1274
        },
        {
          "path": ".claude/skills/doctor/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/doctor/references/hook-validation.md",
          "type": "blob",
          "size": 2357
        },
        {
          "path": ".claude/skills/doctor/references/permission-rules.md",
          "type": "blob",
          "size": 1793
        },
        {
          "path": ".claude/skills/doctor/references/schema-validation.md",
          "type": "blob",
          "size": 2086
        },
        {
          "path": ".claude/skills/e2e-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/e2e-testing/SKILL.md",
          "type": "blob",
          "size": 5674
        },
        {
          "path": ".claude/skills/e2e-testing/capabilities.json",
          "type": "blob",
          "size": 720
        },
        {
          "path": ".claude/skills/e2e-testing/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/e2e-testing/checklists/e2e-checklist.md",
          "type": "blob",
          "size": 3080
        },
        {
          "path": ".claude/skills/e2e-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/e2e-testing/examples/test-patterns.md",
          "type": "blob",
          "size": 7816
        },
        {
          "path": ".claude/skills/e2e-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/e2e-testing/references/playwright-1.57-api.md",
          "type": "blob",
          "size": 6456
        },
        {
          "path": ".claude/skills/e2e-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/e2e-testing/templates/page-object-template.ts",
          "type": "blob",
          "size": 5089
        },
        {
          "path": ".claude/skills/edge-computing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/edge-computing-patterns/SKILL.md",
          "type": "blob",
          "size": 6787
        },
        {
          "path": ".claude/skills/edge-computing-patterns/capabilities.json",
          "type": "blob",
          "size": 750
        },
        {
          "path": ".claude/skills/edge-computing-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/edge-computing-patterns/checklists/edge-deployment-checklist.md",
          "type": "blob",
          "size": 10875
        },
        {
          "path": ".claude/skills/edge-computing-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/edge-computing-patterns/examples/edge-caching-strategies.md",
          "type": "blob",
          "size": 12595
        },
        {
          "path": ".claude/skills/edge-computing-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/edge-computing-patterns/references/cloudflare-workers.md",
          "type": "blob",
          "size": 8043
        },
        {
          "path": ".claude/skills/edge-computing-patterns/references/runtime-differences.md",
          "type": "blob",
          "size": 10474
        },
        {
          "path": ".claude/skills/edge-computing-patterns/references/vercel-edge.md",
          "type": "blob",
          "size": 10285
        },
        {
          "path": ".claude/skills/edge-computing-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/edge-computing-patterns/templates/edge-function.ts",
          "type": "blob",
          "size": 11141
        },
        {
          "path": ".claude/skills/edge-computing-patterns/templates/edge-middleware.ts",
          "type": "blob",
          "size": 12112
        },
        {
          "path": ".claude/skills/embeddings",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/embeddings/SKILL.md",
          "type": "blob",
          "size": 4463
        },
        {
          "path": ".claude/skills/embeddings/capabilities.json",
          "type": "blob",
          "size": 659
        },
        {
          "path": ".claude/skills/embeddings/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/embeddings/checklists/embedding-checklist.md",
          "type": "blob",
          "size": 1365
        },
        {
          "path": ".claude/skills/embeddings/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/embeddings/references/advanced-patterns.md",
          "type": "blob",
          "size": 9668
        },
        {
          "path": ".claude/skills/embeddings/references/chunking-strategies.md",
          "type": "blob",
          "size": 2459
        },
        {
          "path": ".claude/skills/embeddings/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/embeddings/templates/embedding-pipeline.py",
          "type": "blob",
          "size": 4269
        },
        {
          "path": ".claude/skills/error-handling-rfc9457",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/SKILL.md",
          "type": "blob",
          "size": 9783
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/capabilities.json",
          "type": "blob",
          "size": 687
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/checklists/error-handling-checklist.md",
          "type": "blob",
          "size": 5116
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/examples/fastapi-problem-details.md",
          "type": "blob",
          "size": 13446
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/references/rfc9457-spec.md",
          "type": "blob",
          "size": 7050
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/error-handling-rfc9457/templates/problem-detail-exceptions.py",
          "type": "blob",
          "size": 11804
        },
        {
          "path": ".claude/skills/errors",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/errors/SKILL.md",
          "type": "blob",
          "size": 2268
        },
        {
          "path": ".claude/skills/errors/capabilities.json",
          "type": "blob",
          "size": 809
        },
        {
          "path": ".claude/skills/evidence-verification",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/evidence-verification/SKILL.md",
          "type": "blob",
          "size": 10726
        },
        {
          "path": ".claude/skills/evidence-verification/capabilities.json",
          "type": "blob",
          "size": 1017
        },
        {
          "path": ".claude/skills/evidence-verification/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/evidence-verification/checklists/verification-checklist.md",
          "type": "blob",
          "size": 10137
        },
        {
          "path": ".claude/skills/evidence-verification/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/evidence-verification/examples/evidence-example.json",
          "type": "blob",
          "size": 2711
        },
        {
          "path": ".claude/skills/evidence-verification/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/evidence-verification/references/evidence-patterns.md",
          "type": "blob",
          "size": 16799
        },
        {
          "path": ".claude/skills/evidence-verification/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/evidence-verification/templates/build-evidence.md",
          "type": "blob",
          "size": 6985
        },
        {
          "path": ".claude/skills/evidence-verification/templates/evidence-checklist.md",
          "type": "blob",
          "size": 3457
        },
        {
          "path": ".claude/skills/evidence-verification/templates/test-evidence.md",
          "type": "blob",
          "size": 5067
        },
        {
          "path": ".claude/skills/explore",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/explore/SKILL.md",
          "type": "blob",
          "size": 2109
        },
        {
          "path": ".claude/skills/explore/capabilities.json",
          "type": "blob",
          "size": 855
        },
        {
          "path": ".claude/skills/fastapi-advanced",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fastapi-advanced/SKILL.md",
          "type": "blob",
          "size": 12073
        },
        {
          "path": ".claude/skills/fastapi-advanced/capabilities.json",
          "type": "blob",
          "size": 699
        },
        {
          "path": ".claude/skills/fastapi-advanced/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fastapi-advanced/checklists/fastapi-production-checklist.md",
          "type": "blob",
          "size": 5556
        },
        {
          "path": ".claude/skills/fastapi-advanced/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fastapi-advanced/examples/fastapi-lifespan.md",
          "type": "blob",
          "size": 10165
        },
        {
          "path": ".claude/skills/fastapi-advanced/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fastapi-advanced/references/middleware-stack.md",
          "type": "blob",
          "size": 13965
        },
        {
          "path": ".claude/skills/fastapi-advanced/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fastapi-advanced/templates/fastapi-app.py",
          "type": "blob",
          "size": 11536
        },
        {
          "path": ".claude/skills/fix-issue",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fix-issue/SKILL.md",
          "type": "blob",
          "size": 2610
        },
        {
          "path": ".claude/skills/fix-issue/capabilities.json",
          "type": "blob",
          "size": 1013
        },
        {
          "path": ".claude/skills/fix-issue/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/fix-issue/references/commit-template.md",
          "type": "blob",
          "size": 2096
        },
        {
          "path": ".claude/skills/function-calling",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/function-calling/SKILL.md",
          "type": "blob",
          "size": 6424
        },
        {
          "path": ".claude/skills/function-calling/capabilities.json",
          "type": "blob",
          "size": 644
        },
        {
          "path": ".claude/skills/function-calling/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/function-calling/checklists/tool-checklist.md",
          "type": "blob",
          "size": 1265
        },
        {
          "path": ".claude/skills/function-calling/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/function-calling/references/tool-schema.md",
          "type": "blob",
          "size": 2543
        },
        {
          "path": ".claude/skills/function-calling/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/function-calling/templates/function-def.py",
          "type": "blob",
          "size": 3987
        },
        {
          "path": ".claude/skills/github-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/SKILL.md",
          "type": "blob",
          "size": 8507
        },
        {
          "path": ".claude/skills/github-cli/capabilities.json",
          "type": "blob",
          "size": 771
        },
        {
          "path": ".claude/skills/github-cli/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/checklists/gh-cli-checklist.md",
          "type": "blob",
          "size": 10957
        },
        {
          "path": ".claude/skills/github-cli/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/examples/skillforge-gh-workflow.md",
          "type": "blob",
          "size": 16483
        },
        {
          "path": ".claude/skills/github-cli/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/references/automation-patterns.md",
          "type": "blob",
          "size": 7316
        },
        {
          "path": ".claude/skills/github-cli/references/graphql-api.md",
          "type": "blob",
          "size": 7425
        },
        {
          "path": ".claude/skills/github-cli/references/issue-management.md",
          "type": "blob",
          "size": 4465
        },
        {
          "path": ".claude/skills/github-cli/references/pr-workflows.md",
          "type": "blob",
          "size": 6055
        },
        {
          "path": ".claude/skills/github-cli/references/projects-v2.md",
          "type": "blob",
          "size": 7789
        },
        {
          "path": ".claude/skills/github-cli/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/scripts/gh-helpers.sh",
          "type": "blob",
          "size": 8586
        },
        {
          "path": ".claude/skills/github-cli/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/github-cli/templates/skillforge-project-config.json",
          "type": "blob",
          "size": 3866
        },
        {
          "path": ".claude/skills/golden-dataset-curation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-curation/SKILL.md",
          "type": "blob",
          "size": 20497
        },
        {
          "path": ".claude/skills/golden-dataset-curation/capabilities.json",
          "type": "blob",
          "size": 769
        },
        {
          "path": ".claude/skills/golden-dataset-management",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-management/SKILL.md",
          "type": "blob",
          "size": 17743
        },
        {
          "path": ".claude/skills/golden-dataset-management/capabilities.json",
          "type": "blob",
          "size": 859
        },
        {
          "path": ".claude/skills/golden-dataset-management/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-management/checklists/backup-restore-checklist.md",
          "type": "blob",
          "size": 14657
        },
        {
          "path": ".claude/skills/golden-dataset-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-management/examples/skillforge-dataset-workflow.md",
          "type": "blob",
          "size": 19132
        },
        {
          "path": ".claude/skills/golden-dataset-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-management/references/backup-restore.md",
          "type": "blob",
          "size": 9086
        },
        {
          "path": ".claude/skills/golden-dataset-management/references/validation-contracts.md",
          "type": "blob",
          "size": 9103
        },
        {
          "path": ".claude/skills/golden-dataset-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-management/templates/backup-script.py",
          "type": "blob",
          "size": 13834
        },
        {
          "path": ".claude/skills/golden-dataset-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/golden-dataset-validation/SKILL.md",
          "type": "blob",
          "size": 18212
        },
        {
          "path": ".claude/skills/golden-dataset-validation/capabilities.json",
          "type": "blob",
          "size": 721
        },
        {
          "path": ".claude/skills/hyde-retrieval",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/hyde-retrieval/SKILL.md",
          "type": "blob",
          "size": 4522
        },
        {
          "path": ".claude/skills/hyde-retrieval/capabilities.json",
          "type": "blob",
          "size": 749
        },
        {
          "path": ".claude/skills/i18n-date-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/i18n-date-patterns/SKILL.md",
          "type": "blob",
          "size": 6852
        },
        {
          "path": ".claude/skills/i18n-date-patterns/capabilities.json",
          "type": "blob",
          "size": 736
        },
        {
          "path": ".claude/skills/i18n-date-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/i18n-date-patterns/checklists/i18n-checklist.md",
          "type": "blob",
          "size": 3348
        },
        {
          "path": ".claude/skills/i18n-date-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/i18n-date-patterns/examples/component-i18n-example.md",
          "type": "blob",
          "size": 3599
        },
        {
          "path": ".claude/skills/i18n-date-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/i18n-date-patterns/references/formatting-utilities.md",
          "type": "blob",
          "size": 5110
        },
        {
          "path": ".claude/skills/i18n-date-patterns/references/icu-messageformat.md",
          "type": "blob",
          "size": 3216
        },
        {
          "path": ".claude/skills/i18n-date-patterns/references/trans-component.md",
          "type": "blob",
          "size": 4013
        },
        {
          "path": ".claude/skills/implement",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/implement/SKILL.md",
          "type": "blob",
          "size": 3585
        },
        {
          "path": ".claude/skills/implement/capabilities.json",
          "type": "blob",
          "size": 1193
        },
        {
          "path": ".claude/skills/implement/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/implement/references/agent-phases.md",
          "type": "blob",
          "size": 3977
        },
        {
          "path": ".claude/skills/input-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/input-validation/SKILL.md",
          "type": "blob",
          "size": 5025
        },
        {
          "path": ".claude/skills/input-validation/capabilities.json",
          "type": "blob",
          "size": 701
        },
        {
          "path": ".claude/skills/input-validation/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/input-validation/checklists/validation-checklist.md",
          "type": "blob",
          "size": 3348
        },
        {
          "path": ".claude/skills/input-validation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/input-validation/examples/validation-patterns.md",
          "type": "blob",
          "size": 7595
        },
        {
          "path": ".claude/skills/input-validation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/input-validation/references/zod-v4-api.md",
          "type": "blob",
          "size": 6930
        },
        {
          "path": ".claude/skills/input-validation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/input-validation/templates/validation-schemas.ts",
          "type": "blob",
          "size": 7153
        },
        {
          "path": ".claude/skills/integration-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/integration-testing/SKILL.md",
          "type": "blob",
          "size": 4328
        },
        {
          "path": ".claude/skills/integration-testing/capabilities.json",
          "type": "blob",
          "size": 593
        },
        {
          "path": ".claude/skills/integration-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/integration-testing/templates/pytest-integration.py",
          "type": "blob",
          "size": 8358
        },
        {
          "path": ".claude/skills/integration-testing/templates/test-plan-template.md",
          "type": "blob",
          "size": 8785
        },
        {
          "path": ".claude/skills/langfuse-observability",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langfuse-observability/SKILL.md",
          "type": "blob",
          "size": 16919
        },
        {
          "path": ".claude/skills/langfuse-observability/capabilities.json",
          "type": "blob",
          "size": 1048
        },
        {
          "path": ".claude/skills/langfuse-observability/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langfuse-observability/checklists/langfuse-setup-checklist.md",
          "type": "blob",
          "size": 12195
        },
        {
          "path": ".claude/skills/langfuse-observability/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langfuse-observability/examples/skillforge-langfuse-traces.md",
          "type": "blob",
          "size": 13649
        },
        {
          "path": ".claude/skills/langfuse-observability/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langfuse-observability/references/cost-tracking.md",
          "type": "blob",
          "size": 2985
        },
        {
          "path": ".claude/skills/langfuse-observability/references/evaluation-scores.md",
          "type": "blob",
          "size": 4592
        },
        {
          "path": ".claude/skills/langfuse-observability/references/experiments-api.md",
          "type": "blob",
          "size": 12733
        },
        {
          "path": ".claude/skills/langfuse-observability/references/multi-judge-evaluation.md",
          "type": "blob",
          "size": 13632
        },
        {
          "path": ".claude/skills/langfuse-observability/references/prompt-management.md",
          "type": "blob",
          "size": 6299
        },
        {
          "path": ".claude/skills/langfuse-observability/references/session-tracking.md",
          "type": "blob",
          "size": 5105
        },
        {
          "path": ".claude/skills/langfuse-observability/references/tracing-setup.md",
          "type": "blob",
          "size": 3406
        },
        {
          "path": ".claude/skills/langfuse-observability/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langfuse-observability/templates/callback-handler.py",
          "type": "blob",
          "size": 7351
        },
        {
          "path": ".claude/skills/langfuse-observability/templates/observe-decorator.py",
          "type": "blob",
          "size": 4425
        },
        {
          "path": ".claude/skills/langgraph-checkpoints",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/SKILL.md",
          "type": "blob",
          "size": 8483
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/capabilities.json",
          "type": "blob",
          "size": 682
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/checklists/checkpoint-checklist.md",
          "type": "blob",
          "size": 922
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/references/postgres-checkpointer.md",
          "type": "blob",
          "size": 1269
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/references/state-inspection.md",
          "type": "blob",
          "size": 1559
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/references/state-recovery.md",
          "type": "blob",
          "size": 1370
        },
        {
          "path": ".claude/skills/langgraph-checkpoints/references/store-memory.md",
          "type": "blob",
          "size": 1448
        },
        {
          "path": ".claude/skills/langgraph-functional",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-functional/SKILL.md",
          "type": "blob",
          "size": 9861
        },
        {
          "path": ".claude/skills/langgraph-functional/capabilities.json",
          "type": "blob",
          "size": 762
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/SKILL.md",
          "type": "blob",
          "size": 6057
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/capabilities.json",
          "type": "blob",
          "size": 907
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/checklists/hitl-checklist.md",
          "type": "blob",
          "size": 971
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/references/api-integration.md",
          "type": "blob",
          "size": 1994
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/references/approval-gate.md",
          "type": "blob",
          "size": 1510
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/references/feedback-loop.md",
          "type": "blob",
          "size": 1667
        },
        {
          "path": ".claude/skills/langgraph-human-in-loop/references/interrupt-resume.md",
          "type": "blob",
          "size": 1314
        },
        {
          "path": ".claude/skills/langgraph-parallel",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-parallel/SKILL.md",
          "type": "blob",
          "size": 4938
        },
        {
          "path": ".claude/skills/langgraph-parallel/capabilities.json",
          "type": "blob",
          "size": 617
        },
        {
          "path": ".claude/skills/langgraph-parallel/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-parallel/checklists/parallel-checklist.md",
          "type": "blob",
          "size": 761
        },
        {
          "path": ".claude/skills/langgraph-parallel/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-parallel/references/error-isolation.md",
          "type": "blob",
          "size": 1648
        },
        {
          "path": ".claude/skills/langgraph-parallel/references/fanout-fanin.md",
          "type": "blob",
          "size": 1460
        },
        {
          "path": ".claude/skills/langgraph-parallel/references/map-reduce.md",
          "type": "blob",
          "size": 1472
        },
        {
          "path": ".claude/skills/langgraph-parallel/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-parallel/templates/parallel-agent-fanout.py",
          "type": "blob",
          "size": 9656
        },
        {
          "path": ".claude/skills/langgraph-routing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-routing/SKILL.md",
          "type": "blob",
          "size": 4085
        },
        {
          "path": ".claude/skills/langgraph-routing/capabilities.json",
          "type": "blob",
          "size": 611
        },
        {
          "path": ".claude/skills/langgraph-routing/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-routing/checklists/routing-checklist.md",
          "type": "blob",
          "size": 666
        },
        {
          "path": ".claude/skills/langgraph-routing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-routing/references/conditional-edges.md",
          "type": "blob",
          "size": 1144
        },
        {
          "path": ".claude/skills/langgraph-routing/references/retry-loops.md",
          "type": "blob",
          "size": 1269
        },
        {
          "path": ".claude/skills/langgraph-routing/references/semantic-routing.md",
          "type": "blob",
          "size": 1532
        },
        {
          "path": ".claude/skills/langgraph-routing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-routing/templates/semantic-router.py",
          "type": "blob",
          "size": 16576
        },
        {
          "path": ".claude/skills/langgraph-state",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-state/SKILL.md",
          "type": "blob",
          "size": 5173
        },
        {
          "path": ".claude/skills/langgraph-state/capabilities.json",
          "type": "blob",
          "size": 645
        },
        {
          "path": ".claude/skills/langgraph-state/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-state/checklists/state-checklist.md",
          "type": "blob",
          "size": 843
        },
        {
          "path": ".claude/skills/langgraph-state/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-state/references/custom-reducers.md",
          "type": "blob",
          "size": 1379
        },
        {
          "path": ".claude/skills/langgraph-state/references/messages-state.md",
          "type": "blob",
          "size": 1207
        },
        {
          "path": ".claude/skills/langgraph-state/references/pydantic-state.md",
          "type": "blob",
          "size": 1252
        },
        {
          "path": ".claude/skills/langgraph-state/references/typeddict-state.md",
          "type": "blob",
          "size": 1081
        },
        {
          "path": ".claude/skills/langgraph-supervisor",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-supervisor/SKILL.md",
          "type": "blob",
          "size": 6605
        },
        {
          "path": ".claude/skills/langgraph-supervisor/capabilities.json",
          "type": "blob",
          "size": 686
        },
        {
          "path": ".claude/skills/langgraph-supervisor/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-supervisor/checklists/supervisor-checklist.md",
          "type": "blob",
          "size": 862
        },
        {
          "path": ".claude/skills/langgraph-supervisor/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-supervisor/examples/skillforge-analysis-workflow.md",
          "type": "blob",
          "size": 17366
        },
        {
          "path": ".claude/skills/langgraph-supervisor/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-supervisor/references/llm-supervisor.md",
          "type": "blob",
          "size": 1612
        },
        {
          "path": ".claude/skills/langgraph-supervisor/references/priority-routing.md",
          "type": "blob",
          "size": 1656
        },
        {
          "path": ".claude/skills/langgraph-supervisor/references/round-robin.md",
          "type": "blob",
          "size": 1476
        },
        {
          "path": ".claude/skills/langgraph-supervisor/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/langgraph-supervisor/templates/content-analysis-graph.py",
          "type": "blob",
          "size": 13575
        },
        {
          "path": ".claude/skills/langgraph-supervisor/templates/supervisor-workflow.py",
          "type": "blob",
          "size": 11162
        },
        {
          "path": ".claude/skills/llm-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-evaluation/SKILL.md",
          "type": "blob",
          "size": 4748
        },
        {
          "path": ".claude/skills/llm-evaluation/capabilities.json",
          "type": "blob",
          "size": 741
        },
        {
          "path": ".claude/skills/llm-evaluation/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-evaluation/checklists/evaluation-checklist.md",
          "type": "blob",
          "size": 2384
        },
        {
          "path": ".claude/skills/llm-evaluation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-evaluation/examples/evaluation-patterns.md",
          "type": "blob",
          "size": 8226
        },
        {
          "path": ".claude/skills/llm-evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-evaluation/references/evaluation-metrics.md",
          "type": "blob",
          "size": 5045
        },
        {
          "path": ".claude/skills/llm-evaluation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-evaluation/templates/evaluator-template.py",
          "type": "blob",
          "size": 8193
        },
        {
          "path": ".claude/skills/llm-safety-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-safety-patterns/SKILL.md",
          "type": "blob",
          "size": 10539
        },
        {
          "path": ".claude/skills/llm-safety-patterns/capabilities.json",
          "type": "blob",
          "size": 779
        },
        {
          "path": ".claude/skills/llm-safety-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-safety-patterns/checklists/pre-llm-call.md",
          "type": "blob",
          "size": 2560
        },
        {
          "path": ".claude/skills/llm-safety-patterns/checklists/safety-checklist.md",
          "type": "blob",
          "size": 656
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references/context-separation.md",
          "type": "blob",
          "size": 9561
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references/output-guardrails.md",
          "type": "blob",
          "size": 13368
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references/post-llm-attribution.md",
          "type": "blob",
          "size": 11708
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references/pre-llm-filtering.md",
          "type": "blob",
          "size": 9720
        },
        {
          "path": ".claude/skills/llm-safety-patterns/references/prompt-audit.md",
          "type": "blob",
          "size": 13002
        },
        {
          "path": ".claude/skills/llm-safety-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-safety-patterns/templates/prompt_builder.py",
          "type": "blob",
          "size": 8492
        },
        {
          "path": ".claude/skills/llm-safety-patterns/templates/safe_llm_call.py",
          "type": "blob",
          "size": 7770
        },
        {
          "path": ".claude/skills/llm-streaming",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-streaming/SKILL.md",
          "type": "blob",
          "size": 6515
        },
        {
          "path": ".claude/skills/llm-streaming/capabilities.json",
          "type": "blob",
          "size": 628
        },
        {
          "path": ".claude/skills/llm-streaming/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-streaming/checklists/streaming-checklist.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": ".claude/skills/llm-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-testing/SKILL.md",
          "type": "blob",
          "size": 4991
        },
        {
          "path": ".claude/skills/llm-testing/capabilities.json",
          "type": "blob",
          "size": 739
        },
        {
          "path": ".claude/skills/llm-testing/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-testing/checklists/llm-test-checklist.md",
          "type": "blob",
          "size": 2861
        },
        {
          "path": ".claude/skills/llm-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-testing/examples/test-patterns.md",
          "type": "blob",
          "size": 8383
        },
        {
          "path": ".claude/skills/llm-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-testing/references/deepeval-ragas-api.md",
          "type": "blob",
          "size": 7110
        },
        {
          "path": ".claude/skills/llm-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/llm-testing/templates/llm-test-template.py",
          "type": "blob",
          "size": 8892
        },
        {
          "path": ".claude/skills/mcp-server-building",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/mcp-server-building/SKILL.md",
          "type": "blob",
          "size": 10441
        },
        {
          "path": ".claude/skills/mcp-server-building/capabilities.json",
          "type": "blob",
          "size": 726
        },
        {
          "path": ".claude/skills/motion-animation-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/motion-animation-patterns/SKILL.md",
          "type": "blob",
          "size": 11947
        },
        {
          "path": ".claude/skills/motion-animation-patterns/capabilities.json",
          "type": "blob",
          "size": 1008
        },
        {
          "path": ".claude/skills/motion-animation-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/motion-animation-patterns/references/animation-presets.md",
          "type": "blob",
          "size": 6682
        },
        {
          "path": ".claude/skills/msw-mocking",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/msw-mocking/SKILL.md",
          "type": "blob",
          "size": 4798
        },
        {
          "path": ".claude/skills/msw-mocking/capabilities.json",
          "type": "blob",
          "size": 751
        },
        {
          "path": ".claude/skills/msw-mocking/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/msw-mocking/checklists/msw-setup-checklist.md",
          "type": "blob",
          "size": 3107
        },
        {
          "path": ".claude/skills/msw-mocking/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/msw-mocking/examples/handler-patterns.md",
          "type": "blob",
          "size": 10131
        },
        {
          "path": ".claude/skills/msw-mocking/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/msw-mocking/references/msw-2x-api.md",
          "type": "blob",
          "size": 6021
        },
        {
          "path": ".claude/skills/msw-mocking/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/msw-mocking/templates/handlers-template.ts",
          "type": "blob",
          "size": 5870
        },
        {
          "path": ".claude/skills/multi-agent-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/SKILL.md",
          "type": "blob",
          "size": 7056
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/capabilities.json",
          "type": "blob",
          "size": 651
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/checklists/orchestration-checklist.md",
          "type": "blob",
          "size": 705
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/multi-agent-orchestration/references/coordination-patterns.md",
          "type": "blob",
          "size": 2896
        },
        {
          "path": ".claude/skills/observability-monitoring",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/observability-monitoring/SKILL.md",
          "type": "blob",
          "size": 26296
        },
        {
          "path": ".claude/skills/observability-monitoring/capabilities.json",
          "type": "blob",
          "size": 1194
        },
        {
          "path": ".claude/skills/observability-monitoring/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/observability-monitoring/checklists/monitoring-implementation-checklist.md",
          "type": "blob",
          "size": 17356
        },
        {
          "path": ".claude/skills/observability-monitoring/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/observability-monitoring/examples/skillforge-monitoring-dashboard.md",
          "type": "blob",
          "size": 14013
        },
        {
          "path": ".claude/skills/observability-monitoring/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/observability-monitoring/references/alerting-strategies.md",
          "type": "blob",
          "size": 1516
        },
        {
          "path": ".claude/skills/observability-monitoring/references/dashboards.md",
          "type": "blob",
          "size": 1227
        },
        {
          "path": ".claude/skills/observability-monitoring/references/distributed-tracing.md",
          "type": "blob",
          "size": 1646
        },
        {
          "path": ".claude/skills/observability-monitoring/references/metrics-collection.md",
          "type": "blob",
          "size": 1725
        },
        {
          "path": ".claude/skills/observability-monitoring/references/structured-logging.md",
          "type": "blob",
          "size": 1483
        },
        {
          "path": ".claude/skills/observability-monitoring/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/observability-monitoring/templates/alerting-rules.yml",
          "type": "blob",
          "size": 3191
        },
        {
          "path": ".claude/skills/observability-monitoring/templates/health-checks.ts",
          "type": "blob",
          "size": 2628
        },
        {
          "path": ".claude/skills/observability-monitoring/templates/opentelemetry-tracing.ts",
          "type": "blob",
          "size": 3848
        },
        {
          "path": ".claude/skills/observability-monitoring/templates/prometheus-metrics.ts",
          "type": "blob",
          "size": 3353
        },
        {
          "path": ".claude/skills/observability-monitoring/templates/structured-logging.ts",
          "type": "blob",
          "size": 3416
        },
        {
          "path": ".claude/skills/ollama-local",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ollama-local/SKILL.md",
          "type": "blob",
          "size": 5301
        },
        {
          "path": ".claude/skills/ollama-local/capabilities.json",
          "type": "blob",
          "size": 557
        },
        {
          "path": ".claude/skills/ollama-local/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ollama-local/references/model-selection.md",
          "type": "blob",
          "size": 1968
        },
        {
          "path": ".claude/skills/ollama-local/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/ollama-local/templates/ollama-provider-template.py",
          "type": "blob",
          "size": 13185
        },
        {
          "path": ".claude/skills/owasp-top-10",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/owasp-top-10/SKILL.md",
          "type": "blob",
          "size": 6370
        },
        {
          "path": ".claude/skills/owasp-top-10/capabilities.json",
          "type": "blob",
          "size": 567
        },
        {
          "path": ".claude/skills/owasp-top-10/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/owasp-top-10/examples/owasp-top10-fixes.md",
          "type": "blob",
          "size": 3413
        },
        {
          "path": ".claude/skills/owasp-top-10/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/owasp-top-10/references/vulnerability-demos.md",
          "type": "blob",
          "size": 11867
        },
        {
          "path": ".claude/skills/performance-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-optimization/SKILL.md",
          "type": "blob",
          "size": 30269
        },
        {
          "path": ".claude/skills/performance-optimization/capabilities.json",
          "type": "blob",
          "size": 1181
        },
        {
          "path": ".claude/skills/performance-optimization/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-optimization/checklists/performance-audit-checklist.md",
          "type": "blob",
          "size": 16238
        },
        {
          "path": ".claude/skills/performance-optimization/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-optimization/examples/skillforge-performance-wins.md",
          "type": "blob",
          "size": 20778
        },
        {
          "path": ".claude/skills/performance-optimization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-optimization/references/caching-strategies.md",
          "type": "blob",
          "size": 6585
        },
        {
          "path": ".claude/skills/performance-optimization/references/core-web-vitals.md",
          "type": "blob",
          "size": 7998
        },
        {
          "path": ".claude/skills/performance-optimization/references/database-optimization.md",
          "type": "blob",
          "size": 4012
        },
        {
          "path": ".claude/skills/performance-optimization/references/frontend-performance.md",
          "type": "blob",
          "size": 5792
        },
        {
          "path": ".claude/skills/performance-optimization/references/profiling.md",
          "type": "blob",
          "size": 8294
        },
        {
          "path": ".claude/skills/performance-optimization/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-optimization/templates/api-optimization.ts",
          "type": "blob",
          "size": 3043
        },
        {
          "path": ".claude/skills/performance-optimization/templates/caching-patterns.ts",
          "type": "blob",
          "size": 2233
        },
        {
          "path": ".claude/skills/performance-optimization/templates/database-optimization.ts",
          "type": "blob",
          "size": 2440
        },
        {
          "path": ".claude/skills/performance-optimization/templates/frontend-optimization.tsx",
          "type": "blob",
          "size": 3905
        },
        {
          "path": ".claude/skills/performance-optimization/templates/performance-metrics.ts",
          "type": "blob",
          "size": 2960
        },
        {
          "path": ".claude/skills/performance-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-testing/SKILL.md",
          "type": "blob",
          "size": 5048
        },
        {
          "path": ".claude/skills/performance-testing/capabilities.json",
          "type": "blob",
          "size": 643
        },
        {
          "path": ".claude/skills/performance-testing/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-testing/checklists/performance-checklist.md",
          "type": "blob",
          "size": 639
        },
        {
          "path": ".claude/skills/performance-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-testing/references/k6-patterns.md",
          "type": "blob",
          "size": 2210
        },
        {
          "path": ".claude/skills/performance-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/performance-testing/templates/k6-script.js",
          "type": "blob",
          "size": 5373
        },
        {
          "path": ".claude/skills/pgvector-search",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/pgvector-search/SKILL.md",
          "type": "blob",
          "size": 18049
        },
        {
          "path": ".claude/skills/pgvector-search/capabilities.json",
          "type": "blob",
          "size": 1168
        },
        {
          "path": ".claude/skills/pgvector-search/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/pgvector-search/checklists/search-implementation-checklist.md",
          "type": "blob",
          "size": 18683
        },
        {
          "path": ".claude/skills/pgvector-search/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/pgvector-search/examples/skillforge-retrieval.md",
          "type": "blob",
          "size": 10277
        },
        {
          "path": ".claude/skills/pgvector-search/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/pgvector-search/references/hybrid-search-rrf.md",
          "type": "blob",
          "size": 7173
        },
        {
          "path": ".claude/skills/pgvector-search/references/indexing-strategies.md",
          "type": "blob",
          "size": 6208
        },
        {
          "path": ".claude/skills/pgvector-search/references/metadata-filtering.md",
          "type": "blob",
          "size": 9910
        },
        {
          "path": ".claude/skills/pgvector-search/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/pgvector-search/templates/chunk-repository.py",
          "type": "blob",
          "size": 10266
        },
        {
          "path": ".claude/skills/pgvector-search/templates/search-service.py",
          "type": "blob",
          "size": 5589
        },
        {
          "path": ".claude/skills/project-structure-enforcer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/project-structure-enforcer/SKILL.md",
          "type": "blob",
          "size": 10028
        },
        {
          "path": ".claude/skills/project-structure-enforcer/capabilities.json",
          "type": "blob",
          "size": 682
        },
        {
          "path": ".claude/skills/project-structure-enforcer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/project-structure-enforcer/references/violation-examples.md",
          "type": "blob",
          "size": 13091
        },
        {
          "path": ".claude/skills/prompt-caching",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/prompt-caching/SKILL.md",
          "type": "blob",
          "size": 7000
        },
        {
          "path": ".claude/skills/prompt-caching/capabilities.json",
          "type": "blob",
          "size": 598
        },
        {
          "path": ".claude/skills/prompt-caching/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/prompt-caching/templates/prompt-cache-wrapper.py",
          "type": "blob",
          "size": 10737
        },
        {
          "path": ".claude/skills/quality-gates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/quality-gates/SKILL.md",
          "type": "blob",
          "size": 26730
        },
        {
          "path": ".claude/skills/quality-gates/capabilities.json",
          "type": "blob",
          "size": 946
        },
        {
          "path": ".claude/skills/quality-gates/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/quality-gates/checklists/quality-gate-checklist.md",
          "type": "blob",
          "size": 5304
        },
        {
          "path": ".claude/skills/quality-gates/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/quality-gates/examples/skillforge-quality-gates.md",
          "type": "blob",
          "size": 3716
        },
        {
          "path": ".claude/skills/quality-gates/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/quality-gates/references/gate-patterns.md",
          "type": "blob",
          "size": 12164
        },
        {
          "path": ".claude/skills/quality-gates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/quality-gates/templates/complexity-assessment.md",
          "type": "blob",
          "size": 4780
        },
        {
          "path": ".claude/skills/quality-gates/templates/gate-check-template.md",
          "type": "blob",
          "size": 8406
        },
        {
          "path": ".claude/skills/quality-gates/templates/requirements-checklist.md",
          "type": "blob",
          "size": 9858
        },
        {
          "path": ".claude/skills/query-decomposition",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/query-decomposition/SKILL.md",
          "type": "blob",
          "size": 6863
        },
        {
          "path": ".claude/skills/query-decomposition/capabilities.json",
          "type": "blob",
          "size": 731
        },
        {
          "path": ".claude/skills/rag-retrieval",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rag-retrieval/SKILL.md",
          "type": "blob",
          "size": 7284
        },
        {
          "path": ".claude/skills/rag-retrieval/capabilities.json",
          "type": "blob",
          "size": 638
        },
        {
          "path": ".claude/skills/rag-retrieval/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rag-retrieval/examples/chatbot-with-rag-example.ts",
          "type": "blob",
          "size": 1875
        },
        {
          "path": ".claude/skills/rag-retrieval/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rag-retrieval/references/advanced-rag.md",
          "type": "blob",
          "size": 10354
        },
        {
          "path": ".claude/skills/rag-retrieval/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rag-retrieval/templates/rag-pipeline-template.ts",
          "type": "blob",
          "size": 8985
        },
        {
          "path": ".claude/skills/rate-limiting",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rate-limiting/SKILL.md",
          "type": "blob",
          "size": 8579
        },
        {
          "path": ".claude/skills/rate-limiting/capabilities.json",
          "type": "blob",
          "size": 696
        },
        {
          "path": ".claude/skills/rate-limiting/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rate-limiting/checklists/rate-limiting-checklist.md",
          "type": "blob",
          "size": 4589
        },
        {
          "path": ".claude/skills/rate-limiting/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rate-limiting/examples/fastapi-rate-limiting.md",
          "type": "blob",
          "size": 9970
        },
        {
          "path": ".claude/skills/rate-limiting/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rate-limiting/references/token-bucket-algorithm.md",
          "type": "blob",
          "size": 7275
        },
        {
          "path": ".claude/skills/rate-limiting/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/rate-limiting/templates/redis-rate-limiter.py",
          "type": "blob",
          "size": 10467
        },
        {
          "path": ".claude/skills/react-server-components-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/react-server-components-framework/SKILL.md",
          "type": "blob",
          "size": 19449
        },
        {
          "path": ".claude/skills/react-server-components-framework/capabilities.json",
          "type": "blob",
          "size": 1244
        },
        {
          "path": ".claude/skills/react-server-components-framework/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/react-server-components-framework/checklists/rsc-implementation-checklist.md",
          "type": "blob",
          "size": 9742
        },
        {
          "path": ".claude/skills/react-server-components-framework/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/react-server-components-framework/examples/blog-app-example.tsx",
          "type": "blob",
          "size": 12238
        },
        {
          "path": ".claude/skills/react-server-components-framework/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/component-patterns.md",
          "type": "blob",
          "size": 9725
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/data-fetching.md",
          "type": "blob",
          "size": 3249
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/migration-guide.md",
          "type": "blob",
          "size": 3523
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/react-19-patterns.md",
          "type": "blob",
          "size": 19477
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/routing-patterns.md",
          "type": "blob",
          "size": 3343
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/server-actions.md",
          "type": "blob",
          "size": 4282
        },
        {
          "path": ".claude/skills/react-server-components-framework/references/tanstack-router-patterns.md",
          "type": "blob",
          "size": 10121
        },
        {
          "path": ".claude/skills/react-server-components-framework/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/react-server-components-framework/templates/client-component-template.tsx",
          "type": "blob",
          "size": 2078
        },
        {
          "path": ".claude/skills/react-server-components-framework/templates/server-action-template.ts",
          "type": "blob",
          "size": 6083
        },
        {
          "path": ".claude/skills/react-server-components-framework/templates/server-component-template.tsx",
          "type": "blob",
          "size": 1928
        },
        {
          "path": ".claude/skills/reranking-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/reranking-patterns/SKILL.md",
          "type": "blob",
          "size": 7146
        },
        {
          "path": ".claude/skills/reranking-patterns/capabilities.json",
          "type": "blob",
          "size": 725
        },
        {
          "path": ".claude/skills/resilience-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/resilience-patterns/SKILL.md",
          "type": "blob",
          "size": 17045
        },
        {
          "path": ".claude/skills/resilience-patterns/capabilities.json",
          "type": "blob",
          "size": 666
        },
        {
          "path": ".claude/skills/resilience-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/resilience-patterns/checklists/circuit-breaker-setup.md",
          "type": "blob",
          "size": 11666
        },
        {
          "path": ".claude/skills/resilience-patterns/checklists/pre-deployment-resilience.md",
          "type": "blob",
          "size": 4809
        },
        {
          "path": ".claude/skills/resilience-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/resilience-patterns/examples/skillforge-workflow-resilience.md",
          "type": "blob",
          "size": 20115
        },
        {
          "path": ".claude/skills/resilience-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/resilience-patterns/references/bulkhead-pattern.md",
          "type": "blob",
          "size": 12793
        },
        {
          "path": ".claude/skills/resilience-patterns/references/circuit-breaker.md",
          "type": "blob",
          "size": 6615
        },
        {
          "path": ".claude/skills/resilience-patterns/references/error-classification.md",
          "type": "blob",
          "size": 11707
        },
        {
          "path": ".claude/skills/resilience-patterns/references/llm-resilience.md",
          "type": "blob",
          "size": 18153
        },
        {
          "path": ".claude/skills/resilience-patterns/references/retry-strategies.md",
          "type": "blob",
          "size": 11006
        },
        {
          "path": ".claude/skills/resilience-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/resilience-patterns/templates/bulkhead.py",
          "type": "blob",
          "size": 14144
        },
        {
          "path": ".claude/skills/resilience-patterns/templates/circuit-breaker.py",
          "type": "blob",
          "size": 12775
        },
        {
          "path": ".claude/skills/resilience-patterns/templates/llm-fallback-chain.py",
          "type": "blob",
          "size": 14689
        },
        {
          "path": ".claude/skills/resilience-patterns/templates/retry-handler.py",
          "type": "blob",
          "size": 13944
        },
        {
          "path": ".claude/skills/resilience-patterns/templates/token-budget.py",
          "type": "blob",
          "size": 15287
        },
        {
          "path": ".claude/skills/review-pr",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/review-pr/SKILL.md",
          "type": "blob",
          "size": 3095
        },
        {
          "path": ".claude/skills/review-pr/capabilities.json",
          "type": "blob",
          "size": 1087
        },
        {
          "path": ".claude/skills/review-pr/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/review-pr/references/review-template.md",
          "type": "blob",
          "size": 2432
        },
        {
          "path": ".claude/skills/run-tests",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/run-tests/SKILL.md",
          "type": "blob",
          "size": 2249
        },
        {
          "path": ".claude/skills/run-tests/capabilities.json",
          "type": "blob",
          "size": 980
        },
        {
          "path": ".claude/skills/run-tests/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/run-tests/references/test-commands.md",
          "type": "blob",
          "size": 1301
        },
        {
          "path": ".claude/skills/security-scanning",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/security-scanning/SKILL.md",
          "type": "blob",
          "size": 5406
        },
        {
          "path": ".claude/skills/security-scanning/capabilities.json",
          "type": "blob",
          "size": 624
        },
        {
          "path": ".claude/skills/security-scanning/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/security-scanning/examples/api-security-audit.md",
          "type": "blob",
          "size": 4228
        },
        {
          "path": ".claude/skills/security-scanning/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/security-scanning/references/tool-configs.md",
          "type": "blob",
          "size": 11881
        },
        {
          "path": ".claude/skills/security-scanning/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/security-scanning/templates/security-audit-template.md",
          "type": "blob",
          "size": 5288
        },
        {
          "path": ".claude/skills/semantic-caching",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/semantic-caching/SKILL.md",
          "type": "blob",
          "size": 4291
        },
        {
          "path": ".claude/skills/semantic-caching/capabilities.json",
          "type": "blob",
          "size": 622
        },
        {
          "path": ".claude/skills/semantic-caching/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/semantic-caching/examples/skillforge-integration.md",
          "type": "blob",
          "size": 8949
        },
        {
          "path": ".claude/skills/semantic-caching/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/semantic-caching/references/cache-strategies.md",
          "type": "blob",
          "size": 2419
        },
        {
          "path": ".claude/skills/semantic-caching/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/semantic-caching/templates/redis-cache.py",
          "type": "blob",
          "size": 4504
        },
        {
          "path": ".claude/skills/semantic-caching/templates/semantic-cache-service.py",
          "type": "blob",
          "size": 9349
        },
        {
          "path": ".claude/skills/streaming-api-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/streaming-api-patterns/SKILL.md",
          "type": "blob",
          "size": 12674
        },
        {
          "path": ".claude/skills/streaming-api-patterns/capabilities.json",
          "type": "blob",
          "size": 768
        },
        {
          "path": ".claude/skills/streaming-api-patterns/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/streaming-api-patterns/checklists/streaming-checklist.md",
          "type": "blob",
          "size": 2260
        },
        {
          "path": ".claude/skills/streaming-api-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/streaming-api-patterns/examples/skillforge-sse-implementation.md",
          "type": "blob",
          "size": 23037
        },
        {
          "path": ".claude/skills/streaming-api-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/streaming-api-patterns/references/sse-deep-dive.md",
          "type": "blob",
          "size": 18013
        },
        {
          "path": ".claude/skills/streaming-api-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/streaming-api-patterns/templates/sse-endpoint-template.ts",
          "type": "blob",
          "size": 2004
        },
        {
          "path": ".claude/skills/system-design-interrogation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/system-design-interrogation/SKILL.md",
          "type": "blob",
          "size": 12050
        },
        {
          "path": ".claude/skills/system-design-interrogation/capabilities.json",
          "type": "blob",
          "size": 813
        },
        {
          "path": ".claude/skills/system-design-interrogation/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/system-design-interrogation/checklists/before-implementation.md",
          "type": "blob",
          "size": 3866
        },
        {
          "path": ".claude/skills/system-design-interrogation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/system-design-interrogation/references/coherence-questions.md",
          "type": "blob",
          "size": 9483
        },
        {
          "path": ".claude/skills/system-design-interrogation/references/scale-questions.md",
          "type": "blob",
          "size": 5614
        },
        {
          "path": ".claude/skills/system-design-interrogation/references/security-questions.md",
          "type": "blob",
          "size": 7515
        },
        {
          "path": ".claude/skills/test-data-management",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-data-management/SKILL.md",
          "type": "blob",
          "size": 5173
        },
        {
          "path": ".claude/skills/test-data-management/capabilities.json",
          "type": "blob",
          "size": 656
        },
        {
          "path": ".claude/skills/test-data-management/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-data-management/checklists/test-data-checklist.md",
          "type": "blob",
          "size": 749
        },
        {
          "path": ".claude/skills/test-data-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-data-management/references/factory-patterns.md",
          "type": "blob",
          "size": 2297
        },
        {
          "path": ".claude/skills/test-data-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-data-management/templates/factory-boy.py",
          "type": "blob",
          "size": 5909
        },
        {
          "path": ".claude/skills/test-standards-enforcer",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-standards-enforcer/SKILL.md",
          "type": "blob",
          "size": 7599
        },
        {
          "path": ".claude/skills/test-standards-enforcer/capabilities.json",
          "type": "blob",
          "size": 666
        },
        {
          "path": ".claude/skills/test-standards-enforcer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/test-standards-enforcer/references/naming-conventions.md",
          "type": "blob",
          "size": 2278
        },
        {
          "path": ".claude/skills/type-safety-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/type-safety-validation/SKILL.md",
          "type": "blob",
          "size": 20105
        },
        {
          "path": ".claude/skills/type-safety-validation/capabilities.json",
          "type": "blob",
          "size": 1038
        },
        {
          "path": ".claude/skills/type-safety-validation/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/type-safety-validation/checklists/type-safety-checklist.md",
          "type": "blob",
          "size": 9836
        },
        {
          "path": ".claude/skills/type-safety-validation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/type-safety-validation/examples/skillforge-type-safety.md",
          "type": "blob",
          "size": 13883
        },
        {
          "path": ".claude/skills/type-safety-validation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/type-safety-validation/references/prisma-types.md",
          "type": "blob",
          "size": 11241
        },
        {
          "path": ".claude/skills/type-safety-validation/references/trpc-setup.md",
          "type": "blob",
          "size": 13316
        },
        {
          "path": ".claude/skills/type-safety-validation/references/ty-type-checker-patterns.md",
          "type": "blob",
          "size": 7432
        },
        {
          "path": ".claude/skills/type-safety-validation/references/typescript-5-features.md",
          "type": "blob",
          "size": 11600
        },
        {
          "path": ".claude/skills/type-safety-validation/references/zod-patterns.md",
          "type": "blob",
          "size": 10944
        },
        {
          "path": ".claude/skills/type-safety-validation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/type-safety-validation/templates/trpc-router.ts",
          "type": "blob",
          "size": 10303
        },
        {
          "path": ".claude/skills/type-safety-validation/templates/zod-schema.ts",
          "type": "blob",
          "size": 10309
        },
        {
          "path": ".claude/skills/unit-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/unit-testing/SKILL.md",
          "type": "blob",
          "size": 5714
        },
        {
          "path": ".claude/skills/unit-testing/capabilities.json",
          "type": "blob",
          "size": 621
        },
        {
          "path": ".claude/skills/unit-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/unit-testing/examples/skillforge-test-strategy.md",
          "type": "blob",
          "size": 18553
        },
        {
          "path": ".claude/skills/unit-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/unit-testing/references/aaa-pattern.md",
          "type": "blob",
          "size": 1910
        },
        {
          "path": ".claude/skills/unit-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/unit-testing/templates/pytest-fixture.py",
          "type": "blob",
          "size": 4281
        },
        {
          "path": ".claude/skills/unit-testing/templates/test-case-template.md",
          "type": "blob",
          "size": 6821
        },
        {
          "path": ".claude/skills/vcr-http-recording",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/vcr-http-recording/SKILL.md",
          "type": "blob",
          "size": 5602
        },
        {
          "path": ".claude/skills/vcr-http-recording/capabilities.json",
          "type": "blob",
          "size": 686
        },
        {
          "path": ".claude/skills/vcr-http-recording/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/vcr-http-recording/checklists/vcr-checklist.md",
          "type": "blob",
          "size": 1326
        },
        {
          "path": ".claude/skills/vcr-http-recording/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/vcr-http-recording/templates/vcr-cassette.py",
          "type": "blob",
          "size": 5444
        },
        {
          "path": ".claude/skills/verify",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/verify/SKILL.md",
          "type": "blob",
          "size": 2897
        },
        {
          "path": ".claude/skills/verify/capabilities.json",
          "type": "blob",
          "size": 1063
        },
        {
          "path": ".claude/skills/verify/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/verify/references/report-template.md",
          "type": "blob",
          "size": 1984
        },
        {
          "path": ".claude/skills/webapp-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/webapp-testing/SKILL.md",
          "type": "blob",
          "size": 4509
        },
        {
          "path": ".claude/skills/webapp-testing/capabilities.json",
          "type": "blob",
          "size": 985
        },
        {
          "path": ".claude/skills/webapp-testing/checklists",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/webapp-testing/checklists/e2e-testing-checklist.md",
          "type": "blob",
          "size": 11927
        },
        {
          "path": ".claude/skills/webapp-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/webapp-testing/examples/skillforge-e2e-tests.md",
          "type": "blob",
          "size": 21288
        },
        {
          "path": ".claude/skills/webapp-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/webapp-testing/references/generator-agent.md",
          "type": "blob",
          "size": 5953
        },
        {
          "path": ".claude/skills/webapp-testing/references/healer-agent.md",
          "type": "blob",
          "size": 1953
        },
        {
          "path": ".claude/skills/webapp-testing/references/planner-agent.md",
          "type": "blob",
          "size": 4573
        },
        {
          "path": ".claude/skills/webapp-testing/references/playwright-setup.md",
          "type": "blob",
          "size": 3483
        },
        {
          "path": ".claude/skills/webapp-testing/references/visual-regression.md",
          "type": "blob",
          "size": 10936
        },
        {
          "path": ".claude/skills/webapp-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/webapp-testing/templates/playwright-test-template.ts",
          "type": "blob",
          "size": 13446
        },
        {
          "path": ".claude/skills/worktree-coordination",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/worktree-coordination/SKILL.md",
          "type": "blob",
          "size": 6558
        },
        {
          "path": ".claude/skills/worktree-coordination/capabilities.json",
          "type": "blob",
          "size": 656
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/ci.yml",
          "type": "blob",
          "size": 7664
        },
        {
          "path": ".github/workflows/plugin-validation.yml",
          "type": "blob",
          "size": 7723
        },
        {
          "path": ".github/workflows/version-check.yml",
          "type": "blob",
          "size": 5778
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 1161
        },
        {
          "path": ".mcp.json",
          "type": "blob",
          "size": 1040
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 24665
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 16357
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5874
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1080
        },
        {
          "path": "Logo.png",
          "type": "blob",
          "size": 4945489
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 20086
        },
        {
          "path": "agents",
          "type": "blob",
          "size": 14
        },
        {
          "path": "bin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bin/bump-version.sh",
          "type": "blob",
          "size": 5703
        },
        {
          "path": "bin/cc-worktree-new",
          "type": "blob",
          "size": 7735
        },
        {
          "path": "bin/cc-worktree-remove",
          "type": "blob",
          "size": 6712
        },
        {
          "path": "bin/cc-worktree-status",
          "type": "blob",
          "size": 7362
        },
        {
          "path": "bin/cc-worktree-sync",
          "type": "blob",
          "size": 3736
        },
        {
          "path": "bin/count-components.sh",
          "type": "blob",
          "size": 2687
        },
        {
          "path": "bin/migrate-capabilities.sh",
          "type": "blob",
          "size": 8786
        },
        {
          "path": "bin/update-counts.sh",
          "type": "blob",
          "size": 6287
        },
        {
          "path": "bin/validate-counts.sh",
          "type": "blob",
          "size": 3382
        },
        {
          "path": "bin/validate-token-budget.sh",
          "type": "blob",
          "size": 5578
        },
        {
          "path": "hooks",
          "type": "blob",
          "size": 13
        },
        {
          "path": "plugin.json",
          "type": "blob",
          "size": 949
        },
        {
          "path": "skills",
          "type": "blob",
          "size": 14
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/ci",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/ci/lint.sh",
          "type": "blob",
          "size": 8077
        },
        {
          "path": "tests/compliance",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/compliance/test-cc-212-compliance.sh",
          "type": "blob",
          "size": 11586
        },
        {
          "path": "tests/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/config/test-config-system.sh",
          "type": "blob",
          "size": 9323
        },
        {
          "path": "tests/coverage",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/coverage/hook-coverage-report.sh",
          "type": "blob",
          "size": 6602
        },
        {
          "path": "tests/e2e",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/e2e/test-agent-lifecycle.sh",
          "type": "blob",
          "size": 12069
        },
        {
          "path": "tests/e2e/test-coordination-e2e.sh",
          "type": "blob",
          "size": 12773
        },
        {
          "path": "tests/e2e/test-progressive-loading.sh",
          "type": "blob",
          "size": 11895
        },
        {
          "path": "tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/fixtures/hook-inputs.json",
          "type": "blob",
          "size": 1635
        },
        {
          "path": "tests/fixtures/security-payloads.json",
          "type": "blob",
          "size": 3493
        },
        {
          "path": "tests/fixtures/test-helpers.sh",
          "type": "blob",
          "size": 17953
        },
        {
          "path": "tests/integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/integration/test-context-system.sh",
          "type": "blob",
          "size": 9296
        },
        {
          "path": "tests/integration/test-coordination-hooks.sh",
          "type": "blob",
          "size": 8805
        },
        {
          "path": "tests/integration/test-hook-chains.sh",
          "type": "blob",
          "size": 5063
        },
        {
          "path": "tests/integration/test-multi-instance-gates.sh",
          "type": "blob",
          "size": 7649
        },
        {
          "path": "tests/integration/test-plugin-installation.sh",
          "type": "blob",
          "size": 7070
        },
        {
          "path": "tests/performance",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/performance/test-hook-timing.sh",
          "type": "blob",
          "size": 10383
        },
        {
          "path": "tests/performance/test-token-budget.sh",
          "type": "blob",
          "size": 11797
        },
        {
          "path": "tests/run-all-tests.sh",
          "type": "blob",
          "size": 12183
        },
        {
          "path": "tests/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/schemas/context-identity.schema.json",
          "type": "blob",
          "size": 1740
        },
        {
          "path": "tests/schemas/context-session.schema.json",
          "type": "blob",
          "size": 2074
        },
        {
          "path": "tests/security",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/security/test-additional-security.sh",
          "type": "blob",
          "size": 13644
        },
        {
          "path": "tests/security/test-command-injection.sh",
          "type": "blob",
          "size": 7797
        },
        {
          "path": "tests/security/test-input-validation.sh",
          "type": "blob",
          "size": 9158
        },
        {
          "path": "tests/security/test-jq-injection.sh",
          "type": "blob",
          "size": 6217
        },
        {
          "path": "tests/security/test-path-traversal.sh",
          "type": "blob",
          "size": 9246
        },
        {
          "path": "tests/security/test-symlink-attacks.sh",
          "type": "blob",
          "size": 13607
        },
        {
          "path": "tests/security/test-unicode-attacks.sh",
          "type": "blob",
          "size": 13083
        },
        {
          "path": "tests/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/skills/integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/skills/integration/test-skill-agent-integration.sh",
          "type": "blob",
          "size": 9538
        },
        {
          "path": "tests/skills/progressive-loading",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/skills/progressive-loading/test-tier-loading.sh",
          "type": "blob",
          "size": 35743
        },
        {
          "path": "tests/skills/run-skill-tests.sh",
          "type": "blob",
          "size": 17373
        },
        {
          "path": "tests/skills/semantic-matching",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/skills/semantic-matching/test-skill-discovery.sh",
          "type": "blob",
          "size": 44735
        },
        {
          "path": "tests/skills/structure",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/skills/structure/test-capabilities-json.sh",
          "type": "blob",
          "size": 13637
        },
        {
          "path": "tests/skills/structure/test-skill-md.sh",
          "type": "blob",
          "size": 18814
        },
        {
          "path": "tests/subagents",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/subagents/definition",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/subagents/definition/test-agent-definitions.sh",
          "type": "blob",
          "size": 8198
        },
        {
          "path": "tests/unit",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/unit/test-context-schemas.sh",
          "type": "blob",
          "size": 5009
        },
        {
          "path": "tests/unit/test-coordination-lib.sh",
          "type": "blob",
          "size": 14193
        },
        {
          "path": "tests/unit/test-count-components.sh",
          "type": "blob",
          "size": 9732
        },
        {
          "path": "tests/unit/test-edge-cases.sh",
          "type": "blob",
          "size": 9425
        },
        {
          "path": "tests/unit/test-file-lock-hooks.sh",
          "type": "blob",
          "size": 28120
        },
        {
          "path": "tests/unit/test-hook-executability.sh",
          "type": "blob",
          "size": 2425
        },
        {
          "path": "tests/unit/test-hook-json-output.sh",
          "type": "blob",
          "size": 8583
        },
        {
          "path": "tests/unit/test-hooks-unit.sh",
          "type": "blob",
          "size": 7453
        },
        {
          "path": "tests/unit/test-json-validity.sh",
          "type": "blob",
          "size": 2081
        },
        {
          "path": "tests/unit/test-lifecycle-hooks.sh",
          "type": "blob",
          "size": 33542
        },
        {
          "path": "tests/unit/test-permission-posttool-hooks.sh",
          "type": "blob",
          "size": 31720
        },
        {
          "path": "tests/unit/test-pretool-all-hooks.sh",
          "type": "blob",
          "size": 8100
        },
        {
          "path": "tests/unit/test-pretool-mcp-hooks.sh",
          "type": "blob",
          "size": 7048
        },
        {
          "path": "tests/unit/test-pretool-skill-hooks.sh",
          "type": "blob",
          "size": 3788
        },
        {
          "path": "tests/unit/test-pretool-task-hooks.sh",
          "type": "blob",
          "size": 6989
        },
        {
          "path": "tests/unit/test-prompt-hooks.sh",
          "type": "blob",
          "size": 3723
        },
        {
          "path": "tests/unit/test-shell-syntax.sh",
          "type": "blob",
          "size": 1545
        },
        {
          "path": "tests/unit/test-skill-hooks.sh",
          "type": "blob",
          "size": 11693
        },
        {
          "path": "tests/unit/test-subagent-hooks.sh",
          "type": "blob",
          "size": 5646
        },
        {
          "path": "tests/unit/test-worktree-cli.sh",
          "type": "blob",
          "size": 8371
        }
      ],
      "marketplace": {
        "name": "skillforge",
        "version": "4.7.4",
        "description": "The Complete AI Development Toolkit - 90 skills, 20 agents, 96 hooks for full-stack development",
        "owner_info": {
          "name": "Yonatan Gross",
          "email": "yonatan2gross@gmail.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "skf",
            "description": "Complete AI development toolkit with 90 skills covering AI/LLM, backend, frontend, security, and testing patterns. Includes 20 specialized agents and 96 lifecycle hooks.",
            "source": "./",
            "category": "development",
            "version": "4.7.4",
            "author": {
              "name": "Yonatan Gross",
              "email": "yonatan2gross@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add yonatangross/skillforge-claude-plugin",
              "/plugin install skf@skillforge"
            ],
            "signals": {
              "stars": 17,
              "forks": 3,
              "pushed_at": "2026-01-12T07:32:48Z",
              "created_at": "2025-12-31T05:58:52Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "add-golden",
                "description": "Curate and add documents to the golden dataset with multi-agent validation",
                "path": ".claude/skills/add-golden/SKILL.md",
                "frontmatter": {
                  "name": "add-golden",
                  "description": "Curate and add documents to the golden dataset with multi-agent validation",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "curation",
                    "golden-dataset",
                    "evaluation",
                    "testing"
                  ]
                },
                "content": "# Add to Golden Dataset\n\nMulti-agent curation workflow for adding high-quality documents.\n\n## When to Use\n\n- Adding documents to evaluation dataset\n- Curating test content\n- Building retrieval benchmarks\n- Quality control for RAG systems\n\n## Quick Start\n\n```bash\n/add-golden https://example.com/article\n/add-golden https://arxiv.org/abs/2312.xxxxx\n```\n\n## Phase 1: Input Collection\n\nGet URL and detect content type:\n- article (blog post, tech article)\n- tutorial (step-by-step guide)\n- documentation (API docs, reference)\n- research_paper (academic, whitepaper)\n\n## Phase 2: Fetch and Extract\n\nExtract document structure:\n- Title and sections\n- Code blocks\n- Key technical terms\n- Metadata (author, date)\n\n## Phase 3: Parallel Analysis (4 Agents)\n\n| Agent | Task |\n|-------|------|\n| code-quality-reviewer | Quality evaluation |\n| Explore #1 | Difficulty classification |\n| Explore #2 | Domain tagging |\n| Explore #3 | Test query generation |\n\n### Quality Dimensions\n\n| Dimension | Weight |\n|-----------|--------|\n| Accuracy | 0.25 |\n| Coherence | 0.20 |\n| Depth | 0.25 |\n| Relevance | 0.30 |\n\n### Difficulty Levels\n\n- trivial: Direct keyword match (>0.85 score)\n- easy: Common synonyms (>0.70 score)\n- medium: Paraphrased intent (>0.55 score)\n- hard: Multi-hop reasoning (>0.40 score)\n- adversarial: Edge cases, robustness\n\n## Phase 4: Validation Checks\n\n- URL validation (no placeholders)\n- Schema validation (required fields)\n- Duplicate check (>80% similarity)\n- Quality gates (min sections, content length)\n\n## Phase 5: Decision Thresholds\n\n| Score | Decision |\n|-------|----------|\n| >= 0.75 | INCLUDE |\n| >= 0.55 | REVIEW |\n| < 0.55 | EXCLUDE |\n\n## Phase 6: User Approval\n\nPresent results for user decision:\n- Approve: Add with generated queries\n- Modify: Edit details before adding\n- Reject: Do not add\n\n## Phase 7: Write to Dataset\n\nUpdate fixture files:\n- `documents_expanded.json`\n- `source_url_map.json`\n- `queries.json`\n\nValidate fixture consistency after writing.\n\n## Summary\n\n**Total Parallel Agents: 4**\n- 1 code-quality-reviewer\n- 3 Explore agents\n\n**Quality Gates:**\n- Minimum score: 0.55 for review\n- No placeholder URLs\n- No duplicates (>90% similar)\n- At least 2 tags, 2 sections\n\n## References\n\n- [Quality Scoring](references/quality-scoring.md)"
              },
              {
                "name": "agent-loops",
                "description": "Agentic workflow patterns for autonomous LLM reasoning. Use when building ReAct agents, implementing reasoning loops, or creating LLMs that plan and execute multi-step tasks.",
                "path": ".claude/skills/agent-loops/SKILL.md",
                "frontmatter": {
                  "name": "agent-loops",
                  "description": "Agentic workflow patterns for autonomous LLM reasoning. Use when building ReAct agents, implementing reasoning loops, or creating LLMs that plan and execute multi-step tasks.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# Agent Loops\n\nEnable LLMs to reason, plan, and take autonomous actions.\n\n## When to Use\n\n- Multi-step problem solving\n- Tasks requiring planning\n- Autonomous tool use\n- Self-correcting workflows\n\n## ReAct Pattern (Reasoning + Acting)\n\n```python\nREACT_PROMPT = \"\"\"You are an agent that reasons step by step.\n\nFor each step, respond with:\nThought: [your reasoning about what to do next]\nAction: [tool_name(arg1, arg2)]\nObservation: [you'll see the result here]\n\nWhen you have the final answer:\nThought: I now have enough information\nFinal Answer: [your response]\n\nAvailable tools: {tools}\n\nQuestion: {question}\n\"\"\"\n\nasync def react_loop(question: str, tools: dict, max_steps: int = 10) -> str:\n    \"\"\"Execute ReAct reasoning loop.\"\"\"\n    history = REACT_PROMPT.format(tools=list(tools.keys()), question=question)\n\n    for step in range(max_steps):\n        response = await llm.chat([{\"role\": \"user\", \"content\": history}])\n        history += response.content\n\n        # Check for final answer\n        if \"Final Answer:\" in response.content:\n            return response.content.split(\"Final Answer:\")[-1].strip()\n\n        # Extract and execute action\n        if \"Action:\" in response.content:\n            action = parse_action(response.content)\n            result = await tools[action.name](*action.args)\n            history += f\"\\nObservation: {result}\\n\"\n\n    return \"Max steps reached without answer\"\n```\n\n## Plan-and-Execute Pattern\n\n```python\nasync def plan_and_execute(goal: str) -> str:\n    \"\"\"Create plan first, then execute steps.\"\"\"\n    # 1. Generate plan\n    plan = await llm.chat([{\n        \"role\": \"user\",\n        \"content\": f\"Create a step-by-step plan to: {goal}\\n\\nFormat as numbered list.\"\n    }])\n\n    steps = parse_plan(plan.content)\n    results = []\n\n    # 2. Execute each step\n    for i, step in enumerate(steps):\n        result = await execute_step(step, context=results)\n        results.append({\"step\": step, \"result\": result})\n\n        # 3. Check if replanning needed\n        if should_replan(results):\n            return await plan_and_execute(\n                f\"{goal}\\n\\nProgress so far: {results}\"\n            )\n\n    # 4. Synthesize final answer\n    return await synthesize(goal, results)\n```\n\n## Self-Correction Loop\n\n```python\nasync def self_correcting_agent(task: str, max_retries: int = 3) -> str:\n    \"\"\"Agent that validates and corrects its own output.\"\"\"\n    for attempt in range(max_retries):\n        # Generate response\n        response = await llm.chat([{\n            \"role\": \"user\",\n            \"content\": task\n        }])\n\n        # Self-validate\n        validation = await llm.chat([{\n            \"role\": \"user\",\n            \"content\": f\"\"\"Validate this response for the task: {task}\n\nResponse: {response.content}\n\nCheck for:\n1. Correctness - Is it factually accurate?\n2. Completeness - Does it fully answer the task?\n3. Format - Is it properly formatted?\n\nIf valid, respond: VALID\nIf invalid, respond: INVALID: [what's wrong and how to fix]\"\"\"\n        }])\n\n        if \"VALID\" in validation.content:\n            return response.content\n\n        # Correct based on feedback\n        task = f\"{task}\\n\\nPrevious attempt had issues: {validation.content}\"\n\n    return response.content  # Return best attempt\n```\n\n## Memory Management\n\n```python\nclass AgentMemory:\n    \"\"\"Sliding window memory for agents.\"\"\"\n\n    def __init__(self, max_messages: int = 20):\n        self.messages = []\n        self.max_messages = max_messages\n        self.summary = \"\"\n\n    def add(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n        # Summarize old messages when window full\n        if len(self.messages) > self.max_messages:\n            self._compress()\n\n    def _compress(self):\n        \"\"\"Summarize oldest messages.\"\"\"\n        old = self.messages[:10]\n        self.messages = self.messages[10:]\n\n        # Async summarize would be better\n        summary = summarize(old)\n        self.summary = f\"{self.summary}\\n{summary}\"\n\n    def get_context(self) -> list:\n        \"\"\"Get messages with summary prefix.\"\"\"\n        context = []\n        if self.summary:\n            context.append({\n                \"role\": \"system\",\n                \"content\": f\"Previous context summary: {self.summary}\"\n            })\n        return context + self.messages\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Max steps | 5-15 (prevent infinite loops) |\n| Temperature | 0.3-0.7 (balance creativity/focus) |\n| Memory window | 10-20 messages |\n| Validation | Every 3-5 steps |\n\n## Common Mistakes\n\n- No step limit (infinite loops)\n- No memory management (context overflow)\n- No error recovery (crashes on tool failure)\n- Over-complex prompts (agent gets confused)\n\n## Related Skills\n\n- `function-calling` - Tool definitions and execution\n- `multi-agent-orchestration` - Coordinating multiple agents\n- `langgraph-workflows` - Stateful agent graphs\n\n## Capability Details\n\n### react-loop\n**Keywords:** react, reason, act, observe, loop\n**Solves:**\n- Implement ReAct pattern\n- Create reasoning loops\n- Build iterative agents\n\n### tool-use\n**Keywords:** tool, function, call, execution\n**Solves:**\n- Implement tool calling\n- Execute functions from LLM\n- Parse tool responses\n\n### workflow-template\n**Keywords:** template, workflow, agent, typescript\n**Solves:**\n- Agent workflow template\n- TypeScript implementation\n- Copy-paste starter"
              },
              {
                "name": "api-design-framework",
                "description": "Use this skill when designing REST, GraphQL, or gRPC APIs. Provides comprehensive API design patterns, versioning strategies, error handling conventions, authentication approaches, and OpenAPI/AsyncAPI templates. Ensures consistent, well-documented, and developer-friendly APIs across all backend services.",
                "path": ".claude/skills/api-design-framework/SKILL.md",
                "frontmatter": {
                  "name": "api-design-framework",
                  "description": "Use this skill when designing REST, GraphQL, or gRPC APIs. Provides comprehensive API design patterns, versioning strategies, error handling conventions, authentication approaches, and OpenAPI/AsyncAPI templates. Ensures consistent, well-documented, and developer-friendly APIs across all backend services.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.1.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "api",
                    "rest",
                    "graphql",
                    "grpc",
                    "backend",
                    "documentation"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/design-decision-saver.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/design-decision-saver.sh"
                      }
                    ]
                  }
                },
                "content": "# API Design Framework\n\n## Overview\n\nThis skill provides comprehensive guidance for designing robust, scalable, and developer-friendly APIs. Whether building REST, GraphQL, or gRPC services, this framework ensures consistency, usability, and maintainability.\n\n**When to use this skill:**\n- Designing new API endpoints or services\n- Establishing API conventions for a team or organization\n- Reviewing API designs for consistency and best practices\n- Migrating or versioning existing APIs\n- Creating API documentation (OpenAPI, AsyncAPI)\n- Choosing between REST, GraphQL, or gRPC\n\n## API Design Principles\n\n### 1. Developer Experience First\nAPIs should be intuitive and self-documenting:\n- Clear, consistent naming conventions\n- Predictable behavior and responses\n- Comprehensive documentation\n- Helpful error messages\n\n### 2. Consistency Over Cleverness\nFollow established patterns rather than inventing new ones:\n- Standard HTTP methods and status codes (REST)\n- Conventional query structures (GraphQL)\n- Idiomatic proto definitions (gRPC)\n\n### 3. Evolution Without Breaking Changes\nDesign for change from day one:\n- API versioning strategy\n- Backward compatibility considerations\n- Deprecation policies\n- Migration paths\n\n### 4. Performance by Design\nConsider performance implications:\n- Pagination for large datasets\n- Filtering and partial responses\n- Caching strategies\n- Rate limiting\n\n---\n\n## REST API Design\n\n### Resource Naming Conventions\n\n**Use plural nouns for resources:**\n```\n GET /users\n GET /users/123\n GET /users/123/orders\n\n GET /user\n GET /getUser\n GET /user/123\n```\n\n**Use hierarchical relationships:**\n```\n GET /users/123/orders          # Orders for specific user\n GET /teams/5/members           # Members of specific team\n POST /projects/10/tasks        # Create task in project 10\n\n GET /userOrders/123            # Flat structure\n GET /orders?userId=123         # Query param for relationship\n```\n\n**Use kebab-case for multi-word resources:**\n```\n /shopping-carts\n /order-items\n /user-preferences\n\n /shoppingCarts    (camelCase)\n /shopping_carts   (snake_case)\n /ShoppingCarts    (PascalCase)\n```\n\n### HTTP Methods (Verbs)\n\n| Method | Purpose | Idempotent | Safe | Example |\n|--------|---------|------------|------|---------|\n| **GET** | Retrieve resource(s) | Yes | Yes | `GET /users/123` |\n| **POST** | Create resource | No | No | `POST /users` |\n| **PUT** | Replace entire resource | Yes | No | `PUT /users/123` |\n| **PATCH** | Partial update | No* | No | `PATCH /users/123` |\n| **DELETE** | Remove resource | Yes | No | `DELETE /users/123` |\n| **HEAD** | Metadata only (no body) | Yes | Yes | `HEAD /users/123` |\n| **OPTIONS** | Allowed methods | Yes | Yes | `OPTIONS /users` |\n\n*PATCH can be designed to be idempotent\n\n### Status Codes\n\n#### Success (2xx)\n- **200 OK**: Successful GET, PUT, PATCH, or DELETE\n- **201 Created**: Successful POST (include `Location` header)\n- **202 Accepted**: Request accepted, processing async\n- **204 No Content**: Successful DELETE or PUT with no response body\n\n#### Client Errors (4xx)\n- **400 Bad Request**: Invalid request body or parameters\n- **401 Unauthorized**: Missing or invalid authentication\n- **403 Forbidden**: Authenticated but not authorized\n- **404 Not Found**: Resource doesn't exist\n- **405 Method Not Allowed**: HTTP method not supported for resource\n- **409 Conflict**: Resource conflict (e.g., duplicate)\n- **422 Unprocessable Entity**: Validation failed\n- **429 Too Many Requests**: Rate limit exceeded\n\n#### Server Errors (5xx)\n- **500 Internal Server Error**: Generic server error\n- **502 Bad Gateway**: Upstream service error\n- **503 Service Unavailable**: Temporary unavailability\n- **504 Gateway Timeout**: Upstream timeout\n\n### Request/Response Formats\n\n**Request Body (POST/PUT/PATCH):**\n```json\nPOST /users\nContent-Type: application/json\n\n{\n  \"email\": \"jane@example.com\",\n  \"name\": \"Jane Smith\",\n  \"role\": \"developer\"\n}\n```\n\n**Success Response:**\n```json\nHTTP/1.1 201 Created\nLocation: /users/123\nContent-Type: application/json\n\n{\n  \"id\": 123,\n  \"email\": \"jane@example.com\",\n  \"name\": \"Jane Smith\",\n  \"role\": \"developer\",\n  \"created_at\": \"2025-10-31T10:30:00Z\",\n  \"updated_at\": \"2025-10-31T10:30:00Z\"\n}\n```\n\n**Error Response (Standard Format):**\n```json\nHTTP/1.1 422 Unprocessable Entity\nContent-Type: application/json\n\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Request validation failed\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"Email is already registered\",\n        \"code\": \"DUPLICATE_EMAIL\"\n      },\n      {\n        \"field\": \"name\",\n        \"message\": \"Name must be at least 2 characters\",\n        \"code\": \"NAME_TOO_SHORT\"\n      }\n    ],\n    \"timestamp\": \"2025-10-31T10:30:00Z\",\n    \"request_id\": \"req_abc123\"\n  }\n}\n```\n\n### Pagination\n\n**Cursor-Based Pagination (Recommended):**\n```\nGET /users?cursor=eyJpZCI6MTIzfQ&limit=20\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"next_cursor\": \"eyJpZCI6MTQzfQ\",\n    \"has_more\": true\n  }\n}\n```\n\n**Pros**: Consistent results even as data changes\n**Use for**: Large datasets, real-time data, infinite scroll\n\n**Offset-Based Pagination:**\n```\nGET /users?page=2&per_page=20\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"page\": 2,\n    \"per_page\": 20,\n    \"total\": 487,\n    \"total_pages\": 25\n  }\n}\n```\n\n**Pros**: Easy to understand, supports \"jump to page N\"\n**Use for**: Small datasets, admin panels, known bounds\n\n### Filtering and Sorting\n\n**Filtering:**\n```\nGET /users?status=active&role=developer&created_after=2025-01-01\nGET /products?price_min=10&price_max=100&category=electronics\n```\n\n**Sorting:**\n```\nGET /users?sort=created_at:desc\nGET /users?sort=-created_at              # Minus prefix for descending\nGET /users?sort=name:asc,created_at:desc # Multiple fields\n```\n\n**Field Selection (Partial Response):**\n```\nGET /users?fields=id,name,email          # Only specified fields\nGET /users/123?exclude=password_hash     # All except specified\n```\n\n### API Versioning\n\n#### Strategy 1: URI Versioning (Recommended)\n```\n /api/v1/users\n /api/v2/users\n\nPros: Clear, easy to test, cache-friendly\nCons: Verbose URLs\n```\n\n#### Strategy 2: Header Versioning\n```\nGET /api/users\nAccept: application/vnd.company.v2+json\n\nPros: Clean URLs\nCons: Harder to test, not visible in URL\n```\n\n#### Strategy 3: Query Parameter\n```\nGET /api/users?version=2\n\nPros: Simple\nCons: Can be forgotten, mixes with business logic params\n```\n\n**Best Practice:** URI versioning for public APIs, header versioning for internal services\n\n### Rate Limiting\n\n**Response Headers:**\n```\nHTTP/1.1 200 OK\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 987\nX-RateLimit-Reset: 1635724800\n\nResponse when exceeded:\nHTTP/1.1 429 Too Many Requests\nRetry-After: 3600\n\n{\n  \"error\": {\n    \"code\": \"RATE_LIMIT_EXCEEDED\",\n    \"message\": \"API rate limit exceeded\",\n    \"retry_after\": 3600\n  }\n}\n```\n\n### Authentication & Authorization\n\n**Bearer Token (JWT):**\n```\nGET /users/me\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\n```\n\n**API Key:**\n```\nGET /users\nX-API-Key: sk_live_abc123...\n```\n\n**Basic Auth (avoid for production):**\n```\nGET /users\nAuthorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ=\n```\n\n---\n\n## GraphQL API Design\n\n### Schema Design Principles\n\n**1. Nullable by Default**\n```graphql\ntype User {\n  id: ID!              # Non-null (required)\n  email: String!       # Non-null\n  name: String         # Nullable (optional)\n  avatar: String       # Nullable\n}\n```\n\n**2. Use Connections for Lists**\n```graphql\ntype Query {\n  users(first: Int, after: String): UserConnection!\n}\n\ntype UserConnection {\n  edges: [UserEdge!]!\n  pageInfo: PageInfo!\n  totalCount: Int!\n}\n\ntype UserEdge {\n  node: User!\n  cursor: String!\n}\n\ntype PageInfo {\n  hasNextPage: Boolean!\n  hasPreviousPage: Boolean!\n  startCursor: String\n  endCursor: String\n}\n```\n\n**3. Input Types for Mutations**\n```graphql\ninput CreateUserInput {\n  email: String!\n  name: String!\n  role: UserRole!\n}\n\ntype Mutation {\n  createUser(input: CreateUserInput!): CreateUserPayload!\n}\n\ntype CreateUserPayload {\n  user: User!\n  errors: [UserError!]\n}\n\ntype UserError {\n  field: String!\n  message: String!\n  code: String!\n}\n```\n\n### Query Design\n\n**Fetch single resource:**\n```graphql\nquery GetUser {\n  user(id: \"123\") {\n    id\n    name\n    email\n    posts {\n      id\n      title\n    }\n  }\n}\n```\n\n**Fetch list with filters:**\n```graphql\nquery GetUsers {\n  users(\n    first: 10\n    after: \"cursor123\"\n    filter: { role: DEVELOPER, status: ACTIVE }\n  ) {\n    edges {\n      node {\n        id\n        name\n        email\n      }\n    }\n    pageInfo {\n      hasNextPage\n      endCursor\n    }\n  }\n}\n```\n\n### Error Handling\n\n**Field-Level Errors:**\n```graphql\ntype Mutation {\n  createUser(input: CreateUserInput!): CreateUserPayload!\n}\n\ntype CreateUserPayload {\n  user: User\n  errors: [UserError!]\n}\n```\n\n**Response:**\n```json\n{\n  \"data\": {\n    \"createUser\": {\n      \"user\": null,\n      \"errors\": [\n        {\n          \"field\": \"email\",\n          \"message\": \"Email is already taken\",\n          \"code\": \"DUPLICATE_EMAIL\"\n        }\n      ]\n    }\n  }\n}\n```\n\n---\n\n## gRPC API Design\n\n### Proto File Structure\n\n**user.proto:**\n```protobuf\nsyntax = \"proto3\";\n\npackage company.user.v1;\n\nimport \"google/protobuf/timestamp.proto\";\nimport \"google/protobuf/empty.proto\";\n\n// User service definition\nservice UserService {\n  // Get user by ID\n  rpc GetUser(GetUserRequest) returns (GetUserResponse);\n\n  // List users with pagination\n  rpc ListUsers(ListUsersRequest) returns (ListUsersResponse);\n\n  // Create new user\n  rpc CreateUser(CreateUserRequest) returns (CreateUserResponse);\n\n  // Update user\n  rpc UpdateUser(UpdateUserRequest) returns (UpdateUserResponse);\n\n  // Delete user\n  rpc DeleteUser(DeleteUserRequest) returns (google.protobuf.Empty);\n\n  // Stream updates (server streaming)\n  rpc WatchUsers(WatchUsersRequest) returns (stream UserEvent);\n}\n\n// Messages\nmessage User {\n  string id = 1;\n  string email = 2;\n  string name = 3;\n  UserRole role = 4;\n  google.protobuf.Timestamp created_at = 5;\n  google.protobuf.Timestamp updated_at = 6;\n}\n\nenum UserRole {\n  USER_ROLE_UNSPECIFIED = 0;\n  USER_ROLE_ADMIN = 1;\n  USER_ROLE_DEVELOPER = 2;\n  USER_ROLE_VIEWER = 3;\n}\n\nmessage GetUserRequest {\n  string id = 1;\n}\n\nmessage GetUserResponse {\n  User user = 1;\n}\n\nmessage ListUsersRequest {\n  int32 page_size = 1;\n  string page_token = 2;\n  string filter = 3;  // e.g., \"role=DEVELOPER AND status=ACTIVE\"\n}\n\nmessage ListUsersResponse {\n  repeated User users = 1;\n  string next_page_token = 2;\n  int32 total_size = 3;\n}\n\nmessage CreateUserRequest {\n  string email = 1;\n  string name = 2;\n  UserRole role = 3;\n}\n\nmessage CreateUserResponse {\n  User user = 1;\n}\n```\n\n### Error Handling\n\n**Use gRPC status codes:**\n```go\n// OK: Success\n// CANCELLED: Client cancelled\n// INVALID_ARGUMENT: Invalid request (400 equivalent)\n// NOT_FOUND: Resource not found (404 equivalent)\n// ALREADY_EXISTS: Duplicate (409 equivalent)\n// PERMISSION_DENIED: Forbidden (403 equivalent)\n// UNAUTHENTICATED: Auth required (401 equivalent)\n// RESOURCE_EXHAUSTED: Rate limit (429 equivalent)\n// INTERNAL: Server error (500 equivalent)\n```\n\n---\n\n## API Documentation\n\n### OpenAPI 3.1 Structure\n\nSee `/templates/openapi-template.yaml` for complete example.\n\n**Key sections:**\n- **info**: API metadata (title, version, description)\n- **servers**: Base URLs for different environments\n- **paths**: Endpoints with operations\n- **components**: Reusable schemas, responses, parameters\n- **security**: Authentication schemes\n\n### AsyncAPI 3.0 (Event-Driven)\n\nFor documenting message-based APIs (Kafka, RabbitMQ, WebSockets).\n\nSee `/templates/asyncapi-template.yaml` for complete example.\n\n---\n\n## Best Practices\n\n### 1. Use Standard Media Types\n```\nContent-Type: application/json          # JSON\nContent-Type: application/xml           # XML\nContent-Type: application/protobuf      # Protocol Buffers\nContent-Type: application/octet-stream  # Binary data\n```\n\n### 2. HATEOAS (Optional for REST)\nInclude links for related resources:\n```json\n{\n  \"id\": 123,\n  \"name\": \"Jane Smith\",\n  \"_links\": {\n    \"self\": { \"href\": \"/users/123\" },\n    \"orders\": { \"href\": \"/users/123/orders\" },\n    \"avatar\": { \"href\": \"/users/123/avatar\" }\n  }\n}\n```\n\n### 3. Idempotency Keys\nFor preventing duplicate operations:\n```\nPOST /payments\nIdempotency-Key: unique-request-id-123\n```\n\n### 4. Bulk Operations\n```\nPOST /users/bulk-create\nPOST /users/bulk-update\nPOST /users/bulk-delete\n```\n\n### 5. Webhooks\nDocument webhook payloads and retry logic:\n```json\nPOST https://client.example.com/webhook\nX-Webhook-Signature: sha256=abc123...\n\n{\n  \"event\": \"user.created\",\n  \"data\": { ... },\n  \"timestamp\": \"2025-10-31T10:30:00Z\"\n}\n```\n\n---\n\n## Common Pitfalls\n\n **Using verbs in URLs**\n```\nBad:  POST /createUser\nGood: POST /users\n```\n\n **Inconsistent naming**\n```\nBad:  /users, /userOrders, /user_preferences\nGood: /users, /orders, /preferences\n```\n\n **Ignoring HTTP methods**\n```\nBad:  POST /users/123/delete\nGood: DELETE /users/123\n```\n\n **Exposing implementation details**\n```\nBad:  /users-table, /get-user-from-db\nGood: /users, /users/123\n```\n\n **Generic error messages**\n```\nBad:  { \"error\": \"Something went wrong\" }\nGood: { \"error\": { \"code\": \"DUPLICATE_EMAIL\", \"message\": \"Email already exists\" }}\n```\n\n---\n\n## Frontend API Integration (2025 Patterns)\n\nThis section covers how frontend applications should consume APIs with type safety and resilience.\n\n### Runtime Validation with Zod\n\n**CRITICAL**: TypeScript types are erased at runtime. API responses MUST be validated:\n\n```typescript\nimport { z } from 'zod'\n\n// Define schema matching API contract\nconst UserSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  name: z.string(),\n  role: z.enum(['admin', 'developer', 'viewer']),\n  created_at: z.string().datetime(),\n})\n\nconst UsersResponseSchema = z.object({\n  data: z.array(UserSchema),\n  pagination: z.object({\n    next_cursor: z.string().nullable(),\n    has_more: z.boolean(),\n  }),\n})\n\ntype User = z.infer<typeof UserSchema>\ntype UsersResponse = z.infer<typeof UsersResponseSchema>\n\n// Fetch with validation\nasync function fetchUsers(cursor?: string): Promise<UsersResponse> {\n  const url = cursor ? \"/api/v1/users?cursor=\" + cursor : '/api/v1/users'\n  const response = await fetch(url)\n\n  if (!response.ok) {\n    throw new ApiError(response.status, await response.text())\n  }\n\n  const data = await response.json()\n  return UsersResponseSchema.parse(data) // Runtime validation!\n}\n```\n\n**Anti-patterns to avoid:**\n```typescript\n//  NEVER: Trust API response types blindly\nconst data = await response.json() as User  // Unsafe cast!\n\n//  NEVER: Skip validation \"because backend is typed\"\nconst user: User = await response.json()    // Runtime crash waiting to happen\n\n//  ALWAYS: Validate at the boundary\nconst user = UserSchema.parse(await response.json())\n```\n\n### Request Interceptors (ky/axios)\n\nUse interceptors for cross-cutting concerns:\n\n```typescript\nimport ky from 'ky'\n\n// Create configured client\nexport const api = ky.create({\n  prefixUrl: import.meta.env.VITE_API_URL,\n  timeout: 30000,\n  retry: {\n    limit: 2,\n    methods: ['get', 'head', 'options'],\n    statusCodes: [408, 429, 500, 502, 503, 504],\n    backoffLimit: 3000,\n  },\n  hooks: {\n    beforeRequest: [\n      // Auth injection\n      async (request) => {\n        const token = await getAccessToken()\n        if (token) {\n          request.headers.set('Authorization', 'Bearer ' + token)\n        }\n      },\n      // Request ID for tracing\n      (request) => {\n        request.headers.set('X-Request-ID', crypto.randomUUID())\n      },\n    ],\n    afterResponse: [\n      // Token refresh on 401\n      async (request, options, response) => {\n        if (response.status === 401) {\n          const newToken = await refreshToken()\n          if (newToken) {\n            request.headers.set('Authorization', 'Bearer ' + newToken)\n            return ky(request, options)\n          }\n        }\n        return response\n      },\n    ],\n    beforeError: [\n      // Enrich error with response body\n      async (error) => {\n        const { response } = error\n        if (response) {\n          try {\n            const body = await response.json()\n            error.message = body.error?.message || error.message\n            ;(error as any).code = body.error?.code\n          } catch {\n            // Response not JSON, keep original error\n          }\n        }\n        return error\n      },\n    ],\n  },\n})\n\n// Usage with Zod validation\nexport async function getUsers(cursor?: string): Promise<UsersResponse> {\n  const searchParams = cursor ? { cursor } : undefined\n  const data = await api.get('users', { searchParams }).json()\n  return UsersResponseSchema.parse(data)\n}\n```\n\n### Error Enrichment Pattern\n\nStructured error handling with API error codes:\n\n```typescript\n// Custom API error class\nclass ApiError extends Error {\n  constructor(\n    public status: number,\n    public code: string,\n    message: string,\n    public details?: Array<{ field: string; message: string }>\n  ) {\n    super(message)\n    this.name = 'ApiError'\n  }\n\n  get isValidationError(): boolean {\n    return this.status === 422\n  }\n\n  get isAuthError(): boolean {\n    return this.status === 401 || this.status === 403\n  }\n\n  get isRateLimited(): boolean {\n    return this.status === 429\n  }\n}\n\n// Error parsing from API response\nconst ApiErrorSchema = z.object({\n  error: z.object({\n    code: z.string(),\n    message: z.string(),\n    details: z.array(z.object({\n      field: z.string(),\n      message: z.string(),\n    })).optional(),\n  }),\n})\n\nfunction parseApiError(status: number, body: unknown): ApiError {\n  const parsed = ApiErrorSchema.safeParse(body)\n  if (parsed.success) {\n    return new ApiError(\n      status,\n      parsed.data.error.code,\n      parsed.data.error.message,\n      parsed.data.error.details\n    )\n  }\n  return new ApiError(status, 'UNKNOWN_ERROR', 'An unexpected error occurred')\n}\n```\n\n### Integration with TanStack Query\n\n```typescript\nimport { useQuery, useMutation, useQueryClient } from '@tanstack/react-query'\n\n// Query with Zod validation built-in\nexport function useUsers(cursor?: string) {\n  return useQuery({\n    queryKey: ['users', { cursor }],\n    queryFn: () => getUsers(cursor),\n    staleTime: 30_000, // 30 seconds\n  })\n}\n\n// Mutation with optimistic update\nexport function useCreateUser() {\n  const queryClient = useQueryClient()\n\n  return useMutation({\n    mutationFn: (input: CreateUserInput) =>\n      api.post('users', { json: input }).json().then(UserSchema.parse),\n    onMutate: async (newUser) => {\n      await queryClient.cancelQueries({ queryKey: ['users'] })\n      // Optimistic update...\n    },\n    onSettled: () => {\n      queryClient.invalidateQueries({ queryKey: ['users'] })\n    },\n  })\n}\n```\n\n---\n\n## Integration with Agents\n\n### Backend System Architect\n- Uses this framework when designing new APIs\n- References patterns for consistency across services\n- Creates OpenAPI specifications from templates\n\n### Frontend UI Developer\n- Reviews API contracts before implementation\n- Provides feedback on developer experience\n- Integrates with APIs following documented patterns\n- Uses Frontend API Integration patterns for type-safe consumption\n\n### Code Quality Reviewer\n- Validates API designs against this framework\n- Ensures OpenAPI docs are accurate and complete\n- Checks for REST/GraphQL/gRPC best practices\n- Verifies frontend Zod schemas match backend contracts\n\n---\n\n**Skill Version**: 1.1.0\n**Last Updated**: 2025-12-29\n**Maintained by**: AI Agent Hub Team\n\n## Changelog\n\n### v1.1.0 (2025-12-29)\n- Added Frontend API Integration (2025 Patterns) section\n- Added Zod runtime validation patterns for API responses\n- Added request interceptors with ky (auth, retry, error enrichment)\n- Added ApiError class with structured error handling\n- Added TanStack Query integration examples\n- Updated agent integration notes for frontend patterns\n\n## Capability Details\n\n### rest-design\n**Keywords:** rest, restful, http, endpoint, route, path, resource, CRUD\n**Solves:**\n- How do I design RESTful APIs?\n- REST endpoint patterns and conventions\n- HTTP methods and status codes\n- API versioning and pagination\n\n### endpoint-design\n**Keywords:** endpoint, route, path, resource, CRUD\n**Solves:**\n- How do I structure API endpoints?\n- What's the best URL pattern for this resource?\n- RESTful endpoint naming conventions\n\n### pagination\n**Keywords:** pagination, paginate, paging, offset, cursor, limit\n**Solves:**\n- How do I add pagination to an endpoint?\n- Cursor vs offset pagination\n- Pagination best practices\n\n### versioning\n**Keywords:** version, v1, v2, api version, breaking change\n**Solves:**\n- How do I version my API?\n- When to create a new API version\n- URL vs header versioning\n\n### error-handling\n**Keywords:** error, exception, status code, error response, validation error\n**Solves:**\n- How do I structure error responses?\n- Which HTTP status codes to use\n- Error message best practices\n\n### rate-limiting\n**Keywords:** rate limit, throttle, quota, requests per second, 429\n**Solves:**\n- How do I implement rate limiting?\n- Rate limit headers and responses\n- Tiered rate limiting strategies\n\n### skillforge-api\n**Keywords:** skillforge, analysis api, artifact api, sse endpoint\n**Solves:**\n- How does SkillForge API work?\n- SkillForge endpoint design decisions\n- Real-world API design examples\n\n### authentication\n**Keywords:** auth, authentication, bearer, jwt, oauth, api key\n**Solves:**\n- How do I secure API endpoints?\n- JWT vs API key authentication\n- OAuth2 flow for APIs\n\n### openapi-spec\n**Keywords:** openapi, swagger, api spec, documentation, schema\n**Solves:**\n- How do I document my API?\n- Generate OpenAPI specification\n- API documentation best practices"
              },
              {
                "name": "api-versioning",
                "description": "API versioning strategies including URL path, header, and content negotiation. Use when designing version evolution, deprecation policies, or multi-version support.",
                "path": ".claude/skills/api-versioning/SKILL.md",
                "frontmatter": {
                  "name": "api-versioning",
                  "description": "API versioning strategies including URL path, header, and content negotiation. Use when designing version evolution, deprecation policies, or multi-version support.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "tags": [
                    "api",
                    "versioning",
                    "rest",
                    "fastapi",
                    "backward-compatibility",
                    2026
                  ]
                },
                "content": "# API Versioning Strategies\n\nDesign APIs that evolve gracefully without breaking clients.\n\n## When to Use\n\n- Launching a public API\n- Planning breaking changes\n- Supporting multiple client versions\n- Implementing deprecation policies\n- Designing API evolution strategy\n\n## Strategy Comparison\n\n| Strategy | Example | Pros | Cons |\n|----------|---------|------|------|\n| URL Path | `/api/v1/users` | Simple, visible, cacheable | URL pollution |\n| Header | `X-API-Version: 1` | Clean URLs | Hidden, harder to test |\n| Query Param | `?version=1` | Easy testing | Messy, cache issues |\n| Content-Type | `Accept: application/vnd.api.v1+json` | RESTful | Complex |\n\n## URL Path Versioning (Recommended)\n\n### FastAPI Structure\n\n```\nbackend/app/\n api/\n    v1/\n       __init__.py\n       routes/\n          users.py\n          analyses.py\n       router.py\n    v2/\n       __init__.py\n       routes/\n          users.py      # Updated schemas\n          analyses.py\n       router.py\n    router.py             # Combines all versions\n```\n\n### Router Setup\n\n```python\n# backend/app/api/router.py\nfrom fastapi import APIRouter\nfrom app.api.v1.router import router as v1_router\nfrom app.api.v2.router import router as v2_router\n\napi_router = APIRouter()\napi_router.include_router(v1_router, prefix=\"/v1\")\napi_router.include_router(v2_router, prefix=\"/v2\")\n\n# main.py\napp.include_router(api_router, prefix=\"/api\")\n```\n\n### Version-Specific Schemas\n\n```python\n# v1/schemas/user.py\nclass UserResponseV1(BaseModel):\n    id: str\n    name: str  # Single name field\n\n# v2/schemas/user.py\nclass UserResponseV2(BaseModel):\n    id: str\n    first_name: str  # Split into first/last\n    last_name: str\n    full_name: str   # Computed for convenience\n```\n\n### Shared Business Logic\n\n```python\n# services/user_service.py (version-agnostic)\nclass UserService:\n    async def get_user(self, user_id: str) -> User:\n        return await self.repo.get_by_id(user_id)\n\n# v1/routes/users.py\n@router.get(\"/{user_id}\", response_model=UserResponseV1)\nasync def get_user_v1(user_id: str, service: UserService = Depends()):\n    user = await service.get_user(user_id)\n    return UserResponseV1(id=user.id, name=user.full_name)\n\n# v2/routes/users.py\n@router.get(\"/{user_id}\", response_model=UserResponseV2)\nasync def get_user_v2(user_id: str, service: UserService = Depends()):\n    user = await service.get_user(user_id)\n    return UserResponseV2(\n        id=user.id,\n        first_name=user.first_name,\n        last_name=user.last_name,\n        full_name=f\"{user.first_name} {user.last_name}\",\n    )\n```\n\n## Header-Based Versioning\n\n```python\nfrom fastapi import Header, HTTPException\n\nasync def get_api_version(\n    x_api_version: str = Header(default=\"1\", alias=\"X-API-Version\")\n) -> int:\n    try:\n        version = int(x_api_version)\n        if version not in [1, 2]:\n            raise ValueError()\n        return version\n    except ValueError:\n        raise HTTPException(400, \"Invalid API version\")\n\n@router.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: str,\n    version: int = Depends(get_api_version),\n    service: UserService = Depends(),\n):\n    user = await service.get_user(user_id)\n\n    if version == 1:\n        return UserResponseV1(id=user.id, name=user.full_name)\n    else:\n        return UserResponseV2(\n            id=user.id,\n            first_name=user.first_name,\n            last_name=user.last_name,\n        )\n```\n\n## Content Negotiation\n\n```python\nfrom fastapi import Request\n\nMEDIA_TYPES = {\n    \"application/vnd.skillforge.v1+json\": 1,\n    \"application/vnd.skillforge.v2+json\": 2,\n    \"application/json\": 2,  # Default to latest\n}\n\nasync def get_version_from_accept(request: Request) -> int:\n    accept = request.headers.get(\"Accept\", \"application/json\")\n    return MEDIA_TYPES.get(accept, 2)\n\n@router.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: str,\n    version: int = Depends(get_version_from_accept),\n):\n    ...\n```\n\n## Deprecation Headers\n\n```python\nfrom fastapi import Response\nfrom datetime import date\n\ndef add_deprecation_headers(\n    response: Response,\n    deprecated_date: date,\n    sunset_date: date,\n    link: str,\n):\n    response.headers[\"Deprecation\"] = deprecated_date.isoformat()\n    response.headers[\"Sunset\"] = sunset_date.isoformat()\n    response.headers[\"Link\"] = f'<{link}>; rel=\"successor-version\"'\n\n# Usage in v1 endpoints\n@router.get(\"/users/{user_id}\")\nasync def get_user_v1(user_id: str, response: Response):\n    add_deprecation_headers(\n        response,\n        deprecated_date=date(2025, 1, 1),\n        sunset_date=date(2025, 7, 1),\n        link=\"https://api.example.com/v2/users\",\n    )\n    return await service.get_user(user_id)\n```\n\n## Version Lifecycle\n\n```\n\n                     VERSION LIFECYCLE                           \n\n                                                                 \n              \n    ALPHA      BETA       STABLE     DEPRECATED     \n   (dev)       (test)      (prod)       (sunset)       \n              \n                                                                 \n  v3-alpha      v3-beta        v2 (current)   v1 (6 months)     \n                                                                 \n\n  POLICY:                                                        \n   Deprecation notice: 3 months before sunset                   \n   Sunset period: 6 months after deprecation                    \n   Support: Latest stable + 1 previous version                  \n\n```\n\n## Breaking vs Non-Breaking Changes\n\n### Non-Breaking (No Version Bump)\n\n```python\n# Adding optional fields\nclass UserResponse(BaseModel):\n    id: str\n    name: str\n    avatar_url: str | None = None  # New optional field\n\n# Adding new endpoints\n@router.get(\"/users/{user_id}/preferences\")  # New endpoint\n\n# Adding optional query params\n@router.get(\"/users\")\nasync def list_users(\n    limit: int = 100,\n    cursor: str | None = None,  # New pagination\n):\n```\n\n### Breaking (Requires Version Bump)\n\n```python\n# Removing fields\n# Renaming fields\n# Changing field types\n# Changing URL structure\n# Changing authentication\n# Removing endpoints\n# Changing error formats\n```\n\n## OpenAPI Per Version\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi.openapi.utils import get_openapi\n\ndef custom_openapi_v1():\n    return get_openapi(\n        title=\"SkillForge API\",\n        version=\"1.0.0\",\n        routes=v1_router.routes,\n    )\n\ndef custom_openapi_v2():\n    return get_openapi(\n        title=\"SkillForge API\",\n        version=\"2.0.0\",\n        routes=v2_router.routes,\n    )\n\napp.mount(\"/docs/v1\", create_docs_app(custom_openapi_v1))\napp.mount(\"/docs/v2\", create_docs_app(custom_openapi_v2))\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER version internal implementation\nclass UserServiceV1:  # Services should be version-agnostic\n    ...\n\n# NEVER break contracts without versioning\nclass UserResponse(BaseModel):\n    # Changed from `name` to `full_name` without version bump!\n    full_name: str\n\n# NEVER sunset without notice\n# Just removing v1 routes one day\n\n# NEVER support too many versions (max 2-3)\n/api/v1/...  # Ancient\n/api/v2/...\n/api/v3/...\n/api/v4/...  # Too many!\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Strategy | URL path (`/api/v1/`) |\n| Support window | Current + 1 previous |\n| Deprecation notice | 3 months minimum |\n| Sunset period | 6 months after deprecation |\n| Breaking changes | New major version |\n| Additive changes | Same version (backward compatible) |\n\n## Related Skills\n\n- `api-design-framework` - REST API patterns\n- `error-handling-rfc9457` - Consistent errors across versions\n- `observability-monitoring` - Version usage metrics\n\n## Capability Details\n\n### url-versioning\n**Keywords:** url version, path version, /v1/, /v2/\n**Solves:**\n- How to version REST APIs?\n- URL-based API versioning\n\n### header-versioning\n**Keywords:** header version, X-API-Version, custom header\n**Solves:**\n- Clean URL versioning\n- Header-based API version\n\n### deprecation\n**Keywords:** deprecation, sunset, version lifecycle, backward compatible\n**Solves:**\n- How to deprecate API versions?\n- Version sunset policy\n\n### breaking-changes\n**Keywords:** breaking change, non-breaking, backward compatible\n**Solves:**\n- What requires a version bump?\n- Breaking vs non-breaking changes"
              },
              {
                "name": "ASCII Visualizer",
                "description": "Use when visualizing architecture, data flows, or system diagrams in text. Creates ASCII art for plans, workflows, and structures that render in terminals and markdown.",
                "path": ".claude/skills/ascii-visualizer/SKILL.md",
                "frontmatter": {
                  "name": "ASCII Visualizer",
                  "description": "Use when visualizing architecture, data flows, or system diagrams in text. Creates ASCII art for plans, workflows, and structures that render in terminals and markdown.",
                  "version": "1.0.0",
                  "tags": [
                    "ascii",
                    "visualization",
                    "diagrams",
                    "architecture",
                    2025
                  ]
                },
                "content": "# ASCII Visualizer Skill\n\nCreate clear ASCII visualizations for explaining complex concepts.\n\n## When to Use\n\n- Explaining system architecture\n- Showing workflow steps\n- Displaying progress/metrics\n- Before/after comparisons\n- File/directory structures\n\n## Box-Drawing Characters\n\n**IMPORTANT:** Use a fixed-width (monospace) font for proper rendering.\n\n```\n  Standard weight\n  Heavy weight\n    Connectors\n  Double lines\n```\n\n## Quick Examples\n\n### Architecture\n```\n      \n   Frontend      Backend    \n   React 19            FastAPI    \n      \n                              \n                              \n                      \n                        PostgreSQL  \n                      \n```\n\n### Progress\n```\n[] 80% Complete\n Design    (2 days)\n Backend   (5 days)\n Frontend  (3 days)\n Testing   (pending)\n```\n\nSee `references/` for complete patterns.\n\n## Capability Details\n\n### architecture-diagrams\n**Keywords:** architecture, diagram, system design, components, flow\n**Solves:**\n- How do I visualize system architecture?\n- Show component relationships with ASCII\n- Explain system design visually\n- Create architecture diagrams in documentation\n\n### workflows\n**Keywords:** workflow, process, steps, pipeline, flowchart\n**Solves:**\n- How do I visualize process flow?\n- Show step-by-step workflow with ASCII\n- Explain pipeline stages visually\n- Document multi-agent workflows\n\n### comparisons\n**Keywords:** compare, vs, before after, metrics, changes\n**Solves:**\n- How do I compare two options visually?\n- Show before/after metrics\n- Display progress comparison\n- Visualize A/B testing results\n\n### file-trees\n**Keywords:** file tree, directory, structure, folder hierarchy\n**Solves:**\n- How do I show directory structure?\n- Visualize file hierarchy with ASCII\n- Explain codebase organization\n- Document project structure\n\n### progress-tracking\n**Keywords:** progress, status, completion, percentage, metrics\n**Solves:**\n- How do I show progress visually?\n- Create progress bars with ASCII\n- Display completion status\n- Track task completion metrics"
              },
              {
                "name": "auth-patterns",
                "description": "Authentication and authorization patterns. Use when implementing login flows, JWT tokens, session management, password security, OAuth 2.1, Passkeys/WebAuthn, or role-based access control.",
                "path": ".claude/skills/auth-patterns/SKILL.md",
                "frontmatter": {
                  "name": "auth-patterns",
                  "description": "Authentication and authorization patterns. Use when implementing login flows, JWT tokens, session management, password security, OAuth 2.1, Passkeys/WebAuthn, or role-based access control.",
                  "context": "fork",
                  "agent": "security-auditor",
                  "version": "2.0.0",
                  "tags": [
                    "security",
                    "authentication",
                    "oauth",
                    "passkeys",
                    2026
                  ],
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob",
                    "Write",
                    "Edit",
                    "Bash"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/redact-secrets.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# Authentication Patterns\n\nImplement secure authentication with OAuth 2.1, Passkeys, and modern security standards.\n\n## When to Use\n\n- Login/signup flows\n- JWT token management\n- Session security\n- OAuth 2.1 with PKCE\n- Passkeys/WebAuthn\n- Multi-factor authentication\n- Role-based access control\n\n## Quick Reference\n\n### Password Hashing (Argon2id)\n\n```python\nfrom argon2 import PasswordHasher\nph = PasswordHasher()\npassword_hash = ph.hash(password)\nph.verify(password_hash, password)\n```\n\n### JWT Access Token\n\n```python\nimport jwt\npayload = {\n    'user_id': user_id,\n    'type': 'access',\n    'exp': datetime.utcnow() + timedelta(minutes=15),\n}\ntoken = jwt.encode(payload, SECRET_KEY, algorithm='HS256')\n```\n\n### OAuth 2.1 with PKCE (Required)\n\n```python\nimport hashlib, base64, secrets\ncode_verifier = secrets.token_urlsafe(64)\ndigest = hashlib.sha256(code_verifier.encode()).digest()\ncode_challenge = base64.urlsafe_b64encode(digest).rstrip(b'=').decode()\n```\n\n### Session Security\n\n```python\napp.config['SESSION_COOKIE_SECURE'] = True      # HTTPS only\napp.config['SESSION_COOKIE_HTTPONLY'] = True    # No JS access\napp.config['SESSION_COOKIE_SAMESITE'] = 'Strict'\n```\n\n## Token Expiry (2026 Guidelines)\n\n| Token Type | Expiry | Storage |\n|------------|--------|---------|\n| Access | 15 min - 1 hour | Memory only |\n| Refresh | 7-30 days | HTTPOnly cookie |\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n#  NEVER store passwords in plaintext\nuser.password = request.form['password']\n\n#  NEVER use implicit OAuth grant\nresponse_type=token  # Deprecated in OAuth 2.1\n\n#  NEVER skip rate limiting on login\n@app.route('/login')  # No rate limit!\n\n#  NEVER reveal if email exists\nreturn \"Email not found\"  # Information disclosure\n\n#  ALWAYS use Argon2id or bcrypt\npassword_hash = ph.hash(password)\n\n#  ALWAYS use PKCE\ncode_challenge=challenge&code_challenge_method=S256\n\n#  ALWAYS rate limit auth endpoints\n@limiter.limit(\"5 per minute\")\n\n#  ALWAYS use generic error messages\nreturn \"Invalid credentials\"\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Password hash | **Argon2id** > bcrypt |\n| Access token expiry | 15 min - 1 hour |\n| Refresh token expiry | 7-30 days with rotation |\n| Session cookie | HTTPOnly, Secure, SameSite=Strict |\n| Rate limit | 5 attempts per minute |\n| MFA | Passkeys > TOTP > SMS |\n| OAuth | 2.1 with PKCE (no implicit) |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/oauth-2.1-passkeys.md](references/oauth-2.1-passkeys.md) | OAuth 2.1, PKCE, Passkeys/WebAuthn |\n| [examples/auth-implementations.md](examples/auth-implementations.md) | Complete implementation examples |\n| [checklists/auth-checklist.md](checklists/auth-checklist.md) | Security checklist |\n| [templates/auth-middleware-template.py](templates/auth-middleware-template.py) | Flask/FastAPI middleware |\n\n## Related Skills\n\n- `owasp-top-10` - Security fundamentals\n- `input-validation` - Data validation\n- `api-design-framework` - API security\n\n## Capability Details\n\n### password-hashing\n**Keywords:** password, hashing, bcrypt, argon2, hash\n**Solves:**\n- Securely hash passwords with modern algorithms\n- Configure appropriate cost factors\n- Migrate legacy password hashes\n\n### jwt-tokens\n**Keywords:** JWT, token, access token, claims, jsonwebtoken\n**Solves:**\n- Generate and validate JWT access tokens\n- Implement proper token expiration\n- Handle token refresh securely\n\n### oauth2-pkce\n**Keywords:** OAuth, PKCE, OAuth 2.1, authorization code, code verifier\n**Solves:**\n- Implement OAuth 2.1 with PKCE flow\n- Secure authorization for SPAs and mobile apps\n- Handle OAuth provider integration\n\n### passkeys-webauthn\n**Keywords:** passkey, WebAuthn, FIDO2, passwordless, biometric\n**Solves:**\n- Implement passwordless authentication\n- Configure WebAuthn registration and login\n- Support cross-device passkeys\n\n### session-management\n**Keywords:** session, cookie, session storage, logout, invalidate\n**Solves:**\n- Manage user sessions securely\n- Implement session invalidation on logout\n- Handle concurrent sessions\n\n### role-based-access\n**Keywords:** RBAC, role, permission, authorization, access control\n**Solves:**\n- Implement role-based access control\n- Define permission hierarchies\n- Check authorization in routes"
              },
              {
                "name": "backend-architecture-enforcer",
                "description": "Enforce FastAPI Clean Architecture - layer separation, dependency injection, async patterns, no business logic in routers. Blocks violations. Use when building or reviewing backend code.",
                "path": ".claude/skills/backend-architecture-enforcer/SKILL.md",
                "frontmatter": {
                  "name": "backend-architecture-enforcer",
                  "description": "Enforce FastAPI Clean Architecture - layer separation, dependency injection, async patterns, no business logic in routers. Blocks violations. Use when building or reviewing backend code.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "backend",
                    "fastapi",
                    "architecture",
                    "enforcement",
                    "blocking",
                    "clean-architecture",
                    "di"
                  ],
                  "hooks": {
                    "PreToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/backend-file-naming.sh"
                      }
                    ],
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/backend-layer-validator.sh"
                      },
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/di-pattern-enforcer.sh"
                      }
                    ]
                  }
                },
                "content": "# Backend Architecture Enforcer\n\nEnforce FastAPI Clean Architecture with **BLOCKING** validation.\n\n## When to Use\n\n- Building FastAPI endpoints\n- Creating services or repositories\n- Reviewing backend architecture\n- Refactoring legacy code\n\n## Architecture Overview\n\n```\n\n                        ROUTERS LAYER                            \n  HTTP concerns only: request parsing, response formatting       \n  Files: router_*.py, routes_*.py, api_*.py                     \n\n                        SERVICES LAYER                           \n  Business logic: orchestration, validation, transformations     \n  Files: *_service.py                                           \n\n                      REPOSITORIES LAYER                         \n  Data access: database queries, external API calls              \n  Files: *_repository.py, *_repo.py                             \n\n                        MODELS LAYER                             \n  Data structures: SQLAlchemy models, Pydantic schemas          \n  Files: *_model.py (ORM), *_schema.py (Pydantic)              \n\n```\n\n## Validation Rules\n\n### BLOCKING Rules (exit 1)\n\n| Rule | Check | Layer |\n|------|-------|-------|\n| **No DB in Routers** | Database operations blocked in routers | routers/ |\n| **No HTTP in Services** | HTTPException blocked in services | services/ |\n| **No Business Logic in Routers** | Complex logic blocked in routers | routers/ |\n| **Use Depends()** | Direct instantiation blocked | routers/ |\n| **Async Consistency** | Sync calls in async functions blocked | all |\n| **File Naming** | Must follow naming convention | all |\n\n## File Naming Conventions\n\n### Routers\n```\nALLOWED:\n  router_users.py\n  routes_auth.py\n  api_items.py\n  deps.py\n  dependencies.py\n\nBLOCKED:\n  users.py           # Missing prefix\n  user_routes.py     # Wrong format\n  UserRouter.py      # PascalCase\n```\n\n### Services\n```\nALLOWED:\n  user_service.py\n  auth_service.py\n  email_service.py\n\nBLOCKED:\n  users.py           # Missing _service suffix\n  UserService.py     # PascalCase\n  service_user.py    # Wrong order\n```\n\n### Repositories\n```\nALLOWED:\n  user_repository.py\n  user_repo.py\n  auth_repository.py\n\nBLOCKED:\n  users.py              # Missing suffix\n  repository_user.py    # Wrong order\n  UserRepository.py     # PascalCase\n```\n\n### Schemas\n```\nALLOWED:\n  user_schema.py\n  user_dto.py\n  auth_request.py\n  auth_response.py\n\nBLOCKED:\n  users.py           # Missing suffix\n  UserSchema.py      # PascalCase\n```\n\n### Models (SQLAlchemy)\n```\nALLOWED:\n  user_model.py\n  user_entity.py\n  user_orm.py\n  base.py\n\nBLOCKED:\n  users.py           # Missing suffix\n  UserModel.py       # PascalCase\n```\n\n## Layer Separation Rules\n\n### Routers Layer (HTTP Only)\n\nRouters should ONLY handle:\n- Request parsing\n- Response formatting\n- HTTP status codes\n- Authentication/authorization checks\n- Calling services\n\n```python\n# GOOD - Router delegates to service\n@router.post(\"/users\", response_model=UserResponse)\nasync def create_user(\n    user_data: UserCreate,\n    service: UserService = Depends(get_user_service),\n):\n    user = await service.create_user(user_data)\n    return user\n\n# BLOCKED - Business logic in router\n@router.post(\"/users\")\nasync def create_user(\n    user_data: UserCreate,\n    db: AsyncSession = Depends(get_db),\n):\n    #  Database operation in router\n    existing = await db.execute(\n        select(User).where(User.email == user_data.email)\n    )\n    if existing.scalar():\n        raise HTTPException(400, \"Email exists\")\n\n    #  Business logic in router\n    user = User(**user_data.dict())\n    user.created_at = datetime.utcnow()\n    db.add(user)\n    await db.commit()\n    return user\n```\n\n### Services Layer (Business Logic)\n\nServices should:\n- Contain business logic\n- Orchestrate repositories\n- Handle validation\n- Transform data\n- Raise domain exceptions (NOT HTTPException)\n\n```python\n# GOOD - Service with business logic\nclass UserService:\n    def __init__(self, repo: UserRepository):\n        self.repo = repo\n\n    async def create_user(self, data: UserCreate) -> User:\n        # Business validation\n        if await self.repo.exists_by_email(data.email):\n            raise UserAlreadyExistsError(data.email)\n\n        # Business logic\n        user = User(\n            email=data.email,\n            password_hash=hash_password(data.password),\n            created_at=datetime.utcnow(),\n        )\n\n        return await self.repo.create(user)\n\n# BLOCKED - HTTP concerns in service\nclass UserService:\n    async def create_user(self, data: UserCreate) -> User:\n        if await self.repo.exists_by_email(data.email):\n            #  HTTPException in service\n            raise HTTPException(400, \"Email already exists\")\n\n        #  Accessing Request object\n        if request.headers.get(\"X-Admin\"):\n            user.is_admin = True\n```\n\n### Repositories Layer (Data Access)\n\nRepositories should:\n- Execute database queries\n- Call external APIs\n- Handle data persistence\n- Return domain objects or None\n\n```python\n# GOOD - Repository handles data access only\nclass UserRepository:\n    def __init__(self, db: AsyncSession):\n        self.db = db\n\n    async def get_by_id(self, user_id: int) -> User | None:\n        result = await self.db.execute(\n            select(User).where(User.id == user_id)\n        )\n        return result.scalar_one_or_none()\n\n    async def create(self, user: User) -> User:\n        self.db.add(user)\n        await self.db.commit()\n        await self.db.refresh(user)\n        return user\n\n# BLOCKED - HTTP concerns in repository\nclass UserRepository:\n    async def get_by_id(self, user_id: int) -> User:\n        user = await self.db.get(User, user_id)\n        if not user:\n            #  HTTPException in repository\n            raise HTTPException(404, \"User not found\")\n        return user\n```\n\n## Dependency Injection Rules\n\n### Use Depends() for All Dependencies\n\n```python\n# GOOD - Proper DI with Depends()\n@router.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: int,\n    service: UserService = Depends(get_user_service),\n    current_user: User = Depends(get_current_user),\n):\n    return await service.get_user(user_id)\n\n# Dependency provider\ndef get_user_service(\n    repo: UserRepository = Depends(get_user_repository),\n) -> UserService:\n    return UserService(repo)\n\ndef get_user_repository(\n    db: AsyncSession = Depends(get_db),\n) -> UserRepository:\n    return UserRepository(db)\n```\n\n### BLOCKED Patterns\n\n```python\n# BLOCKED - Direct instantiation\n@router.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    service = UserService()  #  Direct instantiation\n    return await service.get_user(user_id)\n\n# BLOCKED - Global instance\nuser_service = UserService()  #  Global instance\n\n@router.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    return await user_service.get_user(user_id)\n\n# BLOCKED - Session without Depends\n@router.get(\"/users\")\nasync def get_users(db: AsyncSession):  #  Missing Depends()\n    return await db.execute(select(User)).scalars().all()\n```\n\n## Async Consistency Rules\n\n### No Sync Calls in Async Functions\n\n```python\n# GOOD - Async all the way\nasync def get_user(user_id: int) -> User:\n    result = await db.execute(select(User).where(User.id == user_id))\n    return result.scalar_one_or_none()\n\n# BLOCKED - Sync call in async function\nasync def get_user(user_id: int) -> User:\n    #  Blocking sync call\n    result = db.execute(select(User).where(User.id == user_id))\n    return result.scalar_one_or_none()\n\n# If you must use sync code, use run_in_executor\nasync def process_file(file_path: str) -> bytes:\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(\n        None,\n        lambda: open(file_path, 'rb').read()\n    )\n```\n\n## Common Violations\n\n### 1. Database Operations in Router\n```\nBLOCKED: Database operations not allowed in routers\n  File: app/routers/router_users.py:42\n  Code: db.add(user)\n  Move to repository layer\n```\n\n### 2. HTTPException in Service\n```\nBLOCKED: HTTP responses not allowed in services\n  File: app/services/user_service.py:28\n  Code: raise HTTPException(400, \"Invalid\")\n  Return data/raise domain exceptions, let routers handle HTTP\n```\n\n### 3. Direct Instantiation\n```\nBLOCKED: Direct service instantiation not allowed\n  File: app/routers/router_users.py:15\n  Code: service = UserService()\n  Use: service: UserService = Depends(get_user_service)\n```\n\n### 4. Wrong File Naming\n```\nBLOCKED: Service files must end with _service.py\n  Got: users.py\n  Example: user_service.py, auth_service.py\n```\n\n### 5. Complex Router Function\n```\nBLOCKED: Router functions too complex (avg 45 lines)\n  File: app/routers/router_orders.py\n  Extract business logic to services/\n```\n\n## Exception Handling Pattern\n\n### Domain Exceptions (Services/Repositories)\n\n```python\n# app/core/exceptions.py\nclass DomainException(Exception):\n    \"\"\"Base domain exception.\"\"\"\n    pass\n\nclass UserNotFoundError(DomainException):\n    def __init__(self, user_id: int):\n        self.user_id = user_id\n        super().__init__(f\"User {user_id} not found\")\n\nclass UserAlreadyExistsError(DomainException):\n    def __init__(self, email: str):\n        self.email = email\n        super().__init__(f\"User with email {email} already exists\")\n```\n\n### Exception Handler (Routers)\n\n```python\n# app/routers/deps.py\nfrom fastapi import HTTPException\n\ndef handle_domain_exception(exc: DomainException) -> HTTPException:\n    \"\"\"Convert domain exceptions to HTTP responses.\"\"\"\n    if isinstance(exc, UserNotFoundError):\n        return HTTPException(404, str(exc))\n    if isinstance(exc, UserAlreadyExistsError):\n        return HTTPException(409, str(exc))\n    return HTTPException(500, \"Internal error\")\n\n# Usage in router\n@router.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: int,\n    service: UserService = Depends(get_user_service),\n):\n    try:\n        return await service.get_user(user_id)\n    except DomainException as e:\n        raise handle_domain_exception(e)\n```\n\n## Complete Example\n\n### Router\n\n```python\n# app/routers/router_users.py\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom app.schemas.user_schema import UserCreate, UserResponse\nfrom app.services.user_service import UserService\nfrom app.routers.deps import get_user_service\n\nrouter = APIRouter(prefix=\"/users\", tags=[\"users\"])\n\n@router.post(\"/\", response_model=UserResponse, status_code=201)\nasync def create_user(\n    user_data: UserCreate,\n    service: UserService = Depends(get_user_service),\n):\n    \"\"\"Create a new user.\"\"\"\n    try:\n        return await service.create_user(user_data)\n    except UserAlreadyExistsError:\n        raise HTTPException(409, \"Email already registered\")\n\n@router.get(\"/{user_id}\", response_model=UserResponse)\nasync def get_user(\n    user_id: int,\n    service: UserService = Depends(get_user_service),\n):\n    \"\"\"Get user by ID.\"\"\"\n    user = await service.get_user(user_id)\n    if not user:\n        raise HTTPException(404, \"User not found\")\n    return user\n```\n\n### Service\n\n```python\n# app/services/user_service.py\nfrom app.repositories.user_repository import UserRepository\nfrom app.schemas.user_schema import UserCreate\nfrom app.models.user_model import User\nfrom app.core.security import hash_password\n\nclass UserService:\n    def __init__(self, repo: UserRepository):\n        self.repo = repo\n\n    async def create_user(self, data: UserCreate) -> User:\n        if await self.repo.exists_by_email(data.email):\n            raise UserAlreadyExistsError(data.email)\n\n        user = User(\n            email=data.email,\n            password_hash=hash_password(data.password),\n        )\n        return await self.repo.create(user)\n\n    async def get_user(self, user_id: int) -> User | None:\n        return await self.repo.get_by_id(user_id)\n```\n\n### Repository\n\n```python\n# app/repositories/user_repository.py\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.models.user_model import User\n\nclass UserRepository:\n    def __init__(self, db: AsyncSession):\n        self.db = db\n\n    async def get_by_id(self, user_id: int) -> User | None:\n        result = await self.db.execute(\n            select(User).where(User.id == user_id)\n        )\n        return result.scalar_one_or_none()\n\n    async def exists_by_email(self, email: str) -> bool:\n        result = await self.db.execute(\n            select(User.id).where(User.email == email)\n        )\n        return result.scalar() is not None\n\n    async def create(self, user: User) -> User:\n        self.db.add(user)\n        await self.db.commit()\n        await self.db.refresh(user)\n        return user\n```\n\n## Related Skills\n\n- `clean-architecture` - DDD patterns\n- `fastapi-advanced` - Advanced FastAPI patterns\n- `dependency-injection` - DI patterns\n- `project-structure-enforcer` - Folder structure\n\n## Capability Details\n\n### layer-separation\n**Keywords:** router, service, repository, layer, clean architecture, separation\n**Solves:**\n- Prevent database operations in routers\n- Block business logic in route handlers\n- Ensure proper layer boundaries\n\n### dependency-injection\n**Keywords:** depends, dependency injection, DI, fastapi depends, inject\n**Solves:**\n- Enforce use of FastAPI Depends() pattern\n- Block direct instantiation in routers\n- Ensure testable code structure\n\n### file-naming\n**Keywords:** naming convention, file name, router_, _service, _repository\n**Solves:**\n- Enforce consistent file naming patterns\n- Validate router/service/repository naming\n- Maintain codebase consistency\n\n### async-patterns\n**Keywords:** async, await, sync, blocking call, asyncio\n**Solves:**\n- Detect sync calls in async functions\n- Prevent blocking operations in async code\n- Ensure async consistency"
              },
              {
                "name": "background-jobs",
                "description": "Async task processing with Celery, ARQ, and Redis for Python backends. Use when offloading long-running tasks, scheduling jobs, or building worker pipelines.",
                "path": ".claude/skills/background-jobs/SKILL.md",
                "frontmatter": {
                  "name": "background-jobs",
                  "description": "Async task processing with Celery, ARQ, and Redis for Python backends. Use when offloading long-running tasks, scheduling jobs, or building worker pipelines.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "tags": [
                    "background-jobs",
                    "celery",
                    "arq",
                    "redis",
                    "async",
                    "python",
                    2026
                  ]
                },
                "content": "# Background Job Patterns\n\nOffload long-running tasks with async job queues.\n\n## When to Use\n\n- Long-running tasks (report generation, data processing)\n- Email/notification sending\n- Scheduled/periodic tasks\n- Webhook processing\n- Data export/import pipelines\n- Non-LLM async operations (use LangGraph for LLM workflows)\n\n## Tool Selection\n\n| Tool | Language | Best For | Complexity |\n|------|----------|----------|------------|\n| ARQ | Python (async) | FastAPI, simple jobs | Low |\n| Celery | Python | Complex workflows, enterprise | High |\n| RQ | Python | Simple Redis queues | Low |\n| Dramatiq | Python | Reliable messaging | Medium |\n\n## ARQ (Async Redis Queue)\n\n### Setup\n\n```python\n# backend/app/workers/arq_worker.py\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\nasync def startup(ctx: dict):\n    \"\"\"Initialize worker resources.\"\"\"\n    ctx[\"db\"] = await create_db_pool()\n    ctx[\"http\"] = httpx.AsyncClient()\n\nasync def shutdown(ctx: dict):\n    \"\"\"Cleanup worker resources.\"\"\"\n    await ctx[\"db\"].close()\n    await ctx[\"http\"].aclose()\n\nclass WorkerSettings:\n    redis_settings = RedisSettings(host=\"redis\", port=6379)\n    functions = [\n        send_email,\n        generate_report,\n        process_webhook,\n    ]\n    on_startup = startup\n    on_shutdown = shutdown\n    max_jobs = 10\n    job_timeout = 300  # 5 minutes\n```\n\n### Task Definition\n\n```python\nfrom arq import func\n\nasync def send_email(\n    ctx: dict,\n    to: str,\n    subject: str,\n    body: str,\n) -> dict:\n    \"\"\"Send email task.\"\"\"\n    http = ctx[\"http\"]\n    response = await http.post(\n        \"https://api.sendgrid.com/v3/mail/send\",\n        json={\"to\": to, \"subject\": subject, \"html\": body},\n        headers={\"Authorization\": f\"Bearer {SENDGRID_KEY}\"},\n    )\n    return {\"status\": response.status_code, \"to\": to}\n\nasync def generate_report(\n    ctx: dict,\n    report_id: str,\n    format: str = \"pdf\",\n) -> dict:\n    \"\"\"Generate report asynchronously.\"\"\"\n    db = ctx[\"db\"]\n    data = await db.fetch_report_data(report_id)\n    pdf_bytes = await render_pdf(data)\n    await db.save_report_file(report_id, pdf_bytes)\n    return {\"report_id\": report_id, \"size\": len(pdf_bytes)}\n```\n\n### Enqueue from FastAPI\n\n```python\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\n# Dependency\nasync def get_arq_pool():\n    return await create_pool(RedisSettings(host=\"redis\"))\n\n@router.post(\"/api/v1/reports\")\nasync def create_report(\n    data: ReportRequest,\n    arq: ArqRedis = Depends(get_arq_pool),\n):\n    report = await service.create_report(data)\n\n    # Enqueue background job\n    job = await arq.enqueue_job(\n        \"generate_report\",\n        report.id,\n        format=data.format,\n    )\n\n    return {\"report_id\": report.id, \"job_id\": job.job_id}\n\n@router.get(\"/api/v1/jobs/{job_id}\")\nasync def get_job_status(\n    job_id: str,\n    arq: ArqRedis = Depends(get_arq_pool),\n):\n    job = Job(job_id, arq)\n    status = await job.status()\n    result = await job.result() if status == JobStatus.complete else None\n    return {\"job_id\": job_id, \"status\": status, \"result\": result}\n```\n\n## Celery (Enterprise)\n\n### Setup\n\n```python\n# backend/app/workers/celery_app.py\nfrom celery import Celery\n\ncelery_app = Celery(\n    \"skillforge\",\n    broker=\"redis://redis:6379/0\",\n    backend=\"redis://redis:6379/1\",\n)\n\ncelery_app.conf.update(\n    task_serializer=\"json\",\n    accept_content=[\"json\"],\n    result_serializer=\"json\",\n    timezone=\"UTC\",\n    task_track_started=True,\n    task_time_limit=600,  # 10 minutes hard limit\n    task_soft_time_limit=540,  # 9 minutes soft limit\n    worker_prefetch_multiplier=1,  # Fair distribution\n    task_acks_late=True,  # Acknowledge after completion\n    task_reject_on_worker_lost=True,\n)\n```\n\n### Task Definition\n\n```python\nfrom celery import shared_task\nfrom celery.utils.log import get_task_logger\n\nlogger = get_task_logger(__name__)\n\n@shared_task(\n    bind=True,\n    max_retries=3,\n    default_retry_delay=60,\n    autoretry_for=(ConnectionError, TimeoutError),\n)\ndef send_email(self, to: str, subject: str, body: str) -> dict:\n    \"\"\"Send email with automatic retry.\"\"\"\n    try:\n        response = requests.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            json={\"to\": to, \"subject\": subject, \"html\": body},\n            headers={\"Authorization\": f\"Bearer {SENDGRID_KEY}\"},\n            timeout=30,\n        )\n        response.raise_for_status()\n        return {\"status\": \"sent\", \"to\": to}\n    except Exception as exc:\n        logger.error(f\"Email failed: {exc}\")\n        raise self.retry(exc=exc)\n\n@shared_task(bind=True)\ndef generate_report(self, report_id: str) -> dict:\n    \"\"\"Long-running report generation.\"\"\"\n    self.update_state(state=\"PROGRESS\", meta={\"step\": \"fetching\"})\n    data = fetch_report_data(report_id)\n\n    self.update_state(state=\"PROGRESS\", meta={\"step\": \"rendering\"})\n    pdf = render_pdf(data)\n\n    self.update_state(state=\"PROGRESS\", meta={\"step\": \"saving\"})\n    save_report(report_id, pdf)\n\n    return {\"report_id\": report_id, \"size\": len(pdf)}\n```\n\n### Chains and Groups\n\n```python\nfrom celery import chain, group, chord\n\n# Sequential execution\nworkflow = chain(\n    extract_data.s(source_id),\n    transform_data.s(),\n    load_data.s(destination_id),\n)\nresult = workflow.apply_async()\n\n# Parallel execution\nparallel = group(\n    process_chunk.s(chunk) for chunk in chunks\n)\nresult = parallel.apply_async()\n\n# Parallel with callback\nchord_workflow = chord(\n    [process_chunk.s(chunk) for chunk in chunks],\n    aggregate_results.s(),\n)\nresult = chord_workflow.apply_async()\n```\n\n### Periodic Tasks (Celery Beat)\n\n```python\nfrom celery.schedules import crontab\n\ncelery_app.conf.beat_schedule = {\n    \"cleanup-expired-sessions\": {\n        \"task\": \"app.workers.tasks.cleanup_sessions\",\n        \"schedule\": crontab(minute=0, hour=\"*/6\"),  # Every 6 hours\n    },\n    \"generate-daily-report\": {\n        \"task\": \"app.workers.tasks.daily_report\",\n        \"schedule\": crontab(minute=0, hour=2),  # 2 AM daily\n    },\n    \"sync-external-data\": {\n        \"task\": \"app.workers.tasks.sync_data\",\n        \"schedule\": 300.0,  # Every 5 minutes\n    },\n}\n```\n\n## FastAPI Integration\n\n```python\nfrom fastapi import BackgroundTasks\n\n@router.post(\"/api/v1/users\")\nasync def create_user(\n    data: UserCreate,\n    background_tasks: BackgroundTasks,\n):\n    user = await service.create_user(data)\n\n    # Simple background task (in-process)\n    background_tasks.add_task(send_welcome_email, user.email)\n\n    return user\n\n# For distributed tasks, use ARQ/Celery\n@router.post(\"/api/v1/exports\")\nasync def create_export(\n    data: ExportRequest,\n    arq: ArqRedis = Depends(get_arq_pool),\n):\n    job = await arq.enqueue_job(\"export_data\", data.dict())\n    return {\"job_id\": job.job_id}\n```\n\n## Job Status Tracking\n\n```python\nfrom enum import Enum\n\nclass JobStatus(Enum):\n    PENDING = \"pending\"\n    STARTED = \"started\"\n    PROGRESS = \"progress\"\n    SUCCESS = \"success\"\n    FAILURE = \"failure\"\n    REVOKED = \"revoked\"\n\n@router.get(\"/api/v1/jobs/{job_id}\")\nasync def get_job(job_id: str):\n    # Celery\n    result = AsyncResult(job_id, app=celery_app)\n    return {\n        \"job_id\": job_id,\n        \"status\": result.status,\n        \"result\": result.result if result.ready() else None,\n        \"progress\": result.info if result.status == \"PROGRESS\" else None,\n    }\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER run long tasks synchronously\n@router.post(\"/api/v1/reports\")\nasync def create_report(data: ReportRequest):\n    pdf = await generate_pdf(data)  # Blocks for minutes!\n    return pdf\n\n# NEVER lose jobs on failure\n@shared_task\ndef risky_task():\n    do_work()  # No retry, no error handling\n\n# NEVER store large results in Redis\n@shared_task\ndef process_file(file_id: str) -> bytes:\n    return large_file_bytes  # Store in S3/DB instead!\n\n# NEVER use BackgroundTasks for distributed work\nbackground_tasks.add_task(long_running_job)  # Lost if server restarts\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Simple async | ARQ (native async) |\n| Complex workflows | Celery (chains, chords) |\n| In-process quick | FastAPI BackgroundTasks |\n| LLM workflows | LangGraph (not Celery) |\n| Result storage | Redis for status, S3/DB for data |\n| Retry strategy | Exponential backoff with jitter |\n\n## Related Skills\n\n- `langgraph-checkpoints` - LLM workflow persistence\n- `resilience-patterns` - Retry and fallback\n- `observability-monitoring` - Job metrics\n\n## Capability Details\n\n### arq-tasks\n**Keywords:** arq, async queue, redis queue, background task\n**Solves:**\n- How to run async background tasks in FastAPI?\n- Simple Redis job queue\n\n### celery-tasks\n**Keywords:** celery, task queue, distributed tasks, worker\n**Solves:**\n- Enterprise task queue\n- Complex job workflows\n\n### celery-workflows\n**Keywords:** chain, group, chord, celery workflow\n**Solves:**\n- Sequential task execution\n- Parallel task processing\n\n### periodic-tasks\n**Keywords:** periodic, scheduled, cron, celery beat\n**Solves:**\n- Run tasks on schedule\n- Cron-like job scheduling"
              },
              {
                "name": "brainstorming",
                "description": "Use when creating or developing anything, before writing code or implementation plans - refines rough ideas into fully-formed designs through structured Socratic questioning, alternative exploration, and incremental validation",
                "path": ".claude/skills/brainstorming/SKILL.md",
                "frontmatter": {
                  "name": "brainstorming",
                  "description": "Use when creating or developing anything, before writing code or implementation plans - refines rough ideas into fully-formed designs through structured Socratic questioning, alternative exploration, and incremental validation",
                  "context": "fork",
                  "agent": "product-strategist",
                  "hooks": {
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/design-decision-saver.sh"
                      }
                    ]
                  }
                },
                "content": "# Brainstorming Ideas Into Designs\n\n## Overview\n\nTransform rough ideas into fully-formed designs through structured questioning and alternative exploration.\n\n**Core principle:** Ask questions to understand, explore alternatives, present design incrementally for validation.\n\n**Announce skill usage at start of session.**\n\n## When to Use This Skill\n\nActivate this skill when:\n- Request contains \"I have an idea for...\" or \"I want to build...\"\n- User asks \"help me design...\" or \"what's the best approach for...\"\n- Requirements are vague or high-level\n- Multiple approaches might work\n- Before writing any code or implementation plans\n- User needs to explore trade-offs between different solutions\n\n## When NOT to Use This Skill\n\n**Skip brainstorming when:**\n- Requirements are crystal clear and specific\n- Only one obvious approach exists\n- User has already designed the solution (just needs implementation)\n- Time-sensitive bug fix or urgent production issue\n- User explicitly says \"just implement it\" without questions\n\n**Examples of clear requirements (no brainstorming needed):**\n- \"Add a print button to this page\"\n- \"Fix this TypeError on line 42\"\n- \"Update the copyright year to 2025\"\n- \"Change the button color to #FF5733\"\n\n## The Three-Phase Process\n\n| Phase | Key Activities | Tool Usage | Output |\n|-------|----------------|------------|--------|\n| **1. Understanding** | Ask questions (one at a time) | AskUserQuestion for choices | Purpose, constraints, criteria |\n| **2. Exploration** | Propose 2-3 approaches | AskUserQuestion for approach selection | Architecture options with trade-offs |\n| **3. Design Presentation** | Present in 200-300 word sections | Open-ended questions | Complete design with validation |\n\n### Phase 1: Understanding\n\n**Goal:** Gather purpose, constraints, and success criteria.\n\n**Process:**\n- Check current project state in working directory\n- Ask ONE question at a time to refine the idea\n- Use AskUserQuestion tool when presenting multiple choice options\n- Gather: Purpose, constraints, success criteria\n\n**Tool Usage:**\nUse AskUserQuestion for clarifying questions with 2-4 clear options.\n\nExample: \"Where should the authentication data be stored?\" with options for Session storage, Local storage, Cookies, each with trade-off descriptions.\n\nSee `references/example-session-auth.md` for complete Phase 1 example.\n\n### Phase 2: Exploration\n\n**Goal:** Propose 2-3 different architectural approaches with explicit trade-offs.\n\n**Process:**\n- Propose 2-3 different approaches\n- For each: Core architecture, trade-offs, complexity assessment\n- Use AskUserQuestion tool to present approaches as structured choices\n- Include trade-off comparison table when helpful\n\n**Trade-off Format:**\n\n| Approach | Pros | Cons | Complexity |\n|----------|------|------|------------|\n| Option 1 | Benefits | Drawbacks | Low/Med/High |\n| Option 2 | Benefits | Drawbacks | Low/Med/High |\n| Option 3 | Benefits | Drawbacks | Low/Med/High |\n\nSee `references/example-session-dashboard.md` for complete Phase 2 example with SSE vs WebSockets vs Polling comparison.\n\n### Phase 3: Design Presentation\n\n**Goal:** Present complete design incrementally, validating each section.\n\n**Process:**\n- Present in 200-300 word sections\n- Cover: Architecture, components, data flow, error handling, testing\n- Ask after each section: \"Does this look right so far?\"\n- Use open-ended questions to allow freeform feedback\n\n**Typical Sections:**\n1. Architecture overview\n2. Component details\n3. Data flow\n4. Error handling\n5. Security considerations\n6. Implementation priorities\n\n**Validation Pattern:**\nAfter each section, pause for feedback before proceeding to next section.\n\n## Tool Usage Guidelines\n\n### Use AskUserQuestion Tool For:\n- Phase 1: Clarifying questions with 2-4 clear options\n- Phase 2: Architectural approach selection (2-3 alternatives)\n- Any decision with distinct, mutually exclusive choices\n- When options have clear trade-offs to explain\n\n**Benefits:**\n- Structured presentation of options with descriptions\n- Clear trade-off visibility\n- Forces explicit choice (prevents vague \"maybe both\" responses)\n\n### Use Open-Ended Questions For:\n- Phase 3: Design validation\n- When detailed feedback or explanation is needed\n- When the user should describe their own requirements\n- When structured options would limit creative input\n\n## Non-Linear Progression\n\n**Flexibility is key.** Go backward when needed - don't force linear progression.\n\n**Return to Phase 1 when:**\n- User reveals new constraint during Phase 2 or 3\n- Validation shows fundamental gap in requirements\n- Something doesn't make sense\n\n**Return to Phase 2 when:**\n- User questions the chosen approach during Phase 3\n- New information suggests a different approach would be better\n\n**Continue forward when:**\n- All requirements are clear\n- Chosen approach is validated\n- No new constraints emerge\n\n## Key Principles\n\n| Principle | Application |\n|-----------|-------------|\n| **One question at a time** | Phase 1: Single question per message, use AskUserQuestion for choices |\n| **Structured choices** | Use AskUserQuestion tool for 2-4 options with trade-offs |\n| **YAGNI ruthlessly** | Remove unnecessary features from all designs |\n| **Explore alternatives** | Always propose 2-3 approaches before settling |\n| **Incremental validation** | Present design in sections, validate each |\n| **Flexible progression** | Go backward when needed - flexibility > rigidity |\n\n## After Brainstorming Completes\n\nConsider these optional next steps:\n- Document the design in project's design documentation\n- Break down the design into actionable implementation tasks\n- Create a git branch or workspace for isolated development\n\nUse templates in `templates/design-doc-template.md` and `templates/decision-matrix-template.md` for structured documentation.\n\n## Socratic Questioning Templates\n\n### Purpose Discovery Questions\n\n**Goal:** Understand the \"why\" behind the feature.\n\n- \"What problem does this solve for your users?\"\n- \"What happens if we don't build this?\"\n- \"How will success be measured?\"\n- \"Who is the primary user of this feature?\"\n- \"What's the most important outcome?\"\n\n### Constraint Identification Questions\n\n**Goal:** Uncover limitations and requirements.\n\n- \"Are there performance requirements? (e.g., must load in < 2s)\"\n- \"What's the expected scale? (users, data volume, requests/sec)\"\n- \"Are there compliance requirements? (GDPR, HIPAA, SOC2)\"\n- \"What's the timeline/budget constraint?\"\n- \"What existing systems must this integrate with?\"\n\n### Trade-Off Exploration Questions\n\n**Goal:** Make implicit preferences explicit.\n\n- \"Would you prefer faster development or better performance?\"\n- \"Is flexibility more important than simplicity?\"\n- \"Should this be user-friendly or developer-friendly?\"\n- \"Optimize for: initial build speed, maintainability, or scalability?\"\n- \"What's more critical: feature completeness or time-to-market?\"\n\n### Alternative Exploration Questions\n\n**Goal:** Ensure we consider all viable approaches.\n\n- \"What if we didn't build this at all? What's the workaround?\"\n- \"How would [competitor/similar product] solve this?\"\n- \"Could we start with a simpler version? What's the MVP?\"\n- \"What if we had unlimited time/budget? What would we add?\"\n- \"What approaches have you already considered and rejected? Why?\"\n\n---\n\n## Common Pitfalls to Avoid\n\n### Pitfall 1: Asking Too Many Questions Upfront\n\n```\n BAD:\n\"Before we start, I need to know:\n1. What's your tech stack?\n2. How many users?\n3. What's the budget?\n4. What's the timeline?\n5. Who's the target audience?\n...\"\n\n GOOD:\n\"What problem does this solve for your users?\"\n[Wait for answer, then ask next most important question]\n```\n\n**Why:** Information overload prevents conversation flow. Ask one at a time.\n\n### Pitfall 2: Proposing Only One Approach\n\n```\n BAD:\n\"Here's the solution: Use Redis for caching...\"\n\n GOOD:\n\"I see three approaches:\n1. Redis (fast, but adds infrastructure)\n2. In-memory (simple, but doesn't scale)\n3. Database query cache (integrated, but slower)\nWhich trade-offs matter most?\"\n```\n\n**Why:** Single approach suggests you haven't explored alternatives.\n\n### Pitfall 3: Over-Engineering from the Start\n\n```\n BAD:\n\"Let's use microservices, Kubernetes, Redis, Kafka,\nmessage queues, and a service mesh...\"\n\n GOOD:\n\"For 100 users/day, a monolith with PostgreSQL\nis sufficient. We can split services later if needed.\"\n```\n\n**Why:** YAGNI (You Aren't Gonna Need It). Start simple, scale when necessary.\n\n### Pitfall 4: Ignoring Existing Code/Patterns\n\n```\n BAD:\n\"Let's rebuild this with a completely different architecture...\"\n\n GOOD:\n[Read existing code first]\n\"I see you're using Express + PostgreSQL. Let's extend\nthat pattern with a new route handler...\"\n```\n\n**Why:** Consistency > novelty. Use existing patterns unless there's a compelling reason to change.\n\n---\n\n## Integration with Other Skills\n\n**After brainstorming completes, consider:**\n\n- **architecture-decision-record**: Document key architectural decisions made during brainstorming\n- **design-system-starter**: Create design tokens and components if building UI\n- **api-design-framework**: Define API contracts if building backend services\n- **testing-strategy-builder**: Plan testing approach for the designed system\n- **security-checklist**: Review security implications of design choices\n\n**Example flow:**\n1. Brainstorming  Design approach selected\n2. Architecture Decision Record  Document \"Why we chose approach X\"\n3. API Design  Define endpoints and contracts\n4. Testing Strategy  Plan how to test the implementation\n\n---\n\n## Tips for Effective Brainstorming\n\n1. **Read the codebase first** - Don't propose changes without understanding existing patterns\n2. **One question at a time** - Conversation flow > information dump\n3. **Always propose 2-3 alternatives** - Shows you've explored options\n4. **Make trade-offs explicit** - \"Fast but complex\" vs \"Slow but simple\"\n5. **Validate incrementally** - Don't present 10-page design at once\n6. **Be ready to backtrack** - Non-linear is fine when new info emerges\n7. **Start simple, scale later** - YAGNI ruthlessly\n8. **Document decisions** - Use ADRs for key architectural choices\n\n---\n\n**Version:** 2.0.0 (January 2026)\n**Status:** Production patterns from SkillForge brainstorming sessions\n\n## Capability Details\n\n### phase-1-understanding\n**Keywords:** brainstorm, idea, explore, requirements, constraints, purpose\n**Solves:**\n- Help me think through this idea\n- What questions should I answer first?\n- Clarify requirements and constraints\n- Understand the purpose of this feature\n\n### socratic-questions\n**Keywords:** why, what problem, how measure, who uses, constraints\n**Solves:**\n- What questions should I ask about this feature?\n- Help me discover requirements through questioning\n- Uncover implicit constraints\n\n### phase-2-exploration\n**Keywords:** alternatives, options, different approach, trade-offs, compare\n**Solves:**\n- What are alternative approaches?\n- Compare implementation options\n- Explore trade-offs between solutions\n- Which approach is best?\n\n### trade-off-analysis\n**Keywords:** pros, cons, trade-off, complexity, cost, performance\n**Solves:**\n- What are the trade-offs of each approach?\n- Compare complexity vs features\n- Speed vs maintainability decisions\n\n### phase-3-design\n**Keywords:** design, architecture, components, data flow, implementation\n**Solves:**\n- Present the complete design incrementally\n- How should I structure this solution?\n- What are the key components?\n- Design validation and feedback\n\n### mvp-scoping\n**Keywords:** mvp, minimum, yagni, simplify, essential, start small\n**Solves:**\n- What's the minimum viable version?\n- How do I avoid over-engineering?\n- Apply YAGNI ruthlessly\n- Start simple, scale later\n\n### real-world-examples\n**Keywords:** example, skillforge, caching, dashboard, authentication\n**Solves:**\n- Show me real examples of brainstorming sessions\n- How was SkillForge designed?\n- Caching strategy examples\n- Real-time dashboard design decisions\n\n### design-documentation\n**Keywords:** document, adr, decision record, design doc\n**Solves:**\n- How do I document this design?\n- Create an architecture decision record\n- Document trade-offs and rationale"
              },
              {
                "name": "browser-content-capture",
                "description": "Capture content from JavaScript-rendered pages, login-protected sites, and multi-page documentation using Playwright MCP tools or Claude Chrome extension. Use when WebFetch fails on SPAs, dynamic content, or auth-required pages. Integrates with SkillForge's analysis pipeline for automatic content processing.",
                "path": ".claude/skills/browser-content-capture/SKILL.md",
                "frontmatter": {
                  "name": "browser-content-capture",
                  "description": "Capture content from JavaScript-rendered pages, login-protected sites, and multi-page documentation using Playwright MCP tools or Claude Chrome extension. Use when WebFetch fails on SPAs, dynamic content, or auth-required pages. Integrates with SkillForge's analysis pipeline for automatic content processing.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "browser",
                    "playwright",
                    "mcp",
                    "scraping",
                    "spa",
                    "authentication",
                    "chrome-extension",
                    2025
                  ]
                },
                "content": "# Browser Content Capture\n\n**Capture web content that traditional scrapers cannot access.**\n\n## Overview\n\nThis skill enables content extraction from sources that require browser-level access:\n- **JavaScript-rendered SPAs** (React, Vue, Angular apps)\n- **Login-protected documentation** (private wikis, gated content)\n- **Dynamic content** (infinite scroll, lazy loading, client-side routing)\n- **Multi-page site crawls** (documentation trees, tutorial series)\n\n## When to Use This Skill\n\n**Use when:**\n- `WebFetch` returns empty or partial content\n- Page requires JavaScript execution to render\n- Content is behind authentication\n- Need to navigate multi-page structures\n- Extracting from client-side routed apps\n\n**Do NOT use when:**\n- Static HTML pages (use `WebFetch` - faster)\n- Public API endpoints (use direct HTTP calls)\n- Simple RSS/Atom feeds\n\n---\n\n## Quick Start\n\n### Check Available MCP Tools\n\n```\nMCPSearch: \"select:mcp__playwright__browser_navigate\"\n```\n\n### Basic Capture Pattern\n\n```python\n# 1. Navigate to URL\nmcp__playwright__browser_navigate(url=\"https://docs.example.com\")\n\n# 2. Wait for content to render\nmcp__playwright__browser_wait_for(selector=\".main-content\", timeout=5000)\n\n# 3. Capture page snapshot\nsnapshot = mcp__playwright__browser_snapshot()\n\n# 4. Extract text content\ncontent = mcp__playwright__browser_evaluate(\n    script=\"document.querySelector('.main-content').innerText\"\n)\n```\n\n---\n\n## MCP Tools Reference\n\n| Tool | Purpose | When to Use |\n|------|---------|-------------|\n| `browser_navigate` | Go to URL | First step of any capture |\n| `browser_snapshot` | Get DOM/accessibility tree | Understanding page structure |\n| `browser_evaluate` | Run custom JS | Extract specific content |\n| `browser_click` | Click elements | Navigate menus, pagination |\n| `browser_fill_form` | Fill inputs | Authentication flows |\n| `browser_wait_for` | Wait for selector | Dynamic content loading |\n| `browser_take_screenshot` | Capture image | Visual verification |\n| `browser_console_messages` | Read JS console | Debug extraction issues |\n| `browser_network_requests` | Monitor XHR/fetch | Find API endpoints |\n\n**Full tool documentation:** See [references/mcp-tools.md](references/mcp-tools.md)\n\n---\n\n## Capture Patterns\n\n### Pattern 1: SPA Content Extraction\n\nFor React/Vue/Angular apps where content renders client-side:\n\n```python\n# Navigate and wait for hydration\nmcp__playwright__browser_navigate(url=\"https://react-docs.example.com\")\nmcp__playwright__browser_wait_for(selector=\"[data-hydrated='true']\", timeout=10000)\n\n# Extract after React mounts\ncontent = mcp__playwright__browser_evaluate(script=\"\"\"\n    // Wait for React to finish rendering\n    await new Promise(r => setTimeout(r, 1000));\n    return document.querySelector('article').innerText;\n\"\"\")\n```\n\n**Details:** See [references/spa-extraction.md](references/spa-extraction.md)\n\n### Pattern 2: Authentication Flow\n\nFor login-protected content:\n\n```python\n# Navigate to login\nmcp__playwright__browser_navigate(url=\"https://docs.example.com/login\")\n\n# Fill credentials (prompt user for values)\nmcp__playwright__browser_fill_form(\n    selector=\"#login-form\",\n    values={\"username\": \"...\", \"password\": \"...\"}\n)\n\n# Click submit and wait for redirect\nmcp__playwright__browser_click(selector=\"button[type='submit']\")\nmcp__playwright__browser_wait_for(selector=\".dashboard\", timeout=10000)\n\n# Now navigate to protected content\nmcp__playwright__browser_navigate(url=\"https://docs.example.com/private-docs\")\n```\n\n**Details:** See [references/auth-handling.md](references/auth-handling.md)\n\n### Pattern 3: Multi-Page Crawl\n\nFor documentation with navigation trees:\n\n```python\n# Get all page links from sidebar\nlinks = mcp__playwright__browser_evaluate(script=\"\"\"\n    return Array.from(document.querySelectorAll('nav a'))\n        .map(a => ({href: a.href, text: a.innerText}));\n\"\"\")\n\n# Iterate and capture each page\nfor link in links:\n    mcp__playwright__browser_navigate(url=link['href'])\n    mcp__playwright__browser_wait_for(selector=\".content\")\n    content = mcp__playwright__browser_evaluate(\n        script=\"document.querySelector('.content').innerText\"\n    )\n    # Queue to SkillForge...\n```\n\n**Details:** See [references/multi-page-crawl.md](references/multi-page-crawl.md)\n\n---\n\n## SkillForge Integration\n\nAfter capturing content, queue it to SkillForge's analysis pipeline:\n\n```python\nimport httpx\n\nasync def queue_to_skillforge(content: str, source_url: str):\n    \"\"\"Send captured content to SkillForge for analysis.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8500/api/v1/analyze\",\n            json={\n                \"url\": source_url,\n                \"content_override\": content,  # Skip scraping, use captured\n                \"source\": \"browser_capture\"\n            }\n        )\n        return response.json()[\"analysis_id\"]\n```\n\n**Full integration:** See [templates/queue-to-skillforge.py](templates/queue-to-skillforge.py)\n\n---\n\n## Fallback Strategy\n\nUse this decision tree for content capture:\n\n```\nUser requests content from URL\n         \n         \n    \n     Try WebFetch  Fast, no browser needed\n    \n         \n    Content OK? Yes Done\n         \n         No (empty/partial)\n         \n         \n    \n     Check URL pattern\n    \n         \n     Known SPA (react, vue, angular)  Playwright MCP\n     Requires login  Chrome Extension (user session)\n     Dynamic content  Playwright MCP with wait_for\n```\n\n---\n\n## Best Practices\n\n### 1. Minimize Browser Usage\n- Always try `WebFetch` first (10x faster, no browser overhead)\n- Cache extracted content to avoid re-scraping\n- Use `browser_evaluate` to extract only needed content\n\n### 2. Handle Dynamic Content\n- Always use `wait_for` after navigation\n- Add delays for heavy SPAs: `await new Promise(r => setTimeout(r, 2000))`\n- Check for loading spinners before extracting\n\n### 3. Respect Rate Limits\n- Add delays between page navigations\n- Don't crawl faster than a human would browse\n- Honor robots.txt and terms of service\n\n### 4. Clean Extracted Content\n- Remove navigation, headers, footers\n- Strip ads and promotional content\n- Convert to clean markdown before sending to SkillForge\n\n---\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Empty content | Add `wait_for` with appropriate selector |\n| Partial render | Increase timeout or add explicit delay |\n| Login required | Use Chrome extension with user session |\n| CAPTCHA blocking | Manual intervention required |\n| Content in iframe | Use `browser_evaluate` to access iframe content |\n\n---\n\n## Related Skills\n\n- `webapp-testing` - Playwright test automation patterns\n- `streaming-api-patterns` - Handle SSE progress updates from SkillForge\n- `ai-native-development` - RAG pipeline integration\n\n---\n\n**Version:** 1.0.0 (December 2025)\n**MCP Requirement:** Playwright MCP server or Claude Chrome extension\n\n## Capability Details\n\n### spa-extraction\n**Keywords:** react, vue, angular, spa, javascript, client-side, hydration, ssr\n**Solves:**\n- WebFetch returns empty content\n- Page requires JavaScript to render\n- React/Vue app content extraction\n\n### auth-handling\n**Keywords:** login, authentication, session, cookie, protected, private, gated\n**Solves:**\n- Content behind login wall\n- Need to authenticate first\n- Private documentation access\n\n### multi-page-crawl\n**Keywords:** crawl, sitemap, navigation, multiple pages, documentation, tutorial series\n**Solves:**\n- Capture entire documentation site\n- Extract multiple pages\n- Follow navigation links\n\n### mcp-tools\n**Keywords:** playwright, mcp, browser_navigate, browser_evaluate, browser_click\n**Solves:**\n- Which MCP tool to use\n- Browser automation commands\n- Playwright MCP reference\n\n### skillforge-integration\n**Keywords:** queue, analyze, pipeline, api, skillforge, content_override\n**Solves:**\n- Send captured content to SkillForge\n- Queue URL for analysis\n- Integrate with analysis pipeline"
              },
              {
                "name": "cache-cost-tracking",
                "description": "LLM cost tracking with Langfuse for cached responses. Use when monitoring cache effectiveness, tracking cost savings, or attributing costs to agents in multi-agent systems.",
                "path": ".claude/skills/cache-cost-tracking/SKILL.md",
                "frontmatter": {
                  "name": "cache-cost-tracking",
                  "description": "LLM cost tracking with Langfuse for cached responses. Use when monitoring cache effectiveness, tracking cost savings, or attributing costs to agents in multi-agent systems.",
                  "context": "fork",
                  "agent": "metrics-architect"
                },
                "content": "# Cache Cost Tracking\n\nMonitor LLM costs and cache effectiveness.\n\n## When to Use\n\n- Cost attribution by agent\n- Cache hit rate monitoring\n- ROI analysis for caching\n- Multi-agent cost rollup\n\n## Langfuse Automatic Tracking\n\n```python\nfrom langfuse.decorators import observe, langfuse_context\n\n@observe(as_type=\"generation\")\nasync def call_llm_with_cache(\n    prompt: str,\n    agent_type: str,\n    analysis_id: UUID\n) -> str:\n    \"\"\"LLM call with automatic cost tracking.\"\"\"\n\n    # Link to parent trace\n    langfuse_context.update_current_trace(\n        name=f\"{agent_type}_generation\",\n        session_id=str(analysis_id)\n    )\n\n    # Check caches\n    if cache_key in lru_cache:\n        langfuse_context.update_current_observation(\n            metadata={\"cache_layer\": \"L1\", \"cache_hit\": True}\n        )\n        return lru_cache[cache_key]\n\n    similar = await semantic_cache.get(prompt, agent_type)\n    if similar:\n        langfuse_context.update_current_observation(\n            metadata={\"cache_layer\": \"L2\", \"cache_hit\": True}\n        )\n        return similar\n\n    # LLM call - Langfuse tracks tokens/cost automatically\n    response = await llm.generate(prompt)\n\n    langfuse_context.update_current_observation(\n        metadata={\n            \"cache_layer\": \"L4\",\n            \"cache_hit\": False,\n            \"prompt_cache_hit\": response.usage.cache_read_input_tokens > 0\n        }\n    )\n\n    return response.content\n```\n\n## Hierarchical Cost Rollup\n\n```python\nclass AnalysisWorkflow:\n    @observe(as_type=\"trace\")\n    async def run_analysis(self, url: str, analysis_id: UUID):\n        \"\"\"Parent trace aggregates child costs.\n\n        Trace Hierarchy:\n        run_analysis (trace)\n         security_agent (generation)\n         tech_agent (generation)\n         synthesis (generation)\n        \"\"\"\n        langfuse_context.update_current_trace(\n            name=\"content_analysis\",\n            session_id=str(analysis_id),\n            tags=[\"multi-agent\"]\n        )\n\n        for agent in self.agents:\n            await self.run_agent(agent, content, analysis_id)\n\n    @observe(as_type=\"generation\")\n    async def run_agent(self, agent, content, analysis_id):\n        \"\"\"Child generation - costs roll up to parent.\"\"\"\n        langfuse_context.update_current_observation(\n            name=f\"{agent.name}_generation\",\n            metadata={\"agent_type\": agent.name}\n        )\n        return await agent.analyze(content)\n```\n\n## Cost Queries\n\n```python\nfrom langfuse import Langfuse\n\nasync def get_analysis_costs(analysis_id: UUID) -> dict:\n    langfuse = Langfuse()\n\n    traces = langfuse.get_traces(session_id=str(analysis_id), limit=1)\n\n    if traces.data:\n        trace = traces.data[0]\n        return {\n            \"total_cost\": trace.total_cost,\n            \"input_tokens\": trace.usage.input_tokens,\n            \"output_tokens\": trace.usage.output_tokens,\n            \"cache_read_tokens\": trace.usage.cache_read_input_tokens,\n        }\n\nasync def get_costs_by_agent() -> list[dict]:\n    generations = langfuse.get_generations(\n        from_timestamp=datetime.now() - timedelta(days=7),\n        limit=1000\n    )\n\n    costs = {}\n    for gen in generations.data:\n        agent = gen.metadata.get(\"agent_type\", \"unknown\")\n        if agent not in costs:\n            costs[agent] = {\"total\": 0, \"calls\": 0, \"cache_hits\": 0}\n\n        costs[agent][\"total\"] += gen.calculated_total_cost or 0\n        costs[agent][\"calls\"] += 1\n        if gen.metadata.get(\"cache_hit\"):\n            costs[agent][\"cache_hits\"] += 1\n\n    return list(costs.values())\n```\n\n## Cache Effectiveness\n\n```python\ncache_hits = 0\ncache_misses = 0\ncost_saved = 0.0\n\nfor gen in generations:\n    if gen.metadata.get(\"cache_hit\"):\n        cache_hits += 1\n        cost_saved += estimate_full_cost(gen)\n    else:\n        cache_misses += 1\n\nhit_rate = cache_hits / (cache_hits + cache_misses)\nprint(f\"Cache Hit Rate: {hit_rate:.1%}\")\nprint(f\"Cost Saved: ${cost_saved:.2f}\")\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Trace grouping | session_id = analysis_id |\n| Cost attribution | metadata.agent_type |\n| Query window | 7-30 days |\n| Dashboard | Langfuse web UI |\n\n## Common Mistakes\n\n- Not linking child to parent trace\n- Missing metadata for attribution\n- Not tracking cache hits separately\n- Ignoring prompt cache savings\n\n## Related Skills\n\n- `semantic-caching` - Redis caching\n- `prompt-caching` - Provider caching\n- `langfuse-observability` - Full observability\n\n## Capability Details\n\n### prompt-caching\n**Keywords:** prompt cache, cache prompt, prefix caching, cache breakpoints\n**Solves:**\n- Reduce token costs with cached prompts\n- Configure cache breakpoints\n- Implement provider-native caching\n\n### response-caching\n**Keywords:** response cache, semantic cache, cache response, LLM cache\n**Solves:**\n- Cache LLM responses for repeated queries\n- Implement semantic similarity caching\n- Reduce API calls with cached responses\n\n### cost-calculation\n**Keywords:** cost, token cost, calculate cost, pricing, usage cost\n**Solves:**\n- Calculate token costs by model\n- Track input/output token pricing\n- Estimate cost before execution\n\n### usage-tracking\n**Keywords:** usage, track usage, token usage, API usage, metrics\n**Solves:**\n- Track LLM API usage over time\n- Monitor token consumption\n- Generate usage reports\n\n### cache-invalidation\n**Keywords:** invalidate, cache invalidation, TTL, expire, refresh\n**Solves:**\n- Implement cache invalidation strategies\n- Configure TTL for cached responses\n- Handle stale cache entries"
              },
              {
                "name": "caching-strategies",
                "description": "Backend caching patterns with Redis including write-through, write-behind, cache-aside, and invalidation strategies. Use when optimizing read performance or reducing database load.",
                "path": ".claude/skills/caching-strategies/SKILL.md",
                "frontmatter": {
                  "name": "caching-strategies",
                  "description": "Backend caching patterns with Redis including write-through, write-behind, cache-aside, and invalidation strategies. Use when optimizing read performance or reducing database load.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "tags": [
                    "caching",
                    "redis",
                    "performance",
                    "fastapi",
                    "python",
                    2026
                  ]
                },
                "content": "# Backend Caching Strategies\n\nOptimize performance with Redis caching patterns and smart invalidation.\n\n## When to Use\n\n- Reducing database load for frequent reads\n- Caching expensive computations (LLM responses, embeddings)\n- Session and user data caching\n- API response caching\n- Distributed caching across instances\n\n## Pattern Selection\n\n| Pattern | Write | Read | Consistency | Use Case |\n|---------|-------|------|-------------|----------|\n| Cache-Aside | DB first | Cache  DB | Eventual | General purpose |\n| Write-Through | Cache + DB | Cache | Strong | Critical data |\n| Write-Behind | Cache, async DB | Cache | Eventual | High write load |\n| Read-Through | Cache handles | Cache  DB | Eventual | Simplified reads |\n\n## Cache-Aside (Lazy Loading)\n\n```python\nimport redis.asyncio as redis\nfrom typing import TypeVar, Callable\nimport json\n\nT = TypeVar(\"T\")\n\nclass CacheAside:\n    def __init__(self, redis_client: redis.Redis, default_ttl: int = 3600):\n        self.redis = redis_client\n        self.ttl = default_ttl\n\n    async def get_or_set(\n        self,\n        key: str,\n        fetch_fn: Callable[[], T],\n        ttl: int | None = None,\n        serialize: Callable[[T], str] = json.dumps,\n        deserialize: Callable[[str], T] = json.loads,\n    ) -> T:\n        \"\"\"Get from cache, or fetch and cache.\"\"\"\n        # Try cache first\n        cached = await self.redis.get(key)\n        if cached:\n            return deserialize(cached)\n\n        # Cache miss - fetch from source\n        value = await fetch_fn()\n\n        # Store in cache\n        await self.redis.setex(\n            key,\n            ttl or self.ttl,\n            serialize(value),\n        )\n        return value\n\n# Usage\ncache = CacheAside(redis_client)\n\nasync def get_analysis(analysis_id: str) -> Analysis:\n    return await cache.get_or_set(\n        key=f\"analysis:{analysis_id}\",\n        fetch_fn=lambda: repo.get_by_id(analysis_id),\n        ttl=1800,  # 30 minutes\n    )\n```\n\n## Write-Through Cache\n\n```python\nclass WriteThroughCache:\n    def __init__(self, redis_client: redis.Redis, ttl: int = 3600):\n        self.redis = redis_client\n        self.ttl = ttl\n\n    async def write(\n        self,\n        key: str,\n        value: T,\n        db_write_fn: Callable[[T], Awaitable[T]],\n    ) -> T:\n        \"\"\"Write to both cache and database synchronously.\"\"\"\n        # Write to database first (consistency)\n        result = await db_write_fn(value)\n\n        # Then update cache\n        await self.redis.setex(key, self.ttl, json.dumps(result))\n\n        return result\n\n    async def read(self, key: str) -> T | None:\n        \"\"\"Read from cache only.\"\"\"\n        cached = await self.redis.get(key)\n        return json.loads(cached) if cached else None\n\n# Usage\ncache = WriteThroughCache(redis_client)\n\nasync def update_analysis(analysis_id: str, data: AnalysisUpdate) -> Analysis:\n    return await cache.write(\n        key=f\"analysis:{analysis_id}\",\n        value=data,\n        db_write_fn=lambda d: repo.update(analysis_id, d),\n    )\n```\n\n## Write-Behind (Write-Back)\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass WriteBehindCache:\n    def __init__(\n        self,\n        redis_client: redis.Redis,\n        flush_interval: float = 5.0,\n        batch_size: int = 100,\n    ):\n        self.redis = redis_client\n        self.flush_interval = flush_interval\n        self.batch_size = batch_size\n        self._pending_writes: deque = deque()\n        self._flush_task: asyncio.Task | None = None\n\n    async def start(self):\n        \"\"\"Start background flush task.\"\"\"\n        self._flush_task = asyncio.create_task(self._flush_loop())\n\n    async def stop(self):\n        \"\"\"Stop and flush remaining writes.\"\"\"\n        if self._flush_task:\n            self._flush_task.cancel()\n        await self._flush_pending()\n\n    async def write(self, key: str, value: T) -> None:\n        \"\"\"Write to cache immediately, queue for DB.\"\"\"\n        await self.redis.set(key, json.dumps(value))\n        self._pending_writes.append((key, value))\n\n        if len(self._pending_writes) >= self.batch_size:\n            await self._flush_pending()\n\n    async def _flush_loop(self):\n        while True:\n            await asyncio.sleep(self.flush_interval)\n            await self._flush_pending()\n\n    async def _flush_pending(self):\n        if not self._pending_writes:\n            return\n\n        batch = []\n        while self._pending_writes and len(batch) < self.batch_size:\n            batch.append(self._pending_writes.popleft())\n\n        # Bulk write to database\n        await repo.bulk_upsert([v for _, v in batch])\n```\n\n## Cache Invalidation Patterns\n\n### TTL-Based (Time to Live)\n\n```python\n# Simple TTL\nawait redis.setex(\"analysis:123\", 3600, data)  # 1 hour\n\n# TTL with jitter (prevent stampede)\nimport random\nbase_ttl = 3600\njitter = random.randint(-300, 300)  # 5 minutes\nawait redis.setex(\"analysis:123\", base_ttl + jitter, data)\n```\n\n### Event-Based Invalidation\n\n```python\nclass CacheInvalidator:\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n\n    async def invalidate(self, key: str) -> None:\n        \"\"\"Delete single key.\"\"\"\n        await self.redis.delete(key)\n\n    async def invalidate_pattern(self, pattern: str) -> int:\n        \"\"\"Delete keys matching pattern.\"\"\"\n        keys = []\n        async for key in self.redis.scan_iter(match=pattern):\n            keys.append(key)\n\n        if keys:\n            return await self.redis.delete(*keys)\n        return 0\n\n    async def invalidate_tags(self, *tags: str) -> int:\n        \"\"\"Invalidate all keys with given tags.\"\"\"\n        count = 0\n        for tag in tags:\n            tag_key = f\"tag:{tag}\"\n            members = await self.redis.smembers(tag_key)\n            if members:\n                count += await self.redis.delete(*members)\n            await self.redis.delete(tag_key)\n        return count\n\n# Usage with tags\nasync def cache_with_tags(key: str, value: T, tags: list[str]):\n    await redis.set(key, json.dumps(value))\n    for tag in tags:\n        await redis.sadd(f\"tag:{tag}\", key)\n\n# Invalidate by tag\nawait invalidator.invalidate_tags(\"user:123\", \"analyses\")\n```\n\n### Version-Based Invalidation\n\n```python\nclass VersionedCache:\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n\n    async def get_version(self, namespace: str) -> int:\n        version = await self.redis.get(f\"version:{namespace}\")\n        return int(version) if version else 1\n\n    async def increment_version(self, namespace: str) -> int:\n        return await self.redis.incr(f\"version:{namespace}\")\n\n    def make_key(self, namespace: str, key: str, version: int) -> str:\n        return f\"{namespace}:v{version}:{key}\"\n\n    async def get(self, namespace: str, key: str) -> T | None:\n        version = await self.get_version(namespace)\n        full_key = self.make_key(namespace, key, version)\n        cached = await self.redis.get(full_key)\n        return json.loads(cached) if cached else None\n\n    async def invalidate_namespace(self, namespace: str) -> None:\n        \"\"\"Increment version to invalidate all keys.\"\"\"\n        await self.increment_version(namespace)\n```\n\n## Cache Stampede Prevention\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\n\nclass StampedeProtection:\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        self._local_locks: dict[str, asyncio.Lock] = {}\n\n    @asynccontextmanager\n    async def lock(self, key: str, timeout: int = 10):\n        \"\"\"Distributed lock to prevent stampede.\"\"\"\n        lock_key = f\"lock:{key}\"\n\n        # Try to acquire distributed lock\n        acquired = await self.redis.set(\n            lock_key, \"1\", nx=True, ex=timeout\n        )\n\n        if not acquired:\n            # Wait for existing computation\n            for _ in range(timeout * 10):\n                if await self.redis.exists(key):\n                    return  # Data available\n                await asyncio.sleep(0.1)\n            raise TimeoutError(f\"Lock timeout for {key}\")\n\n        try:\n            yield\n        finally:\n            await self.redis.delete(lock_key)\n\n# Usage\nasync def get_expensive_data(key: str) -> Data:\n    cached = await redis.get(key)\n    if cached:\n        return json.loads(cached)\n\n    async with stampede.lock(key):\n        # Double-check after acquiring lock\n        cached = await redis.get(key)\n        if cached:\n            return json.loads(cached)\n\n        # Compute expensive data\n        data = await compute_expensive_data()\n        await redis.setex(key, 3600, json.dumps(data))\n        return data\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER cache without TTL (memory leak)\nawait redis.set(\"key\", value)  # No expiration!\n\n# NEVER cache sensitive data without encryption\nawait redis.set(\"user:123:password\", password)\n\n# NEVER use cache as primary storage\nawait redis.set(\"order:123\", order_data)\n# ... database write fails, data lost!\n\n# NEVER ignore cache failures\ntry:\n    await redis.get(key)\nexcept:\n    pass  # Silent failure = stale data\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Default TTL | 1 hour for most data, 5 min for volatile |\n| Serialization | orjson for performance |\n| Key naming | `{entity}:{id}` or `{entity}:{id}:{field}` |\n| Stampede | Use locks for expensive computations |\n| Invalidation | Event-based for writes, TTL for reads |\n\n## Related Skills\n\n- `redis-patterns` - Advanced Redis usage\n- `resilience-patterns` - Fallback strategies\n- `observability-monitoring` - Cache hit metrics\n\n## Capability Details\n\n### cache-aside\n**Keywords:** cache aside, lazy loading, cache miss, get or set\n**Solves:**\n- How to implement lazy loading cache?\n- Cache on read pattern\n\n### write-through\n**Keywords:** write through, cache consistency, synchronous cache\n**Solves:**\n- How to keep cache consistent with database?\n- Strong consistency caching\n\n### write-behind\n**Keywords:** write behind, write back, async cache, batch writes\n**Solves:**\n- High write throughput caching\n- Async database writes\n\n### cache-invalidation\n**Keywords:** invalidation, cache bust, TTL, cache tags\n**Solves:**\n- How to invalidate cache?\n- When to expire cached data\n\n### stampede-prevention\n**Keywords:** stampede, thundering herd, cache lock, singleflight\n**Solves:**\n- Prevent cache stampede\n- Multiple requests hitting DB"
              },
              {
                "name": "clean-architecture",
                "description": "SOLID principles, hexagonal architecture, ports & adapters, and DDD tactical patterns for maintainable Python/FastAPI backends. Use when designing service layers, domain models, or refactoring legacy code.",
                "path": ".claude/skills/clean-architecture/SKILL.md",
                "frontmatter": {
                  "name": "clean-architecture",
                  "description": "SOLID principles, hexagonal architecture, ports & adapters, and DDD tactical patterns for maintainable Python/FastAPI backends. Use when designing service layers, domain models, or refactoring legacy code.",
                  "context": "fork",
                  "agent": "code-quality-reviewer",
                  "version": "1.0.0",
                  "tags": [
                    "architecture",
                    "solid",
                    "hexagonal",
                    "ddd",
                    "python",
                    "fastapi",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/design-decision-saver.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/design-decision-saver.sh"
                      }
                    ]
                  }
                },
                "content": "# Clean Architecture Patterns\n\nBuild maintainable, testable backends with SOLID principles and hexagonal architecture.\n\n## When to Use\n\n- Designing new service layer architecture\n- Refactoring tightly-coupled code\n- Implementing domain-driven design\n- Creating testable business logic\n- Separating infrastructure from domain\n\n## SOLID Principles (2026 Python)\n\n### S - Single Responsibility\n\n```python\n# BAD: One class doing everything\nclass UserManager:\n    def create_user(self, data): ...\n    def send_welcome_email(self, user): ...\n    def generate_report(self, users): ...\n\n# GOOD: Separate responsibilities\nclass UserService:\n    def create_user(self, data: UserCreate) -> User: ...\n\nclass EmailService:\n    def send_welcome(self, user: User) -> None: ...\n\nclass ReportService:\n    def generate_user_report(self, users: list[User]) -> Report: ...\n```\n\n### O - Open/Closed (Protocol-based)\n\n```python\nfrom typing import Protocol\n\nclass PaymentProcessor(Protocol):\n    async def process(self, amount: Decimal) -> PaymentResult: ...\n\nclass StripeProcessor:\n    async def process(self, amount: Decimal) -> PaymentResult:\n        # Stripe implementation\n        ...\n\nclass PayPalProcessor:\n    async def process(self, amount: Decimal) -> PaymentResult:\n        # PayPal implementation - extends without modifying\n        ...\n```\n\n### L - Liskov Substitution\n\n```python\n# Any implementation of Repository can substitute another\nclass IUserRepository(Protocol):\n    async def get_by_id(self, id: str) -> User | None: ...\n    async def save(self, user: User) -> User: ...\n\nclass PostgresUserRepository:\n    async def get_by_id(self, id: str) -> User | None: ...\n    async def save(self, user: User) -> User: ...\n\nclass InMemoryUserRepository:  # For testing - fully substitutable\n    async def get_by_id(self, id: str) -> User | None: ...\n    async def save(self, user: User) -> User: ...\n```\n\n### I - Interface Segregation\n\n```python\n# BAD: Fat interface\nclass IRepository(Protocol):\n    async def get(self, id: str): ...\n    async def save(self, entity): ...\n    async def delete(self, id: str): ...\n    async def search(self, query: str): ...\n    async def bulk_insert(self, entities): ...\n\n# GOOD: Segregated interfaces\nclass IReader(Protocol):\n    async def get(self, id: str) -> T | None: ...\n\nclass IWriter(Protocol):\n    async def save(self, entity: T) -> T: ...\n\nclass ISearchable(Protocol):\n    async def search(self, query: str) -> list[T]: ...\n```\n\n### D - Dependency Inversion\n\n```python\nfrom typing import Protocol\nfrom fastapi import Depends\n\nclass IAnalysisRepository(Protocol):\n    async def get_by_id(self, id: str) -> Analysis | None: ...\n\nclass AnalysisService:\n    def __init__(self, repo: IAnalysisRepository):\n        self._repo = repo  # Depends on abstraction, not concrete\n\n# FastAPI DI\ndef get_analysis_service(\n    db: AsyncSession = Depends(get_db)\n) -> AnalysisService:\n    repo = PostgresAnalysisRepository(db)\n    return AnalysisService(repo)\n```\n\n## Hexagonal Architecture (Ports & Adapters)\n\n```\n\n                      DRIVING ADAPTERS                        \n            \n   FastAPI       CLI        Celery      Tests       \n   Routes      Commands     Tasks       Mocks       \n            \n                                                         \n                                                         \n    \n                      INPUT PORTS                          \n         \n     AnalysisService    UserService                    \n     (Use Cases)        (Use Cases)                    \n         \n    \n                       DOMAIN                            \n        \n      Entities    Value Objects    Domain Events       \n      Analysis    AnalysisType     AnalysisCreated     \n        \n    \n                     OUTPUT PORTS                          \n         \n     IAnalysisRepo       INotificationService          \n     (Protocol)          (Protocol)                    \n         \n    \n                                                           \n     \n   PostgresRepo         EmailNotificationService        \n   (SQLAlchemy)         (SMTP/SendGrid)                 \n     \n                      DRIVEN ADAPTERS                        \n\n```\n\n## DDD Tactical Patterns\n\n### Entity (Identity-based)\n\n```python\nfrom dataclasses import dataclass, field\nfrom uuid import UUID, uuid4\n\n@dataclass\nclass Analysis:\n    id: UUID = field(default_factory=uuid4)\n    source_url: str\n    status: AnalysisStatus\n    created_at: datetime = field(default_factory=datetime.utcnow)\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Analysis):\n            return False\n        return self.id == other.id  # Identity equality\n```\n\n### Value Object (Structural equality)\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass(frozen=True)  # Immutable\nclass AnalysisType:\n    category: str\n    depth: int\n\n    def __post_init__(self):\n        if self.depth < 1 or self.depth > 3:\n            raise ValueError(\"Depth must be 1-3\")\n```\n\n### Aggregate Root\n\n```python\nclass AnalysisAggregate:\n    def __init__(self, analysis: Analysis, artifacts: list[Artifact]):\n        self._analysis = analysis\n        self._artifacts = artifacts\n        self._events: list[DomainEvent] = []\n\n    def complete(self, summary: str) -> None:\n        self._analysis.status = AnalysisStatus.COMPLETED\n        self._analysis.summary = summary\n        self._events.append(AnalysisCompleted(self._analysis.id))\n\n    def collect_events(self) -> list[DomainEvent]:\n        events = self._events.copy()\n        self._events.clear()\n        return events\n```\n\n## Directory Structure\n\n```\nbackend/app/\n api/v1/              # Driving adapters (FastAPI routes)\n domains/\n    analysis/\n        entities.py      # Domain entities\n        value_objects.py # Value objects\n        services.py      # Domain services (use cases)\n        repositories.py  # Output port protocols\n        events.py        # Domain events\n infrastructure/\n    repositories/    # Driven adapters (PostgreSQL)\n    services/        # External service adapters\n    messaging/       # Event publishers\n core/\n     dependencies.py  # FastAPI DI configuration\n     protocols.py     # Shared protocols\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER import infrastructure in domain\nfrom app.infrastructure.database import engine  # In domain layer\n\n# NEVER leak ORM models to API\n@router.get(\"/users/{id}\")\nasync def get_user(id: str, db: Session) -> UserModel:  # Returns ORM model\n    return db.query(UserModel).get(id)\n\n# NEVER have domain depend on framework\nfrom fastapi import HTTPException\nclass UserService:\n    def get(self, id: str):\n        if not user:\n            raise HTTPException(404)  # Framework in domain!\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Protocol vs ABC | Use Protocol (structural typing) |\n| Dataclass vs Pydantic | Dataclass for domain, Pydantic for API |\n| Repository granularity | One per aggregate root |\n| Transaction boundary | Service layer, not repository |\n| Event publishing | Collect in aggregate, publish after commit |\n\n## Related Skills\n\n- `repository-patterns` - Detailed repository implementations\n- `api-design-framework` - REST API patterns\n- `database-schema-designer` - Schema design\n\n## Capability Details\n\n### solid-principles\n**Keywords:** SOLID, single responsibility, open closed, liskov, interface segregation, dependency inversion\n**Solves:**\n- How do I apply SOLID principles in Python?\n- My classes are doing too much\n\n### hexagonal-architecture\n**Keywords:** hexagonal, ports and adapters, clean architecture, onion\n**Solves:**\n- How do I structure my FastAPI app?\n- How to separate infrastructure from domain?\n\n### ddd-tactical\n**Keywords:** entity, value object, aggregate, domain event, DDD\n**Solves:**\n- What's the difference between entity and value object?\n- How to design aggregates?"
              },
              {
                "name": "code-review-playbook",
                "description": "Use this skill when conducting or improving code reviews. Provides structured review processes, conventional comments patterns, language-specific checklists, and feedback templates. Ensures consistent, constructive, and thorough code reviews across teams.",
                "path": ".claude/skills/code-review-playbook/SKILL.md",
                "frontmatter": {
                  "name": "code-review-playbook",
                  "description": "Use this skill when conducting or improving code reviews. Provides structured review processes, conventional comments patterns, language-specific checklists, and feedback templates. Ensures consistent, constructive, and thorough code reviews across teams.",
                  "version": "2.0.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "code-review",
                    "quality",
                    "collaboration",
                    "best-practices"
                  ],
                  "context": "fork",
                  "agent": "code-quality-reviewer",
                  "hooks": {
                    "PreToolUse": [
                      {
                        "matcher": "Edit",
                        "command": "echo \"::warning::Code review skill should not modify code - report only\"\n"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/review-summary-generator.sh"
                      }
                    ]
                  }
                },
                "content": "# Code Review Playbook\n\n## Overview\n\nThis skill provides a comprehensive framework for effective code reviews that improve code quality, share knowledge, and foster collaboration. Whether you're a reviewer giving feedback or an author preparing code for review, this playbook ensures reviews are thorough, consistent, and constructive.\n\n**When to use this skill:**\n- Reviewing pull requests or merge requests\n- Preparing code for review (self-review)\n- Establishing code review standards for teams\n- Training new developers on review best practices\n- Resolving disagreements about code quality\n- Improving review processes and efficiency\n\n## Code Review Philosophy\n\n### Purpose of Code Reviews\n\nCode reviews serve multiple purposes:\n\n1. **Quality Assurance**: Catch bugs, logic errors, and edge cases\n2. **Knowledge Sharing**: Spread domain knowledge across the team\n3. **Consistency**: Ensure codebase follows conventions and patterns\n4. **Mentorship**: Help developers improve their skills\n5. **Collective Ownership**: Build shared responsibility for code\n6. **Documentation**: Create discussion history for future reference\n\n### Principles\n\n**Be Kind and Respectful:**\n- Review the code, not the person\n- Assume positive intent\n- Praise good solutions\n- Frame feedback constructively\n\n**Be Specific and Actionable:**\n- Point to specific lines of code\n- Explain *why* something should change\n- Suggest concrete improvements\n- Provide examples when helpful\n\n**Balance Speed with Thoroughness:**\n- Aim for timely feedback (< 24 hours)\n- Don't rush critical reviews\n- Use automation for routine checks\n- Focus human review on logic and design\n\n**Distinguish Must-Fix from Nice-to-Have:**\n- Use conventional comments to indicate severity\n- Block merges only for critical issues\n- Allow authors to defer minor improvements\n- Capture deferred work in follow-up tickets\n\n---\n\n## Conventional Comments\n\nA standardized format for review comments that makes intent clear.\n\n### Format\n\n```\n<label> [decorations]: <subject>\n\n[discussion]\n```\n\n### Labels\n\n| Label | Meaning | Blocks Merge? |\n|-------|---------|---------------|\n| **praise** | Highlight something positive | No |\n| **nitpick** | Minor, optional suggestion | No |\n| **suggestion** | Propose an improvement | No |\n| **issue** | Problem that should be addressed | Usually |\n| **question** | Request clarification | No |\n| **thought** | Idea to consider | No |\n| **chore** | Routine task (formatting, deps) | No |\n| **note** | Informational comment | No |\n| **todo** | Follow-up work needed | Maybe |\n| **security** | Security concern | **Yes** |\n| **bug** | Potential bug | **Yes** |\n| **breaking** | Breaking change | **Yes** |\n\n### Decorations\n\nOptional modifiers in square brackets:\n\n| Decoration | Meaning |\n|------------|---------|\n| **[blocking]** | Must be addressed before merge |\n| **[non-blocking]** | Optional, can be deferred |\n| **[if-minor]** | Only if it's a quick fix |\n\n### Examples\n\n```typescript\n//  Good: Clear, specific, actionable\n\npraise: Excellent use of TypeScript generics here!\n\nThis makes the function much more reusable while maintaining type safety.\n\n---\n\nnitpick [non-blocking]: Consider using const instead of let\n\nThis variable is never reassigned, so `const` would be more appropriate:\n```typescript\nconst MAX_RETRIES = 3;\n```\n\n---\n\nissue: Missing error handling for API call\n\nIf the API returns a 500 error, this will crash the application.\nAdd a try/catch block:\n```typescript\ntry {\n  const data = await fetchUser(userId);\n  // ...\n} catch (error) {\n  logger.error('Failed to fetch user', { userId, error });\n  throw new UserNotFoundError(userId);\n}\n```\n\n---\n\nquestion: Why use a Map instead of an object here?\n\nIs there a specific reason for this data structure choice?\nIf it's for performance, could you add a comment explaining?\n\n---\n\nsecurity [blocking]: API endpoint is not authenticated\n\nThe `/api/admin/users` endpoint is missing authentication middleware.\nThis allows unauthenticated access to sensitive user data.\n\nAdd the auth middleware:\n```typescript\nrouter.get('/api/admin/users', requireAdmin, getUsers);\n```\n\n---\n\nsuggestion [if-minor]: Extract magic number to named constant\n\nConsider extracting this value:\n```typescript\nconst CACHE_TTL_SECONDS = 3600;\ncache.set(key, value, CACHE_TTL_SECONDS);\n```\n```\n\n---\n\n## Review Process\n\n### 1. Before Reviewing\n\n**Check Context:**\n- Read the PR/MR description\n- Understand the purpose and scope\n- Review linked tickets or issues\n- Check CI/CD pipeline status\n\n**Verify Automated Checks:**\n- [ ] Tests are passing\n- [ ] Linting has no errors\n- [ ] Type checking passes\n- [ ] Code coverage meets targets\n- [ ] No merge conflicts\n\n**Set Aside Time:**\n- Small PR (< 200 lines): 15-30 minutes\n- Medium PR (200-500 lines): 30-60 minutes\n- Large PR (> 500 lines): 1-2 hours (or ask to split)\n\n### 2. During Review\n\n**Follow a Pattern:**\n\n1. **High-Level Review** (5-10 minutes)\n   - Read PR description and understand intent\n   - Skim all changed files to get overview\n   - Verify approach makes sense architecturally\n   - Check that changes align with stated purpose\n\n2. **Detailed Review** (20-45 minutes)\n   - Line-by-line code review\n   - Check logic, edge cases, error handling\n   - Verify tests cover new code\n   - Look for security vulnerabilities\n   - Ensure code follows team conventions\n\n3. **Testing Considerations** (5-10 minutes)\n   - Are tests comprehensive?\n   - Do tests test the right things?\n   - Are edge cases covered?\n   - Is test data realistic?\n\n4. **Documentation Check** (5 minutes)\n   - Are complex sections commented?\n   - Is public API documented?\n   - Are breaking changes noted?\n   - Is README updated if needed?\n\n### 3. After Reviewing\n\n**Provide Clear Decision:**\n-  **Approve**: Code is ready to merge\n-  **Comment**: Feedback provided, no action required\n-  **Request Changes**: Issues must be addressed before merge\n\n**Respond to Author:**\n- Answer questions promptly\n- Re-review after changes made\n- Approve when issues resolved\n- Thank author for addressing feedback\n\n---\n\n## Review Checklists\n\n### General Code Quality\n\n- [ ] **Readability**: Code is easy to understand\n- [ ] **Naming**: Variables and functions have clear, descriptive names\n- [ ] **Comments**: Complex logic is explained\n- [ ] **Formatting**: Code follows team style guide\n- [ ] **DRY**: No unnecessary duplication\n- [ ] **SOLID Principles**: Code follows SOLID where applicable\n- [ ] **Function Size**: Functions are focused and < 50 lines\n- [ ] **Cyclomatic Complexity**: Functions have complexity < 10\n\n### Security\n\n- [ ] **Authentication**: Protected endpoints require auth\n- [ ] **Authorization**: Users can only access their own data\n- [ ] **Input Sanitization**: SQL injection, XSS prevented\n- [ ] **Secrets Management**: No hardcoded credentials or API keys\n- [ ] **Encryption**: Sensitive data encrypted at rest and in transit\n- [ ] **Rate Limiting**: Endpoints protected from abuse\n\n---\n\n## Quick Start Guide\n\n**For Reviewers:**\n1. Read PR description and understand intent\n2. Check that automated checks pass\n3. Do high-level review (architecture, approach)\n4. Do detailed review (logic, edge cases, tests)\n5. Use conventional comments for clear communication\n6. Provide decision: Approve, Comment, or Request Changes\n\n**For Authors:**\n1. Write clear PR description\n2. Perform self-review before requesting review\n3. Ensure all automated checks pass\n4. Keep PR focused and reasonably sized (< 400 lines)\n5. Respond to feedback promptly and respectfully\n6. Make requested changes or explain reasoning\n\n---\n\n**Skill Version**: 2.0.0\n**Last Updated**: 2026-01-08\n**Maintained by**: AI Agent Hub Team\n\n## Capability Details\n\n### review-process\n**Keywords:** code review, pr review, review process, feedback\n**Solves:**\n- How to review PRs\n- Conventional comments format\n- Review best practices\n\n### quality-checks\n**Keywords:** readability, solid, dry, complexity, naming\n**Solves:**\n- Check code quality\n- SOLID principles review\n- Cyclomatic complexity\n\n### security-review\n**Keywords:** security, authentication, authorization, injection, xss\n**Solves:**\n- Security review checklist\n- Find vulnerabilities\n- Auth validation\n\n### language-specific\n**Keywords:** typescript, python, type hints, async await, pep8\n**Solves:**\n- TypeScript review\n- Python review\n- Language-specific patterns\n\n### pr-template\n**Keywords:** pr template, pull request, description\n**Solves:**\n- PR description format\n- Review checklist"
              },
              {
                "name": "commit",
                "description": "Smart git commit with validation, conventional format, and branch protection",
                "path": ".claude/skills/commit/SKILL.md",
                "frontmatter": {
                  "name": "commit",
                  "description": "Smart git commit with validation, conventional format, and branch protection",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "git",
                    "commit",
                    "version-control",
                    "conventional-commits"
                  ]
                },
                "content": "# Smart Commit\n\nSimple, validated commit creation. Run checks locally, no agents needed for standard commits.\n\n## When to Use\n\n- Creating git commits\n- Staging and committing changes\n- Generating commit messages\n\n## Quick Start\n\n```bash\n/commit\n```\n\n## Workflow\n\n### Phase 1: Pre-Commit Safety Check\n\n```bash\n# CRITICAL: Verify we're not on dev/main\nBRANCH=$(git branch --show-current)\nif [[ \"$BRANCH\" == \"dev\" || \"$BRANCH\" == \"main\" || \"$BRANCH\" == \"master\" ]]; then\n  echo \"STOP! Cannot commit directly to $BRANCH\"\n  echo \"Create a feature branch: git checkout -b issue/<number>-<description>\"\n  exit 1\nfi\n```\n\n### Phase 2: Run Validation Locally\n\nRun every check that CI runs:\n\n```bash\n# Backend (Python)\npoetry run ruff format --check app/\npoetry run ruff check app/\npoetry run mypy app/\n\n# Frontend (Node.js)\nnpm run format:check\nnpm run lint\nnpm run typecheck\n```\n\nFix any failures before proceeding.\n\n### Phase 3: Review Changes\n\n```bash\ngit status\ngit diff --staged   # What will be committed\ngit diff            # Unstaged changes\n```\n\n### Phase 4: Stage and Commit\n\n```bash\n# Stage files\ngit add <files>\n# Or all: git add .\n\n# Commit with conventional format\ngit commit -m \"<type>(#<issue>): <brief description>\n\n- [Change 1]\n- [Change 2]\n\nCo-Authored-By: Claude <noreply@anthropic.com>\"\n\n# Verify\ngit log -1 --stat\n```\n\n## Commit Types\n\n| Type | Use For |\n|------|---------|\n| `feat` | New feature |\n| `fix` | Bug fix |\n| `refactor` | Code improvement |\n| `docs` | Documentation |\n| `test` | Tests only |\n| `chore` | Build/deps/CI |\n\n## Rules\n\n1. **Run validation locally** - Don't spawn agents to run lint/test\n2. **NO file creation** - Don't create MD files or documentation\n3. **One logical change per commit** - Keep commits focused\n4. **Reference issues** - Use `#123` format in commit message\n5. **Subject line < 72 chars** - Keep it concise\n\n## Quick Commit\n\nFor trivial changes (typos, single-line fixes):\n\n```bash\ngit add . && git commit -m \"fix(#123): Fix typo in error message\n\nCo-Authored-By: Claude <noreply@anthropic.com>\"\n```\n\n## References\n\n- [Conventional Commits](references/conventional-commits.md)\n- [Recovery](references/recovery.md)"
              },
              {
                "name": "configure",
                "description": "Interactive SkillForge configuration wizard",
                "path": ".claude/skills/configure/SKILL.md",
                "frontmatter": {
                  "name": "configure",
                  "description": "Interactive SkillForge configuration wizard",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "configuration",
                    "setup",
                    "wizard",
                    "customization"
                  ]
                },
                "content": "# SkillForge Configuration\n\nInteractive setup for customizing your SkillForge installation.\n\n## When to Use\n\n- Initial setup of SkillForge\n- Customizing skill categories\n- Toggling agents on/off\n- Configuring hooks\n- Enabling MCP integrations\n\n## Quick Start\n\n```bash\n/configure\n```\n\n## Step 1: Choose Preset\n\nUse AskUserQuestion:\n\n| Preset | Skills | Agents | Hooks | Description |\n|--------|--------|--------|-------|-------------|\n| **Complete** | 78 | 20 | 92 | Everything |\n| **Standard** | 78 | 0 | 92 | Skills, no agents |\n| **Lite** | 10 | 0 | 92 | Essential only |\n| **Hooks-only** | 0 | 0 | 92 | Just safety |\n\n## Step 2: Customize Skill Categories\n\nCategories available:\n- AI/ML (26 skills)\n- Backend (15 skills)\n- Frontend (8 skills)\n- Testing (13 skills)\n- Security (7 skills)\n- DevOps (4 skills)\n- Planning (6 skills)\n\n## Step 3: Customize Agents\n\n**Product Agents (6):**\n- market-intelligence\n- product-strategist\n- requirements-translator\n- ux-researcher\n- prioritization-analyst\n- business-case-builder\n\n**Technical Agents (14):**\n- backend-system-architect\n- frontend-ui-developer\n- database-engineer\n- llm-integrator\n- workflow-architect\n- data-pipeline-engineer\n- test-generator\n- code-quality-reviewer\n- security-auditor\n- security-layer-auditor\n- debug-investigator\n- metrics-architect\n- rapid-ui-designer\n- system-design-reviewer\n\n## Step 4: Configure Hooks\n\n**Safety Hooks (Always On):**\n- git-branch-protection\n- file-guard\n- redact-secrets\n\n**Toggleable Hooks:**\n- Productivity (auto-approve, logging)\n- Quality Gates (coverage, patterns)\n- Team Coordination (locks, conflicts)\n- Notifications (desktop, sound)\n\n## Step 5: Configure MCPs (Optional)\n\nAll MCPs disabled by default. Enable selectively:\n\n| MCP | Purpose |\n|-----|---------|\n| context7 | Library documentation |\n| sequential-thinking | Complex reasoning |\n| memory | Cross-session persistence |\n| playwright | Browser automation |\n\n## Step 6: Preview & Save\n\nSave to: `~/.claude/plugins/skillforge/config.json`\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"preset\": \"complete\",\n  \"skills\": { \"ai_ml\": true, \"backend\": true, ... },\n  \"agents\": { \"product\": true, \"technical\": true },\n  \"hooks\": { \"safety\": true, \"productivity\": true, ... },\n  \"mcps\": { \"context7\": false, ... }\n}\n```\n\n## References\n\n- [Presets](references/presets.md)\n- [MCP Configuration](references/mcp-config.md)"
              },
              {
                "name": "context-compression",
                "description": "Use when conversation context is too long, hitting token limits, or responses are degrading. Compresses history while preserving critical information using anchored summarization and probe-based validation.",
                "path": ".claude/skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "Use when conversation context is too long, hitting token limits, or responses are degrading. Compresses history while preserving critical information using anchored summarization and probe-based validation.",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "context",
                    "compression",
                    "summarization",
                    "memory",
                    "optimization",
                    2026
                  ]
                },
                "content": "# Context Compression\n\n**Reduce context size while preserving information critical to task completion.**\n\n## Overview\n\nContext compression is essential for long-running agent sessions. The goal is NOT maximum compressionit's preserving enough information to complete tasks without re-fetching.\n\n**Key Metric:** Tokens-per-task (total tokens to complete a task), NOT tokens-per-request.\n\n## When to Use\n\n- Long-running conversations approaching context limits\n- Multi-step agent workflows with accumulating history\n- Sessions with large tool outputs\n- Memory management in persistent agents\n\n---\n\n## Three Compression Strategies\n\n### 1. Anchored Iterative Summarization (RECOMMENDED)\n\nMaintains structured, persistent summaries with forced sections:\n\n```\n\n  ANCHORED SUMMARY STRUCTURE                                         \n\n                                                                     \n  ## Session Intent                                                  \n  [What we're trying to accomplish - NEVER lose this]                \n                                                                     \n  ## Files Modified                                                  \n  - path/to/file.ts: Added function X, modified class Y              \n  - path/to/other.py: Fixed bug in method Z                          \n                                                                     \n  ## Decisions Made                                                  \n  - Decision 1: Chose X over Y because [rationale]                   \n  - Decision 2: Deferred Z until [condition]                         \n                                                                     \n  ## Current State                                                   \n  [Where we are in the task - progress indicator]                    \n                                                                     \n  ## Blockers / Open Questions                                       \n  - Question 1: Awaiting user input on...                            \n                                                                     \n  ## Next Steps                                                      \n  1. Complete X                                                      \n  2. Test Y                                                          \n  3. Deploy Z                                                        \n                                                                     \n\n```\n\n**Why it works:**\n- Structure FORCES preservation of critical categories\n- Each section must be explicitly populated (can't silently drop info)\n- Incremental merge (new compressions extend, don't replace)\n\n### 2. Opaque Compression\n\nMaximum compression for reconstruction, sacrificing readability:\n\n```python\n# Produces highly compressed representation\ncompressed = llm.compress(\n    history,\n    target=\"reconstruct_state\",\n    max_tokens=500\n)\n# Output: Dense, not human-readable, but reconstructable\n```\n\n**Trade-offs:**\n-  99%+ compression ratios possible\n-  Cannot verify what's preserved\n-  Not interpretable by humans\n-  Risk of losing critical details silently\n\n**Use only when:** Storage is critical and verification isn't needed.\n\n### 3. Regenerative Full Summary\n\nCreates fresh summary on each compression cycle:\n\n```python\n# Regenerates complete summary each time\nsummary = llm.summarize(\n    history,\n    style=\"comprehensive\",\n    sections=[\"intent\", \"progress\", \"decisions\"]\n)\n```\n\n**Trade-offs:**\n-  Readable, structured output\n-  Detail loss across repeated compressions\n-  Each regeneration may drop different details\n-  \"Telephone game\" effect over multiple cycles\n\n---\n\n## Anchored Summarization Implementation\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass AnchoredSummary:\n    \"\"\"Structured summary with forced sections.\"\"\"\n\n    session_intent: str\n    files_modified: dict[str, list[str]] = field(default_factory=dict)\n    decisions_made: list[dict] = field(default_factory=list)\n    current_state: str = \"\"\n    blockers: list[str] = field(default_factory=list)\n    next_steps: list[str] = field(default_factory=list)\n\n    # Metadata\n    compression_count: int = 0\n    last_compressed_at: Optional[str] = None\n    tokens_before: int = 0\n    tokens_after: int = 0\n\n    def merge(self, new_content: \"AnchoredSummary\") -> \"AnchoredSummary\":\n        \"\"\"Incrementally merge new summary into existing.\"\"\"\n        return AnchoredSummary(\n            session_intent=new_content.session_intent or self.session_intent,\n            files_modified={**self.files_modified, **new_content.files_modified},\n            decisions_made=self.decisions_made + new_content.decisions_made,\n            current_state=new_content.current_state,  # Replace with latest\n            blockers=new_content.blockers,  # Replace with current\n            next_steps=new_content.next_steps,  # Replace with current\n            compression_count=self.compression_count + 1,\n        )\n\n    def to_markdown(self) -> str:\n        \"\"\"Render as markdown for context injection.\"\"\"\n        sections = [\n            f\"## Session Intent\\n{self.session_intent}\",\n            f\"## Files Modified\\n\" + \"\\n\".join(\n                f\"- `{path}`: {', '.join(changes)}\"\n                for path, changes in self.files_modified.items()\n            ),\n            f\"## Decisions Made\\n\" + \"\\n\".join(\n                f\"- **{d['decision']}**: {d['rationale']}\"\n                for d in self.decisions_made\n            ),\n            f\"## Current State\\n{self.current_state}\",\n        ]\n\n        if self.blockers:\n            sections.append(f\"## Blockers\\n\" + \"\\n\".join(f\"- {b}\" for b in self.blockers))\n\n        sections.append(f\"## Next Steps\\n\" + \"\\n\".join(\n            f\"{i+1}. {step}\" for i, step in enumerate(self.next_steps)\n        ))\n\n        return \"\\n\\n\".join(sections)\n\n\ndef compress_with_anchor(\n    messages: list[dict],\n    existing_summary: Optional[AnchoredSummary],\n    llm: Any\n) -> AnchoredSummary:\n    \"\"\"\n    Compress messages using anchored summarization.\n\n    Only summarizes NEW messages since last compression,\n    then merges with existing summary.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Analyze these conversation messages and extract structured information.\n\n    MESSAGES:\n    {format_messages(messages)}\n\n    Extract into these REQUIRED sections (all must have content):\n\n    1. SESSION_INTENT: What is the user trying to accomplish?\n    2. FILES_MODIFIED: List each file path and what changed\n    3. DECISIONS_MADE: Key decisions with rationale\n    4. CURRENT_STATE: Where are we in the task?\n    5. BLOCKERS: Any open questions or blockers?\n    6. NEXT_STEPS: What needs to happen next?\n\n    Respond in JSON format matching AnchoredSummary schema.\n    \"\"\"\n\n    response = llm.generate(prompt)\n    new_summary = AnchoredSummary(**parse_json(response))\n\n    if existing_summary:\n        return existing_summary.merge(new_summary)\n\n    return new_summary\n```\n\n---\n\n## Compression Triggers\n\n### Sliding Window Approach\n\n```python\nclass CompressionManager:\n    def __init__(\n        self,\n        trigger_threshold: float = 0.70,  # Compress at 70% capacity\n        target_threshold: float = 0.50,   # Compress down to 50%\n        preserve_recent: int = 5,         # Keep last N messages uncompressed\n        min_messages_to_compress: int = 10,\n    ):\n        self.trigger = trigger_threshold\n        self.target = target_threshold\n        self.preserve_recent = preserve_recent\n        self.min_messages = min_messages_to_compress\n\n    def should_compress(self, messages: list, context_budget: int) -> bool:\n        \"\"\"Check if compression should trigger.\"\"\"\n        current_tokens = count_tokens(messages)\n        utilization = current_tokens / context_budget\n\n        return (\n            utilization >= self.trigger and\n            len(messages) >= self.min_messages\n        )\n\n    def compress(\n        self,\n        messages: list,\n        existing_summary: Optional[AnchoredSummary],\n        llm: Any\n    ) -> tuple[AnchoredSummary, list]:\n        \"\"\"\n        Compress older messages, preserve recent ones.\n\n        Returns: (updated_summary, preserved_messages)\n        \"\"\"\n        # Split messages\n        to_compress = messages[:-self.preserve_recent]\n        to_preserve = messages[-self.preserve_recent:]\n\n        # Compress older messages\n        new_summary = compress_with_anchor(to_compress, existing_summary, llm)\n\n        return new_summary, to_preserve\n```\n\n---\n\n## Probe-Based Evaluation\n\n**Don't use ROUGE/BLEUtest functional preservation:**\n\n```python\nclass CompressionProbes:\n    \"\"\"\n    Test whether compression preserved task-critical information.\n\n    Probes are questions that MUST be answerable from compressed context.\n    \"\"\"\n\n    @staticmethod\n    def generate_probes(original_messages: list) -> list[dict]:\n        \"\"\"Generate probes from original content.\"\"\"\n        probes = []\n\n        # File path probes\n        for msg in original_messages:\n            if \"file\" in msg.get(\"content\", \"\").lower():\n                paths = extract_file_paths(msg[\"content\"])\n                for path in paths:\n                    probes.append({\n                        \"type\": \"file_path\",\n                        \"question\": f\"What changes were made to {path}?\",\n                        \"expected_contains\": path,\n                    })\n\n        # Decision probes\n        for msg in original_messages:\n            if any(word in msg.get(\"content\", \"\").lower()\n                   for word in [\"decided\", \"chose\", \"will use\", \"going with\"]):\n                probes.append({\n                    \"type\": \"decision\",\n                    \"question\": \"What key decisions were made?\",\n                    \"expected_contains\": extract_decision_keywords(msg[\"content\"]),\n                })\n\n        # Error/blocker probes\n        for msg in original_messages:\n            if any(word in msg.get(\"content\", \"\").lower()\n                   for word in [\"error\", \"failed\", \"blocked\", \"issue\"]):\n                probes.append({\n                    \"type\": \"blocker\",\n                    \"question\": \"What errors or blockers were encountered?\",\n                    \"expected_contains\": extract_error_keywords(msg[\"content\"]),\n                })\n\n        return probes\n\n    @staticmethod\n    def evaluate_compression(\n        probes: list[dict],\n        compressed_summary: str,\n        llm: Any\n    ) -> dict:\n        \"\"\"\n        Evaluate if compressed summary can answer probes.\n\n        Returns score and failed probes.\n        \"\"\"\n        results = {\"passed\": 0, \"failed\": 0, \"failed_probes\": []}\n\n        for probe in probes:\n            # Ask LLM to answer probe from compressed context\n            answer = llm.generate(f\"\"\"\n            Based ONLY on this context:\n            {compressed_summary}\n\n            Answer: {probe['question']}\n            \"\"\")\n\n            # Check if expected content is present\n            if probe[\"expected_contains\"].lower() in answer.lower():\n                results[\"passed\"] += 1\n            else:\n                results[\"failed\"] += 1\n                results[\"failed_probes\"].append(probe)\n\n        results[\"score\"] = results[\"passed\"] / max(len(probes), 1)\n        return results\n```\n\n---\n\n## Integration with SkillForge\n\n### In session/state.json (Context Protocol 2.0)\n\n```json\n{\n  \"compression_state\": {\n    \"summary\": {\n      \"session_intent\": \"Implement user authentication\",\n      \"files_modified\": {\n        \"src/auth/login.ts\": [\"Added OAuth flow\", \"Fixed token refresh\"],\n        \"src/api/users.ts\": [\"Added getCurrentUser endpoint\"]\n      },\n      \"decisions_made\": [\n        {\"decision\": \"Use JWT over sessions\", \"rationale\": \"Stateless, scales better\"}\n      ],\n      \"current_state\": \"OAuth flow complete, testing token refresh\",\n      \"next_steps\": [\"Add refresh token rotation\", \"Write E2E tests\"]\n    },\n    \"compression_count\": 3,\n    \"last_compressed_at\": \"2026-01-05T10:30:00Z\",\n    \"probe_score\": 0.95\n  }\n}\n```\n\n### With TodoWrite\n\nCompression integrates with task tracking:\n\n```python\ndef compress_and_update_todos(\n    messages: list,\n    todos: list[dict],\n    summary: AnchoredSummary\n) -> tuple[AnchoredSummary, list[dict]]:\n    \"\"\"\n    Compress messages and sync with todo state.\n\n    Completed todos become part of summary's \"decisions made\".\n    \"\"\"\n    # Extract completed work from messages\n    new_summary = compress_with_anchor(messages, summary, llm)\n\n    # Mark todos completed if mentioned in compression\n    for todo in todos:\n        if todo[\"status\"] == \"in_progress\":\n            if todo[\"content\"].lower() in new_summary.current_state.lower():\n                todo[\"status\"] = \"completed\"\n\n    return new_summary, todos\n```\n\n---\n\n## Best Practices\n\n### DO\n\n-  Use anchored summarization with forced sections\n-  Preserve recent messages uncompressed (context continuity)\n-  Test compression with probes, not similarity metrics\n-  Merge incrementally (don't regenerate from scratch)\n-  Track compression count and quality scores\n\n### DON'T\n\n-  Compress system prompts (keep at START)\n-  Use opaque compression for critical workflows\n-  Compress below the point of task completion\n-  Trigger compression opportunistically (use fixed thresholds)\n-  Optimize for compression ratio over task success\n\n---\n\n## Compression Decision Tree\n\n```\n                    \n                     Context > 70%       \n                     capacity?           \n                    \n                               \n              \n               NO                               YES\n                                               \n                  \n     Continue                       Messages > 10?      \n     without                       \n     compression                              \n                  \n                                      NO                   YES\n                                                          \n                              \n                            Wait for more       COMPRESS        \n                            messages                            \n                               1. Keep last 5  \n                                                  2. Summarize    \n                                                     rest         \n                                                  3. Run probes   \n                                                  4. Merge with   \n                                                     existing     \n                                                 \n```\n\n---\n\n## Related Skills\n\n- `context-engineering` - Attention mechanics and positioning\n- `memory-systems` - Persistent storage patterns\n- `multi-agent-orchestration` - Context isolation across agents\n- `observability-monitoring` - Tracking compression metrics\n\n---\n\n**Version:** 1.0.0 (January 2026)\n**Key Principle:** Optimize for tokens-per-task, not tokens-per-request\n**Recommended Strategy:** Anchored Iterative Summarization with probe-based evaluation\n\n## Capability Details\n\n### anchored-summarization\n**Keywords:** compress, summarize history, context too long, anchored summary\n**Solves:**\n- Reduce context size while preserving critical information\n- Implement structured compression with required sections\n- Maintain session intent and decisions through compression\n\n### compression-triggers\n**Keywords:** token limit, running out of context, when to compress\n**Solves:**\n- Determine when to trigger compression (70% utilization)\n- Set compression targets (50% utilization)\n- Preserve last 5 messages uncompressed\n\n### probe-evaluation\n**Keywords:** evaluate compression, test compression, probe\n**Solves:**\n- Validate compression quality with functional probes\n- Test information preservation after compression\n- Achieve >90% probe pass rate"
              },
              {
                "name": "context-engineering",
                "description": "Use when designing agent system prompts, optimizing RAG retrieval, or when context is too expensive or slow. Reduces tokens while maintaining quality through strategic positioning and attention-aware design.",
                "path": ".claude/skills/context-engineering/SKILL.md",
                "frontmatter": {
                  "name": "context-engineering",
                  "description": "Use when designing agent system prompts, optimizing RAG retrieval, or when context is too expensive or slow. Reduces tokens while maintaining quality through strategic positioning and attention-aware design.",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "context",
                    "attention",
                    "optimization",
                    "llm",
                    "performance",
                    2026
                  ]
                },
                "content": "# Context Engineering\n\n**The discipline of curating the smallest high-signal token set that achieves desired outcomes.**\n\n## Overview\n\nContext engineering goes beyond prompt engineering. While prompts focus on *what* you ask, context engineering focuses on *everything* the model seessystem instructions, tool definitions, documents, message history, and tool outputs.\n\n**Key Insight:** Context windows are constrained not by raw token capacity but by attention mechanics. As context grows, models experience degradation.\n\n## When to Use\n\n- Designing agent system prompts\n- Optimizing RAG retrieval pipelines\n- Managing long-running conversations\n- Building multi-agent architectures\n- Reducing token costs while maintaining quality\n\n---\n\n## The \"Lost in the Middle\" Phenomenon\n\nModels pay unequal attention across the context window:\n\n```\nAttention\nStrength   \n                                                                 \n        START              MIDDLE (weakest attention)           END\n```\n\n**Practical Implications:**\n\n| Position | Attention | Best For |\n|----------|-----------|----------|\n| START | High | System identity, critical instructions, constraints |\n| MIDDLE | Low | Background context, optional details |\n| END | High | Current task, recent messages, immediate query |\n\n---\n\n## The Five Context Layers\n\n### 1. System Prompts (Identity Layer)\n\nEstablishes agent identity at the right \"altitude\":\n\n```\nTOO HIGH (vague):        \"You are a helpful assistant\"\nTOO LOW (brittle):       \"Always respond with exactly 3 bullet points...\"\nOPTIMAL (principled):    \"You are a senior engineer who values clarity,\n                          tests assumptions, and explains trade-offs\"\n```\n\n**Best Practices:**\n- Define role and expertise level\n- State core principles (not rigid rules)\n- Include what NOT to do (boundaries)\n- Position at START of context\n\n### 2. Tool Definitions (Capability Layer)\n\nTools steer behavior through descriptions:\n\n```python\n#  BAD: Ambiguous - when would you use this?\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    pass\n\n#  GOOD: Clear trigger conditions\n@tool\ndef search_documentation(query: str) -> str:\n    \"\"\"\n    Search internal documentation for technical answers.\n\n    USE WHEN:\n    - User asks about internal APIs or services\n    - Question requires company-specific knowledge\n    - Public information is insufficient\n\n    DO NOT USE WHEN:\n    - Question is general programming knowledge\n    - User explicitly wants external sources\n    \"\"\"\n    pass\n```\n\n**Rule:** If a human cannot definitively say which tool to use, an agent cannot either.\n\n### 3. Retrieved Documents (Knowledge Layer)\n\nJust-in-time loading beats pre-loading:\n\n```python\n#  BAD: Pre-load everything\ncontext = load_all_documentation()  # 50k tokens!\n\n#  GOOD: Progressive disclosure\ndef build_context(query: str) -> str:\n    # Stage 1: Lightweight retrieval (500 tokens)\n    summaries = search_summaries(query, top_k=5)\n\n    # Stage 2: Selective deep loading (only if needed)\n    if needs_detail(summaries):\n        full_docs = load_full_documents(summaries[:2])\n        return summaries + full_docs\n\n    return summaries\n```\n\n### 4. Message History (Memory Layer)\n\nTreat as scratchpad, not permanent storage:\n\n```python\n# Implement sliding window with compression\nMAX_MESSAGES = 20\nCOMPRESSION_TRIGGER = 0.7  # 70% of context budget\n\ndef manage_history(messages: list, budget: int) -> list:\n    current_tokens = count_tokens(messages)\n\n    if current_tokens > budget * COMPRESSION_TRIGGER:\n        # Compress older messages, keep recent\n        old = messages[:-5]\n        recent = messages[-5:]\n\n        summary = summarize(old)  # Anchored compression\n        return [summary] + recent\n\n    return messages\n```\n\n### 5. Tool Outputs (Observation Layer)\n\n**Critical Finding:** Tool outputs can reach 83.9% of total context usage!\n\n```python\n#  BAD: Return raw output\ndef search_web(query: str) -> str:\n    results = web_search(query)\n    return json.dumps(results)  # Could be 10k+ tokens!\n\n#  GOOD: Structured, bounded output\ndef search_web(query: str) -> str:\n    results = web_search(query)\n\n    # Extract only what's needed\n    extracted = [\n        {\n            \"title\": r[\"title\"],\n            \"snippet\": r[\"snippet\"][:200],  # Truncate\n            \"url\": r[\"url\"]\n        }\n        for r in results[:5]  # Limit count\n    ]\n\n    return json.dumps(extracted)  # ~500 tokens max\n```\n\n---\n\n## The 95% Finding\n\nResearch shows what actually drives agent performance:\n\n```\n\n  TOKEN USAGE          80%  \n  TOOL CALLS           10%                                 \n  MODEL CHOICE         5%                                     \n  OTHER                5%                                     \n\n```\n\n**Key Insight:** Optimize context efficiency BEFORE switching models.\n\n---\n\n## Context Budget Management\n\n### Token Budget Calculator\n\n```python\ndef calculate_budget(model: str, task_type: str) -> dict:\n    \"\"\"Calculate optimal token allocation.\"\"\"\n\n    MAX_CONTEXT = {\n        \"gpt-4o\": 128_000,\n        \"claude-3\": 200_000,\n        \"llama-3\": 128_000,\n    }\n\n    # Reserve 20% for response generation\n    available = MAX_CONTEXT[model] * 0.8\n\n    # Allocation by task type\n    ALLOCATIONS = {\n        \"chat\": {\n            \"system\": 0.05,      # 5%\n            \"tools\": 0.05,       # 5%\n            \"history\": 0.60,    # 60%\n            \"retrieval\": 0.20,  # 20%\n            \"current\": 0.10,    # 10%\n        },\n        \"agent\": {\n            \"system\": 0.10,     # 10%\n            \"tools\": 0.15,      # 15%\n            \"history\": 0.30,    # 30%\n            \"retrieval\": 0.25,  # 25%\n            \"observations\": 0.20, # 20%\n        },\n    }\n\n    alloc = ALLOCATIONS[task_type]\n    return {k: int(v * available) for k, v in alloc.items()}\n```\n\n### Compression Triggers\n\n```python\nCOMPRESSION_CONFIG = {\n    \"trigger_threshold\": 0.70,    # Start compressing at 70%\n    \"target_threshold\": 0.50,     # Compress down to 50%\n    \"preserve_recent\": 5,         # Always keep last 5 messages\n    \"preserve_system\": True,      # Never compress system prompt\n}\n```\n\n---\n\n## Attention-Aware Positioning\n\n### Template Structure\n\n```markdown\n[START - HIGH ATTENTION]\n## System Identity\nYou are a {role} specialized in {domain}.\n\n## Critical Constraints\n- NEVER {dangerous_action}\n- ALWAYS {required_behavior}\n\n[MIDDLE - LOWER ATTENTION]\n## Background Context\n{retrieved_documents}\n{older_conversation_history}\n\n[END - HIGH ATTENTION]\n## Current Task\n{recent_messages}\n{user_query}\n\n## Response Guidelines\n{output_format_instructions}\n```\n\n### Priority Positioning Rules\n\n1. **Identity & Constraints**  START (immutable)\n2. **Critical instructions**  START or END\n3. **Retrieved documents**  MIDDLE (expandable)\n4. **Conversation history**  MIDDLE (compressible)\n5. **Current query**  END (always visible)\n6. **Output format**  END (guides generation)\n\n---\n\n## Metrics: Tokens-Per-Task\n\n**Optimize for total task completion, not individual requests:**\n\n```python\n@dataclass\nclass TaskMetrics:\n    task_id: str\n    total_tokens: int = 0\n    request_count: int = 0\n    retrieval_tokens: int = 0\n    generation_tokens: int = 0\n\n    @property\n    def tokens_per_request(self) -> float:\n        return self.total_tokens / max(self.request_count, 1)\n\n    @property\n    def efficiency_ratio(self) -> float:\n        \"\"\"Lower is better - generation vs total context.\"\"\"\n        return self.generation_tokens / max(self.total_tokens, 1)\n```\n\n**Anti-pattern:** Aggressive compression that loses critical details forces expensive re-fetching, consuming MORE tokens overall.\n\n---\n\n## Common Pitfalls\n\n| Pitfall | Problem | Solution |\n|---------|---------|----------|\n| Token stuffing | \"More context = better\" | Quality over quantity |\n| Flat structure | No priority signaling | Use headers, positioning |\n| Static context | Same context for all queries | Dynamic, query-relevant retrieval |\n| Ignoring middle | Important info gets lost | Position critically |\n| No compression | Context grows unbounded | Sliding window + summarization |\n\n---\n\n## Integration with SkillForge\n\n### Agent System Prompts\n\nApply attention-aware positioning to agent definitions:\n\n```markdown\n# Agent: backend-system-architect\n\n[HIGH ATTENTION - START]\n## Identity\nSenior backend architect with 15+ years experience.\n\n## Constraints\n- NEVER suggest unvalidated security patterns\n- ALWAYS consider multi-tenant isolation\n\n[LOWER ATTENTION - MIDDLE]\n## Domain Knowledge\n{dynamically_loaded_patterns}\n\n[HIGH ATTENTION - END]\n## Current Task\n{user_request}\n```\n\n### Skill Loading\n\nProgressive skill disclosure:\n\n```python\n# Stage 1: Load skill metadata only (~100 tokens)\nskill_index = load_skill_summaries()\n\n# Stage 2: Load relevant skill on demand (~500 tokens)\nif task_matches(\"database\"):\n    full_skill = load_skill(\"pgvector-search\")\n```\n\n---\n\n## Related Skills\n\n- `context-compression` - Compression strategies and anchored summarization\n- `multi-agent-orchestration` - Context isolation across agents\n- `rag-retrieval` - Optimizing retrieved document context\n- `prompt-caching` - Reducing redundant context transmission\n\n---\n\n**Version:** 1.0.0 (January 2026)\n**Based on:** Context Engineering research, BrowseComp evaluation findings\n**Key Metric:** 80% of agent performance variance explained by token usage\n\n## Capability Details\n\n### attention-mechanics\n**Keywords:** context window, attention, lost in the middle, token budget\n**Solves:**\n- Understand lost-in-the-middle effect (high attention at START/END)\n- Position critical info strategically\n- Optimize tokens-per-task not tokens-per-request\n\n### context-layers\n**Keywords:** context anatomy, context structure, five layers\n**Solves:**\n- Understand 5 context layers (system, tools, docs, history, outputs)\n- Implement just-in-time document loading\n- Manage tool output truncation\n\n### budget-allocation\n**Keywords:** token budget, context budget, allocation\n**Solves:**\n- Allocate tokens across context layers\n- Implement compression triggers at 70% utilization\n- Target 50% utilization after compression"
              },
              {
                "name": "contextual-retrieval",
                "description": "Anthropic's Contextual Retrieval technique for improved RAG. Use when chunks lose context during retrieval, implementing hybrid BM25+vector search, or reducing retrieval failures.",
                "path": ".claude/skills/contextual-retrieval/SKILL.md",
                "frontmatter": {
                  "name": "contextual-retrieval",
                  "description": "Anthropic's Contextual Retrieval technique for improved RAG. Use when chunks lose context during retrieval, implementing hybrid BM25+vector search, or reducing retrieval failures.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# Contextual Retrieval\nPrepend situational context to chunks before embedding to preserve document-level meaning.\n\n## The Problem\n\nTraditional chunking loses context:\n```\nOriginal document: \"ACME Q3 2024 Earnings Report...\"\nChunk: \"Revenue increased 15% compared to the previous quarter.\"\n\nQuery: \"What was ACME's Q3 2024 revenue growth?\"\nResult: Chunk doesn't mention \"ACME\" or \"Q3 2024\" - retrieval fails\n```\n\n## The Solution\n\n**Contextual Retrieval** prepends a brief context to each chunk:\n```\nContextualized chunk:\n\"This chunk is from ACME Corp's Q3 2024 earnings report, specifically\nthe revenue section. Revenue increased 15% compared to the previous quarter.\"\n```\n\n## Implementation\n\n### Context Generation\n```python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nCONTEXT_PROMPT = \"\"\"\n<document>\n{document}\n</document>\n\nHere is the chunk we want to situate within the document:\n<chunk>\n{chunk}\n</chunk>\n\nPlease give a short, succinct context (1-2 sentences) to situate this chunk\nwithin the overall document. Focus on information that would help retrieval.\nAnswer only with the context, nothing else.\n\"\"\"\n\ndef generate_context(document: str, chunk: str) -> str:\n    \"\"\"Generate context for a single chunk.\"\"\"\n    response = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=150,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": CONTEXT_PROMPT.format(document=document, chunk=chunk)\n        }]\n    )\n    return response.content[0].text\n\ndef contextualize_chunk(document: str, chunk: str) -> str:\n    \"\"\"Prepend context to chunk.\"\"\"\n    context = generate_context(document, chunk)\n    return f\"{context}\\n\\n{chunk}\"\n```\n\n### Batch Processing with Caching\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic()\n\ndef contextualize_chunks_cached(document: str, chunks: list[str]) -> list[str]:\n    \"\"\"\n    Use prompt caching to efficiently process many chunks from same document.\n    Document is cached, only chunk changes per request.\n    \"\"\"\n    results = []\n\n    for i, chunk in enumerate(chunks):\n        response = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=150,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"<document>\\n{document}\\n</document>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"}  # Cache document\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"\"\"\nHere is chunk {i+1} to situate:\n<chunk>\n{chunk}\n</chunk>\n\nGive a short context (1-2 sentences) to situate this chunk.\n\"\"\"\n                    }\n                ]\n            }]\n        )\n        context = response.content[0].text\n        results.append(f\"{context}\\n\\n{chunk}\")\n\n    return results\n```\n\n### Hybrid Search (BM25 + Vector)\n\nContextual Retrieval works best with hybrid search:\n\n```python\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\n\nclass HybridRetriever:\n    def __init__(self, chunks: list[str], embeddings: np.ndarray):\n        self.chunks = chunks\n        self.embeddings = embeddings\n\n        # BM25 index on raw text\n        tokenized = [c.lower().split() for c in chunks]\n        self.bm25 = BM25Okapi(tokenized)\n\n    def search(\n        self,\n        query: str,\n        query_embedding: np.ndarray,\n        top_k: int = 20,\n        bm25_weight: float = 0.4,\n        vector_weight: float = 0.6\n    ) -> list[tuple[int, float]]:\n        \"\"\"Hybrid search combining BM25 and vector similarity.\"\"\"\n        # BM25 scores\n        bm25_scores = self.bm25.get_scores(query.lower().split())\n        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)\n\n        # Vector similarity\n        vector_scores = np.dot(self.embeddings, query_embedding)\n        vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min() + 1e-6)\n\n        # Combine\n        combined = bm25_weight * bm25_scores + vector_weight * vector_scores\n\n        # Top-k\n        top_indices = np.argsort(combined)[::-1][:top_k]\n        return [(i, combined[i]) for i in top_indices]\n```\n\n## Complete Pipeline\n\n```python\nfrom dataclasses import dataclass\nimport hashlib\nimport json\n\n@dataclass\nclass ContextualChunk:\n    original: str\n    contextualized: str\n    embedding: list[float]\n    doc_id: str\n    chunk_index: int\n\nclass ContextualRetriever:\n    def __init__(self, embed_model, llm_client):\n        self.embed_model = embed_model\n        self.llm = llm_client\n        self.chunks: list[ContextualChunk] = []\n        self.bm25 = None\n\n    def add_document(self, doc_id: str, text: str, chunk_size: int = 512):\n        \"\"\"Process and index a document.\"\"\"\n        # 1. Chunk the document\n        raw_chunks = self._chunk_text(text, chunk_size)\n\n        # 2. Generate context for each chunk (with caching)\n        contextualized = self._contextualize_batch(text, raw_chunks)\n\n        # 3. Embed contextualized chunks\n        embeddings = self.embed_model.embed(contextualized)\n\n        # 4. Store\n        for i, (raw, ctx, emb) in enumerate(zip(raw_chunks, contextualized, embeddings)):\n            self.chunks.append(ContextualChunk(\n                original=raw,\n                contextualized=ctx,\n                embedding=emb,\n                doc_id=doc_id,\n                chunk_index=i\n            ))\n\n        # 5. Rebuild BM25 index\n        self._rebuild_bm25()\n\n    def search(self, query: str, top_k: int = 10) -> list[ContextualChunk]:\n        \"\"\"Hybrid search over contextualized chunks.\"\"\"\n        query_emb = self.embed_model.embed([query])[0]\n\n        # BM25 on contextualized text\n        bm25_scores = self.bm25.get_scores(query.lower().split())\n\n        # Vector similarity\n        embeddings = np.array([c.embedding for c in self.chunks])\n        vector_scores = np.dot(embeddings, query_emb)\n\n        # Normalize and combine\n        bm25_norm = self._normalize(bm25_scores)\n        vector_norm = self._normalize(vector_scores)\n        combined = 0.4 * bm25_norm + 0.6 * vector_norm\n\n        # Return top-k\n        top_indices = np.argsort(combined)[::-1][:top_k]\n        return [self.chunks[i] for i in top_indices]\n\n    def _contextualize_batch(self, document: str, chunks: list[str]) -> list[str]:\n        \"\"\"Generate context for all chunks (use prompt caching).\"\"\"\n        results = []\n        for chunk in chunks:\n            context = self._generate_context(document, chunk)\n            results.append(f\"{context}\\n\\n{chunk}\")\n        return results\n\n    def _generate_context(self, document: str, chunk: str) -> str:\n        # Implementation from above\n        pass\n\n    def _chunk_text(self, text: str, chunk_size: int) -> list[str]:\n        \"\"\"Simple sentence-aware chunking.\"\"\"\n        sentences = text.split('. ')\n        chunks = []\n        current = []\n        current_len = 0\n\n        for sent in sentences:\n            if current_len + len(sent) > chunk_size and current:\n                chunks.append('. '.join(current) + '.')\n                current = [sent]\n                current_len = len(sent)\n            else:\n                current.append(sent)\n                current_len += len(sent)\n\n        if current:\n            chunks.append('. '.join(current))\n        return chunks\n\n    def _rebuild_bm25(self):\n        tokenized = [c.contextualized.lower().split() for c in self.chunks]\n        self.bm25 = BM25Okapi(tokenized)\n\n    def _normalize(self, scores: np.ndarray) -> np.ndarray:\n        return (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)\n```\n\n## Optimization Tips\n\n### 1. Cost Reduction with Caching\n```python\n# Prompt caching reduces cost by ~90% when processing\n# many chunks from the same document\n# Document cached on first request, reused for subsequent chunks\n```\n\n### 2. Parallel Processing\n```python\nimport asyncio\n\nasync def contextualize_parallel(document: str, chunks: list[str]) -> list[str]:\n    \"\"\"Process chunks in parallel with rate limiting.\"\"\"\n    semaphore = asyncio.Semaphore(10)  # Max 10 concurrent\n\n    async def process_chunk(chunk: str) -> str:\n        async with semaphore:\n            context = await async_generate_context(document, chunk)\n            return f\"{context}\\n\\n{chunk}\"\n\n    return await asyncio.gather(*[process_chunk(c) for c in chunks])\n```\n\n### 3. Context Quality\n```python\n# Good context examples:\n\"This chunk is from the API authentication section of the FastAPI documentation.\"\n\"This describes the company's Q3 2024 financial performance, specifically operating expenses.\"\n\"This section covers error handling in the user registration flow.\"\n\n# Bad context (too generic):\n\"This is a chunk from the document.\"\n\"Information about the topic.\"\n```\n\n## Results (from Anthropic's research)\n\n| Method | Retrieval Failure Rate |\n|--------|----------------------|\n| Traditional embeddings | 5.7% |\n| + Contextual embeddings | 3.5% |\n| + Contextual + BM25 hybrid | 1.9% |\n| + Contextual + BM25 + reranking | 1.3% |\n\n**67% reduction in retrieval failures** with full contextual retrieval pipeline.\n\n## When to Use\n\n**Use Contextual Retrieval when**:\n- Documents have important metadata (dates, names, versions)\n- Chunks frequently lose meaning without document context\n- Retrieval quality is critical (customer-facing, compliance)\n- You can afford the additional LLM cost during indexing\n\n**Skip if**:\n- Chunks are self-contained (Q&A pairs, definitions)\n- Low latency indexing required (high-volume streaming)\n- Cost-sensitive with many small documents\n\n## Resources\n- Anthropic Blog: https://www.anthropic.com/news/contextual-retrieval\n- Prompt Caching: https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching"
              },
              {
                "name": "create-pr",
                "description": "Create GitHub pull requests with validation and auto-generated descriptions",
                "path": ".claude/skills/create-pr/SKILL.md",
                "frontmatter": {
                  "name": "create-pr",
                  "description": "Create GitHub pull requests with validation and auto-generated descriptions",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "git",
                    "github",
                    "pull-request",
                    "pr",
                    "code-review"
                  ]
                },
                "content": "# Create Pull Request\n\nComprehensive PR creation with validation. All output goes directly to GitHub PR.\n\n## When to Use\n\n- Creating pull requests for feature branches\n- Submitting changes for code review\n- Opening PRs with auto-generated descriptions\n\n## Quick Start\n\n```bash\n/create-pr\n```\n\n## Workflow\n\n### Phase 1: Pre-Flight Checks\n\n```bash\n# Verify branch\nBRANCH=$(git branch --show-current)\nif [[ \"$BRANCH\" == \"dev\" || \"$BRANCH\" == \"main\" ]]; then\n  echo \"Cannot create PR from dev/main. Create a feature branch first.\"\n  exit 1\nfi\n\n# Check for uncommitted changes\nif [[ -n $(git status --porcelain) ]]; then\n  echo \"Uncommitted changes detected. Commit or stash first.\"\n  exit 1\nfi\n\n# Push branch if needed\ngit fetch origin\nif ! git rev-parse --verify origin/$BRANCH &>/dev/null; then\n  git push -u origin $BRANCH\nfi\n```\n\n### Phase 2: Run Local Validation\n\n```bash\n# Backend\ncd backend\npoetry run ruff format --check app/\npoetry run ruff check app/\npoetry run pytest tests/unit/ -v --tb=short -x\n\n# Frontend\ncd ../frontend\nnpm run lint && npm run typecheck\n```\n\n### Phase 3: Gather Context\n\n```bash\nBRANCH=$(git branch --show-current)\nISSUE=$(echo $BRANCH | grep -oE '[0-9]+' | head -1)\n\ngit log --oneline dev..HEAD\ngit diff dev...HEAD --stat\n```\n\n### Phase 4: Create PR\n\n```bash\nTYPE=\"feat\"  # Determine: feat/fix/refactor/docs/test/chore\n\ngh pr create --base dev \\\n  --title \"$TYPE(#$ISSUE): Brief description\" \\\n  --body \"## Summary\n[1-2 sentence description]\n\n## Changes\n- [Change 1]\n- [Change 2]\n\n## Test Plan\n- [x] Unit tests pass\n- [x] Lint/type checks pass\n\nCloses #$ISSUE\n\n---\nGenerated with [Claude Code](https://claude.com/claude-code)\"\n```\n\n### Phase 5: Verify\n\n```bash\nPR_URL=$(gh pr view --json url -q .url)\necho \"PR created: $PR_URL\"\ngh pr view --web\n```\n\n## Rules\n\n1. **NO junk files** - Don't create files in repo root\n2. **Run validation locally** - Don't spawn agents for lint/test\n3. **All content goes to GitHub** - PR body via `gh pr create --body`\n4. **Keep it simple** - One command to create PR\n\n## When to Use Agents\n\nOnly use Task agents for:\n- Complex code analysis requiring multiple files\n- Security review of sensitive changes\n- Architecture review for large refactors\n\n## References\n\n- [PR Template](references/pr-template.md)"
              },
              {
                "name": "database-schema-designer",
                "description": "Use this skill when designing database schemas for relational (SQL) or document (NoSQL) databases. Provides normalization guidelines, indexing strategies, migration patterns, and performance optimization techniques. Ensures scalable, maintainable, and performant data models.",
                "path": ".claude/skills/database-schema-designer/SKILL.md",
                "frontmatter": {
                  "name": "database-schema-designer",
                  "description": "Use this skill when designing database schemas for relational (SQL) or document (NoSQL) databases. Provides normalization guidelines, indexing strategies, migration patterns, and performance optimization techniques. Ensures scalable, maintainable, and performant data models.",
                  "version": "2.0.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "database",
                    "schema-design",
                    "sql",
                    "nosql",
                    "performance",
                    "migrations"
                  ],
                  "context": "fork",
                  "agent": "database-engineer",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/migration-validator.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "echo \"::group::Database Schema Design Complete\"\necho \"Review generated schema for:\"\necho \"  - Normalization (1NF, 2NF, 3NF)\"\necho \"  - Index coverage for query patterns\"\necho \"  - Foreign key constraints\"\necho \"  - Migration reversibility\"\necho \"::endgroup::\"\n"
                      }
                    ]
                  }
                },
                "content": "# Database Schema Designer\n\n## Overview\n\nThis skill provides comprehensive guidance for designing robust, scalable database schemas for both SQL and NoSQL databases. Whether building from scratch or evolving existing schemas, this framework ensures data integrity, performance, and maintainability.\n\n**When to use this skill:**\n- Designing new database schemas\n- Refactoring or migrating existing schemas\n- Optimizing database performance\n- Choosing between SQL and NoSQL approaches\n- Creating database migrations\n- Establishing indexing strategies\n- Modeling complex relationships\n- Planning data archival and partitioning\n\n## Database Design Philosophy\n\n### Core Principles\n\n**1. Model the Domain, Not the UI**\n- Schema reflects business entities and relationships\n- Don't let UI requirements drive data structure\n- Separate presentation concerns from data model\n\n**2. Optimize for Reads or Writes (Not Both)**\n- OLTP (transactional): Normalized, optimized for writes\n- OLAP (analytical): Denormalized, optimized for reads\n- Choose based on access patterns\n\n**3. Plan for Scale From Day One**\n- Indexing strategy\n- Partitioning approach\n- Caching layer\n- Read replicas\n\n**4. Data Integrity Over Performance**\n- Use constraints, foreign keys, validation\n- Performance issues can be optimized later\n- Data corruption is costly to fix\n\n---\n\n## SQL Database Design\n\n### Normalization\n\nDatabase normalization reduces redundancy and ensures data integrity.\n\n#### 1st Normal Form (1NF)\n**Rule**: Each column contains atomic (indivisible) values, no repeating groups.\n\n```sql\n--  Violates 1NF (multiple values in one column)\nCREATE TABLE orders (\n  id INT PRIMARY KEY,\n  customer_id INT,\n  product_ids VARCHAR(255)  -- '101,102,103' (bad!)\n);\n\n--  Follows 1NF\nCREATE TABLE orders (\n  id INT PRIMARY KEY,\n  customer_id INT\n);\n\nCREATE TABLE order_items (\n  id INT PRIMARY KEY,\n  order_id INT,\n  product_id INT,\n  FOREIGN KEY (order_id) REFERENCES orders(id)\n);\n```\n\n#### 2nd Normal Form (2NF)\n**Rule**: Must be in 1NF + all non-key columns depend on the entire primary key.\n\n#### 3rd Normal Form (3NF)\n**Rule**: Must be in 2NF + no transitive dependencies (non-key columns depend only on primary key).\n\n---\n\n### Indexing Strategies\n\nIndexes speed up reads but slow down writes. Use strategically.\n\n#### When to Create Indexes\n\n```sql\n--  Index foreign keys\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\n\n--  Index frequently queried columns\nCREATE INDEX idx_users_email ON users(email);\n\n--  Index columns used in WHERE, ORDER BY, GROUP BY\nCREATE INDEX idx_orders_created_at ON orders(created_at);\n\n--  Composite index for multi-column queries\nCREATE INDEX idx_orders_customer_status ON orders(customer_id, status);\n```\n\n#### Composite Indexes (Column Order Matters)\n\n```sql\n--  Good: Index supports both queries\nCREATE INDEX idx_orders_customer_status ON orders(customer_id, status);\n\n-- Query 1: Uses index efficiently\nSELECT * FROM orders WHERE customer_id = 123 AND status = 'pending';\n\n-- Query 2: Uses index (customer_id only)\nSELECT * FROM orders WHERE customer_id = 123;\n\n--  Query 3: Doesn't use index (status is second column)\nSELECT * FROM orders WHERE status = 'pending';\n```\n\n**Rule of Thumb**: Put most selective column first, or most frequently queried alone.\n\n---\n\n### Constraints\n\nUse constraints to enforce data integrity at the database level.\n\n```sql\nCREATE TABLE products (\n  id INT PRIMARY KEY,\n  price DECIMAL(10, 2) CHECK (price >= 0),\n  stock INT CHECK (stock >= 0),\n  discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100)\n);\n```\n\n---\n\n## Database Migrations\n\n### Migration Best Practices\n\n**1. Always Reversible**\n```sql\n-- Up migration\nALTER TABLE users ADD COLUMN phone VARCHAR(20);\n\n-- Down migration\nALTER TABLE users DROP COLUMN phone;\n```\n\n**2. Backward Compatible**\n```sql\n--  Good: Add nullable column\nALTER TABLE users ADD COLUMN middle_name VARCHAR(50);\n\n--  Bad: Add required column (breaks existing code)\nALTER TABLE users ADD COLUMN middle_name VARCHAR(50) NOT NULL;\n```\n\n**3. Data Migrations Separate from Schema Changes**\n```sql\n-- Migration 1: Schema change\nALTER TABLE orders ADD COLUMN status VARCHAR(20) DEFAULT 'pending';\n\n-- Migration 2: Data migration\nUPDATE orders SET status = 'completed' WHERE completed_at IS NOT NULL;\n```\n\n---\n\n## Quick Start Checklist\n\nWhen designing a new schema:\n\n- [ ] Identify entities and relationships\n- [ ] Choose SQL or NoSQL based on requirements\n- [ ] Normalize to 3NF (SQL) or decide embed/reference (NoSQL)\n- [ ] Define primary keys (INT auto-increment or UUID)\n- [ ] Add foreign key constraints\n- [ ] Choose appropriate data types\n- [ ] Add unique constraints where needed\n- [ ] Plan indexing strategy (foreign keys, WHERE columns)\n- [ ] Add NOT NULL constraints for required fields\n- [ ] Create CHECK constraints for validation\n- [ ] Plan for soft deletes (deleted_at column) if needed\n- [ ] Add timestamps (created_at, updated_at)\n- [ ] Design migration scripts (up and down)\n- [ ] Test migrations on staging\n\n---\n\n**Skill Version**: 2.0.0\n**Last Updated**: 2026-01-08\n**Maintained by**: AI Agent Hub Team\n\n## Capability Details\n\n### schema-design\n**Keywords:** schema, table, entity, relationship, erd\n**Solves:**\n- Design database schema\n- Model relationships\n- ERD creation\n\n### normalization\n**Keywords:** normalize, 1nf, 2nf, 3nf, denormalize\n**Solves:**\n- Normalization levels\n- When to denormalize\n- Reduce redundancy\n\n### indexing\n**Keywords:** index, b-tree, composite, query performance\n**Solves:**\n- Which columns to index\n- Optimize slow queries\n- Index types\n\n### migrations\n**Keywords:** migration, alter table, zero downtime, backward compatible\n**Solves:**\n- Write safe migrations\n- Zero-downtime changes\n- Reversible migrations"
              },
              {
                "name": "defense-in-depth",
                "description": "Use when building secure AI pipelines or hardening LLM integrations. Implements 8 validation layers from edge to storage with no single point of failure.",
                "path": ".claude/skills/defense-in-depth/SKILL.md",
                "frontmatter": {
                  "name": "defense-in-depth",
                  "description": "Use when building secure AI pipelines or hardening LLM integrations. Implements 8 validation layers from edge to storage with no single point of failure.",
                  "context": "fork",
                  "agent": "security-layer-auditor",
                  "version": "1.0.0",
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/redact-secrets.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# Defense in Depth for AI Systems\n\n## Overview\n\nDefense in depth applies multiple security layers so that if one fails, others still protect the system. For AI applications, this means validating at every boundary: edge, gateway, input, authorization, data, LLM, output, and observability.\n\n**Core Principle:** No single security control should be the only thing protecting sensitive operations.\n\n## The 8-Layer Security Architecture\n\n```\n\n  Layer 0: EDGE             WAF, Rate Limiting, DDoS, Bot Detection    \n\n  Layer 1: GATEWAY          JWT Verify, Extract Claims, Build Context  \n\n  Layer 2: INPUT            Schema Validation, PII Detection, Injection\n\n  Layer 3: AUTHORIZATION    RBAC/ABAC, Tenant Check, Resource Access   \n\n  Layer 4: DATA ACCESS      Parameterized Queries, Tenant Filter       \n\n  Layer 5: LLM              Prompt Building (no IDs), Context Separation\n\n  Layer 6: OUTPUT           Schema Validation, Guardrails, Hallucination\n\n  Layer 7: STORAGE          Attribution, Audit Trail, Encryption       \n\n  Layer 8: OBSERVABILITY    Logging (sanitized), Tracing, Metrics      \n\n```\n\n## Layer Details\n\n### Layer 0: Edge Protection\n\n**Purpose:** Stop attacks before they reach your application.\n\n- WAF rules for OWASP Top 10\n- Rate limiting per user/IP\n- DDoS protection\n- Bot detection\n- Geo-blocking if required\n\n### Layer 1: Gateway / Authentication\n\n**Purpose:** Verify identity and build request context.\n\n```python\n@dataclass(frozen=True)\nclass RequestContext:\n    \"\"\"Immutable context that flows through the system\"\"\"\n    # Identity\n    user_id: UUID\n    tenant_id: UUID\n    session_id: str\n    permissions: frozenset[str]\n\n    # Tracing\n    request_id: str\n    trace_id: str\n\n    # Metadata\n    timestamp: datetime\n    client_ip: str\n```\n\n### Layer 2: Input Validation\n\n**Purpose:** Reject bad input early.\n\n- **Schema validation:** Pydantic/Zod for structure\n- **Content validation:** PII detection, malware scan\n- **Injection defense:** SQL, XSS, prompt injection patterns\n\n### Layer 3: Authorization\n\n**Purpose:** Verify permission for the specific action and resource.\n\n```python\nasync def authorize(ctx: RequestContext, action: str, resource: Resource) -> bool:\n    # 1. Check permission exists\n    if action not in ctx.permissions:\n        raise Forbidden(\"Missing permission\")\n\n    # 2. Check tenant ownership\n    if resource.tenant_id != ctx.tenant_id:\n        raise Forbidden(\"Cross-tenant access denied\")\n\n    # 3. Check resource-level access\n    if not await check_resource_access(ctx.user_id, resource):\n        raise Forbidden(\"No access to resource\")\n\n    return True\n```\n\n### Layer 4: Data Access\n\n**Purpose:** Ensure all queries are tenant-scoped.\n\n```python\nclass TenantScopedRepository:\n    def __init__(self, ctx: RequestContext):\n        self.ctx = ctx\n        self._base_filter = {\"tenant_id\": ctx.tenant_id}\n\n    async def find(self, query: dict) -> list[Model]:\n        # ALWAYS merge tenant filter\n        safe_query = {**self._base_filter, **query}\n        return await self.db.find(safe_query)\n```\n\n### Layer 5: LLM Orchestration\n\n**Purpose:** Build prompts with content only, no identifiers.\n\n- Identifiers flow AROUND the LLM, not THROUGH it\n- Prompts contain only content text\n- No user_id, tenant_id, document_id in prompt text\n- See `llm-safety-patterns` skill for details\n\n### Layer 6: Output Validation\n\n**Purpose:** Validate LLM output before use.\n\n- Schema validation (JSON structure)\n- Content guardrails (toxicity, PII generation)\n- Hallucination detection (grounding check)\n- Code injection prevention\n\n### Layer 7: Attribution & Storage\n\n**Purpose:** Reattach context and store with proper attribution.\n\n- Attribution is deterministic, not LLM-generated\n- Context from Layer 1 is attached to results\n- Source references from Layer 4 are attached\n- Audit trail recorded\n\n### Layer 8: Observability\n\n**Purpose:** Monitor without leaking sensitive data.\n\n- Structured logging with sanitization\n- Distributed tracing (Langfuse)\n- Metrics (latency, errors, costs)\n- Alerts for anomalies\n\n## Implementation Checklist\n\nBefore deploying any AI feature, verify:\n\n- [ ] Layer 0: Rate limiting configured\n- [ ] Layer 1: JWT validation active, RequestContext created\n- [ ] Layer 2: Pydantic models validate all input\n- [ ] Layer 3: Authorization check on every endpoint\n- [ ] Layer 4: All queries include tenant_id filter\n- [ ] Layer 5: No IDs in LLM prompts (run audit)\n- [ ] Layer 6: Output schema validation active\n- [ ] Layer 7: Attribution uses context, not LLM output\n- [ ] Layer 8: Logging sanitized, tracing enabled\n\n## Industry Sources\n\n| Pattern | Source | Application |\n|---------|--------|-------------|\n| Defense in Depth | NIST | Multiple validation layers |\n| Zero Trust | Google BeyondCorp | Every request verified |\n| Least Privilege | AWS IAM | Minimal permissions |\n| Complete Mediation | Saltzer & Schroeder | Every access checked |\n\n## Integration with SkillForge\n\nThis skill integrates with:\n- `llm-safety-patterns` - Layer 5 details\n- `security-checklist` - OWASP validations\n- `observability-monitoring` - Layer 8 details\n\n---\n\n**Version:** 1.0.0 (December 2025)\n\n## Capability Details\n\n### 8-layer-architecture\n**Keywords:** defense in depth, security layers, validation layers, multi-layer\n**Solves:**\n- How do I secure my AI application end-to-end?\n- What validation layers do I need?\n- How do I implement defense in depth?\n\n### request-context\n**Keywords:** request context, immutable context, context object, user context\n**Solves:**\n- How do I pass user identity through the system?\n- How do I create an immutable request context?\n- What should be in the request context?\n\n### tenant-isolation\n**Keywords:** multi-tenant, tenant isolation, tenant filter, cross-tenant\n**Solves:**\n- How do I ensure tenant isolation?\n- How do I prevent cross-tenant data access?\n- How do I filter queries by tenant?\n\n### audit-logging\n**Keywords:** audit log, audit trail, logging, compliance\n**Solves:**\n- What should I log for compliance?\n- How do I create audit trails?\n- How do I log without leaking PII?"
              },
              {
                "name": "design-system-starter",
                "description": "Use this skill when creating or evolving design systems for applications. Provides design token structures, component architecture patterns, documentation templates, and accessibility guidelines. Ensures consistent, scalable, and accessible UI design across products.",
                "path": ".claude/skills/design-system-starter/SKILL.md",
                "frontmatter": {
                  "name": "design-system-starter",
                  "description": "Use this skill when creating or evolving design systems for applications. Provides design token structures, component architecture patterns, documentation templates, and accessibility guidelines. Ensures consistent, scalable, and accessible UI design across products.",
                  "context": "fork",
                  "agent": "rapid-ui-designer",
                  "version": "1.0.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "design-system",
                    "ui",
                    "components",
                    "design-tokens",
                    "accessibility",
                    "frontend"
                  ]
                },
                "content": "# Design System Starter\n\n## Overview\n\nThis skill provides comprehensive guidance for building robust, scalable design systems that ensure visual consistency, improve development velocity, and create exceptional user experiences. Whether starting from scratch or evolving an existing system, this framework helps teams design with intention and scale.\n\n**When to use this skill:**\n- Creating a new design system from scratch\n- Evolving or refactoring existing design systems\n- Establishing design token standards\n- Defining component architecture\n- Creating design documentation\n- Ensuring accessibility compliance (WCAG 2.1)\n- Implementing theming and dark mode\n\n**Bundled Resources:**\n- `references/component-examples.md` - Complete component implementations\n- `templates/design-tokens-template.json` - W3C design token format\n- `templates/component-template.tsx` - React component template\n- `checklists/design-system-checklist.md` - Design system audit checklist\n\n## Design System Philosophy\n\n### What is a Design System?\n\nA design system is more than a component libraryit's a collection of:\n\n1. **Design Tokens**: Foundational design decisions (colors, spacing, typography)\n2. **Components**: Reusable UI building blocks\n3. **Patterns**: Common UX solutions and compositions\n4. **Guidelines**: Rules, principles, and best practices\n5. **Documentation**: How to use everything effectively\n\n### Core Principles\n\n**1. Consistency Over Creativity**\n- Predictable patterns reduce cognitive load\n- Users learn once, apply everywhere\n- Designers and developers speak the same language\n\n**2. Accessible by Default**\n- WCAG 2.1 Level AA compliance minimum\n- Keyboard navigation built-in\n- Screen reader support from the start\n\n**3. Scalable and Maintainable**\n- Design tokens enable global changes\n- Component composition reduces duplication\n- Versioning and deprecation strategies\n\n**4. Developer-Friendly**\n- Clear API contracts\n- Comprehensive documentation\n- Easy to integrate and customize\n\n---\n\n## Design Tokens\n\nDesign tokens are the atomic design decisions that define your system's visual language.\n\n### Token Categories\n\n#### 1. Color Tokens\n\n**Primitive Colors** (Raw values):\n```json\n{\n  \"color\": {\n    \"primitive\": {\n      \"blue\": {\n        \"50\": \"#eff6ff\",\n        \"100\": \"#dbeafe\",\n        \"200\": \"#bfdbfe\",\n        \"300\": \"#93c5fd\",\n        \"400\": \"#60a5fa\",\n        \"500\": \"#3b82f6\",\n        \"600\": \"#2563eb\",\n        \"700\": \"#1d4ed8\",\n        \"800\": \"#1e40af\",\n        \"900\": \"#1e3a8a\",\n        \"950\": \"#172554\"\n      }\n    }\n  }\n}\n```\n\n**Semantic Colors** (Contextual meaning):\n```json\n{\n  \"color\": {\n    \"semantic\": {\n      \"brand\": {\n        \"primary\": \"{color.primitive.blue.600}\",\n        \"primary-hover\": \"{color.primitive.blue.700}\",\n        \"primary-active\": \"{color.primitive.blue.800}\"\n      },\n      \"text\": {\n        \"primary\": \"{color.primitive.gray.900}\",\n        \"secondary\": \"{color.primitive.gray.600}\",\n        \"tertiary\": \"{color.primitive.gray.500}\",\n        \"disabled\": \"{color.primitive.gray.400}\",\n        \"inverse\": \"{color.primitive.white}\"\n      },\n      \"background\": {\n        \"primary\": \"{color.primitive.white}\",\n        \"secondary\": \"{color.primitive.gray.50}\",\n        \"tertiary\": \"{color.primitive.gray.100}\"\n      },\n      \"feedback\": {\n        \"success\": \"{color.primitive.green.600}\",\n        \"warning\": \"{color.primitive.yellow.600}\",\n        \"error\": \"{color.primitive.red.600}\",\n        \"info\": \"{color.primitive.blue.600}\"\n      }\n    }\n  }\n}\n```\n\n**Accessibility**: Ensure color contrast ratios meet WCAG 2.1 Level AA:\n- Normal text: 4.5:1 minimum\n- Large text (18pt+ or 14pt+ bold): 3:1 minimum\n- UI components and graphics: 3:1 minimum\n\n#### 2. Typography Tokens\n\n```json\n{\n  \"typography\": {\n    \"fontFamily\": {\n      \"sans\": \"'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif\",\n      \"serif\": \"'Georgia', 'Times New Roman', serif\",\n      \"mono\": \"'Fira Code', 'Courier New', monospace\"\n    },\n    \"fontSize\": {\n      \"xs\": \"0.75rem\",     // 12px\n      \"sm\": \"0.875rem\",    // 14px\n      \"base\": \"1rem\",      // 16px\n      \"lg\": \"1.125rem\",    // 18px\n      \"xl\": \"1.25rem\",     // 20px\n      \"2xl\": \"1.5rem\",     // 24px\n      \"3xl\": \"1.875rem\",   // 30px\n      \"4xl\": \"2.25rem\",    // 36px\n      \"5xl\": \"3rem\"        // 48px\n    },\n    \"fontWeight\": {\n      \"normal\": 400,\n      \"medium\": 500,\n      \"semibold\": 600,\n      \"bold\": 700\n    },\n    \"lineHeight\": {\n      \"tight\": 1.25,\n      \"normal\": 1.5,\n      \"relaxed\": 1.75,\n      \"loose\": 2\n    },\n    \"letterSpacing\": {\n      \"tight\": \"-0.025em\",\n      \"normal\": \"0\",\n      \"wide\": \"0.025em\"\n    }\n  }\n}\n```\n\n#### 3. Spacing Tokens\n\n**Scale**: Use a consistent spacing scale (commonly 4px or 8px base)\n\n```json\n{\n  \"spacing\": {\n    \"0\": \"0\",\n    \"1\": \"0.25rem\",   // 4px\n    \"2\": \"0.5rem\",    // 8px\n    \"3\": \"0.75rem\",   // 12px\n    \"4\": \"1rem\",      // 16px\n    \"5\": \"1.25rem\",   // 20px\n    \"6\": \"1.5rem\",    // 24px\n    \"8\": \"2rem\",      // 32px\n    \"10\": \"2.5rem\",   // 40px\n    \"12\": \"3rem\",     // 48px\n    \"16\": \"4rem\",     // 64px\n    \"20\": \"5rem\",     // 80px\n    \"24\": \"6rem\"      // 96px\n  }\n}\n```\n\n**Component-Specific Spacing**:\n```json\n{\n  \"component\": {\n    \"button\": {\n      \"padding-x\": \"{spacing.4}\",\n      \"padding-y\": \"{spacing.2}\",\n      \"gap\": \"{spacing.2}\"\n    },\n    \"card\": {\n      \"padding\": \"{spacing.6}\",\n      \"gap\": \"{spacing.4}\"\n    }\n  }\n}\n```\n\n#### 4. Border Radius Tokens\n\n```json\n{\n  \"borderRadius\": {\n    \"none\": \"0\",\n    \"sm\": \"0.125rem\",   // 2px\n    \"base\": \"0.25rem\",  // 4px\n    \"md\": \"0.375rem\",   // 6px\n    \"lg\": \"0.5rem\",     // 8px\n    \"xl\": \"0.75rem\",    // 12px\n    \"2xl\": \"1rem\",      // 16px\n    \"full\": \"9999px\"\n  }\n}\n```\n\n#### 5. Shadow Tokens\n\n```json\n{\n  \"shadow\": {\n    \"xs\": \"0 1px 2px 0 rgba(0, 0, 0, 0.05)\",\n    \"sm\": \"0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px -1px rgba(0, 0, 0, 0.1)\",\n    \"base\": \"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.1)\",\n    \"md\": \"0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -4px rgba(0, 0, 0, 0.1)\",\n    \"lg\": \"0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 8px 10px -6px rgba(0, 0, 0, 0.1)\",\n    \"xl\": \"0 25px 50px -12px rgba(0, 0, 0, 0.25)\"\n  }\n}\n```\n\n---\n\n## Component Architecture\n\n### Atomic Design Methodology\n\n**Atoms**  **Molecules**  **Organisms**  **Templates**  **Pages**\n\n#### Atoms (Primitive Components)\nBasic building blocks that can't be broken down further.\n\n**Examples:**\n- Button\n- Input\n- Label\n- Icon\n- Badge\n- Avatar\n\n**Button Component:**\n```typescript\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary' | 'outline' | 'ghost';\n  size?: 'sm' | 'md' | 'lg';\n  disabled?: boolean;\n  loading?: boolean;\n  icon?: React.ReactNode;\n  children: React.ReactNode;\n}\n```\n\nSee `references/component-examples.md` for complete Button implementation with variants, sizes, and styling patterns.\n\n#### Molecules (Simple Compositions)\nGroups of atoms that function together.\n\n**Examples:**\n- SearchBar (Input + Button)\n- FormField (Label + Input + ErrorMessage)\n- Card (Container + Title + Content + Actions)\n\n**FormField Molecule:**\n```typescript\ninterface FormFieldProps {\n  label: string;\n  name: string;\n  error?: string;\n  hint?: string;\n  required?: boolean;\n  children: React.ReactNode;\n}\n```\n\nSee `references/component-examples.md` for FormField, Card (compound component pattern), Input with variants, Modal, and more composition examples.\n\n#### Organisms (Complex Compositions)\nComplex UI components made of molecules and atoms.\n\n**Examples:**\n- Navigation Bar\n- Product Card Grid\n- User Profile Section\n- Modal Dialog\n\n#### Templates (Page Layouts)\nPage-level structures that define content placement.\n\n**Examples:**\n- Dashboard Layout (Sidebar + Header + Main Content)\n- Marketing Page Layout (Hero + Features + Footer)\n- Settings Page Layout (Tabs + Content Panels)\n\n#### Pages (Specific Instances)\nActual pages with real content.\n\n---\n\n## Component API Design\n\n### Props Best Practices\n\n**1. Predictable Prop Names**\n```typescript\n//  Good: Consistent naming\n<Button variant=\"primary\" size=\"md\" />\n<Input variant=\"outlined\" size=\"md\" />\n\n//  Bad: Inconsistent\n<Button type=\"primary\" sizeMode=\"md\" />\n<Input style=\"outlined\" inputSize=\"md\" />\n```\n\n**2. Sensible Defaults**\n```typescript\n//  Good: Provides defaults\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary';  // Default: primary\n  size?: 'sm' | 'md' | 'lg';          // Default: md\n}\n\n//  Bad: Everything required\ninterface ButtonProps {\n  variant: 'primary' | 'secondary';\n  size: 'sm' | 'md' | 'lg';\n  color: string;\n  padding: string;\n}\n```\n\n**3. Composition Over Configuration**\n```typescript\n//  Good: Composable\n<Card>\n  <Card.Header>\n    <Card.Title>Title</Card.Title>\n  </Card.Header>\n  <Card.Body>Content</Card.Body>\n  <Card.Footer>Actions</Card.Footer>\n</Card>\n\n//  Bad: Too many props\n<Card\n  title=\"Title\"\n  content=\"Content\"\n  footerContent=\"Actions\"\n  hasHeader={true}\n  hasFooter={true}\n/>\n```\n\n**4. Polymorphic Components**\nAllow components to render as different HTML elements:\n```typescript\n<Button as=\"a\" href=\"/login\">Login</Button>\n<Button as=\"button\" onClick={handleClick}>Click Me</Button>\n```\n\nSee `references/component-examples.md` for complete polymorphic component TypeScript patterns.\n\n---\n\n## Theming and Dark Mode\n\n### Theme Structure\n\n```typescript\ninterface Theme {\n  colors: {\n    brand: {\n      primary: string;\n      secondary: string;\n    };\n    text: {\n      primary: string;\n      secondary: string;\n    };\n    background: {\n      primary: string;\n      secondary: string;\n    };\n    feedback: {\n      success: string;\n      warning: string;\n      error: string;\n      info: string;\n    };\n  };\n  typography: {\n    fontFamily: {\n      sans: string;\n      mono: string;\n    };\n    fontSize: Record<string, string>;\n  };\n  spacing: Record<string, string>;\n  borderRadius: Record<string, string>;\n  shadow: Record<string, string>;\n}\n```\n\n### Dark Mode Implementation\n\n**Approach 1: Tailwind CSS @theme with Dark Mode (Recommended)**\nUsing Tailwind's `@theme` directive to define design tokens that generate utility classes:\n```css\n@theme {\n  --color-primary: #10b981;\n  --color-primary-hover: #059669;\n  --color-text-primary: #111827;\n  --color-background: #f9fafb;\n  --color-surface: #ffffff;\n}\n\n@media (prefers-color-scheme: dark) {\n  @theme {\n    --color-text-primary: #f9fafb;\n    --color-background: #111827;\n    --color-surface: #1f2937;\n  }\n}\n```\n\nThen use Tailwind utilities in components:\n```tsx\n<div className=\"bg-surface text-text-primary\">\n  Content\n</div>\n```\n\n**Approach 2: Tailwind CSS Dark Mode Variant**\n```tsx\n<div className=\"bg-white dark:bg-gray-900 text-gray-900 dark:text-white\">\n  Content\n</div>\n```\n\n**Approach 3: Styled Components ThemeProvider**\n```typescript\nconst lightTheme = { background: '#fff', text: '#000' };\nconst darkTheme = { background: '#000', text: '#fff' };\n\n<ThemeProvider theme={isDark ? darkTheme : lightTheme}>\n  <App />\n</ThemeProvider>\n```\n\n---\n\n## Accessibility Guidelines\n\n### WCAG 2.1 Level AA Compliance\n\n#### Color Contrast\n- **Normal text** (< 18pt): 4.5:1 minimum\n- **Large text** ( 18pt or  14pt bold): 3:1 minimum\n- **UI components**: 3:1 minimum\n\n**Tools**: Use contrast checkers like [WebAIM Contrast Checker](https://webaim.org/resources/contrastchecker/)\n\n#### Keyboard Navigation\n```typescript\n//  All interactive elements must be keyboard accessible\n<button\n  onClick={handleClick}\n  onKeyDown={(e) => e.key === 'Enter' && handleClick()}\n>\n  Click me\n</button>\n\n//  Focus management\n<Modal>\n  <FocusTrap>\n    {/* Modal content */}\n  </FocusTrap>\n</Modal>\n```\n\n#### ARIA Attributes\nEssential ARIA patterns:\n- `aria-label`: Provide accessible names\n- `aria-expanded`: Communicate expanded/collapsed state\n- `aria-controls`: Associate controls with content\n- `aria-live`: Announce dynamic content changes\n\n#### Screen Reader Support\n- Use semantic HTML elements (`<button>`, `<nav>`, `<main>`)\n- Avoid div/span soup for interactive elements\n- Provide meaningful labels for all controls\n\nSee `references/component-examples.md` for complete accessibility examples including Skip Links, focus traps, and ARIA patterns.\n\n---\n\n## Documentation Standards\n\n### Component Documentation Template\n\nEach component should document:\n- **Purpose**: What the component does\n- **Usage**: Import statement and basic example\n- **Variants**: Available visual styles\n- **Props**: Complete prop table with types, defaults, descriptions\n- **Accessibility**: Keyboard support, ARIA attributes, screen reader behavior\n- **Examples**: Common use cases with code\n\nUse Storybook, Docusaurus, or similar tools for interactive documentation.\n\nSee `templates/component-template.tsx` for the standard component structure.\n\n---\n\n## Design System Workflow\n\n### 1. Design Phase\n- **Audit existing patterns**: Identify inconsistencies\n- **Define design tokens**: Colors, typography, spacing\n- **Create component inventory**: List all needed components\n- **Design in Figma**: Create component library\n\n### 2. Development Phase\n- **Set up tooling**: Storybook, TypeScript, testing\n- **Implement tokens**: CSS variables or theme config\n- **Build atoms first**: Start with primitives\n- **Compose upward**: Build molecules, organisms\n- **Document as you go**: Write docs alongside code\n\n### 3. Adoption Phase\n- **Create migration guide**: Help teams adopt\n- **Provide codemods**: Automate migrations when possible\n- **Run workshops**: Train teams on usage\n- **Gather feedback**: Iterate based on real usage\n\n### 4. Maintenance Phase\n- **Version semantically**: Major/minor/patch releases\n- **Deprecation strategy**: Phase out old components gracefully\n- **Changelog**: Document all changes\n- **Monitor adoption**: Track usage across products\n\n---\n\n## Integration with Agents\n\n### Rapid UI Designer\n- Uses design tokens to create consistent interfaces\n- References component library for quick prototyping\n- Applies accessibility guidelines automatically\n\n### Frontend UI Developer\n- Implements components following design system patterns\n- Ensures consistency with existing design language\n- Validates color contrast and accessibility\n\n### Code Quality Reviewer\n- Checks components adhere to design system standards\n- Validates proper use of design tokens\n- Ensures accessibility requirements met\n\n---\n\n## Quick Start Checklist\n\nWhen creating a new design system:\n\n- [ ] Define design principles and values\n- [ ] Establish design token structure (colors, typography, spacing)\n- [ ] Create primitive color palette (50-950 scale)\n- [ ] Define semantic color tokens (brand, text, background, feedback)\n- [ ] Set typography scale and font families\n- [ ] Establish spacing scale (4px or 8px base)\n- [ ] **Use Tailwind `@theme` directive** to define tokens (generates utility classes)\n- [ ] **Components use Tailwind utilities** (`bg-primary`, `text-text-primary`) NOT CSS variables\n- [ ] Design atomic components (Button, Input, Label, etc.)\n- [ ] Implement theming system (light/dark mode)\n- [ ] Ensure WCAG 2.1 Level AA compliance\n- [ ] Set up documentation (Storybook or similar)\n- [ ] Create usage examples for each component\n- [ ] Establish versioning and release strategy\n- [ ] Create migration guides for adopting teams\n\n**Current Implementation (January 2026):**\n-  All colors defined in `frontend/src/styles/tokens.css` using `@theme` directive\n-  Components use Tailwind utilities: `bg-primary`, `text-text-primary`, `border-border`\n-  DO NOT use CSS variables in className: `bg-[var(--color-primary)]`\n\n---\n\n**Skill Version**: 1.0.0\n**Last Updated**: 2025-10-31\n**Maintained by**: AI Agent Hub Team\n\n## Capability Details\n\n### design-tokens\n**Keywords:** design tokens, css variables, theme, colors, spacing\n**Solves:**\n- Create design token system\n- Color palette\n- Typography scale\n\n### component-architecture\n**Keywords:** component library, atomic design, atoms, molecules\n**Solves:**\n- Structure component library\n- Compound components\n- Variants\n\n### accessibility\n**Keywords:** a11y, wcag, aria, keyboard navigation, focus\n**Solves:**\n- WCAG 2.1 AA compliance\n- ARIA attributes\n- Keyboard support\n\n### theming\n**Keywords:** theme, dark mode, light mode, color scheme\n**Solves:**\n- Implement dark/light mode\n- Theme switching\n- CSS custom properties"
              },
              {
                "name": "DevOps & Deployment",
                "description": "Use when setting up CI/CD pipelines, containerizing applications, deploying to Kubernetes, or writing infrastructure as code. Covers GitHub Actions, Docker, Helm, and Terraform patterns.",
                "path": ".claude/skills/devops-deployment/SKILL.md",
                "frontmatter": {
                  "name": "DevOps & Deployment",
                  "description": "Use when setting up CI/CD pipelines, containerizing applications, deploying to Kubernetes, or writing infrastructure as code. Covers GitHub Actions, Docker, Helm, and Terraform patterns.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "category": "Infrastructure & Deployment",
                  "agents": [
                    "backend-system-architect",
                    "code-quality-reviewer",
                    "studio-coach"
                  ],
                  "keywords": [
                    "CI/CD",
                    "deployment",
                    "Docker",
                    "Kubernetes",
                    "pipeline",
                    "infrastructure",
                    "GitOps",
                    "container",
                    "automation",
                    "release"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# DevOps & Deployment Skill\n\nComprehensive frameworks for CI/CD pipelines, containerization, deployment strategies, and infrastructure automation.\n\n## When to Use\n\n- Setting up CI/CD pipelines\n- Containerizing applications\n- Deploying to Kubernetes or cloud platforms\n- Implementing GitOps workflows\n- Managing infrastructure as code\n- Planning release strategies\n\n## Pipeline Architecture\n\n```\n         \n    Code         Build        Test        Deploy    \n   Commit          & Lint          & Scan         & Release  \n         \n                                                          \n                                                          \n   Triggers         Artifacts          Reports          Monitoring\n```\n\n## Key Concepts\n\n### CI/CD Pipeline Stages\n\n1. **Lint & Type Check** - Code quality gates\n2. **Unit Tests** - Test coverage with reporting\n3. **Security Scan** - npm audit + Trivy vulnerability scanner\n4. **Build & Push** - Docker image to container registry\n5. **Deploy Staging** - Environment-gated deployment\n6. **Deploy Production** - Manual approval or automated\n\n> See `templates/github-actions-pipeline.yml` for complete GitHub Actions workflow\n\n### Container Best Practices\n\n**Multi-stage builds** minimize image size:\n- Stage 1: Install production dependencies only\n- Stage 2: Build application with dev dependencies\n- Stage 3: Production runtime with minimal footprint\n\n**Security hardening**:\n- Non-root user (uid 1001)\n- Read-only filesystem where possible\n- Health checks for orchestrator integration\n\n> See `templates/Dockerfile` and `templates/docker-compose.yml`\n\n### Kubernetes Deployment\n\n**Essential manifests**:\n- Deployment with rolling update strategy\n- Service for internal routing\n- Ingress for external access with TLS\n- HorizontalPodAutoscaler for scaling\n\n**Security context**:\n- `runAsNonRoot: true`\n- `allowPrivilegeEscalation: false`\n- `readOnlyRootFilesystem: true`\n- Drop all capabilities\n\n**Resource management**:\n- Always set requests and limits\n- Use `requests` for scheduling, `limits` for throttling\n\n> See `templates/k8s-manifests.yaml` and `templates/helm-values.yaml`\n\n### Deployment Strategies\n\n| Strategy | Use Case | Risk |\n|----------|----------|------|\n| **Rolling** | Default, gradual replacement | Low - automatic rollback |\n| **Blue-Green** | Instant switch, easy rollback | Medium - double resources |\n| **Canary** | Progressive traffic shift | Low - gradual exposure |\n\n**Rolling Update** (Kubernetes default):\n```yaml\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 25%\n    maxUnavailable: 0  # Zero downtime\n```\n\n**Blue-Green**: Deploy to standby environment, switch service selector\n**Canary**: Use Istio VirtualService for traffic splitting (10%  50%  100%)\n\n### Infrastructure as Code\n\n**Terraform patterns**:\n- Remote state in S3 with DynamoDB locking\n- Module-based architecture (VPC, EKS, RDS)\n- Environment-specific tfvars files\n\n> See `templates/terraform-aws.tf` for AWS VPC + EKS + RDS example\n\n### GitOps with ArgoCD\n\nArgoCD watches Git repository and syncs cluster state:\n- Automated sync with pruning\n- Self-healing (drift detection)\n- Retry policies for transient failures\n\n> See `templates/argocd-application.yaml`\n\n### Secrets Management\n\nUse External Secrets Operator to sync from cloud providers:\n- AWS Secrets Manager\n- HashiCorp Vault\n- Azure Key Vault\n- GCP Secret Manager\n\n> See `templates/external-secrets.yaml`\n\n## Deployment Checklist\n\n### Pre-Deployment\n- [ ] All tests passing in CI\n- [ ] Security scans clean\n- [ ] Database migrations ready\n- [ ] Rollback plan documented\n\n### During Deployment\n- [ ] Monitor deployment progress\n- [ ] Watch error rates\n- [ ] Verify health checks passing\n\n### Post-Deployment\n- [ ] Verify metrics normal\n- [ ] Check logs for errors\n- [ ] Update status page\n\n## Helm Chart Structure\n\n```\ncharts/app/\n Chart.yaml\n values.yaml\n templates/\n    deployment.yaml\n    service.yaml\n    ingress.yaml\n    configmap.yaml\n    secret.yaml\n    hpa.yaml\n    _helpers.tpl\n values/\n     staging.yaml\n     production.yaml\n```\n\n---\n\n## CI/CD Pipeline Patterns\n\n### Branch Strategy\n\n**Recommended: Git Flow with Feature Branches**\n```\nmain (production) \n                               \ndev (staging) \n                           \nfeature/* \n                   \n                    PR required, CI checks, code review\n```\n\n**Branch protection rules:**\n- `main`: Require PR + 2 approvals + all checks pass\n- `dev`: Require PR + 1 approval + all checks pass\n- Feature branches: No direct commits to main/dev\n\n### GitHub Actions Caching Strategy\n\n```yaml\n- name: Cache Dependencies\n  uses: actions/cache@v3\n  with:\n    path: |\n      ~/.npm\n      node_modules\n      backend/.venv\n    key: ${{ runner.os }}-deps-${{ hashFiles('**/package-lock.json', '**/poetry.lock') }}\n    restore-keys: |\n      ${{ runner.os }}-deps-\n```\n\n**Cache hit ratio impact:**\n- Without cache: 2-3 min install time\n- With cache: 10-20 sec install time\n- **~85% time savings** on typical workflows\n\n### Artifact Management\n\n```yaml\n# Build and upload artifact\n- name: Build Application\n  run: npm run build\n\n- name: Upload Build Artifact\n  uses: actions/upload-artifact@v3\n  with:\n    name: build-${{ github.sha }}\n    path: dist/\n    retention-days: 7\n\n# Download in deployment job\n- name: Download Build Artifact\n  uses: actions/download-artifact@v3\n  with:\n    name: build-${{ github.sha }}\n    path: dist/\n```\n\n**Benefits:**\n- Avoid rebuilding in deployment job\n- Deploy exact tested artifact (byte-for-byte match)\n- Retention policies prevent storage bloat\n\n### Matrix Testing\n\n```yaml\nstrategy:\n  matrix:\n    node-version: [18, 20, 22]\n    os: [ubuntu-latest, windows-latest]\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: npm test\n```\n\n---\n\n## Container Optimization Deep Dive\n\n### Multi-Stage Build Example\n\n```dockerfile\n# ============================================================\n# Stage 1: Dependencies (builder)\n# ============================================================\nFROM node:20-alpine AS deps\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production && npm cache clean --force\n\n# ============================================================\n# Stage 2: Build (with dev dependencies)\n# ============================================================\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci  # Include dev dependencies\nCOPY . .\nRUN npm run build && npm run test\n\n# ============================================================\n# Stage 3: Production runtime (minimal)\n# ============================================================\nFROM node:20-alpine AS runner\nWORKDIR /app\n\n# Security: Non-root user\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\n\n# Copy only production dependencies and built artifacts\nCOPY --from=deps --chown=nodejs:nodejs /app/node_modules ./node_modules\nCOPY --from=builder --chown=nodejs:nodejs /app/dist ./dist\nCOPY --chown=nodejs:nodejs package*.json ./\n\nUSER nodejs\nEXPOSE 3000\nENV NODE_ENV=production\nHEALTHCHECK --interval=30s --timeout=3s CMD node healthcheck.js || exit 1\nCMD [\"node\", \"dist/main.js\"]\n```\n\n**Image size comparison:**\n- Single-stage: **850 MB** (includes dev dependencies, source files)\n- Multi-stage: **180 MB** (only runtime + production deps)\n- **78% reduction**\n\n### Layer Caching Optimization\n\n**Order matters for cache efficiency:**\n```dockerfile\n#  BAD: Invalidates cache on any code change\nCOPY . .\nRUN npm install\n\n#  GOOD: Cache package.json layer separately\nCOPY package*.json ./\nRUN npm ci  # Cached unless package.json changes\nCOPY . .    # Source changes don't invalidate npm install\n```\n\n### Security Scanning with Trivy\n\n```yaml\n- name: Build Docker Image\n  run: docker build -t myapp:${{ github.sha }} .\n\n- name: Scan for Vulnerabilities\n  uses: aquasecurity/trivy-action@master\n  with:\n    image-ref: 'myapp:${{ github.sha }}'\n    format: 'sarif'\n    output: 'trivy-results.sarif'\n    severity: 'CRITICAL,HIGH'\n\n- name: Upload Scan Results\n  uses: github/codeql-action/upload-sarif@v2\n  with:\n    sarif_file: 'trivy-results.sarif'\n\n- name: Fail on Critical Vulnerabilities\n  run: |\n    trivy image --severity CRITICAL --exit-code 1 myapp:${{ github.sha }}\n```\n\n---\n\n## Kubernetes Production Patterns\n\n### Health Probes\n\n**Three probe types with distinct purposes:**\n\n```yaml\nspec:\n  containers:\n  - name: app\n    # Startup probe (gives slow-starting apps time to boot)\n    startupProbe:\n      httpGet:\n        path: /health/startup\n        port: 8080\n      initialDelaySeconds: 0\n      periodSeconds: 5\n      failureThreshold: 30  # 30 * 5s = 150s max startup time\n\n    # Liveness probe (restarts pod if failing)\n    livenessProbe:\n      httpGet:\n        path: /health/liveness\n        port: 8080\n      initialDelaySeconds: 60\n      periodSeconds: 10\n      failureThreshold: 3  # 3 failures = restart\n\n    # Readiness probe (removes from service if failing)\n    readinessProbe:\n      httpGet:\n        path: /health/readiness\n        port: 8080\n      initialDelaySeconds: 10\n      periodSeconds: 5\n      failureThreshold: 2  # 2 failures = remove from load balancer\n```\n\n**Probe implementation:**\n```python\n@app.get(\"/health/startup\")\nasync def startup_check():\n    # Check DB connection established\n    if not db.is_connected():\n        raise HTTPException(status_code=503, detail=\"DB not ready\")\n    return {\"status\": \"ok\"}\n\n@app.get(\"/health/liveness\")\nasync def liveness_check():\n    # Basic \"is process running\" check\n    return {\"status\": \"alive\"}\n\n@app.get(\"/health/readiness\")\nasync def readiness_check():\n    # Check all dependencies healthy\n    if not redis.ping() or not db.health_check():\n        raise HTTPException(status_code=503, detail=\"Dependencies unhealthy\")\n    return {\"status\": \"ready\"}\n```\n\n### PodDisruptionBudget\n\nPrevents too many pods from being evicted during node maintenance:\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: app-pdb\nspec:\n  minAvailable: 2  # Always keep at least 2 pods running\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n**Use cases:**\n- Cluster upgrades (node drains)\n- Autoscaler downscaling\n- Manual evictions\n\n### Resource Quotas\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: team-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"10\"      # Total CPU requests\n    requests.memory: 20Gi   # Total memory requests\n    limits.cpu: \"20\"        # Total CPU limits\n    limits.memory: 40Gi     # Total memory limits\n    pods: \"50\"              # Max pods\n```\n\n### StatefulSets for Databases\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  serviceName: postgres\n  replicas: 3\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    # Pod spec here\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 100Gi\n```\n\n**Key differences from Deployment:**\n- Stable pod names (`postgres-0`, `postgres-1`, `postgres-2`)\n- Ordered deployment and scaling\n- Persistent storage per pod\n\n---\n\n## Database Migration Strategies\n\n### Zero-Downtime Migration Pattern\n\n**Problem:** Adding a NOT NULL column breaks old application versions\n\n**Solution: 3-phase migration**\n\n**Phase 1: Add nullable column**\n```sql\n-- Migration v1 (deploy with old code still running)\nALTER TABLE users ADD COLUMN email VARCHAR(255);\n```\n\n**Phase 2: Deploy new code + backfill**\n```python\n# New code writes to both old and new schema\ndef create_user(name: str, email: str):\n    # Write to new column\n    db.execute(\"INSERT INTO users (name, email) VALUES (%s, %s)\", (name, email))\n\n# Backfill existing rows\nasync def backfill_emails():\n    users_without_email = await db.fetch(\"SELECT id FROM users WHERE email IS NULL\")\n    for user in users_without_email:\n        email = generate_email(user.id)\n        await db.execute(\"UPDATE users SET email = %s WHERE id = %s\", (email, user.id))\n```\n\n**Phase 3: Add constraint**\n```sql\n-- Migration v2 (after backfill complete)\nALTER TABLE users ALTER COLUMN email SET NOT NULL;\n```\n\n### Backward/Forward Compatibility\n\n**Backward compatible changes (safe):**\n-  Add nullable column\n-  Add table\n-  Add index\n-  Rename column (with view alias)\n\n**Backward incompatible changes (requires 3-phase):**\n-  Remove column\n-  Rename column (no alias)\n-  Add NOT NULL column\n-  Change column type\n\n### Rollback Procedures\n\n```yaml\n# Helm rollback to previous revision\nhelm rollback myapp 3\n\n# Kubernetes rollback\nkubectl rollout undo deployment/myapp\n\n# Database migration rollback (Alembic example)\nalembic downgrade -1\n```\n\n**Critical: Test rollback procedures regularly!**\n\n---\n\n## Observability & Monitoring\n\n### Prometheus Metrics Exposition\n\n```python\nfrom prometheus_client import Counter, Histogram, generate_latest\n\n# Define metrics\nhttp_requests_total = Counter(\n    'http_requests_total',\n    'Total HTTP requests',\n    ['method', 'endpoint', 'status']\n)\n\nhttp_request_duration_seconds = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'endpoint']\n)\n\n@app.middleware(\"http\")\nasync def prometheus_middleware(request: Request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    duration = time.time() - start_time\n\n    # Record metrics\n    http_requests_total.labels(\n        method=request.method,\n        endpoint=request.url.path,\n        status=response.status_code\n    ).inc()\n\n    http_request_duration_seconds.labels(\n        method=request.method,\n        endpoint=request.url.path\n    ).observe(duration)\n\n    return response\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return Response(content=generate_latest(), media_type=\"text/plain\")\n```\n\n### Grafana Dashboard Queries\n\n```promql\n# Request rate (requests per second)\nrate(http_requests_total[5m])\n\n# Error rate (4xx/5xx as percentage)\nsum(rate(http_requests_total{status=~\"4..|5..\"}[5m])) /\nsum(rate(http_requests_total[5m])) * 100\n\n# p95 latency\nhistogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n\n# Pod CPU usage\nsum(rate(container_cpu_usage_seconds_total{pod=~\"myapp-.*\"}[5m])) by (pod)\n```\n\n### Alerting Rules\n\n```yaml\ngroups:\n- name: app-alerts\n  rules:\n  - alert: HighErrorRate\n    expr: |\n      sum(rate(http_requests_total{status=~\"5..\"}[5m])) /\n      sum(rate(http_requests_total[5m])) > 0.05\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"High error rate detected\"\n      description: \"Error rate is {{ $value | humanizePercentage }}\"\n\n  - alert: HighLatency\n    expr: |\n      histogram_quantile(0.95,\n        rate(http_request_duration_seconds_bucket[5m])\n      ) > 2\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High p95 latency detected\"\n      description: \"p95 latency is {{ $value }}s\"\n```\n\n---\n\n## Real-World SkillForge Examples\n\n### Example 1: Local Development with Docker Compose\n\n**SkillForge's actual docker-compose.yml:**\n```yaml\nversion: '3.8'\nservices:\n  postgres:\n    image: pgvector/pgvector:pg16\n    environment:\n      POSTGRES_USER: skillforge\n      POSTGRES_PASSWORD: dev_password\n      POSTGRES_DB: skillforge_dev\n    ports:\n      - \"5437:5432\"  # Avoid conflict with host postgres\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U skillforge\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru\n    volumes:\n      - redisdata:/data\n\n  backend:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"8500:8500\"\n    environment:\n      DATABASE_URL: postgresql://skillforge:dev_password@postgres:5432/skillforge_dev\n      REDIS_URL: redis://redis:6379\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_started\n    volumes:\n      - ./backend:/app  # Hot reload\n\n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"5173:5173\"\n    environment:\n      VITE_API_URL: http://localhost:8500\n    volumes:\n      - ./frontend:/app\n      - /app/node_modules  # Avoid overwriting node_modules\n\nvolumes:\n  pgdata:\n  redisdata:\n```\n\n**Key patterns:**\n- Port mapping to avoid host conflicts (5437:5432)\n- Health checks before dependent services start\n- Volume mounts for hot reload during development\n- Named volumes for data persistence\n\n### Example 2: GitHub Actions Workflow\n\n**SkillForge's backend CI/CD pipeline:**\n```yaml\nname: Backend CI/CD\n\non:\n  push:\n    branches: [main, dev]\n    paths: ['backend/**']\n  pull_request:\n    branches: [main, dev]\n    paths: ['backend/**']\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: backend\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Cache Poetry Dependencies\n        uses: actions/cache@v3\n        with:\n          path: ~/.cache/pypoetry\n          key: ${{ runner.os }}-poetry-${{ hashFiles('backend/poetry.lock') }}\n\n      - name: Install Poetry\n        run: pip install poetry\n\n      - name: Install Dependencies\n        run: poetry install\n\n      - name: Run Ruff Format Check\n        run: poetry run ruff format --check app/\n\n      - name: Run Ruff Lint\n        run: poetry run ruff check app/\n\n      - name: Run Type Check\n        run: poetry run mypy app/ --ignore-missing-imports\n\n      - name: Run Tests\n        run: poetry run pytest tests/ --cov=app --cov-report=xml\n\n      - name: Upload Coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./backend/coverage.xml\n\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Run Trivy Scan\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: 'backend/'\n          severity: 'CRITICAL,HIGH'\n```\n\n**Key features:**\n- Path filtering (only run on backend changes)\n- Poetry dependency caching\n- Comprehensive quality checks (format, lint, type, test)\n- Security scanning with Trivy\n\n### Example 3: Alembic Database Migrations\n\n**SkillForge migration pattern:**\n```python\n# backend/alembic/versions/2024_12_15_add_langfuse_trace_id.py\n\"\"\"Add Langfuse trace_id to analyses\n\nRevision ID: abc123def456\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    # Add nullable column first (backward compatible)\n    op.add_column('analyses',\n        sa.Column('langfuse_trace_id', sa.String(255), nullable=True)\n    )\n    # Index for lookup performance\n    op.create_index('idx_analyses_langfuse_trace',\n        'analyses', ['langfuse_trace_id']\n    )\n\ndef downgrade():\n    op.drop_index('idx_analyses_langfuse_trace')\n    op.drop_column('analyses', 'langfuse_trace_id')\n```\n\n**Migration workflow:**\n```bash\n# Create new migration\npoetry run alembic revision --autogenerate -m \"Add langfuse trace ID\"\n\n# Review generated migration (ALWAYS review!)\ncat alembic/versions/abc123_add_langfuse_trace_id.py\n\n# Apply migration\npoetry run alembic upgrade head\n\n# Rollback if needed\npoetry run alembic downgrade -1\n```\n\n---\n\n## Extended Thinking Triggers\n\nUse Opus 4.5 extended thinking for:\n- **Architecture decisions** - Kubernetes vs serverless, multi-region setup\n- **Migration planning** - Moving between cloud providers\n- **Incident response** - Complex deployment failures\n- **Security design** - Zero-trust architecture\n\n## Templates Reference\n\n| Template | Purpose |\n|----------|---------|\n| `github-actions-pipeline.yml` | Full CI/CD workflow with 6 stages |\n| `Dockerfile` | Multi-stage Node.js build |\n| `docker-compose.yml` | Development environment |\n| `k8s-manifests.yaml` | Deployment, Service, Ingress |\n| `helm-values.yaml` | Helm chart values |\n| `terraform-aws.tf` | VPC, EKS, RDS infrastructure |\n| `argocd-application.yaml` | GitOps application |\n| `external-secrets.yaml` | Secrets Manager integration |\n\n## Capability Details\n\n### ci-cd\n**Keywords:** ci, cd, pipeline, github actions, gitlab ci, jenkins, workflow\n**Solves:**\n- How do I set up CI/CD?\n- GitHub Actions workflow patterns\n- Pipeline caching strategies\n- Matrix testing setup\n\n### docker\n**Keywords:** docker, dockerfile, container, image, build, compose, multi-stage\n**Solves:**\n- How do I containerize my app?\n- Multi-stage Dockerfile best practices\n- Docker Compose development setup\n- Container security hardening\n\n### kubernetes\n**Keywords:** kubernetes, k8s, deployment, service, ingress, helm, statefulset, pdb\n**Solves:**\n- How do I deploy to Kubernetes?\n- K8s health probes and resource limits\n- Helm chart structure\n- StatefulSet for databases\n\n### infrastructure-as-code\n**Keywords:** terraform, pulumi, iac, infrastructure, provision, gitops, argocd\n**Solves:**\n- How do I set up infrastructure as code?\n- Terraform AWS patterns (VPC, EKS, RDS)\n- GitOps with ArgoCD\n- Secrets management patterns\n\n### deployment-strategies\n**Keywords:** blue green, canary, rolling, deployment strategy, rollback, zero downtime\n**Solves:**\n- Which deployment strategy should I use?\n- Zero-downtime database migrations\n- Blue-green deployment setup\n- Canary release with traffic splitting\n\n### observability\n**Keywords:** prometheus, grafana, metrics, alerting, monitoring, health check\n**Solves:**\n- How do I add monitoring to my app?\n- Prometheus metrics exposition\n- Grafana dashboard queries\n- Alerting rules for SLOs\n\n### skillforge-examples\n**Keywords:** skillforge, docker compose, github actions, alembic, real world\n**Solves:**\n- How does SkillForge set up CI/CD?\n- Real-world Docker Compose examples\n- Database migration patterns\n- Full CI/CD workflow implementation"
              },
              {
                "name": "doctor",
                "description": "SkillForge health diagnostics command that validates plugin configuration and reports issues",
                "path": ".claude/skills/doctor/SKILL.md",
                "frontmatter": {
                  "name": "doctor",
                  "description": "SkillForge health diagnostics command that validates plugin configuration and reports issues",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "health-check",
                    "diagnostics",
                    "validation",
                    "permissions",
                    "hooks"
                  ]
                },
                "content": "# SkillForge Health Diagnostics\n\n## Overview\n\nThe `/skf:doctor` command performs comprehensive health checks on your SkillForge installation. It validates:\n\n1. **Permission Rules** - Detects unreachable rules (CC 2.1.3 feature)\n2. **Hook Health** - Verifies executability and references\n3. **Schema Compliance** - Validates JSON files against schemas\n4. **Coordination System** - Checks lock health and registry integrity\n5. **Context Budget** - Monitors token usage against budget\n\n## When to Use\n\n- After installing or updating SkillForge\n- When hooks aren't firing as expected\n- When permission rules seem to have no effect\n- Before deploying to a team environment\n- When debugging coordination issues\n\n## Quick Start\n\n```bash\n/skf:doctor\n```\n\n## Health Check Categories\n\n### 1. Permission Rules Analysis\n\nLeverages CC 2.1.3's unreachable permission rules detection:\n\n```bash\n# Checks performed:\n# - Rules that can never match (unreachable patterns)\n# - Overlapping rules where one shadows another\n# - Invalid matcher syntax\n# - Missing required fields\n```\n\n**Output:**\n```\nPermission Rules: 12/12 reachable\n- auto-approve-readonly.sh: OK\n- auto-approve-safe-bash.sh: OK (3 patterns)\n- auto-approve-project-writes.sh: OK\n```\n\n### 2. Hook Validation\n\nVerifies all 93 hooks are properly configured:\n\n```bash\n# Checks performed:\n# - chmod +x (executable permission)\n# - Shebang line present (#!/usr/bin/env bash)\n# - Dispatcher references valid\n# - Matcher patterns syntax correct\n# - No missing hook files referenced in settings.json\n```\n\n**Output:**\n```\nHooks: 93/93 valid\n- pretool/bash-dispatcher.sh: executable, 87 lines\n- posttool/dispatcher.sh: executable, 89 lines\n- stop/context-compressor.sh: executable, 207 lines\n```\n\n### 3. Schema Compliance\n\nValidates JSON files against schemas in `.claude/schemas/`:\n\n```bash\n# Files validated:\n# - plugin.json against plugin.schema.json\n# - All capabilities.json files\n# - context/*.json files\n# - coordination/*.json files\n```\n\n**Output:**\n```\nSchemas: 15/15 compliant\n- plugin.json: valid\n- skills/*/capabilities.json: 79/79 valid\n- context/session/state.json: valid\n```\n\n### 4. Coordination System\n\nChecks multi-worktree coordination health:\n\n```bash\n# Checks performed:\n# - work-registry.json integrity\n# - decision-log.json structure\n# - Stale locks (expired > 60s)\n# - Heartbeat status\n```\n\n**Output:**\n```\nCoordination: healthy\n- Active instances: 1\n- Stale locks: 0\n- Decision log entries: 42\n```\n\n### 5. Context Budget\n\nMonitors token usage against the 2200 token budget:\n\n```bash\n# Calculations:\n# - identity.json tokens\n# - session/state.json tokens\n# - knowledge/*.json tokens\n# - Active skill context tokens\n```\n\n**Output:**\n```\nContext Budget: 1850/2200 tokens (84%)\n- identity.json: 200 tokens\n- session/state.json: 450 tokens\n- knowledge/: 1200 tokens\n```\n\n## Report Format\n\n```\n+==================================================================+\n|                    SkillForge Health Report                       |\n+==================================================================+\n| Version: 4.7.2  |  CC: 2.1.4  |  Channel: stable                 |\n+==================================================================+\n| Permission Rules     | 12/12 reachable                           |\n| Hooks                | 93/93 valid                               |\n| Schemas              | 15/15 compliant                           |\n| Context Budget       | 1850/2200 tokens (84%)                    |\n| Coordination         | 0 stale locks                             |\n+==================================================================+\n```\n\n## Interpreting Results\n\n| Status | Meaning | Action |\n|--------|---------|--------|\n| All checks pass | Plugin healthy | None required |\n| Permission warning | Unreachable rules | Review `.claude/settings.json` |\n| Hook error | Missing/broken hook | Check file permissions and paths |\n| Schema error | Invalid JSON | Run schema validation script |\n| Budget warning | >80% context used | Review loaded skills |\n| Coordination error | Stale locks | Run cleanup script |\n\n## Troubleshooting\n\n### \"Permission rule unreachable\"\n\n```bash\n# Check if a more specific rule shadows this one\n# Example: \"*.md\" shadowed by \"README.md\"\ncat .claude/settings.json | jq '.permissions'\n```\n\n### \"Hook not executable\"\n\n```bash\n# Fix permissions\nchmod +x .claude/hooks/path/to/hook.sh\n```\n\n### \"Context budget exceeded\"\n\n```bash\n# Check which skills are loaded\n# Use progressive loading - don't load entire skill directories\n```\n\n## Integration\n\nThis skill works with:\n- `quality-gates` - For CI/CD integration\n- `security-scanning` - For comprehensive audits\n\n## References\n\n- [Permission Rules](references/permission-rules.md)\n- [Hook Validation](references/hook-validation.md)\n- [Schema Validation](references/schema-validation.md)"
              },
              {
                "name": "e2e-testing",
                "description": "End-to-end testing with Playwright 1.57+. Use when testing critical user journeys, browser automation, cross-browser testing, AI-assisted test generation, or validating complete application flows.",
                "path": ".claude/skills/e2e-testing/SKILL.md",
                "frontmatter": {
                  "name": "e2e-testing",
                  "description": "End-to-end testing with Playwright 1.57+. Use when testing critical user journeys, browser automation, cross-browser testing, AI-assisted test generation, or validating complete application flows.",
                  "version": "2.0.0",
                  "tags": [
                    "playwright",
                    "e2e",
                    "testing",
                    "ai-agents",
                    2026
                  ],
                  "context": "fork",
                  "agent": "test-generator",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "echo \"::group::E2E Test Summary\"\necho \"Playwright tests generated - run with: npx playwright test\"\necho \"::endgroup::\"\n"
                      }
                    ]
                  }
                },
                "content": "# E2E Testing with Playwright 1.57+\n\nValidate critical user journeys end-to-end with AI-assisted test generation.\n\n## When to Use\n\n- Critical user flows (checkout, signup, payment)\n- Cross-browser testing\n- Visual regression testing\n- Full stack validation\n- AI-assisted test generation and healing\n\n## Quick Reference - Semantic Locators\n\n```typescript\n//  PREFERRED: Role-based locators (most resilient)\nawait page.getByRole('button', { name: 'Add to cart' }).click();\nawait page.getByRole('link', { name: 'Checkout' }).click();\n\n//  GOOD: Label-based for form controls\nawait page.getByLabel('Email').fill('test@example.com');\n\n//  ACCEPTABLE: Test IDs for stable anchors\nawait page.getByTestId('checkout-button').click();\n\n//  AVOID: CSS selectors and XPath (fragile)\n// await page.click('[data-testid=\"add-to-cart\"]');\n```\n\n**Locator Priority:** `getByRole()` > `getByLabel()` > `getByPlaceholder()` > `getByTestId()`\n\n## Basic Test\n\n```typescript\nimport { test, expect } from '@playwright/test';\n\ntest('user can complete checkout', async ({ page }) => {\n  await page.goto('/products');\n  await page.getByRole('button', { name: 'Add to cart' }).click();\n  await page.getByRole('link', { name: 'Checkout' }).click();\n  await page.getByLabel('Email').fill('test@example.com');\n  await page.getByRole('button', { name: 'Submit' }).click();\n  await expect(page.getByRole('heading', { name: 'Order confirmed' })).toBeVisible();\n});\n```\n\n## AI Agents (1.57+ - NEW)\n\n```bash\n# Generate test plan\nnpx playwright agents planner --url http://localhost:3000/checkout\n\n# Generate tests from plan\nnpx playwright agents generator --plan checkout-test-plan.md\n\n# Auto-repair failing tests\nnpx playwright agents healer --test checkout.spec.ts\n```\n\n## New Features (1.57+)\n\n```typescript\n// Assert individual class names\nawait expect(page.locator('.card')).toContainClass('highlighted');\n\n// Flaky test detection\nexport default defineConfig({\n  failOnFlakyTests: true,\n});\n\n// IndexedDB storage state\nawait page.context().storageState({\n  path: 'auth.json',\n  indexedDB: true  // NEW\n});\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```typescript\n//  NEVER use CSS selectors for user interactions\nawait page.click('.submit-btn');\n\n//  NEVER use hardcoded waits\nawait page.waitForTimeout(2000);\n\n//  NEVER test implementation details\nawait page.click('[data-testid=\"btn-123\"]');\n\n//  ALWAYS use semantic locators\nawait page.getByRole('button', { name: 'Submit' }).click();\n\n//  ALWAYS use Playwright's auto-wait\nawait expect(page.getByRole('alert')).toBeVisible();\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Locators | `getByRole` > `getByLabel` > `getByTestId` |\n| Browser | Chromium (Chrome for Testing in 1.57+) |\n| Execution | 5-30s per test |\n| Retries | 2-3 in CI, 0 locally |\n| Screenshots | On failure only |\n\n## Critical User Journeys to Test\n\n1. **Authentication:** Signup, login, password reset\n2. **Core Transaction:** Purchase, booking, submission\n3. **Data Operations:** Create, update, delete\n4. **User Settings:** Profile update, preferences\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/playwright-1.57-api.md](references/playwright-1.57-api.md) | Complete Playwright 1.57+ API reference |\n| [examples/test-patterns.md](examples/test-patterns.md) | User flows, page objects, visual tests |\n| [checklists/e2e-checklist.md](checklists/e2e-checklist.md) | Test selection and review checklists |\n| [templates/page-object-template.ts](templates/page-object-template.ts) | Page object model template |\n\n## Related Skills\n\n- `integration-testing` - API-level testing\n- `webapp-testing` - Autonomous test agents\n- `performance-testing` - Load testing\n- `llm-testing` - Testing AI/LLM components\n\n## Capability Details\n\n### semantic-locators\n**Keywords:** getByRole, getByLabel, getByText, semantic, locator\n**Solves:**\n- Use accessibility-based locators\n- Avoid brittle CSS/XPath selectors\n- Write resilient element queries\n\n### visual-regression\n**Keywords:** visual regression, screenshot, snapshot, visual diff\n**Solves:**\n- Capture and compare visual snapshots\n- Detect unintended UI changes\n- Configure threshold tolerances\n\n### cross-browser-testing\n**Keywords:** cross browser, chromium, firefox, webkit, browser matrix\n**Solves:**\n- Run tests across multiple browsers\n- Configure browser-specific settings\n- Handle browser differences\n\n### ai-test-generation\n**Keywords:** AI test, generate test, autonomous, test agent, planner\n**Solves:**\n- Generate tests from user journeys\n- Use AI agents for test planning\n- Create comprehensive test coverage\n\n### ai-test-healing\n**Keywords:** test healing, self-heal, auto-fix, resilient test\n**Solves:**\n- Automatically fix broken selectors\n- Adapt tests to UI changes\n- Reduce test maintenance\n\n### authentication-state\n**Keywords:** auth state, storage state, login once, reuse session\n**Solves:**\n- Persist authentication across tests\n- Avoid repeated login flows\n- Share auth state between tests"
              },
              {
                "name": "edge-computing-patterns",
                "description": "Use when deploying to Cloudflare Workers, Vercel Edge, or Deno Deploy. Covers edge middleware, streaming, runtime constraints, and globally distributed low-latency patterns.",
                "path": ".claude/skills/edge-computing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "edge-computing-patterns",
                  "description": "Use when deploying to Cloudflare Workers, Vercel Edge, or Deno Deploy. Covers edge middleware, streaming, runtime constraints, and globally distributed low-latency patterns.",
                  "context": "fork",
                  "agent": "frontend-ui-developer",
                  "version": "1.1.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "edge",
                    "cloudflare",
                    "vercel",
                    "deno",
                    "serverless",
                    2025
                  ]
                },
                "content": "# Edge Computing Patterns\n\n## Overview\n\nEdge computing runs code closer to users worldwide, reducing latency from seconds to milliseconds. This skill covers Cloudflare Workers, Vercel Edge Functions, and Deno Deploy patterns for building globally distributed applications.\n\n**When to use this skill:**\n- Global applications requiring <50ms latency\n- Authentication/authorization at the edge\n- A/B testing and feature flags\n- Geo-routing and localization\n- API rate limiting and DDoS protection\n- Transforming responses (image optimization, HTML rewriting)\n\n## Platform Comparison\n\n| Feature | Cloudflare Workers | Vercel Edge | Deno Deploy |\n|---------|-------------------|-------------|-------------|\n| Cold Start | <1ms | <10ms | <10ms |\n| Locations | 300+ | 100+ | 35+ |\n| Runtime | V8 Isolates | V8 Isolates | Deno |\n| Max Duration | 30s (paid: unlimited) | 25s | 50ms-5min |\n| Free Tier | 100k req/day | 100k req/month | 100k req/month |\n\n## Platform-Specific Implementation\n\nFor detailed code examples and patterns, load the appropriate reference file:\n\n### Cloudflare Workers\n**Reference:** `references/cloudflare-workers.md`\n- Worker fetch handlers and routing\n- KV storage patterns (eventually consistent)\n- Durable Objects for stateful edge\n- Wrangler CLI and wrangler.toml configuration\n- Caching strategies with Cache API\n\n### Vercel Edge Functions\n**Reference:** `references/vercel-edge.md`\n- Edge Middleware for Next.js (auth, A/B testing, geo-routing)\n- Edge API routes with streaming\n- Edge Config for feature flags\n- Geolocation-based routing patterns\n\n### Runtime Differences\n**Reference:** `references/runtime-differences.md`\n- Node.js APIs NOT available at edge\n- Web API compatibility matrix\n- Polyfill strategies for crypto, Buffer, streams\n\n## Edge Runtime Constraints\n\n**Available APIs:**\n- fetch, Request, Response, Headers\n- URL, URLSearchParams\n- TextEncoder, TextDecoder\n- ReadableStream, WritableStream\n- crypto, SubtleCrypto (Web Crypto API)\n- Web APIs (atob, btoa, setTimeout, etc.)\n\n**NOT Available:**\n- Node.js APIs (fs, path, child_process)\n- Native modules and binary dependencies\n- File system access\n- Some npm packages with Node.js dependencies\n\n## Common Patterns Summary\n\n### Authentication at Edge\nVerify JWT tokens at edge for sub-millisecond auth checks. See `references/cloudflare-workers.md` for implementation.\n\n### Rate Limiting\nUse KV (Cloudflare) or Edge Config (Vercel) for distributed rate limiting. Pattern: IP-based key with TTL expiration.\n\n### Edge Caching\nCache API with cache-aside pattern. Check cache first, fetch origin on miss, store with TTL.\n\n### A/B Testing\nAssign users to buckets via cookie, rewrite URLs to variant pages. See `references/vercel-edge.md` for middleware pattern.\n\n### Geo-Routing\nAccess request.cf.country (Cloudflare) or request.geo (Vercel) for location-based routing.\n\n## Best Practices\n\n- Keep bundles small (<1MB compressed)\n- Use streaming for large responses to avoid timeouts\n- Leverage platform caching (KV, Durable Objects, Edge Config)\n- Handle errors gracefully (edge errors cannot be recovered)\n- Test cold starts and warm starts separately\n- Monitor edge function performance and error rates\n- Use environment variables for secrets (never hardcode)\n- Implement proper CORS headers for cross-origin requests\n\n## Decision Guide\n\n| Use Case | Recommended Platform |\n|----------|---------------------|\n| Global CDN + compute | Cloudflare Workers |\n| Next.js middleware | Vercel Edge |\n| TypeScript-first | Deno Deploy |\n| Stateful edge | Cloudflare Durable Objects |\n| Feature flags | Vercel Edge Config |\n| Real-time collaboration | Cloudflare Durable Objects + WebSockets |\n\n## Resources\n\n- [Cloudflare Workers Documentation](https://developers.cloudflare.com/workers/)\n- [Vercel Edge Functions](https://vercel.com/docs/functions/edge-functions)\n- [Deno Deploy](https://deno.com/deploy/docs)\n\n## Capability Details\n\n### cloudflare-workers\n**Keywords:** cloudflare, workers, kv, durable objects, r2, wrangler\n**Reference:** references/cloudflare-workers.md\n**Solves:**\n- How do I deploy to Cloudflare Workers?\n- Cloudflare KV storage patterns\n- Durable Objects for stateful edge\n- Wrangler CLI usage and configuration\n\n### vercel-edge\n**Keywords:** vercel edge, edge functions, edge middleware, geolocation, next.js\n**Reference:** references/vercel-edge.md\n**Solves:**\n- How do I use Vercel Edge Functions?\n- Edge middleware patterns (auth, A/B testing)\n- Geo-based routing and localization\n- Edge streaming responses\n\n### runtime-differences\n**Keywords:** edge runtime, web apis, node.js compatibility, polyfills\n**Reference:** references/runtime-differences.md\n**Solves:**\n- What Node.js APIs are NOT available at edge?\n- Edge-compatible alternatives to Node APIs\n- How to polyfill crypto, base64, buffers\n- Package compatibility for edge runtimes\n\n### edge-caching\n**Keywords:** edge cache, cdn, cache-control, stale-while-revalidate, invalidation\n**Solves:**\n- How do I cache at the edge?\n- CDN caching strategies and headers\n- Stale-while-revalidate patterns\n- Cache invalidation strategies\n- Personalization at edge\n\n### edge-function-template\n**Keywords:** edge function, template, boilerplate, production-ready\n**Solves:**\n- How do I structure an edge function?\n- Production-ready edge function template\n- Error handling and validation patterns\n- CORS, rate limiting, caching setup\n\n### edge-middleware-template\n**Keywords:** middleware, next.js, authentication, a/b testing\n**Solves:**\n- How do I write Next.js edge middleware?\n- Authentication middleware patterns\n- A/B testing and feature flags\n- Geolocation routing middleware\n\n### deployment-checklist\n**Keywords:** deployment, checklist, production, monitoring\n**Reference:** checklists/edge-deployment-checklist.md\n**Solves:**\n- What should I check before deploying to edge?\n- Edge deployment best practices\n- Production readiness checklist\n- Monitoring and debugging setup\n\n## Quick Example\n\n```typescript\n// Cloudflare Worker - Basic fetch handler\nexport default {\n  async fetch(request: Request): Promise<Response> {\n    const url = new URL(request.url);\n\n    // Geo-based routing\n    const country = request.cf?.country || 'US';\n\n    // Edge caching\n    const cacheKey = url.pathname + \"-\" + country;\n    const cached = await caches.default.match(cacheKey);\n    if (cached) return cached;\n\n    const response = await fetch(request);\n    return response;\n  }\n}\n```"
              },
              {
                "name": "embeddings",
                "description": "Text embeddings for semantic search and similarity. Use when converting text to vectors, choosing embedding models, implementing chunking strategies, or building document similarity features.",
                "path": ".claude/skills/embeddings/SKILL.md",
                "frontmatter": {
                  "name": "embeddings",
                  "description": "Text embeddings for semantic search and similarity. Use when converting text to vectors, choosing embedding models, implementing chunking strategies, or building document similarity features.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# Embeddings\n\nConvert text to dense vector representations for semantic search and similarity.\n\n## When to Use\n\n- Building semantic search systems\n- Document similarity comparison\n- RAG retrieval (see: `rag-retrieval` skill)\n- Clustering related content\n- Duplicate detection\n\n## Quick Reference\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n# Single text embedding\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Your text here\"\n)\nvector = response.data[0].embedding  # 1536 dimensions\n```\n\n```python\n# Batch embedding (efficient)\ntexts = [\"text1\", \"text2\", \"text3\"]\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=texts\n)\nvectors = [item.embedding for item in response.data]\n```\n\n## Model Selection\n\n| Model | Dims | Cost | Use Case |\n|-------|------|------|----------|\n| `text-embedding-3-small` | 1536 | $0.02/1M | General purpose |\n| `text-embedding-3-large` | 3072 | $0.13/1M | High accuracy |\n| `nomic-embed-text` (Ollama) | 768 | Free | Local/CI |\n\n## Chunking Strategy\n\n```python\ndef chunk_text(text: str, chunk_size: int = 512, overlap: int = 50) -> list[str]:\n    \"\"\"Split text into overlapping chunks for embedding.\"\"\"\n    words = text.split()\n    chunks = []\n\n    for i in range(0, len(words), chunk_size - overlap):\n        chunk = \" \".join(words[i:i + chunk_size])\n        if chunk:\n            chunks.append(chunk)\n\n    return chunks\n```\n\n**Guidelines:**\n- Chunk size: 256-1024 tokens (512 typical)\n- Overlap: 10-20% for context continuity\n- Include metadata (title, source) with chunks\n\n## Similarity Calculation\n\n```python\nimport numpy as np\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    a, b = np.array(a), np.array(b)\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Usage\nsimilarity = cosine_similarity(vector1, vector2)\n# 1.0 = identical, 0.0 = orthogonal, -1.0 = opposite\n```\n\n## Key Decisions\n\n- **Dimension reduction**: Can truncate `text-embedding-3-large` to 1536 dims\n- **Normalization**: Most models return normalized vectors\n- **Batch size**: 100-500 texts per API call for efficiency\n\n## Common Mistakes\n\n- Embedding queries differently than documents\n- Not chunking long documents (context gets lost)\n- Using wrong similarity metric (cosine vs euclidean)\n- Re-embedding unchanged content (cache embeddings)\n\n## Advanced Patterns\n\nSee `references/advanced-patterns.md` for:\n- **Late Chunking**: Embed full document, extract chunk vectors from contextualized tokens\n- **Batch API**: Production batching with rate limiting and retry\n- **Embedding Cache**: Redis-based caching to avoid re-embedding\n- **Matryoshka Embeddings**: Dimension reduction with text-embedding-3\n\n## Related Skills\n\n- `rag-retrieval` - Using embeddings for RAG pipelines\n- `hyde-retrieval` - Hypothetical document embeddings for vocabulary mismatch\n- `contextual-retrieval` - Anthropic's context-prepending technique\n- `reranking-patterns` - Cross-encoder reranking for precision\n- `ollama-local` - Local embeddings with nomic-embed-text\n\n## Capability Details\n\n### text-to-vector\n**Keywords:** embedding, text to vector, vectorize, embed text\n**Solves:**\n- Convert text to vector embeddings\n- Choose appropriate embedding models\n- Handle embedding API integration\n\n### semantic-search\n**Keywords:** semantic search, vector search, similarity search, find similar\n**Solves:**\n- Implement semantic search over documents\n- Configure similarity thresholds\n- Rank results by relevance\n\n### chunking-strategies\n**Keywords:** chunk, chunking, split, text splitting, overlap\n**Solves:**\n- Split documents into optimal chunks\n- Configure chunk size and overlap\n- Preserve semantic boundaries\n\n### batch-embedding\n**Keywords:** batch, bulk embed, parallel embedding, batch processing\n**Solves:**\n- Embed large document collections efficiently\n- Handle rate limits and retries\n- Optimize embedding costs\n\n### local-embeddings\n**Keywords:** local, ollama, self-hosted, on-premise, offline\n**Solves:**\n- Run embeddings locally with Ollama\n- Deploy self-hosted embedding models\n- Reduce API costs with local models"
              },
              {
                "name": "error-handling-rfc9457",
                "description": "Structured error responses using RFC 9457 Problem Details for HTTP APIs. Use when standardizing error responses, implementing error registries, or improving API error handling.",
                "path": ".claude/skills/error-handling-rfc9457/SKILL.md",
                "frontmatter": {
                  "name": "error-handling-rfc9457",
                  "description": "Structured error responses using RFC 9457 Problem Details for HTTP APIs. Use when standardizing error responses, implementing error registries, or improving API error handling.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "tags": [
                    "error-handling",
                    "rfc9457",
                    "problem-details",
                    "fastapi",
                    "api",
                    2026
                  ]
                },
                "content": "# RFC 9457 Problem Details\n\nStandardize API error responses with machine-readable problem details.\n\n## When to Use\n\n- Standardizing error responses across APIs\n- Building public APIs with clear error contracts\n- Implementing error type registries\n- Replacing ad-hoc error formats\n- Improving client error handling\n\n## RFC 9457 vs RFC 7807\n\n| Feature | RFC 7807 (Old) | RFC 9457 (Current) |\n|---------|----------------|---------------------|\n| Status | Obsolete | Active Standard |\n| Multiple problems | Not specified | Explicitly supported |\n| Error registry | No | Yes (IANA registry) |\n| Extension fields | Implicit | Explicitly allowed |\n\n## Problem Details Schema\n\n```python\nfrom pydantic import BaseModel, Field, HttpUrl\nfrom typing import Any\n\nclass ProblemDetail(BaseModel):\n    \"\"\"RFC 9457 Problem Details for HTTP APIs.\"\"\"\n\n    type: HttpUrl = Field(\n        default=\"about:blank\",\n        description=\"URI identifying the problem type\"\n    )\n    title: str = Field(\n        description=\"Short, human-readable summary\"\n    )\n    status: int = Field(\n        ge=400, le=599,\n        description=\"HTTP status code\"\n    )\n    detail: str | None = Field(\n        default=None,\n        description=\"Human-readable explanation specific to this occurrence\"\n    )\n    instance: str | None = Field(\n        default=None,\n        description=\"URI reference identifying the specific occurrence\"\n    )\n\n    model_config = {\"extra\": \"allow\"}  # Allow extension fields\n```\n\n## FastAPI Integration\n\n### Exception Classes\n\n```python\nfrom fastapi import HTTPException\nfrom typing import Any\n\nclass ProblemException(HTTPException):\n    \"\"\"Base exception for RFC 9457 problem details.\"\"\"\n\n    def __init__(\n        self,\n        status_code: int,\n        problem_type: str,\n        title: str,\n        detail: str | None = None,\n        instance: str | None = None,\n        **extensions: Any,\n    ):\n        self.problem_type = problem_type\n        self.title = title\n        self.detail = detail\n        self.instance = instance\n        self.extensions = extensions\n        super().__init__(status_code=status_code, detail=detail)\n\n    def to_problem_detail(self) -> dict[str, Any]:\n        result = {\n            \"type\": self.problem_type,\n            \"title\": self.title,\n            \"status\": self.status_code,\n        }\n        if self.detail:\n            result[\"detail\"] = self.detail\n        if self.instance:\n            result[\"instance\"] = self.instance\n        result.update(self.extensions)\n        return result\n```\n\n### Specific Problem Types\n\n```python\nclass ValidationProblem(ProblemException):\n    def __init__(self, errors: list[dict], instance: str | None = None):\n        super().__init__(\n            status_code=422,\n            problem_type=\"https://api.example.com/problems/validation-error\",\n            title=\"Validation Error\",\n            detail=\"One or more fields failed validation\",\n            instance=instance,\n            errors=errors,  # Extension field\n        )\n\nclass NotFoundProblem(ProblemException):\n    def __init__(self, resource: str, resource_id: str, instance: str | None = None):\n        super().__init__(\n            status_code=404,\n            problem_type=\"https://api.example.com/problems/resource-not-found\",\n            title=\"Resource Not Found\",\n            detail=f\"{resource} with ID '{resource_id}' was not found\",\n            instance=instance,\n            resource=resource,\n            resource_id=resource_id,\n        )\n\nclass RateLimitProblem(ProblemException):\n    def __init__(self, retry_after: int, instance: str | None = None):\n        super().__init__(\n            status_code=429,\n            problem_type=\"https://api.example.com/problems/rate-limit-exceeded\",\n            title=\"Too Many Requests\",\n            detail=\"Rate limit exceeded. Please retry later.\",\n            instance=instance,\n            retry_after=retry_after,\n        )\n```\n\n### Exception Handler\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\n\napp = FastAPI()\n\n@app.exception_handler(ProblemException)\nasync def problem_exception_handler(request: Request, exc: ProblemException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content=exc.to_problem_detail(),\n        media_type=\"application/problem+json\",\n    )\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    errors = [\n        {\"field\": \".\".join(str(loc) for loc in err[\"loc\"]), \"message\": err[\"msg\"]}\n        for err in exc.errors()\n    ]\n    problem = ValidationProblem(errors=errors, instance=str(request.url))\n    return JSONResponse(\n        status_code=422,\n        content=problem.to_problem_detail(),\n        media_type=\"application/problem+json\",\n    )\n\n@app.exception_handler(Exception)\nasync def generic_exception_handler(request: Request, exc: Exception):\n    return JSONResponse(\n        status_code=500,\n        content={\n            \"type\": \"https://api.example.com/problems/internal-error\",\n            \"title\": \"Internal Server Error\",\n            \"status\": 500,\n            \"detail\": \"An unexpected error occurred\",\n            \"instance\": str(request.url),\n        },\n        media_type=\"application/problem+json\",\n    )\n```\n\n## Usage in Endpoints\n\n```python\n@router.get(\"/api/v1/analyses/{analysis_id}\")\nasync def get_analysis(\n    analysis_id: str,\n    request: Request,\n    service: AnalysisService = Depends(get_analysis_service),\n):\n    analysis = await service.get_by_id(analysis_id)\n    if not analysis:\n        raise NotFoundProblem(\n            resource=\"Analysis\",\n            resource_id=analysis_id,\n            instance=str(request.url),\n        )\n    return analysis\n```\n\n## Response Examples\n\n### 404 Not Found\n\n```json\n{\n  \"type\": \"https://api.example.com/problems/resource-not-found\",\n  \"title\": \"Resource Not Found\",\n  \"status\": 404,\n  \"detail\": \"Analysis with ID 'abc123' was not found\",\n  \"instance\": \"/api/v1/analyses/abc123\",\n  \"resource\": \"Analysis\",\n  \"resource_id\": \"abc123\"\n}\n```\n\n### 422 Validation Error\n\n```json\n{\n  \"type\": \"https://api.example.com/problems/validation-error\",\n  \"title\": \"Validation Error\",\n  \"status\": 422,\n  \"detail\": \"One or more fields failed validation\",\n  \"instance\": \"/api/v1/analyses\",\n  \"errors\": [\n    {\"field\": \"source_url\", \"message\": \"Invalid URL format\"},\n    {\"field\": \"depth\", \"message\": \"Must be between 1 and 3\"}\n  ]\n}\n```\n\n### 429 Rate Limited\n\n```json\n{\n  \"type\": \"https://api.example.com/problems/rate-limit-exceeded\",\n  \"title\": \"Too Many Requests\",\n  \"status\": 429,\n  \"detail\": \"Rate limit exceeded. Please retry later.\",\n  \"instance\": \"/api/v1/analyses\",\n  \"retry_after\": 60\n}\n```\n\n## Error Type Registry\n\n```python\n# app/core/problem_types.py\nPROBLEM_TYPES = {\n    \"validation-error\": {\n        \"uri\": \"https://api.example.com/problems/validation-error\",\n        \"title\": \"Validation Error\",\n        \"status\": 422,\n    },\n    \"resource-not-found\": {\n        \"uri\": \"https://api.example.com/problems/resource-not-found\",\n        \"title\": \"Resource Not Found\",\n        \"status\": 404,\n    },\n    \"rate-limit-exceeded\": {\n        \"uri\": \"https://api.example.com/problems/rate-limit-exceeded\",\n        \"title\": \"Too Many Requests\",\n        \"status\": 429,\n    },\n    \"unauthorized\": {\n        \"uri\": \"https://api.example.com/problems/unauthorized\",\n        \"title\": \"Unauthorized\",\n        \"status\": 401,\n    },\n    \"forbidden\": {\n        \"uri\": \"https://api.example.com/problems/forbidden\",\n        \"title\": \"Forbidden\",\n        \"status\": 403,\n    },\n    \"conflict\": {\n        \"uri\": \"https://api.example.com/problems/conflict\",\n        \"title\": \"Conflict\",\n        \"status\": 409,\n    },\n    \"internal-error\": {\n        \"uri\": \"https://api.example.com/problems/internal-error\",\n        \"title\": \"Internal Server Error\",\n        \"status\": 500,\n    },\n}\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER return plain text errors\nreturn Response(\"Not found\", status_code=404)\n\n# NEVER use inconsistent error formats\nreturn {\"error\": \"Not found\"}  # Different from other errors\nreturn {\"message\": \"Validation failed\", \"errors\": [...]}\n\n# NEVER expose internal details in production\nreturn {\"detail\": str(exc), \"traceback\": traceback.format_exc()}\n\n# NEVER use generic 500 for everything\nexcept Exception:\n    raise HTTPException(500, \"Something went wrong\")\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Media type | `application/problem+json` |\n| Type URI | Use your API domain + `/problems/` |\n| Detail | Include only for user-actionable info |\n| Extensions | Use for machine-readable context |\n| Logging | Log problem types for monitoring |\n\n## Related Skills\n\n- `api-design-framework` - REST API patterns\n- `observability-monitoring` - Error tracking\n- `input-validation` - Validation patterns\n\n## Capability Details\n\n### problem-details\n**Keywords:** problem details, RFC 9457, RFC 7807, structured error\n**Solves:**\n- How to standardize API error responses?\n- What format for API errors?\n\n### fastapi-errors\n**Keywords:** fastapi exception, error handler, HTTPException\n**Solves:**\n- How to handle errors in FastAPI?\n- Custom exception handlers\n\n### error-registry\n**Keywords:** error registry, problem types, error catalog\n**Solves:**\n- How to document all API errors?\n- Error type management"
              },
              {
                "name": "errors",
                "description": "Error pattern analysis and troubleshooting for Claude Code sessions",
                "path": ".claude/skills/errors/SKILL.md",
                "frontmatter": {
                  "name": "errors",
                  "description": "Error pattern analysis and troubleshooting for Claude Code sessions",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "errors",
                    "debugging",
                    "troubleshooting",
                    "patterns"
                  ]
                },
                "content": "# Error Pattern Analysis\n\nAnalyze errors captured from Claude Code sessions to identify patterns and get actionable insights.\n\n## When to Use\n\n- Analyzing recurring error patterns\n- Getting fix suggestions for common errors\n- Reviewing session error logs\n\n## Quick Start\n\n```bash\n/errors\n```\n\n## Quick Analysis\n\n```bash\n# Run batch analysis on last 24h of errors\npython .claude/scripts/analyze_errors.py\n\n# Analyze last 7 days\npython .claude/scripts/analyze_errors.py --days 7\n\n# Generate markdown report\npython .claude/scripts/analyze_errors.py --report\n```\n\n## What Gets Captured\n\nThe error collector hook captures:\n- Tool name (Bash, mcp__postgres-mcp__query, etc.)\n- Error message (first 500 chars)\n- Tool input (command/query that failed)\n- Timestamp and session ID\n\n**Location:** `.claude/logs/errors.jsonl`\n\n## Current Error Rules\n\nCheck learned patterns that trigger warnings:\n\n```bash\ncat .claude/rules/error_rules.json | jq '.rules[] | {id, signature, count: .occurrence_count}'\n```\n\n## Files\n\n| File | Purpose |\n|------|---------|\n| `.claude/hooks/posttool/error-collector.sh` | Captures errors to JSONL |\n| `.claude/hooks/pretool/bash/error-pattern-warner.sh` | Warns before risky commands |\n| `.claude/scripts/analyze_errors.py` | Batch pattern analysis |\n| `.claude/rules/error_rules.json` | Learned error patterns |\n| `.claude/logs/errors.jsonl` | Raw error log |\n\n## Common Patterns\n\n### PostgreSQL Connection Errors\n\n```\npattern: role \"X\" does not exist\nfix: Use Docker connection: docker exec -it skillforge-postgres-dev psql -U skillforge_user -d skillforge_dev\n\npattern: relation \"X\" does not exist\nfix: Check MCP postgres server connection string - may be connected to wrong database\n```\n\n## Adding New Rules\n\nRules are auto-generated by `analyze_errors.py` when patterns repeat 2+ times.\nFor manual rules, edit `.claude/rules/error_rules.json`:\n\n```json\n{\n  \"id\": \"custom-001\",\n  \"pattern\": \"your regex pattern\",\n  \"signature\": \"human readable signature\",\n  \"tool\": \"Bash\",\n  \"occurrence_count\": 1,\n  \"fix_suggestion\": \"How to fix this\"\n}\n```"
              },
              {
                "name": "evidence-verification",
                "description": "Use when completing tasks, code reviews, or deployments to verify work with evidence. Collects test results, build outputs, coverage metrics, and exit codes to prove work is complete.",
                "path": ".claude/skills/evidence-verification/SKILL.md",
                "frontmatter": {
                  "name": "evidence-verification",
                  "description": "Use when completing tasks, code reviews, or deployments to verify work with evidence. Collects test results, build outputs, coverage metrics, and exit codes to prove work is complete.",
                  "version": "2.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "quality",
                    "verification",
                    "testing",
                    "evidence",
                    "completion"
                  ],
                  "context": "fork",
                  "agent": "code-quality-reviewer",
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob",
                    "Bash"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Bash",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/evidence-collector.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/evidence-collector.sh"
                      }
                    ]
                  }
                },
                "content": "# Evidence-Based Verification Skill\n\n**Version:** 2.0.0\n**Type:** Quality Assurance\n**Auto-activate:** Code review, task completion, production deployment\n\n## Overview\n\nThis skill teaches agents how to collect and verify evidence before marking tasks complete. Inspired by production-grade development practices, it ensures all claims are backed by executable proof: test results, coverage metrics, build success, and deployment verification.\n\n**Key Principle:** Show, don't tell. No task is complete without verifiable evidence.\n\n---\n\n## When to Use This Skill\n\n### Auto-Activate Triggers\n- Completing code implementation\n- Finishing code review\n- Marking tasks complete in Squad mode\n- Before agent handoff\n- Production deployment verification\n\n### Manual Activation\n- When user requests \"verify this works\"\n- Before creating pull requests\n- During quality assurance reviews\n- When troubleshooting failures\n\n---\n\n## Core Concepts\n\n### 1. Evidence Types\n\n**Test Evidence**\n- Exit code (must be 0 for success)\n- Test suite results (passed/failed/skipped)\n- Coverage percentage (if available)\n- Test duration\n\n**Build Evidence**\n- Build exit code (0 = success)\n- Compilation errors/warnings\n- Build artifacts created\n- Build duration\n\n**Deployment Evidence**\n- Deployment status (success/failed)\n- Environment deployed to\n- Health check results\n- Rollback capability verified\n\n**Code Quality Evidence**\n- Linter results (errors/warnings)\n- Type checker results\n- Security scan results\n- Accessibility audit results\n\n### 2. Evidence Collection Protocol\n\n```markdown\n## Evidence Collection Steps\n\n1. **Identify Verification Points**\n   - What needs to be proven?\n   - What could go wrong?\n   - What does \"complete\" mean?\n\n2. **Execute Verification**\n   - Run tests\n   - Run build\n   - Run linters\n   - Check deployments\n\n3. **Capture Results**\n   - Record exit codes\n   - Save output snippets\n   - Note timestamps\n   - Document environment\n\n4. **Store Evidence**\n   - Add to shared context\n   - Reference in task completion\n   - Link to artifacts\n```\n\n### 3. Verification Standards\n\n**Minimum Evidence Requirements:**\n-  At least ONE verification type executed\n-  Exit code captured (0 = pass, non-zero = fail)\n-  Timestamp recorded\n-  Evidence stored in context\n\n**Production-Grade Requirements:**\n-  Tests run with exit code 0\n-  Coverage >70% (or project standard)\n-  Build succeeds with exit code 0\n-  No critical linter errors\n-  Security scan passes\n\n---\n\n## Evidence Collection Templates\n\n### Template 1: Test Evidence\n\nUse this template when running tests:\n\n```markdown\n## Test Evidence\n\n**Command:** `npm test` (or equivalent)\n**Exit Code:** 0  / non-zero \n**Duration:** X seconds\n**Results:**\n- Tests passed: X\n- Tests failed: X\n- Tests skipped: X\n- Coverage: X%\n\n**Output Snippet:**\n```\n[First 10 lines of test output]\n```\n\n**Timestamp:** YYYY-MM-DD HH:MM:SS\n**Environment:** Node vX.X.X, OS, etc.\n```\n\n### Template 2: Build Evidence\n\nUse this template when building:\n\n```markdown\n## Build Evidence\n\n**Command:** `npm run build` (or equivalent)\n**Exit Code:** 0  / non-zero \n**Duration:** X seconds\n**Artifacts Created:**\n- dist/bundle.js (245 KB)\n- dist/styles.css (18 KB)\n\n**Errors:** X\n**Warnings:** X\n\n**Output Snippet:**\n```\n[First 10 lines of build output]\n```\n\n**Timestamp:** YYYY-MM-DD HH:MM:SS\n```\n\n### Template 3: Code Quality Evidence\n\nUse this template for linting and type checking:\n\n```markdown\n## Code Quality Evidence\n\n**Linter:** ESLint / Ruff / etc.\n**Command:** `npm run lint`\n**Exit Code:** 0  / non-zero \n**Errors:** X\n**Warnings:** X\n\n**Type Checker:** TypeScript / mypy / etc.\n**Command:** `npm run typecheck`\n**Exit Code:** 0  / non-zero \n**Type Errors:** X\n\n**Timestamp:** YYYY-MM-DD HH:MM:SS\n```\n\n---\n\n## Quality Standards\n\n### Minimum Acceptable\n\n **Tests executed** with captured exit code\n **Timestamp** recorded\n **Evidence stored** in context\n\n### Production-Grade\n\n **Tests pass** (exit code 0)\n **Coverage 70%** (or project standard)\n **Build succeeds** (exit code 0)\n **No critical linter errors**\n **Type checker passes**\n **Security scan** shows no critical issues\n\n### Gold Standard\n\n All production-grade requirements\n **Coverage 80%**\n **No linter warnings**\n **Performance benchmarks** within thresholds\n **Accessibility audit** passes (WCAG 2.1 AA)\n **Integration tests** pass\n **Deployment verification** complete\n\n---\n\n## Common Pitfalls\n\n###  Don't Skip Evidence Collection\n\n**Bad:**\n```\n\"I've implemented the login feature. It should work correctly.\"\n```\n\n**Good:**\n```\n\"I've implemented the login feature. Evidence:\n- Tests: Exit code 0, 12 tests passed, 0 failed\n- Build: Exit code 0, no errors\n- Coverage: 89%\nTask complete with verification.\"\n```\n\n###  Don't Fake Evidence\n\n**Bad:**\n```\n\"Tests passed\" (without actually running them)\n```\n\n**Good:**\n```\n\"Tests passed. Exit code: 0\nCommand: npm test\nOutput: Test Suites: 3 passed, 3 total\nTimestamp: 2025-11-02 10:30:15\"\n```\n\n###  Don't Ignore Failed Evidence\n\n**Bad:**\n```\n\"Build failed with exit code 1, but the code looks correct so marking complete.\"\n```\n\n**Good:**\n```\n\"Build failed with exit code 1. Errors:\n- TypeError: Cannot read property 'id' of undefined (line 42)\nFixing the error now before marking complete.\"\n```\n\n---\n\n## Quick Reference\n\n### Evidence Collection Checklist\n\n```markdown\nBefore marking task complete:\n\n- [ ] Tests executed\n- [ ] Test exit code captured (0 = pass)\n- [ ] Build executed (if applicable)\n- [ ] Build exit code captured (0 = pass)\n- [ ] Code quality checks run (linter, types)\n- [ ] Evidence documented with timestamp\n- [ ] Evidence added to shared context\n- [ ] Evidence summary in completion message\n```\n\n### Common Commands by Language/Framework\n\n**JavaScript/TypeScript:**\n```bash\nnpm test                 # Run tests\nnpm run build           # Build project\nnpm run lint            # Run ESLint\nnpm run typecheck       # Run TypeScript compiler\n```\n\n**Python:**\n```bash\npytest                  # Run tests\npytest --cov           # Run tests with coverage\nruff check .           # Run linter\nmypy .                 # Run type checker\n```\n\n---\n\n**Remember:** Evidence-first development prevents hallucinations, ensures production quality, and builds confidence. When in doubt, collect more evidence, not less.\n\n## Capability Details\n\n### exit-code-validation\n**Keywords:** exit code, return code, success, failure, status, $?, exit 0, non-zero\n**Solves:**\n- How do I verify command succeeded?\n- Check exit codes for evidence (0 = pass)\n- Validate build/test success with exit codes\n- Capture command exit status in evidence\n\n### test-evidence\n**Keywords:** test results, test output, coverage report, test evidence, jest, pytest, test suite, passed, failed\n**Solves:**\n- How do I capture test evidence?\n- Record test results in session state\n- Prove tests passed with exit code 0\n- Document test coverage percentage\n- Capture passed/failed/skipped counts\n\n### build-evidence\n**Keywords:** build log, build output, compile, bundle, webpack, vite, cargo build, npm build\n**Solves:**\n- How do I capture build evidence?\n- Record build success with exit code\n- Verify compilation without errors\n- Document build artifacts created\n- Track build duration and warnings\n\n### code-quality-evidence\n**Keywords:** linter, lint, eslint, ruff, type check, mypy, typescript, code quality, warnings, errors\n**Solves:**\n- How do I capture code quality evidence?\n- Run linter and capture results\n- Execute type checker and record errors\n- Document linter errors and warnings count\n- Prove code quality checks passed\n\n### deployment-evidence\n**Keywords:** deployment, deploy, production, staging, health check, rollback, deployment status\n**Solves:**\n- How do I verify deployment succeeded?\n- Check health endpoints after deploy\n- Verify application started successfully\n- Document deployment status and environment\n- Confirm rollback capability exists\n\n### security-scan-evidence\n**Keywords:** security, vulnerability, npm audit, pip-audit, security scan, cve, critical vulnerabilities\n**Solves:**\n- How do I capture security scan results?\n- Run npm audit or pip-audit\n- Document critical vulnerabilities found\n- Record security scan exit code\n- Prove no critical security issues\n\n### evidence-storage\n**Keywords:** session state, state.json, evidence storage, record evidence, save results, quality_evidence, context 2.0\n**Solves:**\n- How do I store evidence in context?\n- Update session/state.json with results\n- Structure evidence data properly\n- Add timestamp to evidence records\n- Link to evidence log files\n\n### combined-evidence-report\n**Keywords:** evidence report, task completion, verification summary, proof of completion, comprehensive evidence\n**Solves:**\n- How do I create complete evidence report?\n- Combine test, build, and quality evidence\n- Create task completion evidence summary\n- Document all verification checks run\n- Provide comprehensive proof of completion\n\n### evidence-collection-workflow\n**Keywords:** evidence workflow, verification steps, evidence protocol, collection process, verification checklist\n**Solves:**\n- What steps to collect evidence?\n- Follow evidence collection protocol\n- Run all necessary verification checks\n- Complete evidence checklist before marking done\n- Ensure minimum evidence requirements met\n\n### quality-standards\n**Keywords:** quality standards, minimum requirements, production-grade, gold standard, evidence thresholds\n**Solves:**\n- What evidence is required to pass?\n- Understand minimum vs production-grade standards\n- Meet gold standard evidence requirements\n- Know when evidence is sufficient\n- Validate evidence meets project standards\n\n### evidence-pitfalls\n**Keywords:** evidence mistakes, common errors, skip evidence, fake evidence, ignore failures\n**Solves:**\n- What evidence mistakes to avoid?\n- Never skip evidence collection\n- Don't fake evidence results\n- Don't ignore failed evidence\n- Always re-collect after changes"
              },
              {
                "name": "explore",
                "description": "Deep codebase exploration with parallel specialized agents",
                "path": ".claude/skills/explore/SKILL.md",
                "frontmatter": {
                  "name": "explore",
                  "description": "Deep codebase exploration with parallel specialized agents",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "exploration",
                    "code-search",
                    "architecture",
                    "codebase"
                  ]
                },
                "content": "# Codebase Exploration\n\nMulti-angle codebase exploration using 3-5 parallel agents.\n\n## When to Use\n\n- Understanding how a feature works\n- Finding where code is defined\n- Tracing data flow through the system\n- Learning codebase architecture\n\n## Quick Start\n\n```bash\n/explore authentication\n```\n\n## Workflow\n\n### Phase 1: Initial Search\n\n```python\n# PARALLEL - Quick searches\nGrep(pattern=\"$ARGUMENTS\", output_mode=\"files_with_matches\")\nGlob(pattern=\"**/*$ARGUMENTS*\")\n```\n\n### Phase 2: Memory Check\n\n```python\nmcp__memory__search_nodes(query=\"$ARGUMENTS\")\nmcp__memory__search_nodes(query=\"architecture\")\n```\n\n### Phase 3: Parallel Deep Exploration\n\nLaunch 4 specialized explorers in ONE message:\n\n1. **Code Structure Explorer** - Files, classes, functions\n2. **Data Flow Explorer** - Entry points, processing, storage\n3. **Backend Architect** - Patterns, integration, dependencies\n4. **Frontend Developer** - Components, state, routes\n\n### Phase 4: AI System Exploration (If Applicable)\n\nFor AI/ML topics, add exploration of:\n- LangGraph workflows\n- Prompt templates\n- RAG pipeline\n- Caching strategies\n\n### Phase 5: Generate Report\n\n```markdown\n# Exploration Report: $ARGUMENTS\n\n## Quick Answer\n[1-2 sentence summary]\n\n## File Locations\n| File | Purpose |\n|------|---------|\n| `path/to/file.py` | [description] |\n\n## Architecture Overview\n[ASCII diagram]\n\n## Data Flow\n1. [Entry]  2. [Processing]  3. [Storage]\n\n## How to Modify\n1. [Step 1]\n2. [Step 2]\n```\n\n## Common Exploration Queries\n\n- \"How does authentication work?\"\n- \"Where are API endpoints defined?\"\n- \"Find all usages of EventBroadcaster\"\n- \"What's the workflow for content analysis?\"\n\n## Key Project Directories\n\n- `backend/app/workflows/` - LangGraph agent workflows\n- `backend/app/api/` - FastAPI endpoints\n- `backend/app/services/` - Business logic\n- `backend/app/db/` - Database models\n- `frontend/src/features/` - React feature modules"
              },
              {
                "name": "fastapi-advanced",
                "description": "FastAPI 2026 advanced patterns including lifespan, dependencies, middleware, settings, and async best practices. Use when building production FastAPI applications.",
                "path": ".claude/skills/fastapi-advanced/SKILL.md",
                "frontmatter": {
                  "name": "fastapi-advanced",
                  "description": "FastAPI 2026 advanced patterns including lifespan, dependencies, middleware, settings, and async best practices. Use when building production FastAPI applications.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "tags": [
                    "fastapi",
                    "python",
                    "async",
                    "middleware",
                    "dependencies",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ]
                  }
                },
                "content": "# FastAPI Advanced Patterns (2026)\n\nProduction-ready FastAPI patterns for modern Python backends.\n\n## When to Use\n\n- Building production FastAPI applications\n- Managing application lifecycle (startup/shutdown)\n- Creating reusable dependencies\n- Implementing custom middleware\n- Configuring settings with validation\n\n## Lifespan Management (2026)\n\n### Modern Lifespan Context Manager\n\n```python\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nimport redis.asyncio as redis\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan with resource management.\"\"\"\n    # Startup\n    app.state.db_engine = create_async_engine(\n        settings.database_url,\n        pool_size=5,\n        max_overflow=10,\n    )\n    app.state.redis = redis.from_url(settings.redis_url)\n\n    # Health check connections\n    async with app.state.db_engine.connect() as conn:\n        await conn.execute(text(\"SELECT 1\"))\n    await app.state.redis.ping()\n\n    yield  # Application runs\n\n    # Shutdown\n    await app.state.db_engine.dispose()\n    await app.state.redis.close()\n\napp = FastAPI(lifespan=lifespan)\n```\n\n### Lifespan with Services\n\n```python\nfrom app.services import EmbeddingsService, LLMService\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Initialize services\n    app.state.embeddings = EmbeddingsService(\n        model=settings.embedding_model,\n        batch_size=100,\n    )\n    app.state.llm = LLMService(\n        providers=[\"openai\", \"anthropic\"],\n        default=\"anthropic\",\n    )\n\n    # Warm up models (optional)\n    await app.state.embeddings.warmup()\n\n    yield\n\n    # Cleanup\n    await app.state.embeddings.close()\n    await app.state.llm.close()\n```\n\n## Dependency Injection Patterns\n\n### Database Session\n\n```python\nfrom typing import AsyncGenerator\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom fastapi import Depends, Request\n\nasync def get_db(request: Request) -> AsyncGenerator[AsyncSession, None]:\n    \"\"\"Yield database session from app state.\"\"\"\n    async with AsyncSession(\n        request.app.state.db_engine,\n        expire_on_commit=False,\n    ) as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n```\n\n### Service Dependencies\n\n```python\nfrom functools import lru_cache\n\nclass AnalysisService:\n    def __init__(\n        self,\n        db: AsyncSession,\n        embeddings: EmbeddingsService,\n        llm: LLMService,\n    ):\n        self.db = db\n        self.embeddings = embeddings\n        self.llm = llm\n\ndef get_analysis_service(\n    db: AsyncSession = Depends(get_db),\n    request: Request = None,\n) -> AnalysisService:\n    return AnalysisService(\n        db=db,\n        embeddings=request.app.state.embeddings,\n        llm=request.app.state.llm,\n    )\n\n@router.post(\"/analyses\")\nasync def create_analysis(\n    data: AnalysisCreate,\n    service: AnalysisService = Depends(get_analysis_service),\n):\n    return await service.create(data)\n```\n\n### Cached Dependencies\n\n```python\nfrom functools import lru_cache\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    database_url: str\n    redis_url: str\n    api_key: str\n\n    model_config = {\"env_file\": \".env\"}\n\n@lru_cache\ndef get_settings() -> Settings:\n    return Settings()\n\n# Usage in dependencies\ndef get_db_url(settings: Settings = Depends(get_settings)) -> str:\n    return settings.database_url\n```\n\n### Authenticated User\n\n```python\nfrom fastapi import Depends, HTTPException, Security\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(\n    credentials: HTTPAuthorizationCredentials = Security(security),\n    db: AsyncSession = Depends(get_db),\n) -> User:\n    token = credentials.credentials\n    payload = decode_jwt(token)\n\n    user = await db.get(User, payload[\"sub\"])\n    if not user:\n        raise HTTPException(401, \"Invalid credentials\")\n    return user\n\nasync def get_admin_user(\n    user: User = Depends(get_current_user),\n) -> User:\n    if not user.is_admin:\n        raise HTTPException(403, \"Admin access required\")\n    return user\n```\n\n## Middleware Patterns\n\n### Request ID Middleware\n\n```python\nimport uuid\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\n\nclass RequestIDMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        request_id = request.headers.get(\"X-Request-ID\", str(uuid.uuid4()))\n        request.state.request_id = request_id\n\n        response = await call_next(request)\n        response.headers[\"X-Request-ID\"] = request_id\n        return response\n\napp.add_middleware(RequestIDMiddleware)\n```\n\n### Timing Middleware\n\n```python\nimport time\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nclass TimingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        start = time.perf_counter()\n        response = await call_next(request)\n        duration = time.perf_counter() - start\n\n        response.headers[\"X-Response-Time\"] = f\"{duration:.3f}s\"\n        return response\n```\n\n### Structured Logging Middleware\n\n```python\nimport structlog\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nlogger = structlog.get_logger()\n\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        log = logger.bind(\n            request_id=getattr(request.state, \"request_id\", None),\n            method=request.method,\n            path=request.url.path,\n        )\n\n        try:\n            response = await call_next(request)\n            log.info(\n                \"request_completed\",\n                status_code=response.status_code,\n            )\n            return response\n        except Exception as exc:\n            log.exception(\"request_failed\", error=str(exc))\n            raise\n```\n\n### CORS Configuration\n\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.cors_origins,\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"],\n    allow_headers=[\"*\"],\n    expose_headers=[\"X-Request-ID\", \"X-Response-Time\"],\n)\n```\n\n## Settings with Pydantic\n\n```python\nfrom pydantic import Field, field_validator, PostgresDsn\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive=False,\n    )\n\n    # Database\n    database_url: PostgresDsn\n    db_pool_size: int = Field(default=5, ge=1, le=20)\n    db_max_overflow: int = Field(default=10, ge=0, le=50)\n\n    # Redis\n    redis_url: str = \"redis://localhost:6379\"\n\n    # API\n    api_key: str = Field(min_length=32)\n    debug: bool = False\n\n    # LLM\n    openai_api_key: str | None = None\n    anthropic_api_key: str | None = None\n\n    @field_validator(\"database_url\", mode=\"before\")\n    @classmethod\n    def validate_database_url(cls, v: str) -> str:\n        if v and \"+asyncpg\" not in v:\n            return v.replace(\"postgresql://\", \"postgresql+asyncpg://\")\n        return v\n\n    @property\n    def async_database_url(self) -> str:\n        return str(self.database_url)\n```\n\n## Exception Handlers\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom sqlalchemy.exc import IntegrityError\nfrom app.core.exceptions import ProblemException\n\n@app.exception_handler(ProblemException)\nasync def problem_exception_handler(request: Request, exc: ProblemException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content=exc.to_problem_detail(),\n        media_type=\"application/problem+json\",\n    )\n\n@app.exception_handler(IntegrityError)\nasync def integrity_error_handler(request: Request, exc: IntegrityError):\n    return JSONResponse(\n        status_code=409,\n        content={\n            \"type\": \"https://api.example.com/problems/conflict\",\n            \"title\": \"Conflict\",\n            \"status\": 409,\n            \"detail\": \"Resource already exists or constraint violated\",\n        },\n        media_type=\"application/problem+json\",\n    )\n```\n\n## Response Optimization\n\n```python\nfrom fastapi.responses import ORJSONResponse\n\n# Use orjson for faster JSON serialization\napp = FastAPI(default_response_class=ORJSONResponse)\n\n# Streaming response\nfrom fastapi.responses import StreamingResponse\n\n@router.get(\"/export\")\nasync def export_data():\n    async def generate():\n        async for chunk in fetch_large_dataset():\n            yield json.dumps(chunk) + \"\\n\"\n\n    return StreamingResponse(\n        generate(),\n        media_type=\"application/x-ndjson\",\n    )\n```\n\n## Health Checks\n\n```python\nfrom fastapi import APIRouter\n\nhealth_router = APIRouter(tags=[\"health\"])\n\n@health_router.get(\"/health\")\nasync def health_check(request: Request):\n    checks = {}\n\n    # Database\n    try:\n        async with request.app.state.db_engine.connect() as conn:\n            await conn.execute(text(\"SELECT 1\"))\n        checks[\"database\"] = \"healthy\"\n    except Exception as e:\n        checks[\"database\"] = f\"unhealthy: {e}\"\n\n    # Redis\n    try:\n        await request.app.state.redis.ping()\n        checks[\"redis\"] = \"healthy\"\n    except Exception as e:\n        checks[\"redis\"] = f\"unhealthy: {e}\"\n\n    status = \"healthy\" if all(v == \"healthy\" for v in checks.values()) else \"unhealthy\"\n    return {\"status\": status, \"checks\": checks}\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER use global state\ndb_session = None  # Global mutable state!\n\n# NEVER block the event loop\ndef sync_db_query():  # Blocking in async context!\n    return session.query(User).all()\n\n# NEVER skip dependency injection\n@router.get(\"/users\")\nasync def get_users():\n    db = create_session()  # Creating session in route!\n    return db.query(User).all()\n\n# NEVER ignore lifespan cleanup\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    app.state.pool = create_pool()\n    yield\n    # Missing cleanup! Pool never closed\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Lifespan | Use `asynccontextmanager` (not events) |\n| Dependencies | Class-based services with DI |\n| Settings | Pydantic Settings with `.env` |\n| Response | ORJSONResponse for performance |\n| Middleware | Order: CORS  RequestID  Timing  Logging |\n| Health | Check all critical dependencies |\n\n## Related Skills\n\n- `clean-architecture` - Service layer patterns\n- `database-schema-designer` - SQLAlchemy models\n- `observability-monitoring` - Logging and metrics\n\n## Capability Details\n\n### lifespan\n**Keywords:** lifespan, startup, shutdown, asynccontextmanager\n**Solves:**\n- FastAPI startup/shutdown\n- Resource management in FastAPI\n\n### dependencies\n**Keywords:** dependency injection, Depends, get_db, service dependency\n**Solves:**\n- FastAPI dependency injection patterns\n- Reusable dependencies\n\n### middleware\n**Keywords:** middleware, request id, timing, cors, logging middleware\n**Solves:**\n- Custom FastAPI middleware\n- Request/response interceptors\n\n### settings\n**Keywords:** settings, pydantic settings, env, configuration\n**Solves:**\n- FastAPI configuration management\n- Environment variables\n\n### health-checks\n**Keywords:** health check, readiness, liveness, health endpoint\n**Solves:**\n- Kubernetes health checks\n- Service health monitoring"
              },
              {
                "name": "fix-issue",
                "description": "Fix GitHub issue with parallel analysis and implementation",
                "path": ".claude/skills/fix-issue/SKILL.md",
                "frontmatter": {
                  "name": "fix-issue",
                  "description": "Fix GitHub issue with parallel analysis and implementation",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "issue",
                    "bug-fix",
                    "github",
                    "debugging"
                  ]
                },
                "content": "# Fix Issue\n\nSystematic issue resolution with 5-7 parallel agents.\n\n## When to Use\n\n- Fixing GitHub issues\n- Bug resolution\n- Issue investigation and implementation\n\n## Quick Start\n\n```bash\n/fix-issue 123\n/fix-issue 456\n```\n\n## Phase 1: Understand the Issue\n\n```bash\n# Get full issue details\ngh issue view $ARGUMENTS --json title,body,labels,assignees,comments\n\n# Check related PRs\ngh pr list --search \"issue:$ARGUMENTS\"\n```\n\n## Phase 2: Create Feature Branch\n\n```bash\ngit checkout dev\ngit pull origin dev\ngit checkout -b issue/$ARGUMENTS-fix\n```\n\n## Phase 3: Memory Check\n\n```python\nmcp__memory__search_nodes(query=\"issue $ARGUMENTS\")\n```\n\n## Phase 4: Parallel Analysis (5 Agents)\n\n| Agent | Task |\n|-------|------|\n| Explore #1 | Root cause analysis |\n| Explore #2 | Impact analysis |\n| backend-system-architect | Backend fix design |\n| frontend-ui-developer | Frontend fix design |\n| code-quality-reviewer | Test requirements |\n\nAll 5 agents run in ONE message, then synthesize fix plan.\n\n## Phase 5: Context7 for Patterns\n\n```python\nmcp__context7__get-library-docs(libraryId=\"/tiangolo/fastapi\", topic=\"relevant\")\nmcp__context7__get-library-docs(libraryId=\"/facebook/react\", topic=\"relevant\")\n```\n\n## Phase 6: Implement the Fix (2 Agents)\n\n| Agent | Task |\n|-------|------|\n| backend/frontend | Implement fix |\n| code-quality-reviewer | Write tests |\n\nRequirements:\n- Make minimal, focused changes\n- Add proper error handling\n- Include type hints\n- DO NOT over-engineer\n\n## Phase 7: Validation\n\n```bash\n# Backend\ncd backend\npoetry run ruff format --check app/\npoetry run ruff check app/\npoetry run ty check app/\npoetry run pytest tests/unit/ -v --tb=short\n\n# Frontend\ncd frontend\nnpm run format:check\nnpm run lint\nnpm run typecheck\nnpm run test\n```\n\n## Phase 8: Commit and PR\n\n```bash\ngit add .\ngit commit -m \"fix(#$ARGUMENTS): [Brief description]\"\ngit push -u origin issue/$ARGUMENTS-fix\ngh pr create --base dev --title \"fix(#$ARGUMENTS): [Brief description]\"\n```\n\n## Summary\n\n**Total Parallel Agents: 7**\n- Phase 4 (Analysis): 5 agents\n- Phase 6 (Implementation): 2 agents\n\n**Agents Used:**\n- 2 Explore (root cause, impact)\n- 1 backend-system-architect\n- 1 frontend-ui-developer\n- 2 code-quality-reviewer\n\n**Workflow:**\n1. Understand issue\n2. Create branch\n3. Parallel analysis\n4. Design fix\n5. Implement + test\n6. Validate\n7. PR with issue reference\n\n## References\n\n- [Commit Template](references/commit-template.md)"
              },
              {
                "name": "function-calling",
                "description": "LLM function calling and tool use patterns. Use when enabling LLMs to call external tools, defining tool schemas, implementing tool execution loops, or getting structured output from LLMs.",
                "path": ".claude/skills/function-calling/SKILL.md",
                "frontmatter": {
                  "name": "function-calling",
                  "description": "LLM function calling and tool use patterns. Use when enabling LLMs to call external tools, defining tool schemas, implementing tool execution loops, or getting structured output from LLMs.",
                  "agent": "llm-integrator"
                },
                "content": "# Function Calling\n\nEnable LLMs to use external tools and return structured data.\n\n## When to Use\n\n- LLM needs to call APIs or databases\n- Extracting structured data from text\n- Building AI agents with tool use\n- Reliable JSON output from LLMs\n\n## Basic Tool Definition (2026 Best Practice)\n\n```python\n# OpenAI format with strict mode (2026 recommended)\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search_documents\",\n        \"description\": \"Search the document database for relevant content\",\n        \"strict\": True,  #  2026: Enables structured output validation\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search query\"\n                },\n                \"limit\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Max results to return\"\n                }\n            },\n            \"required\": [\"query\", \"limit\"],  # All props required when strict\n            \"additionalProperties\": False     #  2026: Required for strict mode\n        }\n    }\n}]\n\n# Note: With strict=True:\n# - All properties must be listed in \"required\"\n# - additionalProperties must be False\n# - No \"default\" values (provide via code instead)\n```\n\n## Tool Execution Loop\n\n```python\nasync def run_with_tools(messages: list, tools: list) -> str:\n    \"\"\"Execute tool calls until LLM returns final answer.\"\"\"\n    while True:\n        response = await llm.chat(messages=messages, tools=tools)\n\n        # Check if LLM wants to call tools\n        if not response.tool_calls:\n            return response.content\n\n        # Execute each tool call\n        for tool_call in response.tool_calls:\n            result = await execute_tool(\n                tool_call.function.name,\n                json.loads(tool_call.function.arguments)\n            )\n\n            # Add tool result to conversation\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"content\": json.dumps(result)\n            })\n\n        # Continue loop (LLM will process tool results)\n\nasync def execute_tool(name: str, args: dict) -> any:\n    \"\"\"Route to appropriate tool implementation.\"\"\"\n    tools = {\n        \"search_documents\": search_documents,\n        \"get_weather\": get_weather,\n        \"calculate\": calculate,\n    }\n    return await tools[name](**args)\n```\n\n## Structured Output (Guaranteed JSON)\n\n```python\nfrom pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n    key_points: list[str]\n\n# OpenAI structured output\nresponse = await client.beta.chat.completions.parse(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Analyze this text...\"}],\n    response_format=Analysis\n)\n\nanalysis = response.choices[0].message.parsed  # Typed Analysis object\n```\n\n## LangChain Tool Binding\n\n```python\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\n@tool\ndef search_documents(query: str, limit: int = 5) -> list[dict]:\n    \"\"\"Search the document database.\n\n    Args:\n        query: Search query string\n        limit: Maximum results to return\n    \"\"\"\n    return db.search(query, limit=limit)\n\n# Bind to model\nllm_with_tools = llm.bind_tools([search_documents])\n\n# Or with structured output\nclass SearchResult(BaseModel):\n    query: str = Field(description=\"The search query used\")\n    results: list[str] = Field(description=\"Matching documents\")\n\nstructured_llm = llm.with_structured_output(SearchResult)\n```\n\n## Parallel Tool Calls\n\n```python\n# OpenAI supports parallel tool calls\nresponse = await llm.chat(\n    messages=messages,\n    tools=tools,\n    parallel_tool_calls=True  # Default in GPT-4o\n)\n\n# Handle multiple calls in parallel\nif response.tool_calls:\n    results = await asyncio.gather(*[\n        execute_tool(tc.function.name, json.loads(tc.function.arguments))\n        for tc in response.tool_calls\n    ])\n```\n\n** 2026 Compatibility Note:**\n```python\n# Structured outputs with strict=True may not work with parallel_tool_calls\n# If using strict mode schemas, disable parallel calls:\nresponse = await llm.chat(\n    messages=messages,\n    tools=tools_with_strict_true,\n    parallel_tool_calls=False  # Required for strict mode reliability\n)\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Tool count | 5-15 max (more = confusion) |\n| Description length | 1-2 sentences |\n| Parameter validation | Use Pydantic/Zod |\n| Error handling | Return error as tool result |\n| **Schema mode** | **`strict: true` (2026 best practice)** |\n| Output format | Structured Outputs > JSON mode |\n| Parallel calls | Disable with strict mode |\n\n## Common Mistakes\n\n- Vague tool descriptions (LLM won't know when to use)\n- No input validation (LLM sends bad params)\n- Missing error handling (crashes on tool failure)\n- Too many tools (LLM gets confused)\n\n## Related Skills\n\n- `agent-loops` - Multi-step tool use with reasoning\n- `llm-streaming` - Streaming with tool calls\n- `structured-output` - Complex output schemas\n\n## Capability Details\n\n### tool-definition\n**Keywords:** tool, function, define tool, tool schema, function schema\n**Solves:**\n- Define tools with clear descriptions\n- Create JSON schemas for tool parameters\n- Document tool behavior for LLM\n\n### tool-execution-loop\n**Keywords:** execution loop, tool call, agent loop, run tool\n**Solves:**\n- Implement tool execution loops\n- Handle multiple tool calls\n- Process tool results\n\n### structured-output\n**Keywords:** structured output, JSON output, typed response, response schema\n**Solves:**\n- Get structured JSON from LLM\n- Enforce output schemas\n- Parse and validate responses\n\n### parallel-tool-calls\n**Keywords:** parallel, concurrent, multiple tools, batch tools\n**Solves:**\n- Execute multiple tools in parallel\n- Handle concurrent tool results\n- Optimize tool call latency\n\n### strict-mode-schemas\n**Keywords:** strict mode, strict schema, additionalProperties, required fields\n**Solves:**\n- Enforce strict JSON schemas\n- Prevent extra fields in output\n- Ensure schema compliance"
              },
              {
                "name": "github-cli",
                "description": "Use when working with GitHub issues, pull requests, or Projects v2 via CLI. Covers gh commands for automation, PR creation, issue management, and workflow scripts.",
                "path": ".claude/skills/github-cli/SKILL.md",
                "frontmatter": {
                  "name": "github-cli",
                  "description": "Use when working with GitHub issues, pull requests, or Projects v2 via CLI. Covers gh commands for automation, PR creation, issue management, and workflow scripts.",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "github",
                    "gh",
                    "cli",
                    "issues",
                    "pr",
                    "projects",
                    "automation",
                    2025
                  ]
                },
                "content": "# GitHub CLI Skill\n\n## Overview\n\nMaster the GitHub CLI (`gh`) for comprehensive project management. This skill covers issue creation, PR workflows, Projects v2 integration, and automation patterns tailored for SkillForge's development workflow.\n\n**When to use:**\n- Creating/managing GitHub issues and PRs\n- Working with GitHub Projects v2 custom fields\n- Automating bulk operations with `gh`\n- Following SkillForge's branch and PR conventions\n- Running GraphQL queries for complex operations\n\n---\n\n## Quick Reference\n\n### Essential Commands\n\n```bash\n# Issue operations\ngh issue create --title \"...\" --body \"...\" --label \"bug\" --milestone \"Sprint 1\"\ngh issue list --state open --label \"backend\" --assignee @me\ngh issue edit 123 --add-label \"high\" --milestone \"v2.0\"\n\n# PR operations\ngh pr create --title \"...\" --body \"...\" --base dev --reviewer @teammate\ngh pr checks 456 --watch              # Watch CI status\ngh pr merge 456 --squash --delete-branch\ngh pr merge 456 --auto --squash       # Auto-merge when approved\n\n# Project operations\ngh project list --owner @me\ngh project item-add 1 --owner @me --url https://github.com/org/repo/issues/123\n\n# API operations\ngh api repos/:owner/:repo/issues --jq '.[].title'\ngh api graphql -f query='...'\n```\n\n### JSON Output + jq Patterns\n\n```bash\n# Get issue numbers matching criteria\ngh issue list --json number,labels --jq '[.[] | select(.labels[].name == \"bug\")] | .[].number'\n\n# PR summary\ngh pr list --json number,title,author --jq '.[] | \"\\(.number): \\(.title) by \\(.author.login)\"'\n\n# Count open PRs\ngh pr list --json state --jq '[.[] | select(.state == \"OPEN\")] | length'\n```\n\n---\n\n## SkillForge Workflow\n\n### Branch Naming Convention\n\n```bash\n# For GitHub issues\nissue/<number>-<brief-description>\n# Examples:\nissue/372-langfuse-migration\nissue/385-langfuse-mcp-integration\n\n# For features without issues\nfeature/<description>\n\n# For bug fixes\nfix/<description>\n```\n\n### Complete Feature Workflow\n\n```bash\n# 1. Create issue (if not exists)\nISSUE_URL=$(gh issue create \\\n  --title \"feat: Add hybrid search with PGVector\" \\\n  --body \"$(cat <<'EOF'\n## Description\nImplement hybrid search combining BM25 and vector similarity.\n\n## Acceptance Criteria\n- [ ] HNSW index on chunks table\n- [ ] RRF fusion algorithm\n- [ ] Metadata boosting for section titles\n\n## Technical Notes\nSee `.claude/skills/pgvector-search/SKILL.md`\nEOF\n)\" \\\n  --label \"enhancement,backend\" \\\n  --milestone \"Sprint 5: Library & Search\" \\\n  --json url --jq '.url')\n\nISSUE_NUM=$(echo \"$ISSUE_URL\" | grep -o '[0-9]*$')\n\n# 2. Create feature branch\ngit checkout dev && git pull origin dev\ngit checkout -b \"issue/${ISSUE_NUM}-pgvector-hybrid-search\"\n\n# 3. Do work, commit with conventional commits\ngit add . && git commit -m \"feat(#${ISSUE_NUM}): Implement hybrid search with RRF fusion\"\n\n# 4. Push and create PR\ngit push -u origin \"issue/${ISSUE_NUM}-pgvector-hybrid-search\"\n\ngh pr create \\\n  --title \"feat(#${ISSUE_NUM}): Implement hybrid search with PGVector\" \\\n  --body \"$(cat <<'EOF'\n## Summary\n- Added HNSW index to chunks table\n- Implemented RRF fusion algorithm\n- Added metadata boosting for section titles\n\n## Test Plan\n- [ ] Unit tests for search service\n- [ ] Golden dataset ranking evaluation\n\nCloses #${ISSUE_NUM}\n\n Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\nEOF\n)\" \\\n  --base dev \\\n  --label \"enhancement,backend\"\n```\n\n### PR Commit Message Format\n\n```bash\n# Format: type(#issue): description\nfeat(#372): Implement Langfuse tracing integration\nfix(#345): Resolve artifact page rendering bug\ndocs(#336): Update ROADMAP with multimodal milestone\nrefactor(#391): Split useAnalysisProgress hook\ntest(#342): Add 80% coverage for tutor feature\nchore(#376): Upgrade December 2025 dependencies\n```\n\n### Project Board Integration\n\nSkillForge uses GitHub Projects v2 with custom fields. After creating an issue:\n\n```bash\n# Add to project\nITEM_ID=$(gh project item-add 1 --owner yonatangross \\\n  --url \"https://github.com/ArieGoldkin/SkillForge/issues/${ISSUE_NUM}\" \\\n  --format json | jq -r '.id')\n\n# Set Status to \"In Development\"\n# See templates/skillforge-project-config.json for field IDs\n```\n\n> **Note:** Setting custom fields requires GraphQL. See `references/projects-v2.md`.\n\n---\n\n## Labels Reference\n\n### Priority Labels\n| Label | Description | Color |\n|-------|-------------|-------|\n| ` critical` | Critical blocker | Red |\n| ` high` | High priority, sprint critical | Orange |\n| ` medium` | Medium priority | Yellow |\n| ` low` | Low priority | Blue |\n\n### Domain Labels\n| Label | Description |\n|-------|-------------|\n| ` backend` | Backend (Python, FastAPI, LangGraph) |\n| ` frontend` | Frontend (React, TypeScript) |\n| ` langgraph` | LangGraph workflows & agents |\n| ` database` | Database schema, migrations, PGVector |\n| ` sse` | SSE streaming & real-time |\n\n### Type Labels\n| Label | Description |\n|-------|-------------|\n| ` feature` | New features |\n| ` bug` | Bug fixes |\n| ` refactor` | Code improvements |\n| ` documentation` | Documentation updates |\n| ` evaluation` | Evaluation framework & testing |\n\n---\n\n## Milestones Reference\n\nCurrent active milestones:\n\n| Milestone | Due Date | Focus |\n|-----------|----------|-------|\n|  Triple-Consumer Artifacts | Dec 25, 2025 | Schema enhancement |\n|  Langfuse Migration | Jan 7, 2026 | Observability |\n|  Tutoring System | Feb 5, 2026 | Socratic tutoring |\n|  Staging/Production | Feb 26, 2026 | Deployment |\n|  Multimodal Intelligence | May 7, 2026 | Vision analysis |\n\n---\n\n## Detailed Guides\n\nFor specific capabilities, see:\n\n- **Issue Management**: `references/issue-management.md`\n  - Bulk operations, templates, parent/sub-issues\n\n- **PR Workflows**: `references/pr-workflows.md`\n  - Review workflow, merge strategies, auto-merge\n\n- **Projects v2**: `references/projects-v2.md`\n  - Custom fields, GraphQL mutations, SkillForge field IDs\n\n- **GraphQL API**: `references/graphql-api.md`\n  - Complex queries, pagination, bulk operations\n\n- **Automation Patterns**: `references/automation-patterns.md`\n  - Aliases, error handling, rate limits, scripts\n\n---\n\n## Best Practices\n\n1. **Always use `--json` for scripting** - Parse with `--jq` for reliability\n2. **Non-interactive mode for automation** - Use `--title`, `--body` flags\n3. **Check rate limits before bulk operations** - `gh api rate_limit`\n4. **Use heredocs for multi-line content** - \"--body \\\"\\$(cat <<EOF...EOF)\\\"\"\n5. **Link issues in PRs** - `Closes #123`, `Fixes #456`\n6. **Add verification checklists** - Track test plan completion\n7. **Never commit to dev/main directly** - Always use feature branches + PRs\n\n## Capability Details\n\n### issue-management\n**Keywords:** gh issue, create issue, issue labels, milestone, assignee, bulk issues\n**Solves:**\n- How do I create a GitHub issue?\n- Add labels to multiple issues\n- Link issue to milestone\n- Create issue from template\n\n### pr-workflows\n**Keywords:** gh pr, pull request, create PR, review PR, merge, auto-merge, pr checks\n**Solves:**\n- How do I create a PR with gh?\n- Wait for PR checks to pass\n- Auto-merge when approved\n- Request PR review\n\n### projects-v2\n**Keywords:** gh project, project board, custom fields, Status, Priority, Sprint, project views\n**Solves:**\n- How do I add issue to project board?\n- Set project Status field\n- Move item to In Progress\n- Query project items\n\n### graphql-api\n**Keywords:** gh api graphql, GraphQL query, custom query, bulk operations, pagination\n**Solves:**\n- How do I run GraphQL queries with gh?\n- Get all open issues with custom fields\n- Bulk update project items\n- Complex GitHub API queries\n\n### automation-patterns\n**Keywords:** gh automation, gh alias, bulk operations, jq, scripting, retry, rate limit\n**Solves:**\n- Create reusable gh aliases\n- Handle rate limits in scripts\n- Bulk close stale issues\n- Automated workflow scripts\n\n### skillforge-workflow\n**Keywords:** SkillForge, feature branch, conventional commits, PR template, dev branch\n**Solves:**\n- SkillForge branch naming convention\n- Create issue and feature branch\n- SkillForge PR workflow\n- Project field IDs for SkillForge"
              },
              {
                "name": "golden-dataset-curation",
                "description": "Use when creating or improving golden datasets for AI evaluation. Defines quality criteria, curation workflows, and multi-agent analysis patterns for test data.",
                "path": ".claude/skills/golden-dataset-curation/SKILL.md",
                "frontmatter": {
                  "name": "golden-dataset-curation",
                  "description": "Use when creating or improving golden datasets for AI evaluation. Defines quality criteria, curation workflows, and multi-agent analysis patterns for test data.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "golden-dataset",
                    "curation",
                    "quality",
                    "multi-agent",
                    "langfuse",
                    2025
                  ]
                },
                "content": "# Golden Dataset Curation\n\n**Curate high-quality documents for the golden dataset with multi-agent validation**\n\n## Overview\n\nThis skill provides patterns and workflows for **adding new documents** to the golden dataset with thorough quality analysis. It complements `golden-dataset-management` which handles backup/restore.\n\n**When to use this skill:**\n- Adding new documents to the golden dataset\n- Classifying content types and difficulty levels\n- Generating test queries for new documents\n- Running multi-agent quality analysis\n\n---\n\n## Content Type Classification\n\n### Supported Types\n\n| Type | Description | Quality Focus |\n|------|-------------|---------------|\n| `article` | Technical articles, blog posts | Depth, accuracy, actionability |\n| `tutorial` | Step-by-step guides | Completeness, clarity, code quality |\n| `research_paper` | Academic papers, whitepapers | Rigor, citations, methodology |\n| `documentation` | API docs, reference materials | Accuracy, completeness, examples |\n| `video_transcript` | Transcribed video content | Structure, coherence, key points |\n| `code_repository` | README, code analysis | Code quality, documentation |\n\n### Classification Criteria\n\n```python\n# Content Type Decision Tree\ndef classify_content_type(content: str, source_url: str) -> str:\n    \"\"\"Classify content type based on structure and source.\"\"\"\n\n    # URL-based hints\n    if \"arxiv.org\" in source_url or \"papers\" in source_url:\n        return \"research_paper\"\n    if \"docs.\" in source_url or \"/api/\" in source_url:\n        return \"documentation\"\n    if \"github.com\" in source_url:\n        return \"code_repository\"\n\n    # Content-based analysis\n    if has_step_by_step_structure(content):\n        return \"tutorial\"\n    if has_academic_structure(content):  # Abstract, methodology, results\n        return \"research_paper\"\n\n    # Default\n    return \"article\"\n```\n\n---\n\n## Difficulty Classification\n\n### Stratification Levels\n\n| Level | Semantic Complexity | Expected Retrieval Score | Characteristics |\n|-------|---------------------|--------------------------|-----------------|\n| **trivial** | Direct keyword match | >0.85 | Technical terms, exact phrases |\n| **easy** | Common synonyms | >0.70 | Well-known concepts, slight variations |\n| **medium** | Paraphrased intent | >0.55 | Conceptual queries, multi-topic |\n| **hard** | Multi-hop reasoning | >0.40 | Cross-domain, comparative analysis |\n| **adversarial** | Edge cases | Graceful degradation | Robustness tests, off-domain |\n\n### Classification Factors\n\n```python\ndef classify_difficulty(document: dict) -> str:\n    \"\"\"Classify document difficulty for retrieval testing.\"\"\"\n\n    factors = {\n        \"technical_density\": count_technical_terms(document[\"content\"]),\n        \"section_count\": len(document.get(\"sections\", [])),\n        \"cross_references\": count_cross_references(document),\n        \"abstraction_level\": assess_abstraction(document),\n        \"domain_specificity\": assess_domain_specificity(document),\n    }\n\n    # Scoring rubric\n    score = 0\n    if factors[\"technical_density\"] > 50:\n        score += 2\n    if factors[\"section_count\"] > 10:\n        score += 1\n    if factors[\"cross_references\"] > 5:\n        score += 2\n    if factors[\"abstraction_level\"] == \"high\":\n        score += 2\n\n    # Map score to difficulty\n    if score <= 2:\n        return \"trivial\"\n    elif score <= 4:\n        return \"easy\"\n    elif score <= 6:\n        return \"medium\"\n    elif score <= 8:\n        return \"hard\"\n    else:\n        return \"adversarial\"\n```\n\n---\n\n## Quality Evaluation Dimensions\n\n### 1. Accuracy (Weight: 0.25)\n\n**What it measures:** Factual correctness, up-to-date information\n\n**Evaluation criteria:**\n- Technical claims are verifiable\n- Code examples are syntactically correct\n- No outdated information (check dates, versions)\n- Sources/citations where applicable\n\n**Thresholds:**\n- Perfect: 0.95-1.0 (all claims verifiable)\n- Acceptable: 0.70-0.94 (minor inaccuracies)\n- Failing: <0.70 (significant errors)\n\n### 2. Coherence (Weight: 0.20)\n\n**What it measures:** Logical flow, structure, readability\n\n**Evaluation criteria:**\n- Clear introduction and conclusion\n- Logical section ordering\n- Smooth transitions between topics\n- Consistent terminology\n\n**Thresholds:**\n- Perfect: 0.90-1.0 (professional quality)\n- Acceptable: 0.60-0.89 (readable but rough)\n- Failing: <0.60 (confusing structure)\n\n### 3. Depth (Weight: 0.25)\n\n**What it measures:** Thoroughness, detail level, comprehensiveness\n\n**Evaluation criteria:**\n- Covers topic comprehensively\n- Includes edge cases and caveats\n- Provides context and background\n- Appropriate level of detail for audience\n\n**Thresholds:**\n- Perfect: 0.90-1.0 (exhaustive coverage)\n- Acceptable: 0.55-0.89 (covers main points)\n- Failing: <0.55 (superficial treatment)\n\n### 4. Relevance (Weight: 0.30)\n\n**What it measures:** Alignment with SkillForge's technical domains\n\n**Target domains:**\n- AI/ML (LangGraph, RAG, agents, embeddings)\n- Backend (FastAPI, PostgreSQL, APIs)\n- Frontend (React, TypeScript)\n- DevOps (Docker, Kubernetes, CI/CD)\n- Security (OWASP, authentication)\n\n**Thresholds:**\n- Perfect: 0.95-1.0 (core domain, highly relevant)\n- Acceptable: 0.70-0.94 (related domain)\n- Failing: <0.70 (off-topic for SkillForge)\n\n---\n\n## Multi-Agent Analysis Pipeline\n\n### Architecture\n\n```\n\n                    CURATION PIPELINE                             \n\n                                                                  \n  INPUT: URL/Content                                             \n                                                                 \n                                                                 \n                                              \n    FETCH AGENT      WebFetch or file read                     \n    (sequential)     Extract structure, detect type            \n                                              \n                                                                 \n                                                                 \n     \n    PARALLEL ANALYSIS AGENTS                                   \n          \n     Quality     Difficulty  Domain      Query       \n     Evaluator   Classifier  Tagger      Generator   \n          \n                                                          \n     \n                                                             \n                       \n                                                               \n     \n    CONSENSUS AGGREGATOR                                       \n     Weighted quality score                                   \n     Confidence level (agent agreement)                       \n     Final recommendation: include/review/exclude             \n     \n                                                                 \n                                                                 \n                                              \n    USER APPROVAL    Show scores, get confirmation             \n                                              \n                                                                 \n                                                                 \n  OUTPUT: Curated document entry                                 \n                                                                  \n\n```\n\n### Agent Specifications\n\n#### Quality Evaluator Agent\n\n```python\nTask(\n    subagent_type=\"code-quality-reviewer\",\n    prompt=\"\"\"GOLDEN DATASET QUALITY EVALUATION\n\n    Evaluate this content for golden dataset inclusion:\n\n    Content: {content_preview}\n    Source: {source_url}\n    Type: {content_type}\n\n    Score these dimensions (0.0-1.0):\n\n    1. ACCURACY (weight 0.25)\n       - Technical correctness\n       - Code validity\n       - Up-to-date information\n\n    2. COHERENCE (weight 0.20)\n       - Logical structure\n       - Clear flow\n       - Consistent terminology\n\n    3. DEPTH (weight 0.25)\n       - Comprehensive coverage\n       - Edge cases mentioned\n       - Appropriate detail level\n\n    4. RELEVANCE (weight 0.30)\n       - Alignment with AI/ML, backend, frontend, DevOps\n       - Practical applicability\n       - Technical value\n\n    Output JSON:\n    {\n        \"accuracy\": {\"score\": 0.X, \"rationale\": \"...\"},\n        \"coherence\": {\"score\": 0.X, \"rationale\": \"...\"},\n        \"depth\": {\"score\": 0.X, \"rationale\": \"...\"},\n        \"relevance\": {\"score\": 0.X, \"rationale\": \"...\"},\n        \"weighted_total\": 0.X,\n        \"recommendation\": \"include|review|exclude\"\n    }\n    \"\"\",\n    run_in_background=True\n)\n```\n\n#### Difficulty Classifier Agent\n\n```python\nTask(\n    subagent_type=\"Explore\",\n    prompt=\"\"\"DIFFICULTY CLASSIFICATION\n\n    Analyze document complexity for retrieval testing:\n\n    Content: {content_preview}\n    Sections: {section_titles}\n\n    Assess these factors:\n    1. Technical term density (count specialized terms)\n    2. Section complexity (nesting depth, count)\n    3. Cross-domain references (links between topics)\n    4. Abstraction level (concrete vs conceptual)\n    5. Query ambiguity potential (how many ways to ask about this?)\n\n    Output JSON:\n    {\n        \"difficulty\": \"trivial|easy|medium|hard|adversarial\",\n        \"factors\": {\n            \"technical_density\": \"low|medium|high\",\n            \"structure_complexity\": \"simple|moderate|complex\",\n            \"cross_references\": \"none|some|many\",\n            \"abstraction\": \"concrete|mixed|abstract\"\n        },\n        \"expected_retrieval_score\": 0.X,\n        \"rationale\": \"...\"\n    }\n    \"\"\"\n)\n```\n\n#### Domain Tagger Agent\n\n```python\nTask(\n    subagent_type=\"Explore\",\n    prompt=\"\"\"DOMAIN TAGGING\n\n    Extract domain tags for this content:\n\n    Content: {content_preview}\n    Source: {source_url}\n\n    Primary domains (pick 1-2):\n    - ai-ml (LLM, agents, RAG, embeddings, LangGraph)\n    - backend (FastAPI, PostgreSQL, APIs, microservices)\n    - frontend (React, TypeScript, UI/UX)\n    - devops (Docker, K8s, CI/CD, infrastructure)\n    - security (auth, OWASP, encryption)\n    - databases (SQL, NoSQL, vector DBs)\n    - testing (pytest, playwright, TDD)\n\n    Secondary tags (pick 3-5):\n    - Specific technologies mentioned\n    - Patterns/concepts covered\n    - Use cases addressed\n\n    Output JSON:\n    {\n        \"primary_domains\": [\"ai-ml\", \"backend\"],\n        \"tags\": [\"langraph\", \"agents\", \"tool-use\", \"fastapi\"],\n        \"confidence\": 0.X\n    }\n    \"\"\"\n)\n```\n\n#### Query Generator Agent\n\n```python\nTask(\n    subagent_type=\"Explore\",\n    prompt=\"\"\"TEST QUERY GENERATION\n\n    Generate test queries for this golden dataset document:\n\n    Document ID: {document_id}\n    Title: {title}\n    Sections: {section_titles}\n    Content preview: {content_preview}\n\n    Generate 3-5 test queries with varied difficulty:\n\n    1. At least 1 TRIVIAL query (exact keyword match)\n    2. At least 1 EASY query (synonyms, common terms)\n    3. At least 1 MEDIUM query (paraphrased intent)\n    4. Optional: 1 HARD query (cross-section reasoning)\n\n    For each query specify:\n    - Query text\n    - Expected sections to match\n    - Difficulty level\n    - Minimum expected score\n\n    Output JSON:\n    {\n        \"queries\": [\n            {\n                \"id\": \"q-{doc-id}-{num}\",\n                \"query\": \"How to implement X with Y?\",\n                \"difficulty\": \"medium\",\n                \"expected_chunks\": [\"section-id-1\", \"section-id-2\"],\n                \"min_score\": 0.55,\n                \"modes\": [\"semantic\", \"hybrid\"],\n                \"category\": \"specific\",\n                \"description\": \"Tests retrieval of X implementation details\"\n            }\n        ]\n    }\n    \"\"\"\n)\n```\n\n---\n\n## Consensus Aggregation\n\n### Aggregation Logic\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Literal\n\n@dataclass\nclass CurationConsensus:\n    \"\"\"Aggregated result from multi-agent analysis.\"\"\"\n\n    quality_score: float  # Weighted average (0-1)\n    confidence: float     # Agent agreement (0-1)\n    decision: Literal[\"include\", \"review\", \"exclude\"]\n\n    # Individual scores\n    accuracy: float\n    coherence: float\n    depth: float\n    relevance: float\n\n    # Classification results\n    content_type: str\n    difficulty: str\n    tags: list[str]\n\n    # Generated queries\n    suggested_queries: list[dict]\n\n    # Warnings\n    warnings: list[str]\n\ndef aggregate_results(\n    quality_result: dict,\n    difficulty_result: dict,\n    domain_result: dict,\n    query_result: dict,\n) -> CurationConsensus:\n    \"\"\"Aggregate multi-agent results into consensus.\"\"\"\n\n    # Calculate weighted quality score\n    q = quality_result\n    quality_score = (\n        q[\"accuracy\"][\"score\"] * 0.25 +\n        q[\"coherence\"][\"score\"] * 0.20 +\n        q[\"depth\"][\"score\"] * 0.25 +\n        q[\"relevance\"][\"score\"] * 0.30\n    )\n\n    # Calculate confidence (variance-based)\n    scores = [\n        q[\"accuracy\"][\"score\"],\n        q[\"coherence\"][\"score\"],\n        q[\"depth\"][\"score\"],\n        q[\"relevance\"][\"score\"],\n    ]\n    variance = sum((s - quality_score)**2 for s in scores) / len(scores)\n    confidence = 1.0 - min(variance * 4, 1.0)  # Scale variance to confidence\n\n    # Decision thresholds\n    if quality_score >= 0.75 and confidence >= 0.7:\n        decision = \"include\"\n    elif quality_score >= 0.55:\n        decision = \"review\"\n    else:\n        decision = \"exclude\"\n\n    # Collect warnings\n    warnings = []\n    if q[\"accuracy\"][\"score\"] < 0.6:\n        warnings.append(\"Low accuracy score - verify technical claims\")\n    if q[\"relevance\"][\"score\"] < 0.7:\n        warnings.append(\"Low relevance - may be off-topic for SkillForge\")\n    if domain_result[\"confidence\"] < 0.7:\n        warnings.append(\"Low confidence in domain classification\")\n\n    return CurationConsensus(\n        quality_score=quality_score,\n        confidence=confidence,\n        decision=decision,\n        accuracy=q[\"accuracy\"][\"score\"],\n        coherence=q[\"coherence\"][\"score\"],\n        depth=q[\"depth\"][\"score\"],\n        relevance=q[\"relevance\"][\"score\"],\n        content_type=difficulty_result.get(\"content_type\", \"article\"),\n        difficulty=difficulty_result[\"difficulty\"],\n        tags=domain_result[\"tags\"],\n        suggested_queries=query_result[\"queries\"],\n        warnings=warnings,\n    )\n```\n\n---\n\n## Langfuse Integration\n\n### Trace Structure\n\n```python\n# Langfuse trace for curation workflow\ntrace = langfuse.trace(\n    name=\"golden-dataset-curation\",\n    metadata={\n        \"source_url\": url,\n        \"document_id\": doc_id,\n    }\n)\n\n# Spans for each agent\nwith trace.span(name=\"fetch_content\") as span:\n    content = fetch_url(url)\n    span.update(output={\"length\": len(content)})\n\nwith trace.span(name=\"quality_evaluation\") as span:\n    quality_result = await run_quality_agent(content)\n    span.update(output=quality_result)\n    # Log individual dimension scores\n    trace.score(name=\"accuracy\", value=quality_result[\"accuracy\"][\"score\"])\n    trace.score(name=\"coherence\", value=quality_result[\"coherence\"][\"score\"])\n    trace.score(name=\"depth\", value=quality_result[\"depth\"][\"score\"])\n    trace.score(name=\"relevance\", value=quality_result[\"relevance\"][\"score\"])\n\n# Final aggregated score\ntrace.score(name=\"quality_total\", value=consensus.quality_score)\ntrace.event(\n    name=\"curation_decision\",\n    metadata={\"decision\": consensus.decision}\n)\n```\n\n### Prompt Management\n\nAll curation prompts are managed in Langfuse:\n\n| Prompt Name | Purpose | Tags |\n|-------------|---------|------|\n| `golden-content-classifier` | Classify content_type | `golden-dataset`, `classification` |\n| `golden-difficulty-classifier` | Assign difficulty | `golden-dataset`, `difficulty` |\n| `golden-domain-tagger` | Extract tags | `golden-dataset`, `tagging` |\n| `golden-query-generator` | Generate queries | `golden-dataset`, `query-gen` |\n\n---\n\n## Best Practices\n\n### 1. Quality Thresholds\n\n```yaml\n# Recommended thresholds for golden dataset inclusion\nminimum_quality_score: 0.70\nminimum_confidence: 0.65\nrequired_tags: 2  # At least 2 domain tags\nrequired_queries: 3  # At least 3 test queries\n```\n\n### 2. Coverage Balance\n\nMaintain balanced coverage across:\n- Content types (don't over-index on articles)\n- Difficulty levels (need trivial AND hard)\n- Domains (spread across AI/ML, backend, frontend, etc.)\n\n### 3. Duplicate Prevention\n\nBefore adding:\n1. Check URL against existing `source_url_map.json`\n2. Run semantic similarity against existing document embeddings\n3. Warn if >80% similar to existing document\n\n### 4. Provenance Tracking\n\nAlways record:\n- Source URL (canonical)\n- Curation date\n- Agent scores (for audit trail)\n- Langfuse trace ID\n\n---\n\n## Related Skills\n\n- `golden-dataset-management` - Backup/restore operations\n- `golden-dataset-validation` - Validation rules and checks\n- `langfuse-observability` - Tracing patterns\n- `pgvector-search` - Duplicate detection\n\n---\n\n**Version:** 1.0.0 (December 2025)\n**Issue:** #599\n\n## Capability Details\n\n### content-classification\n**Keywords:** content type, classification, document type, golden dataset\n**Solves:**\n- Classify document content types for golden dataset\n- Categorize entries by domain and purpose\n- Identify content requiring special handling\n\n### difficulty-stratification\n**Keywords:** difficulty, stratification, complexity level, challenge rating\n**Solves:**\n- Assign difficulty levels to golden dataset entries\n- Ensure balanced difficulty distribution\n- Identify edge cases and challenging examples\n\n### quality-evaluation\n**Keywords:** quality, evaluation, quality dimensions, quality criteria\n**Solves:**\n- Evaluate entry quality against defined criteria\n- Score entries on multiple quality dimensions\n- Identify entries needing improvement\n\n### multi-agent-analysis\n**Keywords:** multi-agent, parallel analysis, consensus, agent evaluation\n**Solves:**\n- Run parallel agent evaluations on entries\n- Aggregate consensus from multiple analysts\n- Resolve disagreements in classifications"
              },
              {
                "name": "golden-dataset-management",
                "description": "Use when backing up, restoring, or validating golden datasets. Prevents data loss and ensures test data integrity for AI/ML evaluation systems.",
                "path": ".claude/skills/golden-dataset-management/SKILL.md",
                "frontmatter": {
                  "name": "golden-dataset-management",
                  "description": "Use when backing up, restoring, or validating golden datasets. Prevents data loss and ensures test data integrity for AI/ML evaluation systems.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "golden-dataset",
                    "backup",
                    "data-protection",
                    "testing",
                    "regression",
                    2025
                  ],
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob",
                    "Bash"
                  ]
                },
                "content": "# Golden Dataset Management\n\n**Protect and maintain high-quality test datasets for AI/ML systems**\n\n## Overview\n\nA **golden dataset** is a curated collection of high-quality examples used for:\n- **Regression testing:** Ensure new code doesn't break existing functionality\n- **Retrieval evaluation:** Measure search quality (precision, recall, MRR)\n- **Model benchmarking:** Compare different models/approaches\n- **Reproducibility:** Consistent results across environments\n\n**When to use this skill:**\n- Building test datasets for RAG systems\n- Implementing backup/restore for critical data\n- Validating data integrity (URL contracts, embeddings)\n- Migrating data between environments\n\n---\n\n## SkillForge's Golden Dataset\n\n**Stats (Production):**\n- **98 analyses** (completed content analyses)\n- **415 chunks** (embedded text segments)\n- **203 test queries** (with expected results)\n- **91.6% pass rate** (retrieval quality metric)\n\n**Purpose:**\n- Test hybrid search (vector + BM25 + RRF)\n- Validate metadata boosting strategies\n- Detect regressions in retrieval quality\n- Benchmark new embedding models\n\n---\n\n## Core Concepts\n\n### 1. Data Integrity Contracts\n\n**The URL Contract:**\n\nGolden dataset analyses MUST store **real canonical URLs**, not placeholders.\n\n```python\n# WRONG - Placeholder URL (breaks restore)\nanalysis.url = \"https://skillforge.dev/placeholder/123\"\n\n# CORRECT - Real canonical URL (enables re-fetch if needed)\nanalysis.url = \"https://docs.python.org/3/library/asyncio.html\"\n```\n\n**Why this matters:**\n- Enables re-fetching content if embeddings need regeneration\n- Allows validation that source content hasn't changed\n- Provides audit trail for data provenance\n\n**Verification:**\n```python\n# Check for placeholder URLs\ndef verify_url_contract(analyses: list[Analysis]) -> list[str]:\n    \"\"\"Find analyses with placeholder URLs.\"\"\"\n    invalid = []\n    for analysis in analyses:\n        if \"skillforge.dev\" in analysis.url or \"placeholder\" in analysis.url:\n            invalid.append(analysis.id)\n    return invalid\n```\n\n---\n\n### 2. Backup Strategies\n\n#### Strategy 1: JSON Backup (Recommended)\n\n**Pros:**\n- Version controlled (commit to git)\n- Human-readable (easy to inspect)\n- Portable (works across DB versions)\n- Incremental diffs (see what changed)\n\n**Cons:**\n- Must regenerate embeddings on restore\n- Larger file size than SQL dump\n\n**SkillForge uses JSON backup.**\n\n#### Strategy 2: SQL Dump\n\n**Pros:**\n- Fast restore (includes embeddings)\n- Exact replica (binary-identical)\n- Native PostgreSQL format\n\n**Cons:**\n- Not version controlled (binary format)\n- DB version dependent\n- No easy inspection\n\n**Use case:** Local snapshots, not version control.\n\n---\n\n### 3. Backup Format\n\n```json\n{\n  \"version\": \"1.0\",\n  \"created_at\": \"2025-12-19T10:30:00Z\",\n  \"metadata\": {\n    \"total_analyses\": 98,\n    \"total_chunks\": 415,\n    \"total_artifacts\": 98\n  },\n  \"analyses\": [\n    {\n      \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"url\": \"https://docs.python.org/3/library/asyncio.html\",\n      \"content_type\": \"documentation\",\n      \"status\": \"completed\",\n      \"created_at\": \"2025-11-15T08:20:00Z\",\n      \"findings\": [\n        {\n          \"agent\": \"security_agent\",\n          \"category\": \"best_practices\",\n          \"content\": \"Always use asyncio.run() for top-level entry point\",\n          \"confidence\": 0.92\n        }\n      ],\n      \"chunks\": [\n        {\n          \"id\": \"7c9e6679-7425-40de-944b-e07fc1f90ae7\",\n          \"content\": \"asyncio is a library to write concurrent code...\",\n          \"section_title\": \"Introduction to asyncio\",\n          \"section_path\": \"docs/python/asyncio/intro.md\",\n          \"content_type\": \"paragraph\",\n          \"chunk_index\": 0\n          // Note: embedding NOT included (regenerated on restore)\n        }\n      ],\n      \"artifact\": {\n        \"id\": \"a1b2c3d4-e5f6-4a5b-8c7d-9e8f7a6b5c4d\",\n        \"summary\": \"Comprehensive guide to asyncio...\",\n        \"key_findings\": [\"...\"],\n        \"metadata\": {}\n      }\n    }\n  ]\n}\n```\n\n**Key Design Decisions:**\n- Embeddings excluded (regenerate on restore with current model)\n- Nested structure (analyses  chunks  artifacts)\n- Metadata for validation\n- ISO timestamps for reproducibility\n\n---\n\n## Backup Implementation\n\n### Script Structure\n\n```python\n# backend/scripts/backup_golden_dataset.py\n\nimport asyncio\nimport json\nfrom datetime import datetime, UTC\nfrom pathlib import Path\nfrom sqlalchemy import select\nfrom app.db.session import get_session\nfrom app.db.models import Analysis, Chunk, Artifact\n\nBACKUP_DIR = Path(\"backend/data\")\nBACKUP_FILE = BACKUP_DIR / \"golden_dataset_backup.json\"\nMETADATA_FILE = BACKUP_DIR / \"golden_dataset_metadata.json\"\n\nasync def backup_golden_dataset():\n    \"\"\"Backup golden dataset to JSON.\"\"\"\n\n    async with get_session() as session:\n        # Fetch all completed analyses\n        query = (\n            select(Analysis)\n            .where(Analysis.status == \"completed\")\n            .order_by(Analysis.created_at)\n        )\n        result = await session.execute(query)\n        analyses = result.scalars().all()\n\n        # Serialize to JSON\n        backup_data = {\n            \"version\": \"1.0\",\n            \"created_at\": datetime.now(UTC).isoformat(),\n            \"metadata\": {\n                \"total_analyses\": len(analyses),\n                \"total_chunks\": sum(len(a.chunks) for a in analyses),\n                \"total_artifacts\": len([a for a in analyses if a.artifact])\n            },\n            \"analyses\": [\n                serialize_analysis(a) for a in analyses\n            ]\n        }\n\n        # Write backup file\n        BACKUP_DIR.mkdir(exist_ok=True)\n        with open(BACKUP_FILE, \"w\") as f:\n            json.dump(backup_data, f, indent=2, default=str)\n\n        # Write metadata file (quick stats)\n        with open(METADATA_FILE, \"w\") as f:\n            json.dump(backup_data[\"metadata\"], f, indent=2)\n\n        print(f\" Backup completed: {BACKUP_FILE}\")\n        print(f\"   Analyses: {backup_data['metadata']['total_analyses']}\")\n        print(f\"   Chunks: {backup_data['metadata']['total_chunks']}\")\n\ndef serialize_analysis(analysis: Analysis) -> dict:\n    \"\"\"Serialize analysis to dict.\"\"\"\n    return {\n        \"id\": str(analysis.id),\n        \"url\": analysis.url,\n        \"content_type\": analysis.content_type,\n        \"status\": analysis.status,\n        \"created_at\": analysis.created_at.isoformat(),\n        \"findings\": [serialize_finding(f) for f in analysis.findings],\n        \"chunks\": [serialize_chunk(c) for c in analysis.chunks],\n        \"artifact\": serialize_artifact(analysis.artifact) if analysis.artifact else None\n    }\n\ndef serialize_chunk(chunk: Chunk) -> dict:\n    \"\"\"Serialize chunk (WITHOUT embedding).\"\"\"\n    return {\n        \"id\": str(chunk.id),\n        \"content\": chunk.content,\n        \"section_title\": chunk.section_title,\n        \"section_path\": chunk.section_path,\n        \"content_type\": chunk.content_type,\n        \"chunk_index\": chunk.chunk_index\n        # embedding excluded (regenerate on restore)\n    }\n```\n\n**Detailed Implementation:** See `templates/backup-script.py`\n\n---\n\n## Restore Implementation\n\n### Process Overview\n\n1. **Load JSON backup**\n2. **Validate structure** (version, required fields)\n3. **Create analyses** (without embeddings yet)\n4. **Create chunks** (without embeddings yet)\n5. **Generate embeddings** (using current embedding model)\n6. **Create artifacts**\n7. **Verify integrity** (counts, URL contract)\n\n### Key Challenge: Regenerating Embeddings\n\n```python\nasync def restore_golden_dataset(replace: bool = False):\n    \"\"\"Restore golden dataset from JSON backup.\"\"\"\n\n    # Load backup\n    with open(BACKUP_FILE) as f:\n        backup_data = json.load(f)\n\n    async with get_session() as session:\n        if replace:\n            # Delete existing data\n            await session.execute(delete(Chunk))\n            await session.execute(delete(Artifact))\n            await session.execute(delete(Analysis))\n            await session.commit()\n\n        # Restore analyses and chunks\n        from app.shared.services.embeddings import embed_text\n\n        for analysis_data in backup_data[\"analyses\"]:\n            # Create analysis\n            analysis = Analysis(\n                id=UUID(analysis_data[\"id\"]),\n                url=analysis_data[\"url\"],\n                # ... other fields ...\n            )\n            session.add(analysis)\n\n            # Create chunks with regenerated embeddings\n            for chunk_data in analysis_data[\"chunks\"]:\n                # Regenerate embedding using CURRENT model\n                embedding = await embed_text(chunk_data[\"content\"])\n\n                chunk = Chunk(\n                    id=UUID(chunk_data[\"id\"]),\n                    analysis_id=analysis.id,\n                    content=chunk_data[\"content\"],\n                    embedding=embedding,  # Freshly generated!\n                    # ... other fields ...\n                )\n                session.add(chunk)\n\n            await session.commit()\n\n        print(\" Restore completed\")\n```\n\n**Why regenerate embeddings?**\n- Embedding models improve over time\n- Ensures consistency with current model\n- Smaller backup files (exclude large vectors)\n\n**Detailed Implementation:** See `references/backup-restore.md`\n\n---\n\n## Validation\n\n### Validation Checklist\n\n```python\nasync def verify_golden_dataset() -> dict:\n    \"\"\"Verify golden dataset integrity.\"\"\"\n\n    errors = []\n    warnings = []\n\n    async with get_session() as session:\n        # 1. Check counts\n        analysis_count = await session.scalar(select(func.count(Analysis.id)))\n        chunk_count = await session.scalar(select(func.count(Chunk.id)))\n        artifact_count = await session.scalar(select(func.count(Artifact.id)))\n\n        expected = load_metadata()\n        if analysis_count != expected[\"total_analyses\"]:\n            errors.append(f\"Analysis count mismatch: {analysis_count} vs {expected['total_analyses']}\")\n\n        # 2. Check URL contract\n        query = select(Analysis).where(\n            Analysis.url.like(\"%skillforge.dev%\") |\n            Analysis.url.like(\"%placeholder%\")\n        )\n        result = await session.execute(query)\n        invalid_urls = result.scalars().all()\n\n        if invalid_urls:\n            errors.append(f\"Found {len(invalid_urls)} analyses with placeholder URLs\")\n\n        # 3. Check embeddings exist\n        query = select(Chunk).where(Chunk.embedding.is_(None))\n        result = await session.execute(query)\n        missing_embeddings = result.scalars().all()\n\n        if missing_embeddings:\n            errors.append(f\"Found {len(missing_embeddings)} chunks without embeddings\")\n\n        # 4. Check orphaned chunks\n        query = select(Chunk).outerjoin(Analysis).where(Analysis.id.is_(None))\n        result = await session.execute(query)\n        orphaned = result.scalars().all()\n\n        if orphaned:\n            warnings.append(f\"Found {len(orphaned)} orphaned chunks\")\n\n        return {\n            \"valid\": len(errors) == 0,\n            \"errors\": errors,\n            \"warnings\": warnings,\n            \"stats\": {\n                \"analyses\": analysis_count,\n                \"chunks\": chunk_count,\n                \"artifacts\": artifact_count\n            }\n        }\n```\n\n**Detailed Validation:** See `references/validation-contracts.md`\n\n---\n\n## CLI Usage\n\n```bash\ncd backend\n\n# Backup golden dataset\npoetry run python scripts/backup_golden_dataset.py backup\n\n# Verify backup integrity\npoetry run python scripts/backup_golden_dataset.py verify\n\n# Restore from backup (WARNING: Deletes existing data)\npoetry run python scripts/backup_golden_dataset.py restore --replace\n\n# Restore without deleting (adds to existing)\npoetry run python scripts/backup_golden_dataset.py restore\n```\n\n---\n\n## CI/CD Integration\n\n### Automated Backups\n\n```yaml\n# .github/workflows/backup-golden-dataset.yml\nname: Backup Golden Dataset\n\non:\n  schedule:\n    - cron: '0 2 * * 0'  # Weekly on Sunday at 2am\n  workflow_dispatch:  # Manual trigger\n\njobs:\n  backup:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          cd backend\n          poetry install\n\n      - name: Run backup\n        env:\n          DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }}\n        run: |\n          cd backend\n          poetry run python scripts/backup_golden_dataset.py backup\n\n      - name: Commit backup\n        run: |\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add backend/data/golden_dataset_backup.json\n          git add backend/data/golden_dataset_metadata.json\n          git commit -m \"chore: automated golden dataset backup\"\n          git push\n```\n\n---\n\n## Best Practices\n\n### 1. Version Control Backups\n\n```bash\n# Commit backups to git\ngit add backend/data/golden_dataset_backup.json\ngit commit -m \"chore: golden dataset backup (98 analyses, 415 chunks)\"\n```\n\n### 2. Validate Before Deployment\n\n```bash\n# Pre-deployment check\npoetry run python scripts/backup_golden_dataset.py verify\n\n# Should output:\n#  Validation passed\n#    Analyses: 98\n#    Chunks: 415\n#    Artifacts: 98\n#    No errors found\n```\n\n### 3. Test Restore in Staging\n\n```bash\n# Never test restore in production first!\n\n# Staging environment\nexport DATABASE_URL=$STAGING_DATABASE_URL\npoetry run python scripts/backup_golden_dataset.py restore --replace\n\n# Run tests to verify\npoetry run pytest tests/integration/test_retrieval_quality.py\n```\n\n### 4. Document Changes\n\n```json\n// backend/data/golden_dataset_metadata.json\n{\n  \"total_analyses\": 98,\n  \"total_chunks\": 415,\n  \"last_updated\": \"2025-12-19T10:30:00Z\",\n  \"changes\": [\n    {\n      \"date\": \"2025-12-19\",\n      \"action\": \"added\",\n      \"count\": 5,\n      \"description\": \"Added 5 new LangGraph tutorial analyses\"\n    },\n    {\n      \"date\": \"2025-12-10\",\n      \"action\": \"removed\",\n      \"count\": 2,\n      \"description\": \"Removed 2 outdated React 17 analyses\"\n    }\n  ]\n}\n```\n\n---\n\n## Disaster Recovery\n\n### Scenario 1: Accidental Deletion\n\n```bash\n# Oh no! Someone ran DELETE FROM analyses WHERE 1=1\n\n# 1. Restore from backup\npoetry run python scripts/backup_golden_dataset.py restore --replace\n\n# 2. Verify\npoetry run python scripts/backup_golden_dataset.py verify\n\n# 3. Run tests\npoetry run pytest tests/integration/test_retrieval_quality.py\n```\n\n### Scenario 2: Database Migration Gone Wrong\n\n```bash\n# Migration corrupted data\n\n# 1. Rollback migration\nalembic downgrade -1\n\n# 2. Restore from backup\npoetry run python scripts/backup_golden_dataset.py restore --replace\n\n# 3. Re-run migration (fixed)\nalembic upgrade head\n```\n\n### Scenario 3: New Environment Setup\n\n```bash\n# Fresh dev environment, need golden dataset\n\n# 1. Clone repo (includes backup)\ngit clone https://github.com/your-org/skillforge\ncd skillforge/backend\n\n# 2. Setup DB\ndocker compose up -d postgres\nalembic upgrade head\n\n# 3. Restore golden dataset\npoetry run python scripts/backup_golden_dataset.py restore\n\n# 4. Verify\npoetry run pytest tests/integration/test_retrieval_quality.py\n```\n\n---\n\n## References\n\n### SkillForge Implementation\n- `backend/scripts/backup_golden_dataset.py` - Main backup script\n- `backend/data/golden_dataset_backup.json` - JSON backup (version controlled)\n- `backend/data/golden_dataset_metadata.json` - Quick stats\n\n### Related Skills\n- `pgvector-search` - Retrieval evaluation using golden dataset\n- `ai-native-development` - Embedding generation for restore\n- `devops-deployment` - CI/CD backup automation\n\n---\n\n**Version:** 1.0.0 (December 2025)\n**Status:** Production-ready patterns from SkillForge's 98-analysis golden dataset\n\n## Capability Details\n\n### backup\n**Keywords:** golden dataset, backup, export, json backup, version control data\n**Solves:**\n- How do I backup the golden dataset?\n- Export analyses to JSON for version control\n- Protect critical test datasets\n- Create portable database snapshots\n\n### restore\n**Keywords:** restore dataset, import analyses, regenerate embeddings, disaster recovery, new environment\n**Solves:**\n- How do I restore from backup?\n- Import golden dataset to new environment\n- Regenerate embeddings after restore\n- Disaster recovery procedures\n\n### validation\n**Keywords:** verify dataset, url contract, data integrity, validate backup, placeholder urls\n**Solves:**\n- How do I validate dataset integrity?\n- Check URL contracts (no placeholders)\n- Verify embeddings exist\n- Detect orphaned chunks\n\n### ci-cd-automation\n**Keywords:** automated backup, github actions, ci cd backup, scheduled backup\n**Solves:**\n- How do I automate dataset backups?\n- Set up GitHub Actions for weekly backups\n- Commit backups to git automatically\n- CI/CD integration patterns\n\n### disaster-recovery\n**Keywords:** disaster recovery, accidental deletion, migration failure, rollback\n**Solves:**\n- What if I accidentally delete the dataset?\n- Database migration gone wrong\n- Restore after data corruption\n- Rollback procedures\n\n### skillforge-golden-dataset\n**Keywords:** skillforge, 98 analyses, 415 chunks, retrieval evaluation, real world\n**Solves:**\n- What is SkillForge's golden dataset?\n- How does SkillForge protect test data?\n- Real-world backup/restore examples\n- Production golden dataset stats"
              },
              {
                "name": "golden-dataset-validation",
                "description": "Use when validating golden dataset quality. Runs schema checks, duplicate detection, and coverage analysis to ensure dataset integrity for AI evaluation.",
                "path": ".claude/skills/golden-dataset-validation/SKILL.md",
                "frontmatter": {
                  "name": "golden-dataset-validation",
                  "description": "Use when validating golden dataset quality. Runs schema checks, duplicate detection, and coverage analysis to ensure dataset integrity for AI evaluation.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "golden-dataset",
                    "validation",
                    "integrity",
                    "schema",
                    "duplicate-detection",
                    2025
                  ],
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob"
                  ]
                },
                "content": "# Golden Dataset Validation\n\n**Ensure data integrity, prevent duplicates, and maintain quality standards**\n\n## Overview\n\nThis skill provides comprehensive validation patterns for the golden dataset, ensuring every entry meets quality standards before inclusion.\n\n**When to use this skill:**\n- Validating new documents before adding\n- Running integrity checks on existing dataset\n- Detecting duplicate or similar content\n- Analyzing coverage gaps\n- Pre-commit validation hooks\n\n---\n\n## Schema Validation\n\n### Document Schema (v2.0)\n\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"required\": [\"id\", \"title\", \"source_url\", \"content_type\", \"sections\"],\n  \"properties\": {\n    \"id\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[a-z0-9-]+$\",\n      \"description\": \"Unique kebab-case identifier\"\n    },\n    \"title\": {\n      \"type\": \"string\",\n      \"minLength\": 10,\n      \"maxLength\": 200\n    },\n    \"source_url\": {\n      \"type\": \"string\",\n      \"format\": \"uri\",\n      \"description\": \"Canonical source URL (NOT placeholder)\"\n    },\n    \"content_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"article\", \"tutorial\", \"research_paper\", \"documentation\", \"video_transcript\", \"code_repository\"]\n    },\n    \"bucket\": {\n      \"type\": \"string\",\n      \"enum\": [\"short\", \"long\"]\n    },\n    \"language\": {\n      \"type\": \"string\",\n      \"default\": \"en\"\n    },\n    \"tags\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"minItems\": 2,\n      \"maxItems\": 10\n    },\n    \"sections\": {\n      \"type\": \"array\",\n      \"minItems\": 1,\n      \"items\": {\n        \"type\": \"object\",\n        \"required\": [\"id\", \"title\", \"content\"],\n        \"properties\": {\n          \"id\": {\"type\": \"string\", \"pattern\": \"^[a-z0-9-/]+$\"},\n          \"title\": {\"type\": \"string\"},\n          \"content\": {\"type\": \"string\", \"minLength\": 50},\n          \"granularity\": {\"enum\": [\"coarse\", \"fine\", \"summary\"]}\n        }\n      }\n    }\n  }\n}\n```\n\n### Query Schema\n\n```json\n{\n  \"type\": \"object\",\n  \"required\": [\"id\", \"query\", \"difficulty\", \"expected_chunks\", \"min_score\"],\n  \"properties\": {\n    \"id\": {\n      \"type\": \"string\",\n      \"pattern\": \"^q-[a-z0-9-]+$\"\n    },\n    \"query\": {\n      \"type\": \"string\",\n      \"minLength\": 5,\n      \"maxLength\": 500\n    },\n    \"modes\": {\n      \"type\": \"array\",\n      \"items\": {\"enum\": [\"semantic\", \"keyword\", \"hybrid\"]}\n    },\n    \"category\": {\n      \"enum\": [\"specific\", \"broad\", \"negative\", \"edge\", \"coarse-to-fine\"]\n    },\n    \"difficulty\": {\n      \"enum\": [\"trivial\", \"easy\", \"medium\", \"hard\", \"adversarial\"]\n    },\n    \"expected_chunks\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"minItems\": 1\n    },\n    \"min_score\": {\n      \"type\": \"number\",\n      \"minimum\": 0,\n      \"maximum\": 1\n    }\n  }\n}\n```\n\n---\n\n## Validation Rules\n\n### Rule 1: No Placeholder URLs\n\n```python\nFORBIDDEN_URL_PATTERNS = [\n    \"skillforge.dev\",\n    \"placeholder\",\n    \"example.com\",\n    \"localhost\",\n    \"127.0.0.1\",\n]\n\ndef validate_url(url: str) -> tuple[bool, str]:\n    \"\"\"Validate URL is not a placeholder.\"\"\"\n    for pattern in FORBIDDEN_URL_PATTERNS:\n        if pattern in url.lower():\n            return False, f\"URL contains forbidden pattern: {pattern}\"\n\n    # Must be HTTPS (except for specific cases)\n    if not url.startswith(\"https://\"):\n        if not url.startswith(\"http://arxiv.org\"):  # arXiv redirects\n            return False, \"URL must use HTTPS\"\n\n    return True, \"OK\"\n```\n\n### Rule 2: Unique Identifiers\n\n```python\ndef validate_unique_ids(documents: list[dict], queries: list[dict]) -> list[str]:\n    \"\"\"Ensure all IDs are unique across documents and queries.\"\"\"\n    errors = []\n\n    # Document IDs\n    doc_ids = [d[\"id\"] for d in documents]\n    if len(doc_ids) != len(set(doc_ids)):\n        duplicates = [id for id in doc_ids if doc_ids.count(id) > 1]\n        errors.append(f\"Duplicate document IDs: {set(duplicates)}\")\n\n    # Query IDs\n    query_ids = [q[\"id\"] for q in queries]\n    if len(query_ids) != len(set(query_ids)):\n        duplicates = [id for id in query_ids if query_ids.count(id) > 1]\n        errors.append(f\"Duplicate query IDs: {set(duplicates)}\")\n\n    # Section IDs within documents\n    for doc in documents:\n        section_ids = [s[\"id\"] for s in doc.get(\"sections\", [])]\n        if len(section_ids) != len(set(section_ids)):\n            errors.append(f\"Duplicate section IDs in document: {doc['id']}\")\n\n    return errors\n```\n\n### Rule 3: Referential Integrity\n\n```python\ndef validate_references(documents: list[dict], queries: list[dict]) -> list[str]:\n    \"\"\"Ensure query expected_chunks reference valid section IDs.\"\"\"\n    errors = []\n\n    # Build set of all valid section IDs\n    valid_sections = set()\n    for doc in documents:\n        for section in doc.get(\"sections\", []):\n            valid_sections.add(section[\"id\"])\n\n    # Check query references\n    for query in queries:\n        for chunk_id in query.get(\"expected_chunks\", []):\n            if chunk_id not in valid_sections:\n                errors.append(\n                    f\"Query {query['id']} references invalid section: {chunk_id}\"\n                )\n\n    return errors\n```\n\n### Rule 4: Content Quality\n\n```python\ndef validate_content_quality(document: dict) -> list[str]:\n    \"\"\"Validate document content meets quality standards.\"\"\"\n    warnings = []\n\n    # Title length\n    title = document.get(\"title\", \"\")\n    if len(title) < 10:\n        warnings.append(\"Title too short (min 10 chars)\")\n    if len(title) > 200:\n        warnings.append(\"Title too long (max 200 chars)\")\n\n    # Section content\n    for section in document.get(\"sections\", []):\n        content = section.get(\"content\", \"\")\n        if len(content) < 50:\n            warnings.append(f\"Section {section['id']} content too short (min 50 chars)\")\n        if len(content) > 50000:\n            warnings.append(f\"Section {section['id']} content very long (>50k chars)\")\n\n    # Tags\n    tags = document.get(\"tags\", [])\n    if len(tags) < 2:\n        warnings.append(\"Too few tags (min 2)\")\n    if len(tags) > 10:\n        warnings.append(\"Too many tags (max 10)\")\n\n    return warnings\n```\n\n### Rule 5: Difficulty Distribution\n\n```python\ndef validate_difficulty_distribution(queries: list[dict]) -> list[str]:\n    \"\"\"Ensure balanced difficulty distribution.\"\"\"\n    warnings = []\n\n    # Count by difficulty\n    distribution = {}\n    for query in queries:\n        diff = query.get(\"difficulty\", \"unknown\")\n        distribution[diff] = distribution.get(diff, 0) + 1\n\n    # Minimum requirements\n    requirements = {\n        \"trivial\": 3,\n        \"easy\": 3,\n        \"medium\": 5,  # Most common real-world case\n        \"hard\": 3,\n    }\n\n    for level, min_count in requirements.items():\n        actual = distribution.get(level, 0)\n        if actual < min_count:\n            warnings.append(\n                f\"Insufficient {level} queries: {actual}/{min_count}\"\n            )\n\n    return warnings\n```\n\n---\n\n## Duplicate Detection\n\n### Semantic Similarity Check\n\n```python\nimport numpy as np\nfrom typing import Optional\n\nasync def check_duplicate(\n    new_content: str,\n    existing_embeddings: list[tuple[str, np.ndarray]],\n    embedding_service,\n    threshold: float = 0.85,\n) -> Optional[tuple[str, float]]:\n    \"\"\"Check if content is duplicate of existing document.\n\n    Args:\n        new_content: Content to check\n        existing_embeddings: List of (doc_id, embedding) tuples\n        embedding_service: Service to generate embeddings\n        threshold: Similarity threshold for duplicate warning\n\n    Returns:\n        (doc_id, similarity) if duplicate found, None otherwise\n    \"\"\"\n    # Generate embedding for new content\n    new_embedding = await embedding_service.generate_embedding(\n        text=new_content[:8000],  # Truncate for embedding\n        normalize=True,\n    )\n    new_vec = np.array(new_embedding)\n\n    # Compare against existing\n    max_similarity = 0.0\n    most_similar_doc = None\n\n    for doc_id, existing_vec in existing_embeddings:\n        # Cosine similarity (vectors are normalized)\n        similarity = np.dot(new_vec, existing_vec)\n\n        if similarity > max_similarity:\n            max_similarity = similarity\n            most_similar_doc = doc_id\n\n    if max_similarity >= threshold:\n        return (most_similar_doc, max_similarity)\n\n    return None\n```\n\n### URL Duplicate Check\n\n```python\ndef check_url_duplicate(\n    new_url: str,\n    source_url_map: dict[str, str],\n) -> Optional[str]:\n    \"\"\"Check if URL already exists in dataset.\n\n    Returns document ID if duplicate found.\n    \"\"\"\n    # Normalize URL\n    normalized = normalize_url(new_url)\n\n    for doc_id, existing_url in source_url_map.items():\n        if normalize_url(existing_url) == normalized:\n            return doc_id\n\n    return None\n\ndef normalize_url(url: str) -> str:\n    \"\"\"Normalize URL for comparison.\"\"\"\n    from urllib.parse import urlparse, urlunparse\n\n    parsed = urlparse(url.lower())\n\n    # Remove trailing slashes, www prefix\n    netloc = parsed.netloc.replace(\"www.\", \"\")\n    path = parsed.path.rstrip(\"/\")\n\n    # Remove common tracking parameters\n    # (simplified - real implementation would parse query string)\n\n    return urlunparse((\n        parsed.scheme,\n        netloc,\n        path,\n        \"\",  # params\n        \"\",  # query (stripped)\n        \"\",  # fragment\n    ))\n```\n\n---\n\n## Coverage Analysis\n\n### Gap Detection\n\n```python\ndef analyze_coverage_gaps(\n    documents: list[dict],\n    queries: list[dict],\n) -> dict:\n    \"\"\"Analyze dataset coverage and identify gaps.\"\"\"\n\n    # Content type distribution\n    content_types = {}\n    for doc in documents:\n        ct = doc.get(\"content_type\", \"unknown\")\n        content_types[ct] = content_types.get(ct, 0) + 1\n\n    # Domain/tag distribution\n    all_tags = []\n    for doc in documents:\n        all_tags.extend(doc.get(\"tags\", []))\n    tag_counts = {}\n    for tag in all_tags:\n        tag_counts[tag] = tag_counts.get(tag, 0) + 1\n\n    # Difficulty distribution\n    difficulties = {}\n    for query in queries:\n        diff = query.get(\"difficulty\", \"unknown\")\n        difficulties[diff] = difficulties.get(diff, 0) + 1\n\n    # Identify gaps\n    gaps = []\n\n    # Check content type balance\n    total_docs = len(documents)\n    if content_types.get(\"tutorial\", 0) / total_docs < 0.15:\n        gaps.append(\"Under-represented: tutorials (<15%)\")\n    if content_types.get(\"research_paper\", 0) / total_docs < 0.05:\n        gaps.append(\"Under-represented: research papers (<5%)\")\n\n    # Check domain coverage\n    expected_domains = [\"ai-ml\", \"backend\", \"frontend\", \"devops\", \"security\"]\n    for domain in expected_domains:\n        if tag_counts.get(domain, 0) < 5:\n            gaps.append(f\"Under-represented domain: {domain} (<5 docs)\")\n\n    # Check difficulty balance\n    total_queries = len(queries)\n    if difficulties.get(\"hard\", 0) / total_queries < 0.10:\n        gaps.append(\"Under-represented: hard queries (<10%)\")\n    if difficulties.get(\"adversarial\", 0) / total_queries < 0.05:\n        gaps.append(\"Under-represented: adversarial queries (<5%)\")\n\n    return {\n        \"content_type_distribution\": content_types,\n        \"tag_distribution\": dict(sorted(tag_counts.items(), key=lambda x: -x[1])[:20]),\n        \"difficulty_distribution\": difficulties,\n        \"gaps\": gaps,\n        \"total_documents\": total_docs,\n        \"total_queries\": total_queries,\n    }\n```\n\n---\n\n## Validation Workflow\n\n### Pre-Addition Validation\n\n```python\nasync def validate_before_add(\n    document: dict,\n    existing_documents: list[dict],\n    existing_queries: list[dict],\n    source_url_map: dict[str, str],\n    embedding_service,\n) -> dict:\n    \"\"\"Run full validation before adding document.\n\n    Returns:\n        {\n            \"valid\": bool,\n            \"errors\": list[str],  # Blocking issues\n            \"warnings\": list[str],  # Non-blocking issues\n            \"duplicate_check\": {\n                \"is_duplicate\": bool,\n                \"similar_to\": str | None,\n                \"similarity\": float | None,\n            }\n        }\n    \"\"\"\n    errors = []\n    warnings = []\n\n    # 1. Schema validation\n    schema_errors = validate_schema(document)\n    errors.extend(schema_errors)\n\n    # 2. URL validation\n    url_valid, url_msg = validate_url(document.get(\"source_url\", \"\"))\n    if not url_valid:\n        errors.append(url_msg)\n\n    # 3. URL duplicate check\n    url_dup = check_url_duplicate(document.get(\"source_url\", \"\"), source_url_map)\n    if url_dup:\n        errors.append(f\"URL already exists in dataset as: {url_dup}\")\n\n    # 4. Content quality\n    quality_warnings = validate_content_quality(document)\n    warnings.extend(quality_warnings)\n\n    # 5. Semantic duplicate check\n    content = \" \".join(\n        s.get(\"content\", \"\") for s in document.get(\"sections\", [])\n    )\n    existing_embeddings = await load_existing_embeddings(existing_documents)\n    dup_result = await check_duplicate(\n        content, existing_embeddings, embedding_service\n    )\n\n    duplicate_check = {\n        \"is_duplicate\": dup_result is not None,\n        \"similar_to\": dup_result[0] if dup_result else None,\n        \"similarity\": dup_result[1] if dup_result else None,\n    }\n\n    if dup_result and dup_result[1] >= 0.90:\n        errors.append(\n            f\"Content too similar to existing document: {dup_result[0]} \"\n            f\"(similarity: {dup_result[1]:.2f})\"\n        )\n    elif dup_result and dup_result[1] >= 0.80:\n        warnings.append(\n            f\"Content similar to existing document: {dup_result[0]} \"\n            f\"(similarity: {dup_result[1]:.2f})\"\n        )\n\n    return {\n        \"valid\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n        \"duplicate_check\": duplicate_check,\n    }\n```\n\n### Full Dataset Validation\n\n```python\nasync def validate_full_dataset() -> dict:\n    \"\"\"Run comprehensive validation on entire dataset.\n\n    Use this for:\n    - Pre-commit hooks\n    - CI/CD validation\n    - Periodic integrity checks\n    \"\"\"\n    from backend.tests.smoke.retrieval.fixtures.loader import FixtureLoader\n\n    loader = FixtureLoader(use_expanded=True)\n    documents = loader.load_documents()\n    queries = loader.load_queries()\n    source_url_map = loader.load_source_url_map()\n\n    all_errors = []\n    all_warnings = []\n\n    # 1. Schema validation for all documents\n    for doc in documents:\n        errors = validate_schema(doc)\n        all_errors.extend([f\"[{doc['id']}] {e}\" for e in errors])\n\n    # 2. Unique ID validation\n    id_errors = validate_unique_ids(documents, queries)\n    all_errors.extend(id_errors)\n\n    # 3. Referential integrity\n    ref_errors = validate_references(documents, queries)\n    all_errors.extend(ref_errors)\n\n    # 4. URL validation\n    for doc in documents:\n        valid, msg = validate_url(doc.get(\"source_url\", \"\"))\n        if not valid:\n            all_errors.append(f\"[{doc['id']}] {msg}\")\n\n    # 5. Difficulty distribution\n    dist_warnings = validate_difficulty_distribution(queries)\n    all_warnings.extend(dist_warnings)\n\n    # 6. Coverage analysis\n    coverage = analyze_coverage_gaps(documents, queries)\n    all_warnings.extend(coverage[\"gaps\"])\n\n    return {\n        \"valid\": len(all_errors) == 0,\n        \"errors\": all_errors,\n        \"warnings\": all_warnings,\n        \"coverage\": coverage,\n        \"stats\": {\n            \"documents\": len(documents),\n            \"queries\": len(queries),\n            \"sections\": sum(len(d.get(\"sections\", [])) for d in documents),\n        }\n    }\n```\n\n---\n\n## CLI Integration\n\n### Validation Commands\n\n```bash\n# Validate specific document\npoetry run python scripts/data/add_to_golden_dataset.py validate \\\n    --document-id \"new-doc-id\"\n\n# Validate full dataset\npoetry run python scripts/data/add_to_golden_dataset.py validate-all\n\n# Check for duplicates\npoetry run python scripts/data/add_to_golden_dataset.py check-duplicate \\\n    --url \"https://example.com/article\"\n\n# Analyze coverage gaps\npoetry run python scripts/data/add_to_golden_dataset.py coverage\n```\n\n---\n\n## Pre-Commit Hook\n\n```bash\n#!/bin/bash\n# .claude/hooks/pretool/bash/validate-golden-dataset.sh\n\n# Only run if golden dataset files changed\nCHANGED_FILES=$(git diff --cached --name-only)\n\nif echo \"$CHANGED_FILES\" | grep -q \"fixtures/documents_expanded.json\\|fixtures/queries.json\\|fixtures/source_url_map.json\"; then\n    echo \" Validating golden dataset changes...\"\n\n    cd backend\n    poetry run python scripts/data/add_to_golden_dataset.py validate-all\n\n    if [ $? -ne 0 ]; then\n        echo \" Golden dataset validation failed!\"\n        echo \"Fix errors before committing.\"\n        exit 1\n    fi\n\n    echo \" Golden dataset validation passed\"\nfi\n```\n\n---\n\n## Related Skills\n\n- `golden-dataset-curation` - Quality criteria and workflows\n- `golden-dataset-management` - Backup/restore operations\n- `pgvector-search` - Embedding-based duplicate detection\n\n---\n\n**Version:** 1.0.0 (December 2025)\n**Issue:** #599\n\n## Capability Details\n\n### schema-validation\n**Keywords:** schema, validation, schema check, format validation\n**Solves:**\n- Validate entries against document schema\n- Check required fields are present\n- Verify data types and constraints\n\n### duplicate-detection\n**Keywords:** duplicate, detection, deduplication, similarity check\n**Solves:**\n- Detect duplicate or near-duplicate entries\n- Use semantic similarity for fuzzy matching\n- Prevent redundant entries in dataset\n\n### referential-integrity\n**Keywords:** referential, integrity, foreign key, relationship\n**Solves:**\n- Verify relationships between documents and queries\n- Check source URL mappings are valid\n- Ensure cross-references are consistent\n\n### coverage-analysis\n**Keywords:** coverage, analysis, distribution, completeness\n**Solves:**\n- Analyze dataset coverage across domains\n- Identify gaps in difficulty distribution\n- Report coverage metrics and recommendations"
              },
              {
                "name": "hyde-retrieval",
                "description": "HyDE (Hypothetical Document Embeddings) for improved semantic retrieval. Use when queries don't match document vocabulary, retrieval quality is poor, or implementing advanced RAG patterns.",
                "path": ".claude/skills/hyde-retrieval/SKILL.md",
                "frontmatter": {
                  "name": "hyde-retrieval",
                  "description": "HyDE (Hypothetical Document Embeddings) for improved semantic retrieval. Use when queries don't match document vocabulary, retrieval quality is poor, or implementing advanced RAG patterns.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# HyDE (Hypothetical Document Embeddings)\n\nGenerate hypothetical answer documents to bridge vocabulary gaps in semantic search.\n\n## The Problem\n\nDirect query embedding often fails due to vocabulary mismatch:\n```\nQuery: \"scaling async data pipelines\"\nDocs use: \"event-driven messaging\", \"Apache Kafka\", \"message brokers\"\n Low similarity scores despite high relevance\n```\n\n## The Solution\n\nInstead of embedding the query, generate a hypothetical answer document:\n```\nQuery: \"scaling async data pipelines\"\n LLM generates: \"To scale asynchronous data pipelines, use event-driven\n   messaging with Apache Kafka. Message brokers provide backpressure...\"\n Embed the hypothetical document\n Now matches docs using similar terminology\n```\n\n## Implementation\n\n```python\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\n\nclass HyDEResult(BaseModel):\n    \"\"\"Result of HyDE generation.\"\"\"\n    original_query: str\n    hypothetical_doc: str\n    embedding: list[float]\n\nasync def generate_hyde(\n    query: str,\n    llm: AsyncOpenAI,\n    embed_fn: callable,\n    max_tokens: int = 150,\n) -> HyDEResult:\n    \"\"\"Generate hypothetical document and embed it.\"\"\"\n\n    # Generate hypothetical answer\n    response = await llm.chat.completions.create(\n        model=\"gpt-4o-mini\",  # Fast, cheap model\n        messages=[\n            {\"role\": \"system\", \"content\":\n                \"Write a short paragraph that would answer this query. \"\n                \"Use technical terminology that documentation would use.\"},\n            {\"role\": \"user\", \"content\": query}\n        ],\n        max_tokens=max_tokens,\n        temperature=0.3,  # Low temp for consistency\n    )\n\n    hypothetical_doc = response.choices[0].message.content\n\n    # Embed the hypothetical document (not the query!)\n    embedding = await embed_fn(hypothetical_doc)\n\n    return HyDEResult(\n        original_query=query,\n        hypothetical_doc=hypothetical_doc,\n        embedding=embedding,\n    )\n```\n\n## With Caching\n\n```python\nfrom functools import lru_cache\nimport hashlib\n\nclass HyDEService:\n    def __init__(self, llm, embed_fn):\n        self.llm = llm\n        self.embed_fn = embed_fn\n        self._cache: dict[str, HyDEResult] = {}\n\n    def _cache_key(self, query: str) -> str:\n        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n\n    async def generate(self, query: str) -> HyDEResult:\n        key = self._cache_key(query)\n\n        if key in self._cache:\n            return self._cache[key]\n\n        result = await generate_hyde(query, self.llm, self.embed_fn)\n        self._cache[key] = result\n        return result\n```\n\n## Per-Concept HyDE (Advanced)\n\nFor multi-concept queries, generate HyDE for each concept:\n\n```python\nasync def batch_hyde(\n    concepts: list[str],\n    hyde_service: HyDEService,\n) -> list[HyDEResult]:\n    \"\"\"Generate HyDE embeddings for multiple concepts in parallel.\"\"\"\n    import asyncio\n\n    tasks = [hyde_service.generate(concept) for concept in concepts]\n    return await asyncio.gather(*tasks)\n```\n\n## When to Use HyDE\n\n| Scenario | Use HyDE? |\n|----------|-----------|\n| Abstract/conceptual queries | Yes |\n| Exact term searches | No (use keyword) |\n| Code snippet searches | No |\n| Natural language questions | Yes |\n| Vocabulary mismatch suspected | Yes |\n\n## Fallback Strategy\n\n```python\nasync def hyde_with_fallback(\n    query: str,\n    hyde_service: HyDEService,\n    embed_fn: callable,\n    timeout: float = 3.0,\n) -> list[float]:\n    \"\"\"HyDE with fallback to direct embedding on timeout.\"\"\"\n    import asyncio\n\n    try:\n        async with asyncio.timeout(timeout):\n            result = await hyde_service.generate(query)\n            return result.embedding\n    except TimeoutError:\n        # Fallback to direct query embedding\n        return await embed_fn(query)\n```\n\n## Performance Tips\n\n- Use fast model (gpt-4o-mini, claude-3-haiku) for generation\n- Cache aggressively (queries often repeat)\n- Set tight timeouts (2-3s) with fallback\n- Keep hypothetical docs concise (100-200 tokens)\n- Combine with query decomposition for best results\n\n## References\n\n- [Gao et al. 2022 - HyDE Paper](https://arxiv.org/abs/2212.10496)\n- [LangChain HyDE](https://python.langchain.com/docs/use_cases/query_analysis/techniques/hyde)"
              },
              {
                "name": "i18n-date-patterns",
                "description": "Use this skill for internationalization (i18n) in React applications. Covers ALL user-facing strings, date/time handling, locale-aware formatting (useFormatting hook), ICU MessageFormat, Trans component, and RTL/LTR support.",
                "path": ".claude/skills/i18n-date-patterns/SKILL.md",
                "frontmatter": {
                  "name": "i18n-date-patterns",
                  "description": "Use this skill for internationalization (i18n) in React applications. Covers ALL user-facing strings, date/time handling, locale-aware formatting (useFormatting hook), ICU MessageFormat, Trans component, and RTL/LTR support.",
                  "context": "fork",
                  "agent": "frontend-ui-developer",
                  "version": "1.2.0",
                  "author": "Yonatan Gross",
                  "tags": [
                    "i18n",
                    "internationalization",
                    "dayjs",
                    "dates",
                    "react-i18next",
                    "localization",
                    "rtl",
                    "useTranslation",
                    "useFormatting",
                    "ICU",
                    "Trans"
                  ]
                },
                "content": "# i18n and Localization Patterns\n\n## Overview\n\nThis skill provides comprehensive guidance for implementing internationalization in React applications. It ensures ALL user-facing strings, date displays, currency, lists, and time calculations are locale-aware.\n\n**When to use this skill:**\n- Adding ANY user-facing text to components\n- Formatting dates, times, currency, lists, or ordinals\n- Implementing complex pluralization\n- Embedding React components in translated text\n- Supporting RTL languages (Hebrew, Arabic)\n\n**Bundled Resources:**\n- `references/formatting-utilities.md` - useFormatting hook API reference\n- `references/icu-messageformat.md` - ICU plural/select syntax\n- `references/trans-component.md` - Trans component for rich text\n- `checklists/i18n-checklist.md` - Implementation and review checklist\n- `examples/component-i18n-example.md` - Complete component example\n\n**Canonical Reference:** See `docs/i18n-standards.md` for the full i18n standards document.\n\n---\n\n## Core Patterns\n\n### 1. useTranslation Hook (All UI Strings)\n\nEvery visible string MUST use the translation function:\n\n```tsx\nimport { useTranslation } from 'react-i18next';\n\nfunction MyComponent() {\n  const { t } = useTranslation(['patients', 'common']);\n  \n  return (\n    <div>\n      <h1>{t('patients:title')}</h1>\n      <button>{t('common:actions.save')}</button>\n    </div>\n  );\n}\n```\n\n### 2. useFormatting Hook (Locale-Aware Data)\n\nAll locale-sensitive formatting MUST use the centralized hook:\n\n```tsx\nimport { useFormatting } from '@/hooks';\n\nfunction PriceDisplay({ amount, items }) {\n  const { formatILS, formatList, formatOrdinal } = useFormatting();\n  \n  return (\n    <div>\n      <p>Price: {formatILS(amount)}</p>        {/* 1,500.00 */}\n      <p>Items: {formatList(items)}</p>        {/* \"a, b, and c\" */}\n      <p>Position: {formatOrdinal(3)}</p>      {/* \"3rd\" */}\n    </div>\n  );\n}\n```\n\nSee `references/formatting-utilities.md` for the complete API.\n\n### 3. Date Formatting\n\nAll dates MUST use the centralized `@/lib/dates` library:\n\n```tsx\nimport { formatDate, formatDateShort, calculateWaitTime } from '@/lib/dates';\n\nconst date = formatDate(appointment.date);    // \"Jan 6, 2026\"\nconst waitTime = calculateWaitTime('09:30');  // \"15 min\"\n```\n\n### 4. ICU MessageFormat (Complex Plurals)\n\nUse ICU syntax in translation files for pluralization:\n\n```json\n{\n  \"patients\": \"{count, plural, =0 {No patients} one {# patient} other {# patients}}\"\n}\n```\n\n```tsx\nt('patients', { count: 5 })  //  \"5 patients\"\n```\n\nSee `references/icu-messageformat.md` for full syntax.\n\n### 5. Trans Component (Rich Text)\n\nFor embedded React components in translated text:\n\n```tsx\nimport { Trans } from 'react-i18next';\n\n<Trans\n  i18nKey=\"richText.welcome\"\n  values={{ name: userName }}\n  components={{ strong: <strong /> }}\n/>\n```\n\nSee `references/trans-component.md` for patterns.\n\n---\n\n## Translation File Structure\n\n```\nfrontend/src/i18n/locales/\n en/\n    common.json      # Shared: actions, status, time\n    patients.json    # Patient-related strings\n    dashboard.json   # Dashboard strings\n    owner.json       # Owner portal strings\n    invoices.json    # Invoice strings\n he/\n     (same structure)\n```\n\n---\n\n## Anti-Patterns (FORBIDDEN)\n\n```typescript\n//  NEVER hardcode strings\n<h1></h1>                    // Use t('patients:title')\n<button>Save</button>               // Use t('common:actions.save')\n\n//  NEVER use .join() for lists\nitems.join(', ')                    // Use formatList(items)\n\n//  NEVER hardcode currency\n\"\" + price                         // Use formatILS(price)\n\n//  NEVER use new Date() for formatting\nnew Date().toLocaleDateString()     // Use formatDate() from @/lib/dates\n\n//  NEVER use inline plural logic\ncount === 1 ? 'item' : 'items'      // Use ICU MessageFormat\n\n//  NEVER leave console.log in production\nconsole.log('debug')                // Remove before commit\n\n//  NEVER use dangerouslySetInnerHTML for i18n\ndangerouslySetInnerHTML             // Use <Trans> component\n```\n\n---\n\n## Quick Reference\n\n| Need | Solution |\n|------|----------|\n| UI text | `t('namespace:key')` from `useTranslation` |\n| Currency | `formatILS(amount)` from `useFormatting` |\n| Lists | `formatList(items)` from `useFormatting` |\n| Ordinals | `formatOrdinal(n)` from `useFormatting` |\n| Dates | `formatDate(date)` from `@/lib/dates` |\n| Plurals | ICU MessageFormat in translation files |\n| Rich text | `<Trans>` component |\n| RTL check | `isRTL` from `useFormatting` |\n\n---\n\n## Checklist\n\nSee `checklists/i18n-checklist.md` for complete implementation and review checklists.\n\n---\n\n## Integration with Agents\n\n### Frontend UI Developer\n- Uses all i18n patterns for components\n- References this skill for formatting\n- Ensures no hardcoded strings\n\n### Code Quality Reviewer\n- Checks for anti-patterns (`.join()`, `console.log`, etc.)\n- Validates translation key coverage\n- Ensures RTL compatibility\n\n---\n\n**Skill Version**: 1.2.0\n**Last Updated**: 2026-01-06\n**Maintained by**: Yonatan Gross\n\n## Capability Details\n\n### translation-hooks\n**Keywords:** useTranslation, t(), i18n hook, translation hook\n**Solves:**\n- Translate UI strings with useTranslation\n- Implement namespaced translations\n- Handle missing translation keys\n\n### formatting-hooks\n**Keywords:** useFormatting, formatCurrency, formatList, formatOrdinal\n**Solves:**\n- Format currency values with locale\n- Format lists with proper separators\n- Handle ordinal numbers across locales\n\n### icu-messageformat\n**Keywords:** ICU, MessageFormat, plural, select, pluralization\n**Solves:**\n- Implement pluralization rules\n- Handle gender-specific translations\n- Build complex message patterns\n\n### date-time-formatting\n**Keywords:** date format, time format, dayjs, locale date, calendar\n**Solves:**\n- Format dates with dayjs and locale\n- Handle timezone-aware formatting\n- Build calendar components with i18n\n\n### rtl-support\n**Keywords:** RTL, right-to-left, hebrew, arabic, direction\n**Solves:**\n- Support RTL languages like Hebrew\n- Handle bidirectional text\n- Configure RTL-aware layouts\n\n### trans-component\n**Keywords:** Trans, rich text, embedded JSX, interpolation\n**Solves:**\n- Embed React components in translations\n- Handle rich text formatting\n- Implement safe HTML in translations"
              },
              {
                "name": "implement",
                "description": "Full-power feature implementation with parallel subagents, skills, and MCPs",
                "path": ".claude/skills/implement/SKILL.md",
                "frontmatter": {
                  "name": "implement",
                  "description": "Full-power feature implementation with parallel subagents, skills, and MCPs",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "implementation",
                    "feature",
                    "full-stack",
                    "parallel-agents"
                  ]
                },
                "content": "# Implement Feature\n\nMaximum utilization of parallel subagent execution for feature implementation.\n\n## When to Use\n\n- Building new features\n- Full-stack development\n- Complex implementations requiring multiple specialists\n- AI/ML integrations\n\n## Quick Start\n\n```bash\n/implement user authentication\n/implement real-time notifications\n/implement dashboard analytics\n```\n\n## Phase 1: Discovery & Planning\n\n### 1a. Create Task List\n\nBreak into small, deliverable, testable tasks:\n- Each task completable in one focused session\n- Each task MUST include its tests\n- Group by domain (frontend, backend, AI, shared)\n\n### 1b. Research Current Best Practices\n\n```python\n# PARALLEL - Web searches\nWebSearch(\"React 19 best practices 2025\")\nWebSearch(\"FastAPI async patterns 2025\")\nWebSearch(\"TypeScript 5.x strict mode 2025\")\n```\n\n### 1c. Context7 Documentation\n\n```python\n# PARALLEL - Library docs\nmcp__context7__get-library-docs(libraryId=\"/facebook/react\", topic=\"hooks\")\nmcp__context7__get-library-docs(libraryId=\"/tiangolo/fastapi\", topic=\"dependencies\")\n```\n\n## Phase 2: Load Skills\n\n```python\n# PARALLEL - Load capability indexes first\nRead(\".claude/skills/api-design-framework/capabilities.json\")\nRead(\".claude/skills/react-server-components-framework/capabilities.json\")\nRead(\".claude/skills/type-safety-validation/capabilities.json\")\nRead(\".claude/skills/testing-strategy-builder/capabilities.json\")\n```\n\nThen load ONLY specific references needed based on feature type.\n\n## Phase 3: Parallel Architecture Design (5 Agents)\n\n| Agent | Focus |\n|-------|-------|\n| Plan | Architecture planning, dependency graph |\n| backend-system-architect | API, services, database |\n| frontend-ui-developer | Components, state, hooks |\n| ai-ml-engineer | LLM integration (if needed) |\n| ux-researcher | User experience, accessibility |\n\nAll 5 agents run in ONE message, then synthesize into unified plan.\n\n## Phase 4: Parallel Implementation (8 Agents)\n\n| Agent | Task |\n|-------|------|\n| backend-system-architect #1 | API endpoints |\n| backend-system-architect #2 | Database layer |\n| frontend-ui-developer #1 | UI components |\n| frontend-ui-developer #2 | State & API hooks |\n| ai-ml-engineer | AI integration |\n| rapid-ui-designer | Styling |\n| code-quality-reviewer #1 | Test suite |\n| sprint-prioritizer | Progress tracking |\n\n## Phase 5: Integration & Validation (4 Agents)\n\n| Agent | Task |\n|-------|------|\n| backend-system-architect | Backend + database integration |\n| frontend-ui-developer | Frontend + API integration |\n| code-quality-reviewer #1 | Full test suite |\n| code-quality-reviewer #2 | Security audit |\n\n## Phase 6: E2E Verification\n\nIf UI changes, verify with Playwright MCP:\n\n```python\nmcp__playwright__browser_navigate(url=\"http://localhost:5173\")\nmcp__playwright__browser_snapshot()\nmcp__playwright__browser_take_screenshot(filename=\"feature.png\")\n```\n\n## Phase 7: Documentation\n\nSave implementation decisions to memory MCP for future reference.\n\n## Summary\n\n**Total Parallel Agents: 17 across 4 phases**\n\n**MCPs Used:**\n- sequential-thinking (complex reasoning)\n- context7 (library documentation)\n- memory (decision persistence)\n- playwright (E2E verification)\n\n**Key Principles:**\n- Tests are NOT optional\n- Parallel when independent\n- Progressive skill loading\n- Evidence-based completion\n\n## References\n\n- [Agent Phases](references/agent-phases.md)"
              },
              {
                "name": "input-validation",
                "description": "Input validation and sanitization patterns. Use when validating user input, preventing injection attacks, implementing allowlists, or sanitizing HTML/SQL/command inputs.",
                "path": ".claude/skills/input-validation/SKILL.md",
                "frontmatter": {
                  "name": "input-validation",
                  "description": "Input validation and sanitization patterns. Use when validating user input, preventing injection attacks, implementing allowlists, or sanitizing HTML/SQL/command inputs.",
                  "context": "fork",
                  "agent": "security-auditor",
                  "version": "2.0.0",
                  "tags": [
                    "security",
                    "validation",
                    "zod",
                    "pydantic",
                    2026
                  ],
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob",
                    "Write",
                    "Edit"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/redact-secrets.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# Input Validation\n\nValidate and sanitize all untrusted input using Zod v4 and Pydantic.\n\n## When to Use\n\n- Processing user input\n- Query parameters\n- Form submissions\n- API request bodies\n- File uploads\n- URL validation\n\n## Core Principles\n\n1. **Never trust user input**\n2. **Validate on server-side** (client-side is UX only)\n3. **Use allowlists** (not blocklists)\n4. **Validate type, length, format, range**\n\n## Quick Reference\n\n### Zod v4 Schema\n\n```typescript\nimport { z } from 'zod';\n\nconst UserSchema = z.object({\n  email: z.string().email(),\n  name: z.string().min(2).max(100),\n  age: z.coerce.number().int().min(0).max(150),\n  role: z.enum(['user', 'admin']).default('user'),\n});\n\nconst result = UserSchema.safeParse(req.body);\nif (!result.success) {\n  return res.status(400).json({ errors: result.error.flatten() });\n}\n```\n\n### Type Coercion (v4)\n\n```typescript\n// Query params come as strings - coerce to proper types\nz.coerce.number()  // \"123\"  123\nz.coerce.boolean() // \"true\"  true\nz.coerce.date()    // \"2024-01-01\"  Date\n```\n\n### Discriminated Unions\n\n```typescript\nconst ShapeSchema = z.discriminatedUnion('type', [\n  z.object({ type: z.literal('circle'), radius: z.number() }),\n  z.object({ type: z.literal('rectangle'), width: z.number(), height: z.number() }),\n]);\n```\n\n### Pydantic (Python)\n\n```python\nfrom pydantic import BaseModel, EmailStr, Field\n\nclass User(BaseModel):\n    email: EmailStr\n    name: str = Field(min_length=2, max_length=100)\n    age: int = Field(ge=0, le=150)\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```typescript\n//  NEVER rely on client-side validation only\nif (formIsValid) submit();  // No server validation\n\n//  NEVER use blocklists\nconst blocked = ['password', 'secret'];  // Easy to miss fields\n\n//  NEVER trust Content-Type header\nif (file.type === 'image/png') {...}  // Can be spoofed\n\n//  NEVER build queries with string concat\n\"SELECT * FROM users WHERE name = '\" + name + \"'\"  // SQL injection\n\n//  ALWAYS validate server-side\nconst result = schema.safeParse(req.body);\n\n//  ALWAYS use allowlists\nconst allowed = ['name', 'email', 'createdAt'];\n\n//  ALWAYS validate file magic bytes\nconst isPng = buffer[0] === 0x89 && buffer[1] === 0x50;\n\n//  ALWAYS use parameterized queries\ndb.query('SELECT * FROM users WHERE name = ?', [name]);\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Validation library | Zod (TS), Pydantic (Python) |\n| Strategy | Allowlist over blocklist |\n| Location | Server-side always |\n| Error messages | Generic (don't leak info) |\n| File validation | Check magic bytes, not just extension |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/zod-v4-api.md](references/zod-v4-api.md) | Zod v4 API with coercion, transforms |\n| [examples/validation-patterns.md](examples/validation-patterns.md) | Complete validation examples |\n| [checklists/validation-checklist.md](checklists/validation-checklist.md) | Implementation checklist |\n| [templates/validation-schemas.ts](templates/validation-schemas.ts) | Ready-to-use schema templates |\n\n## Related Skills\n\n- `owasp-top-10` - Injection prevention\n- `auth-patterns` - User input in auth\n- `type-safety-validation` - TypeScript patterns\n\n## Capability Details\n\n### schema-validation\n**Keywords:** schema, validate, Zod, Pydantic, validation\n**Solves:**\n- Validate input against schemas\n- Define validation rules declaratively\n- Handle validation errors gracefully\n\n### type-coercion\n**Keywords:** coerce, coercion, type conversion, parse\n**Solves:**\n- Automatically convert input types\n- Parse strings to numbers/dates\n- Handle type mismatches\n\n### allowlist-validation\n**Keywords:** allowlist, whitelist, enum, literal, allowed values\n**Solves:**\n- Validate against allowed values\n- Prevent injection attacks\n- Restrict input to safe options\n\n### html-sanitization\n**Keywords:** sanitize, HTML, XSS, escape, DOMPurify\n**Solves:**\n- Sanitize HTML input safely\n- Prevent XSS attacks\n- Allow safe HTML subset\n\n### file-validation\n**Keywords:** file, upload, MIME type, file size, file type\n**Solves:**\n- Validate file uploads securely\n- Check file content not just extension\n- Enforce size limits\n\n### error-formatting\n**Keywords:** error, error message, validation error, user-friendly\n**Solves:**\n- Format validation errors for users\n- Avoid exposing internal details\n- Provide actionable error messages"
              },
              {
                "name": "integration-testing",
                "description": "Integration testing patterns for APIs and components. Use when testing component interactions, API endpoints with test databases, or service layer integration.",
                "path": ".claude/skills/integration-testing/SKILL.md",
                "frontmatter": {
                  "name": "integration-testing",
                  "description": "Integration testing patterns for APIs and components. Use when testing component interactions, API endpoints with test databases, or service layer integration.",
                  "context": "fork",
                  "agent": "test-generator",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Integration Testing\n\nTest how components work together.\n\n## When to Use\n\n- API endpoint testing\n- Component interactions\n- Service layer testing\n- Database integration\n\n## API Integration Test\n\n```typescript\nimport { describe, test, expect } from 'vitest';\nimport request from 'supertest';\nimport { app } from '../app';\n\ndescribe('POST /api/users', () => {\n  test('creates user and returns 201', async () => {\n    const response = await request(app)\n      .post('/api/users')\n      .send({ email: 'test@example.com', name: 'Test' });\n\n    expect(response.status).toBe(201);\n    expect(response.body.id).toBeDefined();\n    expect(response.body.email).toBe('test@example.com');\n  });\n\n  test('returns 400 for invalid email', async () => {\n    const response = await request(app)\n      .post('/api/users')\n      .send({ email: 'invalid', name: 'Test' });\n\n    expect(response.status).toBe(400);\n    expect(response.body.error).toContain('email');\n  });\n});\n```\n\n## FastAPI Integration Test\n\n```python\nimport pytest\nfrom httpx import AsyncClient\nfrom app.main import app\n\n@pytest.fixture\nasync def client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.mark.asyncio\nasync def test_create_user(client: AsyncClient):\n    response = await client.post(\n        \"/api/users\",\n        json={\"email\": \"test@example.com\", \"name\": \"Test\"}\n    )\n\n    assert response.status_code == 201\n    assert response.json()[\"email\"] == \"test@example.com\"\n\n@pytest.mark.asyncio\nasync def test_get_user_not_found(client: AsyncClient):\n    response = await client.get(\"/api/users/nonexistent\")\n\n    assert response.status_code == 404\n```\n\n## Test Database Setup\n\n```python\nimport pytest\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\n@pytest.fixture(scope=\"function\")\ndef db_session():\n    \"\"\"Fresh database per test.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    Base.metadata.create_all(engine)\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    yield session\n\n    session.close()\n    Base.metadata.drop_all(engine)\n```\n\n## React Component Integration\n\n```typescript\nimport { render, screen } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport { QueryClientProvider } from '@tanstack/react-query';\n\ntest('form submits and shows success', async () => {\n  const user = userEvent.setup();\n\n  render(\n    <QueryClientProvider client={queryClient}>\n      <UserForm />\n    </QueryClientProvider>\n  );\n\n  await user.type(screen.getByLabelText('Email'), 'test@example.com');\n  await user.click(screen.getByRole('button', { name: /submit/i }));\n\n  expect(await screen.findByText(/success/i)).toBeInTheDocument();\n});\n```\n\n## Coverage Targets\n\n| Area | Target |\n|------|--------|\n| API endpoints | 70%+ |\n| Service layer | 80%+ |\n| Component interactions | 70%+ |\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Database | In-memory SQLite or test container |\n| Execution | < 1s per test |\n| External APIs | MSW (frontend), VCR.py (backend) |\n| Cleanup | Fresh state per test |\n\n## Common Mistakes\n\n- Shared test database state\n- No transaction rollback\n- Testing against production APIs\n- Slow setup/teardown\n\n## Related Skills\n\n- `unit-testing` - Isolated tests\n- `msw-mocking` - Network mocking\n- `e2e-testing` - Full flow testing\n\n## Capability Details\n\n### api-testing\n**Keywords:** api, endpoint, httpx, testclient\n**Solves:**\n- Test FastAPI endpoints\n- Integration test patterns\n- API contract testing\n\n### database-testing\n**Keywords:** database, fixture, transaction, rollback\n**Solves:**\n- Test database operations\n- Use transaction rollback\n- Create test fixtures\n\n### test-plan-template\n**Keywords:** plan, template, strategy, coverage\n**Solves:**\n- Integration test plan template\n- Coverage strategy\n- Test organization"
              },
              {
                "name": "langfuse-observability",
                "description": "Use when adding observability to LLM applications with Langfuse. Covers tracing, evaluation, prompt management, and cost tracking for self-hosted deployments.",
                "path": ".claude/skills/langfuse-observability/SKILL.md",
                "frontmatter": {
                  "name": "langfuse-observability",
                  "description": "Use when adding observability to LLM applications with Langfuse. Covers tracing, evaluation, prompt management, and cost tracking for self-hosted deployments.",
                  "context": "fork",
                  "agent": "metrics-architect",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "langfuse",
                    "llm",
                    "observability",
                    "tracing",
                    "evaluation",
                    "prompts",
                    2025
                  ]
                },
                "content": "# Langfuse Observability\n\n## Overview\n\n**Langfuse** is the open-source LLM observability platform that SkillForge uses for tracing, monitoring, evaluation, and prompt management. Unlike LangSmith (deprecated), Langfuse is self-hosted, free, and designed for production LLM applications.\n\n**When to use this skill:**\n- Setting up LLM observability from scratch\n- Debugging slow or incorrect LLM responses\n- Tracking token usage and costs\n- Managing prompts in production\n- Evaluating LLM output quality\n- Migrating from LangSmith to Langfuse\n\n**SkillForge Integration:**\n- **Status**:  Migrated from LangSmith (Dec 2025)\n- **Location**: `backend/app/shared/services/langfuse/`\n- **MCP Server**: `skillforge-langfuse` (optional)\n\n---\n\n## Core Features\n\n### 1. Distributed Tracing\n\nTrack LLM calls across your application with automatic parent-child span relationships.\n\n```python\nfrom langfuse.decorators import observe, langfuse_context\n\n@observe()  # Automatic tracing\nasync def analyze_content(content: str, agent_type: str):\n    \"\"\"Analyze content with automatic Langfuse tracing.\"\"\"\n\n    # Nested span for retrieval\n    @observe(name=\"retrieval\")\n    async def retrieve_context():\n        chunks = await vector_db.search(content)\n        langfuse_context.update_current_observation(\n            metadata={\"chunks_retrieved\": len(chunks)}\n        )\n        return chunks\n\n    # Nested span for generation\n    @observe(name=\"generation\")\n    async def generate_analysis(context):\n        response = await llm.generate(\n            prompt=f\"Context: {context}\\n\\nAnalyze: {content}\"\n        )\n        langfuse_context.update_current_observation(\n            input=content[:500],\n            output=response[:500],\n            model=\"claude-sonnet-4-20250514\",\n            usage={\n                \"input_tokens\": response.usage.input_tokens,\n                \"output_tokens\": response.usage.output_tokens\n            }\n        )\n        return response\n\n    context = await retrieve_context()\n    return await generate_analysis(context)\n```\n\n**Result in Langfuse UI:**\n```\nanalyze_content (2.3s, $0.045)\n retrieval (0.1s)\n    metadata: {chunks_retrieved: 5}\n generation (2.2s, $0.045)\n     model: claude-sonnet-4-20250514\n     tokens: 1500 input, 1000 output\n```\n\n### 2. Token & Cost Tracking\n\nAutomatic cost calculation based on model pricing:\n\n```python\nfrom langfuse import Langfuse\n\nlangfuse = Langfuse()\n\n# Create trace with cost tracking\ntrace = langfuse.trace(\n    name=\"content_analysis\",\n    user_id=\"user_123\",\n    session_id=\"session_abc\"\n)\n\n# Log generation with automatic cost calculation\ngeneration = trace.generation(\n    name=\"security_audit\",\n    model=\"claude-sonnet-4-20250514\",\n    model_parameters={\"temperature\": 1.0, \"max_tokens\": 4096},\n    input=[{\"role\": \"user\", \"content\": \"Analyze for XSS...\"}],\n    output=\"Analysis: Found 3 vulnerabilities...\",\n    usage={\n        \"input\": 1500,\n        \"output\": 1000,\n        \"unit\": \"TOKENS\"\n    }\n)\n\n# Langfuse automatically calculates: $0.0045 + $0.015 = $0.0195\n```\n\n**Pricing Database (Auto-Updated):**\nLangfuse maintains a pricing database for all major models. You can also define custom pricing:\n\n```python\n# Custom model pricing\nlangfuse.create_model(\n    model_name=\"claude-sonnet-4-20250514\",\n    match_pattern=\"claude-sonnet-4.*\",\n    unit=\"TOKENS\",\n    input_price=0.000003,  # $3/MTok\n    output_price=0.000015,  # $15/MTok\n    total_price=None  # Calculated from input+output\n)\n```\n\n### 3. Prompt Management\n\nVersion control for prompts in production:\n\n```python\n# Fetch prompt from Langfuse\nfrom langfuse import Langfuse, get_client\n\nlangfuse = Langfuse()\n\n# Get latest version of security auditor prompt\nprompt = langfuse.get_prompt(\"security_auditor\", label=\"production\")\n\n# Use in LLM call\nresponse = await llm.generate(\n    messages=[\n        {\"role\": \"system\", \"content\": prompt.compile()},\n        {\"role\": \"user\", \"content\": user_input}\n    ]\n)\n```\n\n#### Linking Prompts to Generations (Issue #564 Pattern)\n\n**CRITICAL:** To make the \"Number of Observations\" counter work in Langfuse Prompts UI, you MUST link the `TextPromptClient` object to the generation span:\n\n```python\nfrom langfuse import get_client\n\n# Method 1: update_current_generation (preferred in SkillForge)\nlangfuse = get_client()\nprompt = langfuse.get_prompt(\"security_auditor\", label=\"production\")\n\n# Link prompt to current generation span\nlangfuse.update_current_generation(prompt=prompt)\n\n# Method 2: Pass prompt when starting generation\nwith langfuse.start_as_current_generation(\n    name=\"security-analysis\",\n    model=\"claude-sonnet-4-20250514\",\n    prompt=prompt  # Links automatically!\n) as generation:\n    response = await llm.generate(...)\n    generation.update(output=response)\n```\n\n**SkillForge Pattern (with caching):**\n```python\n# PromptManager returns both content AND TextPromptClient\nprompt_content, prompt_client = await prompt_manager.get_prompt_with_langfuse_client(\n    name=\"analysis-agent-security-auditor\",\n    variables={\"skill_instructions\": \"...\"},\n    label=\"production\",\n)\n\n# Pass prompt_client through agent metadata\nif prompt_client:\n    agent = agent.with_config(metadata={\"langfuse_prompt_client\": prompt_client})\n\n# In invoke_agent(), link prompt to generation\nif prompt_client:\n    langfuse.update_current_generation(prompt=prompt_client)\n```\n\n**Note:** Cache hits (L1/L2) return `None` for `prompt_client` - linkage only happens on L3 Langfuse fetches (~5% of calls). This is acceptable for analytics.\n\n**Prompt Versioning in UI:**\n```\nsecurity_auditor\n v1 (Jan 15, 2025) - production\n    \"You are a security auditor. Analyze code for...\"\n v2 (Jan 20, 2025) - staging\n    \"You are an expert security auditor. Focus on...\"\n v3 (Jan 25, 2025) - draft\n     \"As a cybersecurity expert, thoroughly analyze...\"\n```\n\n### 4. LLM Evaluation (Scores)\n\nTrack quality metrics with custom scores:\n\n```python\nfrom langfuse import Langfuse\n\nlangfuse = Langfuse()\n\n# Create trace\ntrace = langfuse.trace(name=\"content_analysis\", id=\"trace_123\")\n\n# After LLM response, score it\ntrace.score(\n    name=\"relevance\",\n    value=0.85,  # 0-1 scale\n    comment=\"Response addresses query but lacks depth\"\n)\n\ntrace.score(\n    name=\"factuality\",\n    value=0.92,\n    data_type=\"NUMERIC\"\n)\n\n# Use G-Eval for automated scoring\nfrom app.shared.services.g_eval import GEvalScorer\n\nscorer = GEvalScorer()\nscores = await scorer.score(\n    query=user_query,\n    response=llm_response,\n    criteria=[\"relevance\", \"coherence\", \"depth\"]\n)\n\nfor criterion, score in scores.items():\n    trace.score(name=criterion, value=score)\n```\n\n**Scores Dashboard:**\n- View score distributions\n- Track quality trends over time\n- Filter traces by score thresholds\n- Compare prompt versions by scores\n\n### 5. Session Tracking\n\nGroup related traces into user sessions:\n\n```python\n# Start session\nsession_id = f\"analysis_{analysis_id}\"\n\n# All traces with same session_id are grouped\ntrace1 = langfuse.trace(\n    name=\"url_fetch\",\n    session_id=session_id\n)\n\ntrace2 = langfuse.trace(\n    name=\"content_analysis\",\n    session_id=session_id\n)\n\ntrace3 = langfuse.trace(\n    name=\"quality_gate\",\n    session_id=session_id\n)\n\n# View in UI: All 3 traces grouped under session\n```\n\n### 6. User & Metadata Tracking\n\nTrack performance per user or content type:\n\n```python\nlangfuse.trace(\n    name=\"analysis\",\n    user_id=\"user_123\",\n    metadata={\n        \"content_type\": \"article\",\n        \"url\": \"https://example.com/post\",\n        \"analysis_id\": \"abc123\",\n        \"agent_count\": 8,\n        \"total_cost_usd\": 0.15\n    },\n    tags=[\"production\", \"skillforge\", \"security\"]\n)\n```\n\n**Analytics:**\n- Filter by user, tag, metadata\n- Group costs by content_type\n- Track performance by agent type\n- Identify slow or expensive users\n\n---\n\n## SkillForge Integration\n\n### Setup (Already Complete)\n\n```python\n# backend/app/shared/services/langfuse/client.py\nfrom langfuse import Langfuse\nfrom app.core.config import settings\n\nlangfuse_client = Langfuse(\n    public_key=settings.LANGFUSE_PUBLIC_KEY,\n    secret_key=settings.LANGFUSE_SECRET_KEY,\n    host=settings.LANGFUSE_HOST  # Self-hosted or cloud\n)\n```\n\n### Workflow Integration\n\n```python\n# backend/app/workflows/content_analysis.py\nfrom langfuse.decorators import observe\n\n@observe(name=\"content_analysis_workflow\")\nasync def run_content_analysis(analysis_id: str, content: str):\n    \"\"\"Full workflow with automatic Langfuse tracing.\"\"\"\n\n    # Set global metadata\n    langfuse_context.update_current_trace(\n        user_id=f\"analysis_{analysis_id}\",\n        metadata={\n            \"analysis_id\": analysis_id,\n            \"content_length\": len(content)\n        }\n    )\n\n    # Each agent execution automatically creates nested spans\n    results = []\n    for agent in agents:\n        result = await execute_agent(agent, content)  # @observe decorated\n        results.append(result)\n\n    return results\n```\n\n### Cost Tracking Per Analysis\n\n```python\n# After analysis completes\ntrace = langfuse.get_trace(trace_id)\ntotal_cost = sum(\n    gen.calculated_total_cost or 0\n    for gen in trace.observations\n    if gen.type == \"GENERATION\"\n)\n\n# Store in database\nawait analysis_repo.update(\n    analysis_id,\n    langfuse_trace_id=trace.id,\n    total_cost_usd=total_cost\n)\n```\n\n---\n\n## Advanced Features\n\n### 1. CallbackHandler (LangChain Integration)\n\nFor LangChain/LangGraph applications:\n\n```python\nfrom langfuse.callback import CallbackHandler\n\nlangfuse_handler = CallbackHandler(\n    public_key=settings.LANGFUSE_PUBLIC_KEY,\n    secret_key=settings.LANGFUSE_SECRET_KEY\n)\n\n# Use with LangChain\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(\n    model=\"claude-sonnet-4-20250514\",\n    callbacks=[langfuse_handler]\n)\n\nresponse = llm.invoke(\"Analyze this code...\")  # Auto-traced!\n```\n\n### 2. Datasets for Evaluation\n\nCreate test datasets in Langfuse UI and run automated evaluations:\n\n```python\n# Fetch dataset\ndataset = langfuse.get_dataset(\"security_audit_test_set\")\n\n# Run evaluation\nfor item in dataset.items:\n    # Run LLM\n    response = await llm.generate(item.input)\n\n    # Create observation linked to dataset item\n    langfuse.trace(\n        name=\"evaluation_run\",\n        metadata={\"dataset_item_id\": item.id}\n    ).generation(\n        input=item.input,\n        output=response,\n        usage=response.usage\n    )\n\n    # Score\n    score = await evaluate_response(item.expected_output, response)\n    langfuse.score(\n        trace_id=trace.id,\n        name=\"accuracy\",\n        value=score\n    )\n```\n\n### 3. Experimentation (A/B Testing Prompts)\n\n```python\n# Test two prompt versions\nprompt_v1 = langfuse.get_prompt(\"security_auditor\", version=1)\nprompt_v2 = langfuse.get_prompt(\"security_auditor\", version=2)\n\n# Run A/B test\nimport random\n\nfor test_input in test_dataset:\n    prompt = random.choice([prompt_v1, prompt_v2])\n\n    response = await llm.generate(\n        messages=[\n            {\"role\": \"system\", \"content\": prompt.compile()},\n            {\"role\": \"user\", \"content\": test_input}\n        ]\n    )\n\n    # Track which version was used\n    langfuse.trace(\n        name=\"ab_test\",\n        metadata={\"prompt_version\": prompt.version}\n    )\n\n# Compare in Langfuse UI:\n# - Filter by prompt_version\n# - Compare average scores\n# - Analyze cost differences\n```\n\n---\n\n## Monitoring Dashboard Queries\n\n### Top 10 Most Expensive Traces (Last 7 Days)\n\n```sql\nSELECT\n    name,\n    user_id,\n    calculated_total_cost,\n    input_tokens,\n    output_tokens\nFROM traces\nWHERE timestamp > NOW() - INTERVAL '7 days'\nORDER BY calculated_total_cost DESC\nLIMIT 10;\n```\n\n### Average Cost by Agent Type\n\n```sql\nSELECT\n    metadata->>'agent_type' as agent,\n    COUNT(*) as traces,\n    AVG(calculated_total_cost) as avg_cost,\n    SUM(calculated_total_cost) as total_cost\nFROM traces\nWHERE metadata->>'agent_type' IS NOT NULL\nGROUP BY agent\nORDER BY total_cost DESC;\n```\n\n### Quality Scores Trend\n\n```sql\nSELECT\n    DATE(timestamp) as date,\n    AVG(value) FILTER (WHERE name = 'relevance') as avg_relevance,\n    AVG(value) FILTER (WHERE name = 'depth') as avg_depth,\n    AVG(value) FILTER (WHERE name = 'factuality') as avg_factuality\nFROM scores\nWHERE timestamp > NOW() - INTERVAL '30 days'\nGROUP BY DATE(timestamp)\nORDER BY date;\n```\n\n---\n\n## Best Practices\n\n1. **Always use @observe decorator** for automatic tracing\n2. **Set user_id and session_id** for better analytics\n3. **Add meaningful metadata** (content_type, analysis_id, etc.)\n4. **Score all productions traces** for quality monitoring\n5. **Use prompt management** instead of hardcoded prompts\n6. **Monitor costs daily** to catch spikes early\n7. **Create datasets** for regression testing\n8. **Tag production vs staging** traces\n\n---\n\n## References\n\n- [Langfuse Docs](https://langfuse.com/docs)\n- [Python SDK](https://langfuse.com/docs/sdk/python)\n- [Decorators Guide](https://langfuse.com/docs/sdk/python/decorators)\n- [Prompt Management](https://langfuse.com/docs/prompts)\n- [Self-Hosting](https://langfuse.com/docs/deployment/self-host)\n- [SkillForge Integration](https://github.com/yonatan-gross/SkillForge#langfuse-observability)\n\n---\n\n## Migration from LangSmith\n\nSee Langfuse documentation at https://langfuse.com/docs for integration details.\n\n**Key Differences:**\n- Langfuse: Self-hosted, open-source, free\n- LangSmith: Cloud-only, proprietary, paid\n- Langfuse: Prompt management built-in\n- LangSmith: External prompt storage needed\n- Langfuse: @observe decorator\n- LangSmith: @traceable decorator\n\n## Capability Details\n\n### distributed-tracing\n**Keywords:** trace, tracing, observability, span, nested, parent-child, observe\n**Solves:**\n- How do I trace LLM calls across my application?\n- How to debug slow LLM responses?\n- Track execution flow in multi-agent workflows\n- Create nested trace spans\n\n### cost-tracking\n**Keywords:** cost, token usage, pricing, budget, spend, expense\n**Solves:**\n- How do I track LLM costs?\n- Calculate token usage and pricing\n- Monitor AI budget and spending\n- Track cost per user or session\n\n### prompt-management\n**Keywords:** prompt version, prompt template, prompt control, prompt registry\n**Solves:**\n- How do I version control prompts?\n- Manage prompts in production\n- A/B test different prompt versions\n- Link prompts to traces\n\n### llm-evaluation\n**Keywords:** score, quality, evaluation, rating, assessment, g-eval\n**Solves:**\n- How do I evaluate LLM output quality?\n- Score responses with custom metrics\n- Track quality trends over time\n- Compare prompt versions by quality\n\n### session-tracking\n**Keywords:** session, user tracking, conversation, group traces\n**Solves:**\n- How do I group related traces?\n- Track multi-turn conversations\n- Monitor per-user performance\n- Organize traces by session\n\n### langchain-integration\n**Keywords:** langchain, callback, handler, langgraph integration\n**Solves:**\n- How do I integrate Langfuse with LangChain?\n- Use CallbackHandler for tracing\n- Automatic LangGraph workflow tracing\n- LangChain observability setup\n\n### datasets-evaluation\n**Keywords:** dataset, test set, evaluation dataset, benchmark\n**Solves:**\n- How do I create test datasets in Langfuse?\n- Run automated evaluations\n- Regression testing for LLMs\n- Benchmark prompt versions\n\n### ab-testing\n**Keywords:** a/b test, experiment, compare prompts, variant testing\n**Solves:**\n- How do I A/B test prompts?\n- Compare two prompt versions\n- Experimental prompt evaluation\n- Statistical prompt testing\n\n### monitoring-dashboard\n**Keywords:** dashboard, analytics, metrics, monitoring, queries\n**Solves:**\n- What are the most expensive traces?\n- Average cost by agent type\n- Quality score trends\n- Custom monitoring queries\n\n### skillforge-integration\n**Keywords:** skillforge, migration, setup, workflow integration\n**Solves:**\n- How does SkillForge use Langfuse?\n- Migrate from LangSmith to Langfuse\n- SkillForge workflow tracing patterns\n- Cost tracking per analysis\n\n### multi-judge-evaluation\n**Keywords:** multi judge, g-eval, multiple evaluators, ensemble evaluation, weighted scoring\n**Solves:**\n- How do I use multiple LLM judges to evaluate quality?\n- Set up G-Eval criteria evaluation\n- Configure weighted scoring across judges\n- Wire SkillForge's existing langfuse_evaluators.py\n\n### experiments-api\n**Keywords:** experiment, dataset, benchmark, regression test, prompt testing\n**Solves:**\n- How do I run experiments across datasets?\n- A/B test models and prompts systematically\n- Track quality regression over time\n- Compare experiment results"
              },
              {
                "name": "langgraph-checkpoints",
                "description": "LangGraph checkpointing and persistence. Use when implementing fault-tolerant workflows, resuming interrupted executions, debugging with state history, or avoiding re-running expensive operations.",
                "path": ".claude/skills/langgraph-checkpoints/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-checkpoints",
                  "description": "LangGraph checkpointing and persistence. Use when implementing fault-tolerant workflows, resuming interrupted executions, debugging with state history, or avoiding re-running expensive operations.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Checkpointing\n\nPersist workflow state for recovery and debugging.\n\n## When to Use\n\n- Fault-tolerant workflows\n- Resume after crashes\n- Debug state at each step\n- Avoid re-running expensive LLM calls\n\n## Checkpointer Options\n\n```python\nfrom langgraph.checkpoint import MemorySaver\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\n# Development: In-memory\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n\n# Production: SQLite\ncheckpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\napp = workflow.compile(checkpointer=checkpointer)\n\n# Production: PostgreSQL\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\")\napp = workflow.compile(checkpointer=checkpointer)\n```\n\n## Using Thread IDs\n\n```python\n# Start new workflow\nconfig = {\"configurable\": {\"thread_id\": \"analysis-123\"}}\nresult = app.invoke(initial_state, config=config)\n\n# Resume interrupted workflow\nconfig = {\"configurable\": {\"thread_id\": \"analysis-123\"}}\nresult = app.invoke(None, config=config)  # Resumes from checkpoint\n```\n\n## PostgreSQL Setup\n\n```python\ndef create_checkpointer():\n    \"\"\"Create PostgreSQL checkpointer for production.\"\"\"\n    return PostgresSaver.from_conn_string(\n        settings.DATABASE_URL,\n        save_every=1  # Save after each node\n    )\n\n# Compile with checkpointing\napp = workflow.compile(\n    checkpointer=create_checkpointer(),\n    interrupt_before=[\"quality_gate\"]  # Manual review point\n)\n```\n\n## Inspecting Checkpoints\n\n```python\n# Get all checkpoints for a workflow\ncheckpoints = app.get_state_history(config)\n\nfor checkpoint in checkpoints:\n    print(f\"Step: {checkpoint.metadata['step']}\")\n    print(f\"Node: {checkpoint.metadata['source']}\")\n    print(f\"State: {checkpoint.values}\")\n\n# Get current state\ncurrent = app.get_state(config)\nprint(current.values)\n```\n\n## Resuming After Crash\n\n```python\nimport logging\n\nasync def run_with_recovery(workflow_id: str, initial_state: dict):\n    \"\"\"Run workflow with automatic recovery.\"\"\"\n    config = {\"configurable\": {\"thread_id\": workflow_id}}\n\n    try:\n        # Try to resume existing workflow\n        state = app.get_state(config)\n        if state.values:\n            logging.info(f\"Resuming workflow {workflow_id}\")\n            return app.invoke(None, config=config)\n    except Exception:\n        pass  # No existing checkpoint\n\n    # Start fresh\n    logging.info(f\"Starting new workflow {workflow_id}\")\n    return app.invoke(initial_state, config=config)\n```\n\n## Step-by-Step Debugging\n\n```python\n# Execute one node at a time\nfor step in app.stream(initial_state, config):\n    print(f\"After {step['node']}: {step['state']}\")\n    input(\"Press Enter to continue...\")\n\n# Rollback to previous checkpoint\nhistory = list(app.get_state_history(config))\nprevious_state = history[1]  # One step back\napp.update_state(config, previous_state.values)\n```\n\n## Store vs Checkpointer (2026 Best Practice)\n\n```python\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.store.postgres import PostgresStore\n\n# Checkpointer = SHORT-TERM memory (thread-scoped)\n# - Conversation history within a session\n# - Workflow state for resume/recovery\n# - Scoped to thread_id\n\ncheckpointer = PostgresSaver.from_conn_string(DATABASE_URL)\n\n# Store = LONG-TERM memory (cross-thread)\n# - User preferences across sessions\n# - Learned facts about users\n# - Shared across ALL threads for a user\n\nstore = PostgresStore.from_conn_string(DATABASE_URL)\n\n# Compile with BOTH for full memory support\napp = workflow.compile(\n    checkpointer=checkpointer,  # Thread-scoped state\n    store=store                  # Cross-thread memory\n)\n```\n\n## Using Store for Cross-Thread Memory\n\n```python\nfrom langgraph.store.base import BaseStore\n\nasync def agent_with_memory(state: AgentState, *, store: BaseStore):\n    \"\"\"Agent that remembers across conversations.\"\"\"\n    user_id = state[\"user_id\"]\n\n    # Read cross-thread memory (user preferences)\n    memories = await store.aget(namespace=(\"users\", user_id), key=\"preferences\")\n\n    # Use memories in agent logic\n    if memories and memories.value.get(\"prefers_concise\"):\n        state[\"system_prompt\"] += \"\\nBe concise in responses.\"\n\n    # Update cross-thread memory (learned facts)\n    await store.aput(\n        namespace=(\"users\", user_id),\n        key=\"last_topic\",\n        value={\"topic\": state[\"current_topic\"], \"timestamp\": datetime.now().isoformat()}\n    )\n\n    return state\n\n# Register node with store access\nworkflow.add_node(\"agent\", agent_with_memory)\n```\n\n## Memory Architecture\n\n```\n\n                    User: alice                               \n\n  Thread 1 (chat-001)      Thread 2 (chat-002)              \n                      \n   Checkpointer           Checkpointer                  \n   - msg history          - msg history                 \n   - workflow pos         - workflow pos                \n                      \n\n                     Store (cross-thread)                     \n  namespace=(\"users\", \"alice\")                                \n  - preferences: {prefers_concise: true}                     \n  - last_topic: {topic: \"langgraph\", timestamp: \"...\"}       \n\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Development | MemorySaver (fast, no setup) |\n| Production | PostgresSaver (shared, durable) |\n| save_every | 1 for expensive nodes, 5 for cheap |\n| Thread ID | Use deterministic ID (workflow_id) |\n| **Short-term memory** | **Checkpointer (thread-scoped)** |\n| **Long-term memory** | **Store (cross-thread, namespaced)** |\n\n## Common Mistakes\n\n- No checkpointer in production (lose progress)\n- Random thread IDs (can't resume)\n- Not handling missing checkpoints\n- Saving too frequently (overhead)\n- **Using only checkpointer for user preferences (lost across threads)**\n- **Not using namespaces in Store (data collisions)**\n\n## Related Skills\n\n- `langgraph-state` - State design for checkpointing\n- `langgraph-human-in-loop` - Interrupt patterns\n- `database-schema-designer` - PostgreSQL setup\n\n## Capability Details\n\n### checkpoint-saving\n**Keywords:** save checkpoint, checkpoint, persist state, save state\n**Solves:**\n- Save workflow state at key points\n- Implement checkpoint strategies\n- Handle checkpoint serialization\n\n### checkpoint-loading\n**Keywords:** load checkpoint, restore, resume, recovery\n**Solves:**\n- Resume workflows from checkpoints\n- Implement state recovery\n- Handle checkpoint versioning\n\n### memory-backends\n**Keywords:** memory backend, MemorySaver, SqliteSaver, PostgresSaver\n**Solves:**\n- Configure checkpoint storage backends\n- Choose between memory/SQLite/Postgres\n- Implement custom checkpoint storage\n\n### async-checkpoints\n**Keywords:** async checkpoint, AsyncSqliteSaver, async persistence\n**Solves:**\n- Implement async checkpoint operations\n- Handle concurrent checkpoint access\n- Optimize checkpoint performance\n\n### conversation-history\n**Keywords:** conversation, history, message history, thread\n**Solves:**\n- Persist conversation history\n- Implement thread-based checkpoints\n- Manage conversation state"
              },
              {
                "name": "langgraph-functional",
                "description": "LangGraph Functional API with @entrypoint and @task decorators. Use when building workflows with the modern LangGraph pattern, enabling parallel execution, persistence, and human-in-the-loop.",
                "path": ".claude/skills/langgraph-functional/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-functional",
                  "description": "LangGraph Functional API with @entrypoint and @task decorators. Use when building workflows with the modern LangGraph pattern, enabling parallel execution, persistence, and human-in-the-loop.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Functional API\nBuild workflows using decorators instead of explicit graph construction.\n\n## When to Use\n\n- Sequential workflows with conditional branching\n- Orchestrator-worker patterns with parallel execution\n- Workflows needing persistence and checkpointing\n- Human-in-the-loop approval flows\n- Simpler alternative to explicit StateGraph construction\n\n## Core Concepts\n\n### Graph API vs Functional API\n```\nGraph API (explicit):           Functional API (implicit):\nStateGraph  add_node         @task functions +\nadd_edge  compile              @entrypoint orchestration\n```\n\n**When to Use Functional API**:\n- Sequential workflows with conditional logic\n- Orchestrator-worker patterns\n- Simpler debugging (regular Python functions)\n- Parallel task execution\n\n## Quick Start\n\n### Basic Pattern\n```python\nfrom langgraph.func import entrypoint, task\n\n@task\ndef step_one(data: str) -> str:\n    \"\"\"Task returns a future - call .result() to block\"\"\"\n    return process(data)\n\n@task\ndef step_two(result: str) -> str:\n    return transform(result)\n\n@entrypoint()\ndef my_workflow(input_data: str) -> str:\n    # Tasks return futures - enables parallel execution\n    result1 = step_one(input_data).result()\n    result2 = step_two(result1).result()\n    return result2\n\n# Invoke\noutput = my_workflow.invoke(\"hello\")\n```\n\n### Key Rules\n1. **@task** functions return futures - call `.result()` to get value\n2. **@entrypoint** is the workflow entry point - orchestrates tasks\n3. Tasks inside entrypoint are tracked for persistence/streaming\n4. Regular functions (no decorator) execute normally\n\n## Parallel Execution\n\n### Fan-Out Pattern\n```python\n@task\ndef fetch_source_a(query: str) -> dict:\n    return api_a.search(query)\n\n@task\ndef fetch_source_b(query: str) -> dict:\n    return api_b.search(query)\n\n@task\ndef merge_results(results: list[dict]) -> dict:\n    return {\"combined\": results}\n\n@entrypoint()\ndef parallel_search(query: str) -> dict:\n    # Launch in parallel - futures start immediately\n    future_a = fetch_source_a(query)\n    future_b = fetch_source_b(query)\n\n    # Block on both results\n    results = [future_a.result(), future_b.result()]\n\n    return merge_results(results).result()\n```\n\n### Map Over Collection\n```python\n@task\ndef process_item(item: dict) -> dict:\n    return transform(item)\n\n@entrypoint()\ndef batch_workflow(items: list[dict]) -> list[dict]:\n    # Launch all in parallel\n    futures = [process_item(item) for item in items]\n\n    # Collect results\n    return [f.result() for f in futures]\n```\n\n## Persistence & Checkpointing\n\n### Enable Checkpointing\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef resumable_workflow(data: str) -> str:\n    # Workflow state is automatically saved after each task\n    result = expensive_task(data).result()\n    return result\n\n# Use thread_id for persistence\nconfig = {\"configurable\": {\"thread_id\": \"session-123\"}}\nresult = resumable_workflow.invoke(\"input\", config)\n```\n\n### Access Previous State\n```python\nfrom typing import Optional\n\n@entrypoint(checkpointer=checkpointer)\ndef stateful_workflow(data: str, previous: Optional[dict] = None) -> dict:\n    \"\"\"previous contains last return value for this thread_id\"\"\"\n\n    if previous and previous.get(\"step\") == \"complete\":\n        return previous  # Already done\n\n    result = process(data).result()\n    return {\"step\": \"complete\", \"result\": result}\n```\n\n## Human-in-the-Loop\n\n### Interrupt for Approval\n```python\nfrom langgraph.types import interrupt, Command\n\n@entrypoint(checkpointer=checkpointer)\ndef approval_workflow(request: dict) -> dict:\n    # Process request\n    result = analyze_request(request).result()\n\n    # Pause for human approval\n    approved = interrupt({\n        \"question\": \"Approve this action?\",\n        \"details\": result\n    })\n\n    if approved:\n        return execute_action(result).result()\n    else:\n        return {\"status\": \"rejected\"}\n\n# Initial run - pauses at interrupt\nconfig = {\"configurable\": {\"thread_id\": \"approval-1\"}}\nfor chunk in approval_workflow.stream(request, config):\n    print(chunk)\n\n# Resume after human review\nfor chunk in approval_workflow.stream(Command(resume=True), config):\n    print(chunk)\n```\n\n## Conditional Logic\n\n### Branching\n```python\n@task\ndef classify(text: str) -> str:\n    return llm.invoke(f\"Classify: {text}\")  # \"positive\" or \"negative\"\n\n@task\ndef handle_positive(text: str) -> str:\n    return \"Thank you for the positive feedback!\"\n\n@task\ndef handle_negative(text: str) -> str:\n    return \"We're sorry to hear that. Creating support ticket...\"\n\n@entrypoint()\ndef feedback_workflow(text: str) -> str:\n    sentiment = classify(text).result()\n\n    if sentiment == \"positive\":\n        return handle_positive(text).result()\n    else:\n        return handle_negative(text).result()\n```\n\n### Loop Until Done\n```python\n@task\ndef call_llm(messages: list) -> dict:\n    return llm_with_tools.invoke(messages)\n\n@task\ndef call_tool(tool_call: dict) -> str:\n    tool = tools[tool_call[\"name\"]]\n    return tool.invoke(tool_call[\"args\"])\n\n@entrypoint()\ndef agent_loop(query: str) -> str:\n    messages = [{\"role\": \"user\", \"content\": query}]\n\n    while True:\n        response = call_llm(messages).result()\n\n        if not response.get(\"tool_calls\"):\n            return response[\"content\"]\n\n        # Execute tools in parallel\n        tool_futures = [call_tool(tc) for tc in response[\"tool_calls\"]]\n        tool_results = [f.result() for f in tool_futures]\n\n        messages.extend([response, *tool_results])\n```\n\n## Streaming\n\n### Stream Updates\n```python\n@entrypoint()\ndef streaming_workflow(data: str) -> str:\n    step1 = task_one(data).result()\n    step2 = task_two(step1).result()\n    return step2\n\n# Stream task completion updates\nfor update in streaming_workflow.stream(\"input\", stream_mode=\"updates\"):\n    print(f\"Task completed: {update}\")\n```\n\n### Stream Modes\n```python\n# \"updates\" - task completion events\nfor chunk in workflow.stream(input, stream_mode=\"updates\"):\n    print(chunk)\n\n# \"values\" - full state after each task\nfor chunk in workflow.stream(input, stream_mode=\"values\"):\n    print(chunk)\n\n# \"custom\" - custom events from your code\nfor chunk in workflow.stream(input, stream_mode=\"custom\"):\n    print(chunk)\n```\n\n## TypeScript\n\n### Basic Pattern\n```typescript\nimport { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\nconst processData = task(\"processData\", async (data: string) => {\n  return await transform(data);\n});\n\nconst workflow = entrypoint(\n  { name: \"myWorkflow\", checkpointer: new MemorySaver() },\n  async (input: string) => {\n    const result = await processData(input);\n    return result;\n  }\n);\n\n// Invoke\nconst config = { configurable: { thread_id: \"session-1\" } };\nconst result = await workflow.invoke(\"hello\", config);\n```\n\n### Parallel Execution\n```typescript\nconst fetchA = task(\"fetchA\", async (q: string) => api.fetchA(q));\nconst fetchB = task(\"fetchB\", async (q: string) => api.fetchB(q));\n\nconst parallelWorkflow = entrypoint(\"parallel\", async (query: string) => {\n  // Launch in parallel using Promise.all\n  const [resultA, resultB] = await Promise.all([\n    fetchA(query),\n    fetchB(query)\n  ]);\n  return { a: resultA, b: resultB };\n});\n```\n\n## Common Patterns\n\n### Orchestrator-Worker\n```python\n@task\ndef plan(topic: str) -> list[str]:\n    \"\"\"Orchestrator creates work items\"\"\"\n    sections = planner.invoke(f\"Create outline for: {topic}\")\n    return sections\n\n@task\ndef write_section(section: str) -> str:\n    \"\"\"Worker processes one item\"\"\"\n    return llm.invoke(f\"Write section: {section}\")\n\n@task\ndef synthesize(sections: list[str]) -> str:\n    \"\"\"Combine results\"\"\"\n    return \"\\n\\n\".join(sections)\n\n@entrypoint()\ndef report_workflow(topic: str) -> str:\n    sections = plan(topic).result()\n\n    # Fan-out to workers\n    section_futures = [write_section(s) for s in sections]\n    completed = [f.result() for f in section_futures]\n\n    # Fan-in\n    return synthesize(completed).result()\n```\n\n### Retry Pattern\n```python\n@task\ndef unreliable_api(data: str) -> dict:\n    return external_api.call(data)\n\n@entrypoint()\ndef retry_workflow(data: str, max_retries: int = 3) -> dict:\n    for attempt in range(max_retries):\n        try:\n            return unreliable_api(data).result()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            continue\n```\n\n## Anti-Patterns\n\n1. **Forgetting .result()**: Tasks return futures, must call `.result()`\n2. **Blocking in tasks**: Keep tasks focused, don't nest entrypoints\n3. **Missing checkpointer**: Without it, can't resume interrupted workflows\n4. **Sequential when parallel**: Launch tasks before blocking on results\n\n## Migration from Graph API\n\n```python\n# Graph API (before)\nfrom langgraph.graph import StateGraph\n\ndef node_a(state): return {\"data\": process(state[\"input\"])}\ndef node_b(state): return {\"result\": transform(state[\"data\"])}\n\ngraph = StateGraph(State)\ngraph.add_node(\"a\", node_a)\ngraph.add_node(\"b\", node_b)\ngraph.add_edge(\"a\", \"b\")\napp = graph.compile()\n\n# Functional API (after)\n@task\ndef process_data(input: str) -> str:\n    return process(input)\n\n@task\ndef transform_data(data: str) -> str:\n    return transform(data)\n\n@entrypoint()\ndef workflow(input: str) -> str:\n    data = process_data(input).result()\n    return transform_data(data).result()\n```\n\n## Resources\n- LangGraph Functional API: https://langchain-ai.github.io/langgraph/concepts/functional_api/\n- Workflows Tutorial: https://langchain-ai.github.io/langgraph/tutorials/workflows/"
              },
              {
                "name": "langgraph-human-in-loop",
                "description": "LangGraph human-in-the-loop patterns. Use when implementing approval workflows, manual review gates, user feedback integration, or interactive agent supervision.",
                "path": ".claude/skills/langgraph-human-in-loop/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-human-in-loop",
                  "description": "LangGraph human-in-the-loop patterns. Use when implementing approval workflows, manual review gates, user feedback integration, or interactive agent supervision.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Human-in-the-Loop\n\nPause workflows for human intervention and approval.\n\n## When to Use\n\n- Approval before publishing\n- Manual review of AI outputs\n- User feedback integration\n- Interactive agent supervision\n\n## Basic Interrupt\n\n```python\nworkflow = StateGraph(State)\nworkflow.add_node(\"draft\", generate_draft)\nworkflow.add_node(\"review\", human_review)\nworkflow.add_node(\"publish\", publish_content)\n\n# Interrupt before review\napp = workflow.compile(interrupt_before=[\"review\"])\n\n# Step 1: Generate draft (stops at review)\nconfig = {\"configurable\": {\"thread_id\": \"doc-123\"}}\nresult = app.invoke({\"topic\": \"AI\"}, config=config)\n# Workflow pauses here\n```\n\n## Resume After Approval\n\n```python\n# Step 2: Human reviews and updates state\nstate = app.get_state(config)\nprint(f\"Draft: {state.values['draft']}\")\n\n# Human decision\nstate.values[\"approved\"] = True\nstate.values[\"feedback\"] = \"Looks good\"\napp.update_state(config, state.values)\n\n# Step 3: Resume workflow\nresult = app.invoke(None, config=config)  # Continues to publish\n```\n\n## Approval Gate Node\n\n```python\ndef approval_gate(state: WorkflowState) -> WorkflowState:\n    \"\"\"Check if human approved.\"\"\"\n    if not state.get(\"human_reviewed\"):\n        # Will pause here due to interrupt_before\n        return state\n\n    if state[\"approved\"]:\n        state[\"next\"] = \"publish\"\n    else:\n        state[\"next\"] = \"revise\"\n\n    return state\n\nworkflow.add_node(\"approval_gate\", approval_gate)\n\n# Pause before this node\napp = workflow.compile(interrupt_before=[\"approval_gate\"])\n```\n\n## Feedback Loop Pattern\n\n```python\nimport uuid_utils  # pip install uuid-utils (UUID v7 for Python < 3.14)\n\nasync def run_with_feedback(initial_state: dict):\n    \"\"\"Run until human approves.\"\"\"\n    config = {\"configurable\": {\"thread_id\": str(uuid_utils.uuid7())}}\n\n    while True:\n        # Run until interrupt\n        result = app.invoke(initial_state, config=config)\n\n        # Get current state\n        state = app.get_state(config)\n\n        # Present to human\n        print(f\"Output: {state.values['output']}\")\n        feedback = input(\"Approve? (yes/no/feedback): \")\n\n        if feedback.lower() == \"yes\":\n            state.values[\"approved\"] = True\n            app.update_state(config, state.values)\n            return app.invoke(None, config=config)\n        elif feedback.lower() == \"no\":\n            return {\"status\": \"rejected\"}\n        else:\n            # Incorporate feedback and retry\n            state.values[\"feedback\"] = feedback\n            state.values[\"retry_count\"] = state.values.get(\"retry_count\", 0) + 1\n            app.update_state(config, state.values)\n            initial_state = None  # Resume from checkpoint\n```\n\n## API Integration\n\n```python\nfrom fastapi import FastAPI, HTTPException\n\napp = FastAPI()\n\n@app.post(\"/workflows/{workflow_id}/approve\")\nasync def approve_workflow(workflow_id: str, approved: bool, feedback: str = \"\"):\n    \"\"\"API endpoint for human approval.\"\"\"\n    config = {\"configurable\": {\"thread_id\": workflow_id}}\n\n    try:\n        state = langgraph_app.get_state(config)\n    except Exception:\n        raise HTTPException(404, \"Workflow not found\")\n\n    # Update state with human decision\n    state.values[\"approved\"] = approved\n    state.values[\"feedback\"] = feedback\n    state.values[\"human_reviewed\"] = True\n    langgraph_app.update_state(config, state.values)\n\n    # Resume workflow\n    result = langgraph_app.invoke(None, config=config)\n\n    return {\"status\": \"completed\", \"result\": result}\n```\n\n## Multiple Approval Points\n\n```python\n# Interrupt at multiple points\napp = workflow.compile(\n    interrupt_before=[\"first_review\", \"final_review\"]\n)\n\n# First review\nresult = app.invoke(initial_state, config=config)\n# ... human approves first review ...\napp.update_state(config, {\"first_approved\": True})\n\n# Continue to second review\nresult = app.invoke(None, config=config)\n# ... human approves final review ...\napp.update_state(config, {\"final_approved\": True})\n\n# Complete workflow\nresult = app.invoke(None, config=config)\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Interrupt point | Before critical nodes |\n| Timeout | 24-48h for human review |\n| Notification | Email/Slack when paused |\n| Fallback | Auto-reject after timeout |\n\n## Common Mistakes\n\n- No timeout (workflows hang forever)\n- No notification (humans don't know to review)\n- Losing checkpoint (can't resume)\n- No reject path (only approve works)\n\n## Related Skills\n\n- `langgraph-checkpoints` - State persistence\n- `langgraph-routing` - Routing after approval\n- `api-design-framework` - Review API design\n\n## Capability Details\n\n### interrupt-before\n**Keywords:** interrupt, pause, stop, before, gate\n**Solves:**\n- How do I pause a workflow for approval?\n- Add human review before a step\n- Interrupt workflow execution\n\n### resume-workflow\n**Keywords:** resume, continue, approve, proceed, update_state\n**Solves:**\n- How do I resume after human approval?\n- Continue workflow after review\n- Update state and proceed\n\n### approval-patterns\n**Keywords:** approval, approve, reject, decision, gate\n**Solves:**\n- How do I implement approval workflows?\n- Add approval gate to pipeline\n- Handle approve/reject decisions\n\n### feedback-integration\n**Keywords:** feedback, comment, review, notes, human input\n**Solves:**\n- How do I collect human feedback?\n- Integrate reviewer comments\n- Capture feedback in workflow state\n\n### interactive-supervision\n**Keywords:** supervise, monitor, interactive, control, override\n**Solves:**\n- How do I supervise agent execution?\n- Add human oversight to agents\n- Override agent decisions\n\n### state-inspection\n**Keywords:** get_state, inspect, view, current state, debug\n**Solves:**\n- How do I inspect workflow state?\n- View current state at interrupt\n- Debug paused workflows"
              },
              {
                "name": "langgraph-parallel",
                "description": "LangGraph parallel execution patterns. Use when implementing fan-out/fan-in workflows, map-reduce over tasks, or running independent agents concurrently.",
                "path": ".claude/skills/langgraph-parallel/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-parallel",
                  "description": "LangGraph parallel execution patterns. Use when implementing fan-out/fan-in workflows, map-reduce over tasks, or running independent agents concurrently.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Parallel Execution\n\nRun independent nodes concurrently for performance.\n\n## When to Use\n\n- Independent agents can run together\n- Map-reduce over task lists\n- Scatter-gather patterns\n- Performance optimization\n\n## Fan-Out/Fan-In Pattern\n\n```python\nfrom langgraph.graph import StateGraph\n\ndef fan_out(state):\n    \"\"\"Split work into parallel tasks.\"\"\"\n    state[\"tasks\"] = [{\"id\": 1}, {\"id\": 2}, {\"id\": 3}]\n    return state\n\ndef worker(state):\n    \"\"\"Process one task.\"\"\"\n    task = state[\"current_task\"]\n    result = process(task)\n    return {\"results\": [result]}\n\ndef fan_in(state):\n    \"\"\"Combine parallel results.\"\"\"\n    combined = aggregate(state[\"results\"])\n    return {\"final\": combined}\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"fan_out\", fan_out)\nworkflow.add_node(\"worker\", worker)\nworkflow.add_node(\"fan_in\", fan_in)\n\nworkflow.add_edge(\"fan_out\", \"worker\")\nworkflow.add_edge(\"worker\", \"fan_in\")  # Waits for all workers\n```\n\n## Using Send API\n\n```python\nfrom langgraph.constants import Send\n\ndef router(state):\n    \"\"\"Route to multiple workers in parallel.\"\"\"\n    return [\n        Send(\"worker\", {\"task\": task})\n        for task in state[\"tasks\"]\n    ]\n\nworkflow.add_conditional_edges(\"router\", router)\n```\n\n## Parallel Agent Analysis\n\n```python\nfrom typing import Annotated\nfrom operator import add\n\nclass AnalysisState(TypedDict):\n    content: str\n    findings: Annotated[list[dict], add]  # Accumulates\n\nasync def run_parallel_agents(state: AnalysisState):\n    \"\"\"Run multiple agents in parallel.\"\"\"\n    agents = [security_agent, tech_agent, quality_agent]\n\n    # Run all concurrently\n    tasks = [agent.analyze(state[\"content\"]) for agent in agents]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Filter successful results\n    findings = [r for r in results if not isinstance(r, Exception)]\n\n    return {\"findings\": findings}\n```\n\n## Map-Reduce Pattern\n\n```python\ndef map_node(state):\n    \"\"\"Map: Process each item independently.\"\"\"\n    items = state[\"items\"]\n    results = []\n\n    for item in items:\n        result = process_item(item)\n        results.append(result)\n\n    return {\"mapped_results\": results}\n\ndef reduce_node(state):\n    \"\"\"Reduce: Combine all results.\"\"\"\n    results = state[\"mapped_results\"]\n\n    summary = {\n        \"total\": len(results),\n        \"passed\": sum(1 for r in results if r[\"passed\"]),\n        \"failed\": sum(1 for r in results if not r[\"passed\"])\n    }\n\n    return {\"summary\": summary}\n```\n\n## Error Isolation\n\n```python\nasync def parallel_with_isolation(tasks: list):\n    \"\"\"Run parallel tasks, isolate failures.\"\"\"\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    successes = []\n    failures = []\n\n    for task, result in zip(tasks, results):\n        if isinstance(result, Exception):\n            failures.append({\"task\": task, \"error\": str(result)})\n        else:\n            successes.append(result)\n\n    return {\"successes\": successes, \"failures\": failures}\n```\n\n## Timeout per Branch\n\n```python\nimport asyncio\n\nasync def parallel_with_timeout(agents: list, content: str, timeout: int = 30):\n    \"\"\"Run agents with per-agent timeout.\"\"\"\n    async def run_with_timeout(agent):\n        try:\n            return await asyncio.wait_for(\n                agent.analyze(content),\n                timeout=timeout\n            )\n        except asyncio.TimeoutError:\n            return {\"agent\": agent.name, \"error\": \"timeout\"}\n\n    tasks = [run_with_timeout(a) for a in agents]\n    return await asyncio.gather(*tasks)\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Max parallel | 5-10 concurrent (avoid overwhelming APIs) |\n| Error handling | return_exceptions=True (don't fail all) |\n| Timeout | 30-60s per branch |\n| Accumulator | Use `Annotated[list, add]` for results |\n\n## Common Mistakes\n\n- No error isolation (one failure kills all)\n- No timeout (one slow branch blocks)\n- Sequential where parallel possible\n- Forgetting to wait for all branches\n\n## Related Skills\n\n- `langgraph-state` - Accumulating state\n- `multi-agent-orchestration` - Coordination patterns\n- `langgraph-supervisor` - Supervised parallel execution\n\n## Capability Details\n\n### fanout-pattern\n**Keywords:** fanout, parallel, concurrent, scatter\n**Solves:**\n- Run agents in parallel\n- Implement fan-out pattern\n- Distribute work across workers\n\n### fanin-pattern\n**Keywords:** fanin, gather, aggregate, collect\n**Solves:**\n- Aggregate parallel results\n- Implement fan-in pattern\n- Collect worker outputs\n\n### parallel-template\n**Keywords:** template, implementation, parallel, agent\n**Solves:**\n- Parallel agent fanout template\n- Production-ready code\n- Copy-paste implementation"
              },
              {
                "name": "langgraph-routing",
                "description": "LangGraph conditional routing patterns. Use when implementing dynamic routing based on state, creating branching workflows, or building retry loops with conditional edges.",
                "path": ".claude/skills/langgraph-routing/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-routing",
                  "description": "LangGraph conditional routing patterns. Use when implementing dynamic routing based on state, creating branching workflows, or building retry loops with conditional edges.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Conditional Routing\n\nRoute workflow execution dynamically based on state.\n\n## When to Use\n\n- Dynamic branching based on results\n- Retry loops with backoff\n- Quality gates with pass/fail paths\n- Multi-path workflows\n\n## Basic Conditional Edge\n\n```python\nfrom langgraph.graph import StateGraph, END\n\ndef route_based_on_quality(state: WorkflowState) -> str:\n    \"\"\"Decide next step based on quality score.\"\"\"\n    if state[\"quality_score\"] >= 0.8:\n        return \"publish\"\n    elif state[\"retry_count\"] < 3:\n        return \"retry\"\n    else:\n        return \"manual_review\"\n\nworkflow.add_conditional_edges(\n    \"quality_check\",\n    route_based_on_quality,\n    {\n        \"publish\": \"publish_node\",\n        \"retry\": \"generator\",\n        \"manual_review\": \"review_queue\"\n    }\n)\n```\n\n## Quality Gate Pattern\n\n```python\ndef route_after_quality_gate(state: AnalysisState) -> str:\n    \"\"\"Route based on quality gate result.\"\"\"\n    if state[\"quality_passed\"]:\n        return \"compress_findings\"\n    elif state[\"retry_count\"] < 2:\n        return \"supervisor\"  # Retry\n    else:\n        return END  # Return partial results\n\nworkflow.add_conditional_edges(\n    \"quality_gate\",\n    route_after_quality_gate,\n    {\n        \"compress_findings\": \"compress_findings\",\n        \"supervisor\": \"supervisor\",\n        END: END\n    }\n)\n```\n\n## Retry Loop Pattern\n\n```python\ndef llm_call_with_retry(state):\n    \"\"\"Retry failed LLM calls.\"\"\"\n    try:\n        result = call_llm(state[\"input\"])\n        state[\"output\"] = result\n        state[\"retry_count\"] = 0\n        return state\n    except Exception as e:\n        state[\"retry_count\"] += 1\n        state[\"error\"] = str(e)\n        return state\n\ndef should_retry(state) -> str:\n    if state.get(\"output\"):\n        return \"success\"\n    elif state[\"retry_count\"] < 3:\n        return \"retry\"\n    else:\n        return \"failed\"\n\nworkflow.add_conditional_edges(\n    \"llm_call\",\n    should_retry,\n    {\n        \"success\": \"next_step\",\n        \"retry\": \"llm_call\",  # Loop back\n        \"failed\": \"error_handler\"\n    }\n)\n```\n\n## Routing Patterns\n\n```\nSequential:    A  B  C              (simple edges)\nBranching:     A  (B or C)           (conditional edges)\nLooping:       A  B  A              (retry logic)\nConvergence:   (A or B)  C           (multiple inputs)\nDiamond:       A  (B, C)  D         (parallel then merge)\n```\n\n## State-Based Router\n\n```python\ndef dynamic_router(state: WorkflowState) -> str:\n    \"\"\"Route based on multiple state conditions.\"\"\"\n    if state.get(\"error\"):\n        return \"error_handler\"\n    if not state.get(\"validated\"):\n        return \"validator\"\n    if state[\"confidence\"] < 0.5:\n        return \"enhance\"\n    return \"finalize\"\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Max retries | 2-3 for LLM calls |\n| Fallback | Always have END fallback |\n| Routing function | Keep pure (no side effects) |\n| Edge mapping | Explicit mapping for clarity |\n\n## Common Mistakes\n\n- No END fallback (workflow hangs)\n- Infinite loops (no max retry)\n- Side effects in router (hard to debug)\n- Missing edge mappings (runtime error)\n\n## Related Skills\n\n- `langgraph-state` - State design for routing\n- `langgraph-supervisor` - Supervisor routing pattern\n- `agent-loops` - ReAct loop patterns\n\n## Capability Details\n\n### conditional-routing\n**Keywords:** conditional, branch, decision, if-else\n**Solves:**\n- Route based on conditions\n- Implement branching logic\n- Create decision nodes\n\n### semantic-routing\n**Keywords:** semantic, embedding, similarity, intent\n**Solves:**\n- Route by semantic similarity\n- Intent-based routing\n- Embedding-based decisions\n\n### router-template\n**Keywords:** template, router, semantic, implementation\n**Solves:**\n- Semantic router template\n- Production router code\n- Copy-paste implementation"
              },
              {
                "name": "langgraph-state",
                "description": "LangGraph state management patterns. Use when designing workflow state schemas, using TypedDict vs Pydantic, implementing accumulating state with Annotated operators, or managing shared state across nodes.",
                "path": ".claude/skills/langgraph-state/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-state",
                  "description": "LangGraph state management patterns. Use when designing workflow state schemas, using TypedDict vs Pydantic, implementing accumulating state with Annotated operators, or managing shared state across nodes.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph State Management\n\nDesign and manage state schemas for LangGraph workflows.\n\n## When to Use\n\n- Designing workflow state schemas\n- Choosing TypedDict vs Pydantic\n- Multi-agent state accumulation\n- State validation and typing\n\n## TypedDict Approach (Simple)\n\n```python\nfrom typing import TypedDict, Annotated\nfrom operator import add\n\nclass WorkflowState(TypedDict):\n    input: str\n    output: str\n    agent_responses: Annotated[list[dict], add]  # Accumulates\n    metadata: dict\n```\n\n## MessagesState Pattern (2026 Best Practice)\n\n```python\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\n\n# Option 1: Use built-in MessagesState (recommended)\nclass AgentState(MessagesState):\n    \"\"\"Extends MessagesState with custom fields.\"\"\"\n    user_id: str\n    context: dict\n\n# Option 2: Define messages manually with add_messages reducer\nclass CustomState(TypedDict):\n    messages: Annotated[list, add_messages]  # Smart append/update by ID\n    metadata: dict\n```\n\n**Why `add_messages` matters:**\n- Appends new messages (doesn't overwrite)\n- Updates existing messages by ID\n- Handles message deduplication automatically\n\n> **Note**: `MessageGraph` is deprecated in LangGraph v1.0.0. Use `StateGraph` with a `messages` key instead.\n\n## Pydantic Approach (Validation)\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass WorkflowState(BaseModel):\n    input: str = Field(description=\"User input\")\n    output: str = \"\"\n    agent_responses: list[dict] = Field(default_factory=list)\n\n    def add_response(self, agent: str, result: str):\n        self.agent_responses.append({\"agent\": agent, \"result\": result})\n```\n\n## Accumulating State Pattern\n\n```python\nfrom typing import Annotated\nfrom operator import add\n\nclass AnalysisState(TypedDict):\n    url: str\n    raw_content: str\n\n    # Accumulate agent outputs\n    findings: Annotated[list[Finding], add]\n    embeddings: Annotated[list[Embedding], add]\n\n    # Control flow\n    current_agent: str\n    agents_completed: list[str]\n    quality_passed: bool\n```\n\n**Key Pattern: `Annotated[list[T], add]`**\n- Without `add`: Each node replaces the list\n- With `add`: Each node appends to the list\n- Critical for multi-agent workflows\n\n## Custom Reducers\n\n```python\nfrom typing import Annotated\n\ndef merge_dicts(a: dict, b: dict) -> dict:\n    \"\"\"Custom reducer that merges dictionaries.\"\"\"\n    return {**a, **b}\n\nclass State(TypedDict):\n    config: Annotated[dict, merge_dicts]  # Merges updates\n\ndef last_value(a, b):\n    \"\"\"Keep only the latest value.\"\"\"\n    return b\n\nclass State(TypedDict):\n    status: Annotated[str, last_value]  # Overwrites\n```\n\n## State Immutability\n\n```python\ndef node(state: WorkflowState) -> WorkflowState:\n    \"\"\"Return new state, don't mutate in place.\"\"\"\n    # Wrong: state[\"output\"] = \"result\"\n    # Right:\n    return {\n        **state,\n        \"output\": \"result\"\n    }\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| TypedDict vs Pydantic | TypedDict for internal state, Pydantic at boundaries |\n| Messages state | Use `MessagesState` or `add_messages` reducer |\n| Accumulators | Always use `Annotated[list, add]` for multi-agent |\n| Nesting | Keep state flat (easier debugging) |\n| Immutability | Return new state, don't mutate |\n\n**2026 Guidance**: Use TypedDict inside the graph (lightweight, no runtime overhead). Use Pydantic at boundaries (inputs/outputs, user-facing data) for validation.\n\n## Common Mistakes\n\n- Forgetting `add` reducer (overwrites instead of accumulates)\n- Mutating state in place (breaks checkpointing)\n- Deeply nested state (hard to debug)\n- No type hints (lose IDE support)\n\n## Related Skills\n\n- `langgraph-routing` - Using state for routing decisions\n- `langgraph-checkpoints` - State persistence\n- `type-safety-validation` - Pydantic patterns\n\n## Capability Details\n\n### state-definition\n**Keywords:** StateGraph, TypedDict, state schema, define state\n**Solves:**\n- Define workflow state with TypedDict\n- Create Pydantic state models\n- Structure agent state properly\n\n### state-channels\n**Keywords:** channel, Annotated, state channel, MessageChannel\n**Solves:**\n- Configure state channels for data flow\n- Implement message accumulation\n- Handle channel-based state updates\n\n### state-reducers\n**Keywords:** reducer, add_messages, operator.add, accumulate\n**Solves:**\n- Implement state reducers with Annotated\n- Accumulate messages across nodes\n- Handle state merging strategies\n\n### subgraphs\n**Keywords:** subgraph, nested graph, parent state, child graph\n**Solves:**\n- Compose graphs with subgraphs\n- Pass state between parent and child\n- Implement modular workflow components\n\n### state-persistence\n**Keywords:** persist, state persistence, durable state, save state\n**Solves:**\n- Persist state across executions\n- Implement durable workflows\n- Handle state serialization"
              },
              {
                "name": "langgraph-supervisor",
                "description": "LangGraph supervisor-worker pattern. Use when building central coordinator agents that route to specialized workers, implementing round-robin or priority-based agent dispatch.",
                "path": ".claude/skills/langgraph-supervisor/SKILL.md",
                "frontmatter": {
                  "name": "langgraph-supervisor",
                  "description": "LangGraph supervisor-worker pattern. Use when building central coordinator agents that route to specialized workers, implementing round-robin or priority-based agent dispatch.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# LangGraph Supervisor Pattern\n\nCoordinate multiple specialized agents with a central supervisor.\n\n## When to Use\n\n- Multiple specialist agents\n- Central coordination needed\n- Dynamic agent routing\n- Progress tracking across agents\n\n## Basic Supervisor\n\n```python\nfrom langgraph.graph import StateGraph, END\n\ndef supervisor(state: WorkflowState) -> WorkflowState:\n    \"\"\"Route to next worker based on state.\"\"\"\n    if state[\"needs_analysis\"]:\n        state[\"next\"] = \"analyzer\"\n    elif state[\"needs_validation\"]:\n        state[\"next\"] = \"validator\"\n    else:\n        state[\"next\"] = END\n    return state\n\ndef analyzer(state: WorkflowState) -> WorkflowState:\n    \"\"\"Specialized analysis worker.\"\"\"\n    result = analyze(state[\"input\"])\n    state[\"results\"].append(result)\n    return state\n\n# Build graph\nworkflow = StateGraph(WorkflowState)\nworkflow.add_node(\"supervisor\", supervisor)\nworkflow.add_node(\"analyzer\", analyzer)\nworkflow.add_node(\"validator\", validator)\n\n# Supervisor routes dynamically\nworkflow.add_conditional_edges(\n    \"supervisor\",\n    lambda s: s[\"next\"],\n    {\n        \"analyzer\": \"analyzer\",\n        \"validator\": \"validator\",\n        END: END\n    }\n)\n\n# Workers return to supervisor\nworkflow.add_edge(\"analyzer\", \"supervisor\")\nworkflow.add_edge(\"validator\", \"supervisor\")\n\nworkflow.set_entry_point(\"supervisor\")\napp = workflow.compile()\n```\n\n## Round-Robin Supervisor\n\n```python\nALL_AGENTS = [\"security\", \"tech\", \"implementation\", \"tutorial\"]\n\ndef supervisor_node(state: AnalysisState) -> AnalysisState:\n    \"\"\"Route to next available agent.\"\"\"\n    completed = set(state[\"agents_completed\"])\n    available = [a for a in ALL_AGENTS if a not in completed]\n\n    if not available:\n        state[\"next\"] = \"quality_gate\"\n    else:\n        state[\"next\"] = available[0]\n\n    return state\n\n# Register all agent nodes\nfor agent_name in ALL_AGENTS:\n    workflow.add_node(agent_name, create_agent_node(agent_name))\n    workflow.add_edge(agent_name, \"supervisor\")\n```\n\n## Priority-Based Routing\n\n```python\nAGENT_PRIORITIES = {\n    \"security\": 1,    # Run first\n    \"tech\": 2,\n    \"implementation\": 3,\n    \"tutorial\": 4     # Run last\n}\n\ndef priority_supervisor(state: WorkflowState) -> WorkflowState:\n    \"\"\"Route by priority, not round-robin.\"\"\"\n    completed = set(state[\"agents_completed\"])\n    available = [a for a in AGENT_PRIORITIES if a not in completed]\n\n    if not available:\n        state[\"next\"] = \"finalize\"\n    else:\n        # Sort by priority\n        next_agent = min(available, key=lambda a: AGENT_PRIORITIES[a])\n        state[\"next\"] = next_agent\n\n    return state\n```\n\n## LLM-Based Supervisor (2026 Best Practice)\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\n# Define structured output schema\nclass SupervisorDecision(BaseModel):\n    \"\"\"Validated supervisor routing decision.\"\"\"\n    next_agent: Literal[\"security\", \"tech\", \"implementation\", \"tutorial\", \"DONE\"]\n    reasoning: str = Field(description=\"Brief explanation for routing decision\")\n\nasync def llm_supervisor(state: WorkflowState) -> WorkflowState:\n    \"\"\"Use LLM with structured output for reliable routing.\"\"\"\n    available = [a for a in AGENTS if a not in state[\"agents_completed\"]]\n\n    # Use structured output (2026 best practice)\n    decision = await llm.with_structured_output(SupervisorDecision).ainvoke(\n        f\"\"\"Task: {state['input']}\n\nCompleted: {state['agents_completed']}\nAvailable: {available}\n\nSelect the next agent or 'DONE' if all work is complete.\"\"\"\n    )\n\n    # Validated response - no string parsing needed\n    state[\"next\"] = END if decision.next_agent == \"DONE\" else decision.next_agent\n    state[\"routing_reasoning\"] = decision.reasoning  # Track decision rationale\n    return state\n\n# Alternative: OpenAI structured output\nasync def llm_supervisor_openai(state: WorkflowState) -> WorkflowState:\n    \"\"\"OpenAI with strict structured output.\"\"\"\n    response = await client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_format=SupervisorDecision\n    )\n    decision = response.choices[0].message.parsed\n    state[\"next\"] = END if decision.next_agent == \"DONE\" else decision.next_agent\n    return state\n```\n\n## Tracking Progress\n\n```python\ndef agent_node_factory(agent_name: str):\n    \"\"\"Create agent node that tracks completion.\"\"\"\n    async def node(state: WorkflowState) -> WorkflowState:\n        result = await agents[agent_name].run(state[\"input\"])\n\n        return {\n            **state,\n            \"results\": state[\"results\"] + [result],\n            \"agents_completed\": state[\"agents_completed\"] + [agent_name],\n            \"current_agent\": None\n        }\n    return node\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Routing strategy | Round-robin for uniform, priority for critical-first |\n| Max agents | 3-8 specialists (avoid overhead) |\n| Failure handling | Skip failed agent, continue with others |\n| Coordination | Centralized supervisor (simpler debugging) |\n\n## Common Mistakes\n\n- No completion tracking (runs agents forever)\n- Forgetting worker  supervisor edge\n- Missing END condition\n- Heavy supervisor logic (should be lightweight)\n\n## Related Skills\n\n- `langgraph-routing` - Conditional edges\n- `multi-agent-orchestration` - Fan-out patterns\n- `langgraph-state` - State for agent tracking\n\n## Capability Details\n\n### supervisor-design\n**Keywords:** supervisor, orchestration, routing, delegation\n**Solves:**\n- Design supervisor agent patterns\n- Route tasks to specialized workers\n- Coordinate multi-agent workflows\n\n### worker-delegation\n**Keywords:** worker, delegation, specialized, agent\n**Solves:**\n- Create specialized worker agents\n- Define worker capabilities\n- Implement delegation logic\n\n### skillforge-workflow\n**Keywords:** skillforge, analysis, content, workflow\n**Solves:**\n- SkillForge analysis workflow example\n- Production supervisor implementation\n- Real-world orchestration pattern\n\n### supervisor-template\n**Keywords:** template, implementation, code, starter\n**Solves:**\n- Supervisor workflow template\n- Production-ready code\n- Copy-paste implementation\n\n### content-analysis\n**Keywords:** content, analysis, graph, multi-agent\n**Solves:**\n- Content analysis graph template\n- SkillForge-specific workflow\n- Multi-agent content processing"
              },
              {
                "name": "llm-evaluation",
                "description": "LLM output evaluation and quality assessment. Use when implementing LLM-as-judge patterns, quality gates for AI outputs, or automated evaluation pipelines.",
                "path": ".claude/skills/llm-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "llm-evaluation",
                  "description": "LLM output evaluation and quality assessment. Use when implementing LLM-as-judge patterns, quality gates for AI outputs, or automated evaluation pipelines.",
                  "context": "fork",
                  "agent": "llm-integrator",
                  "version": "2.0.0",
                  "tags": [
                    "evaluation",
                    "llm",
                    "quality",
                    "ragas",
                    "langfuse",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/eval-metrics-collector.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/eval-metrics-collector.sh"
                      }
                    ]
                  }
                },
                "content": "# LLM Evaluation\n\nEvaluate and validate LLM outputs for quality assurance using RAGAS and LLM-as-judge patterns.\n\n## When to Use\n\n- Quality gates before publishing AI content\n- Automated testing of LLM outputs\n- Comparing model performance\n- Detecting hallucinations\n- A/B testing models\n\n## Quick Reference\n\n### LLM-as-Judge Pattern\n\n```python\nasync def evaluate_quality(input_text: str, output_text: str, dimension: str) -> float:\n    response = await llm.chat([{\n        \"role\": \"user\",\n        \"content\": f\"\"\"Evaluate for {dimension}. Score 1-10.\nInput: {input_text[:500]}\nOutput: {output_text[:1000]}\nRespond with just the number.\"\"\"\n    }])\n    return int(response.content.strip()) / 10\n```\n\n### Quality Gate\n\n```python\nQUALITY_THRESHOLD = 0.7\n\nasync def quality_gate(state: dict) -> dict:\n    scores = await full_quality_assessment(state[\"input\"], state[\"output\"])\n    passed = scores[\"average\"] >= QUALITY_THRESHOLD\n    return {**state, \"quality_passed\": passed}\n```\n\n### Hallucination Detection\n\n```python\nasync def detect_hallucination(context: str, output: str) -> dict:\n    # Check if output contains claims not in context\n    return {\"has_hallucinations\": bool, \"unsupported_claims\": []}\n```\n\n## RAGAS Metrics (2026)\n\n| Metric | Use Case | Threshold |\n|--------|----------|-----------|\n| Faithfulness | RAG grounding |  0.8 |\n| Answer Relevancy | Q&A systems |  0.7 |\n| Context Precision | Retrieval quality |  0.7 |\n| Context Recall | Retrieval completeness |  0.7 |\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n#  NEVER use same model as judge and evaluated\noutput = await gpt4.complete(prompt)\nscore = await gpt4.evaluate(output)  # Same model!\n\n#  NEVER use single dimension\nif relevance_score > 0.7:  # Only checking one thing\n    return \"pass\"\n\n#  NEVER set threshold too high\nTHRESHOLD = 0.95  # Blocks most content\n\n#  ALWAYS use different judge model\nscore = await gpt4_mini.evaluate(claude_output)\n\n#  ALWAYS use multiple dimensions\nscores = await evaluate_all_dimensions(output)\nif scores[\"average\"] > 0.7:\n    return \"pass\"\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Judge model | GPT-4o-mini or Claude Haiku |\n| Threshold | 0.7 for production, 0.6 for drafts |\n| Dimensions | 3-5 most relevant to use case |\n| Sample size | 50+ for reliable metrics |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/evaluation-metrics.md](references/evaluation-metrics.md) | RAGAS & LLM-as-judge metrics |\n| [examples/evaluation-patterns.md](examples/evaluation-patterns.md) | Complete evaluation examples |\n| [checklists/evaluation-checklist.md](checklists/evaluation-checklist.md) | Setup and review checklists |\n| [templates/evaluator-template.py](templates/evaluator-template.py) | Starter evaluation template |\n\n## Related Skills\n\n- `quality-gates` - Workflow quality control\n- `langfuse-observability` - Tracking evaluation scores\n- `agent-loops` - Self-correcting with evaluation\n\n## Capability Details\n\n### llm-as-judge\n**Keywords:** LLM judge, judge model, evaluation model, grader LLM\n**Solves:**\n- Use LLM to evaluate other LLM outputs\n- Implement judge prompts for quality\n- Configure evaluation criteria\n\n### ragas-metrics\n**Keywords:** RAGAS, faithfulness, answer relevancy, context precision\n**Solves:**\n- Evaluate RAG with RAGAS metrics\n- Measure faithfulness and relevancy\n- Assess context precision and recall\n\n### hallucination-detection\n**Keywords:** hallucination, factuality, grounded, verify facts\n**Solves:**\n- Detect hallucinations in LLM output\n- Verify factual accuracy\n- Implement grounding checks\n\n### quality-gates\n**Keywords:** quality gate, threshold, pass/fail, evaluation gate\n**Solves:**\n- Implement quality thresholds\n- Block low-quality outputs\n- Configure multi-metric gates\n\n### batch-evaluation\n**Keywords:** batch eval, dataset evaluation, bulk scoring, eval suite\n**Solves:**\n- Evaluate over golden datasets\n- Run batch evaluation pipelines\n- Generate evaluation reports\n\n### pairwise-comparison\n**Keywords:** pairwise, A/B comparison, side-by-side, preference\n**Solves:**\n- Compare two model outputs\n- Implement preference ranking\n- Run A/B evaluations"
              },
              {
                "name": "llm-safety-patterns",
                "description": "Use when securing LLM integrations against prompt injection, hallucination, or data leakage. Implements context separation, input validation, and output filtering patterns.",
                "path": ".claude/skills/llm-safety-patterns/SKILL.md",
                "frontmatter": {
                  "name": "llm-safety-patterns",
                  "description": "Use when securing LLM integrations against prompt injection, hallucination, or data leakage. Implements context separation, input validation, and output filtering patterns.",
                  "context": "fork",
                  "agent": "security-auditor",
                  "version": "1.0.0"
                },
                "content": "# LLM Safety Patterns\n\n## Overview\n\nDefensive patterns to protect LLM integrations against prompt injection, hallucination, and data leakage through layered validation and output filtering.\n\n## When to Use\n\n- Securing LLM-powered features in production\n- Implementing context separation for multi-tenant AI\n- Validating and filtering LLM outputs\n- Protecting against prompt injection attacks\n\n## The Core Principle\n\n> **Identifiers flow AROUND the LLM, not THROUGH it.**\n> **The LLM sees only content. Attribution happens deterministically.**\n\n## Why This Matters\n\nWhen identifiers appear in prompts, bad things happen:\n\n1. **Hallucination:** LLM invents IDs that don't exist\n2. **Confusion:** LLM mixes up which ID belongs where\n3. **Injection:** Attacker manipulates IDs via prompt injection\n4. **Leakage:** IDs appear in logs, caches, traces\n5. **Cross-tenant:** LLM could reference other users' data\n\n## The Architecture\n\n```\n\n                                                                         \n   SYSTEM CONTEXT (flows around LLM)                                     \n      \n    user_id  tenant_id  analysis_id  trace_id  permissions        \n      \n                                                                       \n                                                                       \n                                                                       \n                                                  \n    PRE-LLM                    POST-LLM     \n    FILTER          LLM          ATTRIBUTE    \n                                                                 \n    Returns         Sees ONLY:                       Adds:       \n    CONTENT         - content text                   - IDs       \n    (no IDs)        - context text                   - refs      \n           (NO IDs!)                           \n                                                 \n                                                                         \n\n```\n\n## What NEVER Goes in Prompts\n\n### SkillForge Forbidden Parameters\n\n| Parameter | Type | Why Forbidden |\n|-----------|------|---------------|\n| `user_id` | UUID | Can be hallucinated, enables cross-user access |\n| `tenant_id` | UUID | Critical for multi-tenant isolation |\n| `analysis_id` | UUID | Job tracking, not for LLM |\n| `document_id` | UUID | Source tracking, not for LLM |\n| `artifact_id` | UUID | Output tracking, not for LLM |\n| `chunk_id` | UUID | RAG reference, not for LLM |\n| `session_id` | str | Auth context, not for LLM |\n| `trace_id` | str | Observability, not for LLM |\n| Any UUID | UUID | Pattern: `[0-9a-f]{8}-...` |\n\n### Detection Pattern\n\n```python\nimport re\n\nFORBIDDEN_PATTERNS = [\n    r'user[_-]?id',\n    r'tenant[_-]?id',\n    r'analysis[_-]?id',\n    r'document[_-]?id',\n    r'artifact[_-]?id',\n    r'chunk[_-]?id',\n    r'session[_-]?id',\n    r'trace[_-]?id',\n    r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',\n]\n\ndef audit_prompt(prompt: str) -> list[str]:\n    \"\"\"Check for forbidden patterns in prompt\"\"\"\n    violations = []\n    for pattern in FORBIDDEN_PATTERNS:\n        if re.search(pattern, prompt, re.IGNORECASE):\n            violations.append(pattern)\n    return violations\n```\n\n## The Three-Phase Pattern\n\n### Phase 1: Pre-LLM (Filter & Extract)\n\n```python\nasync def prepare_for_llm(\n    query: str,\n    ctx: RequestContext,\n) -> tuple[str, list[str], SourceRefs]:\n    \"\"\"\n    Filter data and extract content for LLM.\n    Returns: (content, context_texts, source_references)\n    \"\"\"\n    # 1. Retrieve with tenant filter\n    documents = await semantic_search(\n        query_embedding=embed(query),\n        ctx=ctx,  # Filters by tenant_id, user_id\n    )\n\n    # 2. Save references for attribution\n    source_refs = SourceRefs(\n        document_ids=[d.id for d in documents],\n        chunk_ids=[c.id for c in chunks],\n    )\n\n    # 3. Extract content only (no IDs)\n    content_texts = [d.content for d in documents]\n\n    return query, content_texts, source_refs\n```\n\n### Phase 2: LLM Call (Content Only)\n\n```python\ndef build_prompt(content: str, context_texts: list[str]) -> str:\n    \"\"\"\n    Build prompt with ONLY content, no identifiers.\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze the following content and provide insights.\n\n    CONTENT:\n    {content}\n\n    RELEVANT CONTEXT:\n    {chr(10).join(f\"- {text}\" for text in context_texts)}\n\n    Provide analysis covering:\n    1. Key concepts\n    2. Prerequisites\n    3. Learning objectives\n    \"\"\"\n\n    # AUDIT: Verify no IDs leaked\n    violations = audit_prompt(prompt)\n    if violations:\n        raise SecurityError(f\"IDs leaked to prompt: {violations}\")\n\n    return prompt\n\nasync def call_llm(prompt: str) -> dict:\n    \"\"\"LLM only sees content, never IDs\"\"\"\n    response = await llm.generate(prompt)\n    return parse_response(response)\n```\n\n### Phase 3: Post-LLM (Attribute)\n\n```python\nasync def save_with_attribution(\n    llm_output: dict,\n    ctx: RequestContext,\n    source_refs: SourceRefs,\n) -> Analysis:\n    \"\"\"\n    Attach context and references to LLM output.\n    Attribution is deterministic, not LLM-generated.\n    \"\"\"\n    return await Analysis.create(\n        # Generated\n        id=uuid4(),\n\n        # From RequestContext (system-provided)\n        user_id=ctx.user_id,\n        tenant_id=ctx.tenant_id,\n        analysis_id=ctx.resource_id,\n        trace_id=ctx.trace_id,\n\n        # From Pre-LLM refs (deterministic)\n        source_document_ids=source_refs.document_ids,\n        source_chunk_ids=source_refs.chunk_ids,\n\n        # From LLM (content only)\n        content=llm_output[\"analysis\"],\n        key_concepts=llm_output[\"key_concepts\"],\n        difficulty=llm_output[\"difficulty\"],\n\n        # Metadata\n        created_at=datetime.utcnow(),\n        model_used=MODEL_NAME,\n    )\n```\n\n## Output Validation\n\nAfter LLM returns, validate:\n\n1. **Schema:** Response matches expected structure\n2. **Guardrails:** No toxic/harmful content\n3. **Grounding:** Claims are supported by provided context\n4. **No IDs:** LLM didn't hallucinate any IDs\n\n```python\nasync def validate_output(\n    llm_output: dict,\n    context_texts: list[str],\n) -> ValidationResult:\n    \"\"\"Validate LLM output before use\"\"\"\n\n    # 1. Schema validation\n    try:\n        parsed = AnalysisOutput.model_validate(llm_output)\n    except ValidationError as e:\n        return ValidationResult(valid=False, reason=f\"Schema error: {e}\")\n\n    # 2. Guardrails\n    if await contains_toxic_content(parsed.content):\n        return ValidationResult(valid=False, reason=\"Toxic content detected\")\n\n    # 3. Grounding check\n    if not is_grounded(parsed.content, context_texts):\n        return ValidationResult(valid=False, reason=\"Ungrounded claims\")\n\n    # 4. No hallucinated IDs\n    if contains_uuid_pattern(parsed.content):\n        return ValidationResult(valid=False, reason=\"Hallucinated IDs\")\n\n    return ValidationResult(valid=True)\n```\n\n## Integration Points in SkillForge\n\n### Content Analysis Workflow\n\n```\nbackend/app/workflows/\n agents/\n    execution.py        # Add context separation\n    prompts/            # Audit all prompts\n tasks/\n    generate_artifact.py  # Add attribution\n```\n\n### Services\n\n```\nbackend/app/services/\n embeddings/            # Pre-LLM filtering\n analysis/              # Post-LLM attribution\n```\n\n## Checklist Before Any LLM Call\n\n- [ ] RequestContext available\n- [ ] Data filtered by tenant_id and user_id\n- [ ] Content extracted without IDs\n- [ ] Source references saved\n- [ ] Prompt passes audit (no forbidden patterns)\n- [ ] Output validated before use\n- [ ] Attribution uses context, not LLM output\n\n---\n\n**Version:** 1.0.0 (December 2025)\n## Capability Details\n\n### context-separation\n**Keywords:** context separation, prompt context, id in prompt, parameterized\n**Solves:**\n- How do I prevent IDs from leaking into prompts?\n- How do I separate system context from prompt content?\n- What should never appear in LLM prompts?\n\n### pre-llm-filtering\n**Keywords:** pre-llm, rag filter, data filter, tenant filter\n**Solves:**\n- How do I filter data before sending to LLM?\n- How do I ensure tenant isolation in RAG?\n- How do I scope retrieval to current user?\n\n### post-llm-attribution\n**Keywords:** attribution, source tracking, provenance, citation\n**Solves:**\n- How do I track which sources the LLM used?\n- How do I attribute results correctly?\n- How do I avoid LLM-generated IDs?\n\n### output-guardrails\n**Keywords:** guardrail, output validation, hallucination, toxicity\n**Solves:**\n- How do I validate LLM output?\n- How do I detect hallucinations?\n- How do I prevent toxic content generation?\n\n### prompt-audit\n**Keywords:** prompt audit, prompt security, prompt injection\n**Solves:**\n- How do I verify no IDs leaked to prompts?\n- How do I audit prompts for security?\n- How do I prevent prompt injection?"
              },
              {
                "name": "llm-streaming",
                "description": "LLM streaming response patterns. Use when implementing real-time token streaming, Server-Sent Events for AI responses, or streaming with tool calls.",
                "path": ".claude/skills/llm-streaming/SKILL.md",
                "frontmatter": {
                  "name": "llm-streaming",
                  "description": "LLM streaming response patterns. Use when implementing real-time token streaming, Server-Sent Events for AI responses, or streaming with tool calls.",
                  "agent": "llm-integrator"
                },
                "content": "# LLM Streaming\n\nDeliver LLM responses in real-time for better UX.\n\n## When to Use\n\n- Chat interfaces (show tokens as generated)\n- Long responses (don't wait for completion)\n- Progress indication for slow operations\n- Streaming with function calls\n\n## Basic Streaming (OpenAI)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync def stream_response(prompt: str):\n    \"\"\"Stream tokens as they're generated.\"\"\"\n    stream = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n\n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n```\n\n## Streaming with Async\n\n```python\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def async_stream(prompt: str):\n    \"\"\"Async streaming for better concurrency.\"\"\"\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n```\n\n## FastAPI SSE Endpoint\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom sse_starlette.sse import EventSourceResponse\n\napp = FastAPI()\n\n@app.get(\"/chat/stream\")\nasync def stream_chat(prompt: str):\n    \"\"\"Server-Sent Events endpoint for streaming.\"\"\"\n    async def generate():\n        async for token in async_stream(prompt):\n            yield {\n                \"event\": \"token\",\n                \"data\": token\n            }\n        yield {\"event\": \"done\", \"data\": \"\"}\n\n    return EventSourceResponse(generate())\n```\n\n## Frontend SSE Consumer\n\n```typescript\nasync function streamChat(prompt: string, onToken: (t: string) => void) {\n  const response = await fetch(\"/chat/stream?prompt=\" + encodeURIComponent(prompt));\n  const reader = response.body?.getReader();\n  const decoder = new TextDecoder();\n\n  while (reader) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const text = decoder.decode(value);\n    const lines = text.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data !== '[DONE]') {\n          onToken(data);\n        }\n      }\n    }\n  }\n}\n\n// Usage\nlet fullResponse = '';\nawait streamChat('Hello', (token) => {\n  fullResponse += token;\n  setDisplayText(fullResponse);  // Update UI incrementally\n});\n```\n\n## Streaming with Tool Calls\n\n```python\nasync def stream_with_tools(messages: list, tools: list):\n    \"\"\"Handle streaming responses that include tool calls.\"\"\"\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=tools,\n        stream=True\n    )\n\n    collected_content = \"\"\n    collected_tool_calls = []\n\n    async for chunk in stream:\n        delta = chunk.choices[0].delta\n\n        # Collect content tokens\n        if delta.content:\n            collected_content += delta.content\n            yield {\"type\": \"content\", \"data\": delta.content}\n\n        # Collect tool call chunks\n        if delta.tool_calls:\n            for tc in delta.tool_calls:\n                # Tool calls come in chunks, accumulate them\n                if tc.index >= len(collected_tool_calls):\n                    collected_tool_calls.append({\n                        \"id\": tc.id,\n                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                    })\n\n                if tc.function.name:\n                    collected_tool_calls[tc.index][\"function\"][\"name\"] += tc.function.name\n                if tc.function.arguments:\n                    collected_tool_calls[tc.index][\"function\"][\"arguments\"] += tc.function.arguments\n\n    # If tool calls, execute them\n    if collected_tool_calls:\n        yield {\"type\": \"tool_calls\", \"data\": collected_tool_calls}\n```\n\n## Backpressure Handling\n\n```python\nimport asyncio\n\nasync def stream_with_backpressure(prompt: str, max_buffer: int = 100):\n    \"\"\"Handle slow consumers with backpressure.\"\"\"\n    buffer = asyncio.Queue(maxsize=max_buffer)\n\n    async def producer():\n        async for token in async_stream(prompt):\n            await buffer.put(token)  # Blocks if buffer full\n        await buffer.put(None)  # Signal completion\n\n    async def consumer():\n        while True:\n            token = await buffer.get()\n            if token is None:\n                break\n            yield token\n            await asyncio.sleep(0)  # Yield control\n\n    # Start producer in background\n    asyncio.create_task(producer())\n\n    # Return consumer generator\n    async for token in consumer():\n        yield token\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Protocol | SSE for web, WebSocket for bidirectional |\n| Buffer size | 50-200 tokens |\n| Timeout | 30-60s for long responses |\n| Retry | Reconnect on disconnect |\n\n## Common Mistakes\n\n- No timeout (hangs on network issues)\n- Missing error handling in stream\n- Not closing connections properly\n- Buffering entire response (defeats purpose)\n\n## Related Skills\n\n- `streaming-api-patterns` - SSE/WebSocket deep dive\n- `function-calling` - Tool calls in streams\n- `react-streaming-ui` - React streaming components\n\n## Capability Details\n\n### token-streaming\n**Keywords:** streaming, token, stream response, real-time, incremental\n**Solves:**\n- Stream tokens as they're generated\n- Display real-time LLM output\n- Reduce time to first byte\n\n### sse-responses\n**Keywords:** SSE, Server-Sent Events, event stream, text/event-stream\n**Solves:**\n- Implement SSE for streaming\n- Handle SSE reconnection\n- Parse SSE event data\n\n### streaming-with-tools\n**Keywords:** stream tools, tool streaming, function call stream\n**Solves:**\n- Stream responses with tool calls\n- Handle partial tool call data\n- Coordinate streaming and tool execution\n\n### partial-json-parsing\n**Keywords:** partial JSON, incremental parse, streaming JSON\n**Solves:**\n- Parse JSON as it streams\n- Handle incomplete JSON safely\n- Display partial structured data\n\n### stream-cancellation\n**Keywords:** cancel, abort, stop stream, AbortController\n**Solves:**\n- Cancel ongoing streams\n- Handle user interrupts\n- Clean up stream resources"
              },
              {
                "name": "llm-testing",
                "description": "Testing patterns for LLM-based applications. Use when testing AI/ML integrations, mocking LLM responses, testing async timeouts, or validating structured outputs from LLMs.",
                "path": ".claude/skills/llm-testing/SKILL.md",
                "frontmatter": {
                  "name": "llm-testing",
                  "description": "Testing patterns for LLM-based applications. Use when testing AI/ML integrations, mocking LLM responses, testing async timeouts, or validating structured outputs from LLMs.",
                  "context": "fork",
                  "agent": "test-generator",
                  "version": "2.0.0",
                  "tags": [
                    "testing",
                    "llm",
                    "ai",
                    "deepeval",
                    "ragas",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/eval-metrics-collector.sh"
                      }
                    ]
                  }
                },
                "content": "# LLM Testing Patterns\n\nTest AI applications with deterministic patterns using DeepEval and RAGAS.\n\n## When to Use\n\n- LLM integration testing\n- Async timeout validation\n- Structured output testing\n- Quality gate testing\n- RAG pipeline evaluation\n\n## Quick Reference\n\n### Mock LLM Responses\n\n```python\nfrom unittest.mock import AsyncMock, patch\n\n@pytest.fixture\ndef mock_llm():\n    mock = AsyncMock()\n    mock.return_value = {\"content\": \"Mocked response\", \"confidence\": 0.85}\n    return mock\n\n@pytest.mark.asyncio\nasync def test_with_mocked_llm(mock_llm):\n    with patch(\"app.core.model_factory.get_model\", return_value=mock_llm):\n        result = await synthesize_findings(sample_findings)\n    assert result[\"summary\"] is not None\n```\n\n### DeepEval Quality Testing\n\n```python\nfrom deepeval import assert_test\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n\ntest_case = LLMTestCase(\n    input=\"What is the capital of France?\",\n    actual_output=\"The capital of France is Paris.\",\n    retrieval_context=[\"Paris is the capital of France.\"],\n)\n\nmetrics = [\n    AnswerRelevancyMetric(threshold=0.7),\n    FaithfulnessMetric(threshold=0.8),\n]\n\nassert_test(test_case, metrics)\n```\n\n### Timeout Testing\n\n```python\nimport asyncio\nimport pytest\n\n@pytest.mark.asyncio\nasync def test_respects_timeout():\n    with pytest.raises(asyncio.TimeoutError):\n        async with asyncio.timeout(0.1):\n            await slow_llm_call()\n```\n\n## Quality Metrics (2026)\n\n| Metric | Threshold | Purpose |\n|--------|-----------|---------|\n| Answer Relevancy |  0.7 | Response addresses question |\n| Faithfulness |  0.8 | Output matches context |\n| Hallucination |  0.3 | No fabricated facts |\n| Context Precision |  0.7 | Retrieved contexts relevant |\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n#  NEVER test against live LLM APIs in CI\nresponse = await openai.chat.completions.create(...)\n\n#  NEVER use random seeds (non-deterministic)\nmodel.generate(seed=random.randint(0, 100))\n\n#  NEVER skip timeout handling\nawait llm_call()  # No timeout!\n\n#  ALWAYS mock LLM in unit tests\nwith patch(\"app.llm\", mock_llm):\n    result = await function_under_test()\n\n#  ALWAYS use VCR.py for integration tests\n@pytest.mark.vcr()\nasync def test_llm_integration():\n    ...\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Mock vs VCR | VCR for integration, mock for unit |\n| Timeout | Always test with < 1s timeout |\n| Schema validation | Test both valid and invalid |\n| Edge cases | Test all null/empty paths |\n| Quality metrics | Use multiple dimensions (3-5) |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/deepeval-ragas-api.md](references/deepeval-ragas-api.md) | DeepEval & RAGAS API reference |\n| [examples/test-patterns.md](examples/test-patterns.md) | Complete test examples |\n| [checklists/llm-test-checklist.md](checklists/llm-test-checklist.md) | Setup and review checklists |\n| [templates/llm-test-template.py](templates/llm-test-template.py) | Starter test template |\n\n## Related Skills\n\n- `vcr-http-recording` - Record LLM responses\n- `llm-evaluation` - Quality assessment\n- `unit-testing` - Test fundamentals\n\n## Capability Details\n\n### llm-response-mocking\n**Keywords:** mock LLM, fake response, stub LLM, mock AI\n**Solves:**\n- Mock LLM responses in tests\n- Create deterministic AI test fixtures\n- Avoid live API calls in CI\n\n### async-timeout-testing\n**Keywords:** timeout, async test, wait for, polling\n**Solves:**\n- Test async LLM operations\n- Handle timeout scenarios\n- Implement polling assertions\n\n### structured-output-validation\n**Keywords:** structured output, JSON validation, schema validation, output format\n**Solves:**\n- Validate structured LLM output\n- Test JSON schema compliance\n- Assert output structure\n\n### deepeval-assertions\n**Keywords:** DeepEval, assert_test, LLMTestCase, metric assertion\n**Solves:**\n- Use DeepEval for LLM assertions\n- Implement metric-based tests\n- Configure quality thresholds\n\n### golden-dataset-testing\n**Keywords:** golden dataset, golden test, reference output, expected output\n**Solves:**\n- Test against golden datasets\n- Compare with reference outputs\n- Implement regression testing\n\n### vcr-recording\n**Keywords:** VCR, cassette, record, replay, HTTP recording\n**Solves:**\n- Record LLM API responses\n- Replay recordings in tests\n- Create deterministic test suites"
              },
              {
                "name": "mcp-server-building",
                "description": "Building MCP (Model Context Protocol) servers for Claude extensibility. Use when creating custom tools, integrating external services, or extending Claude's capabilities.",
                "path": ".claude/skills/mcp-server-building/SKILL.md",
                "frontmatter": {
                  "name": "mcp-server-building",
                  "description": "Building MCP (Model Context Protocol) servers for Claude extensibility. Use when creating custom tools, integrating external services, or extending Claude's capabilities.",
                  "context": "fork",
                  "agent": "backend-system-architect"
                },
                "content": "# MCP Server Building\nBuild custom MCP servers to extend Claude with tools, resources, and prompts.\n\n## When to Use\n\n- Extending Claude with custom tools and capabilities\n- Integrating external APIs and services with Claude\n- Building domain-specific Claude extensions\n- Creating reusable tool packages for Claude Desktop\n\n## Core Concepts\n\n### MCP Architecture\n```\n     JSON-RPC      \n   Claude     MCP Server  \n   (Host)       stdio/SSE/WS      (Tools)    \n                    \n```\n\n**Three Primitives**:\n- **Tools**: Functions Claude can call (with user approval)\n- **Resources**: Data Claude can read (files, API responses)\n- **Prompts**: Pre-defined prompt templates\n\n## Quick Start\n\n### Minimal Python Server (stdio)\n```python\n# server.py\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import Tool, TextContent\n\nserver = Server(\"my-tools\")\n\n@server.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(\n            name=\"greet\",\n            description=\"Greet a user by name\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\", \"description\": \"Name to greet\"}\n                },\n                \"required\": [\"name\"]\n            }\n        )\n    ]\n\n@server.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    if name == \"greet\":\n        return [TextContent(type=\"text\", text=f\"Hello, {arguments['name']}!\")]\n    raise ValueError(f\"Unknown tool: {name}\")\n\nasync def main():\n    async with stdio_server() as (read, write):\n        await server.run(read, write, server.create_initialization_options())\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n### TypeScript Server (recommended for production)\n```typescript\n// src/index.ts\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n  CallToolRequestSchema,\n  ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n  { name: \"my-tools\", version: \"1.0.0\" },\n  { capabilities: { tools: {} } }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => ({\n  tools: [\n    {\n      name: \"fetch_url\",\n      description: \"Fetch content from a URL\",\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          url: { type: \"string\", description: \"URL to fetch\" },\n        },\n        required: [\"url\"],\n      },\n    },\n  ],\n}));\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n  if (request.params.name === \"fetch_url\") {\n    const { url } = request.params.arguments as { url: string };\n    const response = await fetch(url);\n    const text = await response.text();\n    return { content: [{ type: \"text\", text }] };\n  }\n  throw new Error(\"Unknown tool: \" + request.params.name);\n});\n\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n## Tool Definition Patterns\n\n### Input Schema Best Practices\n```python\nTool(\n    name=\"search_database\",\n    description=\"Search the product database. Returns up to 10 results.\",\n    inputSchema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search query (supports wildcards with *)\"\n            },\n            \"category\": {\n                \"type\": \"string\",\n                \"enum\": [\"electronics\", \"clothing\", \"books\"],\n                \"description\": \"Filter by category\"\n            },\n            \"max_results\": {\n                \"type\": \"integer\",\n                \"minimum\": 1,\n                \"maximum\": 50,\n                \"default\": 10,\n                \"description\": \"Maximum results to return\"\n            }\n        },\n        \"required\": [\"query\"]\n    }\n)\n```\n\n**Guidelines**:\n- Always include `description` for each property\n- Use `enum` for fixed option sets\n- Set `minimum`/`maximum` for numbers\n- Mark `required` fields explicitly\n- Provide `default` values where sensible\n\n### Error Handling\n```python\n@server.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    try:\n        if name == \"query_api\":\n            result = await external_api.query(arguments[\"query\"])\n            return [TextContent(type=\"text\", text=json.dumps(result))]\n    except ExternalAPIError as e:\n        # Return error as text - Claude will see and handle it\n        return [TextContent(\n            type=\"text\",\n            text=f\"Error: API returned {e.status_code}: {e.message}\"\n        )]\n    except Exception as e:\n        # Log internally, return user-friendly message\n        logger.exception(\"Tool execution failed\")\n        return [TextContent(\n            type=\"text\",\n            text=f\"Error: {type(e).__name__}: {str(e)}\"\n        )]\n```\n\n## Resource Patterns\n\n### File Resources\n```python\n@server.list_resources()\nasync def list_resources() -> list[Resource]:\n    return [\n        Resource(\n            uri=\"file:///config/settings.json\",\n            name=\"Settings\",\n            mimeType=\"application/json\",\n            description=\"Application configuration\"\n        )\n    ]\n\n@server.read_resource()\nasync def read_resource(uri: str) -> str:\n    if uri == \"file:///config/settings.json\":\n        return Path(\"settings.json\").read_text()\n    raise ValueError(f\"Unknown resource: {uri}\")\n```\n\n### Dynamic Resources (API data)\n```python\n@server.list_resources()\nasync def list_resources() -> list[Resource]:\n    # List available data sources\n    return [\n        Resource(\n            uri=\"api://users/current\",\n            name=\"Current User\",\n            mimeType=\"application/json\"\n        ),\n        Resource(\n            uri=\"api://metrics/today\",\n            name=\"Today's Metrics\",\n            mimeType=\"application/json\"\n        )\n    ]\n\n@server.read_resource()\nasync def read_resource(uri: str) -> str:\n    if uri.startswith(\"api://\"):\n        endpoint = uri.replace(\"api://\", \"\")\n        data = await api_client.get(endpoint)\n        return json.dumps(data, indent=2)\n```\n\n## Transport Options\n\n### stdio (recommended for CLI)\n```json\n// claude_desktop_config.json\n{\n  \"mcpServers\": {\n    \"my-tools\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/server.py\"],\n      \"env\": {\n        \"API_KEY\": \"xxx\"\n      }\n    }\n  }\n}\n```\n\n### SSE (for web deployments)\n```python\nfrom mcp.server.sse import SseServerTransport\nfrom starlette.applications import Starlette\nfrom starlette.routing import Route\n\nsse = SseServerTransport(\"/messages\")\n\nasync def handle_sse(request):\n    async with sse.connect_sse(\n        request.scope, request.receive, request._send\n    ) as streams:\n        await server.run(\n            streams[0], streams[1],\n            server.create_initialization_options()\n        )\n\napp = Starlette(routes=[\n    Route(\"/sse\", endpoint=handle_sse),\n    Route(\"/messages\", endpoint=sse.handle_post_message, methods=[\"POST\"]),\n])\n```\n\n## Configuration in Claude Desktop\n\n```json\n// ~/Library/Application Support/Claude/claude_desktop_config.json (macOS)\n// %APPDATA%\\Claude\\claude_desktop_config.json (Windows)\n{\n  \"mcpServers\": {\n    \"database\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@myorg/db-tools\"],\n      \"env\": {\n        \"DATABASE_URL\": \"postgres://...\"\n      }\n    },\n    \"python-tools\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"-m\", \"my_mcp_server\"],\n      \"cwd\": \"/path/to/project\"\n    }\n  }\n}\n```\n\n## Testing\n\n### Manual Testing\n```bash\n# Test with MCP Inspector\nnpx @modelcontextprotocol/inspector python server.py\n```\n\n### Automated Testing\n```python\nimport pytest\nfrom mcp.client import Client\nfrom mcp.client.stdio import stdio_client\n\n@pytest.mark.asyncio\nasync def test_greet_tool():\n    async with stdio_client(\"python\", [\"server.py\"]) as (read, write):\n        client = Client(\"test\", \"1.0.0\")\n        await client.connect(read, write)\n\n        # List tools\n        tools = await client.list_tools()\n        assert any(t.name == \"greet\" for t in tools.tools)\n\n        # Call tool\n        result = await client.call_tool(\"greet\", {\"name\": \"World\"})\n        assert \"Hello, World!\" in result.content[0].text\n```\n\n## Common Patterns\n\n### Caching Expensive Operations\n```python\nfrom functools import lru_cache\nfrom datetime import datetime, timedelta\n\n_cache = {}\n_cache_ttl = timedelta(minutes=5)\n\nasync def get_cached_data(key: str) -> dict:\n    now = datetime.now()\n    if key in _cache:\n        data, timestamp = _cache[key]\n        if now - timestamp < _cache_ttl:\n            return data\n\n    data = await expensive_fetch(key)\n    _cache[key] = (data, now)\n    return data\n```\n\n### Rate Limiting\n```python\nimport asyncio\nfrom collections import defaultdict\n\n_request_times = defaultdict(list)\nMAX_REQUESTS_PER_MINUTE = 60\n\nasync def rate_limited_call(user_id: str, func, *args):\n    now = asyncio.get_event_loop().time()\n    _request_times[user_id] = [\n        t for t in _request_times[user_id]\n        if now - t < 60\n    ]\n\n    if len(_request_times[user_id]) >= MAX_REQUESTS_PER_MINUTE:\n        raise Exception(\"Rate limit exceeded. Try again in a minute.\")\n\n    _request_times[user_id].append(now)\n    return await func(*args)\n```\n\n## Anti-Patterns\n\n1. **Stateful tools without cleanup**: Always clean up connections/resources\n2. **Blocking synchronous code**: Use `asyncio.to_thread()` for blocking ops\n3. **Missing input validation**: Always validate before processing\n4. **Secrets in tool output**: Never return API keys or credentials\n5. **Unbounded responses**: Limit response sizes (Claude has context limits)\n\n## Resources\n- MCP Specification: https://modelcontextprotocol.io/docs\n- Python SDK: https://github.com/modelcontextprotocol/python-sdk\n- TypeScript SDK: https://github.com/modelcontextprotocol/typescript-sdk\n- Example Servers: https://github.com/modelcontextprotocol/servers"
              },
              {
                "name": "motion-animation-patterns",
                "description": "Use this skill for implementing Motion (Framer Motion) animations in React applications. Covers animation presets, page transitions, modal animations, list stagger effects, hover interactions, skeleton loaders, and RTL-aware animation patterns.",
                "path": ".claude/skills/motion-animation-patterns/SKILL.md",
                "frontmatter": {
                  "name": "motion-animation-patterns",
                  "description": "Use this skill for implementing Motion (Framer Motion) animations in React applications. Covers animation presets, page transitions, modal animations, list stagger effects, hover interactions, skeleton loaders, and RTL-aware animation patterns.",
                  "context": "fork",
                  "agent": "rapid-ui-designer",
                  "version": "1.0.0",
                  "author": "Yonatan Gross",
                  "tags": [
                    "motion",
                    "framer-motion",
                    "animation",
                    "react",
                    "ux",
                    "transitions",
                    "hover",
                    "stagger",
                    "skeleton"
                  ]
                },
                "content": "# Motion Animation Patterns\n\n## Overview\n\nThis skill provides comprehensive guidance for implementing Motion (Framer Motion) animations in React 19 applications. It ensures consistent, performant, and accessible animations across the UI using centralized animation presets.\n\n**When to use this skill:**\n- Adding page transition animations\n- Implementing modal/dialog entrance/exit animations\n- Creating staggered list animations\n- Adding hover and tap micro-interactions\n- Implementing skeleton loading states\n- Creating collapse/expand animations\n- Building toast/notification animations\n\n**Bundled Resources:**\n- `references/animation-presets.md` - Complete preset API reference\n- `examples/component-patterns.md` - Common animation patterns\n\n---\n\n## Core Architecture\n\n### Animation Presets Library (`frontend/src/lib/animations.ts`)\n\nAll animations MUST use the centralized `animations.ts` presets. This ensures:\n- Consistent motion language across the app\n- RTL-aware animations (Hebrew support)\n- Performance optimization\n- Easy maintainability\n\n```typescript\n//  CORRECT: Import from animations.ts\nimport { motion, AnimatePresence } from 'motion/react';\nimport { fadeIn, slideUp, staggerContainer, modalContent } from '@/lib/animations';\n\n//  WRONG: Inline animation values\n<motion.div initial={{ opacity: 0 }} animate={{ opacity: 1 }}>\n```\n\n---\n\n## Available Presets\n\n### Transition Timing\n\n| Preset | Duration | Ease | Use For |\n|--------|----------|------|---------|\n| `transitions.fast` | 0.15s | easeOut | Micro-interactions |\n| `transitions.normal` | 0.2s | easeOut | Most animations |\n| `transitions.slow` | 0.3s | easeInOut | Emphasis effects |\n| `transitions.spring` | spring | 300/25 | Playful elements |\n| `transitions.gentleSpring` | spring | 200/20 | Modals/overlays |\n\n### Basic Animations\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `fadeIn` | Opacity fade | Simple reveal |\n| `fadeScale` | Fade + slight scale | Subtle emphasis |\n| `scaleIn` | Fade + scale from center | Badges, buttons |\n\n### Slide Animations (RTL-Aware)\n\n| Preset | Direction | Use For |\n|--------|-----------|---------|\n| `slideInRight` | Right to center | RTL Hebrew UI (natural) |\n| `slideInLeft` | Left to center | LTR content |\n| `slideUp` | Bottom to center | Cards, panels |\n| `slideDown` | Top to center | Dropdowns |\n\n### List/Stagger Animations\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `staggerContainer` | Parent with stagger | List wrappers |\n| `staggerContainerFast` | Fast stagger | Quick lists |\n| `staggerItem` | Fade + slide child | List items |\n| `staggerItemRight` | RTL slide child | Hebrew lists |\n\n### Modal/Dialog Animations\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `modalBackdrop` | Overlay fade | Modal background |\n| `modalContent` | Scale + fade | Modal body |\n| `sheetContent` | Slide from bottom | Mobile sheets |\n| `dropdownDown` | Scale from top | Dropdown menus |\n| `dropdownUp` | Scale from bottom | Context menus |\n\n### Page Transitions\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `pageFade` | Simple fade | Route changes |\n| `pageSlide` | RTL slide | Navigation |\n\n### Micro-Interactions\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `tapScale` | Scale on tap | Buttons, cards |\n| `hoverLift` | Lift + shadow | Cards, list items |\n| `buttonPress` | Press effect | Interactive buttons |\n| `cardHover` | Hover emphasis | Card components |\n\n### Loading States\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `pulse` | Opacity pulse | Skeleton loaders |\n| `shimmer` | Sliding highlight | Shimmer effect |\n\n### Utility Animations\n\n| Preset | Effect | Use For |\n|--------|--------|---------|\n| `toastSlideIn` | Slide + scale | Notifications |\n| `collapse` | Height animation | Accordions |\n\n---\n\n## Implementation Patterns\n\n### 1. Page Transitions\n\nWrap routes with `AnimatePresence` for smooth page changes:\n\n```tsx\n// frontend/src/components/AnimatedRoutes.tsx\nimport { Routes, Route, useLocation } from 'react-router';\nimport { AnimatePresence, motion } from 'motion/react';\nimport { pageFade } from '@/lib/animations';\n\nexport function AnimatedRoutes() {\n  const location = useLocation();\n\n  return (\n    <AnimatePresence mode=\"wait\">\n      <motion.div key={location.pathname} {...pageFade} className=\"min-h-screen\">\n        <Routes location={location}>\n          {/* routes */}\n        </Routes>\n      </motion.div>\n    </AnimatePresence>\n  );\n}\n```\n\n### 2. Modal Animations\n\nUse `AnimatePresence` for enter/exit animations:\n\n```tsx\nimport { motion, AnimatePresence } from 'motion/react';\nimport { modalBackdrop, modalContent } from '@/lib/animations';\n\nfunction Modal({ isOpen, onClose, children }) {\n  return (\n    <AnimatePresence>\n      {isOpen && (\n        <>\n          <motion.div\n            {...modalBackdrop}\n            className=\"fixed inset-0 z-50 bg-black/50\"\n            onClick={onClose}\n          />\n          <motion.div\n            {...modalContent}\n            className=\"fixed inset-0 z-50 flex items-center justify-center p-4 pointer-events-none\"\n          >\n            <div className=\"bg-white rounded-2xl p-6 pointer-events-auto\">\n              {children}\n            </div>\n          </motion.div>\n        </>\n      )}\n    </AnimatePresence>\n  );\n}\n```\n\n### 3. Staggered List Animations\n\nUse parent container with child variants:\n\n```tsx\nimport { motion } from 'motion/react';\nimport { staggerContainer, staggerItem } from '@/lib/animations';\n\nfunction ItemList({ items }) {\n  return (\n    <motion.ul\n      variants={staggerContainer}\n      initial=\"initial\"\n      animate=\"animate\"\n      className=\"space-y-2\"\n    >\n      {items.map((item) => (\n        <motion.li key={item.id} variants={staggerItem}>\n          <ItemCard item={item} />\n        </motion.li>\n      ))}\n    </motion.ul>\n  );\n}\n```\n\n### 4. Card Hover Interactions\n\nApply micro-interactions to cards:\n\n```tsx\nimport { motion } from 'motion/react';\nimport { cardHover, tapScale } from '@/lib/animations';\n\nfunction Card({ onClick, children }) {\n  return (\n    <motion.div\n      {...cardHover}\n      {...tapScale}\n      onClick={onClick}\n      className=\"p-4 rounded-lg bg-white cursor-pointer\"\n    >\n      {children}\n    </motion.div>\n  );\n}\n```\n\n### 5. Skeleton Loaders with Motion\n\nUse Motion pulse for consistent animation:\n\n```tsx\nimport { motion } from 'motion/react';\nimport { pulse } from '@/lib/animations';\n\nfunction Skeleton({ className }) {\n  return (\n    <motion.div\n      variants={pulse}\n      initial=\"initial\"\n      animate=\"animate\"\n      className={\"bg-gray-200 rounded \" + className}\n      aria-hidden=\"true\"\n    />\n  );\n}\n```\n\n### 6. Collapse/Expand Animations\n\nFor accordions and expandable sections:\n\n```tsx\nimport { motion, AnimatePresence } from 'motion/react';\nimport { collapse } from '@/lib/animations';\n\nfunction Accordion({ isExpanded, children }) {\n  return (\n    <AnimatePresence>\n      {isExpanded && (\n        <motion.div {...collapse} className=\"overflow-hidden\">\n          {children}\n        </motion.div>\n      )}\n    </AnimatePresence>\n  );\n}\n```\n\n---\n\n## AnimatePresence Rules\n\n**MANDATORY**: Use `AnimatePresence` for exit animations:\n\n```tsx\n//  CORRECT: Wrap conditional renders\n<AnimatePresence>\n  {isVisible && (\n    <motion.div {...fadeIn}>Content</motion.div>\n  )}\n</AnimatePresence>\n\n//  WRONG: No exit animation\n{isVisible && (\n  <motion.div {...fadeIn}>Content</motion.div>\n)}\n```\n\n**Mode options:**\n- `mode=\"wait\"` - Wait for exit before enter (page transitions)\n- `mode=\"popLayout\"` - Layout animations for removing items\n- Default - Simultaneous enter/exit\n\n---\n\n## RTL/Hebrew Considerations\n\nThe animation presets are RTL-aware:\n- `slideInRight` - Natural entry direction for Hebrew\n- `staggerItemRight` - RTL list animations\n- `pageSlide` - Pages slide from left (correct for RTL)\n\n---\n\n## Performance Best Practices\n\n1. **Use preset transitions**: Already optimized\n2. **Avoid layout animations on large lists**: Can cause jank\n3. **Use `layout` prop sparingly**: Only when needed\n4. **Prefer opacity/transform**: Hardware accelerated\n5. **Don't animate width/height directly**: Use `collapse` preset\n\n```tsx\n//  CORRECT: Transform-based\n<motion.div {...slideUp}>\n\n//  AVOID: Layout-heavy\n<motion.div animate={{ width: '100%', marginLeft: '20px' }}>\n```\n\n---\n\n## Testing Animations\n\nVerify 60fps performance:\n1. Open Chrome DevTools > Performance tab\n2. Record while triggering animations\n3. Check for frame drops below 60fps\n\n---\n\n## Checklist for New Components\n\nWhen adding animations:\n\n- [ ] Import from `@/lib/animations`, not inline values\n- [ ] Use `AnimatePresence` for conditional renders\n- [ ] Apply appropriate preset for the interaction type\n- [ ] Test with RTL locale (Hebrew)\n- [ ] Verify 60fps performance\n- [ ] Ensure animations don't block user interaction\n\n---\n\n## Anti-Patterns (FORBIDDEN)\n\n```tsx\n//  NEVER use inline animation values\n<motion.div initial={{ opacity: 0 }} animate={{ opacity: 1 }}>\n\n//  NEVER animate without AnimatePresence for conditionals\n{isOpen && <motion.div exit={{ opacity: 0 }}>}\n\n//  NEVER animate layout-heavy properties\n<motion.div animate={{ width: newWidth, height: newHeight }}>\n\n//  NEVER use CSS transitions alongside Motion\n<motion.div {...fadeIn} className=\"transition-all duration-300\">\n```\n\n---\n\n## Integration with Agents\n\n### Frontend UI Developer\n- Uses animation presets for all motion effects\n- References this skill for implementation patterns\n- Ensures consistent animation language\n\n### Rapid UI Designer\n- Specifies animation types in design specs\n- References available presets for motion design\n\n### Code Quality Reviewer\n- Checks for inline animation anti-patterns\n- Validates AnimatePresence usage\n- Ensures performance best practices\n\n---\n\n**Skill Version**: 1.0.0\n**Last Updated**: 2026-01-06\n**Maintained by**: Yonatan Gross\n\n## Capability Details\n\n### animation-presets\n**Keywords:** animation, motion, preset, fadeIn, slideUp, scaleIn\n**Solves:**\n- How do I create consistent animations?\n- What animation presets are available?\n- Where should I define animations?\n\n### page-transitions\n**Keywords:** page, transition, route, navigation, AnimatePresence\n**Solves:**\n- How do I animate page transitions?\n- Add route change animations\n- AnimatePresence for page exits\n\n### modal-animations\n**Keywords:** modal, dialog, overlay, backdrop, entrance, exit\n**Solves:**\n- How do I animate modals?\n- Dialog entrance/exit animations\n- Backdrop fade effects\n\n### stagger-animations\n**Keywords:** stagger, list, children, delay, sequence\n**Solves:**\n- How do I stagger list animations?\n- Animate children sequentially\n- List item entrance effects\n\n### hover-interactions\n**Keywords:** hover, tap, whileHover, whileTap, micro-interaction\n**Solves:**\n- How do I add hover effects?\n- Button press animations\n- Micro-interactions for buttons\n\n### skeleton-loaders\n**Keywords:** skeleton, loading, pulse, placeholder, shimmer\n**Solves:**\n- How do I create skeleton loaders?\n- Animated loading placeholders\n- Pulse animation for loading states\n\n### rtl-animations\n**Keywords:** rtl, ltr, hebrew, arabic, direction, i18n\n**Solves:**\n- How do I handle RTL animations?\n- Direction-aware slide animations\n- Hebrew/Arabic animation support\n\n### collapse-expand\n**Keywords:** collapse, expand, accordion, height, auto\n**Solves:**\n- How do I animate height changes?\n- Accordion expand/collapse\n- Animate to auto height"
              },
              {
                "name": "msw-mocking",
                "description": "Mock Service Worker (MSW) 2.x for API mocking. Use when testing frontend components with network mocking, simulating API errors, or creating deterministic API responses in tests.",
                "path": ".claude/skills/msw-mocking/SKILL.md",
                "frontmatter": {
                  "name": "msw-mocking",
                  "description": "Mock Service Worker (MSW) 2.x for API mocking. Use when testing frontend components with network mocking, simulating API errors, or creating deterministic API responses in tests.",
                  "context": "fork",
                  "agent": "test-generator",
                  "version": "2.0.0",
                  "tags": [
                    "msw",
                    "testing",
                    "mocking",
                    "frontend",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# MSW (Mock Service Worker) 2.x\n\nNetwork-level API mocking for frontend tests using MSW 2.x.\n\n## When to Use\n\n- Frontend component testing\n- Simulating API responses and errors\n- Network delay simulation\n- GraphQL mocking\n- WebSocket mocking (NEW in 2.x)\n\n## Quick Reference\n\n```typescript\n// Core imports\nimport { http, HttpResponse, graphql, ws, delay, passthrough } from 'msw';\nimport { setupServer } from 'msw/node';\n\n// Basic handler\nhttp.get('/api/users/:id', ({ params }) => {\n  return HttpResponse.json({ id: params.id, name: 'User' });\n});\n\n// Error response\nhttp.get('/api/fail', () => {\n  return HttpResponse.json({ error: 'Not found' }, { status: 404 });\n});\n\n// Delay simulation\nhttp.get('/api/slow', async () => {\n  await delay(2000);\n  return HttpResponse.json({ data: 'response' });\n});\n\n// Passthrough (NEW in 2.x)\nhttp.get('/api/real', () => passthrough());\n```\n\n## Test Setup\n\n```typescript\n// vitest.setup.ts\nimport { beforeAll, afterEach, afterAll } from 'vitest';\nimport { server } from './src/mocks/server';\n\nbeforeAll(() => server.listen({ onUnhandledRequest: 'error' }));\nafterEach(() => server.resetHandlers());\nafterAll(() => server.close());\n```\n\n## Runtime Override\n\n```typescript\nimport { http, HttpResponse } from 'msw';\nimport { server } from '../mocks/server';\n\ntest('shows error on API failure', async () => {\n  server.use(\n    http.get('/api/users/:id', () => {\n      return HttpResponse.json({ error: 'Not found' }, { status: 404 });\n    })\n  );\n\n  render(<UserProfile id=\"123\" />);\n  expect(await screen.findByText(/not found/i)).toBeInTheDocument();\n});\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```typescript\n//  NEVER mock fetch directly\njest.spyOn(global, 'fetch').mockResolvedValue(...)\n\n//  NEVER mock axios module\njest.mock('axios')\n\n//  NEVER test implementation details\nexpect(fetch).toHaveBeenCalledWith('/api/...')\n\n//  ALWAYS use MSW\nserver.use(http.get('/api/...', () => HttpResponse.json({...})))\n\n//  ALWAYS test user-visible behavior\nexpect(await screen.findByText('Success')).toBeInTheDocument()\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Handler location | `src/mocks/handlers.ts` |\n| Default behavior | Return success |\n| Override scope | Per-test with `server.use()` |\n| Unhandled requests | Error (catch missing mocks) |\n| GraphQL | Use `graphql.query/mutation` |\n| WebSocket | Use `ws.link()` for WS mocking |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/msw-2x-api.md](references/msw-2x-api.md) | Complete MSW 2.x API reference |\n| [examples/handler-patterns.md](examples/handler-patterns.md) | CRUD, auth, error, and upload examples |\n| [checklists/msw-setup-checklist.md](checklists/msw-setup-checklist.md) | Setup and review checklists |\n| [templates/handlers-template.ts](templates/handlers-template.ts) | Starter template for new handlers |\n\n## Related Skills\n\n- `unit-testing` - Component isolation\n- `integration-testing` - Full integration tests\n- `vcr-http-recording` - Python equivalent\n\n## Capability Details\n\n### http-request-mocking\n**Keywords:** http.get, http.post, http handler, REST mock\n**Solves:**\n- Mock REST API endpoints\n- Intercept HTTP requests at network level\n- Create request handlers for testing\n\n### graphql-mocking\n**Keywords:** graphql.query, graphql.mutation, GraphQL handler, mock GraphQL\n**Solves:**\n- Mock GraphQL queries and mutations\n- Handle GraphQL variables in mocks\n- Test GraphQL error scenarios\n\n### websocket-mocking\n**Keywords:** WebSocket, ws mock, real-time mock, socket mock\n**Solves:**\n- Mock WebSocket connections\n- Simulate real-time events\n- Test WebSocket message handling\n\n### error-simulation\n**Keywords:** error simulation, network error, 500 error, mock error\n**Solves:**\n- Simulate API errors in tests\n- Test error handling UI\n- Mock network failures\n\n### network-delay-simulation\n**Keywords:** delay, latency, slow response, loading state\n**Solves:**\n- Simulate slow network responses\n- Test loading state UI\n- Verify timeout handling\n\n### runtime-handler-override\n**Keywords:** runtime override, use.once, test-specific handler, override\n**Solves:**\n- Override handlers for specific tests\n- Create one-time response handlers\n- Customize responses per test"
              },
              {
                "name": "multi-agent-orchestration",
                "description": "Multi-agent coordination and synthesis patterns. Use when orchestrating multiple specialized agents, implementing fan-out/fan-in workflows, or synthesizing outputs from parallel agents.",
                "path": ".claude/skills/multi-agent-orchestration/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-orchestration",
                  "description": "Multi-agent coordination and synthesis patterns. Use when orchestrating multiple specialized agents, implementing fan-out/fan-in workflows, or synthesizing outputs from parallel agents.",
                  "context": "fork",
                  "agent": "workflow-architect"
                },
                "content": "# Multi-Agent Orchestration\n\nCoordinate multiple specialized agents for complex tasks.\n\n## When to Use\n\n- Tasks requiring multiple expertise areas\n- Parallel analysis from different perspectives\n- Complex workflows with agent handoffs\n- Synthesis of multiple agent outputs\n\n## Fan-Out/Fan-In Pattern\n\n```python\nasync def multi_agent_analysis(content: str) -> dict:\n    \"\"\"Fan-out to specialists, fan-in to synthesize.\"\"\"\n    agents = [\n        (\"security\", security_agent),\n        (\"performance\", performance_agent),\n        (\"code_quality\", quality_agent),\n        (\"architecture\", architecture_agent),\n    ]\n\n    # Fan-out: Run all agents in parallel\n    tasks = [agent(content) for _, agent in agents]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Filter successful results\n    findings = [\n        {\"agent\": name, \"result\": result}\n        for (name, _), result in zip(agents, results)\n        if not isinstance(result, Exception)\n    ]\n\n    # Fan-in: Synthesize findings\n    return await synthesize_findings(findings)\n```\n\n## Supervisor Pattern\n\n```python\nclass Supervisor:\n    \"\"\"Central coordinator that routes to specialists.\"\"\"\n\n    def __init__(self, agents: dict):\n        self.agents = agents  # {\"security\": agent, \"performance\": agent}\n        self.completed = []\n\n    async def run(self, task: str) -> dict:\n        \"\"\"Route task through appropriate agents.\"\"\"\n        # 1. Determine which agents to use\n        plan = await self.plan_routing(task)\n\n        # 2. Execute in dependency order\n        results = {}\n        for agent_name in plan.execution_order:\n            if plan.can_parallelize(agent_name):\n                # Run parallel batch\n                batch = plan.get_parallel_batch(agent_name)\n                batch_results = await asyncio.gather(*[\n                    self.agents[name](task, context=results)\n                    for name in batch\n                ])\n                results.update(dict(zip(batch, batch_results)))\n            else:\n                # Run sequential\n                results[agent_name] = await self.agents[agent_name](\n                    task, context=results\n                )\n\n        return results\n\n    async def plan_routing(self, task: str) -> RoutingPlan:\n        \"\"\"Use LLM to determine agent routing.\"\"\"\n        response = await llm.chat([{\n            \"role\": \"user\",\n            \"content\": f\"\"\"Task: {task}\n\nAvailable agents: {list(self.agents.keys())}\n\nWhich agents should handle this task?\nWhat order? Can any run in parallel?\"\"\"\n        }])\n        return parse_routing_plan(response.content)\n```\n\n## Conflict Resolution\n\n```python\nasync def resolve_conflicts(findings: list[dict]) -> list[dict]:\n    \"\"\"When agents disagree, resolve by confidence or LLM.\"\"\"\n    conflicts = detect_conflicts(findings)\n\n    if not conflicts:\n        return findings\n\n    for conflict in conflicts:\n        # Option 1: Higher confidence wins\n        winner = max(conflict.agents, key=lambda a: a.confidence)\n\n        # Option 2: LLM arbitration\n        resolution = await llm.chat([{\n            \"role\": \"user\",\n            \"content\": f\"\"\"Two agents disagree:\n\nAgent A ({conflict.agent_a.name}): {conflict.agent_a.finding}\nAgent B ({conflict.agent_b.name}): {conflict.agent_b.finding}\n\nWhich is more likely correct and why?\"\"\"\n        }])\n\n        # Record resolution\n        conflict.resolution = parse_resolution(resolution.content)\n\n    return apply_resolutions(findings, conflicts)\n```\n\n## Synthesis Pattern\n\n```python\nasync def synthesize_findings(findings: list[dict]) -> dict:\n    \"\"\"Combine multiple agent outputs into coherent result.\"\"\"\n    # Group by category\n    by_category = {}\n    for f in findings:\n        cat = f.get(\"category\", \"general\")\n        by_category.setdefault(cat, []).append(f)\n\n    # Synthesize each category\n    synthesis = await llm.chat([{\n        \"role\": \"user\",\n        \"content\": f\"\"\"Synthesize these agent findings into a coherent summary:\n\n{json.dumps(by_category, indent=2)}\n\nOutput format:\n- Executive summary (2-3 sentences)\n- Key findings by category\n- Recommendations\n- Confidence score (0-1)\"\"\"\n    }])\n\n    return parse_synthesis(synthesis.content)\n```\n\n## Agent Communication Bus\n\n```python\nclass AgentBus:\n    \"\"\"Message passing between agents.\"\"\"\n\n    def __init__(self):\n        self.messages = []\n        self.subscribers = {}\n\n    def publish(self, from_agent: str, message: dict):\n        \"\"\"Broadcast message to all agents.\"\"\"\n        msg = {\"from\": from_agent, \"data\": message, \"ts\": time.time()}\n        self.messages.append(msg)\n\n        for callback in self.subscribers.values():\n            callback(msg)\n\n    def subscribe(self, agent_id: str, callback):\n        \"\"\"Register agent to receive messages.\"\"\"\n        self.subscribers[agent_id] = callback\n\n    def get_history(self, agent_id: str = None) -> list:\n        \"\"\"Get message history, optionally filtered.\"\"\"\n        if agent_id:\n            return [m for m in self.messages if m[\"from\"] == agent_id]\n        return self.messages\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Agent count | 3-8 specialists |\n| Parallelism | Parallelize independent agents |\n| Conflict resolution | Confidence score or LLM arbitration |\n| Communication | Shared state or message bus |\n\n## Common Mistakes\n\n- No timeout per agent (one slow agent blocks all)\n- No error isolation (one failure crashes workflow)\n- Over-coordination (too much overhead)\n- Missing synthesis (raw agent outputs not useful)\n\n## Related Skills\n\n- `langgraph-supervisor` - LangGraph supervisor pattern\n- `langgraph-parallel` - Fan-out/fan-in with LangGraph\n- `agent-loops` - Single agent patterns\n\n## Capability Details\n\n### agent-communication\n**Keywords:** agent communication, message passing, agent protocol, inter-agent\n**Solves:**\n- Establish communication between agents\n- Implement message passing patterns\n- Handle async agent communication\n\n### task-delegation\n**Keywords:** delegate, task routing, work distribution, agent dispatch\n**Solves:**\n- Route tasks to specialized agents\n- Implement work distribution strategies\n- Handle agent capability matching\n\n### result-aggregation\n**Keywords:** aggregate, combine results, merge outputs, synthesis\n**Solves:**\n- Combine outputs from multiple agents\n- Implement result synthesis patterns\n- Handle conflicting agent outputs\n\n### error-coordination\n**Keywords:** error handling, retry, fallback agent, failure recovery\n**Solves:**\n- Handle agent failures gracefully\n- Implement retry and fallback patterns\n- Coordinate error recovery\n\n### agent-lifecycle\n**Keywords:** lifecycle, spawn agent, terminate, agent pool\n**Solves:**\n- Manage agent creation and termination\n- Implement agent pooling\n- Handle agent health checks"
              },
              {
                "name": "Observability & Monitoring",
                "description": "Use when adding logging, metrics, tracing, or alerting to applications. Covers structured logging, Prometheus metrics, OpenTelemetry tracing, and alerting strategies.",
                "path": ".claude/skills/observability-monitoring/SKILL.md",
                "frontmatter": {
                  "name": "Observability & Monitoring",
                  "description": "Use when adding logging, metrics, tracing, or alerting to applications. Covers structured logging, Prometheus metrics, OpenTelemetry tracing, and alerting strategies.",
                  "context": "fork",
                  "agent": "metrics-architect",
                  "version": "1.0.0",
                  "category": "Operations & Reliability",
                  "agents": [
                    "backend-system-architect",
                    "code-quality-reviewer",
                    "ai-ml-engineer"
                  ],
                  "keywords": [
                    "observability",
                    "monitoring",
                    "logging",
                    "metrics",
                    "tracing",
                    "alerts",
                    "Prometheus",
                    "OpenTelemetry"
                  ]
                },
                "content": "# Observability & Monitoring Skill\n\nComprehensive frameworks for implementing observability including structured logging, metrics, distributed tracing, and alerting.\n\n## When to Use\n\n- Setting up application monitoring\n- Implementing structured logging\n- Adding metrics and dashboards\n- Configuring distributed tracing\n- Creating alerting rules\n- Debugging production issues\n\n## Three Pillars of Observability\n\n```\n\n     LOGS             METRICS          TRACES      \n\n What happened    How is system    How do requests \n at specific      performing       flow through    \n point in time    over time        services        \n\n```\n\n## Structured Logging\n\n### Log Levels\n\n| Level | Use Case |\n|-------|----------|\n| **ERROR** | Unhandled exceptions, failed operations |\n| **WARN** | Deprecated API, retry attempts |\n| **INFO** | Business events, successful operations |\n| **DEBUG** | Development troubleshooting |\n\n### Best Practice\n\n```typescript\n// Good: Structured with context\nlogger.info('User action completed', {\n  action: 'purchase',\n  userId: user.id,\n  orderId: order.id,\n  duration_ms: 150\n});\n\n// Bad: String interpolation\nlogger.info(\"User \" + user.id + \" completed purchase\");\n```\n\n> See `templates/structured-logging.ts` for Winston setup and request middleware\n\n## Metrics Collection\n\n### RED Method (Rate, Errors, Duration)\n\nEssential metrics for any service:\n- **Rate** - Requests per second\n- **Errors** - Failed requests per second\n- **Duration** - Request latency distribution\n\n### Prometheus Buckets\n\n```typescript\n// HTTP request latency\nbuckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5]\n\n// Database query latency\nbuckets: [0.001, 0.01, 0.05, 0.1, 0.5, 1]\n```\n\n> See `templates/prometheus-metrics.ts` for full metrics configuration\n\n## Distributed Tracing\n\n### OpenTelemetry Setup\n\nAuto-instrument common libraries:\n- Express/HTTP\n- PostgreSQL\n- Redis\n\n### Manual Spans\n\n```typescript\ntracer.startActiveSpan('processOrder', async (span) => {\n  span.setAttribute('order.id', orderId);\n  // ... work\n  span.end();\n});\n```\n\n> See `templates/opentelemetry-tracing.ts` for full setup\n\n## Alerting Strategy\n\n### Severity Levels\n\n| Level | Response Time | Examples |\n|-------|---------------|----------|\n| **Critical (P1)** | < 15 min | Service down, data loss |\n| **High (P2)** | < 1 hour | Major feature broken |\n| **Medium (P3)** | < 4 hours | Increased error rate |\n| **Low (P4)** | Next day | Warnings |\n\n### Key Alerts\n\n| Alert | Condition | Severity |\n|-------|-----------|----------|\n| ServiceDown | `up == 0` for 1m | Critical |\n| HighErrorRate | 5xx > 5% for 5m | Critical |\n| HighLatency | p95 > 2s for 5m | High |\n| LowCacheHitRate | < 70% for 10m | Medium |\n\n> See `templates/alerting-rules.yml` for Prometheus alerting rules\n\n## Health Checks\n\n### Kubernetes Probes\n\n| Probe | Purpose | Endpoint |\n|-------|---------|----------|\n| **Liveness** | Is app running? | `/health` |\n| **Readiness** | Ready for traffic? | `/ready` |\n| **Startup** | Finished starting? | `/startup` |\n\n### Readiness Response\n\n```json\n{\n  \"status\": \"healthy|degraded|unhealthy\",\n  \"checks\": {\n    \"database\": { \"status\": \"pass\", \"latency_ms\": 5 },\n    \"redis\": { \"status\": \"pass\", \"latency_ms\": 2 }\n  },\n  \"version\": \"1.0.0\",\n  \"uptime\": 3600\n}\n```\n\n> See `templates/health-checks.ts` for implementation\n\n## Observability Checklist\n\n### Implementation\n- [ ] JSON structured logging\n- [ ] Request correlation IDs\n- [ ] RED metrics (Rate, Errors, Duration)\n- [ ] Business metrics\n- [ ] Distributed tracing\n- [ ] Health check endpoints\n\n### Alerting\n- [ ] Service outage alerts\n- [ ] Error rate thresholds\n- [ ] Latency thresholds\n- [ ] Resource utilization alerts\n\n### Dashboards\n- [ ] Service overview\n- [ ] Error analysis\n- [ ] Performance metrics\n\n---\n\n## Advanced Structured Logging\n\n### Correlation IDs\n\n**Trace requests across services:**\n```python\nimport structlog\nimport uuid_utils  # pip install uuid-utils (UUID v7 support for Python < 3.14)\n\nlogger = structlog.get_logger()\n\n@app.middleware(\"http\")\nasync def correlation_middleware(request: Request, call_next):\n    # Get or generate correlation ID (UUID v7 for time-ordering in distributed traces)\n    correlation_id = request.headers.get(\"X-Correlation-ID\") or str(uuid_utils.uuid7())\n\n    # Bind to logger context (all logs in this request will include it)\n    structlog.contextvars.bind_contextvars(\n        correlation_id=correlation_id,\n        method=request.method,\n        path=request.url.path\n    )\n\n    # Add to response headers\n    response = await call_next(request)\n    response.headers[\"X-Correlation-ID\"] = correlation_id\n\n    return response\n```\n\n**Benefits:**\n- Find all logs related to a single request\n- Track requests across microservices\n- Debug distributed transactions\n\n### Log Sampling\n\n**Problem:** Too many logs in high-traffic endpoints\n**Solution:** Sample less critical logs\n\n```python\nimport random\n\ndef should_sample(level: str, rate: float = 0.1) -> bool:\n    \"\"\"Sample logs based on level and rate.\"\"\"\n    if level in [\"ERROR\", \"CRITICAL\"]:\n        return True  # Always log errors\n    return random.random() < rate\n\n# Log 100% of errors, 10% of info\nif should_sample(\"INFO\", rate=0.1):\n    logger.info(\"User created\", user_id=user.id)\n```\n\n**Sampling rates:**\n- ERROR/CRITICAL: 100% (always log)\n- WARN: 50% (sample half)\n- INFO: 10% (sample 10%)\n- DEBUG: 1% (sample 1% in production)\n\n### Log Aggregation with Loki\n\n**Loki Query Language (LogQL) examples:**\n```logql\n# Find all errors in last hour\n{app=\"backend\"} |= \"ERROR\" | json\n\n# Count errors by endpoint\nsum by (endpoint) (\n  count_over_time({app=\"backend\"} |= \"ERROR\" [5m])\n)\n\n# p95 latency from structured logs\nquantile_over_time(0.95,\n  {app=\"backend\"}\n  | json\n  | unwrap duration_ms [5m]\n)\n\n# Search for specific correlation ID\n{app=\"backend\"} | json | correlation_id=\"abc-123-def\"\n```\n\n---\n\n## Metrics Deep Dive\n\n### Metric Types\n\n**1. Counter** - Monotonically increasing value (resets to 0 on restart)\n```python\nhttp_requests_total = Counter(\n    'http_requests_total',\n    'Total HTTP requests',\n    ['method', 'endpoint', 'status']\n)\n\n# Usage\nhttp_requests_total.labels(method='GET', endpoint='/api/users', status=200).inc()\n```\n\n**Use cases:** Request counts, error counts, bytes processed\n\n**2. Gauge** - Value that can go up or down\n```python\nactive_connections = Gauge(\n    'active_connections',\n    'Number of active database connections'\n)\n\n# Usage\nactive_connections.set(25)  # Set to specific value\nactive_connections.inc()    # Increment by 1\nactive_connections.dec()    # Decrement by 1\n```\n\n**Use cases:** Queue length, memory usage, temperature\n\n**3. Histogram** - Distribution of values (with buckets)\n```python\nrequest_duration = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'endpoint'],\n    buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]  # Important: Choose meaningful buckets!\n)\n\n# Usage\nwith request_duration.labels(method='GET', endpoint='/api/users').time():\n    # ... handle request\n    pass\n```\n\n**Use cases:** Request latency, response size\n\n**4. Summary** - Like Histogram but calculates quantiles on client side\n```python\nrequest_duration = Summary(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'endpoint']\n)\n```\n\n**Histogram vs Summary:**\n- **Histogram**: Calculate quantiles on Prometheus server (recommended)\n- **Summary**: Calculate quantiles on application side (higher client CPU, can't aggregate across instances)\n\n### Cardinality Management\n\n**Problem:** Too many unique label combinations\n\n```python\n#  BAD: Unbounded cardinality (user_id can be millions of values)\nhttp_requests_total = Counter(\n    'http_requests_total',\n    ['method', 'endpoint', 'user_id']  # user_id creates millions of time series!\n)\n\n#  GOOD: Bounded cardinality\nhttp_requests_total = Counter(\n    'http_requests_total',\n    ['method', 'endpoint', 'status']  # Limited to ~10 methods  100 endpoints  10 statuses = 10,000 series\n)\n```\n\n**Cardinality limits:**\n- Good: < 10,000 unique time series per metric\n- Acceptable: 10,000-100,000\n- Bad: > 100,000 (Prometheus performance degrades)\n\n**Rule:** Never use unbounded labels (user IDs, request IDs, timestamps)\n\n### Custom Business Metrics\n\n```python\n# LLM token usage\nllm_tokens_used = Counter(\n    'llm_tokens_used_total',\n    'Total LLM tokens consumed',\n    ['model', 'operation']  # e.g., model='claude-sonnet', operation='analysis'\n)\n\n# LLM cost tracking\nllm_cost_dollars = Counter(\n    'llm_cost_dollars_total',\n    'Total LLM cost in dollars',\n    ['model']\n)\n\n# Cache hit rate\ncache_operations = Counter(\n    'cache_operations_total',\n    'Cache operations',\n    ['operation', 'result']  # operation='get', result='hit|miss'\n)\n\n# Cache hit rate query:\n# sum(rate(cache_operations_total{result=\"hit\"}[5m])) /\n# sum(rate(cache_operations_total[5m]))\n```\n\n---\n\n## Distributed Tracing Patterns\n\n### Span Relationships\n\n```python\nfrom opentelemetry import trace\n\ntracer = trace.get_tracer(__name__)\n\n# Parent span\nwith tracer.start_as_current_span(\"analyze_content\") as parent_span:\n    parent_span.set_attribute(\"content.url\", url)\n    parent_span.set_attribute(\"content.type\", \"article\")\n\n    # Child span (sequential)\n    with tracer.start_as_current_span(\"fetch_content\") as fetch_span:\n        content = await fetch_url(url)\n        fetch_span.set_attribute(\"content.size_bytes\", len(content))\n\n    # Another child span (sequential)\n    with tracer.start_as_current_span(\"generate_embedding\") as embed_span:\n        embedding = await embed_text(content)\n        embed_span.set_attribute(\"embedding.dimensions\", len(embedding))\n\n    # Parallel child spans (using asyncio.gather)\n    async def analyze_with_span(agent_name: str, content: str):\n        with tracer.start_as_current_span(f\"agent_{agent_name}\"):\n            return await agent.analyze(content)\n\n    results = await asyncio.gather(\n        analyze_with_span(\"tech_comparator\", content),\n        analyze_with_span(\"security_auditor\", content),\n        analyze_with_span(\"implementation_planner\", content)\n    )\n```\n\n### Trace Sampling Strategies\n\n**Head-based sampling** (decide at trace start):\n```python\nfrom opentelemetry.sdk.trace.sampling import (\n    TraceIdRatioBased,  # Sample X% of traces\n    ParentBased,        # Follow parent's sampling decision\n    ALWAYS_ON,          # Always sample\n    ALWAYS_OFF          # Never sample\n)\n\n# Sample 10% of traces\nsampler = TraceIdRatioBased(0.1)\n```\n\n**Tail-based sampling** (decide after trace completes):\n- Keep all traces with errors\n- Keep slow traces (p95+ latency)\n- Sample 1% of successful fast traces\n\n**SkillForge sampling:**\n- Development: 100% sampling\n- Production: 10% sampling, 100% for errors\n\n### Trace Analysis Queries\n\n**Find slow traces:**\n```\nduration > 2s\n```\n\n**Find traces with errors:**\n```\nstatus = error\n```\n\n**Find traces for specific user:**\n```\nuser.id = \"abc-123\"\n```\n\n**Find traces hitting specific service:**\n```\nservice.name = \"analysis-worker\"\n```\n\n---\n\n## Alert Fatigue Prevention\n\n### Alert Grouping\n\n**Group related alerts:**\n```yaml\nroute:\n  group_by: ['alertname', 'cluster', 'service']\n  group_wait: 30s        # Wait 30s to collect similar alerts\n  group_interval: 5m     # Send grouped alerts every 5m\n  repeat_interval: 4h    # Re-send alert after 4h if still firing\n\n  routes:\n  - match:\n      severity: critical\n    receiver: pagerduty\n    continue: true        # Continue to other routes\n\n  - match:\n      severity: warning\n    receiver: slack\n```\n\n### Inhibition Rules\n\n**Suppress noisy alerts when root cause is known:**\n```yaml\ninhibit_rules:\n# If ServiceDown is firing, suppress HighErrorRate and HighLatency\n- source_match:\n    alertname: ServiceDown\n  target_match_re:\n    alertname: (HighErrorRate|HighLatency)\n  equal: ['service']\n\n# If DatabaseDown is firing, suppress all DB-related alerts\n- source_match:\n    alertname: DatabaseDown\n  target_match_re:\n    alertname: Database.*\n  equal: ['cluster']\n```\n\n### Escalation Policies\n\n```yaml\n# Escalation: Slack  PagerDuty after 15 min\nroutes:\n- match:\n    severity: critical\n  receiver: slack\n  continue: true\n  routes:\n  - match:\n      severity: critical\n    receiver: pagerduty\n    group_wait: 15m  # Escalate to PagerDuty after 15 min\n```\n\n### Runbook Links\n\n**Add runbook links to alert annotations:**\n```yaml\ngroups:\n- name: app-alerts\n  rules:\n  - alert: HighErrorRate\n    expr: |\n      sum(rate(http_requests_total{status=~\"5..\"}[5m])) /\n      sum(rate(http_requests_total[5m])) > 0.05\n    annotations:\n      summary: \"High error rate detected\"\n      description: \"Error rate is {{ $value | humanizePercentage }}\"\n      runbook_url: \"https://wiki.example.com/runbooks/high-error-rate\"\n```\n\n**Runbook should include:**\n1. What the alert means\n2. Impact on users\n3. Common causes\n4. Investigation steps\n5. Remediation steps\n6. Escalation contacts\n\n---\n\n## Dashboard Design Principles\n\n### Layout Patterns\n\n**Golden Signals Dashboard (top row):**\n```\n\n  Latency       Traffic       Errors        Saturation  \n  (p50/p95)     (req/s)       (5xx rate)    (CPU/mem)   \n\n```\n\n**Service Dashboard Structure:**\n1. **Overview** (single row) - Traffic, errors, latency, saturation\n2. **Request breakdown** - By endpoint, method, status code\n3. **Dependencies** - Database, Redis, external APIs\n4. **Resources** - CPU, memory, disk, network\n5. **Business metrics** - Registrations, purchases, etc.\n\n### Metric Selection\n\n**Start with RED metrics:**\n- **Rate**: `rate(http_requests_total[5m])`\n- **Errors**: `sum(rate(http_requests_total{status=~\"5..\"}[5m]))`\n- **Duration**: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))`\n\n**Add USE metrics for resources:**\n- **Utilization**: % of resource used\n- **Saturation**: Queue depth, wait time\n- **Errors**: Error count\n\n### SLO/SLI Dashboards\n\n**Service Level Indicators (SLIs):**\n```promql\n# Availability SLI: % of successful requests\nsum(rate(http_requests_total{status!~\"5..\"}[30d])) /\nsum(rate(http_requests_total[30d]))\n\n# Latency SLI: % of requests < 1s\nsum(rate(http_request_duration_seconds_bucket{le=\"1\"}[30d])) /\nsum(rate(http_request_duration_seconds_count[30d]))\n```\n\n**Service Level Objectives (SLOs):**\n- Availability: 99.9% (43 min downtime/month)\n- Latency: 99% of requests < 1s\n\n**Error Budget:**\n- 99.9% SLO = 0.1% error budget\n- If error budget consumed, freeze feature work and focus on reliability\n\n---\n\n## Real-World SkillForge Examples\n\n### Example 1: Langfuse Observability Integration\n\n**SkillForge uses Langfuse for LLM observability:**\n```python\nfrom langfuse import Langfuse\nfrom langfuse.decorators import observe, langfuse_context\n\nlangfuse = Langfuse(\n    host=\"https://cloud.langfuse.com\",\n    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\")\n)\n\n@observe(name=\"analyze_content\")\nasync def analyze_content(url: str) -> AnalysisResult:\n    \"\"\"Analyze content with 8-agent workflow.\"\"\"\n\n    # Trace metadata\n    langfuse_context.update_current_trace(\n        name=\"content_analysis\",\n        user_id=\"system\",\n        metadata={\"url\": url, \"workflow\": \"8-agent-supervisor\"}\n    )\n\n    # Fetch content (child span)\n    with langfuse_context.observe(name=\"fetch_content\") as fetch_span:\n        content = await fetch_url(url)\n        fetch_span.metadata = {\"content_size\": len(content)}\n\n    # Generate embedding (child span with cost tracking)\n    with langfuse_context.observe(name=\"generate_embedding\") as embed_span:\n        embedding = await embed_text(content)\n        embed_span.usage = {\n            \"input_tokens\": len(content) // 4,  # Rough estimate\n            \"model\": \"voyage-code-2\"\n        }\n\n    # Run 8-agent analysis (parallel spans)\n    findings = await run_supervisor_workflow(content)\n\n    # Track total cost\n    langfuse_context.update_current_observation(\n        usage={\n            \"total_tokens\": sum(f.token_count for f in findings),\n            \"total_cost\": sum(f.cost for f in findings)\n        }\n    )\n\n    return AnalysisResult(findings=findings)\n```\n\n**Langfuse Dashboard views:**\n- Trace waterfall (see parallel agent execution)\n- Token usage by agent\n- Cost tracking per analysis\n- Prompt/completion inspection\n- Latency breakdown\n\n### Example 2: Structured Logging with Correlation\n\n**SkillForge's actual logging setup:**\n```python\nimport structlog\nfrom structlog.processors import JSONRenderer, TimeStamper, add_log_level\n\n# Configure structlog\nstructlog.configure(\n    processors=[\n        structlog.contextvars.merge_contextvars,  # Merge correlation IDs\n        add_log_level,\n        TimeStamper(fmt=\"iso\"),\n        JSONRenderer()\n    ],\n    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n    context_class=dict,\n    logger_factory=structlog.PrintLoggerFactory(),\n    cache_logger_on_first_use=True\n)\n\nlogger = structlog.get_logger()\n\n# Usage in workflow\n@workflow_node\nasync def supervisor_node(state: AnalysisState):\n    \"\"\"Route to next agent.\"\"\"\n\n    # Bind context for all logs in this function\n    log = logger.bind(\n        correlation_id=state[\"correlation_id\"],\n        analysis_id=state[\"analysis_id\"],\n        workflow_step=\"supervisor\"\n    )\n\n    completed = set(state[\"agents_completed\"])\n    available = [a for a in ALL_AGENTS if a not in completed]\n\n    if not available:\n        log.info(\"all_agents_completed\", total_findings=len(state[\"findings\"]))\n        state[\"next_node\"] = \"quality_gate\"\n    else:\n        next_agent = available[0]\n        log.info(\"routing_to_agent\", agent=next_agent, remaining=len(available))\n        state[\"next_node\"] = next_agent\n\n    return state\n```\n\n**Example log output:**\n```json\n{\n  \"event\": \"routing_to_agent\",\n  \"level\": \"info\",\n  \"timestamp\": \"2025-01-15T10:30:45.123Z\",\n  \"correlation_id\": \"abc-123-def\",\n  \"analysis_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"workflow_step\": \"supervisor\",\n  \"agent\": \"tech_comparator\",\n  \"remaining\": 7\n}\n```\n\n### Example 3: LLM Cost Tracking\n\n**SkillForge tracks LLM costs per model and operation:**\n```python\nfrom prometheus_client import Counter, Histogram\n\n# Token usage counter\nllm_tokens_used = Counter(\n    'llm_tokens_used_total',\n    'Total LLM tokens consumed',\n    ['model', 'operation', 'token_type']  # token_type = input|output\n)\n\n# Cost counter (in dollars)\nllm_cost_dollars = Counter(\n    'llm_cost_dollars_total',\n    'Total LLM cost in dollars',\n    ['model', 'operation']\n)\n\n# Latency histogram\nllm_request_duration = Histogram(\n    'llm_request_duration_seconds',\n    'LLM request duration',\n    ['model', 'operation'],\n    buckets=[0.5, 1, 2, 5, 10, 20, 30]\n)\n\n@observe(name=\"llm_call\")\nasync def call_llm(prompt: str, model: str, operation: str) -> str:\n    \"\"\"Call LLM with cost tracking.\"\"\"\n\n    start_time = time.time()\n\n    response = await anthropic_client.messages.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=1024\n    )\n\n    duration = time.time() - start_time\n\n    # Track metrics\n    input_tokens = response.usage.input_tokens\n    output_tokens = response.usage.output_tokens\n\n    llm_tokens_used.labels(model=model, operation=operation, token_type=\"input\").inc(input_tokens)\n    llm_tokens_used.labels(model=model, operation=operation, token_type=\"output\").inc(output_tokens)\n\n    # Cost calculation (Claude Sonnet 4.5 pricing)\n    input_cost = (input_tokens / 1_000_000) * 3.00   # $3/MTok input\n    output_cost = (output_tokens / 1_000_000) * 15.00  # $15/MTok output\n    total_cost = input_cost + output_cost\n\n    llm_cost_dollars.labels(model=model, operation=operation).inc(total_cost)\n    llm_request_duration.labels(model=model, operation=operation).observe(duration)\n\n    logger.info(\"llm_call_completed\",\n        model=model,\n        operation=operation,\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        cost_dollars=total_cost,\n        duration_seconds=duration\n    )\n\n    return response.content[0].text\n```\n\n**Grafana dashboard queries:**\n```promql\n# Total cost per day\nsum(increase(llm_cost_dollars_total[1d])) by (model)\n\n# Token usage rate\nsum(rate(llm_tokens_used_total[5m])) by (model, token_type)\n\n# Cost per operation\nsum(increase(llm_cost_dollars_total[1h])) by (operation)\n\n# p95 LLM latency\nhistogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))\n```\n\n**SkillForge cost insights:**\n- Baseline: $35k/year  With caching: $2-5k/year (85-95% reduction)\n- Most expensive operation: `quality_assessment` (40% of tokens)\n- Highest cache hit rate: `tech_comparison` (92%)\n\n---\n\n## Extended Thinking Triggers\n\nUse Opus 4.5 extended thinking for:\n- **Incident investigation** - Correlating logs, metrics, traces\n- **Alert tuning** - Reducing noise, catching real issues\n- **Architecture decisions** - Choosing monitoring solutions\n- **Performance debugging** - Cross-service latency analysis\n\n## Templates Reference\n\n| Template | Purpose |\n|----------|---------|\n| `structured-logging.ts` | Winston logger with request middleware |\n| `prometheus-metrics.ts` | HTTP, DB, cache metrics with middleware |\n| `opentelemetry-tracing.ts` | Distributed tracing setup |\n| `alerting-rules.yml` | Prometheus alerting rules |\n| `health-checks.ts` | Liveness, readiness, startup probes |\n\n## Capability Details\n\n### structured-logging\n**Keywords:** logging, structured log, json log, correlation id, log level, winston, pino, structlog\n**Solves:**\n- How do I set up structured logging?\n- Implement correlation IDs across services\n- JSON logging best practices\n- Log aggregation with Loki/LogQL\n\n### correlation-tracking\n**Keywords:** correlation id, request tracking, trace context, distributed logs\n**Solves:**\n- How do I track requests across services?\n- Implement correlation IDs in middleware\n- Find all logs for a single request\n- Debug distributed transactions\n\n### log-sampling\n**Keywords:** log sampling, high traffic logging, sampling rate, log volume\n**Solves:**\n- How do I reduce log volume in production?\n- Sample INFO logs while keeping all errors\n- Manage logging costs at scale\n\n### prometheus-metrics\n**Keywords:** metrics, prometheus, counter, histogram, gauge, summary, red method\n**Solves:**\n- How do I collect application metrics?\n- Implement RED method (Rate, Errors, Duration)\n- Choose between Counter, Gauge, Histogram\n- Avoid high cardinality metrics\n\n### metric-types\n**Keywords:** counter, gauge, histogram, summary, bucket, quantile\n**Solves:**\n- When to use Counter vs Gauge?\n- Histogram vs Summary for latency\n- Configure histogram buckets\n- Calculate p95/p99 latency\n\n### cardinality-management\n**Keywords:** cardinality, label explosion, time series, prometheus performance\n**Solves:**\n- How do I prevent label cardinality explosions?\n- Identify high cardinality metrics\n- Fix unbounded labels (user IDs, request IDs)\n\n### distributed-tracing\n**Keywords:** tracing, distributed tracing, opentelemetry, span, trace id, waterfall\n**Solves:**\n- How do I implement distributed tracing?\n- OpenTelemetry setup with auto-instrumentation\n- Create manual spans for custom operations\n- Trace sampling strategies\n\n### trace-sampling\n**Keywords:** trace sampling, head-based sampling, tail-based sampling, sampling strategy\n**Solves:**\n- How do I reduce trace volume?\n- Sample 10% of traces but keep all errors\n- Tail-based vs head-based sampling\n\n### alerting-strategy\n**Keywords:** alert, alerting, notification, threshold, pagerduty, slack, severity\n**Solves:**\n- How do I set up effective alerts?\n- Define alert severity levels (P1-P4)\n- Create service down and error rate alerts\n- Write runbooks for alerts\n\n### alert-fatigue-prevention\n**Keywords:** alert fatigue, alert grouping, inhibition, escalation\n**Solves:**\n- How do I reduce alert noise?\n- Group related alerts together\n- Suppress alerts with inhibition rules\n- Set up escalation policies\n\n### dashboards\n**Keywords:** dashboard, visualization, grafana, golden signals, red method, use method\n**Solves:**\n- How do I create monitoring dashboards?\n- Design Golden Signals dashboard layout\n- Build SLO/SLI dashboards\n- Calculate error budgets\n\n### health-checks\n**Keywords:** health check, liveness, readiness, startup probe, kubernetes\n**Solves:**\n- How do I implement health check endpoints?\n- Difference between liveness and readiness\n- Health check for database and Redis\n\n### langfuse-observability\n**Keywords:** langfuse, llm observability, llm tracing, token usage, llm cost tracking\n**Solves:**\n- How do I monitor LLM calls with Langfuse?\n- Track LLM token usage and cost\n- Trace multi-agent workflows\n- Real-world SkillForge LLM observability\n\n### llm-cost-tracking\n**Keywords:** llm cost, token tracking, cost optimization, prometheus llm metrics\n**Solves:**\n- How do I track LLM costs with Prometheus?\n- Measure token usage by model and operation\n- Calculate cost per analysis/operation\n- Build LLM cost dashboards"
              },
              {
                "name": "ollama-local",
                "description": "Local LLM inference with Ollama. Use when setting up local models for development, CI pipelines, or cost reduction. Covers model selection, LangChain integration, and performance tuning.",
                "path": ".claude/skills/ollama-local/SKILL.md",
                "frontmatter": {
                  "name": "ollama-local",
                  "description": "Local LLM inference with Ollama. Use when setting up local models for development, CI pipelines, or cost reduction. Covers model selection, LangChain integration, and performance tuning.",
                  "agent": "llm-integrator"
                },
                "content": "# Ollama Local Inference\n\nRun LLMs locally for cost savings, privacy, and offline development.\n\n## When to Use\n\n- CI/CD pipelines (93% cost reduction)\n- Development without API costs\n- Privacy-sensitive data\n- Offline environments\n- High-volume batch processing\n\n## Quick Start\n\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull models\nollama pull deepseek-r1:70b      # Reasoning (GPT-4 level)\nollama pull qwen2.5-coder:32b    # Coding\nollama pull nomic-embed-text     # Embeddings\n\n# Start server\nollama serve\n```\n\n## Recommended Models (M4 Max 256GB)\n\n| Task | Model | Size | Notes |\n|------|-------|------|-------|\n| Reasoning | `deepseek-r1:70b` | ~42GB | GPT-4 level |\n| Coding | `qwen2.5-coder:32b` | ~35GB | 73.7% Aider benchmark |\n| Embeddings | `nomic-embed-text` | ~0.5GB | 768 dims, fast |\n| General | `llama3.2:70b` | ~40GB | Good all-around |\n\n## LangChain Integration\n\n```python\nfrom langchain_ollama import ChatOllama, OllamaEmbeddings\n\n# Chat model\nllm = ChatOllama(\n    model=\"deepseek-r1:70b\",\n    base_url=\"http://localhost:11434\",\n    temperature=0.0,\n    num_ctx=32768,      # Context window\n    keep_alive=\"5m\",    # Keep model loaded\n)\n\n# Embeddings\nembeddings = OllamaEmbeddings(\n    model=\"nomic-embed-text\",\n    base_url=\"http://localhost:11434\",\n)\n\n# Generate\nresponse = await llm.ainvoke(\"Explain async/await\")\nvector = await embeddings.aembed_query(\"search text\")\n```\n\n## Tool Calling with Ollama\n\n```python\nfrom langchain_core.tools import tool\n\n@tool\ndef search_docs(query: str) -> str:\n    \"\"\"Search the document database.\"\"\"\n    return f\"Found results for: {query}\"\n\n# Bind tools\nllm_with_tools = llm.bind_tools([search_docs])\nresponse = await llm_with_tools.ainvoke(\"Search for Python patterns\")\n```\n\n## Structured Output\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass CodeAnalysis(BaseModel):\n    language: str = Field(description=\"Programming language\")\n    complexity: int = Field(ge=1, le=10)\n    issues: list[str] = Field(description=\"Found issues\")\n\nstructured_llm = llm.with_structured_output(CodeAnalysis)\nresult = await structured_llm.ainvoke(\"Analyze this code: ...\")\n# result is typed CodeAnalysis object\n```\n\n## Provider Factory Pattern\n\n```python\nimport os\n\ndef get_llm_provider(task_type: str = \"general\"):\n    \"\"\"Auto-switch between Ollama and cloud APIs.\"\"\"\n    if os.getenv(\"OLLAMA_ENABLED\") == \"true\":\n        models = {\n            \"reasoning\": \"deepseek-r1:70b\",\n            \"coding\": \"qwen2.5-coder:32b\",\n            \"general\": \"llama3.2:70b\",\n        }\n        return ChatOllama(\n            model=models.get(task_type, \"llama3.2:70b\"),\n            keep_alive=\"5m\"\n        )\n    else:\n        # Fall back to cloud API\n        return ChatOpenAI(model=\"gpt-4o\")\n\n# Usage\nllm = get_llm_provider(task_type=\"coding\")\n```\n\n## Environment Configuration\n\n```bash\n# .env.local\nOLLAMA_ENABLED=true\nOLLAMA_HOST=http://localhost:11434\nOLLAMA_MODEL_REASONING=deepseek-r1:70b\nOLLAMA_MODEL_CODING=qwen2.5-coder:32b\nOLLAMA_MODEL_EMBED=nomic-embed-text\n\n# Performance tuning (Apple Silicon)\nOLLAMA_MAX_LOADED_MODELS=3    # Keep 3 models in memory\nOLLAMA_KEEP_ALIVE=5m          # 5 minute keep-alive\n```\n\n## CI Integration\n\n```yaml\n# GitHub Actions (self-hosted runner)\njobs:\n  test:\n    runs-on: self-hosted  # M4 Max runner\n    env:\n      OLLAMA_ENABLED: \"true\"\n    steps:\n      - name: Pre-warm models\n        run: |\n          curl -s http://localhost:11434/api/embeddings \\\n            -d '{\"model\":\"nomic-embed-text\",\"prompt\":\"warmup\"}' > /dev/null\n\n      - name: Run tests\n        run: pytest tests/\n```\n\n## Cost Comparison\n\n| Provider | Monthly Cost | Latency |\n|----------|-------------|---------|\n| Cloud APIs | ~$675/month | 200-500ms |\n| Ollama Local | ~$50 (electricity) | 50-200ms |\n| **Savings** | **93%** | **2-3x faster** |\n\n## Best Practices\n\n- **DO** use `keep_alive=\"5m\"` in CI (avoid cold starts)\n- **DO** pre-warm models before first call\n- **DO** set `num_ctx=32768` on Apple Silicon\n- **DO** use provider factory for cloud/local switching\n- **DON'T** use `keep_alive=-1` (wastes memory)\n- **DON'T** skip pre-warming in CI (30-60s cold start)\n\n## Troubleshooting\n\n```bash\n# Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# List loaded models\nollama list\n\n# Check model memory usage\nollama ps\n\n# Pull specific version\nollama pull deepseek-r1:70b-q4_K_M\n```\n\n## Related Skills\n\n- `embeddings` - Embedding patterns (works with nomic-embed-text)\n- `llm-evaluation` - Testing with local models\n- `cost-optimization` - Broader cost strategies\n\n## Capability Details\n\n### setup\n**Keywords:** setup, install, configure, ollama\n**Solves:**\n- Set up Ollama locally\n- Configure for development\n- Install models\n\n### model-selection\n**Keywords:** model, llama, mistral, qwen, selection\n**Solves:**\n- Choose appropriate model\n- Compare model capabilities\n- Balance speed vs quality\n\n### provider-template\n**Keywords:** provider, template, python, implementation\n**Solves:**\n- Ollama provider template\n- Python implementation\n- Drop-in LLM provider"
              },
              {
                "name": "owasp-top-10",
                "description": "OWASP Top 10 security vulnerabilities and mitigations. Use when conducting security audits, implementing security controls, or reviewing code for common vulnerabilities.",
                "path": ".claude/skills/owasp-top-10/SKILL.md",
                "frontmatter": {
                  "name": "owasp-top-10",
                  "description": "OWASP Top 10 security vulnerabilities and mitigations. Use when conducting security audits, implementing security controls, or reviewing code for common vulnerabilities.",
                  "context": "fork",
                  "agent": "security-auditor",
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/redact-secrets.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# OWASP Top 10\n\nProtect against the most critical web security risks.\n\n## When to Use\n\n- Security audits\n- Code review for vulnerabilities\n- Designing secure systems\n- Compliance requirements\n\n## 1. Broken Access Control\n\n```python\n#  Bad: No authorization check\n@app.route('/api/users/<user_id>')\ndef get_user(user_id):\n    return db.query(f\"SELECT * FROM users WHERE id = {user_id}\")\n\n#  Good: Verify user can access resource\n@app.route('/api/users/<user_id>')\n@login_required\ndef get_user(user_id):\n    if current_user.id != user_id and not current_user.is_admin:\n        abort(403)\n    return db.query(\"SELECT * FROM users WHERE id = ?\", [user_id])\n```\n\n## 2. Cryptographic Failures\n\n```python\n#  Bad: Weak hashing\nimport hashlib\npassword_hash = hashlib.md5(password.encode()).hexdigest()\n\n#  Good: Strong hashing\nfrom argon2 import PasswordHasher\nph = PasswordHasher()\npassword_hash = ph.hash(password)\n```\n\n## 3. Injection\n\n```python\n#  Bad: SQL injection vulnerable\nquery = f\"SELECT * FROM users WHERE email = '{email}'\"\n\n#  Good: Parameterized query\nquery = \"SELECT * FROM users WHERE email = ?\"\ndb.execute(query, [email])\n```\n\n## 4. Insecure Design\n\n- No rate limiting on login\n- Sequential/guessable IDs\n- No CAPTCHA on sensitive operations\n\n**Fix:** Use UUIDs, implement rate limiting, threat model early.\n\n## 5. Security Misconfiguration\n\n```python\n#  Bad: Debug mode in production\napp.debug = True\n\n#  Good: Environment-based config\napp.debug = os.getenv('FLASK_ENV') == 'development'\n```\n\n## 6. Vulnerable Components\n\n```bash\n# Scan for vulnerabilities\nnpm audit\npip-audit\n\n# Fix vulnerabilities\nnpm audit fix\n```\n\n## 7. Authentication Failures\n\n```python\n#  Strong password requirements\ndef validate_password(password):\n    if len(password) < 12:\n        return \"Password must be 12+ characters\"\n    if not re.search(r\"[A-Z]\", password):\n        return \"Must contain uppercase\"\n    if not re.search(r\"[0-9]\", password):\n        return \"Must contain number\"\n    return None\n```\n\n## JWT Security (OWASP Best Practices)\n\n```python\nimport jwt\nimport hashlib\nimport secrets\n\n#  Bad: Trust algorithm from header\npayload = jwt.decode(token, SECRET, algorithms=jwt.get_unverified_header(token)['alg'])\n\n#  Good: Hardcode expected algorithm (prevents algorithm confusion attacks)\ndef verify_jwt(token: str) -> dict:\n    try:\n        payload = jwt.decode(\n            token,\n            SECRET_KEY,\n            algorithms=['HS256'],  # NEVER read from header\n            options={\n                'require': ['exp', 'iat', 'iss', 'aud'],  # Required claims\n            }\n        )\n\n        # Validate issuer and audience\n        if payload['iss'] != EXPECTED_ISSUER:\n            raise jwt.InvalidIssuerError()\n        if payload['aud'] != EXPECTED_AUDIENCE:\n            raise jwt.InvalidAudienceError()\n\n        return payload\n    except jwt.ExpiredSignatureError:\n        raise AuthError(\"Token expired\")\n    except jwt.InvalidTokenError as e:\n        raise AuthError(f\"Invalid token: {e}\")\n\n# Token sidejacking protection (OWASP recommended)\ndef create_protected_token(user_id: str, response) -> str:\n    \"\"\"Create token with user context to prevent sidejacking.\"\"\"\n    # Generate random fingerprint\n    fingerprint = secrets.token_urlsafe(32)\n\n    # Store fingerprint hash in token (not raw value)\n    payload = {\n        'user_id': user_id,\n        'fingerprint': hashlib.sha256(fingerprint.encode()).hexdigest(),\n        'exp': datetime.utcnow() + timedelta(minutes=15),\n        'iat': datetime.utcnow(),\n        'iss': ISSUER,\n        'aud': AUDIENCE,\n    }\n\n    # Send raw fingerprint as hardened cookie\n    response.set_cookie(\n        '__Secure-Fgp',  # Cookie prefix for extra security\n        fingerprint,\n        httponly=True,\n        secure=True,\n        samesite='Strict',\n        max_age=900  # 15 min\n    )\n\n    return jwt.encode(payload, SECRET_KEY, algorithm='HS256')\n```\n\n**JWT Security Checklist:**\n- [ ] Hardcode algorithm (never read from header)\n- [ ] Validate: exp, iat, iss, aud claims\n- [ ] Short expiry (15 min - 1 hour)\n- [ ] Use refresh token rotation for longer sessions\n- [ ] Implement token denylist for logout/revocation\n\n## 8. Data Integrity Failures\n\n```html\n<!-- Use SRI for CDN scripts -->\n<script src=\"https://cdn.example.com/lib.js\"\n        integrity=\"sha384-...\"\n        crossorigin=\"anonymous\"></script>\n```\n\n## 9. Logging Failures\n\n```python\n#  Log security events\n@app.route('/login', methods=['POST'])\ndef login():\n    user = authenticate(email, password)\n    if user:\n        logger.info(f\"Successful login: {email}\")\n    else:\n        logger.warning(f\"Failed login: {email}\")\n```\n\n## 10. SSRF (Server-Side Request Forgery)\n\n```python\n#  Bad: Fetch any URL\nresponse = requests.get(user_provided_url)\n\n#  Good: Allowlist domains\nALLOWED = ['api.example.com']\nif urlparse(url).hostname not in ALLOWED:\n    abort(400)\n```\n\n## Quick Checklist\n\n- [ ] Authorization on all endpoints\n- [ ] Passwords hashed with bcrypt/argon2\n- [ ] Parameterized queries only\n- [ ] Rate limiting enabled\n- [ ] Debug mode off in production\n- [ ] Dependencies scanned regularly\n- [ ] Security events logged\n\n## Related Skills\n\n- `auth-patterns` - Authentication implementation\n- `input-validation` - Sanitization patterns\n- `security-scanning` - Automated scanning\n\n## Capability Details\n\n### injection\n**Keywords:** sql injection, command injection, injection, parameterized\n**Solves:**\n- Prevent SQL injection\n- Fix command injection\n- Use parameterized queries\n\n### access-control\n**Keywords:** access control, authorization, idor, privilege\n**Solves:**\n- Fix broken access control\n- Prevent IDOR vulnerabilities\n- Implement authorization checks\n\n### owasp-fixes\n**Keywords:** fix, mitigation, example, vulnerability\n**Solves:**\n- OWASP vulnerability fixes\n- Mitigation examples\n- Code fix patterns"
              },
              {
                "name": "Performance Optimization",
                "description": "Use when application is slow, bundle is too large, or investigating performance issues. Covers profiling, React concurrent features, bundle analysis, and optimization patterns.",
                "path": ".claude/skills/performance-optimization/SKILL.md",
                "frontmatter": {
                  "name": "Performance Optimization",
                  "description": "Use when application is slow, bundle is too large, or investigating performance issues. Covers profiling, React concurrent features, bundle analysis, and optimization patterns.",
                  "version": "1.1.0",
                  "category": "Quality & Optimization",
                  "agents": [
                    "backend-system-architect",
                    "frontend-ui-developer",
                    "code-quality-reviewer"
                  ],
                  "keywords": [
                    "performance",
                    "optimization",
                    "speed",
                    "latency",
                    "throughput",
                    "caching",
                    "profiling",
                    "bundle",
                    "Core Web Vitals",
                    "react-19",
                    "virtualization",
                    "code-splitting",
                    "tree-shaking"
                  ]
                },
                "content": "# Performance Optimization Skill\n\nComprehensive frameworks for analyzing and optimizing application performance across the entire stack.\n\n## When to Use\n\n- Application feels slow or unresponsive\n- Database queries taking too long\n- Frontend bundle size too large\n- API response times exceed targets\n- Core Web Vitals need improvement\n- Preparing for scale or high traffic\n\n## Performance Targets\n\n### Core Web Vitals (Frontend)\n\n| Metric | Good | Needs Work |\n|--------|------|------------|\n| **LCP** (Largest Contentful Paint) | < 2.5s | < 4s |\n| **INP** (Interaction to Next Paint) | < 200ms | < 500ms |\n| **CLS** (Cumulative Layout Shift) | < 0.1 | < 0.25 |\n| **TTFB** (Time to First Byte) | < 200ms | < 600ms |\n\n### Backend Targets\n\n| Operation | Target |\n|-----------|--------|\n| Simple reads | < 100ms |\n| Complex queries | < 500ms |\n| Write operations | < 200ms |\n| Index lookups | < 10ms |\n\n## Bottleneck Categories\n\n| Category | Symptoms | Tools |\n|----------|----------|-------|\n| **Network** | High TTFB, slow loading | Network tab, WebPageTest |\n| **Database** | Slow queries, pool exhaustion | EXPLAIN ANALYZE, pg_stat_statements |\n| **CPU** | High usage, slow compute | Profiler, flame graphs |\n| **Memory** | Leaks, GC pauses | Heap snapshots |\n| **Rendering** | Layout thrashing | React DevTools, Performance tab |\n\n## Database Optimization\n\n### Key Patterns\n\n1. **Add Missing Indexes** - Turn `Seq Scan` into `Index Scan`\n2. **Fix N+1 Queries** - Use JOINs or `include` instead of loops\n3. **Cursor Pagination** - Never load all records\n4. **Connection Pooling** - Manage connection lifecycle\n\n### Quick Diagnostics\n\n```sql\n-- Find slow queries (PostgreSQL)\nSELECT query, calls, mean_time / 1000 as mean_seconds\nFROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;\n\n-- Verify index usage\nEXPLAIN ANALYZE SELECT * FROM orders WHERE user_id = 123;\n```\n\n> See `templates/database-optimization.ts` for N+1 fixes and pagination patterns\n\n## Caching Strategy\n\n### Cache Hierarchy\n\n```\nL1: In-Memory (LRU, memoization) - fastest\nL2: Distributed (Redis/Memcached) - shared\nL3: CDN (edge, static assets) - global\nL4: Database (materialized views) - fallback\n```\n\n### Cache-Aside Pattern\n\n```typescript\nconst cached = await redis.get(key);\nif (cached) return JSON.parse(cached);\nconst data = await db.query(...);\nawait redis.setex(key, 3600, JSON.stringify(data));\nreturn data;\n```\n\n> See `templates/caching-patterns.ts` for full implementation\n\n## Frontend Optimization\n\n### Bundle Optimization\n\n1. **Code Splitting** - `lazy()` for route-based splitting\n2. **Tree Shaking** - Import only what you need\n3. **Image Optimization** - WebP/AVIF, lazy loading, proper sizing\n\n### Rendering Optimization\n\n1. **Memoization** - `memo()`, `useCallback()`, `useMemo()`\n2. **Virtualization** - Render only visible items in long lists\n3. **Batch DOM Operations** - Read all, then write all\n\n> See `templates/frontend-optimization.tsx` for patterns\n\n### Analysis Commands\n\n```bash\n# Lighthouse audit\nlighthouse http://localhost:3000 --output=json\n\n# Bundle analysis\nnpx @next/bundle-analyzer  # Next.js\nnpx vite-bundle-visualizer # Vite\n```\n\n## API Optimization\n\n### Response Optimization\n\n1. **Field Selection** - Return only requested fields\n2. **Compression** - Enable gzip/brotli (threshold: 1KB)\n3. **ETags** - Enable 304 responses for unchanged data\n4. **Pagination** - Cursor-based for large datasets\n\n> See `templates/api-optimization.ts` for middleware examples\n\n## Monitoring Checklist\n\n### Before Launch\n\n- [ ] Lighthouse score > 90\n- [ ] Core Web Vitals pass\n- [ ] Bundle size within budget\n- [ ] Database queries profiled\n- [ ] Compression enabled\n- [ ] CDN configured\n\n### Ongoing\n\n- [ ] Performance monitoring active\n- [ ] Alerting for degradation\n- [ ] Lighthouse CI in pipeline\n- [ ] Weekly query analysis\n- [ ] Real User Monitoring (RUM)\n\n> See `templates/performance-metrics.ts` for Prometheus metrics setup\n\n---\n\n## Database Query Optimization Deep Dive\n\n### N+1 Query Detection\n\n**Symptoms:**\n- One query to get parent records, then N queries for related data\n- Rapid sequential database calls in logs\n- Linear growth in query count with data size\n\n**Example Problem:**\n```python\n#  BAD: N+1 query (1 + 8 queries)\nanalyses = await session.execute(select(Analysis).limit(8)).scalars().all()\nfor analysis in analyses:\n    # Each iteration hits DB again!\n    chunks = await session.execute(\n        select(Chunk).where(Chunk.analysis_id == analysis.id)\n    ).scalars().all()\n```\n\n**Solution:**\n```python\n#  GOOD: Single query with JOIN (1 query)\nfrom sqlalchemy.orm import selectinload\n\nanalyses = await session.execute(\n    select(Analysis)\n    .options(selectinload(Analysis.chunks))  # Eager load\n    .limit(8)\n).scalars().all()\n\n# Now analyses[0].chunks is already loaded (no extra query)\n```\n\n### Index Selection Strategies\n\n| Index Type | Use Case | Example |\n|------------|----------|---------|\n| **B-tree** | Equality, range queries | `WHERE created_at > '2025-01-01'` |\n| **GIN** | Full-text search, JSONB | `WHERE content_tsvector @@ to_tsquery('python')` |\n| **HNSW** | Vector similarity | `ORDER BY embedding <=> '[0.1, 0.2, ...]'` |\n| **Hash** | Exact equality only | `WHERE id = 'abc123'` (rare) |\n\n**Index Creation Examples:**\n```sql\n-- B-tree for timestamp range queries\nCREATE INDEX idx_analysis_created ON analyses(created_at DESC);\n\n-- GIN for full-text search (pre-computed tsvector)\nCREATE INDEX idx_chunk_tsvector ON chunks USING GIN(content_tsvector);\n\n-- HNSW for vector similarity (pgvector)\nCREATE INDEX idx_chunk_embedding ON chunks\nUSING hnsw (embedding vector_cosine_ops)\nWITH (m = 16, ef_construction = 64);\n```\n\n**SkillForge Impact:**\n- HNSW vs IVFFlat: **17x faster queries** (5ms vs 85ms)\n- Pre-computed tsvector: **5-10x faster** than computing on query\n\n### EXPLAIN ANALYZE Deep Dive\n\n```sql\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT c.* FROM chunks c\nJOIN analyses a ON c.analysis_id = a.id\nWHERE a.status = 'completed'\nORDER BY c.created_at DESC\nLIMIT 10;\n```\n\n**Key Metrics to Watch:**\n- **Seq Scan**  Add index if cost is high\n- **Execution Time**  Total query duration\n- **Planning Time**  Time spent optimizing query\n- **Buffers (shared hit)**  Cache hit ratio (want high)\n\n**Example Output Analysis:**\n```\nLimit  (cost=0.42..1.89 rows=10) (actual time=0.032..0.156 rows=10)\n  Buffers: shared hit=24\n  ->  Nested Loop  (cost=0.42..61.23 rows=415)\n      ->  Index Scan using idx_analysis_status on analyses\n          Index Cond: (status = 'completed')\n          Buffers: shared hit=8\n      ->  Index Scan using idx_chunk_analysis on chunks\n          Index Cond: (analysis_id = a.id)\n          Buffers: shared hit=16\n```\n **Good signs**: Index scans, low actual time, high buffer hits\n\n### pg_stat_statements Usage\n\n```sql\n-- Enable extension (once)\nCREATE EXTENSION pg_stat_statements;\n\n-- Find top 10 slowest queries\nSELECT\n    LEFT(query, 60) AS short_query,\n    calls,\n    ROUND(mean_exec_time::numeric, 2) AS avg_ms,\n    ROUND(total_exec_time::numeric, 2) AS total_ms\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n\n-- Find queries with low cache hit ratio\nSELECT\n    LEFT(query, 60),\n    shared_blks_hit,\n    shared_blks_read,\n    ROUND(100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio\nFROM pg_stat_statements\nWHERE shared_blks_read > 0\nORDER BY cache_hit_ratio ASC\nLIMIT 10;\n```\n\n---\n\n## Advanced Caching Strategies\n\n### Multi-Level Cache Hierarchy\n\n**SkillForge Implementation:**\n```\nL1: Prompt Caching (Claude native) - 90% cost savings, 0ms latency\nL2: Redis Semantic Cache - 70-85% cost savings, 5-10ms latency\nL3: PostgreSQL Query Cache - materialized views, 50-200ms latency\nL4: CDN Edge Cache - static assets, <50ms global latency\n```\n\n### Redis Caching Patterns\n\n**1. Cache-Aside (Read-Through)**\n```python\nasync def get_analysis(analysis_id: str) -> Analysis:\n    # 1. Try cache first\n    cached = await redis.get(f\"analysis:{analysis_id}\")\n    if cached:\n        return Analysis.parse_raw(cached)\n\n    # 2. Cache miss - fetch from DB\n    analysis = await db.get_analysis(analysis_id)\n\n    # 3. Store in cache (5 min TTL)\n    await redis.setex(\n        f\"analysis:{analysis_id}\",\n        300,  # 5 minutes\n        analysis.json()\n    )\n\n    return analysis\n```\n\n**2. Write-Through**\n```python\nasync def update_analysis(analysis: Analysis):\n    # 1. Write to DB first\n    await db.update(analysis)\n\n    # 2. Update cache immediately\n    await redis.setex(\n        f\"analysis:{analysis.id}\",\n        300,\n        analysis.json()\n    )\n```\n\n**3. Semantic Cache (Vector Search)**\n```python\nasync def get_llm_response(query: str) -> str:\n    # 1. Generate query embedding\n    query_embedding = await embed_text(query)\n\n    # 2. Search for similar cached queries (threshold: 0.92)\n    cached = await semantic_cache.search(query_embedding, threshold=0.92)\n    if cached:\n        return cached.content  # 95% cost savings!\n\n    # 3. Cache miss - call LLM\n    response = await llm.complete(query)\n\n    # 4. Store in semantic cache\n    await semantic_cache.store(query_embedding, response)\n\n    return response\n```\n\n### Cache Invalidation Strategies\n\n| Strategy | Use Case | Example |\n|----------|----------|---------|\n| **TTL** | Time-based expiry | News feed (5 min) |\n| **Write-through** | Immediate consistency | User profile updates |\n| **Event-driven** | Publish/subscribe | Invalidate on data change |\n| **Versioned keys** | Immutable data | `analysis:{id}:v2` |\n\n**SkillForge Cache Warming:**\n```python\n# Warm cache with golden dataset queries at startup\nGOLDEN_QUERIES = [\n    \"How to implement RAG with LangChain?\",\n    \"LangGraph supervisor pattern example\",\n    \"pgvector HNSW vs IVFFlat performance\"\n]\n\nasync def warm_cache():\n    for query in GOLDEN_QUERIES:\n        # Pre-compute and cache embeddings + LLM responses\n        await get_llm_response(query)\n```\n\n### HTTP Caching Headers\n\n```python\nfrom fastapi import Response\n\n@app.get(\"/api/v1/analyses/{id}\")\nasync def get_analysis(id: str, response: Response):\n    analysis = await db.get_analysis(id)\n\n    # Enable browser caching (5 minutes)\n    response.headers[\"Cache-Control\"] = \"public, max-age=300\"\n\n    # ETag for conditional requests\n    etag = hashlib.md5(analysis.json().encode()).hexdigest()\n    response.headers[\"ETag\"] = f'\"{etag}\"'\n\n    return analysis\n```\n\n---\n\n## Profiling Tools & Techniques\n\n### Python Profiling (py-spy)\n\n```bash\n# Install py-spy\npip install py-spy\n\n# Profile running FastAPI server (no code changes!)\npy-spy record --pid $(pgrep -f uvicorn) --output profile.svg\n\n# Top functions by time\npy-spy top --pid $(pgrep -f uvicorn)\n\n# Generate flame graph\npy-spy record --pid 12345 --format flamegraph --output flamegraph.svg\n```\n\n**Flame Graph Interpretation:**\n- **Width** = Time spent in function (wider = slower)\n- **Height** = Call stack depth\n- **Hot paths** = Look for wide bars at the top\n\n### Frontend Profiling (Chrome DevTools)\n\n**Performance Tab:**\n1. Open DevTools  Performance\n2. Click Record, interact with app, click Stop\n3. Analyze:\n   - **Main thread activity** (yellow = scripting, purple = rendering)\n   - **Long tasks** (red flag: >50ms blocks main thread)\n   - **Frame drops** (should be 60fps = 16.67ms/frame)\n\n**Memory Tab:**\n1. Take heap snapshot\n2. Interact with app\n3. Take another snapshot\n4. Compare to find leaks\n\n**Example - Finding Memory Leak:**\n```javascript\n//  BAD: Event listener not cleaned up\nuseEffect(() => {\n    window.addEventListener('resize', handleResize);\n    // Missing cleanup!\n}, []);\n\n//  GOOD: Cleanup prevents leak\nuseEffect(() => {\n    window.addEventListener('resize', handleResize);\n    return () => window.removeEventListener('resize', handleResize);\n}, []);\n```\n\n### React Profiler\n\n```javascript\nimport { Profiler } from 'react';\n\nfunction onRenderCallback(\n    id,           // Component name\n    phase,        // \"mount\" or \"update\"\n    actualDuration, // Time spent rendering\n    baseDuration,   // Estimated time without memoization\n    startTime,\n    commitTime\n) {\n    if (actualDuration > 16) {  // > 16ms = dropped frame\n        console.warn(\"Slow render: \" + id + \" took \" + actualDuration + \"ms\");\n    }\n}\n\n<Profiler id=\"AnalysisCard\" onRender={onRenderCallback}>\n    <AnalysisCard analysis={data} />\n</Profiler>\n```\n\n### Bundle Analysis\n\n```bash\n# Vite bundle analyzer\nnpm install --save-dev rollup-plugin-visualizer\n# Add to vite.config.ts:\nimport { visualizer } from 'rollup-plugin-visualizer';\nplugins: [visualizer({ open: true })]\n\n# Next.js bundle analyzer\nnpm install @next/bundle-analyzer\nANALYZE=true npm run build\n```\n\n---\n\n## Frontend Bundle Analysis (2025 Patterns)\n\n### Complete Vite Bundle Analyzer Setup\n\n```typescript\n// vite.config.ts\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\nimport { visualizer } from 'rollup-plugin-visualizer'\n\nexport default defineConfig({\n  plugins: [\n    react(),\n    // Only run visualizer during build:analyze\n    process.env.ANALYZE && visualizer({\n      open: true,\n      filename: 'dist/bundle-stats.html',\n      gzipSize: true,      // Show gzip sizes\n      brotliSize: true,    // Show brotli sizes\n      template: 'treemap', // 'treemap' | 'sunburst' | 'network'\n    }),\n  ].filter(Boolean),\n  build: {\n    rollupOptions: {\n      output: {\n        // Manual chunking for better cache strategy\n        manualChunks: {\n          // Vendor chunks\n          'react-vendor': ['react', 'react-dom'],\n          'router': ['@tanstack/react-router'],\n          'query': ['@tanstack/react-query'],\n          'ui': ['@radix-ui/react-dialog', '@radix-ui/react-tooltip'],\n          // Heavy libraries in separate chunks\n          'mermaid': ['mermaid'],\n          'markdown': ['react-markdown', 'remark-gfm'],\n        },\n      },\n    },\n    // Report chunk sizes\n    chunkSizeWarningLimit: 500, // 500kb warning\n  },\n})\n```\n\n```json\n// package.json\n{\n  \"scripts\": {\n    \"build\": \"tsc -b && vite build\",\n    \"build:analyze\": \"ANALYZE=true npm run build\",\n    \"bundle:report\": \"npm run build:analyze && open dist/bundle-stats.html\"\n  }\n}\n```\n\n### Bundle Size Budgets\n\n```typescript\n// bundle-budget.config.ts\nexport const bundleBudgets = {\n  // Total bundle limits\n  total: {\n    maxSize: 200 * 1024,      // 200KB gzipped\n    warnSize: 150 * 1024,     // Warn at 150KB\n  },\n\n  // Per-chunk limits\n  chunks: {\n    main: 50 * 1024,          // Entry point: 50KB max\n    'react-vendor': 45 * 1024, // React: ~42KB gzipped\n    'router': 30 * 1024,       // TanStack Router\n    'query': 15 * 1024,        // TanStack Query\n    lazy: 30 * 1024,          // Lazy-loaded routes\n  },\n\n  // Individual dependency limits\n  dependencies: {\n    'framer-motion': 30 * 1024, // Watch for growth\n    'mermaid': 150 * 1024,      // Large library (lazy load!)\n    'prismjs': 20 * 1024,       // Syntax highlighter\n  },\n} as const\n```\n\n### CI Bundle Size Check\n\n```yaml\n# .github/workflows/bundle-check.yml\nname: Bundle Size Check\n\non: [pull_request]\n\njobs:\n  bundle-size:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build and analyze\n        run: npm run build\n\n      - name: Check bundle size\n        uses: preactjs/compressed-size-action@v2\n        with:\n          pattern: './dist/**/*.{js,css}'\n          # Fail if bundle increases by more than 5KB\n          compression: 'gzip'\n\n      - name: Report bundle stats\n        run: |\n          echo \"## Bundle Size Report\" >> $GITHUB_STEP_SUMMARY\n          echo \"| Chunk | Size (gzip) |\" >> $GITHUB_STEP_SUMMARY\n          echo \"|-------|-------------|\" >> $GITHUB_STEP_SUMMARY\n          for file in dist/assets/*.js; do\n            size=$(gzip -c \"$file\" | wc -c)\n            echo \"| $(basename $file) | $(numfmt --to=iec $size) |\" >> $GITHUB_STEP_SUMMARY\n          done\n```\n\n### Tree-Shaking Verification\n\n```typescript\n//  BAD: Imports entire library\nimport { motion } from 'framer-motion'  // Pulls in ~30KB!\n\n//  GOOD: Import only what you need\nimport { motion } from 'framer-motion/m'  // Core motion only\n\n//  BAD: Barrel imports\nimport { Button, Card, Dialog } from '@/components'\n\n//  GOOD: Direct imports (better tree-shaking)\nimport { Button } from '@/components/ui/button'\nimport { Card } from '@/components/ui/card'\n\n//  BAD: Dynamic string imports break tree-shaking\nconst icons = ['Home', 'Settings', 'User']\n// icons.forEach(name => import(\"lucide-react/dist/esm/icons/\" + name))\n\n//  GOOD: Static imports\nimport { Home, Settings, User } from 'lucide-react'\n```\n\n### Code Splitting Strategies\n\n```typescript\n// Route-based splitting (TanStack Router)\nconst AnalyzeRoute = createFileRoute('/analyze/$id')({\n  component: lazy(() => import('./features/analysis/AnalyzeResult')),\n  pendingComponent: AnalysisSkeleton,  // Show skeleton while loading\n  errorComponent: AnalysisError,\n})\n\n// Component-based splitting\nconst HeavyChart = lazy(() => import('./components/HeavyChart'))\n\nfunction Dashboard() {\n  return (\n    <Suspense fallback={<ChartSkeleton />}>\n      <HeavyChart data={chartData} />\n    </Suspense>\n  )\n}\n\n// Library-based splitting (heavy dependencies)\nconst MermaidRenderer = lazy(() =>\n  import('./components/MermaidRenderer').then(mod => ({ default: mod.MermaidRenderer }))\n)\n\n// Conditional splitting (feature flags)\nconst AdminPanel = lazy(() =>\n  import('./features/admin/AdminPanel')\n)\n\nfunction App() {\n  return isAdmin ? (\n    <Suspense fallback={<AdminSkeleton />}>\n      <AdminPanel />\n    </Suspense>\n  ) : null\n}\n```\n\n### React 19 Performance Patterns\n\n```typescript\n//  useTransition for non-urgent updates\nimport { useTransition, startTransition } from 'react'\n\nfunction SearchResults({ query }: { query: string }) {\n  const [isPending, startTransition] = useTransition()\n  const [results, setResults] = useState([])\n\n  function handleSearch(query: string) {\n    // Immediate UI update\n    setQuery(query)\n\n    // Non-blocking results update\n    startTransition(() => {\n      setResults(searchDatabase(query))\n    })\n  }\n\n  return (\n    <div>\n      <input value={query} onChange={e => handleSearch(e.target.value)} />\n      {isPending && <Spinner />}\n      <ResultsList results={results} />\n    </div>\n  )\n}\n\n//  use() for Suspense-aware data\nimport { use } from 'react'\n\nfunction UserProfile({ userPromise }: { userPromise: Promise<User> }) {\n  const user = use(userPromise) // Suspends until resolved\n  return <div>{user.name}</div>\n}\n\n//  useOptimistic for instant feedback\nimport { useOptimistic } from 'react'\n\nfunction LikeButton({ initialCount }: { initialCount: number }) {\n  const [optimisticCount, addOptimistic] = useOptimistic(\n    initialCount,\n    (state, action) => state + action\n  )\n\n  async function handleLike() {\n    addOptimistic(1) // Instant UI update\n    await api.like(postId) // Background server update\n  }\n\n  return <button onClick={handleLike}>{optimisticCount} likes</button>\n}\n```\n\n### List Virtualization\n\n```typescript\n//  TanStack Virtual for long lists (>100 items)\nimport { useVirtualizer } from '@tanstack/react-virtual'\n\nfunction VirtualizedList({ items }: { items: Analysis[] }) {\n  const parentRef = useRef<HTMLDivElement>(null)\n\n  const virtualizer = useVirtualizer({\n    count: items.length,\n    getScrollElement: () => parentRef.current,\n    estimateSize: () => 80, // Estimated row height\n    overscan: 5, // Render 5 extra items for smoother scrolling\n  })\n\n  return (\n    <div ref={parentRef} style={{ height: '600px', overflow: 'auto' }}>\n      <div style={{ height: virtualizer.getTotalSize() + \"px\", position: 'relative' }}>\n        {virtualizer.getVirtualItems().map(virtualItem => (\n          <div\n            key={virtualItem.key}\n            style={{\n              position: 'absolute',\n              top: 0,\n              transform: \"translateY(\" + virtualItem.start + \"px)\",\n              height: virtualItem.size + \"px\",\n            }}\n          >\n            <AnalysisCard analysis={items[virtualItem.index]} />\n          </div>\n        ))}\n      </div>\n    </div>\n  )\n}\n\n// When to virtualize:\n// - Lists > 100 items\n// - Tables > 50 rows\n// - Grids with many items\n// - Any scrollable container with many children\n```\n\n### Bundle Analysis Checklist\n\n| Check | Target | Action if Failed |\n|-------|--------|------------------|\n| Total bundle (gzip) | < 200KB | Audit large dependencies |\n| Main chunk | < 50KB | Move code to lazy routes |\n| Vendor chunk | < 80KB | Check for duplicate deps |\n| Largest dependency | < 50KB | Lazy load or find alternative |\n| Tree-shaking | No unused exports | Use direct imports |\n| Code splitting | Routes lazy-loaded | Add lazy() wrappers |\n| Images | WebP/AVIF, lazy | Add next/image or similar |\n\n---\n\n## Real-World SkillForge Examples\n\n### Example 1: Hybrid Search Optimization\n\n**Problem:** Retrieval pass rate was 87.2%, needed >90%\n\n**Investigation:**\n```python\n# Original: 2x fetch multiplier\nHYBRID_FETCH_MULTIPLIER = 2  # Fetch 20 for top-10\n\n# Analysis showed insufficient coverage for RRF fusion\n# Testing: 2x  87.2%, 2.5x  89.7%, 3x  91.6%\n```\n\n**Solution:**\n```python\n# Increase to 3x fetch multiplier\nHYBRID_FETCH_MULTIPLIER = 3  # Fetch 30 for top-10\n\n# Add metadata boosting\nSECTION_TITLE_BOOST_FACTOR = 1.5  # +7.4% MRR improvement\nDOCUMENT_PATH_BOOST_FACTOR = 1.15\nCODE_BLOCK_BOOST_FACTOR = 1.2\n```\n\n**Results:**\n- Pass rate: 87.2%  **91.6%** (+5.1%)\n- MRR: 0.723  **0.777** (+7.4%)\n- Query time: 85ms  **5ms** (HNSW index)\n\n### Example 2: LLM Response Caching\n\n**Problem:** LLM costs projected at $35k/year\n\n**Solution:**\n```python\n# Multi-level cache hierarchy\nL1_PROMPT_CACHE_HIT_RATE = 0.90  # Claude native\nL2_SEMANTIC_CACHE_HIT_RATE = 0.75  # Redis vector search\n\n# Cost calculation\nbaseline_cost = 35000  # $35k/year\nl1_savings = baseline_cost * 0.90 * 0.90  # $28,350 saved\nl2_savings = (baseline_cost - l1_savings) * 0.75 * 0.80  # $4,650 saved\ntotal_savings = l1_savings + l2_savings  # $33,000 saved (94%)\n\nfinal_cost = baseline_cost - total_savings  # $2,100/year\n```\n\n**Results:**\n- Baseline: **$35k/year**  With caching: **$2-5k/year**\n- Cost reduction: **85-95%**\n- Latency: 2000ms  5-10ms (semantic cache hit)\n\n### Example 3: Vector Index Selection\n\n**Problem:** Vector searches taking 85ms, needed <10ms\n\n**Benchmark (415 chunks):**\n```sql\n-- IVFFlat (lists=10)\nEXPLAIN ANALYZE SELECT * FROM chunks\nORDER BY embedding <=> '[0.1, 0.2, ...]' LIMIT 10;\n-- Planning: 2ms, Execution: 85ms\n\n-- HNSW (m=16, ef_construction=64)\nEXPLAIN ANALYZE SELECT * FROM chunks\nORDER BY embedding <=> '[0.1, 0.2, ...]' LIMIT 10;\n-- Planning: 2ms, Execution: 5ms\n```\n\n**Decision Matrix:**\n| Index | Build Time | Query Time | Accuracy | Verdict |\n|-------|------------|------------|----------|---------|\n| IVFFlat | 2s | 85ms | 95% |  Too slow |\n| HNSW | 8s | 5ms | 98% |  **Chosen** |\n\n**Trade-off:** Slower indexing (8s vs 2s) for **17x faster queries**\n\n### Example 4: SSE Event Buffering\n\n**Problem:** Frontend showed 0% progress while backend ran\n\n**Root Cause:**\n```python\n#  BAD: Events published before subscriber connects were lost\nclass EventBroadcaster:\n    def publish(self, channel: str, event: dict):\n        self._subscribers[channel].send(event)  # Lost if no subscriber yet!\n```\n\n**Solution:**\n```python\n#  GOOD: Buffer last 100 events per channel\nfrom collections import deque\n\nclass EventBroadcaster:\n    def __init__(self):\n        self._buffers = {}  # channel  deque(maxlen=100)\n\n    def publish(self, channel: str, event: dict):\n        # Store in buffer\n        if channel not in self._buffers:\n            self._buffers[channel] = deque(maxlen=100)\n        self._buffers[channel].append(event)\n\n        # Send to active subscribers\n        for subscriber in self._subscribers.get(channel, []):\n            subscriber.send(event)\n\n    def subscribe(self, channel: str):\n        # Replay buffered events to new subscriber\n        for event in self._buffers.get(channel, []):\n            yield event\n        # Then continue with live events\n```\n\n**Results:**\n- Race condition eliminated\n- Buffered events: last 100 per channel\n- Memory overhead: ~10KB per active channel\n\n---\n\n## Extended Thinking Triggers\n\nUse Opus 4.5 extended thinking for:\n- **Complex debugging** - Multiple potential causes\n- **Architecture decisions** - Caching strategy selection\n- **Trade-off analysis** - Memory vs CPU vs latency\n- **Root cause analysis** - Performance regression investigation\n\n## Templates Reference\n\n| Template | Purpose |\n|----------|---------|\n| `database-optimization.ts` | N+1 fixes, pagination, pooling |\n| `caching-patterns.ts` | Redis cache-aside, memoization |\n| `frontend-optimization.tsx` | React memo, virtualization, code splitting |\n| `api-optimization.ts` | Compression, ETags, field selection |\n| `performance-metrics.ts` | Prometheus metrics, performance budget |\n\n---\n\n**Skill Version**: 1.1.0\n**Last Updated**: 2025-12-25\n**Maintained by**: AI Agent Hub Team\n\n## Changelog\n\n### v1.1.0 (2025-12-25)\n- Added comprehensive Frontend Bundle Analysis section\n- Added complete Vite bundle analyzer setup with visualizer\n- Added bundle size budgets and CI size checking\n- Added tree-shaking verification patterns\n- Added code splitting strategies (route, component, library)\n- Added React 19 performance patterns (useTransition, use(), useOptimistic)\n- Added TanStack Virtual list virtualization example\n- Added bundle analysis checklist with targets\n- Updated keywords to include react-19, virtualization, code-splitting\n\n### v1.0.0 (2025-12-14)\n- Initial skill with database optimization, caching, and profiling\n\n## Capability Details\n\n### database-optimization\n**Keywords:** slow query, n+1, query optimization, explain analyze, index, postgres performance\n**Solves:**\n- How do I optimize slow database queries?\n- Fix N+1 query problems with eager loading\n- Use EXPLAIN ANALYZE to diagnose queries\n- Add missing indexes for performance\n\n### n+1-query-detection\n**Keywords:** n+1, eager loading, selectinload, joinedload, query loops\n**Solves:**\n- How do I detect N+1 queries?\n- Fix N+1 with SQLAlchemy selectinload\n- Convert query loops to single JOINs\n\n### index-selection\n**Keywords:** index, b-tree, gin, hnsw, hash index, index types\n**Solves:**\n- Which index type should I use?\n- B-tree vs GIN vs HNSW comparison\n- Index full-text search columns\n- Vector similarity index selection\n\n### explain-analyze\n**Keywords:** explain analyze, query plan, seq scan, index scan, query cost\n**Solves:**\n- How do I read EXPLAIN ANALYZE output?\n- Identify Seq Scan problems\n- Analyze query execution time\n- Measure buffer cache hit ratio\n\n### caching-strategies\n**Keywords:** cache, redis, cdn, cache-aside, write-through, semantic cache\n**Solves:**\n- How do I implement multi-level caching?\n- Cache-aside vs write-through patterns\n- Semantic cache for LLM responses\n- Cache invalidation strategies\n\n### semantic-cache\n**Keywords:** semantic cache, vector cache, llm cache, embedding cache\n**Solves:**\n- How do I cache LLM responses by similarity?\n- Implement vector-based semantic cache\n- Reduce LLM costs by 70-95%\n- Real-world SkillForge semantic cache\n\n### cache-invalidation\n**Keywords:** cache invalidation, ttl, write-through, event-driven invalidation\n**Solves:**\n- How do I invalidate cached data?\n- TTL vs write-through invalidation\n- Event-driven cache invalidation\n- Cache warming strategies\n\n### frontend-performance\n**Keywords:** bundle size, lazy load, code splitting, tree shaking, lighthouse, web vitals\n**Solves:**\n- How do I reduce frontend bundle size?\n- Implement code splitting with React.lazy()\n- Optimize Lighthouse scores\n- Fix Core Web Vitals issues\n\n### core-web-vitals\n**Keywords:** lcp, inp, cls, core web vitals, ttfb, fid\n**Solves:**\n- How do I improve Core Web Vitals?\n- Optimize LCP (Largest Contentful Paint)\n- Fix CLS (Cumulative Layout Shift)\n- Improve INP (Interaction to Next Paint)\n\n### profiling\n**Keywords:** profile, flame graph, py-spy, chrome devtools, memory leak, cpu bottleneck\n**Solves:**\n- How do I profile my Python backend?\n- Generate flame graphs with py-spy\n- Profile React components with DevTools\n- Find memory leaks in frontend\n\n### bundle-analysis\n**Keywords:** bundle analyzer, vite visualizer, webpack bundle, tree shaking\n**Solves:**\n- How do I analyze bundle size?\n- Use Vite bundle visualizer\n- Identify large dependencies\n- Optimize bundle with tree shaking\n\n### hybrid-search-optimization\n**Keywords:** hybrid search, rrf, fetch multiplier, metadata boosting, search performance\n**Solves:**\n- How do I optimize hybrid search retrieval?\n- Tune RRF fetch multiplier for better coverage\n- Boost search results by metadata\n- Real-world SkillForge retrieval improvements\n\n### llm-caching\n**Keywords:** llm caching, prompt cache, semantic cache, cost reduction\n**Solves:**\n- How do I reduce LLM costs with caching?\n- Multi-level LLM cache hierarchy\n- Claude prompt caching (90% savings)\n- Redis semantic cache (70-85% savings)\n\n### vector-index-selection\n**Keywords:** pgvector, hnsw, ivfflat, vector index, similarity search performance\n**Solves:**\n- How do I choose HNSW vs IVFFlat?\n- Optimize vector search query time\n- Trade-off: indexing speed vs query speed\n- Real-world SkillForge benchmark results\n\n### sse-event-buffering\n**Keywords:** sse, server-sent events, event buffering, race condition, event broadcaster\n**Solves:**\n- How do I prevent SSE race conditions?\n- Buffer events before subscriber connects\n- Fix 'events lost' in real-time updates\n- Real-world SkillForge SSE debugging"
              },
              {
                "name": "performance-testing",
                "description": "Performance and load testing with k6 and Locust. Use when validating system performance under load, stress testing, identifying bottlenecks, or establishing performance baselines.",
                "path": ".claude/skills/performance-testing/SKILL.md",
                "frontmatter": {
                  "name": "performance-testing",
                  "description": "Performance and load testing with k6 and Locust. Use when validating system performance under load, stress testing, identifying bottlenecks, or establishing performance baselines.",
                  "context": "fork",
                  "agent": "metrics-architect",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Performance Testing\n\nValidate system behavior under load.\n\n## When to Use\n\n- Pre-production validation\n- Capacity planning\n- Bottleneck identification\n- Performance regression detection\n\n## k6 Load Test (JavaScript)\n\n```javascript\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '30s', target: 20 },  // Ramp up\n    { duration: '1m', target: 20 },   // Steady\n    { duration: '30s', target: 0 },   // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(95)<500'],  // 95% under 500ms\n    http_req_failed: ['rate<0.01'],    // <1% errors\n  },\n};\n\nexport default function () {\n  const res = http.get('http://localhost:8500/api/health');\n\n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'response time < 200ms': (r) => r.timings.duration < 200,\n  });\n\n  sleep(1);\n}\n```\n\n## Locust Load Test (Python)\n\n```python\nfrom locust import HttpUser, task, between\n\nclass APIUser(HttpUser):\n    wait_time = between(1, 3)\n\n    @task(3)\n    def get_analyses(self):\n        self.client.get(\"/api/analyses\")\n\n    @task(1)\n    def create_analysis(self):\n        self.client.post(\n            \"/api/analyses\",\n            json={\"url\": \"https://example.com\"}\n        )\n\n    def on_start(self):\n        \"\"\"Login before tasks.\"\"\"\n        self.client.post(\"/api/auth/login\", json={\n            \"email\": \"test@example.com\",\n            \"password\": \"password\"\n        })\n```\n\n## Test Types\n\n### Load Test\n```javascript\n// Normal expected load\nexport const options = {\n  vus: 50,           // Virtual users\n  duration: '5m',    // Duration\n};\n```\n\n### Stress Test\n```javascript\n// Find breaking point\nexport const options = {\n  stages: [\n    { duration: '2m', target: 100 },\n    { duration: '2m', target: 200 },\n    { duration: '2m', target: 300 },\n    { duration: '2m', target: 400 },\n  ],\n};\n```\n\n### Spike Test\n```javascript\n// Sudden traffic surge\nexport const options = {\n  stages: [\n    { duration: '10s', target: 10 },\n    { duration: '1s', target: 1000 },  // Spike!\n    { duration: '3m', target: 1000 },\n    { duration: '10s', target: 10 },\n  ],\n};\n```\n\n### Soak Test\n```javascript\n// Sustained load (memory leaks)\nexport const options = {\n  vus: 50,\n  duration: '4h',\n};\n```\n\n## Metrics to Track\n\n```javascript\nimport { Trend, Counter, Rate } from 'k6/metrics';\n\nconst responseTime = new Trend('response_time');\nconst errors = new Counter('errors');\nconst successRate = new Rate('success_rate');\n\nexport default function () {\n  const start = Date.now();\n  const res = http.get('http://localhost:8500/api/data');\n\n  responseTime.add(Date.now() - start);\n\n  if (res.status !== 200) {\n    errors.add(1);\n    successRate.add(false);\n  } else {\n    successRate.add(true);\n  }\n}\n```\n\n## CI Integration\n\n```yaml\n# GitHub Actions\n- name: Run k6 load test\n  run: |\n    k6 run --out json=results.json tests/load/api.js\n\n- name: Check thresholds\n  run: |\n    if [ $(jq '.thresholds | .[] | select(.ok == false)' results.json | wc -l) -gt 0 ]; then\n      exit 1\n    fi\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Tool | k6 (JS), Locust (Python) |\n| Load profile | Start with expected traffic |\n| Thresholds | p95 < 500ms, errors < 1% |\n| Duration | 5-10 min for load, 4h+ for soak |\n\n## Common Mistakes\n\n- Testing against production without protection\n- No warmup period\n- Unrealistic load profiles\n- Missing error rate thresholds\n\n## Related Skills\n\n- `observability-monitoring` - Metrics collection\n- `performance-optimization` - Fixing bottlenecks\n- `e2e-testing` - Functional validation\n\n## Capability Details\n\n### load-testing\n**Keywords:** load test, concurrent users, k6, Locust, ramp up\n**Solves:**\n- Simulate concurrent user load\n- Configure ramp-up patterns\n- Test system under expected load\n\n### stress-testing\n**Keywords:** stress test, breaking point, peak load, overload\n**Solves:**\n- Find system breaking points\n- Test beyond expected capacity\n- Identify failure modes under stress\n\n### latency-measurement\n**Keywords:** latency, response time, p95, p99, percentile\n**Solves:**\n- Measure response time percentiles\n- Track latency distribution\n- Set latency SLO thresholds\n\n### throughput-testing\n**Keywords:** throughput, requests per second, RPS, TPS\n**Solves:**\n- Measure maximum throughput\n- Test transactions per second\n- Verify capacity requirements\n\n### bottleneck-identification\n**Keywords:** bottleneck, profiling, hot path, performance issue\n**Solves:**\n- Identify performance bottlenecks\n- Profile critical code paths\n- Diagnose slow operations"
              },
              {
                "name": "pgvector-search",
                "description": "Use when implementing vector search with PostgreSQL. Covers PGVector hybrid search with BM25, metadata filtering, and performance optimization for semantic retrieval.",
                "path": ".claude/skills/pgvector-search/SKILL.md",
                "frontmatter": {
                  "name": "pgvector-search",
                  "description": "Use when implementing vector search with PostgreSQL. Covers PGVector hybrid search with BM25, metadata filtering, and performance optimization for semantic retrieval.",
                  "context": "fork",
                  "agent": "database-engineer",
                  "version": "1.2.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "pgvector-0.8",
                    "hybrid-search",
                    "bm25",
                    "rrf",
                    "semantic-search",
                    "retrieval",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/migration-validator.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/migration-validator.sh"
                      }
                    ]
                  }
                },
                "content": "# PGVector Hybrid Search\n\n**Production-grade semantic + keyword search using PostgreSQL**\n\n## Overview\n\nHybrid search combines **semantic similarity** (vector embeddings) with **keyword matching** (BM25) to achieve better retrieval than either alone.\n\n**Architecture:**\n```\nQuery\n  \n[Generate embedding]  Vector Search (PGVector)  Top 30 results\n  \n[Generate ts_query]   Keyword Search (BM25)     Top 30 results\n  \n[Reciprocal Rank Fusion (RRF)]  Merge & re-rank  Top 10 final results\n```\n\n**When to use this skill:**\n- Building semantic search (RAG, knowledge bases, recommendations)\n- Implementing hybrid retrieval (vector + keyword)\n- Optimizing PGVector performance\n- Working with large document collections (1M+ chunks)\n\n---\n\n## Core Concepts\n\n### 1. Semantic Search (Vector Similarity)\n\n**How it works:**\n1. Embed query: `\"database indexing strategies\"`  `[0.23, -0.15, ..., 0.42]` (1024 dims)\n2. Find nearest neighbors: `ORDER BY embedding <=> query_embedding LIMIT 30`\n3. Returns: Conceptually similar documents (even with different words)\n\n**Example:**\n- Query: \"machine learning model training\"\n- Matches: \"neural network optimization\", \"deep learning techniques\"\n- Misses: \"ML model training\" (different embeddings despite similar meaning)\n\n**Strengths:**\n- Captures semantic meaning\n- Works across languages\n- Handles synonyms (\"car\" matches \"automobile\")\n\n**Weaknesses:**\n- Slow for exact keyword matches\n- Sensitive to embedding quality\n- Doesn't handle rare technical terms well\n\n---\n\n### 2. Keyword Search (BM25)\n\n**How it works:**\n1. Tokenize query: `\"database indexing\"`  `database & indexing`\n2. Full-text search: `WHERE content_tsvector @@ to_tsquery('database & indexing')`\n3. Rank by BM25 score (TF-IDF + document length normalization)\n\n**Example:**\n- Query: \"PostgreSQL B-tree index\"\n- Matches: Documents with exact phrase \"PostgreSQL B-tree index\"\n- Misses: \"Postgres tree-based indexing\" (different words)\n\n**Strengths:**\n- Fast exact matches\n- Handles technical terms well\n- Works for rare/specific phrases\n\n**Weaknesses:**\n- No semantic understanding\n- Requires exact word matches\n- Sensitive to typos\n\n---\n\n### 3. Reciprocal Rank Fusion (RRF)\n\n**The Problem:** How do you combine vector scores (0.85) with BM25 scores (42.7)?\n\n**The Solution:** Use **rank** instead of score.\n\n**Algorithm:**\n```python\ndef rrf_score(rank: int, k: int = 60) -> float:\n    \"\"\"\n    Calculate RRF score for a document at given rank.\n\n    Args:\n        rank: Position in result list (1-indexed)\n        k: Smoothing constant (typically 60)\n\n    Returns:\n        Score between 0 and ~0.016 (1/k)\n    \"\"\"\n    return 1.0 / (k + rank)\n\n# Example:\n# Document appears at rank 3 in vector search  score = 1/(60+3) = 0.0159\n# Same document at rank 7 in BM25 search     score = 1/(60+7) = 0.0149\n# Combined RRF score = 0.0159 + 0.0149 = 0.0308\n```\n\n**Why it works:**\n- **Rank-based:** Ignores absolute scores (no normalization needed)\n- **Symmetric:** Treats both searches equally\n- **Robust:** Top results from either search get high scores\n\n**Detailed Implementation:** See `references/hybrid-search-rrf.md`\n\n---\n\n## SkillForge's Hybrid Search Implementation\n\n### Database Schema\n\n```sql\n-- Chunks table with vector and full-text search\nCREATE TABLE chunks (\n    id UUID PRIMARY KEY,\n    document_id UUID REFERENCES documents(id),\n    content TEXT NOT NULL,\n\n    -- Vector embedding (1024 dimensions for Voyage AI)\n    embedding vector(1024),\n\n    -- Pre-computed tsvector for full-text search\n    content_tsvector tsvector GENERATED ALWAYS AS (\n        to_tsvector('english', content)\n    ) STORED,\n\n    -- Metadata\n    section_title TEXT,\n    section_path TEXT,\n    chunk_index INT,\n    content_type TEXT,  -- 'code_block', 'paragraph', 'list', etc.\n\n    -- Timestamps\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes\nCREATE INDEX idx_chunks_embedding ON chunks\n    USING hnsw (embedding vector_cosine_ops);  -- Vector search\n\nCREATE INDEX idx_chunks_content_tsvector ON chunks\n    USING gin (content_tsvector);  -- Full-text search\n\nCREATE INDEX idx_chunks_document_id ON chunks(document_id);\nCREATE INDEX idx_chunks_content_type ON chunks(content_type);\n```\n\n---\n\n### Search Query\n\n```python\n# backend/app/db/repositories/chunk_repository.py\nfrom sqlalchemy import select, func, literal\nfrom pgvector.sqlalchemy import Vector\n\nasync def hybrid_search(\n    query: str,\n    query_embedding: list[float],\n    top_k: int = 10,\n    content_type_filter: list[str] | None = None\n) -> list[Chunk]:\n    \"\"\"\n    Perform hybrid search using RRF.\n\n    Args:\n        query: Search query text\n        query_embedding: Query embedding vector\n        top_k: Number of results to return\n        content_type_filter: Optional filter by content type\n\n    Returns:\n        List of chunks ranked by RRF score\n    \"\"\"\n\n    # Fetch multiplier (retrieve more for better RRF)\n    FETCH_MULTIPLIER = 3\n    fetch_limit = top_k * FETCH_MULTIPLIER  # 30 for top_k=10\n\n    # ===== 1. VECTOR SEARCH =====\n    vector_subquery = (\n        select(\n            Chunk.id,\n            (Chunk.embedding.cosine_distance(query_embedding)).label(\"vector_distance\"),\n            func.row_number().over(\n                order_by=Chunk.embedding.cosine_distance(query_embedding)\n            ).label(\"vector_rank\")\n        )\n        .where(Chunk.embedding.isnot(None))\n    )\n\n    # Apply content type filter\n    if content_type_filter:\n        vector_subquery = vector_subquery.where(\n            Chunk.content_type.in_(content_type_filter)\n        )\n\n    vector_subquery = vector_subquery.limit(fetch_limit).subquery(\"vector_results\")\n\n    # ===== 2. KEYWORD SEARCH (BM25) =====\n    # Generate tsquery\n    ts_query = func.plainto_tsquery(\"english\", query)\n\n    keyword_subquery = (\n        select(\n            Chunk.id,\n            func.ts_rank_cd(Chunk.content_tsvector, ts_query).label(\"bm25_score\"),\n            func.row_number().over(\n                order_by=func.ts_rank_cd(Chunk.content_tsvector, ts_query).desc()\n            ).label(\"keyword_rank\")\n        )\n        .where(Chunk.content_tsvector.op(\"@@\")(ts_query))\n    )\n\n    # Apply content type filter\n    if content_type_filter:\n        keyword_subquery = keyword_subquery.where(\n            Chunk.content_type.in_(content_type_filter)\n        )\n\n    keyword_subquery = keyword_subquery.limit(fetch_limit).subquery(\"keyword_results\")\n\n    # ===== 3. RECIPROCAL RANK FUSION =====\n    K = 60  # RRF smoothing constant\n\n    rrf_query = (\n        select(\n            func.coalesce(vector_subquery.c.id, keyword_subquery.c.id).label(\"chunk_id\"),\n            (\n                func.coalesce(1.0 / (K + vector_subquery.c.vector_rank), 0.0) +\n                func.coalesce(1.0 / (K + keyword_subquery.c.keyword_rank), 0.0)\n            ).label(\"rrf_score\")\n        )\n        .select_from(\n            vector_subquery.outerjoin(\n                keyword_subquery,\n                vector_subquery.c.id == keyword_subquery.c.id,\n                full=True\n            )\n        )\n        .order_by(literal(\"rrf_score\").desc())\n        .limit(top_k)\n    ).subquery(\"rrf_results\")\n\n    # ===== 4. FETCH FULL CHUNKS =====\n    final_query = (\n        select(Chunk)\n        .join(rrf_query, Chunk.id == rrf_query.c.chunk_id)\n        .order_by(rrf_query.c.rrf_score.desc())\n    )\n\n    result = await session.execute(final_query)\n    chunks = result.scalars().all()\n\n    return chunks\n```\n\n**Key Features:**\n1. **3x Fetch Multiplier:** Retrieve 30 results from each search (better RRF coverage)\n2. **Indexed tsvector:** Uses `content_tsvector` column (5-10x faster than `to_tsvector()` on query)\n3. **Full outer join:** Includes results from either search (vector OR keyword)\n4. **Content type filtering:** Optional pre-filter by metadata\n\n---\n\n## Performance Optimizations\n\n### 1. Pre-Computed `tsvector` Column\n\n**Before (Slow):**\n```sql\n-- Computes tsvector on every query (SLOW!)\nWHERE to_tsvector('english', content) @@ to_tsquery('database')\n```\n\n**After (Fast):**\n```sql\n-- Uses pre-computed column with GIN index (FAST!)\nWHERE content_tsvector @@ to_tsquery('database')\n```\n\n**Speedup:** 5-10x faster for keyword search\n\n---\n\n### 2. HNSW vs IVFFlat Indexes\n\n**IVFFlat (Older):**\n```sql\nCREATE INDEX idx_embedding ON chunks\n    USING ivfflat (embedding vector_cosine_ops)\n    WITH (lists = 100);\n```\n- Faster indexing\n- Slower queries\n- Good for < 100k vectors\n\n**HNSW (Recommended):**\n```sql\nCREATE INDEX idx_embedding ON chunks\n    USING hnsw (embedding vector_cosine_ops)\n    WITH (m = 16, ef_construction = 64);\n```\n- Slower indexing\n- **Much faster queries** (10-100x)\n- Scales to millions of vectors\n\n**SkillForge uses HNSW (415 chunks, room to scale to 10M+).**\n\n**Detailed Comparison:** See `references/indexing-strategies.md`\n\n---\n\n### Iterative Index Scans (pgvector 0.8.x)\n\n**The Problem:** With filtered queries, HNSW might not return enough results:\n```sql\n-- ef_search=40, but only 10% of data matches filter\n-- Result: ~4 usable results instead of requested 10\nSELECT * FROM chunks\nWHERE tenant_id = 'abc'\nORDER BY embedding <=> query_embedding\nLIMIT 10;\n```\n\n**The Solution:** Enable iterative scanning to continue searching until conditions are met:\n\n```sql\n-- Enable iterative scan (3 modes: off, strict_order, relaxed_order)\nSET hnsw.iterative_scan = 'relaxed_order';  -- Best performance\nSET hnsw.max_scan_tuples = 20000;           -- Limit for safety\n\n-- Now filtered queries return full results\nSELECT * FROM chunks\nWHERE tenant_id = 'abc' AND content_type = 'code_block'\nORDER BY embedding <=> query_embedding\nLIMIT 10;\n```\n\n**Iterative Scan Modes:**\n| Mode | Ordering | Performance | Use Case |\n|------|----------|-------------|----------|\n| `off` | Exact | Baseline | Unfiltered queries |\n| `strict_order` | Exact | Slower | When exact order matters |\n| `relaxed_order` | Approximate | **Best** | Most production use cases |\n\n**Tuning `ef_search`:**\n```sql\n-- Higher ef_search = better recall, more memory\nSET hnsw.ef_search = 100;  -- Default is 40\n\n-- For complex filtered queries\nSET hnsw.ef_search = 200;\nSET hnsw.iterative_scan = 'relaxed_order';\n```\n\n---\n\n### 3. Metadata Filtering & Boosting\n\n**Problem:** Some chunks are more valuable than others.\n\n**Solution: Boost by metadata:**\n\n```python\n# Boost section titles (1.5x)\nif query_matches_section_title(chunk.section_title, query):\n    rrf_score *= 1.5\n\n# Boost document path (1.15x)\nif query_matches_path(chunk.section_path, query):\n    rrf_score *= 1.15\n\n# Boost code blocks for technical queries (1.2x)\nif is_technical_query(query) and chunk.content_type == \"code_block\":\n    rrf_score *= 1.2\n```\n\n**SkillForge Evaluation:**\n- **Before boosting:** 91.1% pass rate, 0.647 MRR (Hard queries)\n- **After boosting:** 91.6% pass rate, 0.686 MRR (Hard queries)\n- **Improvement:** +0.5% pass rate, +6% MRR\n\n**Detailed Implementation:** See `references/metadata-filtering.md`\n\n---\n\n## Common Patterns\n\n### Pattern 1: Filtered Search\n\n```python\n# Search only code blocks\nresults = await hybrid_search(\n    query=\"binary search implementation\",\n    query_embedding=embedding,\n    content_type_filter=[\"code_block\"]\n)\n```\n\n### Pattern 2: Similarity Threshold\n\n```python\n# Only return results above similarity threshold\nMIN_SIMILARITY = 0.75\n\nresults = await hybrid_search(query, embedding, top_k=50)\nfiltered = [\n    r for r in results\n    if (1 - r.vector_distance) >= MIN_SIMILARITY\n][:10]\n```\n\n### Pattern 3: Multi-Query Retrieval\n\n```python\n# Generate multiple query variations for better recall\nqueries = generate_query_variations(\"machine learning\")\n# [\"machine learning\", \"ML algorithms\", \"neural networks\"]\n\nall_results = []\nfor q in queries:\n    emb = embed(q)\n    results = await hybrid_search(q, emb, top_k=5)\n    all_results.extend(results)\n\n# De-duplicate and re-rank\nfinal_results = rerank_by_rrf(all_results, top_k=10)\n```\n\n---\n\n## Testing Hybrid Search\n\n### Golden Dataset Evaluation\n\n```python\n# backend/tests/integration/test_hybrid_search.py\nimport pytest\nfrom app.db.repositories.chunk_repository import hybrid_search\nfrom app.shared.services.embeddings import embed_text\n\n@pytest.mark.asyncio\nasync def test_hybrid_search_golden_dataset():\n    \"\"\"Test hybrid search against golden queries.\"\"\"\n\n    golden_queries = load_golden_queries()  # 98 queries\n\n    results = []\n    for query_data in golden_queries:\n        query = query_data[\"query\"]\n        expected_chunks = query_data[\"expected_chunk_ids\"]\n\n        # Perform search\n        embedding = await embed_text(query)\n        retrieved = await hybrid_search(query, embedding, top_k=10)\n        retrieved_ids = {c.id for c in retrieved}\n\n        # Check if expected chunks are in top 10\n        found = len(expected_chunks & retrieved_ids)\n        results.append({\n            \"query\": query,\n            \"expected\": len(expected_chunks),\n            \"found\": found,\n            \"pass\": found == len(expected_chunks)\n        })\n\n    # Calculate metrics\n    pass_rate = sum(r[\"pass\"] for r in results) / len(results)\n    mrr = calculate_mrr(results)\n\n    print(f\"Pass Rate: {pass_rate:.1%}\")\n    print(f\"MRR: {mrr:.3f}\")\n\n    assert pass_rate >= 0.90, f\"Pass rate {pass_rate:.1%} below 90% threshold\"\n```\n\n---\n\n## References\n\n### PGVector Documentation\n- [PGVector GitHub](https://github.com/pgvector/pgvector)\n- [HNSW Index Guide](https://github.com/pgvector/pgvector#hnsw)\n\n### SkillForge Implementation\n- `backend/app/db/repositories/chunk_repository.py` - Hybrid search implementation\n- `backend/app/shared/services/search/search_service.py` - Search service layer\n- `backend/app/core/constants.py` - Search constants (fetch multiplier, boosting factors)\n\n### Related Skills\n- `ai-native-development` - Embeddings and vector concepts\n- `database-schema-designer` - Schema design for vector search\n- `performance-optimization` - Query optimization strategies\n\n---\n\n**Version:** 1.2.0 (January 2026)\n**Status:** Production-ready patterns from SkillForge's 415-chunk golden dataset\n**Updated:** pgvector 0.8.1 with improved filtering and iterative scan support\n\n## Capability Details\n\n### hybrid-search-rrf\n**Keywords:** hybrid search, rrf, reciprocal rank fusion, vector bm25, semantic keyword search\n**Solves:**\n- How do I combine vector and keyword search?\n- Implement hybrid retrieval with RRF\n- Merge semantic and BM25 results\n- Improve search recall and precision\n\n### semantic-search\n**Keywords:** semantic search, vector similarity, embedding, nearest neighbor, cosine distance\n**Solves:**\n- How does semantic search work?\n- Understand vector similarity search\n- When to use semantic vs keyword search\n- Semantic search strengths and weaknesses\n\n### keyword-search-bm25\n**Keywords:** bm25, full-text search, tsvector, tsquery, keyword search\n**Solves:**\n- How does BM25 keyword search work?\n- Implement PostgreSQL full-text search\n- Use tsvector and tsquery\n- BM25 vs semantic search trade-offs\n\n### rrf-algorithm\n**Keywords:** rrf, reciprocal rank fusion, rank-based fusion, score normalization\n**Solves:**\n- How does Reciprocal Rank Fusion work?\n- Why use rank instead of scores?\n- RRF smoothing constant (k parameter)\n- Combine incompatible score ranges\n\n### database-schema\n**Keywords:** pgvector schema, chunk table, embedding column, tsvector, generated column\n**Solves:**\n- How do I design schema for hybrid search?\n- Store embeddings with vector(1024)\n- Pre-compute tsvector for performance\n- Index setup for vector and keyword search\n\n### search-query-implementation\n**Keywords:** hybrid search query, sqlalchemy, vector distance, ts_rank_cd, full outer join\n**Solves:**\n- How do I write hybrid search SQL?\n- Implement RRF in SQLAlchemy\n- Use fetch multiplier for better coverage\n- Combine vector and keyword results\n\n### indexing-strategies\n**Keywords:** pgvector index, hnsw, ivfflat, vector index performance, index tuning\n**Solves:**\n- How do I index PGVector for performance?\n- HNSW vs IVFFlat comparison\n- Optimize vector search speed (17x faster)\n- Scale to millions of vectors\n\n### pre-computed-tsvector\n**Keywords:** tsvector, gin index, full-text index, pre-computed column, generated column\n**Solves:**\n- How do I optimize keyword search performance?\n- Pre-compute tsvector vs query-time computation\n- 5-10x speedup with indexed tsvector\n- Use GENERATED ALWAYS AS column\n\n### metadata-filtering\n**Keywords:** metadata filter, faceted search, content type filter, score boosting\n**Solves:**\n- How do I filter search by metadata?\n- Boost results by section title (1.5x)\n- Pre-filter by content type before search\n- Improve search relevance with boosting\n\n### metadata-boosting\n**Keywords:** score boosting, section title boost, document path boost, code block boost\n**Solves:**\n- How do I boost search results by metadata?\n- Implement section title boosting (1.5x)\n- Boost technical queries for code blocks\n- Real-world +6% MRR improvement\n\n### common-patterns\n**Keywords:** filtered search, similarity threshold, multi-query retrieval, search patterns\n**Solves:**\n- How do I filter search by content type?\n- Set minimum similarity threshold\n- Implement multi-query retrieval\n- De-duplicate and re-rank results\n\n### golden-dataset-testing\n**Keywords:** golden dataset, search evaluation, pass rate, mrr, retrieval testing\n**Solves:**\n- How do I test hybrid search quality?\n- Evaluate search with golden queries\n- Calculate pass rate and MRR metrics\n- Benchmark against expected results"
              },
              {
                "name": "project-structure-enforcer",
                "description": "Enforce 2026 folder structure standards - feature-based organization, max nesting depth, unidirectional imports. Blocks structural violations. Use when creating files or reviewing project architecture.",
                "path": ".claude/skills/project-structure-enforcer/SKILL.md",
                "frontmatter": {
                  "name": "project-structure-enforcer",
                  "description": "Enforce 2026 folder structure standards - feature-based organization, max nesting depth, unidirectional imports. Blocks structural violations. Use when creating files or reviewing project architecture.",
                  "context": "fork",
                  "agent": "code-quality-reviewer",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "structure",
                    "architecture",
                    "enforcement",
                    "blocking",
                    "imports",
                    "organization"
                  ],
                  "hooks": {
                    "PreToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/structure-location-validator.sh"
                      }
                    ],
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/import-direction-enforcer.sh"
                      }
                    ]
                  }
                },
                "content": "# Project Structure Enforcer\n\nEnforce 2026 folder structure best practices with **BLOCKING** validation.\n\n## When to Use\n\n- Creating new files\n- Reviewing project architecture\n- Refactoring folder structure\n- Validating import patterns\n\n## Validation Rules\n\n### BLOCKING Rules (exit 1)\n\n| Rule | Check | Example Violation |\n|------|-------|-------------------|\n| **Max Nesting** | Max 4 levels from src/ or app/ | `src/a/b/c/d/e/file.ts` |\n| **No Barrel Files** | No index.ts re-exports | `src/components/index.ts` |\n| **Component Location** | React components in components/ or features/ | `src/utils/Button.tsx` |\n| **Hook Location** | Custom hooks in hooks/ directory | `src/components/useAuth.ts` |\n| **Import Direction** | Unidirectional: shared  features  app | `features/` importing from `app/` |\n\n## Expected Folder Structures\n\n### React/Next.js (Frontend)\n\n```\nsrc/\n app/              # Next.js App Router (pages)\n    (auth)/       # Route groups\n    api/          # API routes\n    layout.tsx\n components/       # Reusable UI components\n    ui/           # Primitive components\n    forms/        # Form components\n features/         # Feature modules (self-contained)\n    auth/\n       components/\n       hooks/\n       services/\n       types.ts\n    dashboard/\n hooks/            # Global custom hooks\n lib/              # Third-party integrations\n services/         # API clients\n types/            # Global TypeScript types\n utils/            # Pure utility functions\n```\n\n### FastAPI (Backend)\n\n```\napp/\n routers/          # API route handlers\n    router_users.py\n    router_auth.py\n    deps.py       # Shared dependencies\n services/         # Business logic layer\n    user_service.py\n    auth_service.py\n repositories/     # Data access layer\n    user_repository.py\n    base_repository.py\n schemas/          # Pydantic models\n    user_schema.py\n    auth_schema.py\n models/           # SQLAlchemy models\n    user_model.py\n    base.py\n core/             # Config, security, deps\n    config.py\n    security.py\n    database.py\n utils/            # Utility functions\n```\n\n## Nesting Depth Rules\n\nMaximum 4 levels from `src/` or `app/`:\n\n```\nALLOWED (4 levels):\n  src/features/auth/components/LoginForm.tsx\n  app/routers/v1/users/router_users.py\n\nBLOCKED (5+ levels):\n  src/features/dashboard/widgets/charts/line/LineChart.tsx\n   Flatten to: src/features/dashboard/charts/LineChart.tsx\n```\n\n## No Barrel Files\n\nBarrel files (`index.ts` that only re-export) cause tree-shaking issues with Vite/webpack:\n\n```typescript\n// BLOCKED: src/components/index.ts\nexport { Button } from './Button';\nexport { Input } from './Input';\nexport { Modal } from './Modal';\n\n// GOOD: Import directly\nimport { Button } from '@/components/Button';\nimport { Input } from '@/components/Input';\n```\n\n**Why?** Barrel files:\n- Break tree-shaking (entire barrel is imported)\n- Cause circular dependency issues\n- Slow down build times\n- Make debugging harder\n\n## Import Direction (Unidirectional Architecture)\n\nCode must flow in ONE direction:\n\n```\n\n                                                         \n   shared/lib    components    features    app       \n                                                         \n   (lowest)                                 (highest)    \n                                                         \n\n```\n\n### Allowed Imports\n\n| Layer | Can Import From |\n|-------|-----------------|\n| `shared/`, `lib/` | Nothing (base layer) |\n| `components/` | `shared/`, `lib/`, `utils/` |\n| `features/` | `shared/`, `lib/`, `components/`, `utils/` |\n| `app/` | Everything above |\n\n### Blocked Imports\n\n```typescript\n// BLOCKED: shared/ importing from features/\n// File: src/shared/utils.ts\nimport { authConfig } from '@/features/auth/config';  // \n\n// BLOCKED: features/ importing from app/\n// File: src/features/auth/useAuth.ts\nimport { RootLayout } from '@/app/layout';  // \n\n// BLOCKED: Cross-feature imports\n// File: src/features/auth/useAuth.ts\nimport { DashboardContext } from '@/features/dashboard/context';  // \n// Fix: Extract to shared/ if needed by multiple features\n```\n\n### Type-Only Imports (Exception)\n\nType-only imports across features are allowed:\n\n```typescript\n// ALLOWED: Type-only import from another feature\nimport type { User } from '@/features/users/types';\n```\n\n## Component Location Rules\n\n### React Components (PascalCase .tsx)\n\n```\nALLOWED:\n  src/components/Button.tsx\n  src/components/ui/Card.tsx\n  src/features/auth/components/LoginForm.tsx\n  src/app/dashboard/page.tsx\n\nBLOCKED:\n  src/utils/Button.tsx       # Components not in utils/\n  src/services/Modal.tsx     # Components not in services/\n  src/hooks/Dropdown.tsx     # Components not in hooks/\n```\n\n### Custom Hooks (useX pattern)\n\n```\nALLOWED:\n  src/hooks/useAuth.ts\n  src/hooks/useLocalStorage.ts\n  src/features/auth/hooks/useLogin.ts\n\nBLOCKED:\n  src/components/useAuth.ts   # Hooks not in components/\n  src/utils/useDebounce.ts    # Hooks not in utils/\n  src/services/useFetch.ts    # Hooks not in services/\n```\n\n## Python File Location Rules\n\n### Routers\n\n```\nALLOWED:\n  app/routers/router_users.py\n  app/routers/routes_auth.py\n  app/routers/api_v1.py\n\nBLOCKED:\n  app/users_router.py          # Not in routers/\n  app/services/router_users.py # Router in services/\n```\n\n### Services\n\n```\nALLOWED:\n  app/services/user_service.py\n  app/services/auth_service.py\n\nBLOCKED:\n  app/user_service.py           # Not in services/\n  app/routers/user_service.py   # Service in routers/\n```\n\n## Common Violations\n\n### 1. Too Deep Nesting\n```\nBLOCKED: Max nesting depth exceeded: 5 levels (max: 4)\n  File: src/features/dashboard/widgets/charts/line/LineChart.tsx\n  Consider flattening: src/features/dashboard/charts/LineChart.tsx\n```\n\n### 2. Barrel File Created\n```\nBLOCKED: Barrel files (index.ts) discouraged - causes tree-shaking issues\n  File: src/components/index.ts\n  Import directly from source files instead\n```\n\n### 3. Component in Wrong Location\n```\nBLOCKED: React components must be in components/, features/, or app/\n  File: src/utils/Button.tsx\n  Move to: src/components/Button.tsx\n```\n\n### 4. Invalid Import Direction\n```\nBLOCKED: Import direction violation (unidirectional architecture)\n  features/ cannot import from app/\n  Import direction: features -> shared, lib, components\n\nAllowed flow: shared/lib -> components -> features -> app\n```\n\n### 5. Cross-Feature Import\n```\nBLOCKED: Cannot import from other features (cross-feature dependency)\n  File: src/features/auth/useAuth.ts\n  Import: from '@/features/dashboard/context'\n  Extract shared code to shared/ or lib/\n```\n\n## Migration Guide\n\n### Flattening Deep Nesting\n\n```bash\n# Before (5 levels)\nsrc/features/dashboard/widgets/charts/line/LineChart.tsx\nsrc/features/dashboard/widgets/charts/line/LineChartTooltip.tsx\n\n# After (4 levels) - Flatten last two levels\nsrc/features/dashboard/charts/LineChart.tsx\nsrc/features/dashboard/charts/LineChartTooltip.tsx\n```\n\n### Removing Barrel Files\n\n```bash\n# Before\nsrc/components/index.ts  # Re-exports everything\nimport { Button, Input } from '@/components';\n\n# After - Direct imports\nimport { Button } from '@/components/Button';\nimport { Input } from '@/components/Input';\n```\n\n### Fixing Cross-Feature Imports\n\n```bash\n# Before - Cross-feature dependency\nsrc/features/auth/useAuth.ts imports from src/features/users/types\n\n# After - Extract to shared\nsrc/shared/types/user.ts\nsrc/features/auth/useAuth.ts imports from src/shared/types/user\nsrc/features/users/... imports from src/shared/types/user\n```\n\n## Related Skills\n\n- `backend-architecture-enforcer` - FastAPI layer separation\n- `clean-architecture` - DDD patterns\n- `type-safety-validation` - TypeScript strictness\n\n## Capability Details\n\n### folder-structure\n**Keywords:** folder structure, directory structure, project layout, organization\n**Solves:**\n- Enforce feature-based organization\n- Validate proper file placement\n- Maintain consistent project structure\n\n### nesting-depth\n**Keywords:** nesting, depth, levels, max depth, deep nesting\n**Solves:**\n- Limit directory nesting to 4 levels\n- Prevent overly complex structures\n- Improve navigability\n\n### import-direction\n**Keywords:** import, unidirectional, circular, dependency direction\n**Solves:**\n- Enforce unidirectional imports\n- Prevent circular dependencies\n- Maintain clean architecture\n\n### component-location\n**Keywords:** component location, file placement, where to put\n**Solves:**\n- Validate React component placement\n- Enforce hook location rules\n- Block barrel files"
              },
              {
                "name": "prompt-caching",
                "description": "Provider-native prompt caching for Claude and OpenAI. Use when optimizing LLM costs with cache breakpoints, caching system prompts, or reducing token costs for repeated prefixes.",
                "path": ".claude/skills/prompt-caching/SKILL.md",
                "frontmatter": {
                  "name": "prompt-caching",
                  "description": "Provider-native prompt caching for Claude and OpenAI. Use when optimizing LLM costs with cache breakpoints, caching system prompts, or reducing token costs for repeated prefixes.",
                  "context": "fork",
                  "agent": "llm-integrator"
                },
                "content": "# Prompt Caching\n\nCache LLM prompt prefixes for 90% token savings.\n\n## When to Use\n\n- Same system prompts across requests\n- Few-shot examples in prompts\n- Schema documentation in prompts\n- High-volume API calls\n\n## Supported Models (2026)\n\n| Provider | Models |\n|----------|--------|\n| Claude | Opus 4.1, Opus 4, Sonnet 4.5, Sonnet 4, Sonnet 3.7, Haiku 4.5, Haiku 3.5, Haiku 3 |\n| OpenAI | gpt-4o, gpt-4o-mini, o1, o1-mini (automatic caching) |\n\n## Claude Prompt Caching\n\n```python\ndef build_cached_messages(\n    system_prompt: str,\n    few_shot_examples: str | None,\n    user_content: str,\n    use_extended_cache: bool = False\n) -> list[dict]:\n    \"\"\"Build messages with cache breakpoints.\n\n    Cache structure (processing order: tools  system  messages):\n    1. System prompt (cached)\n    2. Few-shot examples (cached)\n     CACHE BREAKPOINT \n    3. User content (NOT cached)\n    \"\"\"\n    # TTL: \"5m\" (default, 1.25x write cost) or \"1h\" (extended, 2x write cost)\n    ttl = \"1h\" if use_extended_cache else \"5m\"\n\n    content_parts = []\n\n    # Breakpoint 1: System prompt\n    content_parts.append({\n        \"type\": \"text\",\n        \"text\": system_prompt,\n        \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": ttl}\n    })\n\n    # Breakpoint 2: Few-shot examples (up to 4 breakpoints allowed)\n    if few_shot_examples:\n        content_parts.append({\n            \"type\": \"text\",\n            \"text\": few_shot_examples,\n            \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": ttl}\n        })\n\n    # Dynamic content (NOT cached)\n    content_parts.append({\n        \"type\": \"text\",\n        \"text\": user_content\n    })\n\n    return [{\"role\": \"user\", \"content\": content_parts}]\n```\n\n## Cache Pricing (2026)\n\n```\n\n  Cache Cost Multipliers (relative to base input price)      \n\n  5-minute cache write:  1.25x base input price              \n  1-hour cache write:    2.00x base input price              \n  Cache read:            0.10x base input price (90% off!)   \n\n\nExample: Claude Sonnet 4 @ $3/MTok input\n\nWithout Prompt Caching:\nSystem prompt:     2,000 tokens @ $3/MTok  = $0.006\nFew-shot examples: 5,000 tokens @ $3/MTok  = $0.015\nUser content:     10,000 tokens @ $3/MTok  = $0.030\n\nTotal:            17,000 tokens            = $0.051\n\nWith 5m Caching (first request = cache write):\nCached prefix:     7,000 tokens @ $3.75/MTok = $0.02625 (1.25x)\nUser content:     10,000 tokens @ $3/MTok    = $0.03000\nTotal first req:                             = $0.05625\n\nWith 5m Caching (subsequent = cache read):\nCached prefix:     7,000 tokens @ $0.30/MTok = $0.0021 (0.1x)\nUser content:     10,000 tokens @ $3/MTok    = $0.0300\nTotal cached req:                            = $0.0321\n\nSavings: 37% per cached request, break-even after 2 requests\n```\n\n## Extended Cache (1-hour TTL)\n\nUse 1-hour cache when:\n- Prompt reused > 10 times per hour\n- System prompts are highly stable\n- Token count > 10k (maximize savings)\n\n```python\n# Extended cache: 2x write cost but persists 12x longer\n\"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}\n\n# Break-even: 1h cache pays off after ~8 reads\n# (2x write cost  0.9 savings per read  8 reads)\n```\n\n## OpenAI Automatic Caching\n\n```python\n# OpenAI caches prefixes automatically\n# No cache_control markers needed\n\nresponse = await openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},  # Cached\n        {\"role\": \"user\", \"content\": user_content}      # Not cached\n    ]\n)\n\n# Check cache usage in response\ncache_tokens = response.usage.prompt_tokens_cached\n```\n\n## Cache Processing Order\n\n```\nCache references entire prompt in order:\n1. Tools (cached first)\n2. System messages (cached second)\n3. User messages (cached last)\n\n Extended thinking changes invalidate message caches\n   (but NOT system/tools caches)\n```\n\n## Monitoring Cache Effectiveness\n\n```python\n# Track these fields in API response\nresponse = await client.messages.create(...)\n\ncache_created = response.usage.cache_creation_input_tokens  # New cache\ncache_read = response.usage.cache_read_input_tokens         # Cache hit\nregular = response.usage.input_tokens                        # Not cached\n\n# Calculate cache hit rate\nif cache_created + cache_read > 0:\n    hit_rate = cache_read / (cache_created + cache_read)\n    print(f\"Cache hit rate: {hit_rate:.1%}\")\n```\n\n## Best Practices\n\n```python\n#  Good: Long, stable prefix first\nmessages = [\n    {\"role\": \"system\", \"content\": LONG_SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": FEW_SHOT_EXAMPLES},\n    {\"role\": \"user\", \"content\": user_input}  # Variable\n]\n\n#  Bad: Variable content early (breaks cache)\nmessages = [\n    {\"role\": \"user\", \"content\": user_input},  # Breaks cache!\n    {\"role\": \"system\", \"content\": LONG_SYSTEM_PROMPT}\n]\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Min prefix size | 1,024 tokens (Claude) |\n| Breakpoint count | 2-4 per request |\n| Content order | Stable prefix first |\n| Default TTL | 5m for most cases |\n| Extended TTL | 1h if >10 reads/hour |\n\n## Common Mistakes\n\n- Variable content before cached prefix\n- Too many breakpoints (overhead)\n- Prefix too short (min 1024 tokens)\n- Not checking `cache_read_input_tokens`\n- Using 1h TTL for infrequent calls (wastes 2x write)\n\n## Related Skills\n\n- `semantic-caching` - Redis similarity caching\n- `cache-cost-tracking` - Cost monitoring\n- `llm-streaming` - Streaming with caching\n\n## Capability Details\n\n### anthropic-caching\n**Keywords:** anthropic, claude, cache_control, ephemeral\n**Solves:**\n- Use Anthropic prompt caching\n- Set cache breakpoints\n- Reduce API costs\n\n### openai-caching\n**Keywords:** openai, gpt, cached_tokens, automatic\n**Solves:**\n- Use OpenAI prompt caching\n- Structure prompts for cache hits\n- Monitor cache effectiveness\n\n### wrapper-template\n**Keywords:** wrapper, template, implementation, python\n**Solves:**\n- Prompt cache wrapper template\n- Python implementation\n- Drop-in caching layer"
              },
              {
                "name": "quality-gates",
                "description": "Use when assessing task complexity, before starting complex tasks, or when stuck after multiple attempts. Provides complexity scoring (1-5), blocking thresholds, and escalation workflows to prevent wasted work.",
                "path": ".claude/skills/quality-gates/SKILL.md",
                "frontmatter": {
                  "name": "quality-gates",
                  "description": "Use when assessing task complexity, before starting complex tasks, or when stuck after multiple attempts. Provides complexity scoring (1-5), blocking thresholds, and escalation workflows to prevent wasted work.",
                  "context": "fork",
                  "agent": "code-quality-reviewer",
                  "version": "1.1.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "quality",
                    "complexity",
                    "planning",
                    "escalation",
                    "blocking"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Quality Gates Skill\n\n**Version:** 1.0.0\n**Type:** Quality Assurance & Risk Management\n**Auto-activate:** Task planning, complexity assessment, requirement gathering, before task execution\n\n## Overview\n\nThis skill teaches agents how to assess task complexity, enforce quality gates, and prevent wasted work on incomplete or poorly-defined tasks. Inspired by production-grade development practices, quality gates ensure agents have sufficient context before proceeding and automatically escalate when stuck or blocked.\n\n**Key Principle:** Stop and clarify before proceeding with incomplete information. Better to ask questions than to waste cycles on the wrong solution.\n\n---\n\n## When to Use This Skill\n\n### Auto-Activate Triggers\n- Receiving a new task assignment\n- Starting a complex feature implementation\n- Before allocating work in Squad mode\n- When requirements seem unclear or incomplete\n- After 3 failed attempts at the same task\n- When blocked by dependencies\n\n### Manual Activation\n- User asks for complexity assessment\n- Planning a multi-step project\n- Before committing to a timeline\n- When uncertain about requirements\n\n---\n\n## Core Concepts\n\n### 1. Complexity Scoring (1-5 Scale)\n\nAssess every task on a 1-5 complexity scale:\n\n**Level 1: Trivial**\n- Single file change\n- Simple variable rename\n- Documentation update\n- CSS styling tweak\n- < 50 lines of code\n- < 30 minutes estimated\n- No dependencies\n- No unknowns\n\n**Level 2: Simple**\n- 1-3 file changes\n- Basic function implementation\n- Simple API endpoint (CRUD)\n- Straightforward component\n- 50-200 lines of code\n- 30 minutes - 2 hours estimated\n- 0-1 dependencies\n- Minimal unknowns\n\n**Level 3: Moderate**\n- 3-10 file changes\n- Multiple component coordination\n- API with validation and error handling\n- State management integration\n- Database schema changes\n- 200-500 lines of code\n- 2-8 hours estimated\n- 2-3 dependencies\n- Some unknowns that need research\n\n**Level 4: Complex**\n- 10-25 file changes\n- Cross-cutting concerns\n- Authentication/authorization\n- Real-time features (WebSockets)\n- Payment integration\n- Database migrations with data\n- 500-1500 lines of code\n- 8-24 hours (1-3 days) estimated\n- 4-6 dependencies\n- Significant unknowns\n- Multiple decision points\n\n**Level 5: Very Complex**\n- 25+ file changes\n- Architectural changes\n- New service/microservice\n- Complete feature subsystem\n- Third-party API integration\n- Performance optimization\n- 1500+ lines of code\n- 24+ hours (3+ days) estimated\n- 7+ dependencies\n- Many unknowns\n- Requires research and prototyping\n- High risk of scope creep\n\n### 2. Quality Gate Thresholds\n\n**BLOCKING Conditions** (MUST resolve before proceeding):\n\n1. **Incomplete Requirements** (>3 critical questions)\n   - If you have more than 3 unanswered critical questions, STOP\n   - Examples of critical questions:\n     - \"What should happen when X fails?\"\n     - \"What data structure should I use?\"\n     - \"What's the expected behavior for edge case Y?\"\n     - \"Which API should I call?\"\n     - \"What authentication method?\"\n\n2. **Missing Dependencies** (blocked by another task)\n   - Task depends on incomplete work\n   - Required API endpoint doesn't exist\n   - Database schema not ready\n   - External service not configured\n\n3. **Stuck Detection** (3 attempts at same task)\n   - Tried 3 different approaches, all failed\n   - Keep encountering the same error\n   - Can't find necessary information\n   - Solution keeps breaking other things\n\n4. **Evidence Failure** (tests/builds failing)\n   - Tests fail after 2 fix attempts\n   - Build breaks after changes\n   - Type errors persist\n   - Integration tests failing\n\n5. **Complexity Overflow** (Level 4-5 tasks without breakdown)\n   - Complex task not broken into subtasks\n   - No clear implementation plan\n   - Too many unknowns\n   - Scope unclear\n\n**WARNING Conditions** (Can proceed with caution):\n\n1. **Moderate Complexity** (Level 3)\n   - Can proceed but should verify approach first\n   - Document assumptions\n   - Plan for checkpoints\n\n2. **1-2 Unanswered Questions**\n   - Document assumptions\n   - Proceed with best guess\n   - Note for review later\n\n3. **1-2 Failed Attempts**\n   - Try alternative approach\n   - Document what didn't work\n   - Consider asking for help\n\n### 3. Gate Validation Process\n\n```markdown\n## Quality Gate Check\n\n**Task:** [Task description]\n**Complexity:** [1-5 scale]\n**Dependencies:** [List dependencies]\n\n### Critical Questions (Must answer before proceeding)\n1. [Question 1] -  Answered /  Unknown\n2. [Question 2] -  Answered /  Unknown\n3. [Question 3] -  Answered /  Unknown\n\n**Unanswered Critical Questions:** [Count]\n\n### Dependency Check\n- [ ] All required APIs exist\n- [ ] Database schema ready\n- [ ] Required services running\n- [ ] External APIs accessible\n- [ ] Authentication configured\n\n**Blocked Dependencies:** [List]\n\n### Attempt History\n- Attempt 1: [What was tried, outcome]\n- Attempt 2: [What was tried, outcome]\n- Attempt 3: [What was tried, outcome]\n\n**Failed Attempts:** [Count]\n\n### Gate Status\n-  **PASS** - Can proceed\n-  **WARNING** - Proceed with caution\n-  **BLOCKED** - Must resolve before proceeding\n\n### Blocking Reasons (if blocked)\n- [ ] >3 critical questions unanswered\n- [ ] Missing dependencies\n- [ ] 3+ failed attempts (stuck)\n- [ ] Evidence shows failures\n- [ ] Complexity too high without plan\n\n### Actions Required\n[List actions needed to unblock]\n```\n\n---\n\n## Quality Gate Workflows\n\n### Workflow 1: Pre-Task Gate Validation\n\n**When:** Before starting any task (especially Level 3-5)\n\n**Steps:**\n\n1. **Assess Complexity**\n   ```\n   Read task description\n   Count file changes needed\n   Estimate lines of code\n   Identify dependencies\n   Count unknowns\n    Assign complexity score (1-5)\n   ```\n\n2. **Identify Critical Questions**\n   ```\n   What must I know to complete this?\n   - Data structures?\n   - Expected behaviors?\n   - Edge cases?\n   - Error handling?\n   - API contracts?\n\n    List all critical questions\n    Count unanswered questions\n   ```\n\n3. **Check Dependencies**\n   ```\n   What does this task depend on?\n   - Other tasks?\n   - External services?\n   - Database changes?\n   - Configuration?\n\n    Verify dependencies ready\n    List blockers\n   ```\n\n4. **Gate Decision**\n   ```\n   if (unansweredQuestions > 3)  BLOCK\n   if (missingDependencies > 0)  BLOCK\n   if (complexity >= 4 && !hasPlan)  BLOCK\n   if (complexity == 3)  WARN\n   else  PASS\n   ```\n\n5. **Document in Context**\n   ```javascript\n   context.tasks_pending.push({\n     id: 'task-' + Date.now(),\n     task: \"Task description\",\n     complexity_score: 3,\n     gate_status: 'pass',\n     critical_questions: [...],\n     dependencies: [...],\n     timestamp: new Date().toISOString()\n   });\n   ```\n\n### Workflow 2: Stuck Detection & Escalation\n\n**When:** After multiple failed attempts at same task\n\n**Steps:**\n\n1. **Track Attempts**\n   ```javascript\n   // In context, track attempts\n   if (!context.attempt_tracking) {\n     context.attempt_tracking = {};\n   }\n\n   if (!context.attempt_tracking[taskId]) {\n     context.attempt_tracking[taskId] = {\n       attempts: [],\n       first_attempt: new Date().toISOString()\n     };\n   }\n\n   context.attempt_tracking[taskId].attempts.push({\n     timestamp: new Date().toISOString(),\n     approach: \"Describe what was tried\",\n     outcome: \"Failed because X\",\n     error_message: \"Error details\"\n   });\n   ```\n\n2. **Check Threshold**\n   ```javascript\n   const attemptCount = context.attempt_tracking[taskId].attempts.length;\n\n   if (attemptCount >= 3) {\n     // ESCALATE - stuck\n     return {\n       status: 'blocked',\n       reason: 'stuck_after_3_attempts',\n       escalate_to: 'user',\n       attempts_history: context.attempt_tracking[taskId].attempts\n     };\n   }\n   ```\n\n3. **Escalation Message**\n   ```markdown\n   ##  Escalation: Task Stuck\n\n   **Task:** [Task description]\n   **Attempts:** 3\n   **Status:** BLOCKED - Need human guidance\n\n   ### What Was Tried\n   1. **Attempt 1:** [Approach]  Failed: [Reason]\n   2. **Attempt 2:** [Approach]  Failed: [Reason]\n   3. **Attempt 3:** [Approach]  Failed: [Reason]\n\n   ### Current Blocker\n   [Describe the persistent problem]\n\n   ### Need Guidance On\n   - [Specific question 1]\n   - [Specific question 2]\n\n   **Recommendation:** Human review needed to unblock\n   ```\n\n### Workflow 3: Complexity Breakdown (Level 4-5)\n\n**When:** Assigned a Level 4 or 5 complexity task\n\n**Steps:**\n\n1. **Break Down into Subtasks**\n   ```markdown\n   ## Task Breakdown: [Main Task]\n   **Overall Complexity:** Level 4\n\n   ### Subtasks\n   1. **Subtask 1:** [Description]\n      - Complexity: Level 2\n      - Dependencies: None\n      - Estimated: 2 hours\n\n   2. **Subtask 2:** [Description]\n      - Complexity: Level 3\n      - Dependencies: Subtask 1\n      - Estimated: 4 hours\n\n   3. **Subtask 3:** [Description]\n      - Complexity: Level 2\n      - Dependencies: Subtask 2\n      - Estimated: 2 hours\n\n   **Total Estimated:** 8 hours\n   **Complexity Check:** All subtasks  Level 3 \n   ```\n\n2. **Validate Breakdown**\n   ```\n   Check:\n   - [ ] All subtasks are Level 1-3\n   - [ ] Dependencies clearly mapped\n   - [ ] Each subtask has clear acceptance criteria\n   - [ ] Sum of estimates reasonable\n   - [ ] No overlapping work\n   ```\n\n3. **Create Execution Plan**\n   ```markdown\n   ## Execution Plan\n\n   **Phase 1:** Subtask 1\n   - Start: After requirements confirmed\n   - Gate check: Pass\n   - Evidence: Tests pass, build succeeds\n\n   **Phase 2:** Subtask 2\n   - Start: After Subtask 1 complete\n   - Gate check: Verify Subtask 1 evidence\n   - Evidence: Integration tests pass\n\n   **Phase 3:** Subtask 3\n   - Start: After Subtask 2 complete\n   - Gate check: End-to-end verification\n   - Evidence: Full feature tests pass\n   ```\n\n### Workflow 4: Requirements Completeness Check\n\n**When:** Starting a new feature or significant task\n\n**Steps:**\n\n1. **Functional Requirements Check**\n   ```markdown\n   ## Functional Requirements\n\n   - [ ] **Happy path defined:** What should happen when everything works?\n   - [ ] **Error cases defined:** What should happen when things fail?\n   - [ ] **Edge cases identified:** What are the boundary conditions?\n   - [ ] **Input validation:** What inputs are valid/invalid?\n   - [ ] **Output format:** What should the output look like?\n   - [ ] **Success criteria:** How do we know it works?\n   ```\n\n2. **Technical Requirements Check**\n   ```markdown\n   ## Technical Requirements\n\n   - [ ] **API contracts:** Endpoints, methods, schemas defined?\n   - [ ] **Data structures:** Models, types, interfaces specified?\n   - [ ] **Database changes:** Schema migrations needed?\n   - [ ] **Authentication:** Who can access this?\n   - [ ] **Performance:** Any latency/throughput requirements?\n   - [ ] **Security:** Any special security considerations?\n   ```\n\n3. **Count Critical Unknowns**\n   ```javascript\n   const criticalUnknowns = [\n     !functionalRequirements.happyPath,\n     !functionalRequirements.errorCases,\n     !technicalRequirements.apiContracts,\n     !technicalRequirements.dataStructures\n   ].filter(unknown => unknown).length;\n\n   if (criticalUnknowns > 3) {\n     return {\n       gate_status: 'blocked',\n       reason: 'incomplete_requirements',\n       critical_unknowns: criticalUnknowns,\n       action: 'clarify_requirements'\n     };\n   }\n   ```\n\n---\n\n## Quality Gate Templates\n\n### Template 1: Pre-Task Gate Check\n\n```markdown\n# Quality Gate: [Task Name]\n\n**Date:** [YYYY-MM-DD]\n**Agent:** [Agent name]\n\n## Complexity Assessment\n\n**Estimated Lines of Code:** [X]\n**Estimated Duration:** [X hours]\n**File Changes:** [X files]\n**Dependencies:** [X dependencies]\n**Unknowns:** [X unknowns]\n\n**Complexity Score:** Level [1-5]\n\n## Critical Questions\n\n1. [Question 1] -  Answered /  Unknown\n2. [Question 2] -  Answered /  Unknown\n3. [Question 3] -  Answered /  Unknown\n\n**Unanswered:** [Count]\n\n## Dependency Check\n\n**Required:**\n- [ ] [Dependency 1] - Ready / Blocked\n- [ ] [Dependency 2] - Ready / Blocked\n\n**Blockers:** [List]\n\n## Gate Decision\n\n**Status:**  PASS /  WARNING /  BLOCKED\n\n**Reasoning:** [Why this decision]\n\n**Actions Required:** [If blocked or warning]\n\n**Can Proceed:** Yes / No\n```\n\n### Template 2: Stuck Escalation\n\n```markdown\n# Escalation: Task Stuck\n\n**Task:** [Task description]\n**Agent:** [Agent name]\n**Date:** [YYYY-MM-DD]\n\n## Attempt History\n\n**Attempt 1** ([Timestamp])\n- **Approach:** [What was tried]\n- **Outcome:** Failed\n- **Error:** [Error message or issue]\n\n**Attempt 2** ([Timestamp])\n- **Approach:** [What was tried]\n- **Outcome:** Failed\n- **Error:** [Error message or issue]\n\n**Attempt 3** ([Timestamp])\n- **Approach:** [What was tried]\n- **Outcome:** Failed\n- **Error:** [Error message or issue]\n\n## Current Blocker\n\n[Detailed description of persistent problem]\n\n## Need Guidance\n\n1. [Specific question requiring human input]\n2. [Specific question requiring human input]\n\n## Recommendation\n\n**Escalate to:** User / Studio Coach / Specific Agent\n\n**Suggested Actions:** [What might unblock this]\n```\n\n### Template 3: Complexity Breakdown\n\n```markdown\n# Task Breakdown: [Main Task]\n\n**Original Complexity:** Level [4-5]\n**Goal:** Break down to Level 1-3 subtasks\n\n## Subtasks\n\n### Subtask 1: [Name]\n- **Complexity:** Level [X]\n- **Estimated Duration:** [X hours]\n- **Dependencies:** [None / List]\n- **Acceptance Criteria:**\n  - [ ] [Criterion 1]\n  - [ ] [Criterion 2]\n\n### Subtask 2: [Name]\n- **Complexity:** Level [X]\n- **Estimated Duration:** [X hours]\n- **Dependencies:** [List]\n- **Acceptance Criteria:**\n  - [ ] [Criterion 1]\n  - [ ] [Criterion 2]\n\n### Subtask 3: [Name]\n- **Complexity:** Level [X]\n- **Estimated Duration:** [X hours]\n- **Dependencies:** [List]\n- **Acceptance Criteria:**\n  - [ ] [Criterion 1]\n  - [ ] [Criterion 2]\n\n## Validation\n\n- [ ] All subtasks  Level 3\n- [ ] Dependencies clearly mapped\n- [ ] No circular dependencies\n- [ ] Acceptance criteria clear\n- [ ] Total estimate reasonable\n\n**Can Proceed:** Yes / No\n```\n\n---\n\n## Integration with Context System\n\nQuality gates integrate with the context system for tracking:\n\n```javascript\n// Add gate check to context\ncontext.quality_gates = context.quality_gates || [];\ncontext.quality_gates.push({\n  task_id: taskId,\n  timestamp: new Date().toISOString(),\n  complexity_score: 3,\n  gate_status: 'pass', // pass, warning, blocked\n  critical_questions_count: 1,\n  unanswered_questions: 1,\n  dependencies_blocked: 0,\n  attempt_count: 0,\n  can_proceed: true\n});\n```\n\n## Integration with Evidence System\n\nQuality gates check for evidence before allowing completion:\n\n```javascript\n// Before marking task complete\nconst evidence = context.quality_evidence;\nconst hasPassingEvidence = (\n  evidence?.tests?.exit_code === 0 ||\n  evidence?.build?.exit_code === 0\n);\n\nif (!hasPassingEvidence) {\n  return {\n    gate_status: 'blocked',\n    reason: 'no_passing_evidence',\n    action: 'collect_evidence_first'\n  };\n}\n```\n\n---\n\n## Best Practices\n\n### 1. Always Run Gate Check Before Starting\n```javascript\n//  BAD: Start immediately\nfunction startTask(task) {\n  implementTask(task);\n}\n\n//  GOOD: Gate check first\nfunction startTask(task) {\n  const gateCheck = runQualityGate(task);\n\n  if (gateCheck.status === 'blocked') {\n    escalate(gateCheck.reason);\n    return;\n  }\n\n  if (gateCheck.status === 'warning') {\n    documentAssumptions(gateCheck.warnings);\n  }\n\n  implementTask(task);\n}\n```\n\n### 2. Document All Assumptions\n```markdown\nWhen proceeding with warnings, document assumptions:\n\n## Assumptions Made\n1. **Assumption:** API will return JSON format\n   **Risk:** Low - standard REST practice\n   **Mitigation:** Add try-catch for parsing\n\n2. **Assumption:** User authentication already implemented\n   **Risk:** Medium - might not exist\n   **Mitigation:** Check early, escalate if missing\n```\n\n### 3. Track Attempts for Stuck Detection\n```javascript\n// Track every attempt\nfunction attemptTask(taskId, approach) {\n  trackAttempt(taskId, approach);\n\n  const attemptCount = getAttemptCount(taskId);\n  if (attemptCount >= 3) {\n    escalateToUser(taskId);\n    return 'blocked';\n  }\n\n  return executeApproach(approach);\n}\n```\n\n### 4. Break Down Complex Tasks Proactively\n```javascript\n//  BAD: Tackle Level 5 task directly\nimplementComplexFeature();\n\n//  GOOD: Break down first\nfunction handleComplexTask(task) {\n  if (task.complexity >= 4) {\n    const subtasks = breakDownIntoSubtasks(task);\n\n    subtasks.forEach(subtask => {\n      runQualityGate(subtask);\n      implementSubtask(subtask);\n    });\n  } else {\n    implementTask(task);\n  }\n}\n```\n\n---\n\n## Common Pitfalls\n\n###  Pitfall 1: Skipping Gate Checks for \"Simple\" Tasks\n```markdown\n**Problem:** Assume task is simple, skip gate check, get stuck later\n**Solution:** Always run gate check, even for Level 1-2 tasks (quick check)\n```\n\n###  Pitfall 2: Ignoring Warning Status\n```markdown\n**Problem:** Proceed with warnings without documenting assumptions\n**Solution:** Document every assumption when proceeding with warnings\n```\n\n###  Pitfall 3: Not Tracking Attempts\n```markdown\n**Problem:** Keep trying same approach repeatedly, waste cycles\n**Solution:** Track every attempt, escalate after 3\n```\n\n###  Pitfall 4: Proceeding When Blocked\n```markdown\n**Problem:** Gate says BLOCKED but proceed anyway \"to make progress\"\n**Solution:** NEVER bypass BLOCKED gates - resolve blockers first\n```\n\n---\n\n## Quick Reference\n\n### Complexity Quick Check\n- 1-3 files, < 200 lines, < 2 hours  **Level 1-2**\n- 3-10 files, 200-500 lines, 2-8 hours  **Level 3**\n- 10-25 files, 500-1500 lines, 8-24 hours  **Level 4**\n- 25+ files, 1500+ lines, 24+ hours  **Level 5**\n\n### Blocking Threshold Quick Check\n- >3 critical questions unanswered  **BLOCK**\n- Missing dependencies  **BLOCK**\n- 3+ failed attempts  **BLOCK & ESCALATE**\n- Level 4-5 without breakdown  **BLOCK**\n\n### Gate Decision Quick Flow\n```\n1. Assess complexity (1-5)\n2. Count critical questions unanswered\n3. Check dependencies blocked\n4. Check attempt count\n\nif (questions > 3 || dependencies blocked || attempts >= 3)  BLOCK\nelse if (complexity >= 4 && no plan)  BLOCK\nelse if (complexity == 3 || questions 1-2)  WARNING\nelse  PASS\n```\n\n---\n\n## LLM-as-Judge Quality Validation (v1.1.0)\n\nModern AI workflows benefit from automated quality assessment using LLM-as-judge patterns.\n\n### Quality Aspects to Evaluate\n\nWhen validating LLM-generated content, evaluate these dimensions:\n\n```python\nQUALITY_ASPECTS = [\n    \"relevance\",    # How relevant is the output to the input?\n    \"depth\",        # How thorough and detailed is the analysis?\n    \"coherence\",    # How well-structured and clear is the output?\n    \"accuracy\",     # Are facts and code snippets correct?\n    \"completeness\"  # Are all required sections present?\n]\n```\n\n### Quality Gate Implementation Pattern\n\n```python\nasync def quality_gate_node(state: WorkflowState) -> dict:\n    \"\"\"Validate output quality using LLM-as-judge.\"\"\"\n    THRESHOLD = 0.7  # Minimum score to pass (0.0-1.0)\n    MAX_RETRIES = 2\n\n    # Skip if no content to validate\n    if not state.get(\"output\"):\n        return {\"quality_gate_passed\": True}\n\n    # Evaluate each quality aspect\n    scores = {}\n    for aspect in QUALITY_ASPECTS:\n        try:\n            async with asyncio.timeout(30):  # Timeout protection\n                score = await evaluate_aspect(\n                    input_content=state[\"input\"],\n                    output_content=state[\"output\"],\n                    aspect=aspect\n                )\n                scores[aspect] = score\n        except TimeoutError:\n            scores[aspect] = 0.7  # Fail open with passing score\n\n    # Calculate average (guard against division by zero)\n    avg_score = sum(scores.values()) / len(scores) if scores else 0.0\n\n    # Determine gate result\n    retry_count = state.get(\"retry_count\", 0)\n    gate_passed = avg_score >= THRESHOLD or retry_count >= MAX_RETRIES\n\n    return {\n        \"quality_scores\": scores,\n        \"quality_gate_avg_score\": avg_score,\n        \"quality_gate_passed\": gate_passed,\n        \"quality_gate_retry_count\": retry_count\n    }\n```\n\n### Retry Logic\n\n```python\ndef should_retry_synthesis(state: WorkflowState) -> str:\n    \"\"\"Conditional edge function for quality gate routing.\"\"\"\n    if state.get(\"quality_gate_passed\", True):\n        return \"continue\"  # Proceed to next node\n\n    retry_count = state.get(\"quality_gate_retry_count\", 0)\n    if retry_count < MAX_RETRIES:\n        return \"retry_synthesis\"  # Re-run synthesis\n\n    return \"continue\"  # Max retries reached, fail open\n```\n\n### Fail-Open vs Fail-Closed\n\n**Fail-Open (Recommended for most cases):**\n- If quality validation fails/errors, allow workflow to continue\n- Log the failure for monitoring\n- Prevents workflow from getting stuck\n- Use when partial output is better than no output\n\n**Fail-Closed (Use for critical paths):**\n- If validation fails, block the workflow\n- Use for payment processing, security operations\n- Requires explicit error handling and user notification\n\n### Graceful Degradation Pattern\n\n```python\nasync def safe_quality_evaluation(state: dict) -> dict:\n    \"\"\"Quality gate with full graceful degradation.\"\"\"\n    try:\n        async with asyncio.timeout(60):  # Total timeout\n            return await quality_gate_node(state)\n    except TimeoutError:\n        logger.warning(\"quality_gate_timeout\", analysis_id=state[\"id\"])\n        return {\n            \"quality_gate_passed\": True,  # Fail open\n            \"quality_gate_error\": \"Evaluation timed out\"\n        }\n    except Exception as e:\n        logger.error(\"quality_gate_error\", error=str(e))\n        return {\n            \"quality_gate_passed\": True,  # Fail open\n            \"quality_gate_error\": str(e)\n        }\n```\n\n---\n\n## Triple-Consumer Artifact Design (v1.1.0)\n\nModern artifacts should serve three distinct audiences from the same content:\n\n### 1. AI Coding Assistants (Claude Code, Cursor, Copilot)\n- **Need:** Structured context, implementation steps, code snippets\n- **Format:** Pre-formatted prompts enabling accurate code generation\n- **Quality check:** Are code snippets runnable? Are steps actionable?\n\n### 2. Tutor Systems (Socratic learning)\n- **Need:** Core concepts, exercises, quiz questions, mastery checklists\n- **Format:** Pedagogical structure for progressive skill building\n- **Quality check:** Do exercises have hints and solutions? Are quiz answers valid?\n\n### 3. Human Readers (Developers, learners)\n- **Need:** TL;DR, visual diagrams, glossary, clear explanations\n- **Format:** Scannable in 10-30 seconds with deep-dive capability\n- **Quality check:** Is summary under 500 chars? Do diagrams render correctly?\n\n### Schema Validation for Multi-Consumer Output\n\n```python\nfrom pydantic import BaseModel, Field, model_validator\n\nclass QuizQuestion(BaseModel):\n    \"\"\"Quiz question with validated answer.\"\"\"\n    question: str = Field(min_length=10)\n    options: list[str] = Field(min_length=2, max_length=6)\n    correct_answer: str\n    explanation: str = Field(min_length=20)\n\n    @model_validator(mode='after')\n    def validate_correct_answer(self) -> 'QuizQuestion':\n        \"\"\"Ensure correct_answer is one of the options.\"\"\"\n        if self.correct_answer not in self.options:\n            raise ValueError(\n                f\"correct_answer '{self.correct_answer}' \"\n                f\"must be one of {self.options}\"\n            )\n        return self\n```\n\n---\n\n## Version History\n\n**v1.1.0** - Artifact Quality Initiative Update\n- Added LLM-as-judge quality validation patterns\n- Added retry logic with fail-open behavior\n- Added graceful degradation patterns\n- Added triple-consumer artifact design guidance\n- Added Pydantic v2 validation examples\n\n**v1.0.0** - Initial release\n- Complexity scoring (1-5 scale)\n- Blocking thresholds\n- Stuck detection and escalation\n- Requirements completeness checks\n- Context integration\n- Templates and workflows\n\n---\n\n**Remember:** Quality gates prevent wasted work. Better to ask questions upfront than to build the wrong solution. When in doubt, BLOCK and escalate.\n\n## Capability Details\n\n### complexity-scoring\n**Keywords:** complexity, score, difficulty, estimate, sizing, 1-5 scale\n**Solves:**\n- How complex is this task?\n- Score task complexity on 1-5 scale\n- Assess implementation difficulty\n\n### blocking-thresholds\n**Keywords:** blocking, threshold, gate, stop, escalate, cannot proceed\n**Solves:**\n- When should I block progress?\n- >3 critical questions = BLOCK\n- Missing dependencies = BLOCK\n- 3+ failed attempts = BLOCK\n\n### critical-questions\n**Keywords:** critical questions, unanswered, unknowns, clarify\n**Solves:**\n- What are critical questions?\n- Count unanswered critical questions\n- Block if >3 questions unanswered\n\n### stuck-detection\n**Keywords:** stuck, failed attempts, retry, 3 attempts, escalate\n**Solves:**\n- How do I detect when stuck?\n- After 3 failed attempts, escalate\n- Track attempt history\n\n### gate-validation\n**Keywords:** validate, gate check, pass, fail, gate status\n**Solves:**\n- How do I validate quality gates?\n- Run pre-task gate validation\n- Determine PASS/WARNING/BLOCKED status\n\n### pre-task-gate-check\n**Keywords:** pre-task, before starting, can proceed\n**Solves:**\n- How do I check gates before starting?\n- Assess complexity before proceeding\n- Identify blockers early\n\n### complexity-breakdown\n**Keywords:** breakdown, decompose, subtasks, split task\n**Solves:**\n- How do I break down complex tasks?\n- Split Level 4-5 into Level 1-3 subtasks\n- Create task breakdown with dependencies\n\n### requirements-completeness\n**Keywords:** requirements, incomplete, acceptance criteria\n**Solves:**\n- Are requirements complete enough?\n- Check functional requirements\n- Block if requirements too vague\n\n### escalation-protocol\n**Keywords:** escalate, ask user, need help, human guidance\n**Solves:**\n- When and how to escalate?\n- Escalate after 3 failed attempts\n- Create escalation message\n\n### llm-as-judge\n**Keywords:** llm as judge, g-eval, aspect scoring, quality validation\n**Solves:**\n- How do I use LLM-as-judge?\n- Evaluate relevance, depth, coherence\n- Set quality thresholds (0.0-1.0)"
              },
              {
                "name": "query-decomposition",
                "description": "Query decomposition for multi-concept retrieval. Use when handling complex queries spanning multiple topics, implementing multi-hop retrieval, or improving coverage for compound questions.",
                "path": ".claude/skills/query-decomposition/SKILL.md",
                "frontmatter": {
                  "name": "query-decomposition",
                  "description": "Query decomposition for multi-concept retrieval. Use when handling complex queries spanning multiple topics, implementing multi-hop retrieval, or improving coverage for compound questions.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# Query Decomposition\nBreak complex queries into independent concepts for parallel retrieval and fusion.\n\n## When to Use\n\n- Complex queries spanning multiple topics or concepts\n- Multi-hop questions requiring chained reasoning\n- Queries where single retrieval misses relevant documents\n- Improving recall for compound questions\n\nBreak complex queries into independent concepts for parallel retrieval and fusion.\n\n## The Problem\n\nComplex queries span multiple topics that may not co-occur in single documents:\n```\nQuery: \"How do chunking strategies affect reranking in RAG?\"\n Single search may miss docs about chunking OR reranking\n Poor coverage across all concepts\n```\n\n## The Solution\n\nDecompose into independent concepts, retrieve separately, then fuse:\n```\nQuery: \"How do chunking strategies affect reranking in RAG?\"\n Concepts: [\"chunking strategies\", \"reranking methods\", \"RAG pipeline\"]\n Search each concept independently\n Fuse results with Reciprocal Rank Fusion (RRF)\n Full coverage across all topics\n```\n\n## Implementation\n\n### 1. Heuristic Detection (Fast Path)\n\n```python\nMULTI_CONCEPT_INDICATORS = [\n    \" vs \", \" versus \", \" compared to \", \" or \",\n    \" and \", \" with \", \" affect \", \" impact \",\n    \"difference between\", \"relationship between\",\n]\n\ndef is_multi_concept_heuristic(query: str) -> bool:\n    \"\"\"Fast check for multi-concept indicators (<1ms).\"\"\"\n    query_lower = query.lower()\n    return any(ind in query_lower for ind in MULTI_CONCEPT_INDICATORS)\n```\n\n### 2. LLM Decomposition\n\n```python\nfrom pydantic import BaseModel, Field\nfrom openai import AsyncOpenAI\n\nclass ConceptExtraction(BaseModel):\n    \"\"\"LLM output schema for concept extraction.\"\"\"\n    concepts: list[str] = Field(\n        ...,\n        min_length=1,\n        max_length=5,\n        description=\"Distinct concepts from the query\",\n    )\n    reasoning: str | None = None\n\nasync def decompose_query(\n    query: str,\n    llm: AsyncOpenAI,\n) -> list[str]:\n    \"\"\"Extract independent concepts using LLM.\"\"\"\n\n    response = await llm.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"\"\"\nExtract 2-4 independent concepts from this query.\nEach concept should be searchable on its own.\nOutput JSON: {\"concepts\": [\"concept1\", \"concept2\"], \"reasoning\": \"...\"}\n\"\"\"},\n            {\"role\": \"user\", \"content\": query}\n        ],\n        response_format={\"type\": \"json_object\"},\n        temperature=0,\n    )\n\n    result = ConceptExtraction.model_validate_json(\n        response.choices[0].message.content\n    )\n    return result.concepts\n```\n\n### 3. Parallel Retrieval + RRF Fusion\n\n```python\nimport asyncio\nfrom collections import defaultdict\n\nasync def decomposed_search(\n    query: str,\n    search_fn: callable,\n    llm: AsyncOpenAI,\n    top_k: int = 10,\n) -> list[dict]:\n    \"\"\"Search with query decomposition and RRF fusion.\"\"\"\n\n    # Check if decomposition needed\n    if not is_multi_concept_heuristic(query):\n        return await search_fn(query, limit=top_k)\n\n    # Decompose into concepts\n    concepts = await decompose_query(query, llm)\n\n    if len(concepts) <= 1:\n        return await search_fn(query, limit=top_k)\n\n    # Parallel retrieval for each concept\n    tasks = [search_fn(concept, limit=top_k) for concept in concepts]\n    results_per_concept = await asyncio.gather(*tasks)\n\n    # RRF fusion\n    return reciprocal_rank_fusion(results_per_concept, k=60)\n\n\ndef reciprocal_rank_fusion(\n    result_lists: list[list[dict]],\n    k: int = 60,\n) -> list[dict]:\n    \"\"\"Combine ranked lists using RRF.\"\"\"\n    scores: defaultdict[str, float] = defaultdict(float)\n    docs: dict[str, dict] = {}\n\n    for results in result_lists:\n        for rank, doc in enumerate(results, start=1):\n            doc_id = doc[\"id\"]\n            scores[doc_id] += 1.0 / (k + rank)\n            docs[doc_id] = doc\n\n    # Sort by RRF score\n    ranked_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n    return [docs[doc_id] for doc_id in ranked_ids]\n```\n\n## Complete Service\n\n```python\nclass QueryDecomposer:\n    def __init__(self, llm, search_fn):\n        self.llm = llm\n        self.search_fn = search_fn\n        self._cache: dict[str, list[str]] = {}\n\n    async def search(\n        self,\n        query: str,\n        top_k: int = 10,\n    ) -> list[dict]:\n        \"\"\"Search with automatic decomposition.\"\"\"\n\n        # Fast path: single concept\n        if not is_multi_concept_heuristic(query):\n            return await self.search_fn(query, limit=top_k)\n\n        # Check cache\n        cache_key = query.lower().strip()\n        if cache_key in self._cache:\n            concepts = self._cache[cache_key]\n        else:\n            concepts = await decompose_query(query, self.llm)\n            self._cache[cache_key] = concepts\n\n        # Single concept after decomposition\n        if len(concepts) <= 1:\n            return await self.search_fn(query, limit=top_k)\n\n        # Parallel retrieval\n        tasks = [self.search_fn(c, limit=top_k) for c in concepts]\n        results_per_concept = await asyncio.gather(*tasks)\n\n        # Fuse with RRF\n        return reciprocal_rank_fusion(results_per_concept)[:top_k]\n```\n\n## Combining with HyDE\n\n```python\nasync def decomposed_hyde_search(\n    query: str,\n    decomposer: QueryDecomposer,\n    hyde_service: HyDEService,\n    vector_search: callable,\n    top_k: int = 10,\n) -> list[dict]:\n    \"\"\"Best of both: decomposition + HyDE for each concept.\"\"\"\n\n    # Decompose query\n    concepts = await decomposer.get_concepts(query)\n\n    # Generate HyDE for each concept in parallel\n    hyde_results = await asyncio.gather(*[\n        hyde_service.generate(concept) for concept in concepts\n    ])\n\n    # Search with HyDE embeddings\n    search_tasks = [\n        vector_search(embedding=hr.embedding, limit=top_k)\n        for hr in hyde_results\n    ]\n    results_per_concept = await asyncio.gather(*search_tasks)\n\n    # Fuse results\n    return reciprocal_rank_fusion(results_per_concept)[:top_k]\n```\n\n## When to Decompose\n\n| Query Type | Decompose? |\n|------------|------------|\n| \"What is X?\" | No |\n| \"X vs Y\" | Yes |\n| \"How does X affect Y?\" | Yes |\n| \"Best practices for X\" | No |\n| \"X and Y in Z\" | Yes |\n| \"Difference between X, Y, Z\" | Yes |\n\n## Performance Tips\n\n- Use heuristics first (sub-millisecond)\n- Cache decomposition results\n- Limit to 2-4 concepts max\n- Set timeout with fallback to original query\n- Combine with HyDE for vocabulary bridging\n\n## References\n\n- [Multi-hop Question Answering](https://arxiv.org/abs/2305.14283)\n- [Query2Doc](https://arxiv.org/abs/2303.07678)"
              },
              {
                "name": "rag-retrieval",
                "description": "Retrieval-Augmented Generation patterns for grounded LLM responses. Use when building RAG pipelines, constructing context from retrieved documents, adding citations, or implementing hybrid search.",
                "path": ".claude/skills/rag-retrieval/SKILL.md",
                "frontmatter": {
                  "name": "rag-retrieval",
                  "description": "Retrieval-Augmented Generation patterns for grounded LLM responses. Use when building RAG pipelines, constructing context from retrieved documents, adding citations, or implementing hybrid search.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# RAG Retrieval\n\nCombine vector search with LLM generation for accurate, grounded responses.\n\n## When to Use\n\n- Q&A systems over documents\n- Chatbots with knowledge bases\n- Search with natural language answers\n- Grounding LLM responses in facts\n\n## Basic RAG Pattern\n\n```python\nasync def rag_query(question: str, top_k: int = 5) -> str:\n    \"\"\"Basic RAG: retrieve then generate.\"\"\"\n    # 1. Retrieve relevant documents\n    docs = await vector_db.search(question, limit=top_k)\n\n    # 2. Construct context\n    context = \"\\n\\n\".join([\n        f\"[{i+1}] {doc.text}\"\n        for i, doc in enumerate(docs)\n    ])\n\n    # 3. Generate with context\n    response = await llm.chat([\n        {\"role\": \"system\", \"content\":\n            \"Answer using ONLY the provided context. \"\n            \"If not in context, say 'I don't have that information.'\"},\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n    ])\n\n    return response.content\n```\n\n## RAG with Citations\n\n```python\nasync def rag_with_citations(question: str) -> dict:\n    \"\"\"RAG with inline citations [1], [2], etc.\"\"\"\n    docs = await vector_db.search(question, limit=5)\n\n    context = \"\\n\\n\".join([\n        f\"[{i+1}] {doc.text}\\nSource: {doc.metadata['source']}\"\n        for i, doc in enumerate(docs)\n    ])\n\n    response = await llm.chat([\n        {\"role\": \"system\", \"content\":\n            \"Answer with inline citations like [1], [2]. \"\n            \"End with a Sources section.\"},\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n    ])\n\n    return {\n        \"answer\": response.content,\n        \"sources\": [doc.metadata['source'] for doc in docs]\n    }\n```\n\n## Hybrid Search (Semantic + Keyword)\n\n```python\ndef reciprocal_rank_fusion(\n    semantic_results: list,\n    keyword_results: list,\n    k: int = 60\n) -> list:\n    \"\"\"Combine semantic and keyword search with RRF.\"\"\"\n    scores = {}\n\n    for rank, doc in enumerate(semantic_results):\n        scores[doc.id] = scores.get(doc.id, 0) + 1 / (k + rank + 1)\n\n    for rank, doc in enumerate(keyword_results):\n        scores[doc.id] = scores.get(doc.id, 0) + 1 / (k + rank + 1)\n\n    # Sort by combined score\n    ranked_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n    return [get_doc(id) for id in ranked_ids]\n```\n\n## Context Window Management\n\n```python\ndef fit_context(docs: list, max_tokens: int = 6000) -> list:\n    \"\"\"Truncate context to fit token budget.\"\"\"\n    total_tokens = 0\n    selected = []\n\n    for doc in docs:\n        doc_tokens = count_tokens(doc.text)\n        if total_tokens + doc_tokens > max_tokens:\n            break\n        selected.append(doc)\n        total_tokens += doc_tokens\n\n    return selected\n```\n\n**Guidelines:**\n- Keep context under 75% of model limit\n- Reserve tokens for system prompt + response\n- Prioritize highest-relevance documents\n\n## Context Sufficiency Check (2026 Best Practice)\n\n```python\nfrom pydantic import BaseModel\n\nclass SufficiencyCheck(BaseModel):\n    \"\"\"Pre-generation context validation.\"\"\"\n    is_sufficient: bool\n    confidence: float  # 0.0-1.0\n    missing_info: str | None = None\n\nasync def rag_with_sufficiency(question: str, top_k: int = 5) -> str:\n    \"\"\"RAG with hallucination prevention via sufficiency check.\n\n    Based on Google Research ICLR 2025: Adding a sufficiency check\n    before generation reduces hallucinations from insufficient context.\n    \"\"\"\n    docs = await vector_db.search(question, limit=top_k)\n    context = \"\\n\\n\".join([f\"[{i+1}] {doc.text}\" for i, doc in enumerate(docs)])\n\n    # Pre-generation sufficiency check (prevents hallucination)\n    check = await llm.with_structured_output(SufficiencyCheck).ainvoke(\n        f\"\"\"Does this context contain sufficient information to answer the question?\n\nQuestion: {question}\n\nContext:\n{context}\n\nEvaluate:\n- is_sufficient: Can the question be fully answered from context?\n- confidence: How confident are you? (0.0-1.0)\n- missing_info: What's missing if not sufficient?\"\"\"\n    )\n\n    # Abstain if context insufficient (high-confidence)\n    if not check.is_sufficient and check.confidence > 0.7:\n        return f\"I don't have enough information to answer this question. Missing: {check.missing_info}\"\n\n    # Low confidence  retrieve more context\n    if not check.is_sufficient and check.confidence <= 0.7:\n        more_docs = await vector_db.search(question, limit=top_k * 2)\n        context = \"\\n\\n\".join([f\"[{i+1}] {doc.text}\" for i, doc in enumerate(more_docs)])\n\n    # Generate only with sufficient context\n    response = await llm.chat([\n        {\"role\": \"system\", \"content\":\n            \"Answer using ONLY the provided context. \"\n            \"If information is missing, say so rather than guessing.\"},\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n    ])\n\n    return response.content\n```\n\n**Why this matters (Google Research 2025):**\n- RAG paradoxically increases hallucinations when context is insufficient\n- Additional context increases model confidence  more likely to hallucinate\n- Sufficiency check allows abstention when information is missing\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Top-k | 3-10 documents |\n| Temperature | 0.1-0.3 (factual) |\n| Context budget | 4K-8K tokens |\n| Hybrid ratio | 50/50 semantic/keyword |\n\n## Common Mistakes\n\n- No citation tracking (unverifiable answers)\n- Context too large (dilutes relevance)\n- Temperature too high (hallucinations)\n- Single retrieval method (misses keyword matches)\n\n## Advanced Patterns\n\nSee `references/advanced-rag.md` for:\n- **HyDE Integration**: Hypothetical document embeddings for vocabulary mismatch\n- **Agentic RAG**: Multi-step retrieval with tool use\n- **Self-RAG**: LLM decides when to retrieve and validates outputs\n- **Corrective RAG**: Evaluate retrieval quality and correct if needed\n- **Pipeline Composition**: Combine HyDE + Hybrid + Rerank\n\n## Related Skills\n\n- `embeddings` - Creating vectors for retrieval\n- `hyde-retrieval` - Hypothetical document embeddings\n- `query-decomposition` - Multi-concept query handling\n- `reranking-patterns` - Cross-encoder and LLM reranking\n- `contextual-retrieval` - Anthropic's context-prepending technique\n- `langgraph-functional` - Building agentic RAG workflows\n\n## Capability Details\n\n### retrieval-patterns\n**Keywords:** retrieval, context, chunks, relevance\n**Solves:**\n- Retrieve relevant context for LLM\n- Implement RAG pipeline\n- Optimize retrieval quality\n\n### hybrid-search\n**Keywords:** hybrid, bm25, vector, fusion\n**Solves:**\n- Combine keyword and semantic search\n- Implement reciprocal rank fusion\n- Balance precision and recall\n\n### chatbot-example\n**Keywords:** chatbot, rag, example, typescript\n**Solves:**\n- Build RAG chatbot example\n- TypeScript implementation\n- End-to-end RAG pipeline\n\n### pipeline-template\n**Keywords:** pipeline, template, implementation, starter\n**Solves:**\n- RAG pipeline starter template\n- Production-ready code\n- Copy-paste implementation"
              },
              {
                "name": "rate-limiting",
                "description": "API rate limiting with token bucket, sliding window, and Redis distributed patterns. Use when protecting APIs from abuse, implementing tiered limits, or scaling rate limiting across instances.",
                "path": ".claude/skills/rate-limiting/SKILL.md",
                "frontmatter": {
                  "name": "rate-limiting",
                  "description": "API rate limiting with token bucket, sliding window, and Redis distributed patterns. Use when protecting APIs from abuse, implementing tiered limits, or scaling rate limiting across instances.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "tags": [
                    "rate-limiting",
                    "redis",
                    "token-bucket",
                    "fastapi",
                    "security",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# Rate Limiting Patterns\n\nProtect APIs with distributed rate limiting using Redis and modern algorithms.\n\n## When to Use\n\n- Protecting public APIs from abuse\n- Implementing tiered rate limits (free/pro/enterprise)\n- Scaling rate limiting across multiple instances\n- Preventing brute force attacks on auth endpoints\n- Managing third-party API consumption\n\n## Algorithm Selection\n\n| Algorithm | Use Case | Burst Handling |\n|-----------|----------|----------------|\n| Token Bucket | General API, allows bursts | Excellent |\n| Sliding Window | Precise, no burst spikes | Good |\n| Leaky Bucket | Steady rate, queue excess | None |\n| Fixed Window | Simple, some edge issues | Moderate |\n\n## SlowAPI + Redis (FastAPI)\n\n### Basic Setup\n\n```python\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\nfrom slowapi.middleware import SlowAPIMiddleware\n\nlimiter = Limiter(\n    key_func=get_remote_address,\n    storage_uri=\"redis://localhost:6379\",\n    strategy=\"moving-window\",  # sliding window\n)\n\napp = FastAPI()\napp.state.limiter = limiter\napp.add_middleware(SlowAPIMiddleware)\n```\n\n### Endpoint Limits\n\n```python\nfrom slowapi import Limiter\n\n@router.post(\"/api/v1/auth/login\")\n@limiter.limit(\"10/minute\")  # Strict for auth\nasync def login(request: Request, credentials: LoginRequest):\n    ...\n\n@router.get(\"/api/v1/analyses\")\n@limiter.limit(\"100/minute\")  # Normal for reads\nasync def list_analyses(request: Request):\n    ...\n\n@router.post(\"/api/v1/analyses\")\n@limiter.limit(\"20/minute\")  # Moderate for writes\nasync def create_analysis(request: Request, data: AnalysisCreate):\n    ...\n```\n\n### User-Based Limits\n\n```python\ndef get_user_identifier(request: Request) -> str:\n    \"\"\"Rate limit by user ID if authenticated, else IP.\"\"\"\n    if hasattr(request.state, \"user\"):\n        return f\"user:{request.state.user.id}\"\n    return f\"ip:{get_remote_address(request)}\"\n\nlimiter = Limiter(key_func=get_user_identifier)\n```\n\n## Token Bucket with Redis (Custom)\n\n```python\nimport redis.asyncio as redis\nfrom datetime import datetime\n\nclass TokenBucketLimiter:\n    def __init__(\n        self,\n        redis_client: redis.Redis,\n        capacity: int = 100,\n        refill_rate: float = 10.0,  # tokens per second\n    ):\n        self.redis = redis_client\n        self.capacity = capacity\n        self.refill_rate = refill_rate\n\n    async def is_allowed(self, key: str, tokens: int = 1) -> bool:\n        \"\"\"Check if request is allowed, consume tokens atomically.\"\"\"\n        lua_script = \"\"\"\n        local key = KEYS[1]\n        local capacity = tonumber(ARGV[1])\n        local refill_rate = tonumber(ARGV[2])\n        local tokens_requested = tonumber(ARGV[3])\n        local now = tonumber(ARGV[4])\n\n        local bucket = redis.call('HMGET', key, 'tokens', 'last_update')\n        local current_tokens = tonumber(bucket[1]) or capacity\n        local last_update = tonumber(bucket[2]) or now\n\n        -- Calculate refill\n        local elapsed = now - last_update\n        local refill = elapsed * refill_rate\n        current_tokens = math.min(capacity, current_tokens + refill)\n\n        -- Check and consume\n        if current_tokens >= tokens_requested then\n            current_tokens = current_tokens - tokens_requested\n            redis.call('HMSET', key, 'tokens', current_tokens, 'last_update', now)\n            redis.call('EXPIRE', key, 3600)\n            return 1\n        else\n            return 0\n        end\n        \"\"\"\n        now = datetime.utcnow().timestamp()\n        result = await self.redis.eval(\n            lua_script, 1, key,\n            self.capacity, self.refill_rate, tokens, now\n        )\n        return result == 1\n```\n\n## Sliding Window Counter\n\n```python\nclass SlidingWindowLimiter:\n    def __init__(self, redis_client: redis.Redis, window_seconds: int = 60):\n        self.redis = redis_client\n        self.window = window_seconds\n\n    async def is_allowed(self, key: str, limit: int) -> tuple[bool, int]:\n        \"\"\"Returns (allowed, remaining).\"\"\"\n        now = datetime.utcnow().timestamp()\n        window_start = now - self.window\n\n        pipe = self.redis.pipeline()\n        # Remove old entries\n        pipe.zremrangebyscore(key, 0, window_start)\n        # Count current window\n        pipe.zcard(key)\n        # Add this request\n        pipe.zadd(key, {str(now): now})\n        # Set expiry\n        pipe.expire(key, self.window * 2)\n\n        results = await pipe.execute()\n        current_count = results[1]\n\n        if current_count < limit:\n            return True, limit - current_count - 1\n        return False, 0\n```\n\n## Tiered Rate Limits\n\n```python\nfrom enum import Enum\n\nclass UserTier(Enum):\n    FREE = \"free\"\n    PRO = \"pro\"\n    ENTERPRISE = \"enterprise\"\n\nTIER_LIMITS = {\n    UserTier.FREE: {\"requests\": 100, \"window\": 3600},       # 100/hour\n    UserTier.PRO: {\"requests\": 1000, \"window\": 3600},       # 1000/hour\n    UserTier.ENTERPRISE: {\"requests\": 10000, \"window\": 3600}, # 10000/hour\n}\n\nasync def get_rate_limit(user: User) -> str:\n    limits = TIER_LIMITS[user.tier]\n    return f\"{limits['requests']}/{limits['window']}seconds\"\n\n@router.get(\"/api/v1/data\")\n@limiter.limit(get_rate_limit)\nasync def get_data(request: Request, user: User = Depends(get_current_user)):\n    ...\n```\n\n## Response Headers (RFC 6585)\n\n```python\nfrom fastapi import Response\n\nasync def add_rate_limit_headers(\n    response: Response,\n    limit: int,\n    remaining: int,\n    reset_at: datetime,\n):\n    response.headers[\"X-RateLimit-Limit\"] = str(limit)\n    response.headers[\"X-RateLimit-Remaining\"] = str(remaining)\n    response.headers[\"X-RateLimit-Reset\"] = str(int(reset_at.timestamp()))\n    response.headers[\"Retry-After\"] = str(int((reset_at - datetime.utcnow()).seconds))\n```\n\n## Error Response (429)\n\n```python\nfrom fastapi import HTTPException\nfrom fastapi.responses import JSONResponse\n\ndef rate_limit_exceeded_handler(request: Request, exc: Exception):\n    return JSONResponse(\n        status_code=429,\n        content={\n            \"type\": \"https://api.example.com/errors/rate-limit-exceeded\",\n            \"title\": \"Too Many Requests\",\n            \"status\": 429,\n            \"detail\": \"Rate limit exceeded. Please retry after the reset time.\",\n            \"instance\": str(request.url),\n        },\n        headers={\n            \"Retry-After\": \"60\",\n            \"X-RateLimit-Limit\": \"100\",\n            \"X-RateLimit-Remaining\": \"0\",\n        }\n    )\n```\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER use in-memory counters in distributed systems\nrequest_counts = {}  # Lost on restart, not shared across instances\n\n# NEVER skip rate limiting on internal APIs (defense in depth)\n@router.get(\"/internal/admin\")\nasync def admin_endpoint():  # No rate limit = vulnerable\n    ...\n\n# NEVER use fixed window without considering edge spikes\n# A user can hit 100 at 0:59 and 100 at 1:01 = 200 in 2 seconds\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Storage | Redis (distributed, atomic) |\n| Algorithm | Token bucket for most APIs |\n| Key | User ID if auth, else IP + fingerprint |\n| Auth endpoints | 10/min (strict) |\n| Read endpoints | 100-1000/min (based on tier) |\n| Write endpoints | 20-100/min (moderate) |\n\n## Related Skills\n\n- `auth-patterns` - Authentication integration\n- `resilience-patterns` - Circuit breakers\n- `observability-monitoring` - Rate limit metrics\n\n## Capability Details\n\n### token-bucket\n**Keywords:** token bucket, rate limit, burst, capacity\n**Solves:**\n- How do I implement token bucket rate limiting?\n- Allow bursts while limiting rate\n\n### sliding-window\n**Keywords:** sliding window, moving window, rate limit\n**Solves:**\n- How to implement precise rate limiting?\n- Avoid fixed window edge cases\n\n### slowapi-redis\n**Keywords:** slowapi, fastapi rate limit, redis limiter\n**Solves:**\n- How to add rate limiting to FastAPI?\n- Distributed rate limiting\n\n### tiered-limits\n**Keywords:** tiered, user tier, free pro enterprise\n**Solves:**\n- Different rate limits per subscription tier\n- User-based rate limiting"
              },
              {
                "name": "react-server-components-framework",
                "description": "Use when building Next.js 16+ apps with React Server Components. Covers App Router, streaming SSR, Server Actions, and React 19 patterns for server-first architecture.",
                "path": ".claude/skills/react-server-components-framework/SKILL.md",
                "frontmatter": {
                  "name": "react-server-components-framework",
                  "description": "Use when building Next.js 16+ apps with React Server Components. Covers App Router, streaming SSR, Server Actions, and React 19 patterns for server-first architecture.",
                  "agent": "frontend-ui-developer",
                  "version": "1.3.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "frontend",
                    "react",
                    "react-19.2",
                    "nextjs-16",
                    "server-components",
                    "streaming",
                    2026
                  ]
                },
                "content": "# React Server Components Framework\n\n## Overview\n\nReact Server Components (RSC) represent a paradigm shift in React architecture, enabling server-first rendering with client-side interactivity. This skill provides comprehensive patterns, templates, and best practices for building modern Next.js 16 applications using the App Router with Server Components, Server Actions, and streaming.\n\n**When to use this skill:**\n- Building Next.js 16+ applications with the App Router\n- Designing component boundaries (Server vs Client Components)\n- Implementing data fetching with caching and revalidation\n- Creating mutations with Server Actions\n- Optimizing performance with streaming and Suspense\n- Implementing Partial Prerendering (PPR)\n- Designing advanced routing patterns (parallel, intercepting routes)\n\n---\n\n## Why React Server Components Matter\n\nRSC fundamentally changes how we think about React applications:\n\n- **Server-First Architecture**: Components render on the server by default, reducing client bundle size\n- **Zero Client Bundle**: Server Components don't ship JavaScript to the client\n- **Direct Backend Access**: Access databases, file systems, and APIs directly from components\n- **Automatic Code Splitting**: Only Client Components and their dependencies are bundled\n- **Streaming & Suspense**: Progressive rendering for instant perceived performance\n- **Type-Safe Data Fetching**: End-to-end TypeScript from database to UI\n- **SEO & Performance**: Server rendering improves Core Web Vitals and SEO\n\n---\n\n## Core Concepts\n\n### 1. Server Components vs Client Components\n\n**Server Components** (default):\n- Can be `async` and use `await`\n- Direct database access\n- Cannot use hooks or browser APIs\n- Zero client JavaScript\n\n**Client Components** (with `'use client'`):\n- Can use hooks (`useState`, `useEffect`, etc.)\n- Browser APIs available\n- Cannot be `async`\n- Ships JavaScript to client\n\n**Key Rule**: Server Components can render Client Components, but Client Components cannot directly import Server Components (use `children` prop instead).\n\n**Detailed Patterns**: See `references/component-patterns.md` for:\n- Complete component boundary rules\n- Composition patterns\n- Props passing strategies\n- Common pitfalls and solutions\n\n### 2. Data Fetching\n\nNext.js extends the fetch API with powerful caching and revalidation:\n\n```tsx\n// Static (cached indefinitely)\nawait fetch(url, { cache: 'force-cache' })\n\n// Revalidate every 60 seconds\nawait fetch(url, { next: { revalidate: 60 } })\n\n// Always fresh\nawait fetch(url, { cache: 'no-store' })\n\n// Tag-based revalidation\nawait fetch(url, { next: { tags: ['posts'] } })\n```\n\n**Patterns:**\n- **Parallel fetching**: `Promise.all([fetch1, fetch2, fetch3])`\n- **Sequential fetching**: When data depends on previous results\n- **Route segment config**: Control static/dynamic rendering\n\n**Detailed Implementation**: See `references/data-fetching.md` for:\n- Complete caching strategies\n- Revalidation methods (`revalidatePath`, `revalidateTag`)\n- Database queries in Server Components\n- generateStaticParams for SSG\n- Error handling patterns\n\n### 3. Server Actions\n\nServer Actions enable mutations without API routes:\n\n```tsx\n// app/actions.ts\n'use server'\n\nexport async function createPost(formData: FormData) {\n  const title = formData.get('title') as string\n  const post = await db.post.create({ data: { title } })\n\n  revalidatePath('/posts')\n  redirect(\"/posts/\" + post.id)\n}\n```\n\n**Progressive Enhancement**: Forms work without JavaScript, then enhance with client-side states.\n\n**Detailed Implementation**: See `references/server-actions.md` for:\n- Progressive enhancement patterns\n- useFormStatus and useActionState hooks (React 19)\n- Optimistic UI with useOptimistic + useTransition\n- Validation with Zod\n- Inline vs exported Server Actions\n\n### 4. Streaming with Suspense\n\nStream components independently for better perceived performance:\n\n```tsx\nimport { Suspense } from 'react'\n\nexport default function Dashboard() {\n  return (\n    <div>\n      <Suspense fallback={<ChartSkeleton />}>\n        <RevenueChart />\n      </Suspense>\n\n      <Suspense fallback={<InvoicesSkeleton />}>\n        <LatestInvoices />\n      </Suspense>\n    </div>\n  )\n}\n```\n\n**Benefits**:\n- Show content as it's ready\n- Non-blocking data fetching\n- Better Core Web Vitals\n\n**Templates**: Use `templates/ServerComponent.tsx` for streaming patterns\n\n### 5. Advanced Routing\n\n**Parallel Routes**: Render multiple pages simultaneously\n```\napp/\n  @team/page.tsx\n  @analytics/page.tsx\n  layout.tsx  # Receives both as props\n```\n\n**Intercepting Routes**: Show modals while preserving URLs\n```\napp/\n  photos/[id]/page.tsx      # Direct route\n  (..)photos/[id]/page.tsx  # Intercepted (modal)\n```\n\n**Partial Prerendering (PPR)**: Mix static and dynamic content\n```tsx\nexport const experimental_ppr = true\n\n// Static shell + dynamic Suspense boundaries\n```\n\n**Detailed Implementation**: See `references/routing-patterns.md` for:\n- Parallel routes layout implementation\n- Intercepting routes for modals\n- PPR configuration and patterns\n- Route groups for organization\n- Dynamic, catch-all, and optional catch-all routes\n\n---\n\n## Searching References\n\nUse grep to find specific patterns in references:\n\n```bash\n# Find component patterns\ngrep -r \"Server Component\" references/\n\n# Search for data fetching strategies\ngrep -A 10 \"Caching Strategies\" references/data-fetching.md\n\n# Find Server Actions examples\ngrep -B 5 \"Progressive Enhancement\" references/server-actions.md\n\n# Locate routing patterns\ngrep -n \"Parallel Routes\" references/routing-patterns.md\n\n# Search migration guide\ngrep -i \"pages router\\|getServerSideProps\" references/migration-guide.md\n```\n\n---\n\n## React 19.2 Patterns (2026+)\n\nReact 19.2 introduces significant changes to component patterns. This section covers the modernization requirements including new Activity component, useEffectEvent hook, and Partial Pre-rendering.\n\n**Detailed Implementation**: See `references/react-19-patterns.md` for:\n- Complete migration guide from React 18\n- Code transformation examples\n- Testing patterns for React 19 hooks\n\n### 1. Function Declarations over React.FC\n\n**React 19 deprecates `React.FC`** because it no longer includes `children` in props by default. Always use function declarations:\n\n```tsx\n//  DEPRECATED (React 18 pattern)\nexport const Button: React.FC<ButtonProps> = ({ children, onClick }) => {\n  return <button onClick={onClick}>{children}</button>\n}\n\n//  RECOMMENDED (React 19 pattern)\nexport function Button({ children, onClick }: ButtonProps): React.ReactNode {\n  return <button onClick={onClick}>{children}</button>\n}\n\n//  ALSO VALID (arrow function without React.FC)\nexport const Button = ({ children, onClick }: ButtonProps): React.ReactNode => {\n  return <button onClick={onClick}>{children}</button>\n}\n```\n\n**Benefits**:\n- Simpler type inference\n- Explicit `children` in props when needed\n- Better tree-shaking\n- Clearer component signatures\n\n### 2. Ref as Prop (Removal of forwardRef)\n\n**React 19 removes the need for `forwardRef`**. Refs are now passed as regular props:\n\n```tsx\n//  DEPRECATED (React 18 pattern)\nimport { forwardRef } from 'react'\n\nconst Input = forwardRef<HTMLInputElement, InputProps>((props, ref) => {\n  return <input ref={ref} {...props} />\n})\n\n//  RECOMMENDED (React 19 pattern)\ninterface InputProps extends React.InputHTMLAttributes<HTMLInputElement> {\n  ref?: React.Ref<HTMLInputElement>\n}\n\nexport function Input({ ref, ...props }: InputProps): React.ReactNode {\n  return <input ref={ref} {...props} />\n}\n```\n\n**Note**: For backwards compatibility during migration, you can support both patterns temporarily.\n\n### 3. useActionState (replaces useFormState)\n\n**`useActionState`** is the new API for form state management:\n\n```tsx\n'use client'\n\nimport { useActionState } from 'react'\n\ninterface FormState {\n  message: string\n  success: boolean\n}\n\nasync function submitForm(prevState: FormState, formData: FormData): Promise<FormState> {\n  const email = formData.get('email')\n  // Process form...\n  return { message: 'Submitted!', success: true }\n}\n\nexport function ContactForm(): React.ReactNode {\n  const [state, formAction, isPending] = useActionState(submitForm, {\n    message: '',\n    success: false\n  })\n\n  return (\n    <form action={formAction}>\n      <input name=\"email\" type=\"email\" disabled={isPending} />\n      <SubmitButton />\n      {state.message && <p>{state.message}</p>}\n    </form>\n  )\n}\n```\n\n### 4. useFormStatus for Submit Buttons\n\n```tsx\n'use client'\n\nimport { useFormStatus } from 'react-dom'\n\nexport function SubmitButton(): React.ReactNode {\n  const { pending } = useFormStatus()\n\n  return (\n    <button type=\"submit\" disabled={pending} aria-busy={pending}>\n      {pending ? 'Submitting...' : 'Submit'}\n    </button>\n  )\n}\n```\n\n### 5. useOptimistic for Optimistic Updates\n\n```tsx\n'use client'\n\nimport { useOptimistic, useTransition } from 'react'\n\ninterface Item { id: string; name: string }\n\nexport function ItemList({ items }: { items: Item[] }): React.ReactNode {\n  const [optimisticItems, addOptimisticItem] = useOptimistic(\n    items,\n    (state, newItem: Item) => [...state, newItem]\n  )\n  const [, startTransition] = useTransition()\n\n  const handleAdd = async (item: Item) => {\n    startTransition(() => {\n      addOptimisticItem(item) // Immediate UI update\n    })\n    await saveItem(item) // Server mutation (auto-rollback on error)\n  }\n\n  return <ul>{optimisticItems.map(i => <li key={i.id}>{i.name}</li>)}</ul>\n}\n```\n\n### 6. Activity Component (React 19.2 - NEW)\n\nThe `Activity` component enables preloading UI that users are likely to navigate to, improving perceived performance:\n\n```tsx\n'use client'\n\nimport { Activity, useState } from 'react'\n\nexport function TabPanel({ tabs }: { tabs: Tab[] }): React.ReactNode {\n  const [activeTab, setActiveTab] = useState(tabs[0].id)\n\n  return (\n    <div>\n      <nav>\n        {tabs.map(tab => (\n          <button key={tab.id} onClick={() => setActiveTab(tab.id)}>\n            {tab.label}\n          </button>\n        ))}\n      </nav>\n\n      {tabs.map(tab => (\n        <Activity key={tab.id} mode={activeTab === tab.id ? 'visible' : 'hidden'}>\n          <TabContent tab={tab} />\n        </Activity>\n      ))}\n    </div>\n  )\n}\n```\n\n**Activity Modes:**\n- `visible`: Shows children, mounts effects, processes updates normally\n- `hidden`: Hides children, unmounts effects, defers updates until idle\n\n**Use Cases:**\n- Pre-render tabs user is likely to click\n- Preserve form state when navigating away\n- Background loading for faster perceived navigation\n\n### 7. useEffectEvent Hook (React 19.2 - NEW)\n\nResolves dependency array complexity by defining callbacks outside Effect dependency tracking:\n\n```tsx\n'use client'\n\nimport { useEffect, useEffectEvent } from 'react'\n\nexport function ChatRoom({ roomId, theme }: { roomId: string; theme: string }): React.ReactNode {\n  // This callback always reads fresh props/state but isn't a dependency\n  const onMessage = useEffectEvent((message: Message) => {\n    showNotification(message, theme) // Always uses current theme\n  })\n\n  useEffect(() => {\n    const connection = createConnection(roomId)\n    connection.on('message', onMessage)\n    return () => connection.disconnect()\n  }, [roomId]) // No need to include onMessage or theme!\n\n  return <div>Chat Room: {roomId}</div>\n}\n```\n\n**Benefits:**\n- Cleaner useEffect dependencies\n- No stale closure issues\n- Functions always access fresh props/state\n\n---\n\n## Best Practices\n\n### Component Boundary Design\n\n-  Keep Client Components at the edges (leaves) of the component tree\n-  Use Server Components by default\n-  Extract minimal interactive parts to Client Components\n-  Pass Server Components as `children` to Client Components\n-  Avoid making entire pages Client Components\n\n### Data Fetching\n\n-  Fetch data in Server Components close to where it's used\n-  Use parallel fetching for independent data\n-  Set appropriate cache and revalidate options\n-  Use `generateStaticParams` for static routes\n-  Don't fetch data in Client Components with useEffect (use Server Components)\n\n### Performance\n\n-  Use Suspense boundaries for streaming\n-  Implement loading.tsx for instant loading states\n-  Enable PPR for static/dynamic mix\n-  Optimize images with next/image\n-  Use route segment config to control rendering mode\n\n### Error Handling\n\n-  Implement error.tsx for error boundaries\n-  Use not-found.tsx for 404 pages\n-  Handle fetch errors gracefully\n-  Validate Server Action inputs\n\n---\n\n## Templates\n\nUse provided templates for common patterns:\n\n- **`templates/ServerComponent.tsx`** - Basic async Server Component with data fetching\n- **`templates/ClientComponent.tsx`** - Interactive Client Component with hooks\n- **`templates/ServerAction.tsx`** - Server Action with validation and revalidation\n\n---\n\n## Examples\n\n### Complete Blog App\n\nSee `examples/blog-app/` for a full implementation:\n- Server Components for post listing and details\n- Client Components for comments and likes\n- Server Actions for creating/editing posts\n- Streaming with Suspense\n- Parallel routes for dashboard\n\n---\n\n## Checklists\n\n### RSC Implementation Checklist\n\nSee `checklists/rsc-implementation.md` for comprehensive validation covering:\n- [ ] Component boundaries properly defined (Server vs Client)\n- [ ] Data fetching with appropriate caching strategy\n- [ ] Server Actions for mutations\n- [ ] Streaming with Suspense for slow components\n- [ ] Error handling (error.tsx, not-found.tsx)\n- [ ] Loading states (loading.tsx)\n- [ ] Metadata API for SEO\n- [ ] Route segment config optimized\n\n---\n\n## Common Patterns\n\n### Search with URL State\n\n```tsx\n// app/search/page.tsx\nexport default async function SearchPage({\n  searchParams,\n}: {\n  searchParams: { q?: string }\n}) {\n  const query = searchParams.q || ''\n  const results = query ? await searchProducts(query) : []\n\n  return (\n    <div>\n      <SearchForm initialQuery={query} />\n      <SearchResults results={results} />\n    </div>\n  )\n}\n```\n\n### Authentication\n\n```tsx\nimport { cookies } from 'next/headers'\n\nexport default async function DashboardPage() {\n  const token = cookies().get('token')?.value\n  const user = await verifyToken(token)\n\n  if (!user) {\n    redirect('/login')\n  }\n\n  return <Dashboard user={user} />\n}\n```\n\n### Optimistic UI\n\n```tsx\n'use client'\n\nimport { useOptimistic } from 'react'\n\nexport function TodoList({ todos }) {\n  const [optimisticTodos, addOptimisticTodo] = useOptimistic(\n    todos,\n    (state, newTodo) => [...state, newTodo]\n  )\n\n  return <ul>{/* render optimisticTodos */}</ul>\n}\n```\n\n---\n\n## Migration from Pages Router\n\n**Incremental Adoption**: Both `pages/` and `app/` can coexist\n\n**Key Changes**:\n- `getServerSideProps`  async Server Component\n- `getStaticProps`  async Server Component with caching\n- API routes  Server Actions\n- `_app.tsx`  `layout.tsx`\n- `<Head>`  `generateMetadata` function\n\n**Detailed Migration**: See `references/migration-guide.md` for:\n- Step-by-step migration guide\n- Before/after code examples\n- Common migration pitfalls\n- Layout and metadata migration patterns\n\n---\n\n## Troubleshooting\n\n**Error: \"You're importing a component that needs useState\"**\n- **Fix**: Add `'use client'` directive to the component\n\n**Error: \"async/await is not valid in non-async Server Components\"**\n- **Fix**: Add `async` to function declaration\n\n**Error: \"Cannot use Server Component inside Client Component\"**\n- **Fix**: Pass Server Component as `children` prop instead of importing\n\n**Error: \"Hydration mismatch\"**\n- **Fix**: Use `'use client'` for components using `Date.now()`, `Math.random()`, or browser APIs\n\n---\n\n## Resources\n\n- [Next.js 16 Documentation](https://nextjs.org/docs)\n- [React 19.2 Blog Post](https://react.dev/blog/2025/10/01/react-19-2)\n- [React Server Components RFC](https://github.com/reactjs/rfcs/blob/main/text/0188-server-components.md)\n- [App Router Migration Guide](https://nextjs.org/docs/app/building-your-application/upgrading/app-router-migration)\n- [Server Actions Documentation](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations)\n- [Next.js 16 Upgrade Guide](https://nextjs.org/docs/app/guides/upgrading/version-16)\n\n---\n\n## Next Steps\n\nAfter mastering React Server Components:\n1. Explore **Streaming API Patterns** skill for real-time data\n2. Use **Type Safety & Validation** skill for tRPC integration\n3. Apply **Edge Computing Patterns** skill for global deployment\n4. Reference **Performance Optimization** skill for Core Web Vitals\n\n## Capability Details\n\n### react-19-patterns\n**Keywords:** react 19, React.FC, forwardRef, useActionState, useFormStatus, useOptimistic, function declaration\n**Solves:**\n- How do I replace React.FC in React 19?\n- forwardRef replacement pattern\n- useActionState vs useFormState\n- React 19 component declaration best practices\n- Modernize React components for 2025\n\n### use-hook-suspense\n**Keywords:** use(), use hook, suspense, promise, data fetching, promise cache, cachePromise\n**Solves:**\n- How do I use the use() hook in React 19?\n- Suspense-native data fetching pattern\n- Promise caching to prevent infinite loops\n- When to use use() vs TanStack Query\n- use() hook infinite loop fix\n\n### optimistic-updates-async\n**Keywords:** useOptimistic, useTransition, optimistic update, instant ui, auto rollback, chat, messages\n**Solves:**\n- How to show instant UI updates before API responds?\n- useOptimistic with useTransition pattern\n- Optimistic updates for chat/messaging\n- Auto-rollback on API failure\n- Temp ID pattern for optimistic items\n\n### rsc-patterns\n**Keywords:** rsc, server component, client component, use client, use server\n**Solves:**\n- When to use server vs client components?\n- RSC boundaries and patterns\n- Server component best practices\n\n### server-actions\n**Keywords:** server action, form action, use server, mutation\n**Solves:**\n- How do I create a server action?\n- Form handling with server actions\n- Mutations in Next.js\n\n### data-fetching\n**Keywords:** fetch, data fetching, async component, loading, suspense\n**Solves:**\n- How do I fetch data in RSC?\n- Async server components\n- Suspense and loading states\n\n### streaming-ssr\n**Keywords:** streaming, ssr, suspense boundary, loading ui\n**Solves:**\n- How do I stream server content?\n- Progressive loading patterns\n- Streaming SSR setup\n\n### caching\n**Keywords:** cache, revalidate, static, dynamic, isr\n**Solves:**\n- How do I cache in Next.js 15?\n- Revalidation strategies\n- Static vs dynamic rendering\n\n### tanstack-router-patterns\n**Keywords:** tanstack router, react router, vite, spa, client rendering, prefetch, route loader, zustand, virtualization\n**Solves:**\n- How do I use React 19 features without Next.js?\n- TanStack Router prefetching setup\n- useOptimistic in client-rendered apps\n- useActionState without server actions\n- Route-based data fetching with TanStack Query\n- SSE event handling with Zustand\n- List virtualization with @tanstack/react-virtual\n- assertNever exhaustive type checking"
              },
              {
                "name": "reranking-patterns",
                "description": "Reranking patterns for improving search precision. Use when implementing cross-encoder reranking, LLM-based relevance scoring, or improving retrieval quality in RAG pipelines.",
                "path": ".claude/skills/reranking-patterns/SKILL.md",
                "frontmatter": {
                  "name": "reranking-patterns",
                  "description": "Reranking patterns for improving search precision. Use when implementing cross-encoder reranking, LLM-based relevance scoring, or improving retrieval quality in RAG pipelines.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# Reranking Patterns\nImprove search precision by re-scoring retrieved documents with more powerful models.\n\n## When to Use\n\n- Improving precision after initial retrieval\n- When bi-encoder embeddings miss semantic nuance\n- Combining multiple relevance signals\n- Production RAG systems requiring high accuracy\n\nImprove search precision by re-scoring retrieved documents with more powerful models.\n\n## Why Rerank?\n\nInitial retrieval (bi-encoder) prioritizes speed over accuracy:\n- Bi-encoder: Embeds query and docs separately  fast but approximate\n- Cross-encoder/LLM: Processes query+doc together  slow but accurate\n\n**Solution**: Retrieve many (top-50), rerank few (top-10)\n\n## Pattern 1: Cross-Encoder Reranking\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nclass CrossEncoderReranker:\n    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n        self.model = CrossEncoder(model_name)\n\n    def rerank(\n        self,\n        query: str,\n        documents: list[dict],\n        top_k: int = 10,\n    ) -> list[dict]:\n        \"\"\"Rerank documents using cross-encoder.\"\"\"\n\n        # Create query-document pairs\n        pairs = [(query, doc[\"content\"]) for doc in documents]\n\n        # Score all pairs\n        scores = self.model.predict(pairs)\n\n        # Sort by score\n        scored_docs = list(zip(documents, scores))\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n\n        # Return top-k with updated scores\n        return [\n            {**doc, \"score\": float(score)}\n            for doc, score in scored_docs[:top_k]\n        ]\n```\n\n## Pattern 2: LLM Reranking (Batch)\n\n```python\nfrom openai import AsyncOpenAI\n\nasync def llm_rerank(\n    query: str,\n    documents: list[dict],\n    llm: AsyncOpenAI,\n    top_k: int = 10,\n) -> list[dict]:\n    \"\"\"Rerank using LLM relevance scoring.\"\"\"\n\n    # Build prompt with all candidates\n    docs_text = \"\\n\\n\".join([\n        f\"[Doc {i+1}]\\n{doc['content'][:300]}...\"\n        for i, doc in enumerate(documents)\n    ])\n\n    response = await llm.chat.completions.create(\n        model=\"gpt-4o-mini\",  # Fast, cheap\n        messages=[\n            {\"role\": \"system\", \"content\": \"\"\"\nRate each document's relevance to the query (0.0-1.0).\nOutput one score per line, in order:\n0.95\n0.72\n0.45\n...\"\"\"},\n            {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nDocuments:\\n{docs_text}\"}\n        ],\n        temperature=0,\n    )\n\n    # Parse scores\n    scores = parse_scores(response.choices[0].message.content, len(documents))\n\n    # Sort and return\n    scored_docs = list(zip(documents, scores))\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n\n    return [\n        {**doc, \"score\": score}\n        for doc, score in scored_docs[:top_k]\n    ]\n\n\ndef parse_scores(response: str, expected_count: int) -> list[float]:\n    \"\"\"Parse LLM response into scores.\"\"\"\n    scores = []\n    for line in response.strip().split(\"\\n\"):\n        try:\n            score = float(line.strip())\n            scores.append(max(0.0, min(1.0, score)))\n        except ValueError:\n            scores.append(0.5)  # Default on parse error\n\n    # Pad if needed\n    while len(scores) < expected_count:\n        scores.append(0.5)\n\n    return scores[:expected_count]\n```\n\n## Pattern 3: Cohere Rerank API\n\n```python\nimport cohere\n\nclass CohereReranker:\n    def __init__(self, api_key: str):\n        self.client = cohere.Client(api_key)\n\n    def rerank(\n        self,\n        query: str,\n        documents: list[dict],\n        top_k: int = 10,\n    ) -> list[dict]:\n        \"\"\"Rerank using Cohere's rerank API.\"\"\"\n\n        results = self.client.rerank(\n            model=\"rerank-english-v3.0\",\n            query=query,\n            documents=[doc[\"content\"] for doc in documents],\n            top_n=top_k,\n        )\n\n        return [\n            {**documents[r.index], \"score\": r.relevance_score}\n            for r in results.results\n        ]\n```\n\n## Pattern 4: Combined Scoring\n\nCombine multiple signals with weighted average:\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass ReRankScore:\n    doc_id: str\n    base_score: float      # Original retrieval score\n    llm_score: float       # LLM relevance score\n    recency_score: float   # Metadata-based (e.g., freshness)\n    final_score: float\n\ndef combined_rerank(\n    documents: list[dict],\n    llm_scores: dict[str, float],\n    alpha: float = 0.3,  # Base weight\n    beta: float = 0.5,   # LLM weight\n    gamma: float = 0.2,  # Recency weight\n) -> list[dict]:\n    \"\"\"Combine multiple scoring signals.\"\"\"\n\n    scored = []\n    for doc in documents:\n        base = doc.get(\"score\", 0.5)\n        llm = llm_scores.get(doc[\"id\"], 0.5)\n        recency = calculate_recency_score(doc.get(\"created_at\"))\n\n        final = (alpha * base) + (beta * llm) + (gamma * recency)\n\n        scored.append({\n            **doc,\n            \"score\": final,\n            \"score_components\": {\n                \"base\": base,\n                \"llm\": llm,\n                \"recency\": recency,\n            }\n        })\n\n    scored.sort(key=lambda x: x[\"score\"], reverse=True)\n    return scored\n```\n\n## Complete Reranking Service\n\n```python\nclass ReRankingService:\n    def __init__(\n        self,\n        llm: AsyncOpenAI,\n        timeout_seconds: float = 5.0,\n    ):\n        self.llm = llm\n        self.timeout = timeout_seconds\n\n    async def rerank(\n        self,\n        query: str,\n        documents: list[dict],\n        top_k: int = 10,\n    ) -> list[dict]:\n        \"\"\"Rerank with timeout and fallback.\"\"\"\n        import asyncio\n\n        if len(documents) <= top_k:\n            return documents\n\n        try:\n            async with asyncio.timeout(self.timeout):\n                return await llm_rerank(\n                    query, documents, self.llm, top_k\n                )\n        except TimeoutError:\n            # Fallback: return by original score\n            return sorted(\n                documents,\n                key=lambda x: x.get(\"score\", 0),\n                reverse=True\n            )[:top_k]\n```\n\n## Model Selection Guide\n\n| Model | Latency | Cost | Quality |\n|-------|---------|------|---------|\n| `cross-encoder/ms-marco-MiniLM-L-6-v2` | ~50ms | Free | Good |\n| `BAAI/bge-reranker-large` | ~100ms | Free | Better |\n| `cohere rerank-english-v3.0` | ~200ms | $1/1K | Best |\n| `gpt-4o-mini` (LLM) | ~500ms | $0.15/1M | Great |\n\n## Best Practices\n\n1. **Retrieve more, rerank less**: Retrieve 50-100, rerank to 10\n2. **Truncate content**: 200-400 chars per doc for LLM reranking\n3. **Set timeouts**: Always fallback to base ranking\n4. **Cache scores**: Same query+doc pair = same score\n5. **Batch when possible**: One LLM call for all docs\n\n## References\n\n- [Cohere Rerank](https://docs.cohere.com/docs/rerank)\n- [Sentence Transformers Cross-Encoders](https://www.sbert.net/docs/cross_encoder/usage/usage.html)\n- [BGE Reranker](https://huggingface.co/BAAI/bge-reranker-large)"
              },
              {
                "name": "resilience-patterns",
                "description": "Use when building fault-tolerant systems with circuit breakers, bulkheads, or retry logic. Provides resilience patterns for LLM integrations, distributed workflows, and cascade failure protection.",
                "path": ".claude/skills/resilience-patterns/SKILL.md",
                "frontmatter": {
                  "name": "resilience-patterns",
                  "description": "Use when building fault-tolerant systems with circuit breakers, bulkheads, or retry logic. Provides resilience patterns for LLM integrations, distributed workflows, and cascade failure protection.",
                  "context": "fork",
                  "agent": "backend-system-architect",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "resilience",
                    "circuit-breaker",
                    "bulkhead",
                    "retry",
                    "fault-tolerance"
                  ]
                },
                "content": "# Resilience Patterns Skill\n\nProduction-grade resilience patterns for distributed systems and LLM-based workflows. Covers circuit breakers, bulkheads, retry strategies, and LLM-specific resilience techniques.\n\n## When to Use This Skill\n\n- Building fault-tolerant multi-agent systems\n- Implementing LLM API integrations with proper error handling\n- Designing distributed workflows that need graceful degradation\n- Adding observability to failure scenarios\n- Protecting systems from cascade failures\n\n## Core Patterns\n\n### 1. Circuit Breaker Pattern (reference: circuit-breaker.md)\n\nPrevents cascade failures by \"tripping\" when a service exceeds failure thresholds.\n\n```\n\n                    Circuit Breaker States                        \n\n                                                                  \n         failures >= threshold           \n      CLOSED        OPEN          \n     (normal)                                (reject)        \n                                         \n                                                                \n          success                    timeout                    \n                                     expires                    \n                                                 \n                   HALF_OPEN                \n           (probe)                                   \n                                                   \n                                                                  \n   CLOSED:    Allow requests, count failures                     \n   OPEN:      Reject immediately, return fallback                \n   HALF_OPEN: Allow probe request to test recovery               \n                                                                  \n\n```\n\n**Key Configuration:**\n- `failure_threshold`: Failures before opening (default: 5)\n- `recovery_timeout`: Seconds before attempting recovery (default: 30)\n- `half_open_requests`: Probes to allow in half-open (default: 1)\n\n### 2. Bulkhead Pattern (reference: bulkhead-pattern.md)\n\nIsolates failures by partitioning resources into independent pools.\n\n```\n\n                      Bulkhead Isolation                          \n\n                                                                  \n                         \n    TIER 1: Critical    TIER 2: Standard                     \n     (5 workers)         (3 workers)                         \n                                          \n                                          \n                                          \n                                                       \n                   Queue: 2                            \n                                                       \n     Queue: 0                              \n                                             \n                                                                  \n                                             \n    TIER 3: Optional     = Active request                     \n     (2 workers)         = Available slot                     \n                                                         \n       FULL!      Tier 1: synthesis, quality_gate        \n                  Tier 2: analysis agents                \n     Queue: 5           Tier 3: enrichment, optional features  \n                                             \n                                                                  \n\n```\n\n**Tier Configuration (SkillForge):**\n| Tier | Workers | Queue | Timeout | Use Case |\n|------|---------|-------|---------|----------|\n| 1 (Critical) | 5 | 10 | 300s | Synthesis, quality gate |\n| 2 (Standard) | 3 | 5 | 120s | Content analysis agents |\n| 3 (Optional) | 2 | 3 | 60s | Enrichment, caching |\n\n### 3. Retry Strategies (reference: retry-strategies.md)\n\nIntelligent retry logic with exponential backoff and jitter.\n\n```\n\n                   Exponential Backoff + Jitter                   \n\n                                                                  \n   Attempt 1:   X (fail)                                      \n               wait: 1s  0.5s                                   \n                                                                  \n   Attempt 2:   X (fail)                                      \n               wait: 2s  1s                                     \n                                                                  \n   Attempt 3:   X (fail)                                      \n               wait: 4s  2s                                     \n                                                                  \n   Attempt 4:    (success)                                   \n                                                                  \n   Formula: delay = min(base * 2^attempt, max_delay) * jitter    \n   Jitter:  random(0.5, 1.5) to prevent thundering herd          \n                                                                  \n\n```\n\n**Error Classification for Retries:**\n```python\nRETRYABLE_ERRORS = {\n    # HTTP/Network\n    408, 429, 500, 502, 503, 504,  # HTTP status codes\n    ConnectionError, TimeoutError,  # Network errors\n\n    # LLM-specific\n    \"rate_limit_exceeded\",\n    \"model_overloaded\",\n    \"context_length_exceeded\",  # Retry with truncation\n}\n\nNON_RETRYABLE_ERRORS = {\n    400, 401, 403, 404,  # Client errors\n    \"invalid_api_key\",\n    \"content_policy_violation\",\n    \"invalid_request_error\",\n}\n```\n\n### 4. LLM-Specific Resilience (reference: llm-resilience.md)\n\nPatterns specific to LLM API integrations.\n\n```\n\n                    LLM Fallback Chain                            \n\n                                                                  \n   Request  [Primary Model] success Response             \n                                                                 \n                   fail                                           \n                                                                 \n               [Fallback Model] success Response            \n                                                                 \n                   fail                                           \n                                                                 \n               [Cached Response] hit Response               \n                                                                 \n                   miss                                           \n                                                                 \n               [Default Response]  Graceful Degradation       \n                                                                  \n   Example Chain:                                                 \n   1. claude-sonnet-4-20250514 (primary)                  \n   2. gpt-4o-mini (fallback)                                     \n   3. Semantic cache lookup                                      \n   4. \"Analysis unavailable\" + partial results                   \n                                                                  \n\n```\n\n**Token Budget Management:**\n```\n\n                     Token Budget Guard                           \n\n                                                                  \n   Input: 8,000 tokens                                           \n              \n              \n              \n                                                                 \n                                                                 \n                                    Context Limit (16K)           \n                                                                  \n   Strategy when approaching limit:                              \n   1. Summarize earlier context (compress 4:1)                   \n   2. Drop low-priority content (optional fields)                \n   3. Split into multiple requests                               \n   4. Fail fast with \"content too large\" error                   \n                                                                  \n\n```\n\n## Quick Reference\n\n| Pattern | When to Use | Key Benefit |\n|---------|-------------|-------------|\n| Circuit Breaker | External service calls | Prevent cascade failures |\n| Bulkhead | Multi-tenant/multi-agent | Isolate failures |\n| Retry + Backoff | Transient failures | Automatic recovery |\n| Fallback Chain | Critical operations | Graceful degradation |\n| Token Budget | LLM calls | Cost control, prevent failures |\n\n## SkillForge Integration Points\n\n1. **Workflow Agents**: Each agent wrapped with circuit breaker + bulkhead tier\n2. **LLM Calls**: All model invocations use fallback chain + retry logic\n3. **External APIs**: Circuit breaker on YouTube, arXiv, GitHub APIs\n4. **Database Ops**: Bulkhead isolation for read vs write operations\n\n## Files in This Skill\n\n### References (Conceptual Guides)\n- `references/circuit-breaker.md` - Deep dive on circuit breaker pattern\n- `references/bulkhead-pattern.md` - Bulkhead isolation strategies\n- `references/retry-strategies.md` - Retry algorithms and error classification\n- `references/llm-resilience.md` - LLM-specific patterns\n- `references/error-classification.md` - How to categorize errors\n\n### Templates (Code Patterns)\n- `templates/circuit-breaker.py` - Ready-to-use circuit breaker class\n- `templates/bulkhead.py` - Semaphore-based bulkhead implementation\n- `templates/retry-handler.py` - Configurable retry decorator\n- `templates/llm-fallback-chain.py` - Multi-model fallback pattern\n- `templates/token-budget.py` - Token budget guard implementation\n\n### Examples\n- `examples/skillforge-workflow-resilience.md` - Full SkillForge integration example\n\n### Checklists\n- `checklists/pre-deployment-resilience.md` - Production readiness checklist\n- `checklists/circuit-breaker-setup.md` - Circuit breaker configuration guide\n\n## 2025 Best Practices\n\n1. **Adaptive Thresholds**: Use sliding windows, not fixed counters\n2. **Observability First**: Every circuit trip = alert + metric + trace\n3. **Graceful Degradation**: Always have a fallback, even if partial\n4. **Health Endpoints**: Separate health check from circuit state\n5. **Chaos Testing**: Regularly test failure scenarios in staging\n\n## Capability Details\n\n### circuit-breaker\n**Keywords:** circuit breaker, failure threshold, cascade failure, trip, half-open\n**Solves:**\n- Prevent cascade failures when external services fail\n- Automatically recover when services come back online\n- Fail fast instead of waiting for timeouts\n\n### bulkhead\n**Keywords:** bulkhead, isolation, semaphore, thread pool, resource pool, tier\n**Solves:**\n- Isolate failures to prevent entire system crashes\n- Prioritize critical operations over optional ones\n- Limit concurrent requests to protect resources\n\n### retry-strategies\n**Keywords:** retry, backoff, exponential, jitter, thundering herd\n**Solves:**\n- Handle transient failures automatically\n- Avoid overwhelming recovering services\n- Classify errors as retryable vs non-retryable\n\n### llm-resilience\n**Keywords:** LLM, fallback, model, token budget, rate limit, context length\n**Solves:**\n- Handle LLM API rate limits gracefully\n- Fall back to alternative models when primary fails\n- Manage token budgets to prevent context overflow\n\n### error-classification\n**Keywords:** error, retryable, transient, permanent, classification\n**Solves:**\n- Determine which errors should be retried\n- Categorize errors by severity and recoverability\n- Map HTTP status codes to resilience actions"
              },
              {
                "name": "review-pr",
                "description": "Comprehensive PR review with 6-7 parallel specialized agents",
                "path": ".claude/skills/review-pr/SKILL.md",
                "frontmatter": {
                  "name": "review-pr",
                  "description": "Comprehensive PR review with 6-7 parallel specialized agents",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "code-review",
                    "pull-request",
                    "quality",
                    "security",
                    "testing"
                  ]
                },
                "content": "# Review PR\n\nDeep code review using 6-7 parallel specialized agents.\n\n## When to Use\n\n- Reviewing pull requests\n- Code quality assessment\n- Security auditing PRs\n- Test coverage validation\n\n## Quick Start\n\n```bash\n/review-pr 123\n/review-pr feature-branch\n```\n\n## Phase 1: Gather PR Information\n\n```bash\n# Get PR details\ngh pr view $ARGUMENTS --json title,body,files,additions,deletions,commits,author\n\n# View the diff\ngh pr diff $ARGUMENTS\n\n# Check CI status\ngh pr checks $ARGUMENTS\n```\n\nIdentify:\n- Total files changed\n- Lines added/removed\n- Affected domains (frontend, backend, AI)\n\n## Phase 2: Load Review Skills\n\n```python\n# PARALLEL - Load capabilities\nRead(\".claude/skills/code-review-playbook/capabilities.json\")\nRead(\".claude/skills/security-checklist/capabilities.json\")\nRead(\".claude/skills/testing-strategy-builder/capabilities.json\")\nRead(\".claude/skills/type-safety-validation/capabilities.json\")\n```\n\n## Phase 3: Parallel Code Review (6 Agents)\n\nLaunch SIX specialized reviewers in ONE message:\n\n| Agent | Focus Area |\n|-------|-----------|\n| code-quality-reviewer #1 | Readability, complexity, DRY |\n| code-quality-reviewer #2 | Type safety, Zod, Pydantic |\n| code-quality-reviewer #3 | Security, secrets, injection |\n| code-quality-reviewer #4 | Test coverage, edge cases |\n| backend-system-architect | API, async, transactions |\n| frontend-ui-developer | React 19, hooks, a11y |\n\n### Optional: AI Code Review\n\nIf PR includes AI/ML code, add 7th agent for:\n- Prompt engineering quality\n- LangGraph workflow correctness\n- Token usage optimization\n\n## Phase 4: Run Validation\n\n```bash\n# Backend\ncd backend\npoetry run ruff format --check app/\npoetry run ruff check app/\npoetry run ty check app/\npoetry run pytest tests/unit/ -v --tb=short\n\n# Frontend\ncd frontend\nnpm run format:check\nnpm run lint\nnpm run typecheck\nnpm run test\n```\n\n## Phase 5: Synthesize Review\n\nCombine all agent feedback into structured report:\n\n```markdown\n# PR Review: #$ARGUMENTS\n\n## Summary\n[1-2 sentence overview]\n\n## Code Quality\n| Area | Status | Notes |\n|------|--------|-------|\n| Readability | // | [notes] |\n| Type Safety | // | [notes] |\n| Test Coverage | // | [X%] |\n\n## Security\n| Check | Status |\n|-------|--------|\n| Secrets | / |\n| Input Validation | / |\n| Dependencies | / |\n\n## Blockers (Must Fix)\n- [if any]\n\n## Suggestions (Non-Blocking)\n- [improvements]\n```\n\n## Phase 6: Submit Review\n\n```bash\n# Approve\ngh pr review $ARGUMENTS --approve -b \"Review message\"\n\n# Request changes\ngh pr review $ARGUMENTS --request-changes -b \"Review message\"\n```\n\n## Conventional Comments\n\nUse these prefixes for comments:\n- `praise:` - Positive feedback\n- `nitpick:` - Minor suggestion\n- `suggestion:` - Improvement idea\n- `issue:` - Must fix\n- `question:` - Needs clarification\n\n## References\n\n- [Review Template](references/review-template.md)"
              },
              {
                "name": "run-tests",
                "description": "Comprehensive test execution with parallel analysis and coverage reporting",
                "path": ".claude/skills/run-tests/SKILL.md",
                "frontmatter": {
                  "name": "run-tests",
                  "description": "Comprehensive test execution with parallel analysis and coverage reporting",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "testing",
                    "pytest",
                    "coverage",
                    "test-execution"
                  ]
                },
                "content": "# Run Tests\n\nTest execution with parallel analysis agents for failures.\n\n## When to Use\n\n- Running test suites\n- Checking test coverage\n- Analyzing test failures\n- Verifying code changes\n\n## Quick Start\n\n```bash\n/run-tests\n/run-tests backend\n/run-tests frontend\n/run-tests tests/unit/test_auth.py\n```\n\n## Test Scope\n\n| Argument | Scope |\n|----------|-------|\n| Empty/`all` | All tests |\n| `backend` | Backend only |\n| `frontend` | Frontend only |\n| `path/to/test.py` | Specific file |\n| `test_name` | Specific test |\n\n## Phase 1: Execute Tests\n\n```bash\n# Backend with coverage\ncd backend\npoetry run pytest tests/unit/ -v --tb=short \\\n  --cov=app --cov-report=term-missing\n\n# Frontend with coverage\ncd frontend\nnpm run test -- --coverage\n```\n\n## Phase 2: Failure Analysis\n\nIf tests fail, launch 3 parallel analyzers:\n1. **Backend Failure Analysis** - Root cause, fix suggestions\n2. **Frontend Failure Analysis** - Component issues, mock problems\n3. **Coverage Gap Analysis** - Low coverage areas\n\n## Phase 3: Generate Report\n\n```markdown\n# Test Results Report\n\n## Summary\n| Suite | Total | Passed | Failed | Coverage |\n|-------|-------|--------|--------|----------|\n| Backend | X | Y | Z | XX% |\n| Frontend | X | Y | Z | XX% |\n\n## Status: [ALL PASS | SOME FAILURES]\n\n## Failures (if any)\n| Test | Error | Fix |\n|------|-------|-----|\n| test_name | AssertionError | [suggestion] |\n```\n\n## Quick Commands\n\n```bash\n# All backend tests\npoetry run pytest tests/unit/ -v --tb=short\n\n# With coverage\npoetry run pytest tests/unit/ --cov=app\n\n# Quick (no tracebacks)\npoetry run pytest tests/unit/ --tb=no -q\n\n# Specific test\npoetry run pytest tests/unit/ -k \"test_name\" -v\n\n# Frontend\nnpm run test -- --coverage\n\n# Watch mode\nnpm run test -- --watch\n```\n\n## Key Options\n\n| Option | Purpose |\n|--------|---------|\n| `--maxfail=3` | Stop after 3 failures |\n| `-x` | Stop on first failure |\n| `--lf` | Run only last failed |\n| `-v` | Verbose output |\n| `--tb=short` | Shorter tracebacks |\n\n## References\n\n- [Test Commands](references/test-commands.md)"
              },
              {
                "name": "security-scanning",
                "description": "Automated security scanning for dependencies and code. Use when running npm audit, pip-audit, Semgrep, secret detection, or integrating security checks into CI/CD.",
                "path": ".claude/skills/security-scanning/SKILL.md",
                "frontmatter": {
                  "name": "security-scanning",
                  "description": "Automated security scanning for dependencies and code. Use when running npm audit, pip-audit, Semgrep, secret detection, or integrating security checks into CI/CD.",
                  "context": "fork",
                  "agent": "security-auditor",
                  "allowed-tools": [
                    "Read",
                    "Grep",
                    "Glob",
                    "Bash"
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Bash",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/redact-secrets.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/security-summary.sh"
                      }
                    ]
                  }
                },
                "content": "# Security Scanning\n\nAutomate vulnerability detection in code and dependencies.\n\n## When to Use\n\n- Before code review completion\n- After dependency updates\n- In CI/CD pipelines\n- Before production deployments\n\n## Dependency Scanning\n\n### JavaScript (npm)\n\n```bash\n# Run audit\nnpm audit --json > security-audit.json\n\n# Check severity counts\nCRITICAL=$(npm audit --json | jq '.metadata.vulnerabilities.critical')\nHIGH=$(npm audit --json | jq '.metadata.vulnerabilities.high')\n\nif [ \"$CRITICAL\" -gt 0 ] || [ \"$HIGH\" -gt 0 ]; then\n  echo \" $CRITICAL critical, $HIGH high vulnerabilities\"\nfi\n\n# Auto-fix\nnpm audit fix\n```\n\n### Python (pip-audit)\n\n```bash\npip-audit --format=json > security-audit.json\n\n# Using safety\nsafety check --json > security-audit.json\n```\n\n## Static Analysis (SAST)\n\n### Semgrep\n\n```bash\n# Run with security rules\nsemgrep --config=auto --json > semgrep-results.json\n\n# Count findings\nCRITICAL=$(cat semgrep-results.json | jq '[.results[] | select(.extra.severity == \"ERROR\")] | length')\n```\n\n### Bandit (Python)\n\n```bash\nbandit -r . -f json -o bandit-report.json\n\nHIGH=$(cat bandit-report.json | jq '[.results[] | select(.issue_severity == \"HIGH\")] | length')\n```\n\n## Secret Detection\n\n```bash\n# TruffleHog\ntrufflehog git file://. --json > secrets-scan.json\n\n# Gitleaks\ngitleaks detect --source . --report-format json\n\n# Check results\nSECRET_COUNT=$(cat secrets-scan.json | jq '. | length')\nif [ \"$SECRET_COUNT\" -gt 0 ]; then\n  echo \" $SECRET_COUNT secrets detected!\"\nfi\n```\n\n## Container Scanning\n\n```bash\n# Trivy\ntrivy image myapp:latest --format json > trivy-scan.json\n\nCRITICAL=$(cat trivy-scan.json | jq '[.Results[].Vulnerabilities[]? | select(.Severity == \"CRITICAL\")] | length')\n```\n\n## Pre-commit Hooks (2026 Best Practice)\n\nShift-left security by catching issues before commit:\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  # Secret detection - MUST HAVE\n  - repo: https://github.com/gitleaks/gitleaks\n    rev: v8.18.0\n    hooks:\n      - id: gitleaks\n\n  # Python security\n  - repo: https://github.com/PyCQA/bandit\n    rev: 1.7.7\n    hooks:\n      - id: bandit\n        args: [\"-c\", \"pyproject.toml\", \"-r\", \".\"]\n        exclude: ^tests/\n\n  # Semgrep for SAST\n  - repo: https://github.com/semgrep/semgrep\n    rev: v1.52.0\n    hooks:\n      - id: semgrep\n        args: [\"--config\", \"auto\", \"--error\"]\n\n  # Detect AWS credentials, private keys\n  - repo: https://github.com/Yelp/detect-secrets\n    rev: v1.4.0\n    hooks:\n      - id: detect-secrets\n        args: [\"--baseline\", \".secrets.baseline\"]\n```\n\n```bash\n# Install and setup\npip install pre-commit\npre-commit install\n\n# Run on all files (first time)\npre-commit run --all-files\n\n# Update hooks to latest versions\npre-commit autoupdate\n```\n\n**Baseline for detect-secrets (ignore false positives):**\n```bash\n# Generate baseline\ndetect-secrets scan > .secrets.baseline\n\n# Audit false positives\ndetect-secrets audit .secrets.baseline\n```\n\n## CI Integration\n\n```yaml\n# GitHub Actions\n- name: Security scan\n  run: |\n    npm audit --json > audit.json\n    CRITICAL=$(jq '.metadata.vulnerabilities.critical' audit.json)\n    if [ \"$CRITICAL\" -gt 0 ]; then\n      echo \"::error::Critical vulnerabilities found\"\n      exit 1\n    fi\n```\n\n## Escalation Thresholds\n\n| Severity | Threshold | Action |\n|----------|-----------|--------|\n| Critical | Any | BLOCK |\n| High | > 5 | BLOCK |\n| Moderate | > 20 | WARNING |\n| Low | > 50 | WARNING |\n\n## Evidence Recording\n\n```typescript\ncontext.quality_evidence.security_scan = {\n  executed: true,\n  tool: 'npm audit',\n  critical: 2,\n  high: 5,\n  moderate: 10,\n  timestamp: new Date().toISOString()\n};\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| JS dependencies | npm audit |\n| Python dependencies | pip-audit |\n| Code analysis | Semgrep |\n| Secrets | TruffleHog or Gitleaks |\n| Pre-commit | gitleaks + detect-secrets |\n| Shift-left | Always use pre-commit hooks |\n\n## Common Mistakes\n\n- Ignoring audit warnings\n- No CI integration\n- Not blocking on critical\n- Missing secret scanning\n\n## Related Skills\n\n- `owasp-top-10` - Vulnerability context\n- `devops-deployment` - CI/CD integration\n- `code-review-playbook` - Review process\n\n## Capability Details\n\n### dependency-scanning\n**Keywords:** npm audit, pip-audit, dependency, vulnerability\n**Solves:**\n- Scan npm dependencies\n- Audit Python packages\n- Find vulnerable dependencies\n\n### secret-detection\n**Keywords:** secret, credential, api key, trufflehog, gitleaks\n**Solves:**\n- Detect secrets in code\n- Scan for API keys\n- Find exposed credentials\n\n### api-security-audit\n**Keywords:** api, audit, security, example\n**Solves:**\n- API security audit example\n- Security review checklist\n- Real audit walkthrough\n\n### audit-template\n**Keywords:** template, audit, report, security\n**Solves:**\n- Security audit template\n- Audit report structure\n- Copy-paste audit format"
              },
              {
                "name": "semantic-caching",
                "description": "Redis semantic caching for LLM applications. Use when implementing vector similarity caching, optimizing LLM costs through cached responses, or building multi-level cache hierarchies.",
                "path": ".claude/skills/semantic-caching/SKILL.md",
                "frontmatter": {
                  "name": "semantic-caching",
                  "description": "Redis semantic caching for LLM applications. Use when implementing vector similarity caching, optimizing LLM costs through cached responses, or building multi-level cache hierarchies.",
                  "context": "fork",
                  "agent": "data-pipeline-engineer"
                },
                "content": "# Semantic Caching\n\nCache LLM responses by semantic similarity.\n\n## When to Use\n\n- Repeated similar queries\n- High-volume LLM applications\n- Cost reduction (70-95% savings)\n- Latency optimization\n\n## Cache Hierarchy\n\n```\nRequest  L1 (Exact)  L2 (Semantic)  L3 (Prompt)  L4 (LLM)\n           ~1ms         ~10ms           ~2s          ~3s\n         100% save    100% save       90% save    Full cost\n```\n\n## Redis Semantic Cache\n\n```python\nfrom redisvl.index import SearchIndex\nfrom redisvl.query import VectorQuery\n\nclass SemanticCacheService:\n    def __init__(self, redis_url: str, threshold: float = 0.92):\n        self.client = Redis.from_url(redis_url)\n        self.threshold = threshold\n\n    async def get(self, content: str, agent_type: str) -> dict | None:\n        embedding = await embed_text(content[:2000])\n\n        query = VectorQuery(\n            vector=embedding,\n            vector_field_name=\"embedding\",\n            filter_expression=f\"@agent_type:{{{agent_type}}}\",\n            num_results=1\n        )\n\n        results = self.index.query(query)\n\n        if results:\n            distance = float(results[0].get(\"vector_distance\", 1.0))\n            if distance <= (1 - self.threshold):\n                return json.loads(results[0][\"response\"])\n\n        return None\n\n    async def set(self, content: str, response: dict, agent_type: str):\n        embedding = await embed_text(content[:2000])\n        key = f\"cache:{agent_type}:{hash_content(content)}\"\n\n        self.client.hset(key, mapping={\n            \"agent_type\": agent_type,\n            \"embedding\": embedding,\n            \"response\": json.dumps(response),\n            \"created_at\": time.time(),\n        })\n        self.client.expire(key, 86400)  # 24h TTL\n```\n\n## Similarity Thresholds\n\n| Threshold | Distance | Use Case |\n|-----------|----------|----------|\n| 0.98-1.00 | 0.00-0.02 | Nearly identical |\n| 0.95-0.98 | 0.02-0.05 | Very similar |\n| 0.92-0.95 | 0.05-0.08 | Similar (default) |\n| 0.85-0.92 | 0.08-0.15 | Moderately similar |\n\n## Multi-Level Lookup\n\n```python\nasync def get_llm_response(query: str, agent_type: str) -> dict:\n    # L1: Exact match (in-memory LRU)\n    cache_key = hash_content(query)\n    if cache_key in lru_cache:\n        return lru_cache[cache_key]\n\n    # L2: Semantic similarity (Redis)\n    similar = await semantic_cache.get(query, agent_type)\n    if similar:\n        lru_cache[cache_key] = similar  # Promote to L1\n        return similar\n\n    # L3/L4: LLM call with prompt caching\n    response = await llm.generate(query)\n\n    # Store in caches\n    await semantic_cache.set(query, response, agent_type)\n    lru_cache[cache_key] = response\n\n    return response\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Threshold | Start at 0.92, tune based on hit rate |\n| TTL | 24h for production |\n| Embedding | text-embedding-3-small (fast) |\n| L1 size | 1000-10000 entries |\n\n## Common Mistakes\n\n- Threshold too low (false positives)\n- No cache warming (cold start)\n- Missing metadata filters\n- Not promoting L2 hits to L1\n\n## Related Skills\n\n- `prompt-caching` - Provider-native caching\n- `embeddings` - Vector generation\n- `cache-cost-tracking` - Langfuse integration\n\n## Capability Details\n\n### redis-vector-cache\n**Keywords:** redis, vector, embedding, similarity, cache\n**Solves:**\n- Cache LLM responses by semantic similarity\n- Reduce API costs with smart caching\n- Implement multi-level cache hierarchy\n\n### similarity-threshold\n**Keywords:** threshold, similarity, tuning, cosine\n**Solves:**\n- Set appropriate similarity threshold\n- Balance hit rate vs accuracy\n- Tune cache performance\n\n### skillforge-integration\n**Keywords:** skillforge, integration, roi, cost-savings\n**Solves:**\n- Integrate caching with SkillForge\n- Calculate ROI for caching\n- Production implementation guide\n\n### cache-service\n**Keywords:** service, implementation, template, production\n**Solves:**\n- Production cache service template\n- Complete implementation example\n- Redis integration code"
              },
              {
                "name": "streaming-api-patterns",
                "description": "Use when building real-time features with SSE, WebSockets, or streaming APIs. Covers backpressure handling, reconnection strategies, and LLM token streaming patterns.",
                "path": ".claude/skills/streaming-api-patterns/SKILL.md",
                "frontmatter": {
                  "name": "streaming-api-patterns",
                  "description": "Use when building real-time features with SSE, WebSockets, or streaming APIs. Covers backpressure handling, reconnection strategies, and LLM token streaming patterns.",
                  "agent": "frontend-ui-developer",
                  "version": "1.0.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "streaming",
                    "sse",
                    "websocket",
                    "real-time",
                    "api",
                    2025
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ]
                  }
                },
                "content": "# Streaming API Patterns\n\n## Overview\n\nModern applications require real-time data delivery. This skill covers Server-Sent Events (SSE) for server-to-client streaming, WebSockets for bidirectional communication, and the Streams API for handling backpressure and efficient data flow.\n\n**When to use this skill:**\n- Streaming LLM responses (ChatGPT-style interfaces)\n- Real-time notifications and updates\n- Live data feeds (stock prices, analytics)\n- Chat applications\n- Progress updates for long-running tasks\n- Collaborative editing features\n\n## Core Technologies\n\n### 1. Server-Sent Events (SSE)\n\n**Best for**: Server-to-client streaming (LLM responses, notifications)\n\n```typescript\n// Next.js Route Handler\nexport async function GET(req: Request) {\n  const encoder = new TextEncoder()\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      // Send data\n      controller.enqueue(encoder.encode('data: Hello\\n\\n'))\n\n      // Keep connection alive\n      const interval = setInterval(() => {\n        controller.enqueue(encoder.encode(': keepalive\\n\\n'))\n      }, 30000)\n\n      // Cleanup\n      req.signal.addEventListener('abort', () => {\n        clearInterval(interval)\n        controller.close()\n      })\n    }\n  })\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive',\n    }\n  })\n}\n\n// Client\nconst eventSource = new EventSource('/api/stream')\neventSource.onmessage = (event) => {\n  console.log(event.data)\n}\n```\n\n### 2. WebSockets\n\n**Best for**: Bidirectional real-time communication (chat, collaboration)\n\n```typescript\n// WebSocket Server (Next.js with ws)\nimport { WebSocketServer } from 'ws'\n\nconst wss = new WebSocketServer({ port: 8080 })\n\nwss.on('connection', (ws) => {\n  ws.on('message', (data) => {\n    // Broadcast to all clients\n    wss.clients.forEach((client) => {\n      if (client.readyState === WebSocket.OPEN) {\n        client.send(data)\n      }\n    })\n  })\n})\n\n// Client\nconst ws = new WebSocket('ws://localhost:8080')\nws.onmessage = (event) => console.log(event.data)\nws.send(JSON.stringify({ type: 'message', text: 'Hello' }))\n```\n\n### 3. ReadableStream API\n\n**Best for**: Processing large data streams with backpressure\n\n```typescript\nasync function* generateData() {\n  for (let i = 0; i < 1000; i++) {\n    await new Promise(resolve => setTimeout(resolve, 100))\n    yield \"data-\" + i\n  }\n}\n\nconst stream = new ReadableStream({\n  async start(controller) {\n    for await (const chunk of generateData()) {\n      controller.enqueue(new TextEncoder().encode(chunk + '\\n'))\n    }\n    controller.close()\n  }\n})\n```\n\n## LLM Streaming Pattern\n\n```typescript\n// Server\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI()\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json()\n\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-4-turbo-preview',\n    messages,\n    stream: true\n  })\n\n  const encoder = new TextEncoder()\n\n  return new Response(\n    new ReadableStream({\n      async start(controller) {\n        for await (const chunk of stream) {\n          const content = chunk.choices[0]?.delta?.content\n          if (content) {\n            controller.enqueue(encoder.encode(\"data: \" + JSON.stringify({ content }) + \"\\n\\n\"))\n          }\n        }\n        controller.enqueue(encoder.encode('data: [DONE]\\n\\n'))\n        controller.close()\n      }\n    }),\n    {\n      headers: {\n        'Content-Type': 'text/event-stream',\n        'Cache-Control': 'no-cache'\n      }\n    }\n  )\n}\n\n// Client\nasync function streamChat(messages) {\n  const response = await fetch('/api/chat', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ messages })\n  })\n\n  const reader = response.body.getReader()\n  const decoder = new TextDecoder()\n\n  while (true) {\n    const { done, value } = await reader.read()\n    if (done) break\n\n    const chunk = decoder.decode(value)\n    const lines = chunk.split('\\n')\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6)\n        if (data === '[DONE]') return\n\n        const json = JSON.parse(data)\n        console.log(json.content) // Stream token\n      }\n    }\n  }\n}\n```\n\n## Reconnection Strategy\n\n```typescript\nclass ReconnectingEventSource {\n  private eventSource: EventSource | null = null\n  private reconnectDelay = 1000\n  private maxReconnectDelay = 30000\n\n  constructor(private url: string, private onMessage: (data: string) => void) {\n    this.connect()\n  }\n\n  private connect() {\n    this.eventSource = new EventSource(this.url)\n\n    this.eventSource.onmessage = (event) => {\n      this.reconnectDelay = 1000 // Reset on success\n      this.onMessage(event.data)\n    }\n\n    this.eventSource.onerror = () => {\n      this.eventSource?.close()\n\n      // Exponential backoff\n      setTimeout(() => this.connect(), this.reconnectDelay)\n      this.reconnectDelay = Math.min(this.reconnectDelay * 2, this.maxReconnectDelay)\n    }\n  }\n\n  close() {\n    this.eventSource?.close()\n  }\n}\n```\n\n## Python Async Generator Cleanup (2025 Best Practice)\n\n**CRITICAL**: Async generators can leak resources if not properly cleaned up. Python 3.10+ provides `aclosing()` from `contextlib` to guarantee cleanup.\n\n### The Problem\n\n```python\n#  DANGEROUS: Generator not closed if exception occurs mid-iteration\nasync def stream_analysis():\n    async for chunk in external_api_stream():  # What if exception here?\n        yield process(chunk)  # Generator may be garbage collected without cleanup\n\n#  ALSO DANGEROUS: Using .aclose() manually is error-prone\ngen = stream_analysis()\ntry:\n    async for chunk in gen:\n        process(chunk)\nfinally:\n    await gen.aclose()  # Easy to forget, verbose\n```\n\n### The Solution: `aclosing()`\n\n```python\nfrom contextlib import aclosing\n\n#  CORRECT: aclosing() guarantees cleanup\nasync def stream_analysis():\n    async with aclosing(external_api_stream()) as stream:\n        async for chunk in stream:\n            yield process(chunk)\n\n#  CORRECT: Using aclosing() at consumption site\nasync def consume_stream():\n    async with aclosing(stream_analysis()) as gen:\n        async for chunk in gen:\n            handle(chunk)\n```\n\n### Real-World Pattern: LLM Streaming\n\n```python\nfrom contextlib import aclosing\nfrom langchain_core.runnables import RunnableConfig\n\nasync def stream_llm_response(prompt: str, config: RunnableConfig | None = None):\n    \"\"\"Stream LLM tokens with guaranteed cleanup.\"\"\"\n    async with aclosing(llm.astream(prompt, config=config)) as stream:\n        async for chunk in stream:\n            yield chunk.content\n\n# Consumption with proper cleanup\nasync def generate_response(user_input: str):\n    result_chunks = []\n    async with aclosing(stream_llm_response(user_input)) as response:\n        async for token in response:\n            result_chunks.append(token)\n            yield token  # Stream to client\n\n    # Post-processing after stream completes\n    full_response = \"\".join(result_chunks)\n    await log_response(full_response)\n```\n\n### Database Connection Pattern\n\n```python\nfrom contextlib import aclosing\nfrom typing import AsyncIterator\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nasync def stream_large_query(\n    session: AsyncSession,\n    batch_size: int = 1000\n) -> AsyncIterator[Row]:\n    \"\"\"Stream large query results with automatic connection cleanup.\"\"\"\n    result = await session.execute(\n        select(Model).execution_options(stream_results=True)\n    )\n\n    async with aclosing(result.scalars()) as stream:\n        async for row in stream:\n            yield row\n```\n\n### When to Use `aclosing()`\n\n| Scenario | Use `aclosing()` |\n|----------|------------------|\n| External API streaming (LLM, HTTP) |  **Always** |\n| Database streaming results |  **Always** |\n| File streaming |  **Always** |\n| Simple in-memory generators |  Optional (no cleanup needed) |\n| Generator with `try/finally` cleanup |  **Always** |\n\n### Anti-Patterns to Avoid\n\n```python\n#  NEVER: Consuming without aclosing\nasync for chunk in stream_analysis():\n    process(chunk)\n\n#  NEVER: Manual try/finally (verbose, error-prone)\ngen = stream_analysis()\ntry:\n    async for chunk in gen:\n        process(chunk)\nfinally:\n    await gen.aclose()\n\n#  NEVER: Assuming GC will handle cleanup\ngen = stream_analysis()\n# ... later gen goes out of scope without close\n```\n\n### Testing Async Generators\n\n```python\nimport pytest\nfrom contextlib import aclosing\n\n@pytest.mark.asyncio\nasync def test_stream_cleanup_on_error():\n    \"\"\"Test that cleanup happens even when exception raised.\"\"\"\n    cleanup_called = False\n\n    async def stream_with_cleanup():\n        nonlocal cleanup_called\n        try:\n            yield \"data\"\n            yield \"more\"\n        finally:\n            cleanup_called = True\n\n    with pytest.raises(ValueError):\n        async with aclosing(stream_with_cleanup()) as gen:\n            async for chunk in gen:\n                raise ValueError(\"simulated error\")\n\n    assert cleanup_called, \"Cleanup must run even on exception\"\n```\n\n## Best Practices\n\n### SSE\n-  Use for one-way server-to-client streaming\n-  Implement automatic reconnection\n-  Send keepalive messages every 30s\n-  Handle browser connection limits (6 per domain)\n-  Use HTTP/2 for better performance\n\n### WebSockets\n-  Use for bidirectional real-time communication\n-  Implement heartbeat/ping-pong\n-  Handle reconnection with exponential backoff\n-  Validate and sanitize messages\n-  Implement message queuing for offline periods\n\n### Backpressure\n-  Use ReadableStream with proper flow control\n-  Monitor buffer sizes\n-  Pause production when consumer is slow\n-  Implement timeouts for slow consumers\n\n### Performance\n-  Compress data (gzip/brotli)\n-  Batch small messages\n-  Use binary formats (MessagePack, Protobuf) for large data\n-  Implement client-side buffering\n-  Monitor connection count and resource usage\n\n## Resources\n\n- [Server-Sent Events Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)\n- [WebSocket Protocol](https://datatracker.ietf.org/doc/html/rfc6455)\n- [Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API)\n- [Vercel AI SDK](https://sdk.vercel.ai/docs)\n\n## Capability Details\n\n### sse\n**Keywords:** sse, server-sent events, event stream, one-way stream\n**Solves:**\n- How do I implement SSE?\n- Stream data from server to client\n- Real-time notifications\n\n### sse-protocol\n**Keywords:** sse protocol, event format, event types, sse headers\n**Solves:**\n- SSE protocol fundamentals\n- Event format and types\n- SSE HTTP headers\n\n### sse-buffering\n**Keywords:** event buffering, sse race condition, late subscriber, buffer events\n**Solves:**\n- How do I buffer SSE events?\n- Fix SSE race condition\n- Handle late-joining subscribers\n\n### sse-reconnection\n**Keywords:** sse reconnection, reconnect, last-event-id, retry, exponential backoff\n**Solves:**\n- How do I handle SSE reconnection?\n- Implement automatic reconnection\n- Resume from Last-Event-ID\n\n### skillforge-sse\n**Keywords:** skillforge sse, event broadcaster, workflow events, analysis progress\n**Solves:**\n- How does SkillForge SSE work?\n- EventBroadcaster implementation\n- Real-world SSE example\n\n### websocket\n**Keywords:** websocket, ws, bidirectional, real-time chat, socket\n**Solves:**\n- How do I set up WebSocket server?\n- Build a chat application\n- Bidirectional real-time communication\n\n### llm-streaming\n**Keywords:** llm stream, chatgpt stream, ai stream, token stream, openai stream\n**Solves:**\n- How do I stream LLM responses?\n- ChatGPT-style streaming interface\n- Stream tokens as they arrive\n\n### backpressure\n**Keywords:** backpressure, flow control, buffer, readable stream, transform stream\n**Solves:**\n- Handle slow consumers\n- Implement backpressure\n- Stream large files efficiently\n\n### reconnection\n**Keywords:** reconnect, connection lost, retry, resilient, heartbeat\n**Solves:**\n- Handle connection drops\n- Implement automatic reconnection\n- Keep-alive and heartbeat"
              },
              {
                "name": "system-design-interrogation",
                "description": "Use when planning system architecture to ensure nothing is missed. Provides structured questions covering scalability, security, data, and operational dimensions before implementation.",
                "path": ".claude/skills/system-design-interrogation/SKILL.md",
                "frontmatter": {
                  "name": "system-design-interrogation",
                  "description": "Use when planning system architecture to ensure nothing is missed. Provides structured questions covering scalability, security, data, and operational dimensions before implementation.",
                  "context": "fork",
                  "agent": "system-design-reviewer",
                  "version": "1.0.0"
                },
                "content": "# System Design Interrogation\n\n## Overview\n\nStructured questioning framework to ensure nothing is missed when planning system architecture, covering scalability, security, data, and operational dimensions.\n\n## When to Use\n\n- Before starting complex system implementations\n- Planning new features with architectural implications\n- Reviewing existing designs for gaps\n- Preparing for technical design reviews\n\n## The Problem\n\nRushing to implementation without systematic design thinking leads to:\n- Scalability issues discovered too late\n- Security holes from missing tenant isolation\n- Data model mismatches\n- Frontend/backend contract conflicts\n- Poor user experience\n\n## The Solution: Question Before Implementing\n\n```\n\n                    SYSTEM DESIGN INTERROGATION                             \n\n                                                                            \n                                                             \n                           FEATURE                                        \n                           REQUEST                                        \n                                                             \n                                                                           \n                    \n                                                                        \n                                                                        \n                                            \n   SCALE                 DATA                SECURITY               \n                                            \n                                                                        \n   Users?                Where?                Who access?              \n   Volume?               Pattern?              Isolation?               \n   Growth?               Search?               Attacks?                 \n                                                                        \n                           \n                                                                          \n                       \n                                                                       \n                                                                       \n                                         \n     UX              COHERENCE              TRADE-                 \n                           OFFS                  \n                                                             \n   Latency?            Contracts?            Speed?                    \n   Feedback?           Types?                Quality?                  \n   Errors?             API?                  Cost?                     \n                                                                       \n                            \n                                                                          \n                                                                          \n                                                         \n                      IMPLEMENTATION                                    \n                         READY                                          \n                                                         \n                                                                            \n\n```\n\n## The Five Dimensions\n\n### 1. Scale\n\n**Key Questions:**\n- How many users/tenants will use this?\n- What's the expected data volume (now and in 1 year)?\n- What's the request rate? Read-heavy or write-heavy?\n- Does complexity grow linearly or exponentially with data?\n- What happens at 10x current load? 100x?\n\n**SkillForge Example:**\n```\nFeature: \"Add document tagging\"\n- Users: 1000 active users\n- Documents per user: ~50 average\n- Tags per document: 3-5\n- Total tags: 50,000  500,000\n- Access: Read-heavy (10:1 read:write)\n- Search: Need tag autocomplete (prefix search)\n```\n\n### 2. Data\n\n**Key Questions:**\n- Where does this data naturally belong?\n- What's the primary access pattern?\n- Is it master data or transactional?\n- What's the retention policy?\n- Does it need to be searchable? How?\n\n**SkillForge Example:**\n```\nFeature: \"Add document tagging\"\n- Data: Tags belong WITH documents (denormalized) or separate table?\n- Pattern: Get tags for document (by doc_id), get documents by tag\n- Storage: PostgreSQL (relational) or add to document JSON?\n- Search: Full-text for tag names, filter by tag for documents\n- Decision: Separate `tags` table with many-to-many join\n```\n\n### 3. Security\n\n**Key Questions:**\n- Who can access this data/feature?\n- How is tenant isolation enforced?\n- What happens if authorization fails?\n- What attack vectors does this introduce?\n- Is there PII involved?\n\n**SkillForge Example:**\n```\nFeature: \"Add document tagging\"\n- Access: User can only see/manage their own tags\n- Isolation: All tag queries MUST include tenant_id filter\n- AuthZ: Check user owns document before tagging\n- Attacks: Tag injection? Limit tag length, sanitize input\n- PII: Tags might contain PII  treat as sensitive\n```\n\n### 4. UX Impact\n\n**Key Questions:**\n- What's the expected latency for this operation?\n- What feedback does the user get during the operation?\n- What happens on failure? Can they retry?\n- Is there optimistic UI possible?\n- How does this affect the overall workflow?\n\n**SkillForge Example:**\n```\nFeature: \"Add document tagging\"\n- Latency: < 100ms for add/remove tag\n- Feedback: Optimistic update, show tag immediately\n- Failure: Rollback tag, show error toast\n- Optimistic: Yes - add tag to UI before server confirms\n- Workflow: Tags should be inline editable, no modal\n```\n\n### 5. Coherence\n\n**Key Questions:**\n- Which layers does this touch?\n- What contracts/interfaces change?\n- Are types consistent frontend  backend?\n- Does this break existing clients?\n- How does this affect the API?\n\n**SkillForge Example:**\n```\nFeature: \"Add document tagging\"\n- Layers: DB  Backend API  Frontend UI  State\n- Contracts: Document type needs `tags: Tag[]` field\n- Types: Tag = { id: UUID, name: string, color?: string }\n- Breaking: No - additive change to Document response\n- API: POST /documents/{id}/tags, DELETE /documents/{id}/tags/{tag_id}\n```\n\n## The Process\n\n### Before Writing Any Code\n\n1. **State the Feature** - One sentence description\n2. **Run Through 5 Dimensions** - Answer key questions for each\n3. **Identify Trade-offs** - Speed vs quality, complexity vs flexibility\n4. **Document Decisions** - Record answers in design doc or issue\n5. **Review with Team** - Get alignment before implementing\n\n### Quick Assessment Template\n\n```markdown\n## Feature: [Name]\n\n### Scale\n- Users:\n- Data volume:\n- Access pattern:\n- Growth projection:\n\n### Data\n- Storage location:\n- Schema changes:\n- Search requirements:\n- Retention:\n\n### Security\n- Authorization:\n- Tenant isolation:\n- Attack surface:\n- PII handling:\n\n### UX\n- Target latency:\n- Feedback mechanism:\n- Error handling:\n- Optimistic updates:\n\n### Coherence\n- Affected layers:\n- Type changes:\n- API changes:\n- Breaking changes:\n\n### Decision\n[Final approach with rationale]\n```\n\n## Integration with SkillForge Workflow\n\n### In Brainstorming Phase\n\nBefore implementation, run system design interrogation:\n\n```\n/brainstorm  System Design Questions  Implementation Plan\n```\n\n### In Code Review\n\nReviewer should verify:\n- Scale considerations documented\n- Security layer covered\n- Types consistent across stack\n- UX states handled\n\n### In Testing\n\nTests should cover:\n- Scale: Load tests for expected volume\n- Security: Tenant isolation tests\n- Coherence: Integration tests across layers\n- UX: Error state tests\n\n## Anti-Patterns\n\n```\n \"I'll add an index later if it's slow\"\n    Ask: What's the expected query pattern NOW?\n\n \"We can add tenant filtering in a future PR\"\n    Ask: How is isolation enforced from DAY ONE?\n\n \"The frontend can handle any response shape\"\n    Ask: What's the TypeScript type for this?\n\n \"Users won't do that\"\n    Ask: What's the attack vector? What if they DO?\n\n \"It's just a small feature\"\n    Ask: How does this grow with 100x users?\n```\n\n## Quick Reference Card\n\n| Dimension | Key Question | Red Flag |\n|-----------|--------------|----------|\n| Scale | How many? | \"All users\" |\n| Data | Where stored? | \"I'll figure it out\" |\n| Security | Who can access? | \"Everyone\" |\n| UX | What's the latency? | \"It'll be fast\" |\n| Coherence | What types change? | \"No changes needed\" |\n\n---\n\n**Version:** 1.0.0 (December 2025)\n## Capability Details\n\n### scale-assessment\n**Keywords:** scale, load, traffic, users, concurrent, throughput\n**Solves:**\n- How many users will this feature serve?\n- What's the expected request rate?\n- How does this scale with data growth?\n\n### data-architecture\n**Keywords:** data, storage, database, schema, migration, structure\n**Solves:**\n- Where should this data live?\n- What's the access pattern?\n- How does this affect existing schemas?\n\n### security-considerations\n**Keywords:** security, auth, permission, tenant, isolation, attack\n**Solves:**\n- What are the security implications?\n- How is tenant isolation maintained?\n- What attack vectors exist?\n\n### coherence-validation\n**Keywords:** coherence, consistency, contract, interface, integration\n**Solves:**\n- How does this fit the existing architecture?\n- What contracts need updating?\n- Are frontend/backend aligned?\n\n### ux-impact\n**Keywords:** ux, user experience, latency, feedback, error\n**Solves:**\n- What's the user experience impact?\n- How long will users wait?\n- What feedback do they get?"
              },
              {
                "name": "test-data-management",
                "description": "Test data management with fixtures and factories. Use when creating test data strategies, implementing data factories, managing fixtures, or seeding test databases.",
                "path": ".claude/skills/test-data-management/SKILL.md",
                "frontmatter": {
                  "name": "test-data-management",
                  "description": "Test data management with fixtures and factories. Use when creating test data strategies, implementing data factories, managing fixtures, or seeding test databases.",
                  "context": "fork",
                  "agent": "test-generator",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Test Data Management\n\nCreate and manage test data effectively.\n\n## When to Use\n\n- Test data setup\n- Database seeding\n- Realistic test scenarios\n- Data isolation\n\n## Factory Pattern (Python)\n\n```python\nfrom factory import Factory, Faker, SubFactory, LazyAttribute\nfrom app.models import User, Analysis\n\nclass UserFactory(Factory):\n    class Meta:\n        model = User\n\n    email = Faker('email')\n    name = Faker('name')\n    created_at = Faker('date_time_this_year')\n\nclass AnalysisFactory(Factory):\n    class Meta:\n        model = Analysis\n\n    url = Faker('url')\n    status = 'pending'\n    user = SubFactory(UserFactory)\n\n    @LazyAttribute\n    def title(self):\n        return f\"Analysis of {self.url}\"\n\n# Usage\nuser = UserFactory()\nanalysis = AnalysisFactory(user=user, status='completed')\n```\n\n## Factory Pattern (TypeScript)\n\n```typescript\nimport { faker } from '@faker-js/faker';\n\ninterface User {\n  id: string;\n  email: string;\n  name: string;\n}\n\nconst createUser = (overrides: Partial<User> = {}): User => ({\n  id: faker.string.uuid(),\n  email: faker.internet.email(),\n  name: faker.person.fullName(),\n  ...overrides,\n});\n\nconst createAnalysis = (overrides = {}) => ({\n  id: faker.string.uuid(),\n  url: faker.internet.url(),\n  status: 'pending',\n  userId: createUser().id,\n  ...overrides,\n});\n\n// Usage\nconst user = createUser({ name: 'Test User' });\nconst analysis = createAnalysis({ userId: user.id, status: 'completed' });\n```\n\n## JSON Fixtures\n\n```json\n// fixtures/users.json\n{\n  \"admin\": {\n    \"id\": \"user-001\",\n    \"email\": \"admin@example.com\",\n    \"role\": \"admin\"\n  },\n  \"basic\": {\n    \"id\": \"user-002\",\n    \"email\": \"user@example.com\",\n    \"role\": \"user\"\n  }\n}\n```\n\n```python\nimport json\nimport pytest\n\n@pytest.fixture\ndef users():\n    with open('fixtures/users.json') as f:\n        return json.load(f)\n\ndef test_admin_access(users):\n    admin = users['admin']\n    assert admin['role'] == 'admin'\n```\n\n## Database Seeding\n\n```python\n# seeds/test_data.py\nasync def seed_test_database(db: AsyncSession):\n    \"\"\"Seed database with test data.\"\"\"\n    # Create users\n    users = [\n        UserFactory.build(email=f\"user{i}@test.com\")\n        for i in range(10)\n    ]\n    db.add_all(users)\n\n    # Create analyses for each user\n    for user in users:\n        analyses = [\n            AnalysisFactory.build(user_id=user.id)\n            for _ in range(5)\n        ]\n        db.add_all(analyses)\n\n    await db.commit()\n\n@pytest.fixture\nasync def seeded_db(db_session):\n    await seed_test_database(db_session)\n    yield db_session\n```\n\n## Fixture Composition\n\n```python\n@pytest.fixture\ndef user():\n    return UserFactory()\n\n@pytest.fixture\ndef user_with_analyses(user):\n    analyses = [AnalysisFactory(user=user) for _ in range(3)]\n    return {\"user\": user, \"analyses\": analyses}\n\n@pytest.fixture\ndef completed_workflow(user_with_analyses):\n    for analysis in user_with_analyses[\"analyses\"]:\n        analysis.status = \"completed\"\n    return user_with_analyses\n```\n\n## Test Data Isolation\n\n```python\n@pytest.fixture(autouse=True)\nasync def clean_database(db_session):\n    \"\"\"Reset database between tests.\"\"\"\n    yield\n\n    # Clean up after test\n    await db_session.execute(\"TRUNCATE users, analyses CASCADE\")\n    await db_session.commit()\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Strategy | Factories over fixtures |\n| Faker | Use for realistic random data |\n| Scope | Function-scoped for isolation |\n| Cleanup | Always reset between tests |\n\n## Common Mistakes\n\n- Shared state between tests\n- Hard-coded IDs (conflicts)\n- No cleanup after tests\n- Over-complex fixtures\n\n## Related Skills\n\n- `unit-testing` - Test patterns\n- `integration-testing` - Database tests\n- `database-schema-designer` - Schema design\n\n## Capability Details\n\n### fixture-generation\n**Keywords:** fixture, test fixture, pytest fixture, conftest\n**Solves:**\n- Create reusable test fixtures\n- Implement fixture composition\n- Handle fixture cleanup\n\n### factory-patterns\n**Keywords:** factory, FactoryBoy, test factory, model factory\n**Solves:**\n- Generate test data with factories\n- Implement factory inheritance\n- Create related object graphs\n\n### data-seeding\n**Keywords:** seed, seed data, database seed, initial data\n**Solves:**\n- Seed databases for testing\n- Create consistent test environments\n- Implement idempotent seeding\n\n### cleanup-strategies\n**Keywords:** cleanup, teardown, reset, isolation\n**Solves:**\n- Clean up test data after runs\n- Implement transaction rollback\n- Ensure test isolation\n\n### data-anonymization\n**Keywords:** anonymize, faker, synthetic data, mock data\n**Solves:**\n- Generate realistic fake data\n- Anonymize production data for tests\n- Create synthetic datasets"
              },
              {
                "name": "test-standards-enforcer",
                "description": "Enforce testing best practices - AAA pattern, naming conventions, isolation, coverage thresholds. Blocks non-compliant tests. Use when writing or reviewing tests.",
                "path": ".claude/skills/test-standards-enforcer/SKILL.md",
                "frontmatter": {
                  "name": "test-standards-enforcer",
                  "description": "Enforce testing best practices - AAA pattern, naming conventions, isolation, coverage thresholds. Blocks non-compliant tests. Use when writing or reviewing tests.",
                  "context": "fork",
                  "agent": "test-generator",
                  "version": "1.0.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "testing",
                    "quality",
                    "enforcement",
                    "blocking",
                    "aaa-pattern",
                    "coverage"
                  ],
                  "hooks": {
                    "PreToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-location-validator.sh"
                      }
                    ],
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-pattern-validator.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-threshold-gate.sh"
                      }
                    ]
                  }
                },
                "content": "# Test Standards Enforcer\n\nEnforce 2026 testing best practices with **BLOCKING** validation.\n\n## When to Use\n\n- Writing new test files\n- Reviewing test code\n- Ensuring test coverage\n- Validating test patterns\n\n## Validation Rules\n\n### BLOCKING Rules (exit 1)\n\n| Rule | Check | Example Violation |\n|------|-------|-------------------|\n| **Test Location** | Tests must be in `tests/` or `__tests__/` | `src/utils/helper.test.ts` |\n| **AAA Pattern** | Tests must have Arrange/Act/Assert structure | No clear sections |\n| **Descriptive Names** | Test names must describe behavior | `test('test1')` |\n| **No Shared State** | Tests must not share mutable state | `let globalVar = []` without reset |\n| **Coverage Threshold** | Coverage must be  80% | 75% coverage |\n\n### File Location Rules\n\n```\nALLOWED:\n  tests/unit/user.test.ts\n  tests/integration/api.test.ts\n  __tests__/components/Button.test.tsx\n  app/tests/test_users.py\n  tests/conftest.py\n\nBLOCKED:\n  src/utils/helper.test.ts      # Tests in src/\n  components/Button.test.tsx    # Tests outside test dir\n  app/routers/test_routes.py    # Tests mixed with source\n```\n\n### Naming Conventions\n\n**TypeScript/JavaScript:**\n```typescript\n// GOOD - Descriptive, behavior-focused\ntest('should return empty array when no items exist', () => {})\ntest('throws ValidationError when email is invalid', () => {})\nit('renders loading spinner while fetching', () => {})\n\n// BLOCKED - Too short, not descriptive\ntest('test1', () => {})\ntest('works', () => {})\nit('test', () => {})\n```\n\n**Python:**\n```python\n# GOOD - snake_case, descriptive\ndef test_should_return_user_when_id_exists():\ndef test_raises_not_found_when_user_missing():\n\n# BLOCKED - Not descriptive, wrong case\ndef testUser():      # camelCase\ndef test_1():        # Not descriptive\n```\n\n## AAA Pattern (Required)\n\nEvery test must follow Arrange-Act-Assert:\n\n### TypeScript Example\n\n```typescript\ndescribe('calculateDiscount', () => {\n  test('should apply 10% discount for orders over $100', () => {\n    // Arrange\n    const order = createOrder({ total: 150 });\n    const calculator = new DiscountCalculator();\n\n    // Act\n    const discount = calculator.calculate(order);\n\n    // Assert\n    expect(discount).toBe(15);\n  });\n});\n```\n\n### Python Example\n\n```python\nclass TestCalculateDiscount:\n    def test_applies_10_percent_discount_over_threshold(self):\n        # Arrange\n        order = Order(total=150)\n        calculator = DiscountCalculator()\n\n        # Act\n        discount = calculator.calculate(order)\n\n        # Assert\n        assert discount == 15\n```\n\n## Test Isolation (Required)\n\nTests must not share mutable state:\n\n```typescript\n// BLOCKED - Shared mutable state\nlet items = [];\n\ntest('adds item', () => {\n  items.push('a');\n  expect(items).toHaveLength(1);\n});\n\ntest('removes item', () => {\n  // FAILS - items already has 'a' from previous test\n  expect(items).toHaveLength(0);\n});\n\n// GOOD - Reset state in beforeEach\ndescribe('ItemList', () => {\n  let items: string[];\n\n  beforeEach(() => {\n    items = []; // Fresh state each test\n  });\n\n  test('adds item', () => {\n    items.push('a');\n    expect(items).toHaveLength(1);\n  });\n\n  test('starts empty', () => {\n    expect(items).toHaveLength(0); // Works!\n  });\n});\n```\n\n## Coverage Requirements\n\n| Area | Minimum | Target |\n|------|---------|--------|\n| Overall | 80% | 90% |\n| Business Logic | 90% | 100% |\n| Critical Paths | 95% | 100% |\n| New Code | 100% | 100% |\n\n### Running Coverage\n\n**TypeScript (Vitest/Jest):**\n```bash\nnpm test -- --coverage\nnpx vitest --coverage\n```\n\n**Python (pytest):**\n```bash\npytest --cov=app --cov-report=json\n```\n\n## Parameterized Tests\n\nUse parameterized tests for multiple similar cases:\n\n### TypeScript\n\n```typescript\ndescribe('isValidEmail', () => {\n  test.each([\n    ['user@example.com', true],\n    ['invalid', false],\n    ['@missing.com', false],\n    ['user@domain.co.uk', true],\n    ['user+tag@example.com', true],\n  ])('isValidEmail(%s) returns %s', (email, expected) => {\n    expect(isValidEmail(email)).toBe(expected);\n  });\n});\n```\n\n### Python\n\n```python\nimport pytest\n\nclass TestIsValidEmail:\n    @pytest.mark.parametrize(\"email,expected\", [\n        (\"user@example.com\", True),\n        (\"invalid\", False),\n        (\"@missing.com\", False),\n        (\"user@domain.co.uk\", True),\n    ])\n    def test_email_validation(self, email: str, expected: bool):\n        assert is_valid_email(email) == expected\n```\n\n## Fixture Best Practices (Python)\n\n```python\nimport pytest\n\n# Function scope (default) - Fresh each test\n@pytest.fixture\ndef db_session():\n    session = create_session()\n    yield session\n    session.rollback()\n\n# Module scope - Shared across file\n@pytest.fixture(scope=\"module\")\ndef expensive_model():\n    return load_ml_model()  # Only loads once per file\n\n# Session scope - Shared across all tests\n@pytest.fixture(scope=\"session\")\ndef db_engine():\n    engine = create_engine(TEST_DB_URL)\n    yield engine\n    engine.dispose()\n```\n\n## Common Violations\n\n### 1. Test in Wrong Location\n```\nBLOCKED: Test file must be in tests/ directory\n  File: src/utils/helpers.test.ts\n  Move to: tests/utils/helpers.test.ts\n```\n\n### 2. Missing AAA Structure\n```\nBLOCKED: Test pattern violations detected\n  - Tests should follow AAA pattern (Arrange/Act/Assert)\n  - Add comments or clear separation between sections\n```\n\n### 3. Shared Mutable State\n```\nBLOCKED: Test pattern violations detected\n  - Shared mutable state detected. Use beforeEach to reset state.\n```\n\n### 4. Coverage Below Threshold\n```\nBLOCKED: Coverage 75.2% is below threshold 80%\n\nActions required:\n  1. Add tests for uncovered code\n  2. Run: npm test -- --coverage\n  3. Ensure coverage >= 80% before proceeding\n```\n\n## Related Skills\n\n- `integration-testing` - Component interaction tests\n- `e2e-testing` - End-to-end with Playwright\n- `msw-mocking` - Network mocking\n- `test-data-management` - Fixtures and factories\n\n## Capability Details\n\n### aaa-pattern\n**Keywords:** AAA, arrange act assert, test structure, test pattern\n**Solves:**\n- Enforce Arrange-Act-Assert pattern\n- Ensure clear test structure\n- Improve test readability\n\n### test-naming\n**Keywords:** test name, test naming, descriptive test, test description\n**Solves:**\n- Enforce descriptive test names\n- Block generic test names like test1\n- Improve test documentation\n\n### test-location\n**Keywords:** test location, test directory, tests folder, where tests\n**Solves:**\n- Validate test file placement\n- Block tests mixed with source\n- Enforce test directory structure\n\n### coverage-threshold\n**Keywords:** coverage, test coverage, code coverage, 80%, threshold\n**Solves:**\n- Enforce minimum 80% coverage\n- Block merges with low coverage\n- Maintain quality standards\n\n### test-isolation\n**Keywords:** test isolation, shared state, independent tests, flaky\n**Solves:**\n- Detect shared mutable state\n- Ensure test independence\n- Prevent flaky tests"
              },
              {
                "name": "type-safety-validation",
                "description": "Use when adding runtime validation, type-safe APIs, or end-to-end TypeScript safety. Covers Zod schemas, tRPC, Prisma, and exhaustive type checking patterns.",
                "path": ".claude/skills/type-safety-validation/SKILL.md",
                "frontmatter": {
                  "name": "type-safety-validation",
                  "description": "Use when adding runtime validation, type-safe APIs, or end-to-end TypeScript safety. Covers Zod schemas, tRPC, Prisma, and exhaustive type checking patterns.",
                  "context": "fork",
                  "agent": "frontend-ui-developer",
                  "version": "1.1.0",
                  "author": "AI Agent Hub",
                  "tags": [
                    "typescript",
                    "zod",
                    "trpc",
                    "prisma",
                    "type-safety",
                    "validation",
                    "exhaustive-types",
                    "branded-types",
                    2025
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ]
                  }
                },
                "content": "# Type Safety & Validation\n\n## Overview\n\nEnd-to-end type safety ensures bugs are caught at compile time, not runtime. This skill covers Zod for runtime validation, tRPC for type-safe APIs, Prisma for type-safe database access, and modern TypeScript features.\n\n**When to use this skill:**\n- Building type-safe APIs (REST, RPC, GraphQL)\n- Validating user input and external data\n- Ensuring database queries are type-safe\n- Creating end-to-end typed full-stack applications\n- Migrating from JavaScript to TypeScript\n- Implementing strict validation rules\n\n## Core Stack\n\n### 1. Zod - Runtime Validation\n\n```typescript\nimport { z } from 'zod'\n\n// Define schema\nconst UserSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  age: z.number().int().positive().max(120),\n  role: z.enum(['admin', 'user', 'guest']),\n  metadata: z.record(z.string()).optional(),\n  createdAt: z.date().default(() => new Date())\n})\n\n// Infer TypeScript type from schema\ntype User = z.infer<typeof UserSchema>\n\n// Validate data\nconst result = UserSchema.safeParse(data)\nif (result.success) {\n  const user: User = result.data\n} else {\n  console.error(result.error.issues)\n}\n\n// Transform data\nconst EmailSchema = z.string().email().transform(email => email.toLowerCase())\n```\n\n**Advanced Patterns**:\n```typescript\n// Refinements\nconst PasswordSchema = z.string()\n  .min(8)\n  .refine((pass) => /[A-Z]/.test(pass), 'Must contain uppercase')\n  .refine((pass) => /[0-9]/.test(pass), 'Must contain number')\n\n// Discriminated Unions\nconst EventSchema = z.discriminatedUnion('type', [\n  z.object({ type: z.literal('click'), x: z.number(), y: z.number() }),\n  z.object({ type: z.literal('scroll'), offset: z.number() })\n])\n\n// Recursive Types\nconst CategorySchema: z.ZodType<Category> = z.lazy(() =>\n  z.object({\n    name: z.string(),\n    children: z.array(CategorySchema).optional()\n  })\n)\n```\n\n### 2. tRPC - Type-Safe APIs\n\n```typescript\n// Server: Define procedures\nimport { initTRPC } from '@trpc/server'\nimport { z } from 'zod'\n\nconst t = initTRPC.create()\n\nexport const appRouter = t.router({\n  getUser: t.procedure\n    .input(z.object({ id: z.string() }))\n    .query(async ({ input }) => {\n      return await db.user.findUnique({ where: { id: input.id } })\n    }),\n\n  createUser: t.procedure\n    .input(z.object({\n      email: z.string().email(),\n      name: z.string()\n    }))\n    .mutation(async ({ input }) => {\n      return await db.user.create({ data: input })\n    })\n})\n\nexport type AppRouter = typeof appRouter\n\n// Client: Fully typed!\nimport { createTRPCProxyClient, httpBatchLink } from '@trpc/client'\nimport type { AppRouter } from './server'\n\nconst client = createTRPCProxyClient<AppRouter>({\n  links: [httpBatchLink({ url: 'http://localhost:3000/api/trpc' })]\n})\n\n// TypeScript knows the exact shape!\nconst user = await client.getUser.query({ id: '123' })\n//    ^? User | null\n```\n\n### 3. Prisma - Type-Safe ORM\n\n```prisma\n// schema.prisma\nmodel User {\n  id        String   @id @default(cuid())\n  email     String   @unique\n  posts     Post[]\n  profile   Profile?\n  createdAt DateTime @default(now())\n}\n\nmodel Post {\n  id        String   @id @default(cuid())\n  title     String\n  content   String?\n  published Boolean  @default(false)\n  author    User     @relation(fields: [authorId], references: [id])\n  authorId  String\n}\n```\n\n```typescript\nimport { PrismaClient } from '@prisma/client'\n\nconst prisma = new PrismaClient()\n\n// Fully typed queries\nconst user = await prisma.user.findUnique({\n  where: { id: '123' },\n  include: {\n    posts: {\n      where: { published: true },\n      orderBy: { createdAt: 'desc' }\n    }\n  }\n})\n// user is typed as: User & { posts: Post[] }\n\n// Type-safe creates\nconst newUser = await prisma.user.create({\n  data: {\n    email: 'user@example.com',\n    posts: {\n      create: [\n        { title: 'First Post', content: 'Hello world' }\n      ]\n    }\n  }\n})\n```\n\n### 4. TypeScript 5.7+ Features\n\n```typescript\n// Const type parameters (TS 5.0+)\nfunction firstElement<T extends readonly any[]>(arr: T) {\n  return arr[0]\n}\n\nconst result = firstElement(['a', 'b'] as const)\n// result is typed as 'a'\n\n// Satisfies operator (TS 4.9+)\nconst config = {\n  url: 'https://api.example.com',\n  timeout: 5000\n} satisfies Config  // Ensures config matches Config, but keeps literal types\n\n// Decorators (TS 5.0+)\nfunction logged(target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n  const original = descriptor.value\n  descriptor.value = function (...args: any[]) {\n    console.log(\"Calling \" + propertyKey)\n    return original.apply(this, args)\n  }\n}\n\nclass API {\n  @logged\n  async fetchData() {}\n}\n```\n\n## Full-Stack Example\n\n```typescript\n// ===== BACKEND (Next.js API) =====\n// app/api/trpc/[trpc]/route.ts\nimport { fetchRequestHandler } from '@trpc/server/adapters/fetch'\nimport { appRouter } from '@/server/routers/_app'\n\nexport async function GET(req: Request) {\n  return fetchRequestHandler({\n    endpoint: '/api/trpc',\n    req,\n    router: appRouter,\n    createContext: () => ({})\n  })\n}\n\nexport const POST = GET\n\n// server/routers/_app.ts\nimport { z } from 'zod'\nimport { prisma } from '@/lib/prisma'\nimport { publicProcedure, router } from '../trpc'\n\nexport const appRouter = router({\n  posts: {\n    list: publicProcedure\n      .input(z.object({\n        limit: z.number().min(1).max(100).default(10),\n        cursor: z.string().optional()\n      }))\n      .query(async ({ input }) => {\n        const posts = await prisma.post.findMany({\n          take: input.limit + 1,\n          cursor: input.cursor ? { id: input.cursor } : undefined,\n          orderBy: { createdAt: 'desc' },\n          include: { author: true }\n        })\n\n        return {\n          items: posts.slice(0, input.limit),\n          nextCursor: posts[input.limit]?.id\n        }\n      }),\n\n    create: publicProcedure\n      .input(z.object({\n        title: z.string().min(1).max(200),\n        content: z.string().optional()\n      }))\n      .mutation(async ({ input }) => {\n        return await prisma.post.create({\n          data: input\n        })\n      })\n  }\n})\n\n// ===== FRONTEND (React) =====\n// lib/trpc.ts\nimport { createTRPCReact } from '@trpc/react-query'\nimport type { AppRouter } from '@/server/routers/_app'\n\nexport const trpc = createTRPCReact<AppRouter>()\n\n// components/PostList.tsx\n'use client'\n\nimport { trpc } from '@/lib/trpc'\n\nexport function PostList() {\n  const { data, isLoading } = trpc.posts.list.useQuery({ limit: 10 })\n  const createPost = trpc.posts.create.useMutation()\n\n  if (isLoading) return <div>Loading...</div>\n\n  return (\n    <div>\n      {data?.items.map(post => (\n        <div key={post.id}>\n          <h2>{post.title}</h2>\n          <p>{post.content}</p>\n          <span>By {post.author.name}</span>\n        </div>\n      ))}\n\n      <button onClick={() => createPost.mutate({ title: 'New Post' })}>\n        Create Post\n      </button>\n    </div>\n  )\n}\n```\n\n## Python Type Safety with Ty\n\n**SkillForge uses ty**, a Rust-based static type checker for Python that enforces stricter type safety than mypy.\n\n### Pattern: Safe Dict Extraction (Ty-Compliant)\n\n```python\nfrom typing import cast\n\n# Extract from untyped dict (e.g., agent results)\nresult = {\"findings\": {...}, \"confidence_score\": 0.85}\nfindings_raw = result.get(\"findings\", {})\nconfidence_raw = result.get(\"confidence_score\")\n\n# Type-safe extraction with explicit annotations\nfindings_to_save: dict[str, object] | None = (\n    cast(\"dict[str, object]\", findings_raw) if isinstance(findings_raw, dict) else None\n)\nconfidence_to_save: float | None = (\n    float(confidence_raw) if isinstance(confidence_raw, (int, float)) else None\n)\n```\n\n**Why needed**: Ty requires explicit type annotations + `isinstance()` checks to narrow types from `object | None`.\n\n**Full patterns**: See `references/ty-type-checker-patterns.md` for:\n- Mixed numeric type handling\n- List type narrowing\n- Nested dict extraction\n- Agent result processing examples\n\n## Exhaustive Type Checking (2025 Pattern)\n\nTypeScript's type system can guarantee compile-time exhaustiveness for union types. This prevents runtime bugs when union members are added or changed.\n\n### The assertNever Pattern\n\n```typescript\n//  ALWAYS use this helper function\nfunction assertNever(x: never): never {\n  throw new Error(\"Unexpected value: \" + x)\n}\n\n// Example: Status handling\ntype AnalysisStatus = 'pending' | 'running' | 'completed' | 'failed'\n\nfunction getStatusColor(status: AnalysisStatus): string {\n  switch (status) {\n    case 'pending': return 'gray'\n    case 'running': return 'blue'\n    case 'completed': return 'green'\n    case 'failed': return 'red'\n    default: return assertNever(status) //  Compile-time exhaustiveness check\n  }\n}\n\n// If you add a new status 'cancelled', TypeScript will error at compile time:\n// Error: Argument of type 'string' is not assignable to parameter of type 'never'.\n```\n\n### Exhaustive Record Mapping\n\n```typescript\n// For mapping union types to values, use satisfies with Record\ntype EventType = 'click' | 'scroll' | 'keypress' | 'hover'\n\nconst eventColors = {\n  click: 'red',\n  scroll: 'blue',\n  keypress: 'green',\n  hover: 'yellow',\n} as const satisfies Record<EventType, string>\n\n// TypeScript will error if any EventType is missing from the record\n// Adding new EventType requires updating this record\n```\n\n### Exhaustive Handler Objects\n\n```typescript\n// For complex logic, use handler objects instead of switches\ntype ContentType = 'article' | 'video' | 'podcast' | 'repository'\n\ninterface ContentHandler<T> {\n  article: (data: ArticleData) => T\n  video: (data: VideoData) => T\n  podcast: (data: PodcastData) => T\n  repository: (data: RepoData) => T\n}\n\nfunction createContentHandlers<T>(handlers: ContentHandler<T>): ContentHandler<T> {\n  return handlers\n}\n\n// Usage: TypeScript enforces all content types are handled\nconst renderContent = createContentHandlers({\n  article: (data) => <ArticleCard {...data} />,\n  video: (data) => <VideoPlayer {...data} />,\n  podcast: (data) => <AudioPlayer {...data} />,\n  repository: (data) => <RepoCard {...data} />,\n})\n```\n\n### Exhaustive Union Checks with Type Guards\n\n```typescript\n// When you need runtime type narrowing with exhaustiveness\ntype APIResponse =\n  | { type: 'success'; data: Data }\n  | { type: 'error'; error: Error }\n  | { type: 'loading' }\n\nfunction handleResponse(response: APIResponse): string {\n  switch (response.type) {\n    case 'success':\n      return \"Data: \" + response.data.id\n    case 'error':\n      return \"Error: \" + response.error.message\n    case 'loading':\n      return 'Loading...'\n    default:\n      return assertNever(response) // Ensures all cases handled\n  }\n}\n```\n\n### Template Literal Exhaustiveness\n\n```typescript\n// For string pattern unions\ntype Size = 'sm' | 'md' | 'lg' | 'xl'\ntype Variant = 'primary' | 'secondary' | 'danger'\n// Template literal type (TypeScript feature)\ntype ButtonClass = \"btn-${Size}-${Variant}\" // Pseudo-syntax for documentation\n\n// Exhaustive size mapping\nconst sizeMap = {\n  sm: 'text-sm py-1 px-2',\n  md: 'text-base py-2 px-4',\n  lg: 'text-lg py-3 px-6',\n  xl: 'text-xl py-4 px-8',\n} as const satisfies Record<Size, string>\n\n// Compile-time error if Size is expanded without updating sizeMap\n```\n\n### Branded Types for IDs\n\n**TypeScript Pattern** (Zod runtime validation):\n\n```typescript\nimport { z } from 'zod'\n\n// Create branded types for different ID kinds\nconst UserId = z.string().uuid().brand<'UserId'>()\nconst AnalysisId = z.string().uuid().brand<'AnalysisId'>()\nconst ArtifactId = z.string().uuid().brand<'ArtifactId'>()\n\ntype UserId = z.infer<typeof UserId>\ntype AnalysisId = z.infer<typeof AnalysisId>\ntype ArtifactId = z.infer<typeof ArtifactId>\n\n// Now TypeScript prevents mixing ID types\nfunction deleteAnalysis(id: AnalysisId): void { ... }\nfunction getUser(id: UserId): User { ... }\n\nconst userId: UserId = UserId.parse('...')\nconst analysisId: AnalysisId = AnalysisId.parse('...')\n\ndeleteAnalysis(analysisId) //  OK\ndeleteAnalysis(userId)     //  Error: UserId not assignable to AnalysisId\n```\n\n**Python Pattern** (NewType compile-time safety):\n\n```python\nfrom typing import NewType\nfrom uuid import UUID\n\n# Define branded types (zero runtime overhead)\nAnalysisID = NewType(\"AnalysisID\", UUID)\nArtifactID = NewType(\"ArtifactID\", UUID)\nSessionID = NewType(\"SessionID\", UUID)\nTraceID = NewType(\"TraceID\", str)\n\n# Factory functions for runtime validation\ndef create_analysis_id(value: UUID | str) -> AnalysisID:\n    \"\"\"Create typed AnalysisID with validation.\"\"\"\n    if isinstance(value, str):\n        value = UUID(value)\n    return AnalysisID(value)\n\ndef create_artifact_id(value: UUID | str) -> ArtifactID:\n    \"\"\"Create typed ArtifactID with validation.\"\"\"\n    if isinstance(value, str):\n        value = UUID(value)\n    return ArtifactID(value)\n\n# Type checker (mypy/ty) prevents mixing\ndef delete_analysis(id: AnalysisID) -> None: ...\ndef get_artifact(id: ArtifactID) -> Artifact: ...\n\nanalysis_id = create_analysis_id(\"...\")\nartifact_id = create_artifact_id(\"...\")\n\ndelete_analysis(analysis_id)  #  OK\ndelete_analysis(artifact_id)  #  Error: ArtifactID not assignable to AnalysisID\n```\n\n**Why NewType for Python?**\n- **Zero runtime overhead** - compiled away, no wrapper object\n- **Mypy/Ty enforcement** - catches ID mixing at type-check time\n- **Explicit factories** - centralized validation logic\n- **Better than Pydantic** for this use case - no serialization needed\n\n### Common Anti-Patterns\n\n```typescript\n//  NEVER use non-exhaustive switch\nswitch (status) {\n  case 'pending': return 'gray'\n  case 'running': return 'blue'\n  // Missing cases! Runtime bugs waiting to happen\n}\n\n//  NEVER use default without assertNever\nswitch (status) {\n  case 'pending': return 'gray'\n  case 'running': return 'blue'\n  default: return 'unknown' // Silent bug if new status added\n}\n\n//  NEVER use if-else chains for union types\nif (status === 'pending') return 'gray'\nelse if (status === 'running') return 'blue'\n// No compile-time check for missing cases!\n\n//  ALWAYS use switch with assertNever\nswitch (status) {\n  case 'pending': return 'gray'\n  case 'running': return 'blue'\n  case 'completed': return 'green'\n  case 'failed': return 'red'\n  default: return assertNever(status)\n}\n```\n\n## Best Practices\n\n### Validation\n-  Validate at boundaries (API inputs, form submissions, external data)\n-  Use `.safeParse()` to handle errors gracefully\n-  Provide clear error messages for users\n-  Validate environment variables at startup\n-  Use branded types for IDs (`z.string().brand<'UserId'>()`)\n\n### Type Safety\n-  Enable `strict: true` in `tsconfig.json`\n-  Use `noUncheckedIndexedAccess` for safer array access\n-  Prefer `unknown` over `any`\n-  Use type guards for narrowing\n-  Leverage inference with `typeof` and `ReturnType`\n-  **Exhaustive switches**: Always use `assertNever` in default case\n-  **Exhaustive records**: Use `satisfies Record<UnionType, Value>`\n-  **Branded types (TypeScript)**: Use Zod `.brand<>()` for distinct ID types\n-  **Branded types (Python)**: Use `NewType` for zero-overhead compile-time safety\n-  **Python/Ty**: Use explicit annotations + `isinstance()` for dict extraction\n\n### Performance\n-  Reuse schemas (don't create inline)\n-  Use `.parse()` for known-good data (faster than `.safeParse()`)\n-  Enable Prisma query optimization\n-  Use tRPC batching for multiple queries\n-  Cache validation results when appropriate\n\n## Resources\n\n- [Zod Documentation](https://zod.dev)\n- [tRPC Documentation](https://trpc.io)\n- [Prisma Documentation](https://www.prisma.io/docs)\n- [TypeScript Handbook](https://www.typescriptlang.org/docs/handbook/intro.html)\n\n---\n\n**Skill Version**: 1.2.0\n**Last Updated**: 2025-12-27\n**Maintained by**: AI Agent Hub Team\n\n## Changelog\n\n### v1.2.0 (2025-12-27)\n- Added Python `NewType` pattern for branded types (zero-overhead compile-time safety)\n- Added factory function pattern for typed ID creation\n- Updated branded types section with TypeScript vs Python comparison\n- Updated best practices to include Python NewType usage\n\n### v1.1.0 (2025-12-25)\n- Added comprehensive exhaustive type checking section\n- Added `assertNever` pattern for compile-time exhaustiveness\n- Added exhaustive record mapping with `satisfies`\n- Added exhaustive handler objects pattern\n- Added template literal exhaustiveness examples\n- Added branded types for IDs with Zod\n- Added common anti-patterns for non-exhaustive code\n- Updated best practices with exhaustive type checking guidelines\n\n### v1.0.0 (2025-12-14)\n- Initial skill with Zod, tRPC, Prisma, and TypeScript 5.7+ patterns\n\n## Capability Details\n\n### zod-schemas\n**Keywords:** zod, schema, validation, parse, safeParse, infer, refine, transform\n**Solves:**\n- How do I validate input with Zod?\n- Create runtime validation schema\n- Infer TypeScript types from Zod\n- Transform and refine data with Zod\n- Handle Zod validation errors\n\n### exhaustive-types\n**Keywords:** exhaustive, assertNever, never assertion, switch exhaustive, compile-time exhaustiveness, exhaustive check, missing cases, union exhaustive\n**Solves:**\n- How do I make switch statements exhaustive?\n- Compile-time check for missing union cases\n- assertNever pattern for TypeScript\n- Catch missing cases at compile time\n- Exhaustive handler objects pattern\n- Why does my switch have a default that never runs?\n\n### branded-types\n**Keywords:** branded type, type branding, nominal type, opaque type, NewType, brand, distinct types, id types\n**Solves:**\n- How do I prevent mixing different ID types?\n- Branded types with Zod\n- Python NewType for type safety\n- Prevent UserId where AnalysisId expected\n- Nominal typing in TypeScript\n- Type-safe IDs across the codebase\n\n### trpc\n**Keywords:** trpc, type-safe api, procedure, router, mutation, query, context, middleware\n**Solves:**\n- How do I set up tRPC?\n- Type-safe API calls\n- tRPC with React Query\n- tRPC authentication middleware\n- tRPC error handling\n\n### prisma-types\n**Keywords:** prisma, orm, generated types, model, client, payload, validator\n**Solves:**\n- How do I use Prisma types?\n- Type-safe database queries\n- Prisma schema to TypeScript\n- Extract types from Prisma queries\n- Extend Prisma models with custom fields\n\n### typescript-5-features\n**Keywords:** typescript 5, const parameters, satisfies, decorators, template literals, inferred predicates\n**Solves:**\n- Use TypeScript 5.x features\n- Const type parameters\n- Satisfies operator\n- Modern TypeScript patterns\n- Branded types and template literals\n\n### skillforge-integration\n**Keywords:** skillforge, pydantic, fastapi, openapi, backend types, frontend types\n**Solves:**\n- How can SkillForge use Zod with FastAPI?\n- Convert Pydantic to Zod schemas\n- Share types between Python and TypeScript\n- Validate API responses in frontend\n- Generate types from OpenAPI spec\n\n### implementation-checklist\n**Keywords:** checklist, type safety, validation, testing, deployment\n**Solves:**\n- What steps to implement type safety?\n- Type safety checklist\n- Validation best practices\n- Type safety testing strategy\n- Production deployment checklist\n\n### ty-type-checker\n**Keywords:** ty, rust type checker, strict typing, isinstance, cast, type narrowing, dict extraction, optional types\n**Solves:**\n- How do I make ty type checker pass?\n- Extract values from untyped dicts safely\n- Type-safe agent result processing\n- Handle optional numeric types with ty\n- Type narrowing with isinstance checks\n- Fix 'object | None' type errors in ty"
              },
              {
                "name": "unit-testing",
                "description": "Unit testing patterns and best practices. Use when writing isolated unit tests, implementing AAA pattern, designing test isolation, or setting coverage targets for business logic.",
                "path": ".claude/skills/unit-testing/SKILL.md",
                "frontmatter": {
                  "name": "unit-testing",
                  "description": "Unit testing patterns and best practices. Use when writing isolated unit tests, implementing AAA pattern, designing test isolation, or setting coverage targets for business logic.",
                  "context": "fork",
                  "agent": "test-generator",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Unit Testing\n\nTest isolated business logic with fast, deterministic tests.\n\n## When to Use\n\n- Testing pure functions\n- Business logic isolation\n- Fast feedback loops\n- High coverage targets\n\n## AAA Pattern (Arrange-Act-Assert)\n\n```typescript\ndescribe('calculateDiscount', () => {\n  test('applies 10% discount for orders over $100', () => {\n    // Arrange\n    const order = { items: [{ price: 150 }] };\n\n    // Act\n    const result = calculateDiscount(order);\n\n    // Assert\n    expect(result).toBe(15);\n  });\n});\n```\n\n## Test Isolation\n\n```typescript\ndescribe('UserService', () => {\n  let service: UserService;\n  let mockRepo: MockRepository;\n\n  beforeEach(() => {\n    // Fresh instances per test\n    mockRepo = createMockRepository();\n    service = new UserService(mockRepo);\n  });\n\n  afterEach(() => {\n    // Clean up\n    vi.clearAllMocks();\n  });\n});\n```\n\n## Coverage Targets\n\n| Area | Target |\n|------|--------|\n| Business logic | 90%+ |\n| Critical paths | 100% |\n| New features | 100% |\n| Utilities | 80%+ |\n\n## Parameterized Tests\n\n```typescript\ndescribe('isValidEmail', () => {\n  test.each([\n    ['test@example.com', true],\n    ['invalid', false],\n    ['@missing.com', false],\n    ['user@domain.co.uk', true],\n  ])('isValidEmail(%s) returns %s', (email, expected) => {\n    expect(isValidEmail(email)).toBe(expected);\n  });\n});\n```\n\n## Python Example\n\n```python\nimport pytest\n\nclass TestCalculateDiscount:\n    def test_applies_discount_over_threshold(self):\n        # Arrange\n        order = Order(total=150)\n\n        # Act\n        discount = calculate_discount(order)\n\n        # Assert\n        assert discount == 15\n\n    @pytest.mark.parametrize(\"total,expected\", [\n        (100, 0),\n        (101, 10.1),\n        (200, 20),\n    ])\n    def test_discount_thresholds(self, total, expected):\n        order = Order(total=total)\n        assert calculate_discount(order) == expected\n```\n\n## Fixture Scoping (2026 Best Practice)\n\n```python\nimport pytest\n\n# Function scope (default): Fresh instance per test - ISOLATED\n@pytest.fixture(scope=\"function\")\ndef db_session():\n    \"\"\"Each test gets clean database state.\"\"\"\n    session = create_session()\n    yield session\n    session.rollback()  # Cleanup\n\n# Module scope: Shared across all tests in file - EFFICIENT\n@pytest.fixture(scope=\"module\")\ndef expensive_model():\n    \"\"\"Load once per test file (expensive setup).\"\"\"\n    return load_large_ml_model()  # 5 seconds to load\n\n# Session scope: Shared across ALL tests - MOST EFFICIENT\n@pytest.fixture(scope=\"session\")\ndef db_engine():\n    \"\"\"Single connection pool for entire test run.\"\"\"\n    engine = create_engine(TEST_DB_URL)\n    Base.metadata.create_all(engine)\n    yield engine\n    Base.metadata.drop_all(engine)\n```\n\n**When to use each scope:**\n| Scope | Use Case | Example |\n|-------|----------|---------|\n| function | Isolated tests, mutable state | db_session, mock objects |\n| module | Expensive setup, read-only | ML model, compiled regex |\n| session | Very expensive, immutable | DB engine, external service |\n\n## Indirect Parametrization\n\n```python\n# Defer expensive setup from collection to runtime\n@pytest.fixture\ndef user(request):\n    \"\"\"Create user with different roles based on parameter.\"\"\"\n    role = request.param  # Receives value from parametrize\n    return UserFactory(role=role)\n\n@pytest.mark.parametrize(\"user\", [\"admin\", \"moderator\", \"viewer\"], indirect=True)\ndef test_permissions(user):\n    \"\"\"Test runs 3 times with different user roles.\"\"\"\n    # user fixture is called with each role\n    assert user.can_access(\"/dashboard\") == (user.role in [\"admin\", \"moderator\"])\n\n# Combinatorial testing with stacked decorators\n@pytest.mark.parametrize(\"role\", [\"admin\", \"user\"])\n@pytest.mark.parametrize(\"status\", [\"active\", \"suspended\"])\ndef test_access_matrix(role, status):\n    \"\"\"Runs 4 tests: admin/active, admin/suspended, user/active, user/suspended\"\"\"\n    user = User(role=role, status=status)\n    expected = (role == \"admin\" and status == \"active\")\n    assert user.can_modify() == expected\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Framework | Vitest (modern), Jest (mature), pytest |\n| Execution | < 100ms per test |\n| Dependencies | None (mock everything external) |\n| Coverage tool | c8, nyc, pytest-cov |\n\n## Common Mistakes\n\n- Testing implementation, not behavior\n- Slow tests (external calls)\n- Shared state between tests\n- Over-mocking (testing mocks not code)\n\n## Related Skills\n\n- `integration-testing` - Testing interactions\n- `msw-mocking` - Network mocking\n- `test-data-management` - Fixtures and factories\n\n## Capability Details\n\n### pytest-patterns\n**Keywords:** pytest, python, fixture, parametrize\n**Solves:**\n- Write pytest unit tests\n- Use fixtures effectively\n- Parametrize test cases\n\n### vitest-patterns\n**Keywords:** vitest, jest, typescript, mock\n**Solves:**\n- Write Vitest unit tests\n- Mock dependencies\n- Test React components\n\n### skillforge-strategy\n**Keywords:** skillforge, strategy, coverage, pyramid\n**Solves:**\n- SkillForge test strategy example\n- Test coverage targets\n- Testing pyramid ratios\n\n### test-case-template\n**Keywords:** template, test, structure, arrange\n**Solves:**\n- Test case template\n- Arrange-Act-Assert structure\n- Copy-paste test starter"
              },
              {
                "name": "vcr-http-recording",
                "description": "VCR.py HTTP recording for Python tests. Use when testing Python code making HTTP requests, recording API responses for replay, or creating deterministic tests for external services.",
                "path": ".claude/skills/vcr-http-recording/SKILL.md",
                "frontmatter": {
                  "name": "vcr-http-recording",
                  "description": "VCR.py HTTP recording for Python tests. Use when testing Python code making HTTP requests, recording API responses for replay, or creating deterministic tests for external services.",
                  "context": "fork",
                  "agent": "test-generator",
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# VCR.py HTTP Recording\n\nRecord and replay HTTP interactions for Python tests.\n\n## When to Use\n\n- External API testing\n- Deterministic HTTP tests\n- Avoiding live API calls in CI\n- LLM API response recording\n\n## Basic Setup\n\n```python\n# conftest.py\nimport pytest\n\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    return {\n        \"cassette_library_dir\": \"tests/cassettes\",\n        \"record_mode\": \"once\",\n        \"match_on\": [\"uri\", \"method\"],\n        \"filter_headers\": [\"authorization\", \"x-api-key\"],\n        \"filter_query_parameters\": [\"api_key\", \"token\"],\n    }\n```\n\n## Basic Usage\n\n```python\nimport pytest\n\n@pytest.mark.vcr()\ndef test_fetch_user():\n    response = requests.get(\"https://api.example.com/users/1\")\n\n    assert response.status_code == 200\n    assert response.json()[\"name\"] == \"John Doe\"\n\n@pytest.mark.vcr(\"custom_cassette.yaml\")\ndef test_with_custom_cassette():\n    response = requests.get(\"https://api.example.com/data\")\n    assert response.status_code == 200\n```\n\n## Async Support\n\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.mark.asyncio\n@pytest.mark.vcr()\nasync def test_async_api_call():\n    async with AsyncClient() as client:\n        response = await client.get(\"https://api.example.com/data\")\n\n    assert response.status_code == 200\n    assert \"items\" in response.json()\n```\n\n## Recording Modes\n\n```python\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    import os\n\n    # CI: never record, only replay\n    if os.environ.get(\"CI\"):\n        record_mode = \"none\"\n    else:\n        record_mode = \"new_episodes\"\n\n    return {\"record_mode\": record_mode}\n```\n\n| Mode | Behavior |\n|------|----------|\n| `once` | Record if missing, then replay |\n| `new_episodes` | Record new, replay existing |\n| `none` | Never record (CI) |\n| `all` | Always record (refresh) |\n\n## Filtering Sensitive Data\n\n```python\ndef filter_request_body(request):\n    \"\"\"Redact sensitive data from request body.\"\"\"\n    import json\n    if request.body:\n        try:\n            body = json.loads(request.body)\n            if \"password\" in body:\n                body[\"password\"] = \"REDACTED\"\n            if \"api_key\" in body:\n                body[\"api_key\"] = \"REDACTED\"\n            request.body = json.dumps(body)\n        except json.JSONDecodeError:\n            pass\n    return request\n\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\"authorization\", \"x-api-key\"],\n        \"before_record_request\": filter_request_body,\n    }\n```\n\n## LLM API Testing\n\n```python\ndef llm_request_matcher(r1, r2):\n    \"\"\"Match LLM requests ignoring dynamic fields.\"\"\"\n    import json\n\n    if r1.uri != r2.uri or r1.method != r2.method:\n        return False\n\n    body1 = json.loads(r1.body)\n    body2 = json.loads(r2.body)\n\n    # Ignore dynamic fields\n    for field in [\"request_id\", \"timestamp\"]:\n        body1.pop(field, None)\n        body2.pop(field, None)\n\n    return body1 == body2\n\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    return {\n        \"custom_matchers\": [llm_request_matcher],\n    }\n```\n\n## Cassette File Example\n\n```yaml\n# tests/cassettes/test_fetch_user.yaml\ninteractions:\n- request:\n    body: null\n    headers:\n      Content-Type: application/json\n    method: GET\n    uri: https://api.example.com/users/1\n  response:\n    body:\n      string: '{\"id\": 1, \"name\": \"John Doe\"}'\n    status:\n      code: 200\nversion: 1\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Record mode | `once` for dev, `none` for CI |\n| Cassette format | YAML (readable) |\n| Sensitive data | Always filter headers/body |\n| Custom matchers | Use for LLM APIs |\n\n## Common Mistakes\n\n- Committing cassettes with real API keys\n- Using `all` mode in CI (makes live calls)\n- Not filtering sensitive data\n- Missing cassettes in git\n\n## Related Skills\n\n- `msw-mocking` - Frontend equivalent\n- `integration-testing` - API testing patterns\n- `llm-testing` - LLM-specific patterns\n\n## Capability Details\n\n### http-recording\n**Keywords:** record HTTP, vcr.use_cassette, record mode, capture HTTP\n**Solves:**\n- Record HTTP interactions for replay\n- Capture real API responses\n- Create deterministic test fixtures\n\n### cassette-replay\n**Keywords:** replay, cassette, playback, mock replay\n**Solves:**\n- Replay recorded HTTP interactions\n- Run tests without network access\n- Ensure consistent test results\n\n### async-support\n**Keywords:** async, aiohttp, httpx async, async cassette\n**Solves:**\n- Record async HTTP clients\n- Handle aiohttp and httpx async\n- Test async API integrations\n\n### sensitive-data-filtering\n**Keywords:** filter, scrub, redact, sensitive data, before_record\n**Solves:**\n- Scrub API keys from cassettes\n- Redact sensitive data\n- Implement before_record hooks\n\n### custom-matchers\n**Keywords:** matcher, match on, request matching, custom match\n**Solves:**\n- Configure request matching rules\n- Ignore dynamic request parts\n- Match by method/host/path\n\n### llm-api-testing\n**Keywords:** LLM cassette, OpenAI recording, Anthropic recording\n**Solves:**\n- Record LLM API responses\n- Test AI integrations deterministically\n- Avoid costly API calls in tests"
              },
              {
                "name": "verify",
                "description": "Comprehensive feature verification with parallel analysis agents",
                "path": ".claude/skills/verify/SKILL.md",
                "frontmatter": {
                  "name": "verify",
                  "description": "Comprehensive feature verification with parallel analysis agents",
                  "context": "fork",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "verification",
                    "testing",
                    "security",
                    "code-quality"
                  ]
                },
                "content": "# Verify Feature Branch\n\nComplete verification using subagents, skills, MCPs, and latest best practices.\n\n## When to Use\n\n- Verifying feature branches before merge\n- Running comprehensive code quality checks\n- Security auditing changes\n- Validating against current best practices\n\n## Quick Start\n\n```bash\n/verify                    # Verify current branch\n/verify issue/123-feature  # Verify specific branch\n```\n\n## Step 1: Analyze Scope\n\n```bash\n# What changed?\ngit diff --name-only dev...HEAD\ngit log --oneline dev..HEAD\n```\n\nIdentify libraries used, patterns implemented, and complexity.\n\n## Step 2: Fetch Latest Best Practices\n\n### Web Search\n```python\nWebSearch(\"React 19 best practices 2025\")\nWebSearch(\"FastAPI security patterns 2025\")\nWebSearch(\"Python CVE security vulnerabilities 2025\")\n```\n\n### Context7 Documentation\n```python\nmcp__context7__get-library-docs(libraryId=\"/facebook/react\", topic=\"hooks\")\nmcp__context7__get-library-docs(libraryId=\"/tiangolo/fastapi\", topic=\"dependencies\")\n```\n\n## Step 3: Load Review Skills\n\n```python\nRead(\".claude/skills/code-review-playbook/capabilities.json\")\nRead(\".claude/skills/security-checklist/capabilities.json\")\nRead(\".claude/skills/testing-strategy-builder/capabilities.json\")\n```\n\n## Step 4: Parallel Code Review (3 Agents)\n\nLaunch THREE reviewers in ONE message:\n\n| Agent | Focus |\n|-------|-------|\n| code-quality-reviewer #1 | Backend: Ruff, ty, tests |\n| code-quality-reviewer #2 | Frontend: Biome, ESLint, tsc |\n| code-quality-reviewer #3 | Security: OWASP, secrets, deps |\n\n## Step 5: Run Test Suite\n\n```bash\n# Backend\ncd backend\npoetry run pytest tests/unit/ -v --tb=short \\\n  --cov=app --cov-report=term-missing --cov-fail-under=80\n\n# Frontend\ncd frontend\nnpm run test -- --coverage\n```\n\n## Step 6: E2E Verification\n\nIf UI changes, use Playwright MCP:\n\n```python\nmcp__playwright__browser_navigate(url=\"http://localhost:5173\")\nmcp__playwright__browser_snapshot()\nmcp__playwright__browser_take_screenshot(filename=\"e2e-verification.png\")\n```\n\n## Step 7: Generate Report\n\nOutput structured report:\n\n| Section | Content |\n|---------|---------|\n| Summary | READY / NEEDS ATTENTION / BLOCKED |\n| Code Quality | Lint, types, format checks |\n| Test Results | Pass/fail counts, coverage |\n| Security | Secrets scan, npm/pip audit |\n| Suggestions | Non-blocking improvements |\n| Blockers | Must-fix issues |\n\n## Tools Used\n\n- context7 MCP (library documentation)\n- WebSearch (today's best practices)\n- 3 parallel subagents (backend, frontend, security)\n- Skills (code-review, security, testing)\n- Playwright MCP (E2E verification)\n- Evidence collection (logs, screenshots)\n\n## References\n\n- [Report Template](references/report-template.md)"
              },
              {
                "name": "Webapp Testing",
                "description": "Use when testing web applications with AI-assisted Playwright. Features autonomous test agents for planning, generating, and self-healing tests automatically.",
                "path": ".claude/skills/webapp-testing/SKILL.md",
                "frontmatter": {
                  "name": "Webapp Testing",
                  "description": "Use when testing web applications with AI-assisted Playwright. Features autonomous test agents for planning, generating, and self-healing tests automatically.",
                  "context": "fork",
                  "agent": "test-generator",
                  "version": "1.1.0",
                  "author": "SkillForge AI Agent Hub",
                  "tags": [
                    "playwright",
                    "testing",
                    "e2e",
                    "automation",
                    "agents",
                    2026
                  ],
                  "hooks": {
                    "PostToolUse": [
                      {
                        "matcher": "Write|Edit",
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/test-runner.sh"
                      }
                    ],
                    "Stop": [
                      {
                        "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/skill/coverage-check.sh"
                      }
                    ]
                  }
                },
                "content": "# Webapp Testing Skill\n\n## Overview\n\nAutonomous end-to-end testing with Playwright's three specialized agents for planning, generating, and self-healing tests automatically.\n\n## When to Use\n\n- Setting up E2E testing for web applications\n- Generating tests from user flows\n- Creating self-healing test suites\n- Testing critical user journeys\n\n## The Three Agents\n\n1. **Planner** - Explores app and creates test plans\n2. **Generator** - Writes Playwright tests with best practices\n3. **Healer** - Fixes failing tests automatically\n\n## Quick Setup\n\n```bash\n# 1. Install Playwright\nnpm install --save-dev @playwright/test\n\n# 2. Add MCP server\nclaude mcp add playwright npx '@playwright/mcp@latest'\n\n# 3. Initialize agents\nnpx playwright init-agents --loop=claude\n\n# 4. Create tests/seed.spec.ts (required for Planner)\n```\n\n**Requirements:** VS Code v1.105+ (Oct 9, 2025)\n\n## Agent Workflow\n\n```\n1. PLANNER    Explores app  Creates specs/checkout.md\n                 (uses seed.spec.ts)\n                      \n                      \n2. GENERATOR  Reads spec  Tests live app  Outputs tests/checkout.spec.ts\n                 (verifies selectors actually work)\n                      \n                      \n3. HEALER     Runs tests  Fixes failures  Updates selectors/waits\n                 (self-healing)\n```\n\n## Directory Structure\n\n```\nyour-project/\n specs/               Planner outputs (Markdown plans)\n tests/               Generator outputs (Playwright tests)\n    seed.spec.ts     Required: Planner learns from this\n playwright.config.ts\n```\n\n## Key Concepts\n\n**seed.spec.ts is required** - Planner executes this to learn:\n- Environment setup (fixtures, hooks)\n- Authentication flow\n- Available UI elements\n\n**Generator validates live** - Doesn't just translate Markdown, actually tests app to verify selectors work.\n\n**Healer auto-fixes** - When UI changes break tests, Healer replays, finds new selectors, patches tests.\n\nSee `references/` for detailed agent patterns and commands.\n## Capability Details\n\n### playwright-setup\n**Keywords:** playwright, setup, install, configure, mcp\n**Solves:**\n- How do I set up Playwright testing?\n- Install Playwright MCP server\n- Configure test environment\n- Initialize Playwright agents with Claude\n\n### test-planning\n**Keywords:** test plan, scenarios, user flows, test cases, planner agent\n**Solves:**\n- How do I create a test plan?\n- Use Planner agent to explore app\n- Identify test scenarios automatically\n- Plan user flow testing with seed.spec.ts\n\n### test-generation\n**Keywords:** generate tests, write tests, playwright code, selectors, generator agent\n**Solves:**\n- How do I generate Playwright tests?\n- Use Generator agent to write test code\n- Create semantic locators that validate live\n- Write tests with best practices\n\n### test-healing\n**Keywords:** fix tests, failing tests, self-heal, maintenance, healer agent\n**Solves:**\n- How do I fix failing tests automatically?\n- Use Healer agent to update broken selectors\n- Maintain test suite after UI changes\n- Self-healing test automation\n\n### agent-workflow\n**Keywords:** planner generator healer, test workflow, autonomous testing, playwright agents\n**Solves:**\n- How do the three Playwright agents work together?\n- Complete testing workflow with agents\n- Planner  Generator  Healer pipeline\n- Autonomous test creation and maintenance\n\n### visual-regression\n**Keywords:** visual regression, screenshot, toHaveScreenshot, snapshot, VRT, baseline, pixel diff, visual testing\n**Solves:**\n- How do I set up visual regression testing with Playwright?\n- Replace Percy with Playwright native screenshots\n- Configure toHaveScreenshot thresholds\n- Handle cross-platform screenshot differences\n- Mask dynamic content in screenshots\n- Set up visual regression in CI/CD\n- Update screenshot baselines\n- Debug failed visual comparisons"
              },
              {
                "name": "worktree-coordination",
                "description": "Manage multiple Claude Code instances across git worktrees. Check status, claim/release file locks, sync decisions, and prevent conflicts.",
                "path": ".claude/skills/worktree-coordination/SKILL.md",
                "frontmatter": {
                  "name": "worktree-coordination",
                  "description": "Manage multiple Claude Code instances across git worktrees. Check status, claim/release file locks, sync decisions, and prevent conflicts.",
                  "context": "none",
                  "version": "1.0.0",
                  "author": "SkillForge",
                  "tags": [
                    "coordination",
                    "worktree",
                    "multi-instance",
                    "locking",
                    "parallel-development",
                    2026
                  ]
                },
                "content": "# Worktree Coordination Skill\n\n## Overview\n\nManage multiple Claude Code instances working in parallel across git worktrees with automatic file locking, conflict detection, and decision synchronization.\n\n## When to Use\n\n- Running multiple Claude Code instances simultaneously\n- Working on related features in parallel\n- Coordinating changes across git worktrees\n- Preventing merge conflicts from parallel work\n\n## Commands\n\n### /worktree-status\nShow status of all active Claude Code instances.\n\n**Usage:** `/worktree-status [--json] [--clean]`\n\n**Actions:**\n1. Run `cc-worktree-status` to see all active instances\n2. Check for stale instances (no heartbeat > 5 min)\n3. View file locks across all instances\n\n**Output includes:**\n- Instance ID and branch\n- Current task (if set)\n- Health status (ACTIVE/STALE)\n- Files locked by each instance\n\n### /worktree-claim <file-path>\nExplicitly lock a file for this instance.\n\n**Usage:** `/worktree-claim src/auth/login.ts`\n\n**Actions:**\n1. Check if file is already locked\n2. If locked by another instance, show who holds it\n3. If available, acquire lock\n\n### /worktree-release <file-path>\nRelease lock on a file.\n\n**Usage:** `/worktree-release src/auth/login.ts`\n\n### /worktree-sync\nSync shared context and check for conflicts.\n\n**Usage:** `/worktree-sync [--check-conflicts] [--pull-decisions]`\n\n**Actions:**\n1. `--check-conflicts`: Run merge-tree against other active branches\n2. `--pull-decisions`: Show recent architectural decisions from other instances\n\n### /worktree-decision <decision>\nLog an architectural decision visible to all instances.\n\n**Usage:** `/worktree-decision \"Using Passport.js for OAuth\" --rationale \"Better middleware support\"`\n\n## Automatic Behaviors\n\n### File Lock Check (PreToolUse Hook)\nBefore any Write or Edit operation:\n1. Check if file is locked by another instance\n2. If locked  BLOCK with details about lock holder\n3. If unlocked  Acquire lock and proceed\n\n### Heartbeat (Lifecycle Hook)\nEvery 30 seconds:\n1. Update this instance's heartbeat timestamp\n2. Clean up stale instances (no heartbeat > 5 min)\n3. Release orphaned locks\n\n### Cleanup (Stop Hook)\nWhen Claude Code exits:\n1. Release all file locks held by this instance\n2. Unregister from coordination registry\n\n## File Lock States\n\n```\n\n  FILE: src/auth/oauth.ts                                \n\n  Status: LOCKED                                         \n  Holder: cc-auth-a1b2c3                                 \n  Branch: feature/user-authentication                    \n  Task:   Implementing OAuth2 login flow                 \n  Since:  2 minutes ago                                  \n\n  Action: Wait for release or use /worktree-release     \n\n```\n\n## Registry Schema\n\nLocated at `.claude/coordination/registry.json`:\n\n```json\n{\n  \"instances\": {\n    \"cc-auth-a1b2c3\": {\n      \"worktree\": \"/Users/dev/worktrees/feature-auth\",\n      \"branch\": \"feature/user-authentication\",\n      \"task\": \"Implementing OAuth2\",\n      \"files_locked\": [\"src/auth/oauth.ts\"],\n      \"started\": \"2026-01-08T14:30:00Z\",\n      \"last_heartbeat\": \"2026-01-08T14:45:32Z\"\n    }\n  },\n  \"file_locks\": {\n    \"src/auth/oauth.ts\": {\n      \"instance_id\": \"cc-auth-a1b2c3\",\n      \"acquired_at\": \"2026-01-08T14:35:00Z\",\n      \"reason\": \"edit\"\n    }\n  },\n  \"decisions_log\": [\n    {\n      \"id\": \"dec-001\",\n      \"instance_id\": \"cc-auth-a1b2c3\",\n      \"decision\": \"Use Passport.js for OAuth\",\n      \"rationale\": \"Better middleware support\",\n      \"timestamp\": \"2026-01-08T14:40:00Z\"\n    }\n  ]\n}\n```\n\n## CLI Commands\n\nAvailable in `bin/`:\n\n```bash\n# Create new coordinated worktree\ncc-worktree-new <feature-name> [--base <branch>]\n\n# Check status of all worktrees\ncc-worktree-status [--json] [--clean]\n\n# Sync context and check conflicts\ncc-worktree-sync [--check-conflicts] [--pull-decisions]\n```\n\n## Best Practices\n\n1. **One task per worktree** - Each Claude Code instance should focus on one feature/task\n2. **Claim files early** - Use `/worktree-claim` before starting work on shared files\n3. **Log decisions** - Use `/worktree-decision` for choices that affect other instances\n4. **Check conflicts** - Run `cc-worktree-sync --check-conflicts` before committing\n5. **Clean up** - Exit Claude Code properly to release locks (Ctrl+C or /exit)\n\n## Troubleshooting\n\n### \"File is locked by another instance\"\n1. Check who holds it: `/worktree-status`\n2. If instance is STALE: `cc-worktree-status --clean`\n3. If legitimately held: Coordinate with other instance or work on different files\n\n### \"Instance not registered\"\nThe heartbeat hook will auto-register on first tool use. If issues persist:\n1. Check `.claude-local/instance-id.txt` exists\n2. Verify `.claude/coordination/` is symlinked correctly\n## Capability Details\n\n### status-check\n**Keywords:** worktree, status, instances, active, who\n**Solves:**\n- How to see all active Claude Code instances\n- Check which files are locked\n- Find stale instances\n\n### file-locking\n**Keywords:** lock, claim, release, conflict, blocked\n**Solves:**\n- How to prevent file conflicts between instances\n- Claim a file before editing\n- Release a lock when done\n\n### decision-sync\n**Keywords:** decision, sync, share, coordinate\n**Solves:**\n- Share architectural decisions across instances\n- See what other instances decided\n- Coordinate approach between worktrees\n\n### conflict-prevention\n**Keywords:** conflict, merge, overlap, collision\n**Solves:**\n- Check for merge conflicts before committing\n- Avoid overlapping work\n- Coordinate parallel development"
              }
            ]
          }
        ]
      }
    }
  ]
}