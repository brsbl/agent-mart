{
  "owner": {
    "id": "AdonaiVera",
    "display_name": "Adonai Vera",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/45982251?u=509bde9570734b67e8269d02a3bc674a42c4aa1d&v=4",
    "url": "https://github.com/AdonaiVera",
    "bio": "I’m a Computer Vision engineer, when not fine-tuning algorithms, you might find me skydiving or scuba diving, always eager for new challenges.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 7,
      "total_commands": 0,
      "total_skills": 7,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "AdonaiVera/fiftyone-skills",
      "url": "https://github.com/AdonaiVera/fiftyone-skills",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-11T23:28:52Z",
        "created_at": "2025-12-18T22:53:06Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3120
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 327
        },
        {
          "path": "AGENTS.md",
          "type": "blob",
          "size": 6575
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 10762
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 6255
        },
        {
          "path": "code-style",
          "type": "tree",
          "size": null
        },
        {
          "path": "code-style/plugin.json",
          "type": "blob",
          "size": 489
        },
        {
          "path": "code-style/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "code-style/skills/fiftyone-code-style",
          "type": "tree",
          "size": null
        },
        {
          "path": "code-style/skills/fiftyone-code-style/SKILL.md",
          "type": "blob",
          "size": 3701
        },
        {
          "path": "dataset-import",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-import/plugin.json",
          "type": "blob",
          "size": 807
        },
        {
          "path": "dataset-import/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-import/skills/fiftyone-dataset-import",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-import/skills/fiftyone-dataset-import/SKILL.md",
          "type": "blob",
          "size": 35774
        },
        {
          "path": "dataset-inference",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-inference/plugin.json",
          "type": "blob",
          "size": 546
        },
        {
          "path": "dataset-inference/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-inference/skills/fiftyone-dataset-inference",
          "type": "tree",
          "size": null
        },
        {
          "path": "dataset-inference/skills/fiftyone-dataset-inference/SKILL.md",
          "type": "blob",
          "size": 10833
        },
        {
          "path": "develop-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "develop-plugin/plugin.json",
          "type": "blob",
          "size": 670
        },
        {
          "path": "develop-plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin/JAVASCRIPT-PANEL.md",
          "type": "blob",
          "size": 13668
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin/PLUGIN-STRUCTURE.md",
          "type": "blob",
          "size": 2810
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin/PYTHON-OPERATOR.md",
          "type": "blob",
          "size": 12461
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin/PYTHON-PANEL.md",
          "type": "blob",
          "size": 12713
        },
        {
          "path": "develop-plugin/skills/fiftyone-develop-plugin/SKILL.md",
          "type": "blob",
          "size": 5370
        },
        {
          "path": "embeddings-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "embeddings-visualization/plugin.json",
          "type": "blob",
          "size": 546
        },
        {
          "path": "embeddings-visualization/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "embeddings-visualization/skills/fiftyone-embeddings-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "embeddings-visualization/skills/fiftyone-embeddings-visualization/SKILL.md",
          "type": "blob",
          "size": 16424
        },
        {
          "path": "find-duplicates",
          "type": "tree",
          "size": null
        },
        {
          "path": "find-duplicates/plugin.json",
          "type": "blob",
          "size": 475
        },
        {
          "path": "find-duplicates/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "find-duplicates/skills/fiftyone-find-duplicates",
          "type": "tree",
          "size": null
        },
        {
          "path": "find-duplicates/skills/fiftyone-find-duplicates/SKILL.md",
          "type": "blob",
          "size": 9315
        },
        {
          "path": "pr-triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "pr-triage/plugin.json",
          "type": "blob",
          "size": 512
        },
        {
          "path": "pr-triage/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "pr-triage/skills/fiftyone-pr-triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "pr-triage/skills/fiftyone-pr-triage/SKILL.md",
          "type": "blob",
          "size": 4563
        }
      ],
      "marketplace": {
        "name": "fiftyone-skills",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Voxel51"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "fiftyone-dataset-import",
            "description": "Universal dataset import for FiftyOne supporting all media types (images, videos, point clouds, 3D scenes), all label formats (COCO, YOLO, VOC, CVAT, KITTI, etc.), and multimodal grouped datasets. Use when users want to import any dataset regardless of format, handle autonomous driving data with multiple cameras and LiDAR, or create grouped datasets from multimodal data.",
            "source": "./dataset-import",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-dataset-import@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-dataset-import",
                "description": "Universal dataset import for FiftyOne supporting all media types (images, videos, point clouds, 3D scenes), all label formats (COCO, YOLO, VOC, CVAT, KITTI, etc.), and multimodal grouped datasets. Use when users want to import any dataset regardless of format, automatically detect folder structure, handle autonomous driving data with multiple cameras and LiDAR, or create grouped datasets from multimodal data. Requires FiftyOne MCP server.",
                "path": "dataset-import/skills/fiftyone-dataset-import/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-dataset-import",
                  "description": "Universal dataset import for FiftyOne supporting all media types (images, videos, point clouds, 3D scenes), all label formats (COCO, YOLO, VOC, CVAT, KITTI, etc.), and multimodal grouped datasets. Use when users want to import any dataset regardless of format, automatically detect folder structure, handle autonomous driving data with multiple cameras and LiDAR, or create grouped datasets from multimodal data. Requires FiftyOne MCP server."
                },
                "content": "# Universal Dataset Import for FiftyOne\n\n## Overview\n\nImport any dataset into FiftyOne regardless of media type, label format, or folder structure. Automatically detects and handles:\n\n- **All media types**: images, videos, point clouds, 3D scenes\n- **All label formats**: COCO, YOLO, VOC, CVAT, KITTI, OpenLABEL, and more\n- **Multimodal groups**: Multiple cameras + LiDAR per scene (autonomous driving, robotics)\n- **Complex folder structures**: Nested directories, scene-based organization\n\n**Use this skill when:**\n- Importing datasets from any source or format\n- Working with autonomous driving data (multiple cameras, LiDAR, radar)\n- Loading multimodal data that needs grouping\n- The user doesn't know or specify the exact format\n- Importing point clouds, 3D scenes, or mixed media types\n\n## Prerequisites\n\n- FiftyOne MCP server installed and running\n- `@voxel51/io` plugin for importing data\n- `@voxel51/utils` plugin for dataset management\n\n## Key Directives\n\n**ALWAYS follow these rules:**\n\n### 1. Scan folder FIRST\nBefore any import, deeply scan the directory to understand its structure:\n```bash\n# Use bash to explore\nfind /path/to/data -type f | head -50\nls -la /path/to/data\n```\n\n### 2. Auto-detect everything\nDetect media types, label formats, and grouping patterns automatically. Never ask the user to specify format if it can be inferred.\n\n### 3. Detect multimodal groups\nLook for patterns that indicate grouped data:\n- Scene folders containing multiple media files\n- Filename patterns with common prefixes (e.g., `scene_001_left.jpg`, `scene_001_right.jpg`)\n- Mixed media types that should be grouped (images + point clouds)\n\n### 4. Detect and install required packages\nMany specialized dataset formats require external Python packages. After detecting the format:\n\n1. **Identify required packages** based on the detected format\n2. **Check if packages are installed** using `pip show <package>`\n3. **Search for installation instructions** if needed (use web search or FiftyOne docs)\n4. **Ask user for permission** before installing any packages\n5. **Install required packages** (see installation methods below)\n6. **Verify installation** before proceeding\n\n**Common format-to-package mappings:**\n\n| Dataset Format | Package Name | Install Command |\n|---------------|--------------|-----------------|\n| PandaSet | `pandaset` | `pip install \"git+https://github.com/scaleapi/pandaset-devkit.git#subdirectory=python\"` |\n| nuScenes | `nuscenes-devkit` | `pip install nuscenes-devkit` |\n| Waymo Open | `waymo-open-dataset-tf` | See Waymo docs (requires TensorFlow) |\n| Argoverse 2 | `av2` | `pip install av2` |\n| KITTI 3D | `pykitti` | `pip install pykitti` |\n| Lyft L5 | `l5kit` | `pip install l5kit` |\n| A2D2 | `a2d2` | See Audi A2D2 docs |\n\n**Additional packages for 3D processing:**\n\n| Purpose | Package Name | Install Command |\n|---------|--------------|-----------------|\n| Point cloud conversion to PCD | `open3d` | `pip install open3d` |\n| Point cloud processing | `pyntcloud` | `pip install pyntcloud` |\n| LAS/LAZ point clouds | `laspy` | `pip install laspy` |\n\n**Installation methods (in order of preference):**\n\n1. **PyPI** - Standard pip install:\n   ```bash\n   pip install <package-name>\n   ```\n\n2. **GitHub URL** - When package is not on PyPI:\n   ```bash\n   # Standard GitHub install\n   pip install \"git+https://github.com/<org>/<repo>.git\"\n\n   # With subdirectory (for monorepos)\n   pip install \"git+https://github.com/<org>/<repo>.git#subdirectory=python\"\n\n   # Specific branch or tag\n   pip install \"git+https://github.com/<org>/<repo>.git@v1.0.0\"\n   ```\n\n3. **Clone and install** - For complex builds:\n   ```bash\n   git clone https://github.com/<org>/<repo>.git\n   cd <repo>\n   pip install .\n   ```\n\n**Dynamic package discovery workflow:**\n\nIf the format is not in the table above:\n1. **Search PyPI** for `<format-name>`, `<format-name>-devkit`, or `<format-name>-sdk`\n2. **Search GitHub** for `<format-name> devkit` or `<format-name> python`\n3. **Search web** for \"FiftyOne import <format-name>\" or \"<format-name> python tutorial\"\n4. **Check the dataset's official website** for developer tools/SDK\n5. **Present findings to user** with installation options\n\n**After installation:**\n1. **Verify** the package is installed: `pip show <package-name>`\n2. **Test import** in Python: `python -c \"from <package> import ...\"`\n3. **Search for FiftyOne integration** examples or write custom import code\n\n### 5. Confirm before importing\nPresent findings to user and **explicitly ask for confirmation** before creating the dataset.\nAlways end your scan summary with a clear question like:\n- \"Proceed with import?\"\n- \"Should I create the dataset with these settings?\"\n\n**Wait for user response before proceeding.** Do not create the dataset until the user confirms.\n\n### 6. Check for existing datasets\nBefore creating a dataset, check if the proposed name already exists:\n```python\nlist_datasets()\n```\nIf the dataset name exists, ask the user:\n- **Overwrite**: Delete existing and create new\n- **Rename**: Use a different name (suggest alternatives like `dataset-name-v2`)\n- **Abort**: Cancel the import\n\n### 7. Validate after import\nCompare imported sample count with source file count. Report any discrepancies.\n\n### 8. Report errors minimally to user\nKeep error messages simple for the user. Use detailed error info internally to diagnose issues.\n\n## Complete Workflow\n\n### Step 1: Deep Folder Scan\n\nScan the target directory to understand its structure:\n\n```bash\n# Count files by extension\nfind /path/to/data -type f | sed 's/.*\\.//' | sort | uniq -c | sort -rn\n\n# List directory structure (2 levels deep)\nfind /path/to/data -maxdepth 2 -type d\n\n# Sample some files\nls -la /path/to/data/* | head -20\n\n# IMPORTANT: Scan for ALL annotation/label directories\nls -la /path/to/data/annotations/ 2>/dev/null || ls -la /path/to/data/labels/ 2>/dev/null\n```\n\nBuild an inventory of:\n- Media files by type (images, videos, point clouds, 3D)\n- Label files by format (JSON, XML, TXT, YAML, PKL)\n- Directory structure (flat vs nested vs scene-based)\n- **ALL annotation types present** (cuboids, segmentation, tracking, etc.)\n\n**For 3D/Autonomous Driving datasets, specifically check:**\n```bash\n# List all annotation subdirectories\nfind /path/to/data -type d -name \"annotations\" -o -name \"labels\" | xargs -I {} ls -la {}\n\n# Sample an annotation file to understand its structure\npython3 -c \"import pickle, gzip; print(pickle.load(gzip.open('path/to/annotation.pkl.gz', 'rb'))[:2])\"\n```\n\n### Step 2: Identify Media Types\n\nClassify files by extension:\n\n| Extensions | Media Type | FiftyOne Type |\n|------------|------------|---------------|\n| `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.webp`, `.tiff` | Image | `image` |\n| `.mp4`, `.avi`, `.mov`, `.mkv`, `.webm` | Video | `video` |\n| `.pcd`, `.ply`, `.las`, `.laz` | Point Cloud | `point-cloud` |\n| `.fo3d`, `.obj`, `.gltf`, `.glb` | 3D Scene | `3d` |\n\n### Step 3: Detect Label Format\n\nIdentify label format from file patterns:\n\n| Pattern | Format | Dataset Type |\n|---------|--------|--------------|\n| `annotations.json` or `instances*.json` with COCO structure | COCO | `COCO` |\n| `*.xml` files with Pascal VOC structure | VOC | `VOC` |\n| `*.txt` per image + `classes.txt` | YOLOv4 | `YOLOv4` |\n| `data.yaml` + `labels/*.txt` | YOLOv5 | `YOLOv5` |\n| `*.txt` per image (KITTI format) | KITTI | `KITTI` |\n| Single `annotations.xml` (CVAT format) | CVAT | `CVAT Image` |\n| `*.json` with OpenLABEL structure | OpenLABEL | `OpenLABEL Image` |\n| Folder-per-class structure | Classification | `Image Classification Directory Tree` |\n| `*.csv` with filepath column | CSV | `CSV` |\n| `*.json` with GeoJSON structure | GeoJSON | `GeoJSON` |\n| `.dcm` DICOM files | DICOM | `DICOM` |\n| `.tiff` with geo metadata | GeoTIFF | `GeoTIFF` |\n\n**Specialized Autonomous Driving Formats (require external packages):**\n\n| Directory Pattern | Format | Required Package |\n|------------------|--------|------------------|\n| `camera/`, `lidar/`, `annotations/cuboids/` with `.pkl.gz` | PandaSet | `pandaset-devkit` |\n| `samples/`, `sweeps/`, `v1.0-*` folders | nuScenes | `nuscenes-devkit` |\n| `segment-*` with `.tfrecord` files | Waymo Open | `waymo-open-dataset-tf` |\n| `argoverse-tracking/` structure | Argoverse | `argoverse-api` |\n| `training/`, `testing/` with `calib/`, `velodyne/` | KITTI 3D | `pykitti` |\n| `scenes/`, `aerial_map/` | Lyft L5 | `l5kit` |\n\n### Step 4: Detect Required Packages\n\nAfter identifying the format, check if external packages are needed:\n\n```bash\n# Check if package is installed (use the actual package name, not repo name)\npip show pandaset\n\n# If not found, the package needs to be installed\n```\n\n**If packages are required:**\n\n1. **Inform user** what packages are needed and why\n\n2. **Search for installation method** if not in the common mappings table:\n   - Search PyPI first: `pip search <package>` or check pypi.org\n   - Search GitHub for the devkit/SDK repository\n   - Check the dataset's official documentation\n   - Search web: \"<dataset-name> python install\"\n\n3. **Ask for permission** to install:\n   ```\n   This dataset appears to be in PandaSet format, which requires the `pandaset` package.\n\n   The package is not on PyPI and must be installed from GitHub:\n   pip install \"git+https://github.com/scaleapi/pandaset-devkit.git#subdirectory=python\"\n\n   Would you like me to:\n   - Install the package (recommended)\n   - Search for alternative import methods\n   - Abort and let you install manually\n   ```\n\n4. **Install using the appropriate method**:\n   ```bash\n   # PyPI (if available)\n   pip install <package-name>\n\n   # GitHub URL (if not on PyPI)\n   pip install \"git+https://github.com/<org>/<repo>.git#subdirectory=python\"\n\n   # Clone and install (for complex builds)\n   git clone https://github.com/<org>/<repo>.git && cd <repo> && pip install .\n   ```\n\n5. **Verify installation**:\n   ```bash\n   pip show <package-name>\n   ```\n\n6. **Test the import** in Python:\n   ```bash\n   python -c \"from <package> import <main_class>; print('OK')\"\n   ```\n\n7. **Search for FiftyOne integration code**:\n   - Search: \"FiftyOne <format-name> import example\"\n   - Search: \"<format-name> to FiftyOne grouped dataset\"\n   - Check FiftyOne docs for similar dataset types\n   - If no examples exist, build custom import code using the devkit API\n\n### Step 5: Detect Grouping Pattern\n\nDetermine if data should be grouped:\n\n**Pattern A: Scene Folders (Most Common for Multimodal)**\n```\n/data/\n├── scene_001/\n│   ├── left.jpg\n│   ├── right.jpg\n│   ├── lidar.pcd\n│   └── labels.json\n├── scene_002/\n│   └── ...\n```\nDetection: Each subfolder = one group, files inside = slices\n\n**Pattern B: Filename Prefix**\n```\n/data/\n├── 001_left.jpg\n├── 001_right.jpg\n├── 001_lidar.pcd\n├── 002_left.jpg\n├── 002_right.jpg\n├── 002_lidar.pcd\n```\nDetection: Common prefix = group ID, suffix = slice name\n\n**Pattern C: No Grouping (Flat)**\n```\n/data/\n├── image_001.jpg\n├── image_002.jpg\n├── image_003.jpg\n```\nDetection: Single media type, no clear grouping pattern\n\n### Step 6: Present Findings to User\n\nBefore importing, present a clear summary that includes **ALL detected labels**:\n\n```\nScan Results for /path/to/data:\n\nMedia Found:\n  - 3,000 images (.jpg, .png)\n  - 1,000 point clouds (.pkl.gz → will convert to .pcd)\n  - 0 videos\n\nGrouping Detected:\n  - Pattern: Scene folders\n  - Groups: 1,000 scenes\n  - Slices: left (image), right (image), front (image), lidar (point-cloud)\n\nALL Labels Detected:\n  ├── cuboids/           (3D bounding boxes, 1,000 files)\n  │   └── Format: pickle, Fields: label, position, dimensions, rotation, track_id\n  ├── semseg/            (Semantic segmentation, 1,000 files)\n  │   └── Format: pickle, point-wise class labels\n  └── instances.json     (2D detections, COCO format)\n      └── Classes: 10 (car, pedestrian, cyclist, ...)\n\nRequired Packages:\n  - ✅ pandaset (installed)\n  - ⚠️ open3d (needed for PCD conversion) → pip install open3d\n\nProposed Configuration:\n  - Dataset name: my-dataset\n  - Type: Grouped (multimodal)\n  - Default slice: front_camera\n  - Labels to import:\n    - detections_3d (from cuboids/)\n    - point_labels (from semseg/)\n    - detections (from instances.json)\n\nProceed with import? (yes/no)\n```\n\n**IMPORTANT:**\n- List ALL annotation types found during the scan\n- Show the format/structure of each label type\n- Indicate which labels will be imported and how\n- Wait for user confirmation before proceeding\n\n### Step 7: Check for Existing Dataset\n\nBefore creating, check if the dataset name already exists:\n\n```python\n# Check existing datasets\nlist_datasets()\n```\n\nIf the proposed dataset name exists in the list:\n1. Inform the user: \"A dataset named 'my-dataset' already exists with X samples.\"\n2. Ask for their preference:\n   - **Overwrite**: Delete existing dataset first\n   - **Rename**: Suggest alternatives (e.g., `my-dataset-v2`, `my-dataset-20240107`)\n   - **Abort**: Cancel the import\n\nIf user chooses to overwrite:\n```python\n# Delete existing dataset\nset_context(dataset_name=\"my-dataset\")\nexecute_operator(\n    operator_uri=\"@voxel51/utils/delete_dataset\",\n    params={\"name\": \"my-dataset\"}\n)\n```\n\n### Step 8: Create Dataset\n\n```python\n# Create the dataset\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\n        \"name\": \"my-dataset\",\n        \"persistent\": true\n    }\n)\n\n# Set context\nset_context(dataset_name=\"my-dataset\")\n```\n\n### Step 9A: Import Simple Dataset (No Groups)\n\nFor flat datasets without grouping:\n\n```python\n# Import media only\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"DIRECTORY\",\n        \"directory\": {\"absolute_path\": \"/path/to/images\"}\n    }\n)\n\n# Import with labels\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"COCO\",\n        \"data_path\": {\"absolute_path\": \"/path/to/images\"},\n        \"labels_path\": {\"absolute_path\": \"/path/to/annotations.json\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n```\n\n### Step 9B: Import Grouped Dataset (Multimodal)\n\nFor multimodal data with groups, use Python directly. Guide the user:\n\n```python\nimport fiftyone as fo\n\n# Create dataset\ndataset = fo.Dataset(\"multimodal-dataset\", persistent=True)\n\n# Add group field\ndataset.add_group_field(\"group\", default=\"front\")\n\n# Create samples for each group\nimport os\nfrom pathlib import Path\n\ndata_dir = Path(\"/path/to/data\")\nsamples = []\n\nfor scene_dir in sorted(data_dir.iterdir()):\n    if not scene_dir.is_dir():\n        continue\n\n    # Create a group for this scene\n    group = fo.Group()\n\n    # Add each file as a slice\n    for file in scene_dir.iterdir():\n        if file.suffix in ['.jpg', '.png']:\n            # Determine slice name from filename\n            slice_name = file.stem  # e.g., \"left\", \"right\", \"front\"\n            samples.append(fo.Sample(\n                filepath=str(file),\n                group=group.element(slice_name)\n            ))\n        elif file.suffix == '.pcd':\n            samples.append(fo.Sample(\n                filepath=str(file),\n                group=group.element(\"lidar\")\n            ))\n        elif file.suffix == '.mp4':\n            samples.append(fo.Sample(\n                filepath=str(file),\n                group=group.element(\"video\")\n            ))\n\n# Add all samples\ndataset.add_samples(samples)\nprint(f\"Added {len(dataset)} samples in {len(dataset.distinct('group.id'))} groups\")\n```\n\n### Step 9C: Import Specialized Format Dataset (3D/Autonomous Driving)\n\nFor datasets requiring external packages (PandaSet, nuScenes, etc.), use the devkit to load data and convert to FiftyOne format.\n\n**General approach:**\n1. Search FiftyOne documentation or web for the specific import method\n2. Use the devkit to load the raw data\n3. **Convert point clouds to PCD format** (FiftyOne requires `.pcd` files)\n4. **Create `fo.Scene` objects** for 3D visualization with point clouds\n5. Convert to FiftyOne samples with proper grouping\n6. **Import ALL detected labels** (cuboids, segmentation, etc.) found during scan\n\n#### Converting Point Clouds to PCD\n\nMany autonomous driving datasets store LiDAR data in proprietary formats (`.pkl.gz`, `.bin`, `.npy`). Convert to PCD for FiftyOne:\n\n```python\nimport numpy as np\nimport open3d as o3d\nfrom pathlib import Path\n\ndef convert_to_pcd(points, output_path):\n    \"\"\"\n    Convert point cloud array to PCD file.\n\n    Args:\n        points: numpy array of shape (N, 3) or (N, 4) with XYZ or XYZI\n        output_path: path to save .pcd file\n    \"\"\"\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n\n    # If intensity is available, store as colors (grayscale)\n    if points.shape[1] >= 4:\n        intensity = points[:, 3]\n        intensity_normalized = (intensity - intensity.min()) / (intensity.max() - intensity.min() + 1e-8)\n        colors = np.stack([intensity_normalized] * 3, axis=1)\n        pcd.colors = o3d.utility.Vector3dVector(colors)\n\n    o3d.io.write_point_cloud(str(output_path), pcd)\n    return output_path\n```\n\n**Note:** Install open3d if needed: `pip install open3d`\n\n#### Creating fo.Scene for 3D Visualization\n\nFor each LiDAR frame, create an `fo.Scene` that references the PCD file:\n\n```python\nimport fiftyone as fo\n\n# Create a 3D scene for the point cloud\nscene = fo.Scene()\n\n# Add point cloud to the scene\nscene.add_point_cloud(\n    name=\"lidar\",\n    pcd_path=\"/path/to/frame.pcd\",\n    flag_for_projection=True  # Enable projection to camera views\n)\n\n# Create sample with the scene\nsample = fo.Sample(filepath=\"/path/to/scene.fo3d\")  # Or use scene directly\nsample[\"scene\"] = scene\n```\n\n#### Importing ALL Labels Detected During Scan\n\nDuring the folder scan (Step 1), identify ALL label types present:\n\n```bash\n# Example: List all annotation directories/files\nls -la /path/to/dataset/annotations/\n# Output might show: cuboids/, semseg/, tracking/, instances.json, etc.\n```\n\n**Map detected labels to FiftyOne label types:**\n\n| Annotation Type | FiftyOne Label Type | Field Name |\n|-----------------|---------------------|------------|\n| 3D Cuboids/Bounding Boxes | `fo.Detection` with 3D attributes | `detections_3d` |\n| Semantic Segmentation | `fo.Segmentation` | `segmentation` |\n| Instance Segmentation | `fo.Detections` with masks | `instances` |\n| Tracking IDs | Add `track_id` to detections | `tracks` |\n| Classification | `fo.Classification` | `classification` |\n| Keypoints/Pose | `fo.Keypoints` | `keypoints` |\n\n**Example: PandaSet Full Import with Labels**\n\n```python\nimport fiftyone as fo\nimport numpy as np\nimport open3d as o3d\nfrom pathlib import Path\nimport gzip\nimport pickle\n\ndata_path = Path(\"/path/to/pandaset\")\npcd_output_dir = data_path / \"pcd_converted\"\npcd_output_dir.mkdir(exist_ok=True)\n\n# Create dataset with groups\ndataset = fo.Dataset(\"pandaset\", persistent=True)\ndataset.add_group_field(\"group\", default=\"front_camera\")\n\n# Get camera names\ncamera_names = [d.name for d in (data_path / \"camera\").iterdir() if d.is_dir()]\nframe_count = len(list((data_path / \"camera\" / \"front_camera\").glob(\"*.jpg\")))\n\n# Check what labels exist\nlabels_dir = data_path / \"annotations\"\navailable_labels = [d.name for d in labels_dir.iterdir() if d.is_dir()]\nprint(f\"Found label types: {available_labels}\")  # e.g., ['cuboids', 'semseg']\n\nsamples = []\nfor frame_idx in range(frame_count):\n    frame_id = f\"{frame_idx:02d}\"\n    group = fo.Group()\n\n    # === Add camera images ===\n    for cam_name in camera_names:\n        img_path = data_path / \"camera\" / cam_name / f\"{frame_id}.jpg\"\n        if img_path.exists():\n            sample = fo.Sample(filepath=str(img_path))\n            sample[\"group\"] = group.element(cam_name)\n            sample[\"frame_idx\"] = frame_idx\n            samples.append(sample)\n\n    # === Convert and add LiDAR point cloud ===\n    lidar_pkl = data_path / \"lidar\" / f\"{frame_id}.pkl.gz\"\n    if lidar_pkl.exists():\n        # Load pickle\n        with gzip.open(lidar_pkl, 'rb') as f:\n            lidar_data = pickle.load(f)\n\n        # Extract points (adjust based on actual data structure)\n        if isinstance(lidar_data, dict):\n            points = lidar_data.get('points', lidar_data.get('data'))\n        else:\n            points = np.array(lidar_data)\n\n        # Convert to PCD\n        pcd_path = pcd_output_dir / f\"{frame_id}.pcd\"\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n        o3d.io.write_point_cloud(str(pcd_path), pcd)\n\n        # Create 3D sample with scene\n        lidar_sample = fo.Sample(filepath=str(pcd_path))\n        lidar_sample[\"group\"] = group.element(\"lidar\")\n        lidar_sample[\"frame_idx\"] = frame_idx\n\n        # === Load 3D cuboid labels if available ===\n        # IMPORTANT: Store 3D attributes as flat scalar fields, NOT lists\n        # Using lists (e.g., location=[x,y,z]) causes \"Symbol.iterator\" errors in 3D viewer\n        if \"cuboids\" in available_labels:\n            cuboids_pkl = labels_dir / \"cuboids\" / f\"{frame_id}.pkl.gz\"\n            if cuboids_pkl.exists():\n                with gzip.open(cuboids_pkl, 'rb') as f:\n                    cuboids_df = pickle.load(f)  # PandaSet uses pandas DataFrame\n\n                detections = []\n                for _, row in cuboids_df.iterrows():\n                    detection = fo.Detection(\n                        label=row.get(\"label\", \"object\"),\n                        bounding_box=[0, 0, 0.01, 0.01],  # minimal 2D placeholder\n                    )\n                    # Store 3D attributes as FLAT SCALAR fields (not lists!)\n                    detection[\"pos_x\"] = float(row.get(\"position.x\", 0))\n                    detection[\"pos_y\"] = float(row.get(\"position.y\", 0))\n                    detection[\"pos_z\"] = float(row.get(\"position.z\", 0))\n                    detection[\"dim_x\"] = float(row.get(\"dimensions.x\", 1))\n                    detection[\"dim_y\"] = float(row.get(\"dimensions.y\", 1))\n                    detection[\"dim_z\"] = float(row.get(\"dimensions.z\", 1))\n                    detection[\"yaw\"] = float(row.get(\"yaw\", 0))\n                    detection[\"track_id\"] = str(row.get(\"uuid\", \"\"))\n                    detection[\"stationary\"] = bool(row.get(\"stationary\", False))\n                    detections.append(detection)\n\n                lidar_sample[\"ground_truth\"] = fo.Detections(detections=detections)\n\n        # === Load semantic segmentation if available ===\n        if \"semseg\" in available_labels:\n            semseg_pkl = labels_dir / \"semseg\" / f\"{frame_id}.pkl.gz\"\n            if semseg_pkl.exists():\n                with gzip.open(semseg_pkl, 'rb') as f:\n                    semseg_data = pickle.load(f)\n                # Store as custom field (point-wise labels)\n                lidar_sample[\"point_labels\"] = semseg_data.tolist() if hasattr(semseg_data, 'tolist') else semseg_data\n\n        samples.append(lidar_sample)\n\n# Add all samples\ndataset.add_samples(samples)\ndataset.save()\n\nprint(f\"Imported {len(dataset)} groups with {len(dataset.select_group_slices())} total samples\")\nprint(f\"Slices: {dataset.group_slices}\")\nprint(f\"Labels imported: {available_labels}\")\n```\n\n**Dynamic Import Discovery:**\nIf no example exists for the format:\n1. Search: \"FiftyOne <format-name> import example\"\n2. Search: \"<format-name> devkit python example\"\n3. Read the devkit documentation to understand data structure\n4. Explore the annotation files to understand label format:\n   ```python\n   import pickle, gzip\n   with gzip.open(\"annotations/cuboids/00.pkl.gz\", \"rb\") as f:\n       data = pickle.load(f)\n   print(type(data), data[0] if isinstance(data, list) else data)\n   ```\n5. Build custom import code based on the devkit API and label structure\n\n### Step 10: Import Additional Labels (Optional)\n\nIf labels weren't imported with the specialized format, add them separately:\n\n```python\n# For COCO labels that reference filepaths\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"LABELS_ONLY\",\n        \"dataset_type\": \"COCO\",\n        \"labels_path\": {\"absolute_path\": \"/path/to/annotations.json\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n```\n\n### Step 11: Validate Import\n\n```python\n# Load and verify\nload_dataset(name=\"my-dataset\")\n\n# Check counts match\ndataset_summary(name=\"my-dataset\")\n```\n\nCompare:\n- Imported samples vs source files\n- Groups created vs expected\n- Labels imported vs annotation count\n\n### Step 12: Launch App and View\n\n```python\nlaunch_app(dataset_name=\"my-dataset\")\n\n# For grouped datasets, view different slices\n# In the App, use the slice selector dropdown\n```\n\n## Supported Dataset Types\n\n### Media Types\n\n| Type | Extensions | Description |\n|------|------------|-------------|\n| `image` | `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.webp`, `.tiff` | Static images |\n| `video` | `.mp4`, `.avi`, `.mov`, `.mkv`, `.webm` | Video files with frames |\n| `point-cloud` | `.pcd`, `.ply`, `.las`, `.laz` | 3D point cloud data |\n| `3d` | `.fo3d`, `.obj`, `.gltf`, `.glb` | 3D scenes and meshes |\n\n### Label Formats\n\n| Format | Dataset Type Value | Label Types | File Pattern |\n|--------|-------------------|-------------|--------------|\n| COCO | `COCO` | detections, segmentations, keypoints | `*.json` |\n| VOC/Pascal | `VOC` | detections | `*.xml` per image |\n| KITTI | `KITTI` | detections | `*.txt` per image |\n| YOLOv4 | `YOLOv4` | detections | `*.txt` + `classes.txt` |\n| YOLOv5 | `YOLOv5` | detections | `data.yaml` + `labels/*.txt` |\n| CVAT Image | `CVAT Image` | classifications, detections, polylines, keypoints | Single `*.xml` |\n| CVAT Video | `CVAT Video` | frame labels | XML directory |\n| OpenLABEL Image | `OpenLABEL Image` | all types | `*.json` directory |\n| OpenLABEL Video | `OpenLABEL Video` | all types | `*.json` directory |\n| TF Object Detection | `TF Object Detection` | detections | TFRecords |\n| TF Image Classification | `TF Image Classification` | classification | TFRecords |\n| Image Classification Tree | `Image Classification Directory Tree` | classification | Folder per class |\n| Video Classification Tree | `Video Classification Directory Tree` | classification | Folder per class |\n| Image Segmentation | `Image Segmentation` | segmentation | Mask images |\n| CSV | `CSV` | custom fields | `*.csv` |\n| DICOM | `DICOM` | medical metadata | `.dcm` files |\n| GeoJSON | `GeoJSON` | geolocation | `*.json` |\n| GeoTIFF | `GeoTIFF` | geolocation | `.tiff` with geo |\n| FiftyOne Dataset | `FiftyOne Dataset` | all types | Exported format |\n\n## Common Use Cases\n\n### Use Case 1: Simple Image Dataset with COCO Labels\n\n```python\n# Scan directory\n# Found: 5000 images, annotations.json (COCO format)\n\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"coco-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"coco-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"COCO\",\n        \"data_path\": {\"absolute_path\": \"/path/to/images\"},\n        \"labels_path\": {\"absolute_path\": \"/path/to/annotations.json\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nlaunch_app(dataset_name=\"coco-dataset\")\n```\n\n### Use Case 2: YOLO Dataset\n\n```python\n# Scan directory\n# Found: data.yaml, images/, labels/ (YOLOv5 format)\n\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"yolo-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"yolo-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"YOLOv5\",\n        \"dataset_dir\": {\"absolute_path\": \"/path/to/yolo/dataset\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nlaunch_app(dataset_name=\"yolo-dataset\")\n```\n\n### Use Case 3: Point Cloud Dataset\n\n```python\n# Scan directory\n# Found: 1000 .pcd files, labels/ with KITTI format\n\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"lidar-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"lidar-dataset\")\n\n# Import point clouds\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"GLOB_PATTERN\",\n        \"glob_patt\": {\"absolute_path\": \"/path/to/data/*.pcd\"}\n    }\n)\n\nlaunch_app(dataset_name=\"lidar-dataset\")\n```\n\n### Use Case 4: Autonomous Driving (Multimodal Groups)\n\nThis is the most complex case - multiple cameras + LiDAR per scene:\n\n```python\nimport fiftyone as fo\nfrom pathlib import Path\n\n# Create dataset with group support\ndataset = fo.Dataset(\"driving-dataset\", persistent=True)\ndataset.add_group_field(\"group\", default=\"front_camera\")\n\ndata_dir = Path(\"/path/to/driving_data\")\nsamples = []\n\n# Process each scene folder\nfor scene_dir in sorted(data_dir.iterdir()):\n    if not scene_dir.is_dir():\n        continue\n\n    group = fo.Group()\n\n    # Map files to slices\n    slice_mapping = {\n        \"front\": \"front_camera\",\n        \"left\": \"left_camera\",\n        \"right\": \"right_camera\",\n        \"rear\": \"rear_camera\",\n        \"lidar\": \"lidar\",\n        \"radar\": \"radar\"\n    }\n\n    for file in scene_dir.iterdir():\n        # Determine slice from filename\n        for key, slice_name in slice_mapping.items():\n            if key in file.stem.lower():\n                samples.append(fo.Sample(\n                    filepath=str(file),\n                    group=group.element(slice_name)\n                ))\n                break\n\ndataset.add_samples(samples)\ndataset.save()\n\nprint(f\"Created {len(dataset.distinct('group.id'))} groups\")\nprint(f\"Slices: {dataset.group_slices}\")\nprint(f\"Media types: {dataset.group_media_types}\")\n\n# Launch app\nsession = fo.launch_app(dataset)\n```\n\n### Use Case 5: Classification Directory Tree\n\n```python\n# Scan directory\n# Found: cats/, dogs/, birds/ folders with images inside\n\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"classification-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"classification-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"Image Classification Directory Tree\",\n        \"dataset_dir\": {\"absolute_path\": \"/path/to/classification\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nlaunch_app(dataset_name=\"classification-dataset\")\n```\n\n### Use Case 6: Mixed Media (Images + Videos)\n\n```python\n# Scan directory\n# Found: images/, videos/ folders\n\n# Create dataset\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"mixed-media\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"mixed-media\")\n\n# Import images\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"DIRECTORY\",\n        \"directory\": {\"absolute_path\": \"/path/to/images\"},\n        \"tags\": [\"image\"]\n    }\n)\n\n# Import videos\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"DIRECTORY\",\n        \"directory\": {\"absolute_path\": \"/path/to/videos\"},\n        \"tags\": [\"video\"]\n    }\n)\n\nlaunch_app(dataset_name=\"mixed-media\")\n```\n\n## Working with Groups\n\n### Understanding Group Structure\n\nIn a grouped dataset:\n- Each **group** represents one scene/moment (e.g., one timestamp)\n- Each **slice** represents one modality (e.g., left camera, lidar)\n- All samples in a group share the same `group.id`\n- Each sample has a `group.name` indicating its slice\n\n```python\n# Access group information\nprint(dataset.group_slices)        # ['front_camera', 'left_camera', 'lidar']\nprint(dataset.group_media_types)   # {'front_camera': 'image', 'lidar': 'point-cloud'}\nprint(dataset.default_group_slice) # 'front_camera'\n\n# Iterate over groups\nfor group in dataset.iter_groups():\n    print(f\"Group has {len(group)} slices\")\n    for slice_name, sample in group.items():\n        print(f\"  {slice_name}: {sample.filepath}\")\n\n# Get specific slice view\nfront_images = dataset.select_group_slices(\"front_camera\")\nall_point_clouds = dataset.select_group_slices(media_type=\"point-cloud\")\n```\n\n### Viewing Groups in the App\n\nAfter launching the app:\n1. The slice selector dropdown appears in the top bar\n2. Select different slices to view each modality\n3. Samples are synchronized - selecting a sample shows all its group members\n4. Use the grid view to see multiple slices side by side\n\n## Troubleshooting\n\n**Error: \"Dataset already exists\"**\n- Use a different dataset name\n- Or delete existing: `execute_operator(\"@voxel51/utils/delete_dataset\", {\"name\": \"dataset-name\"})`\n\n**Error: \"No samples found\"**\n- Verify directory path is correct and accessible\n- Check file extensions are supported\n- For nested directories, ensure recursive scanning\n\n**Error: \"Labels path not found\"**\n- Verify labels file/directory exists\n- Check path is absolute, not relative\n- Ensure correct format is detected\n\n**Error: \"Invalid group configuration\"**\n- Each group must have at least one sample\n- Slice names must be consistent across groups\n- Only one `3d` slice allowed per group\n\n**Import is slow**\n- For large datasets, use delegated execution\n- Import in batches if needed\n- Consider using glob patterns to filter files\n\n**Point clouds not rendering**\n- Ensure `.pcd` files are valid\n- Check FiftyOne 3D visualization is enabled\n- Verify point cloud plugin is installed\n\n**Groups not detected**\n- Check folder structure matches expected patterns\n- Verify consistent naming across scenes\n- May need to specify grouping manually\n\n## Best Practices\n\n1. **Always scan first** - Understand the data before importing\n2. **Confirm with user** - Present findings before creating dataset\n3. **Use descriptive names** - Dataset names and label fields should be meaningful\n4. **Validate counts** - Ensure imported samples match source files\n5. **Handle errors gracefully** - Report issues clearly, continue with valid files\n6. **Use groups for multimodal** - Don't flatten data that should be grouped\n7. **Set appropriate default slice** - Choose the most commonly viewed modality\n8. **Tag imports** - Use tags to track import batches or sources\n\n## Performance Notes\n\n**Import time estimates:**\n- 1,000 images: ~10-30 seconds\n- 10,000 images: ~2-5 minutes\n- 100,000 images: ~20-60 minutes\n- Point clouds: ~2x slower than images\n- Videos: Depends on frame extraction settings\n\n**Memory requirements:**\n- ~1KB per sample metadata\n- Media files are referenced, not loaded into memory\n- Large datasets may require increased MongoDB limits\n\n## Resources\n\n- [FiftyOne Dataset Import Guide](https://docs.voxel51.com/user_guide/dataset_creation/index.html)\n- [Grouped Datasets Guide](https://docs.voxel51.com/user_guide/groups.html)\n- [Point Cloud Support](https://docs.voxel51.com/user_guide/3d.html)\n- [Supported Dataset Formats](https://docs.voxel51.com/user_guide/dataset_creation/datasets.html)\n- [FiftyOne I/O Plugin](https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/io)\n\n## License\n\nCopyright 2017-2025, Voxel51, Inc.\nApache 2.0 License"
              }
            ]
          },
          {
            "name": "fiftyone-find-duplicates",
            "description": "Find duplicate or near-duplicate images in FiftyOne datasets using brain similarity computation. Use when users want to deduplicate datasets, find similar images, cluster visually similar content, or remove redundant samples. Requires FiftyOne MCP server with @voxel51/brain plugin installed.",
            "source": "./find-duplicates",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-find-duplicates@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-find-duplicates",
                "description": "Find duplicate or near-duplicate images in FiftyOne datasets using brain similarity computation. Use when users want to deduplicate datasets, find similar images, cluster visually similar content, or remove redundant samples. Requires FiftyOne MCP server with @voxel51/brain plugin installed.",
                "path": "find-duplicates/skills/fiftyone-find-duplicates/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-find-duplicates",
                  "description": "Find duplicate or near-duplicate images in FiftyOne datasets using brain similarity computation. Use when users want to deduplicate datasets, find similar images, cluster visually similar content, or remove redundant samples. Requires FiftyOne MCP server with @voxel51/brain plugin installed."
                },
                "content": "# Find Duplicates in FiftyOne Datasets\n\n## Overview\n\nFind and remove duplicate or near-duplicate images using FiftyOne's brain similarity operators. Uses deep learning embeddings to identify visually similar images.\n\n**Use this skill when:**\n- Removing duplicate images from datasets\n- Finding near-duplicate images (similar but not identical)\n- Clustering visually similar images\n- Cleaning datasets before training\n\n## Prerequisites\n\n- FiftyOne MCP server installed and running\n- `@voxel51/brain` plugin installed and enabled\n- Dataset with image samples loaded in FiftyOne\n\n## Key Directives\n\n**ALWAYS follow these rules:**\n\n### 1. Set context first\n```python\nset_context(dataset_name=\"my-dataset\")\n```\n\n### 2. Launch FiftyOne App\nBrain operators are delegated and require the app:\n```python\nlaunch_app()\n```\nWait 5-10 seconds for initialization.\n\n### 3. Discover operators dynamically\n```python\n# List all brain operators\nlist_operators(builtin_only=False)\n\n# Get schema for specific operator\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_similarity\")\n```\n\n### 4. Compute embeddings before finding duplicates\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\"brain_key\": \"img_sim\", \"model\": \"mobilenet-v2-imagenet-torch\"}\n)\n```\n\n### 5. Close app when done\n```python\nclose_app()\n```\n\n## Complete Workflow\n\n### Step 1: Setup\n```python\n# Set context\nset_context(dataset_name=\"my-dataset\")\n\n# Launch app (required for brain operators)\nlaunch_app()\n```\n\n### Step 2: Verify Brain Plugin\n```python\n# Check if brain plugin is available\nlist_plugins(enabled=True)\n\n# If not installed:\ndownload_plugin(\n    url_or_repo=\"voxel51/fiftyone-plugins\",\n    plugin_names=[\"@voxel51/brain\"]\n)\nenable_plugin(plugin_name=\"@voxel51/brain\")\n```\n\n### Step 3: Discover Brain Operators\n```python\n# List all available operators\nlist_operators(builtin_only=False)\n\n# Get schema for compute_similarity\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_similarity\")\n\n# Get schema for find_duplicates\nget_operator_schema(operator_uri=\"@voxel51/brain/find_duplicates\")\n```\n\n### Step 4: Compute Similarity\n```python\n# Execute operator to compute embeddings\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"img_duplicates\",\n        \"model\": \"mobilenet-v2-imagenet-torch\"\n    }\n)\n```\n\n### Step 5: Find Near Duplicates\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/brain/find_near_duplicates\",\n    params={\n        \"similarity_index\": \"img_duplicates\",\n        \"threshold\": 0.3\n    }\n)\n```\n\n**Threshold guidelines (distance-based, lower = more similar):**\n- `0.1` = Very similar (near-exact duplicates)\n- `0.3` = Near duplicates (recommended default)\n- `0.5` = Similar images\n- `0.7` = Loosely similar\n\nThis operator creates two saved views automatically:\n- `near duplicates`: all samples that are near duplicates\n- `representatives of near duplicates`: one representative from each group\n\n### Step 6: View Duplicates in App\n\nAfter finding duplicates, use `set_view` to display them in the FiftyOne App:\n\n**Option A: Filter by near_dup_id field**\n```python\n# Show all samples that have a near_dup_id (all duplicates)\nset_view(exists=[\"near_dup_id\"])\n```\n\n**Option B: Show specific duplicate group**\n```python\n# Show samples with a specific duplicate group ID\nset_view(filters={\"near_dup_id\": 1})\n```\n\n**Option C: Load saved view (if available)**\n```python\n# Load the automatically created saved view\nset_view(view_name=\"near duplicates\")\n```\n\n**Option D: Clear filter to show all samples**\n```python\nclear_view()\n```\n\nThe `find_near_duplicates` operator adds a `near_dup_id` field to samples. Samples with the same ID are duplicates of each other.\n\n### Step 7: Delete Duplicates\n\n**Option A: Use deduplicate operator (keeps one representative per group)**\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/brain/deduplicate_near_duplicates\",\n    params={}\n)\n```\n\n**Option B: Manual deletion from App UI**\n1. Use `set_view(exists=[\"near_dup_id\"])` to show duplicates\n2. Review samples in the App at http://localhost:5151/\n3. Select samples to delete\n4. Use the delete action in the App\n\n### Step 8: Clean Up\n```python\nclose_app()\n```\n\n## Available Tools\n\n### Session View Tools\n\n| Tool | Description |\n|------|-------------|\n| `set_view(exists=[...])` | Filter samples where field(s) have non-None values |\n| `set_view(filters={...})` | Filter samples by exact field values |\n| `set_view(tags=[...])` | Filter samples by tags |\n| `set_view(sample_ids=[...])` | Select specific sample IDs |\n| `set_view(view_name=\"...\")` | Load a saved view by name |\n| `clear_view()` | Clear filters, show all samples |\n\n### Brain Operators for Duplicates\n\nUse `list_operators()` to discover and `get_operator_schema()` to see parameters:\n\n| Operator | Description |\n|----------|-------------|\n| `@voxel51/brain/compute_similarity` | Compute embeddings and similarity index |\n| `@voxel51/brain/find_near_duplicates` | Find near-duplicate samples |\n| `@voxel51/brain/deduplicate_near_duplicates` | Delete duplicates, keep representatives |\n| `@voxel51/brain/find_exact_duplicates` | Find exact duplicate media files |\n| `@voxel51/brain/deduplicate_exact_duplicates` | Delete exact duplicates |\n| `@voxel51/brain/compute_uniqueness` | Compute uniqueness scores |\n\n## Common Use Cases\n\n### Use Case 1: Remove Exact Duplicates\nFor accidentally duplicated files (identical bytes):\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\nexecute_operator(\n    operator_uri=\"@voxel51/brain/find_exact_duplicates\",\n    params={}\n)\n\nexecute_operator(\n    operator_uri=\"@voxel51/brain/deduplicate_exact_duplicates\",\n    params={}\n)\n\nclose_app()\n```\n\n### Use Case 2: Find and Review Near Duplicates\nFor visually similar but not identical images:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Compute embeddings\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\"brain_key\": \"near_dups\", \"model\": \"mobilenet-v2-imagenet-torch\"}\n)\n\n# Find duplicates\nexecute_operator(\n    operator_uri=\"@voxel51/brain/find_near_duplicates\",\n    params={\"similarity_index\": \"near_dups\", \"threshold\": 0.3}\n)\n\n# View duplicates in the App\nset_view(exists=[\"near_dup_id\"])\n\n# After review, deduplicate\nexecute_operator(\n    operator_uri=\"@voxel51/brain/deduplicate_near_duplicates\",\n    params={}\n)\n\n# Clear view and close\nclear_view()\nclose_app()\n```\n\n### Use Case 3: Sort by Similarity\nFind images similar to a specific sample:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\"brain_key\": \"search\"}\n)\n\nexecute_operator(\n    operator_uri=\"@voxel51/brain/sort_by_similarity\",\n    params={\n        \"brain_key\": \"search\",\n        \"query_id\": \"sample_id_here\",\n        \"k\": 20\n    }\n)\n\nclose_app()\n```\n\n## Troubleshooting\n\n**Error: \"No executor available\"**\n- Cause: Delegated operators require the App executor for UI triggers\n- Solution: Direct user to App UI to view results and complete deletion manually\n- Affected operators: `find_near_duplicates`, `deduplicate_near_duplicates`\n\n**Error: \"Brain key not found\"**\n- Cause: Embeddings not computed\n- Solution: Run `compute_similarity` first with a `brain_key`\n\n**Error: \"Operator not found\"**\n- Cause: Brain plugin not installed\n- Solution: Install with `download_plugin()` and `enable_plugin()`\n\n**Error: \"Missing dependency\" (e.g., torch, tensorflow)**\n- The MCP server detects missing dependencies automatically\n- Response includes `missing_package` and `install_command`\n- Example response:\n  ```json\n  {\n    \"error_type\": \"missing_dependency\",\n    \"missing_package\": \"torch\",\n    \"install_command\": \"pip install torch\"\n  }\n  ```\n- Offer to run the install command for the user\n- After installation, restart MCP server and retry\n\n**Similarity computation is slow**\n- Use faster model: `mobilenet-v2-imagenet-torch`\n- Use GPU if available\n- Process large datasets in batches\n\n## Best Practices\n\n1. **Discover dynamically** - Use `list_operators()` and `get_operator_schema()` to get current operator names and parameters\n2. **Start with default threshold** (0.3) and adjust as needed\n3. **Review before deleting** - Direct user to App to inspect duplicates\n4. **Store embeddings** - Reuse for multiple operations via `brain_key`\n5. **Handle executor errors gracefully** - Guide user to App UI when needed\n\n## Performance Notes\n\n**Embedding computation time:**\n- 1,000 images: ~1-2 minutes\n- 10,000 images: ~10-15 minutes\n- 100,000 images: ~1-2 hours\n\n**Memory requirements:**\n- ~2KB per image for embeddings\n- ~4-8KB per image for similarity index\n\n## Resources\n\n- [FiftyOne Brain Documentation](https://docs.voxel51.com/user_guide/brain.html)\n- [Brain Plugin Source](https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/brain)\n\n## License\n\nCopyright 2017-2025, Voxel51, Inc.\nApache 2.0 License"
              }
            ]
          },
          {
            "name": "fiftyone-dataset-inference",
            "description": "Create FiftyOne datasets from local directories, import labels in standard formats (COCO, YOLO, VOC), and run model inference. Use when users want to load local files, apply ML models for detection, classification, or segmentation, or build end-to-end inference pipelines.",
            "source": "./dataset-inference",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-dataset-inference@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-dataset-inference",
                "description": "Create a FiftyOne dataset from a directory of media files (images, videos, point clouds), optionally import labels in common formats (COCO, YOLO, VOC), run model inference, and store predictions. Use when users want to load local files into FiftyOne, apply ML models for detection, classification, or segmentation, or build end-to-end inference pipelines.",
                "path": "dataset-inference/skills/fiftyone-dataset-inference/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-dataset-inference",
                  "description": "Create a FiftyOne dataset from a directory of media files (images, videos, point clouds), optionally import labels in common formats (COCO, YOLO, VOC), run model inference, and store predictions. Use when users want to load local files into FiftyOne, apply ML models for detection, classification, or segmentation, or build end-to-end inference pipelines."
                },
                "content": "# Create Dataset and Run Inference\n\n## Overview\n\nCreate FiftyOne datasets from local directories, import labels in standard formats, and run model inference to generate predictions.\n\n**Use this skill when:**\n- Loading images, videos, or point clouds from a directory\n- Importing labeled datasets (COCO, YOLO, VOC, CVAT, etc.)\n- Running model inference on media files\n- Building end-to-end ML pipelines\n\n## Prerequisites\n\n- FiftyOne MCP server installed and running\n- `@voxel51/io` plugin for importing data\n- `@voxel51/zoo` plugin for model inference\n- `@voxel51/utils` plugin for dataset management\n\n## Key Directives\n\n**ALWAYS follow these rules:**\n\n### 1. Explore directory first\nScan the user's directory before importing to detect media types and label formats.\n\n### 2. Confirm with user\nPresent findings and get confirmation before creating datasets or running inference.\n\n### 3. Set context before operations\n```python\nset_context(dataset_name=\"my-dataset\")\n```\n\n### 4. Launch App for inference\n```python\nlaunch_app(dataset_name=\"my-dataset\")\n```\n\n### 5. User specifies field names\nAlways ask the user for:\n- Dataset name\n- Label field for predictions\n\n### 6. Close app when done\n```python\nclose_app()\n```\n\n## Workflow\n\n### Step 1: Explore the Directory\n\nUse Bash to scan the user's directory:\n\n```bash\nls -la /path/to/directory\nfind /path/to/directory -type f | head -20\n```\n\nIdentify media files and label files. See **Supported Dataset Types** section for format detection.\n\n### Step 2: Present Findings to User\n\nBefore creating the dataset, confirm with the user:\n\n```\nI found the following in /path/to/directory:\n- 150 image files (.jpg, .png)\n- Labels: COCO format (annotations.json)\n\nProposed dataset name: \"my-dataset\"\nLabel field: \"ground_truth\"\n\nShould I proceed with these settings?\n```\n\n### Step 3: Create Dataset\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\n        \"name\": \"my-dataset\",\n        \"persistent\": true\n    }\n)\n```\n\n### Step 4: Set Context\n\nSet context to the newly created dataset before importing:\n\n```python\nset_context(dataset_name=\"my-dataset\")\n```\n\n### Step 5: Import Samples\n\n**For media only (no labels):**\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"DIRECTORY\",\n        \"directory\": {\"absolute_path\": \"/path/to/images\"}\n    }\n)\n```\n\n**For media with labels:**\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"COCO\",\n        \"data_path\": {\"absolute_path\": \"/path/to/images\"},\n        \"labels_path\": {\"absolute_path\": \"/path/to/annotations.json\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n```\n\n### Step 6: Validate Import\n\nVerify samples imported correctly by comparing with source:\n\n```python\nload_dataset(name=\"my-dataset\")\n```\n\nCompare `num_samples` with the file count from Step 1. Report any discrepancy to the user.\n\n### Step 7: Launch App\n\n```python\nlaunch_app(dataset_name=\"my-dataset\")\n```\n\n### Step 8: Apply Model Inference\n\nAsk user for model name and label field for predictions.\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/zoo/apply_zoo_model\",\n    params={\n        \"tab\": \"BUILTIN\",\n        \"model\": \"yolov8n-coco-torch\",\n        \"label_field\": \"predictions\"\n    }\n)\n```\n\n### Step 9: View Results\n\n```python\nset_view(exists=[\"predictions\"])\n```\n\n### Step 10: Clean Up\n\n```python\nclose_app()\n```\n\n## Supported Media Types\n\n| Extensions | Media Type |\n|------------|------------|\n| `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.webp` | image |\n| `.mp4`, `.avi`, `.mov`, `.mkv`, `.webm` | video |\n| `.pcd` | point-cloud |\n| `.fo3d` | 3d |\n\n## Supported Dataset Types\n\n| Value | File Pattern | Label Types |\n|-------|--------------|-------------|\n| `Image Classification Directory Tree` | Folder per class | classification |\n| `Video Classification Directory Tree` | Folder per class | classification |\n| `COCO` | `*.json` | detections, segmentations, keypoints |\n| `VOC` | `*.xml` per image | detections |\n| `KITTI` | `*.txt` per image | detections |\n| `YOLOv4` | `*.txt` + `classes.txt` | detections |\n| `YOLOv5` | `data.yaml` + `labels/*.txt` | detections |\n| `CVAT Image` | Single `*.xml` file | classifications, detections, polylines, keypoints |\n| `CVAT Video` | XML directory | frame labels |\n| `TF Image Classification` | TFRecords | classification |\n| `TF Object Detection` | TFRecords | detections |\n\n## Common Zoo Models\n\nPopular models for `apply_zoo_model`. Some models require additional packages - if a model fails with a dependency error, the response includes the `install_command`. Offer to run it for the user.\n\n**Detection (PyTorch only):**\n- `faster-rcnn-resnet50-fpn-coco-torch` - Faster R-CNN (no extra deps)\n- `retinanet-resnet50-fpn-coco-torch` - RetinaNet (no extra deps)\n\n**Detection (requires ultralytics):**\n- `yolov8n-coco-torch` - YOLOv8 nano (fast)\n- `yolov8s-coco-torch` - YOLOv8 small\n- `yolov8m-coco-torch` - YOLOv8 medium\n\n**Classification:**\n- `resnet50-imagenet-torch` - ResNet-50\n- `mobilenet-v2-imagenet-torch` - MobileNet v2\n\n**Segmentation:**\n- `sam-vit-base-hq-torch` - Segment Anything\n- `deeplabv3-resnet101-coco-torch` - DeepLabV3\n\n**Embeddings:**\n- `clip-vit-base32-torch` - CLIP embeddings\n- `dinov2-vits14-torch` - DINOv2 embeddings\n\n## Common Use Cases\n\n### Use Case 1: Load Images and Run Detection\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"my-images\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"my-images\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_ONLY\",\n        \"style\": \"DIRECTORY\",\n        \"directory\": {\"absolute_path\": \"/path/to/images\"}\n    }\n)\n\nload_dataset(name=\"my-images\")  # Validate import\n\nlaunch_app(dataset_name=\"my-images\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/zoo/apply_zoo_model\",\n    params={\n        \"tab\": \"BUILTIN\",\n        \"model\": \"faster-rcnn-resnet50-fpn-coco-torch\",\n        \"label_field\": \"predictions\"\n    }\n)\n\nset_view(exists=[\"predictions\"]) \n```\n\n### Use Case 2: Import COCO Dataset and Add Predictions\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"coco-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"coco-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"COCO\",\n        \"data_path\": {\"absolute_path\": \"/path/to/images\"},\n        \"labels_path\": {\"absolute_path\": \"/path/to/annotations.json\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nload_dataset(name=\"coco-dataset\")  # Validate import\n\nlaunch_app(dataset_name=\"coco-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/zoo/apply_zoo_model\",\n    params={\n        \"tab\": \"BUILTIN\",\n        \"model\": \"faster-rcnn-resnet50-fpn-coco-torch\",\n        \"label_field\": \"predictions\"\n    }\n)\n\nset_view(exists=[\"predictions\"]) \n```\n\n### Use Case 3: Import YOLO Dataset\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"yolo-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"yolo-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"YOLOv5\",\n        \"dataset_dir\": {\"absolute_path\": \"/path/to/yolo/dataset\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nload_dataset(name=\"yolo-dataset\")  \n\nlaunch_app(dataset_name=\"yolo-dataset\")\n```\n\n### Use Case 4: Classification with Directory Tree\n\nFor a folder structure like:\n```\n/dataset/\n  /cats/\n    cat1.jpg\n    cat2.jpg\n  /dogs/\n    dog1.jpg\n    dog2.jpg\n```\n\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/utils/create_dataset\",\n    params={\"name\": \"classification-dataset\", \"persistent\": true}\n)\n\nset_context(dataset_name=\"classification-dataset\")\n\nexecute_operator(\n    operator_uri=\"@voxel51/io/import_samples\",\n    params={\n        \"import_type\": \"MEDIA_AND_LABELS\",\n        \"dataset_type\": \"Image Classification Directory Tree\",\n        \"dataset_dir\": {\"absolute_path\": \"/path/to/dataset\"},\n        \"label_field\": \"ground_truth\"\n    }\n)\n\nload_dataset(name=\"classification-dataset\")  \n\nlaunch_app(dataset_name=\"classification-dataset\")\n```\n\n## Troubleshooting\n\n**Error: \"Dataset already exists\"**\n- Use a different dataset name\n- Or delete existing dataset first with `@voxel51/utils/delete_dataset`\n\n**Error: \"No samples found\"**\n- Verify the directory path is correct\n- Check file extensions are supported\n- Ensure files are not in nested subdirectories (use `recursive=true` if needed)\n\n**Error: \"Labels path not found\"**\n- Verify the labels file/directory exists\n- Check the path is absolute, not relative\n\n**Error: \"Model not found\"**\n- Check model name spelling\n- Verify model exists in FiftyOne Zoo\n- Use `list_operators()` and `get_operator_schema()` to discover available models\n\n**Error: \"Missing dependency\" (e.g., torch, ultralytics)**\n- The MCP server detects missing dependencies\n- Response includes `missing_package` and `install_command`\n- Install the required package and restart MCP server\n\n**Slow inference**\n- Use smaller model variant (e.g., `yolov8n` instead of `yolov8x`)\n- Reduce batch size\n- Consider delegated execution for large datasets\n\n## Best Practices\n\n1. **Explore before importing** - Always scan the directory first to understand the data\n2. **Confirm with user** - Present findings and get confirmation before creating datasets\n3. **Use descriptive names** - Dataset names and label fields should be meaningful\n4. **Separate ground truth from predictions** - Use different field names (e.g., `ground_truth` vs `predictions`)\n5. **Start with fast models** - Use lightweight models first, then upgrade if needed\n6. **Check operator schemas** - Use `get_operator_schema()` to discover available parameters\n\n## Resources\n\n- [FiftyOne Dataset Zoo](https://docs.voxel51.com/dataset_zoo/index.html)\n- [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/index.html)\n- [Importing Datasets Guide](https://docs.voxel51.com/user_guide/import_datasets.html)\n- [Applying Models Guide](https://docs.voxel51.com/user_guide/applying_models.html)\n\n## License\n\nCopyright 2017-2025, Voxel51, Inc.\nApache 2.0 License"
              }
            ]
          },
          {
            "name": "fiftyone-embeddings-visualization",
            "description": "Visualize datasets in 2D using embeddings with UMAP or t-SNE dimensionality reduction. Use when users want to explore dataset structure, find clusters in images, identify outliers, color samples by class or metadata, or understand data distribution.",
            "source": "./embeddings-visualization",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-embeddings-visualization@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-embeddings-visualization",
                "description": "Visualize datasets in 2D using embeddings with UMAP or t-SNE dimensionality reduction. Use when users want to explore dataset structure, find clusters in images, identify outliers, color samples by class or metadata, or understand data distribution. Requires FiftyOne MCP server with @voxel51/brain plugin installed.",
                "path": "embeddings-visualization/skills/fiftyone-embeddings-visualization/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-embeddings-visualization",
                  "description": "Visualize datasets in 2D using embeddings with UMAP or t-SNE dimensionality reduction. Use when users want to explore dataset structure, find clusters in images, identify outliers, color samples by class or metadata, or understand data distribution. Requires FiftyOne MCP server with @voxel51/brain plugin installed."
                },
                "content": "# Embeddings Visualization in FiftyOne\n\n## Overview\n\nVisualize your dataset in 2D using deep learning embeddings and dimensionality reduction (UMAP/t-SNE). Explore clusters, find outliers, and color samples by any field.\n\n**Use this skill when:**\n- Visualizing dataset structure in 2D\n- Finding natural clusters in images\n- Identifying outliers or anomalies\n- Exploring data distribution by class or metadata\n- Understanding embedding space relationships\n\n## Prerequisites\n\n- FiftyOne MCP server installed and running\n- `@voxel51/brain` plugin installed and enabled\n- Dataset with image samples loaded in FiftyOne\n\n## Key Directives\n\n**ALWAYS follow these rules:**\n\n### 1. Set context first\n```python\nset_context(dataset_name=\"my-dataset\")\n```\n\n### 2. Launch FiftyOne App\nBrain operators are delegated and require the app:\n```python\nlaunch_app()\n```\nWait 5-10 seconds for initialization.\n\n### 3. Discover operators dynamically\n```python\n# List all brain operators\nlist_operators(builtin_only=False)\n\n# Get schema for specific operator\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n```\n\n### 4. Compute embeddings before visualization\nEmbeddings are required for dimensionality reduction:\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"img_sim\",\n        \"model\": \"clip-vit-base32-torch\",\n        \"embeddings\": \"clip_embeddings\",\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n```\n\n### 5. Close app when done\n```python\nclose_app()\n```\n\n## Complete Workflow\n\n### Step 1: Setup\n```python\n# Set context\nset_context(dataset_name=\"my-dataset\")\n\n# Launch app (required for brain operators)\nlaunch_app()\n```\n\n### Step 2: Verify Brain Plugin\n```python\n# Check if brain plugin is available\nlist_plugins(enabled=True)\n\n# If not installed:\ndownload_plugin(\n    url_or_repo=\"voxel51/fiftyone-plugins\",\n    plugin_names=[\"@voxel51/brain\"]\n)\nenable_plugin(plugin_name=\"@voxel51/brain\")\n```\n\n### Step 3: Discover Brain Operators\n```python\n# List all available operators\nlist_operators(builtin_only=False)\n\n# Get schema for compute_visualization\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n```\n\n### Step 4: Check for Existing Embeddings or Compute New Ones\n\nFirst, check if the dataset already has embeddings by looking at the operator schema:\n```python\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n# Look for existing embeddings fields in the \"embeddings\" choices\n# (e.g., \"clip_embeddings\", \"dinov2_embeddings\")\n```\n\n**If embeddings exist:** Skip to Step 5 and use the existing embeddings field.\n\n**If no embeddings exist:** Compute them:\n```python\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"img_viz\",\n        \"model\": \"clip-vit-base32-torch\",\n        \"embeddings\": \"clip_embeddings\",  # Field name to store embeddings\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n```\n\n**Required parameters for compute_similarity:**\n- `brain_key` - Unique identifier for this brain run\n- `model` - Model from FiftyOne Model Zoo to generate embeddings\n- `embeddings` - Field name where embeddings will be stored\n- `backend` - Similarity backend (use `\"sklearn\"`)\n- `metric` - Distance metric (use `\"cosine\"` or `\"euclidean\"`)\n\n**Recommended embedding models:**\n- `clip-vit-base32-torch` - Best for general visual + semantic similarity\n- `dinov2-vits14-torch` - Best for visual similarity only\n- `resnet50-imagenet-torch` - Classic CNN features\n- `mobilenet-v2-imagenet-torch` - Fast, lightweight option\n\n### Step 5: Compute 2D Visualization\n\nUse existing embeddings field OR the brain_key from Step 4:\n```python\n# Option A: Use existing embeddings field (e.g., clip_embeddings)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"img_viz\",\n        \"embeddings\": \"clip_embeddings\",  # Use existing field\n        \"method\": \"umap\",\n        \"num_dims\": 2\n    }\n)\n\n# Option B: Use brain_key from compute_similarity\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"img_viz\",  # Same key used in compute_similarity\n        \"method\": \"umap\",\n        \"num_dims\": 2\n    }\n)\n```\n\n**Dimensionality reduction methods:**\n- `umap` - (Recommended) Preserves local and global structure, faster. Requires `umap-learn` package.\n- `tsne` - Better local structure, slower on large datasets. No extra dependencies.\n- `pca` - Linear reduction, fastest but less informative\n\n### Step 6: Direct User to Embeddings Panel\n\nAfter computing visualization, direct the user to open the FiftyOne App at http://localhost:5151/ and:\n\n1. Click the **Embeddings** panel icon (scatter plot icon, looks like a grid of dots) in the top toolbar\n2. Select the brain key (e.g., `img_viz`) from the dropdown\n3. Points represent samples in 2D embedding space\n4. Use the **\"Color by\"** dropdown to color points by a field (e.g., `ground_truth`, `predictions`)\n5. Click points to select samples, use lasso tool to select groups\n\n**IMPORTANT:** Do NOT use `set_view(exists=[\"brain_key\"])` - this filters samples and is not needed for visualization. The Embeddings panel automatically shows all samples with computed coordinates.\n\n### Step 7: Explore and Filter (Optional)\n\nTo filter samples while viewing in the Embeddings panel:\n```python\n# Filter to specific class\nset_view(filters={\"ground_truth.label\": \"dog\"})\n\n# Filter by tag\nset_view(tags=[\"validated\"])\n\n# Clear filter to show all\nclear_view()\n```\n\nThese filters will update the Embeddings panel to show only matching samples.\n\n### Step 8: Find Outliers\n\nOutliers appear as isolated points far from clusters:\n\n```python\n# Compute uniqueness scores (higher = more unique/outlier)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_uniqueness\",\n    params={\n        \"brain_key\": \"img_viz\"\n    }\n)\n\n# View most unique samples (potential outliers)\nset_view(sort_by=\"uniqueness\", reverse=True, limit=50)\n```\n\n### Step 9: Find Clusters\n\nUse the App's Embeddings panel to visually identify clusters, then:\n\n**Option A: Lasso selection in App**\n1. Use lasso tool to select a cluster\n2. Selected samples are highlighted\n3. Tag or export selected samples\n\n**Option B: Use similarity to find cluster members**\n```python\n# Sort by similarity to a representative sample\nexecute_operator(\n    operator_uri=\"@voxel51/brain/sort_by_similarity\",\n    params={\n        \"brain_key\": \"img_viz\",\n        \"query_id\": \"sample_id_from_cluster\",\n        \"k\": 100\n    }\n)\n```\n\n### Step 10: Clean Up\n```python\nclose_app()\n```\n\n## Available Tools\n\n### Session View Tools\n\n| Tool | Description |\n|------|-------------|\n| `set_view(filters={...})` | Filter samples by field values |\n| `set_view(tags=[...])` | Filter samples by tags |\n| `set_view(sort_by=\"...\", reverse=True)` | Sort samples by field |\n| `set_view(limit=N)` | Limit to N samples |\n| `clear_view()` | Clear filters, show all samples |\n\n### Brain Operators for Visualization\n\nUse `list_operators()` to discover and `get_operator_schema()` to see parameters:\n\n| Operator | Description |\n|----------|-------------|\n| `@voxel51/brain/compute_similarity` | Compute embeddings and similarity index |\n| `@voxel51/brain/compute_visualization` | Reduce embeddings to 2D/3D for visualization |\n| `@voxel51/brain/compute_uniqueness` | Score samples by uniqueness (outlier detection) |\n| `@voxel51/brain/sort_by_similarity` | Sort by similarity to a query sample |\n\n## Common Use Cases\n\n### Use Case 1: Basic Dataset Exploration\nVisualize dataset structure and explore clusters:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Check for existing embeddings in schema\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n\n# If embeddings exist (e.g., clip_embeddings), use them directly:\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"exploration\",\n        \"embeddings\": \"clip_embeddings\",\n        \"method\": \"umap\",  # or \"tsne\" if umap-learn not installed\n        \"num_dims\": 2\n    }\n)\n\n# Direct user to App Embeddings panel at http://localhost:5151/\n# 1. Click Embeddings panel icon\n# 2. Select \"exploration\" from dropdown\n# 3. Use \"Color by\" to color by ground_truth or predictions\n```\n\n### Use Case 2: Find Outliers in Dataset\nIdentify anomalous or mislabeled samples:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Check for existing embeddings in schema\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n\n# If no embeddings exist, compute them:\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"outliers\",\n        \"model\": \"clip-vit-base32-torch\",\n        \"embeddings\": \"clip_embeddings\",\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n\n# Compute uniqueness scores\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_uniqueness\",\n    params={\"brain_key\": \"outliers\"}\n)\n\n# Generate visualization (use existing embeddings field or brain_key)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"outliers\",\n        \"embeddings\": \"clip_embeddings\",  # Use existing field if available\n        \"method\": \"umap\",  # or \"tsne\" if umap-learn not installed\n        \"num_dims\": 2\n    }\n)\n\n# Direct user to App at http://localhost:5151/\n# 1. Click Embeddings panel icon\n# 2. Select \"outliers\" from dropdown\n# 3. Outliers appear as isolated points far from clusters\n# 4. Optionally sort by uniqueness field in the App sidebar\n```\n\n### Use Case 3: Compare Classes in Embedding Space\nSee how different classes cluster:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Check for existing embeddings in schema\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n\n# If no embeddings exist, compute them:\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"class_viz\",\n        \"model\": \"clip-vit-base32-torch\",\n        \"embeddings\": \"clip_embeddings\",\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n\n# Generate visualization (use existing embeddings field or brain_key)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"class_viz\",\n        \"embeddings\": \"clip_embeddings\",  # Use existing field if available\n        \"method\": \"umap\",  # or \"tsne\" if umap-learn not installed\n        \"num_dims\": 2\n    }\n)\n\n# Direct user to App at http://localhost:5151/\n# 1. Click Embeddings panel icon\n# 2. Select \"class_viz\" from dropdown\n# 3. Use \"Color by\" dropdown to color by ground_truth or predictions\n# Look for:\n# - Well-separated clusters = good class distinction\n# - Overlapping clusters = similar classes or confusion\n# - Scattered points = high variance within class\n```\n\n### Use Case 4: Analyze Model Predictions\nCompare ground truth vs predictions in embedding space:\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Check for existing embeddings in schema\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n\n# If no embeddings exist, compute them:\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"pred_analysis\",\n        \"model\": \"clip-vit-base32-torch\",\n        \"embeddings\": \"clip_embeddings\",\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n\n# Generate visualization (use existing embeddings field or brain_key)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"pred_analysis\",\n        \"embeddings\": \"clip_embeddings\",  # Use existing field if available\n        \"method\": \"umap\",  # or \"tsne\" if umap-learn not installed\n        \"num_dims\": 2\n    }\n)\n\n# Direct user to App at http://localhost:5151/\n# 1. Click Embeddings panel icon\n# 2. Select \"pred_analysis\" from dropdown\n# 3. Color by ground_truth - see true class distribution\n# 4. Color by predictions - see model's view\n# 5. Look for mismatches to find errors\n```\n\n### Use Case 5: t-SNE for Publication-Quality Plots\nUse t-SNE for better local structure (no extra dependencies):\n```python\nset_context(dataset_name=\"my-dataset\")\nlaunch_app()\n\n# Check for existing embeddings in schema\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_visualization\")\n\n# If no embeddings exist, compute them (DINOv2 for visual similarity):\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_similarity\",\n    params={\n        \"brain_key\": \"tsne_viz\",\n        \"model\": \"dinov2-vits14-torch\",\n        \"embeddings\": \"dinov2_embeddings\",\n        \"backend\": \"sklearn\",\n        \"metric\": \"cosine\"\n    }\n)\n\n# Generate t-SNE visualization (no umap-learn dependency needed)\nexecute_operator(\n    operator_uri=\"@voxel51/brain/compute_visualization\",\n    params={\n        \"brain_key\": \"tsne_viz\",\n        \"embeddings\": \"dinov2_embeddings\",  # Use existing field if available\n        \"method\": \"tsne\",\n        \"num_dims\": 2\n    }\n)\n\n# Direct user to App at http://localhost:5151/\n# 1. Click Embeddings panel icon\n# 2. Select \"tsne_viz\" from dropdown\n# 3. t-SNE provides better local cluster structure than UMAP\n```\n\n## Troubleshooting\n\n**Error: \"No executor available\"**\n- Cause: Delegated operators require the App executor\n- Solution: Ensure `launch_app()` was called and wait 5-10 seconds\n\n**Error: \"Brain key not found\"**\n- Cause: Embeddings not computed\n- Solution: Run `compute_similarity` first with a `brain_key`\n\n**Error: \"Operator not found\"**\n- Cause: Brain plugin not installed\n- Solution: Install with `download_plugin()` and `enable_plugin()`\n\n**Error: \"You must install the `umap-learn>=0.5` package\"**\n- Cause: UMAP method requires the `umap-learn` package\n- Solutions:\n  1. **Install umap-learn**: Ask user if they want to run `pip install umap-learn`\n  2. **Use t-SNE instead**: Change `method` to `\"tsne\"` (no extra dependencies)\n  3. **Use PCA instead**: Change `method` to `\"pca\"` (fastest, no extra dependencies)\n- After installing umap-learn, restart Claude Code/MCP server and retry\n\n**Visualization is slow**\n- Use UMAP instead of t-SNE for large datasets\n- Use faster embedding model: `mobilenet-v2-imagenet-torch`\n- Process subset first: `set_view(limit=1000)`\n\n**Embeddings panel not showing**\n- Ensure visualization was computed (not just embeddings)\n- Check brain_key matches in both compute_similarity and compute_visualization\n- Refresh the App page\n\n**Points not colored correctly**\n- Verify the field exists on samples\n- Check field type is compatible (Classification, Detections, or string)\n\n## Best Practices\n\n1. **Discover dynamically** - Use `list_operators()` and `get_operator_schema()` to get current operator names and parameters\n2. **Choose the right model** - CLIP for semantic similarity, DINOv2 for visual similarity\n3. **Start with UMAP** - Faster and often better than t-SNE for exploration\n4. **Use uniqueness for outliers** - More reliable than visual inspection alone\n5. **Store embeddings** - Reuse for multiple visualizations via `brain_key`\n6. **Subset large datasets** - Compute on subset first, then full dataset\n\n## Performance Notes\n\n**Embedding computation time:**\n- 1,000 images: ~1-2 minutes\n- 10,000 images: ~10-15 minutes\n- 100,000 images: ~1-2 hours\n\n**Visualization computation time:**\n- UMAP: ~30 seconds for 10,000 samples\n- t-SNE: ~5-10 minutes for 10,000 samples\n- PCA: ~5 seconds for 10,000 samples\n\n**Memory requirements:**\n- ~2KB per image for embeddings\n- ~16 bytes per image for 2D coordinates\n\n## Resources\n\n- [FiftyOne Brain Documentation](https://docs.voxel51.com/user_guide/brain.html)\n- [Visualizing Embeddings Guide](https://docs.voxel51.com/user_guide/embeddings.html)\n- [Brain Plugin Source](https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/brain)\n\n## License\n\nCopyright 2017-2025, Voxel51, Inc.\nApache 2.0 License"
              }
            ]
          },
          {
            "name": "fiftyone-develop-plugin",
            "description": "Develop custom FiftyOne plugins (operators and panels) from scratch. Use when users want to create, build, or develop a new plugin for FiftyOne App, extend FiftyOne with custom operators, create interactive panels, add new functionality to FiftyOne, or integrate external APIs/services into FiftyOne.",
            "source": "./develop-plugin",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-develop-plugin@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-develop-plugin",
                "description": "Develop custom FiftyOne plugins (operators and panels) from scratch. Use when user wants to create a new plugin, extend FiftyOne with custom operators, build interactive panels, or integrate external APIs into FiftyOne. Guides through requirements, design, coding, testing, and iteration.",
                "path": "develop-plugin/skills/fiftyone-develop-plugin/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-develop-plugin",
                  "description": "Develop custom FiftyOne plugins (operators and panels) from scratch. Use when user wants to create a new plugin, extend FiftyOne with custom operators, build interactive panels, or integrate external APIs into FiftyOne. Guides through requirements, design, coding, testing, and iteration."
                },
                "content": "# Develop FiftyOne Plugins\n\n## Overview\n\nCreate custom FiftyOne plugins with full lifecycle support: requirements gathering, code generation, local testing, and iterative refinement.\n\n**Use this skill when:**\n- User asks to create/build/develop a FiftyOne plugin\n- User wants to add custom functionality to FiftyOne App\n- User needs to integrate an external API or service\n\n## Prerequisites\n\n- FiftyOne installed (`pip install fiftyone`)\n- Python 3.8+ (for Python plugins)\n- Node.js 16+ (only for JavaScript panels)\n\n## Key Directives\n\n**ALWAYS follow these rules:**\n\n### 1. Understand before coding\nAsk clarifying questions. Never assume what the plugin should do.\n\n### 2. Plan before implementing\nPresent file structure and design. Get user approval before generating code.\n\n### 3. Search existing plugins for patterns\n```python\nlist_plugins(enabled=True)\nlist_operators(builtin_only=False)\nget_operator_schema(operator_uri=\"@voxel51/brain/compute_similarity\")\n```\n\n### 4. Test locally before done\nInstall plugin and verify it works in FiftyOne App.\n\n### 5. Iterate on feedback\nRefine until the plugin works as expected.\n\n## Workflow\n\n### Phase 1: Requirements\n\nAsk these questions:\n\n1. \"What should your plugin do?\" (one sentence)\n2. \"Operator (action) or Panel (interactive UI)?\"\n3. \"What inputs from the user?\"\n4. \"What outputs/results?\"\n5. \"External APIs or secrets needed?\"\n6. \"Background execution for long tasks?\"\n\n### Phase 2: Design\n\n1. Search existing plugins for similar patterns\n2. Create plan with:\n   - Plugin name (`@org/plugin-name`)\n   - File structure\n   - Operator/panel specs\n   - Input/output definitions\n3. **Get user approval before coding**\n\nSee [PLUGIN-STRUCTURE.md](PLUGIN-STRUCTURE.md) for file formats.\n\n### Phase 3: Generate Code\n\nCreate these files:\n\n| File | Required | Purpose |\n|------|----------|---------|\n| `fiftyone.yml` | Yes | Plugin manifest |\n| `__init__.py` | Yes | Python operators/panels |\n| `requirements.txt` | If deps | Python dependencies |\n| `package.json` | JS only | Node.js metadata |\n| `src/index.tsx` | JS only | React components |\n\nReference docs:\n- [PYTHON-OPERATOR.md](PYTHON-OPERATOR.md)\n- [PYTHON-PANEL.md](PYTHON-PANEL.md)\n- [JAVASCRIPT-PANEL.md](JAVASCRIPT-PANEL.md)\n\n### Phase 4: Install & Test\n\n```bash\n# Find plugins directory\npython -c \"import fiftyone as fo; print(fo.config.plugins_dir)\"\n\n# Copy plugin\ncp -r ./my-plugin ~/.fiftyone/plugins/\n\n# Verify detection\npython -c \"import fiftyone as fo; print(fo.plugins.list_plugins())\"\n```\n\nTest in App:\n```python\nlaunch_app(dataset_name=\"test-dataset\")\n# Press Cmd/Ctrl + ` to open operator browser\n# Search for your operator\n```\n\n### Phase 5: Iterate\n\n1. Get user feedback\n2. Fix issues\n3. Re-copy to plugins directory\n4. Restart App if needed\n5. Repeat until working\n\n## Quick Reference\n\n### Plugin Types\n\n| Type | Language | Use Case |\n|------|----------|----------|\n| Operator | Python | Data processing, computations |\n| Panel | Python | Simple interactive UI |\n| Panel | JavaScript | Rich React-based UI |\n\n### Operator Config Options\n\n| Option | Effect |\n|--------|--------|\n| `dynamic=True` | Recalculate inputs on change |\n| `execute_as_generator=True` | Stream progress |\n| `allow_delegated_execution=True` | Background execution |\n| `unlisted=True` | Hide from browser |\n\n### Input Types\n\n| Type | Method |\n|------|--------|\n| Text | `inputs.str()` |\n| Number | `inputs.int()` / `inputs.float()` |\n| Boolean | `inputs.bool()` |\n| Dropdown | `inputs.enum()` |\n| File | `inputs.file()` |\n| View | `inputs.view_target()` |\n\n## Minimal Example\n\n**fiftyone.yml:**\n```yaml\nname: \"@myorg/hello-world\"\ntype: plugin\noperators:\n  - hello_world\n```\n\n**__init__.py:**\n```python\nimport fiftyone.operators as foo\nimport fiftyone.operators.types as types\n\nclass HelloWorld(foo.Operator):\n    @property\n    def config(self):\n        return foo.OperatorConfig(\n            name=\"hello_world\",\n            label=\"Hello World\"\n        )\n\n    def resolve_input(self, ctx):\n        inputs = types.Object()\n        inputs.str(\"message\", label=\"Message\", default=\"Hello!\")\n        return types.Property(inputs)\n\n    def execute(self, ctx):\n        print(ctx.params[\"message\"])\n        return {\"status\": \"done\"}\n\ndef register(p):\n    p.register(HelloWorld)\n```\n\n## Troubleshooting\n\n**Plugin not appearing:**\n- Check `fiftyone.yml` exists in plugin root\n- Verify location: `~/.fiftyone/plugins/`\n- Check for Python syntax errors\n- Restart FiftyOne App\n\n**Operator not found:**\n- Verify operator listed in `fiftyone.yml`\n- Check `register()` function\n- Run `list_operators()` to debug\n\n**Secrets not available:**\n- Add to `fiftyone.yml` under `secrets:`\n- Set environment variables before starting FiftyOne\n\n## Resources\n\n- [Plugin Development Guide](https://docs.voxel51.com/plugins/developing_plugins.html)\n- [FiftyOne Plugins Repo](https://github.com/voxel51/fiftyone-plugins)\n- [Operator Types API](https://docs.voxel51.com/api/fiftyone.operators.types.html)\n\n## License\n\nCopyright 2017-2025, Voxel51, Inc.\nApache 2.0 License"
              }
            ]
          },
          {
            "name": "fiftyone-code-style",
            "description": "Write Python code following FiftyOne's official conventions. Use when contributing to FiftyOne, developing plugins, or writing code that integrates with FiftyOne's codebase. Covers module structure, import organization, Google-style docstrings, lazy imports, guard patterns, and error handling.",
            "source": "./code-style",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-code-style@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-code-style",
                "description": "Write Python code following FiftyOne's official conventions. Use when contributing to FiftyOne, developing plugins, or writing code that integrates with FiftyOne's codebase.",
                "path": "code-style/skills/fiftyone-code-style/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-code-style",
                  "description": "Write Python code following FiftyOne's official conventions. Use when contributing to FiftyOne, developing plugins, or writing code that integrates with FiftyOne's codebase."
                },
                "content": "# FiftyOne Code Style\n\n## Module Template\n\n```python\n\"\"\"\nModule description.\n\n| Copyright 2017-2025, Voxel51, Inc.\n| `voxel51.com <https://voxel51.com/>`_\n|\n\"\"\"\nimport logging\nimport os\n\nimport numpy as np\n\nimport eta.core.utils as etau\n\nimport fiftyone as fo\nimport fiftyone.core.fields as fof\nimport fiftyone.core.labels as fol\nimport fiftyone.core.utils as fou\n\nlogger = logging.getLogger(__name__)\n\n\ndef public_function(arg):\n    \"\"\"Public API function.\"\"\"\n    return _helper(arg)\n\n\ndef _helper(arg):\n    \"\"\"Private helper.\"\"\"\n    return arg\n```\n\n\n## Import Organization\n\nFour groups, alphabetized within each, separated by blank lines:\n\n| Group | Example |\n|-------|---------|\n| 1. Standard library | `import logging`, `import os` |\n| 2. Third-party | `import numpy as np` |\n| 3. eta packages | `import eta.core.utils as etau` |\n| 4. FiftyOne | `import fiftyone.core.labels as fol` |\n\n### FiftyOne Import Aliases\n\n| Module | Alias |\n|--------|-------|\n| `fiftyone` | `fo` |\n| `fiftyone.core.labels` | `fol` |\n| `fiftyone.core.fields` | `fof` |\n| `fiftyone.core.media` | `fom` |\n| `fiftyone.core.storage` | `fos` |\n| `fiftyone.core.utils` | `fou` |\n| `fiftyone.utils.image` | `foui` |\n| `fiftyone.utils.video` | `fouv` |\n\n## Docstrings (Google-Style)\n\n```python\ndef get_operator(operator_uri, enabled=True):\n    \"\"\"Gets the operator with the given URI.\n\n    Args:\n        operator_uri: the operator URI\n        enabled (True): whether to include only enabled operators (True) or\n            only disabled operators (False) or all operators (\"all\")\n\n    Returns:\n        an :class:`fiftyone.operators.Operator`\n\n    Raises:\n        ValueError: if the operator is not found\n    \"\"\"\n```\n\n**Key patterns:**\n- Args with defaults: `param (default): description`\n- Multi-line descriptions: indent continuation\n- Cross-references: `:class:`fiftyone.module.Class``\n\n## Lazy Imports\n\nUse `fou.lazy_import()` for optional/heavy dependencies:\n\n```python\no3d = fou.lazy_import(\"open3d\", callback=lambda: fou.ensure_package(\"open3d\"))\n\nmask_utils = fou.lazy_import(\n    \"pycocotools.mask\", callback=lambda: fou.ensure_import(\"pycocotools\")\n)\n```\n\n## Guard Patterns\n\nUse `hasattr()` for optional attributes:\n\n```python\nif hasattr(label, \"confidence\"):\n    if label.confidence is None or label.confidence < threshold:\n        label = label.__class__()\n```\n\n## Error Handling\n\nUse `logger.warning()` for non-fatal errors:\n\n```python\ntry:\n    result = process_data(data)\nexcept Exception as e:\n    logger.warning(\"Failed to process data: %s\", e)\n```\n\n## Avoid Redundant Code\n\nBefore writing new functions, search for existing implementations:\n- Local: search the FiftyOne source if available in the environment\n- Remote: search `https://github.com/voxel51/fiftyone`\n- Check `fiftyone/core/utils.py` and `fiftyone/utils/*` first\n\n## Common Utilities\n\n| Module | Functions |\n|--------|-----------|\n| `fou` | `lazy_import()`, `ensure_package()`, `extract_kwargs_for_class()` |\n| `etau` | `guess_mime_type()`, `ensure_dir()`, `make_temp_dir()` |\n\n## Quick Reference\n\n| Pattern | Convention |\n|---------|------------|\n| Module structure | Docstring → imports → logger → public → private |\n| Private functions | `_prefix` |\n| Docstrings | Google-style with Args/Returns/Raises |\n| Error handling | `logger.warning()` for non-fatal |\n| Lazy imports | `fou.lazy_import()` for optional deps |\n| Guard patterns | `hasattr()` checks |\n| Import aliases | `fo`, `fol`, `fof`, `fom`, `fos`, `fou` |"
              }
            ]
          },
          {
            "name": "fiftyone-pr-triage",
            "description": "Triage FiftyOne GitHub issues by validating status, categorizing resolution, and generating standardized responses. Use when reviewing issues to determine if already fixed, won't fix, not reproducible, no longer relevant, or still valid. Includes investigation workflow and response templates.",
            "source": "./pr-triage",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add AdonaiVera/fiftyone-skills",
              "/plugin install fiftyone-pr-triage@fiftyone-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-11T23:28:52Z",
              "created_at": "2025-12-18T22:53:06Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "fiftyone-pr-triage",
                "description": "Triage FiftyOne GitHub issues by validating status, categorizing resolution, and generating standardized responses. Use when reviewing issues to determine if fixed, won't fix, not reproducible, no longer relevant, or still valid.",
                "path": "pr-triage/skills/fiftyone-pr-triage/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone-pr-triage",
                  "description": "Triage FiftyOne GitHub issues by validating status, categorizing resolution, and generating standardized responses. Use when reviewing issues to determine if fixed, won't fix, not reproducible, no longer relevant, or still valid."
                },
                "content": "# FiftyOne Issue Triage\n\n## Categories\n\n| Category | When to Use |\n|----------|-------------|\n| **Already Fixed** | Resolved in recent commits/releases |\n| **Won't Fix** | By design, out of scope, or external behavior (e.g., browser, OS) |\n| **Not Reproducible** | Cannot reproduce with provided info |\n| **No Longer Relevant** | Outdated version, deprecated feature, or stale (6+ months) |\n| **Still Valid** | Confirmed bug or valid feature request needing work |\n\n## Workflow\n\n### 1. Fetch Issue\n```bash\ngh issue view {number} --repo voxel51/fiftyone --json title,body,author,state,labels,comments\n```\n\n### 2. Analyze\n- Extract: issue type, version, reproduction steps, error message\n- Search related: `gh issue list --repo voxel51/fiftyone --state all --search \"keyword\"`\n- Check git history: `git log --oneline --grep=\"keyword\"`\n\n### 3. Assess Responsibility\n\n```\nIs this FiftyOne's responsibility?\n├─ External behavior (browser, OS, third-party)? → Won't Fix\n├─ User workflow/configuration issue? → Won't Fix (with workaround)\n└─ FiftyOne code/behavior issue? → Continue assessment\n```\n\n### 4. Assess Value (before proposing fixes)\n\nAsk: \"Is a fix/doc change worth the effort?\"\n- How many users affected?\n- Is workaround simple?\n- Would fix add complexity or hurt performance?\n\n### 5. Check Documentation\n\nBefore closing, verify if behavior is documented:\n```bash\ngrep -r \"keyword\" docs/source/ --include=\"*.rst\"\n```\n\n### 6. Categorize and Respond\n\n## Decision Tree\n\n```\nIssue reported\n    │\n    ├─ External/not FiftyOne's responsibility? → Won't Fix\n    │\n    ├─ Can reproduce?\n    │   └─ NO → Not Reproducible\n    │\n    ├─ Fixed in recent commit/release? → Already Fixed\n    │\n    ├─ By design or out of scope? → Won't Fix\n    │\n    ├─ Old version, stale, deprecated? → No Longer Relevant\n    │\n    └─ Confirmed, needs work → Still Valid\n```\n\n## Response Templates\n\n**Tone:** Always start with thanks, be friendly, then explain.\n\n### Won't Fix (External Behavior)\n```markdown\nHi @{author},\n\nThanks for reporting this and for the detailed description!\n\nThis is {expected behavior / external to FiftyOne}. {Brief explanation}.\n\n**Quick fixes:**\n- {Workaround 1}\n- {Workaround 2}\n\nClosing as this is {external behavior}, but hopefully this helps!\n```\n\n### Already Fixed\n```markdown\nHi @{author},\n\nThanks for reporting! This was fixed in {version/PR}.\n\n**To resolve:**\npip install --upgrade fiftyone\n\nLet us know if the issue persists.\n```\n\n### Not Reproducible\n```markdown\nHi @{author},\n\nThanks for reporting! We couldn't reproduce this with the provided info.\n\n**Could you provide:**\n1. Minimal reproducible example\n2. Complete error traceback\n3. Sample data (if applicable)\n\nWe'll reopen once we can reproduce.\n```\n\n### Still Valid\n```markdown\nHi @{author},\n\nThanks for reporting! Confirmed this issue.\n\n**Root cause:** {technical explanation}\n\n**Location:** `{filepath}:{line}`\n\n{Next steps or suggested fix}\n```\n\n## Quick Reference\n\n| Category | Key Indicator | Action |\n|----------|---------------|--------|\n| Already Fixed | Found in git log | Point to PR, suggest upgrade |\n| Won't Fix | External/by design | Explain, provide workaround |\n| Not Reproducible | Can't reproduce | Request more info |\n| No Longer Relevant | Old/stale/deprecated | Explain, suggest new issue |\n| Still Valid | Confirmed, no fix | Document root cause, propose fix |\n\n## Checklist\n\n- [ ] Read full issue + comments\n- [ ] Check if external/FiftyOne responsibility\n- [ ] Search codebase for related code\n- [ ] Check git history for recent fixes\n- [ ] Search closed issues for duplicates\n- [ ] Check if documented\n- [ ] Assess value of potential fix\n- [ ] Attempt reproduction (if bug)\n- [ ] Trace user's reproduction steps in source code\n- [ ] Verify workarounds work by checking actual code path\n- [ ] Keep user response simple (no internal code details)\n- [ ] End with 1-2 sentence internal summary\n\n## If User Willing to Contribute\n\nWhen user indicates willingness to contribute, after validating the issue provide:\n\n- **Contribution guide:** https://docs.voxel51.com/contribute/index.html\n- **Discord:** https://discord.com/invite/fiftyone-community (#github-contribution channel)\n- Point to relevant code files for the fix"
              }
            ]
          }
        ]
      }
    }
  ]
}