{
  "owner": {
    "id": "uw-ssec",
    "display_name": "UW Scientific Software Engineering Center",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/122321194?v=4",
    "url": "https://github.com/uw-ssec",
    "bio": "The UW Scientific Software Engineering Center (SSEC) at the eScience Institute leverages local software engineering talent to advance scientific frontiers.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 2,
      "total_commands": 0,
      "total_skills": 13,
      "total_stars": 8,
      "total_forks": 3
    }
  },
  "repos": [
    {
      "full_name": "uw-ssec/rse-plugins",
      "url": "https://github.com/uw-ssec/rse-plugins",
      "description": "Research Software Engineering Plugins for Coding Agents",
      "homepage": "",
      "signals": {
        "stars": 8,
        "forks": 3,
        "pushed_at": "2026-01-09T22:51:15Z",
        "created_at": "2025-11-06T20:21:06Z",
        "license": "BSD-3-Clause"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1854
        },
        {
          "path": ".gitattributes",
          "type": "blob",
          "size": 122
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/validate-plugins.yml",
          "type": "blob",
          "size": 9682
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 4742
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5685
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1528
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 10807
        },
        {
          "path": "community-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 348
        },
        {
          "path": "community-plugins/holoviz-visualization/.mcp.json",
          "type": "blob",
          "size": 866
        },
        {
          "path": "community-plugins/holoviz-visualization/LICENSE",
          "type": "blob",
          "size": 1525
        },
        {
          "path": "community-plugins/holoviz-visualization/README.md",
          "type": "blob",
          "size": 25001
        },
        {
          "path": "community-plugins/holoviz-visualization/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/agents/data-engineer.md",
          "type": "blob",
          "size": 5137
        },
        {
          "path": "community-plugins/holoviz-visualization/agents/geo-spatial-expert.md",
          "type": "blob",
          "size": 5915
        },
        {
          "path": "community-plugins/holoviz-visualization/agents/panel-specialist.md",
          "type": "blob",
          "size": 4515
        },
        {
          "path": "community-plugins/holoviz-visualization/agents/visualization-designer.md",
          "type": "blob",
          "size": 5236
        },
        {
          "path": "community-plugins/holoviz-visualization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/best-practices/README.md",
          "type": "blob",
          "size": 10190
        },
        {
          "path": "community-plugins/holoviz-visualization/references/colormaps",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/colormaps/accessibility.md",
          "type": "blob",
          "size": 12686
        },
        {
          "path": "community-plugins/holoviz-visualization/references/colormaps/colormap-reference.md",
          "type": "blob",
          "size": 10156
        },
        {
          "path": "community-plugins/holoviz-visualization/references/colormaps/holoviews-styling.md",
          "type": "blob",
          "size": 13840
        },
        {
          "path": "community-plugins/holoviz-visualization/references/colormaps/panel-themes.md",
          "type": "blob",
          "size": 14326
        },
        {
          "path": "community-plugins/holoviz-visualization/references/holoviz-ecosystem.md",
          "type": "blob",
          "size": 8831
        },
        {
          "path": "community-plugins/holoviz-visualization/references/library-matrix.md",
          "type": "blob",
          "size": 8186
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-ai/agents-reference.md",
          "type": "blob",
          "size": 11281
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-ai/custom-agents.md",
          "type": "blob",
          "size": 17462
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-ai/deployment.md",
          "type": "blob",
          "size": 11377
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-ai/llm-providers.md",
          "type": "blob",
          "size": 6816
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/deployment.md",
          "type": "blob",
          "size": 6983
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/examples.md",
          "type": "blob",
          "size": 13747
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/layouts.md",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/python-api.md",
          "type": "blob",
          "size": 12081
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/sources.md",
          "type": "blob",
          "size": 14165
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/transforms.md",
          "type": "blob",
          "size": 13947
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/troubleshooting.md",
          "type": "blob",
          "size": 11027
        },
        {
          "path": "community-plugins/holoviz-visualization/references/lumen-dashboards/views.md",
          "type": "blob",
          "size": 13460
        },
        {
          "path": "community-plugins/holoviz-visualization/references/patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/patterns/README.md",
          "type": "blob",
          "size": 9614
        },
        {
          "path": "community-plugins/holoviz-visualization/references/troubleshooting",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/references/troubleshooting/README.md",
          "type": "blob",
          "size": 10900
        },
        {
          "path": "community-plugins/holoviz-visualization/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/advanced-rendering",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/advanced-rendering/SKILL.md",
          "type": "blob",
          "size": 10423
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/colormaps-styling",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/colormaps-styling/SKILL.md",
          "type": "blob",
          "size": 10527
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/data-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/data-visualization/SKILL.md",
          "type": "blob",
          "size": 10017
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/geospatial-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/geospatial-visualization/SKILL.md",
          "type": "blob",
          "size": 9818
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/lumen-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/lumen-ai/SKILL.md",
          "type": "blob",
          "size": 14520
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/lumen-dashboards",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/lumen-dashboards/SKILL.md",
          "type": "blob",
          "size": 15505
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/panel-dashboards",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/panel-dashboards/SKILL.md",
          "type": "blob",
          "size": 10592
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/parameterization",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/parameterization/SKILL.md",
          "type": "blob",
          "size": 13891
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/plotting-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "community-plugins/holoviz-visualization/skills/plotting-fundamentals/SKILL.md",
          "type": "blob",
          "size": 8344
        },
        {
          "path": "pixi.toml",
          "type": "blob",
          "size": 276
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents/astronomy-astrophysics-expert.md",
          "type": "blob",
          "size": 31634
        },
        {
          "path": "plugins/scientific-python-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 269
        },
        {
          "path": "plugins/scientific-python-development/LICENSE",
          "type": "blob",
          "size": 13
        },
        {
          "path": "plugins/scientific-python-development/README.md",
          "type": "blob",
          "size": 20958
        },
        {
          "path": "plugins/scientific-python-development/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/agents/scientific-python-expert.md",
          "type": "blob",
          "size": 9115
        },
        {
          "path": "plugins/scientific-python-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/SKILL.md",
          "type": "blob",
          "size": 11541
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/assets/pre-commit-config.yaml",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/assets/pyproject-ruff-mypy.toml",
          "type": "blob",
          "size": 1517
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/references/COMMON_ISSUES.md",
          "type": "blob",
          "size": 2745
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/references/CONFIGURATION_PATTERNS.md",
          "type": "blob",
          "size": 3366
        },
        {
          "path": "plugins/scientific-python-development/skills/code-quality-tools/references/TYPE_HINTS.md",
          "type": "blob",
          "size": 3645
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/SKILL.md",
          "type": "blob",
          "size": 12364
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/assets/github-actions-pixi.yml",
          "type": "blob",
          "size": 1112
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/assets/pyproject-multi-env.toml",
          "type": "blob",
          "size": 1453
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/assets/pyproject-pixi-example.toml",
          "type": "blob",
          "size": 837
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/references/COMMON_ISSUES.md",
          "type": "blob",
          "size": 3196
        },
        {
          "path": "plugins/scientific-python-development/skills/pixi-package-manager/references/PATTERNS.md",
          "type": "blob",
          "size": 8635
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/SKILL.md",
          "type": "blob",
          "size": 9277
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/.gitignore",
          "type": "blob",
          "size": 441
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/README-template.md",
          "type": "blob",
          "size": 1521
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/github-actions-publish.yml",
          "type": "blob",
          "size": 636
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/pyproject-full-featured.toml",
          "type": "blob",
          "size": 3067
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/pyproject-minimal.toml",
          "type": "blob",
          "size": 1364
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/assets/sphinx-conf.py",
          "type": "blob",
          "size": 822
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/references/COMMON_ISSUES.md",
          "type": "blob",
          "size": 875
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/references/DOCSTRINGS.md",
          "type": "blob",
          "size": 1839
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/references/METADATA.md",
          "type": "blob",
          "size": 3000
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/references/PATTERNS.md",
          "type": "blob",
          "size": 4562
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-packaging/scripts/cli-example.py",
          "type": "blob",
          "size": 1346
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/SKILL.md",
          "type": "blob",
          "size": 13366
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/assets/conftest-example.py",
          "type": "blob",
          "size": 1069
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/assets/github-actions-tests.yml",
          "type": "blob",
          "size": 851
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/assets/pyproject-pytest.toml",
          "type": "blob",
          "size": 1545
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/references/COMMON_PITFALLS.md",
          "type": "blob",
          "size": 2170
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/references/SCIENTIFIC_PATTERNS.md",
          "type": "blob",
          "size": 5043
        },
        {
          "path": "plugins/scientific-python-development/skills/python-testing/references/TEST_PATTERNS.md",
          "type": "blob",
          "size": 16026
        },
        {
          "path": "plugins/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skills/astropy-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skills/astropy-fundamentals/SKILL.md",
          "type": "blob",
          "size": 13919
        },
        {
          "path": "plugins/skills/astropy-fundamentals/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skills/astropy-fundamentals/references/COMMON_ISSUES.md",
          "type": "blob",
          "size": 20133
        },
        {
          "path": "plugins/skills/astropy-fundamentals/references/EXAMPLES.md",
          "type": "blob",
          "size": 29302
        },
        {
          "path": "plugins/skills/astropy-fundamentals/references/PATTERNS.md",
          "type": "blob",
          "size": 20267
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data/SKILL.md",
          "type": "blob",
          "size": 15096
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data/references/COMMON_ISSUES.md",
          "type": "blob",
          "size": 3287
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data/references/EXAMPLES.md",
          "type": "blob",
          "size": 16945
        },
        {
          "path": "plugins/skills/xarray-for-multidimensional-data/references/PATTERNS.md",
          "type": "blob",
          "size": 15085
        }
      ],
      "marketplace": {
        "name": "rse-plugins",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "UW SSEC Team",
          "url": "https://github.com/uw-ssec"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "scientific-python-development",
            "description": "Agents and skills for Scientific Python development and best practices",
            "source": "./plugins/scientific-python-development",
            "category": "development",
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add uw-ssec/rse-plugins",
              "/plugin install scientific-python-development@rse-plugins"
            ],
            "signals": {
              "stars": 8,
              "forks": 3,
              "pushed_at": "2026-01-09T22:51:15Z",
              "created_at": "2025-11-06T20:21:06Z",
              "license": "BSD-3-Clause"
            },
            "commands": [],
            "skills": [
              {
                "name": "code-quality-tools",
                "description": "Configure and use automated code quality tools (ruff, mypy, pre-commit) for scientific Python projects. Use when setting up linting, formatting, type checking, or automated quality gates. Ideal for enforcing code style, catching type errors, managing pre-commit hooks, or integrating quality checks in CI/CD pipelines.",
                "path": "plugins/scientific-python-development/skills/code-quality-tools/SKILL.md",
                "frontmatter": {
                  "name": "code-quality-tools",
                  "description": "Configure and use automated code quality tools (ruff, mypy, pre-commit) for scientific Python projects. Use when setting up linting, formatting, type checking, or automated quality gates. Ideal for enforcing code style, catching type errors, managing pre-commit hooks, or integrating quality checks in CI/CD pipelines."
                },
                "content": "# Code Quality Tools for Scientific Python\n\nMaster the essential code quality tools that keep scientific Python projects maintainable, consistent, and error-free. Learn how to configure **ruff** for lightning-fast linting and formatting, **mypy** for static type checking, and **pre-commit** hooks for automated quality gates. These tools help catch bugs early, enforce consistent style across teams, and make code reviews focus on logic rather than formatting.\n\n**Key Tools:**\n- **Ruff**: Ultra-fast Python linter and formatter (replaces flake8, black, isort, and more)\n- **MyPy**: Static type checker for Python\n- **Pre-commit**: Git hook framework for automated checks\n\n## Quick Reference Card\n\n### Installation & Setup\n```bash\n# Using pixi (recommended for scientific projects)\npixi add --feature dev ruff mypy pre-commit\n\n# Using pip\npip install ruff mypy pre-commit\n\n# Initialize pre-commit\npre-commit install\n```\n\n### Essential Ruff Commands\n```bash\n# Check code (linting)\nruff check .\n\n# Fix auto-fixable issues\nruff check --fix .\n\n# Format code\nruff format .\n\n# Check and format together\nruff check --fix . && ruff format .\n```\n\n### Essential MyPy Commands\n```bash\n# Type check entire project\nmypy src/\n\n# Type check with strict mode\nmypy --strict src/\n\n# Type check specific file\nmypy src/mymodule/analysis.py\n\n# Generate type coverage report\nmypy --html-report mypy-report src/\n```\n\n### Essential Pre-commit Commands\n```bash\n# Run all hooks on all files\npre-commit run --all-files\n\n# Run hooks on staged files only\npre-commit run\n\n# Update hook versions\npre-commit autoupdate\n\n# Skip hooks temporarily (not recommended)\ngit commit --no-verify\n```\n\n### Quick Decision Tree\n\n```\nNeed to enforce code style and catch common errors?\n  YES → Use Ruff (linting + formatting)\n  NO → Skip to type checking\n\nWant to catch type-related bugs before runtime?\n  YES → Add MyPy\n  NO → Ruff alone is sufficient\n\nNeed to ensure checks run automatically?\n  YES → Set up pre-commit hooks\n  NO → Run tools manually (not recommended for teams)\n\nWorking with legacy code without type hints?\n  YES → Start with Ruff only, add MyPy gradually\n  NO → Use both Ruff and MyPy from the start\n```\n\n## When to Use This Skill\n\nUse this skill when you need to establish or improve code quality practices in scientific Python projects:\n\n- Starting a new scientific Python project and want to establish code quality standards from day one\n- Maintaining existing research code that needs consistency and error prevention\n- Collaborating with multiple contributors who need automated style enforcement\n- Preparing code for publication or package distribution\n- Catching bugs early through static type checking before runtime\n- Automating code reviews to focus on logic rather than style\n- Integrating with CI/CD for automated quality checks\n- Migrating from older tools like black, flake8, or isort to modern alternatives\n\n## Core Concepts\n\n### 1. Ruff: The All-in-One Linter and Formatter\n\n**Ruff** is a blazingly fast Python linter and formatter written in Rust that replaces multiple tools you might be using today.\n\n**What Ruff Replaces:**\n- flake8 (linting)\n- black (formatting)\n- isort (import sorting)\n- pyupgrade (syntax modernization)\n- pydocstyle (docstring linting)\n- And 50+ other tools\n\n**Why Ruff for Scientific Python:**\n\nRuff is 10-100x faster than traditional tools, which matters when you have large codebases with thousands of lines of numerical code. Instead of managing multiple configuration files and tool versions, you get a single tool that handles everything. Ruff can auto-fix most issues automatically, saving time during development. It includes NumPy-aware docstring checking, understanding the conventions used throughout the scientific Python ecosystem. Best of all, it's compatible with existing black and flake8 configurations, making migration straightforward.\n\n**Example:**\n```python\n# Before ruff format\nimport sys\nimport os\nimport numpy as np\n\ndef calculate_mean(data):\n    return np.mean(data)\n\n# After ruff format\nimport os\nimport sys\n\nimport numpy as np\n\n\ndef calculate_mean(data):\n    return np.mean(data)\n```\n\nRuff automatically organizes imports (standard library, third party, local) and applies consistent formatting.\n\n### 2. MyPy: Static Type Checking\n\n**MyPy** analyzes type hints to catch errors before your code ever runs. This is especially valuable in scientific computing where dimension mismatches and type errors can lead to subtle bugs in numerical calculations.\n\n**Example of what MyPy catches:**\n\n```python\nimport numpy as np\nfrom numpy.typing import NDArray\n\ndef calculate_mean(data: NDArray[np.float64]) -> float:\n    \"\"\"Calculate mean of array.\"\"\"\n    return float(np.mean(data))\n\n# MyPy catches this error at type-check time:\nresult: int = calculate_mean(np.array([1.0, 2.0, 3.0]))\n# Error: Incompatible types (expression has type \"float\", variable has type \"int\")\n```\n\n**Benefits for Scientific Code:**\n\nType hints catch dimension mismatches in array operations before you run expensive computations. They validate function signatures, ensuring you pass the right types to numerical functions. Type hints serve as documentation, making it clear what types functions expect and return. They prevent None-related bugs that can crash long-running simulations. Modern IDEs use type hints to provide better autocomplete and inline documentation.\n\n### 3. Pre-commit: Automated Quality Gates\n\n**Pre-commit** runs checks automatically before each commit, ensuring code quality standards are maintained without manual intervention.\n\n**Workflow:**\n1. Developer runs `git commit`\n2. Pre-commit automatically runs ruff, mypy, and other checks\n3. If checks fail, commit is blocked\n4. Developer fixes issues and commits again\n5. Once all checks pass, commit succeeds\n\nThis ensures that code quality issues are caught immediately, before they enter the codebase.\n\n## Configuration\n\nSee [assets/pyproject-ruff-mypy.toml](assets/pyproject-ruff-mypy.toml) for complete Ruff and MyPy configuration examples.\n\nSee [assets/pre-commit-config.yaml](assets/pre-commit-config.yaml) for pre-commit hook configuration.\n\n## Configuration Patterns\n\nSee [references/CONFIGURATION_PATTERNS.md](references/CONFIGURATION_PATTERNS.md) for detailed patterns including:\n- Basic Ruff configuration\n- MyPy configuration for scientific Python\n- Pre-commit configuration\n- Ruff rule selection for scientific Python\n- Fixing common Ruff warnings\n\n## Type Hints\n\nSee [references/TYPE_HINTS.md](references/TYPE_HINTS.md) for type hint patterns including:\n- Gradual type hint adoption\n- NumPy array type hints\n- Handling optional and union types\n\n## Common Issues and Solutions\n\nSee [references/COMMON_ISSUES.md](references/COMMON_ISSUES.md) for solutions to:\n- Ruff and Black formatting conflicts\n- MyPy can't find imports\n- Pre-commit hooks too slow\n- Too many Ruff errors on legacy code\n- Type hints break at runtime\n- MyPy errors in test files\n- Ruff conflicts with project style\n- Pre-commit fails in CI\n\n## Best Practices Checklist\n\n### Initial Setup\n- Install ruff, mypy, and pre-commit in dev environment\n- Create `pyproject.toml` with tool configurations\n- Create `.pre-commit-config.yaml`\n- Run `pre-commit install` to enable git hooks\n- Run `pre-commit run --all-files` to check existing code\n- Add quality check tasks to pixi configuration\n\n### Configuration\n- Set appropriate Python version target\n- Enable NumPy-specific rules (NPY) for scientific code\n- Configure NumPy-style docstring checking\n- Set up per-file ignores for special cases (__init__.py, scripts)\n- Configure mypy strictness appropriate for project maturity\n- Install type stubs for third-party libraries\n\n### Workflow Integration\n- Add quality checks to CI/CD pipeline\n- Document quality standards in CONTRIBUTING.md\n- Create pixi tasks for common quality checks\n- Set up IDE integration (VS Code, PyCharm)\n- Configure editor to run ruff on save\n- Add quality check badge to README\n\n### Team Practices\n- Run `ruff check --fix` before committing\n- Run `ruff format` before committing\n- Address mypy errors (don't use `# type: ignore` without reason)\n- Review pre-commit failures before using `--no-verify`\n- Keep pre-commit hooks updated (`pre-commit autoupdate`)\n- Add type hints to new functions\n- Gradually add types to existing code\n\n### Maintenance\n- Update ruff regularly (fast-moving project)\n- Update pre-commit hook versions monthly\n- Review and adjust ignored rules as project matures\n- Increase mypy strictness gradually\n- Monitor CI/CD for quality check failures\n- Refactor code flagged by quality tools\n\n## Resources and References\n\n### Official Documentation\n- **Ruff**: https://docs.astral.sh/ruff/\n- **MyPy**: https://mypy.readthedocs.io/\n- **Pre-commit**: https://pre-commit.com/\n- **NumPy Typing**: https://numpy.org/devdocs/reference/typing.html\n\n### Ruff Resources\n- Rule reference: https://docs.astral.sh/ruff/rules/\n- Configuration: https://docs.astral.sh/ruff/configuration/\n- Editor integrations: https://docs.astral.sh/ruff/integrations/\n- Migration guide: https://docs.astral.sh/ruff/faq/#how-does-ruff-compare-to-flake8\n\n### MyPy Resources\n- Type hints cheat sheet: https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html\n- Common issues: https://mypy.readthedocs.io/en/stable/common_issues.html\n- Running mypy: https://mypy.readthedocs.io/en/stable/running_mypy.html\n- Type stubs: https://mypy.readthedocs.io/en/stable/stubs.html\n\n### Pre-commit Resources\n- Supported hooks: https://pre-commit.com/hooks.html\n- Creating hooks: https://pre-commit.com/index.html#creating-new-hooks\n- CI integration: https://pre-commit.ci/\n\n### Scientific Python Resources\n- Scientific Python Development Guide: https://learn.scientific-python.org/development/\n- NumPy documentation style: https://numpydoc.readthedocs.io/\n- Type hints for scientific code: https://numpy.org/devdocs/reference/typing.html\n\n## Summary\n\nCode quality tools are essential for maintaining scientific Python projects. Ruff provides fast, comprehensive linting and formatting. MyPy catches type errors before runtime. Pre-commit automates quality checks in your workflow.\n\n**Key takeaways:**\n\nStart with ruff for immediate impact as it replaces multiple tools with a single fast solution. Add mypy gradually as you add type hints to catch bugs early. Use pre-commit to enforce standards automatically without manual intervention. Integrate with pixi for reproducible development environments. Configure tools in pyproject.toml for centralized management. Run quality checks in CI/CD to maintain standards across the team.\n\n**Next steps:**\n\nSet up ruff and pre-commit in your project today. Add type hints to new functions you write. Gradually increase mypy strictness as your codebase matures. Share configurations with your team for consistency. Integrate quality checks into your development workflow.\n\nQuality tools save time by catching errors early and maintaining consistency across your scientific codebase. They make code reviews more productive by automating style discussions, allowing reviewers to focus on scientific correctness and algorithmic choices rather than formatting details."
              },
              {
                "name": "pixi-package-manager",
                "description": "Manage scientific Python dependencies and environments using pixi package manager with unified conda-forge and PyPI support. Use when setting up project environments, managing dependencies, creating reproducible workflows, or working with complex scientific packages requiring compiled libraries. Ideal for multi-environment projects, cross-platform development, and replacing conda/mamba workflows.",
                "path": "plugins/scientific-python-development/skills/pixi-package-manager/SKILL.md",
                "frontmatter": {
                  "name": "pixi-package-manager",
                  "description": "Manage scientific Python dependencies and environments using pixi package manager with unified conda-forge and PyPI support. Use when setting up project environments, managing dependencies, creating reproducible workflows, or working with complex scientific packages requiring compiled libraries. Ideal for multi-environment projects, cross-platform development, and replacing conda/mamba workflows."
                },
                "content": "# Pixi Package Manager for Scientific Python\n\nMaster **pixi**, the modern package manager that unifies conda and PyPI ecosystems for fast, reproducible scientific Python development. Learn how to manage complex scientific dependencies, create isolated environments, and build reproducible workflows using `pyproject.toml` integration.\n\n**Official Documentation**: https://pixi.sh\n**GitHub**: https://github.com/prefix-dev/pixi\n\n## Quick Reference Card\n\n### Installation & Setup\n```bash\n# Install pixi (macOS/Linux)\ncurl -fsSL https://pixi.sh/install.sh | bash\n\n# Install pixi (Windows)\niwr -useb https://pixi.sh/install.ps1 | iex\n\n# Initialize new project with pyproject.toml\npixi init --format pyproject\n\n# Initialize existing Python project\npixi init --format pyproject --import-environment\n```\n\n### Essential Commands\n```bash\n# Add dependencies\npixi add numpy scipy pandas              # conda packages\npixi add --pypi pytest-cov               # PyPI-only packages\npixi add --feature dev pytest ruff       # dev environment\n\n# Install all dependencies\npixi install\n\n# Run commands in environment\npixi run python script.py\npixi run pytest\n\n# Shell with environment activated\npixi shell\n\n# Add tasks\npixi task add test \"pytest tests/\"\npixi task add docs \"sphinx-build docs/ docs/_build\"\n\n# Run tasks\npixi run test\npixi run docs\n\n# Update dependencies\npixi update numpy                         # update specific\npixi update                              # update all\n\n# List packages\npixi list\npixi tree numpy                          # show dependency tree\n```\n\n### Quick Decision Tree: Pixi vs UV vs Both\n\n```\nNeed compiled scientific libraries (NumPy, SciPy, GDAL)?\n├─ YES → Use pixi (conda-forge has pre-built binaries)\n└─ NO → Consider uv for pure Python projects\n\nNeed multi-language support (Python + R, Julia, C++)?\n├─ YES → Use pixi (supports conda ecosystem)\n└─ NO → uv sufficient for Python-only\n\nNeed multiple environments (dev, test, prod, GPU, CPU)?\n├─ YES → Use pixi features for environment management\n└─ NO → Single environment projects work with either\n\nNeed reproducible environments across platforms?\n├─ CRITICAL → Use pixi (lockfiles include all platforms)\n└─ LESS CRITICAL → uv also provides lockfiles\n\nWant to use both conda-forge AND PyPI packages?\n├─ YES → Use pixi (seamless integration)\n└─ ONLY PYPI → uv is simpler and faster\n\nLegacy conda environment files (environment.yml)?\n├─ YES → pixi can import and modernize\n└─ NO → Start fresh with pixi or uv\n```\n\n## When to Use This Skill\n\n- **Setting up scientific Python projects** with complex compiled dependencies (NumPy, SciPy, Pandas, scikit-learn, GDAL, netCDF4)\n- **Building reproducible research environments** that work identically across different machines and platforms\n- **Managing multi-language projects** that combine Python with R, Julia, C++, or Fortran\n- **Creating multiple environment configurations** for different hardware (GPU/CPU), testing scenarios, or deployment targets\n- **Replacing conda/mamba workflows** with faster, more reliable dependency resolution\n- **Developing packages that depend on both conda-forge and PyPI** packages\n- **Migrating from environment.yml or requirements.txt** to modern, reproducible workflows\n- **Running automated scientific workflows** with task runners and CI/CD integration\n- **Working with geospatial, climate, or astronomy packages** that require complex C/Fortran dependencies\n\n## Core Concepts\n\n### 1. Unified Package Management (conda + PyPI)\n\nPixi resolves dependencies from **both conda-forge and PyPI** in a single unified graph, ensuring compatibility:\n\n```toml\n[project]\nname = \"my-science-project\"\ndependencies = [\n    \"numpy>=1.24\",      # from conda-forge (optimized builds)\n    \"pandas>=2.0\",      # from conda-forge\n]\n\n[tool.pixi.pypi-dependencies]\nmy-custom-pkg = \">=1.0\"        # PyPI-only package\n```\n\n**Why this matters for scientific Python:**\n- Get optimized NumPy/SciPy builds from conda-forge (MKL, OpenBLAS)\n- Use PyPI packages not available in conda\n- Single lockfile ensures all dependencies are compatible\n\n### 2. Multi-Platform Lockfiles\n\nPixi generates `pixi.lock` with dependency specifications for **all platforms** (Linux, macOS, Windows, different architectures):\n\n```toml\n# pixi.lock includes:\n# - linux-64\n# - osx-64, osx-arm64\n# - win-64\n```\n\n**Benefits:**\n- Commit lockfile to git → everyone gets identical environments\n- Works on collaborator's different OS without changes\n- CI/CD uses exact same versions as local development\n\n### 3. Feature-Based Environments\n\nCreate multiple environments using **features** without duplicating dependencies:\n\n```toml\n[tool.pixi.feature.test.dependencies]\npytest = \">=7.0\"\npytest-cov = \">=4.0\"\n\n[tool.pixi.feature.gpu.dependencies]\npytorch-cuda = \"11.8.*\"\n\n[tool.pixi.environments]\ntest = [\"test\"]\ngpu = [\"gpu\"]\ngpu-test = [\"gpu\", \"test\"]  # combines features\n```\n\n### 4. Task Automation\n\nDefine reusable commands as tasks:\n\n```toml\n[tool.pixi.tasks]\ntest = \"pytest tests/ -v\"\nformat = \"ruff format src/ tests/\"\nlint = \"ruff check src/ tests/\"\ndocs = \"sphinx-build docs/ docs/_build\"\nanalyse = { cmd = \"python scripts/analyze.py\", depends-on = [\"test\"] }\n```\n\n### 5. Fast Dependency Resolution\n\nPixi uses **rattler** (Rust-based conda resolver) for 10-100x faster resolution than conda:\n\n- Parallel package downloads\n- Efficient caching\n- Smart dependency solver\n\n### 6. pyproject.toml Integration\n\nPixi reads standard Python project metadata from `pyproject.toml`, enabling:\n- Single source of truth for project configuration\n- Compatibility with pip, uv, and other tools\n- Standard Python packaging workflows\n\n## Quick Start\n\n### Minimal Example: Data Analysis Project\n\n```bash\n# Create new project\nmkdir climate-analysis && cd climate-analysis\npixi init --format pyproject\n\n# Add scientific stack\npixi add python=3.11 numpy pandas matplotlib xarray\n\n# Add development tools\npixi add --feature dev pytest ipython ruff\n\n# Create analysis script\ncat > analyze.py << 'EOF'\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Your analysis code\ndata = pd.read_csv(\"data.csv\")\ndata.plot()\nplt.savefig(\"output.png\")\nEOF\n\n# Run in pixi environment\npixi run python analyze.py\n\n# Or activate shell\npixi shell\npython analyze.py\n```\n\n## Patterns\n\nSee [references/PATTERNS.md](references/PATTERNS.md) for detailed patterns including:\n- Converting existing projects to Pixi\n- Multi-environment scientific workflows\n- Scientific library development\n- Conda + PyPI dependency strategy\n- Reproducible research environments\n- Task dependencies and workflows\n\n## File Templates\n\nReady-to-use templates are available in the `assets/` directory:\n\n- **[assets/pyproject-pixi-example.toml](assets/pyproject-pixi-example.toml)** - Basic pixi project configuration\n- **[assets/pyproject-multi-env.toml](assets/pyproject-multi-env.toml)** - Multi-environment configuration example\n- **[assets/github-actions-pixi.yml](assets/github-actions-pixi.yml)** - GitHub Actions workflow for pixi\n\n## Common Issues and Solutions\n\nSee [references/COMMON_ISSUES.md](references/COMMON_ISSUES.md) for solutions to:\n- Package not found in conda-forge\n- Conflicting dependencies\n- Slow environment creation\n- Platform-specific failures\n- PyPI package installation fails\n- Lockfile merge conflicts\n- Editable install of local package\n\n## Best Practices Checklist\n\n### Project Setup\n- [ ] Use `pixi init --format pyproject` for new projects\n- [ ] Set explicit Python version constraint (`python>=3.11,<3.13`)\n- [ ] Organize dependencies by source (conda vs PyPI)\n- [ ] Create separate features for dev, test, docs environments\n- [ ] Define useful tasks for common workflows\n- [ ] Set up `.gitignore` to exclude `.pixi/` directory\n\n### Dependency Management\n- [ ] Prefer conda-forge for compiled scientific packages (NumPy, SciPy, GDAL)\n- [ ] Use PyPI only for pure Python or conda-unavailable packages\n- [ ] Pin exact versions for reproducible research\n- [ ] Use version ranges for libraries (allow updates)\n- [ ] Specify solve groups for independent environment solving\n- [ ] Use `pixi update` regularly to get security patches\n\n### Reproducibility\n- [ ] Commit `pixi.lock` to version control\n- [ ] Include all platforms in lockfile for cross-platform teams\n- [ ] Document environment recreation steps in README\n- [ ] Use exact version pins for published research\n- [ ] Test environment from scratch periodically\n- [ ] Archive environments for long-term preservation\n\n### Performance\n- [ ] Use pixi's parallel downloads (automatic)\n- [ ] Leverage caching in CI/CD (`prefix-dev/setup-pixi` action)\n- [ ] Keep environments minimal (only necessary dependencies)\n- [ ] Use solve groups to isolate independent environments\n- [ ] Clean old packages with `pixi clean cache`\n\n### Development Workflow\n- [ ] Define tasks for common operations (test, lint, format)\n- [ ] Use task dependencies for complex workflows\n- [ ] Create environment-specific tasks when needed\n- [ ] Use `pixi shell` for interactive development\n- [ ] Use `pixi run` for automated scripts and CI\n- [ ] Test in clean environment before releasing\n\n## Resources\n\n### Official Documentation\n- **Pixi Website**: https://pixi.sh\n- **Documentation**: https://pixi.sh/latest/\n- **GitHub Repository**: https://github.com/prefix-dev/pixi\n- **Configuration Reference**: https://pixi.sh/latest/reference/project_configuration/\n\n### Community & Support\n- **Discord**: https://discord.gg/kKV8ZxyzY4\n- **GitHub Discussions**: https://github.com/prefix-dev/pixi/discussions\n- **Issue Tracker**: https://github.com/prefix-dev/pixi/issues\n\n### Related Technologies\n- **Conda-forge**: https://conda-forge.org/\n- **Rattler**: https://github.com/mamba-org/rattler (underlying solver)\n- **PyPI**: https://pypi.org/\n- **UV Package Manager**: https://github.com/astral-sh/uv\n\n### Complementary Skills\n- **scientific-python-packaging**: Modern Python packaging patterns\n- **scientific-python-testing**: Testing strategies with pytest\n- **uv-package-manager**: Fast pure-Python package management\n\n## Summary\n\nPixi revolutionizes scientific Python development by unifying conda and PyPI ecosystems with blazing-fast dependency resolution, reproducible multi-platform lockfiles, and seamless environment management. By leveraging `pyproject.toml` integration, pixi provides a modern, standards-compliant approach to managing complex scientific dependencies while maintaining compatibility with the broader Python ecosystem.\n\n**Key advantages for scientific computing:**\n\n1. **Optimized Scientific Packages**: Access conda-forge's pre-built binaries for NumPy, SciPy, and other compiled packages with MKL/OpenBLAS optimizations\n2. **Complex Dependencies Made Simple**: Handle challenging packages like GDAL, netCDF4, and HDF5 that require C/Fortran/C++ system libraries\n3. **True Reproducibility**: Multi-platform lockfiles ensure identical environments across Linux, macOS, and Windows\n4. **Flexible Environment Management**: Feature-based environments for dev/test/prod, GPU/CPU, or any custom configuration\n5. **Fast and Reliable**: 10-100x faster than conda with Rust-based parallel dependency resolution\n6. **Task Automation**: Built-in task runner for scientific workflows, testing, and documentation\n7. **Best of Both Worlds**: Seamlessly mix conda-forge optimized packages with PyPI's vast ecosystem\n\nWhether you're conducting reproducible research, developing scientific software, or managing complex data analysis pipelines, pixi provides the robust foundation for modern scientific Python development. By replacing conda/mamba with pixi, you gain speed, reliability, and modern workflows while maintaining full access to the scientific Python ecosystem.\n\n**Ready to get started?** Install pixi, initialize your project with `pixi init --format pyproject`, and experience the future of scientific Python package management."
              },
              {
                "name": "python-packaging",
                "description": "Create and publish distributable scientific Python packages following Scientific Python community best practices with pyproject.toml, src layout, and Hatchling. Use when building Python libraries, publishing to PyPI, structuring research software, creating command-line tools, or preparing packages for distribution. Ideal for package metadata configuration, dependency management, and automated publishing workflows.",
                "path": "plugins/scientific-python-development/skills/python-packaging/SKILL.md",
                "frontmatter": {
                  "name": "python-packaging",
                  "description": "Create and publish distributable scientific Python packages following Scientific Python community best practices with pyproject.toml, src layout, and Hatchling. Use when building Python libraries, publishing to PyPI, structuring research software, creating command-line tools, or preparing packages for distribution. Ideal for package metadata configuration, dependency management, and automated publishing workflows."
                },
                "content": "# Scientific Python Packaging\n\nA comprehensive guide to creating, structuring, and distributing Python packages for scientific computing, following the [Scientific Python Community guidelines](https://learn.scientific-python.org/development/guides/packaging-simple/). This skill focuses on modern packaging standards using `pyproject.toml`, PEP 621 metadata, and the Hatchling build backend.\n\n## Quick Decision Tree\n\n**Package Structure Selection:**\n```\nSTART\n  ├─ Pure Python scientific package (most common) → Pattern 1 (src/ layout)\n  ├─ Need data files with package → Pattern 2 (data/ subdirectory)\n  ├─ CLI tool → Pattern 5 (add [project.scripts])\n  └─ Complex multi-feature package → Pattern 3 (full-featured)\n```\n\n**Build Backend Choice:**\n```\nSTART → Use Hatchling (recommended for scientific Python)\n  ├─ Need VCS versioning? → Add hatch-vcs plugin\n  ├─ Simple manual versioning? → version = \"X.Y.Z\" in pyproject.toml\n  └─ Dynamic from __init__.py? → [tool.hatch.version] path\n```\n\n**Dependency Management:**\n```\nSTART\n  ├─ Runtime dependencies → [project] dependencies\n  ├─ Optional features → [project.optional-dependencies]\n  ├─ Development tools → [dependency-groups] (PEP 735)\n  └─ Version constraints → Use >= for minimum, avoid upper caps\n```\n\n**Publishing Workflow:**\n```\n1. Build: python -m build\n2. Check: twine check dist/*\n3. Test: twine upload --repository testpypi dist/*\n4. Verify: pip install --index-url https://test.pypi.org/simple/ pkg\n5. Publish: twine upload dist/*\n```\n\n**Common Task Quick Reference:**\n```bash\n# Setup new package\nmkdir -p my-pkg/src/my_pkg && cd my-pkg\n# Create pyproject.toml with [build-system] and [project] sections\n\n# Development install\npip install -e . --group dev\n\n# Build distributions\npython -m build\n\n# Test installation\npip install dist/*.whl\n\n# Publish\ntwine upload dist/*\n```\n\n## When to Use This Skill\n\n- Creating scientific Python libraries for distribution\n- Building research software packages with proper structure\n- Publishing scientific packages to PyPI\n- Setting up reproducible scientific Python projects\n- Creating installable packages with scientific dependencies\n- Implementing command-line tools for scientific workflows\n- Following community standards for scientific Python development\n- Preparing packages for peer review and publication\n\n## Core Concepts\n\n### 1. Modern Build Systems\n\nPython packages now use standardized build systems instead of classic `setup.py`:\n\n- **PEP 621**: Standardized project metadata in `pyproject.toml`\n- **PEP 517/518**: Build system independence\n- **Build backend**: Hatchling\n- **No classic files**: No `setup.py`, `setup.cfg`, or `MANIFEST.in`\n\n### 2. Build Backend: Hatchling\n\n- **Hatchling**: Excellent balance of speed, configurability, and extendability\n- Modern, standards-compliant build backend\n- Automatic package discovery in `src/` layout\n- VCS-aware file inclusion for SDists\n- Extensible through plugins\n\n### 3. Package Structure\n\n- **src/ layout**: Required for proper isolation (prevents importing uninstalled code)\n- **Automatic discovery**: Hatchling auto-detects packages in `src/`\n- **Standard structure**: Consistent organization for testing and documentation\n\n### 4. Scientific Python Standards\n\n- **Dependency management**: Careful version constraints\n- **Python version support**: Minimum version without upper caps\n- **Development dependencies**: Use dependency-groups (PEP 735)\n- **Documentation**: Include README, LICENSE, and docs folder\n- **Testing**: Dedicated tests folder\n\n## Quick Start\n\n### Minimal Scientific Package Structure\n\n```\nmy-sci-package/\n├── pyproject.toml\n├── README.md\n├── LICENSE\n├── src/\n│   └── my_sci_package/\n│       ├── __init__.py\n│       ├── analysis.py\n│       └── utils.py\n├── tests/\n│   ├── test_analysis.py\n│   └── test_utils.py\n└── docs/\n    └── index.md\n```\n\nSee [assets/pyproject-minimal.toml](assets/pyproject-minimal.toml) for a complete minimal `pyproject.toml` template.\n\n## Package Structure Patterns\n\nSee [references/PATTERNS.md](references/PATTERNS.md) for detailed package structure patterns including:\n- Pure Python scientific package (recommended)\n- Scientific package with data files\n- Versioning strategies\n- Building and publishing workflows\n- Testing installation\n\n## Project Metadata\n\nSee [references/METADATA.md](references/METADATA.md) for detailed information on:\n- License configuration (SPDX format)\n- Python version requirements\n- Dependency management\n- Classifiers\n- Optional dependencies (extras)\n- Development dependencies (dependency groups)\n\n## Command-Line Interface\n\nFor CLI tool implementation, see [scripts/cli-example.py](scripts/cli-example.py) for a complete example using Click.\n\n**Register in pyproject.toml:**\n```toml\n[project.scripts]\nsci-analyze = \"my_sci_package.cli:main\"\n```\n\n## File Templates\n\nReady-to-use templates are available in the `assets/` directory:\n\n- **[assets/.gitignore](assets/.gitignore)** - `.gitignore` for scientific Python packages\n- **[assets/pyproject-minimal.toml](assets/pyproject-minimal.toml)** - Minimal `pyproject.toml` template\n- **[assets/pyproject-full-featured.toml](assets/pyproject-full-featured.toml)** - Full-featured `pyproject.toml` with all options\n- **[assets/README-template.md](assets/README-template.md)** - README template for scientific packages\n- **[assets/sphinx-conf.py](assets/sphinx-conf.py)** - Sphinx documentation configuration\n- **[assets/github-actions-publish.yml](assets/github-actions-publish.yml)** - GitHub Actions workflow for publishing\n\n## Documentation\n\n### NumPy-style Docstrings\n\nSee [references/DOCSTRINGS.md](references/DOCSTRINGS.md) for examples of NumPy-style docstrings and documentation best practices.\n\n## Checklist for Publishing Scientific Packages\n\n- [ ] Code is tested with pytest (>90% coverage recommended)\n- [ ] Documentation is complete (README, docstrings, Sphinx docs)\n- [ ] Version number follows semantic versioning\n- [ ] CHANGELOG.md or NEWS.md updated\n- [ ] LICENSE file included with appropriate license\n- [ ] pyproject.toml has complete metadata\n- [ ] Package uses src/ layout\n- [ ] Package builds without errors (`python -m build`)\n- [ ] SDist contents verified (`tar -tvf dist/*.tar.gz`)\n- [ ] Installation tested in clean environment\n- [ ] CLI tools work if applicable\n- [ ] All classifiers are appropriate\n- [ ] Python version constraint is correct (no upper bound)\n- [ ] Dependencies have appropriate version constraints\n- [ ] Repository is linked in project.urls\n- [ ] Tested on TestPyPI first\n- [ ] GitHub release created (if using)\n- [ ] Documentation published (ReadTheDocs, GitHub Pages)\n- [ ] Citation information included (CITATION.cff or README)\n\n## Best Practices for Scientific Python Packages\n\n1. **Use src/ layout** - Prevents importing uninstalled code, ensures proper testing\n2. **Use pyproject.toml** - Modern standard, tool-independent configuration\n3. **Use Hatchling** - Modern, fast, and configurable build backend\n4. **No classic files** - Avoid setup.py, setup.cfg, MANIFEST.in\n5. **Version constraints** - Minimum versions for dependencies, no upper cap for Python\n6. **Test SDist contents** - Always verify what files are included/excluded\n7. **Use TestPyPI** - Always test publishing before going to production\n8. **Document thoroughly** - README, docstrings, Sphinx documentation\n9. **Include LICENSE** - Use SPDX identifiers, choose appropriate scientific license\n10. **Use dependency-groups** - For development dependencies (PEP 735)\n11. **Semantic versioning** - Clear versioning strategy\n12. **Automate CI/CD** - GitHub Actions for testing and publishing\n13. **Type hints** - Include py.typed marker for typed packages\n14. **Citation information** - Make it easy for users to cite your work\n15. **Community standards** - Follow Scientific Python guidelines\n\n## Common Issues and Solutions\n\nSee [references/COMMON_ISSUES.md](references/COMMON_ISSUES.md) for solutions to:\n- Import errors in tests\n- Missing files in distribution\n- Dependency conflicts\n- Python version incompatibility\n\n## Resources\n\n- **Scientific Python Development Guide**: https://learn.scientific-python.org/development/\n- **Simple Packaging Guide**: https://learn.scientific-python.org/development/guides/packaging-simple/\n- **Python Packaging Guide**: https://packaging.python.org/\n- **PyPI**: https://pypi.org/\n- **TestPyPI**: https://test.pypi.org/\n- **Hatchling documentation**: https://hatch.pypa.io/latest/\n- **build**: https://pypa-build.readthedocs.io/\n- **twine**: https://twine.readthedocs.io/\n- **Scientific Python Cookie**: https://github.com/scientific-python/cookie\n- **NumPy documentation style**: https://numpydoc.readthedocs.io/"
              },
              {
                "name": "python-testing",
                "description": "Write and organize tests for scientific Python packages using pytest following Scientific Python community best practices. Use when setting up test suites, writing unit tests, integration tests, testing numerical algorithms, configuring test fixtures, parametrizing tests, or setting up continuous integration. Ideal for testing scientific computations, validating numerical accuracy, and ensuring code correctness.",
                "path": "plugins/scientific-python-development/skills/python-testing/SKILL.md",
                "frontmatter": {
                  "name": "python-testing",
                  "description": "Write and organize tests for scientific Python packages using pytest following Scientific Python community best practices. Use when setting up test suites, writing unit tests, integration tests, testing numerical algorithms, configuring test fixtures, parametrizing tests, or setting up continuous integration. Ideal for testing scientific computations, validating numerical accuracy, and ensuring code correctness."
                },
                "content": "# Scientific Python Testing with pytest\n\nA comprehensive guide to writing effective tests for scientific Python packages using pytest, following the [Scientific Python Community guidelines](https://learn.scientific-python.org/development/guides/pytest/) and [testing tutorial](https://learn.scientific-python.org/development/tutorials/test/). This skill focuses on modern testing patterns, fixtures, parametrization, and best practices specific to scientific computing.\n\n## Quick Reference Card\n\n**Common Testing Tasks - Quick Decisions:**\n\n```python\n# 1. Basic test → Use simple assert\ndef test_function():\n    assert result == expected\n\n# 2. Floating-point comparison → Use approx\nfrom pytest import approx\nassert result == approx(0.333, rel=1e-6)\n\n# 3. Testing exceptions → Use pytest.raises\nwith pytest.raises(ValueError, match=\"must be positive\"):\n    function(-1)\n\n# 4. Multiple inputs → Use parametrize\n@pytest.mark.parametrize(\"input,expected\", [(1,1), (2,4), (3,9)])\ndef test_square(input, expected):\n    assert input**2 == expected\n\n# 5. Reusable setup → Use fixture\n@pytest.fixture\ndef sample_data():\n    return np.array([1, 2, 3, 4, 5])\n\n# 6. NumPy arrays → Use approx or numpy.testing\nassert np.mean(data) == approx(3.0)\n```\n\n**Decision Tree:**\n- Need multiple test cases with same logic? → **Parametrize**\n- Need reusable test data/setup? → **Fixture**\n- Testing floating-point results? → **pytest.approx**\n- Testing exceptions/warnings? → **pytest.raises / pytest.warns**\n- Complex numerical arrays? → **numpy.testing.assert_allclose**\n- Organizing by speed? → **Markers and separate directories**\n\n## When to Use This Skill\n\n- Writing tests for scientific Python packages and libraries\n- Testing numerical algorithms and scientific computations\n- Setting up test infrastructure for research software\n- Implementing continuous integration for scientific code\n- Testing data analysis pipelines and workflows\n- Validating scientific simulations and models\n- Ensuring reproducibility and correctness of research code\n- Testing code that uses NumPy, SciPy, Pandas, and other scientific libraries\n\n## Core Concepts\n\n### 1. Why pytest for Scientific Python\n\npytest is the de facto standard for testing Python packages because it:\n\n- **Simple syntax**: Just use Python's `assert` statement\n- **Detailed reporting**: Clear, informative failure messages\n- **Powerful features**: Fixtures, parametrization, marks, plugins\n- **Scientific ecosystem**: Native support for NumPy arrays, approximate comparisons\n- **Community standard**: Used by NumPy, SciPy, Pandas, scikit-learn, and more\n\n### 2. Test Structure and Organization\n\n**Standard test directory layout:**\n\n```text\nmy-package/\n├── src/\n│   └── my_package/\n│       ├── __init__.py\n│       ├── analysis.py\n│       └── utils.py\n├── tests/\n│   ├── conftest.py\n│   ├── test_analysis.py\n│   └── test_utils.py\n└── pyproject.toml\n```\n\n**Key principles:**\n\n- Tests directory separate from source code (alongside `src/`)\n- Test files named `test_*.py` (pytest discovery)\n- Test functions named `test_*` (pytest discovery)\n- No `__init__.py` in tests directory (avoid importability issues)\n- Test against installed package, not local source\n\n### 3. pytest Configuration\n\nSee [assets/pyproject-pytest.toml](assets/pyproject-pytest.toml) for a complete pytest configuration example.\n\nBasic configuration in `pyproject.toml`:\n\n```toml\n[tool.pytest.ini_options]\nminversion = \"7.0\"\naddopts = [\n    \"-ra\",              # Show summary of all test outcomes\n    \"--showlocals\",     # Show local variables in tracebacks\n    \"--strict-markers\", # Error on undefined markers\n    \"--strict-config\",  # Error on config issues\n]\ntestpaths = [\"tests\"]\n```\n\n## Testing Principles\n\nFollowing the [Scientific Python testing recommendations](https://learn.scientific-python.org/development/principles/testing/), effective testing provides multiple benefits and should follow key principles:\n\n### Advantages of Testing\n\n- **Trustworthy code**: Well-tested code behaves as expected and can be relied upon\n- **Living documentation**: Tests communicate intent and expected behavior, validated with each run\n- **Preventing failure**: Tests protect against implementation errors and unexpected dependency changes\n- **Confidence when making changes**: Thorough test suites enable adding features, fixing bugs, and refactoring with confidence\n\n### Fundamental Principles\n\n**1. Any test case is better than none**\n\nWhen in doubt, write the test that makes sense at the time:\n- Test critical behaviors, features, and logic\n- Write clear, expressive, well-documented tests\n- Tests are documentation of developer intentions\n- Good tests make it clear what they are testing and how\n\nDon't get bogged down in taxonomy when learning—focus on writing tests that work.\n\n**2. As long as that test is correct**\n\nIt's surprisingly easy to write tests that pass when they should fail:\n- **Check that your test fails when it should**: Deliberately break the code and verify the test fails\n- **Keep it simple**: Excessive mocks and fixtures make it difficult to know what's being tested\n- **Test one thing at a time**: A single test should test a single behavior\n\n**3. Start with Public Interface Tests**\n\nBegin by testing from the perspective of a user:\n- Test code as users will interact with it\n- Keep tests simple and readable for documentation purposes\n- Focus on supported use cases\n- Avoid testing private attributes\n- Minimize use of mocks/patches\n\n**4. Organize Tests into Suites**\n\nDivide tests by type and execution time for efficiency:\n- **Unit tests**: Fast, isolated tests of individual components\n- **Integration tests**: Tests of component interactions and dependencies\n- **End-to-end tests**: Complete workflow testing\n\nBenefits:\n- Run relevant tests quickly and frequently\n- \"Fail fast\" by running fast suites first\n- Easier to read and reason about\n- Avoid false positives from expected external failures\n\n### Outside-In Testing Approach\n\nThe recommended approach is **outside-in**, starting from the user's perspective:\n\n1. **Public Interface Tests**: Test from user perspective, focusing on behavior and features\n2. **Integration Tests**: Test that components work together and with dependencies\n3. **Unit Tests**: Test individual units in isolation, optimized for speed\n\nThis approach ensures you're building the right thing before optimizing implementation details.\n\n## Quick Start\n\n### Minimal Test Example\n\n```python\n# tests/test_basic.py\n\ndef test_simple_math():\n    \"\"\"Test basic arithmetic.\"\"\"\n    assert 4 == 2**2\n\ndef test_string_operations():\n    \"\"\"Test string methods.\"\"\"\n    result = \"hello world\".upper()\n    assert result == \"HELLO WORLD\"\n    assert \"HELLO\" in result\n```\n\n### Scientific Test Example\n\n```python\n# tests/test_scientific.py\nimport numpy as np\nfrom pytest import approx\n\nfrom my_package.analysis import compute_mean, fit_linear\n\ndef test_compute_mean():\n    \"\"\"Test mean calculation.\"\"\"\n    data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    result = compute_mean(data)\n    assert result == approx(3.0)\n\ndef test_fit_linear():\n    \"\"\"Test linear regression.\"\"\"\n    x = np.array([0, 1, 2, 3, 4])\n    y = np.array([0, 2, 4, 6, 8])\n    slope, intercept = fit_linear(x, y)\n    \n    assert slope == approx(2.0)\n    assert intercept == approx(0.0)\n```\n\n## Testing Patterns\n\nSee [references/TEST_PATTERNS.md](references/TEST_PATTERNS.md) for detailed patterns including:\n- Writing simple, focused tests\n- Testing for failures\n- Approximate comparisons\n- Using fixtures\n- Parametrized tests\n- Test organization with markers\n- Mocking and monkeypatching\n- Testing against installed version\n- Import best practices\n\n## Scientific Python Testing Patterns\n\nSee [references/SCIENTIFIC_PATTERNS.md](references/SCIENTIFIC_PATTERNS.md) for scientific-specific patterns:\n- Testing numerical algorithms\n- Testing with different NumPy dtypes\n- Testing random/stochastic code\n- Testing data pipelines\n- Property-based testing with Hypothesis\n\n## Running pytest\n\n### Basic Usage\n\n```bash\n# Run all tests\npytest\n\n# Run specific file\npytest tests/test_analysis.py\n\n# Run specific test\npytest tests/test_analysis.py::test_mean\n\n# Run tests matching pattern\npytest -k \"mean or median\"\n\n# Verbose output\npytest -v\n\n# Show local variables in failures\npytest -l  # or --showlocals\n\n# Stop at first failure\npytest -x\n\n# Show stdout/stderr\npytest -s\n```\n\n### Debugging Tests\n\n```bash\n# Drop into debugger on failure\npytest --pdb\n\n# Drop into debugger at start of each test\npytest --trace\n\n# Run last failed tests\npytest --lf\n\n# Run failed tests first, then rest\npytest --ff\n\n# Show which tests would be run (dry run)\npytest --collect-only\n```\n\n### Coverage\n\n```bash\n# Install pytest-cov\npip install pytest-cov\n\n# Run with coverage\npytest --cov=my_package\n\n# With coverage report\npytest --cov=my_package --cov-report=html\n\n# With missing lines\npytest --cov=my_package --cov-report=term-missing\n\n# Fail if coverage below threshold\npytest --cov=my_package --cov-fail-under=90\n```\n\nSee [assets/pyproject-pytest.toml](assets/pyproject-pytest.toml) for complete coverage configuration.\n\n## File Templates and Examples\n\nReady-to-use templates are available in the `assets/` directory:\n\n- **[assets/pyproject-pytest.toml](assets/pyproject-pytest.toml)** - Complete pytest configuration with coverage\n- **[assets/conftest-example.py](assets/conftest-example.py)** - Example conftest.py with shared fixtures\n- **[assets/github-actions-tests.yml](assets/github-actions-tests.yml)** - GitHub Actions workflow for testing\n\n## Common Pitfalls and Solutions\n\nSee [references/COMMON_PITFALLS.md](references/COMMON_PITFALLS.md) for solutions to:\n- Testing implementation instead of behavior\n- Non-deterministic tests\n- Exact floating-point comparisons\n- Testing too much in one test\n\n## Testing Checklist\n\n- [ ] Tests are in `tests/` directory separate from source\n- [ ] Test files named `test_*.py`\n- [ ] Test functions named `test_*`\n- [ ] Tests run against installed package (use src/ layout)\n- [ ] pytest configured in `pyproject.toml`\n- [ ] Using `pytest.approx` for floating-point comparisons\n- [ ] Tests check exceptions with `pytest.raises`\n- [ ] Tests check warnings with `pytest.warns`\n- [ ] Parametrized tests for multiple inputs\n- [ ] Fixtures for reusable setup\n- [ ] Markers used for test organization\n- [ ] Random tests use fixed seeds\n- [ ] Tests are independent (can run in any order)\n- [ ] Each test focuses on one behavior\n- [ ] Coverage > 80% (preferably > 90%)\n- [ ] All tests pass before committing\n- [ ] Slow tests marked with `@pytest.mark.slow`\n- [ ] Integration tests marked appropriately\n- [ ] CI configured to run tests automatically\n\n## Continuous Integration\n\nSee [assets/github-actions-tests.yml](assets/github-actions-tests.yml) for a complete GitHub Actions workflow example.\n\n## Resources\n\n- **Scientific Python pytest Guide**: <https://learn.scientific-python.org/development/guides/pytest/>\n- **Scientific Python Testing Tutorial**: <https://learn.scientific-python.org/development/tutorials/test/>\n- **Scientific Python Testing Principles**: <https://learn.scientific-python.org/development/principles/testing/>\n- **pytest Documentation**: <https://docs.pytest.org/>\n- **pytest-cov**: <https://pytest-cov.readthedocs.io/>\n- **pytest-mock**: <https://pytest-mock.readthedocs.io/>\n- **Hypothesis (property-based testing)**: <https://hypothesis.readthedocs.io/>\n- **NumPy testing utilities**: <https://numpy.org/doc/stable/reference/routines.testing.html>\n- **Testing best practices**: <https://docs.python-guide.org/writing/tests/>\n\n## Summary\n\nTesting scientific Python code with pytest, following Scientific Python community principles, provides:\n\n1. **Confidence**: Know your code works correctly\n2. **Reproducibility**: Ensure consistent behavior across environments\n3. **Documentation**: Tests show how code should be used and communicate developer intent\n4. **Refactoring safety**: Change code without breaking functionality\n5. **Regression prevention**: Catch bugs before they reach users\n6. **Scientific rigor**: Validate numerical accuracy and physical correctness\n\n**Key testing principles:**\n\n- Start with **public interface tests** from the user's perspective\n- Organize tests into **suites** (unit, integration, e2e) by type and speed\n- Follow **outside-in** approach: public interface → integration → unit tests\n- Keep tests **simple, focused, and independent**\n- Test **behavior rather than implementation**\n- Use pytest's powerful features (fixtures, parametrization, markers) effectively\n- Always verify tests **fail when they should** to avoid false confidence\n\n**Remember**: Any test is better than none, but well-organized tests following these principles create trustworthy, maintainable scientific software that the community can rely on."
              }
            ]
          },
          {
            "name": "holoviz-visualization",
            "description": "Development kit for working with HoloViz ecosystem (Panel, hvPlot, HoloViews, Datashader, GeoViews, Lumen)",
            "source": "./community-plugins/holoviz-visualization",
            "category": "data-science",
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add uw-ssec/rse-plugins",
              "/plugin install holoviz-visualization@rse-plugins"
            ],
            "signals": {
              "stars": 8,
              "forks": 3,
              "pushed_at": "2026-01-09T22:51:15Z",
              "created_at": "2025-11-06T20:21:06Z",
              "license": "BSD-3-Clause"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-rendering",
                "description": "Master high-performance rendering for large datasets with Datashader. Use this skill when working with datasets exceeding 100M+ points, optimizing visualization performance, or implementing efficient rendering strategies with rasterization and colormapping techniques.",
                "path": "community-plugins/holoviz-visualization/skills/advanced-rendering/SKILL.md",
                "frontmatter": {
                  "name": "advanced-rendering",
                  "description": "Master high-performance rendering for large datasets with Datashader. Use this skill when working with datasets exceeding 100M+ points, optimizing visualization performance, or implementing efficient rendering strategies with rasterization and colormapping techniques.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires datashader >= 0.15.0, colorcet >= 3.1.0, holoviews >= 1.18.0, pandas >= 1.0.0, numpy >= 1.15.0"
                },
                "content": "# Advanced Rendering Skill\n\n## Overview\n\nMaster high-performance rendering for large datasets with Datashader and optimization techniques. This skill covers handling 100M+ point datasets, performance tuning, and efficient visualization strategies.\n\n## Dependencies\n\n- datashader >= 0.15.0\n- colorcet >= 3.1.0\n- holoviews >= 1.18.0\n- pandas >= 1.0.0\n- numpy >= 1.15.0\n\n## Core Capabilities\n\n### 1. Datashader Fundamentals\n\nDatashader is designed for rasterizing large datasets:\n\n```python\nimport datashader as ds\nfrom datashader.mpl_ext import _colorize\nimport holoviews as hv\n\n# Load large dataset (can handle 100M+ points)\ndf = pd.read_csv('large_dataset.csv')  # Millions or billions of rows\n\n# Create datashader canvas\ncanvas = ds.Canvas(plot_width=800, plot_height=600)\n\n# Rasterize aggregation\nagg = canvas.points(df, 'x', 'y')\n\n# Convert to image\nimg = agg.to_array(True)\n```\n\n### 2. Efficient Point Rendering\n\n```python\nfrom holoviews.operation.datashader import datashade, aggregate, shade\n\n# Quick datashading with HoloViews\nscatter = hv.Scatter(df, 'x', 'y')\nshaded = datashade(scatter)\n\n# With custom aggregation\nagg = aggregate(scatter, width=800, height=600)\ncolored = shade(agg, cmap='viridis')\n\n# Control rasterization\nfrom holoviews.operation import rasterize\n\nrasterized = rasterize(\n    scatter,\n    aggregator=ds.count(),\n    pixel_ratio=2,\n    upsample_method='interp'\n)\n```\n\n### 3. Color Mapping and Aggregation\n\n```python\nimport datashader as ds\nfrom colorcet import cm\n\n# Count aggregation (heatmap)\ncanvas = ds.Canvas()\nagg = canvas.points(df, 'x', 'y', agg=ds.count())\n\n# Weighted aggregation\nagg = canvas.points(df, 'x', 'y', agg=ds.sum('value'))\n\n# Mean aggregation\nagg = canvas.points(df, 'x', 'y', agg=ds.mean('value'))\n\n# Custom colormapping\nimport datashader.transfer_functions as tf\n\nshaded = tf.shade(agg, cmap=cm['viridis'])\nshaded_with_spread = tf.spread(shaded, px=2)\n```\n\n### 4. Image Compositing\n\n```python\n# Combine multiple datasets\ncanvas = ds.Canvas(x_range=(0, 100), y_range=(0, 100))\n\nagg1 = canvas.points(df1, 'x', 'y')\nagg2 = canvas.points(df2, 'x', 'y')\n\n# Shade separately\nshaded1 = tf.shade(agg1, cmap=cm['reds'])\nshaded2 = tf.shade(agg2, cmap=cm['blues'])\n\n# Composite\nimport datashader.transfer_functions as tf\ncomposite = tf.composite(shaded1, shaded2)\n```\n\n### 5. Interactive Datashader with HoloViews\n\n```python\nfrom holoviews.operation.datashader import datashade\nfrom holoviews import streams\n\n# Interactive scatter with zooming\ndef create_datashaded_plot(data):\n    scatter = hv.Scatter(data, 'x', 'y')\n    return datashade(scatter, cmap='viridis')\n\n# Add interaction\nrange_stream = streams.RangeXY()\ninteractive_plot = hv.DynamicMap(\n    create_datashaded_plot,\n    streams=[range_stream]\n)\n```\n\n### 6. Time Series Data Streaming\n\n```python\n# Efficient streaming plot for time series\nfrom holoviews.operation.datashader import rasterize\nfrom holoviews import streams\n\ndef create_timeseries_plot(df_window):\n    curve = hv.Curve(df_window, 'timestamp', 'value')\n    return curve\n\n# Rasterize for efficiency\nrasterized = rasterize(\n    hv.Curve(df, 'timestamp', 'value'),\n    aggregator=ds.mean('value'),\n    width=1000,\n    height=400\n)\n```\n\n## Performance Optimization Strategies\n\n### 1. Memory Optimization\n\n```python\n# Use data types efficiently\ndf = pd.read_csv(\n    'large_file.csv',\n    dtype={\n        'x': 'float32',\n        'y': 'float32',\n        'value': 'float32',\n        'category': 'category'\n    }\n)\n\n# Chunk processing for extremely large files\nchunk_size = 1_000_000\naggregations = []\n\nfor chunk in pd.read_csv('huge.csv', chunksize=chunk_size):\n    canvas = ds.Canvas()\n    agg = canvas.points(chunk, 'x', 'y')\n    aggregations.append(agg)\n\n# Combine results\ncombined_agg = aggregations[0]\nfor agg in aggregations[1:]:\n    combined_agg = combined_agg + agg\n```\n\n### 2. Resolution and Pixel Ratio\n\n```python\n# Adjust canvas resolution based on data density\ndef auto_canvas(df, target_pixels=500000):\n    data_points = len(df)\n    aspect_ratio = (df['x'].max() - df['x'].min()) / (df['y'].max() - df['y'].min())\n\n    pixels = int(np.sqrt(target_pixels / aspect_ratio))\n    height = pixels\n    width = int(pixels * aspect_ratio)\n\n    return ds.Canvas(\n        plot_width=width,\n        plot_height=height,\n        x_range=(df['x'].min(), df['x'].max()),\n        y_range=(df['y'].min(), df['y'].max())\n    )\n\ncanvas = auto_canvas(df)\nagg = canvas.points(df, 'x', 'y')\n```\n\n### 3. Aggregation Selection\n\n```python\n# Choose appropriate aggregation for your data\ncanvas = ds.Canvas()\n\n# For counting: count()\nagg_count = canvas.points(df, 'x', 'y', agg=ds.count())\n\n# For averages: mean()\nagg_mean = canvas.points(df, 'x', 'y', agg=ds.mean('value'))\n\n# For sums: sum()\nagg_sum = canvas.points(df, 'x', 'y', agg=ds.sum('value'))\n\n# For max/min\nagg_max = canvas.points(df, 'x', 'y', agg=ds.max('value'))\n\n# For percentiles\nagg_p95 = canvas.points(df, 'x', 'y', agg=ds.count_cat('category'))\n```\n\n## Colormapping with Colorcet\n\n### 1. Perceptually Uniform Colormaps\n\n```python\nfrom colorcet import cm, cmap_d\nimport datashader.transfer_functions as tf\n\n# Use perceptually uniform colormaps\ncanvas = ds.Canvas()\nagg = canvas.points(df, 'x', 'y', agg=ds.count())\n\n# Gray scale\nshaded_gray = tf.shade(agg, cmap=cm['gray'])\n\n# Perceptual colormaps\nshaded_viridis = tf.shade(agg, cmap=cm['viridis'])\nshaded_turbo = tf.shade(agg, cmap=cm['turbo'])\n\n# Category colormaps\nshaded_color = tf.shade(agg, cmap=cm['cet_c5'])\n```\n\n### 2. Custom Color Normalization\n\n```python\n# Logarithmic normalization\nfrom datashader.transfer_functions import Log\n\ncanvas = ds.Canvas()\nagg = canvas.points(df, 'x', 'y', agg=ds.sum('value'))\n\n# Log transform for better visualization\nshaded = tf.shade(agg, norm='log', cmap=cm['viridis'])\n\n# Power law normalization\nshaded_power = tf.shade(agg, norm=ds.transfer_functions.eq_hist, cmap=cm['plasma'])\n```\n\n### 3. Multi-Band Compositing\n\n```python\n# Separate visualization of multiple datasets\ncanvas = ds.Canvas()\n\nagg_red = canvas.points(df_red, 'x', 'y')\nagg_green = canvas.points(df_green, 'x', 'y')\nagg_blue = canvas.points(df_blue, 'x', 'y')\n\n# Stack as RGB\nfrom datashader.colors import rgb\nresult = rgb(agg_red, agg_green, agg_blue)\n```\n\n## Integration with Panel and HoloViews\n\n```python\nimport panel as pn\nfrom holoviews.operation.datashader import datashade\n\n# Create interactive dashboard with datashader\nclass LargeDataViewer(param.Parameterized):\n    cmap = param.Selector(default='viridis', objects=list(cm.keys()))\n    show_spread = param.Boolean(default=False)\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    @param.depends('cmap', 'show_spread')\n    def plot(self):\n        scatter = hv.Scatter(self.data, 'x', 'y')\n        shaded = datashade(scatter, cmap=cm[self.cmap])\n\n        if self.show_spread:\n            shaded = tf.spread(shaded, px=2)\n\n        return shaded\n\nviewer = LargeDataViewer(large_df)\n\npn.extension('material')\napp = pn.Column(\n    pn.param.ParamMethod.from_param(viewer.param),\n    viewer.plot\n)\napp.servable()\n```\n\n## Best Practices\n\n### 1. Choose the Right Tool\n```\n< 10k points:        Use standard HoloViews/hvPlot\n10k - 1M points:     Use rasterize() for dense plots\n1M - 100M points:    Use Datashader\n> 100M points:       Use Datashader with chunking\n```\n\n### 2. Appropriate Canvas Size\n```python\n# General rule: 400-1000 pixels on each axis\n# Too small: loses detail\n# Too large: slow rendering, memory waste\n\ncanvas = ds.Canvas(plot_width=800, plot_height=600)  # Good default\n```\n\n### 3. Normalize Large Value Ranges\n```python\n# When data has extreme outliers\ncanvas = ds.Canvas()\nagg = canvas.points(df, 'x', 'y', agg=ds.mean('value'))\n\n# Use appropriate normalization\nshaded = tf.shade(agg, norm='log', cmap=cm['viridis'])\n```\n\n## Common Patterns\n\n### Pattern 1: Progressive Disclosure\n```python\ndef create_progressive_plot(df):\n    # Start with aggregated view\n    agg = canvas.points(df, 'x', 'y')\n    return tf.shade(agg, cmap='viridis')\n\n# User can zoom to see more detail\n# Datashader automatically recalculates at new resolution\n```\n\n### Pattern 2: Categorical Visualization\n```python\ncanvas = ds.Canvas()\n\n# Aggregate by category\nfor category in df['category'].unique():\n    subset = df[df['category'] == category]\n    agg = canvas.points(subset, 'x', 'y', agg=ds.count())\n    shaded = tf.shade(agg, cmap=cm[f'category_{category}'])\n```\n\n### Pattern 3: Time Series Aggregation\n```python\ndef aggregate_time_series(df, time_bucket):\n    df['time_bucket'] = pd.cut(df['timestamp'], bins=time_bucket)\n\n    aggregated = df.groupby('time_bucket').agg({\n        'x': 'mean',\n        'y': 'mean',\n        'value': 'sum'\n    })\n\n    return aggregated\n```\n\n## Common Use Cases\n\n1. **Scatter Plot Analysis**: 100M+ point clouds\n2. **Time Series Visualization**: High-frequency trading data\n3. **Geospatial Heat Maps**: Global-scale location data\n4. **Scientific Visualization**: Climate model outputs\n5. **Network Analysis**: Large graph layouts\n6. **Financial Analytics**: Tick-by-tick market data\n\n## Troubleshooting\n\n### Issue: Poor Color Differentiation\n- Use perceptually uniform colormaps from colorcet\n- Apply appropriate normalization (log, power law)\n- Adjust canvas size for better resolution\n\n### Issue: Memory Issues with Large Data\n- Use chunk processing for files larger than RAM\n- Reduce data type precision (float64 → float32)\n- Aggregate before visualization\n- Use categorical data type for strings\n\n### Issue: Slow Performance\n- Reduce canvas size (fewer pixels)\n- Use simpler aggregation functions\n- Enable GPU acceleration if available\n- Profile with Python profilers to find bottlenecks\n\n## Resources\n\n- [Datashader Documentation](https://datashader.org)\n- [Colorcet Documentation](https://colorcet.holoviz.org)\n- [Datashader Examples](https://datashader.org/getting_started/index.html)\n- [Large Data Visualization Guide](https://holoviews.org/user_guide/Large_Data.html)"
              },
              {
                "name": "colormaps-styling",
                "description": "Master color management and visual styling with Colorcet. Use this skill when selecting appropriate colormaps, creating accessible and colorblind-friendly visualizations, applying consistent themes, or customizing plot aesthetics with perceptually uniform color palettes.",
                "path": "community-plugins/holoviz-visualization/skills/colormaps-styling/SKILL.md",
                "frontmatter": {
                  "name": "colormaps-styling",
                  "description": "Master color management and visual styling with Colorcet. Use this skill when selecting appropriate colormaps, creating accessible and colorblind-friendly visualizations, applying consistent themes, or customizing plot aesthetics with perceptually uniform color palettes.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires colorcet >= 3.1.0, holoviews >= 1.18.0, panel >= 1.3.0, bokeh >= 3.0.0"
                },
                "content": "# Colormaps & Styling Skill\n\n## Overview\n\nMaster color management and visual styling with Colorcet and theme customization. Select appropriate colormaps, create accessible visualizations, and apply consistent application styling.\n\n### What is Colorcet?\n\nColorcet provides perceptually uniform colormaps designed for scientific visualization:\n\n- **Perceptually uniform**: Changes in data correspond to proportional visual changes\n- **Colorblind-friendly**: Palettes designed for accessibility\n- **Purpose-built**: Specific colormaps for different data types\n- **HoloViz integration**: Seamless use across HoloViews, Panel, and Bokeh\n\n## Quick Start\n\n### Installation\n\n```bash\npip install colorcet\n```\n\n### Basic Usage\n\n```python\nimport colorcet as cc\nfrom colorcet import cm\nimport holoviews as hv\n\nhv.extension('bokeh')\n\n# Use a colormap\ndata.hvplot.scatter('x', 'y', c='value', cmap=cm['cet_goertzel'])\n```\n\n## Core Concepts\n\n### 1. Colormap Categories\n\n**Sequential**: Single hue, increasing intensity\n```python\n# Blues, greens, reds, grays\ndata.hvplot('x', 'y', c='value', cmap=cm['cet_blues'])\n```\n\n**Diverging**: Two hues from center point\n```python\n# Emphasize positive/negative\ndata.hvplot('x', 'y', c='value', cmap=cm['cet_coolwarm'])\n```\n\n**Categorical**: Distinct colors for categories\n```python\n# Qualitative data\ndata.hvplot('x', 'y', c='category', cmap=cc.palette['tab10'])\n```\n\n**Cyclic**: Wraps around for angular data\n```python\n# Angles, directions, phases\ndata.hvplot('x', 'y', c='angle', cmap=cm['cet_cyclic_c1'])\n```\n\n**See**: [Colormap Reference](../../references/colormaps/colormap-reference.md) for complete catalog\n\n### 2. Accessibility\n\n**Colorblind-safe palettes**:\n```python\n# Deuteranopia (red-green)\ncmap=cm['cet_d4']\n\n# Protanopia (red-green)\ncmap=cm['cet_p3']\n\n# Tritanopia (blue-yellow)\ncmap=cm['cet_t10']\n\n# Grayscale-safe\ncmap=cm['cet_gray_r']\n```\n\n**See**: [Accessibility Guide](../../references/colormaps/accessibility.md) for comprehensive guidelines\n\n### 3. Colormap Selection Guide\n\n| Data Type | Recommended Colormap | Example |\n|-----------|---------------------|---------|\n| Single channel (positive) | `cet_blues`, `cet_gray_r` | Temperature, density |\n| Diverging (±) | `cet_coolwarm`, `cet_bwy` | Correlation, anomalies |\n| Categorical | `tab10`, `tab20` | Categories, labels |\n| Angular | `cet_cyclic_c1` | Wind direction, phase |\n| Full spectrum | `cet_goertzel` | General purpose |\n\n### 4. HoloViews Styling\n\n```python\nimport holoviews as hv\n\n# Apply colormap\nscatter = hv.Scatter(data, 'x', 'y', vdims=['value']).opts(\n    color=hv.dim('value').norm(),\n    cmap=cm['cet_goertzel'],\n    colorbar=True,\n    width=600,\n    height=400\n)\n\n# Style options\nscatter.opts(\n    size=5,\n    alpha=0.7,\n    tools=['hover'],\n    title='My Plot'\n)\n```\n\n**See**: [HoloViews Styling](../../references/colormaps/holoviews-styling.md) for advanced customization\n\n### 5. Panel Themes\n\n```python\nimport panel as pn\n\n# Apply theme\npn.extension(design='material')\n\n# Custom theme\npn.config.theme = 'dark'\n\n# Accent color\ntemplate = pn.template.FastListTemplate(\n    title='My App',\n    accent='#00aa41'\n)\n```\n\n**See**: [Panel Themes](../../references/colormaps/panel-themes.md) for theme customization\n\n## Common Patterns\n\n### Pattern 1: Heatmap with Diverging Colormap\n\n```python\nimport holoviews as hv\nfrom colorcet import cm\n\nheatmap = hv.HeatMap(data, ['x', 'y'], 'value').opts(\n    cmap=cm['cet_coolwarm'],\n    colorbar=True,\n    width=600,\n    height=400,\n    tools=['hover']\n)\n```\n\n### Pattern 2: Categorical Color Assignment\n\n```python\nimport panel as pn\nfrom colorcet import palette\n\ncategories = ['A', 'B', 'C', 'D']\ncolors = palette['tab10'][:len(categories)]\n\ncolor_map = dict(zip(categories, colors))\nplot = data.hvplot('x', 'y', c='category', cmap=color_map)\n```\n\n### Pattern 3: Consistent App Styling\n\n```python\nimport panel as pn\n\n# Set global theme\npn.extension(design='material')\n\n# Custom CSS\npn.config.raw_css.append(\"\"\"\n.card {\n    border-radius: 10px;\n    box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n}\n\"\"\")\n\n# Accent color throughout\naccent = '#00aa41'\ntemplate = pn.template.FastListTemplate(\n    title='My Dashboard',\n    accent=accent\n)\n```\n\n### Pattern 4: Responsive Colorbar\n\n```python\nfrom holoviews import opts\n\nplot = data.hvplot.scatter('x', 'y', c='value', cmap=cm['cet_blues']).opts(\n    colorbar=True,\n    colorbar_opts={\n        'title': 'Value',\n        'width': 10,\n        'ticker': {'desired_num_ticks': 5}\n    }\n)\n```\n\n### Pattern 5: Colorblind-Safe Visualization\n\n```python\nfrom colorcet import cm\n\n# Use colorblind-safe diverging palette\nplot = data.hvplot('x', 'y', c='value', cmap=cm['cet_d4']).opts(\n    title='Colorblind-Safe Visualization',\n    width=600,\n    height=400\n)\n\n# Alternative: Use patterns/hatching\nplot.opts(hatch_pattern='/')\n```\n\n## Best Practices\n\n### 1. Match Colormap to Data Type\n\n```python\n# ✅ Good: Sequential for positive values\ntemp_plot = data.hvplot(c='temperature', cmap=cm['cet_fire'])\n\n# ✅ Good: Diverging for centered data\ncorrelation = data.hvplot(c='correlation', cmap=cm['cet_coolwarm'])\n\n# ❌ Bad: Rainbow/jet colormap (not perceptually uniform)\nbad_plot = data.hvplot(c='value', cmap='jet')  # Avoid!\n```\n\n### 2. Consider Accessibility\n\n```python\n# ✅ Good: Colorblind-safe\nplot = data.hvplot(c='value', cmap=cm['cet_d4'])\n\n# ✅ Good: Add patterns for print/grayscale\nplot.opts(hatch_pattern='/')\n\n# ✅ Good: Test in grayscale\nplot.opts(cmap=cm['cet_gray_r'])\n```\n\n### 3. Consistent Styling\n\n```python\n# ✅ Good: Define color scheme once\nCOLORS = {\n    'primary': '#00aa41',\n    'secondary': '#616161',\n    'accent': '#ff6f00'\n}\n\n# Use throughout application\npn.template.FastListTemplate(accent=COLORS['primary'])\n```\n\n### 4. Meaningful Labels\n\n```python\n# ✅ Good: Descriptive colorbar\nplot.opts(\n    colorbar=True,\n    colorbar_opts={'title': 'Temperature (°C)'}\n)\n\n# ❌ Bad: No context\nplot.opts(colorbar=True)\n```\n\n### 5. Performance with Large Data\n\n```python\n# For large datasets, limit colormap resolution\nplot.opts(\n    cmap=cm['cet_goertzel'],\n    color_levels=256  # Reduce if performance issues\n)\n```\n\n## Configuration\n\n### Global Colormap Defaults\n\n```python\nimport holoviews as hv\nfrom colorcet import cm\n\n# Set default colormap\nhv.opts.defaults(\n    hv.opts.Image(cmap=cm['cet_goertzel']),\n    hv.opts.Scatter(cmap=cm['cet_blues'])\n)\n```\n\n### Theme Configuration\n\n```python\nimport panel as pn\n\n# Material design\npn.extension(design='material')\n\n# Dark mode\npn.config.theme = 'dark'\n\n# Custom theme JSON\npn.config.theme_json = {\n    'palette': {\n        'primary': '#00aa41',\n        'secondary': '#616161'\n    }\n}\n```\n\n## Troubleshooting\n\n### Colormap Not Showing\n\n```python\n# Check if colormap imported\nfrom colorcet import cm\nprint(cm['cet_goertzel'])  # Should print colormap\n\n# Verify data range\nprint(data['value'].min(), data['value'].max())\n\n# Explicit normalization\nplot.opts(color=hv.dim('value').norm())\n```\n\n### Colors Look Wrong\n\n- **Issue**: Perceptual non-uniformity\n- **Solution**: Use Colorcet instead of matplotlib defaults\n\n```python\n# ❌ Avoid\ncmap='jet', cmap='rainbow'\n\n# ✅ Use\ncmap=cm['cet_goertzel'], cmap=cm['cet_fire']\n```\n\n### Theme Not Applying\n\n```python\n# Ensure extension loaded with design\npn.extension(design='material')\n\n# Check theme setting\nprint(pn.config.theme)  # 'default' or 'dark'\n\n# Reload page after theme change\n```\n\n## Progressive Learning Path\n\n### Level 1: Basics\n1. Install Colorcet\n2. Use basic colormaps\n3. Apply to plots\n\n**Resources**:\n- Quick Start (this doc)\n- [Colormap Reference](../../references/colormaps/colormap-reference.md)\n\n### Level 2: Accessibility\n1. Understand colormap categories\n2. Choose appropriate maps\n3. Test for colorblindness\n\n**Resources**:\n- [Accessibility Guide](../../references/colormaps/accessibility.md)\n\n### Level 3: Advanced Styling\n1. Customize HoloViews opts\n2. Create custom themes\n3. Consistent branding\n\n**Resources**:\n- [HoloViews Styling](../../references/colormaps/holoviews-styling.md)\n- [Panel Themes](../../references/colormaps/panel-themes.md)\n\n## Additional Resources\n\n### Documentation\n- **[Colormap Reference](../../references/colormaps/colormap-reference.md)** - Complete colormap catalog\n- **[Accessibility Guide](../../references/colormaps/accessibility.md)** - Colorblind-friendly design\n- **[HoloViews Styling](../../references/colormaps/holoviews-styling.md)** - Advanced customization\n- **[Panel Themes](../../references/colormaps/panel-themes.md)** - Theme and branding\n\n### External Links\n- [Colorcet Documentation](https://colorcet.holoviz.org/)\n- [Colorcet Gallery](https://colorcet.holoviz.org/user_guide/index.html)\n- [Color Universal Design](https://jfly.uni-koeln.de/color/)\n- [WCAG Color Contrast](https://www.w3.org/WAI/WCAG21/Understanding/contrast-minimum.html)\n\n## Use Cases\n\n### Scientific Visualization\n- Temperature maps\n- Density plots\n- Correlation matrices\n- Geospatial data\n\n### Data Dashboards\n- KPI indicators\n- Time series\n- Category comparison\n- Status displays\n\n### Accessibility\n- Colorblind-friendly visualizations\n- Print-safe graphics\n- High-contrast displays\n- Grayscale compatibility\n\n### Branding\n- Corporate colors\n- Consistent styling\n- Custom themes\n- Professional appearance\n\n## Summary\n\nColorcet provides perceptually uniform, accessible colormaps for scientific visualization.\n\n**Key principles**:\n- Match colormap to data type\n- Choose colorblind-safe palettes\n- Use perceptually uniform maps\n- Maintain consistent styling\n- Test accessibility\n\n**Ideal for**:\n- Scientific visualizations\n- Accessible dashboards\n- Professional applications\n- Print publications\n\n**Colormap selection**:\n- Sequential: Single channel data\n- Diverging: Centered data (±)\n- Categorical: Qualitative categories\n- Cyclic: Angular/periodic data\n\n## Related Skills\n\n- **[Data Visualization](../data-visualization/SKILL.md)** - HoloViews visualization patterns\n- **[Panel Dashboards](../panel-dashboards/SKILL.md)** - Dashboard styling and themes\n- **[Plotting Fundamentals](../plotting-fundamentals/SKILL.md)** - Basic plotting with hvPlot"
              },
              {
                "name": "data-visualization",
                "description": "Master advanced declarative visualization with HoloViews. Use this skill when creating complex multi-dimensional visualizations, composing overlays and layouts, implementing interactive streams and selection, building network or hierarchical visualizations, or exploring data with dynamic maps and faceted displays.",
                "path": "community-plugins/holoviz-visualization/skills/data-visualization/SKILL.md",
                "frontmatter": {
                  "name": "data-visualization",
                  "description": "Master advanced declarative visualization with HoloViews. Use this skill when creating complex multi-dimensional visualizations, composing overlays and layouts, implementing interactive streams and selection, building network or hierarchical visualizations, or exploring data with dynamic maps and faceted displays.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires holoviews >= 1.18.0, pandas >= 1.0.0, numpy >= 1.15.0, bokeh >= 3.0.0, networkx >= 2.0.0 (for network visualizations)"
                },
                "content": "# Data Visualization Skill\n\n## Overview\n\nMaster advanced declarative visualization with HoloViews and composition patterns. This skill covers sophisticated visualization techniques for complex data exploration and presentation.\n\n## Dependencies\n\n- holoviews >= 1.18.0\n- pandas >= 1.0.0\n- numpy >= 1.15.0\n- bokeh >= 3.0.0\n- networkx >= 2.0.0 (for network visualizations)\n\n## Core Capabilities\n\n### 1. Advanced Element Composition\n\nHoloViews allows sophisticated composition of visualization elements:\n\n```python\nimport holoviews as hv\nfrom holoviews import opts\nimport pandas as pd\nimport numpy as np\n\n# Create overlaid elements\ncurve = hv.Curve(df, 'x', 'y', label='Measured')\nscatter = hv.Scatter(df_with_noise, 'x', 'y', label='Noisy')\noverlay = curve * scatter  # Multiplication overlays\n\n# Create layouts\ncol_layout = hv.Column(plot1, plot2, plot3)\nrow_layout = hv.Row(plot1, plot2, plot3)\ngrid_layout = hv.GridMatrix(data_dict)\n\n# Faceted displays\nfaceted = hv.Curve(df, 'date', 'value').facet('category')\n\n# Nested layouts\ncomplex_layout = hv.Column(\n    hv.Row(plot1, plot2),\n    hv.Row(plot3, plot4),\n    hv.Row(plot5, plot6)\n)\n```\n\n### 2. Interactive Streams and Selection\n\nCreate responsive visualizations with interactive selection:\n\n```python\nfrom holoviews import streams\n\n# Selection stream\nrange_stream = streams.RangeXY()\nscatter = hv.Scatter(df, 'x', 'y').opts(tools=['box_select'])\n\n@hv.transform\ndef selected_data(data):\n    if range_stream.selection:\n        x0, x1 = range_stream.selection[0], range_stream.selection[1]\n        y0, y1 = range_stream.selection[2], range_stream.selection[3]\n        mask = (data['x'] >= x0) & (data['x'] <= x1) & \\\n               (data['y'] >= y0) & (data['y'] <= y1)\n        return data[mask]\n    return data\n\nhistogram = selected_data.to(hv.Histogram)\nscatter_with_hist = scatter + histogram\n```\n\n### 3. Dynamic Maps for Responsive Visualization\n\n```python\n# Dynamic updating based on parameters\nfrom holoviews import DynamicMap, streams\n\ndef plot_by_category(category):\n    data = df[df['category'] == category]\n    return hv.Scatter(data, 'x', 'y', title=f'Category: {category}')\n\ncategory_stream = streams.Stream.define('category', category='A')\ndmap = DynamicMap(plot_by_category, streams=[category_stream])\n\n# Parameterized dynamic map\ndef plot_with_params(threshold=0.5):\n    filtered = df[df['value'] > threshold]\n    return hv.Scatter(filtered, 'x', 'y')\n\ndmap_param = DynamicMap(\n    plot_with_params,\n    streams=[streams.Stream.define('threshold', threshold=0.5)]\n)\n```\n\n### 4. Network and Hierarchical Visualizations\n\n```python\nimport networkx as nx\n\n# Network graph\nG = nx.karate_club_graph()\npos = nx.spring_layout(G)\nedges = [(u, v) for u, v in G.edges()]\nnodes = list(G.nodes())\n\n# Create nodes and edges visualization\nedge_plot = hv.Segments(edges, kdims=['source', 'target'])\nnode_plot = hv.Scatter(\n    [(pos[n][0], pos[n][1], n) for n in nodes],\n    kdims=['x', 'y', 'node']\n)\nnetwork = (edge_plot * node_plot).opts(\n    opts.Scatter(size=100, color='red'),\n    opts.Segments(color='gray')\n)\n\n# Treemap for hierarchical data\ntreemap = hv.TreeMap(\n    hierarchical_data,\n    label='Organization'\n).opts(tools=['hover'])\n```\n\n### 5. Statistical and Aggregate Visualizations\n\n```python\n# Aggregate with Rasterize\nfrom holoviews.operation import datashader as dshade\n\n# Box plot for comparison\nbox_plot = hv.BoxWhisker(df, kdims=['category'], vdims=['value'])\n\n# Violin plot\nviolin = hv.Violin(df, kdims=['category'], vdims=['value'])\n\n# Distribution comparison\ndist_layout = hv.Column(*[\n    df[df['category'] == cat]['value'].hvplot.hist()\n    for cat in df['category'].unique()\n])\n```\n\n### 6. Multi-Dimensional Data Exploration\n\n```python\n# HoloMap for multi-dimensional data\ndef plot_by_params(category, metric):\n    data = df[(df['category'] == category) & (df['metric'] == metric)]\n    return hv.Scatter(data, 'x', 'y', title=f'{category} - {metric}')\n\nhmap = hv.HoloMap(\n    {(cat, met): plot_by_params(cat, met)\n     for cat in categories for met in metrics},\n    kdims=['Category', 'Metric']\n)\n\n# NdLayout for structured multi-dimensional display\nndlayout = hv.NdLayout({\n    (cat, met): plot_by_params(cat, met)\n    for cat in categories for met in metrics\n}, kdims=['Category', 'Metric'])\n```\n\n## Advanced Styling and Theming\n\n### 1. Global Options\n\n```python\n# Set global defaults\nopts.defaults(\n    opts.Curve(width=700, height=400, responsive=True),\n    opts.Scatter(size=100, alpha=0.5),\n    opts.Image(cmap='viridis')\n)\n\n# Apply to multiple elements\nstyled_plots = [\n    plot.opts(\n        title='Styled Plot',\n        xlabel='X Axis',\n        ylabel='Y Axis',\n        toolbar='right',\n        active_tools=['pan', 'wheel_zoom']\n    )\n    for plot in plots\n]\n```\n\n### 2. Custom Styling\n\n```python\n# Element-specific styling\nplot = hv.Scatter(df, 'x', 'y').opts(\n    color=hv.dim('category').categorize({\n        'A': '#FF6B6B',\n        'B': '#4ECDC4',\n        'C': '#45B7D1'\n    }),\n    size=hv.dim('value').norm(min=10, max=100),\n    selection_color='red',\n    nonselection_alpha=0.1\n)\n\n# Conditional formatting\nplot.opts(\n    color=hv.dim('status').categorize({\n        'good': 'green',\n        'warning': 'orange',\n        'error': 'red'\n    })\n)\n```\n\n### 3. Interactive Legends and Annotations\n\n```python\n# Annotations\nannotated_plot = hv.Curve(df, 'x', 'y')\nannotations = [\n    hv.Text(x, y, text, fontsize=10)\n    for x, y, text in annotations_data\n]\nplot_with_annotations = annotated_plot * hv.Overlay(annotations)\n\n# Custom legend\nplot = hv.Overlay([\n    hv.Curve(df1, label='Series 1'),\n    hv.Curve(df2, label='Series 2'),\n    hv.Curve(df3, label='Series 3')\n]).opts(\n    legend_position='top_left',\n    legend_muted_alpha=0.2\n)\n```\n\n## Best Practices\n\n### 1. Performance with Large Datasets\n```python\n# Use rasterize for dense plots\nfrom holoviews.operation import rasterize\n\nlarge_scatter = hv.Scatter(large_df, 'x', 'y')\nrasterized = rasterize(large_scatter, pixel_ratio=2)\n\n# Use aggregation\naggregated = df.groupby('category')['value'].mean().hvplot.bar()\n\n# Use datashader for massive datasets (>100M points)\nfrom holoviews.operation.datashader import datashade\ndshaded = datashade(large_scatter)\n```\n\n### 2. Responsive and Accessible Plots\n```python\n# Responsive sizing\nplot = hv.Scatter(df, 'x', 'y').opts(\n    responsive=True,\n    sizing_mode='stretch_width'\n)\n\n# Accessible color palettes\nplot = hv.Scatter(df, 'x', 'y').opts(\n    color=hv.dim('value').norm(),\n    cmap='cet_gray_r'  # Perceptually uniform\n)\n\n# Clear labels\nplot.opts(\n    title='Clear Title',\n    xlabel='Independent Variable (units)',\n    ylabel='Dependent Variable (units)',\n    fontsize=14\n)\n```\n\n### 3. Composition Patterns\n\n```python\n# Avoid deep nesting\n# Bad: ((a + (b + (c + d)))\n# Good: a + b + c + d\n\n# Create helper functions\ndef create_comparison_layout(data_dict):\n    plots = [hv.Scatter(v, label=k) for k, v in data_dict.items()]\n    return hv.Column(*plots)\n\n# Modular composition\nsidebar = hv.Column(title_text, filter_widget)\nmain = hv.Row(plot1, plot2)\napp = hv.Column(sidebar, main)\n```\n\n## Common Patterns\n\n### Pattern 1: Linked Brushing\n```python\ndef create_linked_views(df):\n    scatter = hv.Scatter(df, 'x', 'y').opts(tools=['box_select'])\n\n    def get_histogram(selection):\n        if selection:\n            selected_df = df.iloc[selection.event.inds]\n        else:\n            selected_df = df\n        return hv.Histogram(selected_df['x'], bins=20)\n\n    return scatter + DynamicMap(get_histogram, streams=[streams.Selection1D()])\n```\n\n### Pattern 2: Multi-Scale Exploration\n```python\ndef create_zoomable_view(df):\n    scatter = hv.Scatter(df, 'x', 'y')\n    zoomed = scatter.opts(\n        xlim=(0, 10),\n        ylim=(0, 10)\n    )\n    return hv.Column(scatter, zoomed)\n```\n\n### Pattern 3: Faceted Analysis\n```python\ndef create_faceted_analysis(df, facet_col):\n    return df.hvplot.scatter(\n        x='x',\n        y='y',\n        by=facet_col,\n        subplots=True,\n        layout='vertical'\n    )\n```\n\n## Integration with Other HoloViz Tools\n\n- **Panel**: Embed interactive HoloViews in dashboards\n- **hvPlot**: Quick plotting that produces HoloViews objects\n- **Datashader**: Efficient rendering for large data\n- **Param**: Parameter-driven dynamic visualizations\n- **GeoViews**: Geographic data visualization building on HoloViews\n\n## Common Use Cases\n\n1. **Exploratory Data Analysis**: Multi-dimensional data exploration\n2. **Dashboard Metrics**: KPI and metric visualization\n3. **Scientific Visualization**: Complex data relationships\n4. **Financial Analysis**: Time series and correlation analysis\n5. **Report Generation**: Publication-quality visualizations\n6. **Real-time Monitoring**: Streaming data visualization\n\n## Troubleshooting\n\n### Issue: Plot Elements Overlapping\n- Use layouts instead of overlays for clarity\n- Adjust alpha transparency\n- Use complementary colors\n\n### Issue: Slow Interactive Performance\n- Use rasterize for dense plots\n- Reduce data size with aggregation\n- Use datashader for massive datasets\n- Cache plot computations\n\n### Issue: Unclear Data Relationships\n- Use multiple linked views\n- Apply faceting for categorical comparison\n- Use color and size encoding\n- Add annotations and reference lines\n\n## Resources\n\n- [HoloViews Reference](https://holoviews.org/reference/index.html)\n- [HoloViews User Guide](https://holoviews.org/user_guide/index.html)\n- [Bokeh for Customization](https://docs.bokeh.org)\n- [Datashader for Performance](https://datashader.org)"
              },
              {
                "name": "geospatial-visualization",
                "description": "Master geographic and mapping visualizations with GeoViews. Use this skill when creating interactive maps, visualizing point/polygon/line geographic data, building choropleth maps, performing spatial analysis (joins, buffers, proximity), working with coordinate reference systems, or integrating tile providers and basemaps.",
                "path": "community-plugins/holoviz-visualization/skills/geospatial-visualization/SKILL.md",
                "frontmatter": {
                  "name": "geospatial-visualization",
                  "description": "Master geographic and mapping visualizations with GeoViews. Use this skill when creating interactive maps, visualizing point/polygon/line geographic data, building choropleth maps, performing spatial analysis (joins, buffers, proximity), working with coordinate reference systems, or integrating tile providers and basemaps.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires geoviews >= 1.11.0, geopandas >= 0.10.0, shapely >= 1.8.0, pyproj >= 3.0.0, cartopy >= 0.20.0 (optional)"
                },
                "content": "# Geospatial Visualization Skill\n\n## Overview\n\nMaster geographic and mapping visualizations with GeoViews and spatial data handling. This skill covers creating interactive maps, analyzing geographic data, and visualizing spatial relationships.\n\n## Dependencies\n\n- geoviews >= 1.11.0\n- geopandas >= 0.10.0\n- shapely >= 1.8.0\n- cartopy >= 0.20.0 (optional)\n- pyproj >= 3.0.0\n\n## Core Capabilities\n\n### 1. Basic Geographic Visualization\n\nGeoViews extends HoloViews with geographic support:\n\n```python\nimport geoviews as gv\nimport geopandas as gpd\nfrom geoviews import tile_providers as gvts\n\n# Load geographic data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Basic map visualization\nworld_map = gv.Polygons(world, vdims=['name', 'pop_est']).opts(\n    title='World Population',\n    height=600,\n    width=800,\n    tools=['hover']\n)\n\n# Add tile layer background\ntiled = gvts.ESRI.apply.opts(\n    alpha=0.4,\n    xaxis=None,\n    yaxis=None\n) * world_map\n```\n\n### 2. Point Data on Maps\n\n```python\n# Create point features\ncities_data = {\n    'city': ['New York', 'Los Angeles', 'Chicago'],\n    'latitude': [40.7128, 34.0522, 41.8781],\n    'longitude': [-74.0060, -118.2437, -87.6298],\n    'population': [8337000, 3990456, 2693976]\n}\n\ncities_gdf = gpd.GeoDataFrame(\n    cities_data,\n    geometry=gpd.points_from_xy(cities_data['longitude'], cities_data['latitude']),\n    crs='EPSG:4326'\n)\n\n# Visualize points\npoints = gv.Points(cities_gdf, kdims=['longitude', 'latitude'], vdims=['city', 'population'])\npoints = points.opts(\n    size=gv.dim('population').norm(min=5, max=50),\n    color='red',\n    tools=['hover', 'box_select']\n)\n\n# With tile background\nmap_with_points = gvts.CartoDEM.apply.opts(alpha=0.5) * points\n```\n\n### 3. Choropleth Maps\n\n```python\n# Color regions by data value\nchoropleth = gv.Polygons(world, vdims=['name', 'pop_est']).opts(\n    cmap='viridis',\n    color=gv.dim('pop_est').norm(),\n    colorbar=True,\n    height=600,\n    width=900,\n    tools=['hover']\n)\n\n# Add interactivity\nchoropleth = choropleth.opts(\n    hover_fill_color='red',\n    hover_fill_alpha=0.5\n)\n```\n\n### 4. Interactive Feature Selection\n\n```python\nfrom holoviews import streams\n\n# Create selectable map\nselectable_map = gv.Polygons(world).opts(\n    tools=['box_select', 'tap'],\n    selection_fill_color='red',\n    nonselection_fill_alpha=0.2\n)\n\n# Stream for selection\nselection_stream = streams.Selection1D()\n\ndef get_selected_data(index):\n    if index:\n        return world.iloc[index[0]]\n    return None\n\n# Get info about selected region\nselected_info = hv.DynamicMap(\n    lambda index: hv.Text(0, 0, str(get_selected_data(index))),\n    streams=[selection_stream]\n)\n```\n\n### 5. Vector and Raster Layers\n\n```python\n# Multiple layers\nterrain = gvts.Stamen.Terrain.apply.opts(alpha=0.3)\npoints = gv.Points(cities_gdf, kdims=['longitude', 'latitude'])\nlines = gv.Lines(routes_gdf, kdims=['longitude', 'latitude'])\n\n# Compose layers\nmap_composition = terrain * lines * points\n\n# Faceted geographic display\nfaceted_maps = gv.Polygons(world, vdims=['name', 'continent']).facet('continent')\n```\n\n### 6. Hexbin and Rasterized Aggregation\n\n```python\n# Hexbin aggregation for point data\nhexbin = gv.HexTiles(cities_gdf).opts(\n    cmap='viridis',\n    colorbar=True,\n    height=600,\n    width=800\n)\n\n# With tile background\nmap_hexbin = gvts.CartoDEM.apply.opts(alpha=0.4) * hexbin\n```\n\n## Spatial Analysis Workflows\n\n### 1. Spatial Joins\n\n```python\n# Combine different geographic layers\npoints_gdf = gpd.GeoDataFrame(\n    cities_data,\n    geometry=gpd.points_from_xy(cities_data['longitude'], cities_data['latitude']),\n    crs='EPSG:4326'\n)\n\nregions_gdf = gpd.read_file('regions.geojson')\n\n# Spatial join: which cities are in which regions\njoined = gpd.sjoin(points_gdf, regions_gdf, how='left', predicate='within')\n\n# Visualize result\njoined_map = gv.Points(joined, kdims=['longitude', 'latitude']) * \\\n             gv.Polygons(regions_gdf)\n```\n\n### 2. Buffer and Proximity Analysis\n\n```python\nfrom shapely.geometry import Point\n\n# Create buffer zones\nbuffered = cities_gdf.copy()\nbuffered['geometry'] = buffered.geometry.buffer(1.0)  # 1 degree\n\n# Visualize buffered regions\nbuffers = gv.Polygons(buffered).opts(fill_alpha=0.3)\npoints = gv.Points(cities_gdf)\n\nproximity_map = gvts.CartoDEM.apply.opts(alpha=0.3) * buffers * points\n```\n\n### 3. Distance and Route Analysis\n\n```python\n# Calculate distances between cities\nfrom shapely.geometry import LineString\n\nroutes = []\nfor i in range(len(cities_gdf) - 1):\n    start = cities_gdf.geometry.iloc[i]\n    end = cities_gdf.geometry.iloc[i + 1]\n    route = LineString([start, end])\n    distance = start.distance(end)\n    routes.append({'geometry': route, 'distance': distance})\n\nroutes_gdf = gpd.GeoDataFrame(routes, crs='EPSG:4326')\n\n# Visualize routes\nroute_lines = gv.Lines(routes_gdf, vdims=['distance']).opts(\n    color=gv.dim('distance').norm(),\n    cmap='plasma'\n)\n```\n\n## Tile Providers and Basemaps\n\n```python\n# Available tile providers\nfrom geoviews import tile_providers as gvts\n\n# Different styles\nopenstreetmap = gvts.OpenStreetMap.Mapnik\nsatellite = gvts.ESRI.WorldImagery\nterrain = gvts.Stamen.Terrain\ntoner = gvts.Stamen.Toner\n\n# Use with visualization\nmap_with_osm = gvts.OpenStreetMap.Mapnik * gv.Points(cities_gdf)\n\n# Custom styling\nbase_map = gvts.CartoDEM.apply.opts(\n    alpha=0.5,\n    xaxis=None,\n    yaxis=None\n)\n```\n\n## Best Practices\n\n### 1. Coordinate Reference Systems\n```python\n# Always specify and manage CRS\ngdf = gpd.read_file('data.geojson')\nprint(gdf.crs)\n\n# Reproject if necessary\ngdf_projected = gdf.to_crs('EPSG:3857')  # Web Mercator\n\n# When creating GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    data,\n    geometry=gpd.points_from_xy(lon, lat),\n    crs='EPSG:4326'  # WGS84\n)\n```\n\n### 2. Large Dataset Optimization\n```python\n# Use rasterization for dense point clouds\nfrom holoviews.operation.datashader import rasterize\n\npoints = gv.Points(large_gdf, kdims=['x', 'y'])\nrasterized = rasterize(points)\n\n# Use tile-based rendering for massive datasets\n# Consider breaking into GeoJSON tiles\n```\n\n### 3. Interactive Map Design\n```python\n# Combine multiple interaction tools\nmap_viz = gv.Polygons(gdf).opts(\n    tools=['hover', 'box_select', 'tap'],\n    hover_fill_color='yellow',\n    hover_fill_alpha=0.2,\n    selection_fill_color='red'\n)\n\n# Add complementary visualizations\nstatistics = hv.Text(0, 0, '')  # Update based on selection\nmap_and_stats = hv.Column(map_viz, statistics)\n```\n\n### 4. Color and Scale Management\n```python\n# Use perceptually uniform colormaps\nfrom colorcet import cm\n\nmap_viz = gv.Polygons(gdf, vdims=['value']).opts(\n    color=gv.dim('value').norm(),\n    cmap=cm['viridis'],\n    colorbar=True,\n    clim=(vmin, vmax)\n)\n```\n\n## Common Patterns\n\n### Pattern 1: Multi-Layer Map Dashboard\n```python\ndef create_map_dashboard(layers_dict):\n    base_map = gvts.CartoDEM.apply.opts(alpha=0.4)\n    layers = [gv.Polygons(layers_dict[name]) for name in layers_dict]\n    return base_map * hv.Overlay(layers)\n```\n\n### Pattern 2: Dynamic Filtering Map\n```python\nfrom holoviews import DynamicMap, streams\n\nfilter_stream = streams.Stream.define('filter', year=2020)\n\ndef update_map(year):\n    filtered_gdf = world[world['year'] == year]\n    return gv.Polygons(filtered_gdf, vdims=['name', 'value'])\n\ndmap = DynamicMap(update_map, streams=[filter_stream])\n```\n\n### Pattern 3: Clustered Points Map\n```python\ndef create_clustered_map(points_gdf, zoom_levels=[1, 5, 10, 20]):\n    # Use hexbin for aggregation at different scales\n    aggregated = gv.HexTiles(points_gdf, aggregation='count')\n    return aggregated.opts(responsive=True)\n```\n\n## Integration with Other HoloViz Tools\n\n- **Panel**: Embed maps in web dashboards\n- **hvPlot**: Quick geographic plotting with `.hvplot(geo=True)`\n- **HoloViews**: Underlying visualization framework\n- **Datashader**: Efficient rendering for massive geographic datasets\n- **Param**: Parameter-driven map updates\n\n## Common Use Cases\n\n1. **Real Estate Analysis**: Property locations and market data\n2. **Climate Analysis**: Temperature, precipitation spatial patterns\n3. **Infrastructure Planning**: Network and facility location analysis\n4. **Epidemiology**: Disease spread and hotspot visualization\n5. **Transportation Analysis**: Route optimization and traffic patterns\n6. **Environmental Monitoring**: Land use, vegetation, water quality\n\n## Troubleshooting\n\n### Issue: Map Not Displaying\n- Verify CRS is correctly specified\n- Check coordinates are in correct order (longitude, latitude)\n- Ensure geometry objects are valid with `gdf.is_valid.all()`\n\n### Issue: Performance Problems with Large Datasets\n- Use rasterization for dense points\n- Simplify geometries with `gdf.geometry.simplify(tolerance)`\n- Use tile-based rendering or data pagination\n- Consider reducing zoom levels\n\n### Issue: Inaccurate Spatial Analysis\n- Verify CRS consistency across all layers\n- Use appropriate CRS for distance calculations\n- Check topology validity before operations\n- Test on sample data first\n\n## Resources\n\n- [GeoViews Documentation](https://geoviews.org)\n- [GeoPandas Documentation](https://geopandas.org)\n- [GeoJSON Specification](https://geojson.org)\n- [EPSG Coordinate Reference Systems](https://epsg.io)\n- [Cartopy for Advanced Cartography](https://scitools.org.uk/cartopy)"
              },
              {
                "name": "lumen-ai",
                "description": "Master AI-powered natural language data exploration with Lumen AI. Use this skill when building conversational data analysis interfaces, enabling natural language queries to databases, creating custom AI agents for domain-specific analytics, implementing RAG with document context, or deploying self-service analytics with LLM-generated SQL and visualizations.",
                "path": "community-plugins/holoviz-visualization/skills/lumen-ai/SKILL.md",
                "frontmatter": {
                  "name": "lumen-ai",
                  "description": "Master AI-powered natural language data exploration with Lumen AI. Use this skill when building conversational data analysis interfaces, enabling natural language queries to databases, creating custom AI agents for domain-specific analytics, implementing RAG with document context, or deploying self-service analytics with LLM-generated SQL and visualizations.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires lumen >= 0.10.0 (with AI support), panel >= 1.3.0, openai or anthropic or other LLM provider libraries. Supports OpenAI, Anthropic Claude, Google Gemini, Mistral, and local models via Ollama or LlamaCPP."
                },
                "content": "# Lumen AI Skill\n\n## Overview\n\nLumen AI is an open-source, agent-based framework for conversational data exploration. Users ask questions in plain English and receive visualizations, SQL queries, and insights automatically generated by large language models.\n\n### What is Lumen AI?\n\nLumen AI translates natural language queries into:\n- SQL queries for database exploration\n- Interactive visualizations\n- Statistical summaries\n- Custom domain-specific analyses\n- Data-driven insights\n\n### Key Features\n\n- **Natural Language Interface**: Ask questions in plain English\n- **Multi-LLM Support**: OpenAI, Anthropic, Google, Mistral, local models\n- **Agent Architecture**: Specialized agents for SQL, charts, analyses\n- **Extensible**: Custom agents, tools, and analyses\n- **Privacy-Focused**: Full local deployment option\n- **No Vendor Lock-in**: Switch LLM providers with configuration change\n\n### Lumen AI vs Lumen Dashboards\n\n| Feature | Lumen AI | Lumen Dashboards |\n|---------|----------|------------------|\n| **Interface** | Conversational, natural language | Declarative YAML |\n| **Use Case** | Ad-hoc exploration, varying questions | Fixed dashboards, repeated views |\n| **Users** | Non-technical users, self-service | Developers, dashboard builders |\n| **Cost** | LLM API costs | No LLM costs |\n| **Flexibility** | High - generates any query | Fixed - predefined views |\n\n**Use Lumen AI when**:\n- Users need ad-hoc data exploration\n- Questions vary and aren't predictable\n- Enabling self-service analytics\n- Reducing analyst backlog\n\n**Use Lumen Dashboards when**:\n- Dashboard structure is fixed\n- Same visualizations needed repeatedly\n- No LLM costs desired\n- Full control over outputs needed\n\n## Quick Start\n\n### Installation\n\n```bash\n# Install Lumen with AI support\npip install lumen[ai]\n\n# Install LLM provider (choose one or more)\npip install openai        # OpenAI\npip install anthropic     # Anthropic Claude\n```\n\n### Launch Built-in Interface\n\n```bash\n# Set API key\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Launch with dataset\nlumen-ai serve data/sales.csv\n\n# Or with database\nlumen-ai serve \"postgresql://user:pass@localhost/mydb\"\n```\n\n### Python API - Basic Example\n\n```python\nimport lumen.ai as lmai\nimport panel as pn\nfrom lumen.sources.duckdb import DuckDBSource\n\npn.extension()\n\n# Configure LLM\nlmai.llm.llm_type = \"anthropic\"\nlmai.llm.model = \"claude-3-5-sonnet-20241022\"\n\n# Load data\nsource = DuckDBSource(\n    tables=[\"./data/sales.csv\", \"./data/customers.csv\"]\n)\n\n# Create UI\nui = lmai.ExplorerUI(\n    source=source,\n    title=\"Sales Analytics AI\"\n)\n\nui.servable()\n```\n\n### Example Queries\n\nOnce running, try queries like:\n- \"What tables are available?\"\n- \"Show me total sales by region\"\n- \"Create a scatter plot of price vs quantity\"\n- \"What were the top 10 products last month?\"\n- \"Calculate average order value per customer\"\n\n## Core Concepts\n\n### 1. Agents\n\nSpecialized components that handle specific tasks:\n\n- **TableListAgent**: Shows available tables and schemas\n- **ChatAgent**: General conversation and summaries\n- **SQLAgent**: Generates and executes SQL queries\n- **hvPlotAgent**: Creates interactive visualizations\n- **VegaLiteAgent**: Publication-quality charts\n- **AnalysisAgent**: Custom domain-specific analyses\n\n**See**: [Built-in Agents Reference](../../references/lumen-ai/agents-reference.md) for complete agent documentation.\n\n### 2. LLM Providers\n\nLumen AI works with multiple LLM providers:\n\n**Cloud Providers**:\n- OpenAI (GPT-4o, GPT-4o-mini)\n- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus/Haiku)\n- Google (Gemini 1.5 Pro/Flash)\n- Mistral (Mistral Large/Medium/Small)\n\n**Local Models**:\n- Ollama (Llama 3.1, Mistral, CodeLlama)\n- LlamaCPP (custom models)\n\n**See**: [LLM Provider Configuration](../../references/lumen-ai/llm-providers.md) for setup details and provider comparison.\n\n### 3. Memory and Context\n\nAgents share a memory system:\n- Query results persist across interactions\n- Agents can build on previous work\n- Context maintained throughout conversation\n\n### 4. Tools\n\nExtend agent capabilities:\n- **DocumentLookup**: RAG for document context\n- **TableLookup**: Schema and metadata access\n- **Custom Tools**: External APIs, calculations, etc.\n\n**See**: [Custom Tools Guide](../../references/lumen-ai/custom-tools.md) for building tools.\n\n## Common Patterns\n\n### Pattern 1: Basic Analytics Interface\n\n```python\nimport lumen.ai as lmai\nfrom lumen.sources.duckdb import DuckDBSource\n\n# Configure LLM\nlmai.llm.llm_type = \"openai\"\nlmai.llm.model = \"gpt-4o\"\n\n# Load data\nsource = DuckDBSource(tables=[\"sales.csv\"])\n\n# Create UI\nui = lmai.ExplorerUI(\n    source=source,\n    title=\"Business Analytics\"\n)\n\nui.servable()\n```\n\n### Pattern 2: With Document Context (RAG)\n\n```python\nsource = DuckDBSource(\n    tables=[\"sales.csv\", \"products.parquet\"],\n    documents=[\n        \"./docs/data_dictionary.pdf\",\n        \"./docs/business_rules.md\"\n    ]\n)\n\nui = lmai.ExplorerUI(\n    source=source,\n    tools=[lmai.tools.DocumentLookup]\n)\n```\n\nAgents will automatically search documents for context when needed.\n\n### Pattern 3: Custom Agent\n\n```python\nfrom lumen.ai.agents import Agent\nimport param\n\nclass SentimentAgent(Agent):\n    \"\"\"Analyze sentiment in text data.\"\"\"\n\n    requires = param.List(default=[\"current_source\"])\n    provides = param.List(default=[\"sentiment_analysis\"])\n\n    purpose = \"\"\"\n    Analyzes sentiment in text columns.\n    Use when user asks about sentiment, emotions, or tone.\n    Keywords: sentiment, emotion, positive, negative, tone\n    \"\"\"\n\n    async def respond(self, query: str):\n        # Agent implementation\n        source = self.memory[\"current_source\"]\n        # ... analyze sentiment ...\n        yield \"Sentiment analysis results...\"\n\n# Use custom agent\nui = lmai.ExplorerUI(\n    source=source,\n    agents=[SentimentAgent, lmai.agents.ChatAgent]\n)\n```\n\n**See**: [Custom Agents Guide](../../references/lumen-ai/custom-agents.md) for detailed development guide.\n\n### Pattern 4: Custom Analysis\n\n```python\nfrom lumen.ai.analyses import Analysis\nfrom lumen.pipeline import Pipeline\nimport param\n\nclass CohortAnalysis(Analysis):\n    \"\"\"Customer cohort retention analysis.\"\"\"\n\n    columns = param.List(default=[\n        'customer_id', 'signup_date', 'purchase_date'\n    ])\n\n    def __call__(self, pipeline: Pipeline):\n        # Cohort analysis logic\n        df = pipeline.data\n        # ... calculate cohorts ...\n        return results\n\n# Register analysis\nui = lmai.ExplorerUI(\n    source=source,\n    agents=[\n        lmai.agents.AnalysisAgent(analyses=[CohortAnalysis])\n    ]\n)\n```\n\n**See**: [Custom Analyses Guide](../../references/lumen-ai/custom-analyses.md) for examples.\n\n### Pattern 5: Multi-Source Data\n\n```python\nfrom lumen.sources.duckdb import DuckDBSource\n\nsource = DuckDBSource(\n    tables={\n        \"sales\": \"./data/sales.parquet\",\n        \"customers\": \"./data/customers.csv\",\n        \"products\": \"https://data.company.com/products.csv\"\n    }\n)\n\nui = lmai.ExplorerUI(source=source)\n```\n\n## Configuration\n\n### LLM Selection\n\nQuick reference for choosing LLM:\n\n| Use Case | Provider | Model | Why |\n|----------|----------|-------|-----|\n| Production analytics | OpenAI | gpt-4o | Best balance |\n| Complex SQL | Anthropic | claude-3-5-sonnet | Superior reasoning |\n| High volume | OpenAI | gpt-4o-mini | Cost-effective |\n| Sensitive data | Ollama | llama3.1 | Local only |\n| Development | OpenAI | gpt-4o-mini | Fast, cheap |\n\n**See**: [LLM Provider Configuration](../../references/lumen-ai/llm-providers.md) for complete setup.\n\n### Agent Selection\n\n```python\n# Use only specific agents\nagents = [\n    lmai.agents.TableListAgent,\n    lmai.agents.SQLAgent,\n    lmai.agents.hvPlotAgent,\n    # Exclude VegaLiteAgent if not needed\n]\n\nui = lmai.ExplorerUI(source=source, agents=agents)\n```\n\n### Coordinator Types\n\n**DependencyResolver** (default): Recursively resolves agent dependencies\n```python\nui = lmai.ExplorerUI(source=source, coordinator=\"dependency\")\n```\n\n**Planner**: Creates execution plan upfront\n```python\nui = lmai.ExplorerUI(source=source, coordinator=\"planner\")\n```\n\n### UI Customization\n\n```python\nui = lmai.ExplorerUI(\n    source=source,\n    title=\"Custom Analytics AI\",\n    accent_color=\"#00aa41\",\n    suggestions=[\n        \"Show me revenue trends\",\n        \"What are the top products?\",\n        \"Create customer segmentation\"\n    ]\n)\n```\n\n## Best Practices\n\n### 1. Provider Selection\n\n- **Production**: Use Anthropic Claude 3.5 Sonnet or GPT-4o\n- **Development**: Use GPT-4o-mini for cost savings\n- **Sensitive data**: Use Ollama for local deployment\n\n### 2. Security\n\n```python\nimport os\n\n# ✅ Good: Environment variables\nlmai.llm.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# ❌ Bad: Hardcoded secrets\nlmai.llm.api_key = \"sk-...\"\n```\n\n### 3. Performance\n\n```python\n# Limit table sizes for exploration\nsource = DuckDBSource(\n    tables=[\"large_table.parquet\"],\n    table_kwargs={\"large_table\": {\"nrows\": 100000}}\n)\n```\n\n### 4. User Experience\n\n```python\n# Provide example queries\nui = lmai.ExplorerUI(\n    source=source,\n    suggestions=[\n        \"Show me revenue trends\",\n        \"Top 10 products by sales\",\n        \"Customer segmentation analysis\"\n    ]\n)\n```\n\n## Deployment\n\n### Development\n\n```bash\nlumen-ai serve app.py --autoreload --show\n```\n\n### Production\n\n```bash\npanel serve app.py \\\n  --port 80 \\\n  --num-procs 4 \\\n  --allow-websocket-origin=analytics.company.com\n```\n\n### Docker\n\n```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY app.py data/ ./\nCMD [\"panel\", \"serve\", \"app.py\", \"--port\", \"5006\", \"--address\", \"0.0.0.0\"]\n```\n\n**See**: [Deployment Guide](../../references/lumen-ai/deployment.md) for production deployment, Docker, Kubernetes, and security.\n\n## Troubleshooting\n\n### LLM Not Responding\n\n```python\n# Check API key\nimport os\nprint(os.getenv(\"OPENAI_API_KEY\"))\n\n# Test connection\ncurl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n```\n\n### Agent Not Selected\n\n```python\n# Debug which agent was selected\nprint(ui.agent_manager.last_selected_agent)\n\n# View agent purposes\nfor agent in ui.agents:\n    print(f\"{agent.__class__.__name__}: {agent.purpose}\")\n```\n\n### SQL Generation Errors\n\n- Add data dictionary as document for context\n- Provide example queries in agent prompts\n- Check table schemas match query expectations\n\n**See**: [Troubleshooting Guide](../../references/lumen-ai/troubleshooting.md) for complete troubleshooting reference.\n\n## Progressive Learning Path\n\n### Level 1: Getting Started\n1. Install and launch built-in interface\n2. Try example queries\n3. Configure LLM provider\n\n**Resources**:\n- [LLM Provider Configuration](../../references/lumen-ai/llm-providers.md)\n\n### Level 2: Python API\n1. Create basic ExplorerUI\n2. Configure agents and tools\n3. Add document context (RAG)\n\n**Resources**:\n- [Built-in Agents Reference](../../references/lumen-ai/agents-reference.md)\n\n### Level 3: Customization\n1. Build custom agents\n2. Create custom analyses\n3. Add custom tools\n\n**Resources**:\n- [Custom Agents Guide](../../references/lumen-ai/custom-agents.md)\n- [Custom Analyses Guide](../../references/lumen-ai/custom-analyses.md)\n- [Custom Tools Guide](../../references/lumen-ai/custom-tools.md)\n\n### Level 4: Production\n1. Deploy with authentication\n2. Implement monitoring\n3. Scale horizontally\n\n**Resources**:\n- [Deployment Guide](../../references/lumen-ai/deployment.md)\n\n## Additional Resources\n\n### Documentation\n- **[LLM Provider Configuration](../../references/lumen-ai/llm-providers.md)** - Setup for OpenAI, Anthropic, local models\n- **[Built-in Agents Reference](../../references/lumen-ai/agents-reference.md)** - Complete agent documentation\n- **[Custom Agents Guide](../../references/lumen-ai/custom-agents.md)** - Build specialized agents\n- **[Custom Analyses Guide](../../references/lumen-ai/custom-analyses.md)** - Domain-specific analyses\n- **[Custom Tools Guide](../../references/lumen-ai/custom-tools.md)** - Extend agent capabilities\n- **[Deployment Guide](../../references/lumen-ai/deployment.md)** - Production deployment\n- **[Troubleshooting Guide](../../references/lumen-ai/troubleshooting.md)** - Common issues and solutions\n\n### External Links\n- [Lumen AI Documentation](https://lumen.holoviz.org/lumen_ai/)\n- [Lumen AI Getting Started](https://lumen.holoviz.org/lumen_ai/getting_started/using_lumen_ai.html)\n- [GitHub Repository](https://github.com/holoviz/lumen)\n- [Community Discourse](https://discourse.holoviz.org)\n\n## Use Cases\n\n### Business Analytics\n- Ad-hoc revenue analysis\n- Customer behavior exploration\n- Sales performance tracking\n- Market segmentation\n\n### Data Science\n- Exploratory data analysis\n- Quick statistical summaries\n- Hypothesis testing\n- Pattern discovery\n\n### Operations\n- Real-time monitoring queries\n- Anomaly investigation\n- Performance metrics\n- Incident analysis\n\n### Self-Service Analytics\n- Enabling business users\n- Reducing analyst backlog\n- Democratizing data access\n- Maintaining governance\n\n## Summary\n\nLumen AI transforms data exploration through natural language interfaces powered by LLMs.\n\n**Strengths**:\n- No SQL or coding required for users\n- Flexible LLM support (cloud and local)\n- Extensible architecture\n- Privacy-focused options\n- Reduces analyst workload\n\n**Ideal for**:\n- Ad-hoc data exploration\n- Non-technical users\n- Rapid insights\n- Self-service analytics\n\n**Consider alternatives when**:\n- Fixed dashboards needed → [Lumen Dashboards](../lumen-dashboards/SKILL.md)\n- No LLM budget → Traditional BI tools\n- Highly custom logic → [Panel Dashboards](../panel-dashboards/SKILL.md)\n\n## Related Skills\n\n- **[Lumen Dashboards](../lumen-dashboards/SKILL.md)** - Declarative YAML dashboards\n- **[Panel Dashboards](../panel-dashboards/SKILL.md)** - Interactive dashboard development\n- **[Plotting Fundamentals](../plotting-fundamentals/SKILL.md)** - Quick plotting with hvPlot"
              },
              {
                "name": "lumen-dashboards",
                "description": "Master declarative, no-code data dashboards with Lumen YAML specifications. Use this skill when building standard data exploration dashboards, connecting multiple data sources (files, databases, APIs), creating interactive filters and cross-filtering, designing responsive layouts with indicators and charts, or enabling rapid dashboard prototyping without writing code.",
                "path": "community-plugins/holoviz-visualization/skills/lumen-dashboards/SKILL.md",
                "frontmatter": {
                  "name": "lumen-dashboards",
                  "description": "Master declarative, no-code data dashboards with Lumen YAML specifications. Use this skill when building standard data exploration dashboards, connecting multiple data sources (files, databases, APIs), creating interactive filters and cross-filtering, designing responsive layouts with indicators and charts, or enabling rapid dashboard prototyping without writing code.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires lumen >= 0.10.0, panel >= 1.3.0, holoviews >= 1.18.0, param >= 2.0.0. Supports PostgreSQL, DuckDB, SQLite, CSV, Parquet, Excel, and REST API data sources."
                },
                "content": "# Lumen Dashboards Skill\n\n## Overview\n\nLumen is a declarative framework for creating data dashboards through YAML specifications. Build interactive data exploration dashboards without writing code - just configuration.\n\n### What is Lumen?\n\nLumen provides a declarative approach to building data dashboards:\n\n- **No-code dashboards**: Define everything in YAML\n- **Data pipelines**: Sources → Transforms → Views\n- **Interactive exploration**: Built-in filters and cross-filtering\n- **Component library**: Reusable sources, transforms, views\n- **Live updates**: Auto-reload and real-time data\n\n### Lumen vs Panel vs Lumen AI\n\n| Feature | Lumen Dashboards | Panel | Lumen AI |\n|---------|------------------|-------|----------|\n| **Approach** | Declarative YAML | Imperative Python | Conversational |\n| **Code Required** | No | Yes | No |\n| **Use Case** | Fixed dashboards | Custom apps | Ad-hoc exploration |\n| **Flexibility** | Medium | High | High |\n| **Development Speed** | Very fast | Medium | Very fast |\n\n**Use Lumen when**:\n- Building standard data exploration dashboards\n- Working with non-programmers\n- Want rapid prototyping with configuration\n- Need reproducible dashboard specifications\n\n**Use Panel when**:\n- Need fine-grained control over components\n- Building custom application logic\n- Creating novel interactions\n\n**Use Lumen AI when**:\n- Users need ad-hoc exploration\n- Questions vary unpredictably\n- Enabling self-service analytics\n\n## Quick Start\n\n### Installation\n\n```bash\npip install lumen\n```\n\n### Your First Dashboard\n\n**File: `dashboard.yaml`**\n\n```yaml\nsources:\n  data:\n    type: file\n    tables:\n      penguins: https://datasets.holoviz.org/penguins/v1/penguins.csv\n\npipelines:\n  main:\n    source: data\n    table: penguins\n\n    filters:\n      - type: widget\n        field: species\n\nlayouts:\n  - title: Penguin Explorer\n    views:\n      - type: hvplot\n        pipeline: main\n        kind: scatter\n        x: bill_length_mm\n        y: bill_depth_mm\n        by: species\n        title: Bill Dimensions\n```\n\n**Launch:**\n```bash\nlumen serve dashboard.yaml --show\n```\n\n## Core Concepts\n\n### 1. Sources\n\nData sources provide tables for your dashboard.\n\n**Supported sources**:\n- **File**: CSV, Parquet, Excel, JSON\n- **Database**: PostgreSQL, DuckDB, SQLite\n- **REST API**: JSON endpoints\n- **Intake**: Data catalogs\n\n**Quick example**:\n```yaml\nsources:\n  mydata:\n    type: file\n    tables:\n      sales: ./data/sales.csv\n```\n\n**See**: [Data Sources Reference](../../references/lumen-dashboards/sources.md) for comprehensive source configuration.\n\n### 2. Pipelines\n\nPipelines define data flows: Source → Filters → Transforms → Views\n\n**Basic pipeline**:\n```yaml\npipelines:\n  sales_pipeline:\n    source: mydata\n    table: sales\n\n    filters:\n      - type: widget\n        field: region\n\n    transforms:\n      - type: aggregate\n        by: ['category']\n        aggregate:\n          total_sales: {revenue: sum}\n```\n\n**Components**:\n- **Filters**: Interactive widgets for user input\n- **Transforms**: Data manipulation (filter, aggregate, sort, SQL)\n- **Views**: Visualizations and tables\n\n### 3. Filters\n\nAdd interactive controls:\n\n```yaml\nfilters:\n  # Dropdown select\n  - type: widget\n    field: category\n\n  # Multi-select\n  - type: widget\n    field: region\n    multiple: true\n\n  # Date range\n  - type: widget\n    field: date\n    widget: date_range_slider\n\n  # Numeric slider\n  - type: param\n    parameter: min_revenue\n    widget_type: FloatSlider\n    start: 0\n    end: 100000\n```\n\n### 4. Transforms\n\nProcess data in pipelines:\n\n**Common transforms**:\n- `columns`: Select specific columns\n- `query`: Filter rows with pandas query\n- `aggregate`: Group and aggregate\n- `sort`: Sort data\n- `sql`: Custom SQL queries\n\n**Example**:\n```yaml\ntransforms:\n  - type: columns\n    columns: ['date', 'region', 'revenue']\n\n  - type: query\n    query: \"revenue > 1000\"\n\n  - type: aggregate\n    by: ['region']\n    aggregate:\n      total: {revenue: sum}\n      avg: {revenue: mean}\n```\n\n**See**: [Data Transforms Reference](../../references/lumen-dashboards/transforms.md) for all transform types.\n\n### 5. Views\n\nVisualize data:\n\n**View types**:\n- `hvplot`: Interactive plots (line, scatter, bar, etc.)\n- `table`: Data tables\n- `indicator`: KPI metrics\n- `vega`: Vega-Lite specifications\n- `altair`: Altair charts\n- `plotly`: Plotly charts\n\n**Example**:\n```yaml\nviews:\n  - type: hvplot\n    pipeline: main\n    kind: line\n    x: date\n    y: revenue\n    by: category\n\n  - type: indicator\n    pipeline: main\n    field: total_revenue\n    title: Total Sales\n    format: '${value:,.0f}'\n```\n\n**See**: [Views Reference](../../references/lumen-dashboards/views.md) for all view types and options.\n\n### 6. Layouts\n\nArrange views on the page:\n\n```yaml\nlayouts:\n  - title: Overview\n    layout: [[0, 1, 2], [3], [4, 5]]  # Grid positions\n    views:\n      - type: indicator\n        # View 0 config...\n\n      - type: indicator\n        # View 1 config...\n\n      - type: hvplot\n        # View 2 config...\n```\n\n**Layout types**:\n- **Grid**: `[[0, 1], [2, 3]]`\n- **Tabs**: Multiple layouts become tabs\n- **Responsive**: Adapts to screen size\n\n**See**: [Layouts Reference](../../references/lumen-dashboards/layouts.md) for advanced layout patterns.\n\n## Common Patterns\n\n### Pattern 1: KPI Dashboard\n\n```yaml\nsources:\n  metrics:\n    type: file\n    tables:\n      data: ./metrics.csv\n\npipelines:\n  kpis:\n    source: metrics\n    table: data\n    transforms:\n      - type: aggregate\n        aggregate:\n          total_revenue: {revenue: sum}\n          total_orders: {orders: sum}\n          avg_order_value: {revenue: mean}\n\nlayouts:\n  - title: KPIs\n    layout: [[0, 1, 2]]\n    views:\n      - type: indicator\n        pipeline: kpis\n        field: total_revenue\n        format: '${value:,.0f}'\n\n      - type: indicator\n        pipeline: kpis\n        field: total_orders\n        format: '{value:,.0f}'\n\n      - type: indicator\n        pipeline: kpis\n        field: avg_order_value\n        format: '${value:.2f}'\n```\n\n### Pattern 2: Filtered Exploration\n\n```yaml\npipelines:\n  explorer:\n    source: mydata\n    table: sales\n\n    filters:\n      - type: widget\n        field: region\n        label: Region\n\n      - type: widget\n        field: category\n        label: Category\n        multiple: true\n\n      - type: widget\n        field: date\n        widget: date_range_slider\n\n    views:\n      - type: hvplot\n        kind: scatter\n        x: price\n        y: quantity\n        by: category\n\n      - type: table\n        page_size: 20\n```\n\n### Pattern 3: Multi-Source Dashboard\n\n```yaml\nsources:\n  sales_db:\n    type: postgres\n    connection_string: postgresql://localhost/sales\n    tables: [orders, customers]\n\n  inventory_file:\n    type: file\n    tables:\n      stock: ./inventory.csv\n\npipelines:\n  sales_pipeline:\n    source: sales_db\n    table: orders\n\n  inventory_pipeline:\n    source: inventory_file\n    table: stock\n```\n\n### Pattern 4: Cross-Filtering\n\n```yaml\npipelines:\n  main:\n    source: data\n    table: sales\n\n    filters:\n      - type: widget\n        field: region\n\nlayouts:\n  - title: Analysis\n    views:\n      # Clicking bar filters other views\n      - type: hvplot\n        pipeline: main\n        kind: bar\n        x: category\n        y: revenue\n        selection_group: category_filter\n\n      # Responds to selection above\n      - type: hvplot\n        pipeline: main\n        kind: scatter\n        x: price\n        y: quantity\n        selection_group: category_filter\n```\n\n### Pattern 5: SQL Transform\n\n```yaml\ntransforms:\n  - type: sql\n    query: |\n      SELECT\n        region,\n        category,\n        SUM(revenue) as total_revenue,\n        COUNT(*) as order_count,\n        AVG(revenue) as avg_order_value\n      FROM table\n      WHERE date >= '2024-01-01'\n      GROUP BY region, category\n      HAVING total_revenue > 10000\n      ORDER BY total_revenue DESC\n```\n\n## Python API\n\nWhile Lumen is designed for YAML, you can also use Python:\n\n```python\nfrom lumen.sources import FileSource\nfrom lumen.pipeline import Pipeline\nfrom lumen.views import hvPlotView\nfrom lumen.dashboard import Dashboard\nimport panel as pn\n\n# Create source\nsource = FileSource(tables={'sales': './data/sales.csv'})\n\n# Create pipeline\npipeline = Pipeline(source=source, table='sales')\n\n# Create view\nview = hvPlotView(\n    pipeline=pipeline,\n    kind='scatter',\n    x='price',\n    y='quantity'\n)\n\n# Create dashboard\ndashboard = Dashboard(\n    pipelines={'main': pipeline},\n    layouts=[view]\n)\n\n# Serve\ndashboard.servable()\n```\n\n**See**: [Python API Reference](../../references/lumen-dashboards/python-api.md) for detailed API usage.\n\n## Configuration\n\n### Global Config\n\n```yaml\nconfig:\n  title: My Dashboard\n  theme: dark  # or 'default', 'material'\n  sizing_mode: stretch_width\n  logo: ./logo.png\n  favicon: ./favicon.ico\n  layout: column  # or 'grid', 'tabs'\n```\n\n### Themes\n\n```yaml\nconfig:\n  theme: material\n  theme_json:\n    palette:\n      primary: '#00aa41'\n      secondary: '#616161'\n```\n\n### Authentication\n\n```bash\n# Serve with auth\nlumen serve dashboard.yaml \\\n  --oauth-provider=generic \\\n  --oauth-key=${OAUTH_KEY} \\\n  --oauth-secret=${OAUTH_SECRET}\n```\n\n## Deployment\n\n### Development\n\n```bash\n# Local with auto-reload\nlumen serve dashboard.yaml --autoreload --show\n\n# Specific port\nlumen serve dashboard.yaml --port 5007\n```\n\n### Production\n\n```bash\n# Production server\npanel serve dashboard.yaml \\\n  --port 80 \\\n  --num-procs 4 \\\n  --allow-websocket-origin=analytics.company.com\n```\n\n### Docker\n\n```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY dashboard.yaml data/ ./\nCMD [\"lumen\", \"serve\", \"dashboard.yaml\", \"--port\", \"5006\", \"--address\", \"0.0.0.0\"]\n```\n\n**See**: [Deployment Guide](../../references/lumen-dashboards/deployment.md) for production deployment best practices.\n\n## Best Practices\n\n### 1. Source Organization\n\n```yaml\n# ✅ Good: Descriptive names\nsources:\n  sales_database:\n    type: postgres\n    tables: [orders, customers]\n\n  inventory_files:\n    type: file\n    tables:\n      stock: ./inventory.csv\n\n# ❌ Bad: Generic names\nsources:\n  db1:\n    type: postgres\n  file1:\n    type: file\n```\n\n### 2. Pipeline Reusability\n\n```yaml\n# Define reusable pipelines\npipelines:\n  base_sales:\n    source: data\n    table: sales\n    filters:\n      - type: widget\n        field: region\n\n  summary_sales:\n    pipeline: base_sales  # Extends base_sales\n    transforms:\n      - type: aggregate\n        by: ['category']\n        aggregate:\n          total: {revenue: sum}\n```\n\n### 3. Performance\n\n```yaml\n# Limit data size for large tables\nsources:\n  bigdata:\n    type: postgres\n    tables:\n      events: \"SELECT * FROM events WHERE date >= '2024-01-01' LIMIT 100000\"\n```\n\n### 4. User Experience\n\n```yaml\n# Provide clear labels and formatting\nfilters:\n  - type: widget\n    field: region\n    label: \"Sales Region\"  # Clear label\n\nviews:\n  - type: indicator\n    field: revenue\n    title: \"Total Revenue\"\n    format: '${value:,.0f}'  # Formatted display\n```\n\n## Troubleshooting\n\n### Dashboard Won't Load\n\n```bash\n# Check YAML syntax\npython -c \"import yaml; yaml.safe_load(open('dashboard.yaml'))\"\n\n# Run with debug logging\nlumen serve dashboard.yaml --log-level=debug\n```\n\n### Data Not Showing\n\n- Verify data source path/connection\n- Check table names match YAML config\n- Ensure columns referenced exist in data\n\n### Performance Issues\n\n- Limit query results (use SQL WHERE clauses)\n- Reduce number of rows displayed\n- Use aggregation before visualization\n\n**See**: [Troubleshooting Guide](../../references/lumen-dashboards/troubleshooting.md) for common issues.\n\n## Progressive Learning Path\n\n### Level 1: Basics\n1. Create simple file-based dashboard\n2. Add filters\n3. Create basic views\n\n**Resources**:\n- Quick Start (this doc)\n- [Data Sources Reference](../../references/lumen-dashboards/sources.md)\n\n### Level 2: Transforms\n1. Filter and aggregate data\n2. Use SQL transforms\n3. Chain multiple transforms\n\n**Resources**:\n- [Data Transforms Reference](../../references/lumen-dashboards/transforms.md)\n\n### Level 3: Advanced Layouts\n1. Multi-page dashboards\n2. Cross-filtering\n3. Custom themes\n\n**Resources**:\n- [Layouts Reference](../../references/lumen-dashboards/layouts.md)\n- [Views Reference](../../references/lumen-dashboards/views.md)\n\n### Level 4: Production\n1. Database integration\n2. Authentication\n3. Deployment\n\n**Resources**:\n- [Deployment Guide](../../references/lumen-dashboards/deployment.md)\n\n## Additional Resources\n\n### Documentation\n- **[Data Sources Reference](../../references/lumen-dashboards/sources.md)** - All source types and configuration\n- **[Data Transforms Reference](../../references/lumen-dashboards/transforms.md)** - Complete transform reference\n- **[Views Reference](../../references/lumen-dashboards/views.md)** - All visualization types\n- **[Layouts Reference](../../references/lumen-dashboards/layouts.md)** - Layout patterns and organization\n- **[Python API Reference](../../references/lumen-dashboards/python-api.md)** - Programmatic dashboard creation\n- **[Deployment Guide](../../references/lumen-dashboards/deployment.md)** - Production deployment\n- **[Examples](../../references/lumen-dashboards/examples.md)** - Complete dashboard examples\n- **[Troubleshooting Guide](../../references/lumen-dashboards/troubleshooting.md)** - Common issues\n\n### External Links\n- [Lumen Documentation](https://lumen.holoviz.org/)\n- [Lumen Gallery](https://lumen.holoviz.org/gallery/)\n- [GitHub Repository](https://github.com/holoviz/lumen)\n- [Community Discourse](https://discourse.holoviz.org)\n\n## Use Cases\n\n### Business Intelligence\n- Executive dashboards\n- Sales analytics\n- Financial reporting\n- Operational metrics\n\n### Data Exploration\n- Dataset overview\n- Interactive filtering\n- Drill-down analysis\n- Comparative views\n\n### Real-Time Monitoring\n- Live data feeds\n- Alert dashboards\n- System metrics\n- Performance tracking\n\n### Reporting\n- Scheduled reports\n- Standardized views\n- Shareable dashboards\n- Embedded analytics\n\n## Summary\n\nLumen enables rapid dashboard development through declarative YAML specifications.\n\n**Strengths**:\n- No Python code required\n- Fast development cycle\n- Reproducible specifications\n- Built-in interactivity\n- Standard dashboard patterns\n\n**Ideal for**:\n- Fixed dashboard layouts\n- Standard data patterns\n- Non-programmer dashboard creators\n- Rapid prototyping\n\n**Consider alternatives when**:\n- Need custom application logic → [Panel Dashboards](../panel-dashboards/SKILL.md)\n- Need ad-hoc exploration → [Lumen AI](../lumen-ai/SKILL.md)\n- Building novel interactions → [Panel Dashboards](../panel-dashboards/SKILL.md)\n\n## Related Skills\n\n- **[Lumen AI](../lumen-ai/SKILL.md)** - Conversational data exploration\n- **[Panel Dashboards](../panel-dashboards/SKILL.md)** - Custom Python dashboards\n- **[Plotting Fundamentals](../plotting-fundamentals/SKILL.md)** - Quick plotting with hvPlot"
              },
              {
                "name": "panel-dashboards",
                "description": "Master interactive dashboard and application development with Panel and Param. Use this skill when building custom web applications with Python, creating reactive component-based UIs, handling file uploads and real-time data streaming, implementing multi-page applications, or developing enterprise dashboards with templates and theming.",
                "path": "community-plugins/holoviz-visualization/skills/panel-dashboards/SKILL.md",
                "frontmatter": {
                  "name": "panel-dashboards",
                  "description": "Master interactive dashboard and application development with Panel and Param. Use this skill when building custom web applications with Python, creating reactive component-based UIs, handling file uploads and real-time data streaming, implementing multi-page applications, or developing enterprise dashboards with templates and theming.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires panel >= 1.3.0, param >= 2.0.0, bokeh >= 3.0.0, tornado (web server). Supports Material Design, Bootstrap, and custom themes."
                },
                "content": "# Panel Dashboards Skill\n\n## Overview\n\nMaster interactive dashboard and application development with Panel and Param. This skill covers building web applications, component systems, and responsive dashboards that scale from simple tools to complex enterprise applications.\n\n## Dependencies\n\n- panel >= 1.3.0\n- param >= 2.0.0\n- bokeh >= 3.0.0\n- tornado (web server)\n\n## Core Capabilities\n\n### 1. Component-Based Application Development\n\nPanel provides a comprehensive component library for building rich user interfaces:\n\n- **Layout Components**: Row, Column, Tabs, Accordion, GridBox\n- **Input Widgets**: TextInput, Select, DatePicker, RangeSlider, FileInput\n- **Output Display**: Markdown, HTML, DataFrame, Image, Video\n- **Container Controls**: Card, Alert, ProgressBar\n\n```python\nimport panel as pn\nimport param\n\npn.extension('material')\n\nclass Dashboard(param.Parameterized):\n    title = param.String(default=\"My Dashboard\")\n    refresh_interval = param.Integer(default=5000, bounds=(1000, 60000))\n\n    @param.depends('refresh_interval')\n    def view(self):\n        return pn.Column(\n            pn.pane.Markdown(f\"## {self.title}\"),\n            pn.param.ObjectSelector.from_param(self.param.refresh_interval),\n            pn.Row(self._metric_card(), self._chart())\n        )\n\n    def _metric_card(self):\n        return pn.Card(\n            \"Active Users\",\n            \"42,531\",\n            title=\"Metrics\",\n            styles={\"background\": \"#E8F4F8\"}\n        )\n\n    def _chart(self):\n        return pn.pane.Markdown(\"## Chart Placeholder\")\n\ndashboard = Dashboard()\napp = dashboard.view\n\nif __name__ == '__main__':\n    app.servable()\n```\n\n### 2. Reactive Pipelines and Watchers\n\nPanel excels at creating reactive, event-driven applications:\n\n```python\nimport panel as pn\nimport param\nimport numpy as np\n\nclass DataAnalyzer(param.Parameterized):\n    data_source = param.Selector(default='random', objects=['random', 'file'])\n    num_points = param.Integer(default=100, bounds=(10, 1000))\n    aggregation = param.Selector(default='mean', objects=['mean', 'sum', 'std'])\n\n    @param.depends('data_source', 'num_points', watch=True)\n    def _refresh_data(self):\n        if self.data_source == 'random':\n            self.data = np.random.randn(self.num_points)\n\n    @param.depends('data_source', 'num_points', 'aggregation')\n    def summary(self):\n        if not hasattr(self, 'data'):\n            self._refresh_data()\n\n        agg_func = getattr(np, self.aggregation)\n        result = agg_func(self.data)\n        return f\"{self.aggregation.capitalize()}: {result:.2f}\"\n\nanalyzer = DataAnalyzer()\n\npn.extension('material')\napp = pn.Column(\n    pn.param.ParamMethod.from_param(analyzer.param),\n    analyzer.summary\n)\n```\n\n### 3. Template and Theming\n\nPanel supports multiple templates for different application styles:\n\n- **BootstrapTemplate**: Modern Bootstrap-based design\n- **MaterialTemplate**: Material Design principles\n- **VanillaTemplate**: Clean, minimal design\n- **DarkTemplate**: Dark mode optimized\n\n```python\nimport panel as pn\nimport param\n\npn.extension('material')\n\nclass Config(param.Parameterized):\n    theme = param.Selector(default='dark', objects=['dark', 'light'])\n    sidebar_width = param.Integer(default=300, bounds=(200, 500))\n\nconfig = Config()\n\ntemplate = pn.template.MaterialTemplate(\n    title=\"Advanced Dashboard\",\n    header_background=\"#2E3440\",\n    sidebar_width=config.sidebar_width,\n    main=[pn.pane.Markdown(\"# Main Content\")],\n    sidebar=[\n        pn.param.ParamMethod.from_param(config.param)\n    ]\n)\n\ntemplate.servable()\n```\n\n### 4. File Handling and Data Upload\n\nBuild applications that accept file uploads and process data:\n\n```python\nimport panel as pn\nimport pandas as pd\n\nfile_input = pn.widgets.FileInput(accept='.csv,.xlsx')\n\n@pn.depends(file_input)\ndef process_file(file_input):\n    if file_input is None:\n        return pn.pane.Markdown(\"### Upload a file to proceed\")\n\n    if file_input.filename.endswith('.csv'):\n        df = pd.read_csv(file_input.value)\n    else:\n        df = pd.read_excel(file_input.value)\n\n    return pn.Column(\n        pn.pane.Markdown(f\"### {file_input.filename}\"),\n        pn.pane.DataFrame(df.head(10), width=800),\n        pn.pane.Markdown(f\"Shape: {df.shape}\")\n    )\n\npn.extension('material')\napp = pn.Column(\n    pn.pane.Markdown(\"# Data Upload\"),\n    file_input,\n    process_file\n)\n```\n\n### 5. Real-time Streaming and Updates\n\nCreate dashboards with live data updates:\n\n```python\nimport panel as pn\nimport param\nimport numpy as np\nfrom datetime import datetime\n\nclass LiveMonitor(param.Parameterized):\n    update_frequency = param.Integer(default=1000, bounds=(100, 5000))\n    is_running = param.Boolean(default=False)\n    current_value = param.Number(default=0)\n\n    def __init__(self, **params):\n        super().__init__(**params)\n        self._data_history = []\n\n    def start(self):\n        self.is_running = True\n        pn.state.add_periodic_callback(\n            self._update,\n            period=self.update_frequency,\n            start=True\n        )\n\n    def _update(self):\n        if self.is_running:\n            self.current_value = np.random.randn() + self.current_value * 0.95\n            self._data_history.append({\n                'timestamp': datetime.now(),\n                'value': self.current_value\n            })\n\n    def get_plot(self):\n        if not self._data_history:\n            return pn.pane.Markdown(\"No data yet...\")\n\n        import holoviews as hv\n        df = pd.DataFrame(self._data_history)\n        return hv.Curve(df, 'timestamp', 'value').opts(responsive=True)\n\nmonitor = LiveMonitor()\napp = pn.Column(\n    pn.widgets.Button.from_param(monitor.param.is_running, label=\"Start/Stop\"),\n    monitor.get_plot\n)\n```\n\n## Best Practices\n\n### 1. Parameter Organization\n- Use Param classes to organize all configurable state\n- Leverage type hints and validation in parameter definitions\n- Use watchers for side effects, depends for reactive updates\n\n### 2. Responsive Design\n- Always use `responsive=True` and `sizing_mode` options\n- Test on multiple screen sizes\n- Use GridBox or CSS Grid for complex layouts\n\n### 3. Performance Optimization\n- Lazy-load expensive components using Tabs or Accordion\n- Use caching decorators for expensive computations\n- Implement pagination for large datasets\n- Stream data rather than loading all at once\n\n### 4. Code Organization\n- Separate UI concerns from business logic using Param classes\n- Create reusable component functions\n- Use templates for consistent application structure\n- Organize related components into modules\n\n### 5. Error Handling\n- Validate input parameters with Param bounds and selectors\n- Provide clear error messages to users\n- Use try-catch blocks around external API calls\n- Implement graceful degradation for failed operations\n\n## Common Patterns\n\n### Pattern 1: Multi-Page Application\n```python\nclass MultiPageApp(param.Parameterized):\n    page = param.Selector(default='home', objects=['home', 'analytics', 'settings'])\n\n    @param.depends('page')\n    def current_view(self):\n        pages = {\n            'home': self._home_page,\n            'analytics': self._analytics_page,\n            'settings': self._settings_page,\n        }\n        return pages[self.page]()\n```\n\n### Pattern 2: Form with Validation\n```python\nclass FormValidator(param.Parameterized):\n    email = param.String(default='')\n    age = param.Integer(default=0, bounds=(0, 150))\n\n    @param.depends('email', 'age')\n    def validation_message(self):\n        if not self.email or '@' not in self.email:\n            return pn.pane.Alert(\"Invalid email\", alert_type='danger')\n        if self.age < 18:\n            return pn.pane.Alert(\"Must be 18+\", alert_type='warning')\n        return pn.pane.Alert(\"Validation passed!\", alert_type='success')\n```\n\n### Pattern 3: Data Filtering Pipeline\n```python\nclass FilteredDataView(param.Parameterized):\n    df = param.Parameter(default=None)\n    column_filter = param.String(default='')\n    value_filter = param.String(default='')\n\n    @param.depends('column_filter', 'value_filter')\n    def filtered_data(self):\n        if self.column_filter not in self.df.columns:\n            return self.df\n        return self.df[self.df[self.column_filter].astype(str).str.contains(self.value_filter)]\n```\n\n## Integration with HoloViz Ecosystem\n\nPanel integrates seamlessly with other HoloViz libraries:\n\n- **HoloViews**: Embed interactive plots in Panel applications\n- **hvPlot**: Quick plotting within Panel dashboards\n- **Param**: Unified parameter system for all interactivity\n- **GeoViews**: Embed geographic visualizations\n- **Datashader**: Render large datasets with Panel\n\n## Common Use Cases\n\n1. **Real-time Monitoring Dashboards**: Live metrics and KPI displays\n2. **Data Exploration Tools**: Interactive data analysis applications\n3. **Configuration Interfaces**: Complex multi-step configuration UIs\n4. **Data Input Applications**: Validated form-based data collection\n5. **Report Viewers**: Interactive report generation and browsing\n6. **Administrative Interfaces**: Internal tools for data management\n\n## Troubleshooting\n\n### Issue: Slow Dashboard Load Times\n- Lazy-load components using Tabs or Accordion\n- Implement caching with `@pn.cache` decorator\n- Move expensive computations to initialization\n- Profile with Panel's built-in profiling tools\n\n### Issue: Unresponsive UI During Computation\n- Use `pn.state.add_periodic_callback` for background tasks\n- Implement loading indicators during processing\n- Break long computations into smaller steps\n- Consider async/await patterns\n\n### Issue: Memory Leaks in Long-Running Apps\n- Clean up event listeners with `pn.state.clear_caches()`\n- Monitor callback registration and removal\n- Limit data history sizes in streaming applications\n- Profile with memory profilers\n\n## Resources\n\n- [Panel Documentation](https://panel.holoviz.org)\n- [Panel Gallery](https://panel.holoviz.org/gallery/index.html)\n- [Param Documentation](https://param.holoviz.org)\n- [Panel Discourse Community](https://discourse.holoviz.org)"
              },
              {
                "name": "parameterization",
                "description": "Master declarative parameter systems with Param for type-safe configuration. Use this skill when building parameterized classes with automatic validation, creating reactive dependencies with @param.depends, implementing watchers for side effects, auto-generating UIs from parameters, or organizing application configuration with hierarchical parameter structures.",
                "path": "community-plugins/holoviz-visualization/skills/parameterization/SKILL.md",
                "frontmatter": {
                  "name": "parameterization",
                  "description": "Master declarative parameter systems with Param for type-safe configuration. Use this skill when building parameterized classes with automatic validation, creating reactive dependencies with @param.depends, implementing watchers for side effects, auto-generating UIs from parameters, or organizing application configuration with hierarchical parameter structures.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires param >= 2.0.0, panel >= 1.3.0 (for UI generation), numpy >= 1.15.0, pandas >= 1.0.0"
                },
                "content": "# Parameterization Skill\n\n## Overview\n\nMaster declarative parameter systems with Param and dynamic UI generation. This skill covers building flexible, type-safe, and auto-validated application logic.\n\n## Dependencies\n\n- param >= 2.0.0\n- panel >= 1.3.0 (for UI generation)\n- numpy >= 1.15.0\n- pandas >= 1.0.0\n\n## Core Capabilities\n\n### 1. Parameter Basics\n\nParam provides a framework for parameterized objects with automatic validation:\n\n```python\nimport param\nimport numpy as np\n\nclass DataProcessor(param.Parameterized):\n    # Basic parameters\n    name = param.String(default='Processor', doc='Name of processor')\n    count = param.Integer(default=10, bounds=(1, 1000), doc='Number of items')\n    scale = param.Number(default=1.0, bounds=(0.1, 10.0), doc='Scale factor')\n\n    # String choices\n    method = param.Selector(default='mean', objects=['mean', 'median', 'sum'])\n\n    # Boolean flag\n    normalize = param.Boolean(default=False)\n\n    # List or array\n    tags = param.List(default=[], item_type=str)\n    data_array = param.Array(default=np.array([]))\n\n# Instantiate and use\nprocessor = DataProcessor()\nprint(f\"Name: {processor.name}, Count: {processor.count}\")\n\n# Validate parameters automatically\nprocessor.count = 500  # OK\nprocessor.count = 2000  # Raises error: out of bounds\n```\n\n### 2. Advanced Parameter Types\n\n```python\nclass AdvancedConfig(param.Parameterized):\n    # Date/time parameters\n    date = param.Date(default='2024-01-01', doc='Start date')\n    time = param.Time(default='12:00', doc='Start time')\n    datetime = param.DateTime(default='2024-01-01 12:00:00')\n\n    # File/path parameters\n    input_file = param.Path(default=None, doc='Input file path')\n    output_dir = param.Fspath(default='.', doc='Output directory')\n\n    # Range parameter\n    value_range = param.Range(default=(0, 10), bounds=(0, 100))\n\n    # Color parameter\n    color = param.Color(default='#FF0000')\n\n    # JSON/Dict parameter\n    config = param.Dict(default={}, per_instance=True)\n\n    # DataFrame parameter\n    dataframe = param.Parameter(default=None)\n```\n\n### 3. Dynamic Dependencies with @param.depends\n\n```python\nclass DataAnalyzer(param.Parameterized):\n    data_source = param.Selector(\n        default='random',\n        objects=['random', 'sine', 'exponential']\n    )\n    amplitude = param.Number(default=1.0, bounds=(0.1, 10.0))\n    frequency = param.Number(default=1.0, bounds=(0.1, 10.0))\n    size = param.Integer(default=100, bounds=(10, 10000))\n\n    @param.depends('data_source', 'amplitude', 'frequency', 'size')\n    def get_data(self):\n        \"\"\"Automatically called when any dependency changes\"\"\"\n        np.random.seed(42)\n        x = np.linspace(0, 2*np.pi, self.size)\n\n        if self.data_source == 'random':\n            return x, np.random.randn(self.size) * self.amplitude\n        elif self.data_source == 'sine':\n            return x, self.amplitude * np.sin(self.frequency * x)\n        else:  # exponential\n            return x, self.amplitude * np.exp(self.frequency * x / 10)\n\n    @param.depends('data_source', 'amplitude', 'frequency', 'size')\n    def summary(self):\n        \"\"\"Display summary that updates automatically\"\"\"\n        x, y = self.get_data()\n        return f\"Mean: {y.mean():.2f}, Std: {y.std():.2f}\"\n\n# Use in application\nanalyzer = DataAnalyzer()\nprint(analyzer.summary)\n\n# Change parameters\nanalyzer.amplitude = 2.0\nanalyzer.frequency = 2.0\nprint(analyzer.summary)  # Updated automatically\n```\n\n### 4. Watchers for Side Effects\n\nWatchers allow you to trigger code when parameters change:\n\n```python\nclass DataModel(param.Parameterized):\n    filename = param.String(default='data.csv')\n    data = param.Parameter(default=None, precedence=-1)\n\n    @param.depends('filename', watch=True)\n    def _load_data(self):\n        \"\"\"Automatically load data when filename changes\"\"\"\n        print(f\"Loading {self.filename}...\")\n        # Load file here\n        self.data = pd.read_csv(self.filename)\n\n    # Alternative: explicit watch\n    def __init__(self, **params):\n        super().__init__(**params)\n        self.param.watch(self._on_count_change, 'count')\n\n    def _on_count_change(self, event):\n        print(f\"Count changed from {event.old} to {event.new}\")\n\nmodel = DataModel()\nmodel.filename = 'new_file.csv'  # Triggers _load_data automatically\n```\n\n### 5. Custom Validation\n\n```python\nclass ValidatedModel(param.Parameterized):\n    email = param.String(default='', doc='Email address')\n    age = param.Integer(default=0, bounds=(0, 150))\n    password = param.String(default='')\n\n    @param.validators('email')\n    def validate_email(self, value):\n        if '@' not in value:\n            raise ValueError('Invalid email address')\n        return value\n\n    @param.validators('password')\n    def validate_password(self, value):\n        if len(value) < 8:\n            raise ValueError('Password must be at least 8 characters')\n        return value\n\n    def validate_constraint(self):\n        \"\"\"Cross-parameter validation\"\"\"\n        if self.age < 18 and self.email == 'restricted@example.com':\n            raise ValueError('Minors cannot use this email')\n\nmodel = ValidatedModel()\nmodel.email = 'invalid'  # Raises ValueError\nmodel.password = 'short'  # Raises ValueError\n```\n\n### 6. Hierarchical Parameterization\n\n```python\nclass DatabaseConfig(param.Parameterized):\n    host = param.String(default='localhost')\n    port = param.Integer(default=5432, bounds=(1, 65535))\n    username = param.String(default='user')\n    password = param.String(default='')\n\nclass AppConfig(param.Parameterized):\n    app_name = param.String(default='MyApp')\n    debug = param.Boolean(default=False)\n    database = param.Parameter(default=DatabaseConfig())\n\n    @param.depends('database.host', watch=True)\n    def _on_db_change(self):\n        print(f\"Database configuration changed to {self.database.host}\")\n\nconfig = AppConfig()\nconfig.database.host = 'production.db'  # Triggers watch on parent\n```\n\n## Integration with Panel UI\n\n### 1. Automatic UI Generation\n\n```python\nimport panel as pn\n\nclass DashboardConfig(param.Parameterized):\n    title = param.String(default='Dashboard')\n    refresh_interval = param.Integer(default=5000, bounds=(1000, 60000))\n    metric = param.Selector(default='revenue', objects=['revenue', 'users', 'engagement'])\n    show_legend = param.Boolean(default=True)\n\nconfig = DashboardConfig()\n\n# Panel automatically creates UI widgets from parameters\nwidgets = pn.param.ParamMethod.from_param(config.param)\n\n# Or create individual widgets\ntitle_input = pn.param.TextInput.from_param(config.param.title)\nmetric_select = pn.param.Selector.from_param(config.param.metric)\ninterval_slider = pn.param.IntSlider.from_param(config.param.refresh_interval)\n```\n\n### 2. Reactive Dashboard\n\n```python\nimport holoviews as hv\n\nclass InteractiveDashboard(param.Parameterized):\n    metric = param.Selector(default='sales', objects=['sales', 'users', 'traffic'])\n    time_range = param.Range(default=(0, 100), bounds=(0, 100))\n    aggregation = param.Selector(default='daily', objects=['hourly', 'daily', 'weekly'])\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    @param.depends('metric', 'time_range', 'aggregation')\n    def plot(self):\n        filtered = self.data[\n            (self.data['metric'] == self.metric) &\n            (self.data['value'] >= self.time_range[0]) &\n            (self.data['value'] <= self.time_range[1])\n        ]\n        return filtered.hvplot.line(title=f'{self.metric} ({self.aggregation})')\n\n    @param.depends('metric')\n    def summary(self):\n        subset = self.data[self.data['metric'] == self.metric]\n        return f\"Mean: {subset['value'].mean():.2f}\"\n\ndashboard = InteractiveDashboard(data_df)\n\npn.extension('material')\napp = pn.Column(\n    pn.param.ParamMethod.from_param(dashboard.param),\n    pn.Column(dashboard.plot, dashboard.summary)\n)\n```\n\n## Best Practices\n\n### 1. Parameter Organization\n```python\n# Group related parameters\nclass VideoConfig(param.Parameterized):\n    # Video inputs\n    input_file = param.Path(doc='Input video file')\n    start_frame = param.Integer(default=0, bounds=(0, None))\n    end_frame = param.Integer(default=None, bounds=(0, None))\n\n    # Processing\n    scale = param.Number(default=1.0, bounds=(0.1, 2.0))\n    quality = param.Selector(default='high', objects=['low', 'medium', 'high'])\n\n    # Output\n    output_format = param.Selector(default='mp4', objects=['mp4', 'webm', 'mov'])\n    output_path = param.Path(default='output/')\n```\n\n### 2. Use Appropriate Parameter Types\n```python\n# Bad: string for everything\nconfig = param.Parameterized(\n    count=param.String(default='10'),  # Wrong type\n    flag=param.String(default='true')  # Wrong type\n)\n\n# Good: specific types with validation\nclass ProperConfig(param.Parameterized):\n    count = param.Integer(default=10, bounds=(1, 100))\n    flag = param.Boolean(default=True)\n```\n\n### 3. Leverage Watchers for Side Effects\n```python\nclass FileProcessor(param.Parameterized):\n    input_path = param.Path(default=None)\n    output_path = param.Path(default=None)\n\n    # Use watch for file I/O and external effects\n    @param.depends('input_path', watch=True)\n    def _process_file(self):\n        if self.input_path and self.input_path.exists():\n            self._load_and_process()\n\n    # Use depends for computation\n    @param.depends('input_path')\n    def file_size(self):\n        if self.input_path:\n            return self.input_path.stat().st_size\n        return None\n```\n\n### 4. Documentation and Help\n```python\nclass WellDocumentedModel(param.Parameterized):\n    threshold = param.Number(\n        default=0.5,\n        bounds=(0, 1),\n        doc='Classification threshold. Values above this are classified as positive.',\n        label='Classification Threshold',\n        precedence=1  # Show first\n    )\n\n    @param.depends('threshold')\n    def summary(self):\n        \"\"\"Return human-readable summary\"\"\"\n        return f\"Using threshold: {self.threshold}\"\n\n# Users can access help\nhelp(WellDocumentedModel)\nmodel = WellDocumentedModel()\nprint(model.param)  # Display all parameters with documentation\n```\n\n## Common Patterns\n\n### Pattern 1: Configuration Object\n```python\nclass AppConfiguration(param.Parameterized):\n    \"\"\"Central configuration object for entire application\"\"\"\n    debug = param.Boolean(default=False)\n    log_level = param.Selector(default='INFO', objects=['DEBUG', 'INFO', 'WARNING', 'ERROR'])\n    theme = param.Selector(default='light', objects=['light', 'dark'])\n\n    @classmethod\n    def from_file(cls, path):\n        \"\"\"Load configuration from file\"\"\"\n        import json\n        with open(path) as f:\n            config = json.load(f)\n        return cls(**config)\n```\n\n### Pattern 2: Multi-Step Wizard\n```python\nclass Wizard(param.Parameterized):\n    step = param.Integer(default=0, bounds=(0, 3))\n\n    # Step 1: Data input\n    data_source = param.Selector(default='file', objects=['file', 'database', 'api'])\n\n    # Step 2: Processing\n    algorithm = param.Selector(default='mean', objects=['mean', 'median'])\n\n    # Step 3: Output\n    format = param.Selector(default='csv', objects=['csv', 'json', 'parquet'])\n\n    @param.depends('step')\n    def current_step_view(self):\n        if self.step == 0:\n            return f\"Select data source: {self.data_source}\"\n        elif self.step == 1:\n            return f\"Choose algorithm: {self.algorithm}\"\n        else:\n            return f\"Output format: {self.format}\"\n```\n\n### Pattern 3: Computed Parameters\n```python\nclass Statistics(param.Parameterized):\n    values = param.Array(default=np.array([]))\n\n    @property\n    def mean(self):\n        return np.mean(self.values)\n\n    @property\n    def std(self):\n        return np.std(self.values)\n\n    @param.depends('values')\n    def summary(self):\n        return f\"Mean: {self.mean:.2f}, Std: {self.std:.2f}\"\n```\n\n## Integration with HoloViz Ecosystem\n\n- **Panel**: Auto-generate UIs from parameters\n- **HoloViews**: Create parameter-driven visualizations\n- **hvPlot**: Quick plots responding to parameter changes\n- **Datashader**: Efficient rendering with parameterized aggregation\n\n## Common Use Cases\n\n1. **Configuration Management**: Centralized app configuration\n2. **Data Processing Pipelines**: Parameterized workflows\n3. **Scientific Simulations**: Configurable model parameters\n4. **Interactive Dashboards**: Auto-generated control panels\n5. **Machine Learning**: Hyperparameter tuning interfaces\n6. **Data Validation**: Type-safe data entry\n\n## Troubleshooting\n\n### Issue: Watcher Not Triggering\n- Use `watch=True` on the `@param.depends` decorator\n- Ensure parameter actually changes\n- Check parameter name matches exactly\n\n### Issue: Circular Dependencies\n- Avoid watchers that modify parameters they depend on\n- Use separate input and derived parameters\n- Consider separating concerns into different objects\n\n### Issue: Performance with Expensive Computations\n- Cache results with `@param.depends(..., cache_on=[])`\n- Use explicit parameter changes rather than constant updates\n- Profile with profilers to find bottlenecks\n\n## Resources\n\n- [Param Documentation](https://param.holoviz.org)\n- [Param User Guide](https://param.holoviz.org/user_guide/index.html)\n- [Panel Parameter Support](https://panel.holoviz.org/how_to/parameters/index.html)\n- [Param API Reference](https://param.holoviz.org/API/param.Parameterized.html)"
              },
              {
                "name": "plotting-fundamentals",
                "description": "Master quick plotting and interactive visualization with hvPlot. Use this skill when creating basic plots (line, scatter, bar, histogram, box), visualizing pandas DataFrames with minimal code, adding interactivity and hover tools, composing multiple plots in layouts, or generating publication-quality visualizations rapidly.",
                "path": "community-plugins/holoviz-visualization/skills/plotting-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "plotting-fundamentals",
                  "description": "Master quick plotting and interactive visualization with hvPlot. Use this skill when creating basic plots (line, scatter, bar, histogram, box), visualizing pandas DataFrames with minimal code, adding interactivity and hover tools, composing multiple plots in layouts, or generating publication-quality visualizations rapidly.",
                  "version": "2025-01-07T00:00:00.000Z",
                  "compatibility": "Requires hvplot >= 0.9.0, holoviews >= 1.18.0, pandas >= 1.0.0, numpy >= 1.15.0, bokeh >= 3.0.0"
                },
                "content": "# Plotting Fundamentals Skill\n\n## Overview\n\nMaster quick plotting and interactive visualization with hvPlot and HoloViews basics. This skill covers essential techniques for creating publication-quality plots with minimal code.\n\n## Dependencies\n\n- hvplot >= 0.9.0\n- holoviews >= 1.18.0\n- pandas >= 1.0.0\n- numpy >= 1.15.0\n- bokeh >= 3.0.0\n\n## Core Capabilities\n\n### 1. hvPlot Quick Plotting\n\nhvPlot provides an intuitive, pandas-like API for rapid visualization:\n\n```python\nimport hvplot.pandas\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndf = pd.DataFrame({\n    'date': pd.date_range('2024-01-01', periods=100),\n    'sales': np.cumsum(np.random.randn(100)) + 100,\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100)\n})\n\n# Simple line plot\ndf.hvplot.line(x='date', y='sales', title='Sales Over Time')\n\n# Grouped plot\ndf.hvplot.line(x='date', y='sales', by='region', subplots=True)\n\n# Scatter with size and color\ndf.hvplot.scatter(x='sales', y='date', c='region', size=50)\n```\n\n### 2. Common Plot Types\n\n```python\n# Bar plot\ndf.hvplot.bar(x='region', y='sales', rot=45)\n\n# Histogram\ndf['sales'].hvplot.hist(bins=30, title='Sales Distribution')\n\n# Box plot\ndf.hvplot.box(y='sales', by='region')\n\n# Area plot\ndf.hvplot.area(x='date', y='sales')\n\n# KDE (Kernel Density Estimation)\ndf['sales'].hvplot.kde()\n\n# Hexbin (for large datasets)\ndf.hvplot.hexbin(x='sales', y='date', gridsize=20)\n```\n\n### 3. Customization Options\n\n```python\n# Apply consistent styling\nplot = df.hvplot.line(\n    x='date',\n    y='sales',\n    title='Sales Trend',\n    xlabel='Date',\n    ylabel='Sales ($)',\n    color='#2E86DE',\n    line_width=2,\n    height=400,\n    width=700,\n    responsive=True,\n    legend='top_left'\n)\n\n# Color mapping\ndf.hvplot.scatter(\n    x='sales',\n    y='date',\n    c='sales',\n    cmap='viridis',\n    s=100\n)\n\n# Multiple series\ndf.hvplot.line(\n    x='date',\n    y=['sales'],\n    title='Performance Metrics'\n)\n```\n\n### 4. Interactive Features\n\n```python\n# Hover information\ndf.hvplot.scatter(\n    x='sales',\n    y='date',\n    hover_cols=['region'],\n    tools=['hover', 'pan', 'wheel_zoom']\n)\n\n# Selection and linked views\nimport holoviews as hv\nscatter = df.hvplot.scatter(x='sales', y='date')\nscatter.opts(tools=['box_select'])\n\n# Responsive sizing\nplot = df.hvplot.line(\n    x='date',\n    y='sales',\n    responsive=True,\n    height=400\n)\n```\n\n### 5. Geographic Plotting with hvPlot\n\n```python\nimport geopandas as gpd\n\n# Quick geographic plot\ngdf = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\ngdf.hvplot(\n    c='pop_est',\n    cmap='viridis',\n    geo=True,\n    frame_width=600\n)\n\n# City points on map\ncities = gpd.GeoDataFrame({\n    'name': ['City A', 'City B'],\n    'geometry': [Point(0, 0), Point(1, 1)],\n    'population': [1000000, 500000]\n})\ncities.hvplot(\n    geo=True,\n    c='population',\n    size='population',\n    cmap='plasma'\n)\n```\n\n## HoloViews Fundamentals\n\n### 1. Basic Element Types\n\n```python\nimport holoviews as hv\nfrom holoviews import opts\n\n# Curve\ncurve = hv.Curve(df, 'date', 'sales')\n\n# Scatter\nscatter = hv.Scatter(df, 'sales', 'date')\n\n# Histogram\nhist = hv.Histogram(df['sales'].values)\n\n# Image (heatmap)\nimage = hv.Image(data)\n\n# Bars\nbars = hv.Bars(df, 'region', 'sales')\n\n# Text annotations\ntext = hv.Text(0.5, 0.5, 'Hello HoloViews')\n```\n\n### 2. Styling and Options\n\n```python\n# Using .opts() method\nplot = hv.Curve(df, 'date', 'sales').opts(\n    title='Sales Trend',\n    xlabel='Date',\n    ylabel='Sales',\n    color='#2E86DE',\n    line_width=2,\n    height=400,\n    width=700\n)\n\n# Using opts object\nopts_obj = opts.Curve(\n    title='Sales',\n    color='navy',\n    line_width=2\n)\nplot = hv.Curve(df, 'date', 'sales').opts(opts_obj)\n```\n\n### 3. Composing Visualizations\n\n```python\n# Overlaying multiple plots\noverlay = hv.Curve(df, 'date', 'sales') * hv.Scatter(df_subset, 'date', 'sales')\n\n# Side-by-side layouts\nlayout = hv.Curve(df1, 'date', 'sales') + hv.Scatter(df2, 'date', 'value')\n\n# Grid layouts\ngrid = (\n    (hv.Curve(data1) + hv.Scatter(data2)) /\n    (hv.Histogram(data3) + hv.Image(data4))\n)\n\n# Faceted views\nfaceted = hv.Curve(df, 'date', 'sales').facet('region')\n```\n\n### 4. Interactive Selection and Linking\n\n```python\n# Brush selection\ncurve_selectable = hv.Curve(df, 'date', 'sales').opts(\n    tools=['box_select'],\n    selection_fill_color='red',\n    nonselection_fill_alpha=0.2\n)\n\n# Dynamic linking with streams\nfrom holoviews import streams\n\n# Hover information\nhover = streams.Tap(source=scatter, transient=True)\n\n@hv.transform\ndef get_info(data):\n    if data.empty:\n        return hv.Text(0, 0, 'Hover to select')\n    return hv.Text(0, 0, f\"Point: {data.iloc[0].values}\")\n```\n\n## Best Practices\n\n### 1. Data Preparation\n- Always check data types before plotting\n- Handle missing values explicitly\n- Normalize columns for better visualization\n- Use appropriate data ranges\n\n### 2. Visual Design\n- Choose colors for accessibility (colorblind-friendly palettes)\n- Use title and axis labels\n- Include legends for multiple series\n- Maintain consistent styling across related plots\n\n### 3. Performance\n- Use datashader for datasets with >100k points\n- Downsample or aggregate before plotting\n- Use responsive=True for web dashboards\n- Cache expensive plot computations\n\n### 4. Code Organization\n```python\n# Create a plotting utility module\nclass PlotBuilder:\n    COLORS = {'primary': '#2E86DE', 'secondary': '#A23B72'}\n    DEFAULTS = {'height': 400, 'width': 700, 'responsive': True}\n\n    @staticmethod\n    def style_plot(plot, **kwargs):\n        return plot.opts(**{**PlotBuilder.DEFAULTS, **kwargs})\n\n# Usage\nstyled = PlotBuilder.style_plot(df.hvplot.line(x='date', y='sales'))\n```\n\n## Common Patterns\n\n### Pattern 1: Dashboard with Multiple Plots\n```python\ndef create_sales_dashboard(df):\n    return hv.Column(\n        df.hvplot.line(x='date', y='sales', title='Trend'),\n        df.hvplot.bar(x='region', y='sales', title='By Region'),\n        df['sales'].hvplot.hist(bins=20, title='Distribution')\n    )\n```\n\n### Pattern 2: Conditional Visualization\n```python\ndef plot_data(df, plot_type='line'):\n    if plot_type == 'line':\n        return df.hvplot.line(x='date', y='sales')\n    elif plot_type == 'scatter':\n        return df.hvplot.scatter(x='date', y='sales')\n    else:\n        return df.hvplot.bar(x='region', y='sales')\n```\n\n### Pattern 3: Multi-Series Plot with Legend\n```python\ndef plot_multiple_metrics(df, metrics):\n    plots = [df.hvplot.line(x='date', y=m, label=m) for m in metrics]\n    return hv.Overlay(plots)\n```\n\n## Integration with Other HoloViz Tools\n\n- **Panel**: Embed plots in dashboards\n- **HoloViews**: Advanced composition and interactivity\n- **Datashader**: Large dataset visualization\n- **Param**: Dynamic plot updates based on parameters\n\n## Common Use Cases\n\n1. **Time Series Analysis**: Trends, anomalies, forecasting\n2. **Comparative Analysis**: Category comparisons, rankings\n3. **Distribution Analysis**: Histograms, KDEs, box plots\n4. **Correlation Analysis**: Scatter plots, hexbins\n5. **Geographic Analysis**: Maps, regional data\n6. **Statistical Summaries**: Summary statistics with plots\n\n## Troubleshooting\n\n### Issue: Plot Won't Display\n- Ensure `hvplot.pandas` or `hvplot.xarray` is imported\n- Check data is not empty\n- Verify x and y columns exist in dataframe\n\n### Issue: Poor Performance with Large Data\n- Use datashader for >100k points\n- Implement aggregation or sampling\n- Use hexbin or rasterization\n\n### Issue: Unclear or Overlapping Labels\n- Rotate x-axis labels with `rot=45`\n- Use subplots with `by='column'`\n- Adjust figure size with height/width\n\n## Resources\n\n- [hvPlot Documentation](https://hvplot.holoviz.org)\n- [HoloViews Documentation](https://holoviews.org)\n- [HoloViews Gallery](https://holoviews.org/reference/index.html)\n- [Colorcet for Color Palettes](https://colorcet.holoviz.org)"
              }
            ]
          }
        ]
      }
    }
  ]
}