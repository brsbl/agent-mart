{
  "owner": {
    "id": "withzombies",
    "display_name": "Ryan Stortz",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/4872355?u=de0f48dbc429b586038fa0c3bf45086b93af85b0&v=4",
    "url": "https://github.com/withzombies",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 20,
      "total_stars": 17,
      "total_forks": 1
    }
  },
  "repos": [
    {
      "full_name": "withzombies/hyperpowers",
      "url": "https://github.com/withzombies/hyperpowers",
      "description": "Claude Code superpowers with beads task tracking and refinement",
      "homepage": "",
      "signals": {
        "stars": 17,
        "forks": 1,
        "pushed_at": "2025-12-26T18:52:52Z",
        "created_at": "2025-10-27T19:20:08Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".beads",
          "type": "tree",
          "size": null
        },
        {
          "path": ".beads/.gitignore",
          "type": "blob",
          "size": 207
        },
        {
          "path": ".beads/issues.jsonl",
          "type": "blob",
          "size": 81310
        },
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 518
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 489
        },
        {
          "path": "BUG-brainstorming-skips-questions.md",
          "type": "blob",
          "size": 3540
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 12982
        },
        {
          "path": "HOOKS.md",
          "type": "blob",
          "size": 15985
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 7368
        },
        {
          "path": "REWRITE_COMPLETE.md",
          "type": "blob",
          "size": 17367
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/code-reviewer.md",
          "type": "blob",
          "size": 3942
        },
        {
          "path": "agents/codebase-investigator.md",
          "type": "blob",
          "size": 4952
        },
        {
          "path": "agents/internet-researcher.md",
          "type": "blob",
          "size": 4866
        },
        {
          "path": "agents/test-effectiveness-analyst.md",
          "type": "blob",
          "size": 15787
        },
        {
          "path": "agents/test-runner.md",
          "type": "blob",
          "size": 10855
        },
        {
          "path": "blog-with-ideas.txt",
          "type": "blob",
          "size": 34845
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/analyze-tests.md",
          "type": "blob",
          "size": 184
        },
        {
          "path": "commands/brainstorm.md",
          "type": "blob",
          "size": 133
        },
        {
          "path": "commands/execute-plan.md",
          "type": "blob",
          "size": 541
        },
        {
          "path": "commands/review-implementation.md",
          "type": "blob",
          "size": 135
        },
        {
          "path": "commands/write-plan.md",
          "type": "blob",
          "size": 139
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/REGEX_TESTING.md",
          "type": "blob",
          "size": 5758
        },
        {
          "path": "hooks/block-beads-direct-read.py",
          "type": "blob",
          "size": 1519
        },
        {
          "path": "hooks/context",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/context/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "hooks/context/edit-log.txt",
          "type": "blob",
          "size": 56
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 1940
        },
        {
          "path": "hooks/post-tool-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/post-tool-use/01-track-edits.sh",
          "type": "blob",
          "size": 3052
        },
        {
          "path": "hooks/post-tool-use/02-block-bd-truncation.py",
          "type": "blob",
          "size": 3183
        },
        {
          "path": "hooks/post-tool-use/03-block-pre-commit-bash.py",
          "type": "blob",
          "size": 3805
        },
        {
          "path": "hooks/post-tool-use/04-block-pre-existing-checks.py",
          "type": "blob",
          "size": 3727
        },
        {
          "path": "hooks/post-tool-use/test-hook.sh",
          "type": "blob",
          "size": 3177
        },
        {
          "path": "hooks/pre-tool-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/pre-tool-use/01-block-pre-commit-edits.py",
          "type": "blob",
          "size": 2783
        },
        {
          "path": "hooks/session-start.sh",
          "type": "blob",
          "size": 1557
        },
        {
          "path": "hooks/skill-rules.json",
          "type": "blob",
          "size": 10025
        },
        {
          "path": "hooks/stop",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/stop/10-gentle-reminders.sh",
          "type": "blob",
          "size": 3548
        },
        {
          "path": "hooks/stop/test-reminders.sh",
          "type": "blob",
          "size": 2489
        },
        {
          "path": "hooks/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/test/integration-test.sh",
          "type": "blob",
          "size": 7023
        },
        {
          "path": "hooks/user-prompt-submit",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/user-prompt-submit/10-skill-activator.js",
          "type": "blob",
          "size": 7694
        },
        {
          "path": "hooks/user-prompt-submit/test-hook.sh",
          "type": "blob",
          "size": 1853
        },
        {
          "path": "hooks/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/utils/context-query.sh",
          "type": "blob",
          "size": 1170
        },
        {
          "path": "hooks/utils/format-output.sh",
          "type": "blob",
          "size": 2450
        },
        {
          "path": "hooks/utils/skill-matcher.sh",
          "type": "blob",
          "size": 3452
        },
        {
          "path": "hooks/utils/test-performance.sh",
          "type": "blob",
          "size": 1556
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/analyzing-test-effectiveness",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/analyzing-test-effectiveness/SKILL.md",
          "type": "blob",
          "size": 37409
        },
        {
          "path": "skills/brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/brainstorming/SKILL.md",
          "type": "blob",
          "size": 21240
        },
        {
          "path": "skills/building-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/building-hooks/SKILL.md",
          "type": "blob",
          "size": 15467
        },
        {
          "path": "skills/building-hooks/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/building-hooks/resources/hook-examples.md",
          "type": "blob",
          "size": 14007
        },
        {
          "path": "skills/building-hooks/resources/hook-patterns.md",
          "type": "blob",
          "size": 11532
        },
        {
          "path": "skills/building-hooks/resources/testing-hooks.md",
          "type": "blob",
          "size": 13673
        },
        {
          "path": "skills/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/commands/brainstorm.md",
          "type": "blob",
          "size": 42
        },
        {
          "path": "skills/commands/execute-plan.md",
          "type": "blob",
          "size": 32
        },
        {
          "path": "skills/commands/write-plan.md",
          "type": "blob",
          "size": 42
        },
        {
          "path": "skills/common-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/common-patterns/bd-commands.md",
          "type": "blob",
          "size": 2904
        },
        {
          "path": "skills/common-patterns/common-anti-patterns.md",
          "type": "blob",
          "size": 2800
        },
        {
          "path": "skills/common-patterns/common-rationalizations.md",
          "type": "blob",
          "size": 4854
        },
        {
          "path": "skills/debugging-with-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debugging-with-tools/SKILL.md",
          "type": "blob",
          "size": 13314
        },
        {
          "path": "skills/debugging-with-tools/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debugging-with-tools/resources/debugger-reference.md",
          "type": "blob",
          "size": 2321
        },
        {
          "path": "skills/debugging-with-tools/resources/debugging-session-example.md",
          "type": "blob",
          "size": 2049
        },
        {
          "path": "skills/dispatching-parallel-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dispatching-parallel-agents/SKILL.md",
          "type": "blob",
          "size": 21639
        },
        {
          "path": "skills/executing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/executing-plans/SKILL.md",
          "type": "blob",
          "size": 17280
        },
        {
          "path": "skills/finishing-a-development-branch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/finishing-a-development-branch/SKILL.md",
          "type": "blob",
          "size": 11234
        },
        {
          "path": "skills/fixing-bugs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fixing-bugs/SKILL.md",
          "type": "blob",
          "size": 13081
        },
        {
          "path": "skills/managing-bd-tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/managing-bd-tasks/SKILL.md",
          "type": "blob",
          "size": 17246
        },
        {
          "path": "skills/managing-bd-tasks/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/managing-bd-tasks/resources/metrics-guide.md",
          "type": "blob",
          "size": 4786
        },
        {
          "path": "skills/managing-bd-tasks/resources/task-naming-guide.md",
          "type": "blob",
          "size": 7718
        },
        {
          "path": "skills/refactoring-safely",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/refactoring-safely/SKILL.md",
          "type": "blob",
          "size": 14391
        },
        {
          "path": "skills/refactoring-safely/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/refactoring-safely/resources/example-session.md",
          "type": "blob",
          "size": 2067
        },
        {
          "path": "skills/refactoring-safely/resources/refactoring-patterns.md",
          "type": "blob",
          "size": 2176
        },
        {
          "path": "skills/review-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/review-implementation/SKILL.md",
          "type": "blob",
          "size": 25615
        },
        {
          "path": "skills/root-cause-tracing",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/root-cause-tracing/SKILL.md",
          "type": "blob",
          "size": 14938
        },
        {
          "path": "skills/skills-auto-activation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/skills-auto-activation/SKILL.md",
          "type": "blob",
          "size": 11698
        },
        {
          "path": "skills/skills-auto-activation/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/skills-auto-activation/resources/hook-implementation.md",
          "type": "blob",
          "size": 17474
        },
        {
          "path": "skills/skills-auto-activation/resources/skill-rules-examples.md",
          "type": "blob",
          "size": 9540
        },
        {
          "path": "skills/skills-auto-activation/resources/troubleshooting.md",
          "type": "blob",
          "size": 11237
        },
        {
          "path": "skills/sre-task-refinement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sre-task-refinement/SKILL.md",
          "type": "blob",
          "size": 30469
        },
        {
          "path": "skills/test-driven-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/test-driven-development/SKILL.md",
          "type": "blob",
          "size": 9624
        },
        {
          "path": "skills/test-driven-development/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/test-driven-development/resources/example-workflows.md",
          "type": "blob",
          "size": 7042
        },
        {
          "path": "skills/test-driven-development/resources/language-examples.md",
          "type": "blob",
          "size": 4849
        },
        {
          "path": "skills/testing-anti-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/testing-anti-patterns/SKILL.md",
          "type": "blob",
          "size": 16043
        },
        {
          "path": "skills/using-hyper",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/using-hyper/SKILL.md",
          "type": "blob",
          "size": 12818
        },
        {
          "path": "skills/verification-before-completion",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/verification-before-completion/SKILL.md",
          "type": "blob",
          "size": 8976
        },
        {
          "path": "skills/writing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-plans/SKILL.md",
          "type": "blob",
          "size": 13521
        },
        {
          "path": "skills/writing-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-skills/SKILL.md",
          "type": "blob",
          "size": 17892
        },
        {
          "path": "skills/writing-skills/anthropic-best-practices.md",
          "type": "blob",
          "size": 45798
        },
        {
          "path": "skills/writing-skills/graphviz-conventions.dot",
          "type": "blob",
          "size": 5970
        },
        {
          "path": "skills/writing-skills/persuasion-principles.md",
          "type": "blob",
          "size": 5908
        },
        {
          "path": "skills/writing-skills/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-skills/resources/testing-methodology.md",
          "type": "blob",
          "size": 5590
        }
      ],
      "marketplace": {
        "name": "withzombies-hyper",
        "version": null,
        "description": "Development marketplace for Hyperpowers core skills library",
        "owner_info": {
          "name": "Ryan",
          "email": "ryan@withzombies.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "withzombies-hyper",
            "description": "Ryan's riff on obra/superpowers: strong guidance for Claude Code as a software development assistant",
            "source": "./",
            "category": null,
            "version": "1.6.1",
            "author": {
              "name": "Ryan",
              "email": "ryan@withzombies.com"
            },
            "install_commands": [
              "/plugin marketplace add withzombies/hyperpowers",
              "/plugin install withzombies-hyper@withzombies-hyper"
            ],
            "signals": {
              "stars": 17,
              "forks": 1,
              "pushed_at": "2025-12-26T18:52:52Z",
              "created_at": "2025-10-27T19:20:08Z",
              "license": null
            },
            "commands": [
              {
                "name": "/brainstorm",
                "description": null,
                "path": "skills/commands/brainstorm.md",
                "frontmatter": null,
                "content": "Use your hyperpowers:brainstorming skill.\n"
              },
              {
                "name": "/execute-plan",
                "description": null,
                "path": "skills/commands/execute-plan.md",
                "frontmatter": null,
                "content": "Use your Executing-Plans skill.\n"
              },
              {
                "name": "/write-plan",
                "description": null,
                "path": "skills/commands/write-plan.md",
                "frontmatter": null,
                "content": "Use your hyperpowers:writing-plans skill.\n"
              }
            ],
            "skills": [
              {
                "name": "analyzing-test-effectiveness",
                "description": "Use to audit test quality with Google Fellow SRE scrutiny - identifies tautological tests, coverage gaming, weak assertions, missing corner cases. Creates bd epic with tasks for improvements, then runs SRE task refinement on each.",
                "path": "skills/analyzing-test-effectiveness/SKILL.md",
                "frontmatter": {
                  "name": "analyzing-test-effectiveness",
                  "description": "Use to audit test quality with Google Fellow SRE scrutiny - identifies tautological tests, coverage gaming, weak assertions, missing corner cases. Creates bd epic with tasks for improvements, then runs SRE task refinement on each."
                },
                "content": "<skill_overview>\nAudit test suites for real effectiveness, not vanity metrics. Identify tests that provide false confidence (tautological, mock-testing, line hitters) and missing corner cases. Create bd epic with tracked tasks for improvements. Run SRE task refinement on each task before execution.\n\n**CRITICAL MINDSET: Assume tests were written by junior engineers optimizing for coverage metrics.** Default to skeptical—a test is RED or YELLOW until proven GREEN. You MUST read production code before categorizing tests. GREEN is the exception, not the rule.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow the 5-phase analysis process exactly. Categorization criteria (RED/YELLOW/GREEN) are rigid. Corner case discovery adapts to the specific codebase. Output format is flexible but must include all sections.\n</rigidity_level>\n\n<quick_reference>\n| Phase | Action | Output |\n|-------|--------|--------|\n| 1. Inventory | List all test files and functions | Test catalog |\n| 2. Read Production Code | Read the actual code each test claims to test | Context for analysis |\n| 3. Trace Call Paths | Verify tests exercise production, not mocks/utilities | Call path verification |\n| 4. Categorize (Skeptical) | Apply RED/YELLOW/GREEN - default to harsher rating | Categorized tests |\n| 5. Self-Review | Challenge every GREEN - would a senior SRE agree? | Validated categories |\n| 6. Corner Cases | Identify missing edge cases per module | Gap analysis |\n| 7. Prioritize | Rank by business criticality | Priority matrix |\n| 8. bd Issues | Create epic + tasks, run SRE refinement | Tracked improvement plan |\n\n**MANDATORY: Read production code BEFORE categorizing tests. You cannot assess a test without understanding what it claims to test.**\n\n**Core Questions for Each Test:**\n1. What bug would this catch? (If you can't name one → RED)\n2. Does it exercise PRODUCTION code or a mock/test utility? (Mock → RED or YELLOW)\n3. Could code break while test passes? (If yes → YELLOW or RED)\n4. Meaningful assertion on PRODUCTION output? (`!= nil` or testing fixtures → weak)\n\n**bd Integration (MANDATORY):**\n- Create bd epic for test quality improvement\n- Create bd tasks for: remove RED, strengthen YELLOW, add corner cases\n- Run hyperpowers:sre-task-refinement on all tasks\n- Link tasks to epic with dependencies\n\n**Mutation Testing Validation:**\n- Java: Pitest (`mvn org.pitest:pitest-maven:mutationCoverage`)\n- JS/TS: Stryker (`npx stryker run`)\n- Python: mutmut (`mutmut run`)\n</quick_reference>\n\n<when_to_use>\n**Use this skill when:**\n- Production bugs appear despite high test coverage\n- Suspecting coverage gaming or tautological tests\n- Before major refactoring (ensure tests catch regressions)\n- Onboarding to unfamiliar codebase (assess test quality)\n- After hyperpowers:review-implementation flags test quality issues\n- Planning test improvement initiatives\n\n**Don't use when:**\n- Writing new tests (use hyperpowers:test-driven-development)\n- Debugging test failures (use hyperpowers:debugging-with-tools)\n- Just need to run tests (use hyperpowers:test-runner agent)\n</when_to_use>\n\n<the_process>\n## Announcement\n\n**Announce:** \"I'm using hyperpowers:analyzing-test-effectiveness to audit test quality with Google Fellow SRE-level scrutiny.\"\n\n---\n\n## Phase 1: Test Inventory\n\n**Goal:** Create complete catalog of tests to analyze.\n\n```bash\n# Find all test files (adapt pattern to language)\nfd -e test.ts -e spec.ts -e _test.go -e Test.java -e test.py .\n\n# Or use grep to find test functions\nrg \"func Test|it\\(|test\\(|def test_|@Test\" --type-add 'test:*test*' -t test\n\n# Count tests per module\nfor dir in src/*/; do\n  count=$(rg -c \"func Test|it\\(\" \"$dir\" 2>/dev/null | wc -l)\n  echo \"$dir: $count tests\"\ndone\n```\n\n**Create inventory TodoWrite:**\n```\n- Analyze tests in src/auth/\n- Analyze tests in src/api/\n- Analyze tests in src/parser/\n[... one per module]\n```\n\n---\n\n## Phase 2: Read Production Code First\n\n**MANDATORY: Before categorizing ANY test, you MUST:**\n\n1. **Read the production code** the test claims to exercise\n2. **Understand what the production code actually does**\n3. **Trace the test's call path** to verify it reaches production code\n\n**Why this matters:** Junior engineers commonly:\n- Create test utilities and test THOSE instead of production code\n- Set up mocks that determine the test outcome (mock-testing-mock)\n- Write assertions on values defined IN THE TEST, not from production\n- Copy patterns from examples without understanding the actual code\n\n**If you haven't read production code, you WILL miscategorize tests as GREEN when they're YELLOW or RED.**\n\n---\n\n## Phase 3: Categorize Each Test (Skeptical Default)\n\n**Assume every test is RED or YELLOW until you have concrete evidence it's GREEN.**\n\nFor each test, apply these criteria:\n\n### RED FLAGS - Must Remove or Replace\n\n**2.1 Tautological Tests** (pass by definition)\n\n```typescript\n// ❌ RED: Verifies non-optional return is not nil\ntest('builder returns value', () => {\n  const result = new Builder().build();\n  expect(result).not.toBeNull(); // Always passes - return type guarantees this\n});\n\n// ❌ RED: Verifies enum has cases (compiler checks this)\ntest('status enum has values', () => {\n  expect(Object.values(Status).length).toBeGreaterThan(0);\n});\n\n// ❌ RED: Duplicates implementation\ntest('add returns sum', () => {\n  expect(add(2, 3)).toBe(2 + 3); // Tautology: testing 2+3 == 2+3\n});\n```\n\n**Detection patterns:**\n```bash\n# Find != nil / != null on non-optional types\nrg \"expect\\(.*\\)\\.not\\.toBeNull|assertNotNull|!= nil\" tests/\n\n# Find enum existence checks\nrg \"Object\\.values.*length|cases\\.count\" tests/\n\n# Find tests with no meaningful assertions\nrg -l \"expect\\(\" tests/ | xargs -I {} sh -c 'grep -c \"expect\" {} | grep -q \"^1$\" && echo {}'\n```\n\n**2.2 Mock-Testing Tests** (test the mock, not production)\n\n```typescript\n// ❌ RED: Only verifies mock was called, not actual behavior\ntest('service fetches data', () => {\n  const mockApi = { fetch: jest.fn().mockResolvedValue({ data: [] }) };\n  const service = new Service(mockApi);\n  service.getData();\n  expect(mockApi.fetch).toHaveBeenCalled(); // Tests mock, not service logic\n});\n\n// ❌ RED: Mock determines test outcome\ntest('processor handles data', () => {\n  const mockParser = { parse: jest.fn().mockReturnValue({ valid: true }) };\n  const result = processor.process(mockParser);\n  expect(result.valid).toBe(true); // Just returns what mock returns\n});\n```\n\n**Detection patterns:**\n```bash\n# Find tests that only verify mock calls\nrg \"toHaveBeenCalled|verify\\(mock|\\.called\" tests/\n\n# Find heavy mock setup\nrg -c \"mock|Mock|jest\\.fn|stub\" tests/ | sort -t: -k2 -nr | head -20\n```\n\n**2.3 Line Hitters** (execute without asserting)\n\n```typescript\n// ❌ RED: Calls function, doesn't verify outcome\ntest('processor runs', () => {\n  const processor = new Processor();\n  processor.run(); // No assertion - just verifies no crash\n});\n\n// ❌ RED: Assertion is trivial\ntest('config loads', () => {\n  const config = loadConfig();\n  expect(config).toBeDefined(); // Too weak - doesn't verify correct values\n});\n```\n\n**Detection patterns:**\n```bash\n# Find tests with 0-1 assertions\nrg -l \"test\\(|it\\(\" tests/ | while read f; do\n  assertions=$(rg -c \"expect|assert\" \"$f\" 2>/dev/null || echo 0)\n  tests=$(rg -c \"test\\(|it\\(\" \"$f\" 2>/dev/null || echo 1)\n  ratio=$((assertions / tests))\n  [ \"$ratio\" -lt 2 ] && echo \"$f: low assertion ratio ($assertions assertions, $tests tests)\"\ndone\n```\n\n**2.4 Evergreen/Liar Tests** (always pass)\n\n```typescript\n// ❌ RED: Catches and ignores exceptions\ntest('parser handles input', () => {\n  try {\n    parser.parse(input);\n    expect(true).toBe(true); // Always passes\n  } catch (e) {\n    // Swallowed - test passes even on exception\n  }\n});\n\n// ❌ RED: Test setup bypasses code under test\ntest('validator validates', () => {\n  const validator = new Validator({ skipValidation: true }); // Oops\n  expect(validator.validate(badInput)).toBe(true);\n});\n```\n\n### YELLOW FLAGS - Must Strengthen\n\n**2.5 Happy Path Only**\n\n```typescript\n// ⚠️ YELLOW: Only tests valid input\ntest('parse valid json', () => {\n  const result = parse('{\"name\": \"test\"}');\n  expect(result.name).toBe('test');\n});\n// Missing: empty string, malformed JSON, deeply nested, unicode, huge payload\n```\n\n**2.6 Weak Assertions**\n\n```typescript\n// ⚠️ YELLOW: Assertion too weak\ntest('fetch returns data', () => {\n  const result = await fetch('/api/users');\n  expect(result).not.toBeNull(); // Should verify actual content\n  expect(result.length).toBeGreaterThan(0); // Should verify exact count or specific items\n});\n```\n\n**2.7 Partial Coverage**\n\n```typescript\n// ⚠️ YELLOW: Tests success, not failure\ntest('create user succeeds', () => {\n  const user = createUser({ name: 'test', email: 'test@example.com' });\n  expect(user.id).toBeDefined();\n});\n// Missing: duplicate email, invalid email, missing fields, database error\n```\n\n### GREEN FLAGS - Exceptional Quality Required\n\n**GREEN is the EXCEPTION, not the rule.** A test is GREEN only if ALL of the following are true:\n\n1. **Exercises actual PRODUCTION code** - Not a mock, not a test utility, not a copy of logic\n2. **Has precise assertions** - Exact values, not `!= nil` or `> 0`\n3. **Would fail if production breaks** - You can name the specific bug it catches\n4. **Tests behavior, not implementation** - Won't break on valid refactoring\n\n**Before marking ANY test GREEN, you MUST state:**\n- \"This test exercises [specific production code path]\"\n- \"It would catch [specific bug] because [reason]\"\n- \"The assertion verifies [exact production behavior], not a test fixture\"\n\n**If you cannot fill in those blanks, the test is YELLOW at best.**\n\n**3.1 Behavior Verification (Must exercise PRODUCTION code)**\n\n```typescript\n// ✅ GREEN: Verifies specific behavior with exact values FROM PRODUCTION\ntest('calculateTotal applies discount correctly', () => {\n  const cart = new Cart([{ price: 100, quantity: 2 }]); // Real Cart class\n  cart.applyDiscount('SAVE20'); // Real discount logic\n  expect(cart.total).toBe(160); // 200 - 20% = 160\n});\n// GREEN because: Exercises Cart.applyDiscount production code\n// Would catch: Discount calculation bugs, rounding errors\n// Assertion: Verifies exact computed value from production\n```\n\n**3.2 Edge Case Coverage (Must test PRODUCTION paths)**\n\n```typescript\n// ✅ GREEN: Tests boundary conditions IN PRODUCTION CODE\ntest('username rejects empty string', () => {\n  expect(() => new User({ username: '' })).toThrow(ValidationError);\n});\n// GREEN because: Exercises User constructor validation (production)\n// Would catch: Missing empty string validation\n// Assertion: Exact error type from production code\n\ntest('username handles unicode', () => {\n  const user = new User({ username: '日本語ユーザー' });\n  expect(user.username).toBe('日本語ユーザー');\n});\n// GREEN because: Exercises User constructor and storage (production)\n// Would catch: Unicode corruption, encoding bugs\n// Assertion: Exact value preserved through production code\n```\n\n**3.3 Error Path Testing (Must verify PRODUCTION errors)**\n\n```typescript\n// ✅ GREEN: Verifies error handling IN PRODUCTION CODE\ntest('fetch returns specific error on 404', () => {\n  mockServer.get('/api/user/999').reply(404); // External mock OK\n  await expect(fetchUser(999)).rejects.toThrow(UserNotFoundError);\n});\n// GREEN because: Exercises fetchUser error handling (production)\n// Would catch: Wrong error type, swallowed errors\n// Assertion: Exact error type from production code\n```\n\n**CAUTION:** A test that uses mocks for EXTERNAL dependencies (APIs, databases) can still be GREEN if it exercises PRODUCTION logic. A test that mocks the code under test is RED.\n\n---\n\n## Phase 4: Mandatory Self-Review\n\n**Before finalizing ANY categorization, complete this checklist:**\n\n### For each GREEN test:\n- [ ] Did I read the PRODUCTION code this test exercises?\n- [ ] Does the test call PRODUCTION code or a test utility/mock?\n- [ ] Can I name the SPECIFIC BUG this test would catch?\n- [ ] If production code broke, would this test DEFINITELY fail?\n- [ ] Am I being too generous because the test \"looks reasonable\"?\n\n### For each YELLOW test:\n- [ ] Should this actually be RED? Is there ANY bug-catching value here?\n- [ ] Is the weakness fundamental (tests a mock) or fixable (weak assertion)?\n- [ ] If I changed this to RED, would I lose any bug-catching ability?\n\n### Self-Challenge Questions:\n- \"If a junior engineer showed me this test, would I accept it as GREEN?\"\n- \"Am I marking this GREEN because I want to be done, or because it's genuinely good?\"\n- \"Could I defend this GREEN classification to a Google SRE?\"\n\n**If you have ANY doubt about a GREEN, downgrade to YELLOW.**\n**If you have ANY doubt about a YELLOW, consider RED.**\n\n**Common mistakes that cause false GREENs:**\n- Assuming a well-named test tests what its name says (verify the code!)\n- Trusting test comments (comments lie, code doesn't)\n- Not tracing mock/utility usage to see what's actually exercised\n- Giving benefit of the doubt (junior engineers don't deserve it)\n\n---\n\n## Phase 4b: Line-by-Line Justification for RED/YELLOW\n\n**MANDATORY: For every RED or YELLOW classification, provide detailed justification.**\n\nThis forces you to verify your classification is correct by explaining exactly WHY the test is problematic.\n\n### Required Format for RED/YELLOW Tests:\n\n```markdown\n### [Test Name] - RED/YELLOW\n\n**Test code (file:lines):**\n- Line X: `code` - [what this line does]\n- Line Y: `code` - [what this line does]\n- Line Z: `assertion` - [what this asserts]\n\n**Production code it claims to test (file:lines):**\n- [Brief description of what production code does]\n\n**Why RED/YELLOW:**\n- [Specific reason with line references]\n- [What bug could slip through despite this test passing]\n```\n\n### Example RED Justification:\n\n```markdown\n### testAuthWorks - RED (Tautological)\n\n**Test code (auth_test.ts:45-52):**\n- Line 46: `const auth = new AuthService()` - Creates auth instance\n- Line 47: `const result = auth.login('user', 'pass')` - Calls login\n- Line 48: `expect(result).not.toBeNull()` - Asserts result exists\n\n**Production code (auth.ts:78-95):**\n- login() returns AuthResult object (never null by TypeScript types)\n\n**Why RED:**\n- Line 48 asserts `!= null` but TypeScript guarantees non-null return\n- If login returned {success: false, error: \"invalid\"}, test still passes\n- Bug example: Wrong password accepted → returns {success: true} → test passes\n```\n\n### Example YELLOW Justification:\n\n```markdown\n### testParseJson - YELLOW (Weak Assertion)\n\n**Test code (parser_test.ts:23-30):**\n- Line 24: `const input = '{\"name\": \"test\"}'` - Valid JSON input\n- Line 25: `const result = parse(input)` - Calls production parser\n- Line 26: `expect(result).toBeDefined()` - Asserts result exists\n- Line 27: `expect(result.name).toBe('test')` - Verifies one field\n\n**Production code (parser.ts:12-45):**\n- parse() handles JSON parsing with error handling and validation\n\n**Why YELLOW:**\n- Line 26-27 only test happy path with valid input\n- Missing: malformed JSON, empty string, deeply nested, unicode\n- Bug example: parse('') throws unhandled exception → not caught by test\n- Upgrade path: Add edge case inputs with specific error assertions\n```\n\n### Why This Matters:\n\nWriting the justification FORCES you to:\n1. Actually read the test code line by line\n2. Actually read the production code\n3. Articulate the specific gap\n4. Consider what bugs could slip through\n\n**If you cannot write this justification, you haven't done the analysis properly.**\n\n---\n\n## Phase 5: Corner Case Discovery\n\nFor each module, identify missing corner case tests:\n\n### Input Validation Corner Cases\n\n| Category | Examples | Tests to Add |\n|----------|----------|--------------|\n| Empty values | `\"\"`, `[]`, `{}`, `null` | test_empty_X_rejected/handled |\n| Boundary values | 0, -1, MAX_INT, MAX_LEN | test_boundary_X_handled |\n| Unicode | RTL, emoji, combining chars, null byte | test_unicode_X_preserved |\n| Injection | SQL: `'; DROP`, XSS: `<script>`, cmd: `; rm` | test_injection_X_escaped |\n| Malformed | truncated JSON, invalid UTF-8, wrong type | test_malformed_X_error |\n\n### State Corner Cases\n\n| Category | Examples | Tests to Add |\n|----------|----------|--------------|\n| Uninitialized | Use before init, double init | test_uninitialized_X_error |\n| Already closed | Use after close, double close | test_closed_X_error |\n| Concurrent | Parallel writes, read during write | test_concurrent_X_safe |\n| Re-entrant | Callback calls same method | test_reentrant_X_safe |\n\n### Integration Corner Cases\n\n| Category | Examples | Tests to Add |\n|----------|----------|--------------|\n| Network | timeout, connection refused, DNS fail | test_network_X_timeout |\n| Partial response | truncated, corrupted, slow | test_partial_response_handled |\n| Rate limiting | 429, quota exceeded | test_rate_limit_handled |\n| Service errors | 500, 503, malformed response | test_service_error_handled |\n\n### Resource Corner Cases\n\n| Category | Examples | Tests to Add |\n|----------|----------|--------------|\n| Exhaustion | OOM, disk full, max connections | test_resource_X_graceful |\n| Contention | file locked, resource busy | test_contention_X_handled |\n| Permissions | access denied, read-only | test_permission_X_error |\n\n**For each module, create corner case checklist:**\n\n```markdown\n### Module: src/auth/\n\n**Covered Corner Cases:**\n- [x] Empty password rejected\n- [x] SQL injection in username escaped\n\n**Missing Corner Cases (MUST ADD):**\n- [ ] Unicode username preserved after roundtrip\n- [ ] Concurrent login attempts don't corrupt session\n- [ ] Password with null byte handled\n- [ ] Very long password (10KB) rejected gracefully\n- [ ] Login rate limiting enforced\n\n**Priority:** HIGH (auth is business-critical)\n```\n\n---\n\n## Phase 6: Prioritize by Business Impact\n\n### Priority Matrix\n\n| Priority | Criteria | Action Timeline |\n|----------|----------|-----------------|\n| P0 - Critical | Auth, payments, data integrity | This sprint |\n| P1 - High | Core business logic, user-facing features | Next sprint |\n| P2 - Medium | Internal tools, admin features | Backlog |\n| P3 - Low | Utilities, non-critical paths | As time permits |\n\n**Rank modules:**\n```markdown\n1. P0: src/auth/ - 5 RED tests, 12 missing corner cases\n2. P0: src/payments/ - 2 RED tests, 8 missing corner cases\n3. P1: src/api/ - 8 RED tests, 15 missing corner cases\n4. P2: src/admin/ - 3 RED tests, 6 missing corner cases\n```\n\n---\n\n## Phase 7: Create bd Issues and Improvement Plan\n\n**CRITICAL:** All findings MUST be tracked in bd and go through SRE task refinement.\n\n### Step 5.1: Create bd Epic for Test Quality Improvement\n\n```bash\nbd create \"Test Quality Improvement: [Module/Project]\" \\\n  --type epic \\\n  --priority 1 \\\n  --design \"$(cat <<'EOF'\n## Goal\nImprove test effectiveness by removing tautological tests, strengthening weak tests, and adding missing corner case coverage.\n\n## Success Criteria\n- [ ] All RED tests removed or replaced with meaningful tests\n- [ ] All YELLOW tests strengthened with proper assertions\n- [ ] All P0 missing corner cases covered\n- [ ] Mutation score ≥80% for P0 modules\n\n## Scope\n[Summary of modules analyzed and findings]\n\n## Anti-patterns\n- ❌ Adding tests that only check `!= nil`\n- ❌ Adding tests that verify mock behavior\n- ❌ Adding happy-path-only tests\n- ❌ Leaving tautological tests \"for coverage\"\nEOF\n)\"\n```\n\n### Step 5.2: Create bd Tasks for Each Category\n\n**Task 1: Remove Tautological Tests (Immediate)**\n\n```bash\nbd create \"Remove tautological tests from [module]\" \\\n  --type task \\\n  --priority 0 \\\n  --design \"$(cat <<'EOF'\n## Goal\nRemove tests that provide false confidence by passing regardless of code correctness.\n\n## Tests to Remove\n[List each RED test with file:line]\n- tests/auth.test.ts:45 - testUserExists (tautological: verifies non-optional != nil)\n- tests/auth.test.ts:67 - testEnumHasCases (tautological: compiler checks this)\n\n## Success Criteria\n- [ ] All listed tests deleted\n- [ ] No new tautological tests introduced\n- [ ] Test suite still passes\n- [ ] Coverage may decrease (this is expected and good)\n\n## Anti-patterns\n- ❌ Keeping tests \"just in case\"\n- ❌ Replacing with equally meaningless tests\n- ❌ Adding coverage-only tests to compensate\nEOF\n)\"\n```\n\n**Task 2: Strengthen Weak Tests (This Sprint)**\n\n```bash\nbd create \"Strengthen weak assertions in [module]\" \\\n  --type task \\\n  --priority 1 \\\n  --design \"$(cat <<'EOF'\n## Goal\nReplace weak assertions with meaningful ones that catch real bugs.\n\n## Tests to Strengthen\n[List each YELLOW test with current vs recommended assertion]\n- tests/parser.test.ts:34 - testParse\n  - Current: `expect(result).not.toBeNull()`\n  - Strengthen: `expect(result).toEqual(expectedAST)`\n\n- tests/validator.test.ts:56 - testValidate\n  - Current: `expect(isValid).toBe(true)` (happy path only)\n  - Add edge cases: empty input, unicode, max length\n\n## Success Criteria\n- [ ] All weak assertions replaced with exact value checks\n- [ ] Edge cases added to happy-path-only tests\n- [ ] Each test documents what bug it catches\n\n## Anti-patterns\n- ❌ Replacing `!= nil` with `!= undefined` (still weak)\n- ❌ Adding edge cases without meaningful assertions\nEOF\n)\"\n```\n\n**Task 3: Add Missing Corner Cases (Per Module)**\n\n```bash\nbd create \"Add missing corner case tests for [module]\" \\\n  --type task \\\n  --priority 1 \\\n  --design \"$(cat <<'EOF'\n## Goal\nAdd tests for corner cases that could cause production bugs.\n\n## Corner Cases to Add\n[List each with the bug it prevents]\n- test_empty_password_rejected - prevents auth bypass\n- test_unicode_username_preserved - prevents encoding corruption\n- test_concurrent_login_safe - prevents session corruption\n\n## Implementation Checklist\n- [ ] Write failing test first (RED)\n- [ ] Verify test fails for the right reason\n- [ ] Test catches the specific bug listed\n- [ ] Test has meaningful assertion (not just `!= nil`)\n\n## Success Criteria\n- [ ] All corner case tests written and passing\n- [ ] Each test documents the bug it catches in test name/comment\n- [ ] No tautological tests added\n\n## Anti-patterns\n- ❌ Writing test that passes immediately (didn't test anything)\n- ❌ Testing mock behavior instead of production code\n- ❌ Happy path only (defeats the purpose)\nEOF\n)\"\n```\n\n### Step 5.3: Run SRE Task Refinement\n\n**MANDATORY:** After creating bd tasks, run SRE task refinement:\n\n```\nAnnounce: \"I'm using hyperpowers:sre-task-refinement to review these test improvement tasks.\"\n\nUse Skill tool: hyperpowers:sre-task-refinement\n```\n\nApply all 8 categories to each task, especially:\n- **Category 8 (Test Meaningfulness)**: Verify the proposed tests actually catch bugs\n- **Category 6 (Edge Cases)**: Ensure corner cases are comprehensive\n- **Category 3 (Success Criteria)**: Ensure criteria are measurable\n\n### Step 5.4: Link Tasks to Epic\n\n```bash\n# Link all tasks as children of epic\nbd dep add bd-2 bd-1 --type parent-child\nbd dep add bd-3 bd-1 --type parent-child\nbd dep add bd-4 bd-1 --type parent-child\n\n# Set dependencies (remove before strengthen before add)\nbd dep add bd-3 bd-2  # strengthen depends on remove\nbd dep add bd-4 bd-3  # add depends on strengthen\n```\n\n### Step 5.5: Validation Task\n\n```bash\nbd create \"Validate test improvements with mutation testing\" \\\n  --type task \\\n  --priority 1 \\\n  --design \"$(cat <<'EOF'\n## Goal\nVerify test improvements actually catch more bugs using mutation testing.\n\n## Validation Commands\n```bash\n# Java\nmvn org.pitest:pitest-maven:mutationCoverage\n\n# JavaScript/TypeScript\nnpx stryker run\n\n# Python\nmutmut run\n\n# .NET\ndotnet stryker\n```\n\n## Success Criteria\n- [ ] P0 modules: ≥80% mutation score\n- [ ] P1 modules: ≥70% mutation score\n- [ ] No surviving mutants in critical paths (auth, payments)\n\n## If Score Below Target\n- Identify surviving mutants\n- Create additional tasks to add tests that kill them\n- Re-run validation\nEOF\n)\"\n```\n\n---\n\n## Output Format\n\n```markdown\n# Test Effectiveness Analysis: [Project Name]\n\n## Executive Summary\n\n| Metric | Count | % |\n|--------|-------|---|\n| Total tests analyzed | N | 100% |\n| RED (remove/replace) | N | X% |\n| YELLOW (strengthen) | N | X% |\n| GREEN (keep) | N | X% |\n| Missing corner cases | N | - |\n\n**Overall Assessment:** [CRITICAL / NEEDS WORK / ACCEPTABLE / GOOD]\n\n## Detailed Findings\n\n### RED Tests (Must Remove/Replace)\n\n#### Tautological Tests\n| Test | File:Line | Problem | Action |\n|------|-----------|---------|--------|\n\n#### Mock-Testing Tests\n| Test | File:Line | Problem | Action |\n|------|-----------|---------|--------|\n\n#### Line Hitters\n| Test | File:Line | Problem | Action |\n|------|-----------|---------|--------|\n\n#### Evergreen Tests\n| Test | File:Line | Problem | Action |\n|------|-----------|---------|--------|\n\n### YELLOW Tests (Must Strengthen)\n\n#### Weak Assertions\n| Test | File:Line | Current | Recommended |\n|------|-----------|---------|-------------|\n\n#### Happy Path Only\n| Test | File:Line | Missing Edge Cases |\n|------|-----------|-------------------|\n\n### GREEN Tests (Exemplars)\n\n[List 3-5 tests that exemplify good testing practices for this codebase]\n\n## Missing Corner Cases by Module\n\n### [Module: name] - Priority: P0\n| Corner Case | Bug Risk | Recommended Test |\n|-------------|----------|------------------|\n\n[Repeat for each module]\n\n## bd Issues Created\n\n### Epic\n- **bd-N**: Test Quality Improvement: [Project Name]\n\n### Tasks\n| bd ID | Task | Priority | Status |\n|-------|------|----------|--------|\n| bd-N | Remove tautological tests from [module] | P0 | Created |\n| bd-N | Strengthen weak assertions in [module] | P1 | Created |\n| bd-N | Add missing corner case tests for [module] | P1 | Created |\n| bd-N | Validate with mutation testing | P1 | Created |\n\n### Dependency Tree\n```\nbd-1 (Epic: Test Quality Improvement)\n├── bd-2 (Remove tautological tests)\n├── bd-3 (Strengthen weak assertions) ← depends on bd-2\n├── bd-4 (Add corner case tests) ← depends on bd-3\n└── bd-5 (Validate with mutation testing) ← depends on bd-4\n```\n\n## SRE Task Refinement Status\n\n- [ ] All tasks reviewed with hyperpowers:sre-task-refinement\n- [ ] Category 8 (Test Meaningfulness) applied to each task\n- [ ] Success criteria are measurable\n- [ ] Anti-patterns specified\n\n## Next Steps\n\n1. Run `bd ready` to see tasks ready for implementation\n2. Implement tasks using hyperpowers:executing-plans\n3. Run validation task to verify improvements\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>High coverage but production bugs keep appearing</scenario>\n\n<code>\n# Test suite stats\nCoverage: 92%\nTests: 245 passing\n\n# Yet production issues:\n- Auth bypass via empty password\n- Data corruption on concurrent updates\n- Crash on unicode usernames\n</code>\n\n<why_it_fails>\n- Coverage measures execution, not assertion quality\n- Tests likely tautological or weak assertions\n- Corner cases (empty, concurrent, unicode) not tested\n- High coverage created false confidence\n</why_it_fails>\n\n<correction>\n**Run test effectiveness analysis:**\n\nPhase 1 - Inventory:\n```bash\nfd -e test.ts src/\n# Found: auth.test.ts, user.test.ts, data.test.ts\n```\n\nPhase 2 - Categorize:\n```markdown\n### auth.test.ts\n| Test | Category | Problem |\n|------|----------|---------|\n| testAuthWorks | RED | Only checks `!= null` |\n| testLoginFlow | YELLOW | Happy path only, no empty password |\n| testTokenExpiry | GREEN | Verifies exact error |\n\n### data.test.ts\n| Test | Category | Problem |\n|------|----------|---------|\n| testDataSaves | RED | No assertion, just calls save() |\n| testConcurrentWrites | MISSING | Not tested at all |\n```\n\nPhase 3 - Corner cases:\n```markdown\n### auth module (P0)\nMissing:\n- [ ] test_empty_password_rejected\n- [ ] test_unicode_username_preserved\n- [ ] test_concurrent_login_safe\n```\n\nPhase 5 - Plan:\n```markdown\n### Immediate\n- Remove testAuthWorks (tautological)\n- Remove testDataSaves (line hitter)\n\n### This Sprint\n- Add test_empty_password_rejected\n- Add test_concurrent_writes_safe\n- Strengthen testLoginFlow with edge cases\n```\n\n**Result:** Production bugs prevented by meaningful tests.\n</correction>\n</example>\n\n<example>\n<scenario>Mock-heavy test suite that breaks on every refactor</scenario>\n\n<code>\n# Every refactor breaks 50+ tests\n# But bugs slip through to production\n\ntest('service processes data', () => {\n  const mockDb = jest.fn().mockReturnValue({ data: [] });\n  const mockCache = jest.fn().mockReturnValue(null);\n  const mockLogger = jest.fn();\n  const mockValidator = jest.fn().mockReturnValue(true);\n\n  const service = new Service(mockDb, mockCache, mockLogger, mockValidator);\n  service.process({ id: 1 });\n\n  expect(mockDb).toHaveBeenCalled();\n  expect(mockValidator).toHaveBeenCalled();\n  // Tests mock wiring, not actual behavior\n});\n</code>\n\n<why_it_fails>\n- Tests verify mock setup, not production behavior\n- Changing implementation breaks tests without bugs\n- Real bugs (validation logic, data handling) not caught\n- \"Mocks mocking mocks\" anti-pattern\n</why_it_fails>\n\n<correction>\n**Categorize as RED - mock-testing:**\n\n```markdown\n### service.test.ts\n| Test | Category | Problem | Action |\n|------|----------|---------|--------|\n| testServiceProcesses | RED | Only verifies mocks called | Replace with integration test |\n| testServiceValidates | RED | Mock determines outcome | Test real validator |\n| testServiceCaches | RED | Tests mock cache | Use real cache with test data |\n```\n\n**Replacement strategy:**\n\n```typescript\n// ❌ Before: Tests mock wiring\ntest('service validates', () => {\n  const mockValidator = jest.fn().mockReturnValue(true);\n  const service = new Service(mockValidator);\n  expect(mockValidator).toHaveBeenCalled();\n});\n\n// ✅ After: Tests real behavior\ntest('service rejects invalid data', () => {\n  const service = new Service(new RealValidator());\n  const result = service.process({ id: -1 }); // Invalid ID\n  expect(result.error).toBe('INVALID_ID');\n});\n\ntest('service accepts valid data', () => {\n  const service = new Service(new RealValidator());\n  const result = service.process({ id: 1, name: 'test' });\n  expect(result.success).toBe(true);\n  expect(result.data.name).toBe('test');\n});\n```\n\n**Result:** Tests verify behavior, not implementation. Refactoring doesn't break tests. Real bugs caught.\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Assume junior engineer quality** → Tests are LOW QUALITY until proven otherwise\n2. **Read production code BEFORE categorizing** → You cannot assess without context\n3. **GREEN is the exception** → Most tests are RED or YELLOW; GREEN requires proof\n4. **Every test must answer: \"What bug does this catch?\"** → If no answer, it's RED\n5. **Tautological tests must be removed** → They provide false confidence\n6. **Mock-testing tests must be replaced** → Test production code, not mocks\n7. **Self-review before finalizing** → Challenge every GREEN classification\n8. **Mutation testing validates improvements** → Coverage alone is vanity metric\n9. **All findings tracked in bd** → Create epic + tasks for every issue found\n10. **SRE refinement on all tasks** → Run hyperpowers:sre-task-refinement before execution\n\n## Common Analysis Failures\n\n**You WILL be tempted to:**\n- Mark tests GREEN because they \"look reasonable\" → VERIFY call paths first\n- Trust test names and comments → CODE doesn't lie, comments DO\n- Give benefit of the doubt → Junior engineers don't deserve it\n- Rush categorization → Read production code FIRST\n- Mark YELLOW when it's actually RED → If mock determines outcome, it's RED\n\n**A false GREEN is worse than a false YELLOW.** When in doubt, be harsher.\n\n## Common Excuses\n\nAll of these mean: **STOP. The test is probably RED or YELLOW.**\n\n- \"It's just a smoke test\" (Smoke tests without assertions are useless)\n- \"Coverage requires it\" (Coverage gaming = false confidence)\n- \"It worked before\" (Past success doesn't mean it catches bugs)\n- \"Mocks make it faster\" (Fast but useless is still useless)\n- \"Edge cases are rare\" (Rare bugs in auth/payments are critical)\n- \"We'll add assertions later\" (Tests without assertions aren't tests)\n- \"It's testing the happy path\" (Happy path only = half a test)\n- \"The test looks reasonable\" (Junior engineers write plausible-looking garbage)\n- \"The test name says it tests X\" (Names lie, trace the actual code)\n- \"It exercises the function\" (Calling != testing; assertions matter)\n- \"I'll just fix these without bd\" (Untracked work = forgotten work)\n- \"SRE refinement is overkill for test fixes\" (Test tasks need same rigor as feature tasks)\n</critical_rules>\n\n<verification_checklist>\nBefore completing analysis:\n\n**Analysis Quality (MANDATORY):**\n- [ ] Read production code for EVERY test before categorizing\n- [ ] Traced call paths to verify tests exercise production, not mocks/utilities\n- [ ] Applied skeptical default (assumed RED/YELLOW, required proof for GREEN)\n- [ ] Completed self-review checklist for ALL GREEN tests\n- [ ] Each GREEN test has explicit justification (what production path, what bug it catches)\n- [ ] Each RED test has line-by-line justification with production code context\n- [ ] Each YELLOW test has line-by-line justification with upgrade path\n\n**Per module:**\n- [ ] All tests categorized (RED/YELLOW/GREEN)\n- [ ] RED tests have specific removal/replacement actions\n- [ ] YELLOW tests have specific strengthening actions\n- [ ] Corner cases identified (empty, unicode, concurrent, error)\n- [ ] Priority assigned (P0/P1/P2/P3)\n\n**Overall:**\n- [ ] Executive summary with counts and percentages\n- [ ] GREEN count is MINORITY (if >40% GREEN, re-review with more skepticism)\n- [ ] Detailed findings table for each category\n- [ ] Missing corner cases documented per module\n\n**bd Integration (MANDATORY):**\n- [ ] Created bd epic for test quality improvement\n- [ ] Created bd tasks for each category (remove, strengthen, add)\n- [ ] Linked tasks to epic with parent-child relationships\n- [ ] Set task dependencies (remove → strengthen → add → validate)\n- [ ] Ran hyperpowers:sre-task-refinement on ALL tasks\n- [ ] Created validation task with mutation testing\n\n**SRE Refinement Verification:**\n- [ ] Category 8 (Test Meaningfulness) applied to each task\n- [ ] Success criteria are measurable (not \"tests work\")\n- [ ] Anti-patterns specified for each task\n- [ ] No placeholder text in task designs\n\n**Validation:**\n- [ ] Would removing RED tests lose any bug-catching ability? (No = correct)\n- [ ] Would strengthening YELLOW tests catch more bugs? (Yes = correct)\n- [ ] Would adding corner cases catch known production bugs? (Yes = correct)\n</verification_checklist>\n\n<integration>\n**This skill is called by:**\n- hyperpowers:review-implementation (when test quality issues flagged)\n- User request to audit test quality\n- Before major refactoring efforts\n\n**This skill calls (MANDATORY):**\n- hyperpowers:sre-task-refinement (for ALL bd tasks created)\n- hyperpowers:test-runner agent (to run tests during analysis)\n- hyperpowers:test-effectiveness-analyst agent (for detailed analysis)\n\n**This skill creates:**\n- bd epic for test quality improvement\n- bd tasks for removing, strengthening, and adding tests\n- bd validation task with mutation testing\n\n**Workflow chain:**\n```\nanalyzing-test-effectiveness\n    ↓ (creates bd issues)\nsre-task-refinement (on each task)\n    ↓ (refines tasks)\nexecuting-plans (implements tasks)\n    ↓ (runs validation)\nreview-implementation (verifies quality)\n```\n\n**This skill informs:**\n- hyperpowers:sre-task-refinement (test specifications in plans)\n- hyperpowers:test-driven-development (what makes a good test)\n\n**Mutation testing tools:**\n- Java: [Pitest](https://pitest.org/) (`mvn org.pitest:pitest-maven:mutationCoverage`)\n- JS/TS: [Stryker](https://stryker-mutator.io/) (`npx stryker run`)\n- Python: mutmut (`mutmut run`)\n- .NET: Stryker.NET (`dotnet stryker`)\n</integration>\n\n<resources>\n**Research sources:**\n- [Google Testing Blog: Code Coverage Best Practices](https://testing.googleblog.com/2020/08/code-coverage-best-practices.html)\n- [Software Testing Anti-patterns](https://blog.codepipes.com/testing/software-testing-antipatterns.html)\n- [Tautological Tests](https://randycoulman.com/blog/2016/12/20/tautological-tests/)\n- [Mutation Testing Guide](https://mastersoftwaretesting.com/testing-fundamentals/types-of-testing/mutation-testing)\n- [Codecov: Beyond Coverage Metrics](https://about.codecov.io/blog/measuring-the-effectiveness-of-test-suites-beyond-code-coverage-metrics/)\n- [Google SRE: Testing Reliability](https://sre.google/sre-book/testing-reliability/)\n\n**Key insight from Google:** \"Coverage mainly tells you about code that has no tests: it doesn't tell you about the quality of testing for the code that's 'covered'.\"\n\n**When stuck:**\n- Test seems borderline RED/YELLOW → Ask: \"If I delete this test, what bug could slip through?\" If none, it's RED.\n- Unsure if assertion is weak → Ask: \"Could the code return wrong value while assertion passes?\" If yes, strengthen.\n- Unsure if corner case matters → Ask: \"Has this ever caused a production bug, anywhere?\" If yes, test it.\n</resources>"
              },
              {
                "name": "brainstorming",
                "description": "Use when creating or developing anything, before writing code - refines rough ideas into bd epics with immutable requirements",
                "path": "skills/brainstorming/SKILL.md",
                "frontmatter": {
                  "name": "brainstorming",
                  "description": "Use when creating or developing anything, before writing code - refines rough ideas into bd epics with immutable requirements"
                },
                "content": "<skill_overview>\nTurn rough ideas into validated designs stored as bd epics with immutable requirements; tasks created iteratively as you learn, not upfront.\n</skill_overview>\n\n<rigidity_level>\nHIGH FREEDOM - Adapt Socratic questioning to context, but always create immutable epic before code and only create first task (not full tree).\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | Deliverable |\n|------|--------|-------------|\n| 1 | Ask questions (one at a time) | Understanding of requirements |\n| 2 | Research (agents for codebase/internet) | Existing patterns and approaches |\n| 3 | Propose 2-3 approaches with trade-offs | Recommended option |\n| 4 | Present design in sections (200-300 words) | Validated architecture |\n| 5 | Create bd epic with IMMUTABLE requirements | Epic with anti-patterns |\n| 6 | Create ONLY first task | Ready for executing-plans |\n| 7 | Hand off to executing-plans | Iterative implementation begins |\n\n**Key:** Epic = contract (immutable), Tasks = adaptive (created as you learn)\n</quick_reference>\n\n<when_to_use>\n- User describes new feature to implement\n- User has rough idea that needs refinement\n- About to write code without clear requirements\n- Need to explore approaches before committing\n- Requirements exist but architecture unclear\n\n**Don't use for:**\n- Executing existing plans (use hyperpowers:executing-plans)\n- Fixing bugs (use hyperpowers:fixing-bugs)\n- Refactoring (use hyperpowers:refactoring-safely)\n- Requirements already crystal clear and epic exists\n</when_to_use>\n\n<the_process>\n## 1. Understanding the Idea\n\n**Announce:** \"I'm using the brainstorming skill to refine your idea into a design.\"\n\n**Check current state:**\n- Recent commits, existing docs, codebase structure\n- Dispatch `hyperpowers:codebase-investigator` for existing patterns\n- Dispatch `hyperpowers:internet-researcher` for external APIs/libraries\n\n**REQUIRED: Use AskUserQuestion tool for all questions**\n- One question at a time (don't batch multiple questions)\n- Prefer multiple choice options (easier to answer)\n- Wait for response before asking next question\n- Focus on: purpose, constraints, success criteria\n- Gather enough context to propose approaches\n\n**Do NOT just print questions and wait for \"yes\"** - use the AskUserQuestion tool.\n\n**Example questions:**\n- \"What problem does this solve for users?\"\n- \"Are there existing implementations we should follow?\"\n- \"What's the most important success criterion?\"\n- \"Token storage: cookies, localStorage, or sessionStorage?\"\n\n---\n\n## 2. Exploring Approaches\n\n**Research first:**\n- Similar feature exists → dispatch codebase-investigator\n- New integration → dispatch internet-researcher\n- Review findings before proposing\n\n**IMPORTANT: Capture research findings for Design Rationale**\nAs you research, note down:\n- Codebase findings: file paths, patterns discovered, relevant code\n- External findings: API capabilities, library constraints, doc URLs\n- These will populate the \"Research Findings\" section of the epic\n\n**Propose 2-3 approaches with trade-offs:**\n\n```\nBased on [research findings], I recommend:\n\n1. **[Approach A]** (recommended)\n   - Pros: [benefits, especially \"matches existing pattern\"]\n   - Cons: [drawbacks]\n\n2. **[Approach B]**\n   - Pros: [benefits]\n   - Cons: [drawbacks]\n\n3. **[Approach C]**\n   - Pros: [benefits]\n   - Cons: [drawbacks]\n\nI recommend option 1 because [specific reason, especially codebase consistency].\n```\n\n**Lead with recommended option and explain why.**\n\n---\n\n## 3. Presenting the Design\n\n**Once approach is chosen, present design in sections:**\n- Break into 200-300 word chunks\n- Ask after each: \"Does this look right so far?\"\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify\n\n**Show research findings:**\n- \"Based on codebase investigation: auth/ uses passport.js...\"\n- \"API docs show OAuth flow requires...\"\n- Demonstrate how design builds on existing code\n\n---\n\n## 4. Creating the bd Epic\n\n**After design validated, create epic as immutable contract:**\n\n```bash\nbd create \"Feature: [Feature Name]\" \\\n  --type epic \\\n  --priority [0-4] \\\n  --design \"## Requirements (IMMUTABLE)\n[What MUST be true when complete - specific, testable]\n- Requirement 1: [concrete requirement]\n- Requirement 2: [concrete requirement]\n- Requirement 3: [concrete requirement]\n\n## Success Criteria (MUST ALL BE TRUE)\n- [ ] Criterion 1 (objective, testable - e.g., 'Integration tests pass')\n- [ ] Criterion 2 (objective, testable - e.g., 'Works with existing User model')\n- [ ] All tests passing\n- [ ] Pre-commit hooks passing\n\n## Anti-Patterns (FORBIDDEN)\n- ❌ [Pattern] ([reasoning] - e.g., 'NO localStorage tokens (security: httpOnly prevents XSS token theft)')\n- ❌ [Pattern] ([reasoning] - e.g., 'NO mocking OAuth in integration tests (validation: defeats purpose)')\n\n## Approach\n[2-3 paragraph summary of chosen approach]\n\n## Architecture\n[Key components, data flow, integration points]\n\n## Design Rationale\n### Problem\n[1-2 sentences: what problem this solves, why status quo insufficient]\n\n### Research Findings\n**Codebase:**\n- [file.ts:line] - [what it does, why relevant]\n- [pattern discovered, implications]\n\n**External:**\n- [API/library] - [key capability, constraint discovered]\n- [doc URL] - [relevant guidance found]\n\n### Approaches Considered\n1. **[Chosen Approach]** ✓\n   - Pros: [benefits]\n   - Cons: [drawbacks]\n   - **Chosen because:** [specific reasoning, especially codebase consistency]\n\n2. **[Rejected Approach A]**\n   - Pros: [benefits]\n   - Cons: [drawbacks]\n   - **Rejected because:** [specific reasoning]\n\n3. **[Rejected Approach B]** (if applicable)\n   - Pros: [benefits]\n   - Cons: [drawbacks]\n   - **Rejected because:** [specific reasoning]\n\n### Scope Boundaries\n**In scope:**\n- [explicit inclusions]\n\n**Out of scope (deferred/never):**\n- [explicit exclusions with reasoning]\n\n### Open Questions\n- [uncertainties to resolve during implementation]\n- [decisions deferred to execution phase]\"\n```\n\n**Critical:** Anti-patterns section prevents watering down requirements when blockers occur. Always include reasoning.\n\n**Example anti-patterns:**\n- ❌ NO localStorage tokens (security: httpOnly prevents XSS token theft)\n- ❌ NO new user model (consistency: must integrate with existing db/models/user.ts)\n- ❌ NO mocking OAuth in integration tests (validation: defeats purpose of testing real flow)\n- ❌ NO TODO stubs for core authentication flow (completeness: core flow must be implemented)\n\n---\n\n## 5. Creating ONLY First Task\n\n**Create one task, not full tree:**\n\n```bash\nbd create \"Task 1: [Specific Deliverable]\" \\\n  --type feature \\\n  --priority [match-epic] \\\n  --design \"## Goal\n[What this task delivers - one clear outcome]\n\n## Implementation\n[Detailed step-by-step for this task]\n\n1. Study existing code\n   [Point to 2-3 similar implementations: file.ts:line]\n\n2. Write tests first (TDD)\n   [Specific test cases for this task]\n\n3. Implementation checklist\n   - [ ] file.ts:line - function_name() - [exactly what it does]\n   - [ ] test.ts:line - test_name() - [what scenario it tests]\n\n## Success Criteria\n- [ ] [Specific, measurable outcome]\n- [ ] Tests passing\n- [ ] Pre-commit hooks passing\"\n\nbd dep add bd-2 bd-1 --type parent-child  # Link to epic\n```\n\n**Why only one task?**\n- Subsequent tasks created iteratively by executing-plans\n- Each task reflects learnings from previous\n- Avoids brittle task trees that break when assumptions change\n\n---\n\n## 6. SRE Refinement and Handoff\n\nAfter epic and first task created:\n\n**REQUIRED: Run SRE refinement before handoff**\n\n```\nUse Skill tool: hyperpowers:sre-task-refinement\n```\n\nSRE refinement will:\n- Apply 7-category corner-case analysis (Opus 4.1)\n- Strengthen success criteria\n- Identify edge cases and failure modes\n- Ensure task is ready for implementation\n\n**Do NOT skip SRE refinement.** The first task sets the pattern for the entire epic.\n\n**After refinement approved, present handoff:**\n\n```\n\"Epic bd-1 is ready with immutable requirements and success criteria.\nFirst task bd-2 has been refined and is ready to execute.\n\nReady to start implementation? I'll use executing-plans to work through this iteratively.\n\nThe executing-plans skill will:\n1. Execute the current task\n2. Review what was learned against epic requirements\n3. Create next task based on current reality\n4. Run SRE refinement on new tasks\n5. Repeat until all epic success criteria met\n\nThis approach avoids brittle upfront planning - each task adapts to what we learn.\"\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer skips research, proposes approach without checking codebase</scenario>\n\n<code>\nUser: \"Add OAuth authentication\"\n\nClaude (without brainstorming):\n\"I'll implement OAuth with Auth0...\"\n[Proposes approach without checking if auth exists]\n[Doesn't research existing patterns]\n[Misses that passport.js already set up]\n</code>\n\n<why_it_fails>\n- Proposes Auth0 when passport.js already exists in codebase\n- Creates inconsistent architecture (two auth systems)\n- Wastes time implementing when partial solution exists\n- Doesn't leverage existing code\n- User has to redirect to existing pattern\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n1. **Research first:**\n   - Dispatch codebase-investigator: \"Find existing auth implementation\"\n   - Findings: passport.js at auth/passport-config.ts\n   - Dispatch internet-researcher: \"Passport OAuth2 strategies\"\n\n2. **Propose approaches building on findings:**\n   ```\n   Based on codebase showing passport.js at auth/passport-config.ts:\n\n   1. Extend existing passport setup (recommended)\n      - Add google-oauth20 strategy\n      - Matches codebase pattern\n      - Pros: Consistent, tested library\n      - Cons: Requires OAuth provider setup\n\n   2. Custom JWT implementation\n      - Pros: Full control\n      - Cons: Security complexity, breaks pattern\n\n   I recommend option 1 because it builds on existing auth/ setup.\n   ```\n\n**What you gain:**\n- Leverages existing code (faster)\n- Consistent architecture (maintainable)\n- Research informs design (correct)\n- User sees you understand codebase (trust)\n</correction>\n</example>\n\n<example>\n<scenario>Developer creates full task tree upfront</scenario>\n\n<code>\nbd create \"Epic: Add OAuth\"\nbd create \"Task 1: Configure OAuth provider\"\nbd create \"Task 2: Implement token exchange\"\nbd create \"Task 3: Add refresh token logic\"\nbd create \"Task 4: Create middleware\"\nbd create \"Task 5: Add UI components\"\nbd create \"Task 6: Write integration tests\"\n\n# Starts implementing Task 1\n# Discovers OAuth library handles refresh automatically\n# Now Task 3 is wrong, needs deletion\n# Discovers middleware already exists\n# Now Task 4 is wrong\n# Task tree brittle to reality\n</code>\n\n<why_it_fails>\n- Assumptions about implementation prove wrong\n- Task tree becomes incorrect as you learn\n- Wastes time updating/deleting wrong tasks\n- Rigid plan fights with reality\n- Context switching between fixing plan and implementing\n</why_it_fails>\n\n<correction>\n**Correct approach (iterative):**\n\n```bash\nbd create \"Epic: Add OAuth\" [with immutable requirements]\nbd create \"Task 1: Configure OAuth provider\"\n\n# Execute Task 1\n# Learn: OAuth library handles refresh, middleware exists\n\nbd create \"Task 2: Integrate with existing middleware\"\n# [Created AFTER learning from Task 1]\n\n# Execute Task 2\n# Learn: UI needs OAuth button component\n\nbd create \"Task 3: Add OAuth button to login UI\"\n# [Created AFTER learning from Task 2]\n```\n\n**What you gain:**\n- Tasks reflect current reality (accurate)\n- No wasted time fixing wrong plans (efficient)\n- Each task informed by previous learnings (adaptive)\n- Plan evolves with understanding (flexible)\n- Epic requirements stay immutable (contract preserved)\n</correction>\n</example>\n\n<example>\n<scenario>Epic created without anti-patterns section</scenario>\n\n<code>\nbd create \"Epic: OAuth Authentication\" --design \"\n## Requirements\n- Users authenticate via Google OAuth2\n- Tokens stored securely\n- Session management\n\n## Success Criteria\n- [ ] Login flow works\n- [ ] Tokens secured\n- [ ] All tests pass\n\"\n\n# During implementation, hits blocker:\n# \"Integration tests for OAuth are complex, I'll mock it...\"\n# [No anti-pattern preventing this]\n# Ships with mocked OAuth (defeats validation)\n</code>\n\n<why_it_fails>\n- No explicit forbidden patterns\n- Agent rationalizes shortcuts when blocked\n- \"Tokens stored securely\" too vague (localStorage? cookies?)\n- Requirements can be \"met\" without meeting intent\n- Mocking defeats the purpose of integration tests\n</why_it_fails>\n\n<correction>\n**Correct approach with anti-patterns and design rationale:**\n\n```bash\nbd create \"Epic: OAuth Authentication\" --design \"\n## Requirements (IMMUTABLE)\n- Users authenticate via Google OAuth2\n- Tokens stored in httpOnly cookies (NOT localStorage)\n- Session expires after 24h inactivity\n- Integrates with existing User model at db/models/user.ts\n\n## Success Criteria\n- [ ] Login redirects to Google and back\n- [ ] Tokens in httpOnly cookies\n- [ ] Token refresh works automatically\n- [ ] Integration tests pass WITHOUT mocking OAuth\n- [ ] All tests passing\n\n## Anti-Patterns (FORBIDDEN)\n- ❌ NO localStorage tokens (security: httpOnly prevents XSS token theft)\n- ❌ NO new user model (consistency: must use existing db/models/user.ts)\n- ❌ NO mocking OAuth in integration tests (validation: defeats purpose of testing real flow)\n- ❌ NO skipping token refresh (completeness: explicit requirement from user)\n\n## Approach\nExtend existing passport.js setup at auth/passport-config.ts with Google OAuth2 strategy.\nUse passport-google-oauth20 library. Store tokens in httpOnly cookies via express-session.\nIntegrate with existing User model for profile storage.\n\n## Architecture\n- auth/strategies/google.ts - New OAuth strategy\n- auth/passport-config.ts - Register strategy (existing)\n- db/models/user.ts - Add googleId field (existing)\n- routes/auth.ts - OAuth callback routes\n\n## Design Rationale\n### Problem\nUsers currently have no SSO option - must create accounts manually.\nManual signup has 40% abandonment rate. Google OAuth reduces friction.\n\n### Research Findings\n**Codebase:**\n- auth/passport-config.ts:1-50 - Existing passport setup, uses session-based auth\n- auth/strategies/local.ts:1-30 - Pattern for adding strategies\n- db/models/user.ts:1-80 - User model, already has email field\n\n**External:**\n- passport-google-oauth20 - Official Google strategy, 2M weekly downloads\n- Google OAuth2 docs - Requires client ID, callback URL, scopes\n\n### Approaches Considered\n1. **Extend passport.js with google-oauth20** ✓\n   - Pros: Matches existing pattern, well-documented, session reuse\n   - Cons: Adds dependency\n   - **Chosen because:** Consistent with auth/strategies/local.ts pattern\n\n2. **Custom JWT-based OAuth**\n   - Pros: No new dependencies, full control\n   - Cons: Security complexity, breaks existing session pattern\n   - **Rejected because:** Inconsistent with codebase, security risk\n\n3. **Auth0 integration**\n   - Pros: Managed service, multiple providers\n   - Cons: External dependency, cost, different auth model\n   - **Rejected because:** Overkill for single provider, introduces new pattern\n\n### Scope Boundaries\n**In scope:**\n- Google OAuth login/signup\n- Token storage in httpOnly cookies\n- Profile sync with User model\n\n**Out of scope (deferred/never):**\n- Other OAuth providers (GitHub, Facebook) - deferred to future epic\n- Account linking (connect Google to existing account) - deferred\n- Custom OAuth scopes beyond profile/email - not needed\n\n### Open Questions\n- Should failed OAuth create partial user record? (decide during implementation)\n- Token refresh: silent vs prompt? (default to silent, user can configure)\n\"\n```\n\n**What you gain:**\n- Requirements concrete and specific (testable)\n- Forbidden patterns explicit with reasoning (prevents shortcuts)\n- Agent can't rationalize away requirements (contract enforced)\n- Design rationale preserves context for future tasks\n- Approaches considered show why alternatives were rejected\n- Open questions explicitly tracked for implementation decisions\n</correction>\n</example>\n</examples>\n\n<key_principles>\n- **One question at a time** - Don't overwhelm\n- **Multiple choice preferred** - Easier to answer when possible\n- **Delegate research** - Use codebase-investigator and internet-researcher agents\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Epic is contract** - Requirements immutable, tasks adapt\n- **Anti-patterns prevent shortcuts** - Explicit forbidden patterns stop rationalization\n- **One task only** - Subsequent tasks created iteratively (not upfront)\n</key_principles>\n\n<research_agents>\n## Use codebase-investigator when:\n- Understanding how existing features work\n- Finding where specific functionality lives\n- Identifying patterns to follow\n- Verifying assumptions about structure\n- Checking if feature already exists\n\n## Use internet-researcher when:\n- Finding current API documentation\n- Researching library capabilities\n- Comparing technology options\n- Understanding community recommendations\n- Finding official code examples\n\n## Research protocol:\n1. Codebase pattern exists → Use it (unless clearly unwise)\n2. No codebase pattern → Research external patterns\n3. Research yields nothing → Ask user for direction\n</research_agents>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Use AskUserQuestion tool** → Don't just print questions and wait\n2. **Research BEFORE proposing** → Use agents to understand context\n3. **Propose 2-3 approaches** → Don't jump to single solution\n4. **Epic requirements IMMUTABLE** → Tasks adapt, requirements don't\n5. **Include anti-patterns section** → Prevents watering down requirements\n6. **Create ONLY first task** → Subsequent tasks created iteratively\n7. **Run SRE refinement** → Before handoff to executing-plans\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow the process.**\n\n- \"Requirements obvious, don't need questions\" (Questions reveal hidden complexity)\n- \"I know this pattern, don't need research\" (Research might show better way)\n- \"Can plan all tasks upfront\" (Plans become brittle, tasks adapt as you learn)\n- \"Anti-patterns section overkill\" (Prevents rationalization under pressure)\n- \"Epic can evolve\" (Requirements contract, tasks evolve)\n- \"Can just print questions\" (Use AskUserQuestion tool - it's more interactive)\n- \"SRE refinement overkill for first task\" (First task sets pattern for entire epic)\n- \"User said yes, design is done\" (Still need SRE refinement before execution)\n</critical_rules>\n\n<verification_checklist>\nBefore handing off to executing-plans:\n\n- [ ] Used AskUserQuestion tool for clarifying questions (one at a time)\n- [ ] Researched codebase patterns (if applicable)\n- [ ] Researched external docs/libraries (if applicable)\n- [ ] Proposed 2-3 approaches with trade-offs\n- [ ] Presented design in sections, validated each\n- [ ] Created bd epic with all sections (requirements, success criteria, anti-patterns, approach, architecture, design rationale)\n- [ ] Requirements are IMMUTABLE and specific\n- [ ] Anti-patterns include reasoning (not just \"NO X\" but \"NO X (reason: Y)\")\n- [ ] Design Rationale complete: problem, research findings, approaches considered, scope boundaries, open questions\n- [ ] Created ONLY first task (not full tree)\n- [ ] First task has detailed implementation checklist\n- [ ] Ran SRE refinement on first task (hyperpowers:sre-task-refinement)\n- [ ] Announced handoff to executing-plans after refinement approved\n\n**Can't check all boxes?** Return to process and complete missing steps.\n</verification_checklist>\n\n<integration>\n**This skill calls:**\n- hyperpowers:codebase-investigator (for finding existing patterns)\n- hyperpowers:internet-researcher (for external documentation)\n- hyperpowers:sre-task-refinement (REQUIRED before handoff to executing-plans)\n- hyperpowers:executing-plans (handoff after refinement approved)\n\n**Call chain:**\n```\nbrainstorming → sre-task-refinement → executing-plans\n```\n\n**This skill is called by:**\n- hyperpowers:using-hyper (mandatory before writing code)\n- User requests for new features\n- Beginning of greenfield development\n\n**Agents used:**\n- codebase-investigator (understand existing code)\n- internet-researcher (find external documentation)\n\n**Tools required:**\n- AskUserQuestion (for all clarifying questions)\n</integration>\n\n<resources>\n**Detailed guides:**\n- [bd epic template examples](resources/epic-templates.md)\n- [Socratic questioning patterns](resources/questioning-patterns.md)\n- [Anti-pattern examples by domain](resources/anti-patterns.md)\n\n**When stuck:**\n- User gives vague answer → Ask follow-up multiple choice question\n- Research yields nothing → Ask user for direction explicitly\n- Too many approaches → Narrow to top 2-3, explain why others eliminated\n- User changes requirements mid-design → Acknowledge, return to understanding phase\n</resources>"
              },
              {
                "name": "building-hooks",
                "description": "Use when creating Claude Code hooks - covers hook patterns, composition, testing, progressive enhancement from simple to advanced",
                "path": "skills/building-hooks/SKILL.md",
                "frontmatter": {
                  "name": "building-hooks",
                  "description": "Use when creating Claude Code hooks - covers hook patterns, composition, testing, progressive enhancement from simple to advanced"
                },
                "content": "<skill_overview>\nHooks encode business rules at application level; start with observation, add automation, enforce only when patterns clear.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow progressive enhancement (observe → automate → enforce) strictly. Hook patterns are adaptable, but always start non-blocking and test thoroughly.\n</rigidity_level>\n\n<quick_reference>\n| Phase | Approach | Example |\n|-------|----------|---------|\n| 1. Observe | Non-blocking, report only | Log edits, display reminders |\n| 2. Automate | Background tasks, non-blocking | Auto-format, run builds |\n| 3. Enforce | Blocking only when necessary | Block dangerous ops, require fixes |\n\n**Most used events:** UserPromptSubmit (before processing), Stop (after completion)\n\n**Critical:** Start Phase 1, observe for a week, then Phase 2. Only add Phase 3 if absolutely necessary.\n</quick_reference>\n\n<when_to_use>\nUse hooks for:\n- Automatic quality checks (build, lint, format)\n- Workflow automation (skill activation, context injection)\n- Error prevention (catching issues early)\n- Consistent behavior (formatting, conventions)\n\n**Never use hooks for:**\n- Complex business logic (use tools/scripts)\n- Slow operations that block workflow (use background jobs)\n- Anything requiring LLM reasoning (hooks are deterministic)\n</when_to_use>\n\n<hook_lifecycle_events>\n| Event | When Fires | Use Cases |\n|-------|------------|-----------|\n| UserPromptSubmit | Before Claude processes prompt | Validation, context injection, skill activation |\n| Stop | After Claude finishes | Build checks, formatting, quality reminders |\n| PostToolUse | After each tool execution | Logging, tracking, validation |\n| PreToolUse | Before tool execution | Permission checks, validation |\n| ToolError | When tool fails | Error handling, fallbacks |\n| SessionStart | New session begins | Environment setup, context loading |\n| SessionEnd | Session closes | Cleanup, logging |\n| Error | Unhandled error | Error recovery, notifications |\n</hook_lifecycle_events>\n\n<progressive_enhancement>\n## Phase 1: Observation (Non-Blocking)\n\n**Goal:** Understand patterns before acting\n\n**Examples:**\n- Log file edits (PostToolUse)\n- Display reminders (Stop, non-blocking)\n- Track metrics\n\n**Duration:** Observe for 1 week minimum\n\n---\n\n## Phase 2: Automation (Background)\n\n**Goal:** Automate tedious tasks\n\n**Examples:**\n- Auto-format edited files (Stop)\n- Run builds after changes (Stop)\n- Inject helpful context (UserPromptSubmit)\n\n**Requirement:** Fast (<2 seconds), non-blocking\n\n---\n\n## Phase 3: Enforcement (Blocking)\n\n**Goal:** Prevent errors, enforce standards\n\n**Examples:**\n- Block dangerous operations (PreToolUse)\n- Require fixes before continuing (Stop, blocking)\n- Validate inputs (UserPromptSubmit, blocking)\n\n**Requirement:** Only add when patterns clear from Phase 1-2\n</progressive_enhancement>\n\n<common_hook_patterns>\n## Pattern 1: Build Checker (Stop Hook)\n\n**Problem:** TypeScript errors left behind\n\n**Solution:**\n```bash\n#!/bin/bash\n# Stop hook - runs after Claude finishes\n\n# Check modified repos\nmodified_repos=$(grep -h \"edited\" ~/.claude/edit-log.txt | cut -d: -f1 | sort -u)\n\nfor repo in $modified_repos; do\n  echo \"Building $repo...\"\n  cd \"$repo\" && npm run build 2>&1 | tee /tmp/build-output.txt\n\n  error_count=$(grep -c \"error TS\" /tmp/build-output.txt || echo \"0\")\n\n  if [ \"$error_count\" -gt 0 ]; then\n    if [ \"$error_count\" -ge 5 ]; then\n      echo \"⚠️  Found $error_count errors - consider error-resolver agent\"\n    else\n      echo \"🔴 Found $error_count TypeScript errors:\"\n      grep \"error TS\" /tmp/build-output.txt\n    fi\n  else\n    echo \"✅ Build passed\"\n  fi\ndone\n```\n\n**Configuration:**\n```json\n{\n  \"event\": \"Stop\",\n  \"command\": \"~/.claude/hooks/build-checker.sh\",\n  \"description\": \"Run builds on modified repos\",\n  \"blocking\": false\n}\n```\n\n**Result:** Zero errors left behind\n\n---\n\n## Pattern 2: Auto-Formatter (Stop Hook)\n\n**Problem:** Inconsistent formatting\n\n**Solution:**\n```bash\n#!/bin/bash\n# Stop hook - format all edited files\n\nedited_files=$(tail -20 ~/.claude/edit-log.txt | grep \"^/\" | sort -u)\n\nfor file in $edited_files; do\n  repo_dir=$(dirname \"$file\")\n  while [ \"$repo_dir\" != \"/\" ]; do\n    if [ -f \"$repo_dir/.prettierrc\" ]; then\n      echo \"Formatting $file...\"\n      cd \"$repo_dir\" && npx prettier --write \"$file\"\n      break\n    fi\n    repo_dir=$(dirname \"$repo_dir\")\n  done\ndone\n\necho \"✅ Formatting complete\"\n```\n\n**Result:** All code consistently formatted\n\n---\n\n## Pattern 3: Error Handling Reminder (Stop Hook)\n\n**Problem:** Claude forgets error handling\n\n**Solution:**\n```bash\n#!/bin/bash\n# Stop hook - gentle reminder\n\nedited_files=$(tail -20 ~/.claude/edit-log.txt | grep \"^/\")\n\nrisky_patterns=0\nfor file in $edited_files; do\n  if grep -q \"try\\|catch\\|async\\|await\\|prisma\\|router\\.\" \"$file\"; then\n    ((risky_patterns++))\n  fi\ndone\n\nif [ \"$risky_patterns\" -gt 0 ]; then\n  cat <<EOF\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n📋 ERROR HANDLING SELF-CHECK\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n⚠️  Risky Patterns Detected\n   $risky_patterns file(s) with async/try-catch/database operations\n\n   ❓ Did you add proper error handling?\n   ❓ Are errors logged appropriately?\n\n   💡 Consider: Sentry.captureException(), proper logging\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nEOF\nfi\n```\n\n**Result:** Claude self-checks without blocking\n\n---\n\n## Pattern 4: Skills Auto-Activation\n\n**See:** hyperpowers:skills-auto-activation for complete implementation\n\n**Summary:** Analyzes prompt keywords, injects skill activation reminder before Claude processes.\n</common_hook_patterns>\n\n<hook_composition>\n## Naming for Order Control\n\nMultiple hooks for same event run in **alphabetical order** by filename.\n\n**Use numeric prefixes:**\n\n```\nhooks/\n├── 00-log-prompt.sh       # First (logging)\n├── 10-inject-context.sh   # Second (context)\n├── 20-activate-skills.sh  # Third (skills)\n└── 99-notify.sh           # Last (notifications)\n```\n\n## Hook Dependencies\n\nIf Hook B depends on Hook A's output:\n\n1. **Option 1:** Numeric prefixes (A before B)\n2. **Option 2:** Combine into single hook\n3. **Option 3:** File-based communication\n\n**Example:**\n```bash\n# 10-track-edits.sh writes to edit-log.txt\n# 20-check-builds.sh reads from edit-log.txt\n```\n</hook_composition>\n\n<testing_hooks>\n## Test in Isolation\n\n```bash\n# Manually trigger\nbash ~/.claude/hooks/build-checker.sh\n\n# Check exit code\necho $?  # 0 = success\n```\n\n## Test with Mock Data\n\n```bash\n# Create mock log\necho \"/path/to/test/file.ts\" > /tmp/test-edit-log.txt\n\n# Run with test data\nEDIT_LOG=/tmp/test-edit-log.txt bash ~/.claude/hooks/build-checker.sh\n```\n\n## Test Non-Blocking Behavior\n\n- Hook exits quickly (<2 seconds)\n- Doesn't block Claude\n- Provides clear output\n\n## Test Blocking Behavior\n\n- Blocking decision correct\n- Reason message helpful\n- Escape hatch exists\n\n## Debugging\n\n**Enable logging:**\n```bash\nset -x  # Debug output\nexec 2>~/.claude/hooks/debug.log\n```\n\n**Check execution:**\n```bash\ntail -f ~/.claude/logs/hooks.log\n```\n\n**Common issues:**\n- Timeout (>10 second default)\n- Wrong working directory\n- Missing environment variables\n- File permissions\n</testing_hooks>\n\n<examples>\n<example>\n<scenario>Developer adds blocking hook immediately without observation</scenario>\n\n<code>\n# Developer frustrated by TypeScript errors\n# Creates blocking Stop hook immediately:\n\n#!/bin/bash\nnpm run build\n\nif [ $? -ne 0 ]; then\n  echo \"BUILD FAILED - BLOCKING\"\n  exit 1  # Blocks Claude\nfi\n</code>\n\n<why_it_fails>\n- No observation period to understand patterns\n- Blocks even for minor errors\n- No escape hatch if hook misbehaves\n- Might block during experimentation\n- Frustrates workflow when building is slow\n- Haven't identified when blocking is actually needed\n</why_it_fails>\n\n<correction>\n**Phase 1: Observe (1 week)**\n\n```bash\n#!/bin/bash\n# Non-blocking observation\nnpm run build 2>&1 | tee /tmp/build.log\n\nif grep -q \"error TS\" /tmp/build.log; then\n  echo \"🔴 Build errors found (not blocking)\"\nfi\n```\n\n**After 1 week, review:**\n- How often do errors appear?\n- Are they usually fixed quickly?\n- Do they cause real problems or just noise?\n\n**Phase 2: If errors are frequent, automate**\n\n```bash\n#!/bin/bash\n# Still non-blocking, but more helpful\nnpm run build 2>&1 | tee /tmp/build.log\n\nerror_count=$(grep -c \"error TS\" /tmp/build.log || echo \"0\")\n\nif [ \"$error_count\" -ge 5 ]; then\n  echo \"⚠️  $error_count errors - consider using error-resolver agent\"\nelif [ \"$error_count\" -gt 0 ]; then\n  echo \"🔴 $error_count errors (not blocking):\"\n  grep \"error TS\" /tmp/build.log | head -5\nfi\n```\n\n**Phase 3: Only if observation shows blocking is necessary**\n\nNever reached - non-blocking works fine!\n\n**What you gain:**\n- Understood patterns before acting\n- Non-blocking keeps workflow smooth\n- Helpful messages without friction\n- Can experiment without frustration\n</correction>\n</example>\n\n<example>\n<scenario>Hook is slow, blocks workflow</scenario>\n\n<code>\n#!/bin/bash\n# Stop hook that's too slow\n\n# Run full test suite (takes 45 seconds!)\nnpm test\n\n# Run linter (takes 10 seconds)\nnpm run lint\n\n# Run build (takes 30 seconds)\nnpm run build\n\n# Total: 85 seconds of blocking!\n</code>\n\n<why_it_fails>\n- Hook takes 85 seconds to complete\n- Blocks Claude for entire duration\n- User can't continue working\n- Frustrating, likely to be disabled\n- Defeats purpose of automation\n</why_it_fails>\n\n<correction>\n**Make hook fast (<2 seconds):**\n\n```bash\n#!/bin/bash\n# Stop hook - fast checks only\n\n# Quick syntax check (< 1 second)\nnpm run check-syntax\n\nif [ $? -ne 0 ]; then\n  echo \"🔴 Syntax errors found\"\n  echo \"💡 Run 'npm test' manually for full test suite\"\nfi\n\necho \"✅ Quick checks passed (run 'npm test' for full suite)\"\n```\n\n**Or run slow checks in background:**\n\n```bash\n#!/bin/bash\n# Stop hook - trigger background job\n\n# Start tests in background\n(\n  npm test > /tmp/test-results.txt 2>&1\n  if [ $? -ne 0 ]; then\n    echo \"🔴 Tests failed (see /tmp/test-results.txt)\"\n  fi\n) &\n\necho \"⏳ Tests running in background (check /tmp/test-results.txt)\"\n```\n\n**What you gain:**\n- Hook completes instantly\n- Workflow not blocked\n- Still get quality checks\n- User can continue working\n</correction>\n</example>\n\n<example>\n<scenario>Hook has no error handling, fails silently</scenario>\n\n<code>\n#!/bin/bash\n# Hook with no error handling\n\nfile=$(tail -1 ~/.claude/edit-log.txt)\nprettier --write \"$file\"\n</code>\n\n<why_it_fails>\n- If edit-log.txt missing → hook fails silently\n- If file path invalid → prettier errors not caught\n- If prettier not installed → silent failure\n- No logging, can't debug\n- User has no idea hook ran or failed\n</why_it_fails>\n\n<correction>\n**Add error handling:**\n\n```bash\n#!/bin/bash\nset -euo pipefail  # Exit on error, undefined vars\n\n# Log execution\necho \"[$(date)] Hook started\" >> ~/.claude/hooks/formatter.log\n\n# Validate input\nif [ ! -f ~/.claude/edit-log.txt ]; then\n  echo \"[$(date)] ERROR: edit-log.txt not found\" >> ~/.claude/hooks/formatter.log\n  exit 1\nfi\n\nfile=$(tail -1 ~/.claude/edit-log.txt | grep \"^/.*\\.ts$\")\n\nif [ -z \"$file\" ]; then\n  echo \"[$(date)] No TypeScript file to format\" >> ~/.claude/hooks/formatter.log\n  exit 0\nfi\n\nif [ ! -f \"$file\" ]; then\n  echo \"[$(date)] ERROR: File not found: $file\" >> ~/.claude/hooks/formatter.log\n  exit 1\nfi\n\n# Check prettier exists\nif ! command -v prettier &> /dev/null; then\n  echo \"[$(date)] ERROR: prettier not installed\" >> ~/.claude/hooks/formatter.log\n  exit 1\nfi\n\n# Format\necho \"[$(date)] Formatting: $file\" >> ~/.claude/hooks/formatter.log\nif prettier --write \"$file\" 2>&1 | tee -a ~/.claude/hooks/formatter.log; then\n  echo \"✅ Formatted $file\"\nelse\n  echo \"🔴 Formatting failed (see ~/.claude/hooks/formatter.log)\"\nfi\n```\n\n**What you gain:**\n- Errors logged and visible\n- Graceful handling of missing files\n- Can debug when issues occur\n- Clear feedback to user\n- Hook doesn't fail silently\n</correction>\n</example>\n</examples>\n\n<security>\n**Hooks run with your credentials and have full system access.**\n\n## Best Practices\n\n1. **Review code carefully** - Hooks execute any command\n2. **Use absolute paths** - Don't rely on PATH\n3. **Validate inputs** - Don't trust file paths blindly\n4. **Limit scope** - Only access what's needed\n5. **Log actions** - Track what hooks do\n6. **Test thoroughly** - Especially blocking hooks\n\n## Dangerous Patterns\n\n❌ **Don't:**\n```bash\n# DANGEROUS - executes arbitrary code\ncmd=$(tail -1 ~/.claude/edit-log.txt)\neval \"$cmd\"\n```\n\n✅ **Do:**\n```bash\n# SAFE - validates and sanitizes\nfile=$(tail -1 ~/.claude/edit-log.txt | grep \"^/.*\\.ts$\")\nif [ -f \"$file\" ]; then\n  prettier --write \"$file\"\nfi\n```\n</security>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Start with Phase 1 (observe)** → Understand patterns before acting\n2. **Keep hooks fast (<2 seconds)** → Don't block workflow\n3. **Test thoroughly** → Hooks have full system access\n4. **Add error handling and logging** → Silent failures are debugging nightmares\n5. **Use progressive enhancement** → Observe → Automate → Enforce (only if needed)\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow progressive enhancement.**\n\n- \"Hook is simple, don't need testing\" (Untested hooks fail in production)\n- \"Blocking is fine, need to enforce\" (Start non-blocking, observe first)\n- \"I'll add error handling later\" (Hook errors silent, add now)\n- \"Hook is slow but thorough\" (Slow hooks block workflow, optimize)\n- \"Need access to everything\" (Minimal permissions only)\n</critical_rules>\n\n<verification_checklist>\nBefore deploying hook:\n\n- [ ] Tested in isolation (manual execution)\n- [ ] Tested with mock data\n- [ ] Completes quickly (<2 seconds for non-blocking)\n- [ ] Has error handling (set -euo pipefail)\n- [ ] Has logging (can debug failures)\n- [ ] Validates inputs (doesn't trust blindly)\n- [ ] Uses absolute paths\n- [ ] Started with Phase 1 (observation)\n- [ ] If blocking: has escape hatch\n\n**Can't check all boxes?** Return to development and fix.\n</verification_checklist>\n\n<integration>\n**This skill covers:** Hook creation and patterns\n\n**Related skills:**\n- hyperpowers:skills-auto-activation (complete skill activation hook)\n- hyperpowers:verification-before-completion (quality hooks automate this)\n- hyperpowers:testing-anti-patterns (avoid in hooks)\n\n**Hook patterns support:**\n- Automatic skill activation\n- Build verification\n- Code formatting\n- Error prevention\n- Workflow automation\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Complete hook examples](resources/hook-examples.md)\n- [Hook pattern library](resources/hook-patterns.md)\n- [Testing strategies](resources/testing-hooks.md)\n\n**Official documentation:**\n- [Anthropic Hooks Guide](https://docs.claude.com/en/docs/claude-code/hooks-guide)\n\n**When stuck:**\n- Hook failing silently → Add logging, check ~/.claude/hooks/debug.log\n- Hook too slow → Profile execution, move slow parts to background\n- Hook blocking incorrectly → Return to Phase 1, observe patterns\n- Testing unclear → Start with manual execution, then mock data\n</resources>"
              },
              {
                "name": "debugging-with-tools",
                "description": "Use when encountering bugs or test failures - systematic debugging using debuggers, internet research, and agents to find root cause before fixing",
                "path": "skills/debugging-with-tools/SKILL.md",
                "frontmatter": {
                  "name": "debugging-with-tools",
                  "description": "Use when encountering bugs or test failures - systematic debugging using debuggers, internet research, and agents to find root cause before fixing"
                },
                "content": "<skill_overview>\nRandom fixes waste time and create new bugs. Always use tools to understand root cause BEFORE attempting fixes. Symptom fixes are failure.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Must complete investigation phases (tools → hypothesis → test) before fixing.\n\nCan adapt tool choice to language/context. Never skip investigation or guess at fixes.\n</rigidity_level>\n\n<quick_reference>\n\n| Phase | Tools to Use | Output |\n|-------|--------------|--------|\n| **1. Investigate** | Error messages, internet-researcher agent, debugger, codebase-investigator | Root cause understanding |\n| **2. Hypothesize** | Form theory based on evidence (not guesses) | Testable hypothesis |\n| **3. Test** | Validate hypothesis with minimal change | Confirms or rejects theory |\n| **4. Fix** | Implement proper fix for root cause | Problem solved permanently |\n\n**FORBIDDEN:** Skip investigation → guess at fix → hope it works\n**REQUIRED:** Tools → evidence → hypothesis → test → fix\n\n**Key agents:**\n- `internet-researcher` - Search error messages, known bugs, solutions\n- `codebase-investigator` - Understand code structure, find related code\n- `test-runner` - Run tests without output pollution\n\n</quick_reference>\n\n<when_to_use>\n**Use for ANY technical issue:**\n- Test failures\n- Bugs in production or development\n- Unexpected behavior\n- Build failures\n- Integration issues\n- Performance problems\n\n**ESPECIALLY when:**\n- \"Just one quick fix\" seems obvious\n- Under time pressure (emergencies make guessing tempting)\n- Error message is unclear\n- Previous fix didn't work\n</when_to_use>\n\n<the_process>\n\n## Phase 1: Tool-Assisted Investigation\n\n**BEFORE attempting ANY fix, gather evidence with tools:**\n\n### 1. Read Complete Error Messages\n\n- Entire error message (not just first line)\n- Complete stack trace (all frames)\n- Line numbers, file paths, error codes\n- Stack traces show exact execution path\n\n### 2. Search Internet FIRST (Use internet-researcher Agent)\n\n**Dispatch internet-researcher with:**\n```\n\"Search for error: [exact error message]\n- Check Stack Overflow solutions\n- Look for GitHub issues in [library] version [X]\n- Find official documentation explaining this error\n- Check if this is a known bug\"\n```\n\n**What agent should find:**\n- Exact matches to your error\n- Similar symptoms and solutions\n- Known bugs in your dependency versions\n- Workarounds that worked for others\n\n### 3. Use Debugger to Inspect State\n\n**Claude cannot run debuggers directly. Instead:**\n\n**Option A - Recommend debugger to user:**\n```\n\"Let's use lldb/gdb/DevTools to inspect state at error location.\nPlease run: [specific commands]\nWhen breakpoint hits: [what to inspect]\nShare output with me.\"\n```\n\n**Option B - Add instrumentation Claude can add:**\n```rust\n// Add logging\nprintln!(\"DEBUG: var = {:?}, state = {:?}\", var, state);\n\n// Add assertions\nassert!(condition, \"Expected X but got {:?}\", actual);\n```\n\n### 4. Investigate Codebase (Use codebase-investigator Agent)\n\n**Dispatch codebase-investigator with:**\n```\n\"Error occurs in function X at line Y.\nFind:\n- How is X called? What are the callers?\n- What does variable Z contain at this point?\n- Are there similar functions that work correctly?\n- What changed recently in this area?\"\n```\n\n## Phase 2: Form Hypothesis\n\n**Based on evidence (not guesses):**\n\n1. **State what you know** (from investigation)\n2. **Propose theory** explaining the evidence\n3. **Make prediction** that tests the theory\n\n**Example:**\n```\nKnown: Error \"null pointer\" at auth.rs:45 when email is empty\nTheory: Empty email bypasses validation, passes null to login()\nPrediction: Adding validation before login() will prevent error\nTest: Add validation, verify error doesn't occur with empty email\n```\n\n**NEVER:**\n- Guess without evidence\n- Propose fix without hypothesis\n- Skip to \"try this and see\"\n\n## Phase 3: Test Hypothesis\n\n**Minimal change to validate theory:**\n\n1. Make smallest change that tests hypothesis\n2. Run test/reproduction case\n3. Observe result\n\n**If confirmed:** Proceed to Phase 4\n**If rejected:** Return to Phase 1 with new information\n\n## Phase 4: Implement Fix\n\n**After understanding root cause:**\n\n1. Write test reproducing bug (RED phase - use test-driven-development skill)\n2. Implement proper fix addressing root cause\n3. Verify test passes (GREEN phase)\n4. Run full test suite (regression check)\n5. Commit fix\n\n**The fix should:**\n- Address root cause (not symptom)\n- Be minimal and focused\n- Include test preventing regression\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer encounters test failure, immediately tries \"obvious\" fix without investigation</scenario>\n\n<code>\nTest error:\n```\nFAIL: test_login_expired_token\nAssertionError: Expected Err(TokenExpired), got Ok(User)\n```\n\nDeveloper thinks: \"Obviously the token expiration check is wrong\"\n\nMakes change without investigation:\n```rust\n// \"Fix\" - just check if token is expired\nif token.expires_at < now() {\n    return Err(AuthError::TokenExpired);\n}\n```\n\nCommits without testing other cases.\n</code>\n\n<why_it_fails>\n**No investigation:**\n- Didn't read error completely\n- Didn't check what `expires_at` contains\n- Didn't debug to see token state\n- Didn't search for similar issues\n\n**What actually happened:** Token `expires_at` was being parsed incorrectly, always showing future date. The \"fix\" adds dead code that never runs.\n\n**Result:** Bug not fixed, new dead code added, time wasted.\n</why_it_fails>\n\n<correction>\n**Phase 1 - Investigate with tools:**\n\n```bash\n# 1. Read complete error\nFAIL: test_login_expired_token at line 45\nExpected: Err(TokenExpired)\nGot: Ok(User { id: 123 })\nToken: { expires_at: \"2099-01-01\", ... }\n```\n\n**Dispatch internet-researcher:**\n```\n\"Search for: token expiration always showing future date\n- Check date parsing bugs\n- Look for timezone issues\n- Find JWT expiration handling\"\n```\n\n**Add instrumentation:**\n```rust\nprintln!(\"DEBUG: expires_at = {:?}, now = {:?}, expired = {:?}\",\n         token.expires_at, now(), token.expires_at < now());\n```\n\n**Run test again:**\n```\nDEBUG: expires_at = 2099-01-01T00:00:00Z, now = 2024-01-15T10:30:00Z, expired = false\n```\n\n**Phase 2 - Hypothesis:**\n\"Token `expires_at` is being set to 2099, not actual expiration. Problem is in token creation, not validation.\"\n\n**Phase 3 - Test:**\nCheck token creation code:\n```rust\n// Found the bug!\nfn create_token() -> Token {\n    Token {\n        expires_at: \"2099-01-01\".parse()?, // HARDCODED!\n        ...\n    }\n}\n```\n\n**Phase 4 - Fix root cause:**\n```rust\nfn create_token(duration: Duration) -> Token {\n    Token {\n        expires_at: now() + duration,  // Correct\n        ...\n    }\n}\n```\n\n**Result:** Root cause fixed, test passes, no dead code.\n</correction>\n</example>\n\n<example>\n<scenario>Developer skips internet search, reinvents solution to known problem</scenario>\n\n<code>\nError:\n```\nerror: linking with `cc` failed: exit status: 1\nld: symbol(s) not found for architecture arm64\n```\n\nDeveloper thinks: \"Must be a linking issue, I'll add flags\"\n\nSpends 2 hours trying different linker flags:\n```toml\n[target.aarch64-apple-darwin]\nrustflags = [\"-C\", \"link-arg=-undefined dynamic_lookup\"]\n# Doesn't work, tries more flags...\n```\n</code>\n\n<why_it_fails>\n**Skipped internet search:**\n- This is a common error with known solutions\n- Stack Overflow has exact fix\n- Official docs explain the issue\n- Wasted 2 hours reinventing solution\n\n**Why it happens:** Impatience, thinking \"I can figure this out faster\"\n</why_it_fails>\n\n<correction>\n**Dispatch internet-researcher FIRST:**\n\n```\n\"Search for: 'symbol not found for architecture arm64' Rust linking\n- Check Stack Overflow solutions\n- Look for Xcode/macOS specific fixes\n- Find Cargo configuration for Apple Silicon\"\n```\n\n**Agent returns (30 seconds):**\n```\nFound on Stack Overflow (2.4k upvotes):\nThis occurs when Xcode Command Line Tools aren't installed or outdated.\n\nSolution:\nxcode-select --install\n\nOr update existing:\nsoftwareupdate --all --install --force\n```\n\n**Apply solution:**\n```bash\nxcode-select --install\n# Wait for installation\ncargo build\n# Success!\n```\n\n**Result:** Fixed in 5 minutes, not 2 hours.\n</correction>\n</example>\n\n<example>\n<scenario>Developer fixes symptom without understanding root cause, bug returns different way</scenario>\n\n<code>\nBug: Users can delete other users' posts\n\nDeveloper notices: DELETE /posts/:id doesn't check ownership\n\n\"Fix\":\n```python\n@app.delete(\"/posts/{post_id}\")\ndef delete_post(post_id: int, user: User):\n    post = get_post(post_id)\n    if post.author_id != user.id:\n        raise Forbidden()\n    delete(post)\n```\n\nBug \"fixed\"... but bug report comes back: \"Users can edit other users' posts\"\n</code>\n\n<why_it_fails>\n**Symptom fix:**\n- Fixed DELETE endpoint\n- Didn't investigate root cause\n- Didn't check other endpoints\n\n**Root cause:** No authorization middleware checking ownership across ALL endpoints.\n\n**Result:** Whack-a-mole fixing symptoms, not the underlying issue.\n</why_it_fails>\n\n<correction>\n**Phase 1 - Investigate with codebase-investigator:**\n\n```\n\"User can delete other users' posts. Investigate:\n- What authorization exists across all post endpoints?\n- Is there a pattern for ownership checks?\n- Which other endpoints access posts?\n- Is there middleware handling this?\"\n```\n\n**Agent reports:**\n```\nFound:\n- DELETE /posts/:id - NO ownership check\n- PUT /posts/:id - NO ownership check\n- GET /posts/:id - NO ownership check (allows viewing private posts!)\n- No authorization middleware found\n- Similar pattern in comments, likes endpoints\n```\n\n**Phase 2 - Hypothesis:**\n\"Missing authorization layer. Need middleware checking resource ownership across ALL endpoints.\"\n\n**Phase 4 - Fix root cause:**\n```python\n# Add authorization middleware\nclass OwnershipMiddleware:\n    def check_ownership(self, resource, user):\n        if resource.author_id != user.id:\n            raise Forbidden()\n\n# Apply to all endpoints\n@app.delete(\"/posts/{post_id}\")\n@require_ownership(Post)\ndef delete_post(...):\n    ...\n\n@app.put(\"/posts/{post_id}\")\n@require_ownership(Post)\ndef update_post(...):\n    ...\n```\n\n**Result:** Root cause fixed, ALL endpoints secured, not just one symptom.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **Tools before fixes** → Never guess without investigation\n   - Use internet-researcher for errors\n   - Use debugger or instrumentation for state\n   - Use codebase-investigator for context\n\n2. **Evidence-based hypotheses** → Not guesses or hunches\n   - State what tools revealed\n   - Propose theory explaining evidence\n   - Make testable prediction\n\n3. **Test hypothesis before fixing** → Minimal change to validate\n   - Smallest change that tests theory\n   - Observe result\n   - If wrong, return to investigation\n\n4. **Fix root cause, not symptom** → One fix, many symptoms prevented\n   - Understand why problem occurred\n   - Fix the underlying issue\n   - Don't play whack-a-mole\n\n## Common Excuses\n\nAll of these mean: Stop, use tools to investigate:\n- \"The fix is obvious\"\n- \"I know what this is\"\n- \"Just a quick try\"\n- \"No time for debugging\"\n- \"Error message is clear enough\"\n- \"Internet search will take too long\"\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore proposing any fix:\n- [ ] Read complete error message (not just first line)\n- [ ] Dispatched internet-researcher for unclear errors\n- [ ] Used debugger or added instrumentation to inspect state\n- [ ] Dispatched codebase-investigator to understand context\n- [ ] Formed hypothesis based on evidence (not guesses)\n- [ ] Tested hypothesis with minimal change\n- [ ] Verified hypothesis confirmed before fixing\n\nBefore committing fix:\n- [ ] Written test reproducing bug (RED phase)\n- [ ] Verified test fails before fix\n- [ ] Implemented fix addressing root cause\n- [ ] Verified test passes after fix (GREEN phase)\n- [ ] Ran full test suite (regression check)\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- internet-researcher (search errors, known bugs, solutions)\n- codebase-investigator (understand code structure, find related code)\n- test-driven-development (write test for bug, implement fix)\n- test-runner (run tests without output pollution)\n\n**This skill is called by:**\n- fixing-bugs (complete bug fix workflow)\n- root-cause-tracing (deep debugging for complex issues)\n- Any skill when encountering unexpected behavior\n\n**Agents used:**\n- hyperpowers:internet-researcher (search for error solutions)\n- hyperpowers:codebase-investigator (understand codebase context)\n- hyperpowers:test-runner (run tests, return summary only)\n\n</integration>\n\n<resources>\n\n**Detailed guides:**\n- [Debugger reference](resources/debugger-reference.md) - LLDB, GDB, DevTools commands\n- [Debugging session example](resources/debugging-session-example.md) - Complete walkthrough\n\n**When stuck:**\n- Error unclear → Dispatch internet-researcher with exact error text\n- Don't understand code flow → Dispatch codebase-investigator\n- Need to inspect runtime state → Recommend debugger to user or add instrumentation\n- Tempted to guess → Stop, use tools to gather evidence first\n\n</resources>"
              },
              {
                "name": "dispatching-parallel-agents",
                "description": "Use when facing 3+ independent failures that can be investigated without shared state or dependencies - dispatches multiple Claude agents to investigate and fix independent problems concurrently",
                "path": "skills/dispatching-parallel-agents/SKILL.md",
                "frontmatter": {
                  "name": "dispatching-parallel-agents",
                  "description": "Use when facing 3+ independent failures that can be investigated without shared state or dependencies - dispatches multiple Claude agents to investigate and fix independent problems concurrently"
                },
                "content": "<skill_overview>\nWhen facing 3+ independent failures, dispatch one agent per problem domain to investigate concurrently; verify independence first, dispatch all in single message, wait for all agents, check conflicts, verify integration.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow the 6-step process (identify, create tasks, dispatch, monitor, review, verify) strictly. Independence verification mandatory. Parallel dispatch in single message required. Adapt agent prompt content to problem domain.\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | Critical Rule |\n|------|--------|---------------|\n| 1. Identify Domains | Test independence (fix A doesn't affect B) | 3+ independent domains required |\n| 2. Create Agent Tasks | Write focused prompts (scope, goal, constraints, output) | One prompt per domain |\n| 3. Dispatch Agents | Launch all agents in SINGLE message | Multiple Task() calls in parallel |\n| 4. Monitor Progress | Track completions, don't integrate until ALL done | Wait for all agents |\n| 5. Review Results | Read summaries, check conflicts | Manual conflict resolution |\n| 6. Verify Integration | Run full test suite | Use verification-before-completion |\n\n**Why 3+?** With only 2 failures, coordination overhead often exceeds sequential time.\n\n**Critical:** Dispatch all agents in single message with multiple Task() calls, or they run sequentially.\n</quick_reference>\n\n<when_to_use>\nUse when:\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n- You've verified failures are truly independent\n- Each domain has clear boundaries (different files, modules, features)\n\nDon't use when:\n- Failures are related (fix one might fix others)\n- Need to understand full system state first\n- Agents would interfere (editing same files)\n- Haven't verified independence yet (exploratory phase)\n- Failures share root cause (one bug, multiple symptoms)\n- Need to preserve investigation order (cascading failures)\n- Only 2 failures (overhead exceeds benefit)\n</when_to_use>\n\n<the_process>\n## Step 1: Identify Independent Domains\n\n**Announce:** \"I'm using hyperpowers:dispatching-parallel-agents to investigate these independent failures concurrently.\"\n\n**Create TodoWrite tracker:**\n```\n- Identify independent domains (3+ domains identified)\n- Create agent tasks (one prompt per domain drafted)\n- Dispatch agents in parallel (all agents launched in single message)\n- Monitor agent progress (track completions)\n- Review results (summaries read, conflicts checked)\n- Verify integration (full test suite green)\n```\n\n**Test for independence:**\n\n1. **Ask:** \"If I fix failure A, does it affect failure B?\"\n   - If NO → Independent\n   - If YES → Related, investigate together\n\n2. **Check:** \"Do failures touch same code/files?\"\n   - If NO → Likely independent\n   - If YES → Check if different functions/areas\n\n3. **Verify:** \"Do failures share error patterns?\"\n   - If NO → Independent\n   - If YES → Might be same root cause\n\n**Example independence check:**\n```\nFailure 1: Authentication tests failing (auth.test.ts)\nFailure 2: Database query tests failing (db.test.ts)\nFailure 3: API endpoint tests failing (api.test.ts)\n\nCheck: Does fixing auth affect db queries? NO\nCheck: Does fixing db affect API? YES - API uses db\n\nResult: 2 independent domains:\n  Domain 1: Authentication (auth.test.ts)\n  Domain 2: Database + API (db.test.ts + api.test.ts together)\n```\n\n**Group failures by what's broken:**\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\n---\n\n## Step 2: Create Focused Agent Tasks\n\nEach agent prompt must have:\n\n1. **Specific scope:** One test file or subsystem\n2. **Clear goal:** Make these tests pass\n3. **Constraints:** Don't change other code\n4. **Expected output:** Summary of what you found and fixed\n\n**Good agent prompt example:**\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nNever just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n**What makes this good:**\n- Specific test failures listed\n- Context provided (timing/race conditions)\n- Clear methodology (read, identify, fix)\n- Constraints (don't just increase timeouts)\n- Output format (summary)\n\n**Common mistakes:**\n\n❌ **Too broad:** \"Fix all the tests\" - agent gets lost\n✅ **Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n❌ **No context:** \"Fix the race condition\" - agent doesn't know where\n✅ **Context:** Paste the error messages and test names\n\n❌ **No constraints:** Agent might refactor everything\n✅ **Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n❌ **Vague output:** \"Fix it\" - you don't know what changed\n✅ **Specific:** \"Return summary of root cause and changes\"\n\n---\n\n## Step 3: Dispatch All Agents in Parallel\n\n**CRITICAL:** You must dispatch all agents in a SINGLE message with multiple Task() calls.\n\n```typescript\n// ✅ CORRECT - Single message with multiple parallel tasks\nTask(\"Fix agent-tool-abort.test.ts failures\", prompt1)\nTask(\"Fix batch-completion-behavior.test.ts failures\", prompt2)\nTask(\"Fix tool-approval-race-conditions.test.ts failures\", prompt3)\n// All three run concurrently\n\n// ❌ WRONG - Sequential messages\nTask(\"Fix agent-tool-abort.test.ts failures\", prompt1)\n// Wait for response\nTask(\"Fix batch-completion-behavior.test.ts failures\", prompt2)\n// This is sequential, not parallel!\n```\n\n**After dispatch:**\n- Mark \"Dispatch agents in parallel\" as completed in TodoWrite\n- Mark \"Monitor agent progress\" as in_progress\n- Wait for all agents to complete before integration\n\n---\n\n## Step 4: Monitor Progress\n\nAs agents work:\n- Note which agents have completed\n- Note which are still running\n- Don't start integration until ALL agents done\n\n**If an agent gets stuck (>5 minutes):**\n\n1. Check AgentOutput to see what it's doing\n2. If stuck on wrong path: Cancel and retry with clearer prompt\n3. If needs context from other domain: Wait for other agent, then restart with context\n4. If hit real blocker: Investigate blocker yourself, then retry\n\n---\n\n## Step 5: Review Results and Check Conflicts\n\n**When all agents return:**\n\n1. **Read each summary carefully**\n   - What was the root cause?\n   - What did the agent change?\n   - Were there any uncertainties?\n\n2. **Check for conflicts**\n   - Did multiple agents edit same files?\n   - Did agents make contradictory assumptions?\n   - Are there integration points between domains?\n\n3. **Integration strategy:**\n   - If no conflicts: Apply all changes\n   - If conflicts: Resolve manually before applying\n   - If assumptions conflict: Verify with user\n\n4. **Document what happened**\n   - Which agents fixed what\n   - Any conflicts found\n   - Integration decisions made\n\n---\n\n## Step 6: Verify Integration\n\n**Run full test suite:**\n- Not just the fixed tests\n- Verify no regressions in other areas\n- Use hyperpowers:verification-before-completion skill\n\n**Before completing:**\n```bash\n# Run all tests\nnpm test  # or cargo test, pytest, etc.\n\n# Verify output\n# If all pass → Mark \"Verify integration\" complete\n# If failures → Identify which agent's change caused regression\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer dispatches agents sequentially instead of in parallel</scenario>\n\n<code>\n# Developer sees 3 independent failures\n# Creates 3 agent prompts\n\n# Dispatches first agent\nTask(\"Fix agent-tool-abort.test.ts failures\", prompt1)\n# Waits for response from agent 1\n\n# Then dispatches second agent\nTask(\"Fix batch-completion-behavior.test.ts failures\", prompt2)\n# Waits for response from agent 2\n\n# Then dispatches third agent\nTask(\"Fix tool-approval-race-conditions.test.ts failures\", prompt3)\n\n# Total time: Sum of all three agents (sequential)\n</code>\n\n<why_it_fails>\n- Agents run sequentially, not in parallel\n- No time savings from parallelization\n- Each agent waits for previous to complete\n- Defeats entire purpose of parallel dispatch\n- Same result as sequential investigation\n- Wasted overhead of creating separate agents\n</why_it_fails>\n\n<correction>\n**Dispatch all agents in SINGLE message:**\n\n```typescript\n// Single message with multiple Task() calls\nTask(\"Fix agent-tool-abort.test.ts failures\", `\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n[prompt 1 content]\n`)\n\nTask(\"Fix batch-completion-behavior.test.ts failures\", `\nFix the 2 failing tests in src/agents/batch-completion-behavior.test.ts:\n[prompt 2 content]\n`)\n\nTask(\"Fix tool-approval-race-conditions.test.ts failures\", `\nFix the 1 failing test in src/agents/tool-approval-race-conditions.test.ts:\n[prompt 3 content]\n`)\n\n// All three run concurrently - THIS IS THE KEY\n```\n\n**What happens:**\n- All three agents start simultaneously\n- Each investigates independently\n- All complete in parallel\n- Total time: Max(agent1, agent2, agent3) instead of Sum\n\n**What you gain:**\n- True parallelization - 3 problems solved concurrently\n- Time saved: 3 investigations in time of 1\n- Each agent focused on narrow scope\n- No waiting for sequential completion\n- Proper use of parallel dispatch pattern\n</correction>\n</example>\n\n<example>\n<scenario>Developer assumes failures are independent without verification</scenario>\n\n<code>\n# Developer sees 3 test failures:\n# - API endpoint tests failing\n# - Database query tests failing\n# - Cache invalidation tests failing\n\n# Thinks: \"Different subsystems, must be independent\"\n\n# Dispatches 3 agents immediately without checking independence\n\n# Agent 1 finds: API failing because database schema changed\n# Agent 2 finds: Database queries need migration\n# Agent 3 finds: Cache keys based on old schema\n\n# All three failures caused by same root cause: schema change\n# Agents make conflicting fixes based on different assumptions\n# Integration fails because fixes contradict each other\n</code>\n\n<why_it_fails>\n- Skipped independence verification (Step 1)\n- Assumed independence based on surface appearance\n- All failures actually shared root cause (schema change)\n- Agents worked in isolation without seeing connection\n- Each agent made different assumptions about correct schema\n- Conflicting fixes can't be integrated\n- Wasted time on parallel work that should have been unified\n- Have to throw away agent work and start over\n</why_it_fails>\n\n<correction>\n**Run independence check FIRST:**\n\n```\nCheck: Does fixing API affect database queries?\n- API uses database\n- If database schema changes, API breaks\n- YES - these are related\n\nCheck: Does fixing database affect cache?\n- Cache stores database results\n- If database schema changes, cache keys break\n- YES - these are related\n\nCheck: Do failures share error patterns?\n- All mention \"column not found: user_email\"\n- All started after schema migration\n- YES - shared root cause\n\nResult: NOT INDEPENDENT\nThese are one problem (schema change) manifesting in 3 places\n```\n\n**Correct approach:**\n\n```\nSingle agent investigates: \"Schema migration broke 3 subsystems\"\n\nAgent prompt:\n\"We have 3 test failures all related to schema change:\n1. API endpoints: column not found\n2. Database queries: column not found\n3. Cache invalidation: old keys\n\nInvestigate the schema migration that caused this.\nFix by updating all 3 subsystems consistently.\nReturn: What changed in schema, how you fixed each subsystem.\"\n\n# One agent sees full picture\n# Makes consistent fix across all 3 areas\n# No conflicts, proper integration\n```\n\n**What you gain:**\n- Caught shared root cause before wasting time\n- One agent sees full context\n- Consistent fix across all affected areas\n- No conflicting assumptions\n- No integration conflicts\n- Faster than 3 agents working at cross-purposes\n- Proper problem diagnosis before parallel dispatch\n</correction>\n</example>\n\n<example>\n<scenario>Developer integrates agent results without checking conflicts</scenario>\n\n<code>\n# 3 agents complete successfully\n# Developer quickly reads summaries:\n\nAgent 1: \"Fixed timeout issue by increasing wait time to 5000ms\"\nAgent 2: \"Fixed race condition by adding mutex lock\"\nAgent 3: \"Fixed timing issue by reducing wait time to 1000ms\"\n\n# Developer thinks: \"All agents succeeded, ship it\"\n\n# Applies all changes without checking conflicts\n\n# Result:\n# - Agent 1 and Agent 3 edited same file\n# - Agent 1 increased timeout, Agent 3 decreased it\n# - Final code has inconsistent timeouts\n# - Agent 2's mutex interacts badly with Agent 3's reduced timeout\n# - Tests still fail after integration\n</code>\n\n<why_it_fails>\n- Skipped conflict checking (Step 5)\n- Didn't carefully read what each agent changed\n- Agents made contradictory decisions\n- Agent 1 and Agent 3 had different assumptions about timing\n- Agent 2's locking interacts with timing changes\n- Blindly applying all fixes creates inconsistent state\n- Tests fail after \"successful\" integration\n- Have to manually untangle conflicting changes\n</why_it_fails>\n\n<correction>\n**Review results carefully before integration:**\n\n```markdown\n## Agent Summaries Review\n\nAgent 1: Fixed timeout issue by increasing wait time to 5000ms\n- File: src/agents/tool-executor.ts\n- Change: DEFAULT_TIMEOUT = 5000\n\nAgent 2: Fixed race condition by adding mutex lock\n- File: src/agents/tool-executor.ts\n- Change: Added mutex around tool execution\n\nAgent 3: Fixed timing issue by reducing wait time to 1000ms\n- File: src/agents/tool-executor.ts\n- Change: DEFAULT_TIMEOUT = 1000\n\n## Conflict Analysis\n\n**CONFLICT DETECTED:**\n- Agents 1 and 3 edited same file (tool-executor.ts)\n- Agents 1 and 3 changed same constant (DEFAULT_TIMEOUT)\n- Agent 1: increase to 5000ms\n- Agent 3: decrease to 1000ms\n- Contradictory assumptions about correct timing\n\n**Why conflict occurred:**\n- Domains weren't actually independent (same timeout constant)\n- Both agents tested locally, didn't see interaction\n- Different problem spaces led to different timing needs\n\n## Resolution\n\n**Option 1:** Different timeouts for different operations\n```typescript\nconst TOOL_EXECUTION_TIMEOUT = 5000  // Agent 1's need\nconst TOOL_APPROVAL_TIMEOUT = 1000   // Agent 3's need\n```\n\n**Option 2:** Investigate why timing varies\n- Maybe Agent 1's tests are actually slow (fix slowness)\n- Maybe Agent 3's tests are correct (use 1000ms everywhere)\n\n**Choose Option 2 after investigation:**\n- Agent 1's tests were slow due to unrelated issue\n- Fix the slowness, use 1000ms timeout everywhere\n- Agent 2's mutex is compatible with 1000ms\n\n**Integration steps:**\n1. Apply Agent 2's mutex (no conflict)\n2. Apply Agent 3's 1000ms timeout\n3. Fix Agent 1's slow tests (root cause)\n4. Don't apply Agent 1's timeout increase (symptom fix)\n```\n\n**Run full test suite:**\n```bash\nnpm test\n# All tests pass ✅\n```\n\n**What you gain:**\n- Caught contradiction before breaking integration\n- Understood why agents made different decisions\n- Resolved conflict thoughtfully, not arbitrarily\n- Fixed root cause (slow tests) not symptom (long timeout)\n- Verified integration works correctly\n- Avoided shipping inconsistent code\n- Professional conflict resolution process\n</correction>\n</example>\n</examples>\n\n<failure_modes>\n## Agent Gets Stuck\n\n**Symptoms:** No progress after 5+ minutes\n\n**Causes:**\n- Prompt too vague, agent exploring aimlessly\n- Domain not actually independent, needs context from other agents\n- Agent hit a blocker (missing file, unclear error)\n\n**Recovery:**\n1. Use AgentOutput tool to check what it's doing\n2. If stuck on wrong path: Cancel and retry with clearer prompt\n3. If needs context from other domain: Wait for other agent, then restart with context\n4. If hit real blocker: Investigate blocker yourself, then retry\n\n---\n\n## Agents Return Conflicting Fixes\n\n**Symptoms:** Agents edited same code differently, or made contradictory assumptions\n\n**Causes:**\n- Domains weren't actually independent\n- Shared code between domains\n- Agents made different assumptions about correct behavior\n\n**Recovery:**\n1. Don't apply either fix automatically\n2. Read both fixes carefully\n3. Identify the conflict point\n4. Resolve manually based on which assumption is correct\n5. Consider if domains should be merged\n\n---\n\n## Integration Breaks Other Tests\n\n**Symptoms:** Fixed tests pass, but other tests now fail\n\n**Causes:**\n- Agent changed shared code\n- Agent's fix was too broad\n- Agent misunderstood requirements\n\n**Recovery:**\n1. Identify which agent's change caused the regression\n2. Read the agent's summary - did they mention this change?\n3. Evaluate if change is correct but tests need updating\n4. Or if change broke something, need to refine the fix\n5. Use hyperpowers:verification-before-completion skill for final check\n\n---\n\n## False Independence\n\n**Symptoms:** Fixing one domain revealed it affected another\n\n**Recovery:**\n1. Merge the domains\n2. Have one agent investigate both together\n3. Learn: Better independence test needed upfront\n</failure_modes>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Verify independence first** → Test with questions before dispatching\n2. **3+ domains required** → 2 failures: overhead exceeds benefit, do sequentially\n3. **Single message dispatch** → All agents in one message with multiple Task() calls\n4. **Wait for ALL agents** → Don't integrate until all complete\n5. **Check conflicts manually** → Read summaries, verify no contradictions\n6. **Verify integration** → Run full suite yourself, don't trust agents\n7. **TodoWrite tracking** → Track agent progress explicitly\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow the process.**\n\n- \"Just 2 failures, can still parallelize\" (Overhead exceeds benefit, do sequentially)\n- \"Probably independent, will dispatch and see\" (Verify independence FIRST)\n- \"Can dispatch sequentially to save syntax\" (WRONG - must dispatch in single message)\n- \"Agent failed, but others succeeded - ship it\" (All agents must succeed or re-investigate)\n- \"Conflicts are minor, can ignore\" (Resolve all conflicts explicitly)\n- \"Don't need TodoWrite for just tracking agents\" (Use TodoWrite, track properly)\n- \"Can skip verification, agents ran tests\" (Agents can make mistakes, YOU verify)\n</critical_rules>\n\n<verification_checklist>\nBefore completing parallel agent work:\n\n- [ ] Verified independence with 3 questions (fix A affects B? same code? same error pattern?)\n- [ ] 3+ independent domains identified (not 2 or fewer)\n- [ ] Created focused agent prompts (scope, goal, constraints, output)\n- [ ] Dispatched all agents in single message (multiple Task() calls)\n- [ ] Waited for ALL agents to complete (didn't integrate early)\n- [ ] Read all agent summaries carefully\n- [ ] Checked for conflicts (same files, contradictory assumptions)\n- [ ] Resolved any conflicts manually before integration\n- [ ] Ran full test suite (not just fixed tests)\n- [ ] Used verification-before-completion skill\n- [ ] Documented which agents fixed what\n\n**Can't check all boxes?** Return to the process and complete missing steps.\n</verification_checklist>\n\n<integration>\n**This skill covers:** Parallel investigation of independent failures\n\n**Related skills:**\n- hyperpowers:debugging-with-tools (how to investigate individual failures)\n- hyperpowers:fixing-bugs (complete bug workflow)\n- hyperpowers:verification-before-completion (verify integration)\n- hyperpowers:test-runner (run tests without context pollution)\n\n**This skill uses:**\n- Task tool (dispatch parallel agents)\n- AgentOutput tool (monitor stuck agents)\n- TodoWrite (track agent progress)\n\n**Workflow integration:**\n```\nMultiple independent failures\n    ↓\nVerify independence (Step 1)\n    ↓\nCreate agent tasks (Step 2)\n    ↓\nDispatch in parallel (Step 3)\n    ↓\nMonitor progress (Step 4)\n    ↓\nReview + check conflicts (Step 5)\n    ↓\nVerify integration (Step 6)\n    ↓\nhyperpowers:verification-before-completion\n```\n\n**Real example from session (2025-10-03):**\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes\n- Time saved: 3 problems solved in parallel vs sequentially\n</integration>\n\n<resources>\n**Key principles:**\n- Parallelization only wins with 3+ independent problems\n- Independence verification prevents wasted parallel work\n- Single message dispatch is critical for true parallelism\n- Conflict checking prevents integration disasters\n- Full verification catches agent mistakes\n\n**When stuck:**\n- Agent not making progress → Check AgentOutput, retry with clearer prompt\n- Conflicts after dispatch → Domains weren't independent, merge and retry\n- Integration fails tests → Identify which agent caused regression\n- Unclear if independent → Test with 3 questions (affects? same code? same error?)\n</resources>"
              },
              {
                "name": "executing-plans",
                "description": "Use to execute bd tasks iteratively - executes one task, reviews learnings, creates/refines next task, then STOPS for user review before continuing",
                "path": "skills/executing-plans/SKILL.md",
                "frontmatter": {
                  "name": "executing-plans",
                  "description": "Use to execute bd tasks iteratively - executes one task, reviews learnings, creates/refines next task, then STOPS for user review before continuing"
                },
                "content": "<skill_overview>\nExecute bd tasks one at a time with mandatory checkpoints: Load epic → Execute task → Review learnings → Create next task → Run SRE refinement → STOP. User clears context, reviews implementation, then runs command again to continue. Epic requirements are immutable, tasks adapt to reality.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow exact process: load epic, execute ONE task, review, create next task with SRE refinement, STOP.\n\nEpic requirements are immutable. Tasks adapt to discoveries. Do not skip checkpoints, SRE refinement, or verification. STOP after each task for user review.\n</rigidity_level>\n\n<quick_reference>\n\n| Step | Command | Purpose |\n|------|---------|---------|\n| **Load Epic** | `bd show bd-1` | Read immutable requirements once at start |\n| **Find Task** | `bd ready` | Get next ready task to execute |\n| **Start Task** | `bd update bd-2 --status in_progress` | Mark task active |\n| **Track Substeps** | TodoWrite for each implementation step | Prevent incomplete execution |\n| **Close Task** | `bd close bd-2` | Mark task complete after verification |\n| **Review** | Re-read epic, check learnings | Adapt next task to reality |\n| **Create Next** | `bd create \"Task N\"` | Based on learnings, not assumptions |\n| **Refine** | Use `sre-task-refinement` skill | Corner-case analysis with Opus 4.1 |\n| **STOP** | Present summary to user | User reviews, clears context, runs command again |\n| **Final Check** | Use `review-implementation` skill | Verify all success criteria before closing epic |\n\n**Critical:** Epic = contract (immutable). Tasks = discovery (adapt to reality). STOP after each task for user review.\n\n</quick_reference>\n\n<when_to_use>\n**Use after hyperpowers:writing-plans creates epic and first task.**\n\nSymptoms you need this:\n- bd epic exists with tasks ready to execute\n- Need to implement features iteratively\n- Requirements clear, but implementation path will adapt\n- Want continuous learning between tasks\n</when_to_use>\n\n<the_process>\n\n## 0. Resumption Check (Every Invocation)\n\nThis skill supports explicit resumption. When invoked:\n\n```bash\nbd list --type epic --status open  # Find active epic\nbd ready                           # Check for ready tasks\nbd list --status in_progress       # Check for in-progress tasks\n```\n\n**Fresh start:** No in-progress tasks, proceed to Step 1.\n\n**Resuming:** Found ready or in-progress tasks:\n- In-progress task exists → Resume at Step 2 (continue executing)\n- Ready task exists → Resume at Step 2 (start executing)\n- All tasks closed but epic open → Resume at Step 4 (check criteria)\n\n**Why resumption matters:**\n- User cleared context between tasks (intended workflow)\n- Context limit reached mid-task\n- Previous session ended unexpectedly\n\n**Do not ask \"where did we leave off?\"** - bd state tells you exactly where to resume.\n\n## 1. Load Epic Context (Once at Start)\n\nBefore executing ANY task, load the epic into context:\n\n```bash\nbd list --type epic --status open  # Find epic\nbd show bd-1                       # Load epic details\n```\n\n**Extract and keep in mind:**\n- Requirements (IMMUTABLE)\n- Success criteria (validation checklist)\n- Anti-patterns (FORBIDDEN shortcuts)\n- Approach (high-level strategy)\n\n**Why:** Requirements prevent watering down when blocked.\n\n## 2. Execute Current Ready Task\n\n```bash\nbd ready                           # Find next task\nbd update bd-2 --status in_progress # Start it\nbd show bd-2                       # Read details\n```\n\n**CRITICAL - Create TodoWrite for ALL substeps:**\n\nTasks contain 4-8 implementation steps. Create TodoWrite todos for each to prevent incomplete execution:\n\n```\n- bd-2 Step 1: Write test (pending)\n- bd-2 Step 2: Run test RED (pending)\n- bd-2 Step 3: Implement function (pending)\n- bd-2 Step 4: Run test GREEN (pending)\n- bd-2 Step 5: Refactor (pending)\n- bd-2 Step 6: Commit (pending)\n```\n\n**Execute steps:**\n- Use `test-driven-development` when implementing features\n- Mark each substep completed immediately after finishing\n- Use `test-runner` agent for verifications\n\n**Pre-close verification:**\n- Check TodoWrite: All substeps completed?\n- If incomplete: Continue with remaining substeps\n- If complete: Close task and commit\n\n```bash\nbd close bd-2  # After ALL substeps done\n```\n\n## 3. Review Against Epic and Create Next Task\n\n**CRITICAL:** After each task, adapt plan based on reality.\n\n**Review questions:**\n1. What did we learn?\n2. Discovered any blockers, existing functionality, limitations?\n3. Does this move us toward epic success criteria?\n4. What's next logical step?\n5. Any epic anti-patterns to avoid?\n\n**Re-read epic:**\n```bash\nbd show bd-1  # Keep requirements fresh\n```\n\n**Three cases:**\n\n**A) Next task still valid** → Proceed to Step 2\n\n**B) Next task now redundant** (plan invalidation allowed):\n```bash\nbd delete bd-4  # Remove wasteful task\n# Or update: bd update bd-4 --title \"New work\" --design \"...\"\n```\n\n**C) Need new task** based on learnings:\n```bash\nbd create \"Task N: [Next Step Based on Reality]\" \\\n  --type feature \\\n  --design \"## Goal\n[Deliverable based on what we learned]\n\n## Context\nCompleted bd-2: [discoveries]\n\n## Implementation\n[Steps reflecting current state, not assumptions]\n\n## Success Criteria\n- [ ] Specific outcomes\n- [ ] Tests passing\"\n\nbd dep add bd-N bd-1 --type parent-child\nbd dep add bd-N bd-2 --type blocks\n```\n\n**REQUIRED - Run SRE refinement on new task:**\n```\nUse Skill tool: hyperpowers:sre-task-refinement\n```\n\nSRE refinement will:\n- Apply 7-category corner-case analysis (Opus 4.1)\n- Identify edge cases and failure modes\n- Strengthen success criteria\n- Ensure task is ready for implementation\n\n**Do NOT skip SRE refinement.** New tasks need the same rigor as initial planning.\n\n## 4. Check Epic Success Criteria and STOP\n\n```bash\nbd show bd-1  # Check success criteria\n```\n\n- ALL criteria met? → Step 5 (final validation)\n- Some missing? → **STOP for user review**\n\n## 4a. STOP Checkpoint (Mandatory)\n\n**Present summary to user:**\n\n```markdown\n## Task bd-N Complete - Checkpoint\n\n### What Was Done\n- [Summary of implementation]\n- [Key learnings/discoveries]\n\n### Next Task Ready\n- bd-M: [Title]\n- [Brief description of what's next]\n\n### Epic Progress\n- [X/Y success criteria met]\n- [Remaining criteria]\n\n### To Continue\nRun `/hyperpowers:execute-plan` to execute the next task.\n```\n\n**Why STOP is mandatory:**\n- User can clear context (prevents context exhaustion)\n- User can review implementation before next task\n- User can adjust next task if needed\n- Prevents runaway execution without oversight\n\n**Do NOT rationalize skipping the stop:**\n- \"Good context loaded\" → Context reloads are cheap, wrong decisions aren't\n- \"Momentum\" → Checkpoints ensure quality over speed\n- \"User didn't ask to stop\" → Stopping is the default, continuing requires explicit command\n\n## 5. Final Validation and Closure\n\nWhen all success criteria appear met:\n\n1. **Run full verification** (tests, hooks, manual checks)\n\n2. **REQUIRED - Use review-implementation skill:**\n```\nUse Skill tool: hyperpowers:review-implementation\n```\n\nReview-implementation will:\n- Check each requirement met\n- Verify each success criterion satisfied\n- Confirm no anti-patterns used\n- If approved: Calls `finishing-a-development-branch`\n- If gaps: Create tasks, return to Step 2\n\n3. **Only close epic after review approves**\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer closes task without completing all substeps, claims \"mostly done\"</scenario>\n\n<code>\nbd-2 has 6 implementation steps.\n\nTodoWrite shows:\n- ✅ bd-2 Step 1: Write test\n- ✅ bd-2 Step 2: Run test RED\n- ✅ bd-2 Step 3: Implement function\n- ⏸️ bd-2 Step 4: Run test GREEN (pending)\n- ⏸️ bd-2 Step 5: Refactor (pending)\n- ⏸️ bd-2 Step 6: Commit (pending)\n\nDeveloper thinks: \"Function works, I'll close bd-2 and move on\"\nRuns: bd close bd-2\n</code>\n\n<why_it_fails>\nSteps 4-6 skipped:\n- Tests not verified GREEN (might have broken other tests)\n- Code not refactored (leaves technical debt)\n- Changes not committed (work could be lost)\n\n\"Mostly done\" = incomplete task = will cause issues later.\n</why_it_fails>\n\n<correction>\n**Pre-close verification checkpoint:**\n\nBefore closing ANY task:\n1. Check TodoWrite: All substeps completed?\n2. If incomplete: Continue with remaining substeps\n3. Only when ALL ✅: bd close bd-2\n\n**Result:** Task actually complete, tests passing, code committed.\n</correction>\n</example>\n\n<example>\n<scenario>Developer discovers planned task is redundant, executes it anyway \"because it's in the plan\"</scenario>\n\n<code>\nbd-4 says: \"Implement token refresh middleware\"\n\nWhile executing bd-2, developer discovers:\n- Token refresh middleware already exists in auth/middleware/refresh.ts\n- Works correctly, has tests\n- bd-4 would duplicate existing code\n\nDeveloper thinks: \"bd-4 is in the plan, I should do it anyway\"\nProceeds to implement duplicate middleware\n</code>\n\n<why_it_fails>\n**Wasteful execution:**\n- Duplicates existing functionality\n- Creates maintenance burden (two implementations to keep in sync)\n- Violates DRY principle\n- Wastes time on redundant work\n\n**Why it happens:** Treating tasks as immutable instead of epic.\n</why_it_fails>\n\n<correction>\n**Plan invalidation is allowed:**\n\n1. Verify the discovery:\n```bash\n# Check existing code\ncat auth/middleware/refresh.ts\n# Confirm it works\nnpm test -- refresh.spec.ts\n```\n\n2. Delete redundant task:\n```bash\nbd delete bd-4\n```\n\n3. Document why:\n```\nbd update bd-2 --design \"...\n\nDiscovery: Token refresh middleware already exists (auth/middleware/refresh.ts).\nVerified working with tests. bd-4 deleted as redundant.\"\n```\n\n4. Create new task if needed (maybe \"Integrate existing refresh middleware\" instead)\n\n**Result:** Plan adapts to reality. No wasted work.\n</correction>\n</example>\n\n<example>\n<scenario>Developer hits blocker, waters down epic requirement to \"make it easier\"</scenario>\n\n<code>\nEpic bd-1 anti-patterns say:\n\"FORBIDDEN: Using mocks for database integration tests. Must use real test database.\"\n\nDeveloper encounters:\n- Real database setup is complex\n- Mocking would make tests pass quickly\n\nDeveloper thinks: \"This is too hard, I'll use mocks just for now and refactor later\"\n\nAdds TODO: // TODO: Replace mocks with real DB later\n</code>\n\n<why_it_fails>\n**Violates epic anti-pattern:**\n- Epic explicitly forbids mocks for integration tests\n- \"Later\" never happens (TODO remains forever)\n- Tests don't verify actual integration\n- Defeats purpose of integration testing\n\n**Why it happens:** Rationalizing around blockers instead of solving them.\n</why_it_fails>\n\n<correction>\n**When blocked, re-read epic:**\n\n1. Re-read epic requirements and anti-patterns:\n```bash\nbd show bd-1\n```\n\n2. Check if solution violates anti-pattern:\n- Using mocks? YES, explicitly forbidden\n\n3. Don't rationalize. Instead:\n\n**Option A - Research:**\n```bash\nbd create \"Research: Real DB test setup for [project]\" \\\n  --design \"Find how this project sets up test databases.\nCheck existing test files for patterns.\nDocument setup process that meets anti-pattern requirements.\"\n```\n\n**Option B - Ask user:**\n\"Blocker: Test DB setup complex. Epic forbids mocks for integration.\nIs there existing test DB infrastructure I should use?\"\n\n**Result:** Epic requirements maintained. Blocker solved properly.\n</correction>\n</example>\n\n<example>\n<scenario>Developer skips STOP checkpoint to \"maintain momentum\"</scenario>\n\n<code>\nJust completed bd-2 (authentication middleware).\nCreated bd-3 (rate limiting endpoint).\nRan SRE refinement on bd-3.\n\nDeveloper thinks: \"Good context loaded, I'll just do bd-3 quickly then stop.\nUser approved the epic, they trust me to execute it.\nStopping now is inefficient.\"\n\nContinues directly to execute bd-3 without STOP checkpoint.\n</code>\n\n<why_it_fails>\n**Multiple failures:**\n- User can't review bd-2 implementation before bd-3 starts\n- User can't clear context (may hit context limit mid-task)\n- User can't adjust bd-3 based on bd-2 learnings\n- No checkpoint = no oversight\n\n**The rationalization trap:**\n- \"Good context\" sounds efficient but prevents review\n- \"User trust\" misinterprets approval (one command ≠ blanket permission)\n- \"Quick task\" becomes long task when issues arise\n\n**What actually happens:**\n- bd-3 hits unexpected issue\n- Context exhausted trying to debug\n- User returns to find 2 half-finished tasks instead of 1 complete task\n</why_it_fails>\n\n<correction>\n**Follow the STOP checkpoint:**\n\n1. After completing bd-2 and refining bd-3:\n```markdown\n## Task bd-2 Complete - Checkpoint\n\n### What Was Done\n- Implemented JWT middleware with validation\n- Added token refresh handling\n\n### Next Task Ready\n- bd-3: Implement rate limiting\n- Adds rate limiting to auth endpoints\n\n### Epic Progress\n- 2/4 success criteria met\n- Remaining: password reset, rate limiting\n\n### To Continue\nRun `/hyperpowers:execute-plan` to execute the next task.\n```\n\n2. **STOP and wait for user**\n\n**Result:** User can review, clear context, adjust next task. Each task completes with full oversight.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **STOP after each task** → Present summary, wait for user to run command again\n   - User needs checkpoint to review implementation\n   - User may need to clear context\n   - Continuous execution = no oversight\n\n2. **SRE refinement for new tasks** → Never skip corner-case analysis\n   - New tasks created during execution need same rigor as initial planning\n   - Use Opus 4.1 for thorough analysis\n   - Tasks without refinement will miss edge cases\n\n3. **Epic requirements are immutable** → Never water down when blocked\n   - If blocked: Research solution or ask user\n   - Never violate anti-patterns to \"make it easier\"\n\n4. **All substeps must be completed** → Never close task with pending substeps\n   - Check TodoWrite before closing\n   - \"Mostly done\" = incomplete = will cause issues\n\n5. **Plan invalidation is allowed** → Delete redundant tasks\n   - If discovered existing functionality: Delete duplicate task\n   - If discovered blocker: Update or delete invalid task\n   - Document what you found and why\n\n6. **Review before closing epic** → Use review-implementation skill\n   - Tasks done ≠ success criteria met\n   - All criteria must be verified before closing\n\n## Common Excuses\n\nAll of these mean: Re-read epic, STOP as required, ask for help:\n- \"Good context loaded, don't want to lose it\" → STOP anyway, context reloads\n- \"Just one more quick task\" → STOP anyway, user needs checkpoint\n- \"User didn't ask me to stop\" → Stopping is default, continuing requires explicit command\n- \"SRE refinement is overkill for this task\" → Every task needs refinement, no exceptions\n- \"This requirement is too hard\" → Research or ask, don't water down\n- \"I'll come back to this later\" → Complete now or document why blocked\n- \"Let me fake this to make tests pass\" → Never, defeats purpose\n- \"Existing task is wasteful, but it's planned\" → Delete it, plan adapts to reality\n- \"All tasks done, epic must be complete\" → Verify with review-implementation\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore closing each task:\n- [ ] ALL TodoWrite substeps completed (no pending)\n- [ ] Tests passing (use test-runner agent)\n- [ ] Changes committed\n- [ ] Task actually done (not \"mostly\")\n\nAfter closing each task:\n- [ ] Reviewed learnings against epic\n- [ ] Created/updated next task based on reality\n- [ ] Ran SRE refinement on any new tasks\n- [ ] Presented STOP checkpoint summary to user\n- [ ] STOPPED execution (do not continue to next task)\n\nBefore closing epic:\n- [ ] ALL success criteria met (check epic)\n- [ ] review-implementation skill used and approved\n- [ ] No anti-patterns violated\n- [ ] All tasks closed\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- writing-plans (creates epic and first task before this runs)\n- sre-task-refinement (REQUIRED for new tasks created during execution)\n- test-driven-development (when implementing features)\n- test-runner (for running tests without output pollution)\n- review-implementation (final validation before closing epic)\n- finishing-a-development-branch (after review approves)\n\n**This skill is called by:**\n- User (via /hyperpowers:execute-plan command)\n- After writing-plans creates epic\n- Explicitly to resume after checkpoint (user runs command again)\n\n**Agents used:**\n- hyperpowers:test-runner (run tests, return summary only)\n\n**Workflow pattern:**\n```\n/hyperpowers:execute-plan → Execute task → STOP\n[User clears context, reviews]\n/hyperpowers:execute-plan → Execute next task → STOP\n[Repeat until epic complete]\n```\n\n</integration>\n\n<resources>\n\n**bd command reference:**\n- See [bd commands](../common-patterns/bd-commands.md) for complete command list\n\n**When stuck:**\n- Hit blocker → Re-read epic, check anti-patterns, research or ask\n- Don't understand instruction → Stop and ask (never guess)\n- Verification fails repeatedly → Check epic anti-patterns, ask for help\n- Tempted to skip steps → Check TodoWrite, complete all substeps\n\n</resources>"
              },
              {
                "name": "finishing-a-development-branch",
                "description": "Use when implementation complete and tests pass - closes bd epic, presents integration options (merge/PR/keep/discard), executes choice",
                "path": "skills/finishing-a-development-branch/SKILL.md",
                "frontmatter": {
                  "name": "finishing-a-development-branch",
                  "description": "Use when implementation complete and tests pass - closes bd epic, presents integration options (merge/PR/keep/discard), executes choice"
                },
                "content": "<skill_overview>\nClose bd epic, verify tests pass, present 4 integration options, execute choice, cleanup worktree appropriately.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow the 6-step process exactly. Present exactly 4 options. Never skip test verification. Must confirm before discarding.\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | If Blocked |\n|------|--------|------------|\n| 1 | Close bd epic | Tasks still open → STOP |\n| 2 | Verify tests pass (test-runner agent) | Tests fail → STOP |\n| 3 | Determine base branch | Ask if needed |\n| 4 | Present exactly 4 options | Wait for choice |\n| 5 | Execute choice | Follow option workflow |\n| 6 | Cleanup worktree (options 1,2,4 only) | Option 3 keeps worktree |\n\n**Options:** 1=Merge locally, 2=PR, 3=Keep as-is, 4=Discard (confirm)\n</quick_reference>\n\n<when_to_use>\n- Implementation complete and reviewed\n- All bd tasks for epic are done\n- Ready to integrate work back to main branch\n- Called by hyperpowers:review-implementation (final step)\n\n**Don't use for:**\n- Work still in progress\n- Tests failing\n- Epic has open tasks\n- Mid-implementation (use hyperpowers:executing-plans)\n</when_to_use>\n\n<the_process>\n## Step 1: Close bd Epic\n\n**Announce:** \"I'm using hyperpowers:finishing-a-development-branch to complete this work.\"\n\n**Verify all tasks closed:**\n\n```bash\nbd dep tree bd-1  # Show task tree\nbd list --status open --parent bd-1  # Check for open tasks\n```\n\n**If any tasks still open:**\n```\nCannot close epic bd-1: N tasks still open:\n- bd-3: Task Name (status: in_progress)\n- bd-5: Task Name (status: open)\n\nComplete all tasks before finishing.\n```\n\n**STOP. Do not proceed.**\n\n**If all tasks closed:**\n\n```bash\nbd close bd-1\n```\n\n---\n\n## Step 2: Verify Tests\n\n**IMPORTANT:** Use hyperpowers:test-runner agent to avoid context pollution.\n\nDispatch hyperpowers:test-runner agent:\n```\nRun: cargo test\n(or: npm test / pytest / go test ./...)\n```\n\nAgent returns summary + failures only.\n\n**If tests fail:**\n```\nTests failing (N failures). Must fix before completing:\n\n[Show failures]\n\nCannot proceed until tests pass.\n```\n\n**STOP. Do not proceed.**\n\n**If tests pass:** Continue to Step 3.\n\n---\n\n## Step 3: Determine Base Branch\n\n```bash\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null\n```\n\nOr ask: \"This branch split from main - is that correct?\"\n\n---\n\n## Step 4: Present Options\n\nPresent exactly these 4 options:\n\n```\nImplementation complete. What would you like to do?\n\n1. Merge back to <base-branch> locally\n2. Push and create a Pull Request\n3. Keep the branch as-is (I'll handle it later)\n4. Discard this work\n\nWhich option?\n```\n\n**Don't add explanation.** Keep concise.\n\n---\n\n## Step 5: Execute Choice\n\n### Option 1: Merge Locally\n\n```bash\ngit checkout <base-branch>\ngit pull\ngit merge <feature-branch>\n\n# Verify tests on merged result\nDispatch hyperpowers:test-runner: \"Run: <test command>\"\n\n# If tests pass\ngit branch -d <feature-branch>\n```\n\nThen: Step 6 (cleanup worktree)\n\n---\n\n### Option 2: Push and Create PR\n\n**Get epic info:**\n\n```bash\nbd show bd-1\nbd dep tree bd-1\n```\n\n**Create PR:**\n\n```bash\ngit push -u origin <feature-branch>\n\ngh pr create --title \"feat: <epic-name>\" --body \"$(cat <<'EOF'\n## Epic\n\nCloses bd-<N>: <Epic Title>\n\n## Summary\n<2-3 bullets from epic implementation>\n\n## Tasks Completed\n- bd-2: <Task Name>\n- bd-3: <Task Name>\n\n## Test Plan\n- [ ] All tests passing\n- [ ] <verification steps from epic>\nEOF\n)\"\n```\n\nThen: Step 6 (cleanup worktree)\n\n---\n\n### Option 3: Keep As-Is\n\nReport: \"Keeping branch <name>. Worktree preserved at <path>.\"\n\n**Don't cleanup worktree.**\n\n---\n\n### Option 4: Discard\n\n**Confirm first:**\n\n```\nThis will permanently delete:\n- Branch <name>\n- All commits: <commit-list>\n- Worktree at <path>\n\nType 'discard' to confirm.\n```\n\nWait for exact \"discard\" confirmation.\n\n**If confirmed:**\n\n```bash\ngit checkout <base-branch>\ngit branch -D <feature-branch>\n```\n\nThen: Step 6 (cleanup worktree)\n\n---\n\n## Step 6: Cleanup Worktree\n\n**For Options 1, 2, 4 only:**\n\n```bash\n# Check if in worktree\ngit worktree list | grep $(git branch --show-current)\n\n# If yes\ngit worktree remove <worktree-path>\n```\n\n**For Option 3:** Keep worktree (don't cleanup).\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer skips test verification before presenting options</scenario>\n\n<code>\n# Step 1: Epic closed ✓\nbd close bd-1\n\n# Step 2: SKIPPED test verification\n# Jump directly to presenting options\n\n\"Implementation complete. What would you like to do?\n1. Merge back to main locally\n2. Push and create PR\n...\"\n\nUser selects Option 1\n\ngit checkout main\ngit merge feature-branch\n# Tests fail! Broken code now on main\n</code>\n\n<why_it_fails>\n- Skipped mandatory test verification\n- Merged broken code to main branch\n- Other developers pull broken main\n- CI/CD fails, blocks deployment\n- Must revert, fix, merge again (wasted time)\n</why_it_fails>\n\n<correction>\n**Follow Step 2 strictly:**\n\n```bash\n# After closing epic\nbd close bd-1 ✓\n\n# MANDATORY: Verify tests BEFORE presenting options\nDispatch hyperpowers:test-runner agent: \"Run: cargo test\"\n\n# Agent reports\n\"Test suite passed (127 tests, 0 failures, 2.3s)\"\n\n# NOW present options\n\"Implementation complete. What would you like to do?\n1. Merge back to main locally\n...\"\n```\n\n**What you gain:**\n- Confidence tests pass before integration\n- No broken code merged to main\n- CI/CD stays green\n- Other developers unblocked\n- Professional workflow\n</correction>\n</example>\n\n<example>\n<scenario>Developer auto-cleans worktree for PR option</scenario>\n\n<code>\n# User selects Option 2: Create PR\ngit push -u origin feature-auth\ngh pr create --title \"feat: Add OAuth\" --body \"...\"\n\n# Developer immediately cleans up worktree\ngit worktree remove ../feature-auth-worktree\n\n# PR gets feedback: \"Please add rate limiting\"\n# User: \"Can you address the PR feedback?\"\n# Worktree is gone! Have to recreate it\ngit worktree add ../feature-auth-worktree feature-auth\n# Lost local state, uncommitted experiments, etc.\n</code>\n\n<why_it_fails>\n- Cleaned worktree when PR still active\n- User likely needs worktree for PR feedback\n- Have to recreate worktree for changes\n- Lost any local uncommitted work\n- Inefficient workflow\n</why_it_fails>\n\n<correction>\n**Option 2 workflow (correct):**\n\n```bash\ngit push -u origin feature-auth\ngh pr create --title \"feat: Add OAuth\" --body \"...\"\n\n# Report PR created\n\"Pull request created: https://github.com/user/repo/pull/42\n\nKeeping worktree at ../feature-auth-worktree for PR updates.\"\n\n# NO worktree cleanup\n# User can address PR feedback in same worktree\n```\n\n**Cleanup happens later when:**\n- PR is merged\n- User explicitly requests cleanup\n- User uses finishing-a-development-branch again after PR merges\n\n**What you gain:**\n- Worktree available for PR feedback\n- No need to recreate worktree\n- Preserve local state and experiments\n- Efficient PR iteration workflow\n</correction>\n</example>\n\n<example>\n<scenario>Developer discards work without confirmation</scenario>\n\n<code>\n# User says: \"Actually, discard this work\"\n\n# Developer immediately executes\ngit checkout main\ngit branch -D feature-experimental\ngit worktree remove ../feature-experimental-worktree\n\n# 50 commits deleted\n# User: \"Wait, I meant discard the LAST commit, not the whole branch!\"\n# Too late - work is gone\n</code>\n\n<why_it_fails>\n- No confirmation before destructive action\n- Misunderstood user intent\n- Permanent data loss\n- No way to recover work\n- Catastrophic for user\n</why_it_fails>\n\n<correction>\n**Option 4 workflow (correct):**\n\n```\nUser: \"Discard this work\"\n\n\"This will permanently delete:\n- Branch feature-experimental\n- All commits:\n  * a1b2c3d Add OAuth integration\n  * d4e5f6g Add rate limiting\n  * g7h8i9j Update tests\n  ... (47 more commits)\n- Worktree at ../feature-experimental-worktree\n\nType 'discard' to confirm.\"\n\n# WAIT for exact confirmation\nUser types: \"discard\"\n\n# NOW execute\ngit checkout main\ngit branch -D feature-experimental\ngit worktree remove ../feature-experimental-worktree\n\n\"Branch feature-experimental deleted.\"\n```\n\n**What you gain:**\n- User sees exactly what will be deleted\n- Explicit confirmation required\n- Prevents accidental data loss\n- Time to reconsider or clarify\n- Safe destructive operations\n</correction>\n</example>\n</examples>\n\n<option_matrix>\n| Option | Merge | Push | Keep Worktree | Cleanup Branch | Cleanup Worktree |\n|--------|-------|------|---------------|----------------|------------------|\n| 1. Merge locally | ✓ | - | - | ✓ | ✓ |\n| 2. Create PR | - | ✓ | ✓ | - | - |\n| 3. Keep as-is | - | - | ✓ | - | - |\n| 4. Discard | - | - | - | ✓ (force) | ✓ |\n</option_matrix>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Never skip test verification** → Tests must pass before presenting options\n2. **Present exactly 4 options** → No open-ended questions\n3. **Require confirmation for Option 4** → Type \"discard\" exactly\n4. **Keep worktree for Options 2 & 3** → PR and keep-as-is need worktree\n5. **Verify tests after merge (Option 1)** → Merged result might break\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow the process.**\n\n- \"Tests passed earlier, don't need to verify\" (Might have changed, verify now)\n- \"User knows what they want\" (Present options, let them choose)\n- \"Obvious they want to discard\" (Require explicit confirmation)\n- \"PR done, cleanup worktree\" (PR likely needs updates, keep worktree)\n- \"Too many options\" (Exactly 4, no more, no less)\n</critical_rules>\n\n<verification_checklist>\nBefore completing:\n\n- [ ] bd epic closed (all child tasks closed)\n- [ ] Tests verified passing (via test-runner agent)\n- [ ] Presented exactly 4 options (no open-ended questions)\n- [ ] Waited for user choice (didn't assume)\n- [ ] If Option 4: Got typed \"discard\" confirmation\n- [ ] Worktree cleaned for Options 1, 4 only (not 2, 3)\n- [ ] If Option 1: Verified tests on merged result\n\n**Can't check all boxes?** Return to process and complete missing steps.\n</verification_checklist>\n\n<integration>\n**This skill is called by:**\n- hyperpowers:review-implementation (final step after approval)\n\n**Call chain:**\n```\nhyperpowers:executing-plans → hyperpowers:review-implementation → hyperpowers:finishing-a-development-branch\n                         ↓\n                   (if gaps found: STOP)\n```\n\n**This skill calls:**\n- hyperpowers:test-runner agent (for test verification)\n- bd commands (epic management)\n- gh commands (PR creation)\n\n**CRITICAL:** Never read `.beads/issues.jsonl` directly. Always use bd CLI commands.\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Git worktree management](resources/worktree-guide.md)\n- [PR description templates](resources/pr-templates.md)\n- [bd epic reference in PRs](resources/bd-pr-integration.md)\n\n**When stuck:**\n- Tasks won't close → Check bd status, verify all child tasks done\n- Tests fail → Fix before presenting options (can't proceed)\n- User unsure → Explain options, but don't make choice for them\n- Worktree won't remove → Might have uncommitted changes, ask user\n</resources>"
              },
              {
                "name": "fixing-bugs",
                "description": "Use when encountering a bug - complete workflow from discovery through debugging, bd issue, test-driven fix, verification, and closure",
                "path": "skills/fixing-bugs/SKILL.md",
                "frontmatter": {
                  "name": "fixing-bugs",
                  "description": "Use when encountering a bug - complete workflow from discovery through debugging, bd issue, test-driven fix, verification, and closure"
                },
                "content": "<skill_overview>\nBug fixing is a complete workflow: reproduce, track in bd, debug systematically, write test, fix, verify, close. Every bug gets a bd issue and regression test.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow exact workflow: create bd issue → debug with tools → write failing test → fix → verify → close.\n\nNever skip tracking or regression test. Use debugging-with-tools for investigation, test-driven-development for fix.\n</rigidity_level>\n\n<quick_reference>\n\n| Step | Action | Command/Skill |\n|------|--------|---------------|\n| **1. Track** | Create bd bug issue | `bd create \"Bug: [description]\" --type bug` |\n| **2. Debug** | Systematic investigation | Use `debugging-with-tools` skill |\n| **3. Test (RED)** | Write failing test reproducing bug | Use `test-driven-development` skill |\n| **4. Fix (GREEN)** | Implement fix | Minimal code to pass test |\n| **5. Verify** | Run full test suite | Use `verification-before-completion` skill |\n| **6. Close** | Update and close bd issue | `bd close bd-123` |\n\n**FORBIDDEN:** Fix without bd issue, fix without regression test\n**REQUIRED:** Every bug gets tracked, tested, verified before closing\n\n</quick_reference>\n\n<when_to_use>\n**Use when you discover a bug:**\n- Test failure you need to fix\n- Bug reported by user\n- Unexpected behavior in development\n- Regression from recent change\n- Production issue (non-emergency)\n\n**Production emergencies:** Abbreviated workflow OK (hotfix first), but still create bd issue and add regression tests afterward.\n</when_to_use>\n\n<the_process>\n\n## 1. Create bd Bug Issue\n\n**Track from the start:**\n\n```bash\nbd create \"Bug: [Clear description]\" --type bug --priority P1\n# Returns: bd-123\n```\n\n**Document:**\n```bash\nbd edit bd-123 --design \"\n## Bug Description\n[What's wrong]\n\n## Reproduction Steps\n1. Step one\n2. Step two\n\n## Expected Behavior\n[What should happen]\n\n## Actual Behavior\n[What actually happens]\n\n## Environment\n[Version, OS, etc.]\"\n```\n\n## 2. Debug Systematically\n\n**REQUIRED: Use debugging-with-tools skill**\n\n```\nUse Skill tool: hyperpowers:debugging-with-tools\n```\n\n**debugging-with-tools will:**\n- Use internet-researcher to search for error\n- Recommend debugger or instrumentation\n- Use codebase-investigator to understand context\n- Guide to root cause (not symptom)\n\n**Update bd issue with findings:**\n```bash\nbd edit bd-123 --design \"[previous content]\n\n## Investigation\n[Root cause found via debugging]\n[Tools used: debugger, internet search, etc.]\"\n```\n\n## 3. Write Failing Test (RED Phase)\n\n**REQUIRED: Use test-driven-development skill**\n\nWrite test that reproduces the bug:\n\n```python\ndef test_rejects_empty_email():\n    \"\"\"Regression test for bd-123: Empty email accepted\"\"\"\n    with pytest.raises(ValidationError):\n        create_user(email=\"\")  # Should fail, currently passes\n```\n\n**Run test, verify it FAILS:**\n```bash\npytest tests/test_user.py::test_rejects_empty_email\n# Expected: PASS (bug exists)\n# Should fail AFTER fix\n```\n\n**Why critical:** If test passes before fix, it doesn't test the bug.\n\n## 4. Implement Fix (GREEN Phase)\n\n**Fix the root cause (not symptom):**\n\n```python\ndef create_user(email: str):\n    if not email or not email.strip():  # Fix\n        raise ValidationError(\"Email required\")\n    # ... rest\n```\n\n**Run test, verify it now FAILS (test was written backwards by mistake earlier - fix this):**\n\nActually write the test to FAIL first:\n```python\ndef test_rejects_empty_email():\n    with pytest.raises(ValidationError):\n        create_user(email=\"\")\n```\n\nRun:\n```bash\npytest tests/test_user.py::test_rejects_empty_email\n# Should FAIL before fix (no validation)\n# Should PASS after fix (validation added)\n```\n\n## 5. Verify Complete Fix\n\n**REQUIRED: Use verification-before-completion skill**\n\n```bash\n# Run full test suite (via test-runner agent)\n\"Run: pytest\"\n\n# Agent returns: All tests pass (including regression test)\n```\n\n**Verify:**\n- Regression test passes\n- All other tests still pass\n- No new warnings or errors\n- Pre-commit hooks pass\n\n## 6. Close bd Issue\n\n**Update with fix details:**\n```bash\nbd edit bd-123 --design \"[previous content]\n\n## Fix Implemented\n[Description of fix]\n[File changed: src/auth/user.py:23]\n\n## Regression Test\n[Test added: tests/test_user.py::test_rejects_empty_email]\n\n## Verification\n[All tests pass: 145/145]\"\n\nbd close bd-123\n```\n\n**Commit with bd reference:**\n```bash\ngit commit -m \"fix(bd-123): Reject empty email in user creation\n\nAdds validation to prevent empty strings.\nRegression test: test_rejects_empty_email\n\nCloses bd-123\"\n```\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer fixes bug without creating bd issue or regression test</scenario>\n\n<code>\nDeveloper notices: Empty email accepted in user creation\n\n\"Fixes\" immediately:\n```python\ndef create_user(email: str):\n    if not email:  # Quick fix\n        raise ValidationError(\"Email required\")\n```\n\nCommits: \"fix: validate email\"\n\n[No bd issue, no regression test]\n</code>\n\n<why_it_fails>\n**No tracking:**\n- Work not tracked in bd (can't see what was fixed)\n- No link between commit and bug\n- Can't verify fix meets requirements\n\n**No regression test:**\n- Bug could come back in future\n- Can't prove fix works\n- No protection against breaking this again\n\n**Incomplete fix:**\n- Doesn't handle `email=\" \"` (whitespace)\n- Didn't debug to understand full issue\n\n**Result:** Bug returns when someone changes validation logic.\n</why_it_fails>\n\n<correction>\n**Complete workflow:**\n\n```bash\n# 1. Track\nbd create \"Bug: Empty email accepted\" --type bug\n# Returns: bd-123\n\n# 2. Debug (use debugging-with-tools)\n# Investigation reveals: Email validation missing entirely\n# Also: Whitespace emails like \" \" also accepted\n\n# 3. Write failing test (RED)\ndef test_rejects_empty_email():\n    with pytest.raises(ValidationError):\n        create_user(email=\"\")\n\ndef test_rejects_whitespace_email():\n    with pytest.raises(ValidationError):\n        create_user(email=\"   \")\n\n# Run: Both PASS (bug exists) - WAIT, test should FAIL before fix!\n```\n\nActually:\n```python\n# Test currently PASSES (bug exists - no validation)\n# We expect test to FAIL after we add validation\n\n# 4. Fix\ndef create_user(email: str):\n    if not email or not email.strip():\n        raise ValidationError(\"Email required\")\n\n# 5. Verify\npytest  # All tests pass now, including regression tests\n\n# 6. Close\nbd close bd-123\ngit commit -m \"fix(bd-123): Reject empty/whitespace email\"\n```\n\n**Result:** Bug fixed, tracked, tested, won't regress.\n</correction>\n</example>\n\n<example>\n<scenario>Developer writes test after fix, test passes immediately, doesn't catch regression</scenario>\n\n<code>\nDeveloper fixes validation bug, then writes test:\n\n```python\n# Fix first\ndef validate_email(email):\n    return \"@\" in email and len(email) > 0\n\n# Then test\ndef test_validate_email():\n    assert validate_email(\"user@example.com\") == True\n```\n\nTest runs: PASS\n\nCommits both together.\n\nLater, someone changes validation:\n```python\ndef validate_email(email):\n    return True  # Breaks validation!\n```\n\nTest still PASSES (only checks happy path).\n</code>\n\n<why_it_fails>\n**Test written after fix:**\n- Never saw test fail\n- Only tests happy path remembered\n- Doesn't test the bug that was fixed\n- Missed edge case: `validate_email(\"@@\")` returns True (bug!)\n\n**Why it happens:** Skipping TDD RED phase.\n</why_it_fails>\n\n<correction>\n**TDD approach (RED-GREEN):**\n\n```python\n# 1. Write test FIRST that reproduces the bug\ndef test_validate_email():\n    # Happy path\n    assert validate_email(\"user@example.com\") == True\n    # Bug case (empty email was accepted)\n    assert validate_email(\"\") == False\n    # Edge case discovered during debugging\n    assert validate_email(\"@@\") == False\n\n# 2. Run test - should FAIL (bug exists)\npytest test_validate_email\n# FAIL: validate_email(\"\") returned True, expected False\n\n# 3. Implement fix\ndef validate_email(email):\n    if not email or len(email) == 0:\n        return False\n    return \"@\" in email and email.count(\"@\") == 1\n\n# 4. Run test - should PASS\npytest test_validate_email\n# PASS: All assertions pass\n```\n\n**Later regression:**\n```python\ndef validate_email(email):\n    return True  # Someone breaks it\n```\n\n**Test catches it:**\n```\nFAIL: assert validate_email(\"\") == False\nExpected False, got True\n```\n\n**Result:** Regression test actually prevents bug from returning.\n</correction>\n</example>\n\n<example>\n<scenario>Developer fixes symptom without using debugging-with-tools to find root cause</scenario>\n\n<code>\nBug report: \"Application crashes when processing user data\"\n\nError:\n```\nNullPointerException at UserService.java:45\n```\n\nDeveloper sees line 45:\n```java\nString email = user.getEmail().toLowerCase();  // Line 45\n```\n\n\"Obvious fix\":\n```java\nString email = user.getEmail() != null ? user.getEmail().toLowerCase() : \"\";\n```\n\nBug \"fixed\"... but crashes continue with different data.\n</code>\n\n<why_it_fails>\n**Symptom fix:**\n- Fixed null check at crash point\n- Didn't investigate WHY email is null\n- Didn't use debugging-with-tools to find root cause\n\n**Actual root cause:** User object created without email in registration flow. Email is null for all users created via broken endpoint.\n\n**Result:** Null-check applied everywhere, root cause (broken registration) unfixed.\n</why_it_fails>\n\n<correction>\n**Use debugging-with-tools skill:**\n\n```\n# Dispatch internet-researcher\n\"Search for: NullPointerException UserService getEmail\n- Common causes of null email in user objects\n- User registration validation patterns\"\n\n# Dispatch codebase-investigator\n\"Investigate:\n- How is User object created?\n- Where is email set?\n- Are there paths where email can be null?\n- Which endpoints create users?\"\n\n# Agent reports:\n\"Found: POST /register endpoint creates User without validating email field.\nEmail is optional in UserDTO but required in User domain object.\"\n```\n\n**Root cause found:** Registration doesn't validate email.\n\n**Proper fix:**\n```java\n// In registration endpoint\n@PostMapping(\"/register\")\npublic User register(@RequestBody UserDTO dto) {\n    if (dto.getEmail() == null || dto.getEmail().isEmpty()) {\n        throw new ValidationException(\"Email required\");\n    }\n    return userService.create(dto);\n}\n```\n\n**Regression test:**\n```java\n@Test\nvoid registrationRequiresEmail() {\n    assertThrows(ValidationException.class, () ->\n        register(new UserDTO(null, \"password\")));\n}\n```\n\n**Result:** Root cause fixed, no more null emails created.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **Every bug gets a bd issue** → Track from discovery to closure\n   - Create bd issue before fixing\n   - Document reproduction steps\n   - Update with investigation findings\n   - Close only after verified\n\n2. **Use debugging-with-tools skill** → Systematic investigation required\n   - Never guess at fixes\n   - Use internet-researcher for errors\n   - Use debugger/instrumentation for state\n   - Find root cause, not symptom\n\n3. **Write failing test first (RED)** → Regression prevention\n   - Test must fail before fix\n   - Test must reproduce the bug\n   - Test must pass after fix\n   - If test passes immediately, it doesn't test the bug\n\n4. **Verify complete fix** → Use verification-before-completion\n   - Regression test passes\n   - Full test suite passes\n   - No new warnings\n   - Pre-commit hooks pass\n\n## Common Excuses\n\nAll of these mean: Stop, follow complete workflow:\n- \"Quick fix, no need for bd issue\"\n- \"Obvious bug, no need to debug\"\n- \"I'll add test later\"\n- \"Test passes, must be fixed\"\n- \"Just one line change\"\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore claiming bug fixed:\n- [ ] bd issue created with reproduction steps\n- [ ] Used debugging-with-tools to find root cause\n- [ ] Wrote test that reproduces bug (RED phase)\n- [ ] Verified test FAILS before fix\n- [ ] Implemented fix addressing root cause\n- [ ] Verified test PASSES after fix\n- [ ] Ran full test suite (all pass)\n- [ ] Updated bd issue with fix details\n- [ ] Closed bd issue\n- [ ] Committed with bd reference\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- debugging-with-tools (systematic investigation)\n- test-driven-development (RED-GREEN-REFACTOR cycle)\n- verification-before-completion (verify complete fix)\n\n**This skill is called by:**\n- When bugs discovered during development\n- When test failures need fixing\n- When user reports bugs\n\n**Agents used:**\n- hyperpowers:internet-researcher (via debugging-with-tools)\n- hyperpowers:codebase-investigator (via debugging-with-tools)\n- hyperpowers:test-runner (run tests without output pollution)\n\n</integration>\n\n<resources>\n\n**When stuck:**\n- Don't understand bug → Use debugging-with-tools skill\n- Tempted to skip tracking → Create bd issue first, always\n- Test passes immediately → Not testing the bug, rewrite test\n- Fix doesn't work → Return to debugging-with-tools, find actual root cause\n\n</resources>"
              },
              {
                "name": "managing-bd-tasks",
                "description": "Use for advanced bd operations - splitting tasks mid-flight, merging duplicates, changing dependencies, archiving epics, querying metrics, cross-epic dependencies",
                "path": "skills/managing-bd-tasks/SKILL.md",
                "frontmatter": {
                  "name": "managing-bd-tasks",
                  "description": "Use for advanced bd operations - splitting tasks mid-flight, merging duplicates, changing dependencies, archiving epics, querying metrics, cross-epic dependencies"
                },
                "content": "<skill_overview>\nAdvanced bd operations for managing complex task structures; bd is single source of truth, keep it accurate.\n</skill_overview>\n\n<rigidity_level>\nHIGH FREEDOM - These are operational patterns, not rigid workflows. Adapt operations to your specific situation while following the core principles (keep bd accurate, merge don't delete, document changes).\n</rigidity_level>\n\n<quick_reference>\n| Operation | When | Key Command |\n|-----------|------|-------------|\n| Split task | Task too large mid-flight | Create subtasks, add deps, close parent |\n| Merge duplicates | Found duplicate tasks | Combine designs, move deps, close with reference |\n| Change dependencies | Dependencies wrong/changed | `bd dep remove` then `bd dep add` |\n| Archive epic | Epic complete, hide from views | `bd close bd-X --reason \"Archived\"` |\n| Query metrics | Need status/velocity data | `bd list` + filters + `wc -l` |\n| Cross-epic deps | Task depends on other epic | `bd dep add` works across epics |\n| Bulk updates | Multiple tasks need same change | Loop with careful review first |\n| Recover mistakes | Accidentally closed/wrong dep | `bd update --status` or `bd dep remove` |\n\n**Core principle:** Track all work in bd, update as you go, never batch updates.\n</quick_reference>\n\n<when_to_use>\nUse this skill for **advanced** bd operations:\n- Split task that's too large (discovered mid-implementation)\n- Merge duplicate tasks\n- Reorganize dependencies after work started\n- Archive completed epics (hide from views, keep history)\n- Query bd for metrics (velocity, progress, bottlenecks)\n- Manage cross-epic dependencies\n- Bulk status updates\n- Recover from bd mistakes\n\n**For basic operations:** See skills/common-patterns/bd-commands.md (create, show, close, update)\n</when_to_use>\n\n<operations>\n## Operation 1: Splitting Tasks Mid-Flight\n\n**When:** Task in-progress but turns out too large.\n\n**Example:** Started \"Implement authentication\" - realize it's 8+ hours of work across multiple areas.\n\n**Process:**\n\n### Step 1: Create subtasks for remaining work\n\n```bash\n# Original task bd-5 is in-progress\n# Already completed: Login form\n# Remaining work gets split:\n\nbd create \"Auth API endpoints\" --type task --priority P1 --design \"\nPOST /api/login and POST /api/logout endpoints.\n## Success Criteria\n- [ ] POST /api/login validates credentials, returns JWT\n- [ ] POST /api/logout invalidates token\n- [ ] Tests pass\n\"\n# Returns bd-12\n\nbd create \"Session management\" --type task --priority P1 --design \"\nJWT token tracking and validation.\n## Success Criteria\n- [ ] JWT generated on login\n- [ ] Tokens validated on protected routes\n- [ ] Token expiration handled\n- [ ] Tests pass\n\"\n# Returns bd-13\n\nbd create \"Password hashing\" --type task --priority P1 --design \"\nSecure password hashing with bcrypt.\n## Success Criteria\n- [ ] Passwords hashed before storage\n- [ ] Hash verification on login\n- [ ] Tests pass\n\"\n# Returns bd-14\n```\n\n### Step 2: Set up dependencies\n\n```bash\n# Password hashing must be done first\n# API endpoints depend on password hashing\nbd dep add bd-12 bd-14  # bd-12 depends on bd-14\n\n# Session management depends on API endpoints\nbd dep add bd-13 bd-12  # bd-13 depends on bd-12\n\n# View tree\nbd dep tree bd-5\n```\n\n### Step 3: Update original task and close\n\n```bash\nbd edit bd-5 --design \"\nImplement user authentication.\n\n## Status\n✓ Login form completed (frontend)\n✗ Remaining work split into subtasks:\n  - bd-14: Password hashing (do first)\n  - bd-12: Auth API endpoints (depends on bd-14)\n  - bd-13: Session management (depends on bd-12)\n\n## Success Criteria\n- [x] Login form renders\n- [ ] See subtasks for remaining criteria\n\"\n\nbd close bd-5 --reason \"Split into bd-12, bd-13, bd-14\"\n```\n\n### Step 4: Work on subtasks in order\n\n```bash\nbd ready  # Shows bd-14 (no dependencies)\nbd update bd-14 --status in_progress\n# Complete bd-14...\nbd close bd-14\n\n# Now bd-12 is unblocked\nbd ready  # Shows bd-12\n```\n\n---\n\n## Operation 2: Merging Duplicate Tasks\n\n**When:** Discovered two tasks are same thing.\n\n**Example:**\n```\nbd-7: \"Add email validation\"\nbd-9: \"Validate user email addresses\"\n^ Duplicates\n```\n\n### Step 1: Choose which to keep\n\nBased on:\n- Which has more complete design?\n- Which has more work done?\n- Which has more dependencies?\n\n**Example:** Keep bd-7 (more complete)\n\n### Step 2: Merge designs\n\n```bash\nbd show bd-7\nbd show bd-9\n\n# Combine into bd-7\nbd edit bd-7 --design \"\nAdd email validation to user creation and update.\n\n## Background\nOriginally tracked as bd-7 and bd-9 (now merged).\n\n## Success Criteria\n- [ ] Email validated on creation\n- [ ] Email validated on update\n- [ ] Rejects invalid formats\n- [ ] Rejects empty strings\n- [ ] Tests cover all cases\n\n## Notes from bd-9\nNeed validation on update, not just creation.\n\"\n```\n\n### Step 3: Move dependencies\n\n```bash\n# Check bd-9 dependencies\nbd show bd-9\n\n# If bd-10 depended on bd-9, update to bd-7\nbd dep remove bd-10 bd-9\nbd dep add bd-10 bd-7\n```\n\n### Step 4: Close duplicate with reference\n\n```bash\nbd edit bd-9 --design \"DUPLICATE: Merged into bd-7\n\nThis task was duplicate of bd-7. All work tracked there.\"\n\nbd close bd-9\n```\n\n---\n\n## Operation 3: Changing Dependencies\n\n**When:** Dependencies were wrong or requirements changed.\n\n**Example:** bd-10 depends on bd-8 and bd-9, but bd-9 got merged and bd-10 now also needs bd-11.\n\n```bash\n# Remove obsolete dependency\nbd dep remove bd-10 bd-9\n\n# Add new dependency\nbd dep add bd-10 bd-11\n\n# Verify\nbd dep tree bd-1  # If bd-10 in epic bd-1\nbd show bd-10 | grep \"Blocking\"\n```\n\n**Common scenarios:**\n- Discovered hidden dependency during implementation\n- Requirements changed mid-flight\n- Tasks reordered for better flow\n\n---\n\n## Operation 4: Archiving Completed Epics\n\n**When:** Epic complete, want to hide from default views but keep history.\n\n```bash\n# Verify all tasks closed\nbd list --parent bd-1 --status open\n# Output: [empty] = all closed\n\n# Archive epic\nbd close bd-1 --reason \"Archived - completed Oct 2025\"\n\n# Won't show in open listings\nbd list --status open  # bd-1 won't appear\n\n# Still accessible\nbd show bd-1  # Still shows full epic\n```\n\n**Use archived for:** Completed epics, shipped features, historical reference\n**Use open/in-progress for:** Active work\n**Use closed with note for:** Cancelled work (explain why)\n\n---\n\n## Operation 5: Querying for Metrics\n\n### Velocity\n\n```bash\n# Tasks closed this week\nbd list --status closed | grep \"closed_at\" | grep \"2025-10-\" | wc -l\n\n# Tasks closed by epic\nbd list --parent bd-1 --status closed | wc -l\n```\n\n### Blocked vs Ready\n\n```bash\n# Ready to work on\nbd ready\nbd ready | grep \"^bd-\" | wc -l\n\n# All open tasks\nbd list --status open | wc -l\n\n# Blocked = open - ready\n```\n\n### Epic Progress\n\n```bash\n# Show tree\nbd dep tree bd-1\n\n# Total tasks in epic\nbd list --parent bd-1 | grep \"^bd-\" | wc -l\n\n# Completed tasks\nbd list --parent bd-1 --status closed | grep \"^bd-\" | wc -l\n\n# Percentage = (completed / total) * 100\n```\n\n**For detailed metrics guidance:** See [resources/metrics-guide.md](resources/metrics-guide.md)\n\n---\n\n## Operation 6: Cross-Epic Dependencies\n\n**When:** Task in one epic depends on task in different epic.\n\n**Example:**\n```\nEpic bd-1: User Management\n  - bd-10: User CRUD API\n\nEpic bd-2: Order Management\n  - bd-20: Order creation (needs user API)\n```\n\n```bash\n# Add cross-epic dependency\nbd dep add bd-20 bd-10\n# bd-20 (in bd-2) depends on bd-10 (in bd-1)\n\n# Check dependencies\nbd show bd-20 | grep \"Blocking\"\n\n# Check ready tasks\nbd ready\n# Won't show bd-20 until bd-10 closed\n```\n\n**Best practices:**\n- Document cross-epic dependencies clearly\n- Consider if epics should be merged\n- Coordinate if different people own epics\n\n---\n\n## Operation 7: Bulk Status Updates\n\n**When:** Need to update multiple tasks.\n\n**Example:** Mark all test tasks closed after suite complete.\n\n```bash\n# Get tasks\nbd list --parent bd-1 --status open | grep \"test:\" > test-tasks.txt\n\n# Review list\ncat test-tasks.txt\n\n# Update each\nwhile read task_id; do\n  bd close \"$task_id\"\ndone < test-tasks.txt\n\n# Verify\nbd list --parent bd-1 --status open | grep \"test:\"\n```\n\n**Use bulk for:**\n- Marking completed work closed\n- Reopening related tasks\n- Updating priorities\n\n**Never bulk:**\n- Thoughtless changes\n- Hiding problems (closing unfinished tasks)\n\n---\n\n## Operation 8: Recovering from Mistakes\n\n### Accidentally closed task\n\n```bash\nbd update bd-15 --status open\n# Or if was in progress\nbd update bd-15 --status in_progress\n```\n\n### Wrong dependency\n\n```bash\nbd dep remove bd-10 bd-8  # Remove wrong\nbd dep add bd-10 bd-9     # Add correct\n```\n\n### Undo design changes\n\n```bash\n# bd has no undo, restore from git\ngit log -p -- .beads/issues.jsonl | grep -A 50 \"bd-10\"\n# Find previous version, copy\n\nbd edit bd-10 --design \"[paste previous]\"\n```\n\n### Epic structure wrong\n\n1. Create new tasks with correct structure\n2. Move work to new tasks\n3. Close old tasks with reference\n4. Don't delete (keep audit trail)\n</operations>\n\n<examples>\n<example>\n<scenario>Developer closes duplicate without merging information</scenario>\n\n<code>\n# Found duplicates\nbd-7: \"Add email validation\"\nbd-9: \"Validate user email addresses\"\n\n# Developer just closes bd-9\nbd close bd-9\n\n# Loses information from bd-9's design\n# bd-9 mentioned validation on update (bd-7 didn't)\n# Now that requirement is lost\n# Work on bd-7 completes, but misses update validation\n# Bug ships to production\n</code>\n\n<why_it_fails>\n- Closed duplicate without reading its design\n- Lost requirement mentioned only in duplicate\n- Information not preserved\n- Incomplete implementation ships\n- bd not accurate source of truth\n</why_it_fails>\n\n<correction>\n**Correct process:**\n\n```bash\n# Read BOTH tasks\nbd show bd-7  # Only mentions validation on creation\nbd show bd-9  # Mentions validation on update too\n\n# Merge information\nbd edit bd-7 --design \"\nEmail validation for user creation and update.\n\n## Background\nMerged from bd-9.\n\n## Success Criteria\n- [ ] Validate on creation (from bd-7)\n- [ ] Validate on update (from bd-9)  ← Preserved!\n- [ ] Tests for both cases\n\"\n\n# Then close duplicate with reference\nbd edit bd-9 --design \"DUPLICATE: Merged into bd-7\"\nbd close bd-9\n```\n\n**What you gain:**\n- All requirements preserved\n- bd remains accurate\n- No information lost\n- Complete implementation\n- Audit trail clear\n</correction>\n</example>\n\n<example>\n<scenario>Developer doesn't split large task, struggles through</scenario>\n\n<code>\nbd-15: \"Implement payment processing\" (started)\n\n# 3 hours in, developer realizes:\n# - Need Stripe API integration (4 hours)\n# - Need payment validation (2 hours)\n# - Need retry logic (3 hours)\n# - Need receipt generation (2 hours)\n# Total: 11 more hours!\n\n# Developer thinks: \"Too late to split, I'll power through\"\n# Works 14 hours straight\n# Gets exhausted, makes mistakes\n# Ships buggy code\n# Has to fix in production\n</code>\n\n<why_it_fails>\n- Didn't split when discovered size\n- \"Sunk cost\" rationalization (already started)\n- No clear stopping points\n- Exhaustion leads to bugs\n- Can't track progress granularly\n- If interrupted, hard to resume\n</why_it_fails>\n\n<correction>\n**Correct approach (split mid-flight):**\n\n```bash\n# 3 hours in, stop and split\n\nbd edit bd-15 --design \"\nImplement payment processing.\n\n## Status\n✓ Completed: Payment form UI (3 hours)\n✗ Split remaining work into subtasks:\n  - bd-20: Stripe API integration\n  - bd-21: Payment validation\n  - bd-22: Retry logic\n  - bd-23: Receipt generation\n\"\n\nbd close bd-15 --reason \"Split into bd-20, bd-21, bd-22, bd-23\"\n\n# Create subtasks with dependencies\nbd create \"Stripe API integration\" ...  # bd-20\nbd create \"Payment validation\" ...      # bd-21\nbd create \"Retry logic\" ...             # bd-22\nbd create \"Receipt generation\" ...      # bd-23\n\nbd dep add bd-21 bd-20  # Validation needs API\nbd dep add bd-22 bd-20  # Retry needs API\nbd dep add bd-23 bd-22  # Receipts after retry works\n\n# Work on one at a time\nbd update bd-20 --status in_progress\n# Complete bd-20 (4 hours)\nbd close bd-20\n\n# Take break\n# Next day: bd-21\n```\n\n**What you gain:**\n- Clear stopping points (can pause between tasks)\n- Track progress granularly\n- No exhaustion (spread over days)\n- Better quality (not rushed)\n- If interrupted, easy to resume\n- Each subtask gets proper focus\n</correction>\n</example>\n\n<example>\n<scenario>Developer adds dependency but doesn't update dependent task</scenario>\n\n<code>\n# Initial state\nbd-10: \"Add user dashboard\" (in progress)\nbd-15: \"Add analytics to dashboard\" (blocked on bd-10)\n\n# During bd-10 implementation, discover need for new API\nbd create \"Analytics API endpoints\" ...  # Creates bd-20\n\n# Add dependency\nbd dep add bd-15 bd-20  # bd-15 now depends on bd-20 too\n\n# But bd-10 completes, closes\nbd close bd-10\n\n# bd-15 shows as ready (bd-10 closed)\nbd ready  # Shows bd-15\n\n# Developer starts bd-15\nbd update bd-15 --status in_progress\n\n# Immediately blocked - needs bd-20!\n# bd-20 not done yet\n# Have to stop work on bd-15\n# Time wasted\n</code>\n\n<why_it_fails>\n- Added dependency but didn't document in bd-15\n- bd-15's design doesn't mention bd-20 requirement\n- Appears ready when not actually ready\n- Wastes time starting work that's blocked\n- Dependencies not obvious from task design\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n```bash\n# Create new API task\nbd create \"Analytics API endpoints\" ...  # bd-20\n\n# Add dependency\nbd dep add bd-15 bd-20\n\n# UPDATE bd-15 to document new requirement\nbd edit bd-15 --design \"\nAdd analytics to dashboard.\n\n## Dependencies\n- bd-10: User dashboard (completed)\n- bd-20: Analytics API endpoints (NEW - discovered during bd-10)\n\n## Success Criteria\n- [ ] Integrate with analytics API (bd-20)\n- [ ] Display charts on dashboard\n- [ ] Tests pass\n\"\n\n# Close bd-10\nbd close bd-10\n\n# Check ready\nbd ready  # Does NOT show bd-15 (blocked on bd-20)\n\n# Work on bd-20 first\nbd update bd-20 --status in_progress\n# Complete bd-20\nbd close bd-20\n\n# NOW bd-15 is truly ready\nbd ready  # Shows bd-15\n```\n\n**What you gain:**\n- Dependencies documented in task design\n- Clear why task is blocked\n- No false \"ready\" signals\n- Work proceeds in correct order\n- No wasted time starting blocked work\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Keep bd accurate** → Single source of truth for all work\n2. **Merge duplicates, don't just close** → Preserve information from both\n3. **Split large tasks when discovered** → Not after struggling through\n4. **Document dependency changes** → Update task designs when deps change\n5. **Update as you go** → Never batch updates \"for later\"\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow the operation properly.**\n\n- \"Task too complex to split\" (Every task can be broken down)\n- \"Just close duplicate\" (Merge first, preserve information)\n- \"Won't track this in bd\" (All work tracked, no exceptions)\n- \"bd is out of date, update later\" (Later never comes, update now)\n- \"This dependency doesn't matter\" (Dependencies prevent blocking, they matter)\n- \"Too much overhead to split\" (More overhead to fail huge task)\n</critical_rules>\n\n<bd_best_practices>\n**For detailed guidance on:**\n- Task naming conventions\n- Priority guidelines (P0-P4)\n- Task granularity\n- Success criteria\n- Dependency management\n\n**See:** [resources/task-naming-guide.md](resources/task-naming-guide.md)\n</bd_best_practices>\n\n<red_flags>\nWatch for these patterns:\n\n- **Multiple in-progress tasks** → Focus on one\n- **Tasks stuck in-progress for days** → Blocked? Split it?\n- **Many open tasks, no dependencies** → Prioritize!\n- **Epics with 20+ tasks** → Too large, split epic\n- **Closed tasks, incomplete criteria** → Not done, reopen\n</red_flags>\n\n<verification_checklist>\nAfter advanced bd operations:\n\n- [ ] bd still accurate (reflects reality)\n- [ ] Dependencies correct (nothing blocked incorrectly)\n- [ ] Duplicate information merged (not lost)\n- [ ] Changes documented in task designs\n- [ ] Ready tasks are actually unblocked\n- [ ] Metrics queries return sensible numbers\n- [ ] No orphaned tasks (all part of epics)\n\n**Can't check all boxes?** Review operation and fix issues.\n</verification_checklist>\n\n<integration>\n**This skill covers:** Advanced bd operations\n\n**For basic operations:**\n- skills/common-patterns/bd-commands.md\n\n**Related skills:**\n- hyperpowers:writing-plans (creating epics and tasks)\n- hyperpowers:executing-plans (working through tasks)\n- hyperpowers:verification-before-completion (closing tasks properly)\n\n**CRITICAL:** Use bd CLI commands, never read `.beads/issues.jsonl` directly.\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Metrics guide (cycle time, WIP limits)](resources/metrics-guide.md)\n- [Task naming conventions](resources/task-naming-guide.md)\n- [Dependency patterns](resources/dependency-patterns.md)\n\n**When stuck:**\n- Task seems unsplittable → Ask user how to break it down\n- Duplicates complex → Merge designs carefully, don't rush\n- Dependencies tangled → Draw diagram, untangle systematically\n- bd out of sync → Stop everything, update bd first\n</resources>"
              },
              {
                "name": "refactoring-safely",
                "description": "Use when refactoring code - test-preserving transformations in small steps, running tests between each change",
                "path": "skills/refactoring-safely/SKILL.md",
                "frontmatter": {
                  "name": "refactoring-safely",
                  "description": "Use when refactoring code - test-preserving transformations in small steps, running tests between each change"
                },
                "content": "<skill_overview>\nRefactoring changes code structure without changing behavior; tests must stay green throughout or you're rewriting, not refactoring.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow the change→test→commit cycle strictly, but adapt the specific refactoring patterns to your language and codebase.\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | Verify |\n|------|--------|--------|\n| 1 | Run full test suite | ALL pass |\n| 2 | Create bd refactoring task | Track work |\n| 3 | Make ONE small change | Compiles |\n| 4 | Run tests immediately | ALL still pass |\n| 5 | Commit with descriptive message | History clear |\n| 6 | Repeat 3-5 until complete | Each step safe |\n| 7 | Final verification & close bd | Done |\n\n**Core cycle:** Change → Test → Commit (repeat until complete)\n</quick_reference>\n\n<when_to_use>\n- Improving code structure without changing functionality\n- Extracting duplicated code into shared utilities\n- Renaming for clarity\n- Reorganizing file/module structure\n- Simplifying complex code while preserving behavior\n\n**Don't use for:**\n- Changing functionality (use feature development)\n- Fixing bugs (use hyperpowers:fixing-bugs)\n- Adding features while restructuring (do separately)\n- Code without tests (write tests first using hyperpowers:test-driven-development)\n</when_to_use>\n\n<the_process>\n## 1. Verify Tests Pass\n\n**BEFORE any refactoring:**\n\n```bash\n# Use test-runner agent to keep context clean\nDispatch hyperpowers:test-runner agent: \"Run: cargo test\"\n```\n\n**Verify:** ALL tests pass. If any fail, fix them FIRST, then refactor.\n\n**Why:** Failing tests mean you can't detect if refactoring breaks things.\n\n---\n\n## 2. Create bd Task for Refactoring\n\nTrack the refactoring work:\n\n```bash\nbd create \"Refactor: Extract user validation logic\" \\\n  --type task \\\n  --priority P2\n\nbd edit bd-456 --design \"\n## Goal\nExtract user validation logic from UserService into separate Validator class.\n\n## Why\n- Validation duplicated across 3 services\n- Makes testing individual validations difficult\n- Violates single responsibility\n\n## Approach\n1. Create UserValidator class\n2. Extract email validation\n3. Extract name validation\n4. Extract age validation\n5. Update UserService to use validator\n6. Remove duplication from other services\n\n## Success Criteria\n- All existing tests still pass\n- No behavior changes\n- Validator has 100% test coverage\n\"\n\nbd update bd-456 --status in_progress\n```\n\n---\n\n## 3. Make ONE Small Change\n\nThe smallest transformation that compiles.\n\n**Examples of \"small\":**\n- Extract one method\n- Rename one variable\n- Move one function to different file\n- Inline one constant\n- Extract one interface\n\n**NOT small:**\n- Extracting multiple methods at once\n- Renaming + moving + restructuring\n- \"While I'm here\" improvements\n\n**Example:**\n\n```rust\n// Before\nfn create_user(name: &str, email: &str) -> Result<User> {\n    if email.is_empty() {\n        return Err(Error::InvalidEmail);\n    }\n    if !email.contains('@') {\n        return Err(Error::InvalidEmail);\n    }\n\n    let user = User { name, email };\n    Ok(user)\n}\n\n// After - ONE small change (extract email validation)\nfn create_user(name: &str, email: &str) -> Result<User> {\n    validate_email(email)?;\n\n    let user = User { name, email };\n    Ok(user)\n}\n\nfn validate_email(email: &str) -> Result<()> {\n    if email.is_empty() {\n        return Err(Error::InvalidEmail);\n    }\n    if !email.contains('@') {\n        return Err(Error::InvalidEmail);\n    }\n    Ok(())\n}\n```\n\n---\n\n## 4. Run Tests Immediately\n\nAfter EVERY small change:\n\n```bash\nDispatch hyperpowers:test-runner agent: \"Run: cargo test\"\n```\n\n**Verify:** ALL tests still pass.\n\n**If tests fail:**\n1. STOP\n2. Undo the change: `git restore src/file.rs`\n3. Understand why it broke\n4. Make smaller change\n5. Try again\n\n**Never proceed with failing tests.**\n\n---\n\n## 5. Commit the Small Change\n\nCommit each safe transformation:\n\n```bash\nDispatch hyperpowers:test-runner agent: \"Run: git add src/user_service.rs && git commit -m 'refactor(bd-456): extract email validation to function\n\nNo behavior change. All tests pass.\n\nPart of bd-456'\"\n```\n\n**Why commit so often:**\n- Easy to undo if next step breaks\n- Clear history of transformations\n- Can review each step independently\n- Proves tests passed at each point\n\n---\n\n## 6. Repeat Until Complete\n\nRepeat steps 3-5 for each small transformation:\n\n```\n1. Extract validate_email() ✓ (committed)\n2. Extract validate_name() ✓ (committed)\n3. Extract validate_age() ✓ (committed)\n4. Create UserValidator struct ✓ (committed)\n5. Move validations into UserValidator ✓ (committed)\n6. Update UserService to use validator ✓ (committed)\n7. Remove validation from OrderService ✓ (committed)\n8. Remove validation from AccountService ✓ (committed)\n```\n\n**Pattern:** change → test → commit (repeat)\n\n---\n\n## 7. Final Verification\n\nAfter all transformations complete:\n\n```bash\n# Full test suite\nDispatch hyperpowers:test-runner agent: \"Run: cargo test\"\n\n# Linter\nDispatch hyperpowers:test-runner agent: \"Run: cargo clippy\"\n```\n\n**Review the changes:**\n\n```bash\n# See all refactoring commits\ngit log --oneline | grep \"bd-456\"\n\n# Review full diff\ngit diff main...HEAD\n```\n\n**Checklist:**\n- [ ] All tests pass\n- [ ] No new warnings\n- [ ] No behavior changes\n- [ ] Code is cleaner/simpler\n- [ ] Each commit is small and safe\n\n**Close bd task:**\n\n```bash\nbd edit bd-456 --design \"\n... (append to existing design)\n\n## Completed\n- Created UserValidator class with email, name, age validation\n- Removed duplicated validation from 3 services\n- All tests pass (verified)\n- No behavior changes\n- 8 small transformations, each tested\n\"\n\nbd close bd-456\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer changes behavior while \"refactoring\"</scenario>\n\n<code>\n// Original code\nfn validate_email(email: &str) -> Result<()> {\n    if email.is_empty() {\n        return Err(Error::InvalidEmail);\n    }\n    if !email.contains('@') {\n        return Err(Error::InvalidEmail);\n    }\n    Ok(())\n}\n\n// \"Refactored\" version\nfn validate_email(email: &str) -> Result<()> {\n    if email.is_empty() {\n        return Err(Error::InvalidEmail);\n    }\n    if !email.contains('@') {\n        return Err(Error::InvalidEmail);\n    }\n    // NEW: Added extra validation\n    if !email.contains('.') {  // BEHAVIOR CHANGE\n        return Err(Error::InvalidEmail);\n    }\n    Ok(())\n}\n</code>\n\n<why_it_fails>\n- This changes behavior (now rejects emails like \"user@localhost\")\n- Tests might fail, or worse, pass and ship breaking change\n- Not refactoring - this is modifying functionality\n- Users who relied on old behavior experience regression\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n1. Extract validation (pure refactoring, no behavior change)\n2. Commit with tests passing\n3. THEN add new validation as separate feature with new tests\n4. Two clear commits: refactoring vs. feature addition\n\n**What you gain:**\n- Clear history of what changed when\n- Easy to revert feature without losing refactoring\n- Tests document exact behavior changes\n- No surprises in production\n</correction>\n</example>\n\n<example>\n<scenario>Developer does big-bang refactoring</scenario>\n\n<code>\n# Changes made all at once:\n- Renamed 15 functions across 5 files\n- Extracted 3 new classes\n- Moved code between 10 files\n- Reorganized module structure\n- Updated all import statements\n\n# Then runs tests\n$ cargo test\n... 23 test failures ...\n\n# Now what? Which change broke what?\n</code>\n\n<why_it_fails>\n- Can't identify which specific change broke tests\n- Reverting means losing ALL work\n- Fixing requires re-debugging entire refactoring\n- Wastes hours trying to untangle failures\n- Might give up and revert everything\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n1. Rename ONE function → test → commit\n2. Extract ONE class → test → commit\n3. Move ONE file → test → commit\n4. Continue one change at a time\n\n**If test fails:**\n- Know exactly which change broke it\n- Revert ONE commit, not all work\n- Fix or make smaller change\n- Continue from known-good state\n\n**What you gain:**\n- Tests break → immediately know why\n- Each commit is reviewable independently\n- Can stop halfway with useful progress\n- Confidence from continuous green tests\n- Clear history for future developers\n</correction>\n</example>\n\n<example>\n<scenario>Developer refactors code without tests</scenario>\n\n<code>\n// Legacy code with no tests\nfn process_payment(amount: f64, user_id: i64) -> Result<PaymentId> {\n    // 200 lines of complex payment logic\n    // Multiple edge cases\n    // No tests exist\n}\n\n// Developer refactors without tests:\n// - Extracts 5 methods\n// - Renames variables\n// - Simplifies conditionals\n// - \"Looks good to me!\"\n\n// Deploys to production\n// 💥 Payments fail for amounts over $1000\n// Edge case handling was accidentally changed\n</code>\n\n<why_it_fails>\n- No tests to verify behavior preserved\n- Complex logic has hidden edge cases\n- Subtle behavior changes go unnoticed\n- Breaks in production, not development\n- Costs customer trust and emergency debugging\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n1. **Write tests FIRST** (using hyperpowers:test-driven-development)\n   - Test happy path\n   - Test all edge cases (amounts over $1000, etc.)\n   - Test error conditions\n   - Run tests → all pass (documenting current behavior)\n\n2. **Then refactor with tests as safety net**\n   - Extract method → run tests → commit\n   - Rename → run tests → commit\n   - Simplify → run tests → commit\n\n3. **Tests catch any behavior changes immediately**\n\n**What you gain:**\n- Confidence behavior is preserved\n- Edge cases documented in tests\n- Catches subtle changes before production\n- Future refactoring is also safe\n- Tests serve as documentation\n</correction>\n</example>\n</examples>\n\n<refactor_vs_rewrite>\n## When to Refactor\n\n- Tests exist and pass\n- Changes are incremental\n- Business logic stays same\n- Can transform in small, safe steps\n- Each step independently valuable\n\n## When to Rewrite\n\n- No tests exist (write tests first, then refactor)\n- Fundamental architecture change needed\n- Easier to rebuild than modify\n- Requirements changed significantly\n- After 3+ failed refactoring attempts\n\n**Rule:** If you need to change test assertions (not just add tests), you're rewriting, not refactoring.\n\n## Strangler Fig Pattern (Hybrid)\n\n**When to use:**\n- Need to replace legacy system but can't tolerate downtime\n- Want incremental migration with continuous monitoring\n- System too large to refactor in one go\n\n**How it works:**\n\n1. **Transform:** Create modernized components alongside legacy\n2. **Coexist:** Both systems run in parallel (façade routes requests)\n3. **Eliminate:** Retire old functionality piece by piece\n\n**Example:**\n\n```\nLegacy: Monolithic user service (50K LOC)\nGoal: Microservices architecture\n\nStep 1 (Transform):\n- Create new UserService microservice\n- Implement user creation endpoint\n- Tests pass in isolation\n\nStep 2 (Coexist):\n- Add routing layer (façade)\n- Route POST /users to new service\n- Route GET /users to legacy service (for now)\n- Monitor both, compare results\n\nStep 3 (Eliminate):\n- Once confident, migrate GET /users to new service\n- Remove user creation from legacy\n- Repeat for remaining endpoints\n```\n\n**Benefits:**\n- Incremental replacement reduces risk\n- Legacy continues operating during transition\n- Can pause/rollback at any point\n- Each migration step is independently valuable\n\n**Use refactoring within components, Strangler Fig for replacing systems.**\n</refactor_vs_rewrite>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Tests must stay green** throughout refactoring → If they fail, you changed behavior (stop and undo)\n2. **Commit after each small change** → Large commits hide which change broke what\n3. **One transformation at a time** → Multiple changes = impossible to debug failures\n4. **Run tests after EVERY change** → Delayed testing doesn't tell you which change broke it\n5. **If tests fail 3+ times, question approach** → Might need to rewrite instead, or add tests first\n\n## Common Excuses\n\nAll of these mean: **Stop and return to the change→test→commit cycle**\n\n- \"Small refactoring, don't need tests between steps\"\n- \"I'll test at the end\"\n- \"Tests are slow, I'll run once at the end\"\n- \"Just fixing bugs while refactoring\" (bug fixes = behavior changes = not refactoring)\n- \"Easier to do all at once\"\n- \"I know it works without tests\"\n- \"While I'm here, I'll also...\" (scope creep during refactoring)\n- \"Tests will fail temporarily but I'll fix them\" (tests must stay green)\n</critical_rules>\n\n<verification_checklist>\nBefore marking refactoring complete:\n\n- [ ] All tests pass (verified with hyperpowers:test-runner agent)\n- [ ] No new linter warnings\n- [ ] No behavior changes introduced\n- [ ] Code is cleaner/simpler than before\n- [ ] Each commit in history is small and safe\n- [ ] bd task documents what was done and why\n- [ ] Can explain what each transformation did\n\n**Can't check all boxes?** Return to process and fix before closing bd task.\n</verification_checklist>\n\n<integration>\n**This skill requires:**\n- hyperpowers:test-driven-development (for writing tests before refactoring if none exist)\n- hyperpowers:verification-before-completion (for final verification)\n- hyperpowers:test-runner agent (for running tests without context pollution)\n\n**This skill is called by:**\n- General development workflows when improving code structure\n- After features are complete and working\n- When preparing code for new features\n\n**Agents used:**\n- test-runner (runs tests/commits without polluting main context)\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Common refactoring patterns](resources/refactoring-patterns.md) - Extract Method, Extract Class, Inline, etc.\n- [Complete refactoring session example](resources/example-session.md) - Minute-by-minute walkthrough\n\n**When stuck:**\n- Tests fail after change → Undo (git restore), make smaller change\n- 3+ failures → Question if refactoring is right approach, consider rewrite\n- No tests exist → Use hyperpowers:test-driven-development to write tests first\n- Unsure how small → If it touches more than one function/file, it's too big\n</resources>"
              },
              {
                "name": "review-implementation",
                "description": "Use after hyperpowers:executing-plans completes all tasks - verifies implementation against bd spec, all success criteria met, anti-patterns avoided",
                "path": "skills/review-implementation/SKILL.md",
                "frontmatter": {
                  "name": "review-implementation",
                  "description": "Use after hyperpowers:executing-plans completes all tasks - verifies implementation against bd spec, all success criteria met, anti-patterns avoided"
                },
                "content": "<skill_overview>\nReview completed implementation against bd epic to catch gaps before claiming completion; spec is contract, implementation must fulfill contract completely.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow the 4-step review process exactly. Review with Google Fellow-level scrutiny. Never skip automated checks, quality gates, or code reading. No approval without evidence for every criterion.\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | Deliverable |\n|------|--------|-------------|\n| 1 | Load bd epic + all tasks | TodoWrite with tasks to review |\n| 2 | Review each task (automated checks, quality gates, read code, **audit tests**, verify criteria) | Findings per task |\n| 3 | Report findings (approved / gaps found) | Review decision |\n| 4 | Gate: If approved → finishing-a-development-branch, If gaps → STOP | Next action |\n\n**Review Perspective:** Google Fellow-level SRE with 20+ years experience reviewing junior engineer code.\n\n**Test Quality Gate:** Every new test must catch a real bug. Tautological tests (pass by definition, test mocks, verify compiler-checked facts) = GAPS FOUND.\n</quick_reference>\n\n<when_to_use>\n- hyperpowers:executing-plans completed all tasks\n- Before claiming work is complete\n- Before hyperpowers:finishing-a-development-branch\n- Want to verify implementation matches spec\n\n**Don't use for:**\n- Mid-implementation (use hyperpowers:executing-plans)\n- Before all tasks done\n- Code reviews of external PRs (this is self-review)\n</when_to_use>\n\n<the_process>\n## Step 1: Load Epic Specification\n\n**Announce:** \"I'm using hyperpowers:review-implementation to verify implementation matches spec. Reviewing with Google Fellow-level scrutiny.\"\n\n**Get epic and tasks:**\n\n```bash\nbd show bd-1          # Epic specification\nbd dep tree bd-1      # Task tree\nbd list --parent bd-1 # All tasks\n```\n\n**Create TodoWrite tracker:**\n\n```\nTodoWrite todos:\n- Review bd-2: Task Name\n- Review bd-3: Task Name\n- Review bd-4: Task Name\n- Compile findings and make decision\n```\n\n---\n\n## Step 2: Review Each Task\n\nFor each task:\n\n### A. Read Task Specification\n\n```bash\nbd show bd-3\n```\n\nExtract:\n- Goal (what problem solved?)\n- Success criteria (how verify done?)\n- Implementation checklist (files/functions/tests)\n- Key considerations (edge cases)\n- Anti-patterns (prohibited patterns)\n\n---\n\n### B. Run Automated Code Completeness Checks\n\n```bash\n# TODOs/FIXMEs without issue numbers\nrg -i \"todo|fixme\" src/ tests/ || echo \"✅ None\"\n\n# Stub implementations\nrg \"unimplemented!|todo!|unreachable!|panic!\\(\\\"not implemented\" src/ || echo \"✅ None\"\n\n# Unsafe patterns in production\nrg \"\\.unwrap\\(\\)|\\.expect\\(\" src/ | grep -v \"/tests/\" || echo \"✅ None\"\n\n# Ignored/skipped tests\nrg \"#\\[ignore\\]|#\\[skip\\]|\\.skip\\(\\)\" tests/ src/ || echo \"✅ None\"\n```\n\n---\n\n### C. Run Quality Gates (via test-runner agent)\n\n**IMPORTANT:** Use hyperpowers:test-runner agent to avoid context pollution.\n\n```\nDispatch hyperpowers:test-runner: \"Run: cargo test\"\nDispatch hyperpowers:test-runner: \"Run: cargo fmt --check\"\nDispatch hyperpowers:test-runner: \"Run: cargo clippy -- -D warnings\"\nDispatch hyperpowers:test-runner: \"Run: .git/hooks/pre-commit\"\n```\n\n---\n\n### D. Read Implementation Files\n\n**CRITICAL:** READ actual files, not just git diff.\n\n```bash\n# See changes\ngit diff main...HEAD -- src/auth/jwt.ts\n\n# THEN READ FULL FILE\nRead tool: src/auth/jwt.ts\n```\n\n**While reading, check:**\n- ✅ Code implements checklist items (not stubs)\n- ✅ Error handling uses proper patterns (Result, try/catch)\n- ✅ Edge cases from \"Key Considerations\" handled\n- ✅ Code is clear and maintainable\n- ✅ No anti-patterns present\n\n---\n\n### E. Code Quality Review (Google Fellow Perspective)\n\n**Assume code written by junior engineer. Apply production-grade scrutiny.**\n\n**Error Handling:**\n- Proper use of Result/Option or try/catch?\n- Error messages helpful for production debugging?\n- No unwrap/expect in production?\n- Errors propagate with context?\n- Failure modes graceful?\n\n**Safety:**\n- No unsafe blocks without justification?\n- Proper bounds checking?\n- No potential panics?\n- No data races?\n- No SQL injection, XSS vulnerabilities?\n\n**Clarity:**\n- Would junior understand in 6 months?\n- Single responsibility per function?\n- Descriptive variable names?\n- Complex logic explained?\n- No clever tricks - obvious and boring?\n\n**Testing (CRITICAL - Apply strict scrutiny):**\n- Edge cases covered (empty, max, Unicode)?\n- Tests catch real bugs, not just inflate coverage?\n- Test names describe specific bug prevented?\n- Tests test behavior, not implementation?\n- Failure scenarios tested?\n- No tautological tests (see Test Quality Audit below)?\n\n**Production Readiness:**\n- Comfortable deploying to production?\n- Could cause outage or data loss?\n- Performance acceptable under load?\n- Logging sufficient for debugging?\n\n---\n\n### E2. Test Quality Audit (Mandatory for All New Tests)\n\n**CRITICAL:** Review every new/modified test for meaningfulness. Tautological tests are WORSE than no tests - they give false confidence.\n\n**For each test, ask:**\n1. **What bug would this catch?** → If you can't name a specific failure mode, test is pointless\n2. **Could production code break while this test passes?** → If yes, test is too weak\n3. **Does this test a real user scenario?** → Or just implementation details?\n4. **Is the assertion meaningful?** → `expect(result != nil)` is weaker than `expect(result == expectedValue)`\n\n**Red flags (REJECT implementation until fixed):**\n- ❌ Tests that only verify syntax/existence (\"enum has cases\", \"struct has fields\")\n- ❌ Tautological tests (pass by definition: `expect(builder.build() != nil)` when build() can't return nil)\n- ❌ Tests that duplicate implementation (testing 1+1==2 by asserting 1+1==2)\n- ❌ Tests without meaningful assertions (call code but don't verify outcomes matter)\n- ❌ Tests that verify mock behavior instead of production code\n- ❌ Codable/Equatable round-trip tests with only happy path data\n- ❌ Generic test names (\"test_basic\", \"test_it_works\", \"test_model\")\n\n**Examples of meaningless tests to reject:**\n\n```swift\n// ❌ REJECT: Tautological - compiler ensures enum has cases\nfunc testEnumHasCases() {\n    _ = MyEnum.caseOne  // This proves nothing\n    _ = MyEnum.caseTwo\n}\n\n// ❌ REJECT: Tautological - build() returns non-optional, can't be nil\nfunc testBuilderReturnsValue() {\n    let result = Builder().build()\n    #expect(result != nil)  // Always passes by type system\n}\n\n// ❌ REJECT: Tests mock, not production code\nfunc testServiceCallsAPI() {\n    let mock = MockAPI()\n    let service = Service(api: mock)\n    service.fetchData()\n    #expect(mock.fetchCalled)  // Tests mock behavior, not real logic\n}\n\n// ❌ REJECT: Happy path only, no edge cases\nfunc testCodable() {\n    let original = User(name: \"John\", age: 30)\n    let data = try! encoder.encode(original)\n    let decoded = try! decoder.decode(User.self, from: data)\n    #expect(decoded == original)  // What about empty name? Max age? Unicode?\n}\n```\n\n**Examples of meaningful tests to approve:**\n\n```swift\n// ✅ APPROVE: Catches missing validation bug\nfunc testEmptyPayloadReturnsValidationError() {\n    let result = validator.validate(payload: \"\")\n    #expect(result == .error(.emptyPayload))\n}\n\n// ✅ APPROVE: Catches race condition bug\nfunc testConcurrentWritesDontCorruptData() {\n    let store = ThreadSafeStore()\n    DispatchQueue.concurrentPerform(iterations: 1000) { i in\n        store.write(key: \"k\\(i)\", value: i)\n    }\n    #expect(store.count == 1000)  // Would fail if race condition exists\n}\n\n// ✅ APPROVE: Catches error handling bug\nfunc testMalformedJSONReturns400Not500() {\n    let response = api.parse(json: \"{invalid\")\n    #expect(response.status == 400)  // Not 500 which would indicate unhandled exception\n}\n\n// ✅ APPROVE: Catches encoding bug with edge case\nfunc testUnicodeNamePreservedAfterRoundtrip() {\n    let original = User(name: \"日本語テスト 🎉\")\n    let decoded = roundtrip(original)\n    #expect(decoded.name == original.name)\n}\n```\n\n**Audit process:**\n```bash\n# Find all new/modified test files\ngit diff main...HEAD --name-only | grep -E \"(test|spec)\"\n\n# Read each test file\nRead tool: tests/new_feature_test.swift\n\n# For EACH test function, document:\n# - Test name\n# - What bug it catches (or \"TAUTOLOGICAL\" if none)\n# - Verdict: ✅ Keep / ⚠️ Strengthen / ❌ Remove/Replace\n```\n\n**If tautological tests found:**\n```markdown\n## Test Quality Audit: GAPS FOUND ❌\n\n### Tautological/Meaningless Tests\n| Test | Problem | Action |\n|------|---------|--------|\n| testEnumHasCases | Compiler already ensures this | ❌ Remove |\n| testBuilderReturns | Non-optional return, can't be nil | ❌ Remove |\n| testCodable | Happy path only, no edge cases | ⚠️ Add: empty, unicode, max values |\n| testServiceCalls | Tests mock, not production | ❌ Replace with integration test |\n\n**Cannot approve until tests are meaningful.**\n```\n\n---\n\n### F. Verify Success Criteria with Evidence\n\nFor EACH criterion in bd task:\n- Run verification command\n- Check actual output\n- Don't assume - verify with evidence\n- Use hyperpowers:test-runner for tests/lints\n\n**Example:**\n\n```\nCriterion: \"All tests passing\"\nCommand: cargo test\nEvidence: \"127 tests passed, 0 failures\"\nResult: ✅ Met\n\nCriterion: \"No unwrap in production\"\nCommand: rg \"\\.unwrap\\(\\)\" src/\nEvidence: \"No matches\"\nResult: ✅ Met\n```\n\n---\n\n### G. Check Anti-Patterns\n\nSearch for each prohibited pattern from bd task:\n\n```bash\n# Example anti-patterns from task\nrg \"\\.unwrap\\(\\)\" src/  # If task prohibits unwrap\nrg \"TODO\" src/          # If task prohibits untracked TODOs\nrg \"\\.skip\\(\\)\" tests/  # If task prohibits skipped tests\n```\n\n---\n\n### H. Verify Key Considerations\n\nRead code to confirm edge cases handled:\n- Empty input validation\n- Unicode handling\n- Concurrent access\n- Failure modes\n- Performance concerns\n\n**Example:** Task says \"Must handle empty payload\" → Find validation code for empty payload.\n\n---\n\n### I. Record Findings\n\n```markdown\n### Task: bd-3 - Implement JWT authentication\n\n#### Automated Checks\n- TODOs: ✅ None\n- Stubs: ✅ None\n- Unsafe patterns: ❌ Found `.unwrap()` at src/auth/jwt.ts:45\n- Ignored tests: ✅ None\n\n#### Quality Gates\n- Tests: ✅ Pass (127 tests)\n- Formatting: ✅ Pass\n- Linting: ❌ 3 warnings\n- Pre-commit: ❌ Fails due to linting\n\n#### Files Reviewed\n- src/auth/jwt.ts: ⚠️ Contains `.unwrap()` at line 45\n- tests/auth/jwt_test.rs: ✅ Complete\n\n#### Code Quality\n- Error Handling: ⚠️ Uses unwrap instead of proper error propagation\n- Safety: ✅ Good\n- Clarity: ✅ Good\n- Testing: See Test Quality Audit below\n\n#### Test Quality Audit (New/Modified Tests)\n| Test | Bug It Catches | Verdict |\n|------|----------------|---------|\n| test_valid_token_accepted | Missing validation | ✅ Keep |\n| test_expired_token_rejected | Expiration bypass | ✅ Keep |\n| test_jwt_struct_exists | Nothing (tautological) | ❌ Remove |\n| test_encode_decode | Encoding bug (but happy path only) | ⚠️ Add edge cases |\n\n**Tautological tests found:** 1 (test_jwt_struct_exists)\n**Weak tests found:** 1 (test_encode_decode needs edge cases)\n\n#### Success Criteria\n1. \"All tests pass\": ✅ Met - Evidence: 127 tests passed\n2. \"Pre-commit passes\": ❌ Not met - Evidence: clippy warnings\n3. \"No unwrap in production\": ❌ Not met - Evidence: Found at jwt.ts:45\n\n#### Anti-Patterns\n- \"NO unwrap in production\": ❌ Violated at src/auth/jwt.ts:45\n\n#### Issues\n**Critical:**\n1. unwrap() at jwt.ts:45 - violates anti-pattern, must use proper error handling\n2. Tautological test: test_jwt_struct_exists must be removed\n\n**Important:**\n3. 3 clippy warnings block pre-commit hook\n4. test_encode_decode needs edge cases (empty, unicode, max length)\n```\n\n---\n\n### J. Mark Task Reviewed (TodoWrite)\n\n---\n\n## Step 3: Report Findings\n\nAfter reviewing ALL tasks:\n\n**If NO gaps:**\n\n```markdown\n## Implementation Review: APPROVED ✅\n\nReviewed bd-1 (OAuth Authentication) against implementation.\n\n### Tasks Reviewed\n- bd-2: Configure OAuth provider ✅\n- bd-3: Implement token exchange ✅\n- bd-4: Add refresh logic ✅\n\n### Verification Summary\n- All success criteria verified\n- No anti-patterns detected\n- All key considerations addressed\n- All files implemented per spec\n\n### Evidence\n- Tests: 127 passed, 0 failures (2.3s)\n- Linting: No warnings\n- Pre-commit: Pass\n- Code review: Production-ready\n\nReady to proceed to hyperpowers:finishing-a-development-branch.\n```\n\n**If gaps found:**\n\n```markdown\n## Implementation Review: GAPS FOUND ❌\n\nReviewed bd-1 (OAuth Authentication) against implementation.\n\n### Tasks with Gaps\n\n#### bd-3: Implement token exchange\n**Gaps:**\n- ❌ Success criterion not met: \"Pre-commit hooks pass\"\n  - Evidence: cargo clippy shows 3 warnings\n- ❌ Anti-pattern violation: Found `.unwrap()` at src/auth/jwt.ts:45\n- ⚠️ Key consideration not addressed: \"Empty payload validation\"\n  - No check for empty payload in generateToken()\n\n#### bd-4: Add refresh logic\n**Gaps:**\n- ❌ Success criterion not met: \"All tests passing\"\n  - Evidence: test_verify_expired_token failing\n\n### Cannot Proceed\nImplementation does not match spec. Fix gaps before completing.\n```\n\n---\n\n## Step 4: Gate Decision\n\n**If APPROVED:**\n```\nAnnounce: \"I'm using hyperpowers:finishing-a-development-branch to complete this work.\"\n\nUse Skill tool: hyperpowers:finishing-a-development-branch\n```\n\n**If GAPS FOUND:**\n```\nSTOP. Do not proceed to finishing-a-development-branch.\nFix gaps or discuss with partner.\nRe-run review after fixes.\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer only checks git diff, doesn't read actual files</scenario>\n\n<code>\n# Review process\ngit diff main...HEAD  # Shows changes\n\n# Developer sees:\n+ function generateToken(payload) {\n+   return jwt.sign(payload, secret);\n+ }\n\n# Approves based on diff\n\"Looks good, token generation implemented ✅\"\n\n# Misses: Full context shows no validation\nfunction generateToken(payload) {\n  // No validation of payload!\n  // No check for empty payload (key consideration)\n  // No error handling if jwt.sign fails\n  return jwt.sign(payload, secret);\n}\n</code>\n\n<why_it_fails>\n- Git diff shows additions, not full context\n- Missed that empty payload not validated (key consideration)\n- Missed that error handling missing (quality issue)\n- False approval - gaps exist but not caught\n- Will fail in production when empty payload passed\n</why_it_fails>\n\n<correction>\n**Correct review process:**\n\n```bash\n# See changes\ngit diff main...HEAD -- src/auth/jwt.ts\n\n# THEN READ FULL FILE\nRead tool: src/auth/jwt.ts\n```\n\n**Reading full file reveals:**\n```javascript\nfunction generateToken(payload) {\n  // Missing: empty payload check (key consideration from bd task)\n  // Missing: error handling for jwt.sign failure\n  return jwt.sign(payload, secret);\n}\n```\n\n**Record in findings:**\n```\n⚠️ Key consideration not addressed: \"Empty payload validation\"\n- No check for empty payload in generateToken()\n- Code at src/auth/jwt.ts:15-17\n\n⚠️ Error handling: jwt.sign can throw, not handled\n```\n\n**What you gain:**\n- Caught gaps that git diff missed\n- Full context reveals missing validation\n- Quality issues identified before production\n- Spec compliance verified, not assumed\n</correction>\n</example>\n\n<example>\n<scenario>Developer assumes tests passing means done</scenario>\n\n<code>\n# Run tests\ncargo test\n# Output: 127 tests passed\n\n# Developer concludes\n\"Tests pass, implementation complete ✅\"\n\n# Proceeds to finishing-a-development-branch\n\n# Misses:\n- bd task has 5 success criteria\n- Only checked 1 (tests pass)\n- Anti-pattern: unwrap() present (prohibited)\n- Key consideration: Unicode handling not tested\n- Linter has warnings (blocks pre-commit)\n</code>\n\n<why_it_fails>\n- Tests passing ≠ spec compliance\n- Didn't verify all success criteria\n- Didn't check anti-patterns\n- Didn't verify key considerations\n- Pre-commit will fail (blocks merge)\n- Ships code violating anti-patterns\n</why_it_fails>\n\n<correction>\n**Correct review checks ALL criteria:**\n\n```markdown\nbd task has 5 success criteria:\n1. \"All tests pass\" ✅ - Evidence: 127 passed\n2. \"Pre-commit passes\" ❌ - Evidence: clippy warns (3 warnings)\n3. \"No unwrap in production\" ❌ - Evidence: Found at jwt.ts:45\n4. \"Unicode handling tested\" ⚠️ - Need to verify test exists\n5. \"Rate limiting implemented\" ⚠️ - Need to check code\n\nResult: 1/5 criteria verified met. GAPS EXIST.\n```\n\n**Run additional checks:**\n```bash\n# Check criterion 2\ncargo clippy\n# 3 warnings found ❌\n\n# Check criterion 3\nrg \"\\.unwrap\\(\\)\" src/\n# src/auth/jwt.ts:45 ❌\n\n# Check criterion 4\nrg \"unicode\" tests/\n# No matches ⚠️ Need to verify\n```\n\n**Decision: GAPS FOUND, cannot proceed**\n\n**What you gain:**\n- Verified ALL criteria, not just tests\n- Caught anti-pattern violations\n- Caught pre-commit blockers\n- Prevented shipping non-compliant code\n- Spec contract honored completely\n</correction>\n</example>\n\n<example>\n<scenario>Developer rationalizes skipping rigor for \"simple\" task</scenario>\n\n<code>\nbd task: \"Add logging to error paths\"\n\n# Developer thinks: \"Simple task, just added console.log\"\n# Skips:\n- Automated checks (assumes no issues)\n- Code quality review (seems obvious)\n- Full success criteria verification\n\n# Approves quickly:\n\"Logging added ✅\"\n\n# Misses:\n- console.log used instead of proper logger (anti-pattern)\n- Only added to 2 of 5 error paths (incomplete)\n- No test verifying logs actually output (criterion)\n- Logs contain sensitive data (security issue)\n</code>\n\n<why_it_fails>\n- \"Simple\" tasks have hidden complexity\n- Skipped rigor catches exactly these issues\n- Incomplete implementation (2/5 paths)\n- Security vulnerability shipped\n- Anti-pattern not caught\n- Failed success criterion (test logs)\n</why_it_fails>\n\n<correction>\n**Follow full review process:**\n\n```bash\n# Automated checks\nrg \"console\\.log\" src/\n# Found at error-handler.ts:12, 15 ⚠️\n\n# Read bd task\nbd show bd-5\n\n# Success criteria:\n# 1. \"All error paths logged\"\n# 2. \"No sensitive data in logs\"\n# 3. \"Test verifies log output\"\n\n# Check criterion 1\ngrep -n \"throw new Error\" src/\n# 5 locations found\n# Only 2 have logging ❌ Incomplete\n\n# Check criterion 2\nRead tool: src/error-handler.ts\n# Logs contain password field ❌ Security issue\n\n# Check criterion 3\nrg \"test.*log\" tests/\n# No matches ❌ Test missing\n```\n\n**Decision: GAPS FOUND**\n- Incomplete (3/5 error paths missing logs)\n- Security issue (logs password)\n- Anti-pattern (console.log instead of logger)\n- Missing test\n\n**What you gain:**\n- \"Simple\" task revealed multiple gaps\n- Security vulnerability caught pre-production\n- Rigor prevents incomplete work shipping\n- All criteria must be met, no exceptions\n</correction>\n</example>\n\n<example>\n<scenario>Developer approves implementation with high test coverage but tautological tests</scenario>\n\n<code>\n# Test results show good coverage\ncargo test\n# 45 tests passed ✅\n# Coverage: 92% ✅\n\n# Developer approves based on numbers\n\"Tests pass with 92% coverage, implementation complete ✅\"\n\n# Proceeds to finishing-a-development-branch\n\n# Later in production:\n# - Validation bypassed because test only checked \"validator exists\"\n# - Race condition because test only checked \"lock was acquired\"\n# - Encoding corruption because test only checked \"encode != nil\"\n</code>\n\n<why_it_fails>\n- High coverage doesn't mean meaningful tests\n- Tests verified existence/syntax, not behavior\n- Tautological tests passed by definition:\n  - `expect(validator != nil)` - always passes, doesn't test validation logic\n  - `expect(lock.acquire())` - tests mock, not thread safety\n  - `expect(encoded.count > 0)` - tests non-empty, not correctness\n- Production bugs occurred despite \"good\" test coverage\n- Coverage metrics were gamed with meaningless tests\n</why_it_fails>\n\n<correction>\n**Audit each test for meaningfulness:**\n\n```bash\n# Find new tests\ngit diff main...HEAD --name-only | grep test\n\n# Read and audit each test\nRead tool: tests/validator_test.swift\n```\n\n**For each test, document:**\n\n```markdown\n#### Test Quality Audit\n\n| Test | Assertion | Bug Caught? | Verdict |\n|------|-----------|-------------|---------|\n| testValidatorExists | `!= nil` | ❌ None (compiler checks) | ❌ Remove |\n| testValidInput | `isValid == true` | ⚠️ Happy path only | ⚠️ Add edge cases |\n| testEmptyInputFails | `isValid == false` | ✅ Missing validation | ✅ Keep |\n| testLockAcquired | mock.acquireCalled | ❌ Tests mock | ❌ Replace |\n| testConcurrentAccess | count == expected | ✅ Race condition | ✅ Keep |\n| testEncodeNotNil | `!= nil` | ❌ Type guarantees this | ❌ Remove |\n| testUnicodeRoundtrip | decoded == original | ✅ Encoding corruption | ✅ Keep |\n\n**Tautological tests:** 3 (must remove)\n**Weak tests:** 1 (must strengthen)\n**Meaningful tests:** 3 (keep)\n```\n\n**Decision: GAPS FOUND ❌**\n\n```markdown\n## Test Quality Audit: GAPS FOUND\n\n### Tautological Tests (Must Remove)\n- testValidatorExists: Compiler ensures non-nil, test proves nothing\n- testLockAcquired: Tests mock behavior, not actual thread safety\n- testEncodeNotNil: Return type is non-optional, can never be nil\n\n### Weak Tests (Must Strengthen)\n- testValidInput: Only happy path, add:\n  - testEmptyStringRejected\n  - testMaxLengthRejected\n  - testUnicodeNormalized\n\n### Action Required\nRemove 3 tautological tests, add 3 edge case tests, then re-review.\n```\n\n**What you gain:**\n- Real test quality, not coverage theater\n- Bugs caught before production\n- Tests that actually verify behavior\n- Confidence in test suite\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Review every task** → No skipping \"simple\" tasks\n2. **Run all automated checks** → TODOs, stubs, unwrap, ignored tests\n3. **Read actual files with Read tool** → Not just git diff\n4. **Verify every success criterion** → With evidence, not assumptions\n5. **Check all anti-patterns** → Search for prohibited patterns\n6. **Apply Google Fellow scrutiny** → Production-grade code review\n7. **Audit all new tests for meaningfulness** → Tautological tests = gaps, not coverage\n8. **If gaps found → STOP** → Don't proceed to finishing-a-development-branch\n\n## Common Excuses\n\nAll of these mean: **STOP. Follow full review process.**\n\n- \"Tests pass, must be complete\" (Tests ≠ spec, check all criteria)\n- \"I implemented it, it's done\" (Implementation ≠ compliance, verify)\n- \"No time for thorough review\" (Gaps later cost more than review now)\n- \"Looks good to me\" (Opinion ≠ evidence, run verifications)\n- \"Small gaps don't matter\" (Spec is contract, all criteria matter)\n- \"Will fix in next PR\" (This PR completes this epic, fix now)\n- \"Can check diff instead of files\" (Diff shows changes, not context)\n- \"Automated checks cover it\" (Checks + code review both required)\n- \"Success criteria passing means done\" (Also check anti-patterns, quality, edge cases)\n- \"Tests exist, so testing is complete\" (Tautological tests = false confidence)\n- \"Coverage looks good\" (Coverage can be gamed with meaningless tests)\n- \"Tests are boilerplate, don't need review\" (Every test must catch a real bug)\n- \"It's just a simple existence check\" (Compiler already checks existence)\n\n</critical_rules>\n\n<verification_checklist>\nBefore approving implementation:\n\n**Per task:**\n- [ ] Read bd task specification completely\n- [ ] Ran all automated checks (TODOs, stubs, unwrap, ignored tests)\n- [ ] Ran all quality gates via test-runner agent (tests, format, lint, pre-commit)\n- [ ] Read actual implementation files with Read tool (not just diff)\n- [ ] Reviewed code quality with Google Fellow perspective\n- [ ] **Audited all new tests for meaningfulness (not tautological)**\n- [ ] Verified every success criterion with evidence\n- [ ] Checked every anti-pattern (searched for prohibited patterns)\n- [ ] Verified every key consideration addressed in code\n\n**Overall:**\n- [ ] Reviewed ALL tasks (no exceptions)\n- [ ] TodoWrite tracker shows all tasks reviewed\n- [ ] Compiled findings (approved or gaps)\n- [ ] If approved: all criteria met for all tasks\n- [ ] If gaps: documented exactly what missing\n\n**Can't check all boxes?** Return to Step 2 and complete review.\n</verification_checklist>\n\n<integration>\n**This skill is called by:**\n- hyperpowers:executing-plans (Step 5, after all tasks executed)\n\n**This skill calls:**\n- hyperpowers:finishing-a-development-branch (if approved)\n- hyperpowers:test-runner agent (for quality gates)\n\n**This skill uses:**\n- hyperpowers:verification-before-completion principles (evidence before claims)\n\n**Call chain:**\n```\nhyperpowers:executing-plans → hyperpowers:review-implementation → hyperpowers:finishing-a-development-branch\n                         ↓\n                   (if gaps: STOP)\n```\n\n**CRITICAL:** Use bd commands (bd show, bd list, bd dep tree), never read `.beads/issues.jsonl` directly.\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Code quality standards by language](resources/quality-standards.md)\n- [Common anti-patterns to check](resources/anti-patterns-reference.md)\n- [Production readiness checklist](resources/production-checklist.md)\n\n**When stuck:**\n- Unsure if gap critical → If violates criterion, it's a gap\n- Criteria ambiguous → Ask user for clarification before approving\n- Anti-pattern unclear → Search for it, document if found\n- Quality concern → Document as gap, don't rationalize away\n</resources>"
              },
              {
                "name": "root-cause-tracing",
                "description": "Use when errors occur deep in execution - traces bugs backward through call stack to find original trigger, not just symptom",
                "path": "skills/root-cause-tracing/SKILL.md",
                "frontmatter": {
                  "name": "root-cause-tracing",
                  "description": "Use when errors occur deep in execution - traces bugs backward through call stack to find original trigger, not just symptom"
                },
                "content": "<skill_overview>\nBugs manifest deep in the call stack; trace backward until you find the original trigger, then fix at source, not where error appears.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow the backward tracing process strictly, but adapt instrumentation and debugging techniques to your language and tools.\n</rigidity_level>\n\n<quick_reference>\n| Step | Action | Question |\n|------|--------|----------|\n| 1 | Read error completely | What failed and where? |\n| 2 | Find immediate cause | What code directly threw this? |\n| 3 | Trace backward one level | What called this code? |\n| 4 | Keep tracing up stack | What called that? |\n| 5 | Find where bad data originated | Where was invalid value created? |\n| 6 | Fix at source | Address root cause |\n| 7 | Add defense at each layer | Validate assumptions as backup |\n\n**Core rule:** Never fix just where error appears. Fix where problem originates.\n</quick_reference>\n\n<when_to_use>\n- Error happens deep in execution (not at entry point)\n- Stack trace shows long call chain\n- Unclear where invalid data originated\n- Need to find which test/code triggers problem\n- Error message points to utility/library code\n\n**Example symptoms:**\n- \"Database rejects empty string\" ← Where did empty string come from?\n- \"File not found: ''\" ← Why is path empty?\n- \"Invalid argument to function\" ← Who passed invalid argument?\n- \"Null pointer dereference\" ← What should have been initialized?\n</when_to_use>\n\n<the_process>\n## 1. Observe the Symptom\n\nRead the complete error:\n\n```\nError: Invalid email format: \"\"\n  at validateEmail (validator.ts:42)\n  at UserService.create (user-service.ts:18)\n  at ApiHandler.createUser (api-handler.ts:67)\n  at HttpServer.handleRequest (server.ts:123)\n  at TestCase.test_create_user (user.test.ts:10)\n```\n\n**Symptom:** Email validation fails on empty string\n**Location:** Deep in validator utility\n\n**DON'T fix here yet.** This might be symptom, not source.\n\n---\n\n## 2. Find Immediate Cause\n\nWhat code directly causes this?\n\n```typescript\n// validator.ts:42\nfunction validateEmail(email: string): boolean {\n  if (!email) throw new Error(`Invalid email format: \"${email}\"`);\n  return EMAIL_REGEX.test(email);\n}\n```\n\n**Question:** Why is email empty? Keep tracing.\n\n---\n\n## 3. Trace Backward: What Called This?\n\nUse stack trace:\n\n```typescript\n// user-service.ts:18\ncreate(request: UserRequest): User {\n  validateEmail(request.email); // Called with request.email = \"\"\n  // ...\n}\n```\n\n**Question:** Why is `request.email` empty? Keep tracing.\n\n---\n\n## 4. Keep Tracing Up the Stack\n\n```typescript\n// api-handler.ts:67\nasync createUser(req: Request): Promise<Response> {\n  const userRequest = {\n    name: req.body.name,\n    email: req.body.email || \"\", // ← FOUND IT!\n  };\n  return this.userService.create(userRequest);\n}\n```\n\n**Root cause found:** API handler provides default empty string when email missing.\n\n---\n\n## 5. Identify the Pattern\n\n**Why empty string as default?**\n- Misguided \"safety\": Thought empty string better than undefined\n- Should reject invalid request at API boundary\n- Downstream code assumes data already validated\n\n---\n\n## 6. Fix at Source\n\n```typescript\n// api-handler.ts (SOURCE FIX)\nasync createUser(req: Request): Promise<Response> {\n  if (!req.body.email) {\n    return Response.badRequest(\"Email is required\");\n  }\n  const userRequest = {\n    name: req.body.name,\n    email: req.body.email, // No default, already validated\n  };\n  return this.userService.create(userRequest);\n}\n```\n\n---\n\n## 7. Add Defense in Depth\n\nAfter fixing source, add validation at each layer as backup:\n\n```typescript\n// Layer 1: API - Reject invalid input (PRIMARY FIX)\nif (!req.body.email) return Response.badRequest(\"Email required\");\n\n// Layer 2: Service - Validate assumptions\nassert(request.email, \"email must be present\");\n\n// Layer 3: Utility - Defensive check\nif (!email) throw new Error(\"invariant violated: email empty\");\n```\n\n**Primary fix at source. Defense is backup, not replacement.**\n</the_process>\n\n<debugging_approaches>\n## Option 1: Guide User Through Debugger\n\n**IMPORTANT:** Claude cannot run interactive debuggers. Guide user through debugger commands.\n\n```\n\"Let's use lldb to trace backward through the call stack.\n\nPlease run these commands:\n  lldb target/debug/myapp\n  (lldb) breakpoint set --file validator.rs --line 42\n  (lldb) run\n\nWhen breakpoint hits:\n  (lldb) frame variable email     # Check value here\n  (lldb) bt                       # See full call stack\n  (lldb) up                       # Move to caller\n  (lldb) frame variable request   # Check values in caller\n  (lldb) up                       # Move up again\n  (lldb) frame variable           # Where empty string created?\n\nPlease share:\n  1. Value of 'email' at validator.rs:42\n  2. Value of 'request.email' in user_service.rs\n  3. Value of 'req.body.email' in api_handler.rs\n  4. Where does empty string first appear?\"\n```\n\n---\n\n## Option 2: Add Instrumentation (Claude CAN Do This)\n\nWhen debugger not available or issue intermittent:\n\n```rust\n// Add at error location\nfn validate_email(email: &str) -> Result<()> {\n    eprintln!(\"DEBUG validate_email called:\");\n    eprintln!(\"  email: {:?}\", email);\n    eprintln!(\"  backtrace: {}\", std::backtrace::Backtrace::capture());\n\n    if email.is_empty() {\n        return Err(Error::InvalidEmail);\n    }\n    // ...\n}\n```\n\n**Critical:** Use `eprintln!()` or `console.error()` in tests (not logger - may be suppressed).\n\n**Run and analyze:**\n\n```bash\ncargo test 2>&1 | grep \"DEBUG validate_email\" -A 10\n```\n\nLook for:\n- Test file names in backtraces\n- Line numbers triggering the call\n- Patterns (same test? same parameter?)\n</debugging_approaches>\n\n<finding_polluting_tests>\n## Finding Which Test Pollutes\n\nWhen something appears during tests but you don't know which:\n\n**Binary search approach:**\n\n```bash\n# Run half the tests\nnpm test tests/first-half/*.test.ts\n# Pollution appears? Yes → in first half, No → second half\n\n# Subdivide\nnpm test tests/first-quarter/*.test.ts\n\n# Continue until specific file\nnpm test tests/auth/login.test.ts  ← Found it!\n```\n\n**Or test isolation:**\n\n```bash\n# Run tests one at a time\nfor test in tests/**/*.test.ts; do\n  echo \"Testing: $test\"\n  npm test \"$test\"\n  if [ -d .git ]; then\n    echo \"FOUND POLLUTER: $test\"\n    break\n  fi\ndone\n```\n</finding_polluting_tests>\n\n<examples>\n<example>\n<scenario>Developer fixes symptom, not source</scenario>\n\n<code>\n# Error appears in git utility:\nfn git_init(directory: &str) {\n    Command::new(\"git\")\n        .arg(\"init\")\n        .current_dir(directory)\n        .run()\n}\n\n# Error: \"Invalid argument: empty directory\"\n\n# Developer adds validation at symptom:\nfn git_init(directory: &str) {\n    if directory.is_empty() {\n        panic!(\"Directory cannot be empty\"); // Band-aid\n    }\n    Command::new(\"git\").arg(\"init\").current_dir(directory).run()\n}\n</code>\n\n<why_it_fails>\n- Fixes symptom, not source (where empty string created)\n- Same bug will appear elsewhere directory is used\n- Doesn't explain WHY directory was empty\n- Future code might make same mistake\n- Band-aid hides the real problem\n</why_it_fails>\n\n<correction>\n**Trace backward:**\n\n1. git_init called with directory=\"\"\n2. WorkspaceManager.init(projectDir=\"\")\n3. Session.create(projectDir=\"\")\n4. Test: Project.create(context.tempDir)\n5. **SOURCE:** context.tempDir=\"\" (accessed before beforeEach!)\n\n**Fix at source:**\n\n```typescript\nfunction setupTest() {\n  let _tempDir: string | undefined;\n\n  return {\n    beforeEach() {\n      _tempDir = makeTempDir();\n    },\n    get tempDir(): string {\n      if (!_tempDir) {\n        throw new Error(\"tempDir accessed before beforeEach!\");\n      }\n      return _tempDir;\n    }\n  };\n}\n```\n\n**What you gain:**\n- Fixes actual bug (test timing issue)\n- Prevents same mistake elsewhere\n- Clear error at source, not deep in stack\n- No empty strings propagating through system\n</correction>\n</example>\n\n<example>\n<scenario>Developer stops tracing too early</scenario>\n\n<code>\n# Error in API handler\nasync createUser(req: Request): Promise<Response> {\n  const userRequest = {\n    name: req.body.name,\n    email: req.body.email || \"\", // Suspicious!\n  };\n  return this.userService.create(userRequest);\n}\n\n# Developer sees empty string default and \"fixes\" it:\nemail: req.body.email || \"noreply@example.com\"\n\n# Ships to production\n# Bug: Users created without email input get noreply@example.com\n# Database has fake emails, can't distinguish missing from real\n</code>\n\n<why_it_fails>\n- Stopped at first suspicious code\n- Didn't question WHY empty string was default\n- \"Fixed\" by replacing with different wrong default\n- Root cause: shouldn't accept missing email at all\n- Validation should happen at API boundary\n</why_it_fails>\n\n<correction>\n**Keep tracing to understand intent:**\n\n1. Why was empty string default?\n2. Should email be optional or required?\n3. What does API spec say?\n4. What does database schema say?\n\n**Findings:**\n- Email column is NOT NULL in database\n- API docs say email is required\n- Empty string was workaround, not design\n\n**Fix at source (validate at boundary):**\n\n```typescript\nasync createUser(req: Request): Promise<Response> {\n  // Validate at API boundary\n  if (!req.body.email) {\n    return Response.badRequest(\"Email is required\");\n  }\n\n  const userRequest = {\n    name: req.body.name,\n    email: req.body.email, // No default needed\n  };\n  return this.userService.create(userRequest);\n}\n```\n\n**What you gain:**\n- Validates at correct layer (API boundary)\n- Clear error message to client\n- No invalid data propagates downstream\n- Database constraints enforced\n- Matches API specification\n</correction>\n</example>\n\n<example>\n<scenario>Complex multi-layer trace to find original trigger</scenario>\n\n<code>\n# Problem: .git directory appearing in source code directory during tests\n\n# Symptom location:\nError: Cannot initialize git repo (repo already exists)\nLocation: src/workspace/git.rs:45\n\n# Developer adds check:\nif Path::new(\".git\").exists() {\n    return Err(\"Git already initialized\");\n}\n\n# Doesn't help - still appears in wrong place!\n</code>\n\n<why_it_fails>\n- Detects symptom, doesn't prevent it\n- .git still created in wrong directory\n- Doesn't explain HOW it gets there\n- Pollution still happens, just detected\n</why_it_fails>\n\n<correction>\n**Trace through multiple layers:**\n\n```\n1. git init runs with cwd=\"\"\n   ↓ Why is cwd empty?\n\n2. WorkspaceManager.init(projectDir=\"\")\n   ↓ Why is projectDir empty?\n\n3. Session.create(projectDir=\"\")\n   ↓ Why was empty string passed?\n\n4. Test: Project.create(context.tempDir)\n   ↓ Why is context.tempDir empty?\n\n5. ROOT CAUSE:\n   const context = setupTest(); // tempDir=\"\" initially\n   Project.create(context.tempDir); // Accessed at top level!\n\n   beforeEach(() => {\n     context.tempDir = makeTempDir(); // Assigned here\n   });\n\n   TEST ACCESSED TEMPDIR BEFORE BEFOREEACH RAN!\n```\n\n**Fix at source (make early access impossible):**\n\n```typescript\nfunction setupTest() {\n  let _tempDir: string | undefined;\n\n  return {\n    beforeEach() {\n      _tempDir = makeTempDir();\n    },\n    get tempDir(): string {\n      if (!_tempDir) {\n        throw new Error(\"tempDir accessed before beforeEach!\");\n      }\n      return _tempDir;\n    }\n  };\n}\n```\n\n**Then add defense at each layer:**\n\n```rust\n// Layer 1: Test framework (PRIMARY FIX)\n// Getter throws if accessed early\n\n// Layer 2: Project validation\nfn create(directory: &str) -> Result<Self> {\n    if directory.is_empty() {\n        return Err(\"Directory cannot be empty\");\n    }\n    // ...\n}\n\n// Layer 3: Workspace validation\nfn init(path: &Path) -> Result<()> {\n    if !path.exists() {\n        return Err(\"Path must exist\");\n    }\n    // ...\n}\n\n// Layer 4: Environment guard\nfn git_init(dir: &Path) -> Result<()> {\n    if env::var(\"NODE_ENV\") != Ok(\"test\".to_string()) {\n        if !dir.starts_with(\"/tmp\") {\n            panic!(\"Refusing to git init outside test dir\");\n        }\n    }\n    // ...\n}\n```\n\n**What you gain:**\n- Primary fix prevents early access (source)\n- Each layer validates assumptions (defense)\n- Clear error at source, not deep in stack\n- Environment guard prevents production pollution\n- Multi-layer defense catches future mistakes\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Never fix just where error appears** → Trace backward to find source\n2. **Don't stop at first suspicious code** → Keep tracing to original trigger\n3. **Fix at source first** → Defense is backup, not primary fix\n4. **Use debugger OR instrumentation** → Don't guess at call chain\n5. **Add defense at each layer** → After fixing source, validate assumptions throughout\n\n## Common Excuses\n\nAll of these mean: **STOP. Trace backward to find source.**\n\n- \"Error is obvious here, I'll add validation\" (That's a symptom fix)\n- \"Stack trace shows the problem\" (Shows symptom location, not source)\n- \"This code should handle empty values\" (Why is value empty? Find source.)\n- \"Too deep to trace, I'll add defensive check\" (Defense without source fix = band-aid)\n- \"Multiple places could cause this\" (Trace to find which one actually does)\n</critical_rules>\n\n<verification_checklist>\nBefore claiming root cause fixed:\n\n- [ ] Traced backward through entire call chain\n- [ ] Found where invalid data was created (not just passed)\n- [ ] Identified WHY invalid data was created (pattern/assumption)\n- [ ] Fixed at source (where bad data originates)\n- [ ] Added defense at each layer (validate assumptions)\n- [ ] Verified fix with test (reproduces original bug, passes with fix)\n- [ ] Confirmed no other code paths have same pattern\n\n**Can't check all boxes?** Keep tracing backward.\n</verification_checklist>\n\n<integration>\n**This skill is called by:**\n- hyperpowers:debugging-with-tools (Phase 2: Trace Backward Through Call Stack)\n- When errors occur deep in execution\n- When unclear where invalid data originated\n\n**This skill requires:**\n- Stack traces or debugger access\n- Ability to add instrumentation (logging)\n- Understanding of call chain\n\n**This skill calls:**\n- hyperpowers:test-driven-development (write regression test after finding source)\n- hyperpowers:verification-before-completion (verify fix works)\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Debugger commands by language](resources/debugger-reference.md)\n- [Instrumentation patterns](resources/instrumentation-patterns.md)\n- [Defense-in-depth examples](resources/defense-patterns.md)\n\n**When stuck:**\n- Can't find source → Add instrumentation at each layer, run test\n- Stack trace unclear → Use debugger to inspect variables at each frame\n- Multiple suspects → Add instrumentation to all, find which actually executes\n- Intermittent issue → Add instrumentation and wait for reproduction\n</resources>"
              },
              {
                "name": "skills-auto-activation",
                "description": "Use when skills aren't activating reliably - covers official solutions (better descriptions) and custom hook system for deterministic skill activation",
                "path": "skills/skills-auto-activation/SKILL.md",
                "frontmatter": {
                  "name": "skills-auto-activation",
                  "description": "Use when skills aren't activating reliably - covers official solutions (better descriptions) and custom hook system for deterministic skill activation"
                },
                "content": "<skill_overview>\nSkills often don't activate despite keywords; make activation reliable through better descriptions, explicit triggers, or custom hooks.\n</skill_overview>\n\n<rigidity_level>\nHIGH FREEDOM - Choose solution level based on project needs (Level 1 for simple, Level 3 for complex). Hook implementation is flexible pattern, not rigid process.\n</rigidity_level>\n\n<quick_reference>\n| Level | Solution | Effort | Reliability | When to Use |\n|-------|----------|--------|-------------|-------------|\n| 1 | Better descriptions + explicit requests | Low | Moderate | Small projects, starting out |\n| 2 | CLAUDE.md references | Low | Moderate | Document patterns |\n| 3 | Custom hook system | High | Very High | Large projects, established patterns |\n\n**Hyperpowers includes:** Auto-activation hook at `hooks/user-prompt-submit/10-skill-activator.js`\n</quick_reference>\n\n<when_to_use>\nUse this skill when:\n- Skills you created aren't being used automatically\n- Need consistent skill activation across sessions\n- Large codebases with established patterns\n- Manual \"/use skill-name\" gets tedious\n\n**Prerequisites:**\n- Skills properly configured (name, description, SKILL.md)\n- Code execution enabled (Settings > Capabilities)\n- Skills toggled on (Settings > Capabilities)\n</when_to_use>\n\n<the_problem>\n## What Users Experience\n\n**Symptoms:**\n- Keywords from skill descriptions present → skill not used\n- Working on files that should trigger skills → nothing\n- Skills exist but sit unused\n\n**Community reports:**\n- GitHub Issue #9954: \"Skills not available even if explicitly enabled\"\n- \"Claude knows it should use skills, but it's not reliable\"\n- Skills activation is \"not reliable yet\"\n\n**Root cause:** Skills rely on Claude recognizing relevance (not deterministic)\n</the_problem>\n\n<solution_levels>\n## Level 1: Official Solutions (Start Here)\n\n### 1. Improve Skill Descriptions\n\n❌ **Bad:**\n```yaml\nname: backend-dev\ndescription: Helps with backend development\n```\n\n✅ **Good:**\n```yaml\nname: backend-dev-guidelines\ndescription: Use when creating API routes, controllers, services, or repositories in backend - enforces TypeScript patterns, error handling with Sentry, and Prisma repository pattern\n```\n\n**Key elements:**\n- Specific keywords: \"API routes\", \"controllers\", \"services\"\n- When to use: \"Use when creating...\"\n- What it enforces: Patterns, error handling\n\n### 2. Be Explicit in Requests\n\nInstead of: \"How do I create an endpoint?\"\n\nTry: \"Use my backend-dev-guidelines skill to create an endpoint\"\n\n**Result:** Works, but tedious\n\n### 3. Check Settings\n\n- Settings > Capabilities > Enable code execution\n- Settings > Capabilities > Toggle Skills on\n- Team/Enterprise: Check org-level settings\n\n---\n\n## Level 2: Skill References (Moderate)\n\nReference skills in CLAUDE.md:\n\n```markdown\n## When Working on Backend\n\nBefore making changes:\n1. Check `/skills/backend-dev-guidelines` for patterns\n2. Follow repository pattern for database access\n\nThe backend-dev-guidelines skill contains complete examples.\n```\n\n**Pros:** No custom code\n**Cons:** Claude still might not check\n\n---\n\n## Level 3: Custom Hook System (Advanced)\n\n**How it works:**\n1. UserPromptSubmit hook analyzes prompt before Claude sees it\n2. Matches keywords, intent patterns, file paths\n3. Injects skill activation reminder into context\n4. Claude sees \"🎯 USE these skills\" before processing\n\n**Result:** \"Night and day difference\" - skills consistently used\n\n### Architecture\n\n```\nUser submits prompt\n    ↓\nUserPromptSubmit hook intercepts\n    ↓\nAnalyze prompt (keywords, intent, files)\n    ↓\nCheck skill-rules.json for matches\n    ↓\nInject activation reminder\n    ↓\nClaude sees: \"🎯 USE these skills: ...\"\n    ↓\nClaude loads and uses relevant skills\n```\n\n### Configuration: skill-rules.json\n\n```json\n{\n  \"backend-dev-guidelines\": {\n    \"type\": \"domain\",\n    \"enforcement\": \"suggest\",\n    \"priority\": \"high\",\n    \"promptTriggers\": {\n      \"keywords\": [\"backend\", \"controller\", \"service\", \"API\", \"endpoint\"],\n      \"intentPatterns\": [\n        \"(create|add|build).*?(route|endpoint|controller|service)\",\n        \"(how to|pattern).*?(backend|API)\"\n      ]\n    },\n    \"fileTriggers\": {\n      \"pathPatterns\": [\"backend/src/**/*.ts\", \"server/**/*.ts\"],\n      \"contentPatterns\": [\"express\\\\.Router\", \"export.*Controller\"]\n    }\n  },\n  \"test-driven-development\": {\n    \"type\": \"process\",\n    \"enforcement\": \"suggest\",\n    \"priority\": \"high\",\n    \"promptTriggers\": {\n      \"keywords\": [\"test\", \"TDD\", \"testing\"],\n      \"intentPatterns\": [\n        \"(write|add|create).*?(test|spec)\",\n        \"test.*(first|before|TDD)\"\n      ]\n    },\n    \"fileTriggers\": {\n      \"pathPatterns\": [\"**/*.test.ts\", \"**/*.spec.ts\"],\n      \"contentPatterns\": [\"describe\\\\(\", \"it\\\\(\", \"test\\\\(\"]\n    }\n  }\n}\n```\n\n### Trigger Types\n\n1. **Keyword Triggers** - Simple string matching (case insensitive)\n2. **Intent Pattern Triggers** - Regex for actions + objects\n3. **File Path Triggers** - Glob patterns for file paths\n4. **Content Pattern Triggers** - Regex in file content\n\n### Hook Implementation (High-Level)\n\n```javascript\n#!/usr/bin/env node\n// ~/.claude/hooks/user-prompt-submit/skill-activator.js\n\nconst fs = require('fs');\nconst path = require('path');\n\n// Load skill rules\nconst rules = JSON.parse(fs.readFileSync(\n  path.join(process.env.HOME, '.claude/skill-rules.json'), 'utf8'\n));\n\n// Read prompt from stdin\nlet promptData = '';\nprocess.stdin.on('data', chunk => promptData += chunk);\n\nprocess.stdin.on('end', () => {\n    const prompt = JSON.parse(promptData);\n\n    // Analyze prompt for skill matches\n    const activatedSkills = analyzePrompt(prompt.text);\n\n    if (activatedSkills.length > 0) {\n        // Inject skill activation reminder\n        const context = `\n🎯 SKILL ACTIVATION CHECK\n\nRelevant skills for this prompt:\n${activatedSkills.map(s => `- **${s.skill}** (${s.priority} priority)`).join('\\n')}\n\nCheck if these skills should be used before responding.\n`;\n\n        console.log(JSON.stringify({\n            decision: 'approve',\n            additionalContext: context\n        }));\n    } else {\n        console.log(JSON.stringify({ decision: 'approve' }));\n    }\n});\n\nfunction analyzePrompt(text) {\n    // Match against all skill rules\n    // Return list of activated skills with priorities\n}\n```\n\n**For complete working implementation:** See [resources/hook-implementation.md](resources/hook-implementation.md)\n\n### Progressive Enhancement\n\n**Phase 1 (Week 1):** Basic keyword matching\n```json\n{\"keywords\": [\"backend\", \"API\", \"controller\"]}\n```\n\n**Phase 2 (Week 2):** Add intent patterns\n```json\n{\"intentPatterns\": [\"(create|add).*?(route|endpoint)\"]}\n```\n\n**Phase 3 (Week 3):** Add file triggers\n```json\n{\"fileTriggers\": {\"pathPatterns\": [\"backend/**/*.ts\"]}}\n```\n\n**Phase 4 (Ongoing):** Refine based on observation\n</solution_levels>\n\n<results>\n### Before Hook System\n\n- Skills sit unused despite perfect keywords\n- Manual \"/use skill-name\" every time\n- Inconsistent patterns across codebase\n- Time spent fixing \"creative interpretations\"\n\n### After Hook System\n\n- Skills activate automatically and reliably\n- Consistent patterns enforced\n- Claude self-checks before showing code\n- \"Night and day difference\"\n\n**Real user:** \"Skills went from 'expensive decorations' to actually useful\"\n</results>\n\n<limitations>\n## Hook System Limitations\n\n1. **Requires hook system** - Not built into Claude Code\n2. **Maintenance overhead** - skill-rules.json needs updates\n3. **May over-activate** - Too many skills overwhelm context\n4. **Not perfect** - Still relies on Claude using activated skills\n\n## Considerations\n\n**Token usage:**\n- Activation reminder adds ~50-100 tokens per prompt\n- Multiple skills add more tokens\n- Use priorities to limit activation\n\n**Performance:**\n- Hook adds ~100-300ms to prompt processing\n- Acceptable for quality improvement\n- Optimize regex patterns if slow\n\n**Maintenance:**\n- Update rules when adding new skills\n- Review activation logs monthly\n- Refine patterns based on misses\n</limitations>\n\n<alternatives>\n## Approach 1: MCP Integration\n\nUse Model Context Protocol to provide skills as context.\n\n**Pros:** Built into Claude system\n**Cons:** Still not deterministic, same activation issues\n\n## Approach 2: Custom System Prompt\n\nModify Claude's system prompt to always check certain skills.\n\n**Pros:** Works without hooks\n**Cons:** Limited to Pro plan, can't customize per-project\n\n## Approach 3: Manual Discipline\n\nAlways explicitly request skill usage.\n\n**Pros:** No setup required\n**Cons:** Tedious, easy to forget, doesn't scale\n\n## Approach 4: Skill Consolidation\n\nCombine all guidelines into CLAUDE.md.\n\n**Pros:** Always loaded\n**Cons:** Violates progressive disclosure, wastes tokens\n\n**Recommendation:** Level 3 (hooks) for large projects, Level 1 for smaller projects\n</alternatives>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Try Level 1 first** → Better descriptions and explicit requests before building hooks\n2. **Observe before building** → Watch which prompts should activate skills\n3. **Start with keywords** → Add complexity incrementally (keywords → intent → files)\n4. **Keep hook fast (<1 second)** → Don't block prompt processing\n5. **Maintain skill-rules.json** → Update when skills change\n\n## Common Excuses\n\nAll of these mean: **Try Level 1 first, then decide.**\n\n- \"Skills should just work automatically\" (They should, but don't reliably - workaround needed)\n- \"Hook system too complex\" (Setup takes 2 hours, saves hundreds of hours)\n- \"I'll manually specify skills\" (You'll forget, it gets tedious)\n- \"Improving descriptions will fix it\" (Helps, but not deterministic)\n- \"This is overkill\" (Maybe - start Level 1, upgrade if needed)\n</critical_rules>\n\n<verification_checklist>\nBefore building hook system:\n\n- [ ] Tried improving skill descriptions (Level 1)\n- [ ] Tried explicit skill requests (Level 1)\n- [ ] Checked all settings are enabled\n- [ ] Observed which prompts should activate skills\n- [ ] Identified patterns in failures\n- [ ] Project large enough to justify hook overhead\n- [ ] Have time for 2-hour setup + ongoing maintenance\n\n**If Level 1 works:** Don't build hook system\n\n**If Level 1 insufficient:** Build hook system (Level 3)\n</verification_checklist>\n\n<integration>\n**This skill covers:** Skill activation strategies\n\n**Related skills:**\n- hyperpowers:building-hooks (how to build hook system)\n- hyperpowers:using-hyper (when to use skills generally)\n- hyperpowers:writing-skills (creating skills that activate well)\n\n**This skill enables:**\n- Consistent enforcement of patterns\n- Automatic guideline checking\n- Reliable skill usage across sessions\n\n**Hyperpowers includes:** Auto-activation hook at `hooks/user-prompt-submit/10-skill-activator.js`\n</integration>\n\n<resources>\n**Detailed implementation:**\n- [Complete working hook code](resources/hook-implementation.md)\n- [skill-rules.json examples](resources/skill-rules-examples.md)\n- [Troubleshooting guide](resources/troubleshooting.md)\n\n**Official documentation:**\n- [Anthropic Skills Best Practices](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices)\n- [Claude Code Hooks Guide](https://docs.claude.com/en/docs/claude-code/hooks-guide)\n\n**When stuck:**\n- Skills still not activating → Check Settings > Capabilities\n- Hook not working → Check ~/.claude/logs/hooks.log\n- Over-activation → Reduce keywords, increase priority thresholds\n- Under-activation → Add more keywords, broaden intent patterns\n</resources>"
              },
              {
                "name": "sre-task-refinement",
                "description": "Use when you have to refine subtasks into actionable plans ensuring that all corner cases are handled and we understand all the requirements.",
                "path": "skills/sre-task-refinement/SKILL.md",
                "frontmatter": {
                  "name": "sre-task-refinement",
                  "description": "Use when you have to refine subtasks into actionable plans ensuring that all corner cases are handled and we understand all the requirements."
                },
                "content": "<skill_overview>\nReview bd task plans with Google Fellow SRE perspective to ensure junior engineer can execute without questions; catch edge cases, verify granularity, strengthen criteria, prevent production issues before implementation.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow the 8-category checklist exactly. Apply all categories to every task. No skipping red flag checks. Always verify no placeholder text after updates. Reject plans with critical gaps.\n</rigidity_level>\n\n<quick_reference>\n| Category | Key Questions | Auto-Reject If |\n|----------|---------------|----------------|\n| 1. Granularity | Tasks 4-8 hours? Phases <16 hours? | Any task >16h without breakdown |\n| 2. Implementability | Junior can execute without questions? | Vague language, missing details |\n| 3. Success Criteria | 3+ measurable criteria per task? | Can't verify (\"works well\") |\n| 4. Dependencies | Correct parent-child, blocking relationships? | Circular dependencies |\n| 5. Safety Standards | Anti-patterns specified? Error handling? | No anti-patterns section |\n| 6. Edge Cases | Empty input? Unicode? Concurrency? Failures? | No edge case consideration |\n| 7. Red Flags | Placeholder text? Vague instructions? | \"[detailed above]\", \"TODO\" |\n| 8. Test Meaningfulness | Tests catch real bugs? Not tautological? | Tests only verify syntax/existence |\n\n**Perspective**: Google Fellow SRE with 20+ years experience reviewing junior engineer designs.\n\n**Time**: Don't rush - catching one gap pre-implementation saves hours of rework.\n</quick_reference>\n\n<when_to_use>\nUse when:\n- Reviewing bd epic/feature plans before implementation\n- Need to ensure junior engineer can execute without questions\n- Want to catch edge cases and failure modes upfront\n- Need to verify task granularity (4-8 hour subtasks)\n- After hyperpowers:writing-plans creates initial plan\n- Before hyperpowers:executing-plans starts implementation\n\nDon't use when:\n- Task already being implemented (too late)\n- Just need to understand existing code (use codebase-investigator)\n- Debugging issues (use debugging-with-tools)\n- Want to create plan from scratch (use brainstorming → writing-plans)\n</when_to_use>\n\n<the_process>\n## Announcement\n\n**Announce:** \"I'm using hyperpowers:sre-task-refinement to review this plan with Google Fellow-level scrutiny.\"\n\n---\n\n## Review Checklist (Apply to Every Task)\n\n### 1. Task Granularity\n\n**Check:**\n- [ ] No task >8 hours (subtasks) or >16 hours (phases)?\n- [ ] Large phases broken into 4-8 hour subtasks?\n- [ ] Each subtask independently completable?\n- [ ] Each subtask has clear deliverable?\n\n**If task >16 hours:**\n- Create subtasks with `bd create`\n- Link with `bd dep add child parent --type parent-child`\n- Update parent to coordinator role\n\n---\n\n### 2. Implementability (Junior Engineer Test)\n\n**Check:**\n- [ ] Can junior engineer implement without asking questions?\n- [ ] Function signatures/behaviors described, not just \"implement X\"?\n- [ ] Test scenarios described (what they verify, not just names)?\n- [ ] \"Done\" clearly defined with verifiable criteria?\n- [ ] All file paths specified or marked \"TBD: new file\"?\n\n**Red flags:**\n- \"Implement properly\" (how?)\n- \"Add support\" (for what exactly?)\n- \"Make it work\" (what does working mean?)\n- File paths missing or ambiguous\n\n---\n\n### 3. Success Criteria Quality\n\n**Check:**\n- [ ] Each task has 3+ specific, measurable success criteria?\n- [ ] All criteria testable/verifiable (not subjective)?\n- [ ] Includes automated verification (tests pass, clippy clean)?\n- [ ] No vague criteria like \"works well\" or \"is implemented\"?\n\n**Good criteria examples:**\n- ✅ \"5+ unit tests pass (valid VIN, invalid checksum, various formats)\"\n- ✅ \"Clippy clean with no warnings\"\n- ✅ \"Performance: <100ms for 1000 records\"\n\n**Bad criteria examples:**\n- ❌ \"Code is good quality\"\n- ❌ \"Works correctly\"\n- ❌ \"Is implemented\"\n\n---\n\n### 4. Dependency Structure\n\n**Check:**\n- [ ] Parent-child relationships correct (epic → phases → subtasks)?\n- [ ] Blocking dependencies correct (earlier work blocks later)?\n- [ ] No circular dependencies?\n- [ ] Dependency graph makes logical sense?\n\n**Verify with:**\n```bash\nbd dep tree bd-1  # Show full dependency tree\n```\n\n---\n\n### 5. Safety & Quality Standards\n\n**Check:**\n- [ ] Anti-patterns include unwrap/expect prohibition?\n- [ ] Anti-patterns include TODO prohibition (or must have issue #)?\n- [ ] Anti-patterns include stub implementation prohibition?\n- [ ] Error handling requirements specified (use Result, avoid panic)?\n- [ ] Test requirements specific (test names, scenarios listed)?\n\n**Minimum anti-patterns:**\n- ❌ No unwrap/expect in production code\n- ❌ No TODOs without issue numbers\n- ❌ No stub implementations (unimplemented!, todo!)\n- ❌ No regex without catastrophic backtracking check\n\n---\n\n### 6. Edge Cases & Failure Modes (Fellow SRE Perspective)\n\n**Ask for each task:**\n- [ ] What happens with malformed input?\n- [ ] What happens with empty/nil/zero values?\n- [ ] What happens under high load/concurrency?\n- [ ] What happens when dependencies fail?\n- [ ] What happens with Unicode, special characters, large inputs?\n- [ ] Are these edge cases addressed in the plan?\n\n**Add to Key Considerations section:**\n- Edge case descriptions\n- Mitigation strategies\n- References to similar code handling these cases\n\n---\n\n### 7. Red Flags (AUTO-REJECT)\n\n**Check for these - if found, REJECT plan:**\n- ❌ Any task >16 hours without subtask breakdown\n- ❌ Vague language: \"implement properly\", \"add support\", \"make it work\"\n- ❌ Success criteria that can't be verified: \"code is good\", \"works well\"\n- ❌ Missing test specifications\n- ❌ \"We'll handle this later\" or \"TODO\" in the plan itself\n- ❌ No anti-patterns section\n- ❌ Implementation checklist with fewer than 3 items per task\n- ❌ No effort estimates\n- ❌ Missing error handling considerations\n- ❌ **CRITICAL: Placeholder text in design field** - \"[detailed above]\", \"[as specified]\", \"[complete steps here]\"\n\n---\n\n### 8. Test Meaningfulness (Fellow SRE Perspective)\n\n**Tests must catch real bugs, not inflate coverage.** For every test specification:\n\n**Ask these questions:**\n- [ ] What specific bug would this test catch?\n- [ ] Could production code break while this test still passes?\n- [ ] Does this test exercise a real user scenario or failure mode?\n- [ ] Is the assertion meaningful? (`result == expected` vs `result != nil`)\n\n**Red flags (AUTO-REJECT):**\n- ❌ Tests that only verify syntax/existence (\"enum has cases\", \"struct has fields\")\n- ❌ Tautological tests (pass by definition: `expect(builder.build() != nil)` when build() can't return nil)\n- ❌ Tests that duplicate implementation (testing 1+1==2 by checking 1+1==2)\n- ❌ Tests without meaningful assertions (call code but don't verify outcomes)\n- ❌ Tests that verify mocks instead of production code\n- ❌ Round-trip tests that only use happy path (Codable without edge cases)\n- ❌ Tests named generically (\"test_basic\", \"test_it_works\")\n\n**Good test specifications:**\n- ✅ \"test_empty_payload_returns_validation_error\" - catches missing validation\n- ✅ \"test_concurrent_writes_dont_corrupt_data\" - catches race condition\n- ✅ \"test_malformed_json_returns_400_not_500\" - catches error handling bug\n- ✅ \"test_unicode_name_preserved_after_roundtrip\" - catches encoding bugs\n\n**Bad test specifications (reject or strengthen):**\n- ❌ \"test_user_model_exists\" - tautological, compiler catches this\n- ❌ \"test_builder_returns_value\" - tautological if return type non-optional\n- ❌ \"test_basic_functionality\" - vague, what specific bug does it catch?\n- ❌ \"test_encode_decode\" - only happy path, no edge cases specified\n\n**When reviewing test specifications:**\n```markdown\nFor each test in success criteria, verify:\n\nTest: \"test_vin_validation\"\n- What bug does it catch? ⚠️ Unclear - need specific scenarios\n- Could code break while test passes? ⚠️ Unknown without specifics\n\nSTRENGTHEN TO:\n- test_valid_vin_checksum_accepted\n- test_invalid_vin_checksum_rejected (catches missing checksum validation)\n- test_lowercase_vin_normalized (catches case handling bug)\n- test_vin_with_invalid_chars_rejected (catches input validation bug)\n```\n\n---\n\n## Review Process\n\nFor each task in the plan:\n\n**Step 1: Read the task**\n```bash\nbd show bd-3\n```\n\n**Step 2: Apply all 8 checklist categories**\n- Task Granularity\n- Implementability\n- Success Criteria Quality\n- Dependency Structure\n- Safety & Quality Standards\n- Edge Cases & Failure Modes\n- Red Flags\n- Test Meaningfulness\n\n**Step 3: Document findings**\nTake notes:\n- What's done well\n- What's missing\n- What's vague or ambiguous\n- Hidden failure modes not addressed\n- Better approaches or simplifications\n\n**Step 4: Update the task**\n\nUse `bd update` to add missing information:\n\n```bash\nbd update bd-3 --design \"$(cat <<'EOF'\n## Goal\n[Original goal, preserved]\n\n## Effort Estimate\n[Updated estimate if needed]\n\n## Success Criteria\n- [ ] Existing criteria\n- [ ] NEW: Added missing measurable criteria\n\n## Implementation Checklist\n[Complete checklist with file paths]\n\n## Key Considerations (ADDED BY SRE REVIEW)\n\n**Edge Case: Empty Input**\n- What happens when input is empty string?\n- MUST validate input length before processing\n\n**Edge Case: Unicode Handling**\n- What if string contains RTL or surrogate pairs?\n- Use proper Unicode-aware string methods\n\n**Performance Concern: Regex Backtracking**\n- Pattern `.*[a-z]+.*` has catastrophic backtracking risk\n- MUST test with pathological inputs (e.g., 10000 'a's)\n- Use possessive quantifiers or bounded repetition\n\n**Reference Implementation**\n- Study src/similar/module.rs for pattern to follow\n\n## Anti-patterns\n[Original anti-patterns]\n- ❌ NEW: Specific anti-pattern for this task's risks\nEOF\n)\"\n```\n\n**IMPORTANT:** Use `--design` for full detailed description, NOT `--description` (title only).\n\n**Step 5: Verify no placeholder text (MANDATORY)**\n\nAfter updating, read back with `bd show bd-N` and verify:\n- ✅ All sections contain actual content, not meta-references\n- ✅ No placeholder text like \"[detailed above]\", \"[as specified]\", \"[will be added]\"\n- ✅ Implementation steps fully written with actual code examples\n- ✅ Success criteria explicit, not referencing \"criteria above\"\n- ❌ If ANY placeholder text found: REJECT and rewrite with actual content\n\n---\n\n## Breaking Down Large Tasks\n\nIf task >16 hours, create subtasks:\n\n```bash\n# Create first subtask\nbd create \"Subtask 1: [Specific Component]\" \\\n  --type task \\\n  --priority 1 \\\n  --design \"[Complete subtask design with all 7 categories addressed]\"\n# Returns bd-10\n\n# Create second subtask\nbd create \"Subtask 2: [Another Component]\" \\\n  --type task \\\n  --priority 1 \\\n  --design \"[Complete subtask design]\"\n# Returns bd-11\n\n# Link subtasks to parent with parent-child relationship\nbd dep add bd-10 bd-3 --type parent-child  # bd-10 is child of bd-3\nbd dep add bd-11 bd-3 --type parent-child  # bd-11 is child of bd-3\n\n# Add sequential dependencies if needed (LATER depends on EARLIER)\nbd dep add bd-11 bd-10  # bd-11 depends on bd-10 (do bd-10 first)\n\n# Update parent to coordinator\nbd update bd-3 --design \"$(cat <<'EOF'\n## Goal\nCoordinate implementation of [feature]. Broken into N subtasks.\n\n## Success Criteria\n- [ ] All N child subtasks closed\n- [ ] Integration tests pass\n- [ ] [High-level verification criteria]\nEOF\n)\"\n```\n\n---\n\n## Output Format\n\nAfter reviewing all tasks:\n\n```markdown\n## Plan Review Results\n\n### Epic: [Name] ([epic-id])\n\n### Overall Assessment\n[APPROVE ✅ / NEEDS REVISION ⚠️ / REJECT ❌]\n\n### Dependency Structure Review\n[Output of `bd dep tree [epic-id]`]\n\n**Structure Quality**: [✅ Correct / ❌ Issues found]\n- [Comments on parent-child relationships]\n- [Comments on blocking dependencies]\n- [Comments on granularity]\n\n### Task-by-Task Review\n\n#### [Task Name] (bd-N)\n**Type**: [epic/feature/task]\n**Status**: [✅ Ready / ⚠️ Needs Minor Improvements / ❌ Needs Major Revision]\n**Estimated Effort**: [X hours] ([✅ Good / ❌ Too large - needs breakdown])\n\n**Strengths**:\n- [What's done well]\n\n**Critical Issues** (must fix):\n- [Blocking problems]\n\n**Improvements Needed**:\n- [What to add/clarify]\n\n**Edge Cases Missing**:\n- [Failure modes not addressed]\n\n**Changes Made**:\n- [Specific improvements added via `bd update`]\n\n---\n\n[Repeat for each task/phase/subtask]\n\n### Summary of Changes\n\n**Issues Updated**:\n- bd-3 - Added edge case handling for Unicode, regex backtracking risks\n- bd-5 - Broke into 3 subtasks (was 40 hours, now 3x8 hours)\n- bd-7 - Strengthened success criteria (added test names, verification commands)\n\n### Critical Gaps Across Plan\n1. [Pattern of missing items across multiple tasks]\n2. [Systemic issues in the plan]\n\n### Recommendations\n\n[If APPROVE]:\n✅ Plan is solid and ready for implementation.\n- All tasks are junior-engineer implementable\n- Dependency structure is correct\n- Edge cases and failure modes addressed\n\n[If NEEDS REVISION]:\n⚠️ Plan needs improvements before implementation:\n- [List major items that need addressing]\n- After changes, re-run hyperpowers:sre-task-refinement\n\n[If REJECT]:\n❌ Plan has fundamental issues and needs redesign:\n- [Critical problems]\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer reviews task but skips edge case analysis (Category 6)</scenario>\n\n<code>\n# Review of bd-3: Implement VIN scanner\n\n## Checklist review:\n1. Granularity: ✅ 6-8 hours\n2. Implementability: ✅ Junior can implement\n3. Success Criteria: ✅ Has 5 test scenarios\n4. Dependencies: ✅ Correct\n5. Safety Standards: ✅ Anti-patterns present\n6. Edge Cases: [SKIPPED - \"looks straightforward\"]\n7. Red Flags: ✅ None found\n\nConclusion: \"Task looks good, approve ✅\"\n\n# Task ships without edge case review\n# Production issues occur:\n- VIN scanner matches random 17-char strings (no checksum validation)\n- Lowercase VINs not handled (should normalize)\n- Catastrophic regex backtracking on long inputs (DoS vulnerability)\n</code>\n\n<why_it_fails>\n- Skipped Category 6 (Edge Cases) assuming task was \"straightforward\"\n- Didn't ask: What happens with invalid checksums? Lowercase? Long inputs?\n- Missed critical production issues:\n  - False positives (no checksum validation)\n  - Data handling bugs (case sensitivity)\n  - Security vulnerability (regex DoS)\n- Junior engineer didn't know to handle these (not in task)\n- Production incidents occur after deployment\n- Hours of emergency fixes, customer impact\n- SRE review failed to prevent known failure modes\n</why_it_fails>\n\n<correction>\n**Apply Category 6 rigorously:**\n\n```markdown\n## Edge Case Analysis for bd-3: VIN Scanner\n\nAsk for EVERY task:\n- Malformed input? VIN has checksum - must validate, not just pattern match\n- Empty/nil? What if empty string passed?\n- Concurrency? Read-only scanner, no concurrency issues\n- Dependency failures? No external dependencies\n- Unicode/special chars? VIN is alphanumeric only, but what about lowercase?\n- Large inputs? Regex `.*` patterns can cause catastrophic backtracking\n\nFindings:\n❌ VIN checksum validation not mentioned (will match random strings)\n❌ Case normalization not mentioned (lowercase VINs exist)\n❌ Regex backtracking risk not mentioned (DoS vulnerability)\n```\n\n**Update task:**\n```bash\nbd update bd-3 --design \"$(cat <<'EOF'\n[... original content ...]\n\n## Key Considerations (ADDED BY SRE REVIEW)\n\n**VIN Checksum Complexity**:\n- ISO 3779 requires transliteration table (letters → numbers)\n- Weighted sum algorithm with modulo 11\n- Reference: https://en.wikipedia.org/wiki/Vehicle_identification_number#Check_digit\n- MUST validate checksum, not just pattern - prevents false positives\n\n**Case Normalization**:\n- VINs can appear in lowercase\n- MUST normalize to uppercase before validation\n- Test with mixed case: \"1hgbh41jxmn109186\"\n\n**Regex Backtracking Risk**:\n- CRITICAL: Pattern `.*[A-HJ-NPR-Z0-9]{17}.*` has backtracking risk\n- Test with pathological input: 10000 'X's followed by 16-char string\n- Use possessive quantifiers or bounded repetition\n- Reference: https://www.regular-expressions.info/catastrophic.html\n\n**Edge Cases to Test**:\n- Valid VIN with valid checksum (should match)\n- Valid pattern but invalid checksum (should NOT match)\n- Lowercase VIN (should normalize and validate)\n- Ambiguous chars I/O not valid in VIN (should reject)\n- Very long input (should not DoS)\nEOF\n)\"\n```\n\n**What you gain:**\n- Prevented false positives (checksum validation)\n- Prevented data handling bugs (case normalization)\n- Prevented security vulnerability (regex DoS)\n- Junior engineer has complete requirements\n- Production issues caught pre-implementation\n- Proper SRE review preventing known failure modes\n- Customer trust maintained\n</correction>\n</example>\n\n<example>\n<scenario>Developer approves task with placeholder text (Red Flag #10)</scenario>\n\n<code>\n# Review of bd-5: Implement License Plate Scanner\n\nbd show bd-5:\n\n## Implementation Checklist\n- [ ] Create scanner module\n- [ ] [Complete implementation steps detailed above]\n- [ ] Add tests\n\n## Success Criteria\n- [ ] [As specified in the implementation checklist]\n- [ ] Tests pass\n\n## Key Considerations\n- [Will be added during implementation]\n\n# Developer's review:\n\"Looks comprehensive, has implementation checklist and success criteria ✅\"\n\n# During implementation:\nJunior engineer: \"What are the 'implementation steps detailed above'?\"\nJunior engineer: \"What specific success criteria should I verify?\"\nJunior engineer: \"What key considerations exist?\"\n\n# No answers in the task - junior engineer blocked\n# Have to research and add missing information\n# Implementation delayed by 2 days\n</code>\n\n<why_it_fails>\n- Missed Red Flag #10: Placeholder text present\n- \"[Complete implementation steps detailed above]\" is meta-reference, not content\n- \"[As specified in the implementation checklist]\" is circular reference\n- \"[Will be added during implementation]\" is deferral, not specification\n- Junior engineer can't execute - missing critical information\n- Task looks complete but actually incomplete\n- Implementation blocked until details added\n- SRE review failed to catch placeholder text\n</why_it_fails>\n\n<correction>\n**Check for placeholder text after reading:**\n\n```markdown\n## Red Flag Check (Category 7)\n\nRead through bd-5 line by line:\n\nLine 15: \"[Complete implementation steps detailed above]\"\n❌ PLACEHOLDER - \"detailed above\" is meta-reference, not actual content\n\nLine 22: \"[As specified in the implementation checklist]\"\n❌ PLACEHOLDER - Circular reference to another section, not explicit criteria\n\nLine 30: \"[Will be added during implementation]\"\n❌ PLACEHOLDER - Deferral to future, not actual considerations\n\nDECISION: REJECT ❌\nReason: Contains placeholder text - task not ready for implementation\n```\n\n**Update task with actual content:**\n```bash\nbd update bd-5 --design \"$(cat <<'EOF'\n## Implementation Checklist\n- [ ] Create src/scan/plugins/scanners/license_plate.rs\n- [ ] Implement LicensePlateScanner struct with ScanPlugin trait\n- [ ] Add regex patterns for US states:\n  - CA: `[0-9][A-Z]{3}[0-9]{3}` (e.g., 1ABC123)\n  - NY: `[A-Z]{3}[0-9]{4}` (e.g., ABC1234)\n  - TX: `[A-Z]{3}[0-9]{4}|[0-9]{3}[A-Z]{3}` (e.g., ABC1234 or 123ABC)\n  - Generic: `[A-Z0-9]{5,8}` (fallback)\n- [ ] Implement has_healthcare_context() check\n- [ ] Create test module with 8+ test cases\n- [ ] Register in src/scan/plugins/scanners/mod.rs\n\n## Success Criteria\n- [ ] Valid CA plate \"1ABC123\" detected in healthcare context\n- [ ] Valid NY plate \"ABC1234\" detected in healthcare context\n- [ ] Invalid plate \"123\" NOT detected (too short)\n- [ ] Valid plate NOT detected outside healthcare context\n- [ ] 8+ unit tests pass covering all patterns and edge cases\n- [ ] Clippy clean, no warnings\n- [ ] cargo test passes\n\n## Key Considerations\n\n**False Positive Risk**:\n- License plates are short and generic (5-8 chars)\n- MUST require healthcare context via has_healthcare_context()\n- Without context, will match random alphanumeric sequences\n- Test: Random string \"ABC1234\" should NOT match outside healthcare context\n\n**State Format Variations**:\n- 50 US states have different formats\n- Implement common formats (CA, NY, TX) + generic fallback\n- Document which formats supported in module docstring\n- Consider international plates in future iteration\n\n**Performance**:\n- Regex patterns are simple, no backtracking risk\n- Should process <1ms per chunk\n\n**Reference Implementation**:\n- Study src/scan/plugins/scanners/vehicle_identifier.rs\n- Follow same pattern: regex + context check + tests\nEOF\n)\"\n```\n\n**Verify no placeholder text:**\n```bash\nbd show bd-5\n# Read entire output\n# Confirm: All sections have actual content\n# Confirm: No \"[detailed above]\", \"[as specified]\", \"[will be added]\"\n# ✅ Task ready for implementation\n```\n\n**What you gain:**\n- Junior engineer has complete specification\n- No blocked implementation waiting for details\n- All edge cases documented upfront\n- Success criteria explicit and verifiable\n- Key considerations prevent common mistakes\n- No placeholder text - task truly ready\n- Professional SRE review standard maintained\n</correction>\n</example>\n\n<example>\n<scenario>Developer accepts vague success criteria (Category 3)</scenario>\n\n<code>\n# Review of bd-7: Implement Data Encryption\n\nbd show bd-7:\n\n## Success Criteria\n- [ ] Encryption is implemented correctly\n- [ ] Code is good quality\n- [ ] Tests work properly\n\n# Developer's review:\n\"Has 3 success criteria ✅ Meets minimum requirement\"\n\n# During implementation:\nJunior engineer: \"How do I know if encryption is 'correct'?\"\nJunior engineer: \"What makes code 'good quality'?\"\nJunior engineer: \"What does 'tests work properly' mean?\"\n\n# Junior engineer makes best guesses:\n- Uses ECB mode (insecure, should use GCM)\n- No key rotation (bad practice)\n- Tests only happy path (misses edge cases)\n\n# Code review finds critical security issues\n# Complete rewrite required - 3 days wasted\n</code>\n\n<why_it_fails>\n- Category 3 check: \"All criteria testable/verifiable?\" ❌ FAILED\n- \"Implemented correctly\" - not measurable (correct by what standard?)\n- \"Good quality\" - subjective, not verifiable\n- \"Work properly\" - vague, what is proper?\n- Junior engineer can't verify criteria objectively\n- Makes incorrect assumptions filling gaps\n- Security vulnerabilities introduced\n- Wastes time on implementation that fails review\n- SRE review failed to strengthen criteria\n</why_it_fails>\n\n<correction>\n**Apply Category 3 rigorously:**\n\n```markdown\n## Success Criteria Analysis for bd-7\n\nCurrent criteria:\n- [ ] Encryption is implemented correctly\n  ❌ NOT TESTABLE - \"correctly\" is subjective, no standard specified\n\n- [ ] Code is good quality\n  ❌ NOT TESTABLE - \"good quality\" is opinion, not measurable\n\n- [ ] Tests work properly\n  ❌ NOT TESTABLE - \"properly\" is vague, no definition\n\nMinimum requirement: 3+ specific, measurable, testable criteria\nCurrent: 0 testable criteria\nDECISION: REJECT ❌\n```\n\n**Update with measurable criteria:**\n```bash\nbd update bd-7 --design \"$(cat <<'EOF'\n[... original content ...]\n\n## Success Criteria\n\n**Encryption Implementation**:\n- [ ] Uses AES-256-GCM mode (verified in code review)\n- [ ] Key derivation via PBKDF2 with 100,000 iterations (NIST recommendation)\n- [ ] Unique IV generated per encryption (crypto_random)\n- [ ] Authentication tag verified on decryption\n\n**Code Quality** (automated checks):\n- [ ] Clippy clean with no warnings: `cargo clippy -- -D warnings`\n- [ ] Rustfmt compliant: `cargo fmt --check`\n- [ ] No unwrap/expect in production: `rg \"\\.unwrap\\(\\)|\\.expect\\(\" src/` returns 0\n- [ ] No TODOs without issue numbers: `rg \"TODO\" src/` returns 0\n\n**Test Coverage**:\n- [ ] 12+ unit tests pass covering:\n  - test_encrypt_decrypt_roundtrip (happy path)\n  - test_wrong_key_fails_auth (security)\n  - test_modified_ciphertext_fails_auth (security)\n  - test_empty_plaintext (edge case)\n  - test_large_plaintext_10mb (performance)\n  - test_unicode_plaintext (data handling)\n  - test_concurrent_encryption (thread safety)\n  - test_iv_uniqueness (security)\n  - [4 more specific scenarios]\n- [ ] All tests pass: `cargo test encryption`\n- [ ] Test coverage >90%: `cargo tarpaulin --packages encryption`\n\n**Documentation**:\n- [ ] Module docstring explains encryption scheme (AES-256-GCM)\n- [ ] Function docstrings include examples\n- [ ] Security considerations documented (key management, IV handling)\n\n**Security Review**:\n- [ ] No hardcoded keys or IVs (verified via grep)\n- [ ] Key zeroized after use (verified in code)\n- [ ] Constant-time comparison for auth tag (timing attack prevention)\nEOF\n)\"\n```\n\n**What you gain:**\n- Every criterion objectively verifiable\n- Junior engineer knows exactly what \"done\" means\n- Automated checks (clippy, fmt, grep) provide instant feedback\n- Specific test scenarios prevent missed edge cases\n- Security requirements explicit (GCM, PBKDF2, unique IV)\n- No ambiguity - can verify each criterion with command or code review\n- Professional SRE review standard: measurable, testable, specific\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Apply all 8 categories to every task** → No skipping any category for any task\n2. **Reject plans with placeholder text** → \"[detailed above]\", \"[as specified]\" = instant reject\n3. **Verify no placeholder after updates** → Read back with `bd show` and confirm actual content\n4. **Break tasks >16 hours** → Create subtasks, don't accept large tasks\n5. **Strengthen vague criteria** → \"Works correctly\" → measurable verification commands\n6. **Add edge cases to every task** → Empty? Unicode? Concurrency? Failures?\n7. **Never skip Category 6** → Edge case analysis prevents production issues\n8. **Reject tautological tests** → Tests must catch bugs, not verify compiler-checked facts\n\n## Common Excuses\n\nAll of these mean: **STOP. Apply the full process.**\n\n- \"Task looks straightforward\" (Edge cases hide in \"straightforward\" tasks)\n- \"Has 3 criteria, meets minimum\" (Criteria must be measurable, not just 3+ items)\n- \"Placeholder text is just formatting\" (Placeholders mean incomplete specification)\n- \"Can handle edge cases during implementation\" (Must specify upfront, not defer)\n- \"Junior will figure it out\" (Junior should NOT need to figure out - we specify)\n- \"Too detailed, feels like micromanaging\" (Detail prevents questions and rework)\n- \"Taking too long to review\" (One gap caught saves hours of rework)\n- \"Any tests are better than none\" (Tautological tests are worse - give false confidence)\n- \"Tests are specified, don't need to review them\" (Test quality matters more than quantity)\n- \"Coverage metrics will catch missing tests\" (Coverage gaming = meaningless tests)\n</critical_rules>\n\n<verification_checklist>\nBefore completing SRE review:\n\n**Per task reviewed:**\n- [ ] Applied all 8 categories (Granularity, Implementability, Criteria, Dependencies, Safety, Edge Cases, Red Flags, Test Meaningfulness)\n- [ ] Checked for placeholder text in design field\n- [ ] Updated task with missing information via `bd update --design`\n- [ ] Verified updated task with `bd show` (no placeholders remain)\n- [ ] Broke down any task >16 hours into subtasks\n- [ ] Strengthened vague success criteria to measurable\n- [ ] Added edge case analysis to Key Considerations\n- [ ] Strengthened anti-patterns based on failure modes\n- [ ] Verified test specifications catch real bugs (not tautological)\n\n**Overall plan:**\n- [ ] Reviewed ALL tasks/phases/subtasks (no exceptions)\n- [ ] Verified dependency structure with `bd dep tree`\n- [ ] Documented findings for each task\n- [ ] Created summary of changes made\n- [ ] Provided clear recommendation (APPROVE/NEEDS REVISION/REJECT)\n\n**Can't check all boxes?** Return to review process and complete missing steps.\n</verification_checklist>\n\n<integration>\n**This skill is used after:**\n- hyperpowers:writing-plans (creates initial plan)\n- hyperpowers:brainstorming (establishes requirements)\n\n**This skill is used before:**\n- hyperpowers:executing-plans (implements tasks)\n\n**This skill is also called by:**\n- hyperpowers:executing-plans (REQUIRED for new tasks created during execution)\n\n**Call chains:**\n```\nInitial planning:\nhyperpowers:brainstorming → hyperpowers:writing-plans → hyperpowers:sre-task-refinement → hyperpowers:executing-plans\n                                                    ↓\n                                            (if gaps: revise and re-review)\n\nDuring execution (for new tasks):\nhyperpowers:executing-plans → creates new task → hyperpowers:sre-task-refinement → STOP checkpoint\n```\n\n**This skill uses:**\n- bd commands (show, update, create, dep add, dep tree)\n- Google Fellow SRE perspective (20+ years distributed systems)\n- 8-category checklist (mandatory for every task)\n\n**Time expectations:**\n- Small epic (3-5 tasks): 15-20 minutes\n- Medium epic (6-10 tasks): 25-40 minutes\n- Large epic (10+ tasks): 45-60 minutes\n\n**Don't rush:** Catching one critical gap pre-implementation saves hours of rework.\n</integration>\n\n<resources>\n**Review patterns:**\n- Task too large (>16h) → Break into 4-8h subtasks\n- Vague criteria (\"works correctly\") → Measurable commands/checks\n- Missing edge cases → Add to Key Considerations with mitigations\n- Placeholder text → Rewrite with actual content\n- Tautological tests → Strengthen to catch specific bugs\n\n**Test meaningfulness questions:**\n- \"What bug would this catch?\" → If you can't name one, test is pointless\n- \"Could code break while test passes?\" → If yes, test is too weak\n- \"Is this testing the mock or production code?\" → Mock-testing is useless\n- \"Is the assertion meaningful?\" → `!= nil` is weaker than `== expectedValue`\n\n**When stuck:**\n- Unsure if task too large → Ask: Can junior complete in one day?\n- Unsure if criteria measurable → Ask: Can I verify with command/code review?\n- Unsure if edge case matters → Ask: Could this fail in production?\n- Unsure if placeholder → Ask: Does this reference other content instead of providing content?\n- Unsure if test meaningful → Ask: What specific production bug does this prevent?\n\n**Key principle:** Junior engineer should be able to execute task without asking questions. If they would need to ask, specification is incomplete. Tests must catch bugs, not inflate metrics.\n</resources>"
              },
              {
                "name": "test-driven-development",
                "description": "Use when implementing features or fixing bugs - enforces RED-GREEN-REFACTOR cycle requiring tests to fail before writing code",
                "path": "skills/test-driven-development/SKILL.md",
                "frontmatter": {
                  "name": "test-driven-development",
                  "description": "Use when implementing features or fixing bugs - enforces RED-GREEN-REFACTOR cycle requiring tests to fail before writing code"
                },
                "content": "<skill_overview>\nWrite the test first, watch it fail, write minimal code to pass. If you didn't watch the test fail, you don't know if it tests the right thing.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow these exact steps in order. Do not adapt.\n\nViolating the letter of the rules is violating the spirit of the rules.\n</rigidity_level>\n\n<quick_reference>\n\n| Phase | Action | Command Example | Expected Result |\n|-------|--------|-----------------|-----------------|\n| **RED** | Write failing test | `cargo test test_name` | FAIL (feature missing) |\n| **Verify RED** | Confirm correct failure | Check error message | \"function not found\" or assertion fails |\n| **GREEN** | Write minimal code | Implement feature | Test passes |\n| **Verify GREEN** | All tests pass | `cargo test` | All green, no warnings |\n| **REFACTOR** | Clean up code | Improve while green | Tests still pass |\n\n**Iron Law:** NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n\n</quick_reference>\n\n<when_to_use>\n**Always use for:**\n- New features\n- Bug fixes\n- Refactoring with behavior changes\n- Any production code\n\n**Ask your human partner for exceptions:**\n- Throwaway prototypes (will be deleted)\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n</when_to_use>\n\n<the_process>\n\n## 1. RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n**Requirements:**\n- Test one behavior only (\"and\" in name? Split it)\n- Clear name describing behavior\n- Use real code (no mocks unless unavoidable)\n\nSee [resources/language-examples.md](resources/language-examples.md) for Rust, Swift, TypeScript examples.\n\n## 2. Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\nRun the test and confirm:\n- ✓ Test **fails** (not errors with syntax issues)\n- ✓ Failure message is expected (\"function not found\" or assertion fails)\n- ✓ Fails because feature missing (not typos)\n\n**If test passes:** You're testing existing behavior. Fix the test.\n**If test errors:** Fix syntax error, re-run until it fails correctly.\n\n## 3. GREEN - Write Minimal Code\n\nWrite simplest code to pass the test. Nothing more.\n\n**Key principle:** Don't add features the test doesn't require. Don't refactor other code. Don't \"improve\" beyond the test.\n\n## 4. Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\nRun tests and confirm:\n- ✓ New test passes\n- ✓ All other tests still pass\n- ✓ No errors or warnings\n\n**If test fails:** Fix code, not test.\n**If other tests fail:** Fix now before proceeding.\n\n## 5. REFACTOR - Clean Up\n\n**Only after green:**\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n## 6. Repeat\n\nNext failing test for next feature.\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer writes implementation first, then adds test that passes immediately</scenario>\n\n<code>\n// Code written FIRST\ndef validate_email(email):\n    return \"@\" in email  # Bug: accepts \"@@\"\n\n// Test written AFTER\ndef test_validate_email():\n    assert validate_email(\"user@example.com\")  # Passes immediately!\n    // Missing edge case: assert not validate_email(\"@@\")\n</code>\n\n<why_it_fails>\nWhen test passes immediately:\n- Never proved the test catches bugs\n- Only tested happy path you remembered\n- Forgot edge cases (like \"@@\")\n- Bug ships to production\n\nTests written after verify remembered cases, not required behavior.\n</why_it_fails>\n\n<correction>\n**TDD approach:**\n\n1. **RED** - Write test first (including edge case):\n```python\ndef test_validate_email():\n    assert validate_email(\"user@example.com\")  # Will fail - function doesn't exist\n    assert not validate_email(\"@@\")            # Edge case up front\n```\n\n2. **Verify RED** - Run test, watch it fail:\n```bash\nNameError: function 'validate_email' is not defined\n```\n\n3. **GREEN** - Implement to pass both cases:\n```python\ndef validate_email(email):\n    return \"@\" in email and email.count(\"@\") == 1\n```\n\n4. **Verify GREEN** - Both assertions pass, bug prevented.\n\n**Result:** Test failed first, proving it works. Edge case discovered during test writing, not in production.\n</correction>\n</example>\n\n<example>\n<scenario>Developer has already written 3 hours of code without tests. Wants to keep it as \"reference\" while writing tests.</scenario>\n\n<code>\n// 200 lines of untested code exists\n// Developer thinks: \"I'll keep this and write tests that match it\"\n// Or: \"I'll use it as reference to speed up TDD\"\n</code>\n\n<why_it_fails>\n**Keeping code as \"reference\":**\n- You'll copy it (that's testing after, with extra steps)\n- You'll adapt it (biased by implementation)\n- Tests will match code, not requirements\n- You'll justify shortcuts: \"I already know this works\"\n\n**Result:** All the problems of test-after, none of the benefits of TDD.\n</why_it_fails>\n\n<correction>\n**Delete it. Completely.**\n\n```bash\ngit stash  # Or delete the file\n```\n\n**Then start TDD:**\n1. Write first failing test from requirements (not from code)\n2. Watch it fail\n3. Implement fresh (might be different from original, that's OK)\n4. Watch it pass\n\n**Why delete:**\n- Sunk cost is already gone\n- 3 hours implementing ≠ 3 hours with TDD (TDD might be 2 hours total)\n- Code without tests is technical debt\n- Fresh implementation from tests is usually better\n\n**What you gain:**\n- Tests that actually verify behavior\n- Confidence code works\n- Ability to refactor safely\n- No bugs from untested edge cases\n</correction>\n</example>\n\n<example>\n<scenario>Test is hard to write. Developer thinks \"design must be unclear, but I'll implement first to explore.\"</scenario>\n\n<code>\n// Test attempt:\nfunc testUserServiceCreatesAccount() {\n    // Need to mock database, email service, payment gateway, logger...\n    // This is getting complicated, maybe I should just implement first\n}\n</code>\n\n<why_it_fails>\n**\"Test is hard\" is valuable signal:**\n- Hard to test = hard to use\n- Too many dependencies = coupling too tight\n- Complex setup = design needs simplification\n\n**Implementing first ignores this signal:**\n- Build the complex design\n- Lock in the coupling\n- Now forced to write complex tests (or skip them)\n</why_it_fails>\n\n<correction>\n**Listen to the test.**\n\nHard to test? Simplify the interface:\n\n```swift\n// Instead of:\nclass UserService {\n    init(db: Database, email: EmailService, payments: PaymentGateway, logger: Logger) { }\n    func createAccount(email: String, password: String, paymentToken: String) throws { }\n}\n\n// Make testable:\nclass UserService {\n    func createAccount(request: CreateAccountRequest) -> Result<Account, Error> {\n        // Dependencies injected through request or passed separately\n    }\n}\n```\n\n**Test becomes simple:**\n```swift\nfunc testCreatesAccountFromRequest() {\n    let service = UserService()\n    let request = CreateAccountRequest(email: \"user@example.com\")\n    let result = service.createAccount(request: request)\n    XCTAssertEqual(result.email, \"user@example.com\")\n}\n```\n\n**TDD forces good design.** If test is hard, fix design before implementing.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **Write code before test?** → Delete it. Start over.\n   - Never keep as \"reference\"\n   - Never \"adapt\" while writing tests\n   - Delete means delete\n\n2. **Test passes immediately?** → Not TDD. Fix the test or delete the code.\n   - Passing immediately proves nothing\n   - You're testing existing behavior, not required behavior\n\n3. **Can't explain why test failed?** → Fix until failure makes sense.\n   - \"function not found\" = good (feature doesn't exist)\n   - Weird error = bad (fix test, re-run)\n\n4. **Want to skip \"just this once\"?** → That's rationalization. Stop.\n   - TDD is faster than debugging in production\n   - \"Too simple to test\" = test takes 30 seconds\n   - \"Already manually tested\" = not systematic, not repeatable\n\n## Common Excuses\n\nAll of these mean: Stop, follow TDD:\n- \"This is different because...\"\n- \"I'm being pragmatic, not dogmatic\"\n- \"It's about spirit not ritual\"\n- \"Tests after achieve the same goals\"\n- \"Deleting X hours of work is wasteful\"\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test **fail** before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass with no warnings\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\n**Can't check all boxes?** You skipped TDD. Start over.\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- verification-before-completion (running tests to verify)\n\n**This skill is called by:**\n- fixing-bugs (write failing test reproducing bug)\n- executing-plans (when implementing bd tasks)\n- refactoring-safely (keep tests green while refactoring)\n\n**Agents used:**\n- hyperpowers:test-runner (run tests, return summary only)\n\n</integration>\n\n<resources>\n\n**Detailed language-specific examples:**\n- [Rust, Swift, TypeScript examples](resources/language-examples.md) - Complete RED-GREEN-REFACTOR cycles\n- [Language-specific test commands](resources/language-examples.md#verification-commands-by-language)\n\n**When stuck:**\n- Test too complicated? → Design too complicated, simplify interface\n- Must mock everything? → Code too coupled, use dependency injection\n- Test setup huge? → Extract helpers, or simplify design\n\n</resources>"
              },
              {
                "name": "testing-anti-patterns",
                "description": "Use when writing or changing tests, adding mocks - prevents testing mock behavior, production pollution with test-only methods, and mocking without understanding dependencies",
                "path": "skills/testing-anti-patterns/SKILL.md",
                "frontmatter": {
                  "name": "testing-anti-patterns",
                  "description": "Use when writing or changing tests, adding mocks - prevents testing mock behavior, production pollution with test-only methods, and mocking without understanding dependencies"
                },
                "content": "<skill_overview>\nTests must verify real behavior, not mock behavior; mocks are tools to isolate, not things to test.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - The 3 Iron Laws are absolute (never test mocks, never add test-only methods, never mock without understanding). Apply gate functions strictly.\n</rigidity_level>\n\n<quick_reference>\n## The 3 Iron Laws\n\n1. **NEVER test mock behavior** → Test real component behavior\n2. **NEVER add test-only methods to production** → Use test utilities instead\n3. **NEVER mock without understanding** → Know dependencies before mocking\n\n## Gate Functions (Use Before Action)\n\n**Before asserting on any mock:**\n- Ask: \"Am I testing real behavior or mock existence?\"\n- If mock existence → STOP, delete assertion\n\n**Before adding method to production:**\n- Ask: \"Is this only used by tests?\"\n- If yes → STOP, put in test utilities\n\n**Before mocking:**\n- Ask: \"What side effects does real method have?\"\n- Ask: \"Does test depend on those side effects?\"\n- If depends → Mock lower level, not this method\n</quick_reference>\n\n<when_to_use>\n- Writing new tests\n- Adding mocks to tests\n- Tempted to add method only tests will use\n- Test failing and considering mocking something\n- Unsure whether to mock a dependency\n- Test setup becoming complex with mocks\n\n**Critical moment:** Before you add a mock or test-only method, use this skill's gate functions.\n</when_to_use>\n\n<the_iron_laws>\n## Law 1: Never Test Mock Behavior\n\n**Anti-pattern:**\n```rust\n// ❌ BAD: Testing that mock exists\n#[test]\nfn test_processes_request() {\n    let mock_service = MockApiService::new();\n    let handler = RequestHandler::new(Box::new(mock_service));\n\n    // Testing mock existence, not behavior\n    assert!(handler.service().is_mock());\n}\n```\n\n**Why wrong:** Verifies mock works, not that code works.\n\n**Fix:**\n```rust\n// ✅ GOOD: Test real behavior\n#[test]\nfn test_processes_request() {\n    let service = TestApiService::new();  // Real implementation or full fake\n    let handler = RequestHandler::new(Box::new(service));\n\n    let result = handler.process_request(\"data\");\n    assert_eq!(result.status, StatusCode::OK);\n}\n```\n\n---\n\n## Law 2: Never Add Test-Only Methods to Production\n\n**Anti-pattern:**\n```rust\n// ❌ BAD: reset() only used in tests\npub struct Connection {\n    pool: Arc<ConnectionPool>,\n}\n\nimpl Connection {\n    pub fn reset(&mut self) {  // Looks like production API!\n        self.pool.clear_all();\n    }\n}\n\n// In tests\n#[test]\nfn test_something() {\n    let mut conn = Connection::new();\n    conn.reset();  // Test-only method\n}\n```\n\n**Why wrong:**\n- Production code polluted with test-only methods\n- Dangerous if accidentally called in production\n- Confuses object lifecycle with entity lifecycle\n\n**Fix:**\n```rust\n// ✅ GOOD: Test utilities handle cleanup\n// Connection has no reset()\n\n// In tests/test_utils.rs\npub fn cleanup_connection(conn: &Connection) {\n    if let Some(pool) = conn.get_pool() {\n        pool.clear_test_data();\n    }\n}\n\n// In tests\n#[test]\nfn test_something() {\n    let conn = Connection::new();\n    cleanup_connection(&conn);\n}\n```\n\n---\n\n## Law 3: Never Mock Without Understanding\n\n**Anti-pattern:**\n```rust\n// ❌ BAD: Mock breaks test logic\n#[test]\nfn test_detects_duplicate_server() {\n    // Mock prevents config write that test depends on!\n    let mut config_manager = MockConfigManager::new();\n    config_manager.expect_add_server()\n        .returning(|_| Ok(()));  // No actual config write!\n\n    config_manager.add_server(&config).unwrap();\n    config_manager.add_server(&config).unwrap();  // Should fail - but won't!\n}\n```\n\n**Why wrong:** Mocked method had side effect test depended on (writing config).\n\n**Fix:**\n```rust\n// ✅ GOOD: Mock at correct level\n#[test]\nfn test_detects_duplicate_server() {\n    // Mock the slow part, preserve behavior test needs\n    let server_manager = MockServerManager::new();  // Just mock slow server startup\n    let config_manager = ConfigManager::new_with_manager(server_manager);\n\n    config_manager.add_server(&config).unwrap();  // Config written\n    let result = config_manager.add_server(&config);  // Duplicate detected ✓\n    assert!(result.is_err());\n}\n```\n</the_iron_laws>\n\n<gate_functions>\n## Gate Function 1: Before Asserting on Mock\n\n```\nBEFORE any assertion that checks mock elements:\n\n1. Ask: \"Am I testing real component behavior or just mock existence?\"\n\n2. If testing mock existence:\n   STOP - Delete the assertion or unmock the component\n\n3. Test real behavior instead\n```\n\n**Examples of mock existence testing (all wrong):**\n- `assert!(handler.service().is_mock())`\n- `XCTAssertTrue(manager.delegate is MockDelegate)`\n- `expect(component.database).toBe(mockDb)`\n\n---\n\n## Gate Function 2: Before Adding Method to Production\n\n```\nBEFORE adding any method to production class:\n\n1. Ask: \"Is this only used by tests?\"\n\n2. If yes:\n   STOP - Don't add it\n   Put it in test utilities instead\n\n3. Ask: \"Does this class own this resource's lifecycle?\"\n\n4. If no:\n   STOP - Wrong class for this method\n```\n\n**Red flags:**\n- Method named `reset()`, `clear()`, `cleanup()` in production class\n- Method only has `#[cfg(test)]` callers\n- Method added \"for testing purposes\"\n\n---\n\n## Gate Function 3: Before Mocking\n\n```\nBEFORE mocking any method:\n\nSTOP - Don't mock yet\n\n1. Ask: \"What side effects does the real method have?\"\n2. Ask: \"Does this test depend on any of those side effects?\"\n3. Ask: \"Do I fully understand what this test needs?\"\n\nIf depends on side effects:\n  → Mock at lower level (the actual slow/external operation)\n  → OR use test doubles that preserve necessary behavior\n  → NOT the high-level method the test depends on\n\nIf unsure what test depends on:\n  → Run test with real implementation FIRST\n  → Observe what actually needs to happen\n  → THEN add minimal mocking at the right level\n```\n\n**Red flags:**\n- \"I'll mock this to be safe\"\n- \"This might be slow, better mock it\"\n- Mocking without understanding dependency chain\n</gate_functions>\n\n<examples>\n<example>\n<scenario>Developer tests mock behavior instead of real behavior</scenario>\n\n<code>\n#[test]\nfn test_user_service_initialized() {\n    let mock_db = MockDatabase::new();\n    let service = UserService::new(mock_db);\n\n    // Testing that mock exists\n    assert_eq!(service.database().connection_string(), \"mock://test\");\n    assert!(service.database().is_test_mode());\n}\n</code>\n\n<why_it_fails>\n- Assertions check mock properties, not service behavior\n- Test passes when mock is correct, fails when mock is wrong\n- Tells you nothing about whether UserService works\n- Would pass even if UserService.new() does nothing\n- False confidence - mock works, but does service work?\n</why_it_fails>\n\n<correction>\n**Apply Gate Function 1:**\n\n\"Am I testing real behavior or mock existence?\"\n→ Testing mock existence (connection_string(), is_test_mode() are mock properties)\n\n**Fix:**\n\n```rust\n#[test]\nfn test_user_service_creates_user() {\n    let db = TestDatabase::new();  // Real test implementation\n    let service = UserService::new(db);\n\n    // Test real behavior\n    let user = service.create_user(\"alice\", \"alice@example.com\").unwrap();\n    assert_eq!(user.name, \"alice\");\n    assert_eq!(user.email, \"alice@example.com\");\n\n    // Verify user was saved\n    let retrieved = service.get_user(user.id).unwrap();\n    assert_eq!(retrieved.name, \"alice\");\n}\n```\n\n**What you gain:**\n- Tests actual UserService behavior\n- Validates create and retrieve work\n- Would fail if service broken (even with working mock)\n- Confidence service actually works\n</correction>\n</example>\n\n<example>\n<scenario>Developer adds test-only method to production class</scenario>\n\n<code>\n// Production code\npub struct Database {\n    pool: ConnectionPool,\n}\n\nimpl Database {\n    pub fn new() -> Self { /* ... */ }\n\n    // Added \"for testing\"\n    pub fn reset(&mut self) {\n        self.pool.clear();\n        self.pool.reinitialize();\n    }\n}\n\n// Tests\n#[test]\nfn test_user_creation() {\n    let mut db = Database::new();\n    // ... test logic ...\n    db.reset();  // Clean up\n}\n\n#[test]\nfn test_user_deletion() {\n    let mut db = Database::new();\n    // ... test logic ...\n    db.reset();  // Clean up\n}\n</code>\n\n<why_it_fails>\n- Production Database polluted with test-only reset()\n- reset() looks like legitimate API to other developers\n- Dangerous if accidentally called in production (clears all data!)\n- Violates single responsibility (Database manages connections, not test lifecycle)\n- Every test class now needs reset() added\n</why_it_fails>\n\n<correction>\n**Apply Gate Function 2:**\n\n\"Is this only used by tests?\" → YES\n\"Does Database class own test lifecycle?\" → NO\n\n**Fix:**\n\n```rust\n// Production code (NO reset method)\npub struct Database {\n    pool: ConnectionPool,\n}\n\nimpl Database {\n    pub fn new() -> Self { /* ... */ }\n    // No reset() - production code clean\n}\n\n// Test utilities (tests/test_utils.rs)\npub fn create_test_database() -> Database {\n    Database::new()\n}\n\npub fn cleanup_database(db: &mut Database) {\n    // Access internals properly for cleanup\n    if let Some(pool) = db.get_pool_mut() {\n        pool.clear_test_data();\n    }\n}\n\n// Tests\n#[test]\nfn test_user_creation() {\n    let mut db = create_test_database();\n    // ... test logic ...\n    cleanup_database(&mut db);\n}\n```\n\n**What you gain:**\n- Production code has no test pollution\n- No risk of accidental production calls\n- Clear separation: Database manages connections, test utils manage test lifecycle\n- Test utilities can evolve without changing production code\n</correction>\n</example>\n\n<example>\n<scenario>Developer mocks without understanding dependencies</scenario>\n\n<code>\n#[test]\nfn test_detects_duplicate_server() {\n    // \"I'll mock ConfigManager to speed up the test\"\n    let mut mock_config = MockConfigManager::new();\n    mock_config.expect_add_server()\n        .times(2)\n        .returning(|_| Ok(()));  // Always returns Ok!\n\n    // Test expects duplicate detection\n    mock_config.add_server(&server_config).unwrap();\n    let result = mock_config.add_server(&server_config);\n\n    // Assertion fails! Mock always returns Ok, no duplicate detection\n    assert!(result.is_err());  // FAILS\n}\n</code>\n\n<why_it_fails>\n- Mocked add_server() without understanding it writes config\n- Mock returns Ok() both times (no duplicate detection)\n- Test depends on ConfigManager's internal state tracking\n- Mock eliminates the behavior test needs to verify\n- \"Speeding up\" by mocking broke the test\n</why_it_fails>\n\n<correction>\n**Apply Gate Function 3:**\n\n\"What side effects does add_server() have?\" → Writes to config file, tracks added servers\n\"Does test depend on those?\" → YES! Test needs duplicate detection\n\"Do I understand what test needs?\" → Now yes\n\n**Fix:**\n\n```rust\n#[test]\nfn test_detects_duplicate_server() {\n    // Mock at the RIGHT level - just the slow I/O\n    let mock_file_system = MockFileSystem::new();  // Mock slow file writes\n    let config_manager = ConfigManager::new_with_fs(mock_file_system);\n\n    // ConfigManager's duplicate detection still works\n    config_manager.add_server(&server_config).unwrap();\n    let result = config_manager.add_server(&server_config);\n\n    // Passes! ConfigManager tracks duplicates, only file I/O is mocked\n    assert!(result.is_err());\n}\n```\n\n**What you gain:**\n- Test verifies real duplicate detection logic\n- Only mocked the actual slow part (file I/O)\n- ConfigManager's internal tracking works normally\n- Test actually validates the feature\n</correction>\n</example>\n</examples>\n\n<additional_anti_patterns>\n## Anti-Pattern 4: Incomplete Mocks\n\n**Problem:** Mock only fields you think you need, omit others.\n\n```rust\n// ❌ BAD: Partial mock\nstruct MockResponse {\n    status: String,\n    data: UserData,\n    // Missing: metadata that downstream code uses\n}\n\nimpl ApiResponse for MockResponse {\n    fn metadata(&self) -> &Metadata {\n        panic!(\"metadata not implemented!\")  // Breaks at runtime!\n    }\n}\n```\n\n**Fix:** Mirror real API completely.\n\n```rust\n// ✅ GOOD: Complete mock\nstruct MockResponse {\n    status: String,\n    data: UserData,\n    metadata: Metadata,  // All fields real API returns\n}\n```\n\n**Gate function:**\n```\nBEFORE creating mock responses:\n  1. Examine actual API response structure\n  2. Include ALL fields system might consume\n  3. Verify mock matches real schema completely\n```\n\n---\n\n## Anti-Pattern 5: Over-Complex Mocks\n\n**Warning signs:**\n- Mock setup longer than test logic\n- Mocking everything to make test pass\n- Test breaks when mock changes\n\n**Consider:** Integration tests with real components often simpler than complex mocks.\n</additional_anti_patterns>\n\n<tdd_prevention>\n## TDD Prevents These Anti-Patterns\n\n**Why TDD helps:**\n\n1. **Write test first** → Forces thinking about what you're actually testing\n2. **Watch it fail** → Confirms test tests real behavior, not mocks\n3. **Minimal implementation** → No test-only methods creep in\n4. **Real dependencies** → See what test needs before mocking\n\n**If you're testing mock behavior, you violated TDD** - you added mocks without watching test fail against real code first.\n\n**REQUIRED BACKGROUND:** You MUST understand hyperpowers:test-driven-development before using this skill.\n</tdd_prevention>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Never test mock behavior** → Test real component behavior always\n2. **Never add test-only methods to production** → Pollutes production code\n3. **Never mock without understanding** → Must know dependencies and side effects\n4. **Use gate functions before action** → Before asserting, adding methods, or mocking\n5. **Follow TDD** → Write test first, watch fail, prevents testing mocks\n\n## Common Excuses\n\nAll of these mean: **STOP. Apply the gate function.**\n\n- \"Just checking the mock is wired up\" (Testing mock, not behavior)\n- \"Need reset() for test cleanup\" (Test-only method, use test utilities)\n- \"I'll mock this to be safe\" (Don't understand dependencies)\n- \"Mock setup is complex but necessary\" (Probably over-mocking)\n- \"This will speed up tests\" (Might break test logic)\n</critical_rules>\n\n<verification_checklist>\nBefore claiming tests are correct:\n\n- [ ] No assertions on mock elements (no `is_mock()`, `is MockType`, etc.)\n- [ ] No test-only methods in production classes\n- [ ] All mocks preserve side effects test depends on\n- [ ] Mock at lowest level needed (mock slow I/O, not business logic)\n- [ ] Understand why each mock is necessary\n- [ ] Mock structure matches real API completely\n- [ ] Test logic shorter/equal to mock setup (not longer)\n- [ ] Followed TDD (test failed with real code before mocking)\n\n**Can't check all boxes?** Apply gate functions and refactor.\n</verification_checklist>\n\n<integration>\n**This skill requires:**\n- hyperpowers:test-driven-development (prevents these anti-patterns)\n- Understanding of mocking vs. faking vs. stubbing\n\n**This skill is called by:**\n- When writing tests\n- When adding mocks\n- When test setup becoming complex\n- hyperpowers:test-driven-development (use gate functions during RED phase)\n\n**Red flags triggering this skill:**\n- Assertion checks for `*-mock` test IDs\n- Methods only called in test files\n- Mock setup >50% of test\n- Test fails when you remove mock\n- Can't explain why mock needed\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Mocking vs Faking vs Stubbing](resources/test-doubles.md)\n- [Test utilities patterns](resources/test-utilities.md)\n- [When to use integration tests](resources/integration-vs-unit.md)\n\n**When stuck:**\n- Mock too complex → Consider integration test with real components\n- Unsure what to mock → Run with real implementation first, observe\n- Test failing mysteriously → Check if mock breaks test logic (use Gate Function 3)\n- Production polluted → Move all test helpers to test_utils\n</resources>"
              },
              {
                "name": "using-hyper",
                "description": "Use when starting any conversation - establishes mandatory workflows for finding and using skills",
                "path": "skills/using-hyper/SKILL.md",
                "frontmatter": {
                  "name": "using-hyper",
                  "description": "Use when starting any conversation - establishes mandatory workflows for finding and using skills"
                },
                "content": "<EXTREMELY_IMPORTANT>\nIf you think there is even a 1% chance a skill might apply to what you are doing, you ABSOLUTELY MUST read the skill.\n\n**IF A SKILL APPLIES TO YOUR TASK, YOU DO NOT HAVE A CHOICE. YOU MUST USE IT.**\n\nThis is not negotiable. This is not optional. You cannot rationalize your way out of this.\n</EXTREMELY_IMPORTANT>\n\n<skill_overview>\nSkills are proven workflows; if one exists for your task, using it is mandatory, not optional.\n</skill_overview>\n\n<rigidity_level>\nHIGH FREEDOM - The meta-process (check for skills, use Skill tool, announce usage) is rigid, but each individual skill defines its own rigidity level.\n</rigidity_level>\n\n<quick_reference>\n**Before responding to ANY user message:**\n\n1. List available skills mentally\n2. Ask: \"Does ANY skill match this request?\"\n3. If yes → Use Skill tool to load the skill file\n4. Announce which skill you're using\n5. Follow the skill exactly as written\n\n**Skill has checklist?** Create TodoWrite for every item.\n\n**Finding a relevant skill = mandatory to use it.**\n</quick_reference>\n\n<when_to_use>\nThis skill applies at the start of EVERY conversation and BEFORE every task:\n\n- User asks you to implement a feature\n- User asks you to fix a bug\n- User asks you to refactor code\n- User asks you to debug an issue\n- User asks you to write tests\n- User asks you to review code\n- User describes a problem to solve\n- User provides requirements to implement\n\n**Applies to:** Literally any task that might have a corresponding skill.\n</when_to_use>\n\n<the_process>\n## 1. MANDATORY FIRST RESPONSE PROTOCOL\n\nBefore responding to ANY user message, complete this checklist:\n\n1. ☐ List available skills in your mind\n2. ☐ Ask yourself: \"Does ANY skill match this request?\"\n3. ☐ If yes → Use the Skill tool to read and run the skill file\n4. ☐ Announce which skill you're using\n5. ☐ Follow the skill exactly\n\n**Responding WITHOUT completing this checklist = automatic failure.**\n\n---\n\n## 2. Execute Skills with the Skill Tool\n\n**Always use the Skill tool to load skills.** Never rely on memory.\n\n```\nSkill tool: \"hyperpowers:test-driven-development\"\n```\n\n**Why:**\n- Skills evolve - you need the current version\n- Using the tool ensures you get the full skill content\n- Confirms to user you're following the skill\n\n---\n\n## 3. Announce Skill Usage\n\nBefore using a skill, announce it:\n\n**Format:** \"I'm using [Skill Name] to [what you're doing].\"\n\n**Examples:**\n- \"I'm using hyperpowers:brainstorming to refine your idea into a design.\"\n- \"I'm using hyperpowers:test-driven-development to implement this feature.\"\n- \"I'm using hyperpowers:debugging-with-tools to investigate this error.\"\n\n**Why:** Transparency helps user understand your process and catch errors early. Confirms you actually read the skill.\n\n---\n\n## 4. Follow Mandatory Workflows\n\n**Before writing ANY code:**\n- Use hyperpowers:brainstorming to refine requirements\n- Use hyperpowers:writing-plans to create detailed plan\n- Use hyperpowers:executing-plans to implement iteratively\n\n**When implementing:**\n- Use hyperpowers:test-driven-development (RED-GREEN-REFACTOR cycle)\n- Use hyperpowers:verification-before-completion before claiming done\n\n**When debugging:**\n- Use hyperpowers:debugging-with-tools (tools first, fixes second)\n- Use hyperpowers:fixing-bugs (complete workflow from discovery to closure)\n\n**User instructions describe WHAT to do, not HOW.** \"Add X\" means use brainstorming, TDD, verification. Not permission to skip workflows.\n\n---\n\n## 5. Create TodoWrite for Skill Checklists\n\nIf a skill has a checklist, YOU MUST create TodoWrite todos for EACH item.\n\n**Don't:**\n- Work through checklist mentally\n- Skip creating todos \"to save time\"\n- Batch multiple items into one todo\n- Mark complete without doing them\n\n**Why:** Checklists without TodoWrite tracking = steps get skipped. Every time. The overhead is tiny compared to missing steps.\n\n**Example:**\n\n```\nSkill has verification checklist:\n- [ ] All tests pass\n- [ ] No linter warnings\n- [ ] bd task updated\n\nTodoWrite todos:\n1. Run all tests and verify they pass\n2. Run linter and verify no warnings\n3. Update bd task with completion status\n```\n</the_process>\n\n<examples>\n<example>\n<scenario>User asks to implement a new feature</scenario>\n\n<code>\nUser: \"Add a user profile page with avatar upload\"\n\nClaude (without using-hyper):\n\"Sure! Let me start implementing the user profile page...\"\n[Starts writing code immediately without brainstorming or planning]\n</code>\n\n<why_it_fails>\n- Skipped mandatory brainstorming (requirements unclear)\n- Skipped writing-plans (no detailed implementation plan)\n- Skipped test-driven-development (no tests first)\n- Will likely miss edge cases, make wrong assumptions\n- User has no visibility into approach before code is written\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\nClaude: \"I'm using hyperpowers:brainstorming to refine your requirements into a complete design.\"\n\n[Skill tool loads brainstorming]\n[Asks Socratic questions about avatar size limits, formats, storage, etc.]\n[Creates refined requirements]\n\nClaude: \"Now I'm using hyperpowers:writing-plans to create a detailed implementation plan.\"\n\n[Skill tool loads writing-plans]\n[Creates bd epic with tasks]\n\nClaude: \"Now I'm using hyperpowers:executing-plans to implement the tasks iteratively.\"\n\n[Skill tool loads executing-plans]\n[Implements with TDD, commits incrementally]\n\n**What you gain:**\n- Clear requirements before coding\n- Detailed plan user can review\n- Implementation follows proven workflow\n- Tests written first (TDD)\n- Progress tracked in bd\n</correction>\n</example>\n\n<example>\n<scenario>User reports a bug</scenario>\n\n<code>\nUser: \"Users can't log in after password reset\"\n\nClaude (without using-hyper):\n\"Let me check the password reset code... I found the issue, it's not updating the password hash. Let me fix that...\"\n[Makes immediate fix without investigation]\n</code>\n\n<why_it_fails>\n- Skipped debugging-with-tools (no systematic investigation)\n- Skipped fixing-bugs workflow (no bd issue, no regression test)\n- \"Obvious\" fix might be wrong or incomplete\n- No verification that fix actually works\n- Will likely miss root cause and fix symptom\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\nClaude: \"I'm using hyperpowers:debugging-with-tools to investigate this systematically.\"\n\n[Skill tool loads debugging-with-tools]\n[Uses debugger, checks logs, searches internet for known issues]\n[Identifies root cause: token invalidation timing issue]\n\nClaude: \"Now I'm using hyperpowers:fixing-bugs to implement a complete fix.\"\n\n[Skill tool loads fixing-bugs]\n[Creates bd bug issue]\n[Writes failing test reproducing bug (RED)]\n[Implements minimal fix (GREEN)]\n[Runs full test suite via test-runner agent]\n[Verifies fix and closes bd issue]\n\n**What you gain:**\n- Root cause identified (not symptom)\n- Regression test prevents recurrence\n- Fix verified with evidence\n- Bug tracked in bd\n- Complete workflow documented\n</correction>\n</example>\n\n<example>\n<scenario>User asks to refactor code</scenario>\n\n<code>\nUser: \"This validation logic is duplicated, can you extract it?\"\n\nClaude (without using-hyper):\n\"Sure, let me extract that into a shared function...\"\n[Changes 5 files at once, then runs tests]\n[Tests fail]\n\"Hmm, let me fix these test failures...\"\n</code>\n\n<why_it_fails>\n- Skipped refactoring-safely workflow (change→test→commit cycle)\n- Big-bang refactoring (changed multiple files before testing)\n- Tests failing means behavior changed (not safe refactoring)\n- Can't identify which change broke what\n- Wastes time debugging entire refactoring\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\nClaude: \"I'm using hyperpowers:refactoring-safely to extract this validation logic incrementally.\"\n\n[Skill tool loads refactoring-safely]\n\nStep 1: Verify tests pass BEFORE refactoring\nStep 2: Create bd refactoring task\nStep 3: Extract validation from first file → test → commit\nStep 4: Extract validation from second file → test → commit\nStep 5: Create shared validator → test → commit\nStep 6: Final verification → close bd task\n\n**What you gain:**\n- Tests stay green throughout (safe refactoring)\n- Each commit is reviewable independently\n- Know exactly which change broke if test fails\n- Can stop halfway with useful progress\n- Clear history of transformations\n</correction>\n</example>\n</examples>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **Check for relevant skills BEFORE any task** → If skill exists, use it (not optional)\n2. **Use Skill tool to load skills** → Never rely on memory (skills evolve)\n3. **Announce skill usage** → Transparency helps catch errors early\n4. **Follow mandatory workflows** → brainstorming before coding, TDD for implementation, verification before claiming done\n5. **Create TodoWrite for checklists** → Mental tracking = skipped steps\n\n## Common Rationalizations\n\nAll of these mean: **STOP. Check for and use the relevant skill.**\n\n- \"This is just a simple question\" (Questions are tasks. Check for skills.)\n- \"I can check git/files quickly\" (Files lack context. Check for skills.)\n- \"Let me gather information first\" (Skills tell you HOW to gather. Check for skills.)\n- \"This doesn't need a formal skill\" (If skill exists, use it. Not optional.)\n- \"I remember this skill\" (Skills evolve. Use Skill tool to load current version.)\n- \"This doesn't count as a task\" (Taking action = task. Check for skills.)\n- \"The skill is overkill for this\" (Skills exist because \"simple\" becomes complex.)\n- \"I'll just do this one thing first\" (Check for skills BEFORE doing anything.)\n- \"Instruction was specific so I can skip brainstorming\" (Specific instructions = WHAT, not HOW. Use workflows.)\n</critical_rules>\n\n<understanding_rigidity>\n## Rigid Skills (Follow Exactly)\n\nThese have LOW FREEDOM - follow the exact process:\n\n- hyperpowers:test-driven-development (RED-GREEN-REFACTOR cycle)\n- hyperpowers:verification-before-completion (evidence before claims)\n- hyperpowers:executing-plans (continuous execution, substep tracking)\n\n## Flexible Skills (Adapt Principles)\n\nThese have HIGH FREEDOM - adapt core principles to context:\n\n- hyperpowers:brainstorming (Socratic method, but questions vary)\n- hyperpowers:managing-bd-tasks (operations adapt to project)\n- hyperpowers:sre-task-refinement (corner case analysis, but depth varies)\n\n**The skill itself tells you its rigidity level.** Check `<rigidity_level>` section.\n</understanding_rigidity>\n\n<instructions_vs_workflows>\n## User Instructions Describe WHAT, Not HOW\n\n**User says:** \"Add user authentication\"\n**This means:** Use brainstorming → writing-plans → executing-plans → TDD → verification\n\n**User says:** \"Fix this bug\"\n**This means:** Use debugging-with-tools → fixing-bugs → TDD → verification\n\n**User says:** \"Refactor this code\"\n**This means:** Use refactoring-safely (change→test→commit cycle)\n\n**User instructions are the GOAL, not permission to skip workflows.**\n\n**Red flags that you're rationalizing:**\n- \"Instruction was specific, don't need brainstorming\"\n- \"Seems simple, don't need TDD\"\n- \"Workflow is overkill for this\"\n\n**Why workflows matter MORE when instructions are specific:**\n- Clear requirements = perfect time for structured implementation\n- \"Simple\" tasks often have hidden complexity\n- Skipping process on \"easy\" tasks is how they become hard problems\n</instructions_vs_workflows>\n\n<verification_checklist>\nBefore completing ANY task:\n\n- [ ] Did I check for relevant skills before starting?\n- [ ] Did I use Skill tool to load skills (not rely on memory)?\n- [ ] Did I announce which skill I'm using?\n- [ ] Did I follow the skill's process exactly?\n- [ ] Did I create TodoWrite for any skill checklists?\n- [ ] Did I follow mandatory workflows (brainstorming, TDD, verification)?\n\n**Can't check all boxes?** You skipped critical steps. Review and fix.\n</verification_checklist>\n\n<integration>\n**This skill calls:**\n- ALL other skills (meta-skill that triggers appropriate skill usage)\n\n**This skill is called by:**\n- Session start (always loaded)\n- User requests (check before every task)\n\n**Critical workflows this establishes:**\n- hyperpowers:brainstorming (before writing code)\n- hyperpowers:test-driven-development (during implementation)\n- hyperpowers:verification-before-completion (before claiming done)\n</integration>\n\n<resources>\n**Available skills:**\n- See skill descriptions in Skill tool's \"Available Commands\" section\n- Each skill's description shows when to use it\n\n**When unsure if skill applies:**\n- If there's even 1% chance it applies → use it\n- Better to load and decide \"not needed\" than to skip and fail\n- Skills are optimized, loading them is cheap\n</resources>"
              },
              {
                "name": "verification-before-completion",
                "description": "Use before claiming work complete, fixed, or passing - requires running verification commands and confirming output; evidence before assertions always",
                "path": "skills/verification-before-completion/SKILL.md",
                "frontmatter": {
                  "name": "verification-before-completion",
                  "description": "Use before claiming work complete, fixed, or passing - requires running verification commands and confirming output; evidence before assertions always"
                },
                "content": "<skill_overview>\nClaiming work is complete without verification is dishonesty, not efficiency. Evidence before claims, always.\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - NO exceptions. Run verification command, read output, THEN make claim.\n\nNo shortcuts. No \"should work\". No partial verification. Run it, prove it.\n</rigidity_level>\n\n<quick_reference>\n\n| Claim | Verification Required | Not Sufficient |\n|-------|----------------------|----------------|\n| **Tests pass** | Run full test command, see 0 failures | Previous run, \"should pass\" |\n| **Build succeeds** | Run build, see exit 0 | Linter passing |\n| **Bug fixed** | Test original symptom, passes | Code changed |\n| **Task complete** | Check all success criteria, run verifications | \"Implemented bd-3\" |\n| **Epic complete** | `bd list --status open --parent bd-1` shows 0 | \"All tasks done\" |\n\n**Iron Law:** NO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n\n**Use test-runner agent for:** Tests, pre-commit hooks, commits (keeps verbose output out of context)\n\n</quick_reference>\n\n<when_to_use>\n**ALWAYS before:**\n- Any success/completion claim\n- Any expression of satisfaction\n- Committing, PR creation, task completion\n- Moving to next task\n- ANY communication suggesting completion/correctness\n\n**Red flags you need this:**\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\")\n- About to commit/push without verification\n- Trusting agent success reports\n- Relying on partial verification\n</when_to_use>\n\n<the_process>\n\n## The Gate Function\n\nBefore claiming ANY status:\n\n### 1. Identify\nWhat command proves this claim?\n\n### 2. Run\nExecute the full command (fresh, complete).\n\n**For tests/hooks/commits:** Use `hyperpowers:test-runner` agent\n- Agent captures verbose output in its context\n- Returns only summary + failures\n- Prevents context pollution\n\n**For other commands:** Run directly and capture output\n\n### 3. Read\nFull output, check exit code, count failures.\n\n### 4. Verify\nDoes output confirm the claim?\n- If NO: State actual status with evidence\n- If YES: State claim with evidence\n\n### 5. Only Then\nMake the claim.\n\n**Skip any step = lying, not verifying**\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer claims tests pass without running them</scenario>\n\n<code>\nDeveloper modifies authentication logic.\n\nDeveloper thinks: \"This fix is straightforward, tests should pass now\"\n\nDeveloper writes: \"Great! All tests passing. The bug is fixed.\"\n\n[No test command run, no output shown]\n</code>\n\n<why_it_fails>\n**No evidence:**\n- \"Should pass\" ≠ evidence\n- Confidence ≠ verification\n- Might have broken other tests\n- Might not have fixed the bug\n\n**Why dangerous:**\n- Broken code ships\n- Trust broken with partner\n- Wastes time on false completion\n</why_it_fails>\n\n<correction>\n**Run verification:**\n\n```bash\n# Dispatch test-runner agent\n\"Run: cargo test\"\n```\n\n**Agent returns:**\n```\nSummary: 33 passed, 1 failed\nFailures:\n- test_login_with_expired_token: assertion failed\n  Expected: Err(AuthError::TokenExpired)\n  Got: Ok(User { id: 123 })\n```\n\n**State actual status:**\n```\nTests: 33 passed, 1 failed.\nFailure: test_login_with_expired_token still fails.\nThe fix didn't handle expired tokens.\nInvestigating...\n```\n\n**Result:** Accurate status, bug caught before committing.\n</correction>\n</example>\n\n<example>\n<scenario>Developer closes bd task without verifying success criteria</scenario>\n\n<code>\nbd-3 success criteria:\n- [ ] All functions fully implemented (no stubs, no TODOs)\n- [ ] Tests written and passing: test_generate_token, test_verify_token\n- [ ] Pre-commit hooks pass\n\nDeveloper implements token functions.\n\nDeveloper thinks: \"I implemented everything, task complete\"\n\nRuns: bd close bd-3\n\n[No verification commands run]\n</code>\n\n<why_it_fails>\n**Success criteria not verified:**\n- Might have TODO comments left\n- Specific tests not run\n- Pre-commit hooks not checked\n\n**Result:** Incomplete task closed, issues discovered later.\n</why_it_fails>\n\n<correction>\n**Verify each criterion:**\n\n```bash\n# 1. Check for TODOs\nrg \"TODO|FIXME|todo!|unimplemented!\" src/\n# Output: [nothing found] ✅\n\n# 2. Run specific tests\ncargo test test_generate_token test_verify_token\n# Output: test result: ok. 2 passed ✅\n\n# 3. Run pre-commit hooks (via test-runner agent)\n\"Run: cargo fmt --check && cargo clippy && cargo test\"\n```\n\n**Agent returns:**\n```\nAll checks passed:\n- cargo fmt: no formatting issues\n- cargo clippy: no warnings\n- cargo test: 34/34 passed\n```\n\n**THEN close task:**\n```bash\nbd close bd-3\n```\n\n**Result:** Task actually complete, all criteria met with evidence.\n</correction>\n</example>\n\n<example>\n<scenario>Developer claims build works based on linter passing</scenario>\n\n<code>\nDeveloper fixes TypeScript errors flagged by linter.\n\nRuns: npm run lint\nOutput: ✅ No issues found\n\nDeveloper writes: \"Build passes! Ready to commit.\"\n\n[No build command run]\n</code>\n\n<why_it_fails>\n**Linter ≠ compiler:**\n- Linter checks style and simple errors\n- Doesn't verify imports, types, compilation\n- Build might still fail\n\n**Why it happens:** Partial verification, extrapolation.\n</why_it_fails>\n\n<correction>\n**Run actual build:**\n\n```bash\nnpm run build\n```\n\n**Output:**\n```\nERROR in src/auth/service.ts:23:15\nTS2304: Cannot find name 'validateToken'\nDid you mean 'verifyToken'?\n```\n\n**State actual status:**\n```\nLinter passes, but build fails.\nError: validateToken not defined (should be verifyToken).\nFixing...\n```\n\n**After fix, run build again:**\n```bash\nnpm run build\n# Output: ✓ built in 1.2s\n```\n\n**Now can claim:**\n```\nBuild passes. [Evidence: npm run build exit 0]\nReady to commit.\n```\n\n**Result:** Actual build status verified, error caught.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **No claims without fresh verification** → Run command, see output, THEN claim\n   - \"Should work\" = forbidden\n   - \"Looks correct\" = forbidden\n   - Previous run ≠ fresh verification\n\n2. **Use test-runner agent for verbose commands** → Tests, hooks, commits\n   - Prevents context pollution\n   - Returns summary + failures only\n   - Never run `git commit` or `cargo test` directly if output is verbose\n\n3. **Verify ALL success criteria** → Not just \"tests pass\"\n   - Read each criterion from bd task\n   - Run verification for each\n   - Check all pass before closing\n\n4. **Evidence in every claim** → Show the output\n   - Not: \"Tests pass\"\n   - Yes: \"Tests pass [Ran: cargo test, Output: 34/34 passed]\"\n\n## Common Excuses\n\nAll of these mean: Stop, run verification:\n- \"Should work now\"\n- \"I'm confident this fixes it\"\n- \"Just this once\"\n- \"Linter passed\" (when claiming build works)\n- \"Agent said success\" (without independent verification)\n- \"I'm tired\" (exhaustion ≠ excuse)\n- \"Partial check is enough\"\n\n## Pre-Commit Hook Assumption\n\n**If your project uses pre-commit hooks enforcing tests:**\n- All test failures are from your current changes\n- Never check if errors were \"pre-existing\"\n- Don't run `git checkout <sha> && pytest` to verify\n- Pre-commit hooks guarantee previous commit passed\n- Just fix the error directly\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore claiming tests pass:\n- [ ] Ran full test command (not partial)\n- [ ] Saw output showing 0 failures\n- [ ] Used test-runner agent if output verbose\n\nBefore claiming build succeeds:\n- [ ] Ran build command (not just linter)\n- [ ] Saw exit code 0\n- [ ] Checked for compilation errors\n\nBefore closing bd task:\n- [ ] Re-read success criteria from bd task\n- [ ] Ran verification for each criterion\n- [ ] Saw evidence all pass\n- [ ] THEN closed task\n\nBefore closing bd epic:\n- [ ] Ran `bd list --status open --parent bd-1`\n- [ ] Saw 0 open tasks\n- [ ] Ran `bd dep tree bd-1`\n- [ ] Confirmed all tasks closed\n- [ ] THEN closed epic\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- test-runner (for verbose verification commands)\n\n**This skill is called by:**\n- test-driven-development (verify tests pass/fail)\n- executing-plans (verify task success criteria)\n- refactoring-safely (verify tests still pass)\n- ALL skills before completion claims\n\n**Agents used:**\n- hyperpowers:test-runner (run tests, hooks, commits without output pollution)\n\n</integration>\n\n<resources>\n\n**When stuck:**\n- Tempted to say \"should work\" → Run the verification\n- Agent reports success → Check VCS diff, verify independently\n- Partial verification → Run complete command\n- Tired and want to finish → Run verification anyway, no exceptions\n\n**Verification patterns:**\n- Tests: Use test-runner agent, check 0 failures\n- Build: Run build command, check exit 0\n- bd task: Verify each success criterion\n- bd epic: Check all tasks closed with bd list/dep tree\n\n</resources>"
              },
              {
                "name": "writing-plans",
                "description": "Use to expand bd tasks with detailed implementation steps - adds exact file paths, complete code, verification commands assuming zero context",
                "path": "skills/writing-plans/SKILL.md",
                "frontmatter": {
                  "name": "writing-plans",
                  "description": "Use to expand bd tasks with detailed implementation steps - adds exact file paths, complete code, verification commands assuming zero context"
                },
                "content": "<skill_overview>\nEnhance bd tasks with comprehensive implementation details for engineers with zero codebase context. Expand checklists into explicit steps: which files, complete code examples, exact commands, verification steps.\n</skill_overview>\n\n<rigidity_level>\nMEDIUM FREEDOM - Follow task-by-task validation pattern, use codebase-investigator for verification.\n\nAdapt implementation details to actual codebase state. Never use placeholders or meta-references.\n</rigidity_level>\n\n<quick_reference>\n\n| Step | Action | Critical Rule |\n|------|--------|---------------|\n| **Identify Scope** | Single task, range, or full epic | No artificial limits |\n| **Verify Codebase** | Use `codebase-investigator` agent | NEVER verify yourself, report discrepancies |\n| **Draft Steps** | Write bite-sized (2-5 min) actions | Follow TDD cycle for new features |\n| **Present to User** | Show COMPLETE expansion FIRST | Then ask for approval |\n| **Update bd** | `bd update bd-N --design \"...\"` | Only after user approves |\n| **Continue** | Move to next task automatically | NO asking permission between tasks |\n\n**FORBIDDEN:** Placeholders like `[Full implementation steps as detailed above]`\n**REQUIRED:** Actual content - complete code, exact paths, real commands\n\n</quick_reference>\n\n<when_to_use>\n**Use after hyperpowers:sre-task-refinement or anytime tasks need more detail.**\n\nSymptoms:\n- bd tasks have implementation checklists but need expansion\n- Engineer needs step-by-step guide with zero context\n- Want explicit file paths, complete code examples\n- Need exact verification commands\n\n</when_to_use>\n\n<the_process>\n\n## 1. Identify Tasks to Expand\n\n**User specifies scope:**\n- Single: \"Expand bd-2\"\n- Range: \"Expand bd-2 through bd-5\"\n- Epic: \"Expand all tasks in bd-1\"\n\n**If epic:**\n```bash\nbd dep tree bd-1  # View complete dependency tree\n# Note all child task IDs\n```\n\n**Create TodoWrite tracker:**\n```\n- [ ] bd-2: [Task Title]\n- [ ] bd-3: [Task Title]\n...\n```\n\n## 2. For EACH Task (Loop Until All Done)\n\n### 2a. Mark In Progress and Read Current State\n\n```bash\n# Mark in TodoWrite: in_progress\nbd show bd-3  # Read current task design\n```\n\n### 2b. Verify Codebase State\n\n**CRITICAL: Use codebase-investigator agent, NEVER verify yourself.**\n\n**Provide agent with bd assumptions:**\n```\nAssumptions from bd-3:\n- Auth service should be in src/services/auth.ts with login() and logout()\n- User model in src/models/user.ts with email and password fields\n- Test file at tests/services/auth.test.ts\n- Uses bcrypt dependency for password hashing\n\nVerify these assumptions and report:\n1. What exists vs what bd-3 expects\n2. Structural differences (different paths, functions, exports)\n3. Missing or additional components\n4. Current dependency versions\n```\n\n**Based on investigator report:**\n- ✓ Confirmed assumptions → Use in implementation\n- ✗ Incorrect assumptions → Adjust plan to match reality\n- + Found additional → Document and incorporate\n\n**NEVER write conditional steps:**\n❌ \"Update `index.js` if exists\"\n❌ \"Modify `config.py` (if present)\"\n\n**ALWAYS write definitive steps:**\n✅ \"Create `src/auth.ts`\" (investigator confirmed doesn't exist)\n✅ \"Modify `src/index.ts:45-67`\" (investigator confirmed exists)\n\n### 2c. Draft Expanded Implementation Steps\n\n**Bite-sized granularity (2-5 minutes per step):**\n\nFor new features (follow test-driven-development):\n1. Write the failing test (one step)\n2. Run it to verify it fails (one step)\n3. Implement minimal code to pass (one step)\n4. Run tests to verify they pass (one step)\n5. Commit (one step)\n\n**Include in each step:**\n- Exact file path\n- Complete code example (not pseudo-code)\n- Exact command to run\n- Expected output\n\n### 2d. Present COMPLETE Expansion to User\n\n**CRITICAL: Show the full expansion BEFORE asking for approval.**\n\n**Format:**\n```markdown\n**bd-[N]: [Task Title]**\n\n**From bd issue:**\n- Goal: [From bd show]\n- Effort estimate: [From bd issue]\n- Success criteria: [From bd issue]\n\n**Codebase verification findings:**\n- ✓ Confirmed: [what matched]\n- ✗ Incorrect: [what issue said] - ACTUALLY: [reality]\n- + Found: [unexpected discoveries]\n\n**Implementation steps based on actual codebase state:**\n\n### Step Group 1: [Component Name]\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**Step 1: Write the failing test**\n```python\n# tests/auth/test_login.py\ndef test_login_with_valid_credentials():\n    user = create_test_user(email=\"test@example.com\", password=\"secure123\")\n    result = login(email=\"test@example.com\", password=\"secure123\")\n    assert result.success is True\n    assert result.user_id == user.id\n```\n\n**Step 2: Run test to verify it fails**\n```bash\npytest tests/auth/test_login.py::test_login_with_valid_credentials\n# Expected: ModuleNotFoundError: No module named 'auth.login'\n```\n\n[... continue for all steps ...]\n```\n\n**THEN ask for approval using AskUserQuestion:**\n- Question: \"Is this expansion approved for bd-[N]?\"\n- Options:\n  - \"Approved - continue to next task\"\n  - \"Needs revision\"\n  - \"Other\"\n\n### 2e. If Approved: Update bd and Continue\n\n```bash\nbd update bd-3 --design \"[paste complete expansion]\"\n# Mark completed in TodoWrite\n# IMMEDIATELY continue to next task (NO asking permission)\n```\n\n### 2f. If Needs Revision: Iterate\n\n- Keep as in_progress in TodoWrite\n- Revise based on feedback\n- Present again (step 2d)\n\n## 3. After ALL Tasks Done\n\n```\nAll bd issues now contain detailed implementation steps.\nEpic ready for execution.\n```\n\n**Offer execution choice:**\n\"Ready to execute? I can use hyperpowers:executing-plans to implement iteratively.\"\n\n</the_process>\n\n<examples>\n\n<example>\n<scenario>Developer writes placeholder text instead of actual implementation steps</scenario>\n\n<code>\nbd update bd-3 --design \"## Goal\nImplement user authentication\n\n## Implementation\n[Full implementation steps as detailed above - includes all 6 step groups with complete code examples]\n\n## Tests\n[Complete code examples will be added here]\"\n</code>\n\n<why_it_fails>\n**Placeholders defeat the purpose:**\n- Engineer executing bd-3 has zero context\n- There is no \"above\" to reference (bd issue is the source of truth)\n- Violates \"exact instructions for zero-context engineer\" principle\n- Makes task impossible to execute\n\n**Common placeholder patterns (ALL FORBIDDEN):**\n- `[Full implementation steps as detailed above]`\n- `[See above for detailed steps]`\n- `[As specified in success criteria]`\n- `[Complete code examples will be added here]`\n</why_it_fails>\n\n<correction>\n**Write actual content:**\n\n```bash\nbd update bd-3 --design \"## Goal\nImplement user authentication\n\n## Implementation\n\n### Step 1: Write failing login test\n```python\n# tests/auth/test_login.py\nimport pytest\nfrom auth.service import login\n\ndef test_login_with_valid_credentials():\n    result = login(email='test@example.com', password='pass123')\n    assert result.success is True\n```\n\n### Step 2: Run test (should fail)\n```bash\npytest tests/auth/test_login.py::test_login_with_valid_credentials\n# Expected: ModuleNotFoundError: No module named 'auth.service'\n```\n\n### Step 3: Create login function\n```python\n# src/auth/service.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass LoginResult:\n    success: bool\n    user_id: int | None = None\n\ndef login(email: str, password: str) -> LoginResult:\n    # Minimal implementation\n    return LoginResult(success=True, user_id=1)\n```\n\n[... continue for all steps with complete code ...]\n\n## Tests\nAll test code included in implementation steps above following TDD cycle.\"\n```\n\n**Result:** Engineer can execute without any context.\n</correction>\n</example>\n\n<example>\n<scenario>Developer verifies codebase state themselves instead of using codebase-investigator agent</scenario>\n\n<code>\nDeveloper reads files manually:\n- Reads src/services/auth.ts directly\n- Checks package.json manually\n- Assumes file structure based on quick look\n\nWrites expansion based on quick check:\n\"Modify src/services/auth.ts (if exists)\"\n</code>\n\n<why_it_fails>\n**Manual verification problems:**\n- Misses nuances (existing functions, imports, structure)\n- Creates conditional steps (\"if exists\")\n- Doesn't catch version mismatches\n- Doesn't report discrepancies from bd assumptions\n\n**Result:** Implementation plan may not match actual codebase state.\n</why_it_fails>\n\n<correction>\n**Use codebase-investigator agent:**\n\n```\nDispatch agent with bd-3 assumptions:\n\"bd-3 expects auth service in src/services/auth.ts with login() and logout() functions.\nVerify:\n1. Does src/services/auth.ts exist?\n2. What functions does it export?\n3. How do login() and logout() work currently?\n4. Any other relevant auth code?\n5. What's the bcrypt version?\"\n```\n\n**Agent reports:**\n```\n✓ src/services/auth.ts exists\n✗ ONLY has login() function - NO logout() yet\n+ Found: login() uses argon2 NOT bcrypt\n+ Found: Session management in src/services/session.ts\n✓ argon2 version: 0.31.2\n```\n\n**Write definitive steps based on findings:**\n```\nStep 1: Add logout() function to EXISTING src/services/auth.ts:45-67\n(no \"if exists\" - investigator confirmed location)\n\nStep 2: Use argon2 (already installed 0.31.2) not bcrypt\n(no assumption - investigator confirmed actual dependency)\n```\n\n**Result:** Plan matches actual codebase state.\n</correction>\n</example>\n\n<example>\n<scenario>Developer asks permission between each task validation instead of continuing automatically</scenario>\n\n<code>\nAfter user approves bd-3 expansion:\n\nDeveloper: \"bd-3 expansion approved and updated in bd.\nShould I continue to bd-4 now? What's your preference?\"\n\n[Waits for user response]\n</code>\n\n<why_it_fails>\n**Breaks workflow momentum:**\n- Unnecessary interruption\n- User has to respond multiple times\n- Slows down batch processing\n- TodoWrite list IS the plan\n\n**Why it happens:** Over-asking for permission instead of executing the plan.\n</why_it_fails>\n\n<correction>\n**After user approves bd-3:**\n\n```bash\nbd update bd-3 --design \"[expansion]\"  # Update bd\n# Mark completed in TodoWrite\n```\n\n**IMMEDIATELY continue to bd-4:**\n```bash\nbd show bd-4  # Read next task\n# Dispatch codebase-investigator with bd-4 assumptions\n# Draft expansion\n# Present bd-4 expansion to user\n```\n\n**NO asking:** \"Should I continue?\" or \"What's your preference?\"\n\n**ONLY ask user:**\n1. When presenting each task expansion for validation\n2. At the VERY END after ALL tasks done to offer execution choice\n\n**Between validations: JUST CONTINUE.**\n\n**Result:** Efficient batch processing of all tasks.\n</correction>\n</example>\n\n</examples>\n\n<critical_rules>\n\n## Rules That Have No Exceptions\n\n1. **No placeholders or meta-references** → Write actual content\n   - ❌ FORBIDDEN: `[Full implementation steps as detailed above]`\n   - ✅ REQUIRED: Complete code, exact paths, real commands\n\n2. **Use codebase-investigator agent** → Never verify yourself\n   - Agent gets bd assumptions\n   - Agent reports discrepancies\n   - You adjust plan to match reality\n\n3. **Present COMPLETE expansion before asking** → User must SEE before approving\n   - Show full expansion in message text\n   - Then use AskUserQuestion for approval\n   - Never ask without showing first\n\n4. **Continue automatically between validations** → Don't ask permission\n   - TodoWrite list IS your plan\n   - Execute it completely\n   - Only ask: (a) task validation, (b) final execution choice\n\n5. **Write definitive steps** → Never conditional\n   - ❌ \"Update `index.js` if exists\"\n   - ✅ \"Create `src/auth.ts`\" (investigator confirmed)\n\n## Common Excuses\n\nAll of these mean: Stop, write actual content:\n- \"I'll add the details later\"\n- \"The implementation is obvious from the goal\"\n- \"See above for the steps\"\n- \"User can figure out the code\"\n\n</critical_rules>\n\n<verification_checklist>\n\nBefore marking each task complete in TodoWrite:\n- [ ] Used codebase-investigator agent (not manual verification)\n- [ ] Presented COMPLETE expansion to user (showed full text)\n- [ ] User approved expansion (via AskUserQuestion)\n- [ ] Updated bd with actual content (no placeholders)\n- [ ] No meta-references in design field\n\nBefore finishing all tasks:\n- [ ] All tasks in TodoWrite marked completed\n- [ ] All bd issues updated with expansions\n- [ ] No conditional steps (\"if exists\")\n- [ ] Complete code examples in all steps\n- [ ] Exact file paths and commands throughout\n\n</verification_checklist>\n\n<integration>\n\n**This skill calls:**\n- sre-task-refinement (optional, can run before this)\n- codebase-investigator (REQUIRED for each task verification)\n- executing-plans (offered after all tasks expanded)\n\n**This skill is called by:**\n- User (via /hyperpowers:write-plan command)\n- After brainstorming creates epic\n\n**Agents used:**\n- hyperpowers:codebase-investigator (verify assumptions, report discrepancies)\n\n</integration>\n\n<resources>\n\n**Detailed guidance:**\n- [bd command reference](../common-patterns/bd-commands.md)\n- [Task structure examples](resources/task-examples.md) (if exists)\n\n**When stuck:**\n- Unsure about file structure → Use codebase-investigator\n- Don't know version → Use codebase-investigator\n- Tempted to write \"if exists\" → Use codebase-investigator first\n- About to write placeholder → Stop, write actual content\n- Want to ask permission → Check: Is this task validation or final choice? If neither, don't ask\n\n</resources>"
              },
              {
                "name": "writing-skills",
                "description": "Use when creating new skills, editing existing skills, or verifying skills work - applies TDD to documentation by testing with subagents before writing",
                "path": "skills/writing-skills/SKILL.md",
                "frontmatter": {
                  "name": "writing-skills",
                  "description": "Use when creating new skills, editing existing skills, or verifying skills work - applies TDD to documentation by testing with subagents before writing"
                },
                "content": "<skill_overview>\nWriting skills IS test-driven development applied to process documentation; write test (pressure scenario), watch fail (baseline), write skill, watch pass, refactor (close loopholes).\n</skill_overview>\n\n<rigidity_level>\nLOW FREEDOM - Follow the RED-GREEN-REFACTOR cycle exactly when creating skills. No skill without failing test first. Same Iron Law as TDD.\n</rigidity_level>\n\n<quick_reference>\n| Phase | Action | Verify |\n|-------|--------|--------|\n| **RED** | Create pressure scenarios | Document baseline failures |\n| **RED** | Run WITHOUT skill | Agent violates rule |\n| **GREEN** | Write minimal skill | Addresses baseline failures |\n| **GREEN** | Run WITH skill | Agent now complies |\n| **REFACTOR** | Find new rationalizations | Agent still complies |\n| **REFACTOR** | Add explicit counters | Bulletproof against excuses |\n| **DEPLOY** | Commit and optionally PR | Skill ready for use |\n\n**Iron Law:** NO SKILL WITHOUT FAILING TEST FIRST (applies to new skills AND edits)\n</quick_reference>\n\n<when_to_use>\n**Create skill when:**\n- Technique wasn't intuitively obvious to you\n- You'd reference this again across projects\n- Pattern applies broadly (not project-specific)\n- Others would benefit from this knowledge\n\n**Never create for:**\n- One-off solutions\n- Standard practices well-documented elsewhere\n- Project-specific conventions (put in CLAUDE.md instead)\n\n**Edit existing skill when:**\n- Found new rationalization agents use\n- Discovered loophole in current guidance\n- Need to add clarifying examples\n\n**ALWAYS test before writing or editing. No exceptions.**\n</when_to_use>\n\n<tdd_mapping>\nSkills use the exact same TDD cycle as code:\n\n| TDD Concept | Skill Creation |\n|-------------|----------------|\n| **Test case** | Pressure scenario with subagent |\n| **Production code** | Skill document (SKILL.md) |\n| **Test fails (RED)** | Agent violates rule without skill |\n| **Test passes (GREEN)** | Agent complies with skill present |\n| **Refactor** | Close loopholes while maintaining compliance |\n| **Write test first** | Run baseline scenario BEFORE writing skill |\n| **Watch it fail** | Document exact rationalizations agent uses |\n| **Minimal code** | Write skill addressing those specific violations |\n| **Watch it pass** | Verify agent now complies |\n| **Refactor cycle** | Find new rationalizations → plug → re-verify |\n\n**REQUIRED BACKGROUND:** You MUST understand hyperpowers:test-driven-development before using this skill.\n</tdd_mapping>\n\n<the_process>\n## 1. RED Phase - Create Failing Test\n\n**Create pressure scenarios for subagent:**\n\n```\nTask tool with general-purpose agent:\n\n\"You are implementing a payment processing feature. User requirements:\n- Process credit card payments\n- Handle retries on failure\n- Log all transactions\n\n[PRESSURE 1: Time] You have 10 minutes before deployment.\n[PRESSURE 2: Sunk Cost] You've already written 200 lines of code.\n[PRESSURE 3: Authority] Senior engineer said 'just make it work, tests can wait.'\n\nImplement this feature.\"\n```\n\n**Run WITHOUT skill present.**\n\n**Document baseline behavior:**\n- Exact rationalizations agent uses (\"tests can wait,\" \"simple feature,\" etc.)\n- What agent skips (tests, verification, bd task, etc.)\n- Patterns in failure modes\n\n**Example baseline result:**\n```\nAgent response:\n\"I'll implement the payment processing quickly since time is tight...\"\n[Skips TDD]\n[Skips verification-before-completion]\n[Claims done without evidence]\n```\n\n**This is your failing test.** Agent doesn't follow the workflow without guidance.\n\n---\n\n## 2. GREEN Phase - Write Minimal Skill\n\nWrite skill that addresses the SPECIFIC failures from baseline:\n\n**Structure:**\n\n```markdown\n---\nname: skill-name-with-hyphens\ndescription: Use when [specific triggers] - [what skill does]\n---\n\n<skill_overview>\nOne sentence core principle\n</skill_overview>\n\n<rigidity_level>\nLOW | MEDIUM | HIGH FREEDOM - [What this means]\n</rigidity_level>\n\n[Rest of standard XML structure]\n```\n\n**Frontmatter rules:**\n- Only `name` and `description` fields (max 1024 chars total)\n- Name: letters, numbers, hyphens only (no parentheses/special chars)\n- Description: Start with \"Use when...\", third person, includes triggers\n\n**Description format:**\n```yaml\n# ❌ BAD: Too abstract, first person\ndescription: I can help with async tests when they're flaky\n\n# ✅ GOOD: Starts with \"Use when\", describes problem\ndescription: Use when tests have race conditions or pass/fail inconsistently - replaces arbitrary timeouts with condition polling for reliable async tests\n```\n\n**Write skill addressing baseline failures:**\n- Add explicit counters for rationalizations (\"tests can wait\" → \"NO EXCEPTIONS: tests first\")\n- Create quick reference table for scanning\n- Add concrete examples showing failure modes\n- Use XML structure for all sections\n\n**Run WITH skill present.**\n\n**Verify agent now complies:**\n- Same pressure scenario\n- Agent now follows workflow\n- No rationalizations from baseline appear\n\n**This is your passing test.**\n\n---\n\n## 3. REFACTOR Phase - Close Loopholes\n\n**Find NEW rationalizations:**\n\nRun skill with DIFFERENT pressures:\n- Combine 3+ pressures (time + sunk cost + exhaustion)\n- Try meta-rationalizations (\"this skill doesn't apply because...\")\n- Test with edge cases\n\n**Document new failures:**\n- What rationalizations appear NOW?\n- What loopholes did agent find?\n- What explicit counters are needed?\n\n**Add counters to skill:**\n\n```markdown\n<critical_rules>\n## Common Excuses\n\nAll of these mean: [Action to take]\n- \"Test can wait\" (NO, test first always)\n- \"Simple feature\" (Simple breaks too, test first)\n- \"Time pressure\" (Broken code wastes more time)\n[Add ALL rationalizations found during testing]\n</critical_rules>\n```\n\n**Re-test until bulletproof:**\n- Run scenarios again\n- Verify new counters work\n- Agent complies even under combined pressures\n\n---\n\n## 4. Quality Checks\n\nBefore deployment, verify:\n\n- [ ] Has `<quick_reference>` section (scannable table)\n- [ ] Has `<rigidity_level>` explicit\n- [ ] Has 2-3 `<example>` tags showing failure modes\n- [ ] Description <500 chars, starts with \"Use when...\"\n- [ ] Keywords throughout for search (error messages, symptoms, tools)\n- [ ] One excellent code example (not multi-language)\n- [ ] Supporting files only for tools or heavy reference (>100 lines)\n\n**Token efficiency:**\n- Frequently-loaded skills: <200 words ideally\n- Other skills: <500 words\n- Move heavy content to resources/ files\n\n---\n\n## 5. Deploy\n\n**Commit to git:**\n\n```bash\ngit add skills/skill-name/\ngit commit -m \"feat: add [skill-name] skill\n\nTested with subagents under [pressures used].\nAddresses [baseline failures found].\n\nCloses rationalizations:\n- [Rationalization 1]\n- [Rationalization 2]\"\n```\n\n**Personal skills:** Write to `~/.claude/skills/` for cross-project use\n\n**Plugin skills:** PR to plugin repository if broadly useful\n\n**STOP:** Before moving to next skill, complete this entire process. No batching untested skills.\n</the_process>\n\n<examples>\n<example>\n<scenario>Developer writes skill without testing first</scenario>\n\n<code>\n# Developer writes skill:\n\"---\nname: always-use-tdd\ndescription: Always write tests first\n---\n\nWrite tests first. No exceptions.\"\n\n# Then tries to deploy it\n</code>\n\n<why_it_fails>\n- No baseline behavior documented (don't know what agent does WITHOUT skill)\n- No verification skill actually works (might not address real rationalizations)\n- Generic guidance (\"no exceptions\") without specific counters\n- Will likely miss common excuses agents use\n- Violates Iron Law: no skill without failing test first\n</why_it_fails>\n\n<correction>\n**Correct approach (RED-GREEN-REFACTOR):**\n\n**RED Phase:**\n1. Create pressure scenario (time + sunk cost)\n2. Run WITHOUT skill\n3. Document baseline: Agent says \"I'll test after since time is tight\"\n\n**GREEN Phase:**\n1. Write skill with explicit counter to that rationalization\n2. Add: \"Common excuses: 'Time is tight' → Wrong. Broken code wastes more time. Write test first.\"\n3. Run WITH skill → agent now writes test first\n\n**REFACTOR Phase:**\n1. Try new pressure (exhaustion: \"this is the 5th feature today\")\n2. Agent finds loophole: \"these are all similar, I can skip tests\"\n3. Add counter: \"Similar ≠ identical. Write test for each.\"\n4. Re-test → bulletproof\n\n**What you gain:**\n- Know skill addresses real failures (saw baseline)\n- Confident skill works (saw it fix behavior)\n- Closed all loopholes (tested multiple pressures)\n- Ready for production use\n</correction>\n</example>\n\n<example>\n<scenario>Developer edits skill without testing changes</scenario>\n\n<code>\n# Existing skill works well\n# Developer thinks: \"I'll just add this section about edge cases\"\n\n[Adds 50 lines to skill]\n\n# Commits without testing\n</code>\n\n<why_it_fails>\n- Don't know if new section actually helps (no baseline)\n- Might introduce contradictions with existing guidance\n- Could make skill less effective (more verbose, less clear)\n- Violates Iron Law: applies to edits too\n- Changes might not address actual rationalization patterns\n</why_it_fails>\n\n<correction>\n**Correct approach:**\n\n**RED Phase (for edit):**\n1. Identify specific failure mode you want to address\n2. Create pressure scenario that triggers it\n3. Run WITH current skill → document how agent fails\n\n**GREEN Phase (edit):**\n1. Add ONLY content addressing that failure\n2. Keep changes minimal\n3. Run WITH edited skill → verify agent now complies\n\n**REFACTOR Phase:**\n1. Check edit didn't break existing scenarios\n2. Run previous test cases\n3. Verify all still pass\n\n**What you gain:**\n- Changes address real problems (saw failure)\n- Know edit helps (saw improvement)\n- Didn't break existing guidance (regression tested)\n- Skill stays bulletproof\n</correction>\n</example>\n\n<example>\n<scenario>Skill description too vague for search</scenario>\n\n<code>\n---\nname: async-testing\ndescription: For testing async code\n---\n\n# Skill content...\n</code>\n\n<why_it_fails>\n- Future Claude won't find this when needed\n- \"For testing async code\" too abstract (when would Claude search this?)\n- Doesn't describe symptoms or triggers\n- Missing keywords like \"flaky,\" \"race condition,\" \"timeout\"\n- Won't show up when agent has the actual problem\n</why_it_fails>\n\n<correction>\n**Better description:**\n\n```yaml\n---\nname: condition-based-waiting\ndescription: Use when tests have race conditions, timing dependencies, or pass/fail inconsistently - replaces arbitrary timeouts with condition polling for reliable async tests\n---\n```\n\n**Why this works:**\n- Starts with \"Use when\" (triggers)\n- Lists symptoms: \"race conditions,\" \"pass/fail inconsistently\"\n- Describes problem AND solution\n- Keywords: \"race conditions,\" \"timing,\" \"inconsistent,\" \"timeouts\"\n- Future Claude searching \"why are my tests flaky\" will find this\n\n**What you gain:**\n- Skill actually gets found when needed\n- Claude knows when to use it (clear triggers)\n- Search terms match real developer language\n- Description doubles as activation criteria\n</correction>\n</example>\n</examples>\n\n<skill_types>\n## Technique\nConcrete method with steps to follow.\n\n**Examples:** condition-based-waiting, hyperpowers:root-cause-tracing\n\n**Test approach:** Pressure scenarios with combined pressures\n\n## Pattern\nWay of thinking about problems.\n\n**Examples:** flatten-with-flags, test-invariants\n\n**Test approach:** Present problems the pattern solves, verify agent applies pattern\n\n## Reference\nAPI docs, syntax guides, tool documentation.\n\n**Examples:** Office document manipulation, API reference guides\n\n**Test approach:** Give task requiring reference, verify agent uses it correctly\n\n**For detailed testing methodology by skill type:** See [resources/testing-methodology.md](resources/testing-methodology.md)\n</skill_types>\n\n<file_organization>\n## Self-Contained Skill\n```\ndefense-in-depth/\n  SKILL.md    # Everything inline\n```\n**When:** All content fits, no heavy reference needed\n\n## Skill with Reusable Tool\n```\ncondition-based-waiting/\n  SKILL.md    # Overview + patterns\n  example.ts  # Working helpers to adapt\n```\n**When:** Tool is reusable code, not just narrative\n\n## Skill with Heavy Reference\n```\npptx/\n  SKILL.md       # Overview + workflows\n  pptxgenjs.md   # 600 lines API reference\n  ooxml.md       # 500 lines XML structure\n  scripts/       # Executable tools\n```\n**When:** Reference material too large for inline (>100 lines)\n\n**Keep inline:**\n- Principles and concepts\n- Code patterns (<50 lines)\n- Everything that fits\n</file_organization>\n\n<search_optimization>\n## Claude Search Optimization (CSO)\n\nFuture Claude needs to FIND your skill. Optimize for search.\n\n### 1. Rich Description Field\n\n**Format:** Start with \"Use when...\" + triggers + what it does\n\n```yaml\n# ❌ BAD: Too abstract\ndescription: For async testing\n\n# ❌ BAD: First person\ndescription: I can help you with async tests\n\n# ✅ GOOD: Triggers + problem + solution\ndescription: Use when tests have race conditions or pass/fail inconsistently - replaces arbitrary timeouts with condition polling\n```\n\n### 2. Keyword Coverage\n\nUse words Claude would search for:\n- **Error messages:** \"Hook timed out\", \"ENOTEMPTY\", \"race condition\"\n- **Symptoms:** \"flaky\", \"hanging\", \"zombie\", \"pollution\"\n- **Synonyms:** \"timeout/hang/freeze\", \"cleanup/teardown/afterEach\"\n- **Tools:** Actual commands, library names, file types\n\n### 3. Token Efficiency\n\n**Problem:** Frequently-referenced skills load into EVERY conversation.\n\n**Target word counts:**\n- Frequently-loaded: <200 words\n- Other skills: <500 words\n\n**Techniques:**\n- Move details to tool --help\n- Use cross-references to other skills\n- Compress examples\n- Eliminate redundancy\n\n**Verification:**\n```bash\nwc -w skills/skill-name/SKILL.md\n```\n\n### 4. Cross-Referencing\n\n**Use skill name only, with explicit markers:**\n```markdown\n**REQUIRED BACKGROUND:** You MUST understand hyperpowers:test-driven-development\n**REQUIRED SUB-SKILL:** Use hyperpowers:debugging-with-tools first\n```\n\n**Don't use @ links:** Force-loads files immediately, burns context unnecessarily.\n</search_optimization>\n\n<critical_rules>\n## Rules That Have No Exceptions\n\n1. **NO SKILL WITHOUT FAILING TEST FIRST** → Applies to new skills AND edits\n2. **Test with subagents under pressure** → Combined pressures (time + sunk cost + authority)\n3. **Document baseline behavior** → Exact rationalizations, not paraphrases\n4. **Write minimal skill addressing baseline** → Don't add content not validated by testing\n5. **STOP before next skill** → Complete RED-GREEN-REFACTOR-DEPLOY for each skill\n\n## Common Excuses\n\nAll of these mean: **STOP. Run baseline test first.**\n\n- \"Simple skill, don't need testing\" (If simple, testing is fast. Do it.)\n- \"Just adding documentation\" (Documentation can be wrong. Test it.)\n- \"I'll test after I write a few\" (Batching untested = deploying untested code)\n- \"This is obvious, everyone knows it\" (Then baseline will show agent already complies)\n- \"Testing is overkill for skills\" (TDD applies to documentation too)\n- \"I'll adapt while testing\" (Violates RED phase. Start over.)\n- \"I'll keep untested as reference\" (Delete means delete. No exceptions.)\n\n## The Iron Law\n\nSame as TDD:\n\n```\nNO SKILL WITHOUT FAILING TEST FIRST\n```\n\n**No exceptions for:**\n- \"Simple additions\"\n- \"Just adding a section\"\n- \"Documentation updates\"\n- Edits to existing skills\n\n**Write skill before testing?** Delete it. Start over.\n</critical_rules>\n\n<verification_checklist>\nBefore deploying ANY skill:\n\n**RED Phase:**\n- [ ] Created pressure scenarios (3+ combined pressures for discipline skills)\n- [ ] Ran WITHOUT skill present\n- [ ] Documented baseline behavior verbatim (exact rationalizations)\n- [ ] Identified patterns in failures\n\n**GREEN Phase:**\n- [ ] Name uses only letters, numbers, hyphens\n- [ ] YAML frontmatter: name + description only (max 1024 chars)\n- [ ] Description starts with \"Use when...\" and includes triggers\n- [ ] Description in third person\n- [ ] Has `<quick_reference>` section\n- [ ] Has `<rigidity_level>` explicit\n- [ ] Has 2-3 `<example>` tags\n- [ ] Addresses specific baseline failures\n- [ ] Ran WITH skill present\n- [ ] Verified agent now complies\n\n**REFACTOR Phase:**\n- [ ] Tested with different pressures\n- [ ] Found NEW rationalizations\n- [ ] Added explicit counters\n- [ ] Re-tested until bulletproof\n\n**Quality:**\n- [ ] Keywords throughout for search\n- [ ] One excellent code example (not multi-language)\n- [ ] Token-efficient (check word count)\n- [ ] Supporting files only if needed\n\n**Deploy:**\n- [ ] Committed to git with descriptive message\n- [ ] Pushed to plugin repository (if applicable)\n\n**Can't check all boxes?** Return to process and fix.\n</verification_checklist>\n\n<integration>\n**This skill requires:**\n- hyperpowers:test-driven-development (understand TDD before applying to docs)\n- Task tool (for running subagent tests)\n\n**This skill is called by:**\n- Anyone creating or editing skills\n- Plugin maintainers\n- Users with personal skill repositories\n\n**Agents used:**\n- general-purpose (for testing skills under pressure)\n</integration>\n\n<resources>\n**Detailed guides:**\n- [Testing methodology by skill type](resources/testing-methodology.md) - How to test disciplines, techniques, patterns, reference skills\n- [Anthropic best practices](resources/anthropic-best-practices.md) - Official skill authoring guidance\n- [Graphviz conventions](resources/graphviz-conventions.dot) - Flowchart style rules\n\n**When stuck:**\n- Skill seems too simple to test → If simple, testing is fast. Do it anyway.\n- Don't know what pressures to use → Time + sunk cost + authority always work\n- Agent still rationalizes → Add explicit counter for that exact excuse\n- Testing feels like overhead → Same as TDD: testing prevents bigger problems\n</resources>"
              }
            ]
          }
        ]
      }
    }
  ]
}