{
  "owner": {
    "id": "terrylica",
    "display_name": "Terry Li",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/99227742?u=84558356b98a8456eb4ac0d38b308594aeac2041&v=4",
    "url": "https://github.com/terrylica",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 19,
      "total_commands": 42,
      "total_skills": 54,
      "total_stars": 6,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "terrylica/cc-skills",
      "url": "https://github.com/terrylica/cc-skills",
      "description": "Claude Code Skills Marketplace: plugins, skills for ADR-driven development, DevOps automation, ClickHouse management, semantic versioning, and productivity workflows",
      "homepage": "",
      "signals": {
        "stars": 6,
        "forks": 0,
        "pushed_at": "2026-01-12T22:07:09Z",
        "created_at": "2025-12-04T16:26:10Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 11724
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 452
        },
        {
          "path": ".claude-plugin/plugins",
          "type": "blob",
          "size": 10
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 404
        },
        {
          "path": ".lychee.toml",
          "type": "blob",
          "size": 860
        },
        {
          "path": ".mise",
          "type": "tree",
          "size": null
        },
        {
          "path": ".mise/tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": ".mise/tasks/release",
          "type": "blob",
          "size": 1141
        },
        {
          "path": ".mise/tasks/release:clean",
          "type": "blob",
          "size": 602
        },
        {
          "path": ".mise/tasks/release:dry",
          "type": "blob",
          "size": 573
        },
        {
          "path": ".mise/tasks/release:full",
          "type": "blob",
          "size": 1201
        },
        {
          "path": ".mise/tasks/release:hooks",
          "type": "blob",
          "size": 215
        },
        {
          "path": ".mise/tasks/release:preflight",
          "type": "blob",
          "size": 2038
        },
        {
          "path": ".mise/tasks/release:status",
          "type": "blob",
          "size": 1190
        },
        {
          "path": ".mise/tasks/release:sync",
          "type": "blob",
          "size": 1684
        },
        {
          "path": ".mise/tasks/release:verify",
          "type": "blob",
          "size": 2781
        },
        {
          "path": ".mise/tasks/release:version",
          "type": "blob",
          "size": 667
        },
        {
          "path": ".releaserc.yml",
          "type": "blob",
          "size": 7424
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 240359
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 3989
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 19612
        },
        {
          "path": "bun.lock",
          "type": "blob",
          "size": 42590
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/CLAUDE.md",
          "type": "blob",
          "size": 2067
        },
        {
          "path": "docs/HOOKS.md",
          "type": "blob",
          "size": 5214
        },
        {
          "path": "docs/RELEASE.md",
          "type": "blob",
          "size": 3595
        },
        {
          "path": "docs/adr",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/adr/2025-12-05-centralized-version-management.md",
          "type": "blob",
          "size": 9261
        },
        {
          "path": "docs/adr/2025-12-05-itp-setup-todowrite-workflow.md",
          "type": "blob",
          "size": 16419
        },
        {
          "path": "docs/adr/2025-12-05-itp-todo-insertion-merge.md",
          "type": "blob",
          "size": 13308
        },
        {
          "path": "docs/adr/2025-12-06-pretooluse-posttooluse-hooks.md",
          "type": "blob",
          "size": 7687
        },
        {
          "path": "docs/adr/2025-12-06-release-notes-adr-linking.md",
          "type": "blob",
          "size": 15713
        },
        {
          "path": "docs/adr/2025-12-06-shell-command-portability-zsh.md",
          "type": "blob",
          "size": 8966
        },
        {
          "path": "docs/adr/2025-12-07-gitleaks-setup-integration.md",
          "type": "blob",
          "size": 10351
        },
        {
          "path": "docs/adr/2025-12-07-idempotency-backup-traceability.md",
          "type": "blob",
          "size": 10363
        },
        {
          "path": "docs/adr/2025-12-07-itp-hooks-settings-installer.md",
          "type": "blob",
          "size": 5531
        },
        {
          "path": "docs/adr/2025-12-07-setup-hooks-reminder.md",
          "type": "blob",
          "size": 7696
        },
        {
          "path": "docs/adr/2025-12-08-clickhouse-cloud-management-skill.md",
          "type": "blob",
          "size": 9029
        },
        {
          "path": "docs/adr/2025-12-08-mise-env-centralized-config.md",
          "type": "blob",
          "size": 8951
        },
        {
          "path": "docs/adr/2025-12-08-mise-tasks-skill.md",
          "type": "blob",
          "size": 12149
        },
        {
          "path": "docs/adr/2025-12-09-clickhouse-architect-skill.md",
          "type": "blob",
          "size": 19000
        },
        {
          "path": "docs/adr/2025-12-09-clickhouse-pydantic-config-skill.md",
          "type": "blob",
          "size": 11606
        },
        {
          "path": "docs/adr/2025-12-09-clickhouse-schema-documentation.md",
          "type": "blob",
          "size": 10948
        },
        {
          "path": "docs/adr/2025-12-09-itp-hooks-plan-file-exemption.md",
          "type": "blob",
          "size": 10370
        },
        {
          "path": "docs/adr/2025-12-09-itp-hooks-workflow-aware-graph-easy.md",
          "type": "blob",
          "size": 10164
        },
        {
          "path": "docs/adr/2025-12-10-clickhouse-skill-delegation.md",
          "type": "blob",
          "size": 12682
        },
        {
          "path": "docs/adr/2025-12-10-clickhouse-skill-documentation-gaps.md",
          "type": "blob",
          "size": 7437
        },
        {
          "path": "docs/adr/2025-12-11-link-checker-plugin-extraction.md",
          "type": "blob",
          "size": 11851
        },
        {
          "path": "docs/adr/2025-12-11-ruff-posttooluse-linting.md",
          "type": "blob",
          "size": 8166
        },
        {
          "path": "docs/adr/2025-12-12-mlflow-python-skill.md",
          "type": "blob",
          "size": 8966
        },
        {
          "path": "docs/adr/2025-12-13-itp-hooks-file-tree-detection.md",
          "type": "blob",
          "size": 3574
        },
        {
          "path": "docs/adr/2025-12-14-alpha-forge-worktree-management.md",
          "type": "blob",
          "size": 8979
        },
        {
          "path": "docs/adr/2025-12-15-iterm2-layout-config.md",
          "type": "blob",
          "size": 9044
        },
        {
          "path": "docs/adr/2025-12-17-posttooluse-hook-visibility.md",
          "type": "blob",
          "size": 3779
        },
        {
          "path": "docs/adr/2025-12-20-ralph-rssi-eternal-loop.md",
          "type": "blob",
          "size": 14430
        },
        {
          "path": "docs/adr/2025-12-22-ralph-dual-time-tracking.md",
          "type": "blob",
          "size": 6052
        },
        {
          "path": "docs/adr/2025-12-22-ralph-stop-visibility-observability.md",
          "type": "blob",
          "size": 5449
        },
        {
          "path": "docs/adr/2025-12-22-skill-bash-compatibility-enforcement.md",
          "type": "blob",
          "size": 3192
        },
        {
          "path": "docs/adr/2025-12-24-asciinema-tools-plugin.md",
          "type": "blob",
          "size": 8055
        },
        {
          "path": "docs/adr/2025-12-26-asciinema-daemon-architecture.md",
          "type": "blob",
          "size": 11312
        },
        {
          "path": "docs/adr/2025-12-27-fake-data-guard-universal.md",
          "type": "blob",
          "size": 5766
        },
        {
          "path": "docs/adr/2025-12-29-ralph-constraint-scanning.md",
          "type": "blob",
          "size": 11068
        },
        {
          "path": "docs/adr/2026-01-02-ralph-guidance-freshness-detection.md",
          "type": "blob",
          "size": 19098
        },
        {
          "path": "docs/adr/2026-01-02-session-chronicle-s3-sharing.md",
          "type": "blob",
          "size": 12067
        },
        {
          "path": "docs/adr/2026-01-03-gh-tools-webfetch-enforcement.md",
          "type": "blob",
          "size": 4897
        },
        {
          "path": "docs/adr/2026-01-10-uv-reminder-hook.md",
          "type": "blob",
          "size": 3855
        },
        {
          "path": "docs/adr/2026-01-11-gh-issue-body-file-guard.md",
          "type": "blob",
          "size": 1868
        },
        {
          "path": "docs/adr/2026-01-12-mise-gh-cli-incompatibility.md",
          "type": "blob",
          "size": 1485
        },
        {
          "path": "docs/design",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-05-centralized-version-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-05-centralized-version-management/spec.md",
          "type": "blob",
          "size": 11441
        },
        {
          "path": "docs/design/2025-12-05-itp-setup-todowrite-workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-05-itp-setup-todowrite-workflow/spec.md",
          "type": "blob",
          "size": 5492
        },
        {
          "path": "docs/design/2025-12-05-itp-todo-insertion-merge",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-05-itp-todo-insertion-merge/spec.md",
          "type": "blob",
          "size": 6293
        },
        {
          "path": "docs/design/2025-12-06-pretooluse-posttooluse-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-06-pretooluse-posttooluse-hooks/spec.md",
          "type": "blob",
          "size": 4534
        },
        {
          "path": "docs/design/2025-12-06-release-notes-adr-linking",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-06-release-notes-adr-linking/spec.md",
          "type": "blob",
          "size": 6619
        },
        {
          "path": "docs/design/2025-12-06-shell-command-portability-zsh",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-06-shell-command-portability-zsh/spec.md",
          "type": "blob",
          "size": 10346
        },
        {
          "path": "docs/design/2025-12-07-gitleaks-setup-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-07-gitleaks-setup-integration/spec.md",
          "type": "blob",
          "size": 3149
        },
        {
          "path": "docs/design/2025-12-07-idempotency-backup-traceability",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-07-idempotency-backup-traceability/spec.md",
          "type": "blob",
          "size": 9711
        },
        {
          "path": "docs/design/2025-12-07-itp-hooks-settings-installer",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-07-itp-hooks-settings-installer/spec.md",
          "type": "blob",
          "size": 3713
        },
        {
          "path": "docs/design/2025-12-07-setup-hooks-reminder",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-07-setup-hooks-reminder/spec.md",
          "type": "blob",
          "size": 2271
        },
        {
          "path": "docs/design/2025-12-08-clickhouse-cloud-management-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-08-clickhouse-cloud-management-skill/spec.md",
          "type": "blob",
          "size": 5813
        },
        {
          "path": "docs/design/2025-12-08-mise-env-centralized-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-08-mise-env-centralized-config/spec.md",
          "type": "blob",
          "size": 9042
        },
        {
          "path": "docs/design/2025-12-08-mise-tasks-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-08-mise-tasks-skill/spec.md",
          "type": "blob",
          "size": 5988
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-architect-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-architect-skill/spec.md",
          "type": "blob",
          "size": 9600
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-pydantic-config-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-pydantic-config-skill/spec.md",
          "type": "blob",
          "size": 10755
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-schema-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-09-clickhouse-schema-documentation/spec.md",
          "type": "blob",
          "size": 3819
        },
        {
          "path": "docs/design/2025-12-09-itp-hooks-plan-file-exemption",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-09-itp-hooks-plan-file-exemption/spec.md",
          "type": "blob",
          "size": 3046
        },
        {
          "path": "docs/design/2025-12-09-itp-hooks-workflow-aware-graph-easy",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-09-itp-hooks-workflow-aware-graph-easy/spec.md",
          "type": "blob",
          "size": 3363
        },
        {
          "path": "docs/design/2025-12-10-clickhouse-skill-delegation",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-10-clickhouse-skill-delegation/spec.md",
          "type": "blob",
          "size": 6488
        },
        {
          "path": "docs/design/2025-12-10-clickhouse-skill-documentation-gaps",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-10-clickhouse-skill-documentation-gaps/spec.md",
          "type": "blob",
          "size": 4999
        },
        {
          "path": "docs/design/2025-12-11-link-checker-plugin-extraction",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-11-link-checker-plugin-extraction/spec.md",
          "type": "blob",
          "size": 7562
        },
        {
          "path": "docs/design/2025-12-11-ruff-posttooluse-linting",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-11-ruff-posttooluse-linting/spec.md",
          "type": "blob",
          "size": 5588
        },
        {
          "path": "docs/design/2025-12-12-mlflow-python-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-12-mlflow-python-skill/spec.md",
          "type": "blob",
          "size": 5665
        },
        {
          "path": "docs/design/2025-12-14-alpha-forge-worktree-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-14-alpha-forge-worktree-management/spec.md",
          "type": "blob",
          "size": 8907
        },
        {
          "path": "docs/design/2025-12-15-iterm2-layout-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-15-iterm2-layout-config/spec.md",
          "type": "blob",
          "size": 5004
        },
        {
          "path": "docs/design/2025-12-20-ralph-rssi-eternal-loop",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-20-ralph-rssi-eternal-loop/spec.md",
          "type": "blob",
          "size": 11244
        },
        {
          "path": "docs/design/2025-12-27-fake-data-guard-universal",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2025-12-27-fake-data-guard-universal/spec.md",
          "type": "blob",
          "size": 10969
        },
        {
          "path": "docs/design/2026-01-02-ralph-guidance-freshness-detection",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2026-01-02-ralph-guidance-freshness-detection/spec.md",
          "type": "blob",
          "size": 26862
        },
        {
          "path": "docs/design/2026-01-02-session-chronicle-s3-sharing",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design/2026-01-02-session-chronicle-s3-sharing/spec.md",
          "type": "blob",
          "size": 24436
        },
        {
          "path": "docs/plugin-authoring.md",
          "type": "blob",
          "size": 1693
        },
        {
          "path": "docs/troubleshooting",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/troubleshooting/marketplace-installation.md",
          "type": "blob",
          "size": 14488
        },
        {
          "path": "lychee.toml",
          "type": "blob",
          "size": 1540
        },
        {
          "path": "outputs",
          "type": "tree",
          "size": null
        },
        {
          "path": "outputs/exp-066a-full",
          "type": "tree",
          "size": null
        },
        {
          "path": "outputs/exp-066a-full/threshold_100",
          "type": "tree",
          "size": null
        },
        {
          "path": "outputs/exp-066a-full/threshold_100/threshold_100",
          "type": "tree",
          "size": null
        },
        {
          "path": "outputs/exp-066a-full/threshold_100/threshold_100/folds.jsonl",
          "type": "blob",
          "size": 137129
        },
        {
          "path": "outputs/exp-066a-full/threshold_100/threshold_100/summary.json",
          "type": "blob",
          "size": 1129
        },
        {
          "path": "package-lock.json",
          "type": "blob",
          "size": 143899
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 1160
        },
        {
          "path": "plugin.json",
          "type": "blob",
          "size": 447
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/CLAUDE.md",
          "type": "blob",
          "size": 3591
        },
        {
          "path": "plugins/alpha-forge-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/alpha-forge-worktree/README.md",
          "type": "blob",
          "size": 4162
        },
        {
          "path": "plugins/alpha-forge-worktree/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/SKILL.md",
          "type": "blob",
          "size": 12262
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/references/naming-conventions.md",
          "type": "blob",
          "size": 5955
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/scripts/cleanup-worktree.sh",
          "type": "blob",
          "size": 3146
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/scripts/create-worktree.sh",
          "type": "blob",
          "size": 9445
        },
        {
          "path": "plugins/alpha-forge-worktree/skills/worktree-manager/scripts/detect-stale.sh",
          "type": "blob",
          "size": 2844
        },
        {
          "path": "plugins/asciinema-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/asciinema-tools/README.md",
          "type": "blob",
          "size": 6059
        },
        {
          "path": "plugins/asciinema-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/commands/analyze.md",
          "type": "blob",
          "size": 1774
        },
        {
          "path": "plugins/asciinema-tools/commands/backup.md",
          "type": "blob",
          "size": 1460
        },
        {
          "path": "plugins/asciinema-tools/commands/bootstrap.md",
          "type": "blob",
          "size": 12983
        },
        {
          "path": "plugins/asciinema-tools/commands/convert.md",
          "type": "blob",
          "size": 1498
        },
        {
          "path": "plugins/asciinema-tools/commands/daemon-logs.md",
          "type": "blob",
          "size": 2572
        },
        {
          "path": "plugins/asciinema-tools/commands/daemon-setup.md",
          "type": "blob",
          "size": 15119
        },
        {
          "path": "plugins/asciinema-tools/commands/daemon-start.md",
          "type": "blob",
          "size": 1528
        },
        {
          "path": "plugins/asciinema-tools/commands/daemon-status.md",
          "type": "blob",
          "size": 10576
        },
        {
          "path": "plugins/asciinema-tools/commands/daemon-stop.md",
          "type": "blob",
          "size": 1347
        },
        {
          "path": "plugins/asciinema-tools/commands/finalize.md",
          "type": "blob",
          "size": 7249
        },
        {
          "path": "plugins/asciinema-tools/commands/format.md",
          "type": "blob",
          "size": 1139
        },
        {
          "path": "plugins/asciinema-tools/commands/full-workflow.md",
          "type": "blob",
          "size": 1722
        },
        {
          "path": "plugins/asciinema-tools/commands/hooks.md",
          "type": "blob",
          "size": 1405
        },
        {
          "path": "plugins/asciinema-tools/commands/play.md",
          "type": "blob",
          "size": 1406
        },
        {
          "path": "plugins/asciinema-tools/commands/post-session.md",
          "type": "blob",
          "size": 5397
        },
        {
          "path": "plugins/asciinema-tools/commands/record.md",
          "type": "blob",
          "size": 1259
        },
        {
          "path": "plugins/asciinema-tools/commands/setup.md",
          "type": "blob",
          "size": 1708
        },
        {
          "path": "plugins/asciinema-tools/commands/summarize.md",
          "type": "blob",
          "size": 6927
        },
        {
          "path": "plugins/asciinema-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/scripts/asciinema-chunker.plist.template",
          "type": "blob",
          "size": 2303
        },
        {
          "path": "plugins/asciinema-tools/scripts/bootstrap-claude-session.sh",
          "type": "blob",
          "size": 7554
        },
        {
          "path": "plugins/asciinema-tools/scripts/idle-chunker-daemon.sh",
          "type": "blob",
          "size": 9689
        },
        {
          "path": "plugins/asciinema-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-analyzer/SKILL.md",
          "type": "blob",
          "size": 10505
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-analyzer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-analyzer/references/analysis-tiers.md",
          "type": "blob",
          "size": 7298
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-analyzer/references/domain-keywords.md",
          "type": "blob",
          "size": 5696
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-cast-format",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-cast-format/SKILL.md",
          "type": "blob",
          "size": 5715
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-converter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-converter/SKILL.md",
          "type": "blob",
          "size": 8766
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-player",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-player/SKILL.md",
          "type": "blob",
          "size": 10022
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-recorder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-recorder/SKILL.md",
          "type": "blob",
          "size": 8554
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/SKILL.md",
          "type": "blob",
          "size": 36749
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/references/autonomous-validation.md",
          "type": "blob",
          "size": 18135
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/references/github-workflow.md",
          "type": "blob",
          "size": 6157
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/references/idle-chunker.md",
          "type": "blob",
          "size": 6811
        },
        {
          "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/references/setup-scripts.md",
          "type": "blob",
          "size": 15395
        },
        {
          "path": "plugins/devops-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/devops-tools/README.md",
          "type": "blob",
          "size": 4683
        },
        {
          "path": "plugins/devops-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-cloud-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-cloud-management/SKILL.md",
          "type": "blob",
          "size": 5914
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-cloud-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-cloud-management/references/sql-patterns.md",
          "type": "blob",
          "size": 6978
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/SKILL.md",
          "type": "blob",
          "size": 6961
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/references/dbeaver-format.md",
          "type": "blob",
          "size": 5747
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/references/pydantic-model.md",
          "type": "blob",
          "size": 5625
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/scripts/generate_dbeaver_config.py",
          "type": "blob",
          "size": 7159
        },
        {
          "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/scripts/validate_config.py",
          "type": "blob",
          "size": 3511
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/SKILL.md",
          "type": "blob",
          "size": 5650
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/references/doppler-patterns.md",
          "type": "blob",
          "size": 5664
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/scripts/test_api_auth.py",
          "type": "blob",
          "size": 5160
        },
        {
          "path": "plugins/devops-tools/skills/doppler-secret-validation/scripts/validate_secret.py",
          "type": "blob",
          "size": 5413
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/AWS_SPECIFICATION.yaml",
          "type": "blob",
          "size": 7075
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/AWS_WORKFLOW.md",
          "type": "blob",
          "size": 10865
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/PYPI_REFERENCE.yaml",
          "type": "blob",
          "size": 19479
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/SKILL.md",
          "type": "blob",
          "size": 3486
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/references/aws-credentials.md",
          "type": "blob",
          "size": 2339
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/references/multi-service-patterns.md",
          "type": "blob",
          "size": 662
        },
        {
          "path": "plugins/devops-tools/skills/doppler-workflows/references/pypi-publishing.md",
          "type": "blob",
          "size": 1768
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/SKILL.md",
          "type": "blob",
          "size": 4006
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/examples/bot-wrapper.sh",
          "type": "blob",
          "size": 6348
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/examples/notify-restart.sh",
          "type": "blob",
          "size": 11726
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/examples/setup-example.sh",
          "type": "blob",
          "size": 6004
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/reference.md",
          "type": "blob",
          "size": 16315
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references/common-pitfalls.md",
          "type": "blob",
          "size": 3625
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references/credential-management.md",
          "type": "blob",
          "size": 1458
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references/pushover-integration.md",
          "type": "blob",
          "size": 2304
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references/telegram-html.md",
          "type": "blob",
          "size": 1905
        },
        {
          "path": "plugins/devops-tools/skills/dual-channel-watchexec/references/watchexec-patterns.md",
          "type": "blob",
          "size": 1653
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/SKILL.md",
          "type": "blob",
          "size": 5456
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/references/authentication.md",
          "type": "blob",
          "size": 2166
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/references/migration-from-cli.md",
          "type": "blob",
          "size": 3612
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/references/quantstats-metrics.md",
          "type": "blob",
          "size": 5720
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/references/query-patterns.md",
          "type": "blob",
          "size": 4023
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/scripts/create_experiment.py",
          "type": "blob",
          "size": 3221
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/scripts/get_metric_history.py",
          "type": "blob",
          "size": 3928
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/scripts/log_backtest.py",
          "type": "blob",
          "size": 9076
        },
        {
          "path": "plugins/devops-tools/skills/mlflow-python/scripts/query_experiments.py",
          "type": "blob",
          "size": 6674
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/SKILL.md",
          "type": "blob",
          "size": 23540
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/references/provenance-schema.json",
          "type": "blob",
          "size": 5282
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/references/s3-manifest-schema.json",
          "type": "blob",
          "size": 6000
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/references/s3-retrieval-guide.md",
          "type": "blob",
          "size": 4956
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/references/session-entry-schema.json",
          "type": "blob",
          "size": 3488
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/extract_context.sh",
          "type": "blob",
          "size": 1436
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/extract_session_chain.sh",
          "type": "blob",
          "size": 4111
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/generate_commit_message.sh",
          "type": "blob",
          "size": 2976
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/retrieve_artifact.sh",
          "type": "blob",
          "size": 4160
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/s3_upload.sh",
          "type": "blob",
          "size": 4172
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/search_sessions.sh",
          "type": "blob",
          "size": 1799
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/session_indexer.sh",
          "type": "blob",
          "size": 2104
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/scripts/uuid_tracer.sh",
          "type": "blob",
          "size": 2989
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/AUDIT-REPORT-2026-01-02.md",
          "type": "blob",
          "size": 7871
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/README.md",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/fixtures/expected-manifest.json",
          "type": "blob",
          "size": 342
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/fixtures/mock-session.jsonl",
          "type": "blob",
          "size": 848
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/fixtures/mock-uuid-chain.jsonl",
          "type": "blob",
          "size": 490
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-brotli.sh",
          "type": "blob",
          "size": 1838
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-commit-format.sh",
          "type": "blob",
          "size": 2236
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-credential-access.sh",
          "type": "blob",
          "size": 1924
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-cross-references.sh",
          "type": "blob",
          "size": 4152
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-e2e.sh",
          "type": "blob",
          "size": 1156
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-extract-chain.sh",
          "type": "blob",
          "size": 1509
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-prerequisites.sh",
          "type": "blob",
          "size": 940
        },
        {
          "path": "plugins/devops-tools/skills/session-chronicle/tests/scripts/validate-s3-upload.sh",
          "type": "blob",
          "size": 2551
        },
        {
          "path": "plugins/devops-tools/skills/session-recovery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/session-recovery/SKILL.md",
          "type": "blob",
          "size": 2017
        },
        {
          "path": "plugins/devops-tools/skills/session-recovery/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 5486
        },
        {
          "path": "plugins/devops-tools/skills/telegram-bot-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/telegram-bot-management/SKILL.md",
          "type": "blob",
          "size": 1683
        },
        {
          "path": "plugins/devops-tools/skills/telegram-bot-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/devops-tools/skills/telegram-bot-management/references/operational-commands.md",
          "type": "blob",
          "size": 1812
        },
        {
          "path": "plugins/devops-tools/skills/telegram-bot-management/references/troubleshooting.md",
          "type": "blob",
          "size": 3010
        },
        {
          "path": "plugins/doc-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/doc-tools/README.md",
          "type": "blob",
          "size": 2285
        },
        {
          "path": "plugins/doc-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/SKILL.md",
          "type": "blob",
          "size": 5075
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/references/DELIVERABLES_SUMMARY.md",
          "type": "blob",
          "size": 11265
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/references/INTEGRATION_GUIDE.md",
          "type": "blob",
          "size": 14987
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/references/SCRIPT_DESIGN_REPORT.md",
          "type": "blob",
          "size": 15630
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/ascii-diagram-validator/scripts/check_ascii_alignment.py",
          "type": "blob",
          "size": 21871
        },
        {
          "path": "plugins/doc-tools/skills/documentation-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/documentation-standards/SKILL.md",
          "type": "blob",
          "size": 5410
        },
        {
          "path": "plugins/doc-tools/skills/latex-build",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/SKILL.md",
          "type": "blob",
          "size": 3062
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references/advanced-patterns.md",
          "type": "blob",
          "size": 760
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references/common-commands.md",
          "type": "blob",
          "size": 906
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references/configuration.md",
          "type": "blob",
          "size": 1035
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references/multi-file-projects.md",
          "type": "blob",
          "size": 846
        },
        {
          "path": "plugins/doc-tools/skills/latex-build/references/troubleshooting.md",
          "type": "blob",
          "size": 1266
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/REFERENCE.md",
          "type": "blob",
          "size": 10162
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/SKILL.md",
          "type": "blob",
          "size": 2752
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references/installation.md",
          "type": "blob",
          "size": 764
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references/package-management.md",
          "type": "blob",
          "size": 655
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references/skim-configuration.md",
          "type": "blob",
          "size": 541
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references/troubleshooting.md",
          "type": "blob",
          "size": 821
        },
        {
          "path": "plugins/doc-tools/skills/latex-setup/references/verification.md",
          "type": "blob",
          "size": 709
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/SKILL.md",
          "type": "blob",
          "size": 3020
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references/column-spec.md",
          "type": "blob",
          "size": 770
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references/lines-borders.md",
          "type": "blob",
          "size": 573
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references/migration.md",
          "type": "blob",
          "size": 534
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references/table-patterns.md",
          "type": "blob",
          "size": 1585
        },
        {
          "path": "plugins/doc-tools/skills/latex-tables/references/troubleshooting.md",
          "type": "blob",
          "size": 927
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/SKILL.md",
          "type": "blob",
          "size": 7569
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/assets/build-pdf-example.sh",
          "type": "blob",
          "size": 5223
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/assets/build-pdf.sh",
          "type": "blob",
          "size": 7304
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/assets/hide-details-for-pdf.lua",
          "type": "blob",
          "size": 2254
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/assets/table-spacing-template.tex",
          "type": "blob",
          "size": 6067
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/bibliography-citations.md",
          "type": "blob",
          "size": 2199
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/core-principles.md",
          "type": "blob",
          "size": 8102
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/document-patterns.md",
          "type": "blob",
          "size": 2568
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/latex-parameters.md",
          "type": "blob",
          "size": 11700
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/markdown-for-pdf.md",
          "type": "blob",
          "size": 5829
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/troubleshooting-pandoc.md",
          "type": "blob",
          "size": 6543
        },
        {
          "path": "plugins/doc-tools/skills/pandoc-pdf-generation/references/yaml-structure.md",
          "type": "blob",
          "size": 898
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print/SKILL.md",
          "type": "blob",
          "size": 3556
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print/assets/print-terminal.sh",
          "type": "blob",
          "size": 3901
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc-tools/skills/terminal-print/references/workflow.md",
          "type": "blob",
          "size": 5178
        },
        {
          "path": "plugins/dotfiles-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/dotfiles-tools/README.md",
          "type": "blob",
          "size": 3929
        },
        {
          "path": "plugins/dotfiles-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/commands/hooks.md",
          "type": "blob",
          "size": 1541
        },
        {
          "path": "plugins/dotfiles-tools/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/hooks/chezmoi-stop-guard.mjs",
          "type": "blob",
          "size": 5915
        },
        {
          "path": "plugins/dotfiles-tools/hooks/chezmoi-sync-reminder.sh",
          "type": "blob",
          "size": 5260
        },
        {
          "path": "plugins/dotfiles-tools/hooks/hooks.json",
          "type": "blob",
          "size": 595
        },
        {
          "path": "plugins/dotfiles-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 10091
        },
        {
          "path": "plugins/dotfiles-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/SKILL.md",
          "type": "blob",
          "size": 5311
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/references/configuration.md",
          "type": "blob",
          "size": 2949
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/references/prompt-patterns.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/references/secret-detection.md",
          "type": "blob",
          "size": 2189
        },
        {
          "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/references/setup.md",
          "type": "blob",
          "size": 3989
        },
        {
          "path": "plugins/gh-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/gh-tools/README.md",
          "type": "blob",
          "size": 8118
        },
        {
          "path": "plugins/gh-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/commands/hooks.md",
          "type": "blob",
          "size": 2663
        },
        {
          "path": "plugins/gh-tools/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/hooks/gh-issue-body-file-guard.mjs",
          "type": "blob",
          "size": 1761
        },
        {
          "path": "plugins/gh-tools/hooks/hooks.json",
          "type": "blob",
          "size": 652
        },
        {
          "path": "plugins/gh-tools/hooks/webfetch-github-guard.sh",
          "type": "blob",
          "size": 2388
        },
        {
          "path": "plugins/gh-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 7704
        },
        {
          "path": "plugins/gh-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/skills/pr-gfm-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-tools/skills/pr-gfm-validator/SKILL.md",
          "type": "blob",
          "size": 6020
        },
        {
          "path": "plugins/git-account-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-account-validator/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/git-account-validator/README.md",
          "type": "blob",
          "size": 6095
        },
        {
          "path": "plugins/git-account-validator/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-account-validator/commands/hooks.md",
          "type": "blob",
          "size": 1403
        },
        {
          "path": "plugins/git-account-validator/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-account-validator/hooks/hooks.json",
          "type": "blob",
          "size": 124
        },
        {
          "path": "plugins/git-account-validator/hooks/validate-gh-isolation.sh",
          "type": "blob",
          "size": 10471
        },
        {
          "path": "plugins/git-account-validator/hooks/validate-git-push.sh",
          "type": "blob",
          "size": 16268
        },
        {
          "path": "plugins/git-account-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-account-validator/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 4242
        },
        {
          "path": "plugins/git-town-workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-town-workflow/README.md",
          "type": "blob",
          "size": 4147
        },
        {
          "path": "plugins/git-town-workflow/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-town-workflow/commands/contribute.md",
          "type": "blob",
          "size": 11296
        },
        {
          "path": "plugins/git-town-workflow/commands/fork.md",
          "type": "blob",
          "size": 12247
        },
        {
          "path": "plugins/git-town-workflow/commands/hooks.md",
          "type": "blob",
          "size": 7178
        },
        {
          "path": "plugins/git-town-workflow/commands/setup.md",
          "type": "blob",
          "size": 5720
        },
        {
          "path": "plugins/git-town-workflow/plugin.json",
          "type": "blob",
          "size": 482
        },
        {
          "path": "plugins/git-town-workflow/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-town-workflow/references/cheatsheet.md",
          "type": "blob",
          "size": 4924
        },
        {
          "path": "plugins/iterm2-layout-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/iterm2-layout-config/README.md",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "plugins/iterm2-layout-config/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/iterm2-layout-config/skills/iterm2-layout",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/iterm2-layout-config/skills/iterm2-layout/SKILL.md",
          "type": "blob",
          "size": 5672
        },
        {
          "path": "plugins/itp-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/itp-hooks/README.md",
          "type": "blob",
          "size": 4083
        },
        {
          "path": "plugins/itp-hooks/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/commands/setup.md",
          "type": "blob",
          "size": 3234
        },
        {
          "path": "plugins/itp-hooks/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/hooks/fake-data-patterns.mjs",
          "type": "blob",
          "size": 6893
        },
        {
          "path": "plugins/itp-hooks/hooks/hooks.json",
          "type": "blob",
          "size": 1360
        },
        {
          "path": "plugins/itp-hooks/hooks/posttooluse-reminder.sh",
          "type": "blob",
          "size": 7868
        },
        {
          "path": "plugins/itp-hooks/hooks/pretooluse-fake-data-guard.mjs",
          "type": "blob",
          "size": 4832
        },
        {
          "path": "plugins/itp-hooks/hooks/pretooluse-guard.sh",
          "type": "blob",
          "size": 2080
        },
        {
          "path": "plugins/itp-hooks/hooks/pretooluse-version-guard.mjs",
          "type": "blob",
          "size": 5485
        },
        {
          "path": "plugins/itp-hooks/hooks/ruff.toml",
          "type": "blob",
          "size": 1224
        },
        {
          "path": "plugins/itp-hooks/hooks/silent-failure-detector.sh",
          "type": "blob",
          "size": 11418
        },
        {
          "path": "plugins/itp-hooks/hooks/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/hooks/tests/fake-data-guard.test.mjs",
          "type": "blob",
          "size": 13630
        },
        {
          "path": "plugins/itp-hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/scripts/install-dependencies.sh",
          "type": "blob",
          "size": 6828
        },
        {
          "path": "plugins/itp-hooks/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/SKILL.md",
          "type": "blob",
          "size": 4220
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references/debugging-guide.md",
          "type": "blob",
          "size": 5056
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references/evolution-log.md",
          "type": "blob",
          "size": 1457
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references/hook-templates.md",
          "type": "blob",
          "size": 4259
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references/lifecycle-reference.md",
          "type": "blob",
          "size": 51552
        },
        {
          "path": "plugins/itp-hooks/skills/hooks-development/references/visibility-patterns.md",
          "type": "blob",
          "size": 4301
        },
        {
          "path": "plugins/itp-hooks/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/js",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/js/empty_catch.js",
          "type": "blob",
          "size": 129
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/js/floating_promise.js",
          "type": "blob",
          "size": 202
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/js/good_code.js",
          "type": "blob",
          "size": 353
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/python/bare_except.py",
          "type": "blob",
          "size": 134
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/python/good_code.py",
          "type": "blob",
          "size": 335
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/python/subprocess_no_check.py",
          "type": "blob",
          "size": 238
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/shell",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/shell/good_code.sh",
          "type": "blob",
          "size": 265
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/shell/sc2155_masked_return.sh",
          "type": "blob",
          "size": 202
        },
        {
          "path": "plugins/itp-hooks/tests/fixtures/shell/sc2164_cd_no_check.sh",
          "type": "blob",
          "size": 200
        },
        {
          "path": "plugins/itp-hooks/tests/test_silent_failure_detector.bats",
          "type": "blob",
          "size": 10058
        },
        {
          "path": "plugins/itp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/CHANGELOG.md",
          "type": "blob",
          "size": 6336
        },
        {
          "path": "plugins/itp/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/itp/README.md",
          "type": "blob",
          "size": 21872
        },
        {
          "path": "plugins/itp/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/commands/go.md",
          "type": "blob",
          "size": 30334
        },
        {
          "path": "plugins/itp/commands/hooks.md",
          "type": "blob",
          "size": 1794
        },
        {
          "path": "plugins/itp/commands/release.md",
          "type": "blob",
          "size": 4840
        },
        {
          "path": "plugins/itp/commands/setup.md",
          "type": "blob",
          "size": 9700
        },
        {
          "path": "plugins/itp/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/scripts/install-dependencies.sh",
          "type": "blob",
          "size": 12420
        },
        {
          "path": "plugins/itp/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 10060
        },
        {
          "path": "plugins/itp/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/adr-code-traceability",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/adr-code-traceability/SKILL.md",
          "type": "blob",
          "size": 2761
        },
        {
          "path": "plugins/itp/skills/adr-code-traceability/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/adr-code-traceability/references/language-patterns.md",
          "type": "blob",
          "size": 2696
        },
        {
          "path": "plugins/itp/skills/adr-code-traceability/references/placement-guidelines.md",
          "type": "blob",
          "size": 3374
        },
        {
          "path": "plugins/itp/skills/adr-graph-easy-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/adr-graph-easy-architect/SKILL.md",
          "type": "blob",
          "size": 19892
        },
        {
          "path": "plugins/itp/skills/adr-graph-easy-architect/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/adr-graph-easy-architect/references/diagram-examples.md",
          "type": "blob",
          "size": 3050
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/SKILL.md",
          "type": "blob",
          "size": 3451
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/assets/semgrep-hardcode-rules.yaml",
          "type": "blob",
          "size": 4713
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/references/output-schema.md",
          "type": "blob",
          "size": 3943
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/references/tool-comparison.md",
          "type": "blob",
          "size": 3931
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/references/troubleshooting.md",
          "type": "blob",
          "size": 3857
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts/audit_hardcodes.py",
          "type": "blob",
          "size": 14894
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts/run_gitleaks.py",
          "type": "blob",
          "size": 3839
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts/run_jscpd.py",
          "type": "blob",
          "size": 4322
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts/run_ruff_plr.py",
          "type": "blob",
          "size": 2143
        },
        {
          "path": "plugins/itp/skills/code-hardcode-audit/scripts/run_semgrep.py",
          "type": "blob",
          "size": 2702
        },
        {
          "path": "plugins/itp/skills/graph-easy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/graph-easy/SKILL.md",
          "type": "blob",
          "size": 19425
        },
        {
          "path": "plugins/itp/skills/graph-easy/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/graph-easy/scripts/graph-easy",
          "type": "blob",
          "size": 418
        },
        {
          "path": "plugins/itp/skills/impl-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/impl-standards/SKILL.md",
          "type": "blob",
          "size": 3876
        },
        {
          "path": "plugins/itp/skills/impl-standards/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/impl-standards/references/constants-management.md",
          "type": "blob",
          "size": 3498
        },
        {
          "path": "plugins/itp/skills/impl-standards/references/error-handling.md",
          "type": "blob",
          "size": 3116
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/SKILL.md",
          "type": "blob",
          "size": 4518
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/references/adr-template.md",
          "type": "blob",
          "size": 6925
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/references/claude-code-ephemeral-context.md",
          "type": "blob",
          "size": 4448
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/references/perspectives-taxonomy.md",
          "type": "blob",
          "size": 3950
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/references/workflow-steps.md",
          "type": "blob",
          "size": 7773
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/implement-plan-preflight/scripts/preflight_validator.py",
          "type": "blob",
          "size": 8415
        },
        {
          "path": "plugins/itp/skills/mise-configuration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/mise-configuration/SKILL.md",
          "type": "blob",
          "size": 11440
        },
        {
          "path": "plugins/itp/skills/mise-configuration/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/mise-configuration/references/github-tokens.md",
          "type": "blob",
          "size": 5047
        },
        {
          "path": "plugins/itp/skills/mise-configuration/references/patterns.md",
          "type": "blob",
          "size": 16250
        },
        {
          "path": "plugins/itp/skills/mise-tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/mise-tasks/SKILL.md",
          "type": "blob",
          "size": 12096
        },
        {
          "path": "plugins/itp/skills/mise-tasks/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/mise-tasks/references/advanced.md",
          "type": "blob",
          "size": 7018
        },
        {
          "path": "plugins/itp/skills/mise-tasks/references/arguments.md",
          "type": "blob",
          "size": 6248
        },
        {
          "path": "plugins/itp/skills/mise-tasks/references/patterns.md",
          "type": "blob",
          "size": 8527
        },
        {
          "path": "plugins/itp/skills/pypi-doppler",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/pypi-doppler/SKILL.md",
          "type": "blob",
          "size": 14524
        },
        {
          "path": "plugins/itp/skills/pypi-doppler/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/pypi-doppler/scripts/publish-to-pypi.sh",
          "type": "blob",
          "size": 11140
        },
        {
          "path": "plugins/itp/skills/semantic-release",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/SKILL.md",
          "type": "blob",
          "size": 13894
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/doppler.yaml",
          "type": "blob",
          "size": 1170
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/github-workflow.yml",
          "type": "blob",
          "size": 923
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/package.json",
          "type": "blob",
          "size": 608
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/releaserc-pypi-doppler.json",
          "type": "blob",
          "size": 3388
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/releaserc.yml",
          "type": "blob",
          "size": 807
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/shareable-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/shareable-config/README.md",
          "type": "blob",
          "size": 873
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/shareable-config/index.js",
          "type": "blob",
          "size": 1381
        },
        {
          "path": "plugins/itp/skills/semantic-release/assets/templates/shareable-config/package.json",
          "type": "blob",
          "size": 680
        },
        {
          "path": "plugins/itp/skills/semantic-release/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/authentication.md",
          "type": "blob",
          "size": 7903
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/doc-release-linking.md",
          "type": "blob",
          "size": 7578
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/evolution-log.md",
          "type": "blob",
          "size": 2473
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/local-release-workflow.md",
          "type": "blob",
          "size": 14518
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/major-confirmation.md",
          "type": "blob",
          "size": 9786
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/monorepo-support.md",
          "type": "blob",
          "size": 863
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/python.md",
          "type": "blob",
          "size": 16735
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/rust.md",
          "type": "blob",
          "size": 5793
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/troubleshooting.md",
          "type": "blob",
          "size": 9562
        },
        {
          "path": "plugins/itp/skills/semantic-release/references/version-alignment.md",
          "type": "blob",
          "size": 5017
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts/create_org_config.sh",
          "type": "blob",
          "size": 3634
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts/generate-doc-notes.mjs",
          "type": "blob",
          "size": 20096
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts/init-project.mjs",
          "type": "blob",
          "size": 20052
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts/init_project.sh",
          "type": "blob",
          "size": 11006
        },
        {
          "path": "plugins/itp/skills/semantic-release/scripts/init_user_config.sh",
          "type": "blob",
          "size": 3837
        },
        {
          "path": "plugins/itp/skills/semantic-release/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/itp/skills/semantic-release/tests/AUDIT-REPORT-2026-01-02-MAJOR-CONFIRMATION.md",
          "type": "blob",
          "size": 7289
        },
        {
          "path": "plugins/link-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/README.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "plugins/link-tools/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/config/lychee.toml",
          "type": "blob",
          "size": 1729
        },
        {
          "path": "plugins/link-tools/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/hooks/hooks.json",
          "type": "blob",
          "size": 305
        },
        {
          "path": "plugins/link-tools/hooks/stop-link-check.py",
          "type": "blob",
          "size": 13856
        },
        {
          "path": "plugins/link-tools/plugin.json",
          "type": "blob",
          "size": 256
        },
        {
          "path": "plugins/link-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/scripts/validate_links.py",
          "type": "blob",
          "size": 7097
        },
        {
          "path": "plugins/link-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/skills/link-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/skills/link-validation/SKILL.md",
          "type": "blob",
          "size": 1961
        },
        {
          "path": "plugins/link-tools/skills/link-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/skills/link-validator/SKILL.md",
          "type": "blob",
          "size": 3023
        },
        {
          "path": "plugins/link-tools/skills/link-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/link-tools/skills/link-validator/references/link-patterns.md",
          "type": "blob",
          "size": 5524
        },
        {
          "path": "plugins/mql5",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/mql5/README.md",
          "type": "blob",
          "size": 1610
        },
        {
          "path": "plugins/mql5/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/article-extractor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/article-extractor/SKILL.md",
          "type": "blob",
          "size": 2296
        },
        {
          "path": "plugins/mql5/skills/article-extractor/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/article-extractor/references/data-sources.md",
          "type": "blob",
          "size": 3004
        },
        {
          "path": "plugins/mql5/skills/article-extractor/references/examples.md",
          "type": "blob",
          "size": 6405
        },
        {
          "path": "plugins/mql5/skills/article-extractor/references/extraction-modes.md",
          "type": "blob",
          "size": 2078
        },
        {
          "path": "plugins/mql5/skills/article-extractor/references/troubleshooting.md",
          "type": "blob",
          "size": 3517
        },
        {
          "path": "plugins/mql5/skills/log-reader",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/log-reader/SKILL.md",
          "type": "blob",
          "size": 4912
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/SKILL.md",
          "type": "blob",
          "size": 2445
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references/buffer-patterns.md",
          "type": "blob",
          "size": 852
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references/complete-template.md",
          "type": "blob",
          "size": 3183
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references/debugging.md",
          "type": "blob",
          "size": 894
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references/display-scale.md",
          "type": "blob",
          "size": 804
        },
        {
          "path": "plugins/mql5/skills/mql5-indicator-patterns/references/recalculation.md",
          "type": "blob",
          "size": 2574
        },
        {
          "path": "plugins/mql5/skills/python-workspace",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/python-workspace/SKILL.md",
          "type": "blob",
          "size": 2734
        },
        {
          "path": "plugins/mql5/skills/python-workspace/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mql5/skills/python-workspace/references/capabilities-detailed.md",
          "type": "blob",
          "size": 8955
        },
        {
          "path": "plugins/mql5/skills/python-workspace/references/troubleshooting-errors.md",
          "type": "blob",
          "size": 7661
        },
        {
          "path": "plugins/mql5/skills/python-workspace/references/validation-metrics.md",
          "type": "blob",
          "size": 1812
        },
        {
          "path": "plugins/mql5/skills/python-workspace/references/workflows-complete.md",
          "type": "blob",
          "size": 2564
        },
        {
          "path": "plugins/notion-api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/README.md",
          "type": "blob",
          "size": 2648
        },
        {
          "path": "plugins/notion-api/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/SKILL.md",
          "type": "blob",
          "size": 9163
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/references/block-types.md",
          "type": "blob",
          "size": 6462
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/references/pagination.md",
          "type": "blob",
          "size": 5039
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/references/property-types.md",
          "type": "blob",
          "size": 5638
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/references/rich-text.md",
          "type": "blob",
          "size": 4136
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/scripts/add_blocks.py",
          "type": "blob",
          "size": 5306
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/scripts/create_page.py",
          "type": "blob",
          "size": 4463
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/scripts/notion_wrapper.py",
          "type": "blob",
          "size": 3464
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/scripts/query_database.py",
          "type": "blob",
          "size": 5775
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests/test_block_builders.py",
          "type": "blob",
          "size": 15540
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests/test_client.py",
          "type": "blob",
          "size": 8643
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests/test_filter_builders.py",
          "type": "blob",
          "size": 13820
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests/test_integration.py",
          "type": "blob",
          "size": 10865
        },
        {
          "path": "plugins/notion-api/skills/notion-sdk/tests/test_property_builders.py",
          "type": "blob",
          "size": 14987
        },
        {
          "path": "plugins/plugin-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/README.md",
          "type": "blob",
          "size": 2793
        },
        {
          "path": "plugins/plugin-dev/bun.lock",
          "type": "blob",
          "size": 3615
        },
        {
          "path": "plugins/plugin-dev/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/commands/create.md",
          "type": "blob",
          "size": 16479
        },
        {
          "path": "plugins/plugin-dev/package.json",
          "type": "blob",
          "size": 644
        },
        {
          "path": "plugins/plugin-dev/plugin.json",
          "type": "blob",
          "size": 254
        },
        {
          "path": "plugins/plugin-dev/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/scripts/fix-bash-blocks.ts",
          "type": "blob",
          "size": 6591
        },
        {
          "path": "plugins/plugin-dev/scripts/lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/scripts/lib/constants.ts",
          "type": "blob",
          "size": 4719
        },
        {
          "path": "plugins/plugin-dev/scripts/lib/markdown.ts",
          "type": "blob",
          "size": 6339
        },
        {
          "path": "plugins/plugin-dev/scripts/lib/output.ts",
          "type": "blob",
          "size": 10621
        },
        {
          "path": "plugins/plugin-dev/scripts/lib/patterns.ts",
          "type": "blob",
          "size": 4754
        },
        {
          "path": "plugins/plugin-dev/scripts/lib/types.ts",
          "type": "blob",
          "size": 5233
        },
        {
          "path": "plugins/plugin-dev/scripts/validate-links.ts",
          "type": "blob",
          "size": 9587
        },
        {
          "path": "plugins/plugin-dev/scripts/validate-skill.ts",
          "type": "blob",
          "size": 22813
        },
        {
          "path": "plugins/plugin-dev/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator/SKILL.md",
          "type": "blob",
          "size": 3613
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator/references/silent-failure-patterns.md",
          "type": "blob",
          "size": 3883
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/plugin-validator/scripts/audit_silent_failures.py",
          "type": "blob",
          "size": 13861
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/SKILL.md",
          "type": "blob",
          "size": 15660
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/SYNC-TRACKING.md",
          "type": "blob",
          "size": 6078
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/advanced-topics.md",
          "type": "blob",
          "size": 3270
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/bash-compatibility.md",
          "type": "blob",
          "size": 5238
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/creation-workflow.md",
          "type": "blob",
          "size": 8344
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/error-message-style.md",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/evolution-log.md",
          "type": "blob",
          "size": 7835
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/path-patterns.md",
          "type": "blob",
          "size": 7105
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/progressive-disclosure.md",
          "type": "blob",
          "size": 5042
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/scripts-reference.md",
          "type": "blob",
          "size": 5626
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/security-practices.md",
          "type": "blob",
          "size": 1788
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/structural-patterns.md",
          "type": "blob",
          "size": 3247
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/token-efficiency.md",
          "type": "blob",
          "size": 981
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/validation-reference.md",
          "type": "blob",
          "size": 6552
        },
        {
          "path": "plugins/plugin-dev/skills/skill-architecture/references/workflow-patterns.md",
          "type": "blob",
          "size": 2432
        },
        {
          "path": "plugins/plugin-dev/tsconfig.json",
          "type": "blob",
          "size": 421
        },
        {
          "path": "plugins/productivity-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/productivity-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/productivity-tools/README.md",
          "type": "blob",
          "size": 436
        },
        {
          "path": "plugins/productivity-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/HOW_TO_USE.md",
          "type": "blob",
          "size": 5400
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/SKILL.md",
          "type": "blob",
          "size": 27959
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/command_generator.py",
          "type": "blob",
          "size": 12626
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/expected_output.json",
          "type": "blob",
          "size": 1130
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/presets.json",
          "type": "blob",
          "size": 25124
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/sample_input.json",
          "type": "blob",
          "size": 276
        },
        {
          "path": "plugins/productivity-tools/skills/slash-command-factory/validator.py",
          "type": "blob",
          "size": 11019
        },
        {
          "path": "plugins/quality-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/quality-tools/README.md",
          "type": "blob",
          "size": 1291
        },
        {
          "path": "plugins/quality-tools/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/SKILL.md",
          "type": "blob",
          "size": 13688
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/anti-patterns-and-fixes.md",
          "type": "blob",
          "size": 5647
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/audit-and-diagnostics.md",
          "type": "blob",
          "size": 6684
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/compression-codec-selection.md",
          "type": "blob",
          "size": 7110
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/idiomatic-architecture.md",
          "type": "blob",
          "size": 8421
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/schema-design-workflow.md",
          "type": "blob",
          "size": 5424
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/references/schema-documentation.md",
          "type": "blob",
          "size": 6717
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/clickhouse-architect/scripts/schema-audit.sql",
          "type": "blob",
          "size": 6007
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/SKILL.md",
          "type": "blob",
          "size": 3187
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/clone-rules.yaml",
          "type": "blob",
          "size": 6248
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/references/complete-workflow.md",
          "type": "blob",
          "size": 2459
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/references/detection-commands.md",
          "type": "blob",
          "size": 1147
        },
        {
          "path": "plugins/quality-tools/skills/code-clone-assistant/references/refactoring-strategies.md",
          "type": "blob",
          "size": 1678
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/SKILL.md",
          "type": "blob",
          "size": 10343
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/references/agent_test_template.py",
          "type": "blob",
          "size": 4584
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/references/bug_severity_classification.md",
          "type": "blob",
          "size": 5737
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/references/example_validation_findings.md",
          "type": "blob",
          "size": 4371
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/SKILL.md",
          "type": "blob",
          "size": 11451
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/references/impact_quantification_guide.md",
          "type": "blob",
          "size": 8775
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/references/integration_report_template.md",
          "type": "blob",
          "size": 7168
        },
        {
          "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/references/profiling_template.py",
          "type": "blob",
          "size": 5632
        },
        {
          "path": "plugins/quality-tools/skills/schema-e2e-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/quality-tools/skills/schema-e2e-validation/SKILL.md",
          "type": "blob",
          "size": 6643
        },
        {
          "path": "plugins/ralph",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/GETTING-STARTED.md",
          "type": "blob",
          "size": 8932
        },
        {
          "path": "plugins/ralph/MENTAL-MODEL.md",
          "type": "blob",
          "size": 70395
        },
        {
          "path": "plugins/ralph/README.md",
          "type": "blob",
          "size": 27855
        },
        {
          "path": "plugins/ralph/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/commands/audit-now.md",
          "type": "blob",
          "size": 4247
        },
        {
          "path": "plugins/ralph/commands/config.md",
          "type": "blob",
          "size": 9185
        },
        {
          "path": "plugins/ralph/commands/encourage.md",
          "type": "blob",
          "size": 8772
        },
        {
          "path": "plugins/ralph/commands/forbid.md",
          "type": "blob",
          "size": 9304
        },
        {
          "path": "plugins/ralph/commands/hooks.md",
          "type": "blob",
          "size": 9929
        },
        {
          "path": "plugins/ralph/commands/start.md",
          "type": "blob",
          "size": 30581
        },
        {
          "path": "plugins/ralph/commands/status.md",
          "type": "blob",
          "size": 6952
        },
        {
          "path": "plugins/ralph/commands/stop.md",
          "type": "blob",
          "size": 4900
        },
        {
          "path": "plugins/ralph/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/docs/ALPHA-FORGE-VALIDATION-PROMPT.md",
          "type": "blob",
          "size": 14986
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-ARCHITECTURE.md",
          "type": "blob",
          "size": 30539
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-DESIGN-COMPLETE.md",
          "type": "blob",
          "size": 12822
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-IMPLEMENTATION.md",
          "type": "blob",
          "size": 32617
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-INDEX.md",
          "type": "blob",
          "size": 13257
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-INTEGRATION-DESIGN.md",
          "type": "blob",
          "size": 18748
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-PROMPTS.md",
          "type": "blob",
          "size": 17229
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-AGENT-SUMMARY.md",
          "type": "blob",
          "size": 14224
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-EXAMPLES.md",
          "type": "blob",
          "size": 16314
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-GUIDE.md",
          "type": "blob",
          "size": 8401
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-INDEX.md",
          "type": "blob",
          "size": 13701
        },
        {
          "path": "plugins/ralph/docs/EXPLORE-REFERENCE.md",
          "type": "blob",
          "size": 7760
        },
        {
          "path": "plugins/ralph/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/hooks/adapters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/hooks/adapters/__init__.py",
          "type": "blob",
          "size": 767
        },
        {
          "path": "plugins/ralph/hooks/adapters/alpha_forge.py",
          "type": "blob",
          "size": 15097
        },
        {
          "path": "plugins/ralph/hooks/alpha_forge_filter.py",
          "type": "blob",
          "size": 12471
        },
        {
          "path": "plugins/ralph/hooks/archive-plan.sh",
          "type": "blob",
          "size": 3967
        },
        {
          "path": "plugins/ralph/hooks/completion.py",
          "type": "blob",
          "size": 13364
        },
        {
          "path": "plugins/ralph/hooks/core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/hooks/core/__init__.py",
          "type": "blob",
          "size": 521
        },
        {
          "path": "plugins/ralph/hooks/core/config_schema.py",
          "type": "blob",
          "size": 16872
        },
        {
          "path": "plugins/ralph/hooks/core/constants.py",
          "type": "blob",
          "size": 6419
        },
        {
          "path": "plugins/ralph/hooks/core/path_hash.py",
          "type": "blob",
          "size": 9706
        },
        {
          "path": "plugins/ralph/hooks/core/project_detection.py",
          "type": "blob",
          "size": 4635
        },
        {
          "path": "plugins/ralph/hooks/core/protocols.py",
          "type": "blob",
          "size": 5619
        },
        {
          "path": "plugins/ralph/hooks/core/registry.py",
          "type": "blob",
          "size": 5091
        },
        {
          "path": "plugins/ralph/hooks/discovery.py",
          "type": "blob",
          "size": 21383
        },
        {
          "path": "plugins/ralph/hooks/hooks.json",
          "type": "blob",
          "size": 1218
        },
        {
          "path": "plugins/ralph/hooks/loop-until-done-wrapper.sh",
          "type": "blob",
          "size": 2673
        },
        {
          "path": "plugins/ralph/hooks/loop-until-done.py",
          "type": "blob",
          "size": 40504
        },
        {
          "path": "plugins/ralph/hooks/math_detector.py",
          "type": "blob",
          "size": 3654
        },
        {
          "path": "plugins/ralph/hooks/math_guards.py",
          "type": "blob",
          "size": 7611
        },
        {
          "path": "plugins/ralph/hooks/observability.py",
          "type": "blob",
          "size": 2362
        },
        {
          "path": "plugins/ralph/hooks/pretooluse-loop-guard-wrapper.sh",
          "type": "blob",
          "size": 3116
        },
        {
          "path": "plugins/ralph/hooks/pretooluse-loop-guard.py",
          "type": "blob",
          "size": 5067
        },
        {
          "path": "plugins/ralph/hooks/ralph_discovery.py",
          "type": "blob",
          "size": 11627
        },
        {
          "path": "plugins/ralph/hooks/ralph_evolution.py",
          "type": "blob",
          "size": 6489
        },
        {
          "path": "plugins/ralph/hooks/ralph_history.py",
          "type": "blob",
          "size": 5606
        },
        {
          "path": "plugins/ralph/hooks/ralph_knowledge.py",
          "type": "blob",
          "size": 7304
        },
        {
          "path": "plugins/ralph/hooks/ralph_meta.py",
          "type": "blob",
          "size": 7522
        },
        {
          "path": "plugins/ralph/hooks/ralph_web_discovery.py",
          "type": "blob",
          "size": 10482
        },
        {
          "path": "plugins/ralph/hooks/roadmap_parser.py",
          "type": "blob",
          "size": 9107
        },
        {
          "path": "plugins/ralph/hooks/ruff.toml",
          "type": "blob",
          "size": 153
        },
        {
          "path": "plugins/ralph/hooks/template_loader.py",
          "type": "blob",
          "size": 12604
        },
        {
          "path": "plugins/ralph/hooks/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/hooks/templates/ralph-unified.md",
          "type": "blob",
          "size": 10500
        },
        {
          "path": "plugins/ralph/hooks/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/hooks/tests/poc-task.md",
          "type": "blob",
          "size": 2216
        },
        {
          "path": "plugins/ralph/hooks/tests/run_all_tests.py",
          "type": "blob",
          "size": 3167
        },
        {
          "path": "plugins/ralph/hooks/tests/test_adapters.py",
          "type": "blob",
          "size": 13927
        },
        {
          "path": "plugins/ralph/hooks/tests/test_completion.py",
          "type": "blob",
          "size": 7949
        },
        {
          "path": "plugins/ralph/hooks/tests/test_hook_emission.py",
          "type": "blob",
          "size": 25515
        },
        {
          "path": "plugins/ralph/hooks/tests/test_integration.py",
          "type": "blob",
          "size": 12576
        },
        {
          "path": "plugins/ralph/hooks/tests/test_math_guards.py",
          "type": "blob",
          "size": 8877
        },
        {
          "path": "plugins/ralph/hooks/tests/test_todo_sync.py",
          "type": "blob",
          "size": 5605
        },
        {
          "path": "plugins/ralph/hooks/tests/test_utils.py",
          "type": "blob",
          "size": 10931
        },
        {
          "path": "plugins/ralph/hooks/todo_sync.py",
          "type": "blob",
          "size": 5309
        },
        {
          "path": "plugins/ralph/hooks/utils.py",
          "type": "blob",
          "size": 9053
        },
        {
          "path": "plugins/ralph/hooks/value_metrics.py",
          "type": "blob",
          "size": 10404
        },
        {
          "path": "plugins/ralph/hooks/work_policy.py",
          "type": "blob",
          "size": 7569
        },
        {
          "path": "plugins/ralph/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/scripts/constraint-scanner.py",
          "type": "blob",
          "size": 19430
        },
        {
          "path": "plugins/ralph/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 15042
        },
        {
          "path": "plugins/ralph/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/skills/constraint-discovery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/skills/constraint-discovery/SKILL.md",
          "type": "blob",
          "size": 8186
        },
        {
          "path": "plugins/ralph/skills/session-guidance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ralph/skills/session-guidance/SKILL.md",
          "type": "blob",
          "size": 16308
        },
        {
          "path": "plugins/statusline-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "plugins/statusline-tools/README.md",
          "type": "blob",
          "size": 7110
        },
        {
          "path": "plugins/statusline-tools/bun.lock",
          "type": "blob",
          "size": 13978
        },
        {
          "path": "plugins/statusline-tools/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/commands/hooks.md",
          "type": "blob",
          "size": 2505
        },
        {
          "path": "plugins/statusline-tools/commands/ignore.md",
          "type": "blob",
          "size": 3223
        },
        {
          "path": "plugins/statusline-tools/commands/setup.md",
          "type": "blob",
          "size": 2298
        },
        {
          "path": "plugins/statusline-tools/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/docs/LINK-VALIDATION-TEST-PROMPT.md",
          "type": "blob",
          "size": 3433
        },
        {
          "path": "plugins/statusline-tools/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/hooks/hooks.json",
          "type": "blob",
          "size": 302
        },
        {
          "path": "plugins/statusline-tools/hooks/lychee-stop-hook.sh",
          "type": "blob",
          "size": 8077
        },
        {
          "path": "plugins/statusline-tools/lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/lib/chain-formatter.ts",
          "type": "blob",
          "size": 696
        },
        {
          "path": "plugins/statusline-tools/package.json",
          "type": "blob",
          "size": 632
        },
        {
          "path": "plugins/statusline-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/scripts/lint-relative-paths",
          "type": "blob",
          "size": 2431
        },
        {
          "path": "plugins/statusline-tools/scripts/lint-relative-paths.ts",
          "type": "blob",
          "size": 11090
        },
        {
          "path": "plugins/statusline-tools/scripts/manage-hooks.sh",
          "type": "blob",
          "size": 9305
        },
        {
          "path": "plugins/statusline-tools/scripts/manage-ignore.sh",
          "type": "blob",
          "size": 4827
        },
        {
          "path": "plugins/statusline-tools/scripts/manage-statusline.sh",
          "type": "blob",
          "size": 11002
        },
        {
          "path": "plugins/statusline-tools/scripts/session-chain.ts",
          "type": "blob",
          "size": 669
        },
        {
          "path": "plugins/statusline-tools/statusline",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/statusline/custom-statusline.sh",
          "type": "blob",
          "size": 12898
        },
        {
          "path": "plugins/statusline-tools/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/repo_with_broken_links",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/repo_with_broken_links/README.md",
          "type": "blob",
          "size": 57
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/repo_with_path_violations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/repo_with_path_violations/README.md",
          "type": "blob",
          "size": 61
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/sample_input.json",
          "type": "blob",
          "size": 171
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/sample_repo",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/tests/fixtures/sample_repo/README.md",
          "type": "blob",
          "size": 14
        },
        {
          "path": "plugins/statusline-tools/tests/test_lint_relative.bats",
          "type": "blob",
          "size": 1953
        },
        {
          "path": "plugins/statusline-tools/tests/test_statusline.bats",
          "type": "blob",
          "size": 3484
        },
        {
          "path": "plugins/statusline-tools/tests/test_stop_hook.bats",
          "type": "blob",
          "size": 3180
        },
        {
          "path": "plugins/statusline-tools/tsconfig.json",
          "type": "blob",
          "size": 434
        },
        {
          "path": "plugins/statusline-tools/types",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/statusline-tools/types/session.d.ts",
          "type": "blob",
          "size": 177
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/hooks.schema.json",
          "type": "blob",
          "size": 3421
        },
        {
          "path": "scripts/install-hooks.sh",
          "type": "blob",
          "size": 1628
        },
        {
          "path": "scripts/marketplace.schema.json",
          "type": "blob",
          "size": 3203
        },
        {
          "path": "scripts/migrate-rssi-to-ralph.sh",
          "type": "blob",
          "size": 1849
        },
        {
          "path": "scripts/release-preflight.sh",
          "type": "blob",
          "size": 2049
        },
        {
          "path": "scripts/sync-hooks-to-settings.sh",
          "type": "blob",
          "size": 4202
        },
        {
          "path": "scripts/sync-versions.mjs",
          "type": "blob",
          "size": 6164
        },
        {
          "path": "scripts/validate-plugins.mjs",
          "type": "blob",
          "size": 45721
        }
      ],
      "marketplace": {
        "name": "cc-skills",
        "version": "9.24.6",
        "description": "Claude Code Skills Marketplace",
        "owner_info": {
          "name": "Terry Li",
          "url": "https://github.com/terrylica"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "plugin-dev",
            "description": "Plugin and skill development: structure validation, silent failure auditing, skill architecture meta-skill with TodoWrite templates",
            "source": "./plugins/plugin-dev/",
            "category": "development",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install plugin-dev@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/create",
                "description": "Create a new plugin for Claude Code marketplace with validation, ADR, and release automation",
                "path": "plugins/plugin-dev/commands/create.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(node:*), Bash(git:*), Bash(npm:*), Bash(ls:*), Bash(mkdir:*), Grep, Glob, TodoWrite, TodoRead, AskUserQuestion, Skill, Task",
                  "argument-hint": "[plugin-name] (optional - will prompt if not provided)",
                  "description": "Create a new plugin for Claude Code marketplace with validation, ADR, and release automation"
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n#  Create Plugin  STOP AND READ\n\n**DO NOT ACT ON ASSUMPTIONS. Read this file first.**\n\nThis is a structured workflow command for creating a new plugin in a Claude Code marketplace.\n\nYour FIRST and ONLY action right now: **Execute the TodoWrite below.**\n\n##  MANDATORY FIRST ACTION: TodoWrite Initialization\n\n**YOUR FIRST ACTION MUST BE the TodoWrite call below.**\n\nDO NOT:\n\n-  Create any directories before TodoWrite\n-  Read marketplace.json before TodoWrite\n-  Ask questions before TodoWrite\n-  Jump to any phase without completing Step 0\n\n### Step 0.1: Detect Marketplace Root\n\nBefore executing TodoWrite, verify you're in a marketplace directory:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Check for marketplace.json in cwd\nif [ -f \".claude-plugin/marketplace.json\" ]; then\n  echo \" Marketplace detected: $(jq -r .name .claude-plugin/marketplace.json)\"\nelse\n  echo \" Not a marketplace directory. Run from a marketplace root.\"\n  exit 1\nfi\nPREFLIGHT_EOF\n```\n\n### Step 0.2: Execute MANDATORY TodoWrite\n\n**Execute TodoWrite NOW with this template:**\n\n```\nTodoWrite with todos:\n\n- \"[Plugin] Phase 0: Detect marketplace root\" | in_progress\n- \"[Plugin] Phase 0: Interactive prompts (name, category, components)\" | pending\n- \"[Plugin] Phase 0: Confirm plugin doesn't exist\" | pending\n- \"[Plugin] Phase 1: Skill  plugin-structure (scaffold)\" | pending\n- \"[Plugin] Phase 1: Create plugin directory + plugin.json\" | pending\n- \"[Plugin] Phase 1: Skill  implement-plan-preflight (ADR)\" | pending\n- \"[Plugin] Phase 2: Skill  skill-architecture (if has-skills)\" | pending\n- \"[Plugin] Phase 2: Skill  hook-development (if has-hooks)\" | pending\n- \"[Plugin] Phase 2: Skill  command-development (if has-commands)\" | pending\n- \"[Plugin] Phase 2: Skill  agent-development (if has-agents)\" | pending\n- \"[Plugin] Phase 2: Agent  skill-reviewer (if skills created)\" | pending\n- \"[Plugin] Phase 3: Add to marketplace.json\" | pending\n- \"[Plugin] Phase 3: Run validate-plugins.mjs\" | pending\n- \"[Plugin] Phase 3: Skill  code-hardcode-audit\" | pending\n- \"[Plugin] Phase 3: Agent  plugin-validator\" | pending\n- \"[Plugin] Phase 4: Git commit (conventional format)\" | pending\n- \"[Plugin] Phase 4: Push to remote\" | pending\n- \"[Plugin] Phase 4: Skill  semantic-release\" | pending\n```\n\n**After TodoWrite completes, proceed to Phase 0 section below.**\n\n---\n\n## Quick Reference\n\n### Skills Invoked (Optimized Sequence)\n\n| Order | Skill                    | Phase | Purpose                           | Invocation                              |\n| ----- | ------------------------ | ----- | --------------------------------- | --------------------------------------- |\n| 1     | plugin-structure         | 1     | Directory & manifest              | `Skill(plugin-dev:plugin-structure)`    |\n| 2     | implement-plan-preflight | 1     | ADR + Design Spec + Diagrams      | `Skill(itp:implement-plan-preflight)`   |\n| 3     | skill-architecture       | 2     | Create skills (if has-skills)     | `Skill(plugin-dev:skill-architecture)`  |\n| 4     | hook-development         | 2     | Create hooks (if has-hooks)       | `Skill(plugin-dev:hook-development)`    |\n| 5     | command-development      | 2     | Create commands (if has-commands) | `Skill(plugin-dev:command-development)` |\n| 6     | agent-development        | 2     | Create agents (if has-agents)     | `Skill(plugin-dev:agent-development)`   |\n| 7     | code-hardcode-audit      | 3     | Quality audit                     | `Skill(itp:code-hardcode-audit)`        |\n| 8     | plugin-validator         | 3     | Silent failure audit              | `Skill(plugin-dev:plugin-validator)`    |\n| 9     | semantic-release         | 4     | Version & publish                 | `Skill(itp:semantic-release)`           |\n\n### Skills EXCLUDED (Redundant)\n\n| Skill                        | Reason Excluded                                        |\n| ---------------------------- | ------------------------------------------------------ |\n| plugin-dev:skill-development | Use skill-architecture instead (3x more comprehensive) |\n| plugin-dev:plugin-settings   | Merged into hook-development                           |\n| itp:adr-graph-easy-architect | Invoked BY implement-plan-preflight (not separately)   |\n\n### Agents Spawned\n\n| Phase | Agent            | Purpose                  | Invocation                          |\n| ----- | ---------------- | ------------------------ | ----------------------------------- |\n| 2     | skill-reviewer   | Review skill quality     | `Task(plugin-dev:skill-reviewer)`   |\n| 3     | plugin-validator | Validate final structure | `Task(plugin-dev:plugin-validator)` |\n\n### File Locations\n\n| Artifact         | Path                                    | Notes                      |\n| ---------------- | --------------------------------------- | -------------------------- |\n| Plugin Directory | `plugins/{name}/`                       | Main plugin folder         |\n| Plugin Manifest  | `plugins/{name}/plugin.json`            | Required manifest          |\n| Plugin README    | `plugins/{name}/README.md`              | Documentation              |\n| Marketplace JSON | `.claude-plugin/marketplace.json`       | Must add plugin entry      |\n| ADR              | `docs/adr/YYYY-MM-DD-{name}.md`         | Created by preflight skill |\n| Design Spec      | `docs/design/YYYY-MM-DD-{name}/spec.md` | Created by preflight skill |\n\n---\n\n## Phase 0: Discovery & Validation\n\n### 0.1 Verify Marketplace Root\n\nFirst, confirm we're in a marketplace directory:\n\n```bash\n/usr/bin/env bash << 'PLUGIN_ADD_SCRIPT_EOF'\n# Must have .claude-plugin/marketplace.json\nls -la .claude-plugin/marketplace.json\n\n# Extract marketplace info\nMARKETPLACE_NAME=$(jq -r .name .claude-plugin/marketplace.json)\nMARKETPLACE_VERSION=$(jq -r .version .claude-plugin/marketplace.json)\necho \"Marketplace: $MARKETPLACE_NAME v$MARKETPLACE_VERSION\"\nPLUGIN_ADD_SCRIPT_EOF\n```\n\n### 0.2 Interactive Prompts\n\nUse AskUserQuestion to gather plugin details:\n\n**Q1: Plugin Name** (if not provided as argument)\n\n```\nAskUserQuestion with questions:\n- question: \"What should this plugin be called? Use kebab-case (e.g., 'my-plugin-name')\"\n  header: \"Plugin Name\"\n  options:\n    - label: \"Custom name\"\n      description: \"Enter a kebab-case plugin name\"\n  multiSelect: false\n```\n\n**Q2: Category**\n\n```\nAskUserQuestion with questions:\n- question: \"What category does this plugin belong to?\"\n  header: \"Category\"\n  options:\n    - label: \"development (Recommended)\"\n      description: \"Tools for developers\"\n    - label: \"productivity\"\n      description: \"Workflow automation\"\n    - label: \"devops\"\n      description: \"Infrastructure & operations\"\n    - label: \"documents\"\n      description: \"Documentation tools\"\n  multiSelect: false\n```\n\n**Q3: Components**\n\n```\nAskUserQuestion with questions:\n- question: \"What components will this plugin include?\"\n  header: \"Components\"\n  options:\n    - label: \"Skills\"\n      description: \"Domain knowledge & capabilities (SKILL.md files)\"\n    - label: \"Hooks\"\n      description: \"Event-driven automation (hooks.json)\"\n    - label: \"Commands\"\n      description: \"Slash commands (commands/*.md)\"\n    - label: \"Agents\"\n      description: \"Autonomous subagents (agents/*.md)\"\n  multiSelect: true\n```\n\n**Store responses:**\n\n```bash\n/usr/bin/env bash << 'PLUGIN_ADD_SCRIPT_EOF_2'\nPLUGIN_NAME=\"${ARGUMENTS:-<from-q1>}\"\nPLUGIN_CATEGORY=\"<from-q2>\"\nHAS_SKILLS=<true|false>\nHAS_HOOKS=<true|false>\nHAS_COMMANDS=<true|false>\nHAS_AGENTS=<true|false>\nPLUGIN_ADD_SCRIPT_EOF_2\n```\n\n### 0.3 Confirm Plugin Doesn't Exist\n\n```bash\n# Check if plugin directory already exists\nif [ -d \"plugins/$PLUGIN_NAME\" ]; then\n  echo \" Plugin already exists: plugins/$PLUGIN_NAME\"\n  exit 1\nfi\n\n# Check if already in marketplace.json\nif jq -e \".plugins[] | select(.name == \\\"$PLUGIN_NAME\\\")\" .claude-plugin/marketplace.json > /dev/null 2>&1; then\n  echo \" Plugin already registered in marketplace.json: $PLUGIN_NAME\"\n  exit 1\nfi\n\necho \" Plugin name '$PLUGIN_NAME' is available\"\n```\n\n### Phase 0 Gate\n\n**STOP. Verify before proceeding to Phase 1:**\n\n- [ ] Marketplace root detected (`.claude-plugin/marketplace.json` exists)\n- [ ] Plugin name collected (kebab-case, no spaces)\n- [ ] Category selected\n- [ ] Components selected (skills/hooks/commands/agents)\n- [ ] Plugin directory does NOT exist\n- [ ] Plugin NOT in marketplace.json\n\n---\n\n## Phase 1: Scaffold Plugin\n\n### 1.1 Invoke plugin-structure Skill\n\n**MANDATORY Skill tool call: `plugin-dev:plugin-structure`**  activate NOW.\n\nThis skill provides:\n\n- Directory structure patterns\n- plugin.json template\n- README.md template\n\n### 1.2 Create Plugin Directory\n\n```bash\n# Create plugin directory structure\nmkdir -p plugins/$PLUGIN_NAME\n\n# If has-skills:\nmkdir -p plugins/$PLUGIN_NAME/skills\n\n# If has-hooks:\nmkdir -p plugins/$PLUGIN_NAME/hooks\n\n# If has-commands:\nmkdir -p plugins/$PLUGIN_NAME/commands\n\n# If has-agents:\nmkdir -p plugins/$PLUGIN_NAME/agents\n```\n\n### 1.3 Generate plugin.json\n\nGet version from marketplace for consistency:\n\n```bash\n/usr/bin/env bash << 'PLUGIN_ADD_SCRIPT_EOF_3'\nMARKETPLACE_VERSION=$(jq -r .version .claude-plugin/marketplace.json)\nPLUGIN_ADD_SCRIPT_EOF_3\n```\n\nCreate `plugins/$PLUGIN_NAME/plugin.json`:\n\n```json\n{\n  \"name\": \"$PLUGIN_NAME\",\n  \"version\": \"$MARKETPLACE_VERSION\",\n  \"description\": \"TODO: Add description\",\n  \"author\": {\n    \"name\": \"Terry Li\",\n    \"url\": \"https://github.com/terrylica\"\n  }\n}\n```\n\n### 1.4 Create ADR and Design Spec\n\n**MANDATORY Skill tool call: `itp:implement-plan-preflight`**  activate NOW.\n\nThis skill:\n\n- Creates ADR at `docs/adr/YYYY-MM-DD-$PLUGIN_NAME.md`\n- Creates Design Spec at `docs/design/YYYY-MM-DD-$PLUGIN_NAME/spec.md`\n- Internally invokes `adr-graph-easy-architect` for diagrams\n\n**ADR ID Format:**\n\n```bash\n/usr/bin/env bash << 'PLUGIN_ADD_SCRIPT_EOF_4'\nADR_ID=\"$(date +%Y-%m-%d)-$PLUGIN_NAME\"\nPLUGIN_ADD_SCRIPT_EOF_4\n```\n\n### Phase 1 Gate\n\n**STOP. Verify before proceeding to Phase 2:**\n\n- [ ] Plugin directory exists: `plugins/$PLUGIN_NAME/`\n- [ ] plugin.json created with marketplace version\n- [ ] ADR exists: `docs/adr/$ADR_ID.md`\n- [ ] Design spec exists: `docs/design/$ADR_ID/spec.md`\n- [ ] Both diagrams in ADR (Before/After + Architecture)\n\n---\n\n## Phase 2: Component Creation (Conditional)\n\n**Execute ONLY the skills for components the user selected.**\n\n### 2.1 Skills (if has-skills)\n\n**MANDATORY Skill tool call: `plugin-dev:skill-architecture`**  activate if skills selected.\n\nThis skill (NOT plugin-dev:skill-development) provides:\n\n- 5 TodoWrite templates (A-E)\n- SKILL.md structure\n- References folder patterns\n- Security practices\n\nAfter skill creation, spawn reviewer agent:\n\n**Spawn Agent: `plugin-dev:skill-reviewer`**  validate skill quality.\n\n```\nTask with subagent_type=\"plugin-dev:skill-reviewer\"\nprompt: \"Review the skills created in plugins/$PLUGIN_NAME/skills/ for quality, security, and best practices.\"\n```\n\n### 2.2 Hooks (if has-hooks)\n\n**MANDATORY Skill tool call: `plugin-dev:hook-development`**  activate if hooks selected.\n\nThis skill includes:\n\n- hooks.json structure\n- Event types (PreToolUse, PostToolUse, Stop, etc.)\n- Settings patterns (plugin-settings merged in)\n\n### 2.3 Commands (if has-commands)\n\n**MANDATORY Skill tool call: `plugin-dev:command-development`**  activate if commands selected.\n\nThis skill provides:\n\n- YAML frontmatter fields\n- Argument patterns\n- Dynamic arguments\n\n### 2.4 Agents (if has-agents)\n\n**MANDATORY Skill tool call: `plugin-dev:agent-development`**  activate if agents selected.\n\nThis skill provides:\n\n- Agent frontmatter\n- Triggering conditions\n- Tool restrictions\n\n### Phase 2 Gate\n\n**STOP. Verify before proceeding to Phase 3:**\n\n- [ ] All selected components created\n- [ ] If skills: skill-reviewer agent completed review\n- [ ] Files follow plugin-dev patterns\n\n---\n\n## Phase 3: Registration & Validation\n\n### 3.1 Add to marketplace.json\n\nEdit `.claude-plugin/marketplace.json` to add the new plugin entry:\n\n```json\n{\n  \"name\": \"$PLUGIN_NAME\",\n  \"description\": \"TODO: Add description from ADR\",\n  \"version\": \"$MARKETPLACE_VERSION\",\n  \"source\": \"./plugins/$PLUGIN_NAME/\",\n  \"category\": \"$PLUGIN_CATEGORY\",\n  \"author\": {\n    \"name\": \"Terry Li\",\n    \"url\": \"https://github.com/terrylica\"\n  },\n  \"keywords\": [],\n  \"strict\": false\n}\n```\n\n**If hooks exist**, add the hooks field:\n\n```json\n\"hooks\": \"./plugins/$PLUGIN_NAME/hooks/hooks.json\"\n```\n\n### 3.2 Run Validation Script\n\n```bash\nnode scripts/validate-plugins.mjs\n```\n\nExpected output:\n\n```\n Registered plugins: N+1\n Plugin directories: N+1\n\n All plugins validated successfully!\n```\n\n### 3.3 Quality Audit\n\n**MANDATORY Skill tool call: `itp:code-hardcode-audit`**  activate NOW.\n\nThis skill checks for:\n\n- Hardcoded values\n- Magic numbers\n- Duplicate constants\n- Secrets\n\n### 3.4 Silent Failure Audit\n\n**MANDATORY**: Run silent failure audit on all hook entry points.\n\n```bash\nuv run plugins/plugin-dev/skills/plugin-validator/scripts/audit_silent_failures.py plugins/$PLUGIN_NAME/ --fix\n```\n\nThis script validates:\n\n- **Shellcheck**: Runs on all `hooks/*.sh` files\n- **Silent bash commands**: `mkdir`, `cp`, `mv`, `rm` must use `if !` pattern\n- **Silent Python exceptions**: `except: pass` must emit to stderr\n\n**Critical Rule**: All hook entry points MUST emit to stderr on failure.\n\nIf violations are found, fix them before proceeding:\n\n| Pattern                | Fix                                                              |\n| ---------------------- | ---------------------------------------------------------------- |\n| `mkdir -p \"$DIR\"`      | `if ! mkdir -p \"$DIR\" 2>&1; then echo \"[plugin] Failed\" >&2; fi` |\n| `except OSError: pass` | `except OSError as e: print(f\"[plugin] {e}\", file=sys.stderr)`   |\n\n### 3.5 Plugin Validation Agent\n\n**Spawn Agent: `plugin-dev:plugin-validator`**  validate plugin structure.\n\n```\nTask with subagent_type=\"plugin-dev:plugin-validator\"\nprompt: \"Validate the plugin at plugins/$PLUGIN_NAME/ for correct structure, manifest, and component organization.\"\n```\n\n### Phase 3 Gate\n\n**STOP. Verify before proceeding to Phase 4:**\n\n- [ ] Plugin added to marketplace.json\n- [ ] validate-plugins.mjs passes\n- [ ] code-hardcode-audit passes\n- [ ] silent-failure-audit passes (no errors)\n- [ ] plugin-validator agent approves\n\n---\n\n## Phase 4: Commit & Release\n\n### 4.1 Stage Changes\n\n```bash\ngit add plugins/$PLUGIN_NAME/\ngit add .claude-plugin/marketplace.json\ngit add docs/adr/$ADR_ID.md\ngit add docs/design/$ADR_ID/\n```\n\n### 4.2 Create Conventional Commit\n\n```bash\ngit commit -m \"feat($PLUGIN_NAME): add plugin for [brief description]\n\n- Create plugin directory structure\n- Add plugin.json manifest\n- Register in marketplace.json\n- Add ADR and design spec\n\nADR: $ADR_ID\"\n```\n\n### 4.3 Push to Remote\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\ngit push origin $(git branch --show-current)\nGIT_EOF\n```\n\n### 4.4 Semantic Release\n\n**MANDATORY Skill tool call: `itp:semantic-release`**  activate NOW.\n\nThis skill:\n\n- Tags the release\n- Updates CHANGELOG\n- Creates GitHub release\n- Syncs versions across all plugins\n\n**Invoke with CI=false for local execution:**\n\n```bash\n/usr/bin/env bash << 'PLUGIN_ADD_SCRIPT_EOF_5'\n/usr/bin/env bash -c 'CI=false GITHUB_TOKEN=$(gh auth token) npm run release'\nPLUGIN_ADD_SCRIPT_EOF_5\n```\n\n### Phase 4 Success Criteria\n\n- [ ] All changes committed with conventional commit\n- [ ] Pushed to remote\n- [ ] semantic-release completed\n- [ ] New version tag created\n- [ ] GitHub release published\n\n---\n\n## Completion\n\n**Workflow complete!** The new plugin is now:\n\n1.  Scaffolded with proper structure\n2.  Documented with ADR and design spec\n3.  Components created (as selected)\n4.  Registered in marketplace.json\n5.  Validated by scripts and agents\n6.  Released with semantic versioning\n\n**Output the GitHub release URL:**\n\n```bash\ngh release view --json url -q .url\n```\n\n**Install the plugin in Claude Code:**\n\n```bash\n/plugin marketplace update cc-skills\n/plugin install $PLUGIN_NAME@cc-skills\n```"
              }
            ],
            "skills": [
              {
                "name": "plugin-validator",
                "description": "Validate plugin structure, manifests, and silent failure patterns. TRIGGERS - plugin validation, validate plugin, check plugin, silent failures, hook audit, stderr emission.",
                "path": "plugins/plugin-dev/skills/plugin-validator/SKILL.md",
                "frontmatter": {
                  "name": "plugin-validator",
                  "description": "Validate plugin structure, manifests, and silent failure patterns. TRIGGERS - plugin validation, validate plugin, check plugin, silent failures, hook audit, stderr emission.",
                  "allowed-tools": "Read, Bash, Glob, Grep, TodoWrite"
                },
                "content": "# Plugin Validator\n\nComprehensive validation for Claude Code marketplace plugins.\n\n## Quick Start\n\n```bash\n# Validate a specific plugin\nuv run plugins/plugin-dev/skills/plugin-validator/scripts/audit_silent_failures.py plugins/my-plugin/\n\n# Validate with fix suggestions\nuv run plugins/plugin-dev/skills/plugin-validator/scripts/audit_silent_failures.py plugins/my-plugin/ --fix\n```\n\n## Validation Phases\n\n### Phase 1: Structure Validation\n\nCheck plugin directory structure:\n\n```bash\n/usr/bin/env bash << 'VALIDATE_EOF'\nPLUGIN_PATH=\"${1:-.}\"\n\n# Check plugin.json exists\nif [[ ! -f \"$PLUGIN_PATH/plugin.json\" ]]; then\n    echo \"ERROR: Missing plugin.json\" >&2\n    exit 1\nfi\n\n# Validate JSON syntax\nif ! jq empty \"$PLUGIN_PATH/plugin.json\" 2>/dev/null; then\n    echo \"ERROR: Invalid JSON in plugin.json\" >&2\n    exit 1\nfi\n\n# Check required fields\nREQUIRED_FIELDS=(\"name\" \"version\" \"description\")\nfor field in \"${REQUIRED_FIELDS[@]}\"; do\n    if ! jq -e \".$field\" \"$PLUGIN_PATH/plugin.json\" >/dev/null 2>&1; then\n        echo \"ERROR: Missing required field: $field\" >&2\n        exit 1\n    fi\ndone\n\necho \"Structure validation passed\"\nVALIDATE_EOF\n```\n\n### Phase 2: Silent Failure Audit\n\n**Critical Rule**: All hook entry points MUST emit to stderr on failure.\n\nRun the audit script:\n\n```bash\nuv run plugins/plugin-dev/skills/plugin-validator/scripts/audit_silent_failures.py plugins/my-plugin/\n```\n\n#### What Gets Checked\n\n| Check         | Target Files | Pattern                                |\n| ------------- | ------------ | -------------------------------------- |\n| Shellcheck    | `hooks/*.sh` | SC2155, SC2086, etc.                   |\n| Silent bash   | `hooks/*.sh` | `mkdir\\|cp\\|mv\\|rm\\|jq` without `if !` |\n| Silent Python | `hooks/*.py` | `except.*: pass` without stderr        |\n\n#### Hook Entry Points vs Utility Scripts\n\n| Location                 | Type        | Requirement          |\n| ------------------------ | ----------- | -------------------- |\n| `plugins/*/hooks/*.sh`   | Entry point | MUST emit to stderr  |\n| `plugins/*/hooks/*.py`   | Entry point | MUST emit to stderr  |\n| `plugins/*/scripts/*.sh` | Utility     | Fallback behavior OK |\n| `plugins/*/scripts/*.py` | Utility     | Fallback behavior OK |\n\n### Phase 3: Fix Patterns\n\n#### Bash: Silent mkdir\n\n```bash\n# BAD - silent failure\nmkdir -p \"$DIR\"\n\n# GOOD - emits to stderr\nif ! mkdir -p \"$DIR\" 2>&1; then\n    echo \"[plugin] Failed to create directory: $DIR\" >&2\nfi\n```\n\n#### Python: Silent except pass\n\n```python\n# BAD - silent failure\nexcept (json.JSONDecodeError, OSError):\n    pass\n\n# GOOD - emits to stderr\nexcept (json.JSONDecodeError, OSError) as e:\n    print(f\"[plugin] Warning: {e}\", file=sys.stderr)\n```\n\n## Integration with /plugin-dev:create\n\nThis skill is invoked in Phase 3 of the plugin-add workflow:\n\n```markdown\n### 3.4 Plugin Validation\n\n**MANDATORY**: Run plugin-validator before registration.\n\nTask with subagent_type=\"plugin-dev:plugin-validator\"\nprompt: \"Validate the plugin at plugins/$PLUGIN_NAME/\"\n```\n\n## Exit Codes\n\n| Code | Meaning                             |\n| ---- | ----------------------------------- |\n| 0    | All validations passed              |\n| 1    | Violations found (see output)       |\n| 2    | Error (invalid path, missing files) |\n\n## References\n\n- [Silent Failure Patterns](./references/silent-failure-patterns.md)"
              },
              {
                "name": "skill-architecture",
                "description": "Meta-skill for creating Claude Code skills. TRIGGERS - create skill, YAML frontmatter, validate skill, TodoWrite templates, bundled resources (scripts/references/assets), progressive disclosure, allowed-tools, skill architecture. Use when creating, validating, or structuring skills.",
                "path": "plugins/plugin-dev/skills/skill-architecture/SKILL.md",
                "frontmatter": {
                  "name": "skill-architecture",
                  "description": "Meta-skill for creating Claude Code skills. TRIGGERS - create skill, YAML frontmatter, validate skill, TodoWrite templates, bundled resources (scripts/references/assets), progressive disclosure, allowed-tools, skill architecture. Use when creating, validating, or structuring skills."
                },
                "content": "# Skill Architecture\n\nComprehensive guide for creating effective Claude Code skills following Anthropic's official standards with emphasis on security, CLI-specific features, and progressive disclosure architecture.\n\n>  **Scope**: Claude Code CLI Agent Skills (`~/.claude/skills/`), not Claude.ai API skills\n\n---\n\n## FIRST: TodoWrite Task Templates\n\n**MANDATORY**: Select and load the appropriate template into TodoWrite before any skill work.\n\n> For detailed context on each step, see [Skill Creation Process (Detailed Tutorial)](#skill-creation-process-detailed-tutorial) below.\n\n### Template A: Create New Skill\n\n```\n1. Gather requirements (ask user for functionality, examples, triggers)\n2. Identify reusable resources (scripts, references, assets needed)\n3. Run init script to create skill directory structure\n4. Create bundled resources first (scripts/, references/, assets/)\n5. Write SKILL.md with YAML frontmatter (name, description with triggers)\n6. Add TodoWrite task templates section to SKILL.md\n7. Add Post-Change Checklist section to SKILL.md\n8. Validate with quick_validate.py\n9. Validate links (relative paths only): bun run plugins/plugin-dev/scripts/validate-links.ts <skill-path>\n10. Test skill on real example\n11. Register skill in project CLAUDE.md\n12. Verify against Skill Quality Checklist below\n```\n\n### Template B: Update Existing Skill\n\n```\n1. Read current SKILL.md and understand structure\n2. Identify what needs changing (triggers, workflow, resources)\n3. Make targeted changes to SKILL.md\n4. Update any affected references/ or scripts/\n5. Validate with quick_validate.py\n6. Validate links (relative paths only): bun run plugins/plugin-dev/scripts/validate-links.ts <skill-path>\n7. Test updated behavior\n8. Update project CLAUDE.md if description changed\n9. Verify against Skill Quality Checklist below\n```\n\n### Template C: Add Resources to Skill\n\n```\n1. Read current SKILL.md to understand skill purpose\n2. Determine resource type (script, reference, or asset)\n3. Create resource in appropriate directory\n4. Update SKILL.md to document new resource\n5. Validate with quick_validate.py\n6. Validate links (relative paths only): bun run plugins/plugin-dev/scripts/validate-links.ts <skill-path>\n7. Test resource integration\n8. Verify against Skill Quality Checklist below\n```\n\n### Template D: Convert to Self-Evolving Skill\n\n```\n1. Read current SKILL.md structure\n2. Add TodoWrite Task Templates section (scenario-specific)\n3. Add Post-Change Checklist section\n4. Create references/evolution-log.md (reverse chronological - newest on top)\n5. Create references/config-reference.md (if skill manages external config)\n6. Update description with self-evolution triggers\n7. Validate with quick_validate.py\n8. Validate links (relative paths only): bun run plugins/plugin-dev/scripts/validate-links.ts <skill-path>\n9. Test self-documentation on sample change\n10. Verify against Skill Quality Checklist below\n```\n\n### Template E: Troubleshoot Skill Not Triggering\n\n```\n1. Check YAML frontmatter syntax (no colons in description)\n2. Verify trigger keywords in description match user queries\n3. Check skill location (~/.claude/skills/ or project .claude/skills/)\n4. Validate with quick_validate.py for errors\n5. Validate links: bun run plugins/plugin-dev/scripts/validate-links.ts <skill-path>\n6. Test with explicit trigger phrase\n7. Document findings in skill if new issue discovered\n8. Verify against Skill Quality Checklist below\n```\n\n### Skill Quality Checklist\n\nAfter ANY skill work, verify:\n\n- [ ] YAML frontmatter valid (name lowercase-hyphen, description has triggers)\n- [ ] Description includes WHEN to use (trigger keywords)\n- [ ] TodoWrite templates cover all common scenarios\n- [ ] Post-Change Checklist included for self-maintenance\n- [ ] Final template step references this checklist\n- [ ] Project CLAUDE.md updated if new/renamed skill\n- [ ] Validated with quick_validate.py\n- [ ] All markdown links use relative paths (plugin-portable)\n- [ ] No broken internal links (validate-links.ts passes)\n- [ ] No unsafe path patterns (see [Path Patterns](./references/path-patterns.md)):\n  - No hardcoded `/Users/<user>` or `/home/<user>` (use `$HOME`)\n  - No hardcoded `/tmp` in Python (use `tempfile.TemporaryDirectory`)\n  - No hardcoded binary paths (use `command -v` or PATH)\n- [ ] Bash compatibility verified (see [Bash Compatibility](./references/bash-compatibility.md)):\n  - All bash code blocks wrapped with `/usr/bin/env bash << 'NAME_EOF'`\n  - No `declare -A` (associative arrays) - use parallel indexed arrays\n  - No `grep -P` (Perl regex) - use `grep -E` with awk\n  - No `\\!=` in conditionals - use `!=` directly\n  - Heredoc EOF marker is descriptive (e.g., `PREFLIGHT_EOF`)\n\n---\n\n## Post-Change Checklist (Self-Maintenance)\n\nAfter modifying THIS skill (skill-architecture):\n\n1. [ ] Templates and 6 Steps tutorial remain aligned\n2. [ ] Skill Quality Checklist reflects current best practices\n3. [ ] All referenced files in references/ exist\n4. [ ] Append changes to [evolution-log.md](./references/evolution-log.md)\n5. [ ] Update user's CLAUDE.md if triggers changed\n\n---\n\n## Continuous Improvement (Proactive Self-Evolution)\n\n**CRITICAL**: Skills must actively evolve. Don't wait for explicit requestsupgrade skills when insights emerge.\n\n### During Every Skill Execution\n\nWatch for these improvement signals:\n\n| Signal                    | Example                        | Action                      |\n| ------------------------- | ------------------------------ | --------------------------- |\n| **Friction**              | Step feels awkward or unclear  | Rewrite for clarity         |\n| **Missing edge case**     | Workflow fails on valid input  | Add handling + document     |\n| **Better pattern**        | Discover more elegant approach | Update + log why            |\n| **User confusion**        | Same question asked repeatedly | Add clarification or FAQ    |\n| **Tool evolution**        | Underlying tool gains features | Update to leverage them     |\n| **Repeated manual steps** | Same code written each time    | Create script in `scripts/` |\n\n### Immediate Update Protocol\n\nWhen improvement opportunity identified:\n\n1. **Pause current task** (briefly)\n2. **Make the improvement** to SKILL.md or resources\n3. **Log in evolution-log.md** (one-liner is fine for small changes)\n4. **Resume original task**\n\n> **Rationale**: Small immediate updates compound. Waiting means insights are forgotten. 30 seconds now saves 5 minutes later.\n\n### What NOT to Update Immediately\n\n- Major structural changes (discuss with user first)\n- Changes that would break in-progress work\n- Speculative improvements without concrete evidence\n\n### Self-Reflection Trigger\n\nAfter completing any skill-assisted task, ask:\n\n> \"Did anything about this skill feel suboptimal? If I encountered this again, what would help?\"\n\nIf answer exists  update the skill NOW.\n\n---\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities with specialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific domainstransforming Claude from general-purpose to specialized agent with procedural knowledge no model fully possesses.\n\n### What Skills Provide\n\n1. **Specialized workflows** - Multi-step procedures for specific domains\n2. **Tool integrations** - Instructions for working with specific file formats or APIs\n3. **Domain expertise** - Company-specific knowledge, schemas, business logic\n4. **Bundled resources** - Scripts, references, assets for complex/repetitive tasks\n\n---\n\n## Skill Creation Process (Detailed Tutorial)\n\n> **Note**: Use TodoWrite templates above for execution. This section provides detailed context for each phase.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nClearly understand concrete examples of how the skill will be used. Ask users:\n\n- \"What functionality should this skill support?\"\n- \"Can you give examples of how it would be used?\"\n- \"What would trigger this skill?\"\n\nSkip only when usage patterns are already clearly understood.\n\n### Step 2: Planning Reusable Contents\n\nAnalyze each example to identify what resources would be helpful:\n\n**Example 1 - PDF Editor**:\n\n- Rotating PDFs requires rewriting code each time\n-  Create `scripts/rotate_pdf.py`\n\n**Example 2 - Frontend Builder**:\n\n- Webapps need same HTML/React boilerplate\n-  Create `assets/hello-world/` template\n\n**Example 3 - BigQuery**:\n\n- Queries require rediscovering table schemas\n-  Create `references/schema.md`\n\n### Step 3: Initialize the Skill\n\nRun the marketplace init script (don't copy, use from marketplace):\n\n```bash\nplugins/marketplaces/anthropic-agent-skills/skill-creator/scripts/init_skill.py <skill-name> --path ~/.claude/skills/\n```\n\nCreates: skill directory + SKILL.md template + example resource directories\n\n### Step 4: Edit the Skill\n\n**Writing Style**: Imperative/infinitive form (verb-first), not second person\n\n-  \"To accomplish X, do Y\"\n-  \"You should do X\"\n\n**SKILL.md must include**:\n\n1. What is the purpose? (few sentences)\n2. When should it be used? (trigger keywords in description)\n3. How should Claude use bundled resources?\n4. **TodoWrite Task Templates** - Pre-defined todos for common scenarios\n5. **Post-Change Checklist** - Self-maintenance verification\n\n**Start with resources** (`scripts/`, `references/`, `assets/`), then update SKILL.md\n\n### Step 5: Validate the Skill\n\n**For local development** (validation only, no zip creation):\n\n```bash\nplugins/marketplaces/anthropic-agent-skills/skill-creator/scripts/quick_validate.py <path/to/skill-folder>\n```\n\n**For distribution** (validates AND creates zip):\n\n```bash\nplugins/marketplaces/anthropic-agent-skills/skill-creator/scripts/package_skill.py <path/to/skill-folder>\n```\n\nValidates: YAML frontmatter, naming, description, file organization\n\n**Note**: Use `quick_validate.py` for most workflows. Only use `package_skill.py` when actually distributing the skill to others.\n\n### Step 6: Register and Iterate\n\n1. Register skill in project CLAUDE.md (Workspace Skills section)\n2. Use skill on real tasks\n3. Notice struggles/inefficiencies\n4. Update SKILL.md or resources\n5. Test again\n6. Verify against Skill Quality Checklist above\n\n---\n\n## Skill Anatomy\n\n```\nskill-name/\n SKILL.md                      # Required: YAML frontmatter + instructions\n scripts/                      # Optional: Executable code (Python/Bash)\n references/                   # Optional: Documentation loaded as needed\n    evolution-log.md          # Recommended: Change history (self-evolving)\n assets/                       # Optional: Files used in output\n```\n\n### YAML Frontmatter (Required)\n\n```yaml\n---\nname: skill-name-here\ndescription: What this does and when to use it (max 1024 chars for CLI)\nallowed-tools: Read, Grep, Bash # Optional, CLI-only feature\n---\n```\n\n**Field Requirements:**\n\n| Field           | Rules                                                                           |\n| --------------- | ------------------------------------------------------------------------------- |\n| `name`          | Lowercase, hyphens, numbers. Max 64 chars. Unique.                              |\n| `description`   | WHAT it does + WHEN to use. Max 1024 chars (CLI) / 200 (API). Include triggers! |\n| `allowed-tools` | **CLI-only**. Comma-separated list restricts tools. Optional.                   |\n\n**Good vs Bad Descriptions:**\n\n **Good**: \"Extract text and tables from PDFs, fill forms, merge documents. Use when working with PDF files or when user mentions forms, contracts, document processing.\"\n\n **Bad**: \"Helps with documents\" (too vague, no triggers)\n\n**YAML Description Pitfalls:**\n\n| Pitfall          | Problem                          | Fix                                                                                  |\n| ---------------- | -------------------------------- | ------------------------------------------------------------------------------------ |\n| Multiline syntax | `>` or `\\|` not supported        | Single line only                                                                     |\n| Colons in text   | `CRITICAL: requires` breaks YAML | Use `CRITICAL - requires`                                                            |\n| Quoted strings   | Valid but not idiomatic          | Unquoted preferred (match [anthropics/skills](https://github.com/anthropics/skills)) |\n\n```yaml\n#  BREAKS - colon parsed as YAML key:value\ndescription: ...CRITICAL: requires flag\n\n#  WORKS - dash instead of colon\ndescription: ...CRITICAL - requires flag\n```\n\n**Validation**: GitHub renders frontmatter - invalid YAML shows red error banner.\n\n### Progressive Disclosure (3 Levels)\n\nSkills use progressive loading to manage context efficiently:\n\n1. **Metadata** (name + description) - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (unlimited\\*)\n\n\\*Scripts can execute without reading into context.\n\n---\n\n## Bundled Resources\n\nSkills can include `scripts/`, `references/`, and `assets/` directories. See [Progressive Disclosure](./references/progressive-disclosure.md) for detailed guidance on when to use each.\n\n---\n\n## CLI-Specific Features\n\nCLI skills support `allowed-tools` restriction for security. See [Security Practices](./references/security-practices.md) for details.\n\n---\n\n## Structural Patterns\n\nSee [Structural Patterns](./references/structural-patterns.md) for detailed guidance on:\n\n1. **Workflow Pattern** - Sequential multi-step procedures\n2. **Task Pattern** - Specific, bounded tasks\n3. **Reference Pattern** - Knowledge repository\n4. **Capabilities Pattern** - Tool integrations\n\n---\n\n## User Conventions Integration\n\nThis skill follows common user conventions:\n\n- **Absolute paths**: Always use full paths (terminal Cmd+click compatible)\n- **Unix-only**: macOS, Linux (no Windows support)\n- **Python**: `uv run script.py` with PEP 723 inline dependencies\n- **Planning**: OpenAPI 3.1.1 specs when appropriate\n\n---\n\n## Marketplace Scripts\n\nSee [Scripts Reference](./references/scripts-reference.md) for marketplace script usage.\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Structural Patterns](./references/structural-patterns.md) - 4 skill architecture patterns\n- [Workflow Patterns](./references/workflow-patterns.md) - Workflow skill implementation patterns\n- [Progressive Disclosure](./references/progressive-disclosure.md) - Context management patterns\n- [Creation Workflow](./references/creation-workflow.md) - Step-by-step process\n- [Scripts Reference](./references/scripts-reference.md) - Marketplace script usage\n- [Security Practices](./references/security-practices.md) - Threats and defenses (CVE references)\n- [Token Efficiency](./references/token-efficiency.md) - Context optimization\n- [Advanced Topics](./references/advanced-topics.md) - CLI vs API, composition, bugs\n- [Path Patterns](./references/path-patterns.md) - Safe/unsafe path references (known bugs documented)\n- [Validation Reference](./references/validation-reference.md) - Quality checklist\n- [SYNC-TRACKING](./references/SYNC-TRACKING.md) - Marketplace version tracking\n- [Evolution Log](./references/evolution-log.md) - This skill's change history"
              }
            ]
          },
          {
            "name": "itp",
            "description": "Implement-The-Plan workflow: ADR-driven 4-phase development with preflight, implementation, formatting, and release automation",
            "source": "./plugins/itp/",
            "category": "productivity",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install itp@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/go",
                "description": "WORKFLOW COMMAND - Execute TodoWrite FIRST, then Preflight  Phase 1  2  3. Do NOT read ~/.claude/plans/ until after TodoWrite.",
                "path": "plugins/itp/commands/go.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(git checkout:*), Bash(git pull:*), Bash(git add:*), Bash(git commit:*), Bash(git push:*), Bash(git branch:*), Bash(prettier --write:*), Bash(open:*), Bash(gh repo:*), Bash(cp:*), Bash(mkdir -p:*), Bash(date:*), Bash(PLUGIN_DIR:*), Bash(uv run:*), Grep, Glob, Task",
                  "argument-hint": "Start: [name] [-b] [-r] [-p] | Resume: -c [choice]",
                  "description": "WORKFLOW COMMAND - Execute TodoWrite FIRST, then Preflight  Phase 1  2  3. Do NOT read ~/.claude/plans/ until after TodoWrite."
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n#  ITP Workflow  STOP AND READ\n\n**DO NOT ACT ON ASSUMPTIONS. Read this file first.**\n\nThis is a structured workflow command. Follow the phases in order.\n\nYour FIRST and ONLY action right now: **Execute the TodoWrite below.**\n\n##  MANDATORY FIRST ACTION: Plan-Aware Todo Integration\n\n**YOUR FIRST ACTION MUST BE a MERGED TodoWrite that preserves existing todos.**\n\n<!-- ADR: 2025-12-05-itp-todo-insertion-merge -->\n\nDO NOT:\n\n-  Overwrite existing todos from plan files or previous sessions\n-  Ignore the plan file at `~/.claude/plans/*.md`\n-  Create your own todos without checking for existing ones\n-  Jump to coding without completing Step 0\n-  Create a branch before TodoWrite\n\n### Step 0.1: Check for Existing Plan File\n\n1. Check if a plan file exists in `~/.claude/plans/`\n2. If system-reminder mentions a plan file path: use that specific path\n3. If plan file exists: Read it and extract any tasks/todos from it\n\n### Step 0.2: Check Existing Todos\n\n1. Check your mental model of existing todos (from prior conversation)\n2. Note any `in_progress` or `pending` items that should be preserved\n\n### Step 0.3: Merge Strategy (INTERLEAVE)\n\n**Map plan tasks into ITP phases intelligently:**\n\n| Plan Task Type                 | Maps To ITP Phase      |\n| ------------------------------ | ---------------------- |\n| Research, explore, understand  | Preflight (before ADR) |\n| Design, architecture decisions | Preflight (in ADR)     |\n| Implementation tasks           | Phase 1                |\n| Testing, validation            | Phase 1 (after impl)   |\n| Documentation, cleanup         | Phase 2                |\n| Release, deploy                | Phase 3                |\n\n### Step 0.4: Conflict Resolution\n\n**If a plan task doesn't clearly map to an ITP phase, use AskUserQuestion:**\n\n```\nAskUserQuestion with questions:\n- question: \"Where should '{task_name}' be placed in the ITP workflow?\"\n  header: \"Task placement\"\n  options:\n    - label: \"Before Preflight\"\n      description: \"Do this task first, before ADR creation\"\n    - label: \"Phase 1\"\n      description: \"Do during implementation\"\n    - label: \"After Phase 2\"\n      description: \"Do last, before release\"\n  multiSelect: false\n```\n\n### Step 0.5: Create MERGED TodoWrite\n\n**After mapping, create a MERGED todo list using these prefixes:**\n\n- `[Plan]`  Tasks from the plan file\n- `[ITP]`  ITP workflow tasks\n\n**MANDATORY TodoWrite template (MERGE with existing, do NOT overwrite):**\n\n```\nTodoWrite with todos (MERGED - preserving existing):\n\n# From plan file (if any) - mapped to Preflight\n# Example: \"[Plan] Research existing implementation\" | pending\n\n# ITP Preflight - Skill tool calls marked explicitly\n# CRITICAL: Branch creation MUST be FIRST if -b flag (before any file operations)\n- \"[ITP] Preflight: Create feature branch (if -b flag)  MUST BE FIRST\" | pending\n- \"[ITP] Preflight: Skill tool call  implement-plan-preflight\" | pending\n- \"[ITP] Preflight: Create ADR file with MADR 4.0 frontmatter\" | pending\n- \"[ITP] Preflight: Skill tool call  adr-graph-easy-architect (Before/After + Architecture diagrams)\" | pending\n- \"[ITP] Preflight: Create design spec with YAML frontmatter\" | pending\n- \"[ITP] Preflight: Verify checkpoint (ADR + spec exist)\" | pending\n\n# From plan file (if any) - mapped to Phase 1\n# Example: \"[Plan] Implement the new feature\" | pending\n\n# ITP Phase 1 - Skill tool calls marked explicitly\n- \"[ITP] Phase 1: Sync ADR status proposed  accepted\" | pending\n- \"[ITP] Phase 1: Skill tool call  impl-standards\" | pending\n- \"[ITP] Phase 1: Skill tool call  mise-configuration (if new scripts)\" | pending\n- \"[ITP] Phase 1: Skill tool call  adr-code-traceability\" | pending\n- \"[ITP] Phase 1: Execute implementation tasks from spec.md\" | pending\n- \"[ITP] Phase 1: Skill tool call  code-hardcode-audit\" | pending\n\n# From plan file (if any) - mapped to Phase 2\n# Example: \"[Plan] Update documentation\" | pending\n\n# ITP Phase 2\n- \"[ITP] Phase 2: Format markdown with Prettier\" | pending\n- \"[ITP] Phase 2: Push to GitHub\" | pending\n- \"[ITP] Phase 2: Open files in browser\" | pending\n\n# ITP Phase 3  REQUIRES -r or -p flag on main/master\n- \"[ITP] Phase 3: Pre-release verification (if -r or -p on main)\" | pending\n- \"[ITP] Phase 3: Skill tool call  semantic-release (if -r flag on main)\" | pending\n- \"[ITP] Phase 3: Skill tool call  pypi-doppler (if -p flag on main)\" | pending\n- \"[ITP] Phase 3: Final status sync (if -r or -p on main)\" | pending\n```\n\n**After TodoWrite completes, proceed to Preflight section below.**\n\n---\n\n## Quick Reference\n\n### Skills Invoked\n\n| Skill                      | Phase     | Purpose                         |\n| -------------------------- | --------- | ------------------------------- |\n| `implement-plan-preflight` | Preflight | ADR + Design Spec creation      |\n| `adr-graph-easy-architect` | Preflight | Architecture diagrams           |\n| `impl-standards`           | Phase 1   | Error handling, constants       |\n| `mise-configuration`       | Phase 1   | Env var centralization patterns |\n| `adr-code-traceability`    | Phase 1   | Code-to-ADR references          |\n| `code-hardcode-audit`      | Phase 1   | Pre-release validation          |\n| `semantic-release`         | Phase 3   | Version tagging + release       |\n| `pypi-doppler`             | Phase 3   | PyPI publishing (if applicable) |\n\n### File Locations\n\n| Artifact    | Path                                 | Notes                                |\n| ----------- | ------------------------------------ | ------------------------------------ |\n| ADR         | `/docs/adr/$ADR_ID.md`               | Permanent                            |\n| Design Spec | `/docs/design/$ADR_ID/spec.md`       | Permanent, SSoT after Preflight      |\n| Global Plan | `~/.claude/plans/<adj-verb-noun>.md` | **EPHEMERAL** - replaced on new plan |\n\n### Spec YAML Frontmatter\n\n```yaml\n---\nadr: YYYY-MM-DD-slug # Links to ADR (programmatic)\nsource: ~/.claude/plans/<adj-verb-noun>.md # Global plan (EPHEMERAL)\nimplementation-status: in_progress # in_progress | blocked | completed | abandoned\nphase: preflight # preflight | phase-1 | phase-2 | phase-3\nlast-updated: YYYY-MM-DD\n---\n```\n\n**Note**: The `source` field preserves the global plan filename for traceability, but the file may no longer exist after a new plan is created.\n\n### ADR ID Format\n\n```\nADR_ID=\"$(date +%Y-%m-%d)-<slug>\"\n```\n\nExample: `2025-12-01-clickhouse-aws-ohlcv-ingestion`\n\n### Folder Structure\n\n```text\n/docs/\n  adr/\n    YYYY-MM-DD-slug.md          # ADR file\n  design/\n    YYYY-MM-DD-slug/            # Design folder (1:1 with ADR)\n      spec.md                   # Active implementation spec (SSoT)\n```\n\n**Naming Rule**: Use exact same `YYYY-MM-DD-slug` for both ADR and Design folder.\n\n---\n\n## CRITICAL: Mandatory Workflow Execution\n\n**THIS WORKFLOW IS NON-NEGOTIABLE. DO NOT SKIP ANY PHASE.**\n\nYou MUST execute ALL phases in order, regardless of task complexity:\n\n1. **Step 0**: TodoWrite initialization (FIRST ACTION - NO EXCEPTIONS)\n2. **Preflight**: ADR + Design Spec creation\n3. **Phase 1**: Implementation per spec.md\n4. **Phase 2**: Format & Push\n5. **Phase 3**: Release (if on main/master)\n\n**FORBIDDEN BEHAVIORS:**\n\n-  Deciding \"this is simple, skip the workflow\"\n-  Jumping directly to implementation without TodoWrite\n-  Skipping ADR/Design Spec for \"document fixes\" or \"small changes\"\n-  Making autonomous judgments to bypass phases\n\n**If the task seems too simple for this workflow**: Stop and ask the user if they want to proceed without `/itp:go`. Do NOT silently skip phases.\n\n---\n\n## Arguments\n\nParse `$ARGUMENTS` for:\n\n| Argument     | Short | Description                                                       | Default                         |\n| ------------ | ----- | ----------------------------------------------------------------- | ------------------------------- |\n| `slug`       | -     | Feature name for ADR ID (e.g., `clickhouse-aws-ohlcv-ingestion`)  | Derive from Global Plan context |\n| `--branch`   | `-b`  | Create branch `{type}/{adr-id}` from main/master                  | Work on current branch          |\n| `--continue` | `-c`  | Continue in-progress work; optionally provide decision            | Last \"Recommended Next Steps\"   |\n| `--release`  | `-r`  | Enable semantic-release in Phase 3 (required on main for release) | Skip Phase 3 release            |\n| `--publish`  | `-p`  | Enable PyPI publishing in Phase 3 (required on main for publish)  | Skip Phase 3 publish            |\n\n**Usage Examples**:\n\n```text\n# Fresh start modes (no release)\n/itp:go                   # Derive slug, stay on current branch\n/itp:go my-feature        # Custom slug, stay on current branch\n/itp:go -b                # Derive slug, create {type}/{adr-id} branch\n/itp:go my-feature -b     # Custom slug, create {type}/{adr-id} branch\n\n# Feature branch with release intent (reminder shown, Phase 3 skips)\n/itp:go my-feature -b -r        # Intent to release after merge\n/itp:go my-feature -b -r -p     # Intent to release + publish after merge\n\n# Release modes (on main/master only)\n/itp:go -r                # On main: run semantic-release only\n/itp:go -p                # On main: run PyPI publish only\n/itp:go -r -p             # On main: full release + publish\n\n# Continuation modes\n/itp:go -c                # Continue: auto-detect ADR, resume\n/itp:go -c \"use Redis\"    # Continue with explicit decision\n```\n\n**Mode Selection**:\n\n- Fresh start: `[slug] [-b]`  creates new ADR\n- Continuation: `-c [decision]`  resumes existing ADR\n\nThese modes are **mutually exclusive**. `-c` cannot be combined with `slug` or `-b`.\n\n**Branch Type**: Determine `{type}` from ADR nature (conventional commits):\n\n| Type       | When                                   |\n| ---------- | -------------------------------------- |\n| `feat`     | New capability or feature              |\n| `fix`      | Bug fix                                |\n| `refactor` | Code restructuring, no behavior change |\n| `docs`     | Documentation only                     |\n| `chore`    | Maintenance, tooling, dependencies     |\n| `perf`     | Performance improvement                |\n\n**Slug Derivation**: If no slug is provided, derive an appropriate kebab-case slug from the Global Plan's context (the feature/task being implemented). The slug should be descriptive (3-5 words) and capture the essence of the feature.\n\n**Word Economy Rule**: Each word in the slug MUST convey unique meaning. Avoid redundancy.\n\n| Example                          | Verdict | Reason                                                           |\n| -------------------------------- | ------- | ---------------------------------------------------------------- |\n| `clickhouse-database-migration`  |  Bad  | \"database\" redundant (ClickHouse IS a database)                  |\n| `clickhouse-aws-ohlcv-ingestion` |  Good | clickhouse=tech, aws=platform, ohlcv=data-type, ingestion=action |\n| `user-auth-token-refresh`        |  Good | user=scope, auth=domain, token=artifact, refresh=action          |\n| `api-endpoint-rate-limiting`     |  Good | api=layer, endpoint=target, rate=metric, limiting=action         |\n\n**ADR ID**: See [Quick Reference](#adr-id-format) for format. The ADR ID is the canonical identifier used in:\n\n- ADR file, Design folder, Code references, Branch name (if `-b`)\n\n---\n\n## Workflow Preview\n\n**Detect branch and show expected workflow before starting.**\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\nCURRENT_BRANCH=$(git branch --show-current)\nWILL_BE_ON_MAIN=true\n\n# If -b flag used, will end up on feature branch\nif [ -n \"$BRANCH_FLAG\" ]; then\n  WILL_BE_ON_MAIN=false\nfi\n\n# If already not on main/master\nif [ \"$CURRENT_BRANCH\" != \"main\" ] && [ \"$CURRENT_BRANCH\" != \"master\" ]; then\n  WILL_BE_ON_MAIN=false\nfi\nGIT_EOF\n```\n\n**Show workflow preview based on branch and flags:**\n\n| Condition                      | Workflow                        | Message                                               |\n| ------------------------------ | ------------------------------- | ----------------------------------------------------- |\n| main/master, no flags          | `Preflight  1  2  END`       | \"Phase 3 skipped. Use -r for release, -p for publish\" |\n| main/master, `-r`              | `Preflight  1  2  3.2`       | \"Running semantic-release...\"                         |\n| main/master, `-p`              | `Preflight  1  2  3.3`       | \"Running PyPI publish...\"                             |\n| main/master, `-r -p`           | `Preflight  1  2  3.2  3.3` | \"Running full release...\"                             |\n| feature (`-b`), no `-r`/`-p`   | `Preflight  1  2  END`       | Standard feature branch message                       |\n| feature (`-b`), with `-r`/`-p` | `Preflight  1  2  END`       | Verbose reminder (see Phase 3)                        |\n\n**Phase 3 now requires explicit flags on main/master.** This is a breaking change from previous behavior where Phase 3 ran automatically.\n\n---\n\n## Step 0: Initialize Todo List (ALREADY DONE)\n\n**If you followed the  STOP instruction at the top, this step is complete.**\n\nThe TodoWrite template is at the top of this file. If you haven't executed it yet, **STOP and go back to the top**.\n\n**Mark each todo `in_progress` before starting, `completed` when done.**\n\n### Preflight Gate (MANDATORY)\n\n**You CANNOT proceed to Phase 1 until ALL Preflight todos are marked `completed`.**\n\nBefore starting \"Phase 1: Execute implementation tasks\":\n\n1. Verify all `Preflight:` todos show `completed`\n2. Verify ADR file exists at `/docs/adr/$ADR_ID.md`\n3. Verify design spec exists at `/docs/design/$ADR_ID/spec.md`\n\nIf any Preflight item is not complete, **STOP** and complete it first. Do NOT skip ahead.\n\n---\n\n## Preflight: Artifact Setup\n\n**MANDATORY Skill tool call: `implement-plan-preflight`**  activate NOW before proceeding.\n\nThis skill provides detailed ADR and Design Spec creation instructions.\n\nThe skill provides:\n\n- MADR 4.0 frontmatter template and required sections\n- Perspectives taxonomy (11 types)\n- Step-by-step workflow for ADR and design spec creation\n- Validation script for checkpoint verification\n\n### Preflight Steps (via skill)\n\n1. **P.0**: **Create feature branch FIRST** (if `-b` flag)  MUST happen before ANY file operations\n2. **P.1**: **MANDATORY Skill tool call: `implement-plan-preflight`**  activate NOW for ADR/spec instructions\n3. **P.2**: Create ADR file  path in [Quick Reference](#file-locations)\n4. **P.2.1**: **ADR Diagram Creation (MANDATORY for ALL ADRs)**\n\n   **ALL ADRs require BOTH diagrams  NO EXCEPTIONS, regardless of task complexity.**\n   - INVOKE: **Skill tool call with `adr-graph-easy-architect`**  triggers diagram workflow\n   - CREATE: **Before/After diagram**  visualizes state change in Context section\n   - CREATE: **Architecture diagram**  visualizes component relationships in Architecture section\n   - VERIFY: Confirm BOTH diagrams embedded in ADR before proceeding\n\n   **BLOCKING GATE**: Do NOT proceed to P.3 until BOTH diagrams are verified in ADR.\n\n   **Common mistake**: Skipping diagrams for \"simple\" ADRs. Even documentation-only ADRs benefit from Before/After visualization.\n\n5. **P.3**: Create design spec  path in [Quick Reference](#file-locations)\n6. **P.4**: Verify checkpoint\n\n**WHY P.0 FIRST**: Files created before `git checkout -b` stay on main/master. Branch must exist before ADR/spec creation.\n\n### Preflight Checkpoint (MANDATORY)\n\n**STOP. Verify artifacts exist before proceeding to Phase 1.**\n\nRun validator:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Environment-agnostic path (explicit fallback for marketplace installation)\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\nuv run \"$PLUGIN_DIR/skills/implement-plan-preflight/scripts/preflight_validator.py\" $ADR_ID\nPREFLIGHT_EOF\n```\n\nOr verify manually:\n\n- [ ] ADR file exists at `/docs/adr/$ADR_ID.md`\n- [ ] ADR has YAML frontmatter with all 7 required fields\n- [ ] ADR has `status: proposed` (initial state)\n- [ ] ADR has `**Design Spec**:` link in header\n- [ ] **DIAGRAM CHECK 1**: ADR has **Before/After diagram** in Context section (graph-easy block showing state change)\n- [ ] **DIAGRAM CHECK 2**: ADR has **Architecture diagram** in Architecture section (graph-easy block showing components)\n\n** DIAGRAM VERIFICATION (BLOCKING):**\nIf either diagram is missing, **STOP** and invoke `adr-graph-easy-architect` skill again.\nSearch ADR for `<!-- graph-easy source:`  you need TWO separate blocks.\n\n- [ ] Design spec exists at `/docs/design/$ADR_ID/spec.md`\n- [ ] Design spec has YAML frontmatter with all 5 required fields\n- [ ] Design spec has `implementation-status: in_progress`\n- [ ] Design spec has `phase: preflight`\n- [ ] Design spec has `**ADR**:` backlink in header\n- [ ] Feature branch created (if `-b` flag specified)\n\n**If any item is missing**: Complete it now. Do NOT proceed to Phase 1.\n\n---\n\n## Phase 1: Implementation\n\n### 1.1 Resumption Protocol\n\n**Entry point for both fresh starts and continuations.**\n\n1. **Detect mode**:\n   - If `-c` flag: continuation mode (skip to step 2)\n   - Otherwise: fresh start (skip to step 3)\n\n2. **For continuation (`-c`)**:\n\n   a. Find in-progress ADR:\n   - Search `docs/design/*/spec.md` for `status: in_progress`\n   - Or find todo list item marked `in_progress`\n\n   b. Re-read `spec.md` and check for pending decision:\n   - Look for `## Pending Decision` section\n   - If found AND `-c \"decision\"` provided  apply decision, remove pending marker\n   - If found AND `-c` alone  use last \"Recommended Next Steps\" as default action\n   - If no pending decision  proceed to step c\n\n   c. Check todo list for current task:\n   - Find task with `status: in_progress`\n   - Resume implementation from that task\n\n   d. **Verify branch matches ADR context**:\n   - Check current branch: `git branch --show-current`\n   - If ADR was created on a feature branch, verify you're on that branch\n   - If branch mismatch detected, warn user before proceeding\n\n3. **Sync check** (both modes):\n   - Re-read and update the design spec\n   - Verify: ADR  Design Spec  Todo  Code alignment\n   - Report any drift before proceeding\n\n### 1.2 Implement the Spec\n\nExecute each task in `spec.md`:\n\n1. Mark current task as `in_progress` in todo list\n2. Implement the change\n3. Verify it works\n4. Update `spec.md` to reflect completion\n5. Mark task as `completed`\n6. Move to next task\n\n### 1.3 Engineering Standards\n\n**Skill Execution Order** (invoke sequentially, in this order):\n\n1. **`impl-standards`**  Apply error handling & constants patterns FIRST\n2. **`mise-configuration`**  Centralize config via mise [env] SECOND\n3. **`adr-code-traceability`**  Add ADR references to code THIRD\n4. **`code-hardcode-audit`**  Final audit LAST (before Phase 2)\n\n**MANDATORY Skill tool call: `impl-standards`**  activate NOW for detailed standards.\n\n**MANDATORY Skill tool call: `mise-configuration`**  activate when creating/modifying scripts with configurable values.\n\n**MANDATORY Skill tool call: `adr-code-traceability`**  activate NOW for ADR references in code.\n\n**MANDATORY Skill tool call: `code-hardcode-audit`**  activate NOW before release.\n\n### 1.4 Decision Capture\n\nWhen implementation requires a user decision:\n\n1. **Update spec.md** with pending decision:\n\n   ```markdown\n   ## Pending Decision\n\n   **Topic**: [What needs to be decided]\n   **Options**:\n\n   - A: [Option A description]\n   - B: [Option B description]\n     **Context**: [Why this decision is needed now]\n     **Blocked task**: [Current task waiting on this]\n   ```\n\n2. **Update todo list**: Mark current task as `blocked: awaiting decision`\n\n3. **Then ask**: Use AskUserQuestion with clear options\n\n4. **After answer**:\n   - Remove `## Pending Decision` section from spec.md\n   - Update Decision Log in ADR\n   - Mark task as `in_progress` again\n   - Continue implementation\n\n### 1.5 Status Synchronization Protocol\n\n**Rule**: Spec `implementation-status` drives ADR `status` updates.\n\n| Spec Status            |    | ADR Status    | When                             |\n| ---------------------- | --- | ------------- | -------------------------------- |\n| `in_progress`          |    | `accepted`    | Phase 1 starts                   |\n| `blocked`              |    | `accepted`    | (no change, still accepted)      |\n| `completed`            |    | `accepted`    | Phase 1/2 complete, not released |\n| `completed` + released |    | `implemented` | Phase 3 complete                 |\n| `abandoned`            |    | `rejected`    | Work stopped                     |\n\n**At Phase 1 start** (immediately upon entering Phase 1, BEFORE executing first task):\n\n```bash\n# Update ADR status: proposed  accepted\nsed -i '' 's/^status: proposed/status: accepted/' docs/adr/$ADR_ID.md\n# Update spec phase\nsed -i '' 's/^phase: preflight/phase: phase-1/' docs/design/$ADR_ID/spec.md\n```\n\n**Before Phase 2** (sync checklist):\n\n- [ ] ADR `status: accepted`\n- [ ] Spec `implementation-status: in_progress` or `completed`\n- [ ] Spec `phase: phase-1`\n- [ ] Spec `last-updated: YYYY-MM-DD` is current\n\n### Phase 1 Success Criteria\n\n- [ ] Implementation complete per spec.md\n- [ ] All artifacts synced (ADR  spec  todo  code)\n- [ ] New files include `ADR: {adr-id}` in file header\n- [ ] Non-obvious changes have inline `ADR:` comments\n\n---\n\n## Phase 2: Format & Push\n\n### 2.1 Format Markdown\n\nRun Prettier against ADR and spec:\n\n```bash\nprettier --write --no-config --parser markdown --prose-wrap preserve \\\n  docs/adr/$ADR_ID.md \\\n  docs/design/$ADR_ID/spec.md\n```\n\n### 2.2 Push to GitHub\n\n```bash\n/usr/bin/env bash << 'GIT_EOF_2'\ngit add docs/adr/$ADR_ID.md docs/design/$ADR_ID/\ngit commit -m \"docs: add ADR and design spec for <slug>\"\n\n# If --branch was used:\ngit push -u origin <type>/$ADR_ID\n\n# If working on current branch (default):\ngit push origin $(git branch --show-current)\nGIT_EOF_2\n```\n\n### 2.3 Open in Browser\n\n```bash\n/usr/bin/env bash << 'GIT_EOF_3'\n# Get repo URL from origin remote (works correctly with forks)\nREMOTE_URL=$(git remote get-url origin 2>/dev/null)\n\nif [[ -z \"$REMOTE_URL\" ]]; then\n  echo \"Error: No origin remote configured\"\n  exit 1\nfi\n\n# Convert SSH format to HTTPS for browser URLs\n# Handles: git@github.com:owner/repo.git\n# Handles: git@github.com-username:owner/repo.git (multi-account SSH aliases)\n# Handles: https://github.com/owner/repo.git\nREPO_URL=$(echo \"$REMOTE_URL\" | sed -E 's|git@github\\.com[^:]*:|https://github.com/|' | sed 's|\\.git$||')\n\nBRANCH=$(git branch --show-current)\n\nopen \"$REPO_URL/blob/$BRANCH/docs/adr/$ADR_ID.md\"\nopen \"$REPO_URL/blob/$BRANCH/docs/design/$ADR_ID/spec.md\"\nGIT_EOF_3\n```\n\n### Phase 2 Success Criteria\n\n- [ ] Markdown formatted with Prettier\n- [ ] Pushed to GitHub\n- [ ] Files viewable in browser\n\n---\n\n## Phase 3: Release & Publish (Requires -r or -p Flag on Main)\n\n**Phase 3 requires EXPLICIT flags. It does NOT run automatically.**\n\n### Entry Gate Logic\n\nParse flags from invocation:\n\n- `RELEASE_FLAG`: true if `-r` or `--release` provided\n- `PUBLISH_FLAG`: true if `-p` or `--publish` provided\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF_2'\n# Check branch\nCURRENT_BRANCH=$(git branch --show-current)\n\n# Parse flags\nRELEASE_FLAG=false\nPUBLISH_FLAG=false\n[[ \"$ARGUMENTS\" =~ -r|--release ]] && RELEASE_FLAG=true\n[[ \"$ARGUMENTS\" =~ -p|--publish ]] && PUBLISH_FLAG=true\nPREFLIGHT_EOF_2\n```\n\n**Case 1: Feature Branch (not main/master)**\n\n```bash\nif [ \"$CURRENT_BRANCH\" != \"main\" ] && [ \"$CURRENT_BRANCH\" != \"master\" ]; then\n  echo \"\"\n  echo \"\"\n  if [ \"$RELEASE_FLAG\" = true ] || [ \"$PUBLISH_FLAG\" = true ]; then\n    # Verbose reminder when flags provided on feature branch\n    echo \"    PHASE 3 DEFERRED (Feature Branch)\"\n    echo \"\"\n    echo \"\"\n    echo \"  You provided release/publish flags on a feature branch:\"\n    [ \"$RELEASE_FLAG\" = true ] && echo \"    -r (release): YES\"\n    [ \"$PUBLISH_FLAG\" = true ] && echo \"    -p (publish): YES\"\n    echo \"\"\n    echo \"  Current branch: $CURRENT_BRANCH\"\n    echo \"\"\n    echo \"  Phase 3 CANNOT run on feature branches.\"\n    echo \"  These flags are recorded as YOUR INTENT for after merge.\"\n    echo \"\"\n    echo \"  \"\n    echo \"   NEXT STEPS (you must do these manually):                \"\n    echo \"  \"\n    echo \"   1. Create PR: gh pr create                              \"\n    echo \"   2. Get approval and merge to main/master                \"\n    echo \"   3. Switch: git checkout main && git pull                \"\n    [ \"$RELEASE_FLAG\" = true ] && echo \"   4. Release: /itp:go -r    # semantic-release           \"\n    [ \"$PUBLISH_FLAG\" = true ] && echo \"   5. Publish: /itp:go -p    # PyPI publish               \"\n    echo \"                                                           \"\n    echo \"   Or combine: /itp:go -r -p    # for both                 \"\n    echo \"  \"\n    echo \"\"\n    echo \"  The release/publish steps will NOT happen automatically.\"\n    echo \"  You MUST manually run them after merging to main.\"\n  else\n    # Standard feature branch message (no flags)\n    echo \"   WORKFLOW COMPLETE (Phase 2)\"\n    echo \"\"\n    echo \"\"\n    echo \"  Current branch: $CURRENT_BRANCH\"\n    echo \"  Phase 3 (Release): SKIPPED - not on main/master\"\n    echo \"\"\n    echo \"  Next steps:\"\n    echo \"    1. Create PR: gh pr create\"\n    echo \"    2. Get approval and merge to main/master\"\n    echo \"    3. Run /itp:go -r on main to release (or /itp:go -r -p for both)\"\n  fi\n  echo \"\"\n  echo \"\"\n  exit 0\nfi\n```\n\n**Case 2: Main/Master WITHOUT flags**\n\n```bash\nif [ \"$RELEASE_FLAG\" = false ] && [ \"$PUBLISH_FLAG\" = false ]; then\n  echo \"\"\n  echo \"\"\n  echo \"    PHASE 3 SKIPPED (No Flags)\"\n  echo \"\"\n  echo \"\"\n  echo \"  You are on: $CURRENT_BRANCH\"\n  echo \"  But no release/publish flags were provided.\"\n  echo \"\"\n  echo \"  To release this version, run one of:\"\n  echo \"    /itp:go -r       # semantic-release (version + changelog + GitHub)\"\n  echo \"    /itp:go -p       # PyPI publishing (if applicable)\"\n  echo \"    /itp:go -r -p    # both release and publish\"\n  echo \"\"\n  echo \"  Phase 3 requires explicit intent via flags.\"\n  echo \"\"\n  echo \"\"\n  exit 0\nfi\n```\n\n**Case 3: Main/Master WITH flags**  Proceed to Phase 3 subsections below.\n\n### 3.1 Pre-Release Verification\n\nBefore releasing:\n\n- [ ] All Success Criteria items in design spec are checked off\n- [ ] Status value in design spec is updated (e.g., `Accepted`, `Implemented`)\n- [ ] ADR and spec.md are in sync with final implementation\n- [ ] Version fields use `semantic-release` patterns (no dynamic versioning)\n\n### 3.2 Semantic Release (if -r flag)\n\n**Condition**: Only execute if `-r` or `--release` flag was provided.\n\n```bash\nif [ \"$RELEASE_FLAG\" = true ]; then\n  # Proceed with semantic-release\nfi\n```\n\n**MANDATORY Skill tool call: `semantic-release`**  activate NOW for version tagging and release.\n\n- Follow the [Local Release Workflow](../skills/semantic-release/references/local-release-workflow.md)\n- Conventional commits  tag  release  changelog  push\n\n### 3.3 PyPI Publishing (if -p flag)\n\n**Condition**: Only execute if `-p` or `--publish` flag was provided.\n\n```bash\nif [ \"$PUBLISH_FLAG\" = true ]; then\n  # Proceed with PyPI publishing\nfi\n```\n\nOnly if package pre-exists on PyPI:\n\n- **MANDATORY Skill tool call: `pypi-doppler`**  activate NOW to publish\n\n### 3.4 Earthly Pipeline\n\nUse Earthly as canonical pipeline:\n\n- Non-blocking, observability-first\n- Ensure GitHub Release exists\n- Record stats/errors\n- Pushover alert\n- Wire into GitHub Actions\n\n### Phase 3 Success Criteria\n\n- [ ] ADR status updated to `accepted` or `implemented`\n- [ ] Release completed via semantic-release\n- [ ] If feature branch: PR created, Phase 3 skipped"
              },
              {
                "name": "/hooks",
                "description": "Install/uninstall itp-hooks (ASCII guard, ADR sync reminder, fake-data-guard) to ~/.claude/settings.json",
                "path": "plugins/itp/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall itp-hooks (ASCII guard, ADR sync reminder, fake-data-guard) to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead",
                  "argument-hint": "[install|uninstall|status|restore [latest|<n>]]"
                },
                "content": "<!--\nADR: 2025-12-07-itp-hooks-settings-installer\n-->\n\n# ITP Hooks Manager\n\nManage itp-hooks installation in `~/.claude/settings.json`.\n\nClaude Code only loads hooks from settings.json, not from plugin.json files. This command installs/uninstalls three itp-hooks:\n\n- **PreToolUse guard** - Blocks ASCII diagrams without graph-easy source blocks\n- **PostToolUse reminder** - Prompts ADR/spec sync after file modifications\n- **Fake-data-guard** - Detects fake/synthetic data patterns (np.random, Faker, etc.) in new Python files\n\n## Actions\n\n| Action           | Description                         |\n| ---------------- | ----------------------------------- |\n| `status`         | Show current installation state     |\n| `install`        | Add itp-hooks to settings.json      |\n| `uninstall`      | Remove itp-hooks from settings.json |\n| `restore`        | List available backups with numbers |\n| `restore latest` | Restore most recent backup          |\n| `restore <n>`    | Restore backup by number            |\n\n## Execution\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'HOOKS_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\nACTION=\"${ARGUMENTS:-status}\"\nbash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" $ACTION\nHOOKS_SCRIPT_EOF\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall/restore operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe hooks are loaded at session start. Modifications to settings.json require a restart."
              },
              {
                "name": "/release",
                "description": "Run semantic-release with preflight checks. TRIGGERS - npm run release, version bump, changelog, release automation.",
                "path": "plugins/itp/commands/release.md",
                "frontmatter": {
                  "name": "release",
                  "description": "Run semantic-release with preflight checks. TRIGGERS - npm run release, version bump, changelog, release automation.",
                  "allowed-tools": "Read, Bash, Glob, Grep, Edit, AskUserQuestion, TodoWrite",
                  "argument-hint": "[--dry] [--skip-preflight]"
                },
                "content": "<!--  MANDATORY: LOAD THE SEMANTIC-RELEASE SKILL FIRST  -->\n\n# /itp:release\n\n**FIRST ACTION**: Read the semantic-release skill to load the complete workflow knowledge:\n\n```\nRead: ${CLAUDE_PLUGIN_ROOT}/skills/semantic-release/SKILL.md\nRead: ${CLAUDE_PLUGIN_ROOT}/skills/semantic-release/references/local-release-workflow.md\n```\n\nThis command wraps the [semantic-release skill](../skills/semantic-release/SKILL.md) with automatic preflight validation.\n\n## Arguments\n\n| Flag              | Short | Description                                      |\n| ----------------- | ----- | ------------------------------------------------ |\n| `--dry`           | `-d`  | Dry-run mode (preview changes, no modifications) |\n| `--skip-preflight`| `-s`  | Skip preflight checks (use with caution)         |\n\n## Examples\n\n```bash\n/itp:release          # Full release with preflight\n/itp:release --dry    # Preview what would be released\n/itp:release -d       # Same as --dry\n```\n\n---\n\n##  MANDATORY: Load Skill Knowledge First\n\nBefore executing ANY release steps, you MUST read these files to load the semantic-release skill:\n\n1. **SKILL.md**  Core workflow, conventional commits, MAJOR confirmation\n2. **local-release-workflow.md**  4-phase release process (PREFLIGHT  SYNC  RELEASE  POSTFLIGHT)\n\n```bash\n# Environment-agnostic paths\nSKILL_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/cache/cc-skills/itp/*/skills/semantic-release}\"\n```\n\n**After reading the skill files, follow the Local Release Workflow (4 phases).**\n\n---\n\n## Execution Flow (from skill)\n\n```\n                 Release Workflow Pipeline\n\n -----------      +------+     +---------+      ------------\n| PREFLIGHT | --> | SYNC | --> | RELEASE | --> | POSTFLIGHT |\n -----------      +------+     +---------+      ------------\n```\n\n### Phase 1: Preflight\n\n**From skill: Section 1.1-1.5**\n\n1. **Git Cache Refresh** (MANDATORY first step)\n   ```bash\n   git update-index --refresh -q || true\n   ```\n\n2. **Tooling Check**  gh CLI, semantic-release, git repo, main branch, clean directory\n\n3. **Authentication Check**  Verify correct GitHub account via `gh api user --jq '.login'`\n\n4. **Releasable Commits Validation**  Must have `feat:`, `fix:`, or `BREAKING CHANGE:` since last tag\n\n5. **MAJOR Version Confirmation**  If breaking changes detected, spawn 3 Task subagents + AskUserQuestion\n\n### Phase 2: Sync\n\n```bash\ngit pull --rebase origin main\ngit push origin main\n```\n\n### Phase 3: Release\n\n**If --dry flag:**\n```bash\nnpm run release:dry\n```\n\n**Production:**\n```bash\nnpm run release\n```\n\n### Phase 4: Postflight\n\n1. Verify pristine state: `git status --porcelain`\n2. Verify release: `gh release list --limit 1`\n3. Update tracking refs: `git fetch origin main:refs/remotes/origin/main --no-tags`\n4. Plugin cache sync (cc-skills only): Automatic via successCmd\n\n---\n\n## Quick Reference\n\n| Scenario                  | Command               | Result                              |\n| ------------------------- | --------------------- | ----------------------------------- |\n| Standard release          | `/itp:release`        | Load skill  4-phase workflow       |\n| Preview changes           | `/itp:release --dry`  | Load skill  Dry-run only           |\n| Force release (dangerous) | `/itp:release -s`     | Skip preflight  Release            |\n\n---\n\n## Error Recovery (from skill)\n\n| Error                        | Resolution                                    |\n| ---------------------------- | --------------------------------------------- |\n| Working directory not clean  | `git stash` or `git commit`                   |\n| Not on main branch           | `git checkout main`                           |\n| Wrong GitHub account         | `gh auth switch --user <correct-account>`     |\n| No releasable commits        | Create a `feat:` or `fix:` commit first       |\n| MAJOR version detected       | Follow skill's multi-perspective analysis     |\n| Release failed               | Check [Troubleshooting](../skills/semantic-release/references/troubleshooting.md) |\n\n---\n\n## Skill Reference (MUST READ)\n\n- **[semantic-release SKILL](../skills/semantic-release/SKILL.md)**  Full documentation, MAJOR confirmation workflow\n- **[Local Release Workflow](../skills/semantic-release/references/local-release-workflow.md)**  Canonical 4-phase process\n- [Troubleshooting](../skills/semantic-release/references/troubleshooting.md)  Common issues and solutions\n- [Authentication](../skills/semantic-release/references/authentication.md)  Multi-account GitHub setup"
              },
              {
                "name": "/setup",
                "description": "SETUP COMMAND - Execute TodoWrite FIRST, then Check -> Gate -> Install -> Verify",
                "path": "plugins/itp/commands/setup.md",
                "frontmatter": {
                  "description": "SETUP COMMAND - Execute TodoWrite FIRST, then Check -> Gate -> Install -> Verify",
                  "allowed-tools": "Read, Bash(brew:*), Bash(npm:*), Bash(cpanm:*), Bash(uv:*), Bash(which:*), Bash(command -v:*), Bash(PLUGIN_DIR:*), Bash(source:*), AskUserQuestion, TodoWrite, TodoRead"
                },
                "content": "<!--\nADR: 2025-12-05-itp-setup-todowrite-workflow\n-->\n\n# ITP Setup\n\nVerify and install dependencies required by the `/itp:go` workflow using TodoWrite-driven interactive workflow.\n\n---\n\n## MANDATORY FIRST ACTION\n\n**YOUR FIRST ACTION MUST BE TodoWrite with the template below.**\n\nDO NOT:\n\n- Run any checks before TodoWrite\n- Skip the interactive gate\n- Install without user confirmation\n\n**Execute this TodoWrite template EXACTLY:**\n\n```\nTodoWrite with todos:\n- \"Setup: Detect platform (macOS/Linux)\" | pending | \"Detecting platform\"\n- \"Setup: Check Core Tools (uv, gh, prettier)\" | pending | \"Checking Core Tools\"\n- \"Setup: Check ADR Diagram Tools (cpanm, graph-easy)\" | pending | \"Checking ADR Tools\"\n- \"Setup: Check Code Audit Tools (ruff, semgrep, jscpd, gitleaks)\" | pending | \"Checking Audit Tools\"\n- \"Setup: Check Release Tools (node, semantic-release)\" | pending | \"Checking Release Tools\"\n- \"Setup: Present findings and disclaimer\" | pending | \"Presenting findings\"\n- \"Setup: GATE - Await user decision\" | pending | \"Awaiting user decision\"\n- \"Setup: Install missing tools (if confirmed)\" | pending | \"Installing missing tools\"\n- \"Setup: Verify installation\" | pending | \"Verifying installation\"\n```\n\n**After TodoWrite completes, proceed to Phase 1 below.**\n\n---\n\n## Phase 1: Preflight Check\n\nMark each todo as `in_progress` before starting, `completed` when done.\n\n### Todo 1: Detect Platform\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\nsource \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --detect-only\nSETUP_EOF\n```\n\nPlatform detection sets: `OS`, `PM` (package manager), `HAS_MISE`\n\n### Todo 2: Check Core Tools\n\nCheck each tool using `command -v`:\n\n| Tool     | Check                 | Required |\n| -------- | --------------------- | -------- |\n| uv       | `command -v uv`       | Yes      |\n| gh       | `command -v gh`       | Yes      |\n| prettier | `command -v prettier` | Yes      |\n\nRecord findings:\n\n- Found: `[OK] uv (installed)` -> mark completed\n- Missing: `[x] prettier (missing)` -> note for Phase 3\n\n### Todo 3: Check ADR Diagram Tools\n\n| Tool       | Check                             | Required     |\n| ---------- | --------------------------------- | ------------ |\n| cpanm      | `command -v cpanm`                | For diagrams |\n| graph-easy | `echo \"[A]\" \\| graph-easy` (test) | For diagrams |\n\n### Todo 4: Check Code Audit Tools\n\n| Tool     | Check                 | Required        |\n| -------- | --------------------- | --------------- |\n| ruff     | `command -v ruff`     | For code-audit  |\n| semgrep  | `command -v semgrep`  | For code-audit  |\n| jscpd    | `command -v jscpd`    | For code-audit  |\n| gitleaks | `command -v gitleaks` | For secret-scan |\n\n### Todo 5: Check Release Tools\n\n| Tool             | Check                            | Required      |\n| ---------------- | -------------------------------- | ------------- |\n| node             | `command -v node`                | For release   |\n| semantic-release | `npx semantic-release --version` | For release   |\n| doppler          | `command -v doppler`             | For PyPI only |\n\n---\n\n## Phase 2: Present Findings (Interactive Gate)\n\n### Todo 6: Present Findings\n\n**IMPORTANT: Use mise-first commands when available**\n\nWhen presenting missing tool installation commands:\n\n- If `HAS_MISE=true` (detected in Todo 1): Show mise commands\n- If `HAS_MISE=false`: Show platform package manager commands (brew/apt)\n\n**Mise command reference (use when HAS_MISE=true):**\n\n| Tool     | mise command                     | Notes                          |\n| -------- | -------------------------------- | ------------------------------ |\n| gitleaks | `mise use --global gitleaks`     |                                |\n| ruff     | `mise use --global ruff`         |                                |\n| uv       | `mise use --global uv`           |                                |\n| gh       | `brew install gh`                | **NEVER mise** (iTerm2 issues) |\n| semgrep  | `mise use --global semgrep`      |                                |\n| node     | `mise use --global node`         |                                |\n| doppler  | `mise use --global doppler`      |                                |\n| prettier | `mise use --global npm:prettier` |                                |\n| jscpd    | `npm i -g jscpd` (npm only)      |                                |\n\n> **Warning**: gh CLI must be installed via Homebrew, not mise. mise-installed gh causes iTerm2 tab spawning issues with Claude Code. [ADR](/docs/adr/2026-01-12-mise-gh-cli-incompatibility.md)\n\n**Display summary format (versions derived from actual tool output):**\n\n```\n=== SETUP PREFLIGHT COMPLETE ===\n\nFound: X tools | Missing: Y tools\n\nYour existing installations:\n[OK] uv (<derived from: uv --version>)\n[OK] gh (<derived from: gh --version>)\n[x] gitleaks (missing)\n...\n\nNote: This plugin is developed against latest tool versions.\nYour existing installations are respected.\n\nMissing tools will be installed via mise (detected):\n  gitleaks -> mise use --global gitleaks\n```\n\n**If HAS_MISE=false, show platform commands instead:**\n\n```\nMissing tools will be installed via brew:\n  gitleaks -> brew install gitleaks\n```\n\n**IMPORTANT**: Version numbers must be derived dynamically from running the actual tool's version command. Never hardcode version numbers.\n\n### Todo 7: GATE - Await User Decision\n\n**If missing tools exist, STOP and ask user:**\n\nUse AskUserQuestion with these options:\n\n```\nquestion: \"Would you like to install the missing tools?\"\nheader: \"Install\"\noptions:\n  - label: \"Install missing\"\n    description: \"Automatically install all missing tools\"\n  - label: \"Skip\"\n    description: \"Show manual install commands and exit\"\n```\n\n**IMPORTANT**: Do NOT proceed to Phase 3 until user responds.\n\n**If ALL tools present**: Mark todo completed, skip to \"All set!\" message, mark todos 8-9 as N/A.\n\n---\n\n## Phase 3: Installation (Conditional)\n\n### Todo 8: Install Missing Tools\n\n**Only execute if**:\n\n- User selected \"Install missing\"\n- OR `--install` flag was passed (skip interactive gate)\n\nRun installation commands for missing tools only:\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF_2'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\nbash \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --install\nSETUP_EOF_2\n```\n\n**If user selected \"Skip\"**:\n\n- Display manual install commands\n- Mark todo as skipped\n- Exit cleanly\n\n### Todo 9: Verify Installation\n\nRe-run checks to confirm tools are now available:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\nbash \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --check\nPREFLIGHT_EOF\n```\n\nMark todo completed only if verification passes.\n\n---\n\n## Flag Handling\n\n| Flag        | Behavior                                    |\n| ----------- | ------------------------------------------- |\n| (none)      | Default: Check -> Gate -> Ask permission    |\n| `--check`   | Same as default (hidden alias)              |\n| `--install` | Check -> Skip gate -> Install automatically |\n| `--yes`     | Alias for `--install`                       |\n\nParse `$ARGUMENTS` for flags:\n\n```bash\ncase \"$ARGUMENTS\" in\n  *--install*|*--yes*)\n    SKIP_GATE=true\n    ;;\n  *)\n    SKIP_GATE=false\n    ;;\nesac\n```\n\n---\n\n## Edge Cases\n\n| Case                              | Handling                                                          |\n| --------------------------------- | ----------------------------------------------------------------- |\n| All tools present                 | Todos 1-6 complete, Todo 7 shows \"All set!\", Todos 8-9 marked N/A |\n| Some missing, user says \"install\" | Todos 8-9 execute normally                                        |\n| Some missing, user says \"skip\"    | Show manual commands, mark todos 8-9 as skipped                   |\n| `--install` flag passed           | Skip Todo 7 gate, proceed directly to install                     |\n| macOS vs Linux                    | Todo 1 detects platform, install commands adapt                   |\n\n---\n\n## Troubleshooting\n\n### graph-easy fails to install\n\n```bash\n# Ensure cpanminus is installed first\nbrew install cpanminus\n\n# Then install Graph::Easy\ncpanm Graph::Easy\n```\n\n### semantic-release not found\n\n```bash\n# Install globally with npm\nnpm i -g semantic-release@25\n\n# Or use npx (no global install needed)\nnpx semantic-release --version\n```\n\n### Permission errors with npm\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF'\n# Fix npm permissions\nmkdir -p ~/.npm-global\nnpm config set prefix '~/.npm-global'\n\n# Add to your shell config\nSHELL_RC=\"$([[ \"$SHELL\" == */zsh ]] && echo ~/.zshrc || echo ~/.bashrc)\"\necho 'export PATH=~/.npm-global/bin:$PATH' >> \"$SHELL_RC\"\nsource \"$SHELL_RC\"\nCONFIG_EOF\n```\n\n---\n\n## Next Steps\n\nAfter setup completes, configure itp-hooks for enhanced workflow guidance:\n\n1. **Check hook status**:\n\n   ```bash\n   /itp:hooks status\n   ```\n\n2. **Install hooks** (if not already installed):\n\n   ```bash\n   /itp:hooks install\n   ```\n\n### What hooks provide\n\n- **PreToolUse guard**: Blocks Unicode box-drawing diagrams without `<details>` source blocks\n- **PostToolUse reminder**: Prompts ADR sync and graph-easy skill usage\n\n**IMPORTANT:** Hooks require a Claude Code session restart after installation."
              }
            ],
            "skills": [
              {
                "name": "adr-code-traceability",
                "description": "Add ADR references to code for decision traceability. Use when creating new files, documenting non-obvious implementation choices, or when user mentions ADR traceability, code reference, or document decision in code. Provides language-specific patterns for Python, TypeScript, Rust, Go.",
                "path": "plugins/itp/skills/adr-code-traceability/SKILL.md",
                "frontmatter": {
                  "name": "adr-code-traceability",
                  "description": "Add ADR references to code for decision traceability. Use when creating new files, documenting non-obvious implementation choices, or when user mentions ADR traceability, code reference, or document decision in code. Provides language-specific patterns for Python, TypeScript, Rust, Go."
                },
                "content": "# ADR Code Traceability\n\nAdd Architecture Decision Record references to code for decision traceability. Provides language-specific patterns and placement guidelines.\n\n## When to Use This Skill\n\n- Creating new files as part of an ADR implementation\n- Documenting non-obvious implementation choices\n- User mentions \"ADR traceability\", \"code reference\", \"document decision\"\n- Adding decision context to code during `/itp:go` Phase 1\n\n## Quick Reference\n\n### Reference Format\n\n```\nADR: {adr-id}\n```\n\n**Path Derivation**: `ADR: 2025-12-01-my-feature`  `/docs/adr/2025-12-01-my-feature.md`\n\n### Language Patterns (Summary)\n\n| Language   | New File Header                      | Inline Comment              |\n| ---------- | ------------------------------------ | --------------------------- |\n| Python     | `\"\"\"...\\n\\nADR: {adr-id}\\n\"\"\"`       | `# ADR: {adr-id} - reason`  |\n| TypeScript | `/** ... \\n * @see ADR: {adr-id} */` | `// ADR: {adr-id} - reason` |\n| Rust       | `//! ...\\n//! ADR: {adr-id}`         | `// ADR: {adr-id} - reason` |\n| Go         | `// Package ... \\n// ADR: {adr-id}`  | `// ADR: {adr-id} - reason` |\n\nSee [Language Patterns](./references/language-patterns.md) for complete examples.\n\n---\n\n## Placement Decision Tree\n\n```\nIs this a NEW file created by the ADR?\n Yes  Add reference in file header\n No  Is the change non-obvious?\n     Yes  Add inline comment with reason\n     No  Skip ADR reference\n```\n\nSee [Placement Guidelines](./references/placement-guidelines.md) for detailed guidance.\n\n---\n\n## Examples\n\n### New File (Python)\n\n```python\n\"\"\"\nRedis cache adapter for session management.\n\nADR: 2025-12-01-redis-session-cache\n\"\"\"\n\nclass RedisSessionCache:\n    ...\n```\n\n### Inline Comment (TypeScript)\n\n```typescript\n// ADR: 2025-12-01-rate-limiting - Using token bucket over sliding window\n// for better burst handling in our use case\nconst rateLimiter = new TokenBucketLimiter({ rate: 100, burst: 20 });\n```\n\n---\n\n## Do NOT Add References For\n\n- Every line touched (only where traceability adds value)\n- Trivial changes (formatting, typo fixes)\n- Standard patterns (well-known idioms)\n- Test files (unless test approach is an ADR decision)\n\n---\n\n## Reference Documentation\n\n- [Language Patterns](./references/language-patterns.md) - Python, TS, Rust, Go patterns\n- [Placement Guidelines](./references/placement-guidelines.md) - When and where to add"
              },
              {
                "name": "adr-graph-easy-architect",
                "description": "Create ASCII architecture diagrams for ADRs using graph-easy. Use when writing ADRs, adding diagrams to existing ADRs, or when user mentions \"ADR diagram\", \"architecture diagram\", \"ASCII diagram\". Zero external dependencies for rendering - pure text output embeds directly in markdown.",
                "path": "plugins/itp/skills/adr-graph-easy-architect/SKILL.md",
                "frontmatter": {
                  "name": "adr-graph-easy-architect",
                  "description": "Create ASCII architecture diagrams for ADRs using graph-easy. Use when writing ADRs, adding diagrams to existing ADRs, or when user mentions \"ADR diagram\", \"architecture diagram\", \"ASCII diagram\". Zero external dependencies for rendering - pure text output embeds directly in markdown."
                },
                "content": "# ADR Graph-Easy Architect\n\nCreate comprehensive ASCII architecture diagrams for Architecture Decision Records (ADRs) using graph-easy. Pure text output with automatic layout - no image rendering required.\n\n## When to Use This Skill\n\n- Writing new ADR that involves architectural changes\n- ADR describes migration, integration, or system changes\n- User asks for visual representation of a decision\n- Existing ADR diagram needs review or update\n\n## Preflight Check\n\nRun these checks in order. Each layer depends on the previous.\n\n### Layer 1: Package Manager\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\n# Detect OS and set package manager\ncase \"$(uname -s)\" in\n  Darwin) PM=\"brew\" ;;\n  Linux)  PM=\"apt\" ;;\n  *)      echo \"ERROR: Unsupported OS (require macOS or Linux)\"; exit 1 ;;\nesac\ncommand -v $PM &>/dev/null || { echo \"ERROR: $PM not installed\"; exit 1; }\necho \" Package manager: $PM\"\nSETUP_EOF\n```\n\n### Layer 2: Perl + cpanminus (mise-first approach)\n\n```bash\n# Prefer mise for unified tool management\nif command -v mise &>/dev/null; then\n  # Install Perl via mise\n  mise which perl &>/dev/null || mise install perl\n  # Install cpanminus under mise perl\n  mise exec perl -- cpanm --version &>/dev/null 2>&1 || {\n    echo \"Installing cpanminus under mise perl...\"\n    mise exec perl -- curl -L https://cpanmin.us | mise exec perl -- perl - App::cpanminus\n  }\n  echo \" cpanminus installed (via mise perl)\"\nelse\n  # Fallback: Install cpanminus via system package manager\n  command -v cpanm &>/dev/null || {\n    echo \"Installing cpanminus via $PM...\"\n    case \"$PM\" in\n      brew) brew install cpanminus ;;\n      apt)  sudo apt install -y cpanminus ;;\n    esac\n  }\n  echo \" cpanminus installed\"\nfi\n```\n\n### Layer 3: Graph::Easy Perl module\n\n```bash\n# Check if Graph::Easy is installed (mise-first)\nif command -v mise &>/dev/null; then\n  mise exec perl -- perl -MGraph::Easy -e1 2>/dev/null || {\n    echo \"Installing Graph::Easy via mise perl cpanm...\"\n    mise exec perl -- cpanm Graph::Easy\n  }\n  echo \" Graph::Easy installed (via mise perl)\"\nelse\n  perl -MGraph::Easy -e1 2>/dev/null || {\n    echo \"Installing Graph::Easy via cpanm...\"\n    cpanm Graph::Easy\n  }\n  echo \" Graph::Easy installed\"\nfi\n```\n\n### Layer 4: Verify graph-easy is in PATH\n\n```bash\n# Verify graph-easy is accessible and functional\ncommand -v graph-easy &>/dev/null || {\n  echo \"ERROR: graph-easy not found in PATH\"\n  exit 1\n}\n# Test actual functionality (--version exits with code 2, unreliable)\necho \"[Test] -> [OK]\" | graph-easy &>/dev/null && echo \" graph-easy ready\"\n```\n\n### All-in-One Preflight Script\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Copy-paste this entire block to ensure graph-easy is ready (macOS + Linux)\n# Prefers mise for unified cross-platform tool management\n\n# Check for mise first (recommended)\nif command -v mise &>/dev/null; then\n  echo \"Using mise for Perl management...\"\n  mise which perl &>/dev/null || mise install perl\n  mise exec perl -- cpanm --version &>/dev/null 2>&1 || \\\n    mise exec perl -- curl -L https://cpanmin.us | mise exec perl -- perl - App::cpanminus\n  mise exec perl -- perl -MGraph::Easy -e1 2>/dev/null || mise exec perl -- cpanm Graph::Easy\nelse\n  # Fallback: system package manager\n  echo \" Tip: Install mise for unified tool management: curl https://mise.run | sh\"\n  case \"$(uname -s)\" in\n    Darwin) PM=\"brew\" ;;\n    Linux)  PM=\"apt\" ;;\n    *)      echo \"ERROR: Unsupported OS\"; exit 1 ;;\n  esac\n  command -v $PM &>/dev/null || { echo \"ERROR: $PM not installed\"; exit 1; }\n  command -v cpanm &>/dev/null || { [ \"$PM\" = \"apt\" ] && sudo apt install -y cpanminus || brew install cpanminus; }\n  perl -MGraph::Easy -e1 2>/dev/null || cpanm Graph::Easy\nfi\n\n# Verify graph-easy is in PATH and functional\ncommand -v graph-easy &>/dev/null || {\n  echo \"ERROR: graph-easy not in PATH after installation\"\n  exit 1\n}\n# Test actual functionality (--version exits with code 2, unreliable)\necho \"[Test] -> [OK]\" | graph-easy &>/dev/null && echo \" graph-easy ready\"\nPREFLIGHT_EOF\n```\n\n---\n\n## Part 1: DSL Syntax\n\n### Basic Elements\n\n```\n# Nodes (square brackets)\n[Node Name]\n\n# Edges (arrows)\n[A] -> [B]\n\n# Labeled edges\n[A] -- label --> [B]\n\n# Bidirectional\n[A] <-> [B]\n\n# Chain\n[A] -> [B] -> [C]\n```\n\n### Groups (Containers)\n\n```\n# Named group with dashed border\n( Group Name:\n  [Node A]\n  [Node B]\n)\n\n# Nested connections\n( Before:\n  [Old System]\n)\n( After:\n  [New System]\n)\n[Before] -> [After]\n```\n\n### Node Labels\n\n```\n# Custom label (different from ID)\n[db] { label: \"PostgreSQL Database\"; }\n\n# ASCII markers for visual distinction INSIDE boxes\n# (emojis break box alignment - use ASCII markers instead)\n[deleted] { label: \"[x] Old Component\"; }\n[added] { label: \"[+] New Component\"; }\n[warning] { label: \"[!] Deprecated\"; }\n[success] { label: \"[OK] Passed\"; }\n```\n\n**Character rules for nodes:**\n\n- Graphical emojis (   ) - NEVER (double-width breaks box alignment)\n- Unicode symbols (    ) - OK (single-width, safe)\n- ASCII markers ([x] [+] [!] :) ) - ALWAYS safe (monospace)\n\nUse `graph { label: \"...\"; }` for graphical emojis in title/legend.\n\n**Example: Emoji breaks alignment (DON'T DO THIS)**\n\n```\n# BAD - emoji inside node\n[rocket] { label: \" Launch\"; }\n```\n\nRenders broken:\n\n```\n\n  Launch     <-- box edge misaligned due to double-width emoji\n\n```\n\n**Example: ASCII marker preserves alignment (DO THIS)**\n\n```\n# GOOD - ASCII marker inside node\n[rocket] { label: \"[>] Launch\"; }\n```\n\nRenders correctly:\n\n```\n\n [>] Launch \n\n```\n\n**Example: Emoji safe in graph title (OK)**\n\n```\n# OK - emoji in graph label (outside boxes)\ngraph { label: \" Deployment Pipeline\"; flow: east; }\n[Build] -> [Test] -> [Deploy]\n```\n\nRenders correctly (emoji in title, not in boxes):\n\n```\n         Deployment Pipeline\n\n          \n Build  -->  Test  -->  Deploy \n          \n```\n\n### Flow Direction (MANDATORY: Always specify)\n\n```\n# MANDATORY: Always specify flow direction explicitly\ngraph { flow: south; }   # Top-to-bottom (architecture, decisions)\ngraph { flow: east; }    # Left-to-right (pipelines, sequences)\n```\n\nNever rely on default flow - explicit is clearer.\n\n### Graph Title and Legend (Outside Boxes - Emojis Safe Here)\n\nEmojis break alignment INSIDE boxes but are SAFE in graph titles/legends.\n\n**Emoji Selection Guide** - Choose emoji that matches diagram purpose:\n\n| Diagram Type             | Emoji | Example Title                |\n| ------------------------ | ----- | ---------------------------- |\n| Migration/Change         |     | `\" Database Migration\"`    |\n| Deployment/Release       |     | `\" Deployment Pipeline\"`   |\n| Data Flow                |     | `\" Data Ingestion Flow\"`   |\n| Security/Auth            |     | `\" Authentication Flow\"`   |\n| Error/Failure            |     | `\" Error Handling\"`        |\n| Decision/Branch          |     | `\" Routing Decision\"`      |\n| Architecture             |     | `\" System Architecture\"`   |\n| Network/API              |     | `\" API Integration\"`       |\n| Storage/Database         |     | `\" Storage Layer\"`         |\n| Monitoring/Observability |     | `\" Monitoring Stack\"`      |\n| Hook/Event               |     | `\" Hook Flow\"`             |\n| Before/After comparison  | / | `\" Before\"` / `\" After\"` |\n\n```\n# Title with semantic emoji\ngraph { label: \" Deployment Pipeline\"; flow: east; }\n\n# Title with legend (multiline using \\n)\ngraph { label: \" Hook Flow\\n\\n Allow   Deny   Warn\"; flow: south; }\n```\n\n**Rendered:**\n\n```\nHook Flow\n \n Allow  Deny  Warn\n\n   \n    Start \n   \n```\n\n**Rule**: Emojis ONLY in `graph { label: \"...\"; }` - NEVER inside `[ node ]`\n\n### Node Styling (Best Practices)\n\n```\n# Rounded corners for start/end nodes\n[ Start ] { shape: rounded; }\n[ End ] { shape: rounded; }\n\n# Double border for emphasis\n[ Critical Step ] { border: double; }\n\n# Bold border for important nodes\n[ Key Decision ] { border: bold; }\n\n# Dotted border for optional/skippable\n[ Optional ] { border: dotted; }\n\n# Multiline labels with \\n\n[ Hook Input\\n(stdin JSON) ]\n```\n\n**Rendered examples:**\n\n```\n              \n Rounded                Default \n              \n\n              \n Double                  Bold   \n              \n```\n\n> **Note:** Dotted borders (`{ border: dotted; }`) use `` characters that render inconsistently on GitHub. Use sparingly.\n\n### Edge Styles\n\n```\n[ A ] -> [ B ]      # Solid arrow (default)\n[ A ] ..> [ B ]     # Dotted arrow\n[ A ] ==> [ B ]     # Bold/double arrow\n[ A ] - -> [ B ]    # Dashed arrow\n[ A ] -- label --> [ B ]  # Labeled edge\n```\n\n---\n\n## Part 2: Common Diagram Patterns\n\n### Migration (Before  After)\n\n```\ngraph { flow: south; }\n[Before] -- migrate --> [After]\n```\n\n### Multi-Component System\n\n```\ngraph { flow: south; }\n[A] -> [B] -> [C]\n[B] -> [D]\n```\n\n### Pipeline (Left-to-Right)\n\n```\ngraph { flow: east; }\n[Input] -> [Process] -> [Output]\n```\n\n### Decision with Options\n\n```\ngraph { flow: south; }\n[Decision] -> [Option A]\n[Decision] -> [Option B]\n```\n\n### Grouped Components\n\n```\n( Group:\n  [Component 1]\n  [Component 2]\n)\n[External] -> [Component 1]\n```\n\n### Bidirectional Flow\n\n```\n[Client] <-> [Server]\n[Server] -> [Database]\n```\n\n---\n\n## Part 3: Rendering\n\n### Command (MANDATORY: Always use boxart)\n\n```bash\n# MANDATORY: Always use --as=boxart for clean output\ngraph-easy --as=boxart << 'EOF'\ngraph { flow: south; }\n[A] -> [B] -> [C]\nEOF\n```\n\n**Never use** `--as=ascii` - it produces ugly `+--+` boxes instead of clean `` lines.\n\n### Output Modes\n\n| Mode     | Command       | Usage                                |\n| -------- | ------------- | ------------------------------------ |\n| `boxart` | `--as=boxart` | MANDATORY - clean Unicode lines      |\n| `ascii`  | `--as=ascii`  | NEVER USE - ugly output, legacy only |\n\n### Validation Workflow\n\n```bash\n# 1. Write DSL to heredoc\n# 2. Render with boxart\ngraph-easy --as=boxart << 'EOF'\n[Your] -> [Diagram] -> [Here]\nEOF\n\n# 3. Review output\n# 4. Iterate if needed\n# 5. Copy final ASCII to ADR\n```\n\n---\n\n## Part 4: Embedding in ADR\n\n### Markdown Format (MANDATORY: Always Include Source)\n\n**CRITICAL**: Every rendered diagram MUST be followed by a collapsible `<details>` block containing the graph-easy source code. This is non-negotiable for:\n\n- **Reproducibility**: Future maintainers can regenerate the diagram\n- **Editability**: Source can be modified and re-rendered\n- **Auditability**: Changes to diagrams are trackable in git diffs\n\n````markdown\n## Architecture\n\n```\n          \n  Before   >   After    >  Database \n          \n```\n\n<details>\n<summary>graph-easy source</summary>\n\n```\ngraph { flow: east; }\n[Before] -> [After] -> [Database]\n```\n\n</details>\n````\n\n**The `<details>` block is MANDATORY** - never embed a diagram without its source.\n\n### GFM Collapsible Section Syntax\n\nGitHub Flavored Markdown supports HTML `<details>` and `<summary>` tags for collapsible sections. Key syntax rules:\n\n**Structure:**\n\n```html\n<details>\n  <summary>Click to expand</summary>\n\n  <!-- BLANK LINE REQUIRED HERE -->\n  Content goes here (Markdown supported)\n  <!-- BLANK LINE REQUIRED HERE -->\n</details>\n```\n\n**Critical rules:**\n\n1. **Blank lines required** - Must have empty line after `<summary>` and before `</details>` for Markdown to render\n2. **No indentation** - `<details>` and `<summary>` must be at column 0 (no leading spaces)\n3. **Summary is clickable label** - Text in `<summary>` appears as the collapsed header\n4. **Markdown inside works** - Code blocks, headers, lists all render correctly inside\n\n**Optional: Default expanded:**\n\n```html\n<details open>\n  <summary>Expanded by default</summary>\n\n  Content visible on page load\n</details>\n```\n\n**Common mistake (Markdown won't render):**\n\n```html\n<details>\n  <summary>Broken</summary>\n  No blank line - this won't render as Markdown!\n</details>\n```\n\n**References:**\n\n- [GitHub Docs: Collapsed sections](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-collapsed-sections)\n- [GFM details/summary gist](https://gist.github.com/scmx/eca72d44afee0113ceb0349dd54a84a2)\n\n### File Organization\n\nNo separate asset files needed - diagram is inline in the markdown.\n\n### Regeneration\n\nIf ADR changes, regenerate by running the source through graph-easy again:\n\n```bash\n# Extract source from <details> block, pipe through graph-easy\ngraph-easy --as=boxart << 'EOF'\n# paste source here\nEOF\n```\n\n---\n\n## Reference: Monospace-Safe Symbols\n\n**Avoid emojis** - they have variable width and break box alignment on GitHub.\n\n### Status Markers\n\n| Meaning            | Marker |\n| ------------------ | ------ |\n| Added/New          | `[+]`  |\n| Removed/Deleted    | `[x]`  |\n| Changed/Updated    | `[*]`  |\n| Warning/Deprecated | `[!]`  |\n| Deferred/Pending   | `[~]`  |\n| Current/Active     | `[>]`  |\n| Optional           | `[?]`  |\n| Locked/Fixed       | `[=]`  |\n\n### Box Drawing (U+2500-257F)\n\n```\n             (light)\n             (double)\n```\n\n### Arrows & Pointers\n\n```\n                 (arrows)\n                   (logic - graph-easy uses these)\n< > ^ v              (ASCII arrows)\n```\n\n### Shapes & Bullets\n\n```\n                  (bullets)\n                   (squares)\n                   (diamonds)\n```\n\n### Math & Logic\n\n```\n             (math)\n                  (logic)\n```\n\n## Reference: Common Patterns\n\n```\n# Vertical flow (architecture)\ngraph { flow: south; }\n\n# Horizontal flow (pipeline)\ngraph { flow: east; }\n\n# Labeled edge\n[A] -- label text --> [B]\n\n# Group with border\n( Group Name:\n  [Node A]\n  [Node B]\n)\n\n# Custom node label\n[id] { label: \"Display Name\"; }\n```\n\n---\n\n## Graph Label (MANDATORY: EVERY diagram MUST have emoji + title)\n\n**WARNING**: This is the most commonly forgotten requirement. Diagrams without labels are invalid.\n\n### Correct Example\n\n```\ngraph { label: \" Database Migration\"; flow: south; }\n[Old DB] -> [New DB]\n```\n\n### Anti-Pattern (INVALID - DO NOT DO THIS)\n\n```\ngraph { flow: south; }\n[Old DB] -> [New DB]\n```\n\n**Why this is wrong**: Missing `label:` with emoji. The preflight validator will **BLOCK** any ADR containing diagrams without `graph { label: \"emoji ...\"; }`.\n\n---\n\n## Mandatory Checklist (Before Rendering)\n\n### Graph-Level (MUST have)\n\n- [ ] **`graph { label: \" Title\"; }`** - semantic emoji + title (MOST FORGOTTEN - check first!)\n- [ ] `graph { flow: south; }` or `graph { flow: east; }` - explicit direction\n- [ ] Command uses `--as=boxart` - NEVER `--as=ascii`\n\n### Embedding (MUST have - non-negotiable)\n\n- [ ] **`<details>` block with source** - EVERY diagram MUST have collapsible source code block\n- [ ] Format: rendered diagram in ` ``` ` block, followed immediately by `<details><summary>graph-easy source</summary>` with source in ` ``` ` block\n- [ ] Never commit a diagram without its reproducible source\n\n### Node Styling (Visual hierarchy)\n\n- [ ] Start/end nodes: `{ shape: rounded; }` - entry/exit points\n- [ ] Critical/important nodes: `{ border: double; }` or `{ border: bold; }`\n- [ ] Optional/skippable nodes: `{ border: dotted; }`\n- [ ] Default nodes: no styling (standard `` border)\n- [ ] Long labels use `\\n` for multiline - max ~15 chars per line\n\n### Edge Styling (Semantic meaning)\n\n- [ ] Main/happy path: `->` solid arrow\n- [ ] Conditional/alternate: `..>` dotted arrow\n- [ ] Emphasized/critical: `==>` bold arrow\n- [ ] Edge labels are SHORT (1-3 words): `-- YES -->`, `-- error -->`\n\n### Character Safety (Alignment)\n\n- [ ] NO graphical emojis inside nodes (    break alignment)\n- [ ] Unicode symbols OK inside nodes (   are single-width)\n- [ ] ASCII markers ALWAYS safe ([x] [+] [!] [OK])\n- [ ] Graphical emojis ONLY in `graph { label: \"...\"; }` title\n\n### Structure (Organization)\n\n- [ ] Groups `( Name: ... )` used for logical clustering when 4+ related nodes\n- [ ] Node IDs short, labels descriptive: `[db] { label: \"PostgreSQL\"; }`\n- [ ] No more than 7-10 nodes per diagram (split if larger)\n\n## Success Criteria\n\n### Correctness\n\n1. **Parses without error** - graph-easy accepts the DSL\n2. **Renders cleanly** - no misaligned boxes or broken lines\n3. **Matches content** - all key elements from description represented\n4. **Source preserved (MANDATORY)** - EVERY diagram MUST have `<details>` block with graph-easy DSL source immediately after the rendered output\n\n### Aesthetics\n\n5. **Uses boxart** - clean Unicode lines ``, not ASCII `+--+`\n6. **Visual hierarchy** - start/end rounded, important bold/double, optional dotted\n7. **Consistent styling** - same border style = same semantic meaning throughout\n8. **Readable labels** - multiline with `\\n`, no truncation\n9. **Clear flow** - direction matches natural reading (top-down or left-right)\n\n### Comprehensiveness\n\n10. **Semantic emoji in title** - emoji consciously chosen to match diagram purpose (see Emoji Selection Guide)\n11. **Legend if needed** - multiline title with `\\n` for complex diagrams\n12. **Edge semantics** - solid=normal, dotted=conditional, bold=critical\n13. **Logical grouping** - related nodes in `( Group: ... )` containers\n\n## Troubleshooting\n\n| Issue               | Cause                    | Solution                                      |\n| ------------------- | ------------------------ | --------------------------------------------- |\n| `command not found` | graph-easy not installed | Run preflight check                           |\n| Misaligned boxes    | Used `--as=ascii`        | Always use `--as=boxart`                      |\n| Box border broken   | Graphical emoji in node  | Remove , use  or [x][+]                 |\n| Nodes overlap       | Too complex              | Split into multiple diagrams (max 7-10 nodes) |\n| Edge labels cut off | Label too long           | Shorten to 1-3 words                          |\n| No title showing    | Wrong syntax             | Use `graph { label: \"Title\"; flow: south; }`  |\n| Weird layout        | No flow direction        | Add `graph { flow: south; }` or `flow: east`  |\n| Parse error         | Special chars in node    | Escape or simplify node names                 |\n\n## Resources\n\n- [Graph::Easy on CPAN](https://metacpan.org/dist/Graph-Easy)\n- [Graph::Easy Manual](http://bloodgate.com/perl/graph/manual/)\n- [Graph::Easy GitHub](https://github.com/ironcamel/Graph-Easy)"
              },
              {
                "name": "code-hardcode-audit",
                "description": "Detects hardcoded values, magic numbers, duplicate constants, and leaked secrets using Ruff, Semgrep, jscpd, and gitleaks. Use when auditing for hardcodes, magic numbers, PLR2004, constant detection, secret scanning, or before release.",
                "path": "plugins/itp/skills/code-hardcode-audit/SKILL.md",
                "frontmatter": {
                  "name": "code-hardcode-audit",
                  "description": "Detects hardcoded values, magic numbers, duplicate constants, and leaked secrets using Ruff, Semgrep, jscpd, and gitleaks. Use when auditing for hardcodes, magic numbers, PLR2004, constant detection, secret scanning, or before release.",
                  "allowed-tools": "Bash, Read, Write, Glob, Grep"
                },
                "content": "# Code Hardcode Audit\n\n## When to Use This Skill\n\nUse this skill when the user mentions:\n\n- \"hardcoded values\", \"hardcodes\", \"magic numbers\"\n- \"constant detection\", \"find constants\"\n- \"duplicate constants\", \"DRY violations\"\n- \"code audit\", \"hardcode audit\"\n- \"PLR2004\", \"semgrep\", \"jscpd\", \"gitleaks\"\n- \"secret scanning\", \"leaked secrets\", \"API keys\"\n- \"passwords in code\", \"credential leaks\"\n\n## Quick Start\n\n```bash\n# Full audit (all tools, both outputs)\nuv run --script scripts/audit_hardcodes.py -- src/\n\n# Python magic numbers only (fastest)\nuv run --script scripts/run_ruff_plr.py -- src/\n\n# Pattern-based detection (URLs, ports, paths)\nuv run --script scripts/run_semgrep.py -- src/\n\n# Copy-paste detection\nuv run --script scripts/run_jscpd.py -- src/\n\n# Secret scanning (API keys, tokens, passwords)\nuv run --script scripts/run_gitleaks.py -- src/\n```\n\n## Tool Overview\n\n| Tool             | Detection Focus                 | Language Support | Speed  |\n| ---------------- | ------------------------------- | ---------------- | ------ |\n| **Ruff PLR2004** | Magic value comparisons         | Python           | Fast   |\n| **Semgrep**      | URLs, ports, paths, credentials | Multi-language   | Medium |\n| **jscpd**        | Duplicate code blocks           | Multi-language   | Slow   |\n| **gitleaks**     | Secrets, API keys, passwords    | Any (file-based) | Fast   |\n\n## Output Formats\n\n### JSON (--output json)\n\n```json\n{\n  \"summary\": {\n    \"total_findings\": 42,\n    \"by_tool\": { \"ruff\": 15, \"semgrep\": 20, \"jscpd\": 7 },\n    \"by_severity\": { \"high\": 5, \"medium\": 25, \"low\": 12 }\n  },\n  \"findings\": [\n    {\n      \"id\": \"MAGIC-001\",\n      \"tool\": \"ruff\",\n      \"rule\": \"PLR2004\",\n      \"file\": \"src/config.py\",\n      \"line\": 42,\n      \"column\": 8,\n      \"message\": \"Magic value used in comparison: 8123\",\n      \"severity\": \"medium\",\n      \"suggested_fix\": \"Extract to named constant\"\n    }\n  ],\n  \"refactoring_plan\": [\n    {\n      \"priority\": 1,\n      \"action\": \"Create constants/ports.py\",\n      \"finding_ids\": [\"MAGIC-001\", \"MAGIC-003\"]\n    }\n  ]\n}\n```\n\n### Compiler-like Text (--output text)\n\n```\nsrc/config.py:42:8: PLR2004 Magic value used in comparison: 8123 [ruff]\nsrc/probe.py:15:1: hardcoded-url Hardcoded URL detected [semgrep]\nsrc/client.py:20-35: Clone detected (16 lines, 95% similarity) [jscpd]\n\nSummary: 42 findings (ruff: 15, semgrep: 20, jscpd: 7)\n```\n\n## CLI Options\n\n```\n--output {json,text,both}  Output format (default: both)\n--tools {all,ruff,semgrep,jscpd,gitleaks}  Tools to run (default: all)\n--severity {all,high,medium,low}  Filter by severity (default: all)\n--exclude PATTERN  Glob pattern to exclude (repeatable)\n--parallel  Run tools in parallel (default: true)\n```\n\n## References\n\n- [Tool Comparison](./references/tool-comparison.md) - Detailed tool capabilities\n- [Output Schema](./references/output-schema.md) - JSON schema specification\n- [Troubleshooting](./references/troubleshooting.md) - Common issues and fixes\n\n## Related\n\n- ADR-0046: Semantic Constants Abstraction\n- ADR-0047: Code Hardcode Audit Skill\n- `code-clone-assistant` - PMD CPD-based clone detection (DRY focus)"
              },
              {
                "name": "graph-easy",
                "description": "Create ASCII diagrams for any GitHub Flavored Markdown file using graph-easy. Use for READMEs, design docs, specifications, or any markdown needing architecture diagrams. Zero external dependencies for rendering - pure text output embeds directly in markdown.",
                "path": "plugins/itp/skills/graph-easy/SKILL.md",
                "frontmatter": {
                  "name": "graph-easy",
                  "description": "Create ASCII diagrams for any GitHub Flavored Markdown file using graph-easy. Use for READMEs, design docs, specifications, or any markdown needing architecture diagrams. Zero external dependencies for rendering - pure text output embeds directly in markdown."
                },
                "content": "# Graph-Easy Diagram Skill\n\nCreate ASCII architecture diagrams for any GitHub Flavored Markdown file using graph-easy. Pure text output with automatic layout - no image rendering required.\n\n## When to Use This Skill\n\n- Adding diagrams to README files\n- Design specification documentation\n- Any GFM markdown file needing architecture visualization\n- Creating flowcharts, pipelines, or system diagrams\n- User mentions \"diagram\", \"ASCII diagram\", \"graph-easy\", or \"architecture chart\"\n\n**NOT for ADRs** - Use `adr-graph-easy-architect` for Architecture Decision Records (includes ADR-specific patterns like 2-diagram requirement and before/after templates).\n\n## Preflight Check\n\nRun these checks in order. Each layer depends on the previous.\n\n### Layer 1: Package Manager\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\n# Detect OS and set package manager\ncase \"$(uname -s)\" in\n  Darwin) PM=\"brew\" ;;\n  Linux)  PM=\"apt\" ;;\n  *)      echo \"ERROR: Unsupported OS (require macOS or Linux)\"; exit 1 ;;\nesac\ncommand -v $PM &>/dev/null || { echo \"ERROR: $PM not installed\"; exit 1; }\necho \" Package manager: $PM\"\nSETUP_EOF\n```\n\n### Layer 2: Perl + cpanminus (mise-first approach)\n\n```bash\n# Prefer mise for unified tool management\nif command -v mise &>/dev/null; then\n  # Install Perl via mise\n  mise which perl &>/dev/null || mise install perl\n  # Install cpanminus under mise perl\n  mise exec perl -- cpanm --version &>/dev/null 2>&1 || {\n    echo \"Installing cpanminus under mise perl...\"\n    mise exec perl -- curl -L https://cpanmin.us | mise exec perl -- perl - App::cpanminus\n  }\n  echo \" cpanminus installed (via mise perl)\"\nelse\n  # Fallback: Install cpanminus via system package manager\n  command -v cpanm &>/dev/null || {\n    echo \"Installing cpanminus via $PM...\"\n    case \"$PM\" in\n      brew) brew install cpanminus ;;\n      apt)  sudo apt install -y cpanminus ;;\n    esac\n  }\n  echo \" cpanminus installed\"\nfi\n```\n\n### Layer 3: Graph::Easy Perl module\n\n```bash\n# Check if Graph::Easy is installed (mise-first)\nif command -v mise &>/dev/null; then\n  mise exec perl -- perl -MGraph::Easy -e1 2>/dev/null || {\n    echo \"Installing Graph::Easy via mise perl cpanm...\"\n    mise exec perl -- cpanm Graph::Easy\n  }\n  echo \" Graph::Easy installed (via mise perl)\"\nelse\n  perl -MGraph::Easy -e1 2>/dev/null || {\n    echo \"Installing Graph::Easy via cpanm...\"\n    cpanm Graph::Easy\n  }\n  echo \" Graph::Easy installed\"\nfi\n```\n\n### Layer 4: Verify graph-easy is in PATH\n\n```bash\n# Verify graph-easy is accessible and functional\ncommand -v graph-easy &>/dev/null || {\n  echo \"ERROR: graph-easy not found in PATH\"\n  exit 1\n}\n# Test actual functionality (--version hangs waiting for stdin AND exits with code 2)\necho \"[Test] -> [OK]\" | graph-easy &>/dev/null && echo \" graph-easy ready\"\n```\n\n### All-in-One Preflight Script\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Copy-paste this entire block to ensure graph-easy is ready (macOS + Linux)\n# Prefers mise for unified cross-platform tool management\n\n# Check for mise first (recommended)\nif command -v mise &>/dev/null; then\n  echo \"Using mise for Perl management...\"\n  mise which perl &>/dev/null || mise install perl\n  mise exec perl -- cpanm --version &>/dev/null 2>&1 || \\\n    mise exec perl -- curl -L https://cpanmin.us | mise exec perl -- perl - App::cpanminus\n  mise exec perl -- perl -MGraph::Easy -e1 2>/dev/null || mise exec perl -- cpanm Graph::Easy\nelse\n  # Fallback: system package manager\n  echo \" Tip: Install mise for unified tool management: curl https://mise.run | sh\"\n  case \"$(uname -s)\" in\n    Darwin) PM=\"brew\" ;;\n    Linux)  PM=\"apt\" ;;\n    *)      echo \"ERROR: Unsupported OS\"; exit 1 ;;\n  esac\n  command -v $PM &>/dev/null || { echo \"ERROR: $PM not installed\"; exit 1; }\n  command -v cpanm &>/dev/null || { [ \"$PM\" = \"apt\" ] && sudo apt install -y cpanminus || brew install cpanminus; }\n  perl -MGraph::Easy -e1 2>/dev/null || cpanm Graph::Easy\nfi\n\n# Verify graph-easy is in PATH and functional\ncommand -v graph-easy &>/dev/null || {\n  echo \"ERROR: graph-easy not in PATH after installation\"\n  exit 1\n}\n# Test actual functionality (--version hangs waiting for stdin AND exits with code 2)\necho \"[Test] -> [OK]\" | graph-easy &>/dev/null && echo \" graph-easy ready\"\nPREFLIGHT_EOF\n```\n\n---\n\n## Part 1: DSL Syntax\n\n### Basic Elements\n\n```\n# Nodes (square brackets)\n[Node Name]\n\n# Edges (arrows)\n[A] -> [B]\n\n# Labeled edges\n[A] -- label --> [B]\n\n# Bidirectional\n[A] <-> [B]\n\n# Chain\n[A] -> [B] -> [C]\n```\n\n### Groups (Containers)\n\n```\n# Named group with dashed border\n( Group Name:\n  [Node A]\n  [Node B]\n)\n\n# Nested connections\n( Frontend:\n  [React App]\n  [API Client]\n)\n( Backend:\n  [API Server]\n  [Database]\n)\n[API Client] -> [API Server]\n```\n\n### Node Labels\n\n```\n# Custom label (different from ID)\n[db] { label: \"PostgreSQL Database\"; }\n\n# ASCII markers for visual distinction INSIDE boxes\n# (emojis break box alignment - use ASCII markers instead)\n[deleted] { label: \"[x] Old Component\"; }\n[added] { label: \"[+] New Component\"; }\n[warning] { label: \"[!] Deprecated\"; }\n[success] { label: \"[OK] Passed\"; }\n```\n\n**Character rules for nodes:**\n\n- Graphical emojis (rocket, bulb, checkmark) - NEVER (double-width breaks box alignment)\n- Unicode symbols (check, cross, arrow) - OK (single-width, safe)\n- ASCII markers ([x] [+] [!] :) ) - ALWAYS safe (monospace)\n\nUse `graph { label: \"...\"; }` for graphical emojis in title/legend.\n\n**Example: Emoji breaks alignment (DON'T DO THIS)**\n\n```\n# BAD - emoji inside node\n[rocket] { label: \"Launch\"; }\n```\n\nRenders broken:\n\n```\n+----------------+\n| Launch         |   <-- box edge misaligned due to double-width emoji\n+----------------+\n```\n\n**Example: ASCII marker preserves alignment (DO THIS)**\n\n```\n# GOOD - ASCII marker inside node\n[rocket] { label: \"[>] Launch\"; }\n```\n\nRenders correctly:\n\n```\n+--------------+\n| [>] Launch   |\n+--------------+\n```\n\n### Flow Direction (MANDATORY: Always specify)\n\n```\n# MANDATORY: Always specify flow direction explicitly\ngraph { flow: south; }   # Top-to-bottom (architecture, decisions)\ngraph { flow: east; }    # Left-to-right (pipelines, sequences)\n```\n\nNever rely on default flow - explicit is clearer.\n\n### Graph Title and Legend (Outside Boxes - Emojis Safe Here)\n\nEmojis break alignment INSIDE boxes but are SAFE in graph titles/legends.\n\n**Emoji Selection Guide** - Choose emoji that matches diagram purpose:\n\n| Diagram Type             | Emoji  | Example Title           |\n| ------------------------ | ------ | ----------------------- |\n| Migration/Change         | swap   | `\"Database Migration\"`  |\n| Deployment/Release       | rocket | `\"Deployment Pipeline\"` |\n| Data Flow                | chart  | `\"Data Ingestion Flow\"` |\n| Security/Auth            | lock   | `\"Authentication Flow\"` |\n| Error/Failure            | warn   | `\"Error Handling\"`      |\n| Decision/Branch          | split  | `\"Routing Decision\"`    |\n| Architecture             | build  | `\"System Architecture\"` |\n| Network/API              | globe  | `\"API Integration\"`     |\n| Storage/Database         | disk   | `\"Storage Layer\"`       |\n| Monitoring/Observability | signal | `\"Monitoring Stack\"`    |\n\n```\n# Title with semantic emoji\ngraph { label: \"Deployment Pipeline\"; flow: east; }\n\n# Title with legend (multiline using \\n)\ngraph { label: \"Hook Flow\\n----------\\nAllow  Deny  Warn\"; flow: south; }\n```\n\n### Node Styling (Best Practices)\n\n```\n# Rounded corners for start/end nodes\n[ Start ] { shape: rounded; }\n[ End ] { shape: rounded; }\n\n# Double border for emphasis\n[ Critical Step ] { border: double; }\n\n# Bold border for important nodes\n[ Key Decision ] { border: bold; }\n\n# Dotted border for optional/skippable\n[ Optional ] { border: dotted; }\n\n# Multiline labels with \\n\n[ Hook Input\\n(stdin JSON) ]\n```\n\n**Rendered examples:**\n\n```\n+----------+              +---------+\n| Rounded  |              | Default |\n+----------+              +---------+\n\n+==========+              +=========+\n| Double   |              |  Bold   |\n+==========+              +=========+\n```\n\n> **Note:** Dotted borders (`{ border: dotted; }`) use special characters that render inconsistently on GitHub. Use sparingly.\n\n### Edge Styles\n\n```\n[ A ] -> [ B ]      # Solid arrow (default)\n[ A ] ..> [ B ]     # Dotted arrow\n[ A ] ==> [ B ]     # Bold/double arrow\n[ A ] - -> [ B ]    # Dashed arrow\n[ A ] -- label --> [ B ]  # Labeled edge\n```\n\n---\n\n## Part 2: Common Diagram Patterns\n\n### Pipeline (Left-to-Right)\n\n```\ngraph { flow: east; }\n[Input] -> [Process] -> [Output]\n```\n\n### Multi-Component System\n\n```\ngraph { flow: south; }\n[API Gateway] -> [Service A]\n[API Gateway] -> [Service B]\n[Service A] -> [Database]\n[Service B] -> [Database]\n```\n\n### Decision with Options\n\n```\ngraph { flow: south; }\n[Decision] -> [Option A]\n[Decision] -> [Option B]\n[Decision] -> [Option C]\n```\n\n### Grouped Components\n\n```\n( Frontend:\n  [React App]\n  [Vue App]\n)\n( Backend:\n  [API Server]\n  [Worker]\n)\n[React App] -> [API Server]\n[Vue App] -> [API Server]\n[API Server] -> [Worker]\n```\n\n### Bidirectional Flow\n\n```\n[Client] <-> [Server]\n[Server] -> [Database]\n```\n\n### Layered Architecture\n\n```\ngraph { flow: south; }\n( Presentation:\n  [UI Components]\n)\n( Business:\n  [Services]\n)\n( Data:\n  [Repository]\n  [Database]\n)\n[UI Components] -> [Services]\n[Services] -> [Repository]\n[Repository] -> [Database]\n```\n\n---\n\n## Part 3: Rendering\n\n### Command (Platform-Aware)\n\n```bash\n# For GitHub markdown (RECOMMENDED) - renders as solid lines\ngraph-easy --as=ascii << 'EOF'\ngraph { flow: south; }\n[A] -> [B] -> [C]\nEOF\n\n# For terminal/local viewing - prettier Unicode lines\ngraph-easy --as=boxart << 'EOF'\ngraph { flow: south; }\n[A] -> [B] -> [C]\nEOF\n```\n\n### Output Modes\n\n| Mode     | Command       | When to Use                                                     |\n| -------- | ------------- | --------------------------------------------------------------- |\n| `ascii`  | `--as=ascii`  | **GitHub markdown** - `+--+` renders as solid lines everywhere  |\n| `boxart` | `--as=boxart` | **Terminal only** - `` looks nice locally, dotted on GitHub |\n\n**Why ASCII for GitHub?** GitHub's markdown preview renders Unicode box-drawing characters (``) as **dotted lines**, breaking the visual appearance. Pure ASCII (`+---+`, `|`) renders correctly as solid lines on all platforms.\n\n### Validation Workflow\n\n```bash\n# 1. Write DSL to heredoc\n# 2. Render with ascii (for GitHub) or boxart (for terminal)\ngraph-easy --as=ascii << 'EOF'\n[Your] -> [Diagram] -> [Here]\nEOF\n\n# 3. Review output\n# 4. Iterate if needed\n# 5. Copy final output to markdown\n# 6. Validate alignment (RECOMMENDED)\n```\n\n### Post-Generation Alignment Validation (Recommended)\n\nAfter embedding diagram in markdown, validate alignment to catch rendering issues.\n\n**Use the doc-tools plugin skill:**\n\n```\nSkill: doc-tools:ascii-diagram-validator\n```\n\nOr invoke directly: `Skill(doc-tools:ascii-diagram-validator)` with the target file path.\n\n**Why validate?**\n\n- Catches copy-paste alignment drift\n- Detects font rendering issues\n- Ensures vertical columns align properly\n- Graph-easy output is machine-aligned, but manual edits can break it\n\n**When to skip**: If diagram was just generated by graph-easy and not manually edited, validation is optional (output is inherently aligned).\n\n---\n\n## Part 4: Embedding in Markdown\n\n### Markdown Format (MANDATORY: Always Include Source)\n\n**CRITICAL**: Every rendered diagram MUST be followed by a collapsible `<details>` block containing the graph-easy source code. This is non-negotiable for:\n\n- **Reproducibility**: Future maintainers can regenerate the diagram\n- **Editability**: Source can be modified and re-rendered\n- **Auditability**: Changes to diagrams are trackable in git diffs\n\n````markdown\n## Architecture\n\n```\n+----------+     +----------+     +----------+\n|  Input   | --> | Process  | --> |  Output  |\n+----------+     +----------+     +----------+\n```\n\n<details>\n<summary>graph-easy source</summary>\n\n```\ngraph { flow: east; }\n[Input] -> [Process] -> [Output]\n```\n\n</details>\n````\n\n**The `<details>` block is MANDATORY** - never embed a diagram without its source.\n\n### GFM Collapsible Section Syntax\n\nGitHub Flavored Markdown supports HTML `<details>` and `<summary>` tags for collapsible sections. Key syntax rules:\n\n**Structure:**\n\n```html\n<details>\n  <summary>Click to expand</summary>\n\n  <!-- BLANK LINE REQUIRED HERE -->\n  Content goes here (Markdown supported)\n  <!-- BLANK LINE REQUIRED HERE -->\n</details>\n```\n\n**Critical rules:**\n\n1. **Blank lines required** - Must have empty line after `<summary>` and before `</details>` for Markdown to render\n2. **No indentation** - `<details>` and `<summary>` must be at column 0 (no leading spaces)\n3. **Summary is clickable label** - Text in `<summary>` appears as the collapsed header\n4. **Markdown inside works** - Code blocks, headers, lists all render correctly inside\n\n**Optional: Default expanded:**\n\n```html\n<details open>\n  <summary>Expanded by default</summary>\n\n  Content visible on page load\n</details>\n```\n\n**Common mistake (Markdown won't render):**\n\n```html\n<details>\n  <summary>Broken</summary>\n  No blank line - this won't render as Markdown!\n</details>\n```\n\n**References:**\n\n- [GitHub Docs: Collapsed sections](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-collapsed-sections)\n\n### Regeneration\n\nIf the markdown changes, regenerate by running the source through graph-easy again:\n\n```bash\n# Extract source from <details> block, pipe through graph-easy\ngraph-easy --as=boxart << 'EOF'\n# paste source here\nEOF\n```\n\n---\n\n## Reference: Monospace-Safe Symbols\n\n**Avoid emojis** - they have variable width and break box alignment on GitHub.\n\n### Status Markers\n\n| Meaning            | Marker |\n| ------------------ | ------ |\n| Added/New          | `[+]`  |\n| Removed/Deleted    | `[x]`  |\n| Changed/Updated    | `[*]`  |\n| Warning/Deprecated | `[!]`  |\n| Deferred/Pending   | `[~]`  |\n| Current/Active     | `[>]`  |\n| Optional           | `[?]`  |\n| Locked/Fixed       | `[=]`  |\n\n### Box Drawing (U+2500-257F)\n\n```\n- | + + + + + + + + +   (light)\n= | + + + + + + + + +   (double)\n```\n\n### Arrows & Pointers\n\n```\n-> <- up down            (arrows)\nv ^                      (logic - graph-easy uses these)\n< > ^ v                  (ASCII arrows)\n```\n\n### Shapes & Bullets\n\n```\n* o O                    (bullets)\n[ ] #                    (squares)\n< > <>                   (diamonds)\n```\n\n---\n\n## Graph Label (MANDATORY: EVERY diagram MUST have emoji + title)\n\n**WARNING**: This is the most commonly forgotten requirement. Diagrams without labels are invalid.\n\n### Correct Example\n\n```\ngraph { label: \" Deployment Pipeline\"; flow: east; }\n[Build] -> [Test] -> [Deploy]\n```\n\n### Anti-Pattern (INVALID - DO NOT DO THIS)\n\n```\ngraph { flow: east; }\n[Build] -> [Test] -> [Deploy]\n```\n\n**Why this is wrong**: Missing `label:` with emoji. Every diagram needs context at a glance.\n\n---\n\n## Mandatory Checklist (Before Rendering)\n\n### Graph-Level (MUST have)\n\n- [ ] **`graph { label: \" Title\"; }`** - semantic emoji + title (MOST FORGOTTEN - check first!)\n- [ ] `graph { flow: south; }` or `graph { flow: east; }` - explicit direction\n- [ ] Command uses `--as=ascii` for GitHub markdown (or `--as=boxart` for terminal only)\n\n### Embedding (MUST have - non-negotiable)\n\n- [ ] **`<details>` block with source** - EVERY diagram MUST have collapsible source code block\n- [ ] Format: rendered diagram in code block, followed immediately by `<details><summary>graph-easy source</summary>` with source in code block\n- [ ] Never commit a diagram without its reproducible source\n\n### Post-Embedding Validation (Recommended)\n\n- [ ] Run `ascii-diagram-validator` on the file after embedding diagram\n- [ ] Especially important if diagram was manually edited after generation\n- [ ] Catches alignment drift from copy-paste or font rendering issues\n\n### Node Styling (Visual hierarchy)\n\n- [ ] Start/end nodes: `{ shape: rounded; }` - entry/exit points\n- [ ] Critical/important nodes: `{ border: double; }` or `{ border: bold; }`\n- [ ] Optional/skippable nodes: `{ border: dotted; }`\n- [ ] Default nodes: no styling (standard border)\n- [ ] Long labels use `\\n` for multiline - max ~15 chars per line\n\n### Edge Styling (Semantic meaning)\n\n- [ ] Main/happy path: `->` solid arrow\n- [ ] Conditional/alternate: `..>` dotted arrow\n- [ ] Emphasized/critical: `==>` bold arrow\n- [ ] Edge labels are SHORT (1-3 words): `-- YES -->`, `-- error -->`\n\n### Character Safety (Alignment)\n\n- [ ] NO graphical emojis inside nodes (break alignment)\n- [ ] Unicode symbols OK inside nodes (single-width)\n- [ ] ASCII markers ALWAYS safe ([x] [+] [!] [OK])\n- [ ] Graphical emojis ONLY in `graph { label: \"...\"; }` title\n\n### Structure (Organization)\n\n- [ ] Groups `( Name: ... )` used for logical clustering when 4+ related nodes\n- [ ] Node IDs short, labels descriptive: `[db] { label: \"PostgreSQL\"; }`\n- [ ] No more than 7-10 nodes per diagram (split if larger)\n\n## Success Criteria\n\n### Correctness\n\n1. **Parses without error** - graph-easy accepts the DSL\n2. **Renders cleanly** - no misaligned boxes or broken lines\n3. **Matches content** - all key elements from description represented\n4. **Source preserved (MANDATORY)** - EVERY diagram MUST have `<details>` block with graph-easy DSL source immediately after the rendered output\n\n### Aesthetics\n\n1. **Platform-appropriate output** - `--as=ascii` for GitHub (solid lines), `--as=boxart` for terminal only\n2. **Readable labels** - multiline with `\\n`, no truncation\n3. **Clear flow** - direction matches natural reading (top-down or left-right)\n4. **Consistent styling** - same border style = same semantic meaning throughout\n\n### Comprehensiveness\n\n1. **Edge semantics** - solid=normal, dotted=conditional, bold=critical\n2. **Logical grouping** - related nodes in `( Group: ... )` containers\n\n## Troubleshooting\n\n| Issue               | Cause                    | Solution                                      |\n| ------------------- | ------------------------ | --------------------------------------------- |\n| `command not found` | graph-easy not installed | Run preflight check                           |\n| Dotted lines on GH  | Used `--as=boxart`       | Use `--as=ascii` for GitHub markdown          |\n| Box border broken   | Graphical emoji in node  | Remove emojis, use ASCII markers [x][+]       |\n| Nodes overlap       | Too complex              | Split into multiple diagrams (max 7-10 nodes) |\n| Edge labels cut off | Label too long           | Shorten to 1-3 words                          |\n| No title showing    | Wrong syntax             | Use `graph { label: \"Title\"; flow: south; }`  |\n| Weird layout        | No flow direction        | Add `graph { flow: south; }` or `flow: east`  |\n| Parse error         | Special chars in node    | Escape or simplify node names                 |\n\n## Resources\n\n- [Graph::Easy on CPAN](https://metacpan.org/dist/Graph-Easy)\n- [Graph::Easy Manual](http://bloodgate.com/perl/graph/manual/)\n- [Graph::Easy GitHub](https://github.com/ironcamel/Graph-Easy)"
              },
              {
                "name": "impl-standards",
                "description": "Core engineering standards during implementation. Use when implementing features, writing production code, or when user mentions error handling, constants management, progress logging, or code quality standards.",
                "path": "plugins/itp/skills/impl-standards/SKILL.md",
                "frontmatter": {
                  "name": "impl-standards",
                  "description": "Core engineering standards during implementation. Use when implementing features, writing production code, or when user mentions error handling, constants management, progress logging, or code quality standards."
                },
                "content": "# Implementation Standards\n\nApply these standards during implementation to ensure consistent, maintainable code.\n\n## When to Use This Skill\n\n- During `/itp:go` Phase 1\n- When writing new production code\n- User mentions \"error handling\", \"constants\", \"magic numbers\", \"progress logging\"\n- Before release to verify code quality\n\n## Quick Reference\n\n| Standard         | Rule                                                                     |\n| ---------------- | ------------------------------------------------------------------------ |\n| **Errors**       | Raise + propagate; no fallback/default/retry/silent                      |\n| **Constants**    | Abstract magic numbers into semantic, version-agnostic dynamic constants |\n| **Dependencies** | Prefer OSS libs over custom code; no backward-compatibility needed       |\n| **Progress**     | Operations >1min: log status every 15-60s                                |\n| **Logs**         | `logs/{adr-id}-YYYYMMDD_HHMMSS.log` (nohup)                              |\n| **Metadata**     | Optional: `catalog-info.yaml` for service discovery                      |\n\n---\n\n## Error Handling\n\n**Core Rule**: Raise + propagate; no fallback/default/retry/silent\n\n```python\n#  Correct - raise with context\ndef fetch_data(url: str) -> dict:\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise APIError(f\"Failed to fetch {url}: {response.status_code}\")\n    return response.json()\n\n#  Wrong - silent catch\ntry:\n    result = fetch_data()\nexcept Exception:\n    pass  # Error hidden\n```\n\nSee [Error Handling Reference](./references/error-handling.md) for detailed patterns.\n\n---\n\n## Constants Management\n\n**Core Rule**: Abstract magic numbers into semantic constants\n\n```python\n#  Correct - named constant\nDEFAULT_API_TIMEOUT_SECONDS = 30\nresponse = requests.get(url, timeout=DEFAULT_API_TIMEOUT_SECONDS)\n\n#  Wrong - magic number\nresponse = requests.get(url, timeout=30)\n```\n\nSee [Constants Management Reference](./references/constants-management.md) for patterns.\n\n---\n\n## Progress Logging\n\nFor operations taking more than 1 minute, log status every 15-60 seconds:\n\n```python\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\ndef long_operation(items: list) -> None:\n    total = len(items)\n    last_log = datetime.now()\n\n    for i, item in enumerate(items):\n        process(item)\n\n        # Log every 30 seconds\n        if (datetime.now() - last_log).seconds >= 30:\n            logger.info(f\"Progress: {i+1}/{total} ({100*(i+1)//total}%)\")\n            last_log = datetime.now()\n\n    logger.info(f\"Completed: {total} items processed\")\n```\n\n---\n\n## Log File Convention\n\nSave logs to: `logs/{adr-id}-YYYYMMDD_HHMMSS.log`\n\n```bash\n# Running with nohup\nnohup python script.py > logs/2025-12-01-my-feature-20251201_143022.log 2>&1 &\n```\n\n---\n\n## Related Skills\n\n| Skill                                                        | Purpose                                   |\n| ------------------------------------------------------------ | ----------------------------------------- |\n| [`adr-code-traceability`](../adr-code-traceability/SKILL.md) | Add ADR references to code                |\n| [`code-hardcode-audit`](../code-hardcode-audit/SKILL.md)     | Detect hardcoded values before release    |\n| [`semantic-release`](../semantic-release/SKILL.md)           | Version management and release automation |\n\n---\n\n## Reference Documentation\n\n- [Error Handling](./references/error-handling.md) - Raise + propagate patterns\n- [Constants Management](./references/constants-management.md) - Magic number abstraction"
              },
              {
                "name": "implement-plan-preflight",
                "description": "Execute Preflight phase for /itp:go workflow - create ADR (MADR 4.0), design spec from global plan, verify checkpoint. Use when creating ADRs, design specs, or when /itp:go invokes Preflight phase. Triggers - ADR creation, design spec, MADR format, preflight verification.",
                "path": "plugins/itp/skills/implement-plan-preflight/SKILL.md",
                "frontmatter": {
                  "name": "implement-plan-preflight",
                  "description": "Execute Preflight phase for /itp:go workflow - create ADR (MADR 4.0), design spec from global plan, verify checkpoint. Use when creating ADRs, design specs, or when /itp:go invokes Preflight phase. Triggers - ADR creation, design spec, MADR format, preflight verification."
                },
                "content": "# Implement Plan Preflight\n\nExecute the Preflight phase of the `/itp:go` workflow. Creates ADR and Design Spec artifacts with proper cross-linking and verification.\n\n## When to Use This Skill\n\n- Invoked by `/itp:go` command during Preflight phase\n- User asks to create an ADR for a feature\n- User mentions \"design spec\" or \"MADR format\"\n- Manual preflight verification needed\n\n## Preflight Workflow Overview\n\n```\nP.1: Create Feature Branch (if -b flag)\n         \n         \nP.2: Create ADR File (MADR 4.0)\n         \n         \nP.3: Create Design Spec (from global plan)\n         \n         \nP.4: Verify Checkpoint (MANDATORY)\n```\n\n**CRITICAL**: Do NOT proceed to Phase 1 implementation until ALL preflight steps are complete and verified.\n\n---\n\n## Quick Reference\n\n### ADR ID Format\n\n```\nYYYY-MM-DD-slug\n```\n\nExample: `2025-12-01-clickhouse-aws-ohlcv-ingestion`\n\n### File Locations\n\n| Artifact    | Path                                 |\n| ----------- | ------------------------------------ |\n| ADR         | `/docs/adr/$ADR_ID.md`               |\n| Design Spec | `/docs/design/$ADR_ID/spec.md`       |\n| Global Plan | `~/.claude/plans/<adj-verb-noun>.md` |\n\n### Cross-Links (MANDATORY)\n\n**In ADR header**:\n\n```markdown\n**Design Spec**: [Implementation Spec](/docs/design/YYYY-MM-DD-slug/spec.md)\n```\n\n**In spec.md header**:\n\n```markdown\n**ADR**: [Feature Name ADR](/docs/adr/YYYY-MM-DD-slug.md)\n```\n\n---\n\n## Execution Steps\n\n### Step P.1: Create Feature Branch (Optional)\n\nOnly if `-b` flag specified. See [Workflow Steps](./references/workflow-steps.md) for details.\n\n### Step P.2: Create ADR File\n\n1. Create `/docs/adr/$ADR_ID.md`\n2. Use template from [ADR Template](./references/adr-template.md)\n3. Populate frontmatter from session context\n4. Select perspectives from [Perspectives Taxonomy](./references/perspectives-taxonomy.md)\n5. Use Skill tool to invoke `adr-graph-easy-architect` for diagrams\n\n### Step P.3: Create Design Spec\n\n1. Create folder: `mkdir -p docs/design/$ADR_ID`\n2. Copy global plan: `cp ~/.claude/plans/<adj-verb-noun>.md docs/design/$ADR_ID/spec.md`\n3. Add ADR backlink to spec header\n\n### Step P.4: Verify Checkpoint\n\nRun validator or manual checklist:\n\n```bash\nuv run scripts/preflight_validator.py $ADR_ID\n```\n\n**Checklist** (ALL must be true):\n\n- [ ] ADR file exists at `/docs/adr/$ADR_ID.md`\n- [ ] ADR has YAML frontmatter with all 7 required fields\n- [ ] ADR has `**Design Spec**:` link in header\n- [ ] **DIAGRAM CHECK 1**: ADR has **Before/After diagram** (Context section)\n- [ ] **DIAGRAM CHECK 2**: ADR has **Architecture diagram** (Architecture section)\n- [ ] Design spec exists at `/docs/design/$ADR_ID/spec.md`\n- [ ] Design spec has `**ADR**:` backlink in header\n\n**If any item is missing**: Create it now. Do NOT proceed to Phase 1.\n\n---\n\n## YAML Frontmatter Quick Reference\n\n```yaml\n---\nstatus: proposed\ndate: YYYY-MM-DD\ndecision-maker: [User Name]\nconsulted: [Agent-1, Agent-2]\nresearch-method: single-agent\nclarification-iterations: N\nperspectives: [Perspective1, Perspective2]\n---\n```\n\nSee [ADR Template](./references/adr-template.md) for full field descriptions.\n\n---\n\n## Diagram Requirements (2 DIAGRAMS REQUIRED)\n\n** MANDATORY**: Every ADR must include EXACTLY 2 diagrams:\n\n| Diagram          | Location             | Purpose                       |\n| ---------------- | -------------------- | ----------------------------- |\n| **Before/After** | Context section      | Shows system state change     |\n| **Architecture** | Architecture section | Shows component relationships |\n\n**SKILL INVOCATION**: Invoke `adr-graph-easy-architect` skill NOW to create BOTH diagrams.\n\n**BLOCKING GATE**: Do NOT proceed to design spec until BOTH diagrams are embedded in ADR.\n\n---\n\n## Reference Documentation\n\n- [ADR Template](./references/adr-template.md) - Complete MADR 4.0 template\n- [Perspectives Taxonomy](./references/perspectives-taxonomy.md) - 11 perspective types\n- [Workflow Steps](./references/workflow-steps.md) - Detailed step-by-step guide\n\n---\n\n## Validation Script\n\n```bash\n# Verify preflight artifacts\nuv run scripts/preflight_validator.py <adr-id>\n\n# Example\nuv run scripts/preflight_validator.py 2025-12-01-my-feature\n```"
              },
              {
                "name": "mise-configuration",
                "description": "Configure environment via mise [env] SSoT. TRIGGERS - mise env, mise.toml, environment variables, centralize config, Python venv, mise templates.",
                "path": "plugins/itp/skills/mise-configuration/SKILL.md",
                "frontmatter": {
                  "name": "mise-configuration",
                  "description": "Configure environment via mise [env] SSoT. TRIGGERS - mise env, mise.toml, environment variables, centralize config, Python venv, mise templates.",
                  "allowed-tools": "Read, Bash, Glob, Grep, Edit, Write"
                },
                "content": "# mise Configuration as Single Source of Truth\n\nUse mise `[env]` as centralized configuration with backward-compatible defaults.\n\n## Core Principle\n\nDefine all configurable values in `.mise.toml` `[env]` section. Scripts read via environment variables with fallback defaults. Same code path works WITH or WITHOUT mise installed.\n\n**Key insight**: mise auto-loads `[env]` values when shell has `mise activate` configured. Scripts using `os.environ.get(\"VAR\", \"default\")` pattern work identically whether mise is present or not.\n\n## Quick Reference\n\n### Language Patterns\n\n| Language   | Pattern                            | Notes                       |\n| ---------- | ---------------------------------- | --------------------------- |\n| Python     | `os.environ.get(\"VAR\", \"default\")` | Returns string, cast if int |\n| Bash       | `${VAR:-default}`                  | Standard POSIX expansion    |\n| JavaScript | `process.env.VAR \\|\\| \"default\"`   | Falsy check, watch for \"0\"  |\n| Go         | `os.Getenv(\"VAR\")` with default    | Empty string if unset       |\n| Rust       | `std::env::var(\"VAR\").unwrap_or()` | Returns Result<String>      |\n\n### Special Directives\n\n| Directive       | Purpose                 | Example                                             |\n| --------------- | ----------------------- | --------------------------------------------------- |\n| `_.file`        | Load from .env files    | `_.file = \".env\"`                                   |\n| `_.path`        | Extend PATH             | `_.path = [\"bin\", \"node_modules/.bin\"]`             |\n| `_.source`      | Execute bash scripts    | `_.source = \"./scripts/env.sh\"`                     |\n| `_.python.venv` | Auto-create Python venv | `_.python.venv = { path = \".venv\", create = true }` |\n\n## Python Venv Auto-Creation (Critical)\n\nAuto-create and activate Python virtual environments:\n\n```toml\n[env]\n_.python.venv = { path = \".venv\", create = true }\n```\n\nThis pattern is used in ALL projects. When entering the directory with mise activated:\n\n1. Creates `.venv` if it doesn't exist\n2. Activates the venv automatically\n3. Works with `uv` for fast venv creation\n\n**Alternative via [settings]**:\n\n```toml\n[settings]\npython.uv_venv_auto = true\n```\n\n## Special Directives\n\n### Load from .env Files (`_.file`)\n\n```toml\n[env]\n# Single file\n_.file = \".env\"\n\n# Multiple files with options\n_.file = [\n    \".env\",\n    { path = \".env.secrets\", redact = true }\n]\n```\n\n### Extend PATH (`_.path`)\n\n```toml\n[env]\n_.path = [\n    \"{{config_root}}/bin\",\n    \"{{config_root}}/node_modules/.bin\",\n    \"scripts\"\n]\n```\n\n### Source Bash Scripts (`_.source`)\n\n```toml\n[env]\n_.source = \"./scripts/env.sh\"\n_.source = { path = \".secrets.sh\", redact = true }\n```\n\n### Lazy Evaluation (`tools = true`)\n\nBy default, env vars resolve BEFORE tools install. Use `tools = true` to access tool-generated paths:\n\n```toml\n[env]\n# Access PATH after tools are set up\nGEM_BIN = { value = \"{{env.GEM_HOME}}/bin\", tools = true }\n\n# Load .env files after tool setup\n_.file = { path = \".env\", tools = true }\n```\n\n## Template Syntax (Tera)\n\nmise uses Tera templating. Delimiters: `{{ }}` expressions, `{% %}` statements, `{# #}` comments.\n\n### Built-in Variables\n\n| Variable              | Description                     |\n| --------------------- | ------------------------------- |\n| `{{config_root}}`     | Directory containing .mise.toml |\n| `{{cwd}}`             | Current working directory       |\n| `{{env.VAR}}`         | Environment variable            |\n| `{{mise_bin}}`        | Path to mise binary             |\n| `{{mise_pid}}`        | mise process ID                 |\n| `{{xdg_cache_home}}`  | XDG cache directory             |\n| `{{xdg_config_home}}` | XDG config directory            |\n| `{{xdg_data_home}}`   | XDG data directory              |\n\n### Functions\n\n```toml\n[env]\n# Get env var with fallback\nNODE_VER = \"{{ get_env(name='NODE_VERSION', default='20') }}\"\n\n# Execute shell command\nTIMESTAMP = \"{{ exec(command='date +%Y-%m-%d') }}\"\n\n# System info\nARCH = \"{{ arch() }}\"      # x64, arm64\nOS = \"{{ os() }}\"          # linux, macos, windows\nCPUS = \"{{ num_cpus() }}\"\n\n# File operations\nVERSION = \"{{ read_file(path='VERSION') | trim }}\"\nHASH = \"{{ hash_file(path='config.json', len=8) }}\"\n```\n\n### Filters\n\n```toml\n[env]\n# Case conversion\nSNAKE = \"{{ name | snakecase }}\"\nKEBAB = \"{{ name | kebabcase }}\"\nCAMEL = \"{{ name | lowercamelcase }}\"\n\n# String manipulation\nTRIMMED = \"{{ text | trim }}\"\nUPPER = \"{{ text | upper }}\"\nREPLACED = \"{{ text | replace(from='old', to='new') }}\"\n\n# Path operations\nABSOLUTE = \"{{ path | absolute }}\"\nBASENAME = \"{{ path | basename }}\"\nDIRNAME = \"{{ path | dirname }}\"\n```\n\n### Conditionals\n\n```toml\n[env]\n{% if env.DEBUG %}\nLOG_LEVEL = \"debug\"\n{% else %}\nLOG_LEVEL = \"info\"\n{% endif %}\n```\n\n## Required & Redacted Variables\n\n### Required Variables\n\nEnforce variable definition with helpful messages:\n\n```toml\n[env]\nDATABASE_URL = { required = true }\nAPI_KEY = { required = \"Get from https://example.com/api-keys\" }\n```\n\n### Redacted Variables\n\nHide sensitive values from output:\n\n```toml\n[env]\nSECRET = { value = \"my_secret\", redact = true }\n_.file = { path = \".env.secrets\", redact = true }\n\n# Pattern-based redactions\nredactions = [\"*_TOKEN\", \"*_KEY\", \"PASSWORD\"]\n```\n\n## [settings] Section\n\n```toml\n[settings]\nexperimental = true              # Enable experimental features\npython.uv_venv_auto = true       # Auto-create venv with uv\n```\n\n## [tools] Version Pinning\n\nPin tool versions for reproducibility:\n\n```toml\n[tools]\npython = \"3.11\"  # minimum baseline; use 3.12, 3.13 as needed\nnode = \"latest\"\nuv = \"latest\"\n\n# With options\nrust = { version = \"1.75\", profile = \"minimal\" }\n```\n\n**min_version**: Enforce mise version compatibility:\n\n```toml\nmin_version = \"2024.9.5\"\n```\n\n## Implementation Steps\n\n1. **Identify hardcoded values** - timeouts, paths, thresholds, feature flags\n2. **Create `.mise.toml`** - add `[env]` section with documented variables\n3. **Add venv auto-creation** - `_.python.venv = { path = \".venv\", create = true }`\n4. **Update scripts** - use env vars with original values as defaults\n5. **Add ADR reference** - comment: `# ADR: 2025-12-08-mise-env-centralized-config`\n6. **Test without mise** - verify script works using defaults\n7. **Test with mise** - verify activated shell uses `.mise.toml` values\n\n## GitHub Token Multi-Account Patterns (MANDATORY for Multi-Account Setups) {#github-token-multi-account-patterns}\n\nFor multi-account GitHub setups, mise `[env]` provides per-directory token configuration that overrides gh CLI's global authentication.\n\n### Token Storage\n\nStore tokens in a centralized, secure location:\n\n```bash\nmkdir -p ~/.claude/.secrets\nchmod 700 ~/.claude/.secrets\n\n# Create token files (one per account)\ngh auth login  # authenticate as account\ngh auth token > ~/.claude/.secrets/gh-token-accountname\nchmod 600 ~/.claude/.secrets/gh-token-*\n```\n\n### Per-Directory Configuration\n\n```toml\n# ~/.claude/.mise.toml (terrylica account)\n[env]\nGH_TOKEN = \"{{ read_file(path=config_root ~ '/.secrets/gh-token-terrylica') | trim }}\"\nGITHUB_TOKEN = \"{{ read_file(path=config_root ~ '/.secrets/gh-token-terrylica') | trim }}\"\nGH_ACCOUNT = \"terrylica\"  # For human reference only\n```\n\n```toml\n# ~/eon/.mise.toml (terrylica account - different directory)\n[env]\nGH_TOKEN = \"{{ read_file(path=env.HOME ~ '/.claude/.secrets/gh-token-terrylica') | trim }}\"\nGITHUB_TOKEN = \"{{ read_file(path=env.HOME ~ '/.claude/.secrets/gh-token-terrylica') | trim }}\"\nGH_ACCOUNT = \"terrylica\"\n```\n\n### Variable Naming Convention\n\n| Variable       | Usage Context                                 | Example                     |\n| -------------- | --------------------------------------------- | --------------------------- |\n| `GH_TOKEN`     | mise [env], Doppler, verification tasks       | `.mise.toml`, shell scripts |\n| `GITHUB_TOKEN` | npm scripts, GitHub Actions, semantic-release | `package.json`, workflows   |\n\n**Rule**: Always set BOTH variables in mise [env] pointing to the same token file. Different tools check different variable names.\n\n### Alternative: 1Password Integration\n\nFor enhanced security with automatic token rotation:\n\n```toml\n[env]\nGH_TOKEN = \"{{ op_read('op://Engineering/GitHub Token/credential') }}\"\n```\n\nWith caching for performance:\n\n```toml\n[env]\nGH_TOKEN = \"{{ cache(key='gh_token', duration='1h', run='op read op://Engineering/GitHub Token/credential') }}\"\n```\n\n### Verification\n\n```bash\n/usr/bin/env bash << 'MISE_EOF'\nfor dir in ~/.claude ~/eon ~/own ~/scripts ~/459ecs; do\n  cd \"$dir\" && eval \"$(mise hook-env -s bash)\" && echo \"$dir  $GH_ACCOUNT\"\ndone\nMISE_EOF\n```\n\n**ADR Reference**: [GitHub Multi-Account Authentication](https://github.com/terrylica/claude-config/blob/main/docs/adr/2025-12-17-github-multi-account-authentication.md)\n\n> **SSH ControlMaster Warning**: If using multi-account SSH, ensure `ControlMaster no` is set for GitHub hosts in `~/.ssh/config`. Cached connections can authenticate with the wrong account. See [semantic-release authentication](../semantic-release/references/authentication.md#controlmaster-cache-issues) for troubleshooting.\n\n## Anti-Patterns\n\n| Anti-Pattern                | Why                    | Instead                                    |\n| --------------------------- | ---------------------- | ------------------------------------------ |\n| `mise exec -- script.py`    | Forces mise dependency | Use env vars with defaults                 |\n| Secrets in `.mise.toml`     | Visible in repo        | Use Doppler or `redact = true`             |\n| No defaults in scripts      | Breaks without mise    | Always provide fallback                    |\n| Mixing env/tools resolution | Order matters          | Use `tools = true` for tool-dependent vars |\n\n## Task Orchestration Integration\n\nWhen you detect **workflow opportunities** during mise configuration, prescriptively invoke the `mise-tasks` skill.\n\n### Detection Triggers\n\n- Multi-step workflows (test + build + deploy)\n- Repeatable project commands (lint, format, release)\n- Dependency chains (db-migrate depends on db-init)\n- File-tracked builds (sources/outputs caching)\n\n### Prescriptive AI Reminder\n\n> **PRESCRIPTIVE**: When detecting multi-step project workflows, **invoke [`mise-tasks` skill](../mise-tasks/SKILL.md)** to generate appropriate `[tasks]` definitions with dependency management.\n\n### Example: From Environment to Tasks\n\n**Step 1**: Configure environment (this skill):\n\n```toml\n[env]\nDATABASE_URL = \"postgresql://localhost/mydb\"\n_.python.venv = { path = \".venv\", create = true }\n```\n\n**Step 2**: Define tasks (`mise-tasks` skill):\n\n```toml\n[tasks.test]\ndepends = [\"lint\"]\nrun = \"pytest tests/\"\n\n[tasks.deploy]\ndepends = [\"test\", \"build\"]\nrun = \"deploy.sh\"\n```\n\nTasks automatically inherit `[env]` values.\n\n---\n\n## Additional Resources\n\nFor complete code patterns and examples, see: **[`references/patterns.md`](./references/patterns.md)**\n\n**For task orchestration**, see: **[`mise-tasks` skill](../mise-tasks/SKILL.md)** - Dependencies, arguments, file tracking, watch mode\n\n**ADR Reference**: When implementing mise configuration, create an ADR at `docs/adr/YYYY-MM-DD-mise-env-centralized-config.md` in your project."
              },
              {
                "name": "mise-tasks",
                "description": "Orchestrate workflows with mise [tasks]. TRIGGERS - mise tasks, mise run, task runner, depends, depends_post, workflow automation, task dependencies.",
                "path": "plugins/itp/skills/mise-tasks/SKILL.md",
                "frontmatter": {
                  "name": "mise-tasks",
                  "description": "Orchestrate workflows with mise [tasks]. TRIGGERS - mise tasks, mise run, task runner, depends, depends_post, workflow automation, task dependencies.",
                  "allowed-tools": "Read, Bash, Glob, Grep, Edit, Write"
                },
                "content": "# mise Tasks Orchestration\n\n<!-- ADR: 2025-12-08-mise-tasks-skill -->\n\nOrchestrate multi-step project workflows using mise `[tasks]` section with dependency management, argument handling, and file tracking.\n\n## When to Use This Skill\n\n**Explicit triggers**:\n\n- User mentions `mise tasks`, `mise run`, `[tasks]` section\n- User needs task dependencies: `depends`, `depends_post`\n- User wants workflow automation in `.mise.toml`\n- User mentions task arguments or `usage` spec\n\n**AI Discovery trigger** (prescriptive):\n\n> When `mise-configuration` skill detects multi-step workflows (test suites, build pipelines, migrations), **prescriptively invoke this skill** to generate appropriate `[tasks]` definitions.\n\n## Quick Reference\n\n### Task Definition\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"cargo build --release\"\n```\n\n### Running Tasks\n\n```bash\nmise run build          # Run single task\nmise run test build     # Run multiple tasks\nmise run test ::: build # Run in parallel\nmise r build            # Short form\n```\n\n### Dependency Types\n\n| Type           | Syntax                       | When                    |\n| -------------- | ---------------------------- | ----------------------- |\n| `depends`      | `depends = [\"lint\", \"test\"]` | Run BEFORE task         |\n| `depends_post` | `depends_post = [\"notify\"]`  | Run AFTER task succeeds |\n| `wait_for`     | `wait_for = [\"db\"]`          | Wait only if running    |\n\n---\n\n## Level 1-2: Basic Tasks\n\n### Minimal Task\n\n```toml\n[tasks.hello]\nrun = \"echo 'Hello, World!'\"\n```\n\n### With Description\n\n```toml\n[tasks.test]\ndescription = \"Run test suite\"\nrun = \"pytest tests/\"\n```\n\n### With Alias\n\n```toml\n[tasks.test]\ndescription = \"Run test suite\"\nalias = \"t\"\nrun = \"pytest tests/\"\n```\n\nNow `mise run t` works.\n\n### Working Directory\n\n```toml\n[tasks.frontend]\ndir = \"packages/frontend\"\nrun = \"npm run build\"\n```\n\n### Task-Specific Environment\n\n```toml\n[tasks.test]\nenv = { RUST_BACKTRACE = \"1\", LOG_LEVEL = \"debug\" }\nrun = \"cargo test\"\n```\n\n**Note**: `env` values are NOT passed to dependency tasks.\n\n### GitHub Token Verification Task\n\nFor multi-account GitHub setups, add a verification task:\n\n```toml\n[tasks._verify-gh-auth]\ndescription = \"Verify GitHub token matches expected account\"\nhide = true  # Hidden helper task\nrun = \"\"\"\nexpected=\"${GH_ACCOUNT:-}\"\nif [ -z \"$expected\" ]; then\n  echo \"GH_ACCOUNT not set - skipping verification\"\n  exit 0\nfi\nactual=$(gh api user --jq '.login' 2>/dev/null || echo \"\")\nif [ \"$actual\" != \"$expected\" ]; then\n  echo \"ERROR: GH_TOKEN authenticates as '$actual', expected '$expected'\"\n  exit 1\nfi\necho \" GitHub auth verified: $actual\"\n\"\"\"\n\n[tasks.release]\ndescription = \"Create semantic release\"\ndepends = [\"_verify-gh-auth\"]  # Verify before release\nrun = \"npx semantic-release --no-ci\"\n```\n\nSee [`mise-configuration` skill](../mise-configuration/SKILL.md#github-token-multi-account-patterns) for GH_TOKEN setup.\n\n> **SSH ControlMaster Warning**: If using multi-account SSH, ensure `ControlMaster no` is set for GitHub hosts in `~/.ssh/config`. Cached connections can authenticate with the wrong account.\n\n### Multi-Command Tasks\n\n```toml\n[tasks.setup]\nrun = [\n  \"npm install\",\n  \"npm run build\",\n  \"npm run migrate\"\n]\n```\n\n---\n\n## Level 3-4: Dependencies & Orchestration\n\n### Pre-Execution Dependencies\n\n```toml\n[tasks.deploy]\ndepends = [\"test\", \"build\"]\nrun = \"kubectl apply -f deployment.yaml\"\n```\n\nTasks `test` and `build` run BEFORE `deploy`.\n\n### Post-Execution Tasks\n\n```toml\n[tasks.release]\ndepends = [\"test\"]\ndepends_post = [\"notify\", \"cleanup\"]\nrun = \"npm publish\"\n```\n\nAfter `release` succeeds, `notify` and `cleanup` run automatically.\n\n### Soft Dependencies\n\n```toml\n[tasks.migrate]\nwait_for = [\"database\"]\nrun = \"./migrate.sh\"\n```\n\nIf `database` task is already running, wait for it. Otherwise, proceed.\n\n### Task Chaining Pattern\n\n```toml\n[tasks.ci]\ndescription = \"Full CI pipeline\"\ndepends = [\"lint\", \"test\", \"build\"]\ndepends_post = [\"coverage-report\"]\nrun = \"echo 'CI passed'\"\n```\n\nSingle command: `mise run ci` executes entire chain.\n\n### Parallel Dependencies\n\nDependencies without inter-dependencies run in parallel:\n\n```toml\n[tasks.validate]\ndepends = [\"lint\", \"typecheck\", \"test\"]  # These can run in parallel\nrun = \"echo 'All validations passed'\"\n```\n\n---\n\n## Level 5: Hidden Tasks & Organization\n\n### Hidden Tasks\n\n```toml\n[tasks._check-credentials]\ndescription = \"Verify credentials are set\"\nhide = true\nrun = '''\nif [ -z \"$API_KEY\" ]; then\n  echo \"ERROR: API_KEY not set\"\n  exit 1\nfi\n'''\n\n[tasks.deploy]\ndepends = [\"_check-credentials\"]\nrun = \"deploy.sh\"\n```\n\nHidden tasks don't appear in `mise tasks` output but can be dependencies.\n\nView hidden tasks: `mise tasks --hidden`\n\n### Colon-Prefixed Namespacing\n\n```toml\n[tasks.test]\nrun = \"pytest\"\n\n[tasks.\"test:unit\"]\nrun = \"pytest tests/unit/\"\n\n[tasks.\"test:integration\"]\nrun = \"pytest tests/integration/\"\n\n[tasks.\"test:e2e\"]\nrun = \"playwright test\"\n```\n\nRun all test tasks: `mise run 'test:*'`\n\n### Wildcard Patterns\n\n```bash\nmise run 'test:*'      # All tasks starting with test:\nmise run 'db:**'       # Nested: db:migrate:up, db:seed:test\n```\n\n---\n\n## Level 6: Task Arguments\n\n### Usage Specification (Preferred Method)\n\n```toml\n[tasks.deploy]\ndescription = \"Deploy to environment\"\nusage = '''\narg \"<environment>\" help=\"Target environment\" {\n  choices \"dev\" \"staging\" \"prod\"\n}\nflag \"-f --force\" help=\"Skip confirmation\"\nflag \"--region <region>\" default=\"us-east-1\" env=\"AWS_REGION\"\n'''\nrun = '''\necho \"Deploying to ${usage_environment}\"\n[ \"$usage_force\" = \"true\" ] && echo \"Force mode enabled\"\necho \"Region: ${usage_region}\"\n'''\n```\n\n### Argument Types\n\n**Required positional**:\n\n```toml\nusage = 'arg \"<file>\" help=\"Input file\"'\n```\n\n**Optional positional**:\n\n```toml\nusage = 'arg \"[file]\" default=\"config.toml\"'\n```\n\n**Variadic (multiple values)**:\n\n```toml\nusage = 'arg \"<files>\" var=#true'\n```\n\n### Flag Types\n\n**Boolean flag**:\n\n```toml\nusage = 'flag \"-v --verbose\"'\n# Access: ${usage_verbose:-false}\n```\n\n**Flag with value**:\n\n```toml\nusage = 'flag \"-o --output <file>\" default=\"out.txt\"'\n# Access: ${usage_output}\n```\n\n**Environment-backed flag**:\n\n```toml\nusage = 'flag \"--port <port>\" env=\"PORT\" default=\"8080\"'\n```\n\n### Accessing Arguments\n\nIn `run` scripts, arguments become `usage_<name>` environment variables:\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\n${usage_environment}      # Required arg value\n${usage_verbose:-false}   # Boolean flag with default\n${usage_output}           # Flag with value\nSKILL_SCRIPT_EOF\n```\n\n**DEPRECATION WARNING**: The Tera template method (`{{arg(name=\"...\")}}`) will be removed in mise 2026.11.0. Use `usage` spec instead.\n\nFor complete argument syntax, see: [arguments.md](./references/arguments.md)\n\n---\n\n## Level 7: File Tracking & Caching\n\n### Source Files\n\n```toml\n[tasks.build]\nsources = [\"Cargo.toml\", \"src/**/*.rs\"]\nrun = \"cargo build\"\n```\n\nTask re-runs only when source files change.\n\n### Output Files\n\n```toml\n[tasks.build]\nsources = [\"Cargo.toml\", \"src/**/*.rs\"]\noutputs = [\"target/release/myapp\"]\nrun = \"cargo build --release\"\n```\n\nIf outputs are newer than sources, task is **skipped**.\n\n### Force Execution\n\n```bash\nmise run build --force  # Bypass caching\n```\n\n### Auto Output Detection\n\n```toml\n[tasks.compile]\noutputs = { auto = true }  # Default behavior\nrun = \"gcc -o app main.c\"\n```\n\n---\n\n## Level 8: Advanced Execution\n\n### Confirmation Prompts\n\n```toml\n[tasks.drop-database]\nconfirm = \"This will DELETE all data. Continue?\"\nrun = \"dropdb myapp\"\n```\n\n### Output Control\n\n```toml\n[tasks.quiet-task]\nquiet = true   # Suppress mise's output (not task output)\nrun = \"echo 'This still prints'\"\n\n[tasks.silent-task]\nsilent = true  # Suppress ALL output\nrun = \"background-job.sh\"\n\n[tasks.silent-stderr]\nsilent = \"stderr\"  # Only suppress stderr\nrun = \"noisy-command\"\n```\n\n### Raw Mode (Interactive)\n\n```toml\n[tasks.edit-config]\nraw = true  # Direct stdin/stdout/stderr\nrun = \"vim config.yaml\"\n```\n\n**Warning**: `raw = true` disables parallel execution.\n\n### Task-Specific Tools\n\n```toml\n[tasks.legacy-test]\ntools = { python = \"3.9\", node = \"18\" }\nrun = \"pytest && npm test\"\n```\n\nUse specific tool versions for this task only.\n\n### Custom Shell\n\n```toml\n[tasks.powershell-task]\nshell = \"pwsh -c\"\nrun = \"Get-Process | Select-Object -First 5\"\n```\n\n---\n\n## Level 9: Watch Mode\n\n### Basic Watch\n\n```bash\nmise watch build  # Re-run on source changes\n```\n\nRequires `watchexec`: `mise use -g watchexec@latest`\n\n### Watch Options\n\n```bash\nmise watch build --debounce 500ms  # Wait before re-run\nmise watch build --restart          # Kill and restart on change\nmise watch build --clear            # Clear screen before run\n```\n\n### On-Busy Behavior\n\n```bash\nmise watch build --on-busy-update=queue    # Queue changes\nmise watch build --on-busy-update=restart  # Restart immediately\nmise watch build --on-busy-update=do-nothing  # Ignore (default)\n```\n\n---\n\n## Level 10: Monorepo (Experimental)\n\n**Requires**: `MISE_EXPERIMENTAL=1` and `experimental_monorepo_root = true`\n\n### Path Syntax\n\n```bash\nmise run //projects/frontend:build    # Absolute from root\nmise run :build                       # Current config_root\nmise run //...:test                   # All projects\n```\n\n### Wildcards\n\n```bash\nmise run '//projects/...:build'       # Build all under projects/\nmise run '//projects/frontend:*'      # All tasks in frontend\n```\n\n### Discovery\n\nTasks in subdirectories are auto-discovered with path prefix:\n\n- `packages/api/.mise.toml` tasks  `packages/api:taskname`\n\nFor complete monorepo documentation, see: [advanced.md](./references/advanced.md)\n\n---\n\n## Integration with [env]\n\nTasks automatically inherit `[env]` values:\n\n```toml\n[env]\nDATABASE_URL = \"postgresql://localhost/mydb\"\n_.file = \".env\"  # Load additional env vars\n\n[tasks.migrate]\nrun = \"diesel migration run\"  # $DATABASE_URL available\n```\n\n### Credential Loading Pattern\n\n```toml\n[env]\n_.file = { path = \".env.secrets\", redact = true }\n\n[tasks._check-env]\nhide = true\nrun = '[ -n \"$API_KEY\" ] || { echo \"Missing API_KEY\"; exit 1; }'\n\n[tasks.deploy]\ndepends = [\"_check-env\"]\nrun = \"deploy.sh\"\n```\n\n---\n\n## Anti-Patterns\n\n| Anti-Pattern                    | Why Bad                                       | Instead                                                                  |\n| ------------------------------- | --------------------------------------------- | ------------------------------------------------------------------------ |\n| Replace /itp:go with mise tasks | No TodoWrite, no ADR tracking, no checkpoints | Use mise tasks for project workflows, /itp:go for ADR-driven development |\n| Hardcode secrets in tasks       | Security risk                                 | Use `_.file = \".env.secrets\"` with `redact = true`                       |\n| Giant monolithic tasks          | Hard to debug, no reuse                       | Break into small tasks with dependencies                                 |\n| Skip `description`              | Poor discoverability                          | Always add descriptions                                                  |\n\n---\n\n## Cross-Reference: mise-configuration\n\n**Prerequisites**: Before defining tasks, ensure `[env]` section is configured.\n\n> **PRESCRIPTIVE**: After defining tasks, invoke **[`mise-configuration` skill](../mise-configuration/SKILL.md)** to ensure [env] SSoT patterns are applied.\n\nThe `mise-configuration` skill covers:\n\n- `[env]` - Environment variables with defaults\n- `[settings]` - mise behavior configuration\n- `[tools]` - Version pinning\n- Special directives: `_.file`, `_.path`, `_.python.venv`\n\n---\n\n## Additional Resources\n\n- [Task Patterns](./references/patterns.md) - Real-world task examples\n- [Task Arguments](./references/arguments.md) - Complete usage spec reference\n- [Advanced Features](./references/advanced.md) - Monorepo, watch, experimental"
              },
              {
                "name": "pypi-doppler",
                "description": "LOCAL-ONLY PyPI publishing with Doppler credential management. Use when publishing to PyPI from LOCAL machine ONLY. NEVER use in CI/CD pipelines. Workspace-wide policy enforces local publishing via scripts/publish-to-pypi.sh with CI detection guards.",
                "path": "plugins/itp/skills/pypi-doppler/SKILL.md",
                "frontmatter": {
                  "name": "pypi-doppler",
                  "description": "LOCAL-ONLY PyPI publishing with Doppler credential management. Use when publishing to PyPI from LOCAL machine ONLY. NEVER use in CI/CD pipelines. Workspace-wide policy enforces local publishing via scripts/publish-to-pypi.sh with CI detection guards."
                },
                "content": "# PyPI Publishing with Doppler (Local-Only)\n\n##  WORKSPACE-WIDE POLICY: LOCAL-ONLY PUBLISHING\n\n**This skill supports LOCAL machine publishing ONLY.**\n\n### FORBIDDEN\n\n **Publishing from GitHub Actions**\n **Publishing from any CI/CD pipeline** (GitHub Actions, GitLab CI, Jenkins, CircleCI)\n **`publishCmd` in semantic-release configuration**\n **Building packages in CI** (`uv build` in prepareCmd)\n **Storing PyPI tokens in GitHub secrets**\n\n### REQUIRED\n\n **Use `scripts/publish-to-pypi.sh` on local machine**\n **CI detection guards in publish script**\n **Manual approval before each release**\n **Doppler credential management** (no plaintext tokens)\n **Repository verification** (prevents fork abuse)\n\n### Rationale\n\n- **Security**: No long-lived PyPI tokens in GitHub secrets\n- **Speed**: 30 seconds locally vs 3-5 minutes in CI\n- **Control**: Manual approval step before production release\n- **Flexibility**: Centralized credential management via Doppler\n\n**See**: ADR-0027, `docs/development/PUBLISHING.md`\n\n---\n\n## Overview\n\nThis skill provides **local-only PyPI publishing** using Doppler for secure credential management. It integrates with the workspace-wide release workflow where:\n\n1. **GitHub Actions**: Automated versioning ONLY (tags, releases, CHANGELOG)\n2. **Local Machine**: Manual PyPI publishing with Doppler credentials\n\n## Bundled Scripts\n\n| Script                                                       | Purpose                                        |\n| ------------------------------------------------------------ | ---------------------------------------------- |\n| [`scripts/publish-to-pypi.sh`](./scripts/publish-to-pypi.sh) | Local PyPI publishing with CI detection guards |\n\n**Usage**: Copy to your project's `scripts/` directory:\n\n```bash\n/usr/bin/env bash << 'DOPPLER_EOF'\n# Environment-agnostic path\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\ncp \"$PLUGIN_DIR/skills/pypi-doppler/scripts/publish-to-pypi.sh\" scripts/\nchmod +x scripts/publish-to-pypi.sh\nDOPPLER_EOF\n```\n\n---\n\n## Prerequisites\n\n### One-Time Setup\n\n1. **Install Doppler CLI**:\n\n   ```bash\n   brew install dopplerhq/cli/doppler\n   ```\n\n2. **Authenticate with Doppler**:\n\n   ```bash\n   doppler login\n   ```\n\n3. **Verify access to `claude-config` project**:\n   ```bash\n   doppler whoami\n   doppler projects\n   ```\n\n### PyPI Token Setup\n\n1. **Create PyPI API token**:\n   - Visit: https://pypi.org/manage/account/token/\n   - Enable 2FA if not already enabled (required since 2024)\n   - Create token with scope: \"Entire account\" or specific project\n   - Copy token (starts with `pypi-AgEIcHlwaS5vcmc...`, ~180 characters)\n\n2. **Store token in Doppler**:\n\n   ```bash\n   doppler secrets set PYPI_TOKEN='pypi-AgEIcHlwaS5vcmc...' \\\n     --project claude-config \\\n     --config prd\n   ```\n\n3. **Verify token stored**:\n   ```bash\n   doppler secrets get PYPI_TOKEN \\\n     --project claude-config \\\n     --config prd \\\n     --plain\n   ```\n\n---\n\n## Publishing Workflow\n\n### MANDATORY: Verify Version Increment Before Publishing\n\n**Pre-publish validation**: Before publishing to PyPI, verify that the version has incremented from the previous release. Publishing without a version increment is invalid and wastes resources.\n\n**Autonomous check sequence**:\n\n1. Compare local `pyproject.toml` version against latest PyPI version\n2. If versions match  **STOP** - do not proceed with publishing\n3. Inform user: \"Version not incremented. Run semantic-release first or verify commits include `feat:` or `fix:` types.\"\n\n**Why this matters**: PyPI rejects duplicate versions, but more importantly, users and package managers rely on version increments to detect updates. A release workflow that doesn't increment version is broken.\n\n### Complete Release Workflow\n\n**Step 1: Development & Commit** (Conventional Commits):\n\n```bash\n# Make your changes\ngit add .\n\n# Commit with conventional format (determines version bump)\ngit commit -m \"feat: add new feature\"  # MINOR bump\n\n# Push to main\ngit push origin main\n```\n\n**Step 2: Automated Versioning** (GitHub Actions - 40-60s):\n\nGitHub Actions workflow automatically:\n\n-  Analyzes commits using `@semantic-release/commit-analyzer`\n-  Determines next version (e.g., `v7.1.0`)\n-  Updates `pyproject.toml`, `package.json` versions\n-  Generates and updates `CHANGELOG.md`\n-  Creates git tag (`v7.1.0`)\n-  Creates GitHub release with release notes\n-  Commits changes back to repo with `[skip ci]` message\n\n** PyPI publishing does NOT happen here** (by design - see ADR-0027)\n\n**Step 3: Local PyPI Publishing** (30 seconds):\n\n**After GitHub Actions completes**, publish to PyPI locally:\n\n```bash\n# Pull the latest release commit\ngit pull origin main\n\n# Publish to PyPI (uses pypi-doppler skill)\n./scripts/publish-to-pypi.sh\n```\n\n**Expected output**:\n\n```\n Publishing to PyPI (Local Workflow)\n======================================\n\n Step 0: Verifying Doppler credentials...\n    Doppler token verified\n\n Step 1: Pulling latest release commit...\n   Current version: v7.1.0\n\n Step 2: Cleaning old builds...\n    Cleaned\n\n Step 3: Building package...\n    Built: dist/gapless_crypto_clickhouse-7.1.0-py3-none-any.whl\n\n Step 4: Publishing to PyPI...\n   Using PYPI_TOKEN from Doppler\n    Published to PyPI\n\n Step 5: Verifying on PyPI...\n    Verified: https://pypi.org/project/gapless-crypto-clickhouse/7.1.0/\n\n Complete! Published v7.1.0 to PyPI in 28 seconds\n```\n\n---\n\n## Publishing Command (Local Machine Only)\n\n**CRITICAL**: This command must ONLY run on your local machine, NEVER in CI/CD.\n\n### Using Bundled Script (Recommended)\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\n# First time: copy script from skill to your project (environment-agnostic)\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp}\"\ncp \"$PLUGIN_DIR/skills/pypi-doppler/scripts/publish-to-pypi.sh\" scripts/\nchmod +x scripts/publish-to-pypi.sh\n\n# After semantic-release creates GitHub release\ngit pull origin main\n\n# Publish using local copy of bundled script\n./scripts/publish-to-pypi.sh\nGIT_EOF\n```\n\n**Bundled script features**:\n\n-  CI detection guards (blocks if CI=true)\n-  Repository verification (prevents fork abuse)\n-  Doppler integration (PYPI_TOKEN retrieval)\n-  Build + publish + verify workflow\n-  Clear error messages\n\n### Manual Publishing (Advanced)\n\nFor manual publishing without the canonical script:\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF'\n# Retrieve token from Doppler\nPYPI_TOKEN=$(doppler secrets get PYPI_TOKEN \\\n  --project claude-config \\\n  --config prd \\\n  --plain)\n\n# Build package\nuv build\n\n# Publish to PyPI\nUV_PUBLISH_TOKEN=\"${PYPI_TOKEN}\" uv publish\nCONFIG_EOF\n```\n\n** WARNING**: Manual publishing bypasses CI detection guards and repository verification. Use canonical script unless you have a specific reason not to.\n\n---\n\n## CI Detection Enforcement\n\nThe canonical publish script (`scripts/publish-to-pypi.sh`) includes CI detection guards to prevent accidental execution in CI/CD pipelines.\n\n### Environment Variables Checked\n\n- `$CI` - Generic CI indicator\n- `$GITHUB_ACTIONS` - GitHub Actions\n- `$GITLAB_CI` - GitLab CI\n- `$JENKINS_URL` - Jenkins\n- `$CIRCLECI` - CircleCI\n\n### Behavior\n\n**If any CI variable detected**, script exits with error:\n\n```\n ERROR: This script must ONLY be run on your LOCAL machine\n\n   Detected CI environment variables:\n   - CI: true\n   - GITHUB_ACTIONS: <not set>\n   ...\n\n   This project enforces LOCAL-ONLY PyPI publishing for:\n   - Security: No long-lived PyPI tokens in GitHub secrets\n   - Speed: 30 seconds locally vs 3-5 minutes in CI\n   - Control: Manual approval step before production release\n\n   See: docs/development/PUBLISHING.md (ADR-0027)\n```\n\n### Testing CI Detection\n\n```bash\n# This should FAIL with error message\nCI=true ./scripts/publish-to-pypi.sh\n\n# Expected:  ERROR: This script must ONLY be run on your LOCAL machine\n```\n\n---\n\n## Credential Management\n\n### Doppler Configuration\n\n**Project**: `claude-config`\n**Configs**: `prd` (production), `dev` (development)\n**Secret Name**: `PYPI_TOKEN`\n\n### Token Format\n\nValid PyPI token format:\n\n- Starts with: `pypi-AgEIcHlwaS5vcmc`\n- Length: ~180 characters\n- Example: `pypi-AgEIcHlwaS5vcmcCJGI4YmNhMDA5LTg...`\n\n### Token Permissions\n\n**Account-wide token** (recommended):\n\n- Can publish to all projects under your account\n- Simpler management\n- One token for all repositories\n\n**Project-scoped token**:\n\n- Can only publish to specific project\n- More restrictive\n- Separate token per project needed\n\n### Token Rotation\n\n```bash\n# 1. Create new token on PyPI\n# Visit: https://pypi.org/manage/account/token/\n\n# 2. Update Doppler\ndoppler secrets set PYPI_TOKEN='new-token' \\\n  --project claude-config \\\n  --config prd\n\n# 3. Verify new token works\ndoppler secrets get PYPI_TOKEN \\\n  --project claude-config \\\n  --config prd \\\n  --plain\n\n# 4. Test publish (dry-run not available, use TestPyPI)\n# See: Troubleshooting  TestPyPI Testing\n```\n\n---\n\n## Troubleshooting\n\n### Issue: \"PYPI_TOKEN not found in Doppler\"\n\n**Symptom**: Script fails at Step 0\n\n**Fix**:\n\n```bash\n# Verify token exists\ndoppler secrets --project claude-config --config prd | grep PYPI_TOKEN\n\n# If missing, get new token from PyPI\n# Visit: https://pypi.org/manage/account/token/\n# Create token with scope: \"Entire account\" or specific project\n\n# Store in Doppler\ndoppler secrets set PYPI_TOKEN='your-token' \\\n  --project claude-config \\\n  --config prd\n```\n\n### Issue: \"403 Forbidden from PyPI\"\n\n**Symptom**: Script fails at Step 4 with authentication error\n\n**Root Cause**: Token expired or invalid (PyPI requires 2FA since 2024)\n\n**Fix**:\n\n1. Verify 2FA enabled on PyPI account\n2. Create new token: https://pypi.org/manage/account/token/\n3. Update Doppler: `doppler secrets set PYPI_TOKEN='new-token' --project claude-config --config prd`\n4. Retry publish\n\n### Issue: \"Script blocked with CI detection error\"\n\n**Symptom**:\n\n```\n ERROR: This script must ONLY be run on your LOCAL machine\nDetected CI environment variables:\n- CI: true\n```\n\n**Root Cause**: Running in CI environment OR `CI` variable set locally\n\n**Fix**:\n\n```bash\n# Check if CI variable set in your shell\nenv | grep CI\n\n# If set, unset it\nunset CI\nunset GITHUB_ACTIONS\n\n# Retry publish\n./scripts/publish-to-pypi.sh\n```\n\n**Expected behavior**: This is INTENTIONAL - script should ONLY run locally.\n\n### Issue: \"Version not updated in pyproject.toml\"\n\n**Symptom**: Local publish uses old version number\n\n**Root Cause**: Didn't pull latest release commit from GitHub\n\n**Fix**:\n\n```bash\n# Always pull before publishing\ngit pull origin main\n\n# Verify version updated\ngrep '^version = ' pyproject.toml\n\n# Retry publish\n./scripts/publish-to-pypi.sh\n```\n\n### Issue: \"uv package manager not found\"\n\n**Symptom**: Script fails at startup before any steps\n\n**Root Cause**: uv not installed or not discoverable\n\n**How the script discovers uv** (in priority order):\n\n1. Already in PATH (Homebrew, direct install, shell configured)\n2. Common direct install locations (`~/.local/bin/uv`, `~/.cargo/bin/uv`, `/opt/homebrew/bin/uv`)\n3. Version managers as fallback (mise, asdf)\n\n**Fix**: Install uv using any method:\n\n```bash\n# Official installer (recommended)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Homebrew\nbrew install uv\n\n# Cargo\ncargo install uv\n\n# mise (if you use it)\nmise use uv@latest\n```\n\nThe script doesn't force any particular installation method.\n\n### Issue: Script Hangs with No Output\n\n**Symptom**: Script starts but produces no output, eventually times out\n\n**Root Cause**: Script sources `~/.zshrc` or `~/.bashrc` which waits for interactive input\n\n**Fix**: Never source shell config files in scripts. The bundled script uses:\n\n```bash\n/usr/bin/env bash << 'MISE_EOF'\n# CORRECT - safe for non-interactive shells\neval \"$(mise activate bash 2>/dev/null)\" || true\n\n# WRONG - hangs in non-interactive shells\nsource ~/.zshrc\nMISE_EOF\n```\n\n---\n\n### TestPyPI Testing\n\nTo test publishing workflow without affecting production:\n\n1. **Get TestPyPI token**:\n   - Visit: https://test.pypi.org/manage/account/token/\n   - Create token\n\n2. **Store in Doppler** (separate key):\n\n   ```bash\n   doppler secrets set TESTPYPI_TOKEN='your-test-token' \\\n     --project claude-config \\\n     --config prd\n   ```\n\n3. **Modify publish script temporarily**:\n\n   ```bash\n/usr/bin/env bash << 'DOPPLER_EOF_2'\n   # In scripts/publish-to-pypi.sh, change:\n   uv publish --token \"${PYPI_TOKEN}\"\n\n   # To:\n   TESTPYPI_TOKEN=$(doppler secrets get TESTPYPI_TOKEN --plain)\n   uv publish --repository testpypi --token \"${TESTPYPI_TOKEN}\"\n   \nDOPPLER_EOF_2\n```\n\n4. **Test publish**:\n\n   ```bash\n   ./scripts/publish-to-pypi.sh\n   ```\n\n5. **Verify on TestPyPI**:\n   - https://test.pypi.org/project/your-package/\n\n6. **Restore script** to production configuration\n\n---\n\n## Related Documentation\n\n- **ADR-0027**: `docs/architecture/decisions/0027-local-only-pypi-publishing.md` - Architectural decision for local-only publishing\n- **ADR-0028**: `docs/architecture/decisions/0028-skills-documentation-alignment.md` - Skills alignment with ADR-0027\n- **PUBLISHING.md**: `docs/development/PUBLISHING.md` - Complete release workflow guide\n- **semantic-release Skill**: [`semantic-release`](../semantic-release/SKILL.md) - Versioning automation (NO publishing)\n- **Bundled Script**: [`scripts/publish-to-pypi.sh`](./scripts/publish-to-pypi.sh) - Reference implementation with CI guards\n\n---\n\n## Validation History\n\n- **2025-12-03**: Refactored to discovery-first, environment-agnostic approach\n  - `discover_uv()` checks PATH  direct installs  version managers (priority order)\n  - Supports: curl install, Homebrew, cargo, mise, asdf - doesn't force any method\n  - Early discovery at startup before any workflow steps\n  - Troubleshooting for non-interactive shell issues\n- **2025-11-22**: Created with ADR-0027 alignment (workspace-wide local-only policy)\n- **Validation**: CI detection guards tested, Doppler integration verified\n\n---\n\n**Last Updated**: 2025-12-03\n**Policy**: Workspace-wide local-only PyPI publishing (ADR-0027)\n**Supersedes**: None (created with ADR-0027 compliance from start)"
              },
              {
                "name": "semantic-release",
                "description": "Automate versioning with Node.js semantic-release v25+. TRIGGERS - npm run release, version bump, changelog, conventional commits, release automation.",
                "path": "plugins/itp/skills/semantic-release/SKILL.md",
                "frontmatter": {
                  "name": "semantic-release",
                  "description": "Automate versioning with Node.js semantic-release v25+. TRIGGERS - npm run release, version bump, changelog, conventional commits, release automation.",
                  "allowed-tools": "Read, Bash, Glob, Grep, Edit, Write"
                },
                "content": "# semantic-release\n\n## Overview\n\nAutomate semantic versioning and release management using **semantic-release v25+ (Node.js)** following 2025 best practices. Works with **all languages** (JavaScript, TypeScript, Python, Rust, Go, C++, etc.) via the `@semantic-release/exec` plugin. Create shareable configurations for multi-repository setups, initialize individual projects with automated releases, and configure GitHub Actions workflows with OIDC trusted publishing.\n\n**Important**: This skill uses semantic-release (Node.js) exclusively, NOT python-semantic-release, even for Python projects. Rationale: 23.5x larger community, 100x+ adoption, better future-proofing.\n\n## When to Use This Skill\n\nInvoke when:\n\n- Setting up local releases for a new project (any language)\n- Creating shareable semantic-release configuration for organization-wide use\n- Migrating existing projects to 2025 semantic-release patterns\n- Troubleshooting semantic-release setup or version bumps\n- Setting up Python projects (use Node.js semantic-release, NOT python-semantic-release)\n- Configuring GitHub Actions (optional backup, not recommended as primary due to speed)\n- Rust workspaces using release-plz (see [Rust reference](./references/rust.md))\n\n## Why Node.js semantic-release\n\n**22,900 GitHub stars** - Large, active community\n**1.9M weekly downloads** - Proven adoption\n**126,000 projects using it** - Battle-tested at scale\n**35+ official plugins** - Rich ecosystem\n**Multi-language support** - Works with any language via `@semantic-release/exec`\n\n**Do NOT use python-semantic-release.** It has a 23.5x smaller community (975 vs 22,900 stars), ~100x less adoption, and is not affiliated with the semantic-release organization.\n\n---\n\n## Release Workflow Philosophy: Local-First\n\n**Default approach: Run releases locally, not via GitHub Actions.**\n\n### Why Local Releases\n\n**Primary argument: GitHub Actions is slow**\n\n-  GitHub Actions: 2-5 minute wait for release to complete\n-  Local release: Instant feedback and file updates\n-  Immediate workflow continuity - no waiting for CI/CD\n\n**Additional benefits:**\n\n-  **Instant local file sync** - `package.json`, `CHANGELOG.md`, tags updated immediately\n-  **No pull required** - Continue working without `git pull` after release\n-  **Dry-run testing** - `npm run release:dry` to preview changes before release\n-  **Offline capable** - Can release without CI/CD dependency\n-  **Faster iteration** - Debug release issues immediately, not through CI logs\n\n### GitHub Actions: Optional Backup Only\n\nGitHub Actions workflows are provided as **optional automation**, not the primary method:\n\n- Use for team consistency if required\n- Backup if local environment unavailable\n- **Not recommended as primary workflow due to speed**\n\n### Authentication Setup\n\n```bash\ngh auth login\n# Browser authentication once\n# Credentials stored in keyring\n# All future releases: zero manual intervention\n```\n\n**This is the minimum manual intervention possible** for local semantic-release with GitHub plugin functionality.\n\n### Multi-Account Authentication via mise [env]\n\nFor multi-account GitHub setups, use mise `[env]` to set per-directory GH_TOKEN:\n\n```toml\n# ~/your-project/.mise.toml\n[env]\nGH_TOKEN = \"{{ read_file(path=env.HOME ~ '/.claude/.secrets/gh-token-accountname') | trim }}\"\nGITHUB_TOKEN = \"{{ read_file(path=env.HOME ~ '/.claude/.secrets/gh-token-accountname') | trim }}\"\n```\n\nThis overrides gh CLI's global authentication, ensuring semantic-release uses the correct account for each directory.\n\nSee the [`mise-configuration` skill](../mise-configuration/SKILL.md#github-token-multi-account-patterns) for complete setup.\n\n### mise Task Detection\n\nWhen `.mise.toml` has release tasks, prefer `mise run` over `npm run`:\n\n| Priority | Condition | Command |\n|----------|-----------|---------|\n| **1** | `.mise.toml` has `[tasks.release:*]` | `mise run release:version` |\n| **2** | `package.json` has `scripts.release` | `npm run release` |\n| **3** | Global semantic-release | `semantic-release --no-ci` |\n\nSee [Python Guide](./references/python.md#mise-4-phase-workflow) for complete mise workflow example.\n\n### GitHub Actions Policy\n\n**CRITICAL: No testing or linting in GitHub Actions.** See CLAUDE.md for full policy.\n\n| Forbidden | Allowed |\n|-----------|---------|\n| pytest, npm test, cargo test | semantic-release |\n| ruff, eslint, clippy, prettier | CodeQL, npm audit |\n| mypy | Deployment, Dependabot |\n\n---\n\n## Separation of Concerns (4-Level Architecture)\n\nsemantic-release configuration follows a hierarchical, composable pattern:\n\n**Level 1: Skill** - `${CLAUDE_PLUGIN_ROOT}/skills/semantic-release/` (Generic templates, system-wide tool)\n**Level 2: User Config** - `~/semantic-release-config/` (`@username/semantic-release-config`)\n**Level 3: Organization Config** - npm registry (`@company/semantic-release-config`)\n**Level 4: Project Config** - `.releaserc.yml` in project root\n\n### Configuration Precedence\n\n```\nLevel 4 (Project)  overrides  Level 3 (Org)  overrides  Level 2 (User)  overrides  Defaults\n```\n\n---\n\n## Conventional Commits Format\n\nsemantic-release analyzes commit messages to determine version bumps:\n\n```\n<type>(<scope>): <subject>\n```\n\n### Version Bump Rules (Default)\n\n- `feat:`  MINOR version bump (0.1.0  0.2.0)\n- `fix:`  PATCH version bump (0.1.0  0.1.1)\n- `BREAKING CHANGE:` or `feat!:`  MAJOR version bump (0.1.0  1.0.0)\n- `docs:`, `chore:`, `style:`, `refactor:`, `perf:`, `test:`  No version bump (by default)\n\n### Release Notes Visibility (Important)\n\n**Warning**: The `@semantic-release/release-notes-generator` (Angular preset) only includes these types in release notes:\n\n- `feat:`  **Features** section\n- `fix:`  **Bug Fixes** section\n- `perf:`  **Performance Improvements** section\n\nOther types (`docs:`, `chore:`, `refactor:`, etc.) trigger releases when configured but **do NOT appear in release notes**.\n\n**Recommendation**: For documentation changes that should be visible in release notes, use:\n\n```\nfix(docs): description of documentation improvement\n```\n\nThis ensures the commit appears in the \"Bug Fixes\" section while still being semantically accurate (fixing documentation gaps is a fix).\n\n### Marketplace Plugin Configuration (Always Bump)\n\nFor Claude Code marketplace plugins, **every change requires a version bump** for users to receive updates.\n\n**Option A: Shareable Config (if published)**\n\n```yaml\n# .releaserc.yml\nextends: \"@terryli/semantic-release-config/marketplace\"\n```\n\n**Option B: Inline Configuration**\n\n```yaml\n# .releaserc.yml\nplugins:\n  - - \"@semantic-release/commit-analyzer\"\n    - releaseRules:\n        # Marketplace plugins require version bump for ANY change\n        - { type: \"docs\", release: \"patch\" }\n        - { type: \"chore\", release: \"patch\" }\n        - { type: \"style\", release: \"patch\" }\n        - { type: \"refactor\", release: \"patch\" }\n        - { type: \"test\", release: \"patch\" }\n        - { type: \"build\", release: \"patch\" }\n        - { type: \"ci\", release: \"patch\" }\n```\n\n**Result after configuration:**\n\n| Commit Type                                                        | Release Type       |\n| ------------------------------------------------------------------ | ------------------ |\n| `feat:`                                                            | minor (default)    |\n| `fix:`, `perf:`, `revert:`                                         | patch (default)    |\n| `docs:`, `chore:`, `style:`, `refactor:`, `test:`, `build:`, `ci:` | patch (configured) |\n\n**Why marketplace plugins need this**: Plugin updates are distributed via version tags. Without a version bump, users running `/plugin update` see no changes even if content was modified.\n\n### MANDATORY: Every Release Must Increment Version\n\n**Pre-release validation**: Before running semantic-release, verify releasable commits exist since last tag. A release without version increment is invalid.\n\n**Autonomous check sequence**:\n\n1. List commits since last tag: compare HEAD against latest version tag\n2. Identify commit types: scan for `feat:`, `fix:`, or `BREAKING CHANGE:` prefixes\n3. If NO releasable commits found  **STOP** - do not proceed with release\n4. Inform user: \"No version-bumping commits since last release. Use `feat:` or `fix:` prefix for releasable changes.\"\n\n**Commit type selection guidance**:\n\n- Use `fix:` for any change that improves existing behavior (bug fixes, enhancements, documentation corrections that affect usage)\n- Use `feat:` for new capabilities or significant additions\n- Reserve `chore:`, `docs:`, `refactor:` for changes that truly don't warrant a release\n\n**Why this matters**: A release without version increment creates confusion - users cannot distinguish between releases, package managers may cache old versions, and changelog entries become meaningless.\n\n### MAJOR Version Breaking Change Confirmation\n\n**Trigger**: `BREAKING CHANGE:` footer or `feat!:` / `fix!:` prefix in commits.\n\nWhen MAJOR is detected, this skill runs a **3-phase confirmation workflow**:\n\n1. **Detection**: Scan commits for breaking change markers\n2. **Analysis**: Spawn 3 parallel subagents (User Impact, API Compat, Migration)\n3. **Confirmation**: AskUserQuestion with proceed/downgrade/abort options\n\nSee [MAJOR Confirmation Workflow](./references/major-confirmation.md) for complete details including subagent prompts, decision tree, and example output.\n\n### Examples\n\n**Feature (MINOR)**:\n\n```\nfeat: add BigQuery data source support\n```\n\n**Bug Fix (PATCH)**:\n\n```\nfix: correct timestamp parsing for UTC offsets\n```\n\n**Breaking Change (MAJOR)**:\n\n```\nfeat!: change API to require authentication\n\nBREAKING CHANGE: All API calls now require API key in Authorization header.\n```\n\n---\n\n## Documentation Linking\n\nAuto-include doc changes in release notes. Add to `.releaserc.yml`:\n\n```yaml\n- - \"@semantic-release/exec\"\n  - generateNotesCmd: \"node plugins/itp/skills/semantic-release/scripts/generate-doc-notes.mjs ${lastRelease.gitTag}\"\n```\n\nDetects: ADRs, Design Specs, Skills, Plugin READMEs. See [Doc Release Linking](./references/doc-release-linking.md).\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n| Check | Command | Fix |\n|-------|---------|-----|\n| gh CLI authenticated | `gh auth status` | `gh auth login` |\n| GH_TOKEN for directory | `gh api user --jq '.login'` | See [Authentication](./references/authentication.md) |\n| Git remote is HTTPS | `git remote get-url origin` | `git-ssh-to-https` |\n| semantic-release global | `command -v semantic-release` | See [Troubleshooting](./references/troubleshooting.md#macos-gatekeeper-blocks-node-files) |\n\n### Initialize Project\n\n```bash\n./scripts/init-project.mjs --project   # Initialize current project\n./scripts/init-project.mjs --user      # Create user-level shareable config\n./scripts/init-project.mjs --help      # See all options\n```\n\n### Run Release\n\n| Priority | Condition | Commands |\n|----------|-----------|----------|\n| **1** | `.mise.toml` has release tasks | `mise run release:version` / `mise run release:full` |\n| **2** | `package.json` has scripts | `npm run release:dry` (preview) / `npm run release` |\n| **3** | Global CLI | `semantic-release --no-ci` |\n\nSee [Local Release Workflow](./references/local-release-workflow.md) for the complete 4-phase process.\n\n### Python Projects\n\nsemantic-release handles versioning. For PyPI publishing, see [`pypi-doppler` skill](../pypi-doppler/SKILL.md).\n\n**Version pattern** (importlib.metadata - never hardcode):\n\n```python\nfrom importlib.metadata import PackageNotFoundError, version\ntry:\n    __version__ = version(\"your-package-name\")\nexcept PackageNotFoundError:\n    __version__ = \"0.0.0+dev\"\n```\n\nSee [Python Projects Guide](./references/python.md) for complete setup including Rust+Python hybrids.\n\n### GitHub Actions (Optional)\n\nNot recommended as primary (2-5 minute delay). Repository Settings  Actions  Workflow permissions  Enable \"Read and write permissions\".\n\n---\n\n## Reference Documentation\n\n| Category | Reference | Description |\n|----------|-----------|-------------|\n| **Setup** | [Authentication](./references/authentication.md) | HTTPS-first setup, multi-account patterns |\n| **Workflow** | [Local Release Workflow](./references/local-release-workflow.md) | 4-phase process (PREFLIGHT  RELEASE  POSTFLIGHT) |\n| **Languages** | [Python Projects](./references/python.md) | Python + Rust+Python hybrid patterns |\n| | [Rust Projects](./references/rust.md) | release-plz, cargo-rdme README SSoT |\n| **Config** | [Version Alignment](./references/version-alignment.md) | Git tags as SSoT, manifest patterns |\n| | [Monorepo Support](./references/monorepo-support.md) | pnpm/npm workspaces configuration |\n| **Advanced** | [MAJOR Confirmation](./references/major-confirmation.md) | Breaking change analysis workflow |\n| | [Doc Release Linking](./references/doc-release-linking.md) | Auto-link ADRs/specs in release notes |\n| **Help** | [Troubleshooting](./references/troubleshooting.md) | All common issues consolidated |\n| | [Evolution Log](./references/evolution-log.md) | Skill change history |\n\n**External**: [`pypi-doppler` skill](../pypi-doppler/SKILL.md) - Local PyPI publishing with Doppler\n\n---\n\n## Post-Change Checklist\n\nAfter modifying THIS skill (semantic-release):\n\n1. [ ] SKILL.md and references remain aligned\n2. [ ] New references documented in Reference Documentation table\n3. [ ] All referenced files in references/ exist\n4. [ ] Append changes to [evolution-log.md](./references/evolution-log.md)\n5. [ ] Validate with `bun scripts/validate-plugins.mjs`\n6. [ ] Run `npm run release:dry` to verify no regressions"
              }
            ]
          },
          {
            "name": "gh-tools",
            "description": "GitHub workflow automation: GFM link validation, WebFetch enforcement (use gh CLI instead)",
            "source": "./plugins/gh-tools/",
            "category": "development",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install gh-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/hooks",
                "description": "Install/uninstall gh-tools hooks to ~/.claude/settings.json",
                "path": "plugins/gh-tools/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall gh-tools hooks to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead",
                  "argument-hint": "[install|uninstall|status]"
                },
                "content": "# gh-tools Hooks Manager\n\nManage gh-tools hook installation in `~/.claude/settings.json`.\n\nThis hook soft-blocks WebFetch requests to github.com URLs and suggests using the `gh` CLI instead for better data access.\n\n## Actions\n\n| Action      | Description                                     |\n| ----------- | ----------------------------------------------- |\n| `status`    | Check hook installation status and dependencies |\n| `install`   | Add gh-tools hooks to settings.json             |\n| `uninstall` | Remove gh-tools hooks from settings.json        |\n\n## Why Use gh CLI Instead of WebFetch?\n\n| Aspect         | WebFetch           | gh CLI               |\n| -------------- | ------------------ | -------------------- |\n| Authentication | None               | gh auth token        |\n| Data format    | HTML scraping      | Native JSON API      |\n| Rate limits    | Strict (anonymous) | Higher (authenticated) |\n| Pagination     | Manual             | Automatic            |\n| Metadata       | Limited            | Full (labels, etc.)  |\n\n## Execution\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'GH_TOOLS_HOOKS_SCRIPT'\nset -euo pipefail\n\nACTION=\"${ARGUMENTS:-status}\"\n\n# Auto-detect plugin root\ndetect_plugin_root() {\n    if [[ -n \"${CLAUDE_PLUGIN_ROOT:-}\" ]]; then\n        echo \"$CLAUDE_PLUGIN_ROOT\"\n        return\n    fi\n    local marketplace=\"$HOME/.claude/plugins/marketplaces/cc-skills/plugins/gh-tools\"\n    if [[ -d \"$marketplace/hooks\" ]]; then\n        echo \"$marketplace\"\n        return\n    fi\n    local cache_base=\"$HOME/.claude/plugins/cache/cc-skills/gh-tools\"\n    if [[ -d \"$cache_base\" ]]; then\n        local latest\n        latest=$(ls -1 \"$cache_base\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+' | sort -V | tail -1)\n        if [[ -n \"$latest\" && -d \"$cache_base/$latest/hooks\" ]]; then\n            echo \"$cache_base/$latest\"\n            return\n        fi\n    fi\n    echo \"\"\n}\n\nPLUGIN_DIR=\"$(detect_plugin_root)\"\nif [[ -z \"$PLUGIN_DIR\" ]]; then\n    echo \"ERROR: Cannot detect gh-tools plugin installation\" >&2\n    exit 1\nfi\n\nbash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" \"$ACTION\"\nGH_TOOLS_HOOKS_SCRIPT\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe hooks are loaded at session start. Modifications to settings.json require a restart.\n\n## Reference\n\n- [ADR: gh-tools WebFetch Enforcement](/docs/adr/2026-01-03-gh-tools-webfetch-enforcement.md)"
              }
            ],
            "skills": [
              {
                "name": "pr-gfm-validator",
                "description": "Validate and auto-fix GitHub Flavored Markdown links in PR descriptions. Use when creating pull requests, mentioning PR links, gh pr create, GFM validation, or fixing broken PR links. Converts repo-relative paths to branch-specific blob URLs.",
                "path": "plugins/gh-tools/skills/pr-gfm-validator/SKILL.md",
                "frontmatter": {
                  "name": "pr-gfm-validator",
                  "description": "Validate and auto-fix GitHub Flavored Markdown links in PR descriptions. Use when creating pull requests, mentioning PR links, gh pr create, GFM validation, or fixing broken PR links. Converts repo-relative paths to branch-specific blob URLs."
                },
                "content": "# PR GFM Link Validator\n\nValidate and auto-convert GFM links in pull request descriptions to prevent 404 errors.\n\n## When to Use This Skill\n\nThis skill triggers when:\n\n- Creating a pull request from a feature branch\n- Discussing PR descriptions or body content\n- Mentioning GFM links, PR links, or link validation\n- Using `gh pr create` or `gh pr edit`\n\n## The Problem\n\nRepository-relative links in PR descriptions resolve to the **base branch** (main), not the feature branch:\n\n| Link in PR Body            | GitHub Resolves To            | Result                            |\n| -------------------------- | ----------------------------- | --------------------------------- |\n| `[ADR](/docs/adr/file.md)` | `/blob/main/docs/adr/file.md` | 404 (file only on feature branch) |\n\n## The Solution\n\nConvert repo-relative links to absolute blob URLs with the correct branch:\n\n```\n/docs/adr/file.md\n    \nhttps://github.com/{owner}/{repo}/blob/{branch}/docs/adr/file.md\n```\n\n---\n\n## Workflow\n\n### Step 1: Detect Context\n\nBefore any PR operation, gather repository context:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Get repo owner and name\ngh repo view --json nameWithOwner --jq '.nameWithOwner'\n\n# Get current branch\ngit rev-parse --abbrev-ref HEAD\n\n# Check if on feature branch (not main/master)\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\nif [[ \"$BRANCH\" == \"main\" || \"$BRANCH\" == \"master\" ]]; then\n  echo \"On default branch - no conversion needed\"\n  exit 0\nfi\nPREFLIGHT_EOF\n```\n\n### Step 2: Identify Links to Convert\n\nScan PR body for GFM links matching these patterns:\n\n**CONVERT these patterns:**\n\n- `/path/to/file.md` - Repo-root relative\n- `./relative/path.md` - Current-directory relative\n- `../parent/path.md` - Parent-directory relative\n\n**SKIP these patterns:**\n\n- `https://...` - Already absolute URLs\n- `http://...` - Already absolute URLs\n- `#anchor` - In-page anchors\n- `mailto:...` - Email links\n\n### Step 3: Construct Blob URLs\n\nFor each link to convert:\n\n```python\n# Pattern\nf\"https://github.com/{owner}/{repo}/blob/{branch}/{path}\"\n\n# Example\nowner = \"Eon-Labs\"\nrepo = \"alpha-forge\"\nbranch = \"feat/2025-12-01-eth-block-metrics\"\npath = \"docs/adr/2025-12-01-file.md\"\n\n# Result\n\"https://github.com/Eon-Labs/alpha-forge/blob/feat/2025-12-01-eth-block-metrics/docs/adr/2025-12-01-file.md\"\n```\n\n### Step 4: Apply Conversions\n\nReplace all identified links in the PR body:\n\n```markdown\n# Before\n\n[Plugin Design](/docs/adr/2025-12-01-slug.md)\n\n# After\n\n[Plugin Design](https://github.com/Eon-Labs/alpha-forge/blob/feat/branch/docs/adr/2025-12-01-slug.md)\n```\n\n### Step 5: Validate Result\n\nAfter conversion, verify:\n\n1. All repo-relative links are now absolute blob URLs\n2. External links remain unchanged\n3. Anchor links remain unchanged\n\n---\n\n## Integration with gh pr create\n\nWhen creating a PR, apply this workflow automatically:\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\n# 1. Get context\nREPO_INFO=$(gh repo view --json nameWithOwner --jq '.nameWithOwner')\nOWNER=$(echo \"$REPO_INFO\" | cut -d'/' -f1)\nREPO=$(echo \"$REPO_INFO\" | cut -d'/' -f2)\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\n\n# 2. Process PR body (convert links)\n# ... link conversion logic ...\n\n# 3. Create PR with converted body\ngh pr create --title \"...\" --body \"$CONVERTED_BODY\"\nGIT_EOF\n```\n\n---\n\n## Link Detection Regex\n\nUse this regex pattern to find GFM links:\n\n```regex\n\\[([^\\]]+)\\]\\((/[^)]+|\\.\\.?/[^)]+)\\)\n```\n\nBreakdown:\n\n- `\\[([^\\]]+)\\]` - Capture link text\n- `\\(` - Opening parenthesis\n- `(/[^)]+|\\.\\.?/[^)]+)` - Capture path starting with `/`, `./`, or `../`\n- `\\)` - Closing parenthesis\n\n---\n\n## Examples\n\n### Example 1: Simple Repo-Relative Link\n\n**Input:**\n\n```markdown\nSee the [ADR](/docs/adr/2025-12-01-eth-block-metrics.md) for details.\n```\n\n**Context:**\n\n- Owner: `Eon-Labs`\n- Repo: `alpha-forge`\n- Branch: `feat/2025-12-01-eth-block-metrics-data-plugin`\n\n**Output:**\n\n```markdown\nSee the [ADR](https://github.com/Eon-Labs/alpha-forge/blob/feat/2025-12-01-eth-block-metrics-data-plugin/docs/adr/2025-12-01-eth-block-metrics.md) for details.\n```\n\n### Example 2: Multiple Links\n\n**Input:**\n\n```markdown\n## References\n\n- [Plugin Design](/docs/adr/2025-12-01-slug.md)\n- [Probe Integration](/docs/adr/2025-12-02-slug.md)\n- [External Guide](https://example.com/guide)\n```\n\n**Output:**\n\n```markdown\n## References\n\n- [Plugin Design](https://github.com/Eon-Labs/alpha-forge/blob/feat/branch/docs/adr/2025-12-01-slug.md)\n- [Probe Integration](https://github.com/Eon-Labs/alpha-forge/blob/feat/branch/docs/adr/2025-12-02-slug.md)\n- [External Guide](https://example.com/guide)\n```\n\nNote: External link unchanged.\n\n### Example 3: Credential File Link\n\n**Input:**\n\n```markdown\n**See [`.env.clickhouse`](/.env.clickhouse)** for credentials.\n```\n\n**Output:**\n\n```markdown\n**See [`.env.clickhouse`](https://github.com/Eon-Labs/alpha-forge/blob/feat/branch/.env.clickhouse)** for credentials.\n```\n\n---\n\n## Edge Cases\n\n### Already on main/master\n\n- Skip conversion entirely\n- Repo-relative links will work correctly\n\n### Empty PR Body\n\n- Nothing to convert\n- Proceed with PR creation\n\n### No GFM Links Found\n\n- Nothing to convert\n- Proceed with PR creation\n\n### Mixed Link Types\n\n- Convert only repo-relative links\n- Preserve external URLs, anchors, mailto links\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Regex patterns still match intended link formats\n2. [ ] Examples reflect current behavior\n3. [ ] Edge cases documented\n4. [ ] Workflow steps are executable\n\n---\n\n## References\n\n- [GitHub Blob URLs](https://docs.github.com/en/repositories/working-with-files/using-files/getting-permanent-links-to-files)\n- [GFM Link Syntax](https://github.github.com/gfm/#links)\n- [gh CLI Documentation](https://cli.github.com/manual/gh_pr_create)"
              }
            ]
          },
          {
            "name": "link-tools",
            "description": "Comprehensive link validation: portability checks, lychee broken link detection, path policy linting",
            "source": "./plugins/link-tools/",
            "category": "quality",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install link-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "link-validation",
                "description": "Universal link validation using lychee for Claude Code sessions. Runs at session end to detect broken links and path policy violations.",
                "path": "plugins/link-tools/skills/link-validation/SKILL.md",
                "frontmatter": {
                  "name": "link-validation",
                  "description": "Universal link validation using lychee for Claude Code sessions. Runs at session end to detect broken links and path policy violations.",
                  "triggers": [
                    "link validation",
                    "broken links",
                    "lychee",
                    "check links",
                    "markdown links"
                  ]
                },
                "content": "# Link Validation Skill\n\nValidates markdown links in your workspace using [lychee](https://github.com/lycheeverse/lychee).\n\n## What It Does\n\nAt session end (Stop hook), this skill:\n\n1. **Discovers** all markdown files in your workspace\n2. **Runs lychee** to check for broken links\n3. **Lints paths** for policy violations (absolute paths, excessive traversal)\n4. **Outputs JSON** results for programmatic consumption\n\n## Requirements\n\n- [lychee](https://github.com/lycheeverse/lychee) installed (`brew install lychee`)\n- Python 3.11+ and uv\n\n## Output\n\nResults are written to `.link-check-results.md` in your workspace:\n\n```markdown\n# Link Check Results\n\n**Correlation ID**: `01JEGQXV8KHTNF3YD8G7ZC9XYK`\n\n## Lychee Link Validation\n\nNo broken links found.\n\n## Path Policy Violations\n\nNo path violations found.\n```\n\n## Path Policy Rules\n\n| Rule                 | Severity | Description                            |\n| -------------------- | -------- | -------------------------------------- |\n| NO_ABSOLUTE_PATHS    | Error    | Filesystem absolute paths not allowed  |\n| NO_PARENT_ESCAPES    | Warning  | Excessive `../` may escape repository  |\n| MARKETPLACE_RELATIVE | Warning  | Plugins should use `./` relative paths |\n\n## Configuration\n\nOverride the default lychee config by placing `.lycheerc.toml` in your workspace root.\n\nSee [config/lychee.toml](../../config/lychee.toml) for the default configuration.\n\n## References\n\n- [ADR: Link Checker Plugin Extraction](../../../../docs/adr/2025-12-11-link-checker-plugin-extraction.md)\n- [Design Spec](../../../../docs/design/2025-12-11-link-checker-plugin-extraction/spec.md)\n- [lychee Documentation](https://github.com/lycheeverse/lychee)"
              },
              {
                "name": "link-validator",
                "description": "Validate markdown link portability in Claude Code skills and plugins. Use when checking links, validating portability, fixing broken links, absolute paths, relative paths, or before skill distribution. Ensures relative paths for cross-installation compatibility.",
                "path": "plugins/link-tools/skills/link-validator/SKILL.md",
                "frontmatter": {
                  "name": "link-validator",
                  "description": "Validate markdown link portability in Claude Code skills and plugins. Use when checking links, validating portability, fixing broken links, absolute paths, relative paths, or before skill distribution. Ensures relative paths for cross-installation compatibility."
                },
                "content": "# Link Validator\n\nValidates markdown links in Claude Code skills for portability across installation locations.\n\n## The Problem\n\nSkills with absolute repo paths break when installed elsewhere:\n\n| Path Type       | Example                 | Works When Installed?   |\n| --------------- | ----------------------- | ----------------------- |\n| Absolute repo   | `/skills/foo/SKILL.md`  | No - path doesn't exist |\n| Relative        | `./references/guide.md` | Yes - always resolves   |\n| Relative parent | `../sibling/SKILL.md`   | Yes - always resolves   |\n\n## When to Use This Skill\n\n- Before distributing a skill/plugin\n- After creating new markdown links in skills\n- When CI reports link validation failures\n- To audit existing skills for portability issues\n\n---\n\n## TodoWrite Task Templates\n\n### Template A: Validate Single Skill\n\n```\n1. Identify skill path to validate\n2. Run: uv run scripts/validate_links.py <skill-path>\n3. Review violation report (if any)\n4. For each violation, apply suggested fix\n5. Re-run validator to confirm all fixed\n```\n\n### Template B: Validate Plugin (Multiple Skills)\n\n```\n1. Identify plugin root directory\n2. Run: uv run scripts/validate_links.py <plugin-path>\n3. Review grouped violations by skill\n4. Fix violations skill-by-skill\n5. Re-validate entire plugin\n```\n\n### Template C: Fix Violations\n\n```\n1. Read violation report output\n2. Locate file and line number\n3. Review suggested relative path\n4. Apply fix using Edit tool\n5. Re-run validator on file\n```\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Script remains in sync with latest patterns\n2. [ ] References updated if new patterns added\n3. [ ] Tested on real skill with violations\n\n---\n\n## Quick Start\n\n```bash\n# Validate a single skill\nuv run scripts/validate_links.py ~/.claude/skills/my-skill/\n\n# Validate a plugin with multiple skills\nuv run scripts/validate_links.py ~/.claude/plugins/my-plugin/\n\n# Dry-run in current directory\nuv run scripts/validate_links.py .\n```\n\n## Exit Codes\n\n| Code | Meaning                                 |\n| ---- | --------------------------------------- |\n| 0    | All links valid (relative paths)        |\n| 1    | Violations found (absolute repo paths)  |\n| 2    | Error (invalid path, no markdown files) |\n\n## What Gets Checked\n\n**Flagged as Violations:**\n\n- `/skills/foo/SKILL.md` - Absolute repo path\n- `/docs/guide.md` - Absolute repo path\n\n**Allowed (Pass):**\n\n- `./references/guide.md` - Relative same directory\n- `../sibling/SKILL.md` - Relative parent\n- `https://example.com` - External URL\n- `#section` - Anchor link\n\n## Reference Documentation\n\n- [Link Patterns Reference](./references/link-patterns.md) - Detailed pattern explanations and fix strategies"
              }
            ]
          },
          {
            "name": "devops-tools",
            "description": "DevOps automation: ClickHouse, Doppler, Telegram, MLflow, notifications, session recovery",
            "source": "./plugins/devops-tools/",
            "category": "devops",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install devops-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "clickhouse-cloud-management",
                "description": "This skill should be used when the user asks to \"create ClickHouse user\", \"manage ClickHouse permissions\", \"test ClickHouse connection\", \"troubleshoot ClickHouse Cloud\", or mentions ClickHouse Cloud credentials, API keys, or SQL user management.",
                "path": "plugins/devops-tools/skills/clickhouse-cloud-management/SKILL.md",
                "frontmatter": {
                  "name": "clickhouse-cloud-management",
                  "description": "This skill should be used when the user asks to \"create ClickHouse user\", \"manage ClickHouse permissions\", \"test ClickHouse connection\", \"troubleshoot ClickHouse Cloud\", or mentions ClickHouse Cloud credentials, API keys, or SQL user management.",
                  "allowed-tools": "Read, Bash"
                },
                "content": "# ClickHouse Cloud Management\n\nADR: 2025-12-08-clickhouse-cloud-management-skill\n\n## Overview\n\nClickHouse Cloud user and permission management via SQL commands over HTTP interface. This skill covers database user creation, permission grants, and credential management for ClickHouse Cloud instances.\n\n## When to Use This Skill\n\nInvoke this skill when:\n\n- Creating database users for ClickHouse Cloud\n- Managing user permissions (GRANT/REVOKE)\n- Testing ClickHouse Cloud connectivity\n- Troubleshooting authentication issues\n- Understanding API key vs database user distinction\n\n## Key Concepts\n\n### Management Options\n\nClickHouse Cloud provides two management interfaces with different capabilities:\n\n| Task                 | Via SQL (CLI/HTTP) | Via Cloud Console |\n| -------------------- | ------------------ | ----------------- |\n| Create database user | CREATE USER        | Supported         |\n| Grant permissions    | GRANT              | Supported         |\n| Delete user          | DROP USER          | Supported         |\n| Create API key       | Not possible       | Only here         |\n\n**Key distinction**: Database users (created via SQL) authenticate to ClickHouse itself. API keys (created via console) authenticate to the ClickHouse Cloud management API.\n\n### Connection Details\n\nClickHouse Cloud exposes only HTTP interface publicly:\n\n- **Port**: 443 (HTTPS)\n- **Protocol**: HTTP (not native ClickHouse protocol)\n- **Native protocol**: Requires AWS PrivateLink (not available without enterprise setup)\n\n### Password Requirements\n\nClickHouse Cloud enforces strong password policy:\n\n- Minimum 12 characters\n- At least 1 uppercase letter\n- At least 1 special character\n\nExample compliant password: `StrongPass@2025!`\n\n## Quick Reference\n\n### Create Read-Only User\n\n```bash\ncurl -s \"https://default:PASSWORD@HOST:443/\" --data-binary \\\n  \"CREATE USER my_reader IDENTIFIED BY 'StrongPass@2025!' SETTINGS readonly = 1\"\n```\n\n### Grant Database Access\n\n```bash\ncurl -s \"https://default:PASSWORD@HOST:443/\" --data-binary \\\n  \"GRANT SELECT ON deribit.* TO my_reader\"\n```\n\n### Delete User\n\n```bash\ncurl -s \"https://default:PASSWORD@HOST:443/\" --data-binary \\\n  \"DROP USER my_reader\"\n```\n\nFor comprehensive SQL patterns and advanced permission scenarios, see [SQL Patterns Reference](./references/sql-patterns.md).\n\n## Credential Sources\n\n### 1Password Items (Engineering Vault)\n\n| Item                                             | Purpose                                   |\n| ------------------------------------------------ | ----------------------------------------- |\n| ClickHouse Cloud - API Key (Admin)               | Cloud management API (console operations) |\n| ClickHouse Cloud - API Key (Developer Read-only) | Cloud management API (read-only)          |\n| gapless-deribit-clickhouse                       | Database `default` user credentials       |\n\n### Retrieving Credentials\n\n```bash\n# Database credentials (for SQL commands)\nop item get \"gapless-deribit-clickhouse\" --vault Engineering --reveal\n\n# API key (for cloud management API)\nop item get \"ClickHouse Cloud - API Key (Admin)\" --vault Engineering --reveal\n```\n\n## Common Workflows\n\n### Workflow 1: Create Application User\n\n1. Retrieve `default` user credentials from 1Password\n2. Create new user with appropriate permissions:\n\n```bash\nHOST=\"your-instance.clickhouse.cloud\"\nPASSWORD=\"default-user-password\"\n\n# Create user\ncurl -s \"https://default:$PASSWORD@$HOST:443/\" --data-binary \\\n  \"CREATE USER app_user IDENTIFIED BY 'AppPass@2025!'\"\n\n# Grant specific database access\ncurl -s \"https://default:$PASSWORD@$HOST:443/\" --data-binary \\\n  \"GRANT SELECT, INSERT ON mydb.* TO app_user\"\n```\n\n### Workflow 2: Verify User Exists\n\n```bash\ncurl -s \"https://default:$PASSWORD@$HOST:443/\" --data-binary \"SHOW USERS\"\n```\n\n### Workflow 3: Test Connection\n\n```bash\ncurl -s \"https://user:password@HOST:443/\" --data-binary \"SELECT 1\"\n```\n\nExpected output: `1` (single row with value 1)\n\n## Troubleshooting\n\n### Authentication Failed\n\n- Verify password meets complexity requirements\n- Check host URL includes port 443\n- Ensure using HTTPS (not HTTP)\n\n### Permission Denied\n\n- Verify user has required GRANT statements\n- Check database and table names are correct\n- Confirm user was created with correct settings\n\n### Connection Timeout\n\n- ClickHouse Cloud only exposes port 443 publicly\n- Native protocol (port 9440) requires PrivateLink\n- Use HTTP interface with curl or clickhouse-client HTTP mode\n\n## Next Steps After User Creation\n\n<!-- ADR: 2025-12-10-clickhouse-skill-delegation -->\n\nAfter creating a ClickHouse user, invoke **`devops-tools:clickhouse-pydantic-config`** to generate DBeaver configuration with the new credentials.\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and advanced techniques, consult:\n\n- **[references/sql-patterns.md](./references/sql-patterns.md)** - Complete SQL syntax reference with examples\n\n## Python Driver Policy\n\nFor Python application code connecting to ClickHouse Cloud, use `clickhouse-connect` (official HTTP driver). See [`clickhouse-architect`](../../../quality-tools/skills/clickhouse-architect/SKILL.md#python-driver-policy) for recommended code patterns and why to avoid `clickhouse-driver` (community).\n\n## Related Skills\n\n- `quality-tools:clickhouse-architect` - Schema design, compression codecs, Python driver policy\n- `devops-tools:clickhouse-pydantic-config` - DBeaver configuration generation\n- `devops-tools:doppler-secret-validation` - For storing credentials in Doppler\n- `devops-tools:doppler-workflows` - For credential rotation workflows"
              },
              {
                "name": "clickhouse-pydantic-config",
                "description": "Generate DBeaver configurations from Pydantic ClickHouse connection models. Use when user mentions \"DBeaver config\", \"ClickHouse connection setup\", \"database client configuration\", \"generate connection JSON\", \"mise SSoT connection\", or needs consistent connection configuration across development tools.\n",
                "path": "plugins/devops-tools/skills/clickhouse-pydantic-config/SKILL.md",
                "frontmatter": {
                  "name": "clickhouse-pydantic-config",
                  "description": "Generate DBeaver configurations from Pydantic ClickHouse connection models. Use when user mentions \"DBeaver config\", \"ClickHouse connection setup\", \"database client configuration\", \"generate connection JSON\", \"mise SSoT connection\", or needs consistent connection configuration across development tools.\n",
                  "allowed-tools": "Read, Bash, Grep"
                },
                "content": "# ClickHouse Pydantic Config\n\n<!-- ADR: 2025-12-09-clickhouse-pydantic-config-skill -->\n\nGenerate DBeaver database client configurations from Pydantic v2 models using mise `[env]` as Single Source of Truth (SSoT).\n\n## Critical Design Principle: Semi-Prescriptive Adaptation\n\n**This skill is NOT a rigid template.** It provides a SSoT pattern that MUST be adapted to each repository's structure and local database situation.\n\n### Why This Matters\n\nEach repository has unique:\n\n- Directory layouts (`.dbeaver/` location may vary)\n- Environment variable naming conventions\n- Existing connection management patterns\n- Local vs cloud database mix\n\n**The SSoT principle is the constant; the implementation details are the variables.**\n\n## Quick Start\n\n```bash\n# Generate local connection config\nmise run db-client-generate\n\n# Generate cloud connection config\nmise run db-client:cloud\n\n# Preview without writing\nmise run db-client:dry-run\n\n# Launch DBeaver\nmise run dbeaver\n```\n\n## Credential Prerequisites (Cloud Mode)\n\n<!-- ADR: 2025-12-10-clickhouse-skill-documentation-gaps -->\n\nBefore using cloud mode, obtain credentials via the skill chain:\n\n1. **Create/retrieve user**: Use `clickhouse-cloud-management` skill to create read-only users or retrieve existing credentials from 1Password\n2. **Store in .env**: Add to `.env` file (gitignored):\n\n```bash\nCLICKHOUSE_USER_READONLY=your_user\nCLICKHOUSE_PASSWORD_READONLY=your_password\n```\n\n1. **Generate config**: Run `mise run db-client:cloud`\n\n**Skill chain**: `clickhouse-cloud-management`  `.env`  `clickhouse-pydantic-config`\n\n## mise `[env]` as Single Source of Truth\n\nAll configurable values live in `.mise.toml`:\n\n```toml\n[env]\nCLICKHOUSE_NAME = \"clickhouse-local\"\nCLICKHOUSE_MODE = \"local\"  # \"local\" or \"cloud\"\nCLICKHOUSE_HOST = \"localhost\"\nCLICKHOUSE_PORT = \"8123\"\nCLICKHOUSE_DATABASE = \"default\"\n```\n\nScripts read from `os.environ.get()` with backward-compatible defaultsworks with or without mise installed.\n\n## Credential Handling by Mode\n\n| Mode      | Approach                                | Rationale                                       |\n| --------- | --------------------------------------- | ----------------------------------------------- |\n| **Local** | Hardcode `default` user, empty password | Zero friction, no security concern              |\n| **Cloud** | Pre-populate from `.env`                | Read from environment, write to gitignored JSON |\n\n**Key principle**: The generated `data-sources.json` is gitignored anyway. Pre-populating credentials trades zero security risk for maximum developer convenience.\n\n### Cloud Credentials Setup\n\n```bash\n# .env (gitignored)\nCLICKHOUSE_USER_READONLY=readonly_user\nCLICKHOUSE_PASSWORD_READONLY=your-secret-password\n```\n\n## Repository Adaptation Workflow\n\n### Pre-Implementation Discovery (Phase 0)\n\nBefore writing any code, the executor MUST:\n\n```bash\n# 1. Discover existing configuration patterns\nfd -t f \".mise.toml\" .\nfd -t f \".env*\" .\nfd -t d \".dbeaver\" .\n\n# 2. Test ClickHouse connectivity (local)\nclickhouse-client --host localhost --port 9000 --query \"SELECT 1\"\n\n# 3. Check for existing connection configs\nfd -t f \"data-sources.json\" .\nfd -t f \"dataSources.xml\" .\n```\n\n### Adaptation Decision Matrix\n\n| Discovery Finding                  | Adaptation Action                                      |\n| ---------------------------------- | ------------------------------------------------------ |\n| Existing `.mise.toml` at repo root | Extend existing `[env]` section, don't create new file |\n| Existing `.dbeaver/` directory     | Merge connections, preserve existing entries           |\n| Non-standard CLICKHOUSE\\_\\* vars   | Map to repository's naming convention                  |\n| Multiple databases (local + cloud) | Generate multiple connection entries                   |\n| No ClickHouse available            | Warn and generate placeholder config                   |\n\n### Validation Checklist (Post-Generation)\n\nThe executor MUST verify:\n\n- [ ] Generated JSON is valid (`jq . .dbeaver/data-sources.json`)\n- [ ] DBeaver can import the config (launch and verify connection appears)\n- [ ] mise tasks execute without error (`mise run db-client-generate`)\n- [ ] `.dbeaver/` added to `.gitignore`\n\n## Pydantic Model\n\nThe `ClickHouseConnection` model provides:\n\n- **Type-safe configuration** with Pydantic v2 validation\n- **Computed fields** for JDBC URL and connection ID\n- **Mode-aware defaults** (cloud auto-enables SSL on port 8443)\n- **Environment loading** via `from_env()` class method\n\nSee [references/pydantic-model.md](./references/pydantic-model.md) for complete model documentation.\n\n## DBeaver Format\n\nDBeaver uses `.dbeaver/data-sources.json` with this structure:\n\n```json\n{\n  \"folders\": {},\n  \"connections\": {\n    \"clickhouse-jdbc-{random-hex}\": {\n      \"provider\": \"clickhouse\",\n      \"driver\": \"com_clickhouse\",\n      \"name\": \"Connection Name\",\n      \"configuration\": { ... }\n    }\n  }\n}\n```\n\n**Important**: DBeaver does NOT support `${VAR}` substitutionvalues must be pre-populated at generation time.\n\nSee [references/dbeaver-format.md](./references/dbeaver-format.md) for complete format specification.\n\n## macOS Notes\n\n1. **DBeaver binary**: Use `/Applications/DBeaver.app/Contents/MacOS/dbeaver` (NOT `open -a`)\n2. **Gitignore**: Add `.dbeaver/` to `.gitignore`\n\n## Related Skills\n\n| Skill                                      | Integration                         |\n| ------------------------------------------ | ----------------------------------- |\n| `devops-tools:clickhouse-cloud-management` | Credential retrieval for cloud mode |\n| `quality-tools:clickhouse-architect`       | Schema design context               |\n| `itp:mise-configuration`                   | SSoT environment variable patterns  |\n\n## Python Driver Policy\n\nFor Python application code connecting to ClickHouse (not DBeaver), use `clickhouse-connect` (official HTTP driver). See [`clickhouse-architect`](../../../quality-tools/skills/clickhouse-architect/SKILL.md#python-driver-policy) for:\n\n- Recommended code patterns\n- Why NOT to use `clickhouse-driver` (community)\n- Performance vs maintenance trade-offs\n\n## Additional Resources\n\n| Reference                                                      | Content                      |\n| -------------------------------------------------------------- | ---------------------------- |\n| [references/pydantic-model.md](./references/pydantic-model.md) | Complete model documentation |\n| [references/dbeaver-format.md](./references/dbeaver-format.md) | DBeaver JSON format spec     |"
              },
              {
                "name": "doppler-secret-validation",
                "description": "Validate and test secrets stored in Doppler. Add API tokens/credentials to Doppler, verify storage and retrieval, test authentication with target services. Use when user mentions \"add to Doppler\", \"store secret\", \"validate token\", or provides API tokens needing secure storage.",
                "path": "plugins/devops-tools/skills/doppler-secret-validation/SKILL.md",
                "frontmatter": {
                  "name": "doppler-secret-validation",
                  "description": "Validate and test secrets stored in Doppler. Add API tokens/credentials to Doppler, verify storage and retrieval, test authentication with target services. Use when user mentions \"add to Doppler\", \"store secret\", \"validate token\", or provides API tokens needing secure storage.",
                  "allowed-tools": "Read, Bash"
                },
                "content": "# Doppler Secret Validation\n\n## Overview\n\nWorkflow for securely adding, validating, and testing API tokens and credentials in Doppler secrets management.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- User provides API tokens or credentials (PyPI, GitHub, AWS, etc.)\n- User mentions \"add to Doppler\", \"store secret\", \"validate token\"\n- User wants to test authentication before production use\n- User needs to verify secret storage and retrieval\n\n## Workflow\n\n### Step 1: Test Token Format (Before Adding to Doppler)\n\nBefore storing in Doppler, validate token format:\n\n```bash\n# Check token format, length, prefix\npython3 -c \"token = 'TOKEN_VALUE'; print(f'Prefix: {token[:20]}...'); print(f'Length: {len(token)}')\"\n```\n\n**Common token formats**:\n\n- PyPI: `pypi-...` (179 chars)\n- GitHub: `ghp_...` (40+ chars)\n- AWS: 20-char access key + 40-char secret\n\n### Step 2: Add Secret to Doppler\n\n```bash\ndoppler secrets set SECRET_NAME=\"value\" --project PROJECT --config CONFIG\n```\n\n**Example**:\n\n```bash\ndoppler secrets set PYPI_TOKEN=\"pypi-AgEI...\" \\\n  --project claude-config --config prd\n```\n\n**Important**: CLI doesn't support `--note`. Add notes via dashboard:\n\n1. <https://dashboard.doppler.com>\n2. Navigate: PROJECT  CONFIG  SECRET_NAME\n3. Edit  Add descriptive note\n\n### Step 3: Validate Storage\n\nUse the bundled validation script:\n\n```bash\n/usr/bin/env bash << 'VALIDATE_EOF'\ncd ${CLAUDE_PLUGIN_ROOT}/skills/doppler-secret-validation\nuv run scripts/validate_secret.py \\\n  --project PROJECT \\\n  --config CONFIG \\\n  --secret SECRET_NAME\nVALIDATE_EOF\n```\n\nThis validates:\n\n1. Secret exists in Doppler\n2. Secret retrieval works\n3. Environment injection works via `doppler run`\n\n**Example**:\n\n```bash\nuv run scripts/validate_secret.py \\\n  --project claude-config \\\n  --config prd \\\n  --secret PYPI_TOKEN\n```\n\n### Step 4: Test API Authentication\n\nUse the bundled auth test script (adapt test_api_authentication() for specific API):\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF'\ncd ${CLAUDE_PLUGIN_ROOT}/skills/doppler-secret-validation\ndoppler run --project PROJECT --config CONFIG -- \\\n  uv run scripts/test_api_auth.py \\\n    --secret SECRET_NAME \\\n    --api-url API_ENDPOINT\nCONFIG_EOF\n```\n\n**Example (PyPI)**:\n\n```bash\ndoppler run --project claude-config --config prd -- \\\n  uv run scripts/test_api_auth.py \\\n    --secret PYPI_TOKEN \\\n    --api-url https://upload.pypi.org/legacy/\n```\n\n### Step 5: Document Usage\n\nAfter validation, document the usage pattern for the user:\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF_2'\n# Pattern 1: Doppler run (recommended for CI/scripts)\ndoppler run --project PROJECT --config CONFIG -- COMMAND\n\n# Pattern 2: Manual export (for troubleshooting)\nexport SECRET_NAME=$(doppler secrets get SECRET_NAME \\\n  --project PROJECT --config CONFIG --plain)\nCONFIG_EOF_2\n```\n\n### Step 5b: mise [env] Integration (Recommended for Local Development)\n\nFor multi-account GitHub setups or per-directory credential needs, integrate Doppler secrets with mise `[env]`:\n\n```toml\n# .mise.toml\n[env]\n# Option A: Direct Doppler CLI fetch (slower, always fresh)\nGH_TOKEN = \"{{ exec(command='doppler secrets get GH_TOKEN --project myproject --config prd --plain') }}\"\nGITHUB_TOKEN = \"{{ exec(command='doppler secrets get GH_TOKEN --project myproject --config prd --plain') }}\"\n\n# Option B: Cache for performance (1 hour cache)\nGH_TOKEN = \"{{ cache(key='gh_token', duration='1h', run='doppler secrets get GH_TOKEN --project myproject --config prd --plain') }}\"\nGITHUB_TOKEN = \"{{ cache(key='gh_token', duration='1h', run='doppler secrets get GH_TOKEN --project myproject --config prd --plain') }}\"\n```\n\n**Note**: Set BOTH `GH_TOKEN` and `GITHUB_TOKEN` - different tools check different variable names (gh CLI vs npm scripts).\n\n**Why mise [env]?** Doppler `doppler run` is session-scoped; mise `[env]` provides directory-scoped credentials that persist across commands.\n\nSee [`mise-configuration` skill](../../../itp/skills/mise-configuration/SKILL.md#github-token-multi-account-patterns) for complete patterns.\n\n## Common Patterns\n\n### Multiple Configs (dev, stg, prd)\n\nAdd secret to multiple environments:\n\n```bash\n# Production\ndoppler secrets set TOKEN=\"prod-value\" --project foo --config prd\n\n# Development\ndoppler secrets set TOKEN=\"dev-value\" --project foo --config dev\n```\n\n### Verify Secret Across Configs\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF_3'\nfor config in dev stg prd; do\n  echo \"=== $config ===\"\n  doppler secrets get TOKEN --project foo --config $config --plain | head -c 20\n  echo \"...\"\ndone\nCONFIG_EOF_3\n```\n\n## Security Guidelines\n\n1. **Never log full secrets**: Use `${SECRET:0:20}...` masking\n2. **Prefer doppler run**: Scopes secrets to single command\n3. **Use --plain only for piping**: Human-readable view masks secrets\n4. **Separate configs per environment**: dev/stg/prd isolation\n\n## Bundled Resources\n\n- **scripts/validate_secret.py** - Complete validation suite (existence, retrieval, injection)\n- **scripts/test_api_auth.py** - Template for API authentication testing\n- **references/doppler-patterns.md** - Common CLI patterns and examples\n\n## Reference\n\n- Doppler docs: <https://docs.doppler.com/docs>\n- CLI install: `brew install dopplerhq/cli/doppler`\n- See [doppler-patterns.md](./references/doppler-patterns.md) for comprehensive patterns"
              },
              {
                "name": "doppler-workflows",
                "description": "Manages credentials and publishing workflows via Doppler. Use when publishing Python packages to PyPI, rotating AWS credentials, or managing secrets with Doppler.",
                "path": "plugins/devops-tools/skills/doppler-workflows/SKILL.md",
                "frontmatter": {
                  "name": "doppler-workflows",
                  "description": "Manages credentials and publishing workflows via Doppler. Use when publishing Python packages to PyPI, rotating AWS credentials, or managing secrets with Doppler.",
                  "allowed-tools": "Read, Bash"
                },
                "content": "# Doppler Credential Workflows\n\n## Quick Reference\n\n**When to use this skill:**\n\n- Publishing Python packages to PyPI\n- Rotating AWS access keys\n- Managing credentials across multiple services\n- Troubleshooting authentication failures (403, InvalidClientTokenId)\n- Setting up Doppler credential injection patterns\n- Multi-token/multi-account strategies\n\n## Core Pattern: Doppler CLI\n\n**Standard Usage:**\n\n```bash\ndoppler run --project <project> --config <config> --command='<command>'\n```\n\n**Why --command flag:**\n\n- Official Doppler pattern (auto-detects shell)\n- Ensures variables expand AFTER Doppler injects them\n- Without it: shell expands `$VAR` before Doppler runs  empty string\n\n---\n\n## Quick Start Examples\n\n### PyPI Publishing\n\n```bash\ndoppler run --project claude-config --config dev \\\n  --command='uv publish --token \"$PYPI_TOKEN\"'\n```\n\n### AWS Operations\n\n```bash\ndoppler run --project aws-credentials --config dev \\\n  --command='aws s3 ls --region $AWS_DEFAULT_REGION'\n```\n\n---\n\n## Best Practices\n\n1. Always use --command flag for credential injection\n2. Use project-scoped tokens (PyPI) for better security\n3. Rotate credentials regularly (90 days recommended)\n4. Document with Doppler notes: `doppler secrets notes set <SECRET> \"<note>\"`\n5. Use stdin for storing secrets: `echo -n 'secret' | doppler secrets set`\n6. Test injection before using: `echo ${#VAR}` to verify length\n7. Multi-token naming: `SERVICE_TOKEN_{ABBREV}` for clarity\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [PyPI Publishing](./references/pypi-publishing.md) - Token setup, publishing, troubleshooting\n- [AWS Credentials](./references/aws-credentials.md) - Rotation workflow, setup, troubleshooting\n- [Multi-Service Patterns](./references/multi-service-patterns.md) - Multiple PyPI packages, multiple AWS accounts\n- [AWS Workflow](./AWS_WORKFLOW.md) - Complete AWS credential management guide\n\n**Bundled Specifications:**\n\n- `PYPI_REFERENCE.yaml` - Complete PyPI spec\n- `AWS_SPECIFICATION.yaml` - AWS credential architecture\n\n---\n\n## Using mise [env] for Local Development (Recommended)\n\nFor local development, mise `[env]` provides a simpler alternative to `doppler run`:\n\n```toml\n# .mise.toml\n[env]\n# Fetch from Doppler with caching for performance\nPYPI_TOKEN = \"{{ cache(key='pypi_token', duration='1h', run='doppler secrets get PYPI_TOKEN --project claude-config --config prd --plain') }}\"\n\n# For GitHub multi-account setups\nGH_TOKEN = \"{{ read_file(path=env.HOME ~ '/.claude/.secrets/gh-token-accountname') | trim }}\"\n```\n\n**When to use mise [env]:**\n\n- Per-directory credential configuration\n- Multi-account GitHub setups\n- Credentials that persist across commands (not session-scoped)\n\n**When to use doppler run:**\n\n- CI/CD pipelines\n- Single-command credential scope\n- When you want credentials auto-cleared after command\n\nSee [`mise-configuration` skill](../../../itp/skills/mise-configuration/SKILL.md) for complete patterns.\n\n---\n\n## PyPI Publishing Policy\n\n<!-- ADR: 2025-12-10-clickhouse-skill-documentation-gaps -->\n\nFor PyPI publishing, see [`pypi-doppler` skill](../../../itp/skills/pypi-doppler/SKILL.md) for **LOCAL-ONLY** workspace policy.\n\n**Do NOT** configure PyPI publishing in GitHub Actions or CI/CD pipelines."
              },
              {
                "name": "dual-channel-watchexec-notifications",
                "description": "Sends dual-channel notifications (Telegram + Pushover) on watchexec events with proper formatting. Use when monitoring file changes, process restarts, or setting up alerts.",
                "path": "plugins/devops-tools/skills/dual-channel-watchexec/SKILL.md",
                "frontmatter": {
                  "name": "dual-channel-watchexec-notifications",
                  "description": "Sends dual-channel notifications (Telegram + Pushover) on watchexec events with proper formatting. Use when monitoring file changes, process restarts, or setting up alerts.",
                  "allowed-tools": "Read, Write, Edit, Bash"
                },
                "content": "# Dual-Channel Watchexec Notifications\n\nSend reliable notifications to both Telegram and Pushover when watchexec detects file changes or process crashes.\n\n## Core Pattern\n\n**watchexec wrapper script**  **detect event**  **notify-script**  **Telegram + Pushover**\n\n```bash\n# wrapper.sh - Monitors process and detects restart reasons\nwatchexec --restart -- python bot.py\n\n# On event, call:\nnotify-script.sh <reason> <exit_code> <watchexec_info_file> <crash_context>\n```\n\n---\n\n## Critical Rule: Format Differences\n\n**Telegram**: HTML mode ONLY\n\n```bash\nMESSAGE=\"<b>Alert</b>: <code>file.py</code>\"\n# Escape 3 chars: &  &amp;, <  &lt;, >  &gt;\n```\n\n**Pushover**: Plain text ONLY\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\n# Strip HTML tags before sending\nMESSAGE_PLAIN=$(echo \"$MESSAGE_HTML\" | sed 's/<[^>]*>//g')\nSKILL_SCRIPT_EOF\n```\n\n**Why HTML for Telegram**:\n\n- Markdown requires escaping 40+ chars (`.`, `-`, `_`, etc.)\n- HTML only requires escaping 3 chars (`&`, `<`, `>`)\n- Industry best practice\n\n---\n\n## Quick Reference\n\n### Send to Both Channels\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF_2'\n# 1. Build HTML message for Telegram\nMESSAGE_HTML=\"<b>File</b>: <code>handler_classes.py</code>\"\n\n# 2. Strip HTML for Pushover\nMESSAGE_PLAIN=$(echo \"$MESSAGE_HTML\" | sed 's/<[^>]*>//g')\n\n# 3. Send to Telegram with HTML\ncurl -s -d \"chat_id=$CHAT_ID\" \\\n  -d \"text=$MESSAGE_HTML\" \\\n  -d \"parse_mode=HTML\" \\\n  https://api.telegram.org/bot$BOT_TOKEN/sendMessage\n\n# 4. Send to Pushover with plain text\ncurl -s --form-string \"message=$MESSAGE_PLAIN\" \\\n  https://api.pushover.net/1/messages.json\nSKILL_SCRIPT_EOF_2\n```\n\n### Execution Pattern\n\n```bash\n# Fire-and-forget background notifications (don't block restarts)\n\"$NOTIFY_SCRIPT\" \"crash\" \"$EXIT_CODE\" \"$INFO_FILE\" \"$CONTEXT_FILE\" &\n```\n\n---\n\n## Validation Checklist\n\nBefore deploying:\n\n- [ ] Using HTML parse mode for Telegram (not Markdown)\n- [ ] HTML tags stripped for Pushover (plain text only)\n- [ ] HTML escaping applied to all dynamic content (`&`, `<`, `>`)\n- [ ] Credentials loaded from env vars/Doppler (not hardcoded)\n- [ ] Message archiving enabled for debugging\n- [ ] File detection uses `stat` (not `find -newermt`)\n- [ ] Heredocs use unquoted delimiters for variable expansion\n- [ ] Notifications run in background (fire-and-forget)\n- [ ] Tested with files containing special chars (`_`, `.`, `-`)\n- [ ] Both Telegram and Pushover successfully receiving\n\n---\n\n## Summary\n\n**Key Lessons**:\n\n1. Always use HTML mode for Telegram (simpler escaping)\n2. Always strip HTML tags for Pushover (plain text only)\n3. Escape only 3 chars in HTML: `&`  `&amp;`, `<`  `&lt;`, `>`  `&gt;`\n4. Archive messages before sending for debugging\n5. Use `stat` for file detection on macOS (not `find -newermt`)\n6. Load credentials from env vars/Doppler (never hardcode)\n7. Fire-and-forget background notifications (don't block restarts)\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Telegram HTML](./references/telegram-html.md) - HTML mode formatting and message templates\n- [Pushover Integration](./references/pushover-integration.md) - API calls and priority levels\n- [Credential Management](./references/credential-management.md) - Doppler, env vars, and keychain patterns\n- [Watchexec Patterns](./references/watchexec-patterns.md) - File detection and restart reason detection\n- [Common Pitfalls](./references/common-pitfalls.md) - HTML tags in Pushover, escaping issues, macOS compatibility\n\n**Bundled Examples:**\n\n- `examples/notify-restart.sh` - Complete dual-channel notification script\n- `examples/bot-wrapper.sh` - watchexec wrapper with restart detection\n- `examples/setup-example.sh` - Setup guide and installation steps"
              },
              {
                "name": "mlflow-python",
                "description": "Log experiment metrics, parameters, and artifacts using MLflow Python API. Query and analyze runs with DataFrame operations. Use when user mentions \"log backtest\", \"MLflow metrics\", \"experiment tracking\", \"log parameters\", \"search runs\", \"MLflow query\", or needs to record strategy performance.\n",
                "path": "plugins/devops-tools/skills/mlflow-python/SKILL.md",
                "frontmatter": {
                  "name": "mlflow-python",
                  "description": "Log experiment metrics, parameters, and artifacts using MLflow Python API. Query and analyze runs with DataFrame operations. Use when user mentions \"log backtest\", \"MLflow metrics\", \"experiment tracking\", \"log parameters\", \"search runs\", \"MLflow query\", or needs to record strategy performance.\n",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# MLflow Python Skill\n\nUnified read/write MLflow operations via Python API with QuantStats integration for comprehensive trading metrics.\n\n**ADR**: [2025-12-12-mlflow-python-skill](/docs/adr/2025-12-12-mlflow-python-skill.md)\n\n## When to Use This Skill\n\n**CAN Do**:\n\n- Log backtest metrics (Sharpe, max_drawdown, total_return, etc.)\n- Log experiment parameters (strategy config, timeframes)\n- Create and manage experiments\n- Query runs with SQL-like filtering\n- Calculate 70+ trading metrics via QuantStats\n- Retrieve metric history (time-series data)\n\n**CANNOT Do**:\n\n- Direct database access to MLflow backend\n- Artifact storage management (S3/GCS configuration)\n- MLflow server administration\n\n## Prerequisites\n\n### Authentication Setup\n\nMLflow uses separate environment variables for credentials (NOT embedded in URI):\n\n```bash\n# Option 1: mise + .env.local (recommended)\n# Create .env.local in skill directory with:\nMLFLOW_TRACKING_URI=http://mlflow.eonlabs.com:5000\nMLFLOW_TRACKING_USERNAME=eonlabs\nMLFLOW_TRACKING_PASSWORD=<password>\n\n# Option 2: Direct environment variables\nexport MLFLOW_TRACKING_URI=\"http://mlflow.eonlabs.com:5000\"\nexport MLFLOW_TRACKING_USERNAME=\"eonlabs\"\nexport MLFLOW_TRACKING_PASSWORD=\"<password>\"\n```\n\n### Verify Connection\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\ncd ${CLAUDE_PLUGIN_ROOT}/skills/mlflow-python\nuv run scripts/query_experiments.py experiments\nSKILL_SCRIPT_EOF\n```\n\n## Quick Start Workflows\n\n### A. Log Backtest Results (Primary Use Case)\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF_2'\ncd ${CLAUDE_PLUGIN_ROOT}/skills/mlflow-python\nuv run scripts/log_backtest.py \\\n  --experiment \"crypto-backtests\" \\\n  --run-name \"btc_momentum_v2\" \\\n  --returns path/to/returns.csv \\\n  --params '{\"strategy\": \"momentum\", \"timeframe\": \"1h\"}'\nSKILL_SCRIPT_EOF_2\n```\n\n### B. Search Experiments\n\n```bash\nuv run scripts/query_experiments.py experiments\n```\n\n### C. Query Runs with Filter\n\n```bash\nuv run scripts/query_experiments.py runs \\\n  --experiment \"crypto-backtests\" \\\n  --filter \"metrics.sharpe_ratio > 1.5\" \\\n  --order-by \"metrics.sharpe_ratio DESC\"\n```\n\n### D. Create New Experiment\n\n```bash\nuv run scripts/create_experiment.py \\\n  --name \"crypto-backtests-2025\" \\\n  --description \"Q1 2025 cryptocurrency trading strategy backtests\"\n```\n\n### E. Get Metric History\n\n```bash\nuv run scripts/get_metric_history.py \\\n  --run-id abc123 \\\n  --metrics sharpe_ratio,cumulative_return\n```\n\n## QuantStats Metrics Available\n\nThe `log_backtest.py` script calculates 70+ metrics via QuantStats, including:\n\n| Category     | Metrics                                                           |\n| ------------ | ----------------------------------------------------------------- |\n| **Ratios**   | sharpe, sortino, calmar, omega, treynor                           |\n| **Returns**  | cagr, total_return, avg_return, best, worst                       |\n| **Drawdown** | max_drawdown, avg_drawdown, drawdown_days                         |\n| **Trade**    | win_rate, profit_factor, payoff_ratio, consecutive_wins/losses    |\n| **Risk**     | volatility, var, cvar, ulcer_index, serenity_index                |\n| **Advanced** | kelly_criterion, recovery_factor, risk_of_ruin, information_ratio |\n\nSee [quantstats-metrics.md](./references/quantstats-metrics.md) for full list.\n\n## Bundled Scripts\n\n| Script                  | Purpose                                      |\n| ----------------------- | -------------------------------------------- |\n| `log_backtest.py`       | Log backtest returns with QuantStats metrics |\n| `query_experiments.py`  | Search experiments and runs (replaces CLI)   |\n| `create_experiment.py`  | Create new experiment with metadata          |\n| `get_metric_history.py` | Retrieve metric time-series data             |\n\n## Configuration\n\nThe skill uses mise `[env]` pattern for configuration. See `.mise.toml` for defaults.\n\nCreate `.env.local` (gitignored) for credentials:\n\n```bash\nMLFLOW_TRACKING_URI=http://mlflow.eonlabs.com:5000\nMLFLOW_TRACKING_USERNAME=eonlabs\nMLFLOW_TRACKING_PASSWORD=<password>\n```\n\n## Reference Documentation\n\n- [Authentication Patterns](./references/authentication.md) - Idiomatic MLflow auth\n- [QuantStats Metrics](./references/quantstats-metrics.md) - Full list of 70+ metrics\n- [Query Patterns](./references/query-patterns.md) - DataFrame operations\n- [Migration from CLI](./references/migration-from-cli.md) - CLI to Python API mapping\n\n## Migration from mlflow-query\n\nThis skill replaces the CLI-based `mlflow-query` skill. Key differences:\n\n| Feature        | mlflow-query (old) | mlflow-python (new)    |\n| -------------- | ------------------ | ---------------------- |\n| Log metrics    | Not supported      | `mlflow.log_metrics()` |\n| Log params     | Not supported      | `mlflow.log_params()`  |\n| Query runs     | CLI text parsing   | DataFrame output       |\n| Metric history | Workaround only    | Native support         |\n| Auth pattern   | Embedded in URI    | Separate env vars      |\n\nSee [migration-from-cli.md](./references/migration-from-cli.md) for detailed mapping."
              },
              {
                "name": "session-chronicle",
                "description": "Excavate session logs for provenance tracking. TRIGGERS - who created, document finding, trace origin, session archaeology, provenance, ADR reference.",
                "path": "plugins/devops-tools/skills/session-chronicle/SKILL.md",
                "frontmatter": {
                  "name": "session-chronicle",
                  "description": "Excavate session logs for provenance tracking. TRIGGERS - who created, document finding, trace origin, session archaeology, provenance, ADR reference.",
                  "allowed-tools": "Read, Grep, Glob, Bash, AskUserQuestion"
                },
                "content": "# Session Chronicle\n\nExcavate Claude Code session logs to capture **complete provenance** for research findings, ADR decisions, and code contributions. Traces UUID chains across multiple auto-compacted sessions.\n\n**CRITICAL PRINCIPLE**: Registry entries must be **self-contained**. Record ALL session UUIDs (main + subagent) at commit time. Future maintainers should not need to run archaeology to understand provenance.\n\n**S3 Artifact Sharing**: Artifacts can be uploaded to S3 for team access. See [S3 Sharing ADR](/docs/adr/2026-01-02-session-chronicle-s3-sharing.md).\n\n## When to Use This Skill\n\n- User asks \"who created this?\" or \"where did this come from?\"\n- User says \"document this finding\" with full session context\n- ADR or research finding needs provenance tracking\n- Git commit needs session UUID references\n- Tracing edits across auto-compacted sessions\n- **Creating a registry entry for a research session**\n\n---\n\n## File Ownership Model\n\n| Directory | Committed? | Purpose |\n|-----------|-----------|---------|\n| `findings/registry.jsonl` | YES | Master index (small, append-only NDJSON) |\n| `findings/sessions/<id>/iterations.jsonl` | YES | Iteration records (small, append-only) |\n| `outputs/research_sessions/<id>/` | NO | Research artifacts (large, gitignored) |\n| `tmp/` | NO | Temporary archives before S3 upload |\n| S3 `eonlabs-findings/sessions/<id>/` | N/A | Permanent team-shared archive |\n\n**Key Principle**: Only `findings/` is committed. Research artifacts go to gitignored `outputs/` and S3.\n\n---\n\n## Part 0: Preflight Check\n\n### Step 1: Verify Session Storage Location\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nset -euo pipefail\n\n# Check Claude session storage\nPROJECT_DIR=\"$HOME/.claude/projects\"\nif [[ ! -d \"$PROJECT_DIR\" ]]; then\n  echo \"ERROR: Session storage not found at $PROJECT_DIR\" >&2\n  echo \"  Expected: ~/.claude/projects/\" >&2\n  echo \"  This directory is created by Claude Code on first use.\" >&2\n  exit 1\nfi\n\n# Count project folders (0 is valid - just means no sessions yet)\nPROJECT_COUNT=$(ls -1d \"$PROJECT_DIR\"/*/ 2>/dev/null | wc -l || echo \"0\")\nif [[ \"$PROJECT_COUNT\" -eq 0 ]]; then\n  echo \"WARNING: No project sessions found in $PROJECT_DIR\"\n  echo \"  This may be expected if Claude Code hasn't been used in any projects yet.\"\nelse\n  echo \" Found $PROJECT_COUNT project folders in $PROJECT_DIR\"\nfi\necho \"Ready for session archaeology\"\nPREFLIGHT_EOF\n```\n\n### Step 2: Find Current Project Sessions\n\n```bash\n/usr/bin/env bash << 'FIND_SESSIONS_EOF'\nset -euo pipefail\n\n# Encode current working directory path (Claude Code path encoding)\nCWD=$(pwd)\nENCODED_PATH=$(echo \"$CWD\" | tr '/' '-')\nPROJECT_SESSIONS=\"$HOME/.claude/projects/$ENCODED_PATH\"\n\nif [[ -d \"$PROJECT_SESSIONS\" ]]; then\n  # Count main sessions vs agent sessions (handle empty glob safely)\n  MAIN_COUNT=$(ls -1 \"$PROJECT_SESSIONS\"/*.jsonl 2>/dev/null | grep -v \"agent-\" | wc -l | tr -d ' ' || echo \"0\")\n  AGENT_COUNT=$(ls -1 \"$PROJECT_SESSIONS\"/agent-*.jsonl 2>/dev/null | wc -l | tr -d ' ' || echo \"0\")\n\n  if [[ \"$MAIN_COUNT\" -eq 0 && \"$AGENT_COUNT\" -eq 0 ]]; then\n    echo \"ERROR: Session directory exists but contains no .jsonl files\" >&2\n    echo \"  Location: $PROJECT_SESSIONS\" >&2\n    exit 1\n  fi\n\n  echo \" Found $MAIN_COUNT main sessions + $AGENT_COUNT subagent sessions\"\n  echo \"  Location: $PROJECT_SESSIONS\"\n\n  # Show main sessions with line counts\n  echo -e \"\\n=== Main Sessions ===\"\n  for f in \"$PROJECT_SESSIONS\"/*.jsonl; do\n    [[ ! -f \"$f\" ]] && continue\n    name=$(basename \"$f\" .jsonl)\n    [[ \"$name\" =~ ^agent- ]] && continue\n    lines=$(wc -l < \"$f\" | tr -d ' ')\n    echo \"  $name ($lines entries)\"\n  done\n\n  # Show agent sessions summary\n  echo -e \"\\n=== Subagent Sessions ===\"\n  for f in \"$PROJECT_SESSIONS\"/agent-*.jsonl; do\n    [[ ! -f \"$f\" ]] && continue\n    name=$(basename \"$f\" .jsonl)\n    lines=$(wc -l < \"$f\" | tr -d ' ')\n    echo \"  $name ($lines entries)\"\n  done\nelse\n  echo \"ERROR: No sessions found for current project\" >&2\n  echo \"  Expected: $PROJECT_SESSIONS\" >&2\n  echo \"\" >&2\n  echo \"Available project folders:\" >&2\n  ls -1 \"$HOME/.claude/projects/\" 2>/dev/null | head -10 || echo \"  (none)\"\n  exit 1\nfi\nFIND_SESSIONS_EOF\n```\n\n### Step 3: Verify Required Tools\n\n```bash\n/usr/bin/env bash << 'TOOLS_EOF'\nset -euo pipefail\n\n# All tools are REQUIRED - fail loudly if missing\nMISSING=0\n\n# Check for jq (required for JSONL parsing)\nif ! command -v jq &>/dev/null; then\n  echo \"ERROR: jq not installed (brew install jq)\" >&2\n  MISSING=1\nfi\n\n# Check for brotli (required for compression)\nif ! command -v brotli &>/dev/null; then\n  echo \"ERROR: brotli not installed (brew install brotli)\" >&2\n  MISSING=1\nfi\n\n# Check for aws (required for S3 upload)\nif ! command -v aws &>/dev/null; then\n  echo \"ERROR: aws CLI not installed (brew install awscli)\" >&2\n  MISSING=1\nfi\n\n# Check for op (required for 1Password credential injection)\nif ! command -v op &>/dev/null; then\n  echo \"ERROR: 1Password CLI not installed (brew install 1password-cli)\" >&2\n  MISSING=1\nfi\n\nif [[ $MISSING -eq 1 ]]; then\n  echo \"\" >&2\n  echo \"PREFLIGHT FAILED: Missing required tools. Install them and retry.\" >&2\n  exit 1\nfi\n\necho \" All required tools available: jq, brotli, aws, op\"\nTOOLS_EOF\n```\n\n---\n\n## Part 1: AskUserQuestion Flows\n\n### Flow A: Identify Target for Provenance\n\nWhen the skill is triggered, first identify what the user wants to trace:\n\n```\nAskUserQuestion:\n  question: \"What do you want to trace provenance for?\"\n  header: \"Target\"\n  multiSelect: false\n  options:\n    - label: \"Research finding/session\"\n      description: \"Document a research session with full session context for reproducibility\"\n    - label: \"Specific code/feature\"\n      description: \"Trace who created a specific function, feature, or code block\"\n    - label: \"Configuration/decision\"\n      description: \"Trace when and why a configuration or architectural decision was made\"\n    - label: \"Custom search\"\n      description: \"Search session logs for specific keywords or patterns\"\n```\n\n### Flow B: Confirm GitHub Attribution\n\n**CRITICAL**: Every registry entry MUST have GitHub username attribution.\n\n```\nAskUserQuestion:\n  question: \"Who should be attributed as the creator?\"\n  header: \"Attribution\"\n  multiSelect: false\n  options:\n    - label: \"Use git config user (Recommended)\"\n      description: \"Attribute to $(git config user.name) / $(git config user.email)\"\n    - label: \"Specify GitHub username\"\n      description: \"I'll provide the GitHub username manually\"\n    - label: \"Team attribution\"\n      description: \"Multiple contributors - list all GitHub usernames\"\n```\n\n### Flow C: Confirm Session Scope\n\n**CRITICAL**: Default to ALL sessions. Registry must be self-contained.\n\n```\nAskUserQuestion:\n  question: \"Which sessions should be recorded in the registry?\"\n  header: \"Sessions\"\n  multiSelect: false\n  options:\n    - label: \"ALL sessions (main + subagent) (Recommended)\"\n      description: \"Record every session file - complete provenance for future maintainers\"\n    - label: \"Main sessions only\"\n      description: \"Exclude agent-* subagent sessions (loses context)\"\n    - label: \"Manual selection\"\n      description: \"I'll specify which sessions to include\"\n```\n\n**IMPORTANT**: Always default to recording ALL sessions. Subagent sessions (`agent-*`)\ncontain critical context from Explore, Plan, and specialized agents. Omitting them\nforces future maintainers to re-run archaeology.\n\n### Flow D: Preview Session Contexts Array\n\nBefore writing, show the user exactly what will be recorded:\n\n```\nAskUserQuestion:\n  question: \"Review the session_contexts array that will be recorded:\"\n  header: \"Review\"\n  multiSelect: false\n  options:\n    - label: \"Looks correct - proceed\"\n      description: \"Write this to the registry\"\n    - label: \"Add descriptions\"\n      description: \"Let me add descriptions to some sessions\"\n    - label: \"Filter some sessions\"\n      description: \"Remove sessions that aren't relevant\"\n    - label: \"Cancel\"\n      description: \"Don't write to registry yet\"\n```\n\nDisplay the full session_contexts array before this question:\n```json\n{\n  \"session_contexts\": [\n    {\"session_uuid\": \"abc123\", \"type\": \"main\", \"entries\": 980, \"description\": \"...\"},\n    {\"session_uuid\": \"agent-xyz\", \"type\": \"subagent\", \"entries\": 113, \"description\": \"...\"}\n  ]\n}\n```\n\n### Flow E: Choose Output Format\n\n```\nAskUserQuestion:\n  question: \"What outputs should be generated?\"\n  header: \"Outputs\"\n  multiSelect: true\n  options:\n    - label: \"registry.jsonl entry (Recommended)\"\n      description: \"Master index entry with ALL session UUIDs and GitHub attribution\"\n    - label: \"iterations.jsonl entries\"\n      description: \"Detailed iteration records in sessions/<id>/\"\n    - label: \"Full session chain archive (.jsonl.br)\"\n      description: \"Compress sessions with Brotli for archival\"\n    - label: \"Markdown finding document\"\n      description: \"findings/<name>.md with embedded provenance table\"\n    - label: \"Git commit with provenance\"\n      description: \"Structured commit message with session references\"\n    - label: \"Upload to S3 for team sharing\"\n      description: \"Upload artifacts to S3 with retrieval command in commit\"\n```\n\n### Flow F: Link to Existing ADR\n\nWhen creating a research session registry entry:\n\n```\nAskUserQuestion:\n  question: \"Link this to an existing ADR or design spec?\"\n  header: \"ADR Link\"\n  multiSelect: false\n  options:\n    - label: \"No ADR link\"\n      description: \"This is standalone or ADR doesn't exist yet\"\n    - label: \"Specify ADR slug\"\n      description: \"Link to an existing ADR (e.g., 2025-12-15-feature-name)\"\n    - label: \"Create new ADR\"\n      description: \"This finding warrants a new ADR\"\n```\n\n---\n\n## Part 2: Session Archaeology Process\n\n### Step 1: Full Project Scan\n\nScan ALL session files (main + subagent) to build complete index:\n\n```bash\n/usr/bin/env bash << 'SCAN_EOF'\nset -euo pipefail\n\nCWD=$(pwd)\nENCODED_PATH=$(echo \"$CWD\" | tr '/' '-')\nPROJECT_SESSIONS=\"$HOME/.claude/projects/$ENCODED_PATH\"\n\nif [[ ! -d \"$PROJECT_SESSIONS\" ]]; then\n  echo \"ERROR: Project sessions directory not found: $PROJECT_SESSIONS\" >&2\n  exit 1\nfi\n\necho \"=== Building Session Index ===\"\nMAIN_COUNT=0\nAGENT_COUNT=0\n\n# Main sessions\necho \"Main sessions:\"\nfor f in \"$PROJECT_SESSIONS\"/*.jsonl; do\n  [[ ! -f \"$f\" ]] && continue\n  name=$(basename \"$f\" .jsonl)\n  [[ \"$name\" =~ ^agent- ]] && continue\n\n  lines=$(wc -l < \"$f\" | tr -d ' ')\n  first_ts=$(head -1 \"$f\" | jq -r '.timestamp // \"unknown\"') || first_ts=\"parse-error\"\n  last_ts=$(tail -1 \"$f\" | jq -r '.timestamp // \"unknown\"') || last_ts=\"parse-error\"\n\n  if [[ \"$first_ts\" == \"parse-error\" ]]; then\n    echo \"  WARNING: Failed to parse timestamps in $name\" >&2\n  fi\n\n  echo \"  $name|main|$lines|$first_ts|$last_ts\"\n  ((MAIN_COUNT++)) || true\ndone\n\n# Subagent sessions\necho \"Subagent sessions:\"\nfor f in \"$PROJECT_SESSIONS\"/agent-*.jsonl; do\n  [[ ! -f \"$f\" ]] && continue\n  name=$(basename \"$f\" .jsonl)\n\n  lines=$(wc -l < \"$f\" | tr -d ' ')\n  first_ts=$(head -1 \"$f\" | jq -r '.timestamp // \"unknown\"') || first_ts=\"parse-error\"\n\n  echo \"  $name|subagent|$lines|$first_ts\"\n  ((AGENT_COUNT++)) || true\ndone\n\necho \"\"\necho \" Indexed $MAIN_COUNT main + $AGENT_COUNT subagent sessions\"\n\nif [[ $MAIN_COUNT -eq 0 && $AGENT_COUNT -eq 0 ]]; then\n  echo \"ERROR: No sessions found to index\" >&2\n  exit 1\nfi\nSCAN_EOF\n```\n\n### Step 2: Build session_contexts Array\n\n**CRITICAL**: This array must contain ALL sessions. Example output:\n\n```json\n{\n  \"session_contexts\": [\n    {\n      \"session_uuid\": \"8c821a19-e4f4-45d5-9338-be3a47ac81a3\",\n      \"type\": \"main\",\n      \"entries\": 980,\n      \"timestamp_start\": \"2026-01-03T21:25:07.435Z\",\n      \"description\": \"Primary session - research iterations, PR preparation\"\n    },\n    {\n      \"session_uuid\": \"agent-a728ebe\",\n      \"type\": \"subagent\",\n      \"entries\": 113,\n      \"timestamp_start\": \"2026-01-02T07:25:47.658Z\",\n      \"description\": \"Explore agent - codebase analysis\"\n    }\n  ]\n}\n```\n\n### Step 3: Trace UUID Chain (Optional)\n\nFor detailed provenance of specific edits:\n\n```bash\n/usr/bin/env bash << 'TRACE_EOF'\nset -euo pipefail\n\ntrace_uuid_chain() {\n  local uuid=\"$1\"\n  local session_file=\"$2\"\n  local depth=0\n  local max_depth=100\n\n  if [[ -z \"$uuid\" ]]; then\n    echo \"ERROR: UUID argument required\" >&2\n    return 1\n  fi\n\n  if [[ ! -f \"$session_file\" ]]; then\n    echo \"ERROR: Session file not found: $session_file\" >&2\n    return 1\n  fi\n\n  echo \"Tracing UUID chain from: $uuid\"\n\n  while [[ -n \"$uuid\" && $depth -lt $max_depth ]]; do\n    # Use jq with explicit error handling\n    entry=$(jq -c \"select(.uuid == \\\"$uuid\\\")\" \"$session_file\" 2>&1) || {\n      echo \"ERROR: jq failed parsing $session_file\" >&2\n      return 1\n    }\n\n    if [[ -n \"$entry\" ]]; then\n      parent=$(echo \"$entry\" | jq -r '.parentUuid // empty') || parent=\"\"\n      timestamp=$(echo \"$entry\" | jq -r '.timestamp // \"unknown\"') || timestamp=\"unknown\"\n      type=$(echo \"$entry\" | jq -r '.type // \"unknown\"') || type=\"unknown\"\n\n      echo \"  [$depth] $uuid ($type) @ $timestamp\"\n      echo \"       -> parent: ${parent:-<root>}\"\n\n      uuid=\"$parent\"\n      ((depth++)) || true\n    else\n      echo \"  UUID $uuid not in current session, searching others...\"\n      found=false\n      for session in \"$PROJECT_SESSIONS\"/*.jsonl; do\n        [[ ! -f \"$session\" ]] && continue\n        if grep -q \"\\\"uuid\\\":\\\"$uuid\\\"\" \"$session\"; then\n          session_file=\"$session\"\n          echo \"   Found in $(basename \"$session\")\"\n          found=true\n          break\n        fi\n      done\n      if [[ \"$found\" == \"false\" ]]; then\n        echo \"  WARNING: UUID chain broken - $uuid not found in any session\" >&2\n        break\n      fi\n    fi\n  done\n\n  if [[ $depth -ge $max_depth ]]; then\n    echo \"WARNING: Reached max chain depth ($max_depth) - chain may be incomplete\" >&2\n  fi\n\n  echo \" Chain depth: $depth\"\n}\nTRACE_EOF\n```\n\n---\n\n## Part 3: Registry Schema\n\n### registry.jsonl (Master Index)\n\nEach line is a complete, self-contained JSON object:\n\n```json\n{\n  \"id\": \"2026-01-01-multiyear-momentum\",\n  \"type\": \"research_session\",\n  \"title\": \"Multi-Year Cross-Sectional Momentum Strategy Validation\",\n  \"project\": \"alpha-forge\",\n  \"branch\": \"feat/2026-01-01-multiyear-cs-momentum-research\",\n  \"created_at\": \"2026-01-03T01:00:00Z\",\n  \"created_by\": {\n    \"github_username\": \"terrylica\",\n    \"model\": \"claude-opus-4-5-20251101\",\n    \"session_uuid\": \"8c821a19-e4f4-45d5-9338-be3a47ac81a3\"\n  },\n  \"strategy_type\": \"cross_sectional_momentum\",\n  \"date_range\": {\"start\": \"2022-01-01\", \"end\": \"2025-12-31\"},\n  \"session_contexts\": [\n    {\"session_uuid\": \"8c821a19-...\", \"type\": \"main\", \"entries\": 1128, \"description\": \"Primary session - research iterations, PR preparation\"},\n    {\"session_uuid\": \"agent-a728ebe\", \"type\": \"subagent\", \"entries\": 113, \"timestamp_start\": \"2026-01-02T07:25:47.658Z\", \"description\": \"Explore agent - codebase analysis\"}\n  ],\n  \"metrics\": {\"sharpe_2bps\": 1.05, \"sharpe_13bps\": 0.31, \"max_drawdown\": -0.18},\n  \"tags\": [\"momentum\", \"cross-sectional\", \"multi-year\", \"validated\"],\n  \"artifacts\": {\n    \"adr\": \"docs/adr/2026-01-02-multiyear-momentum-vs-ml.md\",\n    \"strategy_config\": \"examples/02_strategies/cs_momentum_multiyear.yaml\",\n    \"research_log\": \"outputs/research_sessions/2026-01-01-multiyear-momentum/research_log.md\",\n    \"iteration_configs\": \"outputs/research_sessions/2026-01-01-multiyear-momentum/\",\n    \"s3\": \"s3://eonlabs-findings/sessions/2026-01-01-multiyear-momentum/\"\n  },\n  \"status\": \"validated\",\n  \"finding\": \"BiLSTM time-series models show no predictive edge (49.05% hit rate). Simple CS momentum outperforms.\",\n  \"recommendation\": \"Deploy CS Momentum 120+240 strategy. Abandon ML-based approaches for this market regime.\"\n}\n```\n\n**Required Fields**:\n- `id` - Unique identifier (format: `YYYY-MM-DD-slug`)\n- `type` - `research_session` | `finding` | `decision`\n- `created_at` - ISO8601 timestamp\n- `created_by.github_username` - **MANDATORY** - GitHub username\n- `session_contexts` - **MANDATORY** - Array of ALL session UUIDs\n\n**Optional Fields**:\n- `title` - Human-readable title\n- `project` - Project/repository name\n- `branch` - Git branch name\n- `strategy_type` - Strategy classification (for research_session type)\n- `date_range` - `{start, end}` date range covered\n- `metrics` - Key performance metrics object\n- `tags` - Searchable tags array\n- `artifacts` - Object with paths (see Artifact Paths below)\n- `status` - `draft` | `validated` | `production` | `archived`\n- `finding` - Summary of what was discovered\n- `recommendation` - What to do next\n\n**Artifact Paths**:\n| Key | Location | Purpose |\n|-----|----------|---------|\n| `adr` | `docs/adr/...` | Committed ADR document |\n| `strategy_config` | `examples/...` | Committed strategy example |\n| `research_log` | `outputs/research_sessions/.../` | Gitignored research log |\n| `iteration_configs` | `outputs/research_sessions/.../` | Gitignored config files |\n| `s3` | `s3://eonlabs-findings/sessions/<id>/` | S3 archive for team sharing |\n\n### iterations.jsonl (Detailed Records)\n\nLocated at `findings/sessions/<id>/iterations.jsonl`. For iteration-level tracking:\n\n```json\n{\n  \"id\": \"iter-001\",\n  \"registry_id\": \"2026-01-01-multiyear-momentum\",\n  \"type\": \"iteration\",\n  \"created_at\": \"2026-01-01T10:00:00Z\",\n  \"created_by\": {\n    \"github_username\": \"terrylica\",\n    \"model\": \"claude-opus-4-5-20251101\",\n    \"session_uuid\": \"8c821a19-e4f4-45d5-9338-be3a47ac81a3\"\n  },\n  \"hypothesis\": \"Test BiLSTM with conservative clip\",\n  \"config\": {\"strategy\": \"bilstm\", \"clip\": 0.05},\n  \"results\": {\"train_sharpe\": 0.31, \"test_sharpe\": -1.15},\n  \"finding\": \"BiLSTM shows no edge\",\n  \"status\": \"FAILED\"\n}\n```\n\n---\n\n## Part 4: Output Generation\n\n### Compressed Session Context\n\nFor archival, compress sessions with Brotli:\n\n```bash\n/usr/bin/env bash << 'COMPRESS_EOF'\nset -euo pipefail\n\n# Validate required variables\nif [[ -z \"${TARGET_ID:-}\" ]]; then\n  echo \"ERROR: TARGET_ID variable not set\" >&2\n  exit 1\nfi\n\nif [[ -z \"${SESSION_LIST:-}\" ]]; then\n  echo \"ERROR: SESSION_LIST variable not set\" >&2\n  exit 1\nfi\n\nif [[ -z \"${PROJECT_SESSIONS:-}\" ]]; then\n  echo \"ERROR: PROJECT_SESSIONS variable not set\" >&2\n  exit 1\nfi\n\nOUTPUT_DIR=\"outputs/research_sessions/${TARGET_ID}\"\nmkdir -p \"$OUTPUT_DIR\" || {\n  echo \"ERROR: Failed to create output directory: $OUTPUT_DIR\" >&2\n  exit 1\n}\n# NOTE: This directory is gitignored. Artifacts are preserved in S3, not git.\n\n# Compress each session\nARCHIVED_COUNT=0\nFAILED_COUNT=0\n\nfor session_id in $SESSION_LIST; do\n  SESSION_PATH=\"$PROJECT_SESSIONS/${session_id}.jsonl\"\n  if [[ -f \"$SESSION_PATH\" ]]; then\n    if brotli -9 -o \"$OUTPUT_DIR/${session_id}.jsonl.br\" \"$SESSION_PATH\"; then\n      echo \" Archived: ${session_id}\"\n      ((ARCHIVED_COUNT++)) || true\n    else\n      echo \"ERROR: Failed to compress ${session_id}\" >&2\n      ((FAILED_COUNT++)) || true\n    fi\n  else\n    echo \"WARNING: Session file not found: $SESSION_PATH\" >&2\n  fi\ndone\n\nif [[ $ARCHIVED_COUNT -eq 0 ]]; then\n  echo \"ERROR: No sessions were archived\" >&2\n  exit 1\nfi\n\nif [[ $FAILED_COUNT -gt 0 ]]; then\n  echo \"ERROR: $FAILED_COUNT session(s) failed to compress\" >&2\n  exit 1\nfi\n\n# Create manifest with proper JSON\ncat > \"$OUTPUT_DIR/manifest.json\" << MANIFEST\n{\n  \"target_id\": \"$TARGET_ID\",\n  \"sessions_archived\": $ARCHIVED_COUNT,\n  \"created_at\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n}\nMANIFEST\n\necho \" Archived $ARCHIVED_COUNT sessions to $OUTPUT_DIR\"\nCOMPRESS_EOF\n```\n\n### Git Commit Message Template\n\n```\nfeat(finding): <short description>\n\nSession-Chronicle Provenance:\nregistry_id: <registry_id>\ngithub_username: <github_username>\nmain_sessions: <count>\nsubagent_sessions: <count>\ntotal_entries: <total>\n\nArtifacts:\n- findings/registry.jsonl\n- findings/sessions/<id>/iterations.jsonl\n- S3: s3://eonlabs-findings/sessions/<id>/\n\n## S3 Artifact Retrieval\n\n# Download compressed artifacts from S3\nexport AWS_ACCESS_KEY_ID=$(op read \"op://Claude Automation/ise47dxnkftmxopupffavsgby4/access key id\")\nexport AWS_SECRET_ACCESS_KEY=$(op read \"op://Claude Automation/ise47dxnkftmxopupffavsgby4/secret access key\")\nexport AWS_DEFAULT_REGION=\"us-west-2\"\naws s3 sync s3://eonlabs-findings/sessions/<id>/ ./artifacts/\nfor f in ./artifacts/*.br; do brotli -d \"$f\"; done\n\nCo-authored-by: Claude <noreply@anthropic.com>\n```\n\n---\n\n## Part 5: Confirmation Workflow\n\n### Final Confirmation Before Write\n\n**ALWAYS** show the user what will be written before appending:\n\n```\nAskUserQuestion:\n  question: \"Ready to write to registry. Confirm the entry:\"\n  header: \"Confirm\"\n  multiSelect: false\n  options:\n    - label: \"Write to registry\"\n      description: \"Append this entry to findings/registry.jsonl\"\n    - label: \"Edit first\"\n      description: \"Let me modify some fields before writing\"\n    - label: \"Cancel\"\n      description: \"Don't write anything\"\n```\n\nBefore this question, display:\n1. Full JSON entry (pretty-printed)\n2. Count of session_contexts entries\n3. GitHub username attribution\n4. Target file path\n\n### Post-Write Verification\n\nAfter writing, verify:\n\n```bash\n# Validate NDJSON format\ntail -1 findings/registry.jsonl | jq . > /dev/null && echo \"Valid JSON\"\n\n# Show what was written\necho \"Entry added:\"\ntail -1 findings/registry.jsonl | jq '.id, .created_by.github_username, (.session_contexts | length)'\n```\n\n---\n\n## Part 6: Workflow Summary\n\n```\n1. PREFLIGHT\n    Verify session storage location\n    Find ALL sessions (main + subagent)\n    Check required tools (jq, brotli)\n\n2. ASK: TARGET TYPE\n    AskUserQuestion: What to trace?\n\n3. ASK: GITHUB ATTRIBUTION\n    AskUserQuestion: Who created this?\n\n4. ASK: SESSION SCOPE\n    AskUserQuestion: Which sessions? (Default: ALL)\n\n5. BUILD session_contexts ARRAY\n    Enumerate ALL main sessions\n    Enumerate ALL subagent sessions\n    Collect metadata (entries, timestamps)\n\n6. ASK: PREVIEW session_contexts\n    AskUserQuestion: Review before writing\n\n7. ASK: OUTPUT FORMAT\n    AskUserQuestion: What to generate?\n\n8. ASK: ADR LINK\n    AskUserQuestion: Link to ADR?\n\n9. GENERATE OUTPUTS\n    Build registry.jsonl entry (with iterations_path, iterations_count)\n    Build iterations.jsonl entries (if applicable)\n    Prepare commit message\n\n10. ASK: FINAL CONFIRMATION\n     AskUserQuestion: Ready to write?\n\n11. WRITE & VERIFY\n     Append to registry.jsonl\n     Append to sessions/<id>/iterations.jsonl\n     Validate NDJSON format\n\n12. (OPTIONAL) S3 UPLOAD\n     Upload compressed archives\n```\n\n---\n\n## Success Criteria\n\n1. **Complete session enumeration** - ALL main + subagent sessions recorded\n2. **GitHub attribution** - `created_by.github_username` always present\n3. **Self-contained registry** - Future maintainers don't need archaeology\n4. **User confirmation** - Every step has AskUserQuestion confirmation\n5. **Valid NDJSON** - All entries pass `jq` validation\n6. **Reproducible** - Session UUIDs enable full context retrieval\n\n---\n\n## References\n\n- [S3 Sharing ADR](/docs/adr/2026-01-02-session-chronicle-s3-sharing.md)\n- [S3 Retrieval Guide](./references/s3-retrieval-guide.md)\n- [NDJSON Specification](https://github.com/ndjson/ndjson-spec)\n- [jq Manual](https://jqlang.github.io/jq/manual/)\n- [Brotli Compression](https://github.com/google/brotli)"
              },
              {
                "name": "session-recovery",
                "description": "Troubleshoot Claude Code session issues. Use when encountering \"No conversations found\" errors, missing sessions, or session file corruption problems.",
                "path": "plugins/devops-tools/skills/session-recovery/SKILL.md",
                "frontmatter": {
                  "name": "session-recovery",
                  "description": "Troubleshoot Claude Code session issues. Use when encountering \"No conversations found\" errors, missing sessions, or session file corruption problems.",
                  "allowed-tools": "Read, Bash"
                },
                "content": "# Claude Code Session Recovery Skill\n\n## Quick Reference\n\n**When to use this skill:**\n\n- \"No conversations found to resume\" when running `claude -r`\n- New conversations not creating session files\n- Sessions appearing in wrong locations (`/tmp/` instead of `~/.claude/projects/`)\n- Session history missing after environment changes\n- IDE/terminal settings affecting session creation\n- Need to migrate or recover 600+ legacy sessions\n\n## Official Session Storage\n\n**Standard Location:** `~/.claude/projects/`\n\n**Structure:**\n\n```\n~/.claude/projects/\n -home-username-my-project/     # Encoded absolute path\n    364695f1-13e7-4cbb-ad4b-0eb416feb95d.jsonl\n -tmp-another-project/\n     a8e39846-ceca-421d-b4bd-3ba0eb1b3145.jsonl\n```\n\n**Format:** One JSON event per line (JSONL), UUID-based filenames\n\n## Critical Pitfall: HOME Variable\n\n### Problem\n\nClaude Code uses `$HOME` environment variable to determine session storage location. If `$HOME` is incorrect, sessions go to wrong directory or disappear.\n\n### Symptoms\n\n- `claude -r` shows \"No conversations found to resume\"\n- New conversations work but files don't appear in expected location\n- Sessions found in `/tmp/` or other unexpected paths\n- Works on one machine but not another\n\n### Diagnosis\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# Step 1: Check current HOME\necho \"Current HOME: $HOME\"\n\n# Step 2: Check system expectation\necho \"Expected HOME: $(getent passwd $(whoami) | cut -d: -f6)\"\n\n# Step 3: Find where Claude is actually writing\nfind /tmp -name \"*.jsonl\" -path \"*/.claude/projects/*\" 2>/dev/null\nPREFLIGHT_EOF\n```\n\n---\n\n## Reference Documentation\n\nFor detailed diagnostic steps and solutions, see:\n\n- [Troubleshooting Guide](./TROUBLESHOOTING.md) - Detailed diagnostic procedures and fixes"
              },
              {
                "name": "telegram-bot-management",
                "description": "Telegram bot production management, monitoring, and troubleshooting. Use when user mentions telegram bot, claude-orchestrator, bot status, bot restart, or bot monitoring.",
                "path": "plugins/devops-tools/skills/telegram-bot-management/SKILL.md",
                "frontmatter": {
                  "name": "telegram-bot-management",
                  "description": "Telegram bot production management, monitoring, and troubleshooting. Use when user mentions telegram bot, claude-orchestrator, bot status, bot restart, or bot monitoring."
                },
                "content": "# Telegram Bot Management\n\n## Overview\n\nMulti-workspace Telegram bot workflow orchestration with full supervision (launchd + watchexec). Manages the claude-orchestrator Telegram bot for headless Claude Code interactions.\n\n## When to Use This Skill\n\n- Check bot status, restart, or troubleshoot issues\n- Monitor bot health and resource usage\n- View bot logs and debug problems\n- Manage bot lifecycle (start/stop/restart)\n\n## Production Mode\n\nAs of v5.8.0, production mode is the only operational mode.\n\n## Bot Management Commands\n\n### Check Status\n\n```bash\nbot-service.sh status\n# Or use alias\nbot status\n```\n\nShows:\n- launchd supervision status\n- watchexec process (PID, uptime, memory)\n- Bot process (PID, uptime, memory)\n- Full process tree\n- Recent log activity\n\n### View Logs\n\n```bash\nbot-service.sh logs\n# Or use alias\nbot logs\n```\n\nTails all logs:\n- Launchd logs (supervision layer)\n- Bot logs (application layer)\n\n### Restart Bot\n\n```bash\nbot-service.sh restart\n# Or use alias\nbot restart\n```\n\nRarely needed due to automatic code reload via watchexec.\n\n### Stop Bot\n\n```bash\nbot-service.sh stop\n# Or use alias\nbot stop\n```\n\n______________________________________________________________________\n\n## Reference Documentation\n\nFor detailed information, see:\n- [Operational Commands](./references/operational-commands.md) - Status, restart, logs, monitoring commands\n- [Troubleshooting](./references/troubleshooting.md) - Common issues and diagnostic steps"
              }
            ]
          },
          {
            "name": "dotfiles-tools",
            "description": "Chezmoi dotfile management via natural language workflows",
            "source": "./plugins/dotfiles-tools/",
            "category": "utilities",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install dotfiles-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/hooks",
                "description": "Install/uninstall chezmoi hooks to ~/.claude/settings.json",
                "path": "plugins/dotfiles-tools/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall chezmoi hooks to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead",
                  "argument-hint": "[install|uninstall|status|restore [latest|<n>]]"
                },
                "content": "# Dotfiles Hooks Manager\n\nManage chezmoi-sync-reminder hook installation in `~/.claude/settings.json`.\n\nClaude Code only loads hooks from settings.json, not from plugin.json files. This command installs/uninstalls the chezmoi PostToolUse hook that reminds you to sync dotfile changes.\n\n## Actions\n\n| Action           | Description                            |\n| ---------------- | -------------------------------------- |\n| `status`         | Show current installation state        |\n| `install`        | Add chezmoi hook to settings.json      |\n| `uninstall`      | Remove chezmoi hook from settings.json |\n| `restore`        | List available backups with numbers    |\n| `restore latest` | Restore most recent backup             |\n| `restore <n>`    | Restore backup by number               |\n\n## Execution\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'HOOKS_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/dotfiles-tools}\"\nACTION=\"${ARGUMENTS:-status}\"\nbash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" $ACTION\nHOOKS_SCRIPT_EOF\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall/restore operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe hooks are loaded at session start. Modifications to settings.json require a restart."
              }
            ],
            "skills": [
              {
                "name": "chezmoi-workflows",
                "description": "Dotfile backup, sync, and version control with chezmoi. Tracks shell configs (.zshrc, .bashrc, .zshenv), git (.gitconfig), editors (helix, vim, nvim), terminal tools (broot, starship, alacritty, kitty, wezterm), and XDG .config/ files. Operations include track, add, sync, push, pull, backup, restore, status, diff, re-add. Setup for chezmoi init, dotfiles remote, GitHub private repository, cross-machine sync, multi-account SSH. Handles merge conflicts, secret detection, Go templates.",
                "path": "plugins/dotfiles-tools/skills/chezmoi-workflows/SKILL.md",
                "frontmatter": {
                  "name": "chezmoi-workflows",
                  "description": "Dotfile backup, sync, and version control with chezmoi. Tracks shell configs (.zshrc, .bashrc, .zshenv), git (.gitconfig), editors (helix, vim, nvim), terminal tools (broot, starship, alacritty, kitty, wezterm), and XDG .config/ files. Operations include track, add, sync, push, pull, backup, restore, status, diff, re-add. Setup for chezmoi init, dotfiles remote, GitHub private repository, cross-machine sync, multi-account SSH. Handles merge conflicts, secret detection, Go templates.",
                  "allowed-tools": "Read, Edit, Bash"
                },
                "content": "# Chezmoi Workflows\n\n## Architecture\n\n| Component  | Location                         | Purpose                               |\n| ---------- | -------------------------------- | ------------------------------------- |\n| **Source** | `$(chezmoi source-path)`         | Git repository with dotfile templates |\n| **Target** | `~/`                             | Home directory (deployed files)       |\n| **Remote** | GitHub (private recommended)     | Cross-machine sync and backup         |\n| **Config** | `~/.config/chezmoi/chezmoi.toml` | User preferences and settings         |\n\n---\n\n## 1. Status Check\n\n```bash\nchezmoi source-path                    # Show source directory\nchezmoi git -- remote -v               # Show GitHub remote\nchezmoi status                         # Show drift between source and target\nchezmoi managed | wc -l                # Count tracked files\n```\n\n---\n\n## 2. Track File Changes\n\nAfter editing a config file, add it to chezmoi:\n\n```bash\nchezmoi status                         # 1. Verify file shows as modified\nchezmoi diff ~/.zshrc                  # 2. Review changes\nchezmoi add ~/.zshrc                   # 3. Add to source (auto-commits if configured)\nchezmoi git -- log -1 --oneline        # 4. Verify commit created\nchezmoi git -- push                    # 5. Push to remote\n```\n\n---\n\n## 3. Track New File\n\nAdd a previously untracked config file:\n\n```bash\nchezmoi add ~/.config/app/config.toml  # 1. Add file to source\nchezmoi managed | grep app             # 2. Verify in managed list\nchezmoi git -- push                    # 3. Push to remote\n```\n\n---\n\n## 4. Sync from Remote\n\nPull changes from GitHub and apply to home directory:\n\n```bash\nchezmoi update                         # 1. Pull + apply (single command)\nchezmoi verify                         # 2. Verify all files match source\nchezmoi status                         # 3. Confirm no drift\n```\n\n---\n\n## 5. Push All Changes\n\nBulk sync all modified tracked files to remote:\n\n```bash\nchezmoi status                         # 1. Review all drift\nchezmoi re-add                         # 2. Re-add all managed files (auto-commits)\nchezmoi git -- push                    # 3. Push to remote\n```\n\n---\n\n## 6. First-Time Setup\n\n### Install chezmoi\n\n```bash\nbrew install chezmoi                   # macOS\n```\n\n### Initialize (fresh start)\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF'\nchezmoi init                           # Create empty source\nchezmoi add ~/.zshrc ~/.gitconfig      # Add first files\ngh repo create dotfiles --private --source=\"$(chezmoi source-path)\" --push\nCONFIG_EOF\n```\n\n### Initialize (clone existing)\n\n```bash\nchezmoi init git@github.com:<user>/dotfiles.git\nchezmoi apply                          # Deploy to home directory\n```\n\n---\n\n## 7. Configure Source Directory\n\nMove source to custom location (e.g., for multi-account SSH):\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\nmv \"$(chezmoi source-path)\" ~/path/to/dotfiles\nSKILL_SCRIPT_EOF\n```\n\nEdit `~/.config/chezmoi/chezmoi.toml`:\n\n```toml\nsourceDir = \"~/path/to/dotfiles\"\n```\n\nVerify:\n\n```bash\nchezmoi source-path                    # Should show new location\n```\n\n---\n\n## 8. Change Remote\n\nSwitch to different GitHub account or repository:\n\n```bash\nchezmoi git -- remote -v                                              # View current\nchezmoi git -- remote set-url origin git@github.com:<user>/<repo>.git # Change\nchezmoi git -- push -u origin main                                    # Push to new remote\n```\n\n---\n\n## 9. Resolve Merge Conflicts\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\nchezmoi git -- status                  # 1. Identify conflicted files\nchezmoi git -- diff                    # 2. Review conflicts\n# Manually edit files in $(chezmoi source-path)\nchezmoi git -- add <resolved-files>    # 3. Stage resolved files\nchezmoi git -- commit -m \"Resolve merge conflict\"\nchezmoi apply                          # 4. Apply to home directory\nchezmoi git -- push                    # 5. Push resolution\nGIT_EOF\n```\n\n---\n\n## 10. Validation (SLO)\n\nAfter major operations, verify system state:\n\n```bash\nchezmoi verify                         # Exit 0 = all files match source\nchezmoi diff                           # Empty = no drift\nchezmoi managed                        # Lists all tracked files\nchezmoi git -- log --oneline -3        # Recent commit history\n```\n\n---\n\n## Reference\n\n- [Setup Guide](./references/setup.md) - Installation, multi-account GitHub, migration\n- [Prompt Patterns](./references/prompt-patterns.md) - Detailed workflow examples\n- [Configuration](./references/configuration.md) - chezmoi.toml settings, templates\n- [Secret Detection](./references/secret-detection.md) - Handling detected secrets\n\n**Chezmoi docs**: <https://www.chezmoi.io/reference/>"
              }
            ]
          },
          {
            "name": "doc-tools",
            "description": "Comprehensive documentation: ASCII diagrams, markdown standards, LaTeX build, Pandoc PDF generation",
            "source": "./plugins/doc-tools/",
            "category": "documents",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install doc-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "ascii-diagram-validator",
                "description": "Validates ASCII box-drawing diagram alignment in markdown files. Use when creating architecture diagrams, checking diagram alignment, fixing ASCII art, or before committing documentation with diagrams.",
                "path": "plugins/doc-tools/skills/ascii-diagram-validator/SKILL.md",
                "frontmatter": {
                  "name": "ascii-diagram-validator",
                  "description": "Validates ASCII box-drawing diagram alignment in markdown files. Use when creating architecture diagrams, checking diagram alignment, fixing ASCII art, or before committing documentation with diagrams."
                },
                "content": "# ASCII Diagram Validator\n\nValidate and fix alignment issues in ASCII box-drawing diagrams commonly used in architecture documentation, README files, and code comments.\n\n## Overview\n\nASCII diagrams using box-drawing characters ( and double-line variants ) require precise column alignment. This skill provides:\n\n1. **Validation script** - Detects misaligned characters with file:line:column locations\n2. **Actionable fixes** - Specific suggestions for correcting each issue\n3. **Multi-file support** - Validate individual files or entire directories\n\n## When to Use This Skill\n\nInvoke when:\n\n- Creating or editing ASCII architecture diagrams in markdown\n- Reviewing documentation with box-drawing diagrams\n- Fixing \"diagram looks wrong\" complaints\n- Before committing docs/ARCHITECTURE.md or similar files\n- When user mentions \"ASCII alignment\", \"diagram alignment\", or \"box drawing\"\n\n## Supported Characters\n\n### Single-Line Box Drawing\n\n```\nCorners:    \nLines:    \nT-joins:    \nCross:   \n```\n\n### Double-Line Box Drawing\n\n```\nCorners:    \nLines:    \nT-joins:    \nCross:   \n```\n\n### Mixed (Double-Single)\n\n```\n     \n```\n\n## Quick Start\n\n### Validate a Single File\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nuv run ${CLAUDE_PLUGIN_ROOT}/skills/ascii-diagram-validator/scripts/check_ascii_alignment.py docs/ARCHITECTURE.md\nPREFLIGHT_EOF\n```\n\n### Validate Multiple Files\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF_2'\nuv run ${CLAUDE_PLUGIN_ROOT}/skills/ascii-diagram-validator/scripts/check_ascii_alignment.py docs/*.md\nPREFLIGHT_EOF_2\n```\n\n### Validate Directory\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF_3'\nuv run ${CLAUDE_PLUGIN_ROOT}/skills/ascii-diagram-validator/scripts/check_ascii_alignment.py docs/\nPREFLIGHT_EOF_3\n```\n\n## Output Format\n\nThe script outputs issues in a compiler-like format for easy navigation:\n\n```\ndocs/ARCHITECTURE.md:45:12: error: vertical connector '' at column 12 has no matching character above\n   Suggestion: Add '', '', '', '', or '' at line 44, column 12\n\ndocs/ARCHITECTURE.md:67:8: warning: horizontal line '' at column 8 has no terminator\n   Suggestion: Add '', '', '', '', or '' to close the line\n```\n\n### Severity Levels\n\n| Level   | Description                              |\n| ------- | ---------------------------------------- |\n| error   | Broken connections, misaligned verticals |\n| warning | Unterminated lines, potential issues     |\n| info    | Style suggestions (optional cleanup)     |\n\n## Validation Rules\n\nThe script checks for:\n\n1. **Vertical Alignment** - Vertical connectors () must align with characters above/below\n2. **Corner Connections** - Corners () must connect properly to adjacent lines\n3. **Junction Validity** - T-joins and crosses must have correct incoming/outgoing connections\n4. **Line Continuity** - Horizontal lines () should terminate at valid endpoints\n5. **Box Closure** - Boxes should be properly closed (no dangling edges)\n\n## Exit Codes\n\n| Code | Meaning                                         |\n| ---- | ----------------------------------------------- |\n| 0    | No issues found                                 |\n| 1    | Errors detected                                 |\n| 2    | Warnings only (errors ignored with --warn-only) |\n\n## Integration with Claude Code\n\nWhen Claude Code creates or edits ASCII diagrams:\n\n1. Run the validator script on the file\n2. Review any errors in the output\n3. Apply suggested fixes\n4. Re-run until clean\n\n### Example Workflow\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF_4'\n# After editing docs/ARCHITECTURE.md\nuv run ${CLAUDE_PLUGIN_ROOT}/skills/ascii-diagram-validator/scripts/check_ascii_alignment.py docs/ARCHITECTURE.md\n\n# If errors found, Claude Code can read the output and fix:\n# docs/ARCHITECTURE.md:45:12: error: vertical connector '' at column 12 has no matching character above\n#  Edit line 44, column 12 to add the missing connector\nPREFLIGHT_EOF_4\n```\n\n## Limitations\n\n- Detects structural alignment issues, not aesthetic spacing\n- Requires consistent use of box-drawing characters (no mixed ASCII like +---+)\n- Tab characters may cause false positives (convert to spaces first)\n- Unicode normalization not performed (use pre-composed characters)\n\n## Bundled Scripts\n\n| Script                             | Purpose                |\n| ---------------------------------- | ---------------------- |\n| `scripts/check_ascii_alignment.py` | Main validation script |\n\n## Related\n\n- [ARCHITECTURE.md best practices](https://github.com/joelparkerhenderson/architecture-decision-record)\n- [Unicode Box Drawing block](https://www.unicode.org/charts/PDF/U2500.pdf)"
              },
              {
                "name": "documentation-standards",
                "description": "Markdown documentation standards for LLM-optimized architecture including section numbering rules for Pandoc PDF generation. Use when writing markdown documentation, creating skills, or authoring content that may be converted to PDF.",
                "path": "plugins/doc-tools/skills/documentation-standards/SKILL.md",
                "frontmatter": {
                  "name": "documentation-standards",
                  "description": "Markdown documentation standards for LLM-optimized architecture including section numbering rules for Pandoc PDF generation. Use when writing markdown documentation, creating skills, or authoring content that may be converted to PDF."
                },
                "content": "# Documentation Standards\n\n## Overview\n\nStandards for writing markdown documentation optimized for both LLM consumption and conversion to professional PDFs using Pandoc. Ensures consistency across all documentation.\n\n## When to Use This Skill\n\nUse when:\n\n- Writing markdown documentation (README, skills, guides, specifications)\n- Creating new skills that include markdown content\n- Authoring content that may be converted to PDF\n- Reviewing documentation for standards compliance\n\n## Core Principles\n\n### 1. LLM-Optimized Documentation Architecture\n\n**Machine-Readable Priority**: OpenAPI 3.1.0 specs, JSON Schema, YAML specifications take precedence over human documentation.\n\n**Why**: Structured formats provide unambiguous contracts that both humans and LLMs can consume reliably. Human docs supplement, don't replace, machine-readable specs.\n\n**Application**:\n\n- Workflow specifications  OpenAPI 3.1.1 YAML in specifications/\n- Data schemas  JSON Schema with examples\n- Configuration  YAML with validation schemas\n- Human docs  Markdown referencing canonical machine-readable specs\n\n### 2. Hub-and-Spoke Progressive Disclosure\n\n**Pattern**: Central hubs (like CLAUDE.md, INDEX.md) link to detailed spokes (skills, docs directories).\n\n**Structure**:\n\n```\nCLAUDE.md (Hub - Essentials Only)\n     links to\nSkills (Spokes - Progressive Disclosure)\n     SKILL.md (Overview + Quick Start)\n     references/ (Detailed Documentation)\n```\n\n**Rules**:\n\n- Hubs contain essentials only (what + where to find more)\n- Spokes contain progressive detail (load as needed)\n- Single source of truth per topic (no duplication)\n\n### 3. Markdown Section Numbering\n\n**Critical Rule**: Never manually number markdown headings.\n\n **Wrong**:\n\n```markdown\n## 1. Introduction\n\n### 1.1 Background\n\n### 1.2 Objectives\n\n## 2. Implementation\n```\n\n **Correct**:\n\n```markdown\n## Introduction\n\n### Background\n\n### Objectives\n\n## Implementation\n```\n\n**Rationale**:\n\n- Pandoc's `--number-sections` flag auto-numbers all sections when generating PDFs\n- Manual numbering creates duplication: \"1. 1. Introduction\" in rendered output\n- Auto-numbering is consistent, updates automatically when sections reorganize\n- Applies to ALL markdown: documentation, skills, project files, README files\n\n**Rule**: If markdown might ever convert to PDF, never manually number headings. Use semantic heading levels (##, ###) and let tools handle numbering.\n\n## Standards Checklist\n\nUse this checklist when creating or reviewing documentation:\n\n### Structure\n\n- [ ] Follows hub-and-spoke pattern (essentials in main doc, details in references)\n- [ ] Links to deeper documentation for progressive disclosure\n- [ ] Single source of truth (no duplicate content across docs)\n\n### Markdown Formatting\n\n- [ ] No manual section numbering in headings\n- [ ] Semantic heading levels (##, ###, ####) used correctly\n- [ ] Code blocks have language identifiers for syntax highlighting\n- [ ] Links use markdown format `[text](url)`, not bare URLs\n\n### Machine-Readable Content\n\n- [ ] Workflows documented as OpenAPI 3.1.1 specs (when applicable)\n- [ ] Data structures use JSON Schema (when applicable)\n- [ ] Configuration uses YAML with validation (when applicable)\n- [ ] Human docs reference canonical machine-readable specs\n\n### File Organization\n\n- [ ] Documentation lives in appropriate location:\n  - Global standards  docs/standards/\n  - Skill documentation  skills/{skill-name}/references/\n  - Project documentation  {project}/.claude/ or {project}/docs/\n- [ ] Index files provide navigation (INDEX.md, README.md)\n\n## Related Resources\n\n- **ASCII Diagram Validation**: [ascii-diagram-validator](../ascii-diagram-validator/SKILL.md) - Validate ASCII diagrams in markdown\n- **Skill Architecture**: See skill-architecture plugin for creating effective skills\n\n## Examples\n\n### Good Hub-and-Spoke Structure\n\n**Hub (CLAUDE.md)**:\n\n```markdown\n## PDF Generation from Markdown\n\n**Quick Start**: Use pandoc-pdf-generation skill\n\n**Critical Rules**:\n\n1. Never write ad-hoc pandoc commands\n2. Always verify PDFs before presenting\n3. See skill for detailed principles\n```\n\n**Spoke (skill/SKILL.md)**:\n\n- Quick start with examples\n- Link to references/ for detailed documentation\n- Progressive disclosure as needed\n\n### Good Machine-Readable Documentation\n\n**Workflow Specification** (specifications/hook-prompt-capture.yaml):\n\n```yaml\nopenapi: 3.1.1\ninfo:\n  title: Hook Prompt Capture Workflow\n  version: 1.0.0\npaths:\n  /capture-prompt:\n    post:\n      summary: Capture user prompt from hook\n      # ... detailed spec\n```\n\n**Human Documentation** (README.md):\n\n```markdown\n## Workflow\n\nSee [hook-prompt-capture.yaml](./specifications/hook-prompt-capture.yaml)\nfor complete workflow specification.\n\nQuick overview: ...\n```\n\n## Summary\n\nDocumentation standards ensure:\n\n- **Consistency** across all workspace documentation\n- **LLM optimization** through machine-readable formats\n- **Maintainability** via hub-and-spoke + single source of truth\n- **PDF compatibility** through proper markdown formatting\n\nFollow these standards for all documentation."
              },
              {
                "name": "latex-build",
                "description": "Builds LaTeX documents using latexmk with live preview and dependency tracking. Use when setting up builds, live preview, or troubleshooting compilation.",
                "path": "plugins/doc-tools/skills/latex-build/SKILL.md",
                "frontmatter": {
                  "name": "latex-build",
                  "description": "Builds LaTeX documents using latexmk with live preview and dependency tracking. Use when setting up builds, live preview, or troubleshooting compilation.",
                  "allowed-tools": "Read, Edit, Bash"
                },
                "content": "# LaTeX Build Automation\n\n## Quick Reference\n\n**When to use this skill:**\n- Compiling LaTeX documents\n- Setting up live preview with auto-rebuild\n- Managing multi-file projects\n- Troubleshooting build failures\n- Cleaning build artifacts\n- Automating compilation workflows\n\n## Why latexmk?\n\nIndustry standard build tool:\n- Auto-detects dependencies (bibliography, index, etc.)\n- Runs correct number of times (handles cross-references)\n- Live preview mode watches for file changes\n- Works with Skim for SyncTeX auto-reload\n- Bundled with MacTeX (no separate install needed)\n\n______________________________________________________________________\n\n## Basic Usage\n\n### One-Time Build\n```bash\nlatexmk -pdf document.tex\n# Result: document.pdf created\n```\n\n### Live Preview (Watch Mode)\n```bash\nlatexmk -pvc -pdf document.tex\n\n# What happens:\n# - Compiles document initially\n# - Watches for file changes\n# - Auto-recompiles when files change\n# - Auto-reloads PDF in Skim viewer\n```\n\n**Stop watching:** Press `Ctrl+C`\n\n______________________________________________________________________\n\n## Quick Reference Card\n\n```bash\n# Build once\nlatexmk -pdf document.tex\n\n# Live preview (watch mode)\nlatexmk -pvc -pdf document.tex\n\n# Build with SyncTeX\nlatexmk -pdf -synctex=1 document.tex\n\n# Clean artifacts\nlatexmk -c              # Keep PDF\nlatexmk -C              # Remove PDF too\n\n# Force rebuild\nlatexmk -gg -pdf document.tex\n\n# Non-interactive (for CI)\nlatexmk -pdf -interaction=nonstopmode document.tex\n```\n\n______________________________________________________________________\n\n## Build Checklist\n\n- [ ] Verify latexmk installed: `which latexmk`\n- [ ] Test basic build: `latexmk -pdf document.tex`\n- [ ] Enable SyncTeX: Add `-synctex=1` flag\n- [ ] Test live preview: `latexmk -pvc -pdf document.tex`\n- [ ] Configure Skim for auto-reload\n- [ ] Create Makefile for common tasks (optional)\n- [ ] Create .latexmkrc for project-specific settings (optional)\n- [ ] Test clean: `latexmk -c` removes artifacts\n\n______________________________________________________________________\n\n## Reference Documentation\n\nFor detailed information, see:\n- [Common Commands](./references/common-commands.md) - Build options and output formats\n- [Multi-File Projects](./references/multi-file-projects.md) - Automatic dependency tracking for complex documents\n- [Configuration](./references/configuration.md) - .latexmkrc and Makefile integration\n- [Troubleshooting](./references/troubleshooting.md) - Common build issues and solutions\n- [Advanced Patterns](./references/advanced-patterns.md) - Parallel builds and CI/CD integration\n\n**Official Docs**: Run `man latexmk` or `latexmk -help` for complete reference\n\n**See Also**:\n- Use `latex/setup` skill for installing LaTeX and configuring environment\n- Use `latex/tables` skill for creating tables with tabularray"
              },
              {
                "name": "latex-setup",
                "description": "Installs and configures complete LaTeX development environment on macOS with MacTeX, Skim viewer, and SyncTeX support. Use when setting up new machine, installing LaTeX, or configuring PDF viewer.",
                "path": "plugins/doc-tools/skills/latex-setup/SKILL.md",
                "frontmatter": {
                  "name": "latex-setup",
                  "description": "Installs and configures complete LaTeX development environment on macOS with MacTeX, Skim viewer, and SyncTeX support. Use when setting up new machine, installing LaTeX, or configuring PDF viewer.",
                  "allowed-tools": "Read, Edit, Bash"
                },
                "content": "# LaTeX Environment Setup\n\n## Quick Reference\n\n**When to use this skill:**\n- Installing LaTeX on a new machine\n- Setting up MacTeX distribution\n- Configuring Skim PDF viewer with SyncTeX\n- Verifying LaTeX installation\n- Troubleshooting missing packages\n\n## Recommended Stack\n\n| Component        | Purpose                                 | Status         |\n|------------------|-----------------------------------------|----------------|\n| **MacTeX 2025**  | Full LaTeX distribution (TeX Live 2025) |  Recommended  |\n| **Skim 1.7.11**  | PDF viewer with SyncTeX support         |  macOS only   |\n| **TeXShop 5.57** | Integrated LaTeX IDE (optional)         |  Native macOS |\n\n______________________________________________________________________\n\n## Quick Start\n\n### Install MacTeX\n```bash\nbrew install --cask mactex\n# Size: ~4.5 GB (includes everything)\n```\n\n### Verify Installation\n```bash\ntex --version\n# Expected: TeX 3.141592653 (TeX Live 2025)\n\npdflatex --version\nlatexmk --version\n```\n\n### Test Compilation\n```bash\necho '\\documentclass{article}\\begin{document}Hello World!\\end{document}' > test.tex\npdflatex test.tex\nls test.pdf  # Verify PDF created\n```\n\n______________________________________________________________________\n\n## Post-Installation Checklist\n\n- [ ] Verify `tex --version` shows TeX Live 2025\n- [ ] Verify `latexmk --version` shows 4.86a+\n- [ ] Verify `pdflatex test.tex` creates PDF\n- [ ] Install Skim if using mactex-no-gui\n- [ ] Test SyncTeX: compile with `-synctex=1` flag\n- [ ] Configure Skim preferences for editor integration\n- [ ] Add `/Library/TeX/texbin` to PATH if needed\n- [ ] Test package installation: `sudo tlmgr install <package>`\n\n______________________________________________________________________\n\n## Reference Documentation\n\nFor detailed information, see:\n- [Installation](./references/installation.md) - Full MacTeX vs lightweight options, Skim installation\n- [Verification](./references/verification.md) - Check installation, verify PATH, test compilation\n- [Package Management](./references/package-management.md) - Check, install, search for packages with tlmgr\n- [Skim Configuration](./references/skim-configuration.md) - Enable SyncTeX, configure preferences for editor integration\n- [Troubleshooting](./references/troubleshooting.md) - PATH issues, tlmgr problems, permissions\n\n**See Also**:\n- Build Workflows: Use `latex/build` skill for latexmk automation\n- Table Creation: Use `latex/tables` skill for tabularray usage"
              },
              {
                "name": "latex-tables",
                "description": "Creates modern LaTeX tables with tabularray package for fixed-width columns, proper alignment, and clean syntax. Use when creating tables or working with column layouts.",
                "path": "plugins/doc-tools/skills/latex-tables/SKILL.md",
                "frontmatter": {
                  "name": "latex-tables",
                  "description": "Creates modern LaTeX tables with tabularray package for fixed-width columns, proper alignment, and clean syntax. Use when creating tables or working with column layouts.",
                  "allowed-tools": "Read, Edit, Bash"
                },
                "content": "# LaTeX Tables with tabularray\n\n## Quick Reference\n\n**When to use this skill:**\n- Creating tables with fixed-width columns\n- Formatting complex table layouts\n- Need precise column alignment\n- Migrating from tabular/tabularx/longtable/booktabs\n- Troubleshooting table overflow issues\n\n## Why tabularray?\n\nModern LaTeX3 package (replaces old solutions):\n- Fixed-width columns with proper alignment\n- Clean, consistent syntax\n- Replaces: `tabular`, `tabularx`, `longtable`, `booktabs`\n- Better performance than legacy packages\n- Part of TeX Live 2025\n\n______________________________________________________________________\n\n## Installation\n\n```bash\n# Check if installed\nkpsewhich tabularray.sty\n\n# If not found, install:\nsudo tlmgr install tabularray\n```\n\n## Basic Usage\n\n```latex\n\\documentclass{article}\n\\usepackage{tabularray}  % Modern table package\n\n\\begin{document}\n% Simple table\n\\begin{tblr}{colspec={ccc}, hlines, vlines}\n  Header 1 & Header 2 & Header 3 \\\\\n  Data 1   & Data 2   & Data 3   \\\\\n\\end{tblr}\n\\end{document}\n```\n\n______________________________________________________________________\n\n## Quick Reference Card\n\n```latex\n% Minimal table\n\\begin{tblr}{colspec={ccc}}\n  A & B & C \\\\\n\\end{tblr}\n\n% With all lines\n\\begin{tblr}{colspec={ccc}, hlines, vlines}\n  A & B & C \\\\\n\\end{tblr}\n\n% Fixed widths\n\\begin{tblr}{colspec={Q[2cm] Q[3cm] Q[2cm]}, hlines}\n  A & B & C \\\\\n\\end{tblr}\n\n% Bold header\n\\begin{tblr}{\n  colspec={ccc},\n  row{1}={font=\\bfseries}\n}\n  Header & Header & Header \\\\\n  Data   & Data   & Data   \\\\\n\\end{tblr}\n```\n\n______________________________________________________________________\n\n## Best Practices\n\n1. Use Q[width] for fixed columns instead of p{width}\n2. Specify widths explicitly when text might overflow\n3. Use X for flexible columns that should expand\n4. Style headers with row{1} instead of manual formatting\n5. Use colspec for column properties, not inline commands\n6. Check package version: `kpsewhich tabularray.sty` (should be recent)\n\n______________________________________________________________________\n\n## Reference Documentation\n\nFor detailed information, see:\n- [Table Patterns](./references/table-patterns.md) - 5 common table patterns with examples\n- [Column Specification](./references/column-spec.md) - Alignment options and width control\n- [Lines and Borders](./references/lines-borders.md) - All lines, selective lines, thick lines\n- [Troubleshooting](./references/troubleshooting.md) - Table too wide, text not wrapping, alignment issues\n- [Migration](./references/migration.md) - Migrating from tabular and tabularx\n\n**Official Docs**: Run `texdoc tabularray` for complete package documentation\n\n**See Also**:\n- Use `latex/setup` skill for installing tabularray package\n- Use `latex/build` skill for compilation workflows"
              },
              {
                "name": "pandoc-pdf-generation",
                "description": "Use when writing markdown intended for PDF output, creating PDFs from markdown, or printing PDFs. Invoke EARLY when authoring markdown for PDF to prevent mistakes (manual heading numbers, manual ASCII diagrams, inline annotations). Also use for PDF generation with Pandoc/XeLaTeX, section numbering, table of contents, bibliography, landscape/portrait orientation, printing (one-sided, duplex, simplex, lpr), fixing diagrams breaking across pages, code block page breaks, or double numbering issues. Triggers on - markdown for PDF, write document for printing, create printable doc, pandoc, xelatex, print PDF, PDF generation.",
                "path": "plugins/doc-tools/skills/pandoc-pdf-generation/SKILL.md",
                "frontmatter": {
                  "name": "pandoc-pdf-generation",
                  "description": "Use when writing markdown intended for PDF output, creating PDFs from markdown, or printing PDFs. Invoke EARLY when authoring markdown for PDF to prevent mistakes (manual heading numbers, manual ASCII diagrams, inline annotations). Also use for PDF generation with Pandoc/XeLaTeX, section numbering, table of contents, bibliography, landscape/portrait orientation, printing (one-sided, duplex, simplex, lpr), fixing diagrams breaking across pages, code block page breaks, or double numbering issues. Triggers on - markdown for PDF, write document for printing, create printable doc, pandoc, xelatex, print PDF, PDF generation."
                },
                "content": "# Pandoc PDF Generation\n\n## Overview\n\nGenerate professional PDF documents from Markdown using Pandoc with the XeLaTeX engine. This skill covers automatic section numbering, table of contents, bibliography management, LaTeX customization, and common troubleshooting patterns learned through production use.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- Converting Markdown to PDF with professional formatting requirements\n- Needing automatic section numbering and table of contents\n- Managing citations and bibliographies without manual duplication\n- Controlling table formatting and page breaks in LaTeX output\n- Building automated PDF generation workflows\n\n## Quick Start: Universal Build Script\n\n### Single Source of Truth Pattern\n\nThis skill provides production-proven assets in `${CLAUDE_PLUGIN_ROOT}/skills/pandoc-pdf-generation/assets/`:\n\n- `table-spacing-template.tex` - Production-tuned LaTeX preamble (booktabs, colortbl, ToC fixes)\n- `build-pdf.sh` - Universal auto-detecting build script\n\n### From Any Project\n\n```bash\n/usr/bin/env bash << 'DETECT_EOF'\n# Create symlink once per project (git-friendly)\nln -s ${CLAUDE_PLUGIN_ROOT}/skills/pandoc-pdf-generation/assets/build-pdf.sh build-pdf.sh\n\n# Auto-detect single .md file in directory (landscape default)\n./build-pdf.sh\n\n# Portrait mode\n./build-pdf.sh --portrait document.md\n\n# Monospace font for ASCII diagrams\n./build-pdf.sh --monospace diagrams.md\n\n# Explicit input/output\n./build-pdf.sh input.md output.pdf\nDETECT_EOF\n```\n\n**Options:**\n\n| Flag             | Description                                                |\n| ---------------- | ---------------------------------------------------------- |\n| `--landscape`    | Landscape orientation (default)                            |\n| `--portrait`     | Portrait orientation                                       |\n| `--monospace`    | Use DejaVu Sans Mono - ideal for ASCII diagrams            |\n| `--hide-details` | Hide `<details>` blocks (e.g., graph-easy source) from PDF |\n| `-h, --help`     | Show help message                                          |\n\n**Features:**\n\n-  Auto-detects input file (if single .md exists)\n-  Auto-detects bibliography (`references.bib`) and CSL files\n-  Always uses production-proven LaTeX preamble from skill\n-  Pre-flight checks (pandoc, xelatex, files exist)\n-  Post-build validation (file size, page count)\n-  Code blocks stay on same page (no splitting across pages)\n-  Lua filter to hide `<details>` blocks from PDF output\n\n### Landscape PDF (Quick Command)\n\nFor landscape PDFs with blue hyperlinks (no build-pdf.sh dependency):\n\n```bash\npandoc file.md -o file.pdf \\\n  --pdf-engine=xelatex \\\n  -V geometry:a4paper,landscape \\\n  -V geometry:margin=1in \\\n  -V fontsize=11pt \\\n  -V mainfont=\"DejaVu Sans\" \\\n  -V colorlinks=true \\\n  -V linkcolor=blue \\\n  -V urlcolor=blue \\\n  --toc --toc-depth=2 \\\n  --number-sections\n```\n\n**Use landscape for**: Wide data tables, comparison matrices, technical docs with code blocks.\n\n### Manual Command (With LaTeX Preamble)\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\npandoc document.md \\\n  -o document.pdf \\\n  --pdf-engine=xelatex \\\n  --toc \\\n  --toc-depth=3 \\\n  --number-sections \\\n  -V geometry:margin=1in \\\n  -V mainfont=\"DejaVu Sans\" \\\n  -H ${CLAUDE_PLUGIN_ROOT}/skills/pandoc-pdf-generation/assets/table-spacing-template.tex\nSKILL_SCRIPT_EOF\n```\n\n---\n\n## ASCII Diagrams: Always Use graph-easy\n\n**CRITICAL**: Never manually type ASCII diagrams. Always use the `itp:graph-easy` skill.\n\nManual ASCII art causes alignment issues in PDFs. The graph-easy skill ensures:\n\n- Proper boxart character alignment\n- Consistent spacing\n- Reproducible output\n\n```bash\n# Invoke the skill for general diagrams\nSkill(itp:graph-easy)\n\n# For ADR architecture diagrams\nSkill(itp:adr-graph-easy-architect)\n```\n\n**Also important**: Keep annotations OUTSIDE code blocks. Don't add inline comments like `# contains: file1, file2` inside diagram code blocks - they break alignment.\n\n---\n\n## Hiding Content for PDF Output\n\nUse `--hide-details` to remove `<details>` blocks from PDF output. This is useful when:\n\n- **graph-easy source blocks**: Keep source in markdown for diagram regeneration, but hide from printed PDFs\n- **Technical implementation notes**: Show in web/markdown view, hide from printed handouts\n- **Collapsible sections**: HTML `<details>` tags don't render as collapsible in PDF\n\n**Usage:**\n\n```bash\n./build-pdf.sh --hide-details document.md\n```\n\n**Markdown pattern:**\n\n````markdown\n## My Section\n\n```diagram\n     \n Box  >  Box \n     \n```\n````\n\n<details>\n<summary>graph-easy source</summary>\n\n```\n[Box] -> [Box]\n```\n\n</details>\n```\n\nWith `--hide-details`, the entire `<details>` block is stripped from PDF output while remaining visible in markdown/HTML.\n\n---\n\n## Verification Checklist\n\nBefore considering a PDF \"done\", verify:\n\n**Pre-Generation:**\n\n- [ ] No manual section numbering in markdown (use `--number-sections`)\n- [ ] All ASCII diagrams generated via `itp:graph-easy` skill\n- [ ] Annotations are outside code blocks, not inside\n\n**Post-Generation:**\n\n- [ ] Open PDF and visually inspect each page\n- [ ] Verify diagrams don't break across pages\n- [ ] Check section numbering is correct (no \"1. 1. Title\" duplication)\n- [ ] Confirm bullet lists render as bullets, not inline dashes\n\n**Pre-Print:**\n\n- [ ] Get user approval before printing\n- [ ] Confirm orientation preference (landscape/portrait)\n- [ ] Confirm duplex preference (one-sided/two-sided)\n\n---\n\n## Printing Workflow\n\nAlways let the user review the PDF before printing.\n\n**Open for review:**\n\n```bash\nopen output.pdf\n```\n\n**Print one-sided (simplex):**\n\n```bash\nlpr -P \"PRINTER_NAME\" -o Duplex=None output.pdf\n```\n\n**Print two-sided (duplex):**\n\n```bash\nlpr -P \"PRINTER_NAME\" -o Duplex=DuplexNoTumble output.pdf  # Long-edge binding\nlpr -P \"PRINTER_NAME\" -o Duplex=DuplexTumble output.pdf    # Short-edge binding\n```\n\n**Find printer name:**\n\n```bash\nlpstat -p -d\n```\n\n**Never print without user approval** - this wastes paper if issues exist.\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Core Development Principles](./references/core-principles.md) - **START HERE** - Universal principles learned from production failures\n- [Markdown for PDF](./references/markdown-for-pdf.md) - Markdown structure patterns for clean landscape PDFs\n- [YAML Front Matter Structure](./references/yaml-structure.md) - YAML metadata patterns\n- [LaTeX Customization](./references/latex-parameters.md) - Preamble and table formatting\n- [Bibliography & Citations](./references/bibliography-citations.md) - BibTeX and CSL styles\n- [Document Patterns](./references/document-patterns.md) - Document type templates\n- [Troubleshooting](./references/troubleshooting-pandoc.md) - Common issues and fixes"
              },
              {
                "name": "terminal-print",
                "description": "Print iTerm2 terminal output to HP network printer. Strips ANSI escape codes, wraps in monospace code block, generates PDF via pandoc/xelatex, previews, and prints. Invoke on - print terminal, terminal to printer, print session, lpr output, print command output, terminal PDF.",
                "path": "plugins/doc-tools/skills/terminal-print/SKILL.md",
                "frontmatter": {
                  "name": "terminal-print",
                  "description": "Print iTerm2 terminal output to HP network printer. Strips ANSI escape codes, wraps in monospace code block, generates PDF via pandoc/xelatex, previews, and prints. Invoke on - print terminal, terminal to printer, print session, lpr output, print command output, terminal PDF."
                },
                "content": "# Terminal Print\n\nPrint terminal output from iTerm2 to your HP network printer with a single command.\n\n## Quick Start\n\n1. **Copy** terminal output in iTerm2 (Cmd+C)\n2. **Invoke** this skill\n3. **Review** PDF preview, press Enter to print\n\n## How It Works\n\n```\nClipboard  Strip ANSI  Markdown code block  pandoc/xelatex  PDF  Preview  Print\n```\n\n- **ANSI codes stripped**: Colors and escape sequences removed for clean B&W output\n- **Monospace font**: DejaVu Sans Mono for proper character alignment\n- **Landscape orientation**: Fits ~120 characters per line\n- **US Letter paper**: Auto-detected from printer settings\n\n## Execution\n\n```bash\n/usr/bin/env bash << 'PRINT_EOF'\nSKILL_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/doc-tools}/skills/terminal-print\"\nbash \"$SKILL_DIR/assets/print-terminal.sh\"\nPRINT_EOF\n```\n\n## Options\n\nRun with arguments by modifying the execution block:\n\n```bash\n/usr/bin/env bash << 'PRINT_EOF'\nSKILL_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/doc-tools}/skills/terminal-print\"\nbash \"$SKILL_DIR/assets/print-terminal.sh\" --no-preview\nPRINT_EOF\n```\n\n| Flag           | Description                              |\n| -------------- | ---------------------------------------- |\n| `--file FILE`  | Read from file instead of clipboard      |\n| `--no-preview` | Skip PDF preview, print directly         |\n| `--no-print`   | Generate PDF only, don't send to printer |\n| `-h, --help`   | Show help message                        |\n\n## Examples\n\n### Print from clipboard (default)\n\n```bash\n# Copy terminal output in iTerm2, then:\n/usr/bin/env bash << 'EOF'\nbash \"${CLAUDE_PLUGIN_ROOT}/skills/terminal-print/assets/print-terminal.sh\"\nEOF\n```\n\n### Print from file\n\n```bash\n/usr/bin/env bash << 'EOF'\nbash \"${CLAUDE_PLUGIN_ROOT}/skills/terminal-print/assets/print-terminal.sh\" --file ~/session.log\nEOF\n```\n\n### Generate PDF only (no print)\n\n```bash\n/usr/bin/env bash << 'EOF'\nbash \"${CLAUDE_PLUGIN_ROOT}/skills/terminal-print/assets/print-terminal.sh\" --no-print\nEOF\n```\n\n## Prerequisites\n\nAll dependencies are already available on macOS with MacTeX:\n\n| Tool      | Purpose          | Status            |\n| --------- | ---------------- | ----------------- |\n| `pandoc`  | Markdown to PDF  | Required          |\n| `xelatex` | PDF engine       | Required (MacTeX) |\n| `pbpaste` | Clipboard access | Built-in          |\n| `lpr`     | CUPS printing    | Built-in          |\n\n## Output\n\n- **PDF location**: `/tmp/terminal-output-YYYYMMDD_HHMMSS.pdf`\n- **Markdown source**: `/tmp/terminal-YYYYMMDD_HHMMSS.md`\n- **Cleanup**: macOS automatically cleans `/tmp` periodically\n\n## Troubleshooting\n\n### \"No text in clipboard\"\n\nCopy terminal output first using Cmd+C in iTerm2.\n\n### \"Missing pandoc\" or \"Missing xelatex\"\n\nInstall MacTeX: `brew install --cask mactex`\n\n### Printer not found\n\nCheck printer status: `lpstat -p -d`\n\nThe default printer is `HP_LaserJet_Pro_MFP_3101_3108`. Edit the script to change.\n\n## Related Skills\n\n- [pandoc-pdf-generation](../pandoc-pdf-generation/SKILL.md) - General Markdown to PDF conversion\n- [asciinema-converter](../../../asciinema-tools/skills/asciinema-converter/SKILL.md) - Convert terminal recordings"
              }
            ]
          },
          {
            "name": "quality-tools",
            "description": "Code quality and validation: clone detection, multi-agent E2E validation, performance profiling, schema testing",
            "source": "./plugins/quality-tools/",
            "category": "quality",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install quality-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "clickhouse-architect",
                "description": "ClickHouse schema design authority (hub skill). Use when designing schemas, selecting compression codecs, tuning ORDER BY, optimizing queries, or reviewing table structure. **Delegates to**: clickhouse-cloud-management for user creation, clickhouse-pydantic-config for DBeaver config, schema-e2e-validation for YAML contracts. Triggers: \"design ClickHouse schema\", \"compression codecs\", \"MergeTree optimization\", \"ORDER BY tuning\", \"partition key\", \"ClickHouse performance\", \"SharedMergeTree\", \"ReplicatedMergeTree\", \"migrate to ClickHouse\".\n",
                "path": "plugins/quality-tools/skills/clickhouse-architect/SKILL.md",
                "frontmatter": {
                  "name": "clickhouse-architect",
                  "description": "ClickHouse schema design authority (hub skill). Use when designing schemas, selecting compression codecs, tuning ORDER BY, optimizing queries, or reviewing table structure. **Delegates to**: clickhouse-cloud-management for user creation, clickhouse-pydantic-config for DBeaver config, schema-e2e-validation for YAML contracts. Triggers: \"design ClickHouse schema\", \"compression codecs\", \"MergeTree optimization\", \"ORDER BY tuning\", \"partition key\", \"ClickHouse performance\", \"SharedMergeTree\", \"ReplicatedMergeTree\", \"migrate to ClickHouse\".\n",
                  "allowed-tools": "Read, Bash, Grep, Skill"
                },
                "content": "# ClickHouse Architect\n\n<!-- ADR: 2025-12-09-clickhouse-architect-skill -->\n\nPrescriptive schema design, compression selection, and performance optimization for ClickHouse (v24.4+). Covers both ClickHouse Cloud (SharedMergeTree) and self-hosted (ReplicatedMergeTree) deployments.\n\n## Core Methodology\n\n### Schema Design Workflow\n\nFollow this sequence when designing or reviewing ClickHouse schemas:\n\n1. **Define ORDER BY key** (3-5 columns, lowest cardinality first)\n2. **Select compression codecs** per column type\n3. **Configure PARTITION BY** for data lifecycle management\n4. **Add performance accelerators** (projections, indexes)\n5. **Validate with audit queries** (see scripts/)\n6. **Document with COMMENT statements** (see [`references/schema-documentation.md`](./references/schema-documentation.md))\n\n### ORDER BY Key Selection\n\nThe ORDER BY clause is the most critical decision in ClickHouse schema design.\n\n**Rules**:\n\n- Limit to 3-5 columns maximum (each additional column has diminishing returns)\n- Place lowest cardinality columns first (e.g., `tenant_id` before `timestamp`)\n- Include all columns used in WHERE clauses for range queries\n- PRIMARY KEY must be a prefix of ORDER BY (or omit to use full ORDER BY)\n\n**Example**:\n\n```sql\n-- Correct: Low cardinality first, 4 columns\nCREATE TABLE trades (\n    exchange LowCardinality(String),\n    symbol LowCardinality(String),\n    timestamp DateTime64(3),\n    trade_id UInt64,\n    price Float64,\n    quantity Float64\n) ENGINE = MergeTree()\nORDER BY (exchange, symbol, timestamp, trade_id);\n\n-- Wrong: High cardinality first (10x slower queries)\nORDER BY (trade_id, timestamp, symbol, exchange);\n```\n\n### Compression Codec Quick Reference\n\n| Column Type              | Default Codec              | Read-Heavy Alternative    | Example                                            |\n| ------------------------ | -------------------------- | ------------------------- | -------------------------------------------------- |\n| DateTime/DateTime64      | `CODEC(DoubleDelta, ZSTD)` | `CODEC(DoubleDelta, LZ4)` | `timestamp DateTime64(3) CODEC(DoubleDelta, ZSTD)` |\n| Float prices/gauges      | `CODEC(Gorilla, ZSTD)`     | `CODEC(Gorilla, LZ4)`     | `price Float64 CODEC(Gorilla, ZSTD)`               |\n| Integer counters         | `CODEC(T64, ZSTD)`         |                          | `count UInt64 CODEC(T64, ZSTD)`                    |\n| Slowly changing integers | `CODEC(Delta, ZSTD)`       | `CODEC(Delta, LZ4)`       | `version UInt32 CODEC(Delta, ZSTD)`                |\n| String (low cardinality) | `LowCardinality(String)`   |                          | `status LowCardinality(String)`                    |\n| General data             | `CODEC(ZSTD(3))`           | `CODEC(LZ4)`              | Default compression level 3                        |\n\n**When to use LZ4 over ZSTD**: LZ4 provides 1.76x faster decompression. Use LZ4 for read-heavy workloads with monotonic sequences (timestamps, counters). Use ZSTD (default) when compression ratio matters or data patterns are unknown.\n\n**Note on codec combinations**:\n\nDelta/DoubleDelta + Gorilla combinations are blocked by default (`allow_suspicious_codecs`) because Gorilla already performs implicit delta compression internallycombining them is **redundant**, not dangerous. A historical corruption bug (PR #45615, Jan 2023) was fixed, but the blocking remains as a best practice guardrail.\n\nUse each codec family independently for its intended data type:\n\n```sql\n-- Correct usage\nprice Float64 CODEC(Gorilla, ZSTD)              -- Floats: use Gorilla\ntimestamp DateTime64 CODEC(DoubleDelta, ZSTD)   -- Timestamps: use DoubleDelta\ntimestamp DateTime64 CODEC(DoubleDelta, LZ4)    -- Read-heavy: use LZ4\n```\n\n### PARTITION BY Guidelines\n\nPARTITION BY is for **data lifecycle management**, NOT query optimization.\n\n**Rules**:\n\n- Partition by time units (month, week) for TTL and data management\n- Keep partition count under 1000 total across all tables\n- Each partition should contain 1-300 parts maximum\n- Never partition by high-cardinality columns\n\n**Example**:\n\n```sql\n-- Correct: Monthly partitions for TTL management\nPARTITION BY toYYYYMM(timestamp)\n\n-- Wrong: Daily partitions (too many parts)\nPARTITION BY toYYYYMMDD(timestamp)\n\n-- Wrong: High-cardinality partition key\nPARTITION BY user_id\n```\n\n### Anti-Patterns Checklist (v24.4+)\n\n| Pattern                         | Severity | Modern Status      | Fix                                   |\n| ------------------------------- | -------- | ------------------ | ------------------------------------- |\n| Too many parts (>300/partition) | Critical | Still critical     | Reduce partition granularity          |\n| Small batch inserts (<1000)     | Critical | Still critical     | Batch to 10k-100k rows                |\n| High-cardinality first ORDER BY | Critical | Still critical     | Reorder: lowest cardinality first     |\n| No memory limits                | High     | Still critical     | Set `max_memory_usage`                |\n| Denormalization overuse         | High     | Still critical     | Use dictionaries + materialized views |\n| Large JOINs                     | Medium   | **180x improved**  | Still avoid for ultra-low-latency     |\n| Mutations (UPDATE/DELETE)       | Medium   | **1700x improved** | Use lightweight updates (v24.4+)      |\n\n### Table Engine Selection\n\n| Deployment          | Engine                | Use Case                        |\n| ------------------- | --------------------- | ------------------------------- |\n| ClickHouse Cloud    | `SharedMergeTree`     | Default for cloud deployments   |\n| Self-hosted cluster | `ReplicatedMergeTree` | Multi-node with replication     |\n| Self-hosted single  | `MergeTree`           | Single-node development/testing |\n\n**Cloud (SharedMergeTree)**:\n\n```sql\nCREATE TABLE trades (...)\nENGINE = SharedMergeTree('/clickhouse/tables/{shard}/trades', '{replica}')\nORDER BY (exchange, symbol, timestamp);\n```\n\n**Self-hosted (ReplicatedMergeTree)**:\n\n```sql\nCREATE TABLE trades (...)\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/trades', '{replica}')\nORDER BY (exchange, symbol, timestamp);\n```\n\n## Skill Delegation Guide\n\n<!-- ADR: 2025-12-10-clickhouse-skill-delegation -->\n\nThis skill is the **hub** for ClickHouse-related tasks. When the user's needs extend beyond schema design, invoke the related skills below.\n\n### Delegation Decision Matrix\n\n| User Need                                       | Invoke Skill                               | Trigger Phrases                                      |\n| ----------------------------------------------- | ------------------------------------------ | ---------------------------------------------------- |\n| Create database users, manage permissions       | `devops-tools:clickhouse-cloud-management` | \"create user\", \"GRANT\", \"permissions\", \"credentials\" |\n| Configure DBeaver, generate connection JSON     | `devops-tools:clickhouse-pydantic-config`  | \"DBeaver\", \"client config\", \"connection setup\"       |\n| Validate schema contracts against live database | `quality-tools:schema-e2e-validation`      | \"validate schema\", \"Earthly E2E\", \"schema contract\"  |\n\n### Typical Workflow Sequence\n\n1. **Schema Design** (THIS SKILL)  Design ORDER BY, compression, partitioning\n2. **User Setup**  `clickhouse-cloud-management` (if cloud credentials needed)\n3. **Client Config**  `clickhouse-pydantic-config` (generate DBeaver JSON)\n4. **Validation**  `schema-e2e-validation` (CI/CD schema contracts)\n\n### Example: Full Stack Request\n\n**User**: \"I need to design a trades table for ClickHouse Cloud and set up DBeaver to query it.\"\n\n**Expected behavior**:\n\n1. Use THIS skill for schema design\n2. Invoke `clickhouse-cloud-management` for creating database user\n3. Invoke `clickhouse-pydantic-config` for DBeaver configuration\n\n## Performance Accelerators\n\n### Projections\n\nCreate alternative sort orders that ClickHouse automatically selects:\n\n```sql\nALTER TABLE trades ADD PROJECTION trades_by_symbol (\n    SELECT * ORDER BY symbol, timestamp\n);\nALTER TABLE trades MATERIALIZE PROJECTION trades_by_symbol;\n```\n\n### Materialized Views\n\nPre-compute aggregations for dashboard queries:\n\n```sql\nCREATE MATERIALIZED VIEW trades_hourly_mv\nENGINE = SummingMergeTree()\nORDER BY (exchange, symbol, hour)\nAS SELECT\n    exchange,\n    symbol,\n    toStartOfHour(timestamp) AS hour,\n    sum(quantity) AS total_volume,\n    count() AS trade_count\nFROM trades\nGROUP BY exchange, symbol, hour;\n```\n\n### Dictionaries\n\nReplace JOINs with O(1) dictionary lookups for **large-scale star schemas**:\n\n**When to use dictionaries (v24.4+)**:\n\n- Fact tables with 100M+ rows joining dimension tables\n- Dimension tables 1k-500k rows with monotonic keys\n- LEFT ANY JOIN semantics required\n\n**When JOINs are sufficient (v24.4+)**:\n\n- Dimension tables <500 rows (JOIN overhead negligible)\n- v24.4+ predicate pushdown provides 8-180x improvements\n- Complex JOIN types (FULL, RIGHT, multi-condition)\n\n**Benchmark context**: 6.6x speedup measured on Star Schema Benchmark (1.4B rows).\n\n```sql\nCREATE DICTIONARY symbol_info (\n    symbol String,\n    name String,\n    sector String\n)\nPRIMARY KEY symbol\nSOURCE(CLICKHOUSE(TABLE 'symbols'))\nLAYOUT(FLAT())  -- Best for <500k entries with monotonic keys\nLIFETIME(3600);\n\n-- Use in queries (O(1) lookup)\nSELECT\n    symbol,\n    dictGet('symbol_info', 'name', symbol) AS symbol_name\nFROM trades;\n```\n\n## Scripts\n\nExecute comprehensive schema audit:\n\n```bash\nclickhouse-client --multiquery < scripts/schema-audit.sql\n```\n\nThe audit script checks:\n\n- Part count per partition (threshold: 300)\n- Compression ratios by column\n- Query performance patterns\n- Replication lag (if applicable)\n- Memory usage patterns\n\n## Additional Resources\n\n### Reference Files\n\n| Reference                                                                                  | Content                                        |\n| ------------------------------------------------------------------------------------------ | ---------------------------------------------- |\n| [`references/schema-design-workflow.md`](./references/schema-design-workflow.md)           | Complete workflow with examples                |\n| [`references/compression-codec-selection.md`](./references/compression-codec-selection.md) | Decision tree + benchmarks                     |\n| [`references/anti-patterns-and-fixes.md`](./references/anti-patterns-and-fixes.md)         | 13 deadly sins + v24.4+ status                 |\n| [`references/audit-and-diagnostics.md`](./references/audit-and-diagnostics.md)             | Query interpretation guide                     |\n| [`references/idiomatic-architecture.md`](./references/idiomatic-architecture.md)           | Parameterized views, dictionaries, dedup       |\n| [`references/schema-documentation.md`](./references/schema-documentation.md)               | COMMENT patterns + naming for AI understanding |\n\n### External Documentation\n\n- [ClickHouse Best Practices](https://clickhouse.com/docs/best-practices)\n- [Altinity Knowledge Base](https://kb.altinity.com/)\n- [ClickHouse Blog](https://clickhouse.com/blog)\n\n## Python Driver Policy\n\n<!-- ADR: 2025-12-10-clickhouse-python-driver-policy -->\n\n**Use `clickhouse-connect` (official) for all Python integrations.**\n\n```python\n#  RECOMMENDED: clickhouse-connect (official, HTTP)\nimport clickhouse_connect\n\nclient = clickhouse_connect.get_client(\n    host='localhost',\n    port=8123,  # HTTP port\n    username='default',\n    password=''\n)\nresult = client.query(\"SELECT * FROM trades LIMIT 1000\")\ndf = client.query_df(\"SELECT * FROM trades\")  # Pandas integration\n```\n\n### Why NOT `clickhouse-driver`\n\n| Factor          | clickhouse-connect | clickhouse-driver   |\n| --------------- | ------------------ | ------------------- |\n| Maintainer      | ClickHouse Inc.    | Solo developer      |\n| Weekly commits  | Yes (active)       | Sparse (months)     |\n| Open issues     | 41 (addressed)     | 76 (accumulating)   |\n| Downloads/week  | 2.7M               | 1.5M                |\n| Bus factor risk | Low (company)      | **High (1 person)** |\n\n**Do NOT use `clickhouse-driver`** despite its ~26% speed advantage for large exports. The maintenance risk outweighs performance gains:\n\n- Single maintainer (mymarilyn) with no succession plan\n- Issues accumulating without response\n- Risk of abandonment breaks production code\n\n**Exception**: Only consider `clickhouse-driver` if you have extreme performance requirements (exporting millions of rows) AND accept the maintenance risk.\n\n## Related Skills\n\n| Skill                                      | Purpose                       |\n| ------------------------------------------ | ----------------------------- |\n| `devops-tools:clickhouse-cloud-management` | User/permission management    |\n| `devops-tools:clickhouse-pydantic-config`  | DBeaver connection generation |\n| `quality-tools:schema-e2e-validation`      | YAML schema contracts         |\n| `quality-tools:multi-agent-e2e-validation` | Database migration validation |"
              },
              {
                "name": "code-clone-assistant",
                "description": "Detects and refactors code duplication using PMD CPD. Use when identifying code clones, addressing DRY violations, or refactoring duplicate code across repositories.",
                "path": "plugins/quality-tools/skills/code-clone-assistant/SKILL.md",
                "frontmatter": {
                  "name": "code-clone-assistant",
                  "description": "Detects and refactors code duplication using PMD CPD. Use when identifying code clones, addressing DRY violations, or refactoring duplicate code across repositories.",
                  "allowed-tools": "Read, Grep, Bash, Edit, Write"
                },
                "content": "# Code Clone Assistant\n\nDetect code clones and guide refactoring using PMD CPD (exact duplicates) + Semgrep (patterns).\n\n## Tools\n\n- **PMD CPD v7.17.0+**: Exact duplicate detection\n- **Semgrep v1.140.0+**: Pattern-based detection\n\n**Tested**: October 2025 - 30 violations detected across 3 sample files\n**Coverage**: ~3x more violations than using either tool alone\n\n______________________________________________________________________\n\n## When to Use\n\nTriggers: \"find duplicate code\", \"DRY violations\", \"refactor similar code\", \"detect code duplication\", \"similar validation logic\", \"repeated patterns\", \"copy-paste code\", \"exact duplicates\"\n\n______________________________________________________________________\n\n## Why Two Tools?\n\nPMD CPD and Semgrep detect different clone types:\n\n| Aspect       | PMD CPD                          | Semgrep                          |\n|--------------|----------------------------------|----------------------------------|\n| **Detects**  | Exact copy-paste duplicates      | Similar patterns with variations |\n| **Scope**    | Across files                    | Within/across files (Pro only)   |\n| **Matching** | Token-based (ignores formatting) | Pattern-based (AST matching)     |\n| **Rules**    |  No custom rules                |  Custom rules                   |\n\n**Result**: Using both finds ~3x more DRY violations.\n\n### Clone Types\n\n| Type   | Description                     | PMD CPD        | Semgrep    |\n|--------|---------------------------------|----------------|------------|\n| Type-1 | Exact copies                    |  Default      |           |\n| Type-2 | Renamed identifiers             |  `--ignore-*` |           |\n| Type-3 | Near-miss with variations       |  Partial     |  Patterns |\n| Type-4 | Semantic clones (same behavior) |               |           |\n\n______________________________________________________________________\n\n## Quick Start Workflow\n\n```bash\n# Step 1: Detect exact duplicates (PMD CPD)\npmd cpd -d . -l python --minimum-tokens 20 -f markdown > pmd-results.md\n\n# Step 2: Detect pattern violations (Semgrep)\nsemgrep --config=clone-rules.yaml --sarif --quiet > semgrep-results.sarif\n\n# Step 3: Analyze combined results (Claude Code)\n# Parse both outputs, prioritize by severity\n\n# Step 4: Refactor (Claude Code with user approval)\n# Extract shared functions, consolidate patterns, verify tests\n```\n\n______________________________________________________________________\n\n\n______________________________________________________________________\n\n## Reference Documentation\n\nFor detailed information, see:\n- [Detection Commands](./references/detection-commands.md) - PMD CPD and Semgrep command details\n- [Complete Workflow](./references/complete-workflow.md) - Detection, analysis, and presentation phases\n- [Refactoring Strategies](./references/refactoring-strategies.md) - Approaches for addressing violations"
              },
              {
                "name": "multi-agent-e2e-validation",
                "description": "Multi-agent parallel E2E validation workflow for database refactors and system migrations. Use when validating QuestDB deployments, schema migrations, bulk data ingestion pipelines, or any database-centric refactor requiring comprehensive testing across environment, data flow, and query layers.",
                "path": "plugins/quality-tools/skills/multi-agent-e2e-validation/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-e2e-validation",
                  "description": "Multi-agent parallel E2E validation workflow for database refactors and system migrations. Use when validating QuestDB deployments, schema migrations, bulk data ingestion pipelines, or any database-centric refactor requiring comprehensive testing across environment, data flow, and query layers."
                },
                "content": "# Multi-Agent E2E Validation\n\n## Overview\n\nPrescriptive workflow for spawning parallel validation agents to comprehensively test database refactors. Successfully identified 5 critical bugs (100% system failure rate) in QuestDB migration that would have shipped in production.\n\n**When to use this skill:**\n\n- Database refactors (e.g., v3.x file-based  v4.x QuestDB)\n- Schema migrations requiring validation\n- Bulk data ingestion pipeline testing\n- System migrations with multiple validation layers\n- Pre-release validation for database-centric systems\n\n**Key outcomes:**\n\n- Parallel agent execution for comprehensive coverage\n- Structured validation reporting (VALIDATION_FINDINGS.md)\n- Bug discovery with severity classification (Critical/Medium/Low)\n- Release readiness assessment\n\n## Core Methodology\n\n### 1. Validation Architecture (3-Layer Model)\n\n**Layer 1: Environment Setup**\n\n- Container orchestration (Colima/Docker)\n- Database deployment and schema application\n- Connectivity validation (ILP, PostgreSQL, HTTP ports)\n- Configuration file creation and validation\n\n**Layer 2: Data Flow Validation**\n\n- Bulk ingestion testing (CloudFront  QuestDB)\n- Performance benchmarking against SLOs\n- Multi-month data ingestion\n- Deduplication testing (re-ingestion scenarios)\n- Type conversion validation (FLOATLONG casts)\n\n**Layer 3: Query Interface Validation**\n\n- High-level query methods (get_latest, get_range, execute_sql)\n- Edge cases (limit=1, cross-month boundaries)\n- Error handling (invalid symbols, dates, parameters)\n- Gap detection SQL compatibility\n\n### 2. Agent Orchestration Pattern\n\n**Sequential vs Parallel Execution:**\n\n```\nAgent 1 (Environment)  [SEQUENTIAL - prerequisite]\n  \nAgent 2 (Bulk Loader)  [PARALLEL with Agent 3]\nAgent 3 (Query Interface)  [PARALLEL with Agent 2]\n```\n\n**Dependency Rule**: Environment validation must pass before data flow/query validation\n\n**Dynamic Todo Management:**\n\n- Start with high-level plan (ADR-defined phases)\n- Prune completed agents from todo list\n- Grow todo list when bugs discovered (e.g., Bug #5 found by Agent 3)\n- Update VALIDATION_FINDINGS.md incrementally\n\n### 3. Validation Script Structure\n\nEach agent produces:\n\n1. **Test Script** (e.g., `test_bulk_loader.py`)\n   - 5+ test functions with clear pass/fail criteria\n   - Structured output (test name, result, details)\n   - Summary report at end\n2. **Artifacts** (logs, config files, evidence)\n3. **Findings Report** (bugs, severity, fix proposals)\n\n**Example Test Structure:**\n\n```python\ndef test_feature(conn):\n    \"\"\"Test 1: Feature description\"\"\"\n    print(\"=\" * 80)\n    print(\"TEST 1: Feature description\")\n    print(\"=\" * 80)\n\n    results = {}\n\n    # Test 1a: Subtest name\n    print(\"\\n1a. Testing subtest:\")\n    result_1a = perform_test()\n    print(f\"   Result: {result_1a}\")\n    results[\"subtest_1a\"] = result_1a == expected_1a\n\n    # Summary\n    print(\"\\n\" + \"-\" * 80)\n    all_passed = all(results.values())\n    print(f\"Test 1 Results: {' PASS' if all_passed else ' FAIL'}\")\n    for test_name, passed in results.items():\n        print(f\"  - {test_name}: {'' if passed else ''}\")\n\n    return {\"success\": all_passed, \"details\": results}\n```\n\n### 4. Bug Classification and Tracking\n\n**Severity Levels:**\n\n-  **Critical**: 100% system failure (e.g., API mismatch, timestamp corruption)\n-  **Medium**: Degraded functionality (e.g., below SLO performance)\n-  **Low**: Minor issues, edge cases\n\n**Bug Report Format:**\n\n```markdown\n#### Bug N: Descriptive Name (**SEVERITY** - Status)\n\n**Location**: `file/path.py:line`\n\n**Issue**: One-sentence description\n\n**Impact**: Quantified impact (e.g., \"100% ingestion failure\")\n\n**Root Cause**: Technical explanation\n\n**Fix Applied**: Code changes with before/after\n\n**Verification**: Test results proving fix\n\n**Status**:  FIXED /  PARTIAL /  OPEN\n```\n\n### 5. Release Readiness Decision Framework\n\n**Go/No-Go Criteria:**\n\n```\nBLOCKER = Any Critical bug unfixed\nSHIP = All Critical bugs fixed + (Medium bugs acceptable OR fixed)\nDEFER = >3 Medium bugs unfixed OR any High-severity bug\n```\n\n**Example Decision:**\n\n- 5 Critical bugs found  all fixed \n- 1 Medium bug (performance 55% below SLO)  acceptable \n- Verdict: **RELEASE READY**\n\n## Workflow: Step-by-Step\n\n### Step 1: Create Validation Plan (ADR-Driven)\n\n**Input**: ADR document (e.g., ADR-0002 QuestDB Refactor)\n**Output**: Validation plan with 3-7 agents\n\n**Plan Structure:**\n\n```markdown\n## Validation Agents\n\n### Agent 1: Environment Setup\n\n- Deploy QuestDB via Docker\n- Apply schema.sql\n- Validate connectivity (ILP, PG, HTTP)\n- Create .env configuration\n\n### Agent 2: Bulk Loader Validation\n\n- Test CloudFront  QuestDB ingestion\n- Benchmark performance (target: >100K rows/sec)\n- Validate deduplication (re-ingestion test)\n- Multi-month ingestion test\n\n### Agent 3: Query Interface Validation\n\n- Test get_latest() with various limits\n- Test get_range() with date boundaries\n- Test execute_sql() with parameterized queries\n- Test detect_gaps() SQL compatibility\n- Test error handling (invalid inputs)\n```\n\n### Step 2: Execute Agent 1 (Environment)\n\n**Directory Structure:**\n\n```\ntmp/e2e-validation/\n  agent-1-env/\n    test_environment_setup.py\n    questdb.log\n    config.env\n    schema-check.txt\n```\n\n**Validation Checklist:**\n\n-  Container running\n-  Ports accessible (9009 ILP, 8812 PG, 9000 HTTP)\n-  Schema applied without errors\n-  .env file created\n\n### Step 3: Execute Agents 2-3 in Parallel\n\n**Agent 2: Bulk Loader**\n\n```\ntmp/e2e-validation/\n  agent-2-bulk/\n    test_bulk_loader.py\n    ingestion_benchmark.txt\n    deduplication_test.txt\n```\n\n**Agent 3: Query Interface**\n\n```\ntmp/e2e-validation/\n  agent-3-query/\n    test_query_interface.py\n    gap_detection_test.txt\n```\n\n**Execution:**\n\n```bash\n# Terminal 1\ncd tmp/e2e-validation/agent-2-bulk\nuv run python test_bulk_loader.py\n\n# Terminal 2\ncd tmp/e2e-validation/agent-3-query\nuv run python test_query_interface.py\n```\n\n### Step 4: Document Findings in VALIDATION_FINDINGS.md\n\n**Template:**\n\n```markdown\n# E2E Validation Findings Report\n\n**Validation ID**: ADR-XXXX\n**Branch**: feat/database-refactor\n**Date**: YYYY-MM-DD\n**Target Release**: vX.Y.Z\n**Status**: [BLOCKED / READY / IN_PROGRESS]\n\n## Executive Summary\n\nE2E validation discovered **N critical bugs** that would have caused [impact]:\n\n| Finding | Severity | Status | Impact       | Agent   |\n| ------- | -------- | ------ | ------------ | ------- |\n| Bug 1   | Critical | Fixed  | 100% failure | Agent 2 |\n\n**Recommendation**: [RELEASE READY / BLOCKED / DEFER]\n\n## Agent 1: Environment Setup - [STATUS]\n\n...\n\n## Agent 2: [Name] - [STATUS]\n\n...\n```\n\n### Step 5: Iterate on Fixes\n\n**For each bug:**\n\n1. Document in VALIDATION_FINDINGS.md with // severity\n2. Apply fix to source code\n3. Re-run failing test\n4. Update bug status to  FIXED\n5. Commit with semantic message (e.g., `fix: correct timestamp parsing in CSV ingestion`)\n\n**Example Fix Commit:**\n\n```bash\ngit add src/gapless_crypto_clickhouse/collectors/questdb_bulk_loader.py\ngit commit -m \"fix: prevent pandas from treating first CSV column as index\n\nBREAKING CHANGE: All timestamps were defaulting to epoch 0 (1970-01)\ndue to pandas read_csv() auto-indexing. Added index_col=False to\npreserve first column as data.\n\nFixes #ABC-123\"\n```\n\n### Step 6: Final Validation and Release Decision\n\n**Run all tests:**\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\ncd tmp/e2e-validation\nfor agent in agent-*; do\n    echo \"=== Running $agent ===\"\n    cd $agent\n    uv run python test_*.py\n    cd ..\ndone\nSKILL_SCRIPT_EOF\n```\n\n**Update VALIDATION_FINDINGS.md status:**\n\n- Count Critical bugs: X fixed, Y open\n- Count Medium bugs: X fixed, Y open\n- Apply decision framework\n- Update **Status** field to  RELEASE READY or  BLOCKED\n\n## Real-World Example: QuestDB Refactor Validation\n\n**Context**: Migrating from file-based storage (v3.x) to QuestDB (v4.0.0)\n\n**Bugs Found:**\n\n1.  **Sender API mismatch** - Used non-existent `Sender.from_uri()` instead of `Sender.from_conf()`\n2.  **Type conversion** - `number_of_trades` sent as FLOAT, schema expects LONG\n3.  **Timestamp parsing** - pandas treating first column as index  epoch 0 timestamps\n4.  **Deduplication** - WAL mode doesn't provide UPSERT semantics (needed `DEDUP ENABLE UPSERT KEYS`)\n5.  **SQL incompatibility** - detect_gaps() used nested window functions (QuestDB unsupported)\n\n**Impact**: Without this validation, v4.0.0 would ship with 100% data corruption and 100% ingestion failure\n\n**Outcome**: All 5 bugs fixed, system validated, v4.0.0 released successfully\n\n## Common Pitfalls\n\n### 1. Skipping Environment Validation\n\n **Bad**: Assume Docker/database is working, jump to data ingestion tests\n **Good**: Agent 1 validates environment first, catches port conflicts, schema errors early\n\n### 2. Serial Agent Execution\n\n **Bad**: Run Agent 2, wait for completion, then run Agent 3\n **Good**: Run Agent 2 & 3 in parallel (no dependency between them)\n\n### 3. Manual Test Reporting\n\n **Bad**: Copy/paste test output into Slack/email\n **Good**: Structured VALIDATION_FINDINGS.md with severity, status, fix tracking\n\n### 4. Ignoring Medium Bugs\n\n **Bad**: \"Performance is 55% below SLO, but we'll fix it later\"\n **Good**: Document in VALIDATION_FINDINGS.md, make explicit go/no-go decision\n\n### 5. No Re-validation After Fixes\n\n **Bad**: Apply fix, assume it works, move on\n **Good**: Re-run failing test, update status in VALIDATION_FINDINGS.md\n\n## Resources\n\n### scripts/\n\nNot applicable - validation scripts are project-specific (stored in `tmp/e2e-validation/`)\n\n### references/\n\n- `example_validation_findings.md` - Complete VALIDATION_FINDINGS.md template\n- `agent_test_template.py` - Template for creating validation test scripts\n- `bug_severity_classification.md` - Detailed severity criteria and examples\n\n### assets/\n\nNot applicable - validation artifacts are project-specific"
              },
              {
                "name": "multi-agent-performance-profiling",
                "description": "Multi-agent parallel performance profiling workflow for identifying bottlenecks in data pipelines. Use when investigating performance issues, optimizing ingestion pipelines, profiling database operations, or diagnosing throughput bottlenecks in multi-stage data systems.",
                "path": "plugins/quality-tools/skills/multi-agent-performance-profiling/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-performance-profiling",
                  "description": "Multi-agent parallel performance profiling workflow for identifying bottlenecks in data pipelines. Use when investigating performance issues, optimizing ingestion pipelines, profiling database operations, or diagnosing throughput bottlenecks in multi-stage data systems."
                },
                "content": "# Multi-Agent Performance Profiling\n\n## Overview\n\nPrescriptive workflow for spawning parallel profiling agents to comprehensively identify performance bottlenecks across multiple system layers. Successfully discovered that QuestDB ingests at 1.1M rows/sec (11x faster than target), proving database was NOT the bottleneck - CloudFront download was 90% of pipeline time.\n\n**When to use this skill:**\n- Performance below SLO (e.g., 47K vs 100K rows/sec target)\n- Multi-stage pipeline optimization (download  extract  parse  ingest)\n- Database performance investigation\n- Bottleneck identification in complex workflows\n- Pre-optimization analysis (before making changes)\n\n**Key outcomes:**\n- Identify true bottleneck (vs assumed bottleneck)\n- Quantify each stage's contribution to total time\n- Prioritize optimizations by impact (P0/P1/P2)\n- Avoid premature optimization of non-bottlenecks\n\n## Core Methodology\n\n### 1. Multi-Layer Profiling Model (5-Agent Pattern)\n\n**Agent 1: Profiling (Instrumentation)**\n- Empirical timing of each pipeline stage\n- Phase-boundary instrumentation with time.perf_counter()\n- Memory profiling (peak usage, allocations)\n- Bottleneck identification (% of total time)\n\n**Agent 2: Database Configuration Analysis**\n- Server settings review (WAL, heap, commit intervals)\n- Production vs development config comparison\n- Expected impact quantification (<5%, 10%, 50%)\n\n**Agent 3: Client Library Analysis**\n- API usage patterns (dataframe vs row-by-row)\n- Buffer size tuning opportunities\n- Auto-flush behavior analysis\n\n**Agent 4: Batch Size Analysis**\n- Current batch size validation\n- Optimal batch range determination\n- Memory overhead vs throughput tradeoff\n\n**Agent 5: Integration & Synthesis**\n- Consensus-building across agents\n- Prioritization (P0/P1/P2) with impact quantification\n- Implementation roadmap creation\n\n### 2. Agent Orchestration Pattern\n\n**Parallel Execution** (all 5 agents run simultaneously):\n```\nAgent 1 (Profiling)           [PARALLEL]\nAgent 2 (DB Config)           [PARALLEL]\nAgent 3 (Client Library)      [PARALLEL]\nAgent 4 (Batch Size)          [PARALLEL]\nAgent 5 (Integration)         [PARALLEL - reads tmp/ outputs from others]\n```\n\n**Key Principle**: No dependencies between investigation agents (1-4). Integration agent synthesizes findings.\n\n**Dynamic Todo Management:**\n- Start with investigation plan (5 agents)\n- Spawn agents in parallel using single message with multiple Task tool calls\n- Update todos as each agent completes\n- Integration agent waits for all findings before synthesizing\n\n### 3. Profiling Script Structure\n\nEach agent produces:\n1. **Investigation Script** (e.g., `profile_pipeline.py`)\n   - time.perf_counter() instrumentation at phase boundaries\n   - Memory profiling with tracemalloc\n   - Structured output (phase, duration, % of total)\n2. **Report** (markdown with findings, recommendations, impact quantification)\n3. **Evidence** (benchmark results, config dumps, API traces)\n\n**Example Profiling Code:**\n```python\nimport time\n\n# Profile multi-stage pipeline\ndef profile_pipeline():\n    results = {}\n\n    # Phase 1: Download\n    start = time.perf_counter()\n    data = download_from_cdn(url)\n    results[\"download\"] = time.perf_counter() - start\n\n    # Phase 2: Extract\n    start = time.perf_counter()\n    csv_data = extract_zip(data)\n    results[\"extract\"] = time.perf_counter() - start\n\n    # Phase 3: Parse\n    start = time.perf_counter()\n    df = parse_csv(csv_data)\n    results[\"parse\"] = time.perf_counter() - start\n\n    # Phase 4: Ingest\n    start = time.perf_counter()\n    ingest_to_db(df)\n    results[\"ingest\"] = time.perf_counter() - start\n\n    # Analysis\n    total = sum(results.values())\n    for phase, duration in results.items():\n        pct = (duration / total) * 100\n        print(f\"{phase}: {duration:.3f}s ({pct:.1f}%)\")\n\n    return results\n```\n\n### 4. Impact Quantification Framework\n\n**Priority Levels:**\n- **P0 (Critical)**: >5x improvement, addresses primary bottleneck\n- **P1 (High)**: 2-5x improvement, secondary optimizations\n- **P2 (Medium)**: 1.2-2x improvement, quick wins\n- **P3 (Low)**: <1.2x improvement, minor tuning\n\n**Impact Reporting Format:**\n```markdown\n### Recommendation: [Optimization Name] (P0/P1/P2) - [IMPACT LEVEL]\n\n**Impact**: // **Nx improvement**\n**Effort**: High/Medium/Low (N days)\n**Expected Improvement**: CurrentK  TargetK rows/sec\n\n**Rationale**:\n- [Why this matters]\n- [Supporting evidence from profiling]\n- [Comparison to alternatives]\n\n**Implementation**:\n[Code snippet or architecture description]\n```\n\n### 5. Consensus-Building Pattern\n\n**Integration Agent Responsibilities:**\n1. Read all investigation reports (Agents 1-4)\n2. Identify consensus recommendations (all agents agree)\n3. Flag contradictions (agents disagree)\n4. Synthesize master integration report\n5. Create implementation roadmap (P0  P1  P2)\n\n**Consensus Criteria:**\n- 3/4 agents recommend same optimization  Consensus\n- 2/4 agents recommend, 2/4 neutral  Investigate further\n- Agents contradict (one says \"optimize X\", another says \"X is not bottleneck\")  Run tie-breaker experiment\n\n## Workflow: Step-by-Step\n\n### Step 1: Define Performance Problem\n\n**Input**: Performance metric below SLO\n**Output**: Problem statement with baseline metrics\n\n**Example Problem Statement:**\n```\nPerformance Issue: BTCUSDT 1m ingestion at 47K rows/sec\nTarget SLO: >100K rows/sec\nGap: 53% below target\nPipeline: CloudFront download  ZIP extract  CSV parse  QuestDB ILP ingest\n```\n\n### Step 2: Create Investigation Plan\n\n**Directory Structure:**\n```\ntmp/perf-optimization/\n  profiling/              # Agent 1\n    profile_pipeline.py\n    PROFILING_REPORT.md\n  questdb-config/         # Agent 2\n    CONFIG_ANALYSIS.md\n  python-client/          # Agent 3\n    CLIENT_ANALYSIS.md\n  batch-size/             # Agent 4\n    BATCH_ANALYSIS.md\n  MASTER_INTEGRATION_REPORT.md  # Agent 5\n```\n\n**Agent Assignment:**\n- Agent 1: Empirical profiling (instrumentation)\n- Agent 2: Database configuration analysis\n- Agent 3: Client library usage analysis\n- Agent 4: Batch size optimization analysis\n- Agent 5: Synthesis and integration\n\n### Step 3: Spawn Agents in Parallel\n\n**IMPORTANT**: Use single message with multiple Task tool calls for true parallelism\n\n**Example:**\n```\nI'm going to spawn 5 parallel investigation agents:\n\n[Uses Task tool 5 times in a single message]\n- Agent 1: Profiling\n- Agent 2: QuestDB Config\n- Agent 3: Python Client\n- Agent 4: Batch Size\n- Agent 5: Integration (depends on others completing)\n```\n\n**Execution:**\n```bash\n# All agents run simultaneously (user observes 5 parallel tool calls)\n# Each agent writes to its own tmp/ subdirectory\n# Integration agent polls for completed reports\n```\n\n### Step 4: Wait for All Agents to Complete\n\n**Progress Tracking:**\n- Update todo list as each agent completes\n- Integration agent polls tmp/ directory for report files\n- Once 4/4 investigation reports exist  Integration agent synthesizes\n\n**Completion Criteria:**\n- All 4 investigation reports written\n- Integration report synthesizes findings\n- Master recommendations list created\n\n### Step 5: Review Master Integration Report\n\n**Report Structure:**\n```markdown\n# Master Performance Optimization Integration Report\n\n## Executive Summary\n- Critical discovery (what is/isn't the bottleneck)\n- Key findings from each agent (1-sentence summary)\n\n## Top 3 Recommendations (Consensus)\n1. [P0 Optimization] - HIGHEST IMPACT\n2. [P1 Optimization] - HIGH IMPACT\n3. [P2 Optimization] - QUICK WIN\n\n## Agent Investigation Summary\n### Agent 1: Profiling\n### Agent 2: Database Config\n### Agent 3: Client Library\n### Agent 4: Batch Size\n\n## Implementation Roadmap\n### Phase 1: P0 Optimizations (Week 1)\n### Phase 2: P1 Optimizations (Week 2)\n### Phase 3: P2 Quick Wins (As time permits)\n```\n\n### Step 6: Implement Optimizations (P0 First)\n\n**For each recommendation:**\n1. Implement highest-priority optimization (P0)\n2. Re-run profiling script\n3. Verify expected improvement achieved\n4. Update report with actual results\n5. Move to next priority (P1, P2, P3)\n\n**Example Implementation:**\n```bash\n# Before optimization\nuv run python tmp/perf-optimization/profiling/profile_pipeline.py\n# Output: 47K rows/sec, download=857ms (90%)\n\n# Implement P0 recommendation (concurrent downloads)\n# [Make code changes]\n\n# After optimization\nuv run python tmp/perf-optimization/profiling/profile_pipeline.py\n# Output: 450K rows/sec, download=90ms per symbol * 10 concurrent (90%)\n```\n\n## Real-World Example: QuestDB Refactor Performance Investigation\n\n**Context**: Pipeline achieving 47K rows/sec, target 100K rows/sec (53% below SLO)\n\n**Assumptions Before Investigation:**\n- QuestDB ILP ingestion is the bottleneck (4% of time)\n- Need to tune database configuration\n- Need to optimize Sender API usage\n\n**Findings After 5-Agent Investigation:**\n1. **Profiling Agent**: CloudFront download is 90% of time (857ms), ILP ingest only 4% (40ms)\n2. **QuestDB Config Agent**: Database already optimal, tuning provides <5% improvement\n3. **Python Client Agent**: Sender API already optimal (using dataframe() bulk ingestion)\n4. **Batch Size Agent**: 44K batch size is within optimal range\n5. **Integration Agent**: Consensus recommendation - optimize download, NOT database\n\n**Top 3 Recommendations:**\n1.  **P0**: Concurrent multi-symbol downloads (10-20x improvement)\n2.  **P1**: Multi-month pipeline parallelism (2x improvement)\n3.  **P2**: Streaming ZIP extraction (1.3x improvement)\n\n**Impact**: Discovered database ingests at 1.1M rows/sec (11x faster than target) - proving database was never the bottleneck\n\n**Outcome**: Avoided wasting 2-3 weeks optimizing database when download was the real bottleneck\n\n## Common Pitfalls\n\n### 1. Profiling Only One Layer\n **Bad**: Profile database only, assume it's the bottleneck\n **Good**: Profile entire pipeline (download  extract  parse  ingest)\n\n### 2. Serial Agent Execution\n **Bad**: Run Agent 1, wait, then run Agent 2, wait, etc.\n **Good**: Spawn all 5 agents in parallel using single message with multiple Task calls\n\n### 3. Optimizing Without Profiling\n **Bad**: \"Let's optimize the database config first\" (assumption-driven)\n **Good**: Profile first, discover database is only 4% of time, optimize download instead\n\n### 4. Ignoring Low-Hanging Fruit\n **Bad**: Only implement P0 (highest impact, highest effort)\n **Good**: Implement P2 quick wins (1.3x for 4-8 hours effort) while planning P0\n\n### 5. Not Re-Profiling After Changes\n **Bad**: Implement optimization, assume it worked\n **Good**: Re-run profiling script, verify expected improvement achieved\n\n## Resources\n\n### scripts/\nNot applicable - profiling scripts are project-specific (stored in `tmp/perf-optimization/`)\n\n### references/\n- `profiling_template.py` - Template for phase-boundary instrumentation\n- `integration_report_template.md` - Template for master integration report\n- `impact_quantification_guide.md` - How to assess P0/P1/P2 priorities\n\n### assets/\nNot applicable - profiling artifacts are project-specific"
              },
              {
                "name": "schema-e2e-validation",
                "description": "Run Earthly E2E validation for YAML schema contracts. Use when validating YAML schema changes, testing schema contracts against live ClickHouse, or regenerating Python types, DDL, and docs from YAML. For SQL schema design and optimization, use clickhouse-architect skill instead.\n",
                "path": "plugins/quality-tools/skills/schema-e2e-validation/SKILL.md",
                "frontmatter": {
                  "name": "schema-e2e-validation",
                  "description": "Run Earthly E2E validation for YAML schema contracts. Use when validating YAML schema changes, testing schema contracts against live ClickHouse, or regenerating Python types, DDL, and docs from YAML. For SQL schema design and optimization, use clickhouse-architect skill instead.\n",
                  "allowed-tools": "Read, Bash, Grep"
                },
                "content": "# Schema E2E Validation\n\n## When to Use\n\n- Validating schema changes before commit\n- Verifying YAML schema matches live ClickHouse Cloud\n- Regenerating Python types, DDL, or docs\n- Running full schema workflow validation\n\n## Prerequisites\n\n### Docker Runtime (Required)\n\nEarthly requires Docker. Start Colima before running:\n\n```bash\ncolima start\n```\n\n**Check if running:**\n\n```bash\ndocker ps  # Should not error\n```\n\n### Doppler Access (For validation targets)\n\nRequired for `+test-schema-validate` and `+test-schema-e2e`:\n\n```bash\ndoppler configure set token <token_from_1password>\ndoppler setup --project gapless-network-data --config prd\n```\n\n### Earthly Installation\n\n```bash\nbrew install earthly\n```\n\n---\n\n## Quick Commands\n\n### Generation only (no secrets)\n\n```bash\ncd /Users/terryli/eon/gapless-network-data\ncolima start  # If not already running\nearthly +test-schema-generate\n```\n\n### Full E2E with validation (requires Doppler)\n\n```bash\ncd /Users/terryli/eon/gapless-network-data\ncolima start  # If not already running\n./scripts/earthly-with-doppler.sh +test-schema-e2e\n```\n\n### All non-secret targets\n\n```bash\ncd /Users/terryli/eon/gapless-network-data\nearthly +all\n```\n\n---\n\n## Artifacts\n\nAfter running `+test-schema-generate` or `+test-schema-e2e`, check `./earthly-artifacts/`:\n\n| Path                       | Contents                    |\n| -------------------------- | --------------------------- |\n| `types/blocks.py`          | Pydantic + TypedDict models |\n| `types/__init__.py`        | Package init                |\n| `ddl/ethereum_mainnet.sql` | ClickHouse DDL              |\n| `docs/ethereum_mainnet.md` | Markdown documentation      |\n\nFor E2E, artifacts are under `e2e/types/`, `e2e/ddl/`, `e2e/docs/`.\n\n---\n\n## Earthfile Targets Reference\n\n| Target                  | Secrets | Purpose                    |\n| ----------------------- | ------- | -------------------------- |\n| `+deps`                 | No      | Install uv + dependencies  |\n| `+build`                | No      | Copy source files          |\n| `+test-unit`            | No      | Run pytest                 |\n| `+test-schema-generate` | No      | Generate types/DDL/docs    |\n| `+test-schema-validate` | Yes     | Validate vs ClickHouse     |\n| `+test-schema-e2e`      | Yes     | Full workflow + artifacts  |\n| `+all`                  | No      | Run all non-secret targets |\n\n---\n\n## Troubleshooting\n\n### \"could not determine buildkit address - is Docker or Podman running?\"\n\n**Cause**: Docker/Colima not running\n\n**Fix**:\n\n```bash\ncolima start\n# Wait for \"done\" message, then retry\nearthly +test-schema-generate\n```\n\n### \"unable to parse --secret-file argument\"\n\n**Cause**: Wrong flag name or malformed secrets file\n\n**Fix**: The correct flag is `--secret-file-path` (NOT `--secret-file`). The wrapper script handles this, but if running manually:\n\n```bash\n# WRONG\nearthly --secret-file=/path/to/secrets +target\n\n# CORRECT\nearthly --secret-file-path=/path/to/secrets +target\n```\n\nAlso ensure secrets file has no quotes around values:\n\n```bash\n# WRONG format\nCLICKHOUSE_HOST=\"host.cloud\"\n\n# CORRECT format\nCLICKHOUSE_HOST=host.cloud\n```\n\n### \"OSError: Readme file does not exist: README.md\"\n\n**Cause**: hatchling build backend requires README.md in container\n\n**Fix**: Ensure Earthfile copies README.md in deps target:\n\n```earthfile\ndeps:\n    COPY pyproject.toml uv.lock README.md ./  # README.md required!\n```\n\n### \"missing secret\" during validation\n\n**Cause**: Doppler not configured or secrets not passed\n\n**Fix**:\n\n```bash\n# Verify Doppler has the secrets\ndoppler secrets --project gapless-network-data --config prd | grep CLICKHOUSE\n\n# Use the wrapper script (handles secret injection)\n./scripts/earthly-with-doppler.sh +test-schema-validate\n```\n\n### Cache Issues\n\nForce rebuild without cache:\n\n```bash\nearthly --no-cache +test-schema-e2e\n```\n\n---\n\n## Implementation Details\n\n### Doppler Secret Injection\n\nThe wrapper script `scripts/earthly-with-doppler.sh`:\n\n1. Downloads secrets from Doppler\n2. Filters for `CLICKHOUSE_*` variables\n3. Strips quotes (Doppler outputs `KEY=\"value\"`, Earthly needs `KEY=value`)\n4. Passes via `--secret-file-path` flag\n5. Cleans up temp file on exit\n\n### Secrets Required\n\n| Secret                         | Purpose               |\n| ------------------------------ | --------------------- |\n| `CLICKHOUSE_HOST_READONLY`     | ClickHouse Cloud host |\n| `CLICKHOUSE_USER_READONLY`     | Read-only user        |\n| `CLICKHOUSE_PASSWORD_READONLY` | Read-only password    |\n\n---\n\n## Related Files\n\n| File                                                                                | Purpose                  |\n| ----------------------------------------------------------------------------------- | ------------------------ |\n| `/Users/terryli/eon/gapless-network-data/Earthfile`                                 | Main build file          |\n| `/Users/terryli/eon/gapless-network-data/scripts/earthly-with-doppler.sh`           | Secret injection wrapper |\n| `/Users/terryli/eon/gapless-network-data/schema/clickhouse/ethereum_mainnet.yaml`   | SSoT schema              |\n| `/Users/terryli/eon/gapless-network-data/docs/adr/2025-12-03-earthly-schema-e2e.md` | ADR                      |\n\n---\n\n## Validation History\n\n- **2025-12-03**: Created and validated with full E2E run against ClickHouse Cloud\n- **Lessons Learned**:\n  - `--secret-file-path` not `--secret-file` (Earthly v0.8.16)\n  - Doppler `--format env` outputs quotes, must strip with `sed 's/\"//g'`\n  - README.md must be copied for hatchling build backend\n  - Colima must be started before Earthly runs\n\n---\n\n## Design Authority\n\n<!-- ADR: 2025-12-10-clickhouse-skill-delegation -->\n\nThis skill validates schemas but does not design them. For schema design guidance (ORDER BY, compression, partitioning), invoke **`quality-tools:clickhouse-architect`** first.\n\n## Related Skills\n\n| Skill                                      | Purpose                         |\n| ------------------------------------------ | ------------------------------- |\n| `quality-tools:clickhouse-architect`       | Schema design before validation |\n| `devops-tools:clickhouse-cloud-management` | Cloud credentials for E2E tests |\n| `devops-tools:clickhouse-pydantic-config`  | Client configuration            |"
              }
            ]
          },
          {
            "name": "productivity-tools",
            "description": "Slash command generation for Claude Code",
            "source": "./plugins/productivity-tools/",
            "category": "productivity",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install productivity-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "slash-command-factory",
                "description": "Generate custom Claude Code slash commands through intelligent 5-7 question flow. Creates powerful commands for business research, content analysis, healthcare compliance, API integration, documentation automation, and workflow optimization. Outputs organized commands to generated-commands/ with validation and installation guidance.",
                "path": "plugins/productivity-tools/skills/slash-command-factory/SKILL.md",
                "frontmatter": {
                  "name": "slash-command-factory",
                  "description": "Generate custom Claude Code slash commands through intelligent 5-7 question flow. Creates powerful commands for business research, content analysis, healthcare compliance, API integration, documentation automation, and workflow optimization. Outputs organized commands to generated-commands/ with validation and installation guidance."
                },
                "content": "# Slash Command Factory\n\nA comprehensive system for generating production-ready Claude Code slash commands through a simple question-based workflow.\n\n---\n\n## Overview\n\nThis skill helps you create custom slash commands for Claude Code by:\n\n- Asking 5-7 straightforward questions about your command needs\n- Generating complete command .md files with proper YAML frontmatter\n- Providing 10 powerful preset commands for common use cases\n- Validating command format and syntax\n- Creating well-organized folder structures\n- Offering installation guidance\n\n**Output**: Complete slash commands ready to use in Claude Code\n\n---\n\n## Official Command Structure Patterns\n\nThis skill generates commands following **three official patterns** from Anthropic documentation:\n\n### Pattern A: Simple (Context  Task)\n\n**Best for**: Straightforward tasks with clear input/output\n**Example**: Code review, file updates, simple analysis\n**Official Reference**: code-review.md\n\n**Structure**:\n\n```markdown\n---\nallowed-tools: Bash(git diff:*), Bash(git log:*)\ndescription: Purpose description\n---\n\n## Context\n\n- Current state: !`bash command`\n- Additional data: !`another command`\n\n## Your task\n\n[Clear instructions with numbered steps]\n[Success criteria]\n```\n\n**When to use**:\n\n- Simple, focused tasks\n- Quick analysis or reviews\n- Straightforward workflows\n- 1-3 bash commands for context\n\n---\n\n### Pattern B: Multi-Phase (Discovery  Analysis  Task)\n\n**Best for**: Complex discovery and documentation tasks\n**Example**: Codebase analysis, comprehensive audits, system mapping\n**Official Reference**: codebase-analysis.md\n\n**Structure**:\n\n```markdown\n---\nallowed-tools: Bash(find:*), Bash(tree:*), Bash(ls:*), Bash(grep:*), Bash(wc:*), Bash(du:*)\ndescription: Comprehensive purpose\n---\n\n# Command Title\n\n## Phase 1: Project Discovery\n\n### Directory Structure\n\n!`find . -type d | sort`\n\n### File Count Analysis\n\n!`find . -type f | wc -l`\n\n## Phase 2: Detailed Analysis\n\n[More discovery commands]\n[File references with @]\n\n## Phase 3: Your Task\n\nBased on all discovered information, create:\n\n1. **Deliverable 1**\n   - Subsection\n   - Details\n\n2. **Deliverable 2**\n   - Subsection\n   - Details\n\nAt the end, write output to [filename].md\n```\n\n**When to use**:\n\n- Comprehensive analysis needed\n- Multiple discovery phases\n- Large amounts of context gathering\n- 10+ bash commands for data collection\n- Generate detailed documentation files\n\n---\n\n### Pattern C: Agent-Style (Role  Process  Guidelines)\n\n**Best for**: Specialized expert roles and coordination\n**Example**: Domain experts, orchestrators, specialized advisors\n**Official Reference**: openapi-expert.md\n\n**Structure**:\n\n```markdown\n---\nname: command-name\ndescription: |\n  Multi-line description for complex purpose\n  explaining specialized role\ncolor: yellow\n---\n\nYou are a [specialized role] focusing on [domain expertise].\n\n**Core Responsibilities:**\n\n1. **Responsibility Area 1**\n   - Specific tasks\n   - Expected outputs\n\n2. **Responsibility Area 2**\n   - Specific tasks\n   - Expected outputs\n\n**Working Process:**\n\n1. [Step 1 in workflow]\n2. [Step 2 in workflow]\n3. [Step 3 in workflow]\n\n**Important Considerations:**\n\n- [Guideline 1]\n- [Guideline 2]\n- [Constraint or best practice]\n\nWhen you encounter [scenario], [action to take].\n```\n\n**When to use**:\n\n- Need specialized domain expertise\n- Orchestrating complex workflows\n- Coordinating multiple sub-processes\n- Acting as expert advisor\n- Require specific procedural guidelines\n\n---\n\n## Comprehensive Naming Convention\n\n### Command File Naming Rules\n\nAll slash command files MUST follow kebab-case convention:\n\n**Format**: `[verb]-[noun].md`, `[noun]-[verb].md`, or `[domain]-[action].md`\n\n**Rules**:\n\n1. **Case**: Lowercase only with hyphens as separators\n2. **Length**: 2-4 words maximum\n3. **Characters**: Only `[a-z0-9-]` allowed (letters, numbers, hyphens)\n4. **Start/End**: Must begin and end with letter or number (not hyphen)\n5. **No**: Spaces, underscores, camelCase, TitleCase, or special characters\n\n---\n\n### Conversion Algorithm\n\n**User Input**  **Command Name**\n\n```\nInput: \"Analyze customer feedback and generate insights\"\n\n1. Extract action: \"analyze\"\n2. Extract target: \"feedback\"\n3. Combine: \"analyze-feedback\"\n4. Validate: Matches [a-z0-9-]+ pattern \n5. Output: analyze-feedback.md\n```\n\n**More Examples**:\n\n- \"Review pull requests\"  `pr-review.md` or `review-pr.md`\n- \"Generate API documentation\"  `api-document.md` or `document-api.md`\n- \"Update README files\"  `update-readme.md` or `readme-update.md`\n- \"Audit security compliance\"  `security-audit.md` or `compliance-audit.md`\n- \"Research market trends\"  `research-market.md` or `market-research.md`\n- \"Analyze code quality\"  `code-analyze.md` or `analyze-code.md`\n\n---\n\n### Official Examples (From Anthropic Docs)\n\n**Correct**:\n\n-  `code-review.md` (verb-noun)\n-  `codebase-analysis.md` (noun-noun compound)\n-  `update-claude-md.md` (verb-noun-qualifier)\n-  `openapi-expert.md` (domain-role)\n\n**Incorrect**:\n\n-  `code_review.md` (snake_case - wrong)\n-  `CodeReview.md` (PascalCase - wrong)\n-  `codeReview.md` (camelCase - wrong)\n-  `review.md` (too vague - needs target)\n-  `analyze-customer-feedback-data.md` (too long - >4 words)\n\n---\n\n## Bash Permission Patterns\n\n### Critical Rule: Subcommand-Level Specificity\n\n** NEVER ALLOWED**:\n\n```yaml\nallowed-tools: Bash\n```\n\nBlanket Bash permission is **prohibited** per official Anthropic patterns.\n\n** TOO BROAD** (for commands with subcommands):\n\n```yaml\nallowed-tools: Bash(git:*), Bash(gh:*), Bash(npm:*)\n```\n\nCommand-level wildcards allow dangerous operations (`git reset --hard`, `gh repo delete`).\n\n** REQUIRED** (subcommand-level specificity):\n\n```yaml\nallowed-tools: Bash(git add:*), Bash(git commit:*), Bash(git push:*), Bash(gh repo view:*)\n```\n\nMust specify **exact subcommands** for commands with subcommand hierarchies.\n\n** OK** (commands without subcommands):\n\n```yaml\nallowed-tools: Bash(cp:*), Bash(mkdir -p:*), Bash(date:*), Bash(open:*)\n```\n\nSimple commands without subcommand hierarchies can use command-level.\n\n---\n\n### Official Permission Patterns\n\nBased on Anthropic's documented examples:\n\n**Git Operations** (code-review, update-docs):\n\n```yaml\nallowed-tools: Bash(git status:*), Bash(git diff:*), Bash(git log:*), Bash(git branch:*), Bash(git add:*), Bash(git commit:*)\n```\n\n**File Discovery** (codebase-analysis):\n\n```yaml\nallowed-tools: Bash(find:*), Bash(tree:*), Bash(ls:*), Bash(du:*)\n```\n\n**Content Analysis** (comprehensive discovery):\n\n```yaml\nallowed-tools: Bash(grep:*), Bash(wc:*), Bash(head:*), Bash(tail:*), Bash(cat:*)\n```\n\n**Data Processing** (custom analysis):\n\n```yaml\nallowed-tools: Bash(awk:*), Bash(sed:*), Bash(sort:*), Bash(uniq:*)\n```\n\n**Combined Patterns** (multi-phase commands):\n\n```yaml\nallowed-tools: Bash(find:*), Bash(tree:*), Bash(ls:*), Bash(grep:*), Bash(wc:*), Bash(du:*), Bash(head:*), Bash(tail:*), Bash(cat:*), Bash(touch:*)\n```\n\n---\n\n### Permission Selection Guide\n\n| Command Type        | Bash Permissions                            | Example Commands                |\n| ------------------- | ------------------------------------------- | ------------------------------- |\n| **Git Commands**    | `git status, git diff, git log, git branch` | code-review, commit-assist      |\n| **Discovery**       | `find, tree, ls, du`                        | codebase-analyze, structure-map |\n| **Analysis**        | `grep, wc, head, tail, cat`                 | search-code, count-lines        |\n| **Update**          | `git diff, find, grep`                      | update-docs, sync-config        |\n| **Data Processing** | `awk, sed, sort, uniq`                      | parse-data, format-output       |\n| **Comprehensive**   | All of the above                            | full-audit, system-analyze      |\n\n---\n\n## Two Paths to Generate Commands\n\n### Path 1: Quick-Start Presets (30 seconds)\n\nChoose from 10 powerful preset commands:\n\n**Business & Research**:\n\n1. **/research-business** - Comprehensive market research and competitive analysis\n2. **/research-content** - Multi-platform content trend analysis and SEO strategy\n\n**Healthcare & Compliance**: 3. **/medical-translate** - Translate medical terminology to 8th-10th grade (German/English) 4. **/compliance-audit** - HIPAA/GDPR/DSGVO compliance validation\n\n**Development & Integration**: 5. **/api-build** - Generate complete API integration code with tests 6. **/test-auto** - Auto-generate comprehensive test suites\n\n**Documentation & Knowledge**: 7. **/docs-generate** - Automated documentation creation 8. **/knowledge-mine** - Extract and structure insights from documents\n\n**Workflow & Productivity**: 9. **/workflow-analyze** - Analyze and optimize business processes 10. **/batch-agents** - Launch and coordinate multiple agents for complex tasks\n\n### Path 2: Custom Command (5-7 Questions)\n\nCreate a completely custom command for your specific needs.\n\n---\n\n## Question Flow (Custom Path)\n\n### Question 1: Command Purpose\n\n\"What should this slash command do?\n\nBe specific about its purpose and when you'll use it.\n\nExamples:\n\n- 'Analyze customer feedback and generate actionable insights'\n- 'Generate HIPAA-compliant API documentation'\n- 'Research market trends and create content strategy'\n- 'Extract key insights from research papers'\n\nYour command's purpose: \\_\\_\\_\"\n\n---\n\n### Question 2: Arguments (Auto-Determined)\n\nThe skill automatically determines if your command needs arguments based on the purpose.\n\n**If arguments are needed**, they will use `$ARGUMENTS` format:\n\n- User types: `/your-command argument1 argument2`\n- Command receives: `$ARGUMENTS` = \"argument1 argument2\"\n\n**Examples**:\n\n- `/research-business \"Tesla\" \"EV market\"`  $ARGUMENTS = \"Tesla EV market\"\n- `/medical-translate \"Myokardinfarkt\" \"de\"`  $ARGUMENTS = \"Myokardinfarkt de\"\n\n**No user input needed** - skill decides intelligently.\n\n---\n\n### Argument Short-Form Convention (MANDATORY)\n\n**Every flag/option MUST have a short form** for quick command-line entry.\n\n**Rules**:\n\n1. **Prefer 1-letter**: `-b` for `--branch`, `-v` for `--verbose`\n2. **Use 2-letters only if needed**: `-nb` for `--no-branch` (when `-n` conflicts)\n3. **Document both forms**: Always show `[-short|--long]` in argument-hint\n\n**Format in argument-hint**:\n\n```yaml\nargument-hint: \"[slug] [-b|--branch] [-v|--verbose]\"\n```\n\n**Format in Arguments table**:\n\n```markdown\n| Argument    | Short | Description           | Default |\n| ----------- | ----- | --------------------- | ------- |\n| `--branch`  | `-b`  | Create feature branch | false   |\n| `--verbose` | `-v`  | Enable verbose output | false   |\n```\n\n**Letter Selection Priority**:\n\n1. First letter of the flag name (`--branch`  `-b`)\n2. Distinctive letter if first conflicts (`--debug`  `-d`, but if `-d` taken, use `-D` or `-db`)\n3. Mnemonic association (`--quiet`  `-q`, `--force`  `-f`)\n\n**This is a MANDATORY success criterion** - commands without short forms will fail validation.\n\n---\n\n### Question 3: Which Tools?\n\n\"Which Claude Code tools should this command use?\n\nAvailable tools:\n\n- **Read** - Read files\n- **Write** - Create files\n- **Edit** - Modify files\n- **Bash** - Execute shell commands (MUST specify exact commands)\n- **Grep** - Search code\n- **Glob** - Find files by pattern\n- **Task** - Launch agents\n\n**CRITICAL**: For Bash, you MUST specify exact commands, not wildcards.\n\n**Bash Examples**:\n\n-  Bash(git status:_), Bash(git diff:_), Bash(git log:\\*)\n-  Bash(find:_), Bash(tree:_), Bash(ls:\\*)\n-  Bash(grep:_), Bash(wc:_), Bash(head:\\*)\n-  Bash (wildcard not allowed per official patterns)\n\n**Tool Combination Examples**:\n\n- Git command: Read, Bash(git status:_), Bash(git diff:_)\n- Code generator: Read, Write, Edit\n- Discovery command: Bash(find:_), Bash(tree:_), Bash(grep:\\*)\n- Analysis command: Read, Grep, Task (launch agents)\n\nYour tools (comma-separated): \\_\\_\\_\"\n\n---\n\n### Question 4: Agent Integration\n\n\"Does this command need to launch agents for specialized tasks?\n\nExamples of when to use agents:\n\n- Complex analysis (launch rr-architect, rr-security)\n- Implementation tasks (launch rr-frontend, rr-backend)\n- Quality checks (launch rr-qa, rr-test-runner)\n\nOptions:\n\n1. **No agents** - Command handles everything itself\n2. **Launch agents** - Delegate to specialized agents\n\nYour choice (1 or 2): \\_\\_\\_\"\n\nIf \"2\", ask: \"Which agents should it launch? \\_\\_\\_\"\n\n---\n\n### Question 5: Output Type\n\n\"What type of output should this command produce?\n\n1. **Analysis** - Research report, insights, recommendations\n2. **Files** - Generated code, documentation, configs\n3. **Action** - Execute tasks, run workflows, deploy\n4. **Report** - Structured report with findings and next steps\n\nYour choice (1, 2, 3, or 4): \\_\\_\\_\"\n\n---\n\n### Question 6: Model Preference (Optional)\n\n\"Which Claude model should this command use?\n\n1. **Default** - Inherit from main conversation (recommended)\n2. **Sonnet** - Best for complex tasks\n3. **Haiku** - Fastest, cheapest (for simple commands)\n4. **Opus** - Maximum capability (for critical tasks)\n\nYour choice (1, 2, 3, or 4) or press Enter for default: \\_\\_\\_\"\n\n---\n\n### Question 7: Additional Features (Optional)\n\n\"Any special features?\n\nOptional features:\n\n- **Bash execution** - Run shell commands and include output (!`command`)\n- **File references** - Include file contents (@file.txt)\n- **Context gathering** - Read project files for context\n\nFeatures you need (comma-separated) or press Enter to skip: \\_\\_\\_\"\n\n---\n\n## Generation Process\n\nAfter collecting answers:\n\n1. **Generate YAML Frontmatter**:\n\n```yaml\n---\ndescription: [From command purpose]\nargument-hint: [If $ARGUMENTS needed]\nallowed-tools: [From tool selection]\nmodel: [If specified]\n---\n```\n\n1. **Generate Command Body**:\n\n```markdown\n[Purpose-specific instructions]\n\n[If uses agents]:\n\n1. **Launch [agent-name]** with [specific task]\n2. Coordinate workflow\n3. Validate results\n\n[If uses bash]:\n\n- Context: !`bash command`\n\n[If uses file refs]:\n\n- Review: @file.txt\n\nSuccess Criteria: [Based on output type]\n```\n\n1. **Create Folder Structure**:\n\n```\ngenerated-commands/[command-name]/\n [command-name].md    # Command file (ROOT)\n README.md            # Installation guide (ROOT)\n TEST_EXAMPLES.md     # Testing examples (ROOT)\n [folders if needed]  # standards/, examples/, scripts/\n```\n\n1. **Validate Format**:\n\n-  YAML frontmatter valid\n-  $ARGUMENTS syntax correct (if used)\n-  allowed-tools format proper\n-  Folder organization clean\n\n1. **Provide Installation Instructions**:\n\n```\nYour command is ready!\n\nOutput location: generated-commands/[command-name]/\n\nTo install:\n1. Copy the command file:\n   cp generated-commands/[command-name]/[command-name].md .claude/commands/\n\n2. Restart Claude Code (if already running)\n\n3. Test:\n   /[command-name] [arguments]\n```\n\n### Plugin Command Invocation Format\n\nWhen commands are installed in a **plugin** (via `commands/` directory), users invoke them with the full namespace:\n\n```\n/plugin-name:command-name [arguments]\n```\n\n| Installation Location              | Invocation Format           | Example                        |\n| ---------------------------------- | --------------------------- | ------------------------------ |\n| `~/.claude/commands/` (user-level) | `/command-name`             | `/research-business`           |\n| `plugin/commands/` (plugin)        | `/plugin-name:command-name` | `/my-plugin:research-business` |\n\n**Shortcut rules**:\n\n- Commands like `/my-plugin:research-business` can be invoked as `/research-business` if no naming conflicts exist\n- **Exception**: When `command-name` = `plugin-name` (e.g., `/foo:foo`), you **must** use the full formattyping `/foo` alone is interpreted as the plugin prefix, not the command\n\n---\n\n## Preset Command Details\n\n### 1. /research-business\n\n**Purpose**: Comprehensive business and market research\n\n**Arguments**: `$ARGUMENTS` (company or market to research)\n\n**YAML**:\n\n```yaml\n---\ndescription: Comprehensive business and market research with competitor analysis\nargument-hint: [company/market] [industry]\nallowed-tools: Read, Bash, Grep\n---\n```\n\n**What it does**:\n\n- Market size and trends analysis\n- Competitor SWOT analysis\n- Opportunity identification\n- Industry landscape overview\n- Strategic recommendations\n\n---\n\n### 2. /research-content\n\n**Purpose**: Multi-platform content trend analysis\n\n**Arguments**: `$ARGUMENTS` (topic to research)\n\n**YAML**:\n\n```yaml\n---\ndescription: Multi-platform content trend analysis for data-driven content strategy\nargument-hint: [topic] [platforms]\nallowed-tools: Read, Bash\n---\n```\n\n**What it does**:\n\n- Analyze trends across Google, Reddit, YouTube, Medium, LinkedIn, X\n- User intent analysis (informational, commercial, transactional)\n- Content gap identification\n- SEO-optimized outline generation\n- Platform-specific publishing strategies\n\n---\n\n### 3. /medical-translate\n\n**Purpose**: Translate medical terminology to patient-friendly language\n\n**Arguments**: `$ARGUMENTS` (medical term and language)\n\n**YAML**:\n\n```yaml\n---\ndescription: Translate medical terminology to 8th-10th grade reading level (German/English)\nargument-hint: [medical-term] [de|en]\nallowed-tools: Read\n---\n```\n\n**What it does**:\n\n- Translate complex medical terms\n- Simplify to 8th-10th grade reading level\n- Validate with Flesch-Kincaid (EN) or Wiener Sachtextformel (DE)\n- Preserve clinical accuracy\n- Provide patient-friendly explanations\n\n---\n\n### 4. /compliance-audit\n\n**Purpose**: Check code for regulatory compliance\n\n**Arguments**: `$ARGUMENTS` (path and compliance standard)\n\n**YAML**:\n\n```yaml\n---\ndescription: Audit code for HIPAA/GDPR/DSGVO compliance requirements\nargument-hint: [code-path] [hipaa|gdpr|dsgvo|all]\nallowed-tools: Read, Grep, Task\n---\n```\n\n**What it does**:\n\n- Scan for PHI/PII handling\n- Check encryption requirements\n- Verify audit logging\n- Validate data subject rights\n- Generate compliance report\n\n---\n\n### 5. /api-build\n\n**Purpose**: Generate complete API integration code\n\n**Arguments**: `$ARGUMENTS` (API name and endpoints)\n\n**YAML**:\n\n```yaml\n---\ndescription: Generate complete API client with error handling and tests\nargument-hint: [api-name] [endpoints]\nallowed-tools: Read, Write, Edit, Bash, Task\n---\n```\n\n**What it does**:\n\n- Generate API client classes\n- Add error handling and retries\n- Create authentication logic\n- Generate unit and integration tests\n- Add usage documentation\n\n---\n\n### 6. /test-auto\n\n**Purpose**: Auto-generate comprehensive test suites\n\n**Arguments**: `$ARGUMENTS` (file path and test type)\n\n**YAML**:\n\n```yaml\n---\ndescription: Auto-generate comprehensive test suite with coverage analysis\nargument-hint: [file-path] [unit|integration|e2e]\nallowed-tools: Read, Write, Bash\n---\n```\n\n**What it does**:\n\n- Analyze code to test\n- Generate test cases (happy path, edge cases, errors)\n- Add test fixtures and mocks\n- Calculate coverage\n- Provide testing documentation\n\n---\n\n### 7. /docs-generate\n\n**Purpose**: Automated documentation generation\n\n**Arguments**: `$ARGUMENTS` (code path and doc type)\n\n**YAML**:\n\n```yaml\n---\ndescription: Auto-generate documentation from code (API docs, README, architecture)\nargument-hint: [code-path] [api|readme|architecture|all]\nallowed-tools: Read, Write, Grep\n---\n```\n\n**What it does**:\n\n- Extract code structure and functions\n- Generate API documentation\n- Create README with usage examples\n- Build architecture diagrams (Mermaid)\n- Add code examples\n\n---\n\n### 8. /knowledge-mine\n\n**Purpose**: Extract structured insights from documents\n\n**Arguments**: `$ARGUMENTS` (document path and output format)\n\n**YAML**:\n\n```yaml\n---\ndescription: Extract and structure knowledge from documents into actionable insights\nargument-hint: [doc-path] [faq|summary|kb|all]\nallowed-tools: Read, Grep\n---\n```\n\n**What it does**:\n\n- Read and analyze documents\n- Extract key insights\n- Generate FAQs\n- Create knowledge base articles\n- Summarize findings\n\n---\n\n### 9. /workflow-analyze\n\n**Purpose**: Analyze and optimize business workflows\n\n**Arguments**: `$ARGUMENTS` (workflow description)\n\n**YAML**:\n\n```yaml\n---\ndescription: Analyze workflows and provide optimization recommendations\nargument-hint: [workflow-description]\nallowed-tools: Read, Task\n---\n```\n\n**What it does**:\n\n- Map current workflow\n- Identify bottlenecks\n- Suggest automation opportunities\n- Calculate efficiency gains\n- Create implementation roadmap\n\n---\n\n### 10. /batch-agents\n\n**Purpose**: Launch multiple coordinated agents\n\n**Arguments**: `$ARGUMENTS` (agent names and task)\n\n**YAML**:\n\n```yaml\n---\ndescription: Launch and coordinate multiple agents for complex tasks\nargument-hint: [agent-names] [task-description]\nallowed-tools: Task\n---\n```\n\n**What it does**:\n\n- Parse agent list\n- Launch agents in parallel (if safe) or sequential\n- Coordinate outputs\n- Integrate results\n- Provide comprehensive summary\n\n---\n\n## Output Structure\n\nCommands are generated in your project's root directory:\n\n```\n[your-project]/\n generated-commands/\n     [command-name]/\n         [command-name].md      # Command file (ROOT level)\n         README.md              # Installation guide (ROOT level)\n         TEST_EXAMPLES.md       # Testing guide (ROOT level - if applicable)\n        \n         standards/             # Only if command has standards\n         examples/              # Only if command has examples\n         scripts/               # Only if command has helper scripts\n```\n\n**Organization Rules**:\n\n- All .md files in ROOT directory\n- Supporting folders separate (standards/, examples/, scripts/)\n- No mixing of different types in same folder\n- Clean, hierarchical structure\n\n---\n\n## Installation\n\n**After generation**:\n\n1. **Review output**:\n\n   ```bash\n   ls generated-commands/[command-name]/\n   ```\n\n2. **Copy to Claude Code** (when ready):\n\n   ```bash\n   # Project-level (this project only)\n   cp generated-commands/[command-name]/[command-name].md .claude/commands/\n\n   # User-level (all projects)\n   cp generated-commands/[command-name]/[command-name].md ~/.claude/commands/\n   ```\n\n3. **Restart Claude Code** (if running)\n\n4. **Test command**:\n\n   ```bash\n   /[command-name] [arguments]\n   ```\n\n---\n\n## Usage Examples\n\n### Generate a Preset Command\n\n```\n@slash-command-factory\n\nUse the /research-business preset\n```\n\n**Output**: Complete business research command ready to install\n\n---\n\n### Generate a Custom Command\n\n```\n@slash-command-factory\n\nCreate a custom command for analyzing customer feedback and generating product insights\n```\n\n**Skill asks 5-7 questions**  **Generates complete command**  **Validates format**  **Provides installation steps**\n\n---\n\n## Command Format (What Gets Generated)\n\n**Example generated command** (`my-command.md`):\n\n```markdown\n---\ndescription: Brief description of what the command does\nargument-hint: [arg1] [arg2]\nallowed-tools: Read, Write, Bash\nmodel: claude-3-5-sonnet-20241022\n---\n\n# Command Instructions\n\nDo [task] with \"$ARGUMENTS\":\n\n1. **Step 1**: First action\n2. **Step 2**: Second action\n3. **Step 3**: Generate output\n\n**Success Criteria**:\n\n- Criterion 1\n- Criterion 2\n- Criterion 3\n```\n\n---\n\n## Validation\n\nEvery generated command is automatically validated for:\n\n-  Valid YAML frontmatter (proper syntax, required fields)\n-  Correct argument format ($ARGUMENTS, not $1 $2 $3)\n-  **Short forms for all flags** (mandatory 1-2 letter shortcuts)\n-  **argument-hint includes both forms** (`[-b|--branch]`)\n-  **Bash subcommand-level specificity** (no `Bash(git:*)`, use `Bash(git add:*)`)\n-  allowed-tools syntax (comma-separated string)\n-  Clean folder organization (if folders used)\n-  No placeholder text\n\n**If validation fails**, you'll get specific fix instructions.\n\n---\n\n## Best Practices\n\n**For Command Design**:\n\n- Keep commands focused (one clear purpose)\n- Use descriptive names (kebab-case for files)\n- Document expected arguments clearly\n- Include success criteria\n- Add examples in TEST_EXAMPLES.md\n\n**For Tool Selection**:\n\n- Read: For analyzing files\n- Write/Edit: For generating/modifying files\n- Bash: For system commands, web research\n- Task: For launching agents\n- Grep/Glob: For searching code\n\n**For Agent Integration**:\n\n- Use Task tool to launch agents\n- Specify which agents clearly\n- Coordinate outputs\n- Document agent roles\n\n---\n\n## Important Notes\n\n**Arguments**:\n\n-  Always use `$ARGUMENTS` (all arguments as one string)\n-  Never use `$1`, `$2`, `$3` (positional - not used by this factory)\n\n**Folder Organization**:\n\n-  All .md files in command root directory\n-  Supporting folders separate (standards/, examples/, scripts/)\n-  No mixing of different types\n\n**Output Location**:\n\n- Commands generate to: `./generated-commands/[command-name]/`\n- User copies to: `.claude/commands/[command-name].md` (when ready)\n\n---\n\n## Example Invocations\n\n### Use a Preset\n\n```\n@slash-command-factory\n\nGenerate the /research-content preset command\n```\n\n Creates content research command with all features\n\n---\n\n### Create Custom Healthcare Command\n\n```\n@slash-command-factory\n\nCreate a command that generates German PTV 10 therapy applications\n```\n\n**Skill asks**:\n\n- Purpose? (Generate PTV 10 applications)\n- Tools? (Read, Write, Task)\n- Agents? (Yes - health-sdk-builder related agents)\n- Output? (Files - therapy application documents)\n- Model? (Sonnet - for quality)\n\n**Result**: `/generate-ptv10` command ready to use\n\n---\n\n### Create Business Intelligence Command\n\n```\n@slash-command-factory\n\nBuild a command for competitive SWOT analysis\n```\n\n**Skill asks 5-7 questions**  **Generates `/swot-analysis` command**  **Validates**  **Ready to install**\n\n---\n\n## Integration with Factory Agents\n\n**Works with**:\n\n- factory-guide (can delegate to this skill via prompts-guide pattern)\n- Existing slash commands (/build, /validate-output, etc.)\n\n**Complements**:\n\n- skills-guide (builds Skills)\n- prompts-guide (builds Prompts)\n- agents-guide (builds Agents)\n- slash-command-factory (builds Commands)  This skill\n\n**Complete ecosystem** for building all Claude Code augmentations!\n\n---\n\n## Output Validation\n\nGenerated commands are validated for:\n\n**YAML Frontmatter**:\n\n- Has `description` field\n- Proper YAML syntax\n- Valid frontmatter fields only\n\n**Arguments**:\n\n- Uses $ARGUMENTS if needed\n- Has argument-hint if $ARGUMENTS used\n- No $1, $2, $3 positional args\n- **All flags have short forms** (1-2 letters)\n- **argument-hint shows `[-short|--long]` format**\n\n**Tools**:\n\n- Valid tool names\n- Proper comma-separated format\n- Appropriate for command purpose\n- **Bash uses subcommand-level** for git/gh/npm (not `Bash(git:*)`)\n- **No blanket `Bash`** permission\n\n**Organization**:\n\n- .md files in root\n- Folders properly separated\n- No scattered files\n\n---\n\n## Success Criteria\n\nGenerated commands should:\n\n-  Have valid YAML frontmatter\n-  Use $ARGUMENTS (never positional)\n-  **All flags have short forms** (1-2 letters, e.g., `-b|--branch`)\n-  **argument-hint shows both forms** (`[-b|--branch]`)\n-  **Bash uses subcommand-level specificity** for commands with subcommands\n  -  `Bash(git:*)` - too broad\n  -  `Bash(git add:*)`, `Bash(git commit:*)` - correct\n-  **No blanket Bash permission** (`Bash` alone is prohibited)\n-  Work when copied to .claude/commands/\n-  Execute correctly with arguments\n-  Produce expected output\n-  Follow organizational standards\n\n---\n\n**Build powerful custom slash commands in minutes!**"
              }
            ]
          },
          {
            "name": "mql5",
            "description": "MQL5 development: indicator patterns, mql5.com article extraction, Python workspace, log reading",
            "source": "./plugins/mql5/",
            "category": "trading",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install mql5@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "article-extractor",
                "description": "Extracts and organizes MQL5 articles and documentation. Use when researching MQL5 features, MetaTrader API documentation, Python MT5 integration, or algorithmic trading resources.",
                "path": "plugins/mql5/skills/article-extractor/SKILL.md",
                "frontmatter": {
                  "name": "article-extractor",
                  "description": "Extracts and organizes MQL5 articles and documentation. Use when researching MQL5 features, MetaTrader API documentation, Python MT5 integration, or algorithmic trading resources.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# MQL5 Article Extractor\n\nExtract technical trading articles from mql5.com for training data collection. **Scope limited to mql5.com domain only.**\n\n## Scope Boundaries\n\n**VALID requests:**\n\n- \"Extract this mql5.com article: <https://www.mql5.com/en/articles/19625>\"\n- \"Get all articles from MQL5 user 29210372\"\n- \"Download trading articles from mql5.com\"\n- \"Extract 5 MQL5 articles for testing\"\n\n**OUT OF SCOPE:**\n\n- \"Extract from yahoo.com\" - NOT SUPPORTED (mql5.com only)\n- \"Scrape news from reuters\" - NOT SUPPORTED (mql5.com only)\n- \"Get stock data from Bloomberg\" - NOT SUPPORTED (mql5.com only)\n\nIf user requests non-mql5.com extraction, respond: \"This skill extracts articles from mql5.com ONLY. For other sites, use different tools.\"\n\n## Repository Location\n\nWorking directory: `$HOME/eon/mql5` (adjust path for your environment)\n\nAlways execute commands from this directory:\n\n```bash\ncd \"$HOME/eon/mql5\"\n```\n\n## Valid Input Types\n\n### 1. Article URL (Most Specific)\n\n**Format**: `https://www.mql5.com/en/articles/[ID]`\n**Example**: `https://www.mql5.com/en/articles/19625`\n**Action**: Extract single article\n\n### 2. User ID (Numeric or Username)\n\n**Format**: Numeric (e.g., `29210372`) or username (e.g., `jslopes`)\n**Source**: From mql5.com profile URL\n**Action**: Auto-discover and extract all user's articles\n\n### 3. URL List File\n\n**Format**: Text file with one URL per line\n**Action**: Batch process multiple articles\n\n### 4. Vague Request\n\nIf user says \"extract mql5 articles\" without specifics, prompt for:\n\n1. Article URL OR User ID\n1. Quantity limit (for testing)\n1. Output location preference\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Extraction Modes](./references/extraction-modes.md) - Single, batch, auto-discovery, official docs modes\n- [Data Sources](./references/data-sources.md) - User collections and official documentation\n- [Troubleshooting](./references/troubleshooting.md) - Common issues and solutions\n- [Examples](./references/examples.md) - Usage examples and patterns"
              },
              {
                "name": "log-reader",
                "description": "Reads MetaTrader 5 log files to validate indicator execution, unit tests, and compilation errors. Use when user mentions Experts pane, MT5 logs, errors, or asks \"did it work\".",
                "path": "plugins/mql5/skills/log-reader/SKILL.md",
                "frontmatter": {
                  "name": "log-reader",
                  "description": "Reads MetaTrader 5 log files to validate indicator execution, unit tests, and compilation errors. Use when user mentions Experts pane, MT5 logs, errors, or asks \"did it work\".",
                  "allowed-tools": "Read, Bash, Grep"
                },
                "content": "# MT5 Log Reader\n\nRead MetaTrader 5 log files directly to access Print() output from indicators, scripts, and expert advisors without requiring manual Experts pane inspection.\n\n## Purpose\n\nImplement \"Option 3\" dual logging pattern:\n\n- **Print()** - MT5 log files (human-readable via Experts pane)\n- **CSV files** - Structured data (programmatic analysis)\n\nClaude Code CLI can autonomously read both outputs without user intervention.\n\n## When to Use\n\nUse this skill when:\n\n- Validating MT5 indicator/script execution\n- Checking compilation or runtime errors\n- Analyzing Print() debug output\n- Verifying unit test results (Test_PatternDetector, Test_ArrowManager)\n- User mentions checking \"Experts pane\" manually\n\n## Log File Location\n\nMT5 logs are stored at:\n\n```\n$MQL5_ROOT/Program Files/MetaTrader 5/MQL5/Logs/YYYYMMDD.log\n```\n\n**File Format**:\n\n- Encoding: UTF-16LE (Little Endian)\n- Structure: Tab-separated fields (timestamp, source, message)\n- Size: Grows throughout day (typically 10-100KB)\n\n## Instructions\n\n### 1. Construct today's log path\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\n# Determine current date\nTODAY=$(date +\"%Y%m%d\")\n\n# Build absolute path\nLOG_FILE=\"$MQL5_ROOT/Program Files/MetaTrader 5/MQL5/Logs/${TODAY}.log\"\nSKILL_SCRIPT_EOF\n```\n\n### 2. Read the entire log file\n\nUse Read tool:\n\n- File path: Absolute path from step 1\n- The file contains all Print() statements from MT5 indicators/scripts\n- UTF-16LE encoding is automatically handled by Read tool\n\n### 3. Search for specific content (optional)\n\nUse Grep to filter entries:\n\n```\nPattern: indicator name, \"error\", \"test.*passed\", etc.\nPath: Log file path from step 1\nOutput mode: \"content\" with -n (line numbers)\nContext: -A 5 for 5 lines after matches\n```\n\n### 4. Analyze recent entries (optional)\n\nUse Bash with tail for latest output:\n\n```bash\ntail -n 50 \"$LOG_FILE\"\n```\n\n## Common Validation Patterns\n\n### Check unit test results\n\nSearch for test pass/fail indicators:\n\n```\nPattern: test.*passed|test.*failed|Tests Passed|Tests Failed|ALL TESTS PASSED\nOutput mode: content\nContext: -B 2 -A 2\n```\n\n### Find compilation errors\n\n```\nPattern: error|ERROR|warning|WARNING|failed to create\nOutput mode: content\nContext: -A 3\n```\n\n### Monitor specific indicator\n\n```\nPattern: CCI Rising Test|PatternDetector|ArrowManager\nOutput mode: content\nContext: -A 2\n```\n\n### View initialization messages\n\n```\nPattern: OnInit|initialization|Initialization complete|Phase \\d+\nOutput mode: content\n```\n\n## Examples\n\n### Example 1: Validate unit test completion\n\n```\nInput: User compiled Test_PatternDetector.mq5\nAction:\n  1. Read today's log file\n  2. Grep for \"Test.*PatternDetector|Tests Passed|Tests Failed\"\n  3. Report results (e.g., \"17 tests passed, 0 failed\")\nOutput: Test status without user checking Experts pane\n```\n\n### Example 2: Check for runtime errors\n\n```\nInput: User reports indicator not working\nAction:\n  1. Read today's log file\n  2. Grep for \"ERROR|error|failed\" with -A 3 context\n  3. Analyze error messages\nOutput: Specific error details and line numbers\n```\n\n### Example 3: Verify Phase 2 arrow creation\n\n```\nInput: User asks \"did the test arrow get created?\"\nAction:\n  1. Read today's log file\n  2. Grep for \"Phase 2|Test arrow created|Failed to create\"\n  3. Check for success/failure messages\nOutput: Arrow creation status with timestamp\n```\n\n## Security Considerations\n\n- Log files may contain sensitive trading data (symbol names, account info)\n- Restricted to Read, Bash, Grep tools only (no network access via WebFetch)\n- Do not expose absolute paths unnecessarily in user-facing output\n- Filter sensitive information when reporting results\n- No file modification operations allowed\n\n## Integration with Dual Logging\n\nThis skill enables programmatic access to one half of the dual logging pattern:\n\n1. **MT5 Log Files** (this skill) - Human-readable Print() output\n2. **CSV Files** (CSVLogger.mqh) - Structured audit trails for validation\n\nBoth are accessible without user intervention:\n\n- MT5 logs: Read via this skill\n- CSV files: Read directly via Read tool or validate_export.py\n\n## Validation Checklist\n\nWhen using this skill:\n\n- [ ] Log file exists for today's date\n- [ ] File size > 0 (not empty)\n- [ ] Contains expected indicator/script output\n- [ ] Timestamps match execution time\n- [ ] Error messages (if any) are actionable\n- [ ] Test results (if applicable) show pass/fail counts\n\n## References\n\n- MT5 file locations: `docs/guides/MT5_FILE_LOCATIONS.md`\n- Dual logging implementation: `docs/plans/cci-rising-pattern-marker.yaml` Phase 3-4\n- CSVLogger library: `Program Files/MetaTrader 5/MQL5/Indicators/Custom/Development/CCINeutrality/lib/CSVLogger.mqh`"
              },
              {
                "name": "mql5-indicator-patterns",
                "description": "Develop custom MQL5 indicators with proper patterns. Use when creating indicators, debugging OnCalculate(), working with buffers, or implementing multi-timeframe indicators in MetaTrader 5.",
                "path": "plugins/mql5/skills/mql5-indicator-patterns/SKILL.md",
                "frontmatter": {
                  "name": "mql5-indicator-patterns",
                  "description": "Develop custom MQL5 indicators with proper patterns. Use when creating indicators, debugging OnCalculate(), working with buffers, or implementing multi-timeframe indicators in MetaTrader 5.",
                  "allowed-tools": "Read, Grep, Edit, Write"
                },
                "content": "# MQL5 Visual Indicator Patterns\n\nBattle-tested patterns for creating custom MQL5 indicators with proper display, buffer management, and real-time updates.\n\n## Quick Reference\n\n### Essential Patterns\n\n**Display Scale** (for small values < 1.0):\n\n```mql5\nIndicatorSetDouble(INDICATOR_MINIMUM, 0.0);\nIndicatorSetDouble(INDICATOR_MAXIMUM, 0.1);\n```\n\n**Buffer Setup** (visible + hidden):\n\n```mql5\nSetIndexBuffer(0, BufVisible, INDICATOR_DATA);        // Visible\nSetIndexBuffer(1, BufHidden, INDICATOR_CALCULATIONS); // Hidden\n```\n\n**New Bar Detection** (prevents drift):\n\n```mql5\nstatic int last_processed_bar = -1;\nbool is_new_bar = (i > last_processed_bar);\n```\n\n**Warmup Calculation**:\n\n```mql5\nint StartCalcPosition = underlying_warmup + own_warmup;\nPlotIndexSetInteger(0, PLOT_DRAW_BEGIN, StartCalcPosition);\n```\n\n---\n\n## Common Pitfalls\n\n**Blank Display**: Set explicit scale (see Display Scale reference)\n\n**Rolling Window Drift**: Use new bar detection with hidden buffer (see Recalculation reference)\n\n**Misaligned Plots**: Calculate correct PLOT_DRAW_BEGIN (see Complete Template reference)\n\n**Forward-Indexed Arrays**: Always set `ArraySetAsSeries(buffer, false)`\n\n---\n\n## Key Patterns\n\n**For production MQL5 indicators**:\n\n1. Explicit scale for small values (< 1.0 range)\n2. Hidden buffers for recalculation tracking\n3. New bar detection prevents rolling window drift\n4. Static variables maintain state efficiently\n5. Proper warmup calculation prevents misalignment\n6. Forward indexing for code clarity\n\nThese patterns solve the most common indicator development issues encountered in real-world MT5 development.\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Display Scale](./references/display-scale.md) - Fix blank indicator windows for small values\n- [Buffer Patterns](./references/buffer-patterns.md) - Visible and hidden buffer architecture\n- [Recalculation](./references/recalculation.md) - Bar detection and rolling window state management\n- [Complete Template](./references/complete-template.md) - Full working example with all patterns\n- [Debugging](./references/debugging.md) - Checklist for troubleshooting display issues"
              },
              {
                "name": "python-workspace",
                "description": "Configures Python development workspace for MQL5 integration. Use when setting up MetaTrader 5 Python API, configuring mt5 package, or establishing MQL5-Python workflows.",
                "path": "plugins/mql5/skills/python-workspace/SKILL.md",
                "frontmatter": {
                  "name": "python-workspace",
                  "description": "Configures Python development workspace for MQL5 integration. Use when setting up MetaTrader 5 Python API, configuring mt5 package, or establishing MQL5-Python workflows."
                },
                "content": "# MQL5-Python Translation Workspace Skill\n\nSeamless MQL5 indicator translation to Python with autonomous validation and self-correction.\n\n---\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n\n- Export market data or indicator values from MetaTrader 5\n- Translate MQL5 indicators to Python implementations\n- Validate Python indicator accuracy against MQL5 reference\n- Understand MQL5-Python workflow capabilities and limitations\n- Troubleshoot common translation issues\n\n**Activation Phrases**: \"MQL5\", \"MetaTrader\", \"indicator translation\", \"Python validation\", \"export data\", \"mql5-crossover workspace\"\n\n---\n\n## Core Mission\n\n**Main Theme**: Make MQL5-Python translation **as seamless as possible** through:\n\n1. **Autonomous workflows** (headless export, CLI compilation, automated validation)\n1. **Validation-driven iteration** (>=0.999 correlation gates all work)\n1. **Self-correction** (documented failures prevent future mistakes)\n1. **Clear boundaries** (what works vs what doesn't, with alternatives)\n\n**Project Root**: `/Users/terryli/Library/Application Support/CrossOver/Bottles/MetaTrader 5/drive_c`\n\n---\n\n## Workspace Capabilities Matrix\n\n### WHAT THIS WORKSPACE CAN DO\n\n#### 1. Automated Headless Market Data Export (v3.0.0)\n\n**Status**: PRODUCTION (0.999920 correlation validated)\n\n**What It Does**:\n\n- Fetches OHLCV data + built-in indicators (RSI, SMA) from any symbol/timeframe\n- True headless via Wine Python + MetaTrader5 API\n- No GUI initialization required (cold start supported)\n- Execution time: 6-8 seconds for 5000 bars\n\n**Command Example**:\n\n```bash\nCX_BOTTLE=\"MetaTrader 5\" \\\nWINEPREFIX=\"$HOME/Library/Application Support/CrossOver/Bottles/MetaTrader 5\" \\\nwine \"C:\\\\Program Files\\\\Python312\\\\python.exe\" \\\n  \"C:\\\\users\\\\crossover\\\\export_aligned.py\" \\\n  --symbol EURUSD --period M1 --bars 5000\n```\n\n**Use When**: User needs automated market data exports without GUI interaction\n\n**Limitations**: Cannot access custom indicator buffers (API restriction)\n\n**Reference**: `/docs/guides/WINE_PYTHON_EXECUTION.md`\n\n---\n\n## Reference Documentation\n\nFor detailed information, see:\n\n- [Capabilities Detailed](./references/capabilities-detailed.md) - In-depth capability documentation\n- [Complete Workflows](./references/workflows-complete.md) - End-to-end user workflows\n- [Troubleshooting & Errors](./references/troubleshooting-errors.md) - Requirements, assumptions, error patterns\n- [Validation Metrics](./references/validation-metrics.md) - Success metrics and version history"
              }
            ]
          },
          {
            "name": "itp-hooks",
            "description": "ITP workflow enforcement: Ruff Python linting, ASCII art blocking, graph-easy reminders, ADR/Spec sync, code-to-ADR traceability",
            "source": "./plugins/itp-hooks/",
            "category": "enforcement",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install itp-hooks@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/setup",
                "description": "Check and install dependencies for itp-hooks (silent failure detection + fake-data-guard)",
                "path": "plugins/itp-hooks/commands/setup.md",
                "frontmatter": {
                  "description": "Check and install dependencies for itp-hooks (silent failure detection + fake-data-guard)",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead, AskUserQuestion",
                  "argument-hint": "[--install|--check]"
                },
                "content": "# ITP Hooks Setup\n\nVerify and install dependencies for the itp-hooks plugin:\n\n- **jq** (required) - JSON processing for hook input/output\n- **bun** or **node** (required) - Runtime for fake-data-guard.mjs hook\n- **ruff** (optional) - Python silent failure detection\n- **shellcheck** (optional) - Shell script analysis\n- **oxlint** (optional) - JavaScript/TypeScript linting\n\n## Quick Start\n\nRun dependency check:\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp-hooks}\"\nbash \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --check\nSETUP_EOF\n```\n\n## Interactive Setup Workflow\n\n### Step 1: Check Dependencies\n\n```bash\n/usr/bin/env bash << 'CHECK_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp-hooks}\"\nbash \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --check\nCHECK_EOF\n```\n\n### Step 2: Present Findings\n\nAfter running the check, present the findings to the user:\n\n| Tool       | Status | Purpose                       |\n| ---------- | ------ | ----------------------------- |\n| jq         | ?      | Required for hook I/O         |\n| bun/node   | ?      | Required for fake-data-guard  |\n| ruff       | ?      | Python silent failure rules   |\n| shellcheck | ?      | Shell script analysis         |\n| oxlint     | ?      | JavaScript/TypeScript linting |\n\n### Step 3: User Decision (if missing tools)\n\nIf optional linters are missing, use AskUserQuestion:\n\n```\nquestion: \"Install optional linters for full silent failure detection coverage?\"\nheader: \"Linters\"\noptions:\n  - label: \"Install all\"\n    description: \"Install ruff, shellcheck, and oxlint for Python, Shell, and JS/TS coverage\"\n  - label: \"Skip\"\n    description: \"Continue with graceful degradation (detection only for installed linters)\"\n```\n\n### Step 4: Install (if confirmed)\n\n```bash\n/usr/bin/env bash << 'INSTALL_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/itp-hooks}\"\nbash \"$PLUGIN_DIR/scripts/install-dependencies.sh\" --install\nINSTALL_EOF\n```\n\n## Flags\n\n| Flag        | Behavior                             |\n| ----------- | ------------------------------------ |\n| (none)      | Check dependencies, show status      |\n| `--check`   | Same as default                      |\n| `--install` | Check then install all missing tools |\n\n## Graceful Degradation\n\nThe hooks work with graceful degradation:\n\n| Tool Missing | Effect                                 |\n| ------------ | -------------------------------------- |\n| bun/node     | Fake-data-guard hook fails             |\n| ruff         | Python files skip silent failure check |\n| shellcheck   | Shell scripts skip analysis            |\n| oxlint       | JS/TS files skip linting               |\n| jq           | All hooks fail (required for JSON I/O) |\n\n## Next Steps\n\nAfter setup, install the hooks to your settings:\n\n```bash\n/itp:hooks install\n```\n\n**IMPORTANT**: Restart Claude Code session for hooks to take effect."
              }
            ],
            "skills": [
              {
                "name": "hooks-development",
                "description": "Claude Code hooks development guide covering all 10 hook events lifecycle, PostToolUse visibility patterns, PreToolUse guards, Stop hook schema, and debugging. Use when creating hooks, troubleshooting hook output, understanding hook lifecycle, or when user mentions decision block, hook JSON output, stop hook, or Claude Code hooks.",
                "path": "plugins/itp-hooks/skills/hooks-development/SKILL.md",
                "frontmatter": {
                  "name": "hooks-development",
                  "description": "Claude Code hooks development guide covering all 10 hook events lifecycle, PostToolUse visibility patterns, PreToolUse guards, Stop hook schema, and debugging. Use when creating hooks, troubleshooting hook output, understanding hook lifecycle, or when user mentions decision block, hook JSON output, stop hook, or Claude Code hooks."
                },
                "content": "# Hooks Development\n\nGuide for developing Claude Code hooks with proper output visibility patterns.\n\n## When to Use This Skill\n\n- Creating a new PostToolUse or PreToolUse hook\n- Hook output is not visible to Claude (most common issue)\n- User asks about `decision: block` pattern\n- Debugging why hook messages don't appear\n- User mentions \"Claude Code hooks\" or \"hook visibility\"\n\n---\n\n## Quick Reference: Visibility Patterns\n\n**Critical insight**: PostToolUse hook stdout is only visible to Claude when JSON contains `\"decision\": \"block\"`.\n\n| Output Format                  | Claude Visibility |\n| ------------------------------ | ----------------- |\n| Plain text                     | Not visible       |\n| JSON without `decision: block` | Not visible       |\n| JSON with `decision: block`    | Visible           |\n\n**Exit code behavior**:\n\n| Exit Code | stdout Behavior                         | Claude Visibility             |\n| --------- | --------------------------------------- | ----------------------------- |\n| **0**     | JSON parsed, shown in verbose mode only | Only if `\"decision\": \"block\"` |\n| **2**     | Ignored, uses stderr instead            | stderr shown to Claude        |\n| **Other** | stderr shown in verbose mode            | Not shown to Claude           |\n\n---\n\n## Minimal Working Pattern\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Read hook payload from stdin\nPAYLOAD=$(cat)\nFILE_PATH=$(echo \"$PAYLOAD\" | jq -r '.tool_input.file_path // empty')\n\n[[ -z \"$FILE_PATH\" ]] && exit 0\n\n# Your condition here\nif [[ condition_met ]]; then\n    jq -n \\\n        --arg reason \"[HOOK] Your message to Claude\" \\\n        '{decision: \"block\", reason: $reason}'\nfi\n\nexit 0\nSKILL_SCRIPT_EOF\n```\n\n**Key points**:\n\n1. Use `jq -n` to generate valid JSON\n2. Include `\"decision\": \"block\"` for visibility\n3. Exit with code 0\n4. The \"blocking error\" label is cosmetic - operation continues\n\n---\n\n## TodoWrite Templates\n\n### Creating a PostToolUse Hook\n\n```markdown\n1. [pending] Create hook script with shebang and set -euo pipefail\n2. [pending] Parse PAYLOAD from stdin with jq\n3. [pending] Add condition check for when to trigger\n4. [pending] Output JSON with decision:block pattern\n5. [pending] Register hook in hooks.json with matcher\n6. [pending] Test by editing a matching file\n7. [pending] Verify Claude sees the message in system-reminder\n```\n\n### Debugging Invisible Hook Output\n\n```markdown\n1. [pending] Verify hook executes (add debug log to /tmp)\n2. [pending] Check JSON format is valid (pipe to jq .)\n3. [pending] Confirm decision:block is present in output\n4. [pending] Verify exit code is 0\n5. [pending] Check hooks.json matcher pattern\n6. [pending] Restart Claude Code session\n```\n\n---\n\n## Reference Documentation\n\n- [Lifecycle Reference](./references/lifecycle-reference.md) - All 10 hook events, diagrams, use cases, configuration pitfalls\n- [Visibility Patterns](./references/visibility-patterns.md) - Full exit code and JSON schema details\n- [Hook Templates](./references/hook-templates.md) - Copy-paste templates for common patterns\n- [Debugging Guide](./references/debugging-guide.md) - Troubleshooting invisible output\n\n---\n\n## Post-Change Checklist (Self-Evolution)\n\nWhen this skill is updated:\n\n- [ ] Update [evolution-log.md](./references/evolution-log.md) with discovery\n- [ ] Verify code examples still work\n- [ ] Check if ADR needs updating: [PostToolUse Hook Visibility ADR](../../../../docs/adr/2025-12-17-posttooluse-hook-visibility.md)\n\n---\n\n## Related Resources\n\n- [ADR: PostToolUse Hook Visibility](../../../../docs/adr/2025-12-17-posttooluse-hook-visibility.md)\n- [GitHub Issue #3983](https://github.com/anthropics/claude-code/issues/3983) - Original bug report\n- [Claude Code Hooks Reference](https://code.claude.com/docs/en/hooks) - Official documentation"
              }
            ]
          },
          {
            "name": "git-account-validator",
            "description": "Pre-push validation for multi-account GitHub: blocks HTTPS URLs, validates SSH account matches git config, port 443 fallback",
            "source": "./plugins/git-account-validator/",
            "category": "enforcement",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install git-account-validator@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/hooks",
                "description": "Install/uninstall git-account-validator hooks to ~/.claude/settings.json",
                "path": "plugins/git-account-validator/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall git-account-validator hooks to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead",
                  "argument-hint": "[install|uninstall|status]"
                },
                "content": "# Git Account Validator Hooks Manager\n\nManage git push validation hooks installation in `~/.claude/settings.json`.\n\nClaude Code only loads hooks from settings.json, not from plugin hooks.json files. This command installs/uninstalls the PreToolUse hook that validates git push operations for multi-account authentication.\n\n## Actions\n\n| Action      | Description                                     |\n| ----------- | ----------------------------------------------- |\n| `status`    | Show current installation state                 |\n| `install`   | Add git-account-validator hook to settings.json |\n| `uninstall` | Remove hook from settings.json                  |\n\n## Execution\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'HOOKS_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/git-account-validator}\"\nACTION=\"${ARGUMENTS:-status}\"\nbash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" $ACTION\nHOOKS_SCRIPT_EOF\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe hooks are loaded at session start. Modifications to settings.json require a restart."
              }
            ],
            "skills": []
          },
          {
            "name": "alpha-forge-worktree",
            "description": "Git worktree management for alpha-forge with ADR-style naming and dynamic iTerm2 tab detection",
            "source": "./plugins/alpha-forge-worktree/",
            "category": "development",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install alpha-forge-worktree@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "worktree-manager",
                "description": "Create alpha-forge git worktrees with automatic branch naming from descriptions. Use when user says create worktree, new worktree, alpha-forge worktree, AF worktree, or describes a feature to work on in alpha-forge.",
                "path": "plugins/alpha-forge-worktree/skills/worktree-manager/SKILL.md",
                "frontmatter": {
                  "name": "worktree-manager",
                  "description": "Create alpha-forge git worktrees with automatic branch naming from descriptions. Use when user says create worktree, new worktree, alpha-forge worktree, AF worktree, or describes a feature to work on in alpha-forge."
                },
                "content": "# Alpha-Forge Worktree Manager\n\n<!-- ADR: /docs/adr/2025-12-14-alpha-forge-worktree-management.md -->\n\nCreate and manage git worktrees for the alpha-forge repository with automatic branch naming, consistent conventions, and lifecycle management.\n\n## Triggers\n\nInvoke this skill when user mentions:\n\n- \"create worktree for [description]\"\n- \"new worktree [description]\"\n- \"alpha-forge worktree\"\n- \"AF worktree\"\n- \"worktree from origin/...\"\n- \"worktree for feat/...\"\n\n## Operational Modes\n\nThis skill supports three distinct modes based on user input:\n\n| Mode             | User Input Example                            | Action                                |\n| ---------------- | --------------------------------------------- | ------------------------------------- |\n| **New Branch**   | \"create worktree for sharpe validation\"       | Derive slug, create branch + worktree |\n| **Remote Track** | \"create worktree from origin/feat/existing\"   | Track remote branch in new worktree   |\n| **Local Branch** | \"create worktree for feat/2025-12-15-my-feat\" | Use existing branch in new worktree   |\n\n---\n\n## Mode 1: New Branch from Description (Primary)\n\nThis is the most common workflow. User provides a natural language description, Claude derives the slug.\n\n### Step 1: Parse Description and Derive Slug\n\n**Claude derives kebab-case slugs following these rules:**\n\n**Word Economy Rule**:\n\n- Each word in slug MUST convey unique meaning\n- Remove filler words: the, a, an, for, with, and, to, from, in, on, of, by\n- Avoid redundancy (e.g., \"database\" after \"ClickHouse\")\n- Limit to 3-5 words maximum\n\n**Conversion Steps**:\n\n1. Parse description from user input\n2. Convert to lowercase\n3. Apply word economy (remove filler words)\n4. Replace spaces with hyphens\n\n**Examples**:\n\n| User Description                        | Derived Slug                    |\n| --------------------------------------- | ------------------------------- |\n| \"sharpe statistical validation\"         | `sharpe-statistical-validation` |\n| \"fix the memory leak in metrics\"        | `memory-leak-metrics`           |\n| \"implement user authentication for API\" | `user-authentication-api`       |\n| \"add BigQuery data source support\"      | `bigquery-data-source`          |\n\n### Step 2: Verify Main Worktree Status\n\n**CRITICAL**: Before proceeding, check that main worktree is on `main` branch.\n\n```bash\n/usr/bin/env bash << 'GIT_EOF'\ncd ~/eon/alpha-forge\nCURRENT=$(git branch --show-current)\nGIT_EOF\n```\n\n**If NOT on main/master**:\n\nUse AskUserQuestion to warn user:\n\n```yaml\nquestion: \"Main worktree is on '$CURRENT', not main. Best practice is to keep main worktree clean. Continue anyway?\"\nheader: \"Warning\"\noptions:\n  - label: \"Continue anyway\"\n    description: \"Proceed with worktree creation\"\n  - label: \"Switch main to 'main' first\"\n    description: \"I'll switch the main worktree to main branch before creating\"\nmultiSelect: false\n```\n\nIf user selects \"Switch main to 'main' first\":\n\n```bash\ncd ~/eon/alpha-forge\ngit checkout main\n```\n\n### Step 3: Fetch Remote and Display Branches\n\n```bash\ncd ~/eon/alpha-forge\ngit fetch --all --prune\n\n# Display available branches for user reference\necho \"Available remote branches:\"\ngit branch -r | grep -v HEAD | head -20\n```\n\n### Step 4: Prompt for Branch Type\n\nUse AskUserQuestion:\n\n```yaml\nquestion: \"What type of branch is this?\"\nheader: \"Branch type\"\noptions:\n  - label: \"feat\"\n    description: \"New feature or capability\"\n  - label: \"fix\"\n    description: \"Bug fix or correction\"\n  - label: \"refactor\"\n    description: \"Code restructuring (no behavior change)\"\n  - label: \"chore\"\n    description: \"Maintenance, tooling, dependencies\"\nmultiSelect: false\n```\n\n### Step 5: Prompt for Base Branch\n\nUse AskUserQuestion:\n\n```yaml\nquestion: \"Which branch should this be based on?\"\nheader: \"Base branch\"\noptions:\n  - label: \"main (Recommended)\"\n    description: \"Base from main branch\"\n  - label: \"develop\"\n    description: \"Base from develop branch\"\nmultiSelect: false\n```\n\nIf user needs a different branch, they can select \"Other\" and provide the branch name.\n\n### Step 6: Construct Branch Name\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\nTYPE=\"feat\"           # From Step 4\nDATE=$(date +%Y-%m-%d)\nSLUG=\"sharpe-statistical-validation\"  # From Step 1\nBASE=\"main\"           # From Step 5\n\nBRANCH=\"${TYPE}/${DATE}-${SLUG}\"\n# Result: feat/2025-12-15-sharpe-statistical-validation\nSKILL_SCRIPT_EOF\n```\n\n### Step 7: Create Worktree (Atomic)\n\n```bash\n/usr/bin/env bash << 'GIT_EOF_2'\ncd ~/eon/alpha-forge\n\nWORKTREE_PATH=\"$HOME/eon/alpha-forge.worktree-${DATE}-${SLUG}\"\n\n# Atomic branch + worktree creation\ngit worktree add -b \"${BRANCH}\" \"${WORKTREE_PATH}\" \"origin/${BASE}\"\nGIT_EOF_2\n```\n\n### Step 8: Generate Tab Name and Report\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF_2'\n# Generate acronym from slug\nACRONYM=$(echo \"$SLUG\" | tr '-' '\\n' | cut -c1 | tr -d '\\n')\nTAB_NAME=\"AF-${ACRONYM}\"\nSKILL_SCRIPT_EOF_2\n```\n\nReport success:\n\n```\n Worktree created successfully\n\n  Path:    ~/eon/alpha-forge.worktree-2025-12-15-sharpe-statistical-validation\n  Branch:  feat/2025-12-15-sharpe-statistical-validation\n  Tab:     AF-ssv\n  Env:     .envrc created (loads shared secrets)\n\n  iTerm2: Restart iTerm2 to see the new tab\n```\n\n---\n\n## Mode 2: Remote Branch Tracking\n\nWhen user specifies `origin/branch-name`, create a local tracking branch.\n\n**Detection**: User input contains `origin/` prefix.\n\n**Example**: \"create worktree from origin/feat/2025-12-10-existing-feature\"\n\n### Workflow\n\n```bash\n/usr/bin/env bash << 'GIT_EOF_3'\ncd ~/eon/alpha-forge\ngit fetch --all --prune\n\nREMOTE_BRANCH=\"origin/feat/2025-12-10-existing-feature\"\nLOCAL_BRANCH=\"feat/2025-12-10-existing-feature\"\n\n# Extract date and slug for worktree naming\n# Pattern: type/YYYY-MM-DD-slug\nif [[ \"$LOCAL_BRANCH\" =~ ^(feat|fix|refactor|chore)/([0-9]{4}-[0-9]{2}-[0-9]{2})-(.+)$ ]]; then\n    DATE=\"${BASH_REMATCH[2]}\"\n    SLUG=\"${BASH_REMATCH[3]}\"\nelse\n    DATE=$(date +%Y-%m-%d)\n    SLUG=\"${LOCAL_BRANCH##*/}\"\nfi\n\nWORKTREE_PATH=\"$HOME/eon/alpha-forge.worktree-${DATE}-${SLUG}\"\n\n# Create tracking branch + worktree\ngit worktree add -b \"${LOCAL_BRANCH}\" \"${WORKTREE_PATH}\" \"${REMOTE_BRANCH}\"\nGIT_EOF_3\n```\n\n---\n\n## Mode 3: Existing Local Branch\n\nWhen user specifies a local branch name (without `origin/`), use it directly.\n\n**Detection**: User input is a valid branch name format (e.g., `feat/2025-12-15-slug`).\n\n**Example**: \"create worktree for feat/2025-12-15-my-feature\"\n\n### Workflow\n\n```bash\n/usr/bin/env bash << 'VALIDATE_EOF'\ncd ~/eon/alpha-forge\n\nBRANCH=\"feat/2025-12-15-my-feature\"\n\n# Verify branch exists\nif ! git show-ref --verify \"refs/heads/${BRANCH}\" 2>/dev/null; then\n    echo \"ERROR: Local branch '${BRANCH}' not found\"\n    echo \"Available local branches:\"\n    git branch | head -20\n    exit 1\nfi\n\n# Extract date and slug\nif [[ \"$BRANCH\" =~ ^(feat|fix|refactor|chore)/([0-9]{4}-[0-9]{2}-[0-9]{2})-(.+)$ ]]; then\n    DATE=\"${BASH_REMATCH[2]}\"\n    SLUG=\"${BASH_REMATCH[3]}\"\nelse\n    DATE=$(date +%Y-%m-%d)\n    SLUG=\"${BRANCH##*/}\"\nfi\n\nWORKTREE_PATH=\"$HOME/eon/alpha-forge.worktree-${DATE}-${SLUG}\"\n\n# Create worktree for existing branch (no -b flag)\ngit worktree add \"${WORKTREE_PATH}\" \"${BRANCH}\"\nVALIDATE_EOF\n```\n\n---\n\n## Naming Conventions\n\n### Worktree Folder Naming (ADR-Style)\n\n**Format**: `alpha-forge.worktree-YYYY-MM-DD-slug`\n\n**Location**: `~/eon/`\n\n| Branch                                          | Worktree Folder                                                 |\n| ----------------------------------------------- | --------------------------------------------------------------- |\n| `feat/2025-12-14-sharpe-statistical-validation` | `alpha-forge.worktree-2025-12-14-sharpe-statistical-validation` |\n| `feat/2025-12-13-feature-genesis-skills`        | `alpha-forge.worktree-2025-12-13-feature-genesis-skills`        |\n| `fix/quick-patch`                               | `alpha-forge.worktree-{TODAY}-quick-patch`                      |\n\n### iTerm2 Tab Naming (Acronym-Based)\n\n**Format**: `AF-{acronym}` where acronym = first character of each word in slug\n\n| Worktree Slug                   | Tab Name   |\n| ------------------------------- | ---------- |\n| `sharpe-statistical-validation` | `AF-ssv`   |\n| `feature-genesis-skills`        | `AF-fgs`   |\n| `eth-block-metrics-data-plugin` | `AF-ebmdp` |\n\n---\n\n## Stale Worktree Detection\n\nCheck for worktrees whose branches are already merged to main:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\ncd ~/eon/alpha-forge\n\n# Get branches merged to main\nMERGED=$(git branch --merged main | grep -v '^\\*' | grep -v 'main' | tr -d ' ')\n\n# Check each worktree\ngit worktree list --porcelain | grep '^branch' | cut -d' ' -f2 | while read branch; do\n    branch_name=\"${branch##refs/heads/}\"\n    if echo \"$MERGED\" | grep -q \"^${branch_name}$\"; then\n        path=$(git worktree list | grep \"\\[${branch_name}\\]\" | awk '{print $1}')\n        echo \"STALE: $branch_name at $path\"\n    fi\ndone\nPREFLIGHT_EOF\n```\n\nIf stale worktrees found, prompt user to cleanup using AskUserQuestion.\n\n---\n\n## Cleanup Workflow\n\n### Remove Stale Worktree\n\n```bash\n# Remove worktree (keeps branch)\ngit worktree remove ~/eon/alpha-forge.worktree-{DATE}-{SLUG}\n\n# Optionally delete merged branch\ngit branch -d {BRANCH}\n```\n\n### Prune Orphaned Entries\n\n```bash\ngit worktree prune\n```\n\n---\n\n## Error Handling\n\n| Scenario                 | Action                                           |\n| ------------------------ | ------------------------------------------------ |\n| Branch already exists    | Suggest using Mode 3 (existing branch) or rename |\n| Remote branch not found  | List available remote branches                   |\n| Main worktree on feature | Warn via AskUserQuestion, offer to switch        |\n| Empty description        | Show usage examples                              |\n| Network error on fetch   | Allow offline mode with local branches only      |\n| Worktree path exists     | Suggest cleanup or different slug                |\n\n### Branch Not Found\n\n```\n Branch 'feat/nonexistent' not found\n\n  Available branches:\n  - feat/2025-12-14-sharpe-statistical-validation\n  - main\n\n  To create from remote:\n  Specify: \"create worktree from origin/branch-name\"\n```\n\n### Worktree Already Exists\n\n```\n Worktree already exists for this branch\n\n  Existing path: ~/eon/alpha-forge.worktree-2025-12-14-sharpe-statistical-validation\n\n  To use existing worktree:\n  cd ~/eon/alpha-forge.worktree-2025-12-14-sharpe-statistical-validation\n```\n\n---\n\n## Integration\n\n### direnv Environment Setup\n\nWorktrees automatically get a `.envrc` file that loads shared credentials from `~/eon/.env.alpha-forge`.\n\n**What happens on worktree creation**:\n\n1. Script checks if `~/eon/.env.alpha-forge` exists\n2. Creates `.envrc` in the new worktree with `dotenv` directive\n3. Runs `direnv allow` to approve the new `.envrc`\n\n**Shared secrets file** (`~/eon/.env.alpha-forge`):\n\n```bash\n# ClickHouse credentials, API keys, etc.\nCLICKHOUSE_HOST_READONLY=\"...\"\nCLICKHOUSE_USER_READONLY=\"...\"\nCLICKHOUSE_PASSWORD_READONLY=\"...\"\n```\n\n**Generated `.envrc`** (in each worktree):\n\n```bash\n# alpha-forge worktree direnv config\n# Auto-generated by create-worktree.sh\n\n# Load shared alpha-forge secrets\ndotenv /Users/terryli/eon/.env.alpha-forge\n\n# Worktree-specific overrides can be added below\n```\n\n**Prerequisites**:\n\n- direnv installed via mise (`mise use -g direnv@latest`)\n- Shell hook configured (`eval \"$(direnv hook zsh)\"` in `~/.zshrc`)\n- Shared secrets file at `~/eon/.env.alpha-forge`\n\n### iTerm2 Dynamic Detection\n\nThe `default-layout.py` script auto-discovers worktrees:\n\n1. Globs `~/eon/alpha-forge.worktree-*`\n2. Validates each against `git worktree list`\n3. Generates `AF-{acronym}` tab names\n4. Inserts tabs after main AF tab\n\n---\n\n## References\n\n- [Naming Conventions](./references/naming-conventions.md)\n- [ADR: Alpha-Forge Git Worktree Management](/docs/adr/2025-12-14-alpha-forge-worktree-management.md)\n- [Design Spec](/docs/design/2025-12-14-alpha-forge-worktree-management/spec.md)"
              }
            ]
          },
          {
            "name": "ralph",
            "description": "Autonomous AI orchestration with Ralph Wiggum technique - keeps AI in loop until task complete. Long-running automation, evolutionary development",
            "source": "./plugins/ralph/",
            "category": "automation",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install ralph@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/audit-now",
                "description": "Force immediate validation round",
                "path": "plugins/ralph/commands/audit-now.md",
                "frontmatter": {
                  "description": "Force immediate validation round",
                  "allowed-tools": "Bash",
                  "argument-hint": "[round-number]"
                },
                "content": "# Ralph Loop: Audit Now\n\nForce the loop to enter validation mode on the next iteration.\nUseful for triggering early validation before natural completion signals.\n\n## Usage\n\n- `/ralph:audit-now` - Start validation from round 1\n- `/ralph:audit-now 4` - Start from round 4 (Adversarial Probing)\n- `/ralph:audit-now 5` - Start from round 5 (Cross-Period Robustness)\n\n## Execution\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility\n/usr/bin/env bash << 'RALPH_AUDIT_SCRIPT'\n# RALPH_AUDIT_SCRIPT marker - required for PreToolUse hook bypass\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\nSTATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\n\n# Get optional round number argument\nROUND=\"${ARGUMENTS:-1}\"\n\n# Validate round number (1-5)\nif ! [[ \"$ROUND\" =~ ^[1-5]$ ]]; then\n    echo \"Error: Round must be 1-5\"\n    echo \"\"\n    echo \"5-Round Validation System:\"\n    echo \"  1: Critical Issues (ruff errors, imports, syntax)\"\n    echo \"  2: Verification (verify fixes, regression check)\"\n    echo \"  3: Documentation (docstrings, coverage gaps)\"\n    echo \"  4: Adversarial Probing (edge cases, math validation)\"\n    echo \"  5: Cross-Period Robustness (Bull/Bear/Sideways)\"\n    exit 1\nfi\n\n# Ensure config file exists\nif [[ ! -f \"$CONFIG_FILE\" ]]; then\n    echo '{}' > \"$CONFIG_FILE\"\nfi\n\n# Check if loop is running\nCURRENT_STATE=\"stopped\"\nif [[ -f \"$STATE_FILE\" ]]; then\n    CURRENT_STATE=$(jq -r '.state // \"stopped\"' \"$STATE_FILE\" 2>/dev/null || echo \"stopped\")\nfi\n\nif [[ \"$CURRENT_STATE\" != \"running\" ]]; then\n    echo \"Warning: Ralph loop not running (state: $CURRENT_STATE)\"\n    echo \"Force validation flag will be set but may not take effect until loop starts.\"\nfi\n\n# Set force validation flag\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\nif ! jq --argjson round \"$ROUND\" --arg ts \"$TIMESTAMP\" \\\n    '.force_validation = {enabled: true, round: $round, timestamp: $ts}' \\\n    \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n    echo \"ERROR: Failed to update config file (jq error)\" >&2\n    rm -f \"$CONFIG_FILE.tmp\"\n    exit 1\nfi\nmv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n\necho \"Force validation enabled\"\necho \"\"\necho \"Settings:\"\necho \"  Starting round: $ROUND\"\necho \"  Timestamp: $TIMESTAMP\"\necho \"\"\necho \"5-Round Validation System:\"\ncase \"$ROUND\" in\n    1) echo \"   Round 1: Critical Issues (ruff errors, imports, syntax)\" ;;\n    2) echo \"   Round 2: Verification (verify fixes, regression check)\" ;;\n    3) echo \"   Round 3: Documentation (docstrings, coverage gaps)\" ;;\n    4) echo \"   Round 4: Adversarial Probing (edge cases, math validation)\" ;;\n    5) echo \"   Round 5: Cross-Period Robustness (Bull/Bear/Sideways)\" ;;\nesac\necho \"\"\necho \"Effect: Loop will enter validation mode on next iteration\"\necho \"The force_validation flag will be cleared after validation starts\"\nRALPH_AUDIT_SCRIPT\n```\n\nRun the bash script above to force validation mode.\n\n## How It Works\n\n1. **Sets force_validation flag**: Written to `.claude/ralph-config.json`\n2. **Loop checks flag**: On next Stop hook, enters validation mode\n3. **Flag cleared**: After validation starts, flag is reset to prevent loops\n4. **Round selection**: Can start from any round (1-5) for targeted auditing\n\n## 5-Round Validation System\n\n| Round | Name                    | Purpose                                           |\n| ----- | ----------------------- | ------------------------------------------------- |\n| 1     | Critical Issues         | Find blocking bugs (ruff errors, imports, syntax) |\n| 2     | Verification            | Confirm round 1 fixes, regression check           |\n| 3     | Documentation           | Docstrings, coverage gaps                         |\n| 4     | Adversarial Probing     | Edge cases, math validation, stress testing       |\n| 5     | Cross-Period Robustness | Bull/Bear/Sideways market regime testing          |\n\n## Use Cases\n\n- **Early validation**: Trigger before natural completion\n- **Targeted auditing**: Skip to round 4/5 for specific checks\n- **Math validation**: Use `/ralph:audit-now 4` for adversarial probing\n- **Robustness check**: Use `/ralph:audit-now 5` for regime testing"
              },
              {
                "name": "/config",
                "description": "View or modify loop configuration",
                "path": "plugins/ralph/commands/config.md",
                "frontmatter": {
                  "description": "View or modify loop configuration",
                  "allowed-tools": "Read, Write, Bash, AskUserQuestion",
                  "argument-hint": "[show|edit|reset|set <key>=<value>] (runtime configurable)"
                },
                "content": "# Ralph Loop: Config\n\nView or modify the Ralph Wiggum loop configuration (v3.0 unified schema).\n\n**Runtime configurable**: Works with or without active Ralph loop. Changes apply on next iteration.\n\n## Arguments\n\n- `show` (default): Display current configuration with all sections\n- `edit`: Interactively modify settings via AskUserQuestion\n- `reset`: Reset to defaults (removes project config)\n- `set <key>=<value>`: Set a specific config value (e.g., `set loop_limits.min_hours=2`)\n\n## Configuration Schema (v3.0)\n\nThe unified config file `.claude/ralph-config.json` contains all configurable values:\n\n### Loop Limits\n\n| Setting          | Default | POC Mode | Description                          |\n| ---------------- | ------- | -------- | ------------------------------------ |\n| `min_hours`      | 4.0     | 0.083    | Minimum runtime before completion    |\n| `max_hours`      | 9.0     | 0.167    | Maximum runtime (hard stop)          |\n| `min_iterations` | 50      | 10       | Minimum iterations before completion |\n| `max_iterations` | 99      | 20       | Maximum iterations (safety limit)    |\n\n### Loop Detection\n\n| Setting                | Default | Description                              |\n| ---------------------- | ------- | ---------------------------------------- |\n| `similarity_threshold` | 0.9     | RapidFuzz ratio for detecting repetition |\n| `window_size`          | 5       | Number of outputs to track for detection |\n\n### Completion Detection\n\n| Setting                         | Default | Description                              |\n| ------------------------------- | ------- | ---------------------------------------- |\n| `confidence_threshold`          | 0.7     | Minimum confidence to trigger completion |\n| `explicit_marker_confidence`    | 1.0     | Confidence for `[x] TASK_COMPLETE`       |\n| `frontmatter_status_confidence` | 0.95    | Confidence for `implementation-status`   |\n| `all_checkboxes_confidence`     | 0.9     | Confidence when all checkboxes checked   |\n| `no_pending_items_confidence`   | 0.85    | Confidence when has `[x]` but no `[ ]`   |\n| `semantic_phrases_confidence`   | 0.7     | Confidence for \"task complete\" phrases   |\n\n### Validation Phase\n\n| Setting                 | Default | Description                      |\n| ----------------------- | ------- | -------------------------------- |\n| `enabled`               | true    | Enable 3-round validation phase  |\n| `score_threshold`       | 0.8     | Score needed to pass validation  |\n| `max_iterations`        | 3       | Maximum validation cycles        |\n| `improvement_threshold` | 0.1     | Required improvement to continue |\n\n### Protection\n\n| Setting              | Default                                   | Description                       |\n| -------------------- | ----------------------------------------- | --------------------------------- |\n| `protected_files`    | `loop-enabled`, `ralph-config.json`, etc. | Files protected from deletion     |\n| `stop_script_marker` | `RALPH_STOP_SCRIPT`                       | Marker to bypass PreToolUse guard |\n\n### Guidance (v3.0.0+)\n\n| Setting      | Default | Description                                   |\n| ------------ | ------- | --------------------------------------------- |\n| `forbidden`  | `[]`    | Items Ralph should avoid (from AUQ or manual) |\n| `encouraged` | `[]`    | Items Ralph should prioritize                 |\n| `timestamp`  | `\"\"`    | ISO 8601 timestamp of last update             |\n\n### Constraint Scanning (v3.0.0+)\n\n| Setting                | Default | Description                       |\n| ---------------------- | ------- | --------------------------------- |\n| `skip_constraint_scan` | `false` | Skip preflight constraint scanner |\n| `constraint_scan`      | `null`  | Results from last constraint scan |\n\n### Mode Flags (v3.0.0+)\n\n| Setting           | Default | Description                            |\n| ----------------- | ------- | -------------------------------------- |\n| `poc_mode`        | `false` | Use POC time/iteration limits          |\n| `production_mode` | `false` | Use production settings (auditability) |\n| `no_focus`        | `false` | Skip focus file tracking               |\n\n## Execution\n\nBased on `$ARGUMENTS`:\n\n### For `show` or empty\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_CONFIG_SHOW'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\necho \"=== Ralph Configuration (v3.0) ===\"\necho \"\"\n\n# State\necho \"--- State ---\"\nSTATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\nif [[ -f \"$STATE_FILE\" ]]; then\n    echo \"State: $(jq -r '.state // \"stopped\"' \"$STATE_FILE\")\"\nelse\n    echo \"State: stopped (no state file)\"\nfi\necho \"\"\n\n# Project config\necho \"--- Project Config ---\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\nif [[ -f \"$CONFIG_FILE\" ]]; then\n    cat \"$CONFIG_FILE\" | python3 -m json.tool\nelse\n    echo \"(using defaults - no project config)\"\nfi\necho \"\"\n\n# Legacy config (if different)\nLEGACY_FILE=\"$PROJECT_DIR/.claude/loop-config.json\"\nif [[ -f \"$LEGACY_FILE\" ]]; then\n    echo \"--- Legacy Config (backward compat) ---\"\n    cat \"$LEGACY_FILE\" | python3 -m json.tool\nfi\n\n# Global defaults\necho \"\"\necho \"--- Global Defaults Location ---\"\necho \"$HOME/.claude/ralph-defaults.json\"\nif [[ -f \"$HOME/.claude/ralph-defaults.json\" ]]; then\n    cat \"$HOME/.claude/ralph-defaults.json\" | python3 -m json.tool\nelse\n    echo \"(not found - using built-in defaults)\"\nfi\nRALPH_CONFIG_SHOW\n```\n\n### For `reset`\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_CONFIG_RESET'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nrm -f \"$PROJECT_DIR/.claude/ralph-config.json\"\nrm -f \"$PROJECT_DIR/.claude/loop-config.json\"\nrm -f \"$PROJECT_DIR/.claude/ralph-state.json\"\necho \"Project config reset. Using built-in defaults.\"\necho \"\"\necho \"To create global defaults, write to: $HOME/.claude/ralph-defaults.json\"\nRALPH_CONFIG_RESET\n```\n\n### For `set <key>=<value>`\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_CONFIG_SET'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\n\n# Parse key=value from ARGUMENTS\nARGS=\"${ARGUMENTS:-}\"\nKEY_VALUE=$(echo \"$ARGS\" | sed 's/^set //')\nKEY=$(echo \"$KEY_VALUE\" | cut -d= -f1)\nVALUE=$(echo \"$KEY_VALUE\" | cut -d= -f2-)\n\nif [[ -z \"$KEY\" || -z \"$VALUE\" ]]; then\n    echo \"Usage: /ralph:config set <key>=<value>\"\n    echo \"Example: /ralph:config set loop_limits.min_hours=2\"\n    exit 1\nfi\n\n# Create config if doesn't exist\nif [[ ! -f \"$CONFIG_FILE\" ]]; then\n    echo '{\"version\": \"3.0.0\"}' > \"$CONFIG_FILE\"\nfi\n\n# Use jq to set nested key (supports dot notation)\n# Convert dot notation to jq path: loop_limits.min_hours -> .loop_limits.min_hours\nJQ_PATH=\".$(echo \"$KEY\" | sed 's/\\./\\./g')\"\n\n# Detect if value is numeric or string and apply with error handling\nupdate_config() {\n    local jq_expr=\"$1\"\n    if ! jq \"$jq_expr\" \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n        echo \"ERROR: Failed to update config (jq error)\" >&2\n        rm -f \"$CONFIG_FILE.tmp\"\n        exit 1\n    fi\n    mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n}\n\nif [[ \"$VALUE\" =~ ^[0-9]+(\\.[0-9]+)?$ ]]; then\n    # Numeric value\n    update_config \"$JQ_PATH = $VALUE\"\nelif [[ \"$VALUE\" == \"true\" || \"$VALUE\" == \"false\" ]]; then\n    # Boolean value\n    update_config \"$JQ_PATH = $VALUE\"\nelse\n    # String value\n    update_config \"$JQ_PATH = \\\"$VALUE\\\"\"\nfi\n\necho \"Set $KEY = $VALUE\"\necho \"\"\ncat \"$CONFIG_FILE\" | python3 -m json.tool\nRALPH_CONFIG_SET\n```\n\n### For `edit`\n\nUse the AskUserQuestion tool to prompt for new values across all configuration sections, then write to `$PROJECT_DIR/.claude/ralph-config.json`.\n\nExample full config:\n\n```json\n{\n  \"version\": \"3.0.0\",\n  \"state\": \"stopped\",\n  \"poc_mode\": false,\n  \"production_mode\": false,\n  \"no_focus\": false,\n  \"skip_constraint_scan\": false,\n  \"loop_limits\": {\n    \"min_hours\": 4.0,\n    \"max_hours\": 9.0,\n    \"min_iterations\": 50,\n    \"max_iterations\": 99\n  },\n  \"loop_detection\": {\n    \"similarity_threshold\": 0.99,\n    \"window_size\": 5\n  },\n  \"completion\": {\n    \"confidence_threshold\": 0.7,\n    \"explicit_marker_confidence\": 1.0,\n    \"frontmatter_status_confidence\": 0.95,\n    \"all_checkboxes_confidence\": 0.9,\n    \"no_pending_items_confidence\": 0.85,\n    \"semantic_phrases_confidence\": 0.7,\n    \"completion_phrases\": [\"task complete\", \"all done\", \"finished\"]\n  },\n  \"validation\": {\n    \"enabled\": true,\n    \"score_threshold\": 0.8,\n    \"max_rounds\": 5,\n    \"improvement_threshold\": 0.1\n  },\n  \"protection\": {\n    \"protected_files\": [\n      \".claude/loop-enabled\",\n      \".claude/ralph-config.json\",\n      \".claude/ralph-state.json\"\n    ],\n    \"bypass_markers\": [\"RALPH_STOP_SCRIPT\", \"RALPH_START_SCRIPT\"],\n    \"stop_script_marker\": \"RALPH_STOP_SCRIPT\"\n  },\n  \"guidance\": {\n    \"forbidden\": [],\n    \"encouraged\": [],\n    \"timestamp\": \"\"\n  }\n}\n```"
              },
              {
                "name": "/encourage",
                "description": "Add item to encouraged list mid-loop",
                "path": "plugins/ralph/commands/encourage.md",
                "frontmatter": {
                  "description": "Add item to encouraged list mid-loop",
                  "allowed-tools": "Bash, AskUserQuestion",
                  "argument-hint": "<phrase> | --list | --clear | --remove (live: applies next iteration)"
                },
                "content": "# Ralph Loop: Encourage\n\nAdd items to the encouraged list during an active loop session.\nEncouraged items get priority in opportunity discovery and override forbidden patterns.\n\n**Runtime configurable**: Works with or without active Ralph loop. Changes apply on next iteration.\n\n## Usage\n\n- `/ralph:encourage Sharpe ratio` - Add \"Sharpe ratio\" to encouraged list\n- `/ralph:encourage --list` - Show current encouraged items\n- `/ralph:encourage --clear` - Clear all encouraged items\n- `/ralph:encourage --remove` - Interactive picker to remove specific items\n- `/ralph:encourage --remove <phrase>` - Remove item matching phrase (fuzzy)\n\n## Execution\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility\n/usr/bin/env bash << 'RALPH_ENCOURAGE_SCRIPT'\n# RALPH_ENCOURAGE_SCRIPT marker - required for PreToolUse hook bypass\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\nSTATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\n\n# Get arguments (everything after the command)\nARGS=\"${ARGUMENTS:-}\"\n\n# Ensure config file exists with guidance structure\nif [[ ! -f \"$CONFIG_FILE\" ]]; then\n    echo '{\"guidance\": {\"forbidden\": [], \"encouraged\": []}}' > \"$CONFIG_FILE\"\nfi\n\n# Ensure guidance structure exists\nif ! jq -e '.guidance' \"$CONFIG_FILE\" >/dev/null 2>&1; then\n    if ! jq '. + {guidance: {forbidden: [], encouraged: []}}' \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n        echo \"ERROR: Failed to initialize guidance structure (jq error)\" >&2\n        rm -f \"$CONFIG_FILE.tmp\"\n        exit 1\n    fi\n    mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\nfi\n\n# Handle commands\ncase \"$ARGS\" in\n    \"--list\"|\"-l\")\n        echo \"Current encouraged items:\"\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        COUNT=$(jq -r '.guidance.encouraged | length' \"$CONFIG_FILE\")\n        echo \"\"\n        echo \"Total: $COUNT items\"\n        ;;\n    \"--clear\"|\"-c\")\n        if ! jq '.guidance.encouraged = []' \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to clear encouraged items (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Cleared all encouraged items\"\n        ;;\n    \"--remove\"|\"-r\")\n        # Interactive removal - list items for AUQ picker\n        COUNT=$(jq -r '.guidance.encouraged | length' \"$CONFIG_FILE\")\n        if [[ \"$COUNT\" -eq 0 ]]; then\n            echo \"No encouraged items to remove.\"\n            exit 0\n        fi\n        echo \"REMOVE_MODE=interactive\"\n        echo \"Select items to remove from encouraged list:\"\n        echo \"\"\n        INDEX=0\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"[$INDEX] $item\"\n            INDEX=$((INDEX + 1))\n        done\n        echo \"\"\n        echo \"Use AskUserQuestion with multiSelect to let user pick items to remove.\"\n        echo \"Then call: /ralph:encourage --remove-by-index <indices>\"\n        ;;\n    --remove-by-index\\ *)\n        # Remove by comma-separated indices (e.g., --remove-by-index 0,2,3)\n        INDICES=\"${ARGS#--remove-by-index }\"\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        # Convert comma-separated to jq array deletion\n        # Build jq filter to delete indices in reverse order (to preserve index validity)\n        SORTED_INDICES=$(echo \"$INDICES\" | tr ',' '\\n' | sort -rn | tr '\\n' ' ')\n        for IDX in $SORTED_INDICES; do\n            if ! jq --argjson idx \"$IDX\" --arg ts \"$TS\" \\\n                '.guidance.encouraged |= (to_entries | map(select(.key != $idx)) | map(.value)) | .guidance.timestamp = $ts' \\\n                \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n                echo \"ERROR: Failed to remove item at index $IDX (jq error)\" >&2\n                rm -f \"$CONFIG_FILE.tmp\"\n                exit 1\n            fi\n            mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        done\n        echo \"Removed items at indices: $INDICES\"\n        echo \"\"\n        echo \"Remaining encouraged items:\"\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    --remove\\ *)\n        # Fuzzy removal by phrase\n        PHRASE=\"${ARGS#--remove }\"\n        # Find best match using case-insensitive substring\n        MATCH=$(jq -r --arg phrase \"$PHRASE\" \\\n            '.guidance.encouraged[] | select(. | ascii_downcase | contains($phrase | ascii_downcase))' \\\n            \"$CONFIG_FILE\" | head -1)\n        if [[ -z \"$MATCH\" ]]; then\n            echo \"No encouraged item matches: $PHRASE\"\n            echo \"\"\n            echo \"Current encouraged items:\"\n            jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n                echo \"   $item\"\n            done\n            exit 1\n        fi\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        if ! jq --arg match \"$MATCH\" --arg ts \"$TS\" \\\n            '.guidance.encouraged |= map(select(. != $match)) | .guidance.timestamp = $ts' \\\n            \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to remove encouraged item (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Removed from encouraged list: $MATCH\"\n        echo \"(matched phrase: $PHRASE)\"\n        echo \"\"\n        echo \"Remaining encouraged items:\"\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    \"\")\n        echo \"Usage: /ralph:encourage <phrase> | --list | --clear | --remove [phrase]\"\n        echo \"\"\n        echo \"Current encouraged items:\"\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    *)\n        # Add item to encouraged list (deduplicated) with timestamp\n        # ADR: /docs/adr/2026-01-02-ralph-guidance-freshness-detection.md\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        if ! jq --arg item \"$ARGS\" --arg ts \"$TS\" \\\n            '.guidance.encouraged = ((.guidance.encouraged // []) + [$item] | unique) | .guidance.timestamp = $ts' \\\n            \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to add encouraged item (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Added to encouraged list: $ARGS\"\n        echo \"\"\n        echo \"Effect: Will apply on next iteration (Stop hook reads config fresh)\"\n        echo \"\"\n        echo \"Current encouraged items:\"\n        jq -r '.guidance.encouraged[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\nesac\nRALPH_ENCOURAGE_SCRIPT\n```\n\nRun the bash script above to manage encouraged items.\n\n## How It Works\n\n1. **Immediate config update**: Changes are written to `.claude/ralph-config.json`\n2. **Next iteration applies**: The Stop hook reads config fresh on each message end\n3. **All phases**: Guidance appears in both implementation and exploration phases (unified template)\n4. **Priority override**: Encouraged items override forbidden patterns during filtering\n5. **Persistent**: Settings persist until cleared or session ends\n\n## Interactive Removal (--remove)\n\nWhen the user runs `/ralph:encourage --remove` without a phrase, use `AskUserQuestion` with `multiSelect: true`:\n\n1. Run the bash script to get the list of items with indices\n2. Parse the output to extract items\n3. Present items as options in AskUserQuestion\n4. After user selects, run `/ralph:encourage --remove-by-index <comma-separated-indices>`\n\n**Example AskUserQuestion**:\n```json\n{\n  \"question\": \"Which encouraged items do you want to remove?\",\n  \"header\": \"Remove items\",\n  \"options\": [\n    {\"label\": \"Sharpe ratio optimization\", \"description\": \"Index 0\"},\n    {\"label\": \"Risk-adjusted returns\", \"description\": \"Index 1\"}\n  ],\n  \"multiSelect\": true\n}\n```\n\n**Fuzzy matching** (`--remove <phrase>`): Finds first item containing the phrase (case-insensitive).\n\n## Template Rendering (v8.7.0+)\n\nThe unified Ralph template (`ralph-unified.md`) renders guidance in the `## USER GUIDANCE` section:\n\n```markdown\n### ENCOURAGED (User Priorities)\n\n**Focus your work on these high-value areas:**\n\n1. **Your first encouraged item**\n2. **Your second encouraged item**\n\n These override forbidden patterns.\n```\n\nThis section appears **regardless of phase** (implementation or exploration), ensuring your priorities are always visible to Claude."
              },
              {
                "name": "/forbid",
                "description": "Add item to forbidden list mid-loop",
                "path": "plugins/ralph/commands/forbid.md",
                "frontmatter": {
                  "description": "Add item to forbidden list mid-loop",
                  "allowed-tools": "Bash, AskUserQuestion",
                  "argument-hint": "<phrase> | --list | --clear | --remove (live: HARD BLOCKS next iteration)"
                },
                "content": "# Ralph Loop: Forbid\n\nAdd items to the forbidden list during an active loop session.\nForbidden items are HARD BLOCKED from opportunity discovery (not just skipped).\n\n**Runtime configurable**: Works with or without active Ralph loop. Changes apply on next iteration.\n\n## Usage\n\n- `/ralph:forbid documentation updates` - Add \"documentation updates\" to forbidden list\n- `/ralph:forbid --list` - Show current forbidden items\n- `/ralph:forbid --clear` - Clear all forbidden items\n- `/ralph:forbid --remove` - Interactive picker to remove specific items\n- `/ralph:forbid --remove <phrase>` - Remove item matching phrase (fuzzy)\n\n## Execution\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility\n/usr/bin/env bash << 'RALPH_FORBID_SCRIPT'\n# RALPH_FORBID_SCRIPT marker - required for PreToolUse hook bypass\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\nSTATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\n\n# Get arguments (everything after the command)\nARGS=\"${ARGUMENTS:-}\"\n\n# Ensure config file exists with guidance structure\nif [[ ! -f \"$CONFIG_FILE\" ]]; then\n    echo '{\"guidance\": {\"forbidden\": [], \"encouraged\": []}}' > \"$CONFIG_FILE\"\nfi\n\n# Ensure guidance structure exists\nif ! jq -e '.guidance' \"$CONFIG_FILE\" >/dev/null 2>&1; then\n    if ! jq '. + {guidance: {forbidden: [], encouraged: []}}' \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n        echo \"ERROR: Failed to initialize guidance structure (jq error)\" >&2\n        rm -f \"$CONFIG_FILE.tmp\"\n        exit 1\n    fi\n    mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\nfi\n\n# Handle commands\ncase \"$ARGS\" in\n    \"--list\"|\"-l\")\n        echo \"Current forbidden items (HARD BLOCKED):\"\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        COUNT=$(jq -r '.guidance.forbidden | length' \"$CONFIG_FILE\")\n        echo \"\"\n        echo \"Total: $COUNT items\"\n        ;;\n    \"--clear\"|\"-c\")\n        if ! jq '.guidance.forbidden = []' \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to clear forbidden items (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Cleared all forbidden items\"\n        ;;\n    \"--remove\"|\"-r\")\n        # Interactive removal - list items for AUQ picker\n        COUNT=$(jq -r '.guidance.forbidden | length' \"$CONFIG_FILE\")\n        if [[ \"$COUNT\" -eq 0 ]]; then\n            echo \"No forbidden items to remove.\"\n            exit 0\n        fi\n        echo \"REMOVE_MODE=interactive\"\n        echo \"Select items to remove from forbidden list:\"\n        echo \"\"\n        INDEX=0\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"[$INDEX] $item\"\n            INDEX=$((INDEX + 1))\n        done\n        echo \"\"\n        echo \"Use AskUserQuestion with multiSelect to let user pick items to remove.\"\n        echo \"Then call: /ralph:forbid --remove-by-index <indices>\"\n        ;;\n    --remove-by-index\\ *)\n        # Remove by comma-separated indices (e.g., --remove-by-index 0,2,3)\n        INDICES=\"${ARGS#--remove-by-index }\"\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        # Convert comma-separated to jq array deletion\n        # Build jq filter to delete indices in reverse order (to preserve index validity)\n        SORTED_INDICES=$(echo \"$INDICES\" | tr ',' '\\n' | sort -rn | tr '\\n' ' ')\n        for IDX in $SORTED_INDICES; do\n            if ! jq --argjson idx \"$IDX\" --arg ts \"$TS\" \\\n                '.guidance.forbidden |= (to_entries | map(select(.key != $idx)) | map(.value)) | .guidance.timestamp = $ts' \\\n                \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n                echo \"ERROR: Failed to remove item at index $IDX (jq error)\" >&2\n                rm -f \"$CONFIG_FILE.tmp\"\n                exit 1\n            fi\n            mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        done\n        echo \"Removed items at indices: $INDICES\"\n        echo \"\"\n        echo \"Remaining forbidden items:\"\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    --remove\\ *)\n        # Fuzzy removal by phrase\n        PHRASE=\"${ARGS#--remove }\"\n        # Find best match using case-insensitive substring\n        MATCH=$(jq -r --arg phrase \"$PHRASE\" \\\n            '.guidance.forbidden[] | select(. | ascii_downcase | contains($phrase | ascii_downcase))' \\\n            \"$CONFIG_FILE\" | head -1)\n        if [[ -z \"$MATCH\" ]]; then\n            echo \"No forbidden item matches: $PHRASE\"\n            echo \"\"\n            echo \"Current forbidden items:\"\n            jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n                echo \"   $item\"\n            done\n            exit 1\n        fi\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        if ! jq --arg match \"$MATCH\" --arg ts \"$TS\" \\\n            '.guidance.forbidden |= map(select(. != $match)) | .guidance.timestamp = $ts' \\\n            \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to remove forbidden item (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Removed from forbidden list: $MATCH\"\n        echo \"(matched phrase: $PHRASE)\"\n        echo \"\"\n        echo \"Remaining forbidden items:\"\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    \"\")\n        echo \"Usage: /ralph:forbid <phrase> | --list | --clear | --remove [phrase]\"\n        echo \"\"\n        echo \"Current forbidden items (HARD BLOCKED):\"\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\n    *)\n        # Add item to forbidden list (deduplicated) with timestamp\n        # ADR: /docs/adr/2026-01-02-ralph-guidance-freshness-detection.md\n        TS=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n        if ! jq --arg item \"$ARGS\" --arg ts \"$TS\" \\\n            '.guidance.forbidden = ((.guidance.forbidden // []) + [$item] | unique) | .guidance.timestamp = $ts' \\\n            \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\"; then\n            echo \"ERROR: Failed to add forbidden item (jq error)\" >&2\n            rm -f \"$CONFIG_FILE.tmp\"\n            exit 1\n        fi\n        mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n        echo \"Added to forbidden list: $ARGS\"\n        echo \"\"\n        echo \"Effect: Will HARD BLOCK on next iteration (Stop hook reads config fresh)\"\n        echo \"Note: User-forbidden items get FilterResult.BLOCK, not SKIP\"\n        echo \"\"\n        echo \"Current forbidden items:\"\n        jq -r '.guidance.forbidden[]?' \"$CONFIG_FILE\" | while read -r item; do\n            echo \"   $item\"\n        done\n        ;;\nesac\nRALPH_FORBID_SCRIPT\n```\n\nRun the bash script above to manage forbidden items.\n\n## How It Works\n\n1. **Immediate config update**: Changes are written to `.claude/ralph-config.json`\n2. **Next iteration applies**: The Stop hook reads config fresh on each message end\n3. **All phases**: Guidance appears in both implementation and exploration phases (unified template)\n4. **HARD BLOCK**: User-forbidden items get `FilterResult.BLOCK` (not SKIP)\n5. **Cannot be fallback**: Unlike built-in busywork, user-forbidden items cannot be chosen as fallback\n6. **Persistent**: Settings persist until cleared or session ends\n\n## Interactive Removal (--remove)\n\nWhen the user runs `/ralph:forbid --remove` without a phrase, use `AskUserQuestion` with `multiSelect: true`:\n\n1. Run the bash script to get the list of items with indices\n2. Parse the output to extract items\n3. Present items as options in AskUserQuestion\n4. After user selects, run `/ralph:forbid --remove-by-index <comma-separated-indices>`\n\n**Example AskUserQuestion**:\n```json\n{\n  \"question\": \"Which forbidden items do you want to remove?\",\n  \"header\": \"Remove items\",\n  \"options\": [\n    {\"label\": \"documentation updates\", \"description\": \"Index 0\"},\n    {\"label\": \"refactoring\", \"description\": \"Index 1\"}\n  ],\n  \"multiSelect\": true\n}\n```\n\n**Fuzzy matching** (`--remove <phrase>`): Finds first item containing the phrase (case-insensitive).\n\n## Template Rendering (v8.7.0+)\n\nThe unified Ralph template (`ralph-unified.md`) renders forbidden items in the `## USER GUIDANCE` section:\n\n```markdown\n### FORBIDDEN (User-Defined)\n\n**YOU SHALL NOT work on:**\n\n- Your first forbidden item\n- Your second forbidden item\n\n These are user-specified constraints.\n```\n\nThis section appears **regardless of phase** (implementation or exploration), ensuring your constraints are always enforced.\n\n## Difference from Built-in Busywork\n\n| Type              | Filter Result    | Behavior                             |\n| ----------------- | ---------------- | ------------------------------------ |\n| Built-in busywork | SKIP             | Soft-skip, can be chosen as fallback |\n| User-forbidden    | BLOCK            | Hard-block, cannot be chosen at all  |\n| User-encouraged   | ALLOW (priority) | Always allowed, overrides forbidden  |"
              },
              {
                "name": "/hooks",
                "description": "Install/uninstall ralph hooks to ~/.claude/settings.json",
                "path": "plugins/ralph/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall ralph hooks to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead",
                  "argument-hint": "[install|uninstall|status]"
                },
                "content": "# Ralph Hooks Manager\n\nManage ralph loop hooks installation in `~/.claude/settings.json`.\n\nClaude Code only loads hooks from settings.json, not from plugin hooks.json files. This command installs/uninstalls the ralph Stop and PreToolUse hooks that enable autonomous loop mode.\n\n## Actions\n\n| Action      | Description                                      |\n| ----------- | ------------------------------------------------ |\n| `status`    | Comprehensive preflight check (deps, paths, etc) |\n| `install`   | Add ralph hooks to settings.json                 |\n| `uninstall` | Remove ralph hooks from settings.json            |\n\n## Execution\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_HOOKS_SCRIPT'\nset -euo pipefail\n\nACTION=\"${ARGUMENTS:-status}\"\nSETTINGS=\"$HOME/.claude/settings.json\"\nINSTALL_TS_FILE=\"$HOME/.claude/ralph-hooks-installed-at\"\nMARKER=\"ralph/hooks/\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nCYAN='\\033[0;36m'\nNC='\\033[0m'\n\n# Auto-detect plugin root (same logic as manage-hooks.sh)\ndetect_plugin_root() {\n    if [[ -n \"${CLAUDE_PLUGIN_ROOT:-}\" ]]; then\n        echo \"$CLAUDE_PLUGIN_ROOT\"\n        return\n    fi\n    local marketplace=\"$HOME/.claude/plugins/marketplaces/cc-skills/plugins/ralph\"\n    if [[ -d \"$marketplace/hooks\" ]]; then\n        echo \"$marketplace\"\n        return\n    fi\n    local cache_base=\"$HOME/.claude/plugins/cache/cc-skills/ralph\"\n    if [[ -d \"$cache_base\" ]]; then\n        local latest\n        latest=$(ls -1 \"$cache_base\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+' | sort -V | tail -1)\n        if [[ -n \"$latest\" && -d \"$cache_base/$latest/hooks\" ]]; then\n            echo \"$cache_base/$latest\"\n            return\n        fi\n    fi\n    echo \"\"\n}\n\n# Comprehensive preflight check\ndo_preflight() {\n    local errors=0\n    local warnings=0\n    local PLUGIN_ROOT\n    PLUGIN_ROOT=\"$(detect_plugin_root)\"\n\n    echo -e \"${CYAN}=== Ralph Hooks Preflight Check ===${NC}\"\n    echo \"\"\n\n    # 1. Plugin Root Detection\n    echo -e \"${CYAN}Plugin Location:${NC}\"\n    if [[ -n \"$PLUGIN_ROOT\" && -d \"$PLUGIN_ROOT\" ]]; then\n        echo -e \"  ${GREEN}${NC} $PLUGIN_ROOT\"\n    else\n        echo -e \"  ${RED}${NC} Could not detect plugin installation\"\n        echo -e \"      Expected: marketplace, cache, or CLAUDE_PLUGIN_ROOT\"\n        ((errors++))\n    fi\n    echo \"\"\n\n    # 2. Dependency Checks\n    echo -e \"${CYAN}Dependencies:${NC}\"\n\n    # Check jq\n    if command -v jq &>/dev/null; then\n        echo -e \"  ${GREEN}${NC} jq $(jq --version 2>/dev/null | head -1)\"\n    else\n        echo -e \"  ${RED}${NC} jq - REQUIRED. Install: brew install jq\"\n        ((errors++))\n    fi\n\n    # Check uv\n    if command -v uv &>/dev/null; then\n        echo -e \"  ${GREEN}${NC} uv $(uv --version 2>/dev/null | awk '{print $2}')\"\n    else\n        echo -e \"  ${RED}${NC} uv - REQUIRED. Install: brew install uv\"\n        ((errors++))\n    fi\n\n    # Check Python version\n    local py_version\n    py_version=$(python3 -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")' 2>/dev/null || echo \"\")\n    if [[ -n \"$py_version\" ]]; then\n        local major=\"${py_version%%.*}\"\n        local minor=\"${py_version#*.}\"\n        if [[ \"$major\" -ge 3 && \"$minor\" -ge 11 ]]; then\n            echo -e \"  ${GREEN}${NC} Python $py_version\"\n        else\n            echo -e \"  ${RED}${NC} Python $py_version - REQUIRES 3.11+\"\n            ((errors++))\n        fi\n    else\n        echo -e \"  ${RED}${NC} Python - not found\"\n        ((errors++))\n    fi\n    echo \"\"\n\n    # 3. Hook Script Checks (3 hooks total)\n    echo -e \"${CYAN}Hook Scripts:${NC}\"\n    if [[ -n \"$PLUGIN_ROOT\" ]]; then\n        local stop_hook=\"$PLUGIN_ROOT/hooks/loop-until-done.py\"\n        local archive_hook=\"$PLUGIN_ROOT/hooks/archive-plan.sh\"\n        local guard_hook=\"$PLUGIN_ROOT/hooks/pretooluse-loop-guard.py\"\n\n        # Stop hook\n        if [[ -f \"$stop_hook\" ]]; then\n            if [[ -x \"$stop_hook\" ]]; then\n                echo -e \"  ${GREEN}${NC} loop-until-done.py (Stop)\"\n            else\n                echo -e \"  ${YELLOW}${NC} loop-until-done.py (not executable)\"\n                ((warnings++))\n            fi\n        else\n            echo -e \"  ${RED}${NC} loop-until-done.py - NOT FOUND\"\n            ((errors++))\n        fi\n\n        # PreToolUse: archive-plan.sh\n        if [[ -f \"$archive_hook\" ]]; then\n            if [[ -x \"$archive_hook\" ]]; then\n                echo -e \"  ${GREEN}${NC} archive-plan.sh (PreToolUse)\"\n            else\n                echo -e \"  ${YELLOW}${NC} archive-plan.sh (not executable)\"\n                ((warnings++))\n            fi\n        else\n            echo -e \"  ${RED}${NC} archive-plan.sh - NOT FOUND\"\n            ((errors++))\n        fi\n\n        # PreToolUse: pretooluse-loop-guard.py\n        if [[ -f \"$guard_hook\" ]]; then\n            if [[ -x \"$guard_hook\" ]]; then\n                echo -e \"  ${GREEN}${NC} pretooluse-loop-guard.py (PreToolUse)\"\n            else\n                echo -e \"  ${YELLOW}${NC} pretooluse-loop-guard.py (not executable)\"\n                ((warnings++))\n            fi\n        else\n            echo -e \"  ${RED}${NC} pretooluse-loop-guard.py - NOT FOUND\"\n            ((errors++))\n        fi\n    else\n        echo -e \"  ${RED}${NC} Cannot check - plugin root unknown\"\n        ((errors++))\n    fi\n    echo \"\"\n\n    # 4. Hook Registration Check\n    echo -e \"${CYAN}Hook Registration:${NC}\"\n    local hook_count=0\n    if [[ -f \"$SETTINGS\" ]] && command -v jq &>/dev/null; then\n        hook_count=$(jq '[.hooks | to_entries[]? | .value[]? | .hooks[]? | select(.command | contains(\"'\"$MARKER\"'\"))] | length' \"$SETTINGS\" 2>/dev/null || echo \"0\")\n    fi\n\n    if [[ \"$hook_count\" -gt 0 ]]; then\n        echo -e \"  ${GREEN}${NC} $hook_count hook(s) registered in settings.json\"\n        # Show which hooks\n        jq -r '.hooks | to_entries[]? | select(.value[]? | .hooks[]? | .command | contains(\"'\"$MARKER\"'\")) | \"      - \\(.key)\"' \"$SETTINGS\" 2>/dev/null | sort -u\n    else\n        echo -e \"  ${RED}${NC} No hooks registered\"\n        echo -e \"      Run: /ralph:hooks install\"\n        ((errors++))\n    fi\n    echo \"\"\n\n    # 5. Session Restart Detection (Critical)\n    echo -e \"${CYAN}Session Status:${NC}\"\n    if [[ -f \"$INSTALL_TS_FILE\" ]]; then\n        local install_ts\n        install_ts=$(cat \"$INSTALL_TS_FILE\")\n\n        # Get session start time from .claude directory mtime as proxy\n        local session_ts\n        session_ts=$(stat -f %m \"$HOME/.claude\" 2>/dev/null || stat -c %Y \"$HOME/.claude\" 2>/dev/null || echo \"0\")\n\n        # Also check projects dir which changes more frequently\n        local projects_dir=\"$HOME/.claude/projects\"\n        if [[ -d \"$projects_dir\" ]]; then\n            local projects_ts\n            projects_ts=$(stat -f %m \"$projects_dir\" 2>/dev/null || stat -c %Y \"$projects_dir\" 2>/dev/null || echo \"0\")\n            if [[ \"$projects_ts\" -gt \"$session_ts\" ]]; then\n                session_ts=\"$projects_ts\"\n            fi\n        fi\n\n        if [[ \"$install_ts\" -gt \"$session_ts\" ]]; then\n            echo -e \"  ${RED}${NC} Hooks installed AFTER session started!\"\n            local install_date\n            install_date=$(date -r \"$install_ts\" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || date -d \"@$install_ts\" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || echo \"unknown\")\n            echo -e \"      Installed at: $install_date\"\n            echo -e \"      ${YELLOW}ACTION: Restart Claude Code for hooks to activate${NC}\"\n            ((errors++))\n        else\n            echo -e \"  ${GREEN}${NC} Hooks installed before this session\"\n        fi\n    else\n        if [[ \"$hook_count\" -gt 0 ]]; then\n            echo -e \"  ${YELLOW}${NC} No install timestamp (legacy install)\"\n            echo -e \"      Consider re-running: /ralph:hooks install\"\n            ((warnings++))\n        else\n            echo -e \"  ${CYAN}${NC} No hooks installed yet\"\n        fi\n    fi\n    echo \"\"\n\n    # Summary\n    echo -e \"${CYAN}=== Summary ===${NC}\"\n    if [[ \"$errors\" -eq 0 && \"$warnings\" -eq 0 ]]; then\n        echo -e \"${GREEN}All preflight checks passed!${NC}\"\n        echo \"Ralph is ready. Run: /ralph:start\"\n    elif [[ \"$errors\" -eq 0 ]]; then\n        echo -e \"${YELLOW}$warnings warning(s), but system is usable${NC}\"\n    else\n        echo -e \"${RED}$errors error(s) must be fixed before using Ralph${NC}\"\n    fi\n    echo \"\"\n\n    # Documentation Links (always show)\n    echo -e \"${CYAN}=== Documentation ===${NC}\"\n    echo -e \"  ${GREEN}${NC} README (Hook Architecture, Safety Features):\"\n    echo -e \"    https://github.com/terrylica/cc-skills/blob/main/plugins/ralph/README.md\"\n    echo \"\"\n    echo -e \"  ${GREEN}${NC} Mental Model (Alpha Forge ML Research Workflows):\"\n    echo -e \"    https://github.com/terrylica/cc-skills/blob/main/plugins/ralph/MENTAL-MODEL.md\"\n    echo \"\"\n\n    if [[ \"$errors\" -gt 0 ]]; then\n        return 1\n    fi\n    return 0\n}\n\n# Route action\ncase \"$ACTION\" in\n    status)\n        do_preflight\n        ;;\n    install|uninstall)\n        PLUGIN_DIR=\"$(detect_plugin_root)\"\n        if [[ -z \"$PLUGIN_DIR\" ]]; then\n            echo -e \"${RED}ERROR:${NC} Cannot detect plugin installation\" >&2\n            exit 1\n        fi\n        bash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" \"$ACTION\"\n        ;;\n    *)\n        echo \"Usage: /ralph:hooks [install|uninstall|status]\"\n        exit 1\n        ;;\nesac\nRALPH_HOOKS_SCRIPT\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe hooks are loaded at session start. Modifications to settings.json require a restart."
              },
              {
                "name": "/start",
                "description": "Enable autonomous loop mode for long-running tasks",
                "path": "plugins/ralph/commands/start.md",
                "frontmatter": {
                  "description": "Enable autonomous loop mode for long-running tasks",
                  "allowed-tools": "Read, Write, Bash, AskUserQuestion, Glob",
                  "argument-hint": "[-f <file>] [--poc | --production] [--no-focus] [<task description>...]"
                },
                "content": "# Ralph Loop: Start\n\nEnable the Ralph Wiggum autonomous improvement loop. Claude will continue working until:\n\n- Task is truly complete (respecting minimum time/iteration thresholds)\n- Maximum time limit reached (default: 9 hours)\n- Maximum iterations reached (default: 99)\n- Kill switch activated (`.claude/STOP_LOOP` file created)\n\n## Arguments\n\n- `-f <file>`: Specify target file for completion tracking (plan, spec, or ADR)\n- `--poc`: Use proof-of-concept settings (5 min / 10 min limits, 10/20 iterations)\n- `--production`: Use production settings (4h / 9h limits, 50/99 iterations) - skips preset prompt\n- `--no-focus`: Skip focus file tracking (100% autonomous, no plan file)\n- `--skip-constraint-scan`: Skip constraint scanner (power users, v3.0.0+)\n- `<task description>`: Natural language task prompt (remaining text after flags)\n\n## Step 1: Focus File Discovery (Auto-Select)\n\nDiscover and auto-select focus files WITHOUT prompting the user (autonomous mode):\n\n1. **Check for explicit file**: If `-f <file>` was provided, use that path. Skip to Step 2.\n\n2. **Check for --no-focus flag**: If `--no-focus` is present, skip to Step 2 with `NO_FOCUS=true`.\n\n3. **Auto-discover focus file** (if no explicit file):\n\n   **For Alpha Forge projects** (detected by `outputs/research_sessions/` existing):\n   - Auto-select up to 3 most recent `outputs/research_sessions/*/research_log.md` files\n   - NO user prompt - proceed directly to Step 2\n   - Store paths in config for the hook to read\n\n   **For other projects** ( hooks will skip  see [Alpha-Forge Exclusivity](../README.md#alpha-forge-exclusivity-v802)), discover in priority order:\n   - Plan mode system-reminder (if in plan mode, the system-assigned plan file)\n   - ITP design specs with `implementation-status: in_progress` in `docs/design/*/spec.md`\n   - ITP ADRs with `status: accepted` in `docs/adr/*.md`\n   - Newest `.md` file in `.claude/plans/` (local or global)\n\n4. **Only prompt if truly ambiguous** (multiple ITP specs or ADRs with same priority):\n   - Use AskUserQuestion to let user choose\n   - Otherwise, auto-select the most recent file and proceed\n\n5. **If nothing discovered**: Proceed with `NO_FOCUS=true` (exploration mode)\n\n## Step 1.4: Constraint Scanning (Alpha Forge Only)\n\n**Purpose**: Detect environment constraints before loop starts. Results inform Ralph behavior.\n\n**Skip if**: `--skip-constraint-scan` flag provided (power users).\n\nRun the constraint scanner:\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility\n/usr/bin/env bash << 'CONSTRAINT_SCAN_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nARGS=\"${ARGUMENTS:-}\"\n\n# Check for skip flag\nif [[ \"$ARGS\" == *\"--skip-constraint-scan\"* ]]; then\n    echo \"Constraint scan: SKIPPED (--skip-constraint-scan flag)\"\n    exit 0\nfi\n\n# Find scanner script in plugin cache\nRALPH_CACHE=\"$HOME/.claude/plugins/cache/cc-skills/ralph\"\nSCANNER_SCRIPT=\"\"\n\nif [[ -d \"$RALPH_CACHE/local\" ]]; then\n    SCANNER_SCRIPT=\"$RALPH_CACHE/local/scripts/constraint-scanner.py\"\nelif [[ -d \"$RALPH_CACHE\" ]]; then\n    # Get highest version\n    RALPH_VERSION=$(ls \"$RALPH_CACHE\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+\\.[0-9]+$' | sort -V | tail -1)\n    if [[ -n \"$RALPH_VERSION\" ]]; then\n        SCANNER_SCRIPT=\"$RALPH_CACHE/$RALPH_VERSION/scripts/constraint-scanner.py\"\n    fi\nfi\n\n# Skip if scanner not found (older version without scanner)\nif [[ -z \"$SCANNER_SCRIPT\" ]] || [[ ! -f \"$SCANNER_SCRIPT\" ]]; then\n    echo \"Constraint scan: SKIPPED (scanner not found, upgrade to v9.2.0+)\"\n    exit 0\nfi\n\n# Run scanner - discover UV with same fallback pattern as main script\nUV_CMD=\"\"\ndiscover_uv() {\n    command -v uv &>/dev/null && echo \"uv\" && return 0\n    for loc in \"$HOME/.local/bin/uv\" \"$HOME/.cargo/bin/uv\" \"/opt/homebrew/bin/uv\" \"/usr/local/bin/uv\" \"$HOME/.local/share/mise/shims/uv\"; do\n        [[ -x \"$loc\" ]] && echo \"$loc\" && return 0\n    done\n    # Dynamic mise version discovery\n    local mise_base=\"$HOME/.local/share/mise/installs/uv\"\n    if [[ -d \"$mise_base\" ]]; then\n        local ver=$(ls -1 \"$mise_base\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+' | sort -V | tail -1)\n        if [[ -n \"$ver\" ]]; then\n            local plat=$(ls -1 \"$mise_base/$ver\" 2>/dev/null | head -1)\n            [[ -n \"$plat\" && -x \"$mise_base/$ver/$plat/uv\" ]] && echo \"$mise_base/$ver/$plat/uv\" && return 0\n            [[ -x \"$mise_base/$ver/uv\" ]] && echo \"$mise_base/$ver/uv\" && return 0\n        fi\n    fi\n    command -v mise &>/dev/null && mise which uv &>/dev/null 2>&1 && echo \"mise exec -- uv\" && return 0\n    return 1\n}\nUV_CMD=$(discover_uv) || { echo \"Constraint scan: SKIPPED (uv not found)\"; exit 0; }\n\necho \"Running constraint scanner...\"\nSCAN_OUTPUT=$($UV_CMD run -q \"$SCANNER_SCRIPT\" --project \"$PROJECT_DIR\" 2>&1)\nSCAN_EXIT=$?\n\nif [[ $SCAN_EXIT -eq 2 ]]; then\n    echo \"\"\n    echo \"========================================\"\n    echo \"  CRITICAL CONSTRAINTS DETECTED\"\n    echo \"========================================\"\n    echo \"\"\n    echo \"$SCAN_OUTPUT\" | jq -r '.constraints[] | select(.severity == \"critical\") | \"   \\(.description)\"' 2>/dev/null || echo \"$SCAN_OUTPUT\"\n    echo \"\"\n    echo \"Action: Address critical constraints before starting loop.\"\n    echo \"        Use --skip-constraint-scan to bypass (not recommended).\"\n    exit 2\nelif [[ $SCAN_EXIT -eq 0 ]]; then\n    # Parse and display summary\n    CRITICAL_COUNT=$(echo \"$SCAN_OUTPUT\" | jq '[.constraints[] | select(.severity == \"critical\")] | length' 2>/dev/null || echo \"0\")\n    HIGH_COUNT=$(echo \"$SCAN_OUTPUT\" | jq '[.constraints[] | select(.severity == \"high\")] | length' 2>/dev/null || echo \"0\")\n    TOTAL_COUNT=$(echo \"$SCAN_OUTPUT\" | jq '.constraints | length' 2>/dev/null || echo \"0\")\n\n    echo \"Constraint scan complete:\"\n    echo \"  Critical: $CRITICAL_COUNT | High: $HIGH_COUNT | Total: $TOTAL_COUNT\"\n\n    # Save results for AUQ to read (NDJSON format with .jsonl extension)\n    mkdir -p \"$PROJECT_DIR/.claude\"\n    echo \"$SCAN_OUTPUT\" > \"$PROJECT_DIR/.claude/ralph-constraint-scan.jsonl\"\nelse\n    echo \"Constraint scan: WARNING (scanner returned exit code $SCAN_EXIT)\"\n    echo \"$SCAN_OUTPUT\" | head -5\nfi\nCONSTRAINT_SCAN_SCRIPT\n```\n\nIf the scanner exits with code 2 (critical constraints), stop and inform user.\n\n## Step 1.4.5: Explore-Based Constraint Discovery\n\n**Purpose**: Spawn Explore agents to discover constraints the static scanner cannot detect.\n\n**MANDATORY Skill tool call: `ralph:constraint-discovery`**  activate NOW.\n\nThis skill spawns 5 parallel Explore agents that:\n\n1. Read project memory files (CLAUDE.md, .claude/, ROADMAP.md, docs/adr/)\n2. Follow ALL @ links with unlimited depth\n3. Extract constraints from all discovered files\n4. Return NDJSON with source tracking\n\n---\n\n## Step 1.4.6: BLOCKING GATE - Collect Agent Results\n\n** MANDATORY: Do NOT proceed to Step 1.5 until this gate passes**\n\nClaude MUST execute TaskOutput calls with `block: true` for each agent:\n\n```\nFor EACH agent spawned by constraint-discovery skill:\n  TaskOutput(task_id: \"<agent_id>\", block: true, timeout: 30000)\n```\n\n**Wait for ALL 5 agents** (or timeout after 30s each). Extract NDJSON constraints from each agent's output.\n\n**Merge agent findings** into constraint scan file:\n\n```bash\n/usr/bin/env bash << 'AGENT_MERGE_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nSCAN_FILE=\"$PROJECT_DIR/.claude/ralph-constraint-scan.jsonl\"\n\n# Claude MUST append each agent's NDJSON findings here:\n# For each constraint JSON from agent output:\n#   echo '{\"_type\":\"constraint\",\"source\":\"agent-env\",\"severity\":\"HIGH\",\"description\":\"...\"}' >> \"$SCAN_FILE\"\n\necho \"=== AGENT FINDINGS MERGED ===\"\necho \"Constraints in scan file:\"\nwc -l < \"$SCAN_FILE\" 2>/dev/null || echo \"0\"\nAGENT_MERGE_SCRIPT\n```\n\n**Gate verification**: Before proceeding, confirm:\n\n- [ ] All 5 TaskOutput calls completed (or timed out)\n- [ ] Agent findings appended to `.claude/ralph-constraint-scan.jsonl`\n- [ ] Can proceed to Step 1.5\n\n**If timeout on some agents**: Proceed with available results. Log which agents timed out.\n\n---\n\n## Step 1.5: Preset Confirmation (ALWAYS)\n\n**ALWAYS prompt for preset confirmation.** Flags pre-select the option but user confirms before execution.\n\n**If `--poc` flag was provided:**\n\nUse AskUserQuestion with questions:\n\n- question: \"Confirm loop configuration:\"\n  header: \"Preset\"\n  multiSelect: false\n  options:\n  - label: \"POC Mode (Recommended)\"\n    description: \"5min-10min, 10-20 iterations - selected via --poc flag\"\n  - label: \"Production Mode\"\n    description: \"4h-9h, 50-99 iterations\"\n  - label: \"Custom\"\n    description: \"Specify your own time/iteration limits\"\n\n**If `--production` flag was provided:**\n\nUse AskUserQuestion with questions:\n\n- question: \"Confirm loop configuration:\"\n  header: \"Preset\"\n  multiSelect: false\n  options:\n  - label: \"Production Mode (Recommended)\"\n    description: \"4h-9h, 50-99 iterations - selected via --production flag\"\n  - label: \"POC Mode\"\n    description: \"5min-10min, 10-20 iterations\"\n  - label: \"Custom\"\n    description: \"Specify your own time/iteration limits\"\n\n**If no preset flag was provided:**\n\nUse AskUserQuestion with questions:\n\n- question: \"Select loop configuration preset:\"\n  header: \"Preset\"\n  multiSelect: false\n  options:\n  - label: \"Production Mode (Recommended)\"\n    description: \"4h-9h, 50-99 iterations - standard autonomous work\"\n  - label: \"POC Mode (Fast)\"\n    description: \"5min-10min, 10-20 iterations - ideal for testing\"\n  - label: \"Custom\"\n    description: \"Specify your own time/iteration limits\"\n\nBased on selection:\n\n- **\"Production Mode\"**  Proceed to Step 1.6 (if Alpha Forge) or Step 2 with production defaults\n- **\"POC Mode\"**  Proceed to Step 1.6 (if Alpha Forge) or Step 2 with POC settings\n- **\"Custom\"**  Ask follow-up questions for time/iteration limits:\n\n  Use AskUserQuestion with questions:\n  - question: \"Select time limits:\"\n    header: \"Time\"\n    multiSelect: false\n    options:\n    - label: \"1h - 2h\"\n      description: \"Short session\"\n    - label: \"2h - 4h\"\n      description: \"Medium session\"\n    - label: \"4h - 9h (Production)\"\n      description: \"Standard session\"\n\n  - question: \"Select iteration limits:\"\n    header: \"Iterations\"\n    multiSelect: false\n    options:\n    - label: \"10 - 20\"\n      description: \"Quick test\"\n    - label: \"25 - 50\"\n      description: \"Medium session\"\n    - label: \"50 - 99 (Production)\"\n      description: \"Standard session\"\n\n## Step 1.6: Session Guidance (Alpha Forge Only)\n\n**Only for Alpha Forge projects** (detected by adapter). Other projects skip to Step 2.\n\n**MANDATORY Skill tool call: `ralph:session-guidance`**  activate NOW.\n\nThis skill handles the complete guidance configuration workflow:\n\n1. Check for previous guidance (keep/reconfigure decision)\n2. Load constraint scan results (NDJSON with learned filtering)\n3. Forbidden items selection (dynamic from constraints + static fallbacks)\n4. Custom forbidden items (optional follow-up)\n5. Encouraged items selection (closed list)\n6. Custom encouraged items (optional follow-up)\n7. Write config with validation and learned behavior\n\n**If user selects \"Keep existing guidance\"**: Skill returns early, proceed to Step 2.\n\n**After skill completes**: Guidance is saved to `.claude/ralph-config.json`, proceed to Step 2.\n\n---\n\n## Step 2: Execution\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_START_SCRIPT'\n# Get project directory\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nSETTINGS=\"$HOME/.claude/settings.json\"\nMARKER=\"ralph/hooks/\"\n\n# ===== VERSION BANNER =====\n# Retrieve version from cache directory (source of truth: installed plugin version)\n# FAIL FAST: Exit if version cannot be determined (no fallbacks)\nRALPH_CACHE=\"$HOME/.claude/plugins/cache/cc-skills/ralph\"\nRALPH_VERSION=\"\"\nRALPH_SOURCE=\"cache\"\n\nif [[ -d \"$RALPH_CACHE\" ]]; then\n    # Check for 'local' directory first (development symlink takes priority)\n    if [[ -d \"$RALPH_CACHE/local\" ]]; then\n        RALPH_SOURCE=\"local\"\n        # Follow symlink to find source repo and read version from package.json\n        LOCAL_PATH=$(readlink -f \"$RALPH_CACHE/local\" 2>/dev/null || readlink \"$RALPH_CACHE/local\" 2>/dev/null)\n        if [[ -n \"$LOCAL_PATH\" ]]; then\n            # Navigate up from plugins/ralph to repo root to find package.json\n            REPO_ROOT=$(cd \"$LOCAL_PATH\" && cd ../.. && pwd 2>/dev/null)\n            if [[ -f \"$REPO_ROOT/package.json\" ]]; then\n                RALPH_VERSION=$(jq -r '.version // empty' \"$REPO_ROOT/package.json\" 2>/dev/null)\n            fi\n        fi\n    else\n        # Get highest semantic version from cache directories\n        RALPH_VERSION=$(ls \"$RALPH_CACHE\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+\\.[0-9]+$' | sort -V | tail -1)\n    fi\nfi\n\n# FAIL FAST: Version must be determined\nif [[ -z \"$RALPH_VERSION\" ]]; then\n    echo \"ERROR: Cannot determine Ralph version!\"\n    echo \"\"\n    echo \"Possible causes:\"\n    echo \"  1. Plugin not installed: Run /plugin install cc-skills\"\n    echo \"  2. Local symlink broken: Check ~/.claude/plugins/cache/cc-skills/ralph/local\"\n    echo \"  3. Missing package.json in source repo\"\n    echo \"\"\n    echo \"Cache directory: $RALPH_CACHE\"\n    echo \"Source: $RALPH_SOURCE\"\n    exit 1\nfi\n\necho \"========================================\"\necho \"  RALPH WIGGUM v${RALPH_VERSION} (${RALPH_SOURCE})\"\necho \"  Autonomous Loop Mode\"\necho \"========================================\"\necho \"\"\n\n# ===== UV DISCOVERY =====\n# Robust UV detection with multi-level fallback (matches canonical cc-skills pattern)\n# Returns UV command (path or \"mise exec -- uv\") on stdout, exits 1 if not found\nUV_CMD=\"\"\ndiscover_uv() {\n    # Priority 1: Already in PATH (shell configured, Homebrew, direct install)\n    if command -v uv &>/dev/null; then\n        echo \"uv\"\n        return 0\n    fi\n\n    # Priority 2: Common direct installation locations\n    local uv_locations=(\n        \"$HOME/.local/bin/uv\"                           # Official curl installer\n        \"$HOME/.cargo/bin/uv\"                           # cargo install\n        \"/opt/homebrew/bin/uv\"                          # Homebrew Apple Silicon\n        \"/usr/local/bin/uv\"                             # Homebrew Intel / manual\n        \"$HOME/.local/share/mise/shims/uv\"              # mise shims\n        \"$HOME/.local/share/mise/installs/uv/latest/uv\" # mise direct\n    )\n\n    for loc in \"${uv_locations[@]}\"; do\n        if [[ -x \"$loc\" ]]; then\n            echo \"$loc\"\n            return 0\n        fi\n    done\n\n    # Priority 3: Find mise-installed uv dynamically (version directories)\n    local mise_uv_base=\"$HOME/.local/share/mise/installs/uv\"\n    if [[ -d \"$mise_uv_base\" ]]; then\n        local latest_version\n        latest_version=$(ls -1 \"$mise_uv_base\" 2>/dev/null | grep -E '^[0-9]+\\.[0-9]+' | sort -V | tail -1)\n        if [[ -n \"$latest_version\" ]]; then\n            # Handle nested platform directory (e.g., uv-aarch64-apple-darwin/uv)\n            local platform_dir\n            platform_dir=$(ls -1 \"$mise_uv_base/$latest_version\" 2>/dev/null | head -1)\n            if [[ -n \"$platform_dir\" && -x \"$mise_uv_base/$latest_version/$platform_dir/uv\" ]]; then\n                echo \"$mise_uv_base/$latest_version/$platform_dir/uv\"\n                return 0\n            fi\n            # Direct binary\n            if [[ -x \"$mise_uv_base/$latest_version/uv\" ]]; then\n                echo \"$mise_uv_base/$latest_version/uv\"\n                return 0\n            fi\n        fi\n    fi\n\n    # Priority 4: Use mise exec as fallback\n    if command -v mise &>/dev/null && mise which uv &>/dev/null 2>&1; then\n        echo \"mise exec -- uv\"\n        return 0\n    fi\n\n    return 1\n}\n\n# Discover UV once at script start\nif ! UV_CMD=$(discover_uv); then\n    echo \"ERROR: 'uv' is required but not installed.\"\n    echo \"\"\n    echo \"The Stop hook uses 'uv run' to execute loop-until-done.py\"\n    echo \"\"\n    echo \"Searched locations:\"\n    echo \"  - PATH (command -v uv)\"\n    echo \"  - \\$HOME/.local/bin/uv\"\n    echo \"  - /opt/homebrew/bin/uv\"\n    echo \"  - \\$HOME/.local/share/mise/shims/uv\"\n    echo \"  - \\$HOME/.local/share/mise/installs/uv/*/...\"\n    echo \"\"\n    echo \"Install with one of:\"\n    echo \"   curl -LsSf https://astral.sh/uv/install.sh | sh\"\n    echo \"   brew install uv\"\n    echo \"   mise use -g uv@latest\"\n    exit 1\nfi\n\n# ===== STRICT PRE-FLIGHT CHECKS =====\n# These checks ensure the loop will actually work before starting\n\nINSTALL_TS_FILE=\"$HOME/.claude/ralph-hooks-installed-at\"\n\n# 1. Check if hooks were installed after session started (restart detection)\nif [[ -f \"$INSTALL_TS_FILE\" ]]; then\n    INSTALL_TS=$(cat \"$INSTALL_TS_FILE\")\n    # Use .claude dir mtime as session start proxy\n    SESSION_TS=$(stat -f %m \"$HOME/.claude\" 2>/dev/null || stat -c %Y \"$HOME/.claude\" 2>/dev/null || echo \"0\")\n    # Also check projects dir\n    if [[ -d \"$HOME/.claude/projects\" ]]; then\n        PROJECTS_TS=$(stat -f %m \"$HOME/.claude/projects\" 2>/dev/null || stat -c %Y \"$HOME/.claude/projects\" 2>/dev/null || echo \"0\")\n        if [[ \"$PROJECTS_TS\" -gt \"$SESSION_TS\" ]]; then\n            SESSION_TS=\"$PROJECTS_TS\"\n        fi\n    fi\n\n    if [[ \"$INSTALL_TS\" -gt \"$SESSION_TS\" ]]; then\n        echo \"ERROR: Hooks were installed AFTER this session started!\"\n        echo \"\"\n        echo \"The Stop hook won't run until you restart Claude Code.\"\n        echo \"Installed at: $(date -r \"$INSTALL_TS\" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || date -d \"@$INSTALL_TS\" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || echo \"unknown\")\"\n        echo \"\"\n        echo \"ACTION: Exit and restart Claude Code, then run /ralph:start again\"\n        exit 1\n    fi\nfi\n\n# 2. UV already verified by discover_uv() above - display discovered path\necho \"UV detected: $UV_CMD\"\n\n# 3. Verify Python 3.11+ (required for Stop hook)\nPY_VERSION=$(python3 -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")' 2>/dev/null || echo \"\")\nif [[ -n \"$PY_VERSION\" ]]; then\n    PY_MAJOR=\"${PY_VERSION%%.*}\"\n    PY_MINOR=\"${PY_VERSION#*.}\"\n    if [[ \"$PY_MAJOR\" -lt 3 ]] || [[ \"$PY_MAJOR\" -eq 3 && \"$PY_MINOR\" -lt 11 ]]; then\n        echo \"ERROR: Python 3.11+ required (found: $PY_VERSION)\"\n        echo \"\"\n        echo \"The Stop hook uses Python 3.11+ features.\"\n        echo \"\"\n        echo \"Upgrade with: brew upgrade python@3.11\"\n        exit 1\n    fi\nelse\n    echo \"ERROR: Python not found\"\n    echo \"\"\n    echo \"Install with: brew install python@3.11\"\n    exit 1\nfi\n\n# 4. Verify jq is available (required for config management)\nif ! command -v jq &>/dev/null; then\n    echo \"ERROR: 'jq' is required but not installed.\"\n    echo \"\"\n    echo \"Install with: brew install jq\"\n    exit 1\nfi\n\n# Check if hooks are installed\nHOOKS_INSTALLED=false\nif command -v jq &>/dev/null && [[ -f \"$SETTINGS\" ]]; then\n    HOOK_COUNT=$(jq '[.hooks | to_entries[] | .value[] | .hooks[] | select(.command | contains(\"'\"$MARKER\"'\"))] | length' \"$SETTINGS\" 2>/dev/null || echo \"0\")\n    if [[ \"$HOOK_COUNT\" -gt 0 ]]; then\n        HOOKS_INSTALLED=true\n    fi\nfi\n\n# Warn if hooks not installed\nif [[ \"$HOOKS_INSTALLED\" == \"false\" ]]; then\n    echo \"WARNING: Hooks not installed!\"\n    echo \"\"\n    echo \"The loop will NOT work without hooks registered.\"\n    echo \"\"\n    echo \"To fix:\"\n    echo \"  1. Run: /ralph:hooks install\"\n    echo \"  2. Restart Claude Code\"\n    echo \"  3. Run: /ralph:start again\"\n    echo \"\"\n    exit 1\nfi\n\n# ===== ARGUMENT PARSING =====\n# Syntax: /ralph:start [-f <file>] [--poc] [--no-focus] [<task description>...]\n\nARGS=\"${ARGUMENTS:-}\"\nTARGET_FILE=\"\"\nPOC_MODE=false\nNO_FOCUS=false\nTASK_PROMPT=\"\"\n\n# Extract -f flag with regex (handles paths without spaces)\nif [[ \"$ARGS\" =~ -f[[:space:]]+([^[:space:]]+) ]]; then\n    TARGET_FILE=\"${BASH_REMATCH[1]}\"\n    # Remove -f and path from ARGS for remaining processing\n    ARGS=\"${ARGS//-f ${TARGET_FILE}/}\"\nfi\n\n# Detect --poc flag\nif [[ \"$ARGS\" == *\"--poc\"* ]]; then\n    POC_MODE=true\n    ARGS=\"${ARGS//--poc/}\"\nfi\n\n# Detect --production flag (skips preset prompt, uses production defaults)\nPRODUCTION_MODE=false\nif [[ \"$ARGS\" == *\"--production\"* ]]; then\n    PRODUCTION_MODE=true\n    ARGS=\"${ARGS//--production/}\"\nfi\n\n# Detect --no-focus flag\nif [[ \"$ARGS\" == *\"--no-focus\"* ]]; then\n    NO_FOCUS=true\n    ARGS=\"${ARGS//--no-focus/}\"\nfi\n\n# Detect --skip-constraint-scan flag (v3.0.0+)\nSKIP_CONSTRAINT_SCAN=false\nif [[ \"$ARGS\" == *\"--skip-constraint-scan\"* ]]; then\n    SKIP_CONSTRAINT_SCAN=true\n    ARGS=\"${ARGS//--skip-constraint-scan/}\"\nfi\n\n# Remaining text after flags = task_prompt (trim whitespace)\nTASK_PROMPT=$(echo \"$ARGS\" | xargs 2>/dev/null || echo \"$ARGS\")\n\n# Resolve relative path to absolute\nif [[ -n \"$TARGET_FILE\" && \"$TARGET_FILE\" != /* ]]; then\n    TARGET_FILE=\"$PROJECT_DIR/$TARGET_FILE\"\nfi\n\n# Validate file exists (warn but continue)\nif [[ -n \"$TARGET_FILE\" && ! -e \"$TARGET_FILE\" ]]; then\n    echo \"WARNING: Target file does not exist: $TARGET_FILE\"\n    echo \"         Loop will proceed but file discovery may be used instead.\"\n    echo \"\"\nfi\n\n# ===== STATE MACHINE TRANSITION =====\n# State machine: STOPPED  RUNNING  DRAINING  STOPPED\nmkdir -p \"$PROJECT_DIR/.claude\"\n\n# Check current state (if any)\nSTATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\nCURRENT_STATE=\"stopped\"\nif [[ -f \"$STATE_FILE\" ]]; then\n    CURRENT_STATE=$(jq -r '.state // \"stopped\"' \"$STATE_FILE\" 2>/dev/null || echo \"stopped\")\nfi\n\n# Validate state transition: STOPPED  RUNNING\nif [[ \"$CURRENT_STATE\" != \"stopped\" ]]; then\n    echo \"ERROR: Loop already in state '$CURRENT_STATE'\"\n    echo \"       Run /ralph:stop first to reset state\"\n    exit 1\nfi\n\n# Transition to RUNNING state\necho '{\"state\": \"running\"}' > \"$STATE_FILE\"\n\n# ERROR TRAP: Reset state if script fails from this point forward\n# This prevents orphaned \"running\" state when setup fails (e.g., adapter detection, config parsing)\ncleanup_on_error() {\n    echo \"\"\n    echo \"ERROR: Script failed after state transition. Resetting state to 'stopped'.\"\n    echo '{\"state\": \"stopped\"}' > \"$STATE_FILE\"\n    rm -f \"$PROJECT_DIR/.claude/loop-enabled\"\n    rm -f \"$PROJECT_DIR/.claude/loop-start-timestamp\"\n    rm -f \"$PROJECT_DIR/.claude/ralph-config.json\"\n    rm -f \"$PROJECT_DIR/.claude/loop-config.json\"\n    exit 1\n}\ntrap cleanup_on_error ERR\n\n# Create legacy markers for backward compatibility\ntouch \"$PROJECT_DIR/.claude/loop-enabled\"\ndate +%s > \"$PROJECT_DIR/.claude/loop-start-timestamp\"\n\n# Clear previous stop reason cache (new session = fresh slate)\nrm -f \"$HOME/.claude/ralph-stop-reason.json\"\n\n# Build unified config JSON with all configurable values\n# Note: --poc and --production flags skip preset prompts (backward compatibility)\nif $POC_MODE; then\n    MIN_HOURS=0.083\n    MAX_HOURS=0.167\n    MIN_ITERS=10\n    MAX_ITERS=20\nelif $PRODUCTION_MODE; then\n    MIN_HOURS=4\n    MAX_HOURS=9\n    MIN_ITERS=50\n    MAX_ITERS=99\nelse\n    # Default: Production settings (will be overridden by AskUserQuestion if no preset flag)\n    MIN_HOURS=${SELECTED_MIN_HOURS:-4}\n    MAX_HOURS=${SELECTED_MAX_HOURS:-9}\n    MIN_ITERS=${SELECTED_MIN_ITERS:-50}\n    MAX_ITERS=${SELECTED_MAX_ITERS:-99}\nfi\n\n# ===== STALE GUIDANCE DETECTION =====\n# ADR: /docs/adr/2026-01-02-ralph-guidance-freshness-detection.md\n# Clear guidance if timestamp is > 24h old (from previous session)\nEXISTING_GUIDANCE='{}'\nif [[ -f \"$PROJECT_DIR/.claude/ralph-config.json\" ]]; then\n    GUIDANCE_TS=$(jq -r '.guidance.timestamp // \"\"' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null)\n    if [[ -n \"$GUIDANCE_TS\" ]]; then\n        # Parse ISO 8601 timestamp (macOS format)\n        GUIDANCE_EPOCH=$(date -j -f \"%Y-%m-%dT%H:%M:%SZ\" \"$GUIDANCE_TS\" +%s 2>/dev/null || echo \"0\")\n        NOW_EPOCH=$(date +%s)\n        if [[ \"$GUIDANCE_EPOCH\" -gt 0 ]]; then\n            AGE_HOURS=$(( (NOW_EPOCH - GUIDANCE_EPOCH) / 3600 ))\n            if [[ $AGE_HOURS -gt 24 ]]; then\n                echo \"Clearing stale guidance (${AGE_HOURS}h old, threshold: 24h)\"\n                # Clear stale guidance, keep empty structure\n                EXISTING_GUIDANCE='{\"forbidden\": [], \"encouraged\": [], \"timestamp\": \"\"}'\n            else\n                # Fresh guidance - preserve it\n                EXISTING_GUIDANCE=$(jq '.guidance // {}' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null || echo '{}')\n            fi\n        else\n            # Timestamp parse failed - treat as stale\n            echo \"Clearing legacy guidance (no valid timestamp)\"\n            EXISTING_GUIDANCE='{\"forbidden\": [], \"encouraged\": [], \"timestamp\": \"\"}'\n        fi\n    else\n        # No timestamp - legacy config, treat as stale\n        echo \"Clearing legacy guidance (missing timestamp)\"\n        EXISTING_GUIDANCE='{\"forbidden\": [], \"encouraged\": [], \"timestamp\": \"\"}'\n    fi\nfi\n\n# Generate unified ralph-config.json (v3.0.0 schema - Pydantic migration)\nCONFIG_JSON=$(jq -n \\\n    --arg state \"running\" \\\n    --argjson poc_mode \"$POC_MODE\" \\\n    --argjson production_mode \"$PRODUCTION_MODE\" \\\n    --argjson no_focus \"$NO_FOCUS\" \\\n    --argjson skip_constraint_scan \"$SKIP_CONSTRAINT_SCAN\" \\\n    --arg target_file \"$TARGET_FILE\" \\\n    --arg task_prompt \"$TASK_PROMPT\" \\\n    --argjson min_hours \"$MIN_HOURS\" \\\n    --argjson max_hours \"$MAX_HOURS\" \\\n    --argjson min_iterations \"$MIN_ITERS\" \\\n    --argjson max_iterations \"$MAX_ITERS\" \\\n    --argjson existing_guidance \"$EXISTING_GUIDANCE\" \\\n    '{\n        version: \"3.0.0\",\n        state: $state,\n        poc_mode: $poc_mode,\n        production_mode: $production_mode,\n        no_focus: $no_focus,\n        skip_constraint_scan: $skip_constraint_scan,\n        loop_limits: {\n            min_hours: $min_hours,\n            max_hours: $max_hours,\n            min_iterations: $min_iterations,\n            max_iterations: $max_iterations\n        }\n    }\n    + (if $target_file != \"\" then {target_file: $target_file} else {} end)\n    + (if $task_prompt != \"\" then {task_prompt: $task_prompt} else {} end)\n    + (if $existing_guidance != {} then {guidance: $existing_guidance} else {} end)'\n)\n\necho \"$CONFIG_JSON\" > \"$PROJECT_DIR/.claude/ralph-config.json\"\n\n# Legacy config for backward compatibility\nLEGACY_CONFIG=$(jq -n \\\n    --argjson min_hours \"$MIN_HOURS\" \\\n    --argjson max_hours \"$MAX_HOURS\" \\\n    --argjson min_iterations \"$MIN_ITERS\" \\\n    --argjson max_iterations \"$MAX_ITERS\" \\\n    --argjson no_focus \"$NO_FOCUS\" \\\n    --arg target_file \"$TARGET_FILE\" \\\n    --arg task_prompt \"$TASK_PROMPT\" \\\n    '{\n        min_hours: $min_hours,\n        max_hours: $max_hours,\n        min_iterations: $min_iterations,\n        max_iterations: $max_iterations,\n        no_focus: $no_focus\n    }\n    + (if $target_file != \"\" then {target_file: $target_file} else {} end)\n    + (if $task_prompt != \"\" then {task_prompt: $task_prompt} else {} end)'\n)\necho \"$LEGACY_CONFIG\" > \"$PROJECT_DIR/.claude/loop-config.json\"\n\n# ===== ADAPTER DETECTION =====\n# Detect project-specific adapter using Python\n# Use same path logic as version detection above\nif [[ -d \"$RALPH_CACHE/local\" ]]; then\n    HOOKS_DIR=\"$RALPH_CACHE/local/hooks\"\nelse\n    HOOKS_DIR=\"$RALPH_CACHE/$RALPH_VERSION/hooks\"\nfi\nADAPTER_NAME=\"\"\nif [[ -d \"$HOOKS_DIR\" ]]; then\n    ADAPTER_NAME=$(cd \"$HOOKS_DIR\" && python3 -c \"\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, '.')\ntry:\n    from core.registry import AdapterRegistry\n    AdapterRegistry.discover(Path('adapters'))\n    adapter = AdapterRegistry.get_adapter(Path('$PROJECT_DIR'))\n    if adapter:\n        print(adapter.name)\n    else:\n        print('')\nexcept Exception:\n    print('')\n\" 2>/dev/null || echo \"\")\nfi\n\n# ===== STATUS OUTPUT =====\nif $POC_MODE; then\n    echo \"Ralph Loop: POC MODE\"\n    echo \"Time limits: 5 min minimum / 10 min maximum\"\n    echo \"Iterations: 10 minimum / 20 maximum\"\nelif $PRODUCTION_MODE; then\n    echo \"Ralph Loop: PRODUCTION MODE (via --production flag)\"\n    echo \"Time limits: 4h minimum / 9h maximum\"\n    echo \"Iterations: 50 minimum / 99 maximum\"\nelse\n    echo \"Ralph Loop: PRODUCTION MODE\"\n    echo \"Time limits: ${MIN_HOURS}h minimum / ${MAX_HOURS}h maximum\"\n    echo \"Iterations: ${MIN_ITERS} minimum / ${MAX_ITERS} maximum\"\nfi\n\necho \"\"\nif [[ \"$ADAPTER_NAME\" == \"alpha-forge\" ]]; then\n    echo \"Adapter: alpha-forge\"\n    echo \"   Expert-synthesis convergence (WFE, diminishing returns, patience)\"\n    echo \"   Reads metrics from outputs/runs/*/summary.json\"\nelse\n    echo \"  WARNING: Not an Alpha Forge project\"\n    echo \"   Ralph hooks will SKIP this project (v8.0.2+)\"\n    echo \"   Ralph is designed exclusively for Alpha Forge ML workflows\"\n    echo \"   Detection: pyproject.toml, packages/alpha-forge-core/, outputs/runs/\"\nfi\n\nif $NO_FOCUS; then\n    echo \"\"\n    echo \"Focus mode: DISABLED (100% autonomous, no plan tracking)\"\nelif [[ -n \"$TARGET_FILE\" ]]; then\n    echo \"\"\n    echo \"Target file: $TARGET_FILE\"\nfi\n\nif [[ -n \"$TASK_PROMPT\" ]]; then\n    echo \"\"\n    echo \"Task: $TASK_PROMPT\"\nfi\n\necho \"\"\necho \"State: RUNNING (was: $CURRENT_STATE)\"\necho \"Config: $PROJECT_DIR/.claude/ralph-config.json\"\necho \"\"\necho \"To stop: /ralph:stop\"\necho \"Kill switch: touch $PROJECT_DIR/.claude/STOP_LOOP\"\necho \"\"\necho \"Note: If you just installed hooks, restart Claude Code for them to take effect.\"\nRALPH_START_SCRIPT\n```\n\nRun the bash script above to enable loop mode.\n\n## Step 3: Alpha Forge - OODA Initialization\n\n**If this is an Alpha Forge project** (detected by `outputs/research_sessions/` existing):\n\nAfter enabling loop mode, begin the OODA cycle immediately:\n\n### OBSERVE\n\n1. Read `outputs/research_sessions/*/research_summary.md` (most recent)\n2. Read `outputs/research_sessions/*/research_log.md` for expert recommendations\n3. Check `ROADMAP.md` for current P0/P1 priorities\n\n### ORIENT\n\n1. Compare metrics to previous session (look for delta)\n2. Synthesize expert recommendations from research_log.md\n3. Self-critique: Does the planned approach align with ROADMAP?\n\n### DECIDE\n\nUse the checkpoint gate:\n\n- Sharpe improved > 10%?  CONTINUE\n- Sharpe improved < 5% for 2 sessions?  PIVOT to next ROADMAP item\n- WFE < 0.5?  STOP and address overfitting first\n\n### ACT\n\nInvoke `/research` with the appropriate strategy:\n\n```\n/research <path/to/strategy.yaml> --iterations=5 --objective=sharpe\n```\n\nRalph is **supplementary** to alpha-forge's `/research`:\n\n- `/research` owns the inner loop (5 iterations, 5 expert subagents)\n- Ralph owns the outer loop (session-to-session learning, OODA decisions)\n\n**Do NOT ask the user what to work on.** Proceed autonomously through OODA."
              },
              {
                "name": "/status",
                "description": "Show current loop status and session metrics",
                "path": "plugins/ralph/commands/status.md",
                "frontmatter": {
                  "description": "Show current loop status and session metrics",
                  "allowed-tools": "Read, Bash",
                  "argument-hint": ""
                },
                "content": "# Ralph Loop: Status\n\nDisplay the current state of the Ralph Wiggum autonomous improvement loop.\n\n## Execution\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_STATUS_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nSETTINGS=\"$HOME/.claude/settings.json\"\nMARKER=\"ralph/hooks/\"\n\n# Helper function for time calculation (fallback if bc not available)\ncalc_hours() {\n    local secs=\"$1\"\n    if command -v bc &>/dev/null; then\n        echo \"scale=2; $secs / 3600\" | bc\n    else\n        # Fallback: integer division with awk\n        awk \"BEGIN {printf \\\"%.2f\\\", $secs / 3600}\"\n    fi\n}\n\necho \"=== Ralph Loop Status ===\"\necho \"\"\n\n# Check hook registration in settings.json\nHOOKS_REGISTERED=false\nif command -v jq &>/dev/null && [[ -f \"$SETTINGS\" ]]; then\n    HOOK_COUNT=$(jq '[.hooks | to_entries[] | .value[] | .hooks[] | select(.command | contains(\"'\"$MARKER\"'\"))] | length' \"$SETTINGS\" 2>/dev/null || echo \"0\")\n    if [[ \"$HOOK_COUNT\" -gt 0 ]]; then\n        HOOKS_REGISTERED=true\n    fi\nfi\n\n# Check if loop is enabled (marker file)\nLOOP_ENABLED=false\nif [[ -f \"$PROJECT_DIR/.claude/loop-enabled\" ]]; then\n    LOOP_ENABLED=true\nfi\n\n# Determine overall status\nif [[ \"$HOOKS_REGISTERED\" == \"true\" ]] && [[ \"$LOOP_ENABLED\" == \"true\" ]]; then\n    echo \"Status: ACTIVE\"\n    echo \"Hooks are registered and loop enabled\"\nelif [[ \"$HOOKS_REGISTERED\" == \"true\" ]] && [[ \"$LOOP_ENABLED\" == \"false\" ]]; then\n    echo \"Status: READY (not started)\"\n    echo \"Run /ralph:start to begin loop\"\nelif [[ \"$HOOKS_REGISTERED\" == \"false\" ]] && [[ \"$LOOP_ENABLED\" == \"true\" ]]; then\n    echo \"Status: ENABLED (hooks not installed)\"\n    echo \"Run /ralph:hooks install, then restart Claude Code\"\nelif [[ \"$HOOKS_REGISTERED\" == \"false\" ]] && [[ \"$LOOP_ENABLED\" == \"false\" ]]; then\n    echo \"Status: INACTIVE\"\n    echo \"Run /ralph:hooks install first\"\nfi\necho \"\"\n\n# Show component status\necho \"Components:\"\nif [[ \"$HOOKS_REGISTERED\" == \"true\" ]]; then\n    echo \"  [x] Hooks registered in settings.json ($HOOK_COUNT entries)\"\nelse\n    echo \"  [ ] Hooks NOT registered - run /ralph:hooks install\"\nfi\n\nif [[ \"$LOOP_ENABLED\" == \"true\" ]]; then\n    echo \"  [x] Loop enabled (marker file exists)\"\nelse\n    echo \"  [ ] Loop not enabled - run /ralph:start\"\nfi\n\n# Check for kill switch\nif [[ -f \"$PROJECT_DIR/.claude/STOP_LOOP\" ]]; then\n    echo \"  [!] Kill Switch: TRIGGERED\"\nfi\n\necho \"\"\n\n# Show config if exists\nif [[ -f \"$PROJECT_DIR/.claude/loop-config.json\" ]]; then\n    echo \"=== Configuration ===\"\n    cat \"$PROJECT_DIR/.claude/loop-config.json\" | python3 -m json.tool 2>/dev/null || cat \"$PROJECT_DIR/.claude/loop-config.json\"\n    echo \"\"\nfi\n\n# Show guidance (encouraged/forbidden) from ralph-config.json\nif [[ -f \"$PROJECT_DIR/.claude/ralph-config.json\" ]]; then\n    GUIDANCE=$(jq -r '.guidance // empty' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null)\n    if [[ -n \"$GUIDANCE\" && \"$GUIDANCE\" != \"null\" ]]; then\n        echo \"=== Current Guidance ===\"\n\n        # Show encouraged items\n        ENCOURAGED=$(jq -r '.guidance.encouraged // []' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null)\n        ENCOURAGED_COUNT=$(echo \"$ENCOURAGED\" | jq 'length' 2>/dev/null || echo \"0\")\n        if [[ \"$ENCOURAGED_COUNT\" -gt 0 ]]; then\n            echo \"\"\n            echo \"ENCOURAGED ($ENCOURAGED_COUNT items):\"\n            echo \"$ENCOURAGED\" | jq -r '.[] | \"   \" + .' 2>/dev/null\n        else\n            echo \"ENCOURAGED: (none)\"\n        fi\n\n        # Show forbidden items\n        FORBIDDEN=$(jq -r '.guidance.forbidden // []' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null)\n        FORBIDDEN_COUNT=$(echo \"$FORBIDDEN\" | jq 'length' 2>/dev/null || echo \"0\")\n        if [[ \"$FORBIDDEN_COUNT\" -gt 0 ]]; then\n            echo \"\"\n            echo \"FORBIDDEN ($FORBIDDEN_COUNT items):\"\n            echo \"$FORBIDDEN\" | jq -r '.[] | \"   \" + .' 2>/dev/null\n        else\n            echo \"FORBIDDEN: (none)\"\n        fi\n        echo \"\"\n        echo \"Modify with: /ralph:encourage <item> or /ralph:forbid <item>\"\n        echo \"\"\n    fi\nfi\n\n# Show session state if exists (ralph-state.json is canonical, loop-state.json is legacy)\nif [[ -f \"$PROJECT_DIR/.claude/ralph-state.json\" ]]; then\n    echo \"=== Session State ===\"\n    cat \"$PROJECT_DIR/.claude/ralph-state.json\" | python3 -m json.tool 2>/dev/null || cat \"$PROJECT_DIR/.claude/ralph-state.json\"\n    echo \"\"\nfi\n\n# Show time tracking (v7.9.0: dual time tracking)\necho \"=== Time Tracking ===\"\n# Runtime: from session state (accumulated_runtime_seconds)\nSTATE_DIR=\"$HOME/.claude/automation/loop-orchestrator/state/sessions\"\nif [[ -d \"$STATE_DIR\" ]]; then\n    # Find the most recent session state file for this project\n    SESSION_STATE=$(find \"$STATE_DIR\" -name \"*.json\" -exec grep -l \"\\\"accumulated_runtime_seconds\\\"\" {} \\; 2>/dev/null | head -1)\n    if [[ -n \"$SESSION_STATE\" ]] && [[ -f \"$SESSION_STATE\" ]]; then\n        RUNTIME_SECS=$(jq -r '.accumulated_runtime_seconds // 0' \"$SESSION_STATE\")\n        if [[ \"$RUNTIME_SECS\" != \"null\" ]] && [[ \"$RUNTIME_SECS\" != \"0\" ]]; then\n            RUNTIME_HOURS=$(calc_hours \"$RUNTIME_SECS\")\n            echo \"Runtime (CLI active): ${RUNTIME_HOURS}h\"\n        else\n            echo \"Runtime (CLI active): 0.00h (session just started)\"\n        fi\n    else\n        echo \"Runtime (CLI active): N/A (no session state)\"\n    fi\nelse\n    echo \"Runtime (CLI active): N/A (state directory not found)\"\nfi\n\n# Wall-clock: from loop-start-timestamp\nif [[ -f \"$PROJECT_DIR/.claude/loop-start-timestamp\" ]]; then\n    START_TS=$(cat \"$PROJECT_DIR/.claude/loop-start-timestamp\")\n    NOW_TS=$(date +%s)\n    WALL_SECS=$((NOW_TS - START_TS))\n    WALL_HOURS=$(calc_hours \"$WALL_SECS\")\n    echo \"Wall-clock (since start): ${WALL_HOURS}h\"\nelse\n    echo \"Wall-clock (since start): N/A (loop not started)\"\nfi\necho \"\"\necho \"Note: Runtime = actual CLI working time (pauses excluded)\"\necho \"      Wall-clock = calendar time since /ralph:start\"\necho \"\"\n\n# Show last stop reason if exists\nSTOP_CACHE=\"$HOME/.claude/ralph-stop-reason.json\"\nif [[ -f \"$STOP_CACHE\" ]]; then\n    echo \"=== Last Stop Reason ===\"\n    LAST_STOP=$(jq -r '.reason // \"Unknown\"' \"$STOP_CACHE\")\n    STOP_TIME=$(jq -r '.timestamp // \"Unknown\"' \"$STOP_CACHE\")\n    STOP_TYPE=$(jq -r '.type // \"normal\"' \"$STOP_CACHE\")\n    STOP_SESSION=$(jq -r '.session_id // \"Unknown\"' \"$STOP_CACHE\")\n\n    if [[ \"$STOP_TYPE\" == \"hard\" ]]; then\n        echo \"Type: HARD STOP\"\n    else\n        echo \"Type: Normal\"\n    fi\n    echo \"Reason: $LAST_STOP\"\n    echo \"Time: $STOP_TIME\"\n    echo \"Session: ${STOP_SESSION:0:8}...\"\n    echo \"\"\nfi\n\n# Reminder about restart\nif [[ \"$HOOKS_REGISTERED\" == \"true\" ]]; then\n    echo \"Note: If you just installed hooks, restart Claude Code for them to take effect.\"\nfi\nRALPH_STATUS_SCRIPT\n```\n\nRun the bash script above to show status."
              },
              {
                "name": "/stop",
                "description": "Disable autonomous loop mode immediately",
                "path": "plugins/ralph/commands/stop.md",
                "frontmatter": {
                  "description": "Disable autonomous loop mode immediately",
                  "allowed-tools": "Bash",
                  "argument-hint": ""
                },
                "content": "# Ralph Loop: Stop\n\n**EXECUTE IMMEDIATELY**: Use the Bash tool to run the following script. Do NOT summarize or acknowledge - EXECUTE the script first.\n\n```bash\n# Use /usr/bin/env bash for macOS zsh compatibility (see ADR: shell-command-portability-zsh)\n/usr/bin/env bash << 'RALPH_STOP_SCRIPT'\n# RALPH_STOP_SCRIPT marker - required for PreToolUse hook bypass\n\n# ===== HOLISTIC PROJECT DIRECTORY RESOLUTION =====\n# Uses multiple detection methods with priority and validation\n# Fix for: cross-directory invocation bug (v7.16.0)\n\nresolve_project_dir() {\n    local resolved=\"\"\n\n    # Priority 1: CLAUDE_PROJECT_DIR (highest priority - set by Claude Code)\n    if [[ -n \"${CLAUDE_PROJECT_DIR:-}\" && -d \"$CLAUDE_PROJECT_DIR\" ]]; then\n        resolved=\"$CLAUDE_PROJECT_DIR\"\n    fi\n\n    # Priority 2: Git root (provides repo boundary)\n    if [[ -z \"$resolved\" ]]; then\n        local git_root\n        git_root=$(git rev-parse --show-toplevel 2>/dev/null)\n        if [[ -n \"$git_root\" && -d \"$git_root\" ]]; then\n            resolved=\"$git_root\"\n        fi\n    fi\n\n    # Priority 3: pwd (lowest priority fallback)\n    if [[ -z \"$resolved\" ]]; then\n        resolved=\"$(pwd)\"\n    fi\n\n    echo \"$resolved\"\n}\n\n# ===== SESSION DISCOVERY =====\nSESSIONS_DIR=\"$HOME/.claude/automation/loop-orchestrator/state/sessions\"\nSTOPPED_COUNT=0\ndeclare -A STOPPED_PROJECTS  # Track already-stopped projects (dedup)\n\nstop_project() {\n    local PROJECT_DIR=\"$1\"\n    local SOURCE=\"$2\"\n    local STATE_FILE=\"$PROJECT_DIR/.claude/ralph-state.json\"\n    local CONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\n\n    # Skip if already stopped this project (dedup)\n    if [[ -n \"${STOPPED_PROJECTS[$PROJECT_DIR]:-}\" ]]; then\n        return 0\n    fi\n\n    # Skip if no .claude directory (not a Ralph-enabled project)\n    if [[ ! -d \"$PROJECT_DIR/.claude\" ]]; then\n        return 0\n    fi\n\n    # Set state to stopped\n    echo '{\"state\": \"stopped\"}' > \"$STATE_FILE\"\n\n    # Create kill switch\n    touch \"$PROJECT_DIR/.claude/STOP_LOOP\"\n\n    # Update config if exists\n    if [[ -f \"$CONFIG_FILE\" ]]; then\n        jq '.state = \"stopped\"' \"$CONFIG_FILE\" > \"$CONFIG_FILE.tmp\" && mv \"$CONFIG_FILE.tmp\" \"$CONFIG_FILE\"\n    fi\n\n    # Clean legacy markers\n    rm -f \"$PROJECT_DIR/.claude/loop-enabled\"\n    rm -f \"$PROJECT_DIR/.claude/loop-start-timestamp\"\n\n    echo \"  [$SOURCE] Stopped: $PROJECT_DIR\"\n    STOPPED_PROJECTS[\"$PROJECT_DIR\"]=1\n    ((STOPPED_COUNT++))\n}\n\necho \"Discovering active sessions (holistic resolution)...\"\n\n# Method 1: Scan session state files for project_path\nif [[ -d \"$SESSIONS_DIR\" ]]; then\n    for STATE_FILE in \"$SESSIONS_DIR\"/*.json; do\n        [[ -f \"$STATE_FILE\" ]] || continue\n\n        PROJECT_PATH=$(jq -r '.project_path // empty' \"$STATE_FILE\" 2>/dev/null)\n\n        if [[ -n \"$PROJECT_PATH\" && -d \"$PROJECT_PATH\" ]]; then\n            stop_project \"$PROJECT_PATH\" \"session-state\"\n\n            # Also update session state to prevent continuation\n            if jq '.adapter_convergence.should_continue = false' \"$STATE_FILE\" > \"$STATE_FILE.tmp\"; then\n                mv \"$STATE_FILE.tmp\" \"$STATE_FILE\"\n            else\n                echo \"Warning: Failed to update session state (jq error)\" >&2\n                rm -f \"$STATE_FILE.tmp\"\n            fi\n        fi\n    done\nfi\n\n# Method 2: Resolve current context using holistic detection\nCURRENT_PROJECT=$(resolve_project_dir)\nif [[ -d \"$CURRENT_PROJECT/.claude\" ]]; then\n    CURRENT_STATE=$(jq -r '.state // \"stopped\"' \"$CURRENT_PROJECT/.claude/ralph-state.json\" 2>/dev/null || echo \"stopped\")\n    if [[ \"$CURRENT_STATE\" != \"stopped\" ]]; then\n        stop_project \"$CURRENT_PROJECT\" \"holistic\"\n    fi\nfi\n\n# Method 3: Check parent directories for nested repos (monorepo support)\ncheck_parents() {\n    local dir=\"$1\"\n    local max_depth=3\n    local depth=0\n\n    while [[ \"$dir\" != \"/\" && $depth -lt $max_depth ]]; do\n        if [[ -f \"$dir/.claude/ralph-state.json\" ]]; then\n            local state\n            state=$(jq -r '.state // \"stopped\"' \"$dir/.claude/ralph-state.json\" 2>/dev/null || echo \"stopped\")\n            if [[ \"$state\" != \"stopped\" ]]; then\n                stop_project \"$dir\" \"parent-walk\"\n            fi\n        fi\n        dir=$(dirname \"$dir\")\n        ((depth++))\n    done\n}\ncheck_parents \"$(pwd)\"\n\n# Method 4: Create global stop signal (version-agnostic, works across cached versions)\n# This signal is checked by the Stop hook BEFORE any project-specific checks\necho '{\"state\": \"stopped\", \"timestamp\": \"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\"}' > \"$HOME/.claude/ralph-global-stop.json\"\necho \"  [global] Created ~/.claude/ralph-global-stop.json\"\n((STOPPED_COUNT++))\n\n# Summary\necho \"\"\necho \"Stopped $STOPPED_COUNT location(s).\"\necho \"Loop stop complete.\"\nRALPH_STOP_SCRIPT\n```\n\nAfter execution, confirm the loop has been stopped by checking the output."
              }
            ],
            "skills": [
              {
                "name": "constraint-discovery",
                "description": "Spawn 5 parallel Explore agents to discover project constraints. TRIGGERS - constraint scan, degrees of freedom, /ralph:start Step 1.4.5, project memory analysis.",
                "path": "plugins/ralph/skills/constraint-discovery/SKILL.md",
                "frontmatter": {
                  "name": "constraint-discovery",
                  "description": "Spawn 5 parallel Explore agents to discover project constraints. TRIGGERS - constraint scan, degrees of freedom, /ralph:start Step 1.4.5, project memory analysis.",
                  "allowed-tools": "Task, TaskOutput, Bash, Read, Grep, Glob"
                },
                "content": "# Constraint Discovery Skill\n\nSpawn 5 parallel Explore agents to discover constraints that limit Claude's degrees of freedom.\n\n## When to Use\n\n- Invoked by `/ralph:start` Step 1.4.5 via Skill tool\n- User asks to analyze project constraints\n- User mentions \"degrees of freedom\" or \"constraint scan\"\n- Standalone constraint analysis needed\n\n## Agents\n\n### Agent 1: Project Memory & Philosophy Constraints\n\n```\nTask tool parameters:\n  description: \"Analyze project memory constraints\"\n  subagent_type: \"Explore\"\n  run_in_background: true\n  prompt: |\n    DEEP DIVE into project memory files AND FOLLOW ALL @ LINKS to discover constraints.\n\n    STEP 1 - READ THESE FILES FIRST:\n    - CLAUDE.md (project instructions, philosophy, forbidden patterns)\n    - .claude/ directory (memories, settings, agents/*.md)\n    - .claude/agents/*.md (agent definitions with @ references)\n    - ROADMAP.md (P0/P1 priorities, explicit scope limits)\n    - docs/adr/ (Architecture Decision Records)\n\n    STEP 2 - FOLLOW ALL @ LINKS (UNLIMITED DEPTH):\n    Parse each file for @ link patterns:\n    - @path/to/file.md (relative to project root)\n    - @ai_context/PHILOSOPHY.md (ai_context directory)\n    - @projectname/path/to/file.md (project prefix)\n    - @AGENTS.md, @README.md (root files)\n\n    For EACH @ link found:\n    1. Read the linked file\n    2. Parse it for more @ links\n    3. Recursively follow until no new @ links found\n\n    STEP 3 - EXTRACT CONSTRAINTS FROM ALL FILES:\n    - \"Do NOT modify X\" instructions\n    - Philosophy rules (e.g., \"prefer simplicity over features\")\n    - Explicit forbidden patterns\n    - Scope limits from ROADMAP\n\n    Return NDJSON: {\"source\":\"agent-memory\",\"severity\":\"CRITICAL|HIGH|MEDIUM\",\"description\":\"...\",\"file\":\"...\",\"linked_from\":\"...\",\"recommendation\":\"Ralph should avoid...\"}\n```\n\n### Agent 2: Architecture & Coupling Constraints\n\n```\nTask tool parameters:\n  description: \"Analyze architectural constraints\"\n  subagent_type: \"Explore\"\n  run_in_background: true\n  prompt: |\n    Analyze architectural patterns that constrain safe modification.\n\n    STEP 1 - READ THESE FILES:\n    - pyproject.toml, setup.py (package structure, entry points)\n    - Core module __init__.py files (public API surface)\n    - docs/adr/ (past architectural decisions)\n    - docs/reference/interfaces.md (if exists)\n\n    STEP 2 - FOLLOW @ LINKS (UNLIMITED DEPTH):\n    Parse for @ link patterns in ADRs and docs:\n    - @docs/reference/*.md, @docs/architecture/*.md\n    - @ai_context/*.md (philosophy files)\n    Recursively follow until no new @ links found.\n\n    STEP 3 - EXTRACT CONSTRAINTS:\n    - Circular imports, tightly coupled modules\n    - Public API that cannot change without breaking users\n    - Package structure assumptions\n    - Cross-layer dependencies\n\n    Return NDJSON: {\"source\":\"agent-arch\",\"severity\":\"HIGH|MEDIUM|LOW\",\"description\":\"...\",\"modules\":[\"A\",\"B\"],\"linked_from\":\"...\",\"recommendation\":\"...\"}\n```\n\n### Agent 3: Research Session Lessons Learned\n\n```\nTask tool parameters:\n  description: \"Extract research session constraints\"\n  subagent_type: \"Explore\"\n  run_in_background: true\n  prompt: |\n    Analyze past research sessions to find lessons learned and forbidden patterns.\n\n    STEP 1 - READ THESE FILES:\n    - outputs/research_sessions/*/research_summary.md (most recent 3)\n    - outputs/research_sessions/*/research_log.md (if exists)\n    - outputs/research_sessions/*/production_config.yaml\n    - Any \"lessons_learned\" or \"warnings\" sections\n\n    STEP 2 - FOLLOW @ LINKS:\n    Research summaries may reference:\n    - @strategies/*.yaml (strategy configs that failed)\n    - @docs/guides/*.md (guides with constraints)\n    Recursively follow until no new @ links found.\n\n    STEP 3 - EXTRACT CONSTRAINTS:\n    - Failed experiments (don't repeat these)\n    - Hyperparameter ranges that caused issues\n    - Strategies that were abandoned and why\n    - Explicit warnings from past sessions\n    - \"Do not explore below X\" thresholds\n\n    Return NDJSON: {\"source\":\"agent-research\",\"severity\":\"HIGH|MEDIUM\",\"description\":\"Past session found: ...\",\"session\":\"...\",\"linked_from\":\"...\",\"recommendation\":\"Avoid...\"}\n```\n\n### Agent 4: Testing & Validation Constraints\n\n```\nTask tool parameters:\n  description: \"Find testing constraints\"\n  subagent_type: \"Explore\"\n  run_in_background: true\n  prompt: |\n    Find testing gaps and validation requirements that constrain safe changes.\n\n    STEP 1 - READ THESE FILES:\n    - tests/ directory structure\n    - pytest.ini, pyproject.toml [tool.pytest] section\n    - CI/CD workflows (.github/workflows/)\n    - docs/development/testing.md (if exists)\n\n    STEP 2 - FOLLOW @ LINKS:\n    Testing docs may reference:\n    - @docs/development/*.md (dev guides)\n    - @ai_context/*.md (philosophy that affects testing)\n    Recursively follow until no new @ links found.\n\n    STEP 3 - EXTRACT CONSTRAINTS:\n    - Modules with zero test coverage (risky to modify)\n    - Integration tests that must pass\n    - Validation thresholds (e.g., min Sharpe ratio, max drawdown)\n    - Pre-commit hooks and their requirements\n    - \"Tests must pass before X\" gates\n\n    Return NDJSON: {\"source\":\"agent-testing\",\"severity\":\"HIGH|MEDIUM|LOW\",\"description\":\"...\",\"location\":\"...\",\"linked_from\":\"...\",\"recommendation\":\"...\"}\n```\n\n### Agent 5: Degrees of Freedom Analysis\n\n```\nTask tool parameters:\n  description: \"Analyze degrees of freedom\"\n  subagent_type: \"Explore\"\n  run_in_background: true\n  prompt: |\n    Find explicit and implicit limits on what Ralph can explore.\n\n    STEP 1 - READ THESE FILES:\n    - CLAUDE.md (explicit instructions)\n    - .claude/ralph-config.json (previous session guidance)\n    - .claude/agents/*.md (agent definitions)\n    - Config files (*.yaml, *.toml) for hardcoded limits\n\n    STEP 2 - FOLLOW ALL @ LINKS (UNLIMITED DEPTH):\n    Parse each file for @ link patterns:\n    - @ai_context/IMPLEMENTATION_PHILOSOPHY.md\n    - @ai_context/MODULAR_DESIGN_PHILOSOPHY.md\n    - @docs/reference/*.md\n    - @DISCOVERIES.md, @ai_working/decisions/\n    Recursively follow until no new @ links found.\n\n    STEP 3 - EXTRACT FREEDOM CONSTRAINTS:\n    - Hard gates (if not X, skip silently)\n    - One-way state transitions\n    - Configuration that cannot be overridden at runtime\n    - Feature flags and their current state\n    - Philosophy constraints (e.g., \"ruthless simplicity\")\n    - Escape hatches (--skip-X flags, override mechanisms)\n\n    Return NDJSON: {\"source\":\"agent-freedom\",\"severity\":\"CRITICAL|HIGH|MEDIUM\",\"description\":\"...\",\"gate\":\"...\",\"linked_from\":\"...\",\"recommendation\":\"...\"}\n```\n\n## Execution\n\n**MANDATORY: Spawn ALL 5 Task tools in a SINGLE message** (parallel execution).\n\nUse `run_in_background: true` for all agents.\n\n## Blocking Gate\n\nAfter spawning, use TaskOutput with `block: true` and `timeout: 30000` for each agent:\n\n```\nFor EACH agent spawned:\n  TaskOutput(task_id: \"<agent_id>\", block: true, timeout: 30000)\n```\n\n**Wait for ALL 5 agents** (or timeout after 30s each).\n\n## Aggregation\n\nMerge agent findings into constraint scan file:\n\n```bash\n/usr/bin/env bash << 'AGENT_MERGE_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nSCAN_FILE=\"$PROJECT_DIR/.claude/ralph-constraint-scan.jsonl\"\n\n# Claude MUST append each agent's NDJSON findings here:\n# For each constraint JSON from agent output:\n#   echo '{\"_type\":\"constraint\",\"source\":\"agent-env\",\"severity\":\"HIGH\",\"description\":\"...\"}' >> \"$SCAN_FILE\"\n\necho \"=== AGENT FINDINGS MERGED ===\"\necho \"Constraints in scan file:\"\nwc -l < \"$SCAN_FILE\" 2>/dev/null || echo \"0\"\nAGENT_MERGE_SCRIPT\n```\n\n## Output\n\nEach agent returns NDJSON with:\n- `source`: Which agent found it (agent-memory, agent-arch, agent-research, agent-testing, agent-freedom)\n- `severity`: CRITICAL, HIGH, MEDIUM, or LOW\n- `description`: Human-readable constraint description\n- `linked_from`: Which file the constraint was discovered from (for @ link tracing)\n- `recommendation`: What Ralph should avoid or be careful about"
              },
              {
                "name": "session-guidance",
                "description": "Configure Ralph loop guidance via AUQ flows - forbidden/encouraged items from constraint scan. TRIGGERS - session guidance, loop configuration, /ralph:start Step 1.6.",
                "path": "plugins/ralph/skills/session-guidance/SKILL.md",
                "frontmatter": {
                  "name": "session-guidance",
                  "description": "Configure Ralph loop guidance via AUQ flows - forbidden/encouraged items from constraint scan. TRIGGERS - session guidance, loop configuration, /ralph:start Step 1.6.",
                  "allowed-tools": "Bash, Read, AskUserQuestion, Write"
                },
                "content": "# Session Guidance Skill\n\nConfigure Ralph loop session guidance through AskUserQuestion flows. Loads constraint scan results, presents dynamic options based on severity, and writes guidance to config.\n\n## When to Use\n\n- Invoked by `/ralph:start` Step 1.6 via Skill tool\n- User asks to reconfigure Ralph guidance\n- User mentions \"forbidden items\" or \"encouraged items\"\n\n## Prerequisites\n\n- Step 1.4 constraint scan completed (`.claude/ralph-constraint-scan.jsonl` exists)\n- Step 1.5 preset confirmation completed\n\n## Workflow Overview\n\n```\n1.6.1: Check for Previous Guidance\n         \n1.6.2: Binary Keep/Reconfigure (if guidance exists)\n         \n1.6.2.5: Load Constraint Scan Results (NDJSON)\n         \n1.6.3: Forbidden Items (multiSelect, DYNAMIC from constraints)\n         \n1.6.4: Custom Forbidden (follow-up)\n         \n1.6.5: Encouraged Items (multiSelect, closed list)\n         \n1.6.6: Custom Encouraged (follow-up)\n         \n1.6.7: Update Config (with validation + learned behavior)\n```\n\n---\n\n## Step 1.6.1: Check for Previous Guidance\n\nCheck if guidance exists in the config file:\n\n```bash\n/usr/bin/env bash << 'CHECK_GUIDANCE_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nGUIDANCE_EXISTS=\"false\"\nif [[ -f \"$PROJECT_DIR/.claude/ralph-config.json\" ]]; then\n    GUIDANCE_EXISTS=$(jq -r 'if .guidance then \"true\" else \"false\" end' \"$PROJECT_DIR/.claude/ralph-config.json\" 2>/dev/null || echo \"false\")\nfi\necho \"GUIDANCE_EXISTS=$GUIDANCE_EXISTS\"\nCHECK_GUIDANCE_SCRIPT\n```\n\n---\n\n## Step 1.6.2: Binary Keep/Reconfigure (Conditional)\n\n**If `GUIDANCE_EXISTS == \"true\"`:**\n\nUse AskUserQuestion:\n\n- question: \"Previous session had custom guidance. Keep it or reconfigure?\"\n  header: \"Guidance\"\n  options:\n  - label: \"Keep existing guidance (Recommended)\"\n    description: \"Use stored forbidden/encouraged lists from last session\"\n  - label: \"Reconfigure guidance\"\n    description: \"Set new forbidden/encouraged lists\"\n  multiSelect: false\n\n- If \"Keep existing\"  **STOP skill execution** (guidance already in config, return to start.md Step 2)\n- If \"Reconfigure\"  Continue to Step 1.6.2.5\n\n**If `GUIDANCE_EXISTS == \"false\"` (first run):**\n\nProceed directly to Step 1.6.2.5. No user prompt needed.\n\n---\n\n## Step 1.6.2.5: Load Constraint Scan Results (NDJSON)\n\n**Purpose**: Load constraint scan results in NDJSON format, filtering out previously acknowledged constraints.\n\n**Learned behavior**: Constraints the user previously selected as \"forbidden\" are stored in `.claude/ralph-acknowledged-constraints.jsonl` and filtered from future displays.\n\n```bash\n/usr/bin/env bash << 'LOAD_SCAN_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nSCAN_FILE=\"$PROJECT_DIR/.claude/ralph-constraint-scan.jsonl\"\nACK_FILE=\"$PROJECT_DIR/.claude/ralph-acknowledged-constraints.jsonl\"\n\nif [[ -f \"$SCAN_FILE\" ]]; then\n    # Load acknowledged constraint IDs (if file exists)\n    ACKNOWLEDGED_IDS=\"\"\n    if [[ -f \"$ACK_FILE\" ]]; then\n        ACKNOWLEDGED_IDS=$(jq -r 'select(._type == \"constraint\") | .id' \"$ACK_FILE\" 2>/dev/null | tr '\\n' '|' | sed 's/|$//')\n        ACK_COUNT=$(grep -c '\"_type\":\"constraint\"' \"$ACK_FILE\" 2>/dev/null || echo \"0\")\n        echo \"=== ACKNOWLEDGED CONSTRAINTS ===\"\n        echo \"Previously acknowledged: $ACK_COUNT constraints (filtered from display)\"\n        echo \"\"\n    fi\n\n    # NDJSON format: each line is a JSON object with _type field\n    # Filter out acknowledged constraints and count by severity\n    if [[ -n \"$ACKNOWLEDGED_IDS\" ]]; then\n        FILTERED=$(grep '\"_type\":\"constraint\"' \"$SCAN_FILE\" 2>/dev/null | \\\n            jq -c --arg ack_pattern \"$ACKNOWLEDGED_IDS\" 'select(.id | test($ack_pattern) | not)' 2>/dev/null)\n    else\n        FILTERED=$(grep '\"_type\":\"constraint\"' \"$SCAN_FILE\" 2>/dev/null)\n    fi\n\n    # Count by severity (from filtered NDJSON lines)\n    CRITICAL_COUNT=$(echo \"$FILTERED\" | jq -s '[.[] | select(.severity == \"critical\")] | length' 2>/dev/null || echo \"0\")\n    HIGH_COUNT=$(echo \"$FILTERED\" | jq -s '[.[] | select(.severity == \"high\")] | length' 2>/dev/null || echo \"0\")\n    MEDIUM_COUNT=$(echo \"$FILTERED\" | jq -s '[.[] | select(.severity == \"medium\")] | length' 2>/dev/null || echo \"0\")\n    LOW_COUNT=$(echo \"$FILTERED\" | jq -s '[.[] | select(.severity == \"low\")] | length' 2>/dev/null || echo \"0\")\n    TOTAL_COUNT=$((CRITICAL_COUNT + HIGH_COUNT + MEDIUM_COUNT + LOW_COUNT))\n    BUSYWORK_COUNT=$(grep -c '\"_type\":\"busywork\"' \"$SCAN_FILE\" 2>/dev/null || echo \"0\")\n\n    echo \"=== CONSTRAINT SCAN SUMMARY ===\"\n    echo \"SEVERITY_COUNTS: critical=$CRITICAL_COUNT high=$HIGH_COUNT medium=$MEDIUM_COUNT low=$LOW_COUNT total=$TOTAL_COUNT\"\n    echo \"BUSYWORK_COUNT: $BUSYWORK_COUNT\"\n    echo \"\"\n    echo \"=== CONSTRAINTS (NDJSON) ===\"\n    echo \"$FILTERED\"\n    echo \"\"\n    echo \"=== BUSYWORK CATEGORIES (NDJSON) ===\"\n    grep '\"_type\":\"busywork\"' \"$SCAN_FILE\" 2>/dev/null\n    echo \"\"\n    echo \"=== END SCAN RESULTS ===\"\nelse\n    echo \"=== CONSTRAINT SCAN SUMMARY ===\"\n    echo \"SEVERITY_COUNTS: critical=0 high=0 medium=0 low=0 total=0\"\n    echo \"BUSYWORK_COUNT: 0\"\n    echo \"=== NO SCAN FILE FOUND ===\"\nfi\nLOAD_SCAN_SCRIPT\n```\n\n**Claude MUST parse this output**:\n\n1. **Extract severity counts** from `SEVERITY_COUNTS:` line for question text\n2. **Parse NDJSON constraints** between `=== CONSTRAINTS (NDJSON) ===` and `=== BUSYWORK CATEGORIES ===`\n3. **Parse NDJSON busywork** between `=== BUSYWORK CATEGORIES (NDJSON) ===` and `=== END SCAN RESULTS ===`\n4. **Build dynamic AUQ options** with constraint-derived items first, then static fallbacks\n\n**NDJSON constraint format** (one per line):\n```json\n{\"id\":\"hardcoded-001\",\"severity\":\"high\",\"category\":\"hardcoded_path\",\"description\":\"Hardcoded path: /Users/terryli/...\",\"file\":\"pyproject.toml\",\"line\":15,\"recommendation\":\"Use environment variable\"}\n```\n\n---\n\n## Step 1.6.3: Forbidden Items (multiSelect, DYNAMIC)\n\n**MANDATORY: Build options dynamically from Step 1.6.2.5 output**\n\n**AUQ Limit**: Maximum 4 options total. Priority order:\n1. CRITICAL severity constraints (up to 2)\n2. HIGH severity constraints (up to 2)\n3. If <4 constraint options, fill with static categories\n\n**Algorithm** - Claude MUST execute this logic:\n\n```\nStep 1: Parse severity counts from SEVERITY_COUNTS line\n  - Extract: critical=N, high=M, total=T\n\nStep 2: Build constraint options (max 4, severity priority)\n  options = []\n\n  # First: CRITICAL constraints (max 2)\n  for each NDJSON line where severity == \"critical\":\n    if len(options) >= 2: break\n    options.append({\n      label: description[:55] + \"...\" if len > 55 else description,\n      description: \"(CRITICAL) \" + file + \":\" + line + \" - \" + recommendation[:40]\n    })\n\n  # Second: HIGH constraints (max 2 more)\n  for each NDJSON line where severity == \"high\":\n    if len(options) >= 4: break\n    options.append({\n      label: description[:55] + \"...\" if len > 55 else description,\n      description: \"(HIGH) \" + file + \":\" + line + \" - \" + recommendation[:40]\n    })\n\n  # Third: Fill remaining with static categories\n  static_categories = [\"Documentation updates\", \"Dependency upgrades\", \"Refactoring\", \"CI/CD modifications\"]\n  while len(options) < 4 and static_categories:\n    options.append(static_categories.pop(0))\n\nStep 3: Build question text\n  if critical > 0 or high > 0:\n    question = \"What should Ralph avoid? ({critical} critical, {high} high detected)\"\n  else:\n    question = \"What should Ralph avoid? (no high-severity constraints)\"\n```\n\n**Example transformation**:\n\nNDJSON input:\n```\n{\"severity\":\"critical\",\"description\":\"Hardcoded API key in config.py\",\"file\":\"config.py\",\"line\":42,\"recommendation\":\"Move to env var\"}\n{\"severity\":\"high\",\"description\":\"Circular import: core  utils\",\"file\":\"core.py\",\"line\":1,\"recommendation\":\"Extract interface\"}\n```\n\nBecomes AUQ options:\n```yaml\noptions:\n  - label: \"Hardcoded API key in config.py\"\n    description: \"(CRITICAL) config.py:42 - Move to env var\"\n  - label: \"Circular import: core  utils\"\n    description: \"(HIGH) core.py:1 - Extract interface\"\n  - label: \"Documentation updates\"\n    description: \"README, CHANGELOG, docstrings, comments\"\n  - label: \"Dependency upgrades\"\n    description: \"Version bumps, renovate PRs, package updates\"\n```\n\nUse AskUserQuestion with the dynamically built options above.\n\n**Static fallback categories** (used when no constraints or to fill remaining slots):\n- \"Documentation updates\" - \"README, CHANGELOG, docstrings, comments\"\n- \"Dependency upgrades\" - \"Version bumps, renovate PRs, package updates\"\n- \"Refactoring\" - \"Code restructuring without behavior change\"\n- \"CI/CD modifications\" - \"Workflow files, GitHub Actions, pipelines\"\n\n**If total=0**: Show only 4 static categories with question `\"What should Ralph avoid? (no constraints detected)\"`\n\n---\n\n## Step 1.6.4: Custom Forbidden (Follow-up)\n\nAfter multiSelect, ask for custom additions:\n\nUse AskUserQuestion:\n\n- question: \"Add custom forbidden items? (comma-separated)\"\n  header: \"Custom\"\n  multiSelect: false\n  options:\n  - label: \"Enter custom items\"\n    description: \"Type additional forbidden phrases, e.g., 'database migrations, API changes'\"\n  - label: \"Skip custom items\"\n    description: \"Use only selected categories above\"\n\nIf \"Enter custom items\" selected  Parse user's \"Other\" input, split by comma, trim whitespace.\n\n---\n\n## Step 1.6.5: Encouraged Items (multiSelect, closed list)\n\nUse AskUserQuestion:\n\n- question: \"What should Ralph prioritize? (Select all that apply)\"\n  header: \"Encouraged\"\n  multiSelect: true\n  options:\n  - label: \"ROADMAP P0 items\"\n    description: \"Highest priority tasks from project roadmap\"\n  - label: \"Performance improvements\"\n    description: \"Speed, memory, efficiency optimizations\"\n  - label: \"Bug fixes\"\n    description: \"Fix known issues and regressions\"\n  - label: \"Research experiments\"\n    description: \"Try new approaches (Alpha Forge /research)\"\n\n---\n\n## Step 1.6.6: Custom Encouraged (Follow-up)\n\nSame pattern as 1.6.4:\n\nUse AskUserQuestion:\n\n- question: \"Add custom encouraged items? (comma-separated)\"\n  header: \"Custom\"\n  multiSelect: false\n  options:\n  - label: \"Enter custom items\"\n    description: \"Type additional encouraged phrases, e.g., 'Sharpe ratio, feature engineering'\"\n  - label: \"Skip custom items\"\n    description: \"Use only selected categories above\"\n\n---\n\n## Step 1.6.7: Update Config (with Validation + Learned Behavior)\n\n**IMPORTANT**: After collecting responses from Steps 1.6.3-1.6.6, you MUST:\n1. Write guidance to config WITH validation\n2. Append constraint-derived selections to `.jsonl` for learned filtering\n\n1. **Collect responses** from the AUQ steps above:\n   - `FORBIDDEN_ITEMS`: Selected labels from 1.6.3 + custom items from 1.6.4 (if any)\n   - `ENCOURAGED_ITEMS`: Selected labels from 1.6.5 + custom items from 1.6.6 (if any)\n   - `SELECTED_CONSTRAINT_IDS`: IDs of constraint-derived options user selected (from NDJSON parsing)\n\n2. **Write to config with post-write validation** using the Bash tool (substitute actual values):\n\n```bash\n/usr/bin/env bash << 'GUIDANCE_WRITE_SCRIPT'\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nCONFIG_FILE=\"$PROJECT_DIR/.claude/ralph-config.json\"\nSCAN_FILE=\"$PROJECT_DIR/.claude/ralph-constraint-scan.jsonl\"\nACK_FILE=\"$PROJECT_DIR/.claude/ralph-acknowledged-constraints.jsonl\"\nBACKUP_FILE=\"${CONFIG_FILE}.backup\"\n\n# Substitute these with actual AUQ responses:\nFORBIDDEN_JSON='[\"Documentation updates\", \"Dependency upgrades\"]'  # From 1.6.3 + 1.6.4\nENCOURAGED_JSON='[\"ROADMAP P0 items\", \"Research experiments\"]'     # From 1.6.5 + 1.6.6\n\n# Substitute with constraint IDs user selected (from NDJSON constraint options)\nSELECTED_CONSTRAINT_IDS=\"hardcoded-001 hardcoded-002\"  # From 1.6.3 constraint-derived selections\n\n# Generate timestamp\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\n# Load constraint scan data (if exists) for persistence\nCONSTRAINT_SCAN_JSON='null'\nif [[ -f \"$SCAN_FILE\" ]]; then\n    METADATA=$(grep '\"_type\":\"metadata\"' \"$SCAN_FILE\" 2>/dev/null | head -1)\n    CONSTRAINTS=$(grep '\"_type\":\"constraint\"' \"$SCAN_FILE\" 2>/dev/null | jq -s '.' 2>/dev/null || echo '[]')\n    BUSYWORK=$(grep '\"_type\":\"busywork\"' \"$SCAN_FILE\" 2>/dev/null | jq -s '.' 2>/dev/null || echo '[]')\n\n    CONSTRAINT_SCAN_JSON=$(jq -n \\\n        --argjson metadata \"$METADATA\" \\\n        --argjson constraints \"$CONSTRAINTS\" \\\n        --argjson busywork \"$BUSYWORK\" \\\n        '{\n            scan_timestamp: $metadata.scan_timestamp,\n            project_dir: $metadata.project_dir,\n            worktree_type: $metadata.worktree_type,\n            constraints: $constraints,\n            builtin_busywork: $busywork\n        }' 2>/dev/null || echo 'null')\nfi\n\n# Create backup before write\nmkdir -p \"$PROJECT_DIR/.claude\"\nif [[ -f \"$CONFIG_FILE\" ]]; then\n    cp \"$CONFIG_FILE\" \"$BACKUP_FILE\"\nfi\n\n# Write config\nif [[ -f \"$CONFIG_FILE\" ]]; then\n    jq --argjson forbidden \"$FORBIDDEN_JSON\" \\\n       --argjson encouraged \"$ENCOURAGED_JSON\" \\\n       --arg timestamp \"$TIMESTAMP\" \\\n       --argjson constraint_scan \"$CONSTRAINT_SCAN_JSON\" \\\n       '.guidance = {forbidden: $forbidden, encouraged: $encouraged, timestamp: $timestamp} | .constraint_scan = $constraint_scan' \\\n       \"$CONFIG_FILE\" > \"${CONFIG_FILE}.tmp\" && mv \"${CONFIG_FILE}.tmp\" \"$CONFIG_FILE\"\nelse\n    jq -n --argjson forbidden \"$FORBIDDEN_JSON\" \\\n          --argjson encouraged \"$ENCOURAGED_JSON\" \\\n          --arg timestamp \"$TIMESTAMP\" \\\n          --argjson constraint_scan \"$CONSTRAINT_SCAN_JSON\" \\\n          '{version: \"3.0.0\", guidance: {forbidden: $forbidden, encouraged: $encouraged, timestamp: $timestamp}, constraint_scan: $constraint_scan}' \\\n          > \"$CONFIG_FILE\"\nfi\n\n# === LEARNED BEHAVIOR: Append to .jsonl ===\nif [[ -n \"$SELECTED_CONSTRAINT_IDS\" && -f \"$SCAN_FILE\" ]]; then\n    for CONSTRAINT_ID in $SELECTED_CONSTRAINT_IDS; do\n        if [[ -f \"$ACK_FILE\" ]] && grep -q \"\\\"id\\\":\\\"$CONSTRAINT_ID\\\"\" \"$ACK_FILE\" 2>/dev/null; then\n            continue\n        fi\n        CONSTRAINT_DATA=$(grep \"\\\"id\\\":\\\"$CONSTRAINT_ID\\\"\" \"$SCAN_FILE\" 2>/dev/null | head -1 | \\\n            jq -c --arg ts \"$TIMESTAMP\" '. + {acknowledged_at: $ts}' 2>/dev/null)\n        if [[ -n \"$CONSTRAINT_DATA\" ]]; then\n            echo \"$CONSTRAINT_DATA\" >> \"$ACK_FILE\"\n        fi\n    done\n    NEW_ACK_COUNT=$(echo \"$SELECTED_CONSTRAINT_IDS\" | wc -w | tr -d ' ')\n    echo \"=== LEARNED BEHAVIOR ===\"\n    echo \"Appended $NEW_ACK_COUNT constraint(s) to $ACK_FILE\"\nfi\n\n# === POST-WRITE VALIDATION ===\nvalidate_config() {\n    local file=\"$1\"\n    [[ -f \"$file\" && -r \"$file\" ]] || return 1\n    jq empty \"$file\" >/dev/null 2>&1 || return 2\n    jq -e '.guidance.forbidden and .guidance.encouraged and .guidance.timestamp' \"$file\" >/dev/null 2>&1 || return 3\n    jq -e '.guidance.forbidden | type == \"array\"' \"$file\" >/dev/null 2>&1 || return 4\n    jq -e '.guidance.encouraged | type == \"array\"' \"$file\" >/dev/null 2>&1 || return 5\n    return 0\n}\n\nif validate_config \"$CONFIG_FILE\"; then\n    echo \" Guidance saved to $CONFIG_FILE\"\n    echo \"\"\n    echo \"=== VALIDATION PASSED ===\"\n    jq '{guidance: .guidance}' \"$CONFIG_FILE\"\n    rm -f \"$BACKUP_FILE\"\nelse\n    VALIDATION_ERROR=$?\n    echo \" VALIDATION FAILED (error code: $VALIDATION_ERROR)\"\n    echo \"=== ROLLING BACK ===\"\n    if [[ -f \"$BACKUP_FILE\" ]]; then\n        mv \"$BACKUP_FILE\" \"$CONFIG_FILE\"\n        echo \"Restored previous config from backup\"\n    else\n        rm -f \"$CONFIG_FILE\"\n        echo \"Removed invalid config\"\n    fi\n    exit 1\nfi\nGUIDANCE_WRITE_SCRIPT\n```\n\n**Key substitutions Claude MUST make**:\n- `FORBIDDEN_JSON`: Array of selected forbidden labels\n- `ENCOURAGED_JSON`: Array of selected encouraged labels\n- `SELECTED_CONSTRAINT_IDS`: Space-separated list of constraint IDs from NDJSON options user selected\n\n---\n\n## Output\n\nAfter completing all steps, the skill returns control to `/ralph:start` Step 2 (Execution).\n\nThe config file `.claude/ralph-config.json` will contain:\n```json\n{\n  \"version\": \"3.0.0\",\n  \"guidance\": {\n    \"forbidden\": [\"Documentation updates\", \"...\"],\n    \"encouraged\": [\"ROADMAP P0 items\", \"...\"],\n    \"timestamp\": \"2026-01-01T00:00:00Z\"\n  },\n  \"constraint_scan\": { ... }\n}\n```"
              }
            ]
          },
          {
            "name": "iterm2-layout-config",
            "description": "iTerm2 workspace layout configuration with TOML-based separation of private paths from publishable code",
            "source": "./plugins/iterm2-layout-config/",
            "category": "development",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install iterm2-layout-config@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "iterm2-layout",
                "description": "Configure iTerm2 workspace layouts with TOML-based configuration. Use when user mentions iTerm2 layout, workspace tabs, layout.toml, AutoLaunch script, or configuring terminal workspaces.",
                "path": "plugins/iterm2-layout-config/skills/iterm2-layout/SKILL.md",
                "frontmatter": {
                  "name": "iterm2-layout",
                  "description": "Configure iTerm2 workspace layouts with TOML-based configuration. Use when user mentions iTerm2 layout, workspace tabs, layout.toml, AutoLaunch script, or configuring terminal workspaces."
                },
                "content": "# iTerm2 Layout Configuration\n\n<!-- ADR: /docs/adr/2025-12-15-iterm2-layout-config.md -->\n\nConfigure iTerm2 workspace layouts with proper separation of concerns: private paths in TOML config, publishable code in Python script.\n\n## Triggers\n\nInvoke this skill when user mentions:\n\n- \"iTerm2 layout\"\n- \"workspace tabs\"\n- \"layout.toml\"\n- \"AutoLaunch script\"\n- \"default-layout.py\"\n- \"configure terminal workspaces\"\n- \"add workspace tab\"\n\n## Configuration Overview\n\n### File Locations\n\n| File             | Location                               | Purpose                |\n| ---------------- | -------------------------------------- | ---------------------- |\n| Config (private) | `~/.config/iterm2/layout.toml`         | User's workspace paths |\n| Script (public)  | `~/scripts/iterm2/default-layout.py`   | Layout logic           |\n| Template         | `~/scripts/iterm2/layout.example.toml` | Example config         |\n\n### Config File Format\n\n```toml\n# ~/.config/iterm2/layout.toml\n\n[layout]\nleft_pane_ratio = 0.20    # 0.0 to 1.0\nsettle_time = 0.3         # seconds\n\n[commands]\nleft = \"br --sort-by-type-dirs-first\"\nright = \"zsh\"\n\n[worktrees]\n# Optional: Enable git worktree discovery\n# main_repo_root = \"~/projects/my-project\"\n# worktree_pattern = \"my-project.worktree-*\"\n\n[[tabs]]\nname = \"home\"\ndir = \"~\"\n\n[[tabs]]\nname = \"projects\"\ndir = \"~/projects\"\n\n[[tabs]]\ndir = \"~/Documents\"  # name defaults to \"Documents\"\n```\n\n## Setup Instructions\n\n### First-Time Setup\n\n```bash\n/usr/bin/env bash << 'CONFIG_EOF'\n# 1. Ensure config directory exists\nmkdir -p ~/.config/iterm2\n\n# 2. Copy template\ncp ~/scripts/iterm2/layout.example.toml ~/.config/iterm2/layout.toml\n\n# 3. Edit with your workspace paths\n# Add [[tabs]] entries for each workspace\n\n# 4. Restart iTerm2 to test\nCONFIG_EOF\n```\n\n### Adding a New Tab\n\nAdd a `[[tabs]]` entry to `~/.config/iterm2/layout.toml`:\n\n```toml\n[[tabs]]\nname = \"MyProject\"  # Tab display name (optional)\ndir = \"~/path/to/project\"\n```\n\n**Name field**:\n\n- If omitted, uses directory basename\n- Custom names useful for abbreviations (e.g., \"AF\" instead of \"alpha-forge\")\n\n### Removing a Tab\n\nDelete or comment out the `[[tabs]]` entry:\n\n```toml\n# [[tabs]]\n# name = \"OldProject\"\n# dir = \"~/old/project\"\n```\n\n## Configuration Schema\n\n| Section       | Key                | Type   | Default        | Description               |\n| ------------- | ------------------ | ------ | -------------- | ------------------------- |\n| `[layout]`    | `left_pane_ratio`  | float  | 0.20           | Left pane width (0.0-1.0) |\n| `[layout]`    | `settle_time`      | float  | 0.3            | Wait after cd (seconds)   |\n| `[commands]`  | `left`             | string | br...          | Left pane command         |\n| `[commands]`  | `right`            | string | zsh            | Right pane command        |\n| `[worktrees]` | `alpha_forge_root` | string | null           | Worktree root (optional)  |\n| `[worktrees]` | `worktree_pattern` | string | `*.worktree-*` | Glob pattern              |\n| `[[tabs]]`    | `dir`              | string | **required**   | Directory path            |\n| `[[tabs]]`    | `name`             | string | basename       | Tab display name          |\n\n## Troubleshooting\n\n### Error: \"Layout configuration not found\"\n\n**Symptom**: Script Console shows error about missing config\n\n**Solution**:\n\n```bash\n# Create config from template\ncp ~/scripts/iterm2/layout.example.toml ~/.config/iterm2/layout.toml\n```\n\n### Error: \"Invalid TOML syntax\"\n\n**Symptom**: Script Console shows TOML parse error\n\n**Solution**:\n\n1. Check TOML syntax (quotes, brackets)\n2. Validate with: `python3 -c \"import tomllib; tomllib.load(open('~/.config/iterm2/layout.toml', 'rb'))\"`\n\n### Tabs Not Appearing\n\n**Symptom**: iTerm2 opens but no custom tabs created\n\n**Causes**:\n\n1. No `[[tabs]]` entries in config\n2. Config file in wrong location\n3. Script not in AutoLaunch\n\n**Solution**:\n\n```bash\n# Verify config location\nls -la ~/.config/iterm2/layout.toml\n\n# Verify AutoLaunch symlink\nls -la ~/Library/Application\\ Support/iTerm2/Scripts/AutoLaunch/\n\n# Check Script Console for errors\n# iTerm2 > Scripts > Manage > Console\n```\n\n### Directory Does Not Exist Warning\n\n**Symptom**: Tab skipped with warning in Script Console\n\n**Solution**: Verify directory path exists or create it:\n\n```bash\nmkdir -p ~/path/to/missing/directory\n```\n\n## Error Handling Behavior\n\nThe script uses \"print + early return\" pattern:\n\n1. **Missing config**: Logs instructions to Script Console, exits cleanly\n2. **Invalid TOML**: Logs parse error with details, exits cleanly\n3. **Missing directory**: Logs warning, skips tab, continues with others\n\n**Viewing errors**: Scripts > Manage > Console in iTerm2\n\n## Git Worktree Detection (Optional)\n\nEnable dynamic tab creation for git worktrees:\n\n```toml\n[worktrees]\nmain_repo_root = \"~/projects/my-project\"\nworktree_pattern = \"my-project.worktree-*\"\n```\n\n**How it works**:\n\n1. Script globs for `~/projects/my-project.worktree-*` directories\n2. Validates each against `git worktree list`\n3. Generates acronym-based tab names (e.g., `AF-ssv` for `sharpe-statistical-validation`)\n4. Inserts worktree tabs after main project tab\n\n## References\n\n- [iTerm2 Python API](https://iterm2.com/python-api/)\n- [TOML Specification](https://toml.io/)\n- [XDG Base Directory Spec](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html)\n- [ADR: iTerm2 Layout Config](/docs/adr/2025-12-15-iterm2-layout-config.md)"
              }
            ]
          },
          {
            "name": "statusline-tools",
            "description": "Custom status line with git status, link validation (L), and path linting (P) indicators",
            "source": "./plugins/statusline-tools/",
            "category": "utilities",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install statusline-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/hooks",
                "description": "Install/uninstall statusline-tools Stop hook to ~/.claude/settings.json",
                "path": "plugins/statusline-tools/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall statusline-tools Stop hook to ~/.claude/settings.json",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead, AskUserQuestion",
                  "argument-hint": "[install|uninstall|status]"
                },
                "content": "# Status Line Hooks Manager\n\nManage Stop hook installation for link validation and path linting.\n\nThe Stop hook runs at session end to:\n\n1. Validate markdown links using lychee\n2. Check for relative path violations using lint-relative-paths\n3. Cache results for status line display\n\n## Actions\n\n| Action      | Description                         |\n| ----------- | ----------------------------------- |\n| `install`   | Add Stop hook to settings.json      |\n| `uninstall` | Remove Stop hook from settings.json |\n| `status`    | Show current hook configuration     |\n\n## Coexistence Note\n\nThis hook can coexist with other Stop hooks (like check-links-hybrid.sh). Both will run on session end - statusline-tools caches results for display, while other hooks may take different actions.\n\n## Execution\n\n### Skip Logic\n\n- If action provided (`install`, `uninstall`, `status`) -> execute directly\n- If no arguments -> check current status, then use AskUserQuestion flow\n\n### Workflow\n\n1. **Check Current State**: Run `status` to show current hook configuration\n2. **Action Selection**: Use AskUserQuestion to select action:\n   - \"Install hook\" -> add Stop hook for link validation\n   - \"Uninstall hook\" -> remove Stop hook\n   - \"Just show status\" -> display and exit\n3. **Execute**: Run the management script\n4. **Verify**: Confirm changes applied\n\n### AskUserQuestion Flow (No Arguments)\n\nWhen invoked without arguments, guide the user interactively:\n\n```\nQuestion: \"What would you like to do with the statusline-tools Stop hook?\"\nOptions:\n  - \"Install\" -> \"Add Stop hook for link validation and path linting on session end\"\n  - \"Uninstall\" -> \"Remove the Stop hook from settings.json\"\n  - \"Status\" -> \"Show current hook configuration\"\n```\n\n### Direct Execution (With Arguments)\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'HOOKS_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/statusline-tools}\"\nACTION=\"${ARGUMENTS:-status}\"\nbash \"$PLUGIN_DIR/scripts/manage-hooks.sh\" $ACTION\nHOOKS_SCRIPT_EOF\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nHooks are loaded at session start. Modifications to settings.json require a restart."
              },
              {
                "name": "/ignore",
                "description": "Manage global ignore patterns for lint-relative-paths",
                "path": "plugins/statusline-tools/commands/ignore.md",
                "frontmatter": {
                  "description": "Manage global ignore patterns for lint-relative-paths",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead, AskUserQuestion",
                  "argument-hint": "[add|list|remove] [pattern]"
                },
                "content": "# Global Ignore Patterns\n\nManage global ignore patterns for the `lint-relative-paths` linter.\n\n## Purpose\n\nSome repositories intentionally use relative paths in markdown (e.g., `../docs/file.md`)\ninstead of repo-root paths (e.g., `/docs/file.md`). This command manages a global ignore\nfile that skips path validation for matching workspaces.\n\n## Actions\n\n| Action   | Description                             | Example                                   |\n| -------- | --------------------------------------- | ----------------------------------------- |\n| `add`    | Add a pattern to the global ignore file | `/statusline-tools:ignore add my-repo`    |\n| `list`   | Show current patterns                   | `/statusline-tools:ignore list`           |\n| `remove` | Remove a pattern from the ignore file   | `/statusline-tools:ignore remove my-repo` |\n\n## Pattern Matching\n\nPatterns use **substring matching**. A pattern matches if the workspace path contains the pattern.\n\n**Example**: Pattern `alpha-forge` matches:\n\n- `/Users/user/projects/alpha-forge`\n- `/Users/user/eon/alpha-forge.worktree-feature-x`\n- `/home/user/code/alpha-forge-v2`\n\n## Ignore File Location\n\n`~/.claude/lint-relative-paths-ignore`\n\nLines starting with `#` are comments.\n\n## Execution\n\n### Skip Logic\n\n- If action + pattern provided -> execute directly\n- If only `list` provided -> show patterns immediately\n- If no arguments -> use AskUserQuestion flow\n\n### Workflow\n\n1. **Check Current State**: Run `list` to show existing patterns\n2. **Action Selection**: Use AskUserQuestion to select action:\n   - \"Add a new pattern\" -> prompt for pattern\n   - \"Remove an existing pattern\" -> show current patterns to select\n   - \"Just view current patterns\" -> display and exit\n3. **Pattern Input**: For add/remove, AskUserQuestion with examples\n4. **Execute**: Run the management script\n5. **Verify**: Confirm changes applied\n\n### AskUserQuestion Flow (No Arguments)\n\nWhen invoked without arguments, guide the user interactively:\n\n```\nQuestion: \"What would you like to do with lint-relative-paths ignore patterns?\"\nOptions:\n  - \"Add pattern\" -> \"Add a new repository pattern to skip path linting\"\n  - \"List patterns\" -> \"Show all current ignore patterns\"\n  - \"Remove pattern\" -> \"Remove an existing pattern from the ignore list\"\n```\n\nFor \"Add pattern\":\n\n```\nQuestion: \"Enter the repository pattern to ignore\"\nNote: Patterns use substring matching. Example: 'alpha-forge' matches any path containing 'alpha-forge'.\n```\n\n### Direct Execution (With Arguments)\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'IGNORE_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/statusline-tools}\"\nbash \"$PLUGIN_DIR/scripts/manage-ignore.sh\" $ARGUMENTS\nIGNORE_SCRIPT_EOF\n```\n\n## Manual Editing\n\nThe ignore file can also be edited manually:\n\n```bash\n# View current patterns\ncat ~/.claude/lint-relative-paths-ignore\n\n# Add a pattern manually\necho \"my-repo-pattern\" >> ~/.claude/lint-relative-paths-ignore\n```"
              },
              {
                "name": "/setup",
                "description": "Configure statusline-tools status line and dependencies",
                "path": "plugins/statusline-tools/commands/setup.md",
                "frontmatter": {
                  "description": "Configure statusline-tools status line and dependencies",
                  "allowed-tools": "Read, Bash, TodoWrite, TodoRead, AskUserQuestion",
                  "argument-hint": "[install|uninstall|status|deps]"
                },
                "content": "# Status Line Setup\n\nManage custom status line installation and dependencies.\n\n## Actions\n\n| Action      | Description                                 |\n| ----------- | ------------------------------------------- |\n| `install`   | Install status line to settings.json        |\n| `uninstall` | Remove status line from settings.json       |\n| `status`    | Show current configuration and dependencies |\n| `deps`      | Install lychee via mise                     |\n\n## Execution\n\n### Skip Logic\n\n- If action provided (`install`, `uninstall`, `status`, `deps`) -> execute directly\n- If no arguments -> check current status, then use AskUserQuestion flow\n\n### Workflow\n\n1. **Check Current State**: Run `status` to show current configuration\n2. **Action Selection**: Use AskUserQuestion to select action:\n   - \"Install status line\" -> configure settings.json\n   - \"Uninstall status line\" -> remove configuration\n   - \"Install dependencies\" -> install lychee via mise\n   - \"Just show status\" -> display and exit\n3. **Execute**: Run the management script\n4. **Verify**: Confirm changes applied\n\n### AskUserQuestion Flow (No Arguments)\n\nWhen invoked without arguments, guide the user interactively:\n\n```\nQuestion: \"What would you like to do with statusline-tools?\"\nOptions:\n  - \"Install\" -> \"Install the custom status line to settings.json\"\n  - \"Uninstall\" -> \"Remove the status line configuration\"\n  - \"Install deps\" -> \"Install lychee for link validation via mise\"\n  - \"Status\" -> \"Show current configuration and dependencies\"\n```\n\n### Direct Execution (With Arguments)\n\nParse `$ARGUMENTS` and run the management script:\n\n```bash\n/usr/bin/env bash << 'SETUP_SCRIPT_EOF'\nPLUGIN_DIR=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/plugins/marketplaces/cc-skills/plugins/statusline-tools}\"\nACTION=\"${ARGUMENTS:-status}\"\nbash \"$PLUGIN_DIR/scripts/manage-statusline.sh\" $ACTION\nSETUP_SCRIPT_EOF\n```\n\n## Post-Action Reminder\n\nAfter install/uninstall operations:\n\n**IMPORTANT: Restart Claude Code session for changes to take effect.**\n\nThe statusLine is loaded at session start. Modifications to settings.json require a restart."
              }
            ],
            "skills": []
          },
          {
            "name": "notion-api",
            "description": "Notion API integration using notion-client Python SDK - create pages, manipulate blocks, query databases with preflight credential prompting",
            "source": "./plugins/notion-api/",
            "category": "productivity",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install notion-api@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "notion-sdk",
                "description": "Control Notion via Python SDK. TRIGGERS - Notion API, create page, query database, add blocks, automate Notion. PREFLIGHT - requires token from notion.so/my-integrations.",
                "path": "plugins/notion-api/skills/notion-sdk/SKILL.md",
                "frontmatter": {
                  "name": "notion-sdk",
                  "description": "Control Notion via Python SDK. TRIGGERS - Notion API, create page, query database, add blocks, automate Notion. PREFLIGHT - requires token from notion.so/my-integrations.",
                  "allowed-tools": "Read, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "# Notion SDK Skill\n\nControl Notion programmatically using the official `notion-client` Python SDK (v2.6.0+).\n\n## Preflight: Token Collection\n\nBefore any Notion API operation, collect the integration token:\n\n```\nAskUserQuestion(questions=[{\n    \"question\": \"Please provide your Notion Integration Token (starts with ntn_ or secret_)\",\n    \"header\": \"Notion Token\",\n    \"options\": [\n        {\"label\": \"I have a token ready\", \"description\": \"Token from notion.so/my-integrations\"},\n        {\"label\": \"Need to create one\", \"description\": \"Go to notion.so/my-integrations  New integration\"}\n    ],\n    \"multiSelect\": false\n}])\n```\n\nAfter user provides token:\n\n1. Validate format (must start with `ntn_` or `secret_`)\n2. Test with `validate_token()` from `scripts/notion_wrapper.py`\n3. Remind user: **Each page/database must be shared with the integration**\n\n## Quick Start\n\n### 1. Create a Page in Database\n\n```python\nfrom notion_client import Client\nfrom scripts.create_page import (\n    create_database_page,\n    title_property,\n    status_property,\n    date_property,\n)\n\nclient = Client(auth=\"ntn_...\")\npage = create_database_page(\n    client,\n    data_source_id=\"abc123...\",  # Database ID\n    properties={\n        \"Name\": title_property(\"My New Task\"),\n        \"Status\": status_property(\"In Progress\"),\n        \"Due Date\": date_property(\"2025-12-31\"),\n    }\n)\nprint(f\"Created: {page['url']}\")\n```\n\n### 2. Add Content Blocks\n\n```python\nfrom scripts.add_blocks import (\n    append_blocks,\n    heading,\n    paragraph,\n    bullet,\n    code_block,\n    callout,\n)\n\nblocks = [\n    heading(\"Overview\", level=2),\n    paragraph(\"This page was created via the Notion API.\"),\n    callout(\"Remember to share the page with your integration!\", emoji=\"\"),\n    heading(\"Tasks\", level=3),\n    bullet(\"First task\"),\n    bullet(\"Second task\"),\n    code_block(\"print('Hello, Notion!')\", language=\"python\"),\n]\nappend_blocks(client, page[\"id\"], blocks)\n```\n\n### 3. Query Database\n\n```python\nfrom scripts.query_database import (\n    query_data_source,\n    checkbox_filter,\n    status_filter,\n    and_filter,\n    sort_by_property,\n)\n\n# Find incomplete high-priority items\nresults = query_data_source(\n    client,\n    data_source_id=\"abc123...\",\n    filter_obj=and_filter(\n        checkbox_filter(\"Done\", False),\n        status_filter(\"Priority\", \"High\")\n    ),\n    sorts=[sort_by_property(\"Due Date\", \"ascending\")]\n)\nfor page in results:\n    title = page[\"properties\"][\"Name\"][\"title\"][0][\"plain_text\"]\n    print(f\"- {title}\")\n```\n\n## Available Scripts\n\n| Script              | Purpose                                       |\n| ------------------- | --------------------------------------------- |\n| `notion_wrapper.py` | Client setup, token validation, retry wrapper |\n| `create_page.py`    | Create pages, property builders               |\n| `add_blocks.py`     | Append blocks, block type builders            |\n| `query_database.py` | Query, filter, sort, search                   |\n\n## References\n\n- [Property Types](./references/property-types.md) - All 24 property types with examples\n- [Block Types](./references/block-types.md) - All block types with structures\n- [Rich Text](./references/rich-text.md) - Formatting, links, mentions\n- [Pagination](./references/pagination.md) - Handling large result sets\n\n## Important Constraints\n\n### Rate Limits\n\n- **3 requests/second** average (burst tolerated briefly)\n- Use `api_call_with_retry()` for automatic rate limit handling\n- 429 responses include `Retry-After` header\n\n### Authentication Model\n\n- **Page-level sharing** required (not workspace-wide)\n- User must explicitly add integration to each page/database:\n  - Page  ... menu  Connections  Add connection  Select integration\n\n### API Version (v2.6.0+)\n\n- Uses `data_source_id` instead of `database_id` for multi-source databases\n- Legacy `database_id` still works for simple databases\n- Scripts handle both patterns automatically\n\n### Operations NOT Supported\n\n- Workspace settings modification\n- User permissions management\n- Template creation/management\n- Billing/subscription access\n\n## API Behavior Patterns\n\nInsights discovered through integration testing (test citations for verification).\n\n### Rate Limiting & Retry Logic\n\n`api_call_with_retry()` handles transient failures automatically:\n\n| Error Type       | Behavior          | Wait Strategy                            |\n| ---------------- | ----------------- | ---------------------------------------- |\n| 429 Rate Limited | Retries           | Respects Retry-After header (default 1s) |\n| 500 Server Error | Retries           | Exponential backoff: 1s, 2s, 4s          |\n| Auth/Validation  | Fails immediately | No retry                                 |\n\n_Citation: `test_client.py::TestRetryLogic` (lines 146-193)_\n\n### Read-After-Write Consistency\n\nNewly created blocks may not be immediately queryable. Add 0.5s minimum delay:\n\n```python\nappend_blocks(client, page_id, blocks)\ntime.sleep(0.5)  # Eventual consistency delay\nchildren = client.blocks.children.list(page_id)\n```\n\n_Citation: `test_integration.py::TestBlockAppend::test_retrieve_appended_blocks` (line 298)_\n\n### v2.6.0 API Migration\n\n| Old Pattern                     | New Pattern (v2.6.0+)              |\n| ------------------------------- | ---------------------------------- |\n| `client.databases.query()`      | `client.data_sources.query()`      |\n| `filter: {\"value\": \"database\"}` | `filter: {\"value\": \"data_source\"}` |\n\n_Citation: `test_integration.py::TestDatabaseQuery` (line 110)_\n\n### Archive-Only Deletion\n\nPages cannot be permanently deleted via API - only archived (moved to trash):\n\n```python\nclient.pages.update(page_id, archived=True)  # Trash, not delete\n```\n\n_Citation: `test_integration.py` cleanup fixture (lines 72-76)_\n\n## Edge Cases & Validation\n\n### Property Builder Edge Cases\n\n| Input             | Behavior                      | Valid? |\n| ----------------- | ----------------------------- | ------ |\n| Empty string `\"\"` | Creates empty content         | Yes    |\n| Empty array `[]`  | Clears multi-select/relations | Yes    |\n| `None` for number | Clears property value         | Yes    |\n| Zero `0`          | Valid number (not falsy)      | Yes    |\n| Negative `-42`    | Valid number                  | Yes    |\n| Unicode/emoji     | Fully preserved               | Yes    |\n\n_Citation: `test_property_builders.py::TestPropertyBuildersEdgeCases` (lines 302-341)_\n\n### Input Validation Responsibility\n\nBuilders are intentionally permissive - validation happens at API level:\n\n| Property | Builder Accepts | API Validates    |\n| -------- | --------------- | ---------------- |\n| Date     | Any string      | ISO 8601 only    |\n| URL      | Any string      | Valid URL format |\n| Checkbox | Truthy values   | Boolean expected |\n\n**Best Practice**: Validate in your application before building properties.\n\n_Citation: `test_property_builders.py::TestPropertyBuildersInvalidInputs` (lines 347-376)_\n\n### Token Validation\n\n- Case-sensitive: Only lowercase `ntn_` and `secret_` valid\n- Format check happens before API call (saves unnecessary requests)\n- Empty/whitespace tokens rejected immediately\n\n_Citation: `test_client.py::TestClientEdgeCases` (lines 196-224)_\n\n## Query & Filter Patterns\n\n### Compound Filter Composition\n\n```python\n# Empty compound (matches all)\nand_filter()  # {\"and\": []}\n\n# Deep nesting supported\nand_filter(\n    or_filter(filter_a, filter_b),\n    and_filter(filter_c, filter_d)\n)\n```\n\n_Citation: `test_filter_builders.py::TestFilterEdgeCases` (lines 323-360)_\n\n### Filter Limitations\n\nFilters don't exclude NULL properties - check in Python:\n\n```python\nif row[\"properties\"][\"Rating\"][\"number\"] is not None:\n    # Process non-null values\n```\n\n_Citation: `test_integration.py::TestDatabaseQuery::test_query_database_with_filter` (lines 120-135)_\n\n### Pagination Invariants\n\n| Condition          | `has_more` | `next_cursor`      |\n| ------------------ | ---------- | ------------------ |\n| More results exist | `True`     | Present, non-None  |\n| No more results    | `False`    | May be absent/None |\n\nAlways check `has_more` before using `next_cursor`.\n\n_Citation: `test_integration.py::TestDatabaseQuery::test_query_database_with_pagination` (lines 137-151)_\n\n## Error Handling\n\n```python\nfrom notion_client import APIResponseError, APIErrorCode\n\ntry:\n    result = client.pages.create(...)\nexcept APIResponseError as e:\n    if e.code == APIErrorCode.ObjectNotFound:\n        print(\"Page/database not found or not shared with integration\")\n    elif e.code == APIErrorCode.Unauthorized:\n        print(\"Token invalid or expired\")\n    elif e.code == APIErrorCode.RateLimited:\n        print(f\"Rate limited. Retry after {e.additional_data.get('retry_after')}s\")\n    else:\n        raise\n```\n\n## Installation\n\n```bash\nuv pip install notion-client>=2.6.0\n```\n\nOr use PEP 723 inline dependencies (scripts include them)."
              }
            ]
          },
          {
            "name": "asciinema-tools",
            "description": "Terminal recording automation: asciinema capture, launchd daemon for background chunking, Keychain PAT storage, Pushover notifications, cast conversion, and semantic analysis",
            "source": "./plugins/asciinema-tools/",
            "category": "utilities",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install asciinema-tools@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Semantic analysis of converted recordings. TRIGGERS - analyze cast, keyword extraction, find patterns.",
                "path": "plugins/asciinema-tools/commands/analyze.md",
                "frontmatter": {
                  "description": "Semantic analysis of converted recordings. TRIGGERS - analyze cast, keyword extraction, find patterns.",
                  "allowed-tools": "Bash, Grep, AskUserQuestion, Read",
                  "argument-hint": "[file] [-d domains] [-t type] [--json] [--md] [--density] [--jump]"
                },
                "content": "# /asciinema-tools:analyze\n\nRun semantic analysis on converted .txt recordings.\n\n## Arguments\n\n| Argument        | Description                                |\n| --------------- | ------------------------------------------ |\n| `file`          | Path to .txt file                          |\n| `-d, --domains` | Domains: `trading,ml,dev,claude`           |\n| `-t, --type`    | Type: `curated`, `auto`, `full`, `density` |\n| `--json`        | Output in JSON format                      |\n| `--md`          | Save as markdown report                    |\n| `--density`     | Include density analysis                   |\n| `--jump`        | Jump to peak section after analysis        |\n\n## Execution\n\nInvoke the `asciinema-analyzer` skill with user-selected options.\n\n### Skip Logic\n\n- If `file` provided -> skip Phase 1 (file selection)\n- If `-t` provided -> skip Phase 2 (analysis type)\n- If `-d` provided -> skip Phase 3 (domain selection)\n- If `--json/--md` provided -> skip Phase 6 (report format)\n- If `--jump` provided -> auto-execute jump after analysis\n\n### Workflow\n\n1. **Preflight**: Check for .txt file\n2. **Discovery**: Find .txt files\n3. **Selection**: AskUserQuestion for file\n4. **Type**: AskUserQuestion for analysis type\n5. **Domain**: AskUserQuestion for domains (multi-select)\n6. **Curated**: Run ripgrep searches\n7. **Auto**: Run YAKE if selected\n8. **Density**: Calculate density windows if selected\n9. **Format**: AskUserQuestion for report format\n10. **Next**: AskUserQuestion for follow-up action"
              },
              {
                "name": "/backup",
                "description": "Stream-backup active recordings to GitHub. TRIGGERS - backup recording, sync cast, streaming backup.",
                "path": "plugins/asciinema-tools/commands/backup.md",
                "frontmatter": {
                  "description": "Stream-backup active recordings to GitHub. TRIGGERS - backup recording, sync cast, streaming backup.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob, Write",
                  "argument-hint": "[install|status|stop|history] [-r repo] [-i interval] [--chunk] [--meta]"
                },
                "content": "# /asciinema-tools:backup\n\nConfigure and manage streaming backup to GitHub orphan branch.\n\n## Arguments\n\n| Argument         | Description                            |\n| ---------------- | -------------------------------------- |\n| `install`        | Configure and start backup automation  |\n| `status`         | Show active backups and last sync      |\n| `stop`           | Disable backup for current session     |\n| `history`        | View recent backup commits             |\n| `-r, --repo`     | GitHub repository (e.g., `owner/repo`) |\n| `-i, --interval` | Sync interval (e.g., `30s`, `5m`)      |\n| `--chunk`        | Split at idle time                     |\n| `--meta`         | Include session metadata               |\n\n## Execution\n\nInvoke the `asciinema-streaming-backup` skill with user-selected options.\n\n### Skip Logic\n\n- If action provided -> skip Phase 1 (action selection)\n- If `-r` and `-i` provided -> skip Phase 2-3 (config and repo)\n\n### Workflow\n\n1. **Preflight**: Check gh CLI and fswatch\n2. **Action**: AskUserQuestion for action type\n3. **Config**: AskUserQuestion for backup settings\n4. **Repo**: AskUserQuestion for repository selection\n5. **Execute**: Run selected action"
              },
              {
                "name": "/bootstrap",
                "description": "Pre-session bootstrap - generates script to start recording BEFORE entering Claude Code. Chunking handled by daemon. TRIGGERS - bootstrap, pre-session, start recording, before claude.",
                "path": "plugins/asciinema-tools/commands/bootstrap.md",
                "frontmatter": {
                  "description": "Pre-session bootstrap - generates script to start recording BEFORE entering Claude Code. Chunking handled by daemon. TRIGGERS - bootstrap, pre-session, start recording, before claude.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob, Write, Read",
                  "argument-hint": "[-r repo] [-b branch] [--setup-orphan] [-y|--yes]"
                },
                "content": "# /asciinema-tools:bootstrap\n\nGenerate a bootstrap script that runs OUTSIDE Claude Code CLI to start a recording session.\n\n**Important**: Chunking is handled by the launchd daemon. Run `/asciinema-tools:daemon-setup` first if you haven't already.\n\n## Architecture\n\n```\n\n                        DAEMON-BASED RECORDING WORKFLOW                      \n\n                                                                             \n  1. ONE-TIME SETUP (if not done):                                           \n     /asciinema-tools:daemon-setup                                           \n      Configures launchd daemon with Keychain credentials                   \n                                                                             \n  2. GENERATE BOOTSTRAP (in Claude Code):                                    \n     /asciinema-tools:bootstrap                                              \n      Generates tmp/bootstrap-claude-session.sh                             \n                                                                             \n  3. EXIT CLAUDE and RUN BOOTSTRAP:                                          \n     $ ./tmp/bootstrap-claude-session.sh     NOT source!                    \n      Writes config for daemon                                              \n      Starts asciinema recording                                            \n                                                                             \n  4. WORK IN RECORDING:                                                      \n     $ claude                                                                \n      Daemon automatically pushes chunks to GitHub                          \n                                                                             \n  5. EXIT (two times):                                                       \n     Ctrl+D (exit Claude)  exit (end recording)                             \n      Daemon pushes final chunk                                             \n                                                                             \n\n```\n\n## Arguments\n\n| Argument         | Description                                          |\n| ---------------- | ---------------------------------------------------- |\n| `-r, --repo`     | GitHub repository (e.g., `owner/repo`)               |\n| `-b, --branch`   | Orphan branch name (default: `asciinema-recordings`) |\n| `--setup-orphan` | Force create orphan branch                           |\n| `-y, --yes`      | Skip confirmation prompts                            |\n\n## Execution\n\n### Phase 0: Preflight Check\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nMISSING=()\nfor tool in asciinema zstd git; do\n  command -v \"$tool\" &>/dev/null || MISSING+=(\"$tool\")\ndone\n\nif [[ ${#MISSING[@]} -gt 0 ]]; then\n  echo \"MISSING: ${MISSING[*]}\"\n  exit 1\nfi\n\necho \"PREFLIGHT: OK\"\nasciinema --version | head -1\n\n# Check daemon status\nif launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n  echo \"DAEMON: RUNNING\"\nelse\n  echo \"DAEMON: NOT_RUNNING\"\nfi\nPREFLIGHT_EOF\n```\n\n**If DAEMON: NOT_RUNNING, use AskUserQuestion:**\n\n```\nQuestion: \"The chunker daemon is not running. Chunks won't be pushed to GitHub without it.\"\nHeader: \"Daemon\"\nOptions:\n  - label: \"Run daemon setup (Recommended)\"\n    description: \"Switch to /asciinema-tools:daemon-setup to configure the daemon\"\n  - label: \"Continue anyway\"\n    description: \"Generate bootstrap script without daemon (local recording only)\"\n```\n\n### Phase 1: Detect Repository Context\n\n**MANDATORY**: Run before AskUserQuestion to auto-populate options.\n\n```bash\n/usr/bin/env bash << 'DETECT_CONTEXT_EOF'\nIN_GIT_REPO=\"false\"\nCURRENT_REPO_URL=\"\"\nCURRENT_REPO_OWNER=\"\"\nCURRENT_REPO_NAME=\"\"\nORPHAN_BRANCH_EXISTS=\"false\"\nLOCAL_CLONE_EXISTS=\"false\"\nORPHAN_BRANCH=\"asciinema-recordings\"\n\nif git rev-parse --git-dir &>/dev/null 2>&1; then\n  IN_GIT_REPO=\"true\"\n\n  if git remote get-url origin &>/dev/null 2>&1; then\n    CURRENT_REPO_URL=$(git remote get-url origin)\n  elif [[ -n \"$(git remote)\" ]]; then\n    REMOTE=$(git remote | head -1)\n    CURRENT_REPO_URL=$(git remote get-url \"$REMOTE\")\n  fi\n\n  if [[ -n \"$CURRENT_REPO_URL\" ]]; then\n    if [[ \"$CURRENT_REPO_URL\" =~ github\\.com[:/]([^/]+)/([^/.]+) ]]; then\n      CURRENT_REPO_OWNER=\"${BASH_REMATCH[1]}\"\n      CURRENT_REPO_NAME=\"${BASH_REMATCH[2]%.git}\"\n    fi\n\n    if git ls-remote --heads \"$CURRENT_REPO_URL\" \"$ORPHAN_BRANCH\" 2>/dev/null | grep -q \"$ORPHAN_BRANCH\"; then\n      ORPHAN_BRANCH_EXISTS=\"true\"\n    fi\n\n    LOCAL_CLONE_PATH=\"$HOME/asciinema_recordings/$CURRENT_REPO_NAME\"\n    if [[ -d \"$LOCAL_CLONE_PATH/.git\" ]]; then\n      LOCAL_CLONE_EXISTS=\"true\"\n    fi\n  fi\nfi\n\necho \"IN_GIT_REPO=$IN_GIT_REPO\"\necho \"CURRENT_REPO_URL=$CURRENT_REPO_URL\"\necho \"CURRENT_REPO_OWNER=$CURRENT_REPO_OWNER\"\necho \"CURRENT_REPO_NAME=$CURRENT_REPO_NAME\"\necho \"ORPHAN_BRANCH_EXISTS=$ORPHAN_BRANCH_EXISTS\"\necho \"LOCAL_CLONE_EXISTS=$LOCAL_CLONE_EXISTS\"\nDETECT_CONTEXT_EOF\n```\n\n### Phase 2: Repository Selection (MANDATORY AskUserQuestion)\n\nBased on detection results:\n\n**If IN_GIT_REPO=true, ORPHAN_BRANCH_EXISTS=true:**\n\n```\nQuestion: \"Orphan branch found in {repo}. Use it?\"\nHeader: \"Destination\"\nOptions:\n  - label: \"Use existing (Recommended)\"\n    description: \"Branch 'asciinema-recordings' already configured in {repo}\"\n  - label: \"Use different repository\"\n    description: \"Store recordings in a different repo\"\n```\n\n**If IN_GIT_REPO=true, ORPHAN_BRANCH_EXISTS=false:**\n\n```\nQuestion: \"No orphan branch in {repo}. Create one?\"\nHeader: \"Setup\"\nOptions:\n  - label: \"Create orphan branch (Recommended)\"\n    description: \"Initialize with GitHub Actions workflow for brotli\"\n  - label: \"Use different repository\"\n    description: \"Store recordings elsewhere\"\n```\n\n**If IN_GIT_REPO=false:**\n\n```\nQuestion: \"Not in a git repo. Where to store recordings?\"\nHeader: \"Destination\"\nOptions:\n  - label: \"Dedicated recordings repo\"\n    description: \"Use {owner}/asciinema-recordings\"\n  - label: \"Enter repository\"\n    description: \"Specify owner/repo manually\"\n```\n\n### Phase 3: Create Orphan Branch (if needed)\n\nClear SSH caches first, then create orphan branch:\n\n```bash\n/usr/bin/env bash << 'CREATE_ORPHAN_EOF'\nREPO_URL=\"${1:?}\"\nBRANCH=\"${2:-asciinema-recordings}\"\nLOCAL_PATH=\"$HOME/asciinema_recordings/$(basename \"$REPO_URL\" .git)\"\n\n# Clear SSH caches first\nrm -f ~/.ssh/control-* 2>/dev/null || true\nssh -O exit git@github.com 2>/dev/null || true\n\n# Get GitHub token for HTTPS clone\nGH_TOKEN=$(gh auth token 2>/dev/null || echo \"\")\nif [[ -n \"$GH_TOKEN\" ]]; then\n  # Parse owner/repo\n  if [[ \"$REPO_URL\" =~ github\\.com[:/]([^/]+)/([^/.]+) ]]; then\n    OWNER_REPO=\"${BASH_REMATCH[1]}/${BASH_REMATCH[2]}\"\n    AUTH_URL=\"https://${GH_TOKEN}@github.com/${OWNER_REPO}.git\"\n    CLEAN_URL=\"https://github.com/${OWNER_REPO}.git\"\n  else\n    AUTH_URL=\"$REPO_URL\"\n    CLEAN_URL=\"$REPO_URL\"\n  fi\nelse\n  AUTH_URL=\"$REPO_URL\"\n  CLEAN_URL=\"$REPO_URL\"\nfi\n\n# Clone and create orphan\nTEMP_DIR=$(mktemp -d)\ngit clone \"$AUTH_URL\" \"$TEMP_DIR/repo\"\ncd \"$TEMP_DIR/repo\"\n\n# Create orphan branch\ngit checkout --orphan \"$BRANCH\"\ngit reset --hard\ngit commit --allow-empty -m \"Initialize asciinema recordings\"\ngit push origin \"$BRANCH\"\n\n# Cleanup temp\nrm -rf \"$TEMP_DIR\"\n\n# Clone orphan branch locally\nmkdir -p \"$(dirname \"$LOCAL_PATH\")\"\ngit clone --single-branch --branch \"$BRANCH\" --depth 1 \"$AUTH_URL\" \"$LOCAL_PATH\"\n\n# Strip token from remote\ngit -C \"$LOCAL_PATH\" remote set-url origin \"$CLEAN_URL\"\n\nmkdir -p \"$LOCAL_PATH/chunks\"\n\necho \"ORPHAN_CREATED: $LOCAL_PATH\"\nCREATE_ORPHAN_EOF\n```\n\n### Phase 4: Generate Bootstrap Script\n\nGenerate the simplified bootstrap script (daemon handles chunking):\n\n```bash\n/usr/bin/env bash << 'GENERATE_SCRIPT_EOF'\nREPO_URL=\"${1:?}\"\nBRANCH=\"${2:-asciinema-recordings}\"\nLOCAL_REPO=\"${3:-$HOME/asciinema_recordings/$(basename \"$REPO_URL\" .git)}\"\nOUTPUT_FILE=\"${4:-$PWD/tmp/bootstrap-claude-session.sh}\"\n\nmkdir -p \"$(dirname \"$OUTPUT_FILE\")\"\n\ncat > \"$OUTPUT_FILE\" << 'SCRIPT_EOF'\n#!/usr/bin/env bash\n# bootstrap-claude-session.sh - Start asciinema recording session\n# Generated by /asciinema-tools:bootstrap\n# Chunking handled by launchd daemon\n\nif [[ \"${BASH_SOURCE[0]}\" != \"$0\" ]]; then\n    echo \"ERROR: Do not source this script. Run directly: ./${BASH_SOURCE[0]##*/}\"\n    return 1\nfi\n\nset -uo pipefail\n\nSCRIPT_EOF\n\n# Append configuration\ncat >> \"$OUTPUT_FILE\" << SCRIPT_CONFIG\nREPO_URL=\"$REPO_URL\"\nBRANCH=\"$BRANCH\"\nLOCAL_REPO=\"$LOCAL_REPO\"\nSCRIPT_CONFIG\n\ncat >> \"$OUTPUT_FILE\" << 'SCRIPT_BODY'\nWORKSPACE=\"$(basename \"$PWD\")\"\nDATETIME=\"$(date +%Y-%m-%d_%H-%M)\"\nASCIINEMA_DIR=\"$HOME/.asciinema\"\nACTIVE_DIR=\"$ASCIINEMA_DIR/active\"\nCAST_FILE=\"$ACTIVE_DIR/${WORKSPACE}_${DATETIME}.cast\"\nCONFIG_FILE=\"${CAST_FILE%.cast}.json\"\n\necho \"\"\necho \"  asciinema Recording Session                                   \"\necho \"\"\n\n# Check daemon\nif ! launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    echo \"  WARNING: Daemon not running! Run /asciinema-tools:daemon-start\"\n    echo \"\"\nfi\n\n# Clear SSH caches\nrm -f ~/.ssh/control-* 2>/dev/null || true\nssh -O exit git@github.com 2>/dev/null || true\n\n# Setup\nmkdir -p \"$ACTIVE_DIR\" \"$LOCAL_REPO/chunks\"\n\n# Write config for daemon\ncat > \"$CONFIG_FILE\" <<EOF\n{\n    \"repo_url\": \"$REPO_URL\",\n    \"branch\": \"$BRANCH\",\n    \"local_repo\": \"$LOCAL_REPO\",\n    \"workspace\": \"$WORKSPACE\",\n    \"started\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n}\nEOF\n\ncleanup() {\n    echo \"\"\n    echo \"Recording ended. Daemon will push final chunk.\"\n    echo \"Check status: /asciinema-tools:daemon-status\"\n}\ntrap cleanup EXIT\n\necho \"  Recording to: $CAST_FILE\"\necho \"  Run 'claude' inside this session. Exit twice to end.         \"\necho \"\"\necho \"\"\n\nasciinema rec --stdin \"$CAST_FILE\"\nSCRIPT_BODY\n\nchmod +x \"$OUTPUT_FILE\"\necho \"SCRIPT_GENERATED: $OUTPUT_FILE\"\nGENERATE_SCRIPT_EOF\n```\n\n### Phase 5: Display Instructions\n\n```markdown\n## Bootstrap Complete\n\nScript generated at: `tmp/bootstrap-claude-session.sh`\n\n### Quick Start\n\n1. Exit Claude Code: `exit` or Ctrl+D\n2. Run bootstrap: `./tmp/bootstrap-claude-session.sh`  NOT source!\n3. Inside recording, run: `claude`\n4. Work normally - daemon pushes chunks to GitHub\n5. Exit twice: Ctrl+D (Claude)  `exit` (recording)\n\n### What Happens\n\n- asciinema records to `~/.asciinema/active/{workspace}_{datetime}.cast`\n- Daemon monitors for idle periods\n- On idle, chunk is compressed and pushed via Keychain PAT\n- Daemon sends Pushover notification on failures\n- Recording is decoupled from Claude Code session\n\n### Daemon Commands\n\n| Command                          | Description         |\n| -------------------------------- | ------------------- |\n| `/asciinema-tools:daemon-status` | Check daemon health |\n| `/asciinema-tools:daemon-logs`   | View logs           |\n| `/asciinema-tools:daemon-start`  | Start daemon        |\n| `/asciinema-tools:daemon-stop`   | Stop daemon         |\n```\n\n## Skip Logic\n\n- If `-r` and `-b` provided -> skip repository selection\n- If `-y` provided -> skip all confirmations\n- If `--setup-orphan` provided -> force create orphan branch"
              },
              {
                "name": "/convert",
                "description": "Convert .cast to .txt for Claude Code analysis. TRIGGERS - convert cast, cast to txt, prepare analysis.",
                "path": "plugins/asciinema-tools/commands/convert.md",
                "frontmatter": {
                  "description": "Convert .cast to .txt for Claude Code analysis. TRIGGERS - convert cast, cast to txt, prepare analysis.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob, Write",
                  "argument-hint": "[file] [-o output] [--index] [--chunks] [--dims] [--analyze]"
                },
                "content": "# /asciinema-tools:convert\n\nConvert asciinema .cast recordings to clean .txt files.\n\n## Arguments\n\n| Argument       | Description                        |\n| -------------- | ---------------------------------- |\n| `file`         | Path to .cast file                 |\n| `-o, --output` | Output path (default: same dir)    |\n| `--index`      | Create timestamp indexed version   |\n| `--chunks`     | Split at 30s+ idle pauses          |\n| `--dims`       | Preserve terminal dimensions       |\n| `--analyze`    | Auto-run /analyze after conversion |\n\n## Execution\n\nInvoke the `asciinema-converter` skill with user-selected options.\n\n### Skip Logic\n\n- If `file` provided -> skip Phase 1 (file selection)\n- If options provided -> skip Phase 2 (options)\n- If `-o` provided -> skip Phase 3 (output location)\n- If `--analyze` provided -> skip Phase 5 and auto-run analyze\n\n### Workflow\n\n1. **Preflight**: Check asciinema convert command\n2. **Discovery**: Find .cast files\n3. **Selection**: AskUserQuestion for file\n4. **Options**: AskUserQuestion for conversion options\n5. **Location**: AskUserQuestion for output location\n6. **Execute**: Run asciinema convert\n7. **Report**: Display compression ratio\n8. **Next**: AskUserQuestion for follow-up action"
              },
              {
                "name": "/daemon-logs",
                "description": "View asciinema chunker daemon logs. TRIGGERS - daemon logs, chunker logs, backup logs.",
                "path": "plugins/asciinema-tools/commands/daemon-logs.md",
                "frontmatter": {
                  "description": "View asciinema chunker daemon logs. TRIGGERS - daemon logs, chunker logs, backup logs.",
                  "allowed-tools": "Bash",
                  "argument-hint": "[-n lines] [--follow] [--errors]"
                },
                "content": "# /asciinema-tools:daemon-logs\n\nView logs from the asciinema chunker daemon.\n\n## Arguments\n\n| Argument   | Description                        |\n| ---------- | ---------------------------------- |\n| `-n N`     | Show last N lines (default: 50)    |\n| `--follow` | Follow log output (like `tail -f`) |\n| `--errors` | Show only ERROR lines              |\n\n## Execution\n\n### Default: Show Recent Logs\n\n```bash\n/usr/bin/env bash << 'LOGS_EOF'\nLOG_FILE=\"$HOME/.asciinema/logs/chunker.log\"\nLAUNCHD_STDOUT=\"$HOME/.asciinema/logs/launchd-stdout.log\"\nLAUNCHD_STDERR=\"$HOME/.asciinema/logs/launchd-stderr.log\"\n\nif [[ ! -f \"$LOG_FILE\" ]]; then\n  echo \"No daemon logs found.\"\n  echo \"\"\n  echo \"Log locations:\"\n  echo \"  Daemon log: $LOG_FILE\"\n  echo \"  launchd stdout: $LAUNCHD_STDOUT\"\n  echo \"  launchd stderr: $LAUNCHD_STDERR\"\n  exit 0\nfi\n\necho \"=== Daemon Log (last 50 lines) ===\"\necho \"File: $LOG_FILE\"\necho \"\"\ntail -50 \"$LOG_FILE\"\nLOGS_EOF\n```\n\n### With --follow: Stream Logs\n\n```bash\n/usr/bin/env bash << 'FOLLOW_EOF'\nLOG_FILE=\"$HOME/.asciinema/logs/chunker.log\"\n\nif [[ ! -f \"$LOG_FILE\" ]]; then\n  echo \"No daemon logs found. Start the daemon first.\"\n  exit 1\nfi\n\necho \"=== Following Daemon Log (Ctrl+C to stop) ===\"\necho \"File: $LOG_FILE\"\necho \"\"\ntail -f \"$LOG_FILE\"\nFOLLOW_EOF\n```\n\n### With --errors: Show Only Errors\n\n```bash\n/usr/bin/env bash << 'ERRORS_EOF'\nLOG_FILE=\"$HOME/.asciinema/logs/chunker.log\"\n\nif [[ ! -f \"$LOG_FILE\" ]]; then\n  echo \"No daemon logs found.\"\n  exit 0\nfi\n\necho \"=== Error Log Entries ===\"\necho \"\"\ngrep -E \"ERROR|WARN|FAIL\" \"$LOG_FILE\" | tail -30 || echo \"(no errors found)\"\nERRORS_EOF\n```\n\n## Log Format\n\n```\n[2025-12-26 15:30:00] === Daemon started (PID: 12345) ===\n[2025-12-26 15:30:00] Config: idle=30s, zstd=3, active_dir=/Users/user/.asciinema/active\n[2025-12-26 15:30:00] Credentials loaded (Pushover: enabled)\n[2025-12-26 15:30:00] SSH caches cleared\n[2025-12-26 15:30:02] Idle detected (35s) for workspace_2025-12-26.cast, creating chunk...\n[2025-12-26 15:30:03] Pushed: chunk_20251226_153002.cast.zst to https://github.com/...\n```\n\n## Additional Log Files\n\n| File                                   | Content         |\n| -------------------------------------- | --------------- |\n| `~/.asciinema/logs/chunker.log`        | Main daemon log |\n| `~/.asciinema/logs/launchd-stdout.log` | launchd stdout  |\n| `~/.asciinema/logs/launchd-stderr.log` | launchd stderr  |"
              },
              {
                "name": "/daemon-setup",
                "description": "Set up asciinema chunker daemon with interactive wizard. Guides through PAT creation, Keychain storage, Pushover setup, and launchd installation. TRIGGERS - daemon setup, install chunker, configure backup.",
                "path": "plugins/asciinema-tools/commands/daemon-setup.md",
                "frontmatter": {
                  "description": "Set up asciinema chunker daemon with interactive wizard. Guides through PAT creation, Keychain storage, Pushover setup, and launchd installation. TRIGGERS - daemon setup, install chunker, configure backup.",
                  "allowed-tools": "Bash, AskUserQuestion, Write, Read",
                  "argument-hint": "[--reinstall] [--skip-pushover]"
                },
                "content": "# /asciinema-tools:daemon-setup\n\nInteractive wizard to set up the asciinema chunker daemon. This daemon runs independently of Claude Code CLI, using dedicated credentials stored in macOS Keychain.\n\n## Why a Daemon?\n\n| Problem with old approach     | Daemon solution                  |\n| ----------------------------- | -------------------------------- |\n| Uses `gh auth token` (shared) | Uses dedicated PAT from Keychain |\n| Dies when terminal closes     | launchd keeps it running         |\n| Silent push failures          | Logs + Pushover notifications    |\n| Tied to Claude Code session   | Completely decoupled             |\n\n## Execution\n\n### Phase 1: Preflight Check\n\n**Check required tools:**\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nMISSING=()\nfor tool in asciinema zstd git curl jq; do\n  command -v \"$tool\" &>/dev/null || MISSING+=(\"$tool\")\ndone\n\n# macOS-specific: security command for Keychain\nif [[ \"$(uname)\" == \"Darwin\" ]]; then\n  command -v security &>/dev/null || MISSING+=(\"security (macOS Keychain)\")\nfi\n\nif [[ ${#MISSING[@]} -gt 0 ]]; then\n  echo \"MISSING:${MISSING[*]}\"\n  exit 1\nfi\n\necho \"PREFLIGHT:OK\"\nPREFLIGHT_EOF\n```\n\n**If MISSING not empty, use AskUserQuestion:**\n\n```\nQuestion: \"Missing required tools: {MISSING}. How would you like to proceed?\"\nHeader: \"Dependencies\"\nOptions:\n  - label: \"Install via Homebrew (Recommended)\"\n    description: \"Run: brew install {MISSING}\"\n  - label: \"I'll install manually\"\n    description: \"Pause setup and show install instructions\"\n  - label: \"Abort setup\"\n    description: \"Exit the setup wizard\"\n```\n\n**If \"Install via Homebrew\"**: Run `brew install {MISSING}` and continue.\n\n---\n\n### Phase 2: Check Existing Installation\n\n```bash\n/usr/bin/env bash << 'CHECK_EXISTING_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\nDAEMON_RUNNING=\"false\"\n\nif [[ -f \"$PLIST_PATH\" ]]; then\n  echo \"PLIST_EXISTS:true\"\n  if launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    DAEMON_RUNNING=\"true\"\n  fi\nelse\n  echo \"PLIST_EXISTS:false\"\nfi\n\necho \"DAEMON_RUNNING:$DAEMON_RUNNING\"\n\n# Check if PAT already in Keychain\nif security find-generic-password -s \"asciinema-github-pat\" -a \"$USER\" -w &>/dev/null 2>&1; then\n  echo \"PAT_EXISTS:true\"\nelse\n  echo \"PAT_EXISTS:false\"\nfi\nCHECK_EXISTING_EOF\n```\n\n**If PLIST_EXISTS=true, use AskUserQuestion:**\n\n```\nQuestion: \"Existing daemon installation found. What would you like to do?\"\nHeader: \"Existing\"\nOptions:\n  - label: \"Reinstall (keep credentials)\"\n    description: \"Update daemon script and plist, keep Keychain credentials\"\n  - label: \"Fresh install (reset everything)\"\n    description: \"Remove existing credentials and start fresh\"\n  - label: \"Cancel\"\n    description: \"Exit without changes\"\n```\n\n---\n\n### Phase 3: GitHub PAT Setup\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Do you already have a GitHub Fine-Grained PAT for asciinema backups?\"\nHeader: \"GitHub PAT\"\nOptions:\n  - label: \"No, guide me through creating one (Recommended)\"\n    description: \"Opens GitHub in browser with step-by-step instructions\"\n  - label: \"Yes, I have a PAT ready\"\n    description: \"I'll paste my existing PAT\"\n  - label: \"What's a Fine-Grained PAT?\"\n    description: \"Show explanation before proceeding\"\n```\n\n**If \"No, guide me through\":**\n\n1. Open browser:\n\n```bash\nopen \"https://github.com/settings/tokens?type=beta\"\n```\n\n1. Display instructions:\n\n```markdown\n## Create GitHub Fine-Grained PAT\n\nFollow these steps in the browser window that just opened:\n\n1. Click **\"Generate new token\"**\n\n2. **Token name**: `asciinema-chunker`\n\n3. **Expiration**: 90 days (recommended) or custom\n   - Longer expiration = less frequent token rotation\n   - Shorter = more secure\n\n4. **Repository access**: Click **\"Only select repositories\"**\n   - Select your asciinema recording repositories\n   - Example: `your-org/your-repository`\n\n5. **Permissions** (expand \"Repository permissions\"):\n   - **Contents**: Read and write \n   - **Metadata**: Read-only \n\n6. Click **\"Generate token\"**\n\n7. **IMPORTANT**: Copy the token immediately!\n   It starts with `github_pat_...`\n   You won't be able to see it again.\n```\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Have you copied your new GitHub PAT?\"\nHeader: \"PAT Ready\"\nOptions:\n  - label: \"Yes, I've copied it\"\n    description: \"Proceed to enter the PAT\"\n  - label: \"Not yet, still creating\"\n    description: \"I need more time\"\n  - label: \"I need help\"\n    description: \"Show troubleshooting tips\"\n```\n\n**If \"Yes, I've copied it\" - Use AskUserQuestion to get PAT:**\n\n```\nQuestion: \"Paste your GitHub PAT (will be stored securely in macOS Keychain):\"\nHeader: \"PAT Input\"\nOptions:\n  - label: \"Enter my PAT\"\n    description: \"Use the 'Other' field below to paste your token\"\n```\n\nUser enters PAT via the \"Other\" option.\n\n**Store in Keychain:**\n\n```bash\n/usr/bin/env bash << 'STORE_PAT_EOF'\nPAT_VALUE=\"${1:?PAT required}\"\n\n# Store in Keychain (update if exists)\nsecurity add-generic-password \\\n  -s \"asciinema-github-pat\" \\\n  -a \"$USER\" \\\n  -w \"$PAT_VALUE\" \\\n  -U 2>/dev/null || \\\nsecurity add-generic-password \\\n  -s \"asciinema-github-pat\" \\\n  -a \"$USER\" \\\n  -w \"$PAT_VALUE\"\n\necho \"PAT stored in Keychain\"\nSTORE_PAT_EOF\n```\n\n**Verify PAT works:**\n\n```bash\n/usr/bin/env bash << 'VERIFY_PAT_EOF'\nPAT_VALUE=\"${1:?PAT required}\"\n\nRESPONSE=$(curl -s -H \"Authorization: Bearer $PAT_VALUE\" \\\n  https://api.github.com/user 2>&1)\n\nif echo \"$RESPONSE\" | jq -e '.login' &>/dev/null; then\n  USERNAME=$(echo \"$RESPONSE\" | jq -r '.login')\n  echo \"PAT_VALID:$USERNAME\"\nelse\n  ERROR=$(echo \"$RESPONSE\" | jq -r '.message // \"Unknown error\"')\n  echo \"PAT_INVALID:$ERROR\"\nfi\nVERIFY_PAT_EOF\n```\n\n**If PAT_INVALID, use AskUserQuestion:**\n\n```\nQuestion: \"PAT verification failed: {error}. What would you like to do?\"\nHeader: \"PAT Error\"\nOptions:\n  - label: \"Try a different PAT\"\n    description: \"Enter a new PAT\"\n  - label: \"Check PAT permissions\"\n    description: \"Review required permissions\"\n  - label: \"Continue anyway (not recommended)\"\n    description: \"Proceed without verification\"\n```\n\n---\n\n### Phase 4: Pushover Setup (Optional)\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Enable Pushover notifications for push failures?\"\nHeader: \"Notifications\"\nOptions:\n  - label: \"Yes, set up Pushover (Recommended)\"\n    description: \"Get notified on your phone when backups fail\"\n  - label: \"No, skip notifications\"\n    description: \"Failures will only be logged to file\"\n  - label: \"What is Pushover?\"\n    description: \"Learn about Pushover notifications\"\n```\n\n**If \"What is Pushover?\":**\n\n```markdown\n## What is Pushover?\n\nPushover is a notification service that sends real-time alerts to your phone.\n\n**Why use it?**\n\n- Know immediately when asciinema backups fail\n- Don't discover backup failures hours later\n- Works even when you're away from your computer\n\n**Cost**: One-time $5 purchase per platform (iOS, Android, Desktop)\n\n**Website**: https://pushover.net\n```\n\nThen loop back to the question.\n\n**If \"Yes, set up Pushover\":**\n\n1. Open browser:\n\n```bash\nopen \"https://pushover.net/apps/build\"\n```\n\n1. Display instructions:\n\n```markdown\n## Create Pushover Application\n\n1. Log in or create a Pushover account at pushover.net\n\n2. Click **\"Create an Application/API Token\"**\n\n3. Fill in the form:\n   - **Name**: `asciinema-chunker`\n   - **Type**: Script\n   - **Description**: asciinema backup notifications\n\n4. Click **\"Create Application\"**\n\n5. Copy the **API Token/Key** (starts with `a...`)\n```\n\n**Use AskUserQuestion for App Token:**\n\n```\nQuestion: \"Paste your Pushover App Token:\"\nHeader: \"App Token\"\nOptions:\n  - label: \"Enter App Token\"\n    description: \"Use the 'Other' field to paste your token\"\n```\n\n**Use AskUserQuestion for User Key:**\n\n```\nQuestion: \"Paste your Pushover User Key (from your Pushover dashboard, not the app token):\"\nHeader: \"User Key\"\nOptions:\n  - label: \"Enter User Key\"\n    description: \"Use the 'Other' field to paste your key\"\n```\n\n**Store both in Keychain:**\n\n```bash\n/usr/bin/env bash << 'STORE_PUSHOVER_EOF'\nAPP_TOKEN=\"${1:?App token required}\"\nUSER_KEY=\"${2:?User key required}\"\n\nsecurity add-generic-password -s \"asciinema-pushover-app\" -a \"$USER\" -w \"$APP_TOKEN\" -U 2>/dev/null || \\\nsecurity add-generic-password -s \"asciinema-pushover-app\" -a \"$USER\" -w \"$APP_TOKEN\"\n\nsecurity add-generic-password -s \"asciinema-pushover-user\" -a \"$USER\" -w \"$USER_KEY\" -U 2>/dev/null || \\\nsecurity add-generic-password -s \"asciinema-pushover-user\" -a \"$USER\" -w \"$USER_KEY\"\n\necho \"Pushover credentials stored in Keychain\"\nSTORE_PUSHOVER_EOF\n```\n\n**Send test notification:**\n\n```bash\n/usr/bin/env bash << 'TEST_PUSHOVER_EOF'\nAPP_TOKEN=\"${1:?}\"\nUSER_KEY=\"${2:?}\"\n\nRESPONSE=$(curl -s \\\n  --form-string \"token=$APP_TOKEN\" \\\n  --form-string \"user=$USER_KEY\" \\\n  --form-string \"title=asciinema-chunker\" \\\n  --form-string \"message=Setup complete! Notifications are working.\" \\\n  --form-string \"sound=cosmic\" \\\n  https://api.pushover.net/1/messages.json)\n\nif echo \"$RESPONSE\" | grep -q '\"status\":1'; then\n  echo \"TEST_OK\"\nelse\n  echo \"TEST_FAILED:$RESPONSE\"\nfi\nTEST_PUSHOVER_EOF\n```\n\n---\n\n### Phase 5: Daemon Configuration\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Configure chunking settings:\"\nHeader: \"Settings\"\nOptions:\n  - label: \"Default (30s idle, zstd-3) (Recommended)\"\n    description: \"Balanced chunking frequency and compression\"\n  - label: \"Fast (15s idle, zstd-1)\"\n    description: \"More frequent chunks, less compression\"\n  - label: \"Compact (60s idle, zstd-6)\"\n    description: \"Less frequent chunks, higher compression\"\n  - label: \"Custom\"\n    description: \"Enter specific values\"\n```\n\n**If \"Custom\", use AskUserQuestion:**\n\n```\nQuestion: \"Enter idle threshold in seconds (how long to wait before pushing a chunk):\"\nHeader: \"Idle\"\nOptions:\n  - label: \"Enter value\"\n    description: \"Recommended: 15-120 seconds\"\n```\n\nThen:\n\n```\nQuestion: \"Enter zstd compression level (1-19, higher = smaller files but slower):\"\nHeader: \"Compression\"\nOptions:\n  - label: \"Enter value\"\n    description: \"Recommended: 1-6 for real-time use\"\n```\n\n---\n\n### Phase 6: Install launchd Service\n\n**Generate plist from template:**\n\n```bash\n/usr/bin/env bash << 'GENERATE_PLIST_EOF'\nIDLE_THRESHOLD=\"${1:-30}\"\nZSTD_LEVEL=\"${2:-3}\"\n\nTEMPLATE_PATH=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/skills/asciinema-tools}/scripts/asciinema-chunker.plist.template\"\nDAEMON_PATH=\"${CLAUDE_PLUGIN_ROOT:-$HOME/.claude/skills/asciinema-tools}/scripts/idle-chunker-daemon.sh\"\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\n# Validate required files exist\nif [[ ! -f \"$TEMPLATE_PATH\" ]]; then\n  echo \"ERROR: Template not found at: $TEMPLATE_PATH\"\n  echo \"Ensure asciinema-tools plugin is properly installed.\"\n  exit 1\nfi\n\nif [[ ! -f \"$DAEMON_PATH\" ]]; then\n  echo \"ERROR: Daemon script not found at: $DAEMON_PATH\"\n  echo \"Ensure asciinema-tools plugin is properly installed.\"\n  exit 1\nfi\n\nif ! mkdir -p \"$HOME/Library/LaunchAgents\" 2>&1; then\n  echo \"ERROR: Cannot create LaunchAgents directory\"\n  exit 1\nfi\n\nif ! mkdir -p \"$HOME/.asciinema/logs\" 2>&1; then\n  echo \"ERROR: Cannot create logs directory at ~/.asciinema/logs\"\n  exit 1\nfi\n\n# Read template and substitute placeholders\nsed \\\n  -e \"s|{{HOME}}|$HOME|g\" \\\n  -e \"s|{{USER}}|$USER|g\" \\\n  -e \"s|{{DAEMON_PATH}}|$DAEMON_PATH|g\" \\\n  -e \"s|{{IDLE_THRESHOLD}}|$IDLE_THRESHOLD|g\" \\\n  -e \"s|{{ZSTD_LEVEL}}|$ZSTD_LEVEL|g\" \\\n  \"$TEMPLATE_PATH\" > \"$PLIST_PATH\"\n\necho \"PLIST_GENERATED:$PLIST_PATH\"\nGENERATE_PLIST_EOF\n```\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Ready to install the launchd service. This will:\"\nHeader: \"Install\"\ndescription: |\n  - Install to: ~/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\n  - Start on login: Yes\n  - Auto-restart on crash: Yes\n  - Idle threshold: {idle}s\n  - Compression: zstd-{level}\nOptions:\n  - label: \"Install and start now (Recommended)\"\n    description: \"Install plist and start the daemon immediately\"\n  - label: \"Install but don't start yet\"\n    description: \"Install plist only, start manually later\"\n  - label: \"Show plist file first\"\n    description: \"Display the generated plist content\"\n```\n\n**If \"Show plist file first\":**\n\nDisplay plist content, then loop back to question.\n\n**If \"Install and start now\":**\n\n```bash\n/usr/bin/env bash << 'INSTALL_DAEMON_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\n# Unload if already running (may fail if not loaded - that's expected)\nif ! launchctl unload \"$PLIST_PATH\" 2>/dev/null; then\n  echo \"INFO: No existing daemon to unload (first install)\"\nfi\n\n# Load and start\nif launchctl load \"$PLIST_PATH\"; then\n  echo \"INSTALL_OK\"\n  sleep 2\n  if launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    echo \"DAEMON_RUNNING\"\n  else\n    echo \"DAEMON_NOT_RUNNING\"\n  fi\nelse\n  echo \"INSTALL_FAILED\"\nfi\nINSTALL_DAEMON_EOF\n```\n\n---\n\n### Phase 7: Verification\n\n**Check daemon status:**\n\n```bash\n/usr/bin/env bash << 'VERIFY_DAEMON_EOF'\nHEALTH_FILE=\"$HOME/.asciinema/health.json\"\n\n# Wait for health file\nsleep 3\n\nif [[ -f \"$HEALTH_FILE\" ]]; then\n  STATUS=$(jq -r '.status' \"$HEALTH_FILE\")\n  MESSAGE=$(jq -r '.message' \"$HEALTH_FILE\")\n  PID=$(jq -r '.pid' \"$HEALTH_FILE\")\n  echo \"HEALTH_STATUS:$STATUS\"\n  echo \"HEALTH_MESSAGE:$MESSAGE\"\n  echo \"HEALTH_PID:$PID\"\nelse\n  echo \"HEALTH_FILE_MISSING\"\nfi\n\n# Check launchctl\nif launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n  echo \"LAUNCHCTL_OK\"\nelse\n  echo \"LAUNCHCTL_NOT_FOUND\"\nfi\nVERIFY_DAEMON_EOF\n```\n\n**Use AskUserQuestion:**\n\n```\nQuestion: \"Setup complete! Daemon status: {status}. What would you like to do next?\"\nHeader: \"Complete\"\nOptions:\n  - label: \"Show health status\"\n    description: \"Display daemon health information\"\n  - label: \"View logs\"\n    description: \"Show recent log entries\"\n  - label: \"Done\"\n    description: \"Exit setup wizard\"\n```\n\n**If \"Show health status\":**\n\n```bash\ncat ~/.asciinema/health.json | jq .\n```\n\n**If \"View logs\":**\n\n```bash\ntail -20 ~/.asciinema/logs/chunker.log\n```\n\n---\n\n## Final Success Message\n\n```markdown\n##  Daemon Setup Complete\n\n**Status**: Running\n**PID**: {pid}\n**Health file**: ~/.asciinema/health.json\n**Logs**: ~/.asciinema/logs/chunker.log\n\n### Quick Commands\n\n| Command                          | Description         |\n| -------------------------------- | ------------------- |\n| `/asciinema-tools:daemon-status` | Check daemon health |\n| `/asciinema-tools:daemon-logs`   | View logs           |\n| `/asciinema-tools:daemon-stop`   | Stop daemon         |\n| `/asciinema-tools:daemon-start`  | Start daemon        |\n\n### Next Steps\n\n1. Run `/asciinema-tools:bootstrap` to start a recording session\n2. The daemon will automatically push chunks to GitHub\n3. You'll receive Pushover notifications if pushes fail\n\nThe daemon is now completely independent of Claude Code CLI.\nYou can switch `gh auth` accounts freely without affecting backups.\n```"
              },
              {
                "name": "/daemon-start",
                "description": "Start the asciinema chunker daemon. TRIGGERS - start daemon, resume chunker, enable backup.",
                "path": "plugins/asciinema-tools/commands/daemon-start.md",
                "frontmatter": {
                  "description": "Start the asciinema chunker daemon. TRIGGERS - start daemon, resume chunker, enable backup.",
                  "allowed-tools": "Bash",
                  "argument-hint": ""
                },
                "content": "# /asciinema-tools:daemon-start\n\nStart the asciinema chunker daemon via launchd.\n\n## Execution\n\n### Check if Already Running\n\n```bash\n/usr/bin/env bash << 'CHECK_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\nif ! [[ -f \"$PLIST_PATH\" ]]; then\n  echo \"ERROR: Daemon not installed. Run /asciinema-tools:daemon-setup first.\"\n  exit 1\nfi\n\nif launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n  echo \"ALREADY_RUNNING\"\n  cat ~/.asciinema/health.json 2>/dev/null | jq -r '\"Status: \\(.status) | PID: \\(.pid) | Last push: \\(.last_push)\"' || true\n  exit 0\nfi\n\necho \"NOT_RUNNING\"\nCHECK_EOF\n```\n\n### Start Daemon\n\n```bash\n/usr/bin/env bash << 'START_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\nif launchctl load \"$PLIST_PATH\"; then\n  echo \"Daemon started\"\n  sleep 2\n\n  if launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    echo \"\"\n    echo \"Status:\"\n    cat ~/.asciinema/health.json 2>/dev/null | jq . || echo \"Waiting for health file...\"\n  else\n    echo \"WARNING: Daemon may not have started correctly. Check logs:\"\n    echo \"  /asciinema-tools:daemon-logs\"\n  fi\nelse\n  echo \"ERROR: Failed to start daemon\"\n  exit 1\nfi\nSTART_EOF\n```\n\n## Output\n\nOn success:\n\n```\nDaemon started\n\nStatus:\n{\n  \"status\": \"ok\",\n  \"message\": \"Monitoring ~/.asciinema/active\",\n  \"pid\": 12345,\n  ...\n}\n```"
              },
              {
                "name": "/daemon-status",
                "description": "Check asciinema status - daemon, running processes, and unhandled .cast files. TRIGGERS - daemon status, check backup, chunker health, recording status, unhandled files.",
                "path": "plugins/asciinema-tools/commands/daemon-status.md",
                "frontmatter": {
                  "description": "Check asciinema status - daemon, running processes, and unhandled .cast files. TRIGGERS - daemon status, check backup, chunker health, recording status, unhandled files.",
                  "allowed-tools": "Bash",
                  "argument-hint": "[--verbose] [--files-only] [--processes-only]"
                },
                "content": "# /asciinema-tools:daemon-status\n\nCheck comprehensive asciinema status including daemon, running processes, and unhandled .cast files.\n\n## Execution\n\n### Collect Status Information\n\n```bash\n/usr/bin/env bash << 'STATUS_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\nHEALTH_FILE=\"$HOME/.asciinema/health.json\"\nLOG_FILE=\"$HOME/.asciinema/logs/chunker.log\"\nRECORDINGS_DIR=\"$HOME/asciinema_recordings\"\n\necho \"\"\necho \"  asciinema Status Overview                                     \"\necho \"\"\n\n# ========== RUNNING PROCESSES ==========\necho \"  RUNNING PROCESSES                                             \"\necho \"\"\n\nPROCS=$(ps aux | grep -E \"asciinema rec\" | grep -v grep)\nif [[ -n \"$PROCS\" ]]; then\n  PROC_COUNT=$(echo \"$PROCS\" | wc -l | tr -d ' ')\n  printf \"  Active asciinema rec: %-38s \\n\" \"$PROC_COUNT process(es)\"\n  echo \"$PROCS\" | while read -r line; do\n    PID=$(echo \"$line\" | awk '{print $2}')\n    # Extract .cast file path from command\n    CAST_FILE=$(echo \"$line\" | grep -oE '[^ ]+\\.cast' | head -1)\n    if [[ -n \"$CAST_FILE\" ]]; then\n      BASENAME=$(basename \"$CAST_FILE\")\n      SIZE=$(ls -lh \"$CAST_FILE\" 2>/dev/null | awk '{print $5}' || echo \"?\")\n      printf \"    PID %-6s %-35s %5s \\n\" \"$PID\" \"${BASENAME:0:35}\" \"$SIZE\"\n    else\n      printf \"    PID %-6s (no file detected)                          \\n\" \"$PID\"\n    fi\n  done\nelse\n  echo \"  Active asciinema rec: None                                    \"\nfi\n\necho \"\"\n\n# ========== DAEMON STATUS ==========\necho \"  CHUNKER DAEMON                                                \"\necho \"\"\n\nif [[ -f \"$PLIST_PATH\" ]]; then\n  echo \"  Installed: Yes                                                \"\n  if launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    echo \"  Running: Yes                                                  \"\n  else\n    echo \"  Running: No                                                   \"\n  fi\n\n  if [[ -f \"$HEALTH_FILE\" ]]; then\n    STATUS=$(jq -r '.status // \"unknown\"' \"$HEALTH_FILE\")\n    LAST_PUSH=$(jq -r '.last_push // \"never\"' \"$HEALTH_FILE\")\n    CHUNKS=$(jq -r '.chunks_pushed // 0' \"$HEALTH_FILE\")\n    printf \"  Health: %-52s \\n\" \"$STATUS\"\n    printf \"  Last push: %-49s \\n\" \"$LAST_PUSH\"\n    printf \"  Chunks pushed: %-44s \\n\" \"$CHUNKS\"\n  fi\nelse\n  echo \"  Installed: No - run /asciinema-tools:daemon-setup             \"\nfi\n\necho \"\"\n\n# ========== UNHANDLED .CAST FILES ==========\necho \"  UNHANDLED .CAST FILES (not on orphan branch)                  \"\necho \"\"\n\n# Find .cast files in common locations\nUNHANDLED=()\nwhile IFS= read -r -d '' file; do\n  UNHANDLED+=(\"$file\")\ndone < <(find ~/eon -name \"*.cast\" -size +1M -mtime -30 -print0 2>/dev/null)\n\n# Also check tmp directories\nwhile IFS= read -r -d '' file; do\n  UNHANDLED+=(\"$file\")\ndone < <(find /tmp -maxdepth 2 -name \"*.cast\" -size +1M -print0 2>/dev/null)\n\nif [[ ${#UNHANDLED[@]} -gt 0 ]]; then\n  printf \"  Found: %-53s \\n\" \"${#UNHANDLED[@]} file(s) need attention\"\n  for file in \"${UNHANDLED[@]:0:5}\"; do\n    BASENAME=$(basename \"$file\")\n    SIZE=$(ls -lh \"$file\" 2>/dev/null | awk '{print $5}')\n    MTIME=$(stat -f \"%Sm\" -t \"%Y-%m-%d\" \"$file\" 2>/dev/null || stat -c \"%y\" \"$file\" 2>/dev/null | cut -d' ' -f1)\n    printf \"    %-40s %5s  %s \\n\" \"${BASENAME:0:40}\" \"$SIZE\" \"$MTIME\"\n  done\n  if [[ ${#UNHANDLED[@]} -gt 5 ]]; then\n    printf \"    ... and %d more                                          \\n\" \"$((${#UNHANDLED[@]} - 5))\"\n  fi\n  echo \"                                                                \"\n  echo \"   Run /asciinema-tools:finalize to process these files        \"\nelse\n  echo \"  No unhandled .cast files found                                \"\nfi\n\necho \"\"\n\n# ========== CREDENTIALS ==========\necho \"  CREDENTIALS                                                   \"\necho \"\"\n\nif security find-generic-password -s \"asciinema-github-pat\" -a \"$USER\" -w &>/dev/null 2>&1; then\n  echo \"  GitHub PAT:  Configured                                      \"\nelse\n  echo \"  GitHub PAT:  Not configured                                  \"\nfi\n\nif security find-generic-password -s \"asciinema-pushover-app\" -a \"$USER\" -w &>/dev/null 2>&1; then\n  echo \"  Pushover:  Configured                                        \"\nelse\n  echo \"  Pushover:  Not configured (optional)                         \"\nfi\n\necho \"\"\n\n# Recent log entries\nif [[ -f \"$LOG_FILE\" ]]; then\n  echo \"\"\n  echo \"Recent daemon logs:\"\n  echo \"-------------------\"\n  tail -5 \"$LOG_FILE\"\nfi\nSTATUS_EOF\n```\n\n## Output Example\n\n```\n\n  asciinema Status Overview                                     \n\n  RUNNING PROCESSES                                             \n\n  Active asciinema rec: 2 process(es)                           \n    PID 41749  alpha-forge-research_2025.cast            12G    \n    PID 49655  alpha-forge_2025-12-23.cast              4.5G    \n\n  CHUNKER DAEMON                                                \n\n  Installed: Yes                                                \n  Running: Yes                                                  \n  Health: ok                                                    \n  Last push: 2025-12-26T15:30:00Z                               \n  Chunks pushed: 7                                              \n\n  UNHANDLED .CAST FILES (not on orphan branch)                  \n\n  Found: 3 file(s) need attention                               \n    alpha-forge-research.cast                 12G   2025-12-30  \n    alpha-forge_session.cast                 4.5G   2025-12-26  \n    debug-session.cast                       234M   2025-12-28  \n                                                                \n   Run /asciinema-tools:finalize to process these files        \n\n  CREDENTIALS                                                   \n\n  GitHub PAT:  Configured                                      \n  Pushover:  Not configured (optional)                         \n\n\nRecent daemon logs:\n-------------------\n[2025-12-26 15:30:00] Pushed: chunk_20251226_153000.cast.zst\n[2025-12-26 15:25:00] Idle detected (32s) for workspace_2025-12-26.cast\n```"
              },
              {
                "name": "/daemon-stop",
                "description": "Stop the asciinema chunker daemon. TRIGGERS - stop daemon, pause chunker, disable backup.",
                "path": "plugins/asciinema-tools/commands/daemon-stop.md",
                "frontmatter": {
                  "description": "Stop the asciinema chunker daemon. TRIGGERS - stop daemon, pause chunker, disable backup.",
                  "allowed-tools": "Bash",
                  "argument-hint": ""
                },
                "content": "# /asciinema-tools:daemon-stop\n\nStop the asciinema chunker daemon via launchd.\n\n## Execution\n\n### Check if Running\n\n```bash\n/usr/bin/env bash << 'CHECK_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\nif ! [[ -f \"$PLIST_PATH\" ]]; then\n  echo \"Daemon not installed.\"\n  exit 0\nfi\n\nif ! launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n  echo \"Daemon not running.\"\n  exit 0\nfi\n\necho \"RUNNING\"\nCHECK_EOF\n```\n\n### Stop Daemon\n\n```bash\n/usr/bin/env bash << 'STOP_EOF'\nPLIST_PATH=\"$HOME/Library/LaunchAgents/com.cc-skills.asciinema-chunker.plist\"\n\nif launchctl unload \"$PLIST_PATH\"; then\n  echo \"Daemon stopped\"\n\n  # Verify\n  sleep 1\n  if launchctl list 2>/dev/null | grep -q \"asciinema-chunker\"; then\n    echo \"WARNING: Daemon may still be running\"\n  else\n    echo \"Confirmed: Daemon is no longer running\"\n  fi\nelse\n  echo \"ERROR: Failed to stop daemon\"\n  exit 1\nfi\nSTOP_EOF\n```\n\n## Output\n\nOn success:\n\n```\nDaemon stopped\nConfirmed: Daemon is no longer running\n```\n\n## Notes\n\n- Stopping the daemon does NOT delete credentials from Keychain\n- To restart: `/asciinema-tools:daemon-start`\n- The daemon will NOT auto-start on next login until started again"
              },
              {
                "name": "/finalize",
                "description": "Finalize orphaned recordings - stop processes, compress, push to orphan branch. TRIGGERS - finalize recording, stop asciinema, orphaned recording, cleanup recording, push recording.",
                "path": "plugins/asciinema-tools/commands/finalize.md",
                "frontmatter": {
                  "description": "Finalize orphaned recordings - stop processes, compress, push to orphan branch. TRIGGERS - finalize recording, stop asciinema, orphaned recording, cleanup recording, push recording.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob, Read",
                  "argument-hint": "[file|--all] [--force] [--no-push] [--keep-local]"
                },
                "content": "# /asciinema-tools:finalize\n\nFinalize orphaned asciinema recordings: stop running processes gracefully, compress, and push to the orphan branch.\n\n## Arguments\n\n| Argument       | Description                                          |\n| -------------- | ---------------------------------------------------- |\n| `file`         | Specific .cast file to finalize                      |\n| `--all`        | Finalize all unhandled .cast files                   |\n| `--force`      | Use SIGKILL if graceful stop fails                   |\n| `--no-push`    | Skip pushing to orphan branch (local only)           |\n| `--keep-local` | Keep local .cast after compression                   |\n\n## Workflow\n\n1. **Discovery**: Find running asciinema processes and unhandled .cast files\n2. **Selection**: AskUserQuestion for which files to process\n3. **Stop**: Gracefully stop running processes (SIGTERM  SIGINT  SIGKILL)\n4. **Verify**: Check file integrity after stop\n5. **Compress**: zstd compress .cast files\n6. **Push**: Push to orphan branch (if configured)\n7. **Cleanup**: Remove local .cast (optional)\n\n## Execution\n\n### Phase 1: Discovery\n\n```bash\n/usr/bin/env bash << 'DISCOVER_EOF'\necho \"=== Running asciinema processes ===\"\nPROCS=$(ps aux | grep -E \"asciinema rec\" | grep -v grep)\nif [[ -n \"$PROCS\" ]]; then\n  echo \"$PROCS\" | while read -r line; do\n    PID=$(echo \"$line\" | awk '{print $2}')\n    CAST_FILE=$(echo \"$line\" | grep -oE '[^ ]+\\.cast' | head -1)\n    if [[ -n \"$CAST_FILE\" ]]; then\n      SIZE=$(ls -lh \"$CAST_FILE\" 2>/dev/null | awk '{print $5}' || echo \"?\")\n      echo \"PID $PID: $CAST_FILE ($SIZE)\"\n    else\n      echo \"PID $PID: (no file detected)\"\n    fi\n  done\nelse\n  echo \"No running asciinema processes\"\nfi\n\necho \"\"\necho \"=== Unhandled .cast files ===\"\nfind ~/eon -name \"*.cast\" -size +1M -mtime -30 2>/dev/null | while read -r f; do\n  SIZE=$(ls -lh \"$f\" | awk '{print $5}')\n  echo \"$f ($SIZE)\"\ndone\nDISCOVER_EOF\n```\n\n### Phase 2: Selection\n\n```yaml\nAskUserQuestion:\n  question: \"Which recordings should be finalized?\"\n  header: \"Select\"\n  multiSelect: true\n  options:\n    - label: \"All running processes\"\n      description: \"Stop all asciinema rec processes and finalize their files\"\n    - label: \"All unhandled files\"\n      description: \"Finalize all .cast files found in ~/eon\"\n    - label: \"Specific file\"\n      description: \"Enter path to specific .cast file\"\n```\n\n### Phase 3: Stop Running Processes\n\n```bash\n/usr/bin/env bash << 'STOP_EOF'\n# Arguments: PID list\nPIDS=\"$@\"\n\nfor PID in $PIDS; do\n  echo \"Stopping PID $PID...\"\n\n  # Try SIGTERM first (graceful)\n  kill -TERM \"$PID\" 2>/dev/null\n  sleep 2\n\n  if kill -0 \"$PID\" 2>/dev/null; then\n    echo \"  SIGTERM ignored, trying SIGINT...\"\n    kill -INT \"$PID\" 2>/dev/null\n    sleep 2\n  fi\n\n  if kill -0 \"$PID\" 2>/dev/null; then\n    echo \"  Process still running. Use --force for SIGKILL\"\n    # Only SIGKILL with --force flag\n    if [[ \"$FORCE\" == \"true\" ]]; then\n      echo \"  Sending SIGKILL (file may be truncated)...\"\n      kill -9 \"$PID\" 2>/dev/null\n      sleep 1\n    fi\n  fi\n\n  if ! kill -0 \"$PID\" 2>/dev/null; then\n    echo \"   Process stopped\"\n  else\n    echo \"   Process still running\"\n  fi\ndone\nSTOP_EOF\n```\n\n### Phase 4: File Integrity Check\n\n```bash\n/usr/bin/env bash << 'CHECK_EOF'\nCAST_FILE=\"$1\"\n\necho \"Checking file integrity: $CAST_FILE\"\n\n# Check if file exists\nif [[ ! -f \"$CAST_FILE\" ]]; then\n  echo \"   File not found\"\n  exit 1\nfi\n\n# Check file size\nSIZE=$(stat -f%z \"$CAST_FILE\" 2>/dev/null || stat -c%s \"$CAST_FILE\")\necho \"  Size: $(numfmt --to=iec-i \"$SIZE\" 2>/dev/null || echo \"$SIZE bytes\")\"\n\n# Check last line (NDJSON should have complete JSON arrays)\nLAST_LINE=$(tail -c 500 \"$CAST_FILE\" | tail -1)\nif [[ \"$LAST_LINE\" == *\"]\"* ]]; then\n  echo \"   File appears complete (ends with JSON array)\"\nelse\n  echo \"   File may be truncated (incomplete JSON)\"\n  echo \"  Note: asciinema 2.0+ streams to disk, so most data is preserved\"\nfi\n\n# Test with asciinema cat (quick validation)\nif timeout 5 asciinema cat \"$CAST_FILE\" > /dev/null 2>&1; then\n  echo \"   File is playable\"\nelse\n  echo \"   File may have issues (but often still usable)\"\nfi\nCHECK_EOF\n```\n\n### Phase 5: Compress\n\n```bash\n/usr/bin/env bash << 'COMPRESS_EOF'\nCAST_FILE=\"$1\"\nZSTD_LEVEL=\"${2:-6}\"\n\necho \"Compressing: $CAST_FILE\"\n\nOUTPUT=\"${CAST_FILE}.zst\"\nif zstd -\"$ZSTD_LEVEL\" -f \"$CAST_FILE\" -o \"$OUTPUT\"; then\n  ORIG_SIZE=$(stat -f%z \"$CAST_FILE\" 2>/dev/null || stat -c%s \"$CAST_FILE\")\n  COMP_SIZE=$(stat -f%z \"$OUTPUT\" 2>/dev/null || stat -c%s \"$OUTPUT\")\n  RATIO=$(echo \"scale=1; $ORIG_SIZE / $COMP_SIZE\" | bc 2>/dev/null || echo \"?\")\n  echo \"   Compressed: $(basename \"$OUTPUT\")\"\n  echo \"  Compression ratio: ${RATIO}:1\"\nelse\n  echo \"   Compression failed\"\n  exit 1\nfi\nCOMPRESS_EOF\n```\n\n### Phase 6: Push to Orphan Branch\n\n```bash\n/usr/bin/env bash << 'PUSH_EOF'\nCOMPRESSED_FILE=\"$1\"\nRECORDINGS_DIR=\"$HOME/asciinema_recordings\"\n\n# Find the local recordings clone\nREPO_DIR=$(find \"$RECORDINGS_DIR\" -maxdepth 1 -type d -name \"*\" | head -1)\nif [[ -z \"$REPO_DIR\" ]] || [[ ! -d \"$REPO_DIR/.git\" ]]; then\n  echo \"   No orphan branch clone found at $RECORDINGS_DIR\"\n  echo \"  Run /asciinema-tools:bootstrap to set up orphan branch\"\n  exit 1\nfi\n\necho \"Pushing to orphan branch...\"\n\n# Copy compressed file\nBASENAME=$(basename \"$COMPRESSED_FILE\")\nDEST=\"$REPO_DIR/recordings/$BASENAME\"\nmkdir -p \"$(dirname \"$DEST\")\"\ncp \"$COMPRESSED_FILE\" \"$DEST\"\n\n# Commit and push\ncd \"$REPO_DIR\"\ngit add -A\ngit commit -m \"finalize: $BASENAME\" 2>/dev/null || true\n\n# Push with token\nGH_TOKEN=$(gh auth token 2>/dev/null)\nif [[ -n \"$GH_TOKEN\" ]]; then\n  REMOTE_URL=$(git remote get-url origin)\n  # Convert to token-authenticated URL\n  TOKEN_URL=$(echo \"$REMOTE_URL\" | sed \"s|https://github.com|https://$GH_TOKEN@github.com|\")\n  if git push \"$TOKEN_URL\" HEAD 2>/dev/null; then\n    echo \"   Pushed to orphan branch\"\n  else\n    echo \"   Push failed (check credentials)\"\n  fi\nelse\n  echo \"   No GitHub token, skipping push\"\nfi\nPUSH_EOF\n```\n\n### Phase 7: Cleanup Confirmation\n\n```yaml\nAskUserQuestion:\n  question: \"Delete local .cast file after successful compression/push?\"\n  header: \"Cleanup\"\n  options:\n    - label: \"Yes, delete local .cast\"\n      description: \"Remove original .cast file (compressed version preserved)\"\n    - label: \"No, keep local\"\n      description: \"Keep both .cast and .cast.zst files\"\n```\n\n## Example Usage\n\n```bash\n# Interactive mode - discover and select\n/asciinema-tools:finalize\n\n# Finalize specific file\n/asciinema-tools:finalize ~/eon/project/tmp/session.cast\n\n# Finalize all with force stop\n/asciinema-tools:finalize --all --force\n\n# Local only (no push)\n/asciinema-tools:finalize session.cast --no-push\n```\n\n## Related Commands\n\n- `/asciinema-tools:daemon-status` - View status and find unhandled files\n- `/asciinema-tools:convert` - Convert .cast to .txt for analysis\n- `/asciinema-tools:summarize` - AI-powered analysis of recordings"
              },
              {
                "name": "/format",
                "description": "Reference for asciinema v3 .cast NDJSON format. TRIGGERS - cast format, asciicast spec, event codes.",
                "path": "plugins/asciinema-tools/commands/format.md",
                "frontmatter": {
                  "description": "Reference for asciinema v3 .cast NDJSON format. TRIGGERS - cast format, asciicast spec, event codes.",
                  "allowed-tools": "Read, AskUserQuestion, Bash",
                  "argument-hint": "[header|events|parsing|all] [-f file] [--live]"
                },
                "content": "# /asciinema-tools:format\n\nDisplay reference documentation for the asciinema v3 .cast format.\n\n## Arguments\n\n| Argument     | Description                     |\n| ------------ | ------------------------------- |\n| `header`     | Show header field specification |\n| `events`     | Show event codes deep-dive      |\n| `parsing`    | Show jq/bash parsing examples   |\n| `all`        | Show complete format reference  |\n| `-f, --file` | Use specific .cast for examples |\n| `--live`     | Run parsing examples on file    |\n\n## Execution\n\nInvoke the `asciinema-cast-format` skill with user-selected section.\n\n### Skip Logic\n\n- If section provided -> skip Phase 1 (section selection)\n- If `-f` provided with `parsing` -> skip Phase 2 (example file)\n\n### Workflow\n\n1. **Selection**: AskUserQuestion for section\n2. **Example**: AskUserQuestion for example file (if parsing)\n3. **Display**: Show requested documentation"
              },
              {
                "name": "/full-workflow",
                "description": "Full workflow - record + backup + convert + analyze. TRIGGERS - full workflow, complete recording, end-to-end.",
                "path": "plugins/asciinema-tools/commands/full-workflow.md",
                "frontmatter": {
                  "description": "Full workflow - record + backup + convert + analyze. TRIGGERS - full workflow, complete recording, end-to-end.",
                  "allowed-tools": "Bash, Grep, AskUserQuestion, Glob, Write",
                  "argument-hint": "[-t title] [-q|--quick] [-f|--full] [-d domains] [--no-backup] [--no-analyze]"
                },
                "content": "# /asciinema-tools:full-workflow\n\nComplete end-to-end workflow: record, backup, convert, and analyze.\n\n## Arguments\n\n| Argument        | Description                           |\n| --------------- | ------------------------------------- |\n| `-t, --title`   | Recording title                       |\n| `-q, --quick`   | Quick analysis after recording        |\n| `-f, --full`    | Full analysis after recording         |\n| `-d, --domains` | Domains for analysis                  |\n| `--no-backup`   | Skip streaming backup                 |\n| `--no-analyze`  | Skip analysis (just record + convert) |\n\n## Execution\n\nChains multiple skills: record -> backup -> convert -> analyze\n\n### Skip Logic\n\n- If `-t` provided -> use title directly\n- If `-q` or `-f` provided -> skip workflow configuration\n- If `--no-backup` -> skip backup step\n- If `--no-analyze` -> skip analysis step\n\n### Workflow\n\n1. **Config**: AskUserQuestion for workflow options\n2. **Record**: Invoke asciinema-recorder\n3. **Backup**: Invoke asciinema-streaming-backup (if enabled)\n4. **Convert**: Invoke asciinema-converter\n5. **Analyze**: Invoke asciinema-analyzer (if enabled)\n6. **Report**: Display summary\n\n## Example Usage\n\n```bash\n# Quick workflow with title\n/asciinema-tools:full-workflow -t \"Feature dev\" -q\n\n# Full analysis on trading domain\n/asciinema-tools:full-workflow -f -d trading,ml\n\n# Record only, analyze later\n/asciinema-tools:full-workflow --no-analyze\n```"
              },
              {
                "name": "/hooks",
                "description": "Install/uninstall hooks for auto-backup on session end. TRIGGERS - hooks, auto backup, session hooks.",
                "path": "plugins/asciinema-tools/commands/hooks.md",
                "frontmatter": {
                  "description": "Install/uninstall hooks for auto-backup on session end. TRIGGERS - hooks, auto backup, session hooks.",
                  "allowed-tools": "Bash, Read, Write, AskUserQuestion",
                  "argument-hint": "[install|uninstall|status] [--backup-on-stop] [--convert-on-stop] [-y|--yes]"
                },
                "content": "# /asciinema-tools:hooks\n\nManage Claude Code hooks for asciinema-tools automation.\n\n## Arguments\n\n| Argument            | Description                     |\n| ------------------- | ------------------------------- |\n| `install`           | Add hooks to settings.json      |\n| `uninstall`         | Remove asciinema-tools hooks    |\n| `status`            | Show current hook configuration |\n| `--backup-on-stop`  | Auto-backup when session ends   |\n| `--convert-on-stop` | Auto-convert on session end     |\n| `-y, --yes`         | Skip confirmation prompts       |\n\n## Hook Definitions\n\n### PostToolUse Hook (backup-on-stop)\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"command\": \"asciinema-backup-if-active\"\n      }\n    ]\n  }\n}\n```\n\n## Execution\n\n### Skip Logic\n\n- If action provided -> execute directly\n- If hook type flags provided -> use specific hooks\n\n### Workflow\n\n1. **Status**: Read current ~/.claude/settings.json\n2. **Action**: AskUserQuestion for action type\n3. **Hooks**: AskUserQuestion for hook selection\n4. **Execute**: Modify settings.json\n5. **Verify**: Confirm changes applied"
              },
              {
                "name": "/play",
                "description": "Play .cast recordings in iTerm2 with speed controls. TRIGGERS - play recording, asciinema play, view cast.",
                "path": "plugins/asciinema-tools/commands/play.md",
                "frontmatter": {
                  "description": "Play .cast recordings in iTerm2 with speed controls. TRIGGERS - play recording, asciinema play, view cast.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob",
                  "argument-hint": "[file] [-s speed] [-i idle-limit] [-l loop] [-r resize] [-m markers]"
                },
                "content": "# /asciinema-tools:play\n\nPlay terminal recordings in a dedicated iTerm2 window.\n\n## Arguments\n\n| Argument                | Description                      |\n| ----------------------- | -------------------------------- |\n| `file`                  | Path to .cast file               |\n| `-s, --speed`           | Playback speed (e.g., `-s 6`)    |\n| `-i, --idle-time-limit` | Max idle time in seconds         |\n| `-l, --loop`            | Loop playback                    |\n| `-r, --resize`          | Match terminal to recording size |\n| `-m, --markers`         | Pause on markers                 |\n\n## Execution\n\nInvoke the `asciinema-player` skill with user-selected options.\n\n### Skip Logic\n\n- If `file` provided -> skip Phase 1 (file selection)\n- If `-s` provided -> skip Phase 2 (speed selection)\n- If any of `-i/-l/-r/-m` provided -> skip Phase 3 (options)\n\n### Workflow\n\n1. **Preflight**: Check iTerm2 and asciinema\n2. **Discovery**: Find .cast files\n3. **Selection**: AskUserQuestion for file\n4. **Speed**: AskUserQuestion for playback speed\n5. **Options**: AskUserQuestion for additional options\n6. **Launch**: Open iTerm2 via AppleScript"
              },
              {
                "name": "/post-session",
                "description": "Complete post-session workflow - finalize orphaned recordings, convert, and AI summarize. TRIGGERS - post session, analyze recording, session review, complete workflow.",
                "path": "plugins/asciinema-tools/commands/post-session.md",
                "frontmatter": {
                  "description": "Complete post-session workflow - finalize orphaned recordings, convert, and AI summarize. TRIGGERS - post session, analyze recording, session review, complete workflow.",
                  "allowed-tools": "Bash, Grep, AskUserQuestion, Glob, Write, Read, Task",
                  "argument-hint": "[file] [--finalize] [-q|--quick] [-f|--full] [--summarize] [--output file]"
                },
                "content": "# /asciinema-tools:post-session\n\nComplete post-session workflow: finalize orphaned recordings  convert to text  AI-powered summarize.\n\n## Arguments\n\n| Argument        | Description                                          |\n| --------------- | ---------------------------------------------------- |\n| `file`          | Path to .cast file (or auto-detect)                  |\n| `--finalize`    | Include finalize step (stop processes, compress)     |\n| `-q, --quick`   | Quick analysis (keyword grep + brief summary)        |\n| `-f, --full`    | Full analysis (convert + AI deep-dive summarize)     |\n| `--summarize`   | Include AI summarize step (iterative deep-dive)      |\n| `--output`      | Save findings to markdown file                       |\n\n## Workflow Modes\n\n### Quick Mode (`-q`)\n```\n[file]  convert  keyword grep  brief summary\n```\n\n### Full Mode (`-f`)\n```\n[file]  convert  AI summarize (iterative deep-dive)\n```\n\n### Complete Mode (`--finalize --full`)\n```\nstop processes  compress  push  convert  AI summarize\n```\n\n## Execution\n\n### Phase 1: Discovery\n\n```yaml\nAskUserQuestion:\n  question: \"What would you like to do?\"\n  header: \"Workflow\"\n  options:\n    - label: \"Quick analysis (Recommended)\"\n      description: \"Convert + keyword search + brief summary\"\n    - label: \"Full AI analysis\"\n      description: \"Convert + iterative AI deep-dive with guidance\"\n    - label: \"Complete workflow\"\n      description: \"Finalize orphans + convert + AI summarize\"\n    - label: \"Finalize only\"\n      description: \"Stop processes and push to orphan branch\"\n```\n\n### Phase 2: File Selection\n\nIf no file specified, discover available recordings:\n\n```bash\n/usr/bin/env bash << 'DISCOVER_EOF'\necho \"=== Running asciinema processes ===\"\nps aux | grep -E \"asciinema rec\" | grep -v grep | while read -r line; do\n  PID=$(echo \"$line\" | awk '{print $2}')\n  CAST=$(echo \"$line\" | grep -oE '[^ ]+\\.cast' | head -1)\n  if [[ -n \"$CAST\" ]]; then\n    SIZE=$(ls -lh \"$CAST\" 2>/dev/null | awk '{print $5}' || echo \"?\")\n    echo \"  [RUNNING] PID $PID: $CAST ($SIZE)\"\n  fi\ndone\n\necho \"\"\necho \"=== Recent .cast files ===\"\nfind ~/eon -name \"*.cast\" -size +1M -mtime -7 2>/dev/null | while read -r f; do\n  SIZE=$(ls -lh \"$f\" | awk '{print $5}')\n  MTIME=$(stat -f \"%Sm\" -t \"%m-%d %H:%M\" \"$f\" 2>/dev/null)\n  echo \"  $f ($SIZE, $MTIME)\"\ndone | head -10\n\necho \"\"\necho \"=== Recent .txt files (already converted) ===\"\nfind ~/eon -name \"*.txt\" -size +100M -mtime -7 2>/dev/null | while read -r f; do\n  SIZE=$(ls -lh \"$f\" | awk '{print $5}')\n  echo \"  $f ($SIZE)\"\ndone | head -5\nDISCOVER_EOF\n```\n\n```yaml\nAskUserQuestion:\n  question: \"Which recording to analyze?\"\n  header: \"Select\"\n  options:\n    # Dynamically populated from discovery\n    - label: \"{filename} ({size})\"\n      description: \"{path}\"\n```\n\n### Phase 3: Finalize (if selected)\n\nChain to `/asciinema-tools:finalize`:\n1. Stop running asciinema processes\n2. Verify file integrity\n3. Compress with zstd\n4. Push to orphan branch\n\n### Phase 4: Convert\n\n```bash\n/usr/bin/env bash << 'CONVERT_EOF'\nCAST_FILE=\"$1\"\nTXT_FILE=\"${CAST_FILE%.cast}.txt\"\n\necho \"Converting: $CAST_FILE\"\necho \"Output: $TXT_FILE\"\n\nif asciinema convert -f txt \"$CAST_FILE\" \"$TXT_FILE\"; then\n  ORIG=$(ls -lh \"$CAST_FILE\" | awk '{print $5}')\n  CONV=$(ls -lh \"$TXT_FILE\" | awk '{print $5}')\n  echo \" Converted: $ORIG  $CONV\"\nelse\n  echo \" Conversion failed\"\n  exit 1\nfi\nCONVERT_EOF\n```\n\n### Phase 5: Analysis\n\n**Quick mode**: Keyword grep + brief summary\n```bash\n# Run curated keyword searches\ngrep -c -i \"error\\|fail\\|exception\" \"$TXT_FILE\"\ngrep -c -i \"success\\|complete\\|done\" \"$TXT_FILE\"\ngrep -c -i \"sharpe\\|drawdown\\|backtest\" \"$TXT_FILE\"\n# ... summarize counts\n```\n\n**Full mode**: Chain to `/asciinema-tools:summarize`\n- Initial guidance via AskUserQuestion\n- Strategic sampling (head/middle/tail)\n- Iterative deep-dive with user guidance\n- Synthesis into findings report\n\n### Phase 6: Output\n\n```yaml\nAskUserQuestion:\n  question: \"Analysis complete. What next?\"\n  header: \"Output\"\n  options:\n    - label: \"Display summary\"\n      description: \"Show findings in terminal\"\n    - label: \"Save to markdown\"\n      description: \"Write findings to {filename}_findings.md\"\n    - label: \"Continue exploring\"\n      description: \"Deep-dive into specific sections\"\n    - label: \"Done\"\n      description: \"Exit workflow\"\n```\n\n## Example Usage\n\n```bash\n# Interactive mode - auto-detect and guide\n/asciinema-tools:post-session\n\n# Quick analysis on specific file\n/asciinema-tools:post-session session.cast -q\n\n# Full AI analysis with output\n/asciinema-tools:post-session session.cast -f --output findings.md\n\n# Complete workflow including finalize\n/asciinema-tools:post-session --finalize -f\n```\n\n## Related Commands\n\n- `/asciinema-tools:daemon-status` - View status and find unhandled files\n- `/asciinema-tools:finalize` - Finalize orphaned recordings\n- `/asciinema-tools:convert` - Convert .cast to .txt\n- `/asciinema-tools:summarize` - AI-powered deep analysis\n- `/asciinema-tools:analyze` - Keyword-based analysis"
              },
              {
                "name": "/record",
                "description": "Start terminal recording with asciinema. TRIGGERS - record session, capture terminal, start recording.",
                "path": "plugins/asciinema-tools/commands/record.md",
                "frontmatter": {
                  "description": "Start terminal recording with asciinema. TRIGGERS - record session, capture terminal, start recording.",
                  "allowed-tools": "Bash, AskUserQuestion, Glob",
                  "argument-hint": "[file] [-t title] [-i idle-limit] [--backup] [--append]"
                },
                "content": "# /asciinema-tools:record\n\nStart a terminal recording session with asciinema.\n\n## Arguments\n\n| Argument                | Description                        |\n| ----------------------- | ---------------------------------- |\n| `file`                  | Output path (e.g., `session.cast`) |\n| `-t, --title`           | Recording title                    |\n| `-i, --idle-time-limit` | Max idle time in seconds           |\n| `--backup`              | Enable streaming backup to GitHub  |\n| `--append`              | Append to existing recording       |\n\n## Execution\n\nInvoke the `asciinema-recorder` skill with user-selected options.\n\n### Skip Logic\n\n- If `file` provided -> skip Phase 1 (output location)\n- If `-t` and `-i` provided -> skip Phase 2 (options)\n\n### Workflow\n\n1. **Preflight**: Check asciinema installed\n2. **Location**: AskUserQuestion for output path\n3. **Options**: AskUserQuestion for recording options\n4. **Generate**: Build and display recording command\n5. **Guidance**: Show step-by-step instructions"
              },
              {
                "name": "/setup",
                "description": "Check and install dependencies for asciinema-tools. TRIGGERS - setup, check deps, preflight.",
                "path": "plugins/asciinema-tools/commands/setup.md",
                "frontmatter": {
                  "description": "Check and install dependencies for asciinema-tools. TRIGGERS - setup, check deps, preflight.",
                  "allowed-tools": "Bash, AskUserQuestion",
                  "argument-hint": "[check|install|repair] [--all] [--core] [--optional] [-y|--yes]"
                },
                "content": "# /asciinema-tools:setup\n\nCheck and install all dependencies for asciinema-tools.\n\n## Arguments\n\n| Argument     | Description                       |\n| ------------ | --------------------------------- |\n| `check`      | Run preflight check (default)     |\n| `install`    | Install missing dependencies      |\n| `repair`     | Reinstall/upgrade all components  |\n| `--all`      | Install all (core + optional)     |\n| `--core`     | Install core only (asciinema, rg) |\n| `--optional` | Install optional only             |\n| `-y, --yes`  | Skip confirmation prompts         |\n\n## Dependencies\n\n| Component | Type     | Installation                 |\n| --------- | -------- | ---------------------------- |\n| asciinema | Core     | `brew install asciinema`     |\n| ripgrep   | Core     | `brew install ripgrep`       |\n| iTerm2    | Optional | `brew install --cask iterm2` |\n| fswatch   | Optional | `brew install fswatch`       |\n| gh CLI    | Optional | `brew install gh`            |\n| YAKE      | Optional | `uv run --with yake`         |\n\n## Execution\n\n### Skip Logic\n\n- If action provided -> skip Phase 1 (action selection)\n- If `--core/--all/--optional` provided -> skip Phase 2\n- If `-y` provided -> skip all confirmations\n\n### Workflow\n\n1. **Check**: Run preflight for all dependencies\n2. **Action**: AskUserQuestion for action type\n3. **Selection**: AskUserQuestion for components\n4. **Install**: Run selected installations\n5. **Verify**: Confirm installation success"
              },
              {
                "name": "/summarize",
                "description": "AI-powered iterative deep-dive analysis of converted recordings. TRIGGERS - summarize recording, analyze session, what happened, session summary, deep analysis, findings extraction.",
                "path": "plugins/asciinema-tools/commands/summarize.md",
                "frontmatter": {
                  "description": "AI-powered iterative deep-dive analysis of converted recordings. TRIGGERS - summarize recording, analyze session, what happened, session summary, deep analysis, findings extraction.",
                  "allowed-tools": "Bash, Grep, Read, AskUserQuestion, Task, Write",
                  "argument-hint": "[file] [--topic topic] [--depth quick|medium|deep] [--output file]"
                },
                "content": "# /asciinema-tools:summarize\n\nAI-powered iterative deep-dive analysis for large .txt recordings. Uses guided sampling and AskUserQuestion to progressively explore the content.\n\n## Philosophy\n\nLarge recordings (1GB+) cannot be read entirely. This command uses:\n1. **Initial guidance** - What are you looking for?\n2. **Strategic sampling** - Head, middle, tail + keyword-targeted sections\n3. **Iterative refinement** - AskUserQuestion to drill deeper into findings\n4. **Progressive synthesis** - Build understanding through multiple passes\n\n## Arguments\n\n| Argument        | Description                                          |\n| --------------- | ---------------------------------------------------- |\n| `file`          | Path to .txt file (converted from .cast)             |\n| `--topic`       | Initial focus area (e.g., \"ML training\", \"errors\")   |\n| `--depth`       | Analysis depth: `quick`, `medium`, `deep`            |\n| `--output`      | Save findings to markdown file                       |\n\n## Workflow\n\n### Phase 1: Initial Guidance\n\n```yaml\nAskUserQuestion:\n  question: \"What are you trying to understand from this recording?\"\n  header: \"Focus\"\n  options:\n    - label: \"General overview\"\n      description: \"What happened in this session? Key activities and outcomes\"\n    - label: \"Key findings/decisions\"\n      description: \"Important discoveries, conclusions, or decisions made\"\n    - label: \"Errors and debugging\"\n      description: \"What went wrong? How was it resolved?\"\n    - label: \"Specific topic\"\n      description: \"I'll specify what I'm looking for\"\n```\n\n### Phase 2: File Statistics\n\n```bash\n/usr/bin/env bash << 'STATS_EOF'\nFILE=\"$1\"\n\necho \"=== File Statistics ===\"\nSIZE=$(ls -lh \"$FILE\" | awk '{print $5}')\nLINES=$(wc -l < \"$FILE\")\necho \"Size: $SIZE\"\necho \"Lines: $LINES\"\n\necho \"\"\necho \"=== Content Sampling ===\"\necho \"First 20 lines:\"\nhead -20 \"$FILE\"\n\necho \"\"\necho \"Last 20 lines:\"\ntail -20 \"$FILE\"\n\necho \"\"\necho \"=== Keyword Density ===\"\necho \"Errors/failures:\"\ngrep -c -i \"error\\|fail\\|exception\" \"$FILE\" || echo \"0\"\necho \"Success indicators:\"\ngrep -c -i \"success\\|complete\\|done\\|pass\" \"$FILE\" || echo \"0\"\necho \"Key decisions:\"\ngrep -c -i \"decision\\|chose\\|selected\\|using\" \"$FILE\" || echo \"0\"\nSTATS_EOF\n```\n\n### Phase 3: Strategic Sampling\n\nBased on file size, sample strategically:\n\n**For files < 100MB:**\n```bash\n# Sample head, middle, tail (1000 lines each)\nhead -1000 \"$FILE\" > /tmp/sample_head.txt\ntail -1000 \"$FILE\" > /tmp/sample_tail.txt\nTOTAL=$(wc -l < \"$FILE\")\nMIDDLE=$((TOTAL / 2))\nsed -n \"${MIDDLE},$((MIDDLE + 1000))p\" \"$FILE\" > /tmp/sample_middle.txt\n```\n\n**For files > 100MB:**\n```bash\n# Keyword-targeted sampling\ngrep -B5 -A20 -i \"$TOPIC_KEYWORDS\" \"$FILE\" | head -5000 > /tmp/sample_targeted.txt\n```\n\n### Phase 4: Initial Analysis\n\nRead the samples and provide initial findings. Then ask:\n\n```yaml\nAskUserQuestion:\n  question: \"Based on initial analysis, what would you like to explore deeper?\"\n  header: \"Drill down\"\n  multiSelect: true\n  options:\n    - label: \"Specific timeframe\"\n      description: \"Jump to a particular section (e.g., 'around line 50000')\"\n    - label: \"Follow keyword trail\"\n      description: \"Search for specific patterns and expand context\"\n    - label: \"Error investigation\"\n      description: \"Deep dive into errors and their resolution\"\n    - label: \"Success moments\"\n      description: \"What worked? What were the wins?\"\n    - label: \"Generate summary\"\n      description: \"Synthesize findings into a report\"\n```\n\n### Phase 5: Iterative Deep-Dive\n\nFor each selected focus area:\n\n1. **Extract relevant sections** using grep with context\n2. **Read and analyze** the extracted content\n3. **Report findings** to user\n4. **Ask for next action** via AskUserQuestion\n\n```yaml\nAskUserQuestion:\n  question: \"Found {N} relevant sections. What next?\"\n  header: \"Continue\"\n  options:\n    - label: \"Show me the most significant\"\n      description: \"Display top 3 most relevant excerpts\"\n    - label: \"Search for related patterns\"\n      description: \"Expand search to related keywords\"\n    - label: \"Move on\"\n      description: \"I have enough on this topic\"\n```\n\n### Phase 6: Synthesis\n\n```yaml\nAskUserQuestion:\n  question: \"Ready to generate summary. What format?\"\n  header: \"Output\"\n  options:\n    - label: \"Concise bullet points\"\n      description: \"Key findings in 10-15 bullets\"\n    - label: \"Detailed markdown report\"\n      description: \"Full report with sections and evidence\"\n    - label: \"Executive summary\"\n      description: \"1-paragraph high-level summary\"\n    - label: \"Save to file\"\n      description: \"Write findings to markdown file\"\n```\n\n## Keyword Libraries\n\n### Trading/ML Domain\n```\nsharpe|drawdown|backtest|overfitting|regime|validation\nmodel|training|loss|epoch|gradient|convergence\nfeature|indicator|signal|position|portfolio\n```\n\n### Development Domain\n```\nerror|exception|fail|bug|fix|debug\ncommit|push|merge|branch|deploy\ntest|assert|verify|validate|check\n```\n\n### Claude Code Domain\n```\ntool|bash|read|write|edit|grep\ntask|agent|subagent|spawn\npermission|approve|reject|block\n```\n\n## Example Usage\n\n```bash\n# Interactive exploration\n/asciinema-tools:summarize session.txt\n\n# Focused on ML findings\n/asciinema-tools:summarize session.txt --topic \"ML training results\"\n\n# Quick overview\n/asciinema-tools:summarize session.txt --depth quick\n\n# Full analysis with report\n/asciinema-tools:summarize session.txt --depth deep --output findings.md\n```\n\n## Example Output\n\n```markdown\n# Session Summary: alpha-forge-research_20251226\n\n## Overview\n- **Duration**: 4 days (Dec 26-30, 2025)\n- **Size**: 12GB recording  3.2GB text\n- **Primary Focus**: ML robustness research\n\n## Key Findings\n\n### 1. Training-Evaluation Mismatch (CRITICAL)\n- MSE loss optimizes magnitude, but Sharpe evaluates direction\n- Result: 80% Sharpe collapse from 2024 to 2025\n\n### 2. Fishr =0.1 Solution (BREAKTHROUGH)\n- Gradient variance penalty solves V-REx binary threshold\n- Feb'24 Sharpe: -6.14  +6.14\n\n### 3. Model Rankings\n| Model | Window | Sharpe |\n|-------|--------|--------|\n| TFT   | 15mo   | 1.02   |\n| BiLSTM| 12mo   | 0.50   |\n\n## Evidence Locations\n- Line 15234: \"Fishr =0.1 SOLVES the V-REx binary threshold problem\"\n- Line 48102: Phase 4 results summary table\n\n## Next Steps Identified\n1. TFT 15mo + Fishr training\n2. DSR/PBO statistical validation\n3. Agent research synthesis\n```\n\n## Related Commands\n\n- `/asciinema-tools:convert` - Convert .cast to .txt first\n- `/asciinema-tools:analyze` - Keyword-based analysis (faster, less deep)\n- `/asciinema-tools:finalize` - Process orphaned recordings"
              }
            ],
            "skills": [
              {
                "name": "asciinema-analyzer",
                "description": "Semantic analysis for Claude Code consumption. TRIGGERS - analyze cast, keyword extraction, density analysis, find patterns, sharpe, backtest, iteration, trading keywords, ML keywords. Use when extracting insights from recordings.",
                "path": "plugins/asciinema-tools/skills/asciinema-analyzer/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-analyzer",
                  "description": "Semantic analysis for Claude Code consumption. TRIGGERS - analyze cast, keyword extraction, density analysis, find patterns, sharpe, backtest, iteration, trading keywords, ML keywords. Use when extracting insights from recordings.",
                  "allowed-tools": "Read, Bash, Grep, Glob, AskUserQuestion"
                },
                "content": "# asciinema-analyzer\n\nSemantic analysis of converted .txt recordings for Claude Code consumption. Uses tiered analysis: ripgrep (primary, 50-200ms) -> YAKE (secondary, 1-5s) -> TF-IDF (optional).\n\n> **Platform**: macOS, Linux (requires ripgrep, optional YAKE)\n\n---\n\n## Analysis Tiers\n\n| Tier | Tool    | Speed (4MB) | When to Use                    |\n| ---- | ------- | ----------- | ------------------------------ |\n| 1    | ripgrep | 50-200ms    | Always start here (curated)    |\n| 2    | YAKE    | 1-5s        | Auto-discover unexpected terms |\n| 3    | TF-IDF  | 5-30s       | Topic modeling (optional)      |\n\n**Decision**: Start with Tier 1 (ripgrep + curated keywords). Only use Tier 2 (YAKE) when auto-discovery is explicitly requested.\n\n---\n\n## Requirements\n\n| Component   | Required | Installation           | Notes                   |\n| ----------- | -------- | ---------------------- | ----------------------- |\n| **ripgrep** | Yes      | `brew install ripgrep` | Primary search tool     |\n| **YAKE**    | Optional | `uv run --with yake`   | For auto-discovery tier |\n\n---\n\n## Workflow Phases (ALL MANDATORY)\n\n**IMPORTANT**: All phases are MANDATORY. Do NOT skip any phase. AskUserQuestion MUST be used at each decision point.\n\n### Phase 0: Preflight Check\n\n**Purpose**: Verify input file exists and check for .txt (converted) format.\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nINPUT_FILE=\"${1:-}\"\n\nif [[ -z \"$INPUT_FILE\" ]]; then\n  echo \"NO_FILE_PROVIDED\"\nelif [[ ! -f \"$INPUT_FILE\" ]]; then\n  echo \"FILE_NOT_FOUND: $INPUT_FILE\"\nelif [[ \"$INPUT_FILE\" == *.cast ]]; then\n  echo \"WRONG_FORMAT: Convert to .txt first with /asciinema-tools:convert\"\nelif [[ \"$INPUT_FILE\" == *.txt ]]; then\n  SIZE=$(ls -lh \"$INPUT_FILE\" | awk '{print $5}')\n  LINES=$(wc -l < \"$INPUT_FILE\" | tr -d ' ')\n  echo \"READY: $INPUT_FILE ($SIZE, $LINES lines)\"\nelse\n  echo \"UNKNOWN_FORMAT: Expected .txt file\"\nfi\nPREFLIGHT_EOF\n```\n\nIf no .txt file found, suggest running `/asciinema-tools:convert` first.\n\n---\n\n### Phase 1: File Selection (MANDATORY)\n\n**Purpose**: Discover .txt files and let user select which to analyze.\n\n#### Step 1.1: Discover .txt Files\n\n```bash\n/usr/bin/env bash << 'DISCOVER_TXT_EOF'\n# Find .txt files that look like converted recordings\nfor file in $(fd -e txt . --max-depth 3 2>/dev/null | head -10); do\n  SIZE=$(ls -lh \"$file\" 2>/dev/null | awk '{print $5}')\n  LINES=$(wc -l < \"$file\" 2>/dev/null | tr -d ' ')\n  BASENAME=$(basename \"$file\")\n  echo \"FILE:$file|SIZE:$SIZE|LINES:$LINES|NAME:$BASENAME\"\ndone\nDISCOVER_TXT_EOF\n```\n\n#### Step 1.2: Present File Selection (MANDATORY AskUserQuestion)\n\n```\nQuestion: \"Which file would you like to analyze?\"\nHeader: \"File\"\nOptions:\n  - Label: \"{filename}.txt ({size})\"\n    Description: \"{line_count} lines\"\n  - Label: \"{filename2}.txt ({size2})\"\n    Description: \"{line_count2} lines\"\n  - Label: \"Enter path\"\n    Description: \"Provide a custom path to a .txt file\"\n  - Label: \"Convert first\"\n    Description: \"Run /asciinema-tools:convert before analysis\"\n```\n\n---\n\n### Phase 2: Analysis Type (MANDATORY)\n\n**Purpose**: Let user choose analysis depth.\n\n```\nQuestion: \"What type of analysis do you need?\"\nHeader: \"Type\"\nOptions:\n  - Label: \"Curated keywords (Recommended)\"\n    Description: \"Fast search (50-200ms) with domain-specific keyword sets\"\n  - Label: \"Auto-discover keywords\"\n    Description: \"YAKE unsupervised extraction (1-5s) - finds unexpected patterns\"\n  - Label: \"Full analysis\"\n    Description: \"Both curated + auto-discovery for comprehensive results\"\n  - Label: \"Density analysis\"\n    Description: \"Find high-concentration sections (peak activity windows)\"\n```\n\n---\n\n### Phase 3: Domain Selection (MANDATORY)\n\n**Purpose**: Let user select which keyword domains to search.\n\n```\nQuestion: \"Which domain keywords to search?\"\nHeader: \"Domain\"\nmultiSelect: true\nOptions:\n  - Label: \"Trading/Quantitative\"\n    Description: \"sharpe, sortino, calmar, backtest, drawdown, pnl, cagr, alpha, beta\"\n  - Label: \"ML/AI\"\n    Description: \"epoch, loss, accuracy, sota, training, model, validation, inference\"\n  - Label: \"Development\"\n    Description: \"iteration, refactor, fix, test, deploy, build, commit, merge\"\n  - Label: \"Claude Code\"\n    Description: \"Skill, TodoWrite, Read, Edit, Bash, Grep, iteration complete\"\n```\n\nSee [Domain Keywords Reference](./references/domain-keywords.md) for complete keyword lists.\n\n---\n\n### Phase 4: Execute Curated Analysis\n\n**Purpose**: Run Grep searches for selected domain keywords.\n\n#### Step 4.1: Trading Domain\n\n```bash\n/usr/bin/env bash << 'TRADING_EOF'\nINPUT_FILE=\"${1:?}\"\necho \"=== Trading/Quantitative Keywords ===\"\n\nKEYWORDS=\"sharpe sortino calmar backtest drawdown pnl cagr alpha beta roi volatility\"\nfor kw in $KEYWORDS; do\n  COUNT=$(rg -c -i \"$kw\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\n  if [[ \"$COUNT\" -gt 0 ]]; then\n    echo \"  $kw: $COUNT\"\n  fi\ndone\nTRADING_EOF\n```\n\n#### Step 4.2: ML/AI Domain\n\n```bash\n/usr/bin/env bash << 'ML_EOF'\nINPUT_FILE=\"${1:?}\"\necho \"=== ML/AI Keywords ===\"\n\nKEYWORDS=\"epoch loss accuracy sota training model validation inference tensor gradient\"\nfor kw in $KEYWORDS; do\n  COUNT=$(rg -c -i \"$kw\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\n  if [[ \"$COUNT\" -gt 0 ]]; then\n    echo \"  $kw: $COUNT\"\n  fi\ndone\nML_EOF\n```\n\n#### Step 4.3: Development Domain\n\n```bash\n/usr/bin/env bash << 'DEV_EOF'\nINPUT_FILE=\"${1:?}\"\necho \"=== Development Keywords ===\"\n\nKEYWORDS=\"iteration refactor fix test deploy build commit merge debug error\"\nfor kw in $KEYWORDS; do\n  COUNT=$(rg -c -i \"$kw\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\n  if [[ \"$COUNT\" -gt 0 ]]; then\n    echo \"  $kw: $COUNT\"\n  fi\ndone\nDEV_EOF\n```\n\n#### Step 4.4: Claude Code Domain\n\n```bash\n/usr/bin/env bash << 'CLAUDE_EOF'\nINPUT_FILE=\"${1:?}\"\necho \"=== Claude Code Keywords ===\"\n\nKEYWORDS=\"Skill TodoWrite Read Edit Bash Grep Write\"\nfor kw in $KEYWORDS; do\n  COUNT=$(rg -c \"$kw\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\n  if [[ \"$COUNT\" -gt 0 ]]; then\n    echo \"  $kw: $COUNT\"\n  fi\ndone\n\n# Special patterns\nITERATION=$(rg -c \"iteration complete\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\necho \"  'iteration complete': $ITERATION\"\nCLAUDE_EOF\n```\n\n---\n\n### Phase 5: YAKE Auto-Discovery (if selected)\n\n**Purpose**: Run unsupervised keyword extraction.\n\n```bash\n/usr/bin/env bash << 'YAKE_EOF'\nINPUT_FILE=\"${1:?}\"\necho \"=== Auto-discovered Keywords (YAKE) ===\"\n\nuv run --with yake python3 -c \"\nimport yake\n\nkw = yake.KeywordExtractor(\n    lan='en',\n    n=2,           # bi-grams\n    dedupLim=0.9,  # dedup threshold\n    top=20         # top keywords\n)\n\nwith open('$INPUT_FILE') as f:\n    text = f.read()\n\nkeywords = kw.extract_keywords(text)\nfor score, keyword in keywords:\n    print(f'{score:.4f}  {keyword}')\n\"\nYAKE_EOF\n```\n\n---\n\n### Phase 6: Density Analysis (if selected)\n\n**Purpose**: Find sections with highest keyword concentration.\n\n```bash\n/usr/bin/env bash << 'DENSITY_EOF'\nINPUT_FILE=\"${1:?}\"\nKEYWORD=\"${2:-sharpe}\"\nWINDOW_SIZE=100  # lines\n\necho \"=== Density Analysis: '$KEYWORD' ===\"\necho \"Window size: $WINDOW_SIZE lines\"\necho \"\"\n\nTOTAL_LINES=$(wc -l < \"$INPUT_FILE\" | tr -d ' ')\nTOTAL_MATCHES=$(rg -c -i \"$KEYWORD\" \"$INPUT_FILE\" 2>/dev/null || echo \"0\")\n\necho \"Total matches: $TOTAL_MATCHES in $TOTAL_LINES lines\"\necho \"Overall density: $(echo \"scale=4; $TOTAL_MATCHES / $TOTAL_LINES * 1000\" | bc) per 1000 lines\"\necho \"\"\n\n# Find peak windows\necho \"Top 5 densest windows:\"\nawk -v ws=\"$WINDOW_SIZE\" -v kw=\"$KEYWORD\" '\nBEGIN { IGNORECASE=1 }\n{\n  lines[NR] = $0\n  if (tolower($0) ~ tolower(kw)) matches[NR] = 1\n}\nEND {\n  for (start = 1; start <= NR - ws; start += ws/2) {\n    count = 0\n    for (i = start; i < start + ws && i <= NR; i++) {\n      if (matches[i]) count++\n    }\n    if (count > 0) {\n      printf \"Lines %d-%d: %d matches (%.1f per 100)\\n\", start, start+ws-1, count, count*100/ws\n    }\n  }\n}\n' \"$INPUT_FILE\" | sort -t: -k2 -rn | head -5\nDENSITY_EOF\n```\n\n---\n\n### Phase 7: Report Format (MANDATORY)\n\n**Purpose**: Let user choose output format.\n\n```\nQuestion: \"How should results be presented?\"\nHeader: \"Output\"\nOptions:\n  - Label: \"Summary table (Recommended)\"\n    Description: \"Keyword counts + top 5 peak sections\"\n  - Label: \"Detailed report\"\n    Description: \"Full analysis with timestamps and surrounding context\"\n  - Label: \"JSON export\"\n    Description: \"Machine-readable output for further processing\"\n  - Label: \"Markdown report\"\n    Description: \"Save formatted report to file\"\n```\n\n---\n\n### Phase 8: Follow-up Actions (MANDATORY)\n\n**Purpose**: Guide user to next action.\n\n```\nQuestion: \"Analysis complete. What's next?\"\nHeader: \"Next\"\nOptions:\n  - Label: \"Jump to peak section\"\n    Description: \"Read the highest-density section in the file\"\n  - Label: \"Search for specific keyword\"\n    Description: \"Grep for a custom term with context\"\n  - Label: \"Cross-reference with .cast\"\n    Description: \"Map findings back to original timestamps\"\n  - Label: \"Done\"\n    Description: \"Exit - no further action needed\"\n```\n\n---\n\n## TodoWrite Task Template\n\n```\n1. [Preflight] Check input file exists and is .txt format\n2. [Preflight] Suggest /convert if .cast file provided\n3. [Discovery] Find .txt files with line counts\n4. [Selection] AskUserQuestion: file to analyze\n5. [Type] AskUserQuestion: analysis type (curated/auto/full/density)\n6. [Domain] AskUserQuestion: keyword domains (multi-select)\n7. [Curated] Run Grep searches for selected domains\n8. [Auto] Run YAKE if auto-discovery selected\n9. [Density] Calculate density windows if requested\n10. [Format] AskUserQuestion: report format\n11. [Next] AskUserQuestion: follow-up actions\n```\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] All bash blocks use heredoc wrapper\n2. [ ] Curated keywords match references/domain-keywords.md\n3. [ ] Analysis tiers match references/analysis-tiers.md\n4. [ ] YAKE invocation uses `uv run --with yake`\n5. [ ] All AskUserQuestion phases are present\n6. [ ] TodoWrite template matches actual workflow\n\n---\n\n## Reference Documentation\n\n- [Domain Keywords Reference](./references/domain-keywords.md)\n- [Analysis Tiers Reference](./references/analysis-tiers.md)\n- [ripgrep Manual](https://github.com/BurntSushi/ripgrep)\n- [YAKE Documentation](https://github.com/LIAAD/yake)"
              },
              {
                "name": "asciinema-cast-format",
                "description": "Reference for asciinema v3 NDJSON format. TRIGGERS - cast format, asciicast spec, event codes, cast header, cast structure, parse cast file. Use when understanding or parsing .cast files.",
                "path": "plugins/asciinema-tools/skills/asciinema-cast-format/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-cast-format",
                  "description": "Reference for asciinema v3 NDJSON format. TRIGGERS - cast format, asciicast spec, event codes, cast header, cast structure, parse cast file. Use when understanding or parsing .cast files.",
                  "allowed-tools": "Read, Bash"
                },
                "content": "# asciinema-cast-format\n\nReference documentation for the asciinema v3 .cast file format (asciicast v2 specification).\n\n> **Platform**: All platforms (documentation only)\n\n---\n\n## Format Overview\n\nAsciinema v3 uses NDJSON (Newline Delimited JSON) format:\n\n- Line 1: Header object with recording metadata\n- Lines 2+: Event arrays with timestamp, type, and data\n\n---\n\n## Header Specification\n\nThe first line is a JSON object with these fields:\n\n| Field       | Type   | Required | Description                                 |\n| ----------- | ------ | -------- | ------------------------------------------- |\n| `version`   | int    | Yes      | Format version (always 2 for v3 recordings) |\n| `width`     | int    | Yes      | Terminal width in columns                   |\n| `height`    | int    | Yes      | Terminal height in rows                     |\n| `timestamp` | int    | No       | Unix timestamp of recording start           |\n| `duration`  | float  | No       | Total duration in seconds                   |\n| `title`     | string | No       | Recording title                             |\n| `env`       | object | No       | Environment variables (SHELL, TERM)         |\n| `theme`     | object | No       | Terminal color theme                        |\n\n### Example Header\n\n```json\n{\n  \"version\": 2,\n  \"width\": 120,\n  \"height\": 40,\n  \"timestamp\": 1703462400,\n  \"duration\": 3600.5,\n  \"title\": \"Claude Code Session\",\n  \"env\": { \"SHELL\": \"/bin/zsh\", \"TERM\": \"xterm-256color\" }\n}\n```\n\n---\n\n## Event Codes\n\nEach event after the header is a 3-element array:\n\n```json\n[timestamp, event_type, data]\n```\n\n| Code | Name   | Description                 | Data Format          |\n| ---- | ------ | --------------------------- | -------------------- |\n| `o`  | Output | Terminal output (stdout)    | String               |\n| `i`  | Input  | Terminal input (stdin)      | String               |\n| `m`  | Marker | Named marker for navigation | String (marker name) |\n| `r`  | Resize | Terminal resize event       | `\"WIDTHxHEIGHT\"`     |\n| `x`  | Exit   | Extension for custom data   | Varies               |\n\n### Event Examples\n\n```json\n[0.5, \"o\", \"$ ls -la\\r\\n\"]\n[1.2, \"o\", \"total 48\\r\\n\"]\n[1.3, \"o\", \"drwxr-xr-x  12 user  staff  384 Dec 24 10:00 .\\r\\n\"]\n[5.0, \"m\", \"file-listing-complete\"]\n[10.5, \"r\", \"80x24\"]\n```\n\n---\n\n## Timestamp Behavior\n\n- Timestamps are **relative to recording start** (first event is 0.0)\n- Measured in seconds with millisecond precision\n- Used for playback timing and navigation\n\n### Calculating Absolute Time\n\n```bash\n/usr/bin/env bash << 'CALC_TIME_EOF'\nHEADER_TIMESTAMP=$(head -1 recording.cast | jq -r '.timestamp')\nEVENT_OFFSET=1234.5  # From event array\n\nABSOLUTE=$(echo \"$HEADER_TIMESTAMP + $EVENT_OFFSET\" | bc)\ndate -r \"$ABSOLUTE\"  # macOS\n# date -d \"@$ABSOLUTE\"  # Linux\nCALC_TIME_EOF\n```\n\n---\n\n## Parsing Examples\n\n### Extract Header with jq\n\n```bash\n/usr/bin/env bash << 'HEADER_EOF'\nhead -1 recording.cast | jq '.'\nHEADER_EOF\n```\n\n### Get Recording Duration\n\n```bash\n/usr/bin/env bash << 'DURATION_EOF'\nhead -1 recording.cast | jq -r '.duration // \"unknown\"'\nDURATION_EOF\n```\n\n### Count Events by Type\n\n```bash\n/usr/bin/env bash << 'COUNT_EOF'\ntail -n +2 recording.cast | jq -r '.[1]' | sort | uniq -c\nCOUNT_EOF\n```\n\n### Extract All Output Events\n\n```bash\n/usr/bin/env bash << 'OUTPUT_EOF'\ntail -n +2 recording.cast | jq -r 'select(.[1] == \"o\") | .[2]'\nOUTPUT_EOF\n```\n\n### Find Markers\n\n```bash\n/usr/bin/env bash << 'MARKERS_EOF'\ntail -n +2 recording.cast | jq -r 'select(.[1] == \"m\") | \"\\(.[0])s: \\(.[2])\"'\nMARKERS_EOF\n```\n\n### Get Event at Specific Time\n\n```bash\n/usr/bin/env bash << 'TIME_EOF'\nTARGET_TIME=60  # seconds\ntail -n +2 recording.cast | jq -r \"select(.[0] >= $TARGET_TIME and .[0] < $((TARGET_TIME + 1))) | .[2]\"\nTIME_EOF\n```\n\n---\n\n## Large File Considerations\n\nFor recordings >100MB:\n\n| File Size | Line Count | Approach                              |\n| --------- | ---------- | ------------------------------------- |\n| <100MB    | <1M        | jq streaming works fine               |\n| 100-500MB | 1-5M       | Use `--stream` flag, consider ripgrep |\n| 500MB+    | 5M+        | Convert to .txt first with asciinema  |\n\n### Memory-Efficient Streaming\n\n```bash\n/usr/bin/env bash << 'STREAM_EOF'\n# Stream process large files\njq --stream -n 'fromstream(1|truncate_stream(inputs))' recording.cast | head -1000\nSTREAM_EOF\n```\n\n### Use asciinema convert\n\nFor very large files, convert to plain text first:\n\n```bash\nasciinema convert -f txt recording.cast recording.txt\n```\n\nThis strips ANSI codes and produces clean text (typically 950:1 compression).\n\n---\n\n## TodoWrite Task Template\n\n```\n1. [Reference] Identify .cast file to analyze\n2. [Header] Extract and display header metadata\n3. [Events] Count events by type (o, i, m, r)\n4. [Analysis] Extract relevant event data based on user need\n5. [Navigation] Find markers or specific timestamps if needed\n```\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Event code table matches asciinema v2 specification\n2. [ ] Parsing examples use heredoc wrapper for bash compatibility\n3. [ ] Large file guidance reflects actual performance characteristics\n4. [ ] All jq commands tested with sample .cast files\n\n---\n\n## Reference Documentation\n\n- [asciinema asciicast v2 Format](https://docs.asciinema.org/manual/asciicast/v2/)\n- [asciinema CLI Usage](https://docs.asciinema.org/manual/cli/usage/)\n- [jq Manual](https://jqlang.github.io/jq/manual/)"
              },
              {
                "name": "asciinema-converter",
                "description": "Convert .cast to .txt for analysis. TRIGGERS - convert cast, cast to txt, strip ANSI, timestamp index. Use when preparing recordings for Claude Code.",
                "path": "plugins/asciinema-tools/skills/asciinema-converter/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-converter",
                  "description": "Convert .cast to .txt for analysis. TRIGGERS - convert cast, cast to txt, strip ANSI, timestamp index. Use when preparing recordings for Claude Code.",
                  "allowed-tools": "Read, Bash, Glob, Write, AskUserQuestion"
                },
                "content": "# asciinema-converter\n\nConvert asciinema .cast recordings to clean .txt files for Claude Code analysis. Achieves 950:1 compression (3.8GB -> 4MB) by stripping ANSI codes and JSON structure.\n\n> **Platform**: macOS, Linux (requires asciinema CLI v2.4+)\n\n---\n\n## Why Convert?\n\n| Format | Size (22h session) | Claude Code Compatible | Searchable |\n| ------ | ------------------ | ---------------------- | ---------- |\n| .cast  | 3.8GB              | No (NDJSON + ANSI)     | Via jq     |\n| .txt   | ~4MB               | Yes (clean text)       | Grep/Read  |\n\n**Key benefit**: Claude Code's Read and Grep tools work directly on .txt output.\n\n---\n\n## Requirements\n\n| Component     | Required | Installation             | Notes                 |\n| ------------- | -------- | ------------------------ | --------------------- |\n| **asciinema** | Yes      | `brew install asciinema` | v2.4+ for convert cmd |\n\n---\n\n## Workflow Phases (ALL MANDATORY)\n\n**IMPORTANT**: All phases are MANDATORY. Do NOT skip any phase. AskUserQuestion MUST be used at each decision point.\n\n### Phase 0: Preflight Check\n\n**Purpose**: Verify asciinema is installed and supports convert command.\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\nif command -v asciinema &>/dev/null; then\n  VERSION=$(asciinema --version | head -1)\n  echo \"asciinema: $VERSION\"\n\n  # Check if convert command exists (v2.4+)\n  if asciinema convert --help &>/dev/null 2>&1; then\n    echo \"convert: available\"\n  else\n    echo \"convert: MISSING (update asciinema to v2.4+)\"\n  fi\nelse\n  echo \"asciinema: MISSING\"\nfi\nPREFLIGHT_EOF\n```\n\nIf asciinema is NOT installed or convert is missing, use AskUserQuestion:\n\n```\nQuestion: \"asciinema CLI issue detected. How would you like to proceed?\"\nHeader: \"Setup\"\nOptions:\n  - Label: \"Install/upgrade asciinema (Recommended)\"\n    Description: \"Run: brew install asciinema (or upgrade if outdated)\"\n  - Label: \"Show manual instructions\"\n    Description: \"Display installation commands for all platforms\"\n  - Label: \"Cancel\"\n    Description: \"Exit without converting\"\n```\n\n---\n\n### Phase 1: File Discovery & Selection (MANDATORY)\n\n**Purpose**: Discover .cast files and let user select which to convert.\n\n#### Step 1.1: Discover .cast Files\n\n```bash\n/usr/bin/env bash << 'DISCOVER_EOF'\n# Search for .cast files with metadata\nfor file in $(fd -e cast . --max-depth 5 2>/dev/null | head -10); do\n  SIZE=$(ls -lh \"$file\" 2>/dev/null | awk '{print $5}')\n  LINES=$(wc -l < \"$file\" 2>/dev/null | tr -d ' ')\n  DURATION=$(head -1 \"$file\" 2>/dev/null | jq -r '.duration // \"unknown\"' 2>/dev/null)\n  BASENAME=$(basename \"$file\")\n  echo \"FILE:$file|SIZE:$SIZE|LINES:$LINES|DURATION:$DURATION|NAME:$BASENAME\"\ndone\nDISCOVER_EOF\n```\n\n#### Step 1.2: Present File Selection (MANDATORY AskUserQuestion)\n\nUse discovery results to populate options:\n\n```\nQuestion: \"Which recording would you like to convert?\"\nHeader: \"Recording\"\nOptions:\n  - Label: \"{filename} ({size})\"\n    Description: \"{line_count} events, {duration}s duration\"\n  - Label: \"{filename2} ({size2})\"\n    Description: \"{line_count2} events, {duration2}s duration\"\n  - Label: \"Browse for file\"\n    Description: \"Search in a different directory\"\n  - Label: \"Enter path\"\n    Description: \"Provide a custom path to a .cast file\"\n```\n\n---\n\n### Phase 2: Output Options (MANDATORY)\n\n**Purpose**: Let user configure conversion behavior.\n\n```\nQuestion: \"Select conversion options:\"\nHeader: \"Options\"\nmultiSelect: true\nOptions:\n  - Label: \"Plain text output (Recommended)\"\n    Description: \"Convert to .txt with all ANSI codes stripped\"\n  - Label: \"Create timestamp index\"\n    Description: \"Generate [HH:MM:SS] indexed version for navigation\"\n  - Label: \"Split by idle time\"\n    Description: \"Create separate chunks at 30s+ pauses\"\n  - Label: \"Preserve terminal dimensions\"\n    Description: \"Add header with original terminal size\"\n```\n\n---\n\n### Phase 3: Output Location (MANDATORY)\n\n**Purpose**: Let user choose where to save the output.\n\n```\nQuestion: \"Where should the output be saved?\"\nHeader: \"Output\"\nOptions:\n  - Label: \"Same directory as source (Recommended)\"\n    Description: \"Save {filename}.txt next to {filename}.cast\"\n  - Label: \"Workspace tmp/\"\n    Description: \"Save to ${PWD}/tmp/\"\n  - Label: \"Custom path\"\n    Description: \"Specify a custom output location\"\n```\n\n---\n\n### Phase 4: Execute Conversion\n\n**Purpose**: Run the conversion and report results.\n\n#### Step 4.1: Run asciinema convert\n\n```bash\n/usr/bin/env bash << 'CONVERT_EOF'\nINPUT_FILE=\"${1:?Input file required}\"\nOUTPUT_FILE=\"${2:?Output file required}\"\n\necho \"Converting: $INPUT_FILE\"\necho \"Output:     $OUTPUT_FILE\"\necho \"\"\n\n# Run conversion\nasciinema convert -f txt \"$INPUT_FILE\" \"$OUTPUT_FILE\"\n\nif [[ $? -eq 0 && -f \"$OUTPUT_FILE\" ]]; then\n  echo \"Conversion successful\"\nelse\n  echo \"ERROR: Conversion failed\"\n  exit 1\nfi\nCONVERT_EOF\n```\n\n#### Step 4.2: Report Compression\n\n```bash\n/usr/bin/env bash << 'REPORT_EOF'\nINPUT_FILE=\"${1:?}\"\nOUTPUT_FILE=\"${2:?}\"\n\n# Get file sizes (macOS compatible)\nINPUT_SIZE=$(stat -f%z \"$INPUT_FILE\" 2>/dev/null || stat -c%s \"$INPUT_FILE\" 2>/dev/null)\nOUTPUT_SIZE=$(stat -f%z \"$OUTPUT_FILE\" 2>/dev/null || stat -c%s \"$OUTPUT_FILE\" 2>/dev/null)\n\n# Calculate ratio\nif [[ $OUTPUT_SIZE -gt 0 ]]; then\n  RATIO=$((INPUT_SIZE / OUTPUT_SIZE))\nelse\n  RATIO=0\nfi\n\n# Human-readable sizes\nINPUT_HR=$(numfmt --to=iec \"$INPUT_SIZE\" 2>/dev/null || echo \"$INPUT_SIZE bytes\")\nOUTPUT_HR=$(numfmt --to=iec \"$OUTPUT_SIZE\" 2>/dev/null || echo \"$OUTPUT_SIZE bytes\")\n\necho \"\"\necho \"=== Conversion Complete ===\"\necho \"Input:       $INPUT_HR\"\necho \"Output:      $OUTPUT_HR\"\necho \"Compression: ${RATIO}:1\"\necho \"Output path: $OUTPUT_FILE\"\nREPORT_EOF\n```\n\n---\n\n### Phase 5: Create Timestamp Index (if selected)\n\n**Purpose**: Generate indexed version for navigation.\n\n```bash\n/usr/bin/env bash << 'INDEX_EOF'\nINPUT_CAST=\"${1:?}\"\nOUTPUT_INDEX=\"${2:?}\"\n\necho \"Creating timestamp index...\"\n\n# Process .cast file to indexed format\n(\n  echo \"# Recording Index\"\n  echo \"# Format: [HH:MM:SS] content\"\n  echo \"#\"\n\n  cumtime=0\n  tail -n +2 \"$INPUT_CAST\" | while IFS= read -r line; do\n    # Extract timestamp and content\n    ts=$(echo \"$line\" | jq -r '.[0]' 2>/dev/null)\n    type=$(echo \"$line\" | jq -r '.[1]' 2>/dev/null)\n    content=$(echo \"$line\" | jq -r '.[2]' 2>/dev/null)\n\n    if [[ \"$type\" == \"o\" && -n \"$content\" ]]; then\n      # Format timestamp as HH:MM:SS\n      hours=$((${ts%.*} / 3600))\n      mins=$(((${ts%.*} % 3600) / 60))\n      secs=$((${ts%.*} % 60))\n      timestamp=$(printf \"%02d:%02d:%02d\" \"$hours\" \"$mins\" \"$secs\")\n\n      # Clean and output (strip ANSI, limit length)\n      clean=$(echo \"$content\" | sed 's/\\x1b\\[[0-9;]*[a-zA-Z]//g' | tr -d '\\r' | head -c 200)\n      [[ -n \"$clean\" ]] && echo \"[$timestamp] $clean\"\n    fi\n  done\n) > \"$OUTPUT_INDEX\"\n\necho \"Index created: $OUTPUT_INDEX\"\nwc -l \"$OUTPUT_INDEX\"\nINDEX_EOF\n```\n\n---\n\n### Phase 6: Next Steps (MANDATORY)\n\n**Purpose**: Guide user to next action.\n\n```\nQuestion: \"Conversion complete. What's next?\"\nHeader: \"Next\"\nOptions:\n  - Label: \"Analyze with /asciinema-tools:analyze\"\n    Description: \"Run keyword extraction on the converted file\"\n  - Label: \"Open in editor\"\n    Description: \"View the converted text file\"\n  - Label: \"Done\"\n    Description: \"Exit - no further action needed\"\n```\n\n---\n\n## TodoWrite Task Template\n\n```\n1. [Preflight] Check asciinema CLI and convert command\n2. [Preflight] Offer installation if missing\n3. [Discovery] Find .cast files with metadata\n4. [Selection] AskUserQuestion: file to convert\n5. [Options] AskUserQuestion: conversion options (multi-select)\n6. [Location] AskUserQuestion: output location\n7. [Convert] Run asciinema convert -f txt\n8. [Report] Display compression ratio and output path\n9. [Index] Create timestamp index if requested\n10. [Next] AskUserQuestion: next steps\n```\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Preflight check detects asciinema version correctly\n2. [ ] Discovery uses heredoc wrapper for bash compatibility\n3. [ ] Compression calculation handles macOS stat syntax\n4. [ ] All AskUserQuestion phases are present\n5. [ ] TodoWrite template matches actual workflow\n\n---\n\n## CLI Quick Reference\n\n```bash\n# Basic conversion\nasciinema convert -f txt recording.cast recording.txt\n\n# Check asciinema version\nasciinema --version\n\n# Verify convert command exists\nasciinema convert --help\n```\n\n---\n\n## Reference Documentation\n\n- [asciinema convert command](https://docs.asciinema.org/manual/cli/usage/)\n- [asciinema-cast-format skill](../asciinema-cast-format/SKILL.md)"
              },
              {
                "name": "asciinema-player",
                "description": "Play .cast terminal recordings in iTerm2 with full CLI controls. TRIGGERS - asciinema, .cast file, terminal recording, play cast, recording playback, play recording. Uses AskUserQuestion for speed/options selection, spawns clean iTerm2 window via AppleScript.",
                "path": "plugins/asciinema-tools/skills/asciinema-player/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-player",
                  "description": "Play .cast terminal recordings in iTerm2 with full CLI controls. TRIGGERS - asciinema, .cast file, terminal recording, play cast, recording playback, play recording. Uses AskUserQuestion for speed/options selection, spawns clean iTerm2 window via AppleScript.",
                  "allowed-tools": "Read, Bash, Glob, AskUserQuestion"
                },
                "content": "# asciinema-player\n\nPlay terminal session recordings (.cast files) in a dedicated iTerm2 window with full playback controls. Opens a **clean window** (bypasses default arrangements) for distraction-free viewing.\n\n> **Platform**: macOS only (requires iTerm2)\n\n---\n\n## Why iTerm2 Instead of Browser?\n\n| Aspect               | Browser Player          | iTerm2 CLI        |\n| -------------------- | ----------------------- | ----------------- |\n| Large files (>100MB) | Crashes (memory limit)  | Streams from disk |\n| Memory usage         | 2-4GB for 700MB file    | Minimal           |\n| Startup time         | Slow (download + parse) | Instant           |\n| Native feel          | Web-based               | True terminal     |\n\n**Decision**: iTerm2 CLI is the only reliable method for large recordings.\n\n---\n\n## Requirements\n\n| Component         | Required | Installation                 |\n| ----------------- | -------- | ---------------------------- |\n| **iTerm2**        | Yes      | `brew install --cask iterm2` |\n| **asciinema CLI** | Yes      | `brew install asciinema`     |\n\n> **Note**: This skill is macOS-only. Linux users should run `asciinema play` directly in their terminal.\n\n---\n\n## Workflow Phases (ALL MANDATORY)\n\n**IMPORTANT**: All phases are MANDATORY. Do NOT skip any phase. AskUserQuestion MUST be used at each decision point.\n\n### Phase 0: Preflight Checks\n\n**Purpose**: Verify iTerm2 and asciinema are installed.\n\n#### Step 0.1: Check Dependencies\n\n```bash\n# Check iTerm2 is installed\nls -d /Applications/iTerm.app 2>/dev/null && echo \"iTerm2: OK\" || echo \"iTerm2: MISSING\"\n\n# Check asciinema CLI\nwhich asciinema && asciinema --version\n```\n\n#### Step 0.2: Report Status and Ask for Installation\n\n**MANDATORY AskUserQuestion** if any dependency is missing:\n\n```\nQuestion: \"Required dependencies are missing. Install them?\"\nHeader: \"Setup\"\nOptions:\n  - Label: \"Install all (Recommended)\"\n    Description: \"Will install: {list of missing: iTerm2, asciinema}\"\n  - Label: \"Cancel\"\n    Description: \"Abort - cannot proceed without dependencies\"\n```\n\n#### Step 0.3: Install Missing Dependencies (if confirmed)\n\n```bash\n# Install iTerm2\nbrew install --cask iterm2\n\n# Install asciinema CLI\nbrew install asciinema\n```\n\n---\n\n### Phase 1: File Selection (MANDATORY)\n\n**Purpose**: Discover and select the recording to play.\n\n#### Step 1.1: Discover Recordings\n\n```bash\n# Search for .cast files in common locations\nfd -e cast . --max-depth 5 2>/dev/null | head -20\n\n# Also check common locations\nls -lh ~/scripts/tmp/*.cast 2>/dev/null\nls -lh ~/.local/share/asciinema/*.cast 2>/dev/null\nls -lh ./tmp/*.cast 2>/dev/null\n```\n\n#### Step 1.2: Get File Info\n\n```bash\n# Get file size and line count for selected file\nls -lh {file_path}\nwc -l {file_path}\n```\n\n#### Step 1.3: Present File Selection (MANDATORY AskUserQuestion)\n\n**If user provided path directly**, confirm:\n\n```\nQuestion: \"Play this recording?\"\nHeader: \"Confirm\"\nOptions:\n  - Label: \"Yes, play {filename}\"\n    Description: \"{size}, {line_count} events\"\n  - Label: \"Choose different file\"\n    Description: \"Browse for other recordings\"\n```\n\n**If no path provided**, discover and present options:\n\n```\nQuestion: \"Which recording would you like to play?\"\nHeader: \"Recording\"\nOptions:\n  - Label: \"{filename} ({size})\"\n    Description: \"{line_count} events\"\n  - ... (up to 4 most recent)\n```\n\n---\n\n### Phase 2: Playback Settings (MANDATORY)\n\n**Purpose**: Configure playback options before launching iTerm2.\n\n#### Step 2.1: Ask Playback Speed (MANDATORY AskUserQuestion)\n\n```\nQuestion: \"Select playback speed:\"\nHeader: \"Speed\"\nOptions:\n  - Label: \"2x (fast)\"\n    Description: \"Good for review, see everything\"\n  - Label: \"6x (very fast)\"\n    Description: \"Quick scan of long sessions\"\n  - Label: \"16x (ultra fast)\"\n    Description: \"Rapid skim for 700MB+ files\"\n  - Label: \"Custom\"\n    Description: \"Enter your own speed multiplier\"\n```\n\n**If \"Custom\" selected**, ask for speed value (use Other option for numeric input).\n\n#### Step 2.2: Ask Additional Options (MANDATORY AskUserQuestion)\n\n```\nQuestion: \"Select additional playback options:\"\nHeader: \"Options\"\nmultiSelect: true\nOptions:\n  - Label: \"Limit idle time (2s)\"\n    Description: \"Cap pauses to 2 seconds max (recommended)\"\n  - Label: \"Loop playback\"\n    Description: \"Restart automatically when finished\"\n  - Label: \"Resize terminal\"\n    Description: \"Match terminal size to recording dimensions\"\n  - Label: \"Pause on markers\"\n    Description: \"Auto-pause at marked points (for demos)\"\n```\n\n---\n\n### Phase 3: Launch in iTerm2\n\n**Purpose**: Open clean iTerm2 window and start playback.\n\n#### Step 3.1: Build Command\n\nConstruct the `asciinema play` command based on user selections:\n\n```bash\n# Example with all options\nasciinema play -s 6 -i 2 -l -r /path/to/recording.cast\n```\n\n**Option flags:**\n\n- `-s {speed}` - Playback speed\n- `-i 2` - Idle time limit (if selected)\n- `-l` - Loop (if selected)\n- `-r` - Resize terminal (if selected)\n- `-m` - Pause on markers (if selected)\n\n#### Step 3.2: Launch iTerm2 Window\n\nUse AppleScript to open a **clean window** (bypasses default arrangements):\n\n```bash\nosascript -e 'tell application \"iTerm2\"\n    create window with default profile\n    tell current window\n        tell current session\n            write text \"asciinema play -s {speed} {options} {file_path}\"\n        end tell\n    end tell\nend tell'\n```\n\n#### Step 3.3: Display Controls Reference\n\n```markdown\n## Playback Started\n\n**Recording:** `{filename}`\n**Speed:** {speed}x\n**Options:** {options_summary}\n\n### Keyboard Controls\n\n| Key      | Action                            |\n| -------- | --------------------------------- |\n| `Space`  | Pause / Resume                    |\n| `Ctrl+C` | Stop playback                     |\n| `.`      | Step forward (when paused)        |\n| `]`      | Skip to next marker (when paused) |\n\n### Tips\n\n- Press `Space` to pause anytime\n- Use `.` to step through frame by frame\n- `Ctrl+C` to exit when done\n```\n\n---\n\n## TodoWrite Task Template (MANDATORY)\n\n**Load this template into TodoWrite before starting**:\n\n```\n1. [Preflight] Check iTerm2 installed\n2. [Preflight] Check asciinema CLI installed\n3. [Preflight] AskUserQuestion: install missing deps (if needed)\n4. [Preflight] Install dependencies (if confirmed)\n5. [Selection] Get file info (size, events)\n6. [Selection] AskUserQuestion: confirm file selection\n7. [Settings] AskUserQuestion: playback speed\n8. [Settings] AskUserQuestion: additional options (multi-select)\n9. [Launch] Build asciinema play command\n10. [Launch] Execute AppleScript to open iTerm2\n11. [Launch] Display controls reference\n```\n\n---\n\n## CLI Options Reference\n\n| Option     | Flag | Values              | Description                      |\n| ---------- | ---- | ------------------- | -------------------------------- |\n| Speed      | `-s` | 0.5, 1, 2, 6, 16... | Playback speed multiplier        |\n| Idle limit | `-i` | seconds (e.g., 2)   | Cap idle/pause time              |\n| Loop       | `-l` | (flag)              | Continuous loop                  |\n| Resize     | `-r` | (flag)              | Match terminal to recording size |\n| Markers    | `-m` | (flag)              | Auto-pause at markers            |\n| Quiet      | `-q` | (flag)              | Suppress info messages           |\n\n---\n\n## AppleScript Reference\n\n### Open Clean iTerm2 Window (No Default Arrangement)\n\n```applescript\ntell application \"iTerm2\"\n    create window with default profile\n    tell current window\n        tell current session\n            write text \"your command here\"\n        end tell\n    end tell\nend tell\n```\n\n**Why this works**: `create window with default profile` creates a fresh window, bypassing any saved window arrangements.\n\n### One-liner for Bash\n\n```bash\nosascript -e 'tell application \"iTerm2\"\n    create window with default profile\n    tell current window\n        tell current session\n            write text \"asciinema play -s 6 -i 2 /path/to/file.cast\"\n        end tell\n    end tell\nend tell'\n```\n\n---\n\n## Troubleshooting\n\n### \"Device not configured\" error\n\n**Cause**: Running `asciinema play` from a non-TTY context (e.g., Claude Code's Bash tool)\n\n**Fix**: Use AppleScript to open a real iTerm2 window (this skill does this automatically)\n\n### Recording plays too fast/slow\n\n**Fix**: Use the speed AskUserQuestion to select appropriate speed:\n\n- 2x for careful review\n- 6x for quick scan\n- 16x for ultra-fast skim of very long recordings\n\n### iTerm2 not opening\n\n**Cause**: iTerm2 not installed or AppleScript permissions not granted\n\n**Fix**:\n\n1. Install iTerm2: `brew install --cask iterm2`\n2. Grant permissions: System Settings  Privacy & Security  Automation  Allow Terminal/Claude to control iTerm2\n\n### Large file (>500MB) considerations\n\nThe CLI player streams from disk, so file size doesn't cause memory issues. However:\n\n- Very long recordings may benefit from higher speeds (6x, 16x)\n- Use `-i 2` to skip idle time\n- Consider splitting very long recordings\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Preflight checks verify iTerm2 and asciinema\n2. [ ] AskUserQuestion phases use proper multiSelect where applicable\n3. [ ] AppleScript uses heredoc wrapper for bash compatibility\n4. [ ] Speed options match CLI capability (-s flag)\n5. [ ] TodoWrite template matches actual workflow phases\n\n---\n\n## Reference Documentation\n\n- [asciinema play Usage](https://docs.asciinema.org/manual/cli/usage/)\n- [asciinema CLI Options](https://man.archlinux.org/man/extra/asciinema/asciinema-play.1.en)\n- [iTerm2 AppleScript Documentation](https://iterm2.com/documentation-scripting.html)\n- [asciinema Markers](https://docs.asciinema.org/manual/cli/markers/)"
              },
              {
                "name": "asciinema-recorder",
                "description": "Record Claude Code sessions with asciinema. TRIGGERS - record session, asciinema record, capture terminal, record claude, demo recording, record ASCII, ASCII terminal, terminal screen capture, shell screen capture, ASCII screen capture, screen recording. Generates ready-to-copy commands with dynamic workspace-based filenames.",
                "path": "plugins/asciinema-tools/skills/asciinema-recorder/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-recorder",
                  "description": "Record Claude Code sessions with asciinema. TRIGGERS - record session, asciinema record, capture terminal, record claude, demo recording, record ASCII, ASCII terminal, terminal screen capture, shell screen capture, ASCII screen capture, screen recording. Generates ready-to-copy commands with dynamic workspace-based filenames.",
                  "allowed-tools": "Read, Bash, Glob, AskUserQuestion"
                },
                "content": "# asciinema-recorder\n\nGenerate ready-to-copy commands for recording Claude Code sessions with asciinema. Dynamically creates filenames based on workspace and datetime.\n\n> **Platform**: macOS, Linux (requires asciinema CLI)\n\n---\n\n## Why This Skill?\n\nThis skill generates ready-to-copy recording commands with:\n\n- Dynamic workspace-based filename\n- Datetime stamp for uniqueness\n- Saves to project's tmp/ folder (gitignored)\n\n---\n\n## Requirements\n\n| Component         | Required | Installation             |\n| ----------------- | -------- | ------------------------ |\n| **asciinema CLI** | Yes      | `brew install asciinema` |\n\n---\n\n## Workflow Phases\n\n### Phase 0: Preflight Check\n\n**Purpose**: Verify asciinema is installed.\n\n```bash\n# Check asciinema CLI\nwhich asciinema && asciinema --version\n```\n\nIf asciinema is NOT installed, use AskUserQuestion:\n\n- question: \"asciinema not found. How would you like to proceed?\"\n  header: \"Setup\"\n  multiSelect: false\n  options:\n  - label: \"Install asciinema (Recommended)\"\n    description: \"Run: brew install asciinema (macOS) or apt install asciinema (Linux)\"\n  - label: \"Show manual instructions\"\n    description: \"Display installation commands for all platforms\"\n  - label: \"Cancel\"\n    description: \"Exit without recording\"\n\nBased on selection:\n\n- **\"Install asciinema\"**  Run appropriate install command based on OS:\n\n  ```bash\n  # macOS\n  brew install asciinema\n\n  # Linux (apt)\n  sudo apt install asciinema\n\n  # Linux (pip)\n  pip install asciinema\n  ```\n\n  Then proceed to Phase 1.0.\n\n- **\"Show manual instructions\"**  Display the commands above, then exit skill.\n\n- **\"Cancel\"**  Exit skill without action.\n\n---\n\n### Phase 1.0: Recording Location\n\n**Purpose**: Let user choose where to save the recording.\n\nUse AskUserQuestion:\n\n- question: \"Where should the recording be saved?\"\n  header: \"Location\"\n  multiSelect: false\n  options:\n  - label: \"$PWD/tmp/ (Recommended)\"\n    description: \"Project tmp directory (gitignored)\"\n  - label: \"~/asciinema/\"\n    description: \"Global recordings directory\"\n  - label: \"Custom path\"\n    description: \"Specify your own directory\"\n\nBased on selection:\n\n- **\"$PWD/tmp/\"**  Set `OUTPUT_DIR=\"$PWD/tmp\"`\n- **\"~/asciinema/\"**  Set `OUTPUT_DIR=\"$HOME/asciinema\"` and create if missing: `mkdir -p ~/asciinema`\n- **\"Custom path\"**  Use user's \"Other\" input as `OUTPUT_DIR`\n\n---\n\n### Phase 1.1: Recording Options\n\n**Purpose**: Let user configure recording behavior.\n\nUse AskUserQuestion:\n\n- question: \"Which recording options would you like?\"\n  header: \"Options\"\n  multiSelect: true\n  options:\n  - label: \"Add title/description\"\n    description: \"Include session title in recording metadata (-t flag)\"\n  - label: \"Disable idle time limit\"\n    description: \"Keep full pauses instead of 2s max (--idle-time-limit 0)\"\n  - label: \"Quiet mode\"\n    description: \"Suppress asciinema status messages (-q flag)\"\n\nBased on selections, build command flags:\n\n- **\"Add title\"**  Continue to title selection question, add `-t \"title\"` flag\n- **\"Disable idle time limit\"**  Add `--idle-time-limit 0` flag\n- **\"Quiet mode\"**  Add `-q` flag\n\n**If \"Add title\" was selected**, follow up with:\n\n- question: \"Enter a title for this recording:\"\n  header: \"Title\"\n  multiSelect: false\n  options:\n  - label: \"Use workspace name\"\n    description: \"Title: ${WORKSPACE}\"\n  - label: \"Use workspace + datetime\"\n    description: \"Title: ${WORKSPACE} ${DATETIME}\"\n  - label: \"Custom title\"\n    description: \"Enter your own title\"\n\nBased on title selection:\n\n- **\"Use workspace name\"**  Set `TITLE=\"${WORKSPACE}\"`\n- **\"Use workspace + datetime\"**  Set `TITLE=\"${WORKSPACE} ${DATETIME}\"`\n- **\"Custom title\"**  Use user's \"Other\" input as `TITLE`\n\n---\n\n### Phase 1.2: Detect Context & Generate Command\n\n**Purpose**: Generate a copy-paste ready recording command.\n\n#### Step 1.2.1: Detect Workspace\n\nExtract workspace name from `$PWD`:\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\nWORKSPACE=$(basename \"$PWD\")\necho \"Workspace: $WORKSPACE\"\nSKILL_SCRIPT_EOF\n```\n\n#### Step 1.2.2: Generate Datetime\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF_2'\nDATETIME=$(date +%Y-%m-%d_%H-%M)\necho \"Datetime: $DATETIME\"\nSKILL_SCRIPT_EOF_2\n```\n\n#### Step 1.2.3: Ensure Output Directory Exists\n\nCreate the output directory selected in Phase 1.0:\n\n```bash\nmkdir -p \"${OUTPUT_DIR}\"\n```\n\n#### Step 1.2.4: Construct Command\n\nBuild the recording command using:\n\n- `OUTPUT_DIR` from Phase 1.0 (location selection)\n- Flags from Phase 1.1 (options selection)\n- `TITLE` if \"Add title\" was selected\n\n```bash\n# Base command\nCMD=\"asciinema rec\"\n\n# Add flags from Phase 1.1 options\n# (Claude builds this based on user selections)\n\n# Final command format:\nasciinema rec ${FLAGS} \"${OUTPUT_DIR}/${WORKSPACE}_${DATETIME}.cast\"\n```\n\n**Example outputs:**\n\n```bash\n# Default (no options selected):\nasciinema rec /home/user/projects/my-app/tmp/my-app_2025-12-21_14-30.cast\n\n# With title + quiet mode:\nasciinema rec -t \"my-app Demo\" -q /home/user/projects/my-app/tmp/my-app_2025-12-21_14-30.cast\n\n# With all options:\nasciinema rec -t \"my-app 2025-12-21 14:30\" -q --idle-time-limit 0 ~/asciinema/my-app_2025-12-21_14-30.cast\n```\n\n---\n\n### Phase 2: User Guidance\n\n**Purpose**: Explain the recording workflow step-by-step.\n\nPresent these instructions:\n\n```markdown\n## Recording Instructions\n\n1. **Exit Claude Code** - Type `exit` or press `Ctrl+D`\n2. **Copy the command** shown above\n3. **Paste and run** in your terminal (starts a recorded shell)\n4. **Run `claude`** to start Claude Code inside the recording\n5. Work normally - everything is captured\n6. **Exit Claude Code** - Type `exit` or press `Ctrl+D`\n7. **Exit the recording shell** - Type `exit` or press `Ctrl+D` again\n\nYour recording will be saved to:\n`$PWD/tmp/{workspace}_{datetime}.cast`\n```\n\n---\n\n### Phase 3: Additional Info\n\n**Purpose**: Provide helpful tips for after recording.\n\n```markdown\n## Tips\n\n- **Environment variable**: `ASCIINEMA_REC=1` is set during recording\n- **Playback**: Use `asciinema-player` skill or `asciinema play file.cast`\n- **Upload (optional)**: `asciinema upload file.cast` (requires account)\n- **Markers**: Add `asciinema marker` during recording for navigation points\n```\n\n---\n\n## TodoWrite Task Templates\n\n### Template: Record Claude Code Session\n\n```\n1. [Preflight] Check asciinema CLI installed\n2. [Preflight] Offer installation if missing\n3. [Context] Detect current workspace from $PWD\n4. [Context] Generate datetime slug\n5. [Context] Ensure tmp/ directory exists\n6. [Command] Construct full recording command\n7. [Guidance] Display step-by-step instructions\n8. [Guidance] Show additional tips (playback, upload)\n9. Verify against Skill Quality Checklist\n```\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Command generation still uses `$PWD` (no hardcoded paths)\n2. [ ] Guidance steps remain clear and platform-agnostic\n3. [ ] TodoWrite template matches actual workflow\n4. [ ] README.md entry remains accurate\n5. [ ] Validate with quick_validate.py\n\n---\n\n## CLI Options Reference\n\n| Option | Flag | Description                         |\n| ------ | ---- | ----------------------------------- |\n| Title  | `-t` | Recording title (for asciinema.org) |\n| Quiet  | `-q` | Suppress status messages            |\n| Append | `-a` | Append to existing recording        |\n\n---\n\n## Troubleshooting\n\n### \"Cannot record from within Claude Code\"\n\n**Cause**: asciinema must wrap the program, not be started from inside.\n\n**Fix**: Exit Claude Code first, then run the generated command.\n\n### \"Recording file too large\"\n\n**Cause**: Long sessions produce large files.\n\n**Fix**:\n\n- Use `asciinema upload` to store online instead of locally\n- Split long sessions into smaller recordings\n\n### \"Playback shows garbled output\"\n\n**Cause**: Terminal size mismatch.\n\n**Fix**: Use `-r` flag during playback to resize terminal.\n\n---\n\n## Reference Documentation\n\n- [asciinema rec Usage](https://docs.asciinema.org/manual/cli/usage/)\n- [asciinema CLI Options](https://man.archlinux.org/man/extra/asciinema/asciinema-rec.1.en)\n- [asciinema Markers](https://docs.asciinema.org/manual/cli/markers/)"
              },
              {
                "name": "asciinema-streaming-backup",
                "description": "Real-time asciinema recording backup to GitHub orphan branch with idle-based chunking and brotli archival. TRIGGERS - streaming backup, recording backup, asciinema backup, continuous recording, session backup, orphan branch recording, zstd streaming, chunked recording, real-time backup, github recording storage.",
                "path": "plugins/asciinema-tools/skills/asciinema-streaming-backup/SKILL.md",
                "frontmatter": {
                  "name": "asciinema-streaming-backup",
                  "description": "Real-time asciinema recording backup to GitHub orphan branch with idle-based chunking and brotli archival. TRIGGERS - streaming backup, recording backup, asciinema backup, continuous recording, session backup, orphan branch recording, zstd streaming, chunked recording, real-time backup, github recording storage.",
                  "allowed-tools": "Read, Bash, Glob, Write, Edit, AskUserQuestion"
                },
                "content": "# asciinema-streaming-backup\n\nComplete system for streaming asciinema recordings to GitHub with automatic brotli archival. Uses idle-detection for intelligent chunking, zstd for concatenatable streaming compression, and GitHub Actions for final brotli recompression.\n\n> **Platform**: macOS, Linux\n> **Isolation**: Uses Git orphan branch (separate history, cannot pollute main)\n\n---\n\n## Architecture Overview\n\n```\n     zstd chunks           Actions      \n  asciinema rec       GitHub Orphan      brotli archive \n  + idle-chunker    (concatenatable)     gh-recordings                      (300x compress)\n                                        \n                                                 \n          Idle 30s triggers chunk                Separate history\n                                                  Cannot PR to main\n    ~/asciinema_recordings/                                 \n     repo-name/                          .github/workflows/\n         chunks/*.zst                     recompress.yml\n```\n\n---\n\n## Requirements\n\n| Component         | Required | Installation             | Version       |\n| ----------------- | -------- | ------------------------ | ------------- |\n| **asciinema CLI** | Yes      | `brew install asciinema` | 3.0+ (Rust)   |\n| **zstd**          | Yes      | `brew install zstd`      | Any           |\n| **brotli**        | Yes      | `brew install brotli`    | Any           |\n| **git**           | Yes      | Pre-installed            | 2.20+         |\n| **gh CLI**        | Yes      | `brew install gh`        | Any           |\n| **fswatch**       | Optional | `brew install fswatch`   | For real-time |\n\n---\n\n## Workflow Phases\n\n### Phase 0: Preflight Validation\n\n**Purpose**: Verify all tools installed, offer self-correction if missing.\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# preflight-check.sh - Validates all requirements\n\nMISSING=()\n\n# Check each tool\nfor tool in asciinema zstd brotli git gh; do\n  if ! command -v \"$tool\" &>/dev/null; then\n    MISSING+=(\"$tool\")\n  fi\ndone\n\nif [[ ${#MISSING[@]} -gt 0 ]]; then\n  echo \"Missing tools: ${MISSING[*]}\"\n  echo \"\"\n  echo \"Install with:\"\n  echo \"  brew install ${MISSING[*]}\"\n  exit 1\nfi\n\n# Check asciinema version (need 3.0+ for Rust version)\nASCIINEMA_VERSION=$(asciinema --version 2>&1 | grep -oE '[0-9]+\\.[0-9]+' | head -1)\nif [[ \"${ASCIINEMA_VERSION%%.*}\" -lt 3 ]]; then\n  echo \"Warning: asciinema $ASCIINEMA_VERSION detected. Version 3.0+ recommended.\"\n  echo \"Upgrade: brew upgrade asciinema\"\nfi\n\necho \"All requirements satisfied\"\nPREFLIGHT_EOF\n```\n\n**AskUserQuestion** (if tools missing):\n\n```yaml\nAskUserQuestion:\n  question: \"Required tools are missing. How would you like to proceed?\"\n  header: \"Preflight Check\"\n  options:\n    - label: \"Install all missing tools (Recommended)\"\n      description: \"Run: brew install ${MISSING[*]}\"\n    - label: \"Show manual installation commands\"\n      description: \"Display commands without executing\"\n    - label: \"Continue anyway (may fail later)\"\n      description: \"Skip installation and proceed\"\n```\n\n**Self-Correction**: If tools are missing, generate installation command and offer to run it.\n\n---\n\n### Phase 1: GitHub Account Detection\n\n**Purpose**: Detect available GitHub accounts and let user choose which to use for recording storage.\n\n#### Detection Sources\n\nProbe these 5 sources to detect GitHub accounts:\n\n| Source     | Command                                | What it finds                                     |\n| ---------- | -------------------------------------- | ------------------------------------------------- |\n| SSH config | `grep -A5 \"Host github\" ~/.ssh/config` | Match directives with IdentityFile                |\n| SSH keys   | `ls ~/.ssh/id_ed25519_*`               | Account-named keys (e.g., `id_ed25519_terrylica`) |\n| gh CLI     | `gh auth status`                       | Authenticated accounts                            |\n| mise env   | `grep GH_ACCOUNT .mise.toml`           | GH_ACCOUNT variable                               |\n| git config | `git config user.name`                 | Global git username                               |\n\n#### Detection Script\n\n```bash\n/usr/bin/env bash << 'DETECT_ACCOUNTS_EOF'\n# detect-github-accounts.sh - Probe all sources for GitHub accounts\n# Uses portable parallel arrays (works in bash 3.2+ and when wrapped for zsh)\n\nACCOUNT_NAMES=()\nACCOUNT_SOURCES=()\n\nlog() { echo \"[detect] $*\"; }\n\n# Helper: add account with source (updates existing or appends new)\nadd_account() {\n  local account=\"$1\" source=\"$2\"\n  local idx\n  for idx in \"${!ACCOUNT_NAMES[@]}\"; do\n    if [[ \"${ACCOUNT_NAMES[$idx]}\" == \"$account\" ]]; then\n      ACCOUNT_SOURCES[$idx]+=\"$source \"\n      return\n    fi\n  done\n  ACCOUNT_NAMES+=(\"$account\")\n  ACCOUNT_SOURCES+=(\"$source \")\n}\n\n# 1. SSH config Match directives\nif [[ -f ~/.ssh/config ]]; then\n  while IFS= read -r line; do\n    if [[ \"$line\" =~ IdentityFile.*id_ed25519_([a-zA-Z0-9_-]+) ]]; then\n      add_account \"${BASH_REMATCH[1]}\" \"ssh-config\"\n    fi\n  done < ~/.ssh/config\nfi\n\n# 2. SSH key filenames\nfor keyfile in ~/.ssh/id_ed25519_*; do\n  if [[ -f \"$keyfile\" && \"$keyfile\" != *.pub ]]; then\n    account=$(basename \"$keyfile\" | sed 's/id_ed25519_//')\n    add_account \"$account\" \"ssh-key\"\n  fi\ndone\n\n# 3. gh CLI authenticated accounts\nif command -v gh &>/dev/null; then\n  while IFS= read -r account; do\n    [[ -n \"$account\" ]] && add_account \"$account\" \"gh-cli\"\n  done < <(gh auth status 2>&1 | grep -oE 'Logged in to github.com account [a-zA-Z0-9_-]+' | awk '{print $NF}')\nfi\n\n# 4. mise env GH_ACCOUNT\nif [[ -f .mise.toml ]]; then\n  account=$(grep -E 'GH_ACCOUNT\\s*=' .mise.toml 2>/dev/null | sed 's/.*=\\s*\"\\([^\"]*\\)\".*/\\1/')\n  [[ -n \"$account\" ]] && add_account \"$account\" \"mise-env\"\nfi\n\n# 5. git config user.name\ngit_user=$(git config user.name 2>/dev/null)\n[[ -n \"$git_user\" ]] && add_account \"$git_user\" \"git-config\"\n\n# Score and display\nlog \"=== Detected GitHub Accounts ===\"\nRECOMMENDED=\"\"\nMAX_SOURCES=0\nfor idx in \"${!ACCOUNT_NAMES[@]}\"; do\n  account=\"${ACCOUNT_NAMES[$idx]}\"\n  sources=\"${ACCOUNT_SOURCES[$idx]}\"\n  count=$(echo \"$sources\" | wc -w | tr -d ' ')\n  log \"$account: $count sources ($sources)\"\n  if (( count > MAX_SOURCES )); then\n    MAX_SOURCES=$count\n    RECOMMENDED=\"$account\"\n    RECOMMENDED_SOURCES=\"$sources\"\n  fi\ndone\n\necho \"\"\necho \"RECOMMENDED=$RECOMMENDED\"\necho \"SOURCES=$RECOMMENDED_SOURCES\"\nDETECT_ACCOUNTS_EOF\n```\n\n#### AskUserQuestion\n\n```yaml\nAskUserQuestion:\n  question: \"Which GitHub account should be used for recording storage?\"\n  header: \"GitHub Account Selection\"\n  options:\n    - label: \"${RECOMMENDED} (Recommended)\"\n      description: \"Detected via: ${SOURCES}\"\n    # Additional detected accounts appear here dynamically\n    - label: \"Enter manually\"\n      description: \"Type a GitHub username not listed above\"\n```\n\n**Post-Selection**: If user selects an account, ensure gh CLI is using that account:\n\n```bash\n/usr/bin/env bash << 'POST_SELECT_EOF'\n# Ensure gh CLI is authenticated as selected account\nSELECTED_ACCOUNT=\"${1:?Usage: provide selected account}\"\n\nif ! gh auth status 2>&1 | grep -q \"Logged in to github.com account $SELECTED_ACCOUNT\"; then\n  echo \"Switching gh CLI to account: $SELECTED_ACCOUNT\"\n  gh auth switch --user \"$SELECTED_ACCOUNT\" 2>/dev/null || \\\n    echo \"Warning: Could not switch accounts. Manual auth may be needed.\"\nfi\nPOST_SELECT_EOF\n```\n\n---\n\n### Phase 1.5: Current Repository Detection\n\n**Purpose**: Detect current git repository context to provide intelligent defaults for Phase 2 questions.\n\n#### Detection Script\n\n```bash\n/usr/bin/env bash << 'DETECT_REPO_EOF'\n# Detect current repository context for intelligent defaults\n\nCURRENT_REPO_URL=\"\"\nCURRENT_REPO_OWNER=\"\"\nCURRENT_REPO_NAME=\"\"\nDETECTED_FROM=\"\"\n\n# Check if we're in a git repository\nif git rev-parse --git-dir &>/dev/null; then\n  # Try origin remote first\n  if git remote get-url origin &>/dev/null; then\n    CURRENT_REPO_URL=$(git remote get-url origin)\n    DETECTED_FROM=\"origin remote\"\n  # Fallback to first available remote\n  elif [[ -n \"$(git remote)\" ]]; then\n    REMOTE=$(git remote | head -1)\n    CURRENT_REPO_URL=$(git remote get-url \"$REMOTE\")\n    DETECTED_FROM=\"$REMOTE remote\"\n  fi\n\n  # Parse owner and name from URL (SSH or HTTPS)\n  if [[ -n \"$CURRENT_REPO_URL\" ]]; then\n    if [[ \"$CURRENT_REPO_URL\" =~ github\\.com[:/]([^/]+)/([^/.]+) ]]; then\n      CURRENT_REPO_OWNER=\"${BASH_REMATCH[1]}\"\n      CURRENT_REPO_NAME=\"${BASH_REMATCH[2]%.git}\"\n    fi\n  fi\nfi\n\n# Output for Claude to parse\necho \"CURRENT_REPO_URL=$CURRENT_REPO_URL\"\necho \"CURRENT_REPO_OWNER=$CURRENT_REPO_OWNER\"\necho \"CURRENT_REPO_NAME=$CURRENT_REPO_NAME\"\necho \"DETECTED_FROM=$DETECTED_FROM\"\nDETECT_REPO_EOF\n```\n\n**Claude Action**: Store detected values (`CURRENT_REPO_OWNER`, `CURRENT_REPO_NAME`, `DETECTED_FROM`) for use in subsequent AskUserQuestion calls. If no repo detected, proceed without defaults.\n\n---\n\n### Phase 2: Core Configuration\n\n**Purpose**: Gather essential configuration from user.\n\n#### 2.1 Repository URL\n\n**If current repo detected** (from Phase 1.5):\n\n```yaml\nAskUserQuestion:\n  question: \"Which repository should store the recordings?\"\n  header: \"Repository\"\n  options:\n    - label: \"${CURRENT_REPO_OWNER}/${CURRENT_REPO_NAME} (Recommended)\"\n      description: \"Current repo detected from ${DETECTED_FROM}\"\n    - label: \"Create dedicated repo: ${GITHUB_ACCOUNT}/asciinema-recordings\"\n      description: \"Separate repository for all recordings\"\n    - label: \"Enter different repository\"\n      description: \"Specify another repository (user/repo format)\"\n```\n\n**If no current repo detected**:\n\n```yaml\nAskUserQuestion:\n  question: \"Enter the GitHub repository URL for storing recordings:\"\n  header: \"Repository URL\"\n  options:\n    - label: \"Create dedicated repo: ${GITHUB_ACCOUNT}/asciinema-recordings\"\n      description: \"Separate repository for all recordings (Recommended)\"\n    - label: \"Enter repository manually\"\n      description: \"SSH (git@github.com:user/repo.git), HTTPS, or shorthand (user/repo)\"\n```\n\n**URL Normalization** (handles multiple formats):\n\n```bash\n/usr/bin/env bash << 'NORMALIZE_URL_EOF'\n# Normalize to SSH format for consistent handling\nnormalize_repo_url() {\n  local url=\"$1\"\n\n  # Shorthand: user/repo -> git@github.com:user/repo.git\n  if [[ \"$url\" =~ ^[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+$ ]]; then\n    echo \"git@github.com:${url}.git\"\n  # HTTPS: https://github.com/user/repo -> git@github.com:user/repo.git\n  elif [[ \"$url\" =~ ^https://github\\.com/([^/]+)/([^/]+)/?$ ]]; then\n    echo \"git@github.com:${BASH_REMATCH[1]}/${BASH_REMATCH[2]%.git}.git\"\n  # Already SSH format\n  else\n    echo \"$url\"\n  fi\n}\n\nURL=\"${1:?Usage: provide URL to normalize}\"\nnormalize_repo_url \"$URL\"\nNORMALIZE_URL_EOF\n```\n\n**Confirmation for free-form input** (if user selected \"Enter different/manually\"):\n\n```yaml\nAskUserQuestion:\n  question: \"You entered '${USER_INPUT}'. Normalized to: ${NORMALIZED_URL}. Is this correct?\"\n  header: \"Confirm Repository\"\n  options:\n    - label: \"Yes, use ${NORMALIZED_URL}\"\n      description: \"Proceed with this repository\"\n    - label: \"No, let me re-enter\"\n      description: \"Go back to repository selection\"\n```\n\n#### 2.2 Recording Directory\n\n```yaml\nAskUserQuestion:\n  question: \"Where should recordings be stored locally?\"\n  header: \"Recording Directory\"\n  options:\n    - label: \"~/asciinema_recordings/${RESOLVED_REPO_NAME} (Recommended)\"\n      description: \"Example: ~/asciinema_recordings/alpha-forge\"\n    - label: \"Custom path\"\n      description: \"Enter a different directory path\"\n```\n\n**Note**: `${RESOLVED_REPO_NAME}` is the actual repo name from Phase 1.5 or Phase 2.1, not a variable placeholder. Display the concrete path to user.\n\n#### 2.3 Branch Name\n\n```yaml\nAskUserQuestion:\n  question: \"What should the orphan branch be named?\"\n  header: \"Branch Name\"\n  options:\n    - label: \"asciinema-recordings (Recommended)\"\n      description: \"Matches ~/asciinema_recordings/ parent directory pattern\"\n    - label: \"gh-recordings\"\n      description: \"GitHub-prefixed alternative (gh = GitHub storage)\"\n    - label: \"recordings\"\n      description: \"Minimal name\"\n    - label: \"Custom\"\n      description: \"Enter a custom branch name\"\n```\n\n**Naming Convention**: The default `asciinema-recordings` matches the parent directory `~/asciinema_recordings/` for consistency.\n\n---\n\n### Phase 3: Advanced Configuration\n\n**Purpose**: Allow customization of compression and behavior parameters.\n\n#### Configuration Parameters\n\n| Parameter      | Default | Options                                     |\n| -------------- | ------- | ------------------------------------------- |\n| Idle threshold | 30s     | 15s, 30s (Recommended), 60s, Custom (5-300) |\n| zstd level     | 3       | 1 (fast), 3 (Recommended), 6, Custom (1-22) |\n| Brotli level   | 9       | 6, 9 (Recommended), 11, Custom (1-11)       |\n| Auto-push      | Yes     | Yes (Recommended), No                       |\n| Poll interval  | 5s      | 2s, 5s (Recommended), 10s                   |\n\n#### AskUserQuestion Sequence\n\n**3.1 Idle Threshold**:\n\n```yaml\nAskUserQuestion:\n  question: \"How long should the chunker wait before creating a chunk?\"\n  header: \"Idle Threshold\"\n  options:\n    - label: \"15 seconds\"\n      description: \"More frequent chunks, smaller files\"\n    - label: \"30 seconds (Recommended)\"\n      description: \"Balanced chunk size and frequency\"\n    - label: \"60 seconds\"\n      description: \"Larger chunks, less frequent uploads\"\n    - label: \"Custom (5-300 seconds)\"\n      description: \"Enter a custom threshold\"\n```\n\n**3.2 zstd Compression Level**:\n\n```yaml\nAskUserQuestion:\n  question: \"What zstd compression level for streaming chunks?\"\n  header: \"zstd Level\"\n  options:\n    - label: \"1 (Fast)\"\n      description: \"Fastest compression, larger files\"\n    - label: \"3 (Recommended)\"\n      description: \"Good balance of speed and compression\"\n    - label: \"6 (Better compression)\"\n      description: \"Slower but smaller chunks\"\n    - label: \"Custom (1-22)\"\n      description: \"Enter a custom level\"\n```\n\n**3.3 Brotli Compression Level**:\n\n```yaml\nAskUserQuestion:\n  question: \"What brotli compression level for final archives?\"\n  header: \"Brotli Level\"\n  options:\n    - label: \"6\"\n      description: \"Faster archival, slightly larger files\"\n    - label: \"9 (Recommended)\"\n      description: \"Great compression with reasonable speed\"\n    - label: \"11 (Maximum)\"\n      description: \"Best compression, slowest (may timeout on large files)\"\n    - label: \"Custom (1-11)\"\n      description: \"Enter a custom level\"\n```\n\n**3.4 Auto-Push**:\n\n```yaml\nAskUserQuestion:\n  question: \"Should chunks be automatically pushed to GitHub?\"\n  header: \"Auto-Push\"\n  options:\n    - label: \"Yes (Recommended)\"\n      description: \"Push immediately after each chunk\"\n    - label: \"No\"\n      description: \"Manual push when ready\"\n```\n\n**3.5 Poll Interval**:\n\n```yaml\nAskUserQuestion:\n  question: \"How often should the chunker check for idle state?\"\n  header: \"Poll Interval\"\n  options:\n    - label: \"2 seconds\"\n      description: \"More responsive, slightly higher CPU\"\n    - label: \"5 seconds (Recommended)\"\n      description: \"Good balance\"\n    - label: \"10 seconds\"\n      description: \"Lower resource usage\"\n```\n\n---\n\n### Phase 4: Orphan Branch Setup\n\n**Purpose**: Create or configure the orphan branch with GitHub Actions workflow.\n\n#### Check for Existing Branch\n\n```bash\n/usr/bin/env bash << 'CHECK_BRANCH_EOF'\n# Check if branch exists on remote\nREPO_URL=\"${1:?Usage: provide repo URL}\"\nBRANCH=\"${2:-asciinema-recordings}\"  # From Phase 2 (default changed)\n\nif git ls-remote --heads \"$REPO_URL\" \"$BRANCH\" 2>/dev/null | grep -q \"$BRANCH\"; then\n  echo \"Branch '$BRANCH' already exists on remote\"\n  echo \"BRANCH_EXISTS=true\"\nelse\n  echo \"Branch '$BRANCH' does not exist\"\n  echo \"BRANCH_EXISTS=false\"\nfi\nCHECK_BRANCH_EOF\n```\n\n#### AskUserQuestion (if branch exists)\n\n```yaml\nAskUserQuestion:\n  question: \"Branch '${BRANCH}' already exists on remote. How should we proceed?\"\n  header: \"Existing Branch\"\n  options:\n    - label: \"Clone locally (Recommended)\"\n      description: \"Use existing branch, clone to local directory\"\n    - label: \"Reset and recreate fresh\"\n      description: \"Delete remote branch and start over (DESTRUCTIVE)\"\n    - label: \"Keep existing and verify\"\n      description: \"Check existing setup matches configuration\"\n    - label: \"Show manual instructions\"\n      description: \"Display commands without executing\"\n```\n\n#### Branch Creation (if new)\n\n```bash\n/usr/bin/env bash << 'SETUP_ORPHAN_EOF'\n# setup-orphan-branch.sh - Creates asciinema-recordings orphan branch\n\nREPO_URL=\"${1:?Usage: setup-orphan-branch.sh <repo_url> [branch] [local_dir] [brotli_level]}\"\nBRANCH=\"${2:-asciinema-recordings}\"  # Default changed to match parent dir pattern\nLOCAL_DIR=\"${3:-$HOME/asciinema_recordings/$(basename \"$REPO_URL\" .git)}\"\nBROTLI_LEVEL=\"${4:-9}\"  # Embedded from Phase 3 selection\n\n# Create temporary clone for setup\nTEMP_DIR=$(mktemp -d)\ntrap \"rm -rf $TEMP_DIR\" EXIT\n\ngit clone --depth 1 \"$REPO_URL\" \"$TEMP_DIR\"\ncd \"$TEMP_DIR\"\n\n# Create orphan branch\ngit checkout --orphan \"$BRANCH\"\ngit rm -rf .\n\n# Setup directory structure\nmkdir -p .github/workflows chunks archives\n\n# Create workflow with user-selected brotli level (EMBEDDED at creation time)\ncat > .github/workflows/recompress.yml << WORKFLOW_EOF\nname: Recompress to Brotli\n\non:\n  push:\n    branches: [$BRANCH]\n    paths: ['chunks/**/*.zst']\n  workflow_dispatch:\n\njobs:\n  recompress:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install compression tools\n        run: sudo apt-get update && sudo apt-get install -y zstd brotli\n\n      - name: Recompress chunks to brotli\n        run: |\n          if compgen -G \"chunks/*.zst\" > /dev/null; then\n            mkdir -p archives\n            ARCHIVE_NAME=\"archive_\\$(date +%Y%m%d_%H%M%S).cast.br\"\n            ls -1 chunks/*.zst | sort | xargs cat | zstd -d | brotli -${BROTLI_LEVEL} -o \"archives/\\$ARCHIVE_NAME\"\n            rm -f chunks/*.zst\n            echo \"Created: archives/\\$ARCHIVE_NAME\"\n            echo \"ARCHIVE_NAME=\\$ARCHIVE_NAME\" >> \\$GITHUB_ENV\n          else\n            echo \"No chunks to process\"\n          fi\n\n      - name: Commit archive\n        if: env.ARCHIVE_NAME != ''\n        uses: stefanzweifel/git-auto-commit-action@v5\n        with:\n          commit_message: \"chore: archive recording to brotli (\\${{ env.ARCHIVE_NAME }})\"\n          file_pattern: 'archives/*.br chunks/'\nWORKFLOW_EOF\n\n# Create placeholder files\necho '# Recording chunks (zstd compressed)' > chunks/README.md\necho '# Brotli archives (final compressed)' > archives/README.md\n\n# Create README\ncat > README.md << 'README_EOF'\n# Recording Storage (Orphan Branch)\n\nThis branch stores asciinema recording backups. It is completely isolated from the main codebase.\n\n## Structure\n\n- `chunks/` - Streaming zstd-compressed chunks (auto-deleted after archival)\n- `archives/` - Final brotli-compressed recordings (~300x compression)\n\n## How It Works\n\n1. Local idle-chunker monitors asciinema recording\n2. When idle 30s, creates zstd chunk and pushes here\n3. GitHub Action concatenates chunks and recompresses to brotli\n4. Chunks are deleted, archive is retained\n\n## Isolation Guarantee\n\nThis is an orphan branch with no shared history with main.\nGit refuses to merge: \"refusing to merge unrelated histories\"\nREADME_EOF\n\n# Commit and push\ngit add .\ngit commit -m \"init: recording storage (orphan branch)\"\ngit push -u origin \"$BRANCH\"\n\ncd -\n\n# Clone to local recordings directory\nmkdir -p \"$(dirname \"$LOCAL_DIR\")\"\ngit clone --single-branch --branch \"$BRANCH\" --depth 1 \"$REPO_URL\" \"$LOCAL_DIR\"\necho \"Setup complete: $LOCAL_DIR\"\nSETUP_ORPHAN_EOF\n```\n\n---\n\n### Phase 5: Local Environment Setup\n\n**Purpose**: Configure local directory and generate chunker script with user parameters.\n\n#### Setup Local Directory\n\n```bash\n/usr/bin/env bash << 'SETUP_LOCAL_EOF'\nREPO_NAME=\"${1:?Usage: provide repo name}\"\nREPO_URL=\"${2:?Usage: provide repo URL}\"\nBRANCH=\"${3:-asciinema-recordings}\"\n\nLOCAL_DIR=\"$HOME/asciinema_recordings/${REPO_NAME}\"\n\n# Ensure directories exist\nmkdir -p \"$LOCAL_DIR/chunks\"\nmkdir -p \"$LOCAL_DIR/archives\"\n\n# Clone if not present\nif [[ ! -d \"$LOCAL_DIR/.git\" ]]; then\n  git clone --single-branch --branch \"$BRANCH\" --depth 1 \"$REPO_URL\" \"$LOCAL_DIR\"\nfi\n\necho \"LOCAL_DIR=$LOCAL_DIR\"\nSETUP_LOCAL_EOF\n```\n\n#### Generate Customized idle-chunker.sh\n\nGenerate the chunker script with user-selected parameters embedded:\n\n```bash\n/usr/bin/env bash << 'GEN_CHUNKER_EOF'\n# Parameters from Phase 3 (passed as arguments)\nLOCAL_DIR=\"${1:?Usage: provide LOCAL_DIR}\"\nIDLE_THRESHOLD=\"${2:-30}\"\nZSTD_LEVEL=\"${3:-3}\"\nPOLL_INTERVAL=\"${4:-5}\"\nPUSH_ENABLED=\"${5:-true}\"\n\ncat > \"$LOCAL_DIR/idle-chunker.sh\" << CHUNKER_EOF\n#!/usr/bin/env bash\n# idle-chunker.sh - Generated with user configuration\n#\n# Configuration (embedded from setup):\n#   IDLE_THRESHOLD=${IDLE_THRESHOLD}\n#   ZSTD_LEVEL=${ZSTD_LEVEL}\n#   POLL_INTERVAL=${POLL_INTERVAL}\n#   PUSH_ENABLED=${PUSH_ENABLED}\n\nset -euo pipefail\n\nCAST_FILE=\"\\${1:?Usage: idle-chunker.sh <cast_file>}\"\n\n# Embedded configuration\nIDLE_THRESHOLD=${IDLE_THRESHOLD}\nZSTD_LEVEL=${ZSTD_LEVEL}\nPOLL_INTERVAL=${POLL_INTERVAL}\nPUSH_ENABLED=${PUSH_ENABLED}\n\ncd \"\\$(dirname \"\\$0\")\"\nlast_pos=0\n\necho \"Monitoring: \\$CAST_FILE\"\necho \"Idle threshold: \\${IDLE_THRESHOLD}s | zstd level: \\${ZSTD_LEVEL} | Poll: \\${POLL_INTERVAL}s\"\n\nwhile [[ -f \"\\$CAST_FILE\" ]] || sleep 2; do\n  [[ -f \"\\$CAST_FILE\" ]] || continue\n  mtime=\\$(stat -f%m \"\\$CAST_FILE\" 2>/dev/null || stat -c%Y \"\\$CAST_FILE\")\n  idle=\\$((\\$(date +%s) - mtime))\n  size=\\$(stat -f%z \"\\$CAST_FILE\" 2>/dev/null || stat -c%s \"\\$CAST_FILE\")\n\n  if (( idle >= IDLE_THRESHOLD && size > last_pos )); then\n    chunk=\"chunks/chunk_\\$(date +%Y%m%d_%H%M%S).cast\"\n    tail -c +\\$((last_pos + 1)) \"\\$CAST_FILE\" > \"\\$chunk\"\n    zstd -\\${ZSTD_LEVEL} --rm \"\\$chunk\"\n\n    if [[ \"\\$PUSH_ENABLED\" == \"true\" ]]; then\n      git add chunks/ && git commit -m \"chunk \\$(date +%H:%M)\" && git push\n    fi\n\n    last_pos=\\$size\n    echo \"[\\$(date +%H:%M:%S)] Created: \\${chunk}.zst\"\n  fi\n\n  sleep \\$POLL_INTERVAL\ndone\nCHUNKER_EOF\n\nchmod +x \"$LOCAL_DIR/idle-chunker.sh\"\necho \"Generated: $LOCAL_DIR/idle-chunker.sh\"\nGEN_CHUNKER_EOF\n```\n\n#### Display Configuration Summary\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\necho \"\"\necho \"=== Setup Complete ===\"\necho \"\"\necho \"Configuration:\"\necho \"  Repository: $REPO_URL\"\necho \"  Branch: $BRANCH\"\necho \"  Local directory: $LOCAL_DIR\"\necho \"\"\necho \"Parameters:\"\necho \"  Idle threshold: ${IDLE_THRESHOLD}s\"\necho \"  zstd level: $ZSTD_LEVEL\"\necho \"  Brotli level: $BROTLI_LEVEL\"\necho \"  Auto-push: $PUSH_ENABLED\"\necho \"  Poll interval: ${POLL_INTERVAL}s\"\necho \"\"\necho \"To start recording:\"\necho \"  1. asciinema rec /path/to/session.cast\"\necho \"  2. $LOCAL_DIR/idle-chunker.sh /path/to/session.cast\"\nSETUP_EOF\n```\n\n---\n\n### Phase 6: Autonomous Validation\n\n**Purpose**: Claude executes validation tests automatically, displaying results in CLI. Only interrupts user when human action is required.\n\n#### Validation Test Categories\n\n| Test                        | Autonomous? | Reason                      |\n| --------------------------- | ----------- | --------------------------- |\n| 1. Tool preflight           |  YES      | Bash checks tools           |\n| 2. zstd round-trip          |  YES      | Synthetic test data         |\n| 3. Brotli round-trip        |  YES      | Synthetic test data         |\n| 4. zstd concatenation       |  YES      | Critical for streaming      |\n| 5. Git/gh auth check        |  YES      | Query auth status           |\n| 6. Orphan branch validation |  YES      | Check remote/local          |\n| 7. Workflow file check      |  YES      | Read file contents          |\n| 8. GitHub Actions trigger   |  YES      | `gh workflow run` + watch   |\n| 9. Recording test           |  USER     | Requires starting asciinema |\n| 10. Chunker live test       |  USER     | Requires active recording   |\n\n#### Autonomous Execution\n\nClaude runs the validation script and displays formatted results:\n\n```\n\n AUTONOMOUS VALIDATION - Claude Code Executes All Tests         \n\n                                                                 \n  Phase 1: Tool Check                                           \n                                               \n  [RUN] Checking asciinema...  installed (v3.0.0)              \n  [RUN] Checking zstd...  installed (v1.5.5)                   \n  [RUN] Checking brotli...  installed (v1.1.0)                 \n  [RUN] Checking git...  installed (v2.43.0)                   \n  [RUN] Checking gh...  installed (v2.40.0)                    \n                                                                 \n  Phase 2: Compression Tests                                    \n                                        \n  [RUN] zstd round-trip...  PASSED                             \n  [RUN] brotli round-trip...  PASSED                           \n  [RUN] zstd concatenation...  PASSED (critical for streaming) \n                                                                 \n  Phase 3: Repository Validation                                \n                                   \n  [RUN] Checking gh auth...  authenticated as terrylica        \n  [RUN] Checking orphan branch...  gh-recordings exists        \n  [RUN] Checking local clone...  ~/asciinema_recordings/repo   \n  [RUN] Checking workflow file...  recompress.yml present      \n                                                                 \n  Phase 4: GitHub Actions Test                                  \n                                   \n  [RUN] Triggering workflow_dispatch...  triggered             \n  [RUN] Watching run #12345...  in_progress                   \n  [RUN] Watching run #12345...  completed (success)            \n                                                                 \n     \n  AUTONOMOUS TESTS: 8/8 PASSED                                  \n     \n\n```\n\n#### User-Required Tests\n\nOnly TWO tests require user action:\n\n**Test 9: Recording Validation**\n\n```yaml\nAskUserQuestion:\n  question: \"Ready to test recording? This requires you to start asciinema in another terminal.\"\n  header: \"Recording Test\"\n  options:\n    - label: \"Guide me through it (Recommended)\"\n      description: \"Step-by-step instructions\"\n    - label: \"Skip this test\"\n      description: \"I'll verify manually later\"\n    - label: \"I've already verified recording works\"\n      description: \"Mark as passed\"\n```\n\nIf \"Guide me through it\" selected, display:\n\n```\n\n USER ACTION REQUIRED: Recording Test                           \n\n                                                                 \n  In a NEW terminal, run:                                       \n      \n   asciinema rec ~/asciinema_recordings/test_session.cast     \n      \n                                                                 \n  Then type a few commands and exit with Ctrl+D                 \n                                                                 \n  Come back here when done.                                     \n\n```\n\nThen Claude autonomously validates the created file:\n\n```bash\n# Claude runs after user confirms:\n[RUN] Checking test_session.cast exists... \n[RUN] Validating JSON header...  {\"version\": 2, ...}\n[RUN] Checking line count...  23 events recorded\n```\n\n**Test 10: Chunker Live Test**\n\n```yaml\nAskUserQuestion:\n  question: \"Ready to test live chunking? This requires running recording + chunker simultaneously.\"\n  header: \"Chunker Test\"\n  options:\n    - label: \"Guide me (Recommended)\"\n      description: \"Two-terminal workflow instructions\"\n    - label: \"Skip - I trust the setup\"\n      description: \"Skip live test\"\n```\n\n#### Full Validation Script\n\nSee [references/autonomous-validation.md](./references/autonomous-validation.md) for the complete validation script.\n\n#### Troubleshooting on Failure\n\nIf any test fails, Claude displays inline troubleshooting:\n\n```\n[RUN] Checking gh auth...  FAILED\n\n      Troubleshooting:\n      1. Run: gh auth login\n      2. Select: GitHub.com\n      3. Choose: HTTPS or SSH\n      4. Follow prompts to authenticate\n\n      Then re-run validation.\n```\n\n---\n\n## Quick Start\n\n### First-Time Setup\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF'\n# 1. Check requirements\nfor tool in asciinema zstd brotli git gh; do\n  command -v \"$tool\" &>/dev/null && echo \"$tool: OK\" || echo \"$tool: MISSING\"\ndone\n\n# 2. Create orphan branch (replace with your repo)\nREPO=\"git@github.com:YOUR/REPO.git\"\n./setup-orphan-branch.sh \"$REPO\"\n\n# 3. Validate setup\n./validate-setup.sh \"$HOME/asciinema_recordings/REPO\"\nPREFLIGHT_EOF\n```\n\n### Recording Session\n\n```bash\n/usr/bin/env bash << 'SKILL_SCRIPT_EOF'\n# Terminal 1: Start recording\nWORKSPACE=$(basename \"$PWD\")\nasciinema rec $PWD/tmp/${WORKSPACE}_$(date +%Y-%m-%d_%H-%M).cast\n\n# Terminal 2: Start idle-chunker\n~/asciinema_recordings/REPO/idle-chunker.sh $PWD/tmp/${WORKSPACE}_*.cast\nSKILL_SCRIPT_EOF\n```\n\n---\n\n## TodoWrite Task Templates\n\n### Template: Full Setup\n\n```\n1. [Preflight] Validate all tools installed (asciinema, zstd, brotli, git, gh)\n2. [Preflight] AskUserQuestion: offer installation for missing tools\n3. [Account] Detect GitHub accounts from 5 sources\n4. [Account] AskUserQuestion: select GitHub account\n5. [Config] AskUserQuestion: repository URL\n6. [Config] AskUserQuestion: recording directory\n7. [Config] AskUserQuestion: branch name\n8. [Advanced] AskUserQuestion: idle threshold\n9. [Advanced] AskUserQuestion: zstd level\n10. [Advanced] AskUserQuestion: brotli level\n11. [Advanced] AskUserQuestion: auto-push\n12. [Advanced] AskUserQuestion: poll interval\n13. [Branch] Check if orphan branch exists on remote\n14. [Branch] AskUserQuestion: handle existing branch\n15. [Branch] Create orphan branch if needed\n16. [Branch] Create GitHub Actions workflow with embedded parameters\n17. [Local] Clone orphan branch to ~/asciinema_recordings/\n18. [Local] Generate idle-chunker.sh with embedded parameters\n19. [Validate] Run autonomous validation (8 tests)\n20. [Validate] AskUserQuestion: recording test (user action)\n21. [Validate] AskUserQuestion: chunker live test (user action)\n22. [Guide] Display configuration summary and usage instructions\n```\n\n### Template: Recording Session\n\n```\n1. [Context] Detect workspace from $PWD\n2. [Context] Generate datetime for filename\n3. [Context] Ensure tmp/ directory exists\n4. [Command] Generate asciinema rec command\n5. [Command] Generate idle-chunker command\n6. [Guide] Display two-terminal workflow instructions\n```\n\n---\n\n## Troubleshooting\n\n### \"Cannot push to orphan branch\"\n\n**Cause**: Authentication or permissions issue.\n\n**Fix**:\n\n```bash\n# Check gh auth status\ngh auth status\n\n# Re-authenticate if needed\ngh auth login\n```\n\n### \"Chunks not being created\"\n\n**Cause**: Idle threshold not reached, or file not growing.\n\n**Fix**:\n\n- Verify recording is active: `tail -f $CAST_FILE`\n- Lower threshold: `IDLE_THRESHOLD=15`\n- Check file permissions\n\n### \"GitHub Action not triggering\"\n\n**Cause**: Workflow file missing or wrong branch filter.\n\n**Fix**:\n\n```bash\n# Verify workflow exists\ncat ~/asciinema_recordings/REPO/.github/workflows/recompress.yml\n\n# Check branch filter includes gh-recordings\ngrep -A2 \"branches:\" ~/asciinema_recordings/REPO/.github/workflows/recompress.yml\n```\n\n### \"Brotli archive empty or corrupted\"\n\n**Cause**: zstd chunks not concatenating properly (overlapping data).\n\n**Fix**: Ensure idle-chunker uses `last_chunk_pos` to avoid overlap:\n\n```bash\n/usr/bin/env bash << 'PREFLIGHT_EOF_2'\n# Check for overlaps - each chunk should be sequential\nfor f in chunks/*.zst; do\n  zstd -d \"$f\" -c | head -1\ndone\nPREFLIGHT_EOF_2\n```\n\n---\n\n## Key Design Decisions\n\n| Decision                | Rationale                                          |\n| ----------------------- | -------------------------------------------------- |\n| **zstd for streaming**  | Supports frame concatenation (brotli doesn't)      |\n| **brotli for archival** | Best compression ratio (~300x for .cast files)     |\n| **Orphan branch**       | Complete isolation, can't pollute main history     |\n| **Idle-based chunking** | Semantic breakpoints, not mid-output splits        |\n| **Shallow clone**       | Minimal disk usage, can't accidentally access main |\n| **30s idle threshold**  | Balances chunk frequency vs semantic completeness  |\n\n---\n\n## Post-Change Checklist\n\nAfter modifying this skill:\n\n1. [ ] Orphan branch creation scripts use heredoc wrapper\n2. [ ] All bash blocks compatible with zsh (no declare -A, no grep -P)\n3. [ ] GitHub Actions workflow validates brotli recompression\n4. [ ] Idle chunker handles both macOS and Linux stat syntax\n5. [ ] Detection flow outputs parseable key=value format\n6. [ ] References validate links to external documentation\n\n---\n\n## Reference Documentation\n\n- [Idle Chunker Script](./references/idle-chunker.md) - Complete chunker implementation\n- [GitHub Workflow](./references/github-workflow.md) - Full Actions workflow\n- [Setup Scripts](./references/setup-scripts.md) - All setup and validation scripts\n- [Autonomous Validation](./references/autonomous-validation.md) - Validation script and user-required tests\n- [asciinema 3.0 Docs](https://docs.asciinema.org/)\n- [zstd Frame Format](https://github.com/facebook/zstd)\n- [Git Orphan Branches](https://graphite.dev/guides/git-orphan-branches)"
              }
            ]
          },
          {
            "name": "git-town-workflow",
            "description": "Prescriptive git-town workflow enforcement for fork-based development: fork creation, contribution workflow, enforcement hooks that block forbidden raw git commands",
            "source": "./plugins/git-town-workflow/",
            "category": "devops",
            "version": "9.24.6",
            "author": {
              "name": "Terry Li",
              "url": "https://github.com/terrylica"
            },
            "install_commands": [
              "/plugin marketplace add terrylica/cc-skills",
              "/plugin install git-town-workflow@cc-skills"
            ],
            "signals": {
              "stars": 6,
              "forks": 0,
              "pushed_at": "2026-01-12T22:07:09Z",
              "created_at": "2025-12-04T16:26:10Z",
              "license": null
            },
            "commands": [
              {
                "name": "/contribute",
                "description": "Complete contribution workflow using git-town. Create branch  commit  PR  ship. Preflight at every step. TRIGGERS - contribute, feature branch, create PR, submit PR, git-town contribute.",
                "path": "plugins/git-town-workflow/commands/contribute.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(git town:*), Bash(git remote:*), Bash(git config:*), Bash(git status:*), Bash(git log:*), Bash(git branch:*), Bash(git add:*), Bash(git commit:*), Bash(git diff:*), Bash(gh pr:*), Bash(gh api:*), Grep, Glob, AskUserQuestion, TodoWrite",
                  "argument-hint": "[feature-name] | --pr | --ship",
                  "description": "Complete contribution workflow using git-town. Create branch  commit  PR  ship. Preflight at every step. TRIGGERS - contribute, feature branch, create PR, submit PR, git-town contribute."
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n# Git-Town Contribution Workflow  STOP AND READ\n\n**This workflow guides you through a complete contribution cycle using git-town.**\n\n##  WORKFLOW ENFORCEMENT\n\n**YOU MUST USE GIT-TOWN COMMANDS. RAW GIT BRANCH COMMANDS ARE FORBIDDEN.**\n\n| Step |  Correct |  Forbidden |\n|------|-----------|--------------|\n| Create branch | `git town hack` | `git checkout -b` |\n| Update branch | `git town sync` | `git pull`, `git fetch`, `git merge` |\n| Create PR | `git town propose` | Manual GitHub UI |\n| Merge PR | `git town ship` | `git merge` + `git push` |\n\n---\n\n## Phase 0: Preflight  MANDATORY\n\n### Step 0.1: Create TodoWrite\n\n```\nTodoWrite with todos:\n- \"[Contribute] Phase 0: Verify fork workflow is configured\" | in_progress\n- \"[Contribute] Phase 0: Check workspace is clean\" | pending\n- \"[Contribute] Phase 0: Sync with upstream\" | pending\n- \"[Contribute] Phase 1: GATE - Confirm feature branch creation\" | pending\n- \"[Contribute] Phase 1: Create feature branch with git town hack\" | pending\n- \"[Contribute] Phase 2: Implement changes\" | pending\n- \"[Contribute] Phase 2: Commit changes (raw git allowed here)\" | pending\n- \"[Contribute] Phase 2: Sync branch before PR\" | pending\n- \"[Contribute] Phase 3: GATE - Confirm PR creation\" | pending\n- \"[Contribute] Phase 3: Create PR with git town propose\" | pending\n- \"[Contribute] Phase 4: (Optional) Ship PR with git town ship\" | pending\n```\n\n### Step 0.2: Verify Fork Workflow Configured\n\n```bash\n/usr/bin/env bash << 'VERIFY_FORK_EOF'\necho \"=== FORK WORKFLOW VERIFICATION ===\"\n\n# Check remotes\nORIGIN=$(git remote get-url origin 2>/dev/null)\nUPSTREAM=$(git remote get-url upstream 2>/dev/null)\n\nif [[ -z \"$UPSTREAM\" ]]; then\n    echo \" FATAL: upstream remote not configured\"\n    echo \"Run: /git-town-workflow:fork to configure\"\n    exit 1\nfi\n\necho \" origin: $ORIGIN\"\necho \" upstream: $UPSTREAM\"\n\n# Check git-town config\nSYNC_UPSTREAM=$(git config git-town.sync-upstream 2>/dev/null)\nif [[ \"$SYNC_UPSTREAM\" != \"true\" ]]; then\n    echo \" WARNING: git-town.sync-upstream is not true\"\n    echo \"Run: git config git-town.sync-upstream true\"\nfi\n\n# Check current branch\nCURRENT_BRANCH=$(git branch --show-current)\necho \"Current branch: $CURRENT_BRANCH\"\n\nVERIFY_FORK_EOF\n```\n\n**If verification fails:**\n```\nAskUserQuestion with questions:\n- question: \"Fork workflow is not configured. Run fork setup first?\"\n  header: \"Setup Required\"\n  options:\n    - label: \"Yes, run /git-town-workflow:fork now\"\n      description: \"Configure fork workflow first\"\n    - label: \"No, abort\"\n      description: \"Cannot proceed without fork setup\"\n  multiSelect: false\n```\n\n### Step 0.3: Check Workspace Clean\n\n```bash\n/usr/bin/env bash -c 'git status --porcelain'\n```\n\n**If workspace has changes:**\n```\nAskUserQuestion with questions:\n- question: \"Workspace has uncommitted changes. How to proceed?\"\n  header: \"Dirty Workspace\"\n  options:\n    - label: \"Stash changes (Recommended)\"\n      description: \"git stash, create branch, git stash pop\"\n    - label: \"Commit changes first\"\n      description: \"Create commit before new branch\"\n    - label: \"Discard changes\"\n      description: \"WARNING: Loses uncommitted work\"\n    - label: \"Abort\"\n      description: \"Handle manually\"\n  multiSelect: false\n```\n\n### Step 0.4: Sync with Upstream\n\n**ALWAYS sync before creating feature branch:**\n\n```bash\ngit town sync\n```\n\n**If conflicts occur:**\n1. Display conflict files\n2. Wait for user to resolve\n3. Run `git town continue`\n\n---\n\n## Phase 1: Create Feature Branch\n\n### Step 1.1: GATE  Confirm Branch Creation\n\n```\nAskUserQuestion with questions:\n- question: \"What is the feature branch name?\"\n  header: \"Branch Name\"\n  options:\n    - label: \"feat/{feature-name}\"\n      description: \"Standard feature branch\"\n    - label: \"fix/{bug-name}\"\n      description: \"Bug fix branch\"\n    - label: \"docs/{doc-name}\"\n      description: \"Documentation branch\"\n    - label: \"Enter custom name\"\n      description: \"I'll provide the full branch name\"\n  multiSelect: false\n```\n\n### Step 1.2: Create Branch with git-town\n\n** NEVER use `git checkout -b`. ALWAYS use:**\n\n```bash\ngit town hack {branch-name}\n```\n\n**This command:**\n1. Fetches from origin and upstream\n2. Creates branch from updated main\n3. Sets up tracking correctly\n4. Updates parent chain\n\n### Step 1.3: Verify Branch Created\n\n```bash\n/usr/bin/env bash << 'VERIFY_BRANCH_EOF'\nBRANCH=$(git branch --show-current)\necho \"Current branch: $BRANCH\"\n\n# Verify parent is main\ngit town branch\nVERIFY_BRANCH_EOF\n```\n\n---\n\n## Phase 2: Implement & Commit\n\n### Step 2.1: Implement Changes\n\n**User implements their changes here.**\n\n(This phase is handled by the user or other skills)\n\n### Step 2.2: Stage and Commit (Raw git allowed)\n\n**Raw git IS allowed for commits:**\n\n```bash\ngit add .\ngit commit -m \"feat: description of change\"\n```\n\n**Commit message format:**\n- `feat:` - New feature\n- `fix:` - Bug fix\n- `docs:` - Documentation\n- `refactor:` - Code refactoring\n- `test:` - Tests\n- `chore:` - Maintenance\n\n### Step 2.3: Sync Before PR\n\n** NEVER use `git pull` or `git push`. ALWAYS use:**\n\n```bash\ngit town sync\n```\n\n**This:**\n1. Pulls changes from upstream/main\n2. Rebases/merges feature branch\n3. Pushes to origin (your fork)\n\n**If conflicts:**\n```\nAskUserQuestion with questions:\n- question: \"Sync encountered conflicts. What next?\"\n  header: \"Conflicts\"\n  options:\n    - label: \"I'll resolve conflicts manually\"\n      description: \"Fix conflicts, then run: git town continue\"\n    - label: \"Skip conflicting changes\"\n      description: \"Run: git town skip (may lose changes)\"\n    - label: \"Abort sync\"\n      description: \"Run: git town undo\"\n  multiSelect: false\n```\n\n---\n\n## Phase 3: Create Pull Request\n\n### Step 3.1: GATE  Confirm PR Creation\n\n```\nAskUserQuestion with questions:\n- question: \"Ready to create a pull request to upstream?\"\n  header: \"Create PR\"\n  options:\n    - label: \"Yes, create PR to upstream\"\n      description: \"Run: git town propose\"\n    - label: \"No, keep working\"\n      description: \"Continue development, create PR later\"\n    - label: \"Create draft PR\"\n      description: \"Create PR but mark as draft\"\n  multiSelect: false\n```\n\n### Step 3.2: Create PR with git-town\n\n** NEVER create PR manually. ALWAYS use:**\n\n```bash\ngit town propose\n```\n\n**This:**\n1. Pushes latest changes to origin\n2. Opens browser to create PR\n3. Targets correct upstream repository\n4. Fills in branch info\n\n**For draft PR:**\n```bash\ngit town propose --draft\n```\n\n### Step 3.3: Verify PR Created\n\n```bash\n/usr/bin/env bash -c 'gh pr view --json url,state,title'\n```\n\n---\n\n## Phase 4: Ship (After PR Approved)\n\n### Step 4.1: GATE  Confirm Ship\n\n```\nAskUserQuestion with questions:\n- question: \"Has your PR been approved and ready to merge?\"\n  header: \"Ship PR\"\n  options:\n    - label: \"Yes, ship it (merge to main)\"\n      description: \"Run: git town ship\"\n    - label: \"Not yet, PR is pending review\"\n      description: \"Wait for approval\"\n    - label: \"PR was merged via GitHub UI\"\n      description: \"Just cleanup local branches\"\n  multiSelect: false\n```\n\n### Step 4.2: Ship with git-town\n\n** NEVER merge manually. ALWAYS use:**\n\n```bash\ngit town ship\n```\n\n**This:**\n1. Verifies PR is approved\n2. Merges to main\n3. Deletes feature branch (local + remote)\n4. Updates local main\n\n### Step 4.3: Post-Ship Cleanup\n\n```bash\n/usr/bin/env bash << 'CLEANUP_EOF'\necho \"=== POST-SHIP STATUS ===\"\n\n# Show current branch\ngit branch --show-current\n\n# Show recent commits on main\ngit log --oneline -5\n\n# Verify feature branch deleted\ngit branch -a | grep -v \"^*\" | head -10\n\necho \" Ship complete\"\nCLEANUP_EOF\n```\n\n---\n\n## Stacked Branches (Advanced)\n\n### Creating Child Branches\n\nIf your feature needs to be split into smaller PRs:\n\n```bash\n# On feature branch, create child\ngit town append child-feature\n\n# Creates stack:\n# main\n#    feature\n#          child-feature\n```\n\n### Navigating Stacks\n\n```bash\ngit town up      # Go to parent branch\ngit town down    # Go to child branch\ngit town branch  # Show full stack hierarchy\n```\n\n### Shipping Stacks\n\n**Ship from bottom up:**\n```bash\ngit town ship feature        # Ships feature first\ngit town ship child-feature  # Then ship child\n```\n\n---\n\n## Error Recovery\n\n### Undo Last git-town Command\n\n```bash\ngit town undo\n```\n\n### Continue After Resolving Conflicts\n\n```bash\ngit town continue\n```\n\n### Skip Conflicting Branch in Sync\n\n```bash\ngit town skip\n```\n\n### Check git-town Status\n\n```bash\ngit town status\n```\n\n---\n\n## Quick Reference Card\n\n```\n\n                GIT-TOWN CONTRIBUTION FLOW               \n\n                                                         \n  1. SYNC         git town sync                          \n                                                        \n  2. BRANCH       git town hack feature-name             \n                                                        \n  3. COMMIT       git add . && git commit -m \"...\"       \n                                                        \n  4. SYNC         git town sync                          \n                                                        \n  5. PR           git town propose                       \n                                                        \n  6. SHIP         git town ship (after approval)         \n                                                         \n\n    FORBIDDEN: git checkout -b, git pull, git merge    \n   ALLOWED: git add, git commit, git log, git diff     \n\n```\n\n---\n\n## Arguments\n\n- `[feature-name]` - Optional: Branch name for new feature\n- `--pr` - Skip to PR creation (branch already exists)\n- `--ship` - Skip to ship (PR already approved)\n\n## Examples\n\n```bash\n# Start new contribution\n/git-town-workflow:contribute feat/add-dark-mode\n\n# Create PR for existing branch\n/git-town-workflow:contribute --pr\n\n# Ship after PR approved\n/git-town-workflow:contribute --ship\n```"
              },
              {
                "name": "/fork",
                "description": "Create or configure a fork workflow with git-town. Preflight checks at every step. TRIGGERS - fork repo, setup fork, git-town fork, create fork, fork workflow, upstream setup.",
                "path": "plugins/git-town-workflow/commands/fork.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(git town:*), Bash(git remote:*), Bash(git config:*), Bash(git status:*), Bash(git log:*), Bash(git branch:*), Bash(gh repo:*), Bash(gh api:*), Bash(gh auth:*), Bash(which:*), Bash(brew:*), Grep, Glob, AskUserQuestion, TodoWrite",
                  "argument-hint": "[upstream-url] | --check | --fix",
                  "description": "Create or configure a fork workflow with git-town. Preflight checks at every step. TRIGGERS - fork repo, setup fork, git-town fork, create fork, fork workflow, upstream setup."
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n# Git-Town Fork Workflow  STOP AND READ\n\n**DO NOT ACT ON ASSUMPTIONS. Read this file first.**\n\nThis is a **prescriptive, gated workflow**. Every step requires:\n1. **Preflight check** - Verify preconditions\n2. **User confirmation** - AskUserQuestion before action\n3. **Validation** - Verify action succeeded\n\n##  WORKFLOW PHILOSOPHY\n\n**GIT-TOWN IS CANONICAL. RAW GIT IS FORBIDDEN FOR BRANCH OPERATIONS.**\n\n| Operation |  Use |  Never Use |\n|-----------|--------|--------------|\n| Create branch | `git town hack` | `git checkout -b` |\n| Update branch | `git town sync` | `git pull`, `git merge` |\n| Create PR | `git town propose` | Manual web UI |\n| Merge PR | `git town ship` | `git merge` + push |\n| Switch branch | `git town switch` | `git checkout` |\n\n**Exception**: Raw git for commits, staging, log viewing, diff (git-town doesn't replace these).\n\n---\n\n## Phase 0: Preflight  MANDATORY FIRST\n\n**Execute this BEFORE any other action.**\n\n### Step 0.1: Create TodoWrite\n\n```\nTodoWrite with todos:\n- \"[Fork] Phase 0: Check git-town installation\" | in_progress\n- \"[Fork] Phase 0: Check GitHub CLI installation\" | pending\n- \"[Fork] Phase 0: Detect current repository context\" | pending\n- \"[Fork] Phase 0: Detect existing remotes\" | pending\n- \"[Fork] Phase 0: Detect GitHub account(s)\" | pending\n- \"[Fork] Phase 1: GATE - Present findings and get user confirmation\" | pending\n- \"[Fork] Phase 2: Create fork (if needed)\" | pending\n- \"[Fork] Phase 2: Configure remotes\" | pending\n- \"[Fork] Phase 2: Initialize git-town\" | pending\n- \"[Fork] Phase 3: Validate setup\" | pending\n- \"[Fork] Phase 3: Display workflow cheatsheet\" | pending\n```\n\n### Step 0.2: Check git-town Installation\n\n```bash\n/usr/bin/env bash -c 'which git-town && git-town --version'\n```\n\n**If NOT installed:**\n```\nAskUserQuestion with questions:\n- question: \"git-town is not installed. Would you like to install it now?\"\n  header: \"Install\"\n  options:\n    - label: \"Yes, install via Homebrew (Recommended)\"\n      description: \"Run: brew install git-town\"\n    - label: \"No, abort workflow\"\n      description: \"Cannot proceed without git-town\"\n  multiSelect: false\n```\n\nIf \"Yes\": Run `brew install git-town`, then re-check.\nIf \"No\": **STOP. Do not proceed.**\n\n### Step 0.3: Check GitHub CLI Installation\n\n```bash\n/usr/bin/env bash -c 'which gh && gh --version && gh auth status'\n```\n\n**If NOT installed or NOT authenticated:**\n```\nAskUserQuestion with questions:\n- question: \"GitHub CLI is required for fork operations. How to proceed?\"\n  header: \"GitHub CLI\"\n  options:\n    - label: \"Install and authenticate (Recommended)\"\n      description: \"Run: brew install gh && gh auth login\"\n    - label: \"I'll handle this manually\"\n      description: \"Provide instructions and exit\"\n  multiSelect: false\n```\n\n### Step 0.4: Detect Repository Context\n\n**Run detection script BEFORE any AskUserQuestion:**\n\n```bash\n/usr/bin/env bash << 'DETECT_REPO_EOF'\necho \"=== REPOSITORY DETECTION ===\"\n\n# Check if in git repo\nif ! git rev-parse --git-dir &>/dev/null; then\n    echo \"ERROR: Not in a git repository\"\n    exit 1\nfi\n\n# Detect remotes\necho \"--- Existing Remotes ---\"\ngit remote -v\n\n# Detect current branch\necho \"--- Current Branch ---\"\ngit branch --show-current\n\n# Detect repo URL patterns\necho \"--- Remote URLs ---\"\nORIGIN_URL=$(git remote get-url origin 2>/dev/null || echo \"NONE\")\nUPSTREAM_URL=$(git remote get-url upstream 2>/dev/null || echo \"NONE\")\n\necho \"origin: $ORIGIN_URL\"\necho \"upstream: $UPSTREAM_URL\"\n\n# Parse GitHub owner/repo from URLs\nif [[ \"$ORIGIN_URL\" =~ github\\.com[:/]([^/]+)/([^/.]+) ]]; then\n    echo \"ORIGIN_OWNER=${BASH_REMATCH[1]}\"\n    echo \"ORIGIN_REPO=${BASH_REMATCH[2]%.git}\"\nfi\n\nif [[ \"$UPSTREAM_URL\" =~ github\\.com[:/]([^/]+)/([^/.]+) ]]; then\n    echo \"UPSTREAM_OWNER=${BASH_REMATCH[1]}\"\n    echo \"UPSTREAM_REPO=${BASH_REMATCH[2]%.git}\"\nfi\n\n# Check git-town config\necho \"--- Git-Town Config ---\"\ngit town config 2>/dev/null || echo \"git-town not configured\"\n\nDETECT_REPO_EOF\n```\n\n### Step 0.5: Detect GitHub Account(s)\n\n```bash\n/usr/bin/env bash << 'DETECT_ACCOUNT_EOF'\necho \"=== GITHUB ACCOUNT DETECTION ===\"\n\n# Method 1: gh CLI auth status\necho \"--- gh CLI Account ---\"\nGH_USER=$(gh api user --jq '.login' 2>/dev/null || echo \"NONE\")\necho \"gh auth user: $GH_USER\"\n\n# Method 2: SSH config\necho \"--- SSH Config Hosts ---\"\ngrep -E \"^Host github\" ~/.ssh/config 2>/dev/null | head -5 || echo \"No GitHub SSH hosts\"\n\n# Method 3: Git global config\necho \"--- Git Global Config ---\"\ngit config --global user.name 2>/dev/null || echo \"No global user.name\"\ngit config --global user.email 2>/dev/null || echo \"No global user.email\"\n\n# Method 4: mise env (if available)\necho \"--- mise env ---\"\nmise env 2>/dev/null | grep -i github || echo \"No GitHub vars in mise\"\n\nDETECT_ACCOUNT_EOF\n```\n\n---\n\n## Phase 1: GATE  Present Findings\n\n**MANDATORY: Present ALL detection results and get explicit user confirmation.**\n\n### Step 1.1: Synthesize Findings\n\nCreate a summary table of detected state:\n\n| Aspect | Detected Value | Status |\n|--------|----------------|--------|\n| Repository | {owner}/{repo} | / |\n| Origin remote | {url} | / |\n| Upstream remote | {url} | //MISSING |\n| GitHub account | {username} | / |\n| git-town configured | yes/no | / |\n\n### Step 1.2: Determine Workflow Type\n\n```\nAskUserQuestion with questions:\n- question: \"What fork workflow do you need?\"\n  header: \"Workflow\"\n  options:\n    - label: \"Fresh fork - Create new fork from upstream\"\n      description: \"You want to fork someone else's repo to contribute\"\n    - label: \"Fix existing - Reconfigure existing fork's remotes\"\n      description: \"Origin/upstream are misconfigured, need to fix\"\n    - label: \"Verify only - Check current setup is correct\"\n      description: \"Just validate, don't change anything\"\n  multiSelect: false\n```\n\n### Step 1.3: Confirm Remote URLs (if Fresh Fork)\n\n```\nAskUserQuestion with questions:\n- question: \"Confirm the upstream repository (the original you're forking FROM):\"\n  header: \"Upstream\"\n  options:\n    - label: \"{detected_upstream_owner}/{detected_upstream_repo} (Detected)\"\n      description: \"Detected from current remotes\"\n    - label: \"Enter different URL\"\n      description: \"I want to fork a different repository\"\n  multiSelect: false\n```\n\n### Step 1.4: Confirm Fork Destination\n\n```\nAskUserQuestion with questions:\n- question: \"Where should the fork be created?\"\n  header: \"Fork Owner\"\n  options:\n    - label: \"{gh_auth_user} (Your account - Recommended)\"\n      description: \"Fork to your personal GitHub account\"\n    - label: \"Organization account\"\n      description: \"Fork to a GitHub organization you have access to\"\n  multiSelect: false\n```\n\n### Step 1.5: Final Confirmation Gate\n\n```\nAskUserQuestion with questions:\n- question: \"Ready to proceed with fork setup?\"\n  header: \"Confirm\"\n  options:\n    - label: \"Yes, create/configure fork\"\n      description: \"Proceed with: upstream={upstream_url}, fork_owner={fork_owner}\"\n    - label: \"No, abort\"\n      description: \"Cancel and make no changes\"\n  multiSelect: false\n```\n\n**If \"No, abort\": STOP. Do not proceed.**\n\n---\n\n## Phase 2: Execute Fork Setup\n\n### Step 2.1: Create Fork (if needed)\n\n**Only if fork doesn't exist:**\n\n```bash\n/usr/bin/env bash -c 'gh repo fork {upstream_owner}/{upstream_repo} --clone=false --remote=false'\n```\n\n**Validate:**\n```bash\n/usr/bin/env bash -c 'gh repo view {fork_owner}/{repo} --json url'\n```\n\n### Step 2.2: Configure Remotes\n\n**Set origin to fork (SSH preferred):**\n```bash\ngit remote set-url origin git@github.com:{fork_owner}/{repo}.git\n```\n\n**Add upstream (if missing):**\n```bash\ngit remote add upstream git@github.com:{upstream_owner}/{repo}.git\n```\n\n**Or fix upstream (if wrong):**\n```bash\ngit remote set-url upstream git@github.com:{upstream_owner}/{repo}.git\n```\n\n### Step 2.3: Initialize git-town\n\n```bash\n/usr/bin/env bash << 'INIT_GITTOWN_EOF'\n# Initialize git-town with fork settings\ngit town config setup\n\n# Ensure sync-upstream is enabled\ngit config git-town.sync-upstream true\n\n# Set dev-remote to origin (your fork)\ngit config git-town.dev-remote origin\n\nINIT_GITTOWN_EOF\n```\n\n---\n\n## Phase 3: Validation\n\n### Step 3.1: Verify Remote Configuration\n\n```bash\n/usr/bin/env bash << 'VALIDATE_REMOTES_EOF'\necho \"=== REMOTE VALIDATION ===\"\n\nORIGIN=$(git remote get-url origin)\nUPSTREAM=$(git remote get-url upstream)\n\necho \"origin: $ORIGIN\"\necho \"upstream: $UPSTREAM\"\n\n# Validate origin points to fork owner\nif [[ \"$ORIGIN\" =~ {fork_owner} ]]; then\n    echo \" origin correctly points to your fork\"\nelse\n    echo \" origin does NOT point to your fork\"\n    exit 1\nfi\n\n# Validate upstream points to original\nif [[ \"$UPSTREAM\" =~ {upstream_owner} ]]; then\n    echo \" upstream correctly points to original repo\"\nelse\n    echo \" upstream does NOT point to original repo\"\n    exit 1\nfi\n\nVALIDATE_REMOTES_EOF\n```\n\n### Step 3.2: Verify git-town Configuration\n\n```bash\n/usr/bin/env bash -c 'git town config'\n```\n\n**Expected output should show:**\n- `sync-upstream: true`\n- `dev-remote: origin`\n\n### Step 3.3: Test git-town Sync\n\n```\nAskUserQuestion with questions:\n- question: \"Run a test sync to verify everything works?\"\n  header: \"Test\"\n  options:\n    - label: \"Yes, run git town sync --dry-run\"\n      description: \"Preview what sync would do (safe)\"\n    - label: \"Yes, run git town sync for real\"\n      description: \"Actually sync branches\"\n    - label: \"Skip test\"\n      description: \"I'll test manually later\"\n  multiSelect: false\n```\n\nIf test selected:\n```bash\ngit town sync --dry-run  # or without --dry-run\n```\n\n### Step 3.4: Display Workflow Cheatsheet\n\n**Always display at end:**\n\n```markdown\n##  Fork Workflow Configured Successfully\n\n### Daily Commands (USE THESE, NOT RAW GIT)\n\n| Task | Command |\n|------|---------|\n| Create feature branch | `git town hack feature-name` |\n| Update all branches | `git town sync` |\n| Create PR to upstream | `git town propose` |\n| Merge approved PR | `git town ship` |\n| Switch branches | `git town switch` |\n\n###  FORBIDDEN (Will Break Workflow)\n\n|  Never Use |  Use Instead |\n|--------------|----------------|\n| `git checkout -b` | `git town hack` |\n| `git pull` | `git town sync` |\n| `git merge` | `git town sync` or `git town ship` |\n| `git push origin main` | `git town sync` |\n\n### Quick Reference\n\n- **Sync with upstream**: `git town sync` (automatic)\n- **Create stacked branches**: `git town append child-feature`\n- **Undo last git-town command**: `git town undo`\n- **See branch hierarchy**: `git town branch`\n```\n\n---\n\n## Error Handling\n\n### If Fork Creation Fails\n\n```\nAskUserQuestion with questions:\n- question: \"Fork creation failed. How to proceed?\"\n  header: \"Error\"\n  options:\n    - label: \"Retry\"\n      description: \"Try creating the fork again\"\n    - label: \"Fork exists - configure existing\"\n      description: \"Fork already exists, just configure remotes\"\n    - label: \"Abort\"\n      description: \"Cancel and investigate manually\"\n  multiSelect: false\n```\n\n### If Remote Configuration Fails\n\nDisplay the error and provide manual commands:\n```bash\n# Manual fix commands:\ngit remote set-url origin git@github.com:{fork_owner}/{repo}.git\ngit remote add upstream git@github.com:{upstream_owner}/{repo}.git\n```\n\n---\n\n## Arguments\n\n- `[upstream-url]` - Optional: URL of repository to fork\n- `--check` - Only run validation, don't make changes\n- `--fix` - Auto-fix detected issues without prompting\n\n## Examples\n\n```bash\n# Fork a new repository\n/git-town-workflow:fork https://github.com/EonLabs-Spartan/alpha-forge\n\n# Check existing fork setup\n/git-town-workflow:fork --check\n\n# Auto-fix misconfigured remotes\n/git-town-workflow:fork --fix\n```"
              },
              {
                "name": "/hooks",
                "description": "Install/uninstall hooks that enforce git-town over raw git commands in Claude Code. Blocks forbidden git commands. TRIGGERS - enforce git-town, install hooks, git-town hooks, prevent raw git.",
                "path": "plugins/git-town-workflow/commands/hooks.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(cat:*), Bash(jq:*), Grep, Glob, AskUserQuestion",
                  "argument-hint": "[install|uninstall|status]",
                  "description": "Install/uninstall hooks that enforce git-town over raw git commands in Claude Code. Blocks forbidden git commands. TRIGGERS - enforce git-town, install hooks, git-town hooks, prevent raw git."
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n# Git-Town Enforcement Hooks  Installation\n\n**This command installs Claude Code hooks that BLOCK forbidden raw git commands.**\n\n## What Gets Blocked\n\n| Forbidden Command | Reason | Replacement |\n|-------------------|--------|-------------|\n| `git checkout -b` | Creates untracked branches | `git town hack` |\n| `git pull` | Bypasses sync workflow | `git town sync` |\n| `git merge` | Manual merges break flow | `git town sync` |\n| `git push origin main` | Direct main push dangerous | `git town sync` |\n| `git branch -d` | Manual branch deletion | `git town delete` |\n| `git rebase` | Complex, use git-town | `git town sync` |\n\n## What's Allowed\n\n| Allowed Command | Reason |\n|-----------------|--------|\n| `git add` | Staging files (git-town doesn't replace) |\n| `git commit` | Creating commits (git-town doesn't replace) |\n| `git status` | Viewing status (read-only) |\n| `git log` | Viewing history (read-only) |\n| `git diff` | Viewing changes (read-only) |\n| `git stash` | Stashing changes (utility) |\n| `git remote` | Remote management (setup only) |\n| `git config` | Configuration (setup only) |\n\n---\n\n## Hook Definition\n\nThe following hook will be added to `~/.claude/settings.json`:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/usr/bin/env bash -c 'CMD=\\\"$CLAUDE_TOOL_INPUT_command\\\"; case \\\"$CMD\\\" in \\\"git checkout -b\\\"*|\\\"git checkout -B\\\"*) echo \\\"BLOCKED: Use git town hack instead of git checkout -b\\\"; exit 1;; \\\"git pull\\\"*) echo \\\"BLOCKED: Use git town sync instead of git pull\\\"; exit 1;; \\\"git merge\\\"*) echo \\\"BLOCKED: Use git town sync or git town ship instead of git merge\\\"; exit 1;; \\\"git push origin main\\\"*|\\\"git push origin master\\\"*) echo \\\"BLOCKED: Use git town sync instead of pushing to main\\\"; exit 1;; \\\"git branch -d\\\"*|\\\"git branch -D\\\"*) echo \\\"BLOCKED: Use git town delete instead of git branch -d\\\"; exit 1;; \\\"git rebase\\\"*) echo \\\"BLOCKED: Use git town sync (rebase strategy) instead of git rebase\\\"; exit 1;; esac'\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Installation\n\n### Step 1: Check Current Settings\n\n```bash\n/usr/bin/env bash -c 'cat ~/.claude/settings.json 2>/dev/null || echo \"{}\"'\n```\n\n### Step 2: AskUserQuestion Confirmation\n\n```\nAskUserQuestion with questions:\n- question: \"Install git-town enforcement hooks to block forbidden raw git commands?\"\n  header: \"Install Hooks\"\n  options:\n    - label: \"Yes, install hooks (Recommended)\"\n      description: \"Blocks: git checkout -b, git pull, git merge, git push main\"\n    - label: \"No, don't install\"\n      description: \"I want to use raw git commands freely\"\n    - label: \"Show what will be blocked\"\n      description: \"Display full list of blocked commands\"\n  multiSelect: false\n```\n\n### Step 3: Merge Hook into Settings\n\n**Read existing settings, merge hooks, write back:**\n\n```bash\n/usr/bin/env bash << 'INSTALL_HOOK_EOF'\nSETTINGS_FILE=\"$HOME/.claude/settings.json\"\n\n# Create file if doesn't exist\nif [[ ! -f \"$SETTINGS_FILE\" ]]; then\n    echo '{}' > \"$SETTINGS_FILE\"\nfi\n\n# Read existing settings\nEXISTING=$(cat \"$SETTINGS_FILE\")\n\n# Define the new hook\nNEW_HOOK='{\n  \"matcher\": \"Bash\",\n  \"hooks\": [\n    {\n      \"type\": \"command\",\n      \"command\": \"/usr/bin/env bash -c '\\''CMD=\\\"$CLAUDE_TOOL_INPUT_command\\\"; case \\\"$CMD\\\" in \\\"git checkout -b\\\"*|\\\"git checkout -B\\\"*) echo \\\"BLOCKED: Use git town hack instead of git checkout -b\\\"; exit 1;; \\\"git pull\\\"*) echo \\\"BLOCKED: Use git town sync instead of git pull\\\"; exit 1;; \\\"git merge\\\"*) echo \\\"BLOCKED: Use git town sync or git town ship instead of git merge\\\"; exit 1;; \\\"git push origin main\\\"*|\\\"git push origin master\\\"*) echo \\\"BLOCKED: Use git town sync instead of pushing to main\\\"; exit 1;; \\\"git branch -d\\\"*|\\\"git branch -D\\\"*) echo \\\"BLOCKED: Use git town delete instead of git branch -d\\\"; exit 1;; \\\"git rebase\\\"*) echo \\\"BLOCKED: Use git town sync (rebase strategy) instead of git rebase\\\"; exit 1;; esac'\\''\"\n    }\n  ]\n}'\n\n# Merge using jq\necho \"$EXISTING\" | jq --argjson hook \"$NEW_HOOK\" '\n  .hooks.PreToolUse = ((.hooks.PreToolUse // []) + [$hook] | unique_by(.matcher + (.hooks[0].command // \"\")))\n' > \"$SETTINGS_FILE.tmp\" && mv \"$SETTINGS_FILE.tmp\" \"$SETTINGS_FILE\"\n\necho \" Hook installed successfully\"\ncat \"$SETTINGS_FILE\" | jq '.hooks'\n\nINSTALL_HOOK_EOF\n```\n\n### Step 4: Verify Installation\n\n```bash\n/usr/bin/env bash -c 'cat ~/.claude/settings.json | jq \".hooks.PreToolUse\"'\n```\n\n---\n\n## Uninstallation\n\n### Step 1: Confirm Uninstall\n\n```\nAskUserQuestion with questions:\n- question: \"Remove git-town enforcement hooks?\"\n  header: \"Uninstall\"\n  options:\n    - label: \"Yes, remove hooks\"\n      description: \"Allow raw git commands again\"\n    - label: \"No, keep hooks\"\n      description: \"Keep enforcement active\"\n  multiSelect: false\n```\n\n### Step 2: Remove Hook\n\n```bash\n/usr/bin/env bash << 'UNINSTALL_HOOK_EOF'\nSETTINGS_FILE=\"$HOME/.claude/settings.json\"\n\nif [[ ! -f \"$SETTINGS_FILE\" ]]; then\n    echo \"No settings file found\"\n    exit 0\nfi\n\n# Remove git-town enforcement hook\ncat \"$SETTINGS_FILE\" | jq '\n  .hooks.PreToolUse = [.hooks.PreToolUse[]? | select(.matcher != \"Bash\" or (.hooks[0].command | contains(\"git town\") | not))]\n' > \"$SETTINGS_FILE.tmp\" && mv \"$SETTINGS_FILE.tmp\" \"$SETTINGS_FILE\"\n\necho \" Hook removed successfully\"\n\nUNINSTALL_HOOK_EOF\n```\n\n---\n\n## Status Check\n\n### Show Current Hook Status\n\n```bash\n/usr/bin/env bash << 'STATUS_HOOK_EOF'\nSETTINGS_FILE=\"$HOME/.claude/settings.json\"\n\necho \"=== GIT-TOWN ENFORCEMENT HOOK STATUS ===\"\n\nif [[ ! -f \"$SETTINGS_FILE\" ]]; then\n    echo \" No settings file found\"\n    echo \"   Run: /git-town-workflow:hooks install\"\n    exit 0\nfi\n\n# Check if hook exists\nHOOK_EXISTS=$(cat \"$SETTINGS_FILE\" | jq '[.hooks.PreToolUse[]? | select(.hooks[0].command | contains(\"git town\"))] | length')\n\nif [[ \"$HOOK_EXISTS\" -gt 0 ]]; then\n    echo \" Git-town enforcement hook is ACTIVE\"\n    echo \"\"\n    echo \"Blocked commands:\"\n    echo \"  - git checkout -b  use git town hack\"\n    echo \"  - git pull  use git town sync\"\n    echo \"  - git merge  use git town sync\"\n    echo \"  - git push origin main  use git town sync\"\n    echo \"  - git branch -d  use git town delete\"\n    echo \"  - git rebase  use git town sync\"\nelse\n    echo \" Git-town enforcement hook is NOT installed\"\n    echo \"   Run: /git-town-workflow:hooks install\"\nfi\n\nSTATUS_HOOK_EOF\n```\n\n---\n\n## Arguments\n\n- `install` - Install enforcement hooks\n- `uninstall` - Remove enforcement hooks\n- `status` - Show current hook status\n\n## Examples\n\n```bash\n# Install hooks\n/git-town-workflow:hooks install\n\n# Check status\n/git-town-workflow:hooks status\n\n# Remove hooks\n/git-town-workflow:hooks uninstall\n```"
              },
              {
                "name": "/setup",
                "description": "Initialize git-town in current repository with fork-aware configuration. One-time setup. TRIGGERS - git-town setup, initialize git-town, configure git-town, git town init.",
                "path": "plugins/git-town-workflow/commands/setup.md",
                "frontmatter": {
                  "allowed-tools": "Read, Write, Edit, Bash(git town:*), Bash(git config:*), Bash(git remote:*), Bash(which:*), Bash(brew:*), Bash(gh:*), Grep, Glob, AskUserQuestion, TodoWrite",
                  "argument-hint": "[--check]",
                  "description": "Initialize git-town in current repository with fork-aware configuration. One-time setup. TRIGGERS - git-town setup, initialize git-town, configure git-town, git town init."
                },
                "content": "<!--  MANDATORY: READ THIS ENTIRE FILE BEFORE ANY ACTION  -->\n\n# Git-Town Setup  One-Time Configuration\n\n**Run this ONCE per repository to configure git-town.**\n\n## Prerequisites\n\n1. git-town installed (`brew install git-town`)\n2. GitHub CLI authenticated (`gh auth login`)\n3. Repository cloned with remotes configured\n\n---\n\n## Phase 0: Preflight\n\n### Step 0.1: Create TodoWrite\n\n```\nTodoWrite with todos:\n- \"[Setup] Check git-town installation\" | in_progress\n- \"[Setup] Check GitHub CLI\" | pending\n- \"[Setup] Detect repository configuration\" | pending\n- \"[Setup] GATE - Confirm setup options\" | pending\n- \"[Setup] Run git-town interactive setup\" | pending\n- \"[Setup] Configure fork-specific settings\" | pending\n- \"[Setup] Verify configuration\" | pending\n- \"[Setup] Install enforcement hooks\" | pending\n```\n\n### Step 0.2: Check Dependencies\n\n```bash\n/usr/bin/env bash << 'CHECK_DEPS_EOF'\necho \"=== DEPENDENCY CHECK ===\"\n\n# git-town\nif which git-town &>/dev/null; then\n    echo \" git-town: $(git-town --version)\"\nelse\n    echo \" git-town: NOT INSTALLED\"\n    echo \"   Run: brew install git-town\"\nfi\n\n# gh CLI\nif which gh &>/dev/null; then\n    echo \" gh CLI: $(gh --version | head -1)\"\n    if gh auth status &>/dev/null; then\n        echo \" gh auth: authenticated\"\n    else\n        echo \" gh auth: NOT authenticated\"\n        echo \"   Run: gh auth login\"\n    fi\nelse\n    echo \" gh CLI: NOT INSTALLED\"\n    echo \"   Run: brew install gh\"\nfi\n\n# git\necho \" git: $(git --version)\"\n\nCHECK_DEPS_EOF\n```\n\n### Step 0.3: Detect Repository\n\n```bash\n/usr/bin/env bash << 'DETECT_REPO_EOF'\necho \"=== REPOSITORY DETECTION ===\"\n\n# Check if in git repo\nif ! git rev-parse --git-dir &>/dev/null; then\n    echo \" FATAL: Not in a git repository\"\n    exit 1\nfi\n\n# Remotes\necho \"--- Remotes ---\"\ngit remote -v\n\n# Branches\necho \"--- Branches ---\"\ngit branch -a | head -10\n\n# Current git-town config\necho \"--- Current git-town config ---\"\ngit town config 2>/dev/null || echo \"(not configured)\"\n\n# Main branch detection\necho \"--- Main branch ---\"\ngit config init.defaultBranch 2>/dev/null || echo \"main (default)\"\n\nDETECT_REPO_EOF\n```\n\n---\n\n## Phase 1: GATE  Configuration Options\n\n```\nAskUserQuestion with questions:\n- question: \"How should git-town sync branches?\"\n  header: \"Sync Strategy\"\n  options:\n    - label: \"Merge (Recommended for most teams)\"\n      description: \"git town sync uses merge commits\"\n    - label: \"Rebase (Clean history)\"\n      description: \"git town sync uses rebase\"\n  multiSelect: false\n```\n\n```\nAskUserQuestion with questions:\n- question: \"What's the main branch in this repository?\"\n  header: \"Main Branch\"\n  options:\n    - label: \"main\"\n      description: \"Modern default\"\n    - label: \"master\"\n      description: \"Legacy default\"\n    - label: \"Other\"\n      description: \"Custom main branch name\"\n  multiSelect: false\n```\n\n```\nAskUserQuestion with questions:\n- question: \"Is this a fork of another repository?\"\n  header: \"Fork Setup\"\n  options:\n    - label: \"Yes, configure fork workflow\"\n      description: \"Enable upstream sync, set dev-remote to origin\"\n    - label: \"No, single-origin repository\"\n      description: \"Standard setup without upstream\"\n  multiSelect: false\n```\n\n---\n\n## Phase 2: Run git-town Setup\n\n### Step 2.1: Interactive Setup (if preferred)\n\n```bash\ngit town config setup\n```\n\n### Step 2.2: Programmatic Setup\n\n```bash\n/usr/bin/env bash << 'SETUP_EOF'\n# Set main branch\ngit config git-town.main-branch main  # or master\n\n# Set sync strategy\ngit config git-town.sync-feature-strategy merge  # or rebase\n\n# Enable push for new branches\ngit config git-town.push-new-branches true\n\n# Set push hook (prompt before push)\ngit config git-town.push-hook true\n\n# For forks: enable upstream sync\nif git remote get-url upstream &>/dev/null; then\n    git config git-town.sync-upstream true\n    git config git-town.dev-remote origin\n    echo \" Fork workflow configured\"\nfi\n\nSETUP_EOF\n```\n\n---\n\n## Phase 3: Verify Configuration\n\n```bash\n/usr/bin/env bash -c 'git town config'\n```\n\n**Expected output for fork workflow:**\n```\nBranches:\n  main branch: main\n  perennial branches: (none)\n  ...\n\nHosting:\n  hosting platform: github\n  dev-remote: origin\n  ...\n\nSync:\n  sync-feature-strategy: merge\n  sync-upstream: true\n  ...\n```\n\n---\n\n## Phase 4: Install Enforcement Hooks\n\n```\nAskUserQuestion with questions:\n- question: \"Install Claude Code hooks to enforce git-town usage?\"\n  header: \"Enforcement\"\n  options:\n    - label: \"Yes, install hooks (Recommended)\"\n      description: \"Blocks: git checkout -b, git pull, git merge\"\n    - label: \"No, skip hooks\"\n      description: \"Allow raw git commands\"\n  multiSelect: false\n```\n\nIf \"Yes\": Run `/git-town-workflow:hooks install`\n\n---\n\n## Post-Setup Checklist\n\n```\n git-town installed and configured\n Main branch identified\n Sync strategy set\n Fork workflow configured (if applicable)\n Enforcement hooks installed (optional)\n\nNext steps:\n- Start contributing: /git-town-workflow:contribute feat/my-feature\n- View branch hierarchy: git town branch\n- Sync all branches: git town sync --all\n```\n\n---\n\n## Arguments\n\n- `--check` - Only verify current setup, don't change anything\n\n## Examples\n\n```bash\n# Full setup wizard\n/git-town-workflow:setup\n\n# Check current configuration\n/git-town-workflow:setup --check\n```"
              }
            ],
            "skills": []
          }
        ]
      }
    }
  ]
}