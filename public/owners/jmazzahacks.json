{
  "owner": {
    "id": "jmazzahacks",
    "display_name": "Jason Byteforgia",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/3486?u=63d805c70c3ae911413ed5ed50f0768fc82f1d91&v=4",
    "url": "https://github.com/jmazzahacks",
    "bio": "Code, crypto & podcasts.  Just another night_city_net_runner.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 5,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "jmazzahacks/byteforge-claude-skills",
      "url": "https://github.com/jmazzahacks/byteforge-claude-skills",
      "description": "Collection of Claude Code skills that codify best practices and reusable patterns for software development",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-12-18T13:23:38Z",
        "created_at": "2025-10-23T15:42:54Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 832
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 466
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 63
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 4240
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 10844
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/flask-docker-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/flask-docker-deployment/SKILL.md",
          "type": "blob",
          "size": 14859
        },
        {
          "path": "skills/flask-smorest-api",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/flask-smorest-api/SKILL.md",
          "type": "blob",
          "size": 16633
        },
        {
          "path": "skills/mz-configure-loki-logging",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/mz-configure-loki-logging/SKILL.md",
          "type": "blob",
          "size": 9138
        },
        {
          "path": "skills/postgres-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/postgres-setup/SKILL.md",
          "type": "blob",
          "size": 14083
        },
        {
          "path": "skills/python-pypi-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/python-pypi-setup/SKILL.md",
          "type": "blob",
          "size": 13425
        }
      ],
      "marketplace": {
        "name": "byteforge-claude-skills",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Jason Byteforge",
          "email": "jason@byteforge.dev"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "byteforge-skills",
            "description": "Collection of development setup skills including PostgreSQL database setup, Python PyPI package publishing, Flask REST API with OpenAPI docs and dataclass models, Flask Docker deployment, and Mazza-specific Grafana Loki logging configuration",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add jmazzahacks/byteforge-claude-skills",
              "/plugin install byteforge-skills@byteforge-claude-skills"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-18T13:23:38Z",
              "created_at": "2025-10-23T15:42:54Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "flask-docker-deployment",
                "description": "Set up Docker deployment for Flask applications with Gunicorn, automated versioning, and container registry publishing. Use when dockerizing a Flask app, containerizing for production, or setting up CI/CD with Docker.",
                "path": "skills/flask-docker-deployment/SKILL.md",
                "frontmatter": {
                  "name": "flask-docker-deployment",
                  "description": "Set up Docker deployment for Flask applications with Gunicorn, automated versioning, and container registry publishing. Use when dockerizing a Flask app, containerizing for production, or setting up CI/CD with Docker."
                },
                "content": "# Flask Docker Deployment Pattern\n\nThis skill helps you containerize Flask applications using Docker with Gunicorn for production, automated version management, and seamless container registry publishing.\n\n## When to Use This Skill\n\nUse this skill when:\n- You have a Flask application ready to deploy\n- You want production-grade containerization with Gunicorn\n- You need automated version management for builds\n- You're publishing to a container registry (Docker Hub, GHCR, ECR, etc.)\n- You want a repeatable, idempotent deployment pipeline\n\n## What This Skill Creates\n\n1. **Dockerfile** - Multi-stage production-ready container with security best practices\n2. **build-publish.sh** - Automated build script with version management\n3. **VERSION** file - Auto-incrementing version tracking (gitignored)\n4. **.gitignore** - Entry for VERSION file\n5. **Optional .dockerignore** - Exclude unnecessary files from build context\n\n## Prerequisites\n\nBefore using this skill, ensure:\n1. Flask application is working locally\n2. `requirements.txt` exists with all dependencies\n3. Docker is installed and running\n4. You're authenticated to your container registry (if publishing)\n\n## Step 1: Gather Project Information\n\n**IMPORTANT**: Before creating files, ask the user these questions:\n\n1. **\"What is your Flask application entry point?\"**\n   - Format: `{module_name}:{app_variable}`\n   - Example: `hyperopt_daemon:app` or `api_server:create_app()`\n\n2. **\"What port does your Flask app use?\"**\n   - Default: 5000\n   - Example: 5678, 8080, 3000\n\n3. **\"What is your container registry URL?\"**\n   - Examples:\n     - GitHub: `ghcr.io/{org}/{project}`\n     - Docker Hub: `docker.io/{user}/{project}`\n     - AWS ECR: `{account}.dkr.ecr.{region}.amazonaws.com/{project}`\n\n4. **\"Do you have private Git dependencies?\"** (yes/no)\n   - If yes: Will need GitHub Personal Access Token (CR_PAT)\n   - If no: Can skip git installation step\n\n5. **\"How many Gunicorn workers do you want?\"**\n   - Default: 4\n   - Recommendation: 2-4 × CPU cores\n   - Note: For background job workers, use 1\n\n## Step 2: Create Dockerfile\n\nCreate `Dockerfile` in the project root:\n\n```dockerfile\nFROM python:3.13-slim\n\n# Build argument for GitHub Personal Access Token (if needed for private deps)\nARG CR_PAT\nENV CR_PAT=${CR_PAT}\n\n# Install curl (for health checks) and git (for private GitHub dependencies)\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\n# Copy requirements and install dependencies\nCOPY requirements.txt .\n\n# Configure git to use PAT for GitHub access (if private deps)\nRUN git config --global url.\"https://${CR_PAT}@github.com/\".insteadOf \"https://github.com/\" \\\n    && pip install --no-cache-dir -r requirements.txt \\\n    && git config --global --unset url.\"https://${CR_PAT}@github.com/\".insteadOf\n\n# Copy application code\nCOPY . .\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash appuser\nRUN chown -R appuser:appuser /app\nUSER appuser\n\n# Expose the application port\nEXPOSE {port}\n\n# Set environment variables\nENV PYTHONPATH=/app\nENV PORT={port}\n\n# Run with gunicorn for production\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:{port}\", \"--workers\", \"{workers}\", \"{module}:{app}\"]\n```\n\n**CRITICAL Replacements:**\n- `{port}` → Application port (e.g., 5678)\n- `{workers}` → Number of workers (e.g., 4, or 1 for background jobs)\n- `{module}` → Python module name (e.g., hyperopt_daemon)\n- `{app}` → App variable name (e.g., app or create_app())\n\n**If NO private dependencies**, remove these lines:\n```dockerfile\n# Remove ARG CR_PAT, ENV CR_PAT, git installation, and git config commands\n```\n\nSimplified version without private deps:\n```dockerfile\nFROM python:3.13-slim\n\n# Install curl for health checks\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nRUN useradd --create-home --shell /bin/bash appuser\nRUN chown -R appuser:appuser /app\nUSER appuser\n\nEXPOSE {port}\nENV PYTHONPATH=/app\nENV PORT={port}\n\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:{port}\", \"--workers\", \"{workers}\", \"{module}:{app}\"]\n```\n\n## Step 3: Create build-publish.sh Script\n\nCreate `build-publish.sh` in the project root:\n\n```bash\n#!/bin/sh\n\n# VERSION file path\nVERSION_FILE=\"VERSION\"\n\n# Parse command line arguments\nNO_CACHE=\"\"\nif [ \"$1\" = \"--no-cache\" ]; then\n    NO_CACHE=\"--no-cache\"\n    echo \"Building with --no-cache flag\"\nfi\n\n# Check if VERSION file exists, if not create it with version 1\nif [ ! -f \"$VERSION_FILE\" ]; then\n    echo \"1\" > \"$VERSION_FILE\"\n    echo \"Created VERSION file with initial version 1\"\nfi\n\n# Read current version from file\nCURRENT_VERSION=$(cat \"$VERSION_FILE\" 2>/dev/null)\n\n# Validate that the version is a number\nif ! echo \"$CURRENT_VERSION\" | grep -qE '^[0-9]+$'; then\n    echo \"Error: Invalid version format in $VERSION_FILE. Expected a number, got: $CURRENT_VERSION\"\n    exit 1\nfi\n\n# Increment version\nVERSION=$((CURRENT_VERSION + 1))\n\necho \"Building version $VERSION (incrementing from $CURRENT_VERSION)\"\n\n# Build the image with optional --no-cache flag\ndocker build $NO_CACHE --build-arg CR_PAT=$CR_PAT --platform linux/amd64 -t {registry_url}:$VERSION .\n\n# Tag the same image as latest\ndocker tag {registry_url}:$VERSION {registry_url}:latest\n\n# Push both tags\ndocker push {registry_url}:$VERSION\ndocker push {registry_url}:latest\n\n# Update the VERSION file with the new version\necho \"$VERSION\" > \"$VERSION_FILE\"\necho \"Updated $VERSION_FILE to version $VERSION\"\n```\n\n**CRITICAL Replacements:**\n- `{registry_url}` → Full container registry URL (e.g., `ghcr.io/mazza-vc/hyperopt-server`)\n\n**If NO private dependencies**, remove `--build-arg CR_PAT=$CR_PAT`:\n```bash\ndocker build $NO_CACHE --platform linux/amd64 -t {registry_url}:$VERSION .\n```\n\nMake the script executable:\n```bash\nchmod +x build-publish.sh\n```\n\n## Step 4: Create Environment Configuration\n\n### File: `example.env`\n\nCreate or update `example.env` with required environment variables for running the containerized application:\n\n```bash\n# Server Configuration\nPORT={port}\n\n# Database Configuration (if applicable)\n{PROJECT_NAME}_DB_HOST=localhost\n{PROJECT_NAME}_DB_NAME={project_name}\n{PROJECT_NAME}_DB_USER={project_name}\n{PROJECT_NAME}_DB_PASSWORD=your_password_here\n\n# Build Configuration (for private dependencies)\nCR_PAT=your_github_personal_access_token\n\n# Optional: Additional app-specific variables\nDEBUG=False\nLOG_LEVEL=INFO\n```\n\n**CRITICAL**: Replace:\n- `{port}` → Application port (e.g., 5678)\n- `{PROJECT_NAME}` → Uppercase project name (e.g., \"HYPEROPT_SERVER\")\n- `{project_name}` → Snake case project name (e.g., \"hyperopt_server\")\n\n**Note:** Remove CR_PAT if you don't have private dependencies.\n\n### Update .gitignore\n\nAdd VERSION file and .env to `.gitignore`:\n\n```gitignore\n# Environment variables\n.env\n\n# Version file (used by build system, not tracked)\nVERSION\n```\n\nThis prevents the VERSION file and environment secrets from being committed.\n\n## Step 5: Create .dockerignore (Optional but Recommended)\n\nCreate `.dockerignore` to exclude unnecessary files from Docker build context:\n\n```\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\n.venv/\nENV/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Environment files (secrets should not be in image)\n.env\n*.env\n!example.env\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n.tox/\n\n# IDEs\n.vscode/\n.idea/\n*.swp\n*.swo\n*~\n\n# Git\n.git/\n.gitignore\n\n# CI/CD\n.github/\n\n# Documentation\n*.md\ndocs/\n\n# Build artifacts\nVERSION\n*.log\n\n# OS\n.DS_Store\nThumbs.db\n```\n\n## Step 6: Usage Instructions\n\n### Setup\n\n```bash\n# Copy example environment file and configure\ncp example.env .env\n# Edit .env and fill in actual values\n```\n\n### Building and Publishing\n\n**Load environment variables** (if using .env):\n```bash\n# Export variables from .env for build process\nset -a\nsource .env\nset +a\n```\n\n**Standard build** (increments version, uses cache):\n```bash\n./build-publish.sh\n```\n\n**Fresh build** (no cache, pulls latest dependencies):\n```bash\n./build-publish.sh --no-cache\n```\n\n### Running the Container\n\n**Using environment file:**\n```bash\ndocker run -p {port}:{port} \\\n  --env-file .env \\\n  {registry_url}:latest\n```\n\n**Using explicit environment variables:**\n```bash\ndocker run -p {port}:{port} \\\n  -e PORT={port} \\\n  -e {PROJECT_NAME}_DB_PASSWORD=secret \\\n  -e {PROJECT_NAME}_DB_HOST=db.example.com \\\n  {registry_url}:latest\n```\n\n### Local Testing\n\nTest the container locally before publishing:\n```bash\n# Build without pushing\ndocker build --platform linux/amd64 -t {project}:test .\n\n# Run locally\ndocker run -p {port}:{port} {project}:test\n\n# Test the endpoint\ncurl http://localhost:{port}/health\n```\n\n## Design Principles\n\nThis pattern follows these principles:\n\n### Security:\n1. **Non-root user** - Container runs as unprivileged user\n2. **Minimal base image** - python:3.11-slim reduces attack surface\n3. **Build-time secrets** - CR_PAT only available during build, not in final image\n4. **Explicit permissions** - chown ensures correct file ownership\n\n### Reliability:\n1. **Gunicorn workers** - Production-grade WSGI server with process management\n2. **Platform specification** - `--platform linux/amd64` ensures compatibility\n3. **Version tracking** - Auto-incrementing versions for rollback capability\n4. **Immutable builds** - Each version is reproducible\n\n### Performance:\n1. **Layer caching** - Dependencies cached separately from code\n2. **No-cache option** - Force fresh builds when needed\n3. **Slim base image** - Faster pulls and smaller storage\n4. **Multi-worker** - Concurrent request handling\n\n### DevOps:\n1. **Automated versioning** - No manual version management\n2. **Dual tagging** - Both version and latest tags for flexibility\n3. **Idempotent builds** - Safe to run multiple times\n4. **Simple CLI** - Single script handles build and publish\n\n## Common Patterns\n\n### Pattern 1: Standard Web API\n```dockerfile\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers\", \"4\", \"app:create_app()\"]\n```\n- Multiple workers for concurrent requests\n- Factory pattern with `create_app()`\n\n### Pattern 2: Background Job Worker\n```dockerfile\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5678\", \"--workers\", \"1\", \"daemon:app\"]\n```\n- Single worker to avoid job conflicts\n- Direct app instance\n\n### Pattern 3: High-Traffic API\n```dockerfile\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8080\", \"--workers\", \"8\", \"--timeout\", \"120\", \"api:app\"]\n```\n- More workers for higher concurrency\n- Increased timeout for long-running requests\n\n## Integration with Other Skills\n\n### flask-smorest-api Skill\nCreate the API first, then dockerize:\n```\n1. User: \"Set up Flask API server\"\n2. [flask-smorest-api skill runs]\n3. User: \"Now dockerize it\"\n4. [flask-docker-deployment skill runs]\n```\n\n### postgres-setup Skill\nFor database-dependent apps:\n```dockerfile\n# Add psycopg2-binary to requirements.txt\n# Set database env vars in docker run:\ndocker run -e DB_HOST=db.example.com -e DB_PASSWORD=secret ...\n```\n\n## Container Registry Setup\n\n### GitHub Container Registry (GHCR)\n\n**Login:**\n```bash\necho $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin\n```\n\n**Registry URL format:**\n```\nghcr.io/{org}/{project}\n```\n\n### Docker Hub\n\n**Login:**\n```bash\ndocker login docker.io\n```\n\n**Registry URL format:**\n```\ndocker.io/{username}/{project}\n```\n\n### AWS ECR\n\n**Login:**\n```bash\naws ecr get-login-password --region us-east-1 | \\\n  docker login --username AWS --password-stdin \\\n  {account}.dkr.ecr.us-east-1.amazonaws.com\n```\n\n**Registry URL format:**\n```\n{account}.dkr.ecr.{region}.amazonaws.com/{project}\n```\n\n## Troubleshooting\n\n### Build fails with \"permission denied\"\n```bash\nchmod +x build-publish.sh\n```\n\n### Private dependency installation fails\n```bash\n# Verify CR_PAT is set\necho $CR_PAT\n\n# Test GitHub access\ncurl -H \"Authorization: token $CR_PAT\" https://api.github.com/user\n```\n\n### Container won't start\n```bash\n# Check logs\ndocker logs {container_id}\n\n# Run interactively to debug\ndocker run -it {registry_url}:latest /bin/bash\n```\n\n### Version file conflicts\n```bash\n# If VERSION file gets corrupted, delete and rebuild\nrm VERSION\n./build-publish.sh\n```\n\n## Example: Complete Workflow\n\n**User:** \"Dockerize my Flask hyperopt server\"\n\n**Claude asks:**\n- Entry point? → `hyperopt_daemon:app`\n- Port? → `5678`\n- Registry? → `ghcr.io/mazza-vc/hyperopt-server`\n- Private deps? → `yes` (arcana-core)\n- Workers? → `1` (background job processor)\n\n**Claude creates:**\n1. `Dockerfile` with gunicorn, 1 worker, port 5678\n2. `build-publish.sh` with GHCR registry URL\n3. Adds `VERSION` to `.gitignore`\n4. Creates `.dockerignore`\n\n**User runs:**\n```bash\nexport CR_PAT=ghp_abc123\n./build-publish.sh\n```\n\n**Result:**\n- ✅ Builds `ghcr.io/mazza-vc/hyperopt-server:1`\n- ✅ Tags as `ghcr.io/mazza-vc/hyperopt-server:latest`\n- ✅ Pushes both tags\n- ✅ Updates VERSION to `1`\n\n**Subsequent builds:**\n```bash\n./build-publish.sh          # Builds version 2\n./build-publish.sh          # Builds version 3\n./build-publish.sh --no-cache  # Builds version 4 (fresh)\n```\n\n## Best Practices\n\n1. **Use --no-cache strategically** - Only when dependencies updated or debugging\n2. **Test locally first** - Build and run locally before pushing\n3. **Keep VERSION in .gitignore** - Let build system manage it\n4. **Use explicit versions** - Don't rely only on `latest` tag for production\n5. **Document env vars** - List all required environment variables in README\n6. **Health checks** - Add `/health` endpoint for container orchestration\n7. **Logging** - Configure logging to stdout for container logs\n8. **Resource limits** - Set memory/CPU limits in production deployment\n\n## Advanced: Multi-Stage Builds\n\nFor smaller images, use multi-stage builds:\n\n```dockerfile\n# Build stage\nFROM python:3.13-slim as builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\n# Runtime stage\nFROM python:3.13-slim\n\n# Install curl for health checks\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nENV PATH=/root/.local/bin:$PATH\nRUN useradd --create-home appuser && chown -R appuser:appuser /app\nUSER appuser\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers\", \"4\", \"app:app\"]\n```\n\nThis pattern:\n- Installs dependencies in builder stage\n- Copies only installed packages to runtime\n- Results in smaller final image\n- Includes curl for health checks"
              },
              {
                "name": "flask-smorest-api",
                "description": "Set up Flask REST API with flask-smorest, OpenAPI docs, blueprint architecture, and dataclass models. Use when creating a new Flask API server, building REST endpoints, or setting up a production API.",
                "path": "skills/flask-smorest-api/SKILL.md",
                "frontmatter": {
                  "name": "flask-smorest-api",
                  "description": "Set up Flask REST API with flask-smorest, OpenAPI docs, blueprint architecture, and dataclass models. Use when creating a new Flask API server, building REST endpoints, or setting up a production API."
                },
                "content": "# Flask REST API with flask-smorest Pattern\n\nThis skill helps you set up a Flask REST API following a standardized pattern with flask-smorest for OpenAPI documentation, blueprint architecture, and dataclass models for request/response handling.\n\n## When to Use This Skill\n\nUse this skill when:\n- Starting a new Flask REST API project\n- You want automatic OpenAPI/Swagger documentation\n- You need a clean, modular blueprint architecture\n- You want type-safe data models using dataclasses with to_dict/from_dict patterns\n- You're building a production-ready API server\n\n## What This Skill Creates\n\n1. **Main application file** - Flask app initialization with flask-smorest\n2. **Blueprint structure** - Modular endpoint organization\n3. **Data models** - Dataclasses with to_dict/from_dict methods and validation\n4. **Singleton manager pattern** - Centralized service/database initialization\n5. **CORS support** - Cross-origin request handling\n6. **Requirements file** - All necessary dependencies\n\n## Step 1: Gather Project Information\n\n**IMPORTANT**: Before creating files, ask the user these questions:\n\n1. **\"What is your project name?\"** (e.g., \"materia-server\", \"trading-api\", \"myapp\")\n   - Use this to derive:\n     - Main module: `{project_name}.py` (e.g., `materia_server.py`)\n     - Port number (suggest based on project, default: 5000)\n\n2. **\"What features/endpoints do you need?\"** (e.g., \"users\", \"tokens\", \"orders\")\n   - Each feature will become a blueprint\n\n3. **\"Do you need database integration?\"** (yes/no)\n   - If yes, reference postgres-setup skill for database layer\n\n4. **\"What port should the server run on?\"** (default: 5000)\n\n## Step 2: Create Directory Structure\n\nCreate these directories if they don't exist:\n```\n{project_root}/\n├── blueprints/          # Blueprint modules (one per feature)\n│   ├── __init__.py\n│   └── {feature}.py\n├── models/              # Dataclass models with to_dict/from_dict\n│   ├── __init__.py\n│   └── {feature}.py\n└── {project_name}.py    # Main application file\n```\n\n## Step 3: Create Main Application File\n\nCreate `{project_name}.py` using this template:\n\n```python\nimport os\nimport logging\nfrom flask import Flask\nfrom flask_cors import CORS\nfrom flask_smorest import Api\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_app():\n    app = Flask(__name__)\n    app.config['API_TITLE'] = '{Project Name} API'\n    app.config['API_VERSION'] = 'v1'\n    app.config['OPENAPI_VERSION'] = '3.0.2'\n    app.config['OPENAPI_URL_PREFIX'] = '/'\n    app.config['OPENAPI_SWAGGER_UI_PATH'] = '/swagger'\n    app.config['OPENAPI_SWAGGER_UI_URL'] = 'https://cdn.jsdelivr.net/npm/swagger-ui-dist/'\n\n    CORS(app)\n    api = Api(app)\n\n    from blueprints.{feature} import blp as {feature}_blp\n    api.register_blueprint({feature}_blp)\n\n    logger.info(\"Flask app initialized\")\n    return app\n\nif __name__ == '__main__':\n    port = int(os.environ.get('PORT', {port_number}))\n    app = create_app()\n    logger.info(f\"Swagger UI: http://localhost:{port}/swagger\")\n    app.run(host='0.0.0.0', port=port)\n```\n\n**CRITICAL**: Replace:\n- `{Project Name}` → Human-readable project name (e.g., \"Materia Server\")\n- `{project_name}` → Snake case project name (e.g., \"materia_server\")\n- `{port_number}` → Actual port number (e.g., 5151)\n- `{feature}` → Feature name from user's response\n\n## Step 4: Create Data Models\n\nFor each feature, create a models file with dataclasses that include `to_dict` and `from_dict` methods:\n\n### File: `models/{feature}.py`\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\n@dataclass\nclass {Feature}:\n    \"\"\"\n    {Feature} data model.\n\n    Includes validation in from_dict and serialization via to_dict.\n    \"\"\"\n    id: str\n    name: str\n    created_at: int\n    updated_at: Optional[int] = None\n\n    def to_dict(self) -> dict:\n        \"\"\"Serialize to dictionary for JSON response.\"\"\"\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"created_at\": self.created_at,\n            \"updated_at\": self.updated_at\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"{Feature}\":\n        \"\"\"\n        Create instance from dictionary with validation.\n\n        Args:\n            data: Dictionary with {feature} data\n\n        Returns:\n            {Feature} instance\n\n        Raises:\n            ValueError: If required fields are missing or invalid\n        \"\"\"\n        if \"id\" not in data:\n            raise ValueError(\"id is required\")\n        if \"name\" not in data:\n            raise ValueError(\"name is required\")\n        if \"created_at\" not in data:\n            raise ValueError(\"created_at is required\")\n\n        return cls(\n            id=str(data[\"id\"]),\n            name=str(data[\"name\"]),\n            created_at=int(data[\"created_at\"]),\n            updated_at=int(data[\"updated_at\"]) if data.get(\"updated_at\") else None\n        )\n```\n\n**CRITICAL**: Replace:\n- `{Feature}` → PascalCase feature name (e.g., \"TradableToken\")\n- `{feature}` → Snake case feature name (e.g., \"tradable_token\")\n\n## Step 5: Create Blueprint Files\n\nFor each feature/endpoint, create a blueprint file:\n\n### File: `blueprints/{feature}.py`\n\n```python\nimport logging\nfrom flask import request, jsonify\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint, abort\n\nfrom models.{feature} import {Feature}\n\nlogger = logging.getLogger(__name__)\n\nblp = Blueprint('{feature}', __name__, url_prefix='/api', description='{Feature} API')\n\n\n@blp.route('/{feature}')\nclass {Feature}ListResource(MethodView):\n    def get(self):\n        \"\"\"Get list of {feature}s.\"\"\"\n        try:\n            limit = request.args.get('limit', 100, type=int)\n            offset = request.args.get('offset', 0, type=int)\n\n            # TODO: Implement logic to fetch {feature}s\n            items = []\n\n            return jsonify({\n                \"data\": [item.to_dict() for item in items],\n                \"limit\": limit,\n                \"offset\": offset\n            })\n        except ValueError as e:\n            logger.warning(f\"Bad request: {e}\")\n            abort(400, message=str(e))\n        except Exception as e:\n            logger.exception(f\"Error fetching {feature}s: {e}\")\n            abort(500, message=\"Internal server error\")\n\n    def post(self):\n        \"\"\"Create a new {feature}.\"\"\"\n        try:\n            data = request.get_json()\n            if not data:\n                abort(400, message=\"Request body is required\")\n\n            item = {Feature}.from_dict(data)\n\n            # TODO: Implement logic to save {feature}\n\n            return jsonify(item.to_dict()), 201\n        except ValueError as e:\n            logger.warning(f\"Validation error: {e}\")\n            abort(400, message=str(e))\n        except Exception as e:\n            logger.exception(f\"Error creating {feature}: {e}\")\n            abort(500, message=\"Internal server error\")\n\n\n@blp.route('/{feature}/<string:item_id>')\nclass {Feature}Resource(MethodView):\n    def get(self, item_id: str):\n        \"\"\"Get a single {feature} by ID.\"\"\"\n        try:\n            # TODO: Implement logic to fetch {feature} by ID\n            item = None\n\n            if not item:\n                abort(404, message=f\"{Feature} not found: {item_id}\")\n\n            return jsonify(item.to_dict())\n        except Exception as e:\n            logger.exception(f\"Error fetching {feature}: {e}\")\n            abort(500, message=\"Internal server error\")\n\n    def put(self, item_id: str):\n        \"\"\"Update a {feature}.\"\"\"\n        try:\n            data = request.get_json()\n            if not data:\n                abort(400, message=\"Request body is required\")\n\n            # TODO: Implement logic to update {feature}\n\n            return jsonify({\"message\": \"Updated\"})\n        except ValueError as e:\n            logger.warning(f\"Validation error: {e}\")\n            abort(400, message=str(e))\n        except Exception as e:\n            logger.exception(f\"Error updating {feature}: {e}\")\n            abort(500, message=\"Internal server error\")\n\n    def delete(self, item_id: str):\n        \"\"\"Delete a {feature}.\"\"\"\n        try:\n            # TODO: Implement logic to delete {feature}\n\n            return jsonify({\"message\": \"Deleted\"})\n        except Exception as e:\n            logger.exception(f\"Error deleting {feature}: {e}\")\n            abort(500, message=\"Internal server error\")\n```\n\n**CRITICAL**: Replace:\n- `{Feature}` → PascalCase feature name (e.g., \"TradableToken\")\n- `{feature}` → Snake case feature name (e.g., \"tradable_token\")\n\n## Step 6: Create Common Singleton Manager (If Needed)\n\nIf the project needs shared services (database, API clients, etc.), create a singleton manager:\n\n### File: `common.py`\n\n```python\n\"\"\"\nSingleton manager for shared service instances.\n\nProvides centralized initialization of database connections, API clients,\nand other shared resources.\n\"\"\"\n\nimport os\nimport logging\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ServiceManager:\n    \"\"\"\n    Singleton manager for shared service instances.\n\n    Ensures only one instance of each service is created and reused\n    across all blueprints.\n    \"\"\"\n\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(ServiceManager, cls).__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n\n    def __init__(self):\n        if self._initialized:\n            return\n\n        # Initialize services\n        self._db = None\n        self._initialized = True\n        logger.info(\"ServiceManager initialized\")\n\n    def get_database(self):\n        \"\"\"\n        Get database connection instance.\n\n        Returns:\n            Database connection instance (lazy initialization)\n        \"\"\"\n        if self._db is None:\n            # Import database driver\n            from src.{project_name}.database import Database\n\n            # Get connection parameters from environment\n            db_host = os.environ.get('{PROJECT_NAME}_DB_HOST', 'localhost')\n            db_name = os.environ.get('{PROJECT_NAME}_DB_NAME', '{project_name}')\n            db_user = os.environ.get('{PROJECT_NAME}_DB_USER', '{project_name}')\n            db_passwd = os.environ.get('{PROJECT_NAME}_DB_PASSWORD')\n\n            if not db_passwd:\n                raise ValueError(\"{PROJECT_NAME}_DB_PASSWORD environment variable required\")\n\n            self._db = Database(db_host, db_name, db_user, db_passwd)\n            logger.info(\"Database connection initialized\")\n\n        return self._db\n\n\n# Global singleton instance\nservice_manager = ServiceManager()\n```\n\n**CRITICAL**: Replace:\n- `{PROJECT_NAME}` → Uppercase project name (e.g., \"MATERIA_SERVER\")\n- `{project_name}` → Snake case project name (e.g., \"materia_server\")\n\n## Step 7: Create Environment Configuration\n\n### File: `example.env`\n\nCreate or update `example.env` with required environment variables:\n\n```bash\n# Server Configuration\nPORT={port_number}\nDEBUG=False\n\n# Database Configuration (if applicable)\n{PROJECT_NAME}_DB_HOST=localhost\n{PROJECT_NAME}_DB_NAME={project_name}\n{PROJECT_NAME}_DB_USER={project_name}\n{PROJECT_NAME}_DB_PASSWORD=your_password_here\n\n# Optional: CORS Configuration\nALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080\n```\n\n**CRITICAL**: Replace:\n- `{port_number}` → Actual port number (e.g., 5151)\n- `{PROJECT_NAME}` → Uppercase project name (e.g., \"MATERIA_SERVER\")\n- `{project_name}` → Snake case project name (e.g., \"materia_server\")\n\n### File: `.env` (gitignored)\n\nInstruct the user to copy `example.env` to `.env` and fill in actual values:\n\n```bash\n# Copy example.env to .env and update with actual values\ncp example.env .env\n```\n\n### Update .gitignore\n\nAdd `.env` to `.gitignore` if not already present:\n\n```\n# Environment variables\n.env\n```\n\n## Step 8: Create Requirements File\n\nCreate `requirements.txt` with dependencies (no version pinning):\n\n```txt\n# Flask and API framework\nFlask\nflask-smorest\nflask-cors\n\n# Environment variable management\npython-dotenv\n\n# Production server (optional but recommended)\ngunicorn\n\n# Database (if needed)\npsycopg2-binary\n```\n\n## Step 9: Create Blueprints __init__.py\n\nCreate `blueprints/__init__.py`:\n\n```python\n\"\"\"\nBlueprint modules for {Project Name} API.\n\nEach blueprint represents a distinct feature or resource endpoint.\n\"\"\"\n```\n\n## Step 10: Document Usage\n\nCreate or update README.md with:\n\n### Setup\n\n```bash\n# Copy example environment file\ncp example.env .env\n\n# Edit .env and fill in actual values\n# Then install dependencies\npip install -r requirements.txt\n```\n\n### Running the Server\n\n```bash\n# Development mode\npython {project_name}.py\n\n# Production mode with Gunicorn\ngunicorn -w 4 -b 0.0.0.0:{port_number} '{project_name}:create_app()'\n```\n\n### Environment Variables\n\nCopy `example.env` to `.env` and configure:\n\n**Server Configuration:**\n- `PORT` - Server port (default: {port_number})\n- `DEBUG` - Enable debug mode (default: False)\n\n**Database (if applicable):**\n- `{PROJECT_NAME}_DB_HOST` - Database host (default: localhost)\n- `{PROJECT_NAME}_DB_NAME` - Database name (default: {project_name})\n- `{PROJECT_NAME}_DB_USER` - Database user (default: {project_name})\n- `{PROJECT_NAME}_DB_PASSWORD` - Database password (REQUIRED)\n\n### API Documentation\n\nOnce running, access Swagger UI at:\n```\nhttp://localhost:{port_number}/swagger\n```\n\n## Design Principles\n\nThis pattern follows these principles:\n\n### Architecture:\n1. **Blueprint Organization** - Modular endpoint organization, one blueprint per feature\n2. **MethodView Classes** - Class-based views for HTTP methods (get, post, put, delete)\n3. **Separation of Concerns** - Routes, models, and business logic separated\n4. **Singleton Manager** - Centralized service initialization prevents duplicate connections\n5. **Application Factory** - `create_app()` pattern for testing and flexibility\n\n### Data Models:\n1. **Dataclasses** - Type-safe data models using Python dataclasses\n2. **to_dict/from_dict** - Consistent serialization and deserialization pattern\n3. **Validation in from_dict** - Input validation with clear error messages\n4. **Self-Contained** - Each model handles its own validation and serialization\n\n### API Design:\n1. **OpenAPI/Swagger** - Automatic documentation via flask-smorest\n2. **Error Handling** - Consistent error responses with proper HTTP status codes\n3. **CORS Support** - Cross-origin requests for frontend consumption\n4. **JSON Responses** - All endpoints return JSON via jsonify()\n\n### Best Practices:\n1. **Environment-Based Config** - All secrets via environment variables\n2. **Logging** - Structured logging throughout\n3. **Idempotent Operations** - Safe to call multiple times\n4. **Production Ready** - Gunicorn support out of the box\n5. **Testing Friendly** - Application factory enables easy testing\n\n## Integration with Other Skills\n\n### Database Integration\nIf database is needed, use **postgres-setup** skill first:\n```\nUser: \"Set up postgres database for my project\"\n```\n\nThen reference the database in your blueprints via the singleton manager:\n```python\nfrom common import service_manager\n\ndb = service_manager.get_database()\n```\n\n### Package Structure\nIf publishing as a package, use **python-pypi-setup** skill:\n```\nUser: \"Set up Python package for PyPI\"\n```\n\n## Example Usage in Claude Code\n\n**User:** \"Set up Flask API server for my project\"\n\n**Claude:** \"What is your project name?\"\n\n**User:** \"crypto-tracker\"\n\n**Claude:** \"What features/endpoints do you need?\"\n\n**User:** \"prices, tokens, portfolio\"\n\n**Claude:** \"Do you need database integration?\"\n\n**User:** \"yes\"\n\n**Claude:** \"What port should the server run on?\"\n\n**User:** \"8080\"\n\n**Claude:**\n1. Creates `crypto_tracker.py` with Flask app\n2. Creates `models/` directory with dataclass models:\n   - `prices.py`, `tokens.py`, `portfolio.py`\n3. Creates `blueprints/` directory with endpoint handlers:\n   - `prices.py`, `tokens.py`, `portfolio.py`\n4. Creates `common.py` with ServiceManager singleton\n5. Creates `requirements.txt` with dependencies\n6. Documents environment variables needed\n7. Provides startup instructions\n\n## Optional: Docker Support\n\nIf user requests Docker, reference the **flask-docker-deployment** skill for production-ready containerization with automated versioning and health checks."
              },
              {
                "name": "mz-configure-loki-logging",
                "description": "Configure Grafana Loki logging using mazza-base library for Python/Flask applications with CA certificate (Mazza-specific). Use when setting up Loki logging for Mazza projects or configuring centralized logging.",
                "path": "skills/mz-configure-loki-logging/SKILL.md",
                "frontmatter": {
                  "name": "mz-configure-loki-logging",
                  "description": "Configure Grafana Loki logging using mazza-base library for Python/Flask applications with CA certificate (Mazza-specific). Use when setting up Loki logging for Mazza projects or configuring centralized logging."
                },
                "content": "# Loki Logging with mazza-base\n\nThis skill helps you integrate Grafana Loki logging using the mazza-base utility library, which handles structured JSON logging and Loki shipping.\n\n## When to Use This Skill\n\nUse this skill when:\n- Starting a new Python/Flask application\n- You want centralized logging with Loki\n- You need structured logs for production\n- You want easy local development with console logs\n\n## What This Skill Creates\n\n1. **requirements.txt entry** - Adds mazza-base dependency\n2. **CA certificate file** - Places mazza.vc_CA.pem in project root\n3. **Dockerfile updates** - Copies CA certificate to container\n4. **Logging initialization** - Adds configure_logging() call to main application file\n5. **Environment variable documentation** - All required Loki configuration\n\n## Step 1: Gather Project Information\n\n**IMPORTANT**: Before making changes, ask the user these questions:\n\n1. **\"What is your application tag/name?\"** (e.g., \"materia-server\", \"trading-api\")\n   - This identifies your service in Loki logs\n\n2. **\"What is your main application file?\"** (e.g., \"app.py\", \"server.py\", \"materia_server.py\")\n   - Where to add the logging configuration\n\n3. **\"Do you have the mazza.vc_CA.pem certificate file?\"**\n   - Required for secure Loki connection\n   - If no, user needs to obtain it from Mazza infrastructure team\n\n4. **\"Do you have a CR_PAT environment variable set?\"** (GitHub Personal Access Token)\n   - Required to install mazza-base from private GitHub repo\n\n## Step 2: Add mazza-base to requirements.txt\n\nAdd this line to `requirements.txt`:\n\n```txt\n# Logging configuration with Loki support\nmazza-base @ git+https://${CR_PAT}@github.com/mazza-vc/python-mazza-base.git@main\n```\n\n**NOTE**: The `CR_PAT` environment variable must be set when running `pip install`:\n```bash\nexport CR_PAT=\"your_github_personal_access_token\"\npip install -r requirements.txt\n```\n\n## Step 3: Add CA Certificate File\n\nEnsure `mazza.vc_CA.pem` file is in your project root:\n\n```\n{project_root}/\n├── mazza.vc_CA.pem    # CA certificate for secure Loki connection\n├── requirements.txt\n├── Dockerfile\n└── {app_file}.py\n```\n\nIf you don't have this file, contact the Mazza infrastructure team.\n\n## Step 4: Update Dockerfile\n\nAdd the CA certificate to your Dockerfile. Place this **before** installing requirements:\n\n```dockerfile\nFROM python:3.11-alpine\n\nARG CR_PAT\nENV CR_PAT=${CR_PAT}\n\nWORKDIR /app\n\n# Copy CA certificate\nCOPY mazza.vc_CA.pem .\n\n# Copy requirements and install\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# ... rest of Dockerfile\n```\n\n**CRITICAL**: The COPY line must appear before `pip install -r requirements.txt`\n\nThe certificate will be available at `/app/mazza.vc_CA.pem` in the container.\n\n## Step 5: Configure Logging in Application\n\nAdd to the **top** of your main application file (e.g., `{app_file}.py`):\n\n```python\nimport os\nfrom mazza_base import configure_logging\n\n# Configure logging with mazza_base\n# Use debug_local=True for local development, False for production with Loki\ndebug_mode = os.environ.get('DEBUG_LOCAL', 'true').lower() == 'true'\nlog_level = os.environ.get('LOG_LEVEL', 'INFO')\nconfigure_logging(\n    application_tag='{application_tag}',\n    debug_local=debug_mode,\n    local_level=log_level\n)\n```\n\n**CRITICAL**: Replace:\n- `{app_file}` → Your main application filename (e.g., \"materia_server\")\n- `{application_tag}` → Your service name (e.g., \"materia-server\")\n\nPlace this **before** creating your Flask app or any other initialization.\n\n## Step 6: Document Environment Variables\n\nAdd to README.md or .env.example:\n\n### Environment Variables\n\n**Logging Configuration (Local Development):**\n- `DEBUG_LOCAL` - Set to 'true' for local development (console logs), 'false' for production (Loki)\n  - Default: 'true'\n  - Production: 'false'\n- `LOG_LEVEL` - Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL\n  - Default: 'INFO'\n\n**Loki Configuration (Production Only - required when DEBUG_LOCAL=false):**\n- `MZ_LOKI_ENDPOINT` - Loki server URL (e.g., https://loki.mazza.vc:8443/loki/api/v1/push)\n- `MZ_LOKI_USER` - Loki username for authentication\n- `MZ_LOKI_PASSWORD` - Loki password for authentication\n- `MZ_LOKI_CA_BUNDLE_PATH` - Path to CA certificate (e.g., /app/mazza.vc_CA.pem)\n\n**GitHub Access (for pip install):**\n- `CR_PAT` - GitHub Personal Access Token with repo access\n  - Required to install mazza-base from private repository\n\n### Logging Behavior\n\n**Local Development** (`DEBUG_LOCAL=true`):\n- Logs output to console with pretty formatting\n- Easy to read during development\n- No Loki connection required\n- No need to set MZ_LOKI_* variables\n\n**Production** (`DEBUG_LOCAL=false`):\n- Logs output as structured JSON to Loki\n- All MZ_LOKI_* variables must be set\n- Queryable in Grafana\n- Secure connection via mazza.vc_CA.pem\n\n## Step 7: Usage Examples\n\n### Local Development\n\n```bash\n# In .env or shell\nexport DEBUG_LOCAL=true\nexport LOG_LEVEL=DEBUG\nexport CR_PAT=your_github_token\n\npip install -r requirements.txt\npython {app_file}.py\n```\n\n### Production Deployment\n\nDocker Compose example:\n\n```yaml\nservices:\n  {app_name}:\n    build:\n      context: .\n      args:\n        - CR_PAT=${CR_PAT}\n    environment:\n      - DEBUG_LOCAL=false\n      - LOG_LEVEL=INFO\n      - MZ_LOKI_ENDPOINT=${MZ_LOKI_ENDPOINT}\n      - MZ_LOKI_USER=${MZ_LOKI_USER}\n      - MZ_LOKI_PASSWORD=${MZ_LOKI_PASSWORD}\n      - MZ_LOKI_CA_BUNDLE_PATH=/app/mazza.vc_CA.pem\n```\n\n**NOTE**: Set these in your .env file:\n```\nMZ_LOKI_ENDPOINT=https://loki.mazza.vc:8443/loki/api/v1/push\nMZ_LOKI_USER=your_loki_user\nMZ_LOKI_PASSWORD=your_loki_password\n```\n\n## How It Works\n\nThe `mazza-base` library provides:\n\n1. **Automatic mode detection** - Console logs for local dev, Loki for production\n2. **Structured logging** - Consistent JSON format for Loki\n3. **Secure connection** - Uses CA certificate for encrypted Loki communication\n4. **Easy integration** - One function call to configure everything\n5. **Application tagging** - Identifies your service in centralized logs\n\n**You don't need to:**\n- Write JSON formatters\n- Configure logging handlers\n- Manage Loki client setup\n- Handle certificate validation\n\n**Just call `configure_logging()` and you're done!**\n\n## Integration with Other Skills\n\n### Flask API Server\nIf using **flask-smorest-api** skill, add logging **before** creating Flask app:\n\n```python\nimport os\nfrom flask import Flask\nfrom mazza_base import configure_logging\n\n# Configure logging FIRST\ndebug_mode = os.environ.get('DEBUG_LOCAL', 'true').lower() == 'true'\nconfigure_logging(application_tag='my-api', debug_local=debug_mode)\n\n# Then create Flask app\napp = Flask(__name__)\n# ... rest of setup\n```\n\n### Docker Deployment\nIn your Dockerfile:\n\n```dockerfile\nARG CR_PAT\nENV CR_PAT=${CR_PAT}\nCOPY mazza.vc_CA.pem .\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n```\n\n## Troubleshooting\n\n**Cannot install mazza-base:**\n- Ensure `CR_PAT` environment variable is set\n- Verify token has repo access to `mazza-vc/python-mazza-base`\n- Check token is not expired\n\n**Missing CA certificate error:**\n- Ensure `mazza.vc_CA.pem` is in project root\n- Verify file is copied in Dockerfile: `COPY mazza.vc_CA.pem .`\n- Check MZ_LOKI_CA_BUNDLE_PATH points to correct location\n\n**Runtime error: Missing required environment variables:**\n- Only occurs when `DEBUG_LOCAL=false`\n- Ensure all MZ_LOKI_* variables are set\n- Check spelling (MZ_LOKI_, not LOKI_ or MATERIA_LOKI_)\n\n**Logs not appearing in Loki (production):**\n- Verify `DEBUG_LOCAL=false` is set\n- Check all MZ_LOKI_* variables are correct\n- Test CA certificate path is accessible in container\n- Verify Loki endpoint is reachable from container\n\n**Import error for mazza_base:**\n- Run `pip install -r requirements.txt` with CR_PAT set\n- Verify mazza-base installed: `pip list | grep mazza-base`\n\n## Example Implementation\n\nSee the materia-server project for a reference implementation:\n\n```python\n# materia_server.py\nimport os\nfrom mazza_base import configure_logging\n\ndebug_mode = os.environ.get('DEBUG_LOCAL', 'true').lower() == 'true'\nlog_level = os.environ.get('LOG_LEVEL', 'INFO')\nconfigure_logging(\n    application_tag='materia-server',\n    debug_local=debug_mode,\n    local_level=log_level\n)\n\n# ... rest of Flask app setup\n```\n\n```dockerfile\n# Dockerfile\nFROM python:3.11-alpine\nARG CR_PAT\nENV CR_PAT=${CR_PAT}\nWORKDIR /app\nCOPY mazza.vc_CA.pem .\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n# ... rest of Dockerfile\n```\n\n```yaml\n# docker-compose.yaml\nservices:\n  materia-server:\n    environment:\n      - MZ_LOKI_USER=${MZ_LOKI_USER}\n      - MZ_LOKI_ENDPOINT=${MZ_LOKI_ENDPOINT}\n      - MZ_LOKI_PASSWORD=${MZ_LOKI_PASSWORD}\n      - MZ_LOKI_CA_BUNDLE_PATH=/app/mazza.vc_CA.pem\n```\n\nThis provides structured logging locally during development and automatic Loki shipping in production with secure encrypted connections."
              },
              {
                "name": "postgres-setup",
                "description": "Set up PostgreSQL database with standardized schema.sql pattern. Use when starting a new project that needs PostgreSQL, setting up database schema, or creating setup scripts for postgres.",
                "path": "skills/postgres-setup/SKILL.md",
                "frontmatter": {
                  "name": "postgres-setup",
                  "description": "Set up PostgreSQL database with standardized schema.sql pattern. Use when starting a new project that needs PostgreSQL, setting up database schema, or creating setup scripts for postgres."
                },
                "content": "# PostgreSQL Database Setup Pattern\n\nThis skill helps you set up a PostgreSQL database following a standardized pattern with proper separation of schema and setup scripts.\n\n## When to Use This Skill\n\nUse this skill when:\n- Starting a new project that needs PostgreSQL\n- You want a clean separation between schema definition (SQL) and setup logic (Python)\n- You need support for both production and test databases\n- You want consistent environment variable patterns\n\n## What This Skill Creates\n\n1. **`database/schema.sql`** - SQL schema with table definitions\n2. **`dev_scripts/setup_database.py`** - Python setup script\n3. **Documentation** of required environment variables\n\n## Step 1: Gather Project Information\n\n**IMPORTANT**: Before creating files, ask the user these questions:\n\n1. **\"What is your project name?\"** (e.g., \"arcana\", \"trading-bot\", \"myapp\")\n   - Use this to derive:\n     - Database name: `{project_name}` (e.g., `arcana`)\n     - User name: `{project_name}` (e.g., `arcana`)\n     - Password env var: `{PROJECT_NAME}_PG_PASSWORD` (e.g., `ARCANA_PG_PASSWORD`)\n\n2. **\"What tables do you need in your schema?\"** (optional - can create skeleton if unknown)\n\n## Step 2: Create Directory Structure\n\nCreate these directories if they don't exist:\n```\n{project_root}/\n├── database/\n└── dev_scripts/\n```\n\n## Step 3: Create schema.sql\n\nCreate `database/schema.sql` with:\n\n### Best Practices to Follow:\n- Use `CREATE TABLE IF NOT EXISTS` for idempotency\n- Use `UUID` for primary keys with `gen_random_uuid()` as default\n- Use `BIGINT` (Unix timestamps) for all date/time fields (NOT TIMESTAMP, NOT TIMESTAMPTZ)\n- Add proper foreign key constraints with `ON DELETE CASCADE` or `ON DELETE SET NULL`\n- Add indexes on foreign keys and commonly queried fields\n- Use `TEXT` instead of `VARCHAR` (PostgreSQL best practice)\n- Add comments using `COMMENT ON COLUMN` for documentation\n\n### Template Structure:\n```sql\n-- Enable required extensions\nCREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n-- Example table\nCREATE TABLE IF NOT EXISTS example_table (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name TEXT NOT NULL,\n    created_at BIGINT NOT NULL DEFAULT extract(epoch from now())::bigint,\n    updated_at BIGINT\n);\n\n-- Add indexes\nCREATE INDEX IF NOT EXISTS idx_example_created_at ON example_table(created_at);\n\n-- Add comments\nCOMMENT ON TABLE example_table IS 'Description of what this table stores';\nCOMMENT ON COLUMN example_table.created_at IS 'Unix timestamp of creation';\n```\n\nIf user provides specific tables, create schema accordingly. Otherwise, create a skeleton with one example table.\n\n## Step 4: Create setup_database.py\n\nCreate `dev_scripts/setup_database.py` using this template, **substituting project-specific values**:\n\n```python\n#!/usr/bin/env python\n\"\"\"\nDatabase setup script for {PROJECT_NAME}\nCreates the {project_name} database and user with proper permissions, then applies database/schema.sql\n\nUsage:\n  python setup_database.py --pg-password <postgres_password>\n  python setup_database.py --pg-password <postgres_password> --pg-user <superuser>\n  python setup_database.py --pg-password <postgres_password> --test-db\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport psycopg2\nfrom psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n\n\ndef main():\n    \"\"\"Setup {project_name} database and user\"\"\"\n    parser = argparse.ArgumentParser(description='Setup {PROJECT_NAME} database')\n    parser.add_argument('--pg-password', required=True,\n                       help='PostgreSQL superuser password (required)')\n    parser.add_argument('--pg-user', default='postgres',\n                       help='PostgreSQL superuser name (default: postgres)')\n    parser.add_argument('--test-db', action='store_true',\n                       help='Create {project_name}_test database instead of main {project_name} database')\n    args = parser.parse_args()\n\n    pg_host = os.environ.get('POSTGRES_HOST', 'localhost')\n    pg_port = os.environ.get('POSTGRES_PORT', '5432')\n    pg_user = args.pg_user\n    pg_password = args.pg_password\n\n    if args.test_db:\n        {project_name}_db = '{project_name}_test'\n        print(\"Setting up TEST database '{project_name}_test'...\")\n    else:\n        {project_name}_db = os.environ.get('{PROJECT_NAME}_PG_DB', '{project_name}')\n    {project_name}_user = os.environ.get('{PROJECT_NAME}_PG_USER', '{project_name}')\n    {project_name}_password = os.environ.get('{PROJECT_NAME}_PG_PASSWORD', None)\n\n    if {project_name}_password is None:\n        print(\"Error: {PROJECT_NAME}_PG_PASSWORD environment variable is required\")\n        sys.exit(1)\n\n    print(f\"Setting up database '{{project_name}_db}' and user '{{project_name}_user}'...\")\n    print(f\"Connecting to PostgreSQL at {pg_host}:{pg_port} as {pg_user}\")\n\n    try:\n        conn = psycopg2.connect(\n            host=pg_host,\n            port=pg_port,\n            database='postgres',\n            user=pg_user,\n            password=pg_password\n        )\n        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n\n        with conn.cursor() as cursor:\n            cursor.execute(\"SELECT 1 FROM pg_roles WHERE rolname = %s\", ({project_name}_user,))\n            if not cursor.fetchone():\n                print(f\"Creating user '{{project_name}_user}'...\")\n                cursor.execute(f\"CREATE USER {project_name}_user WITH PASSWORD %s\", ({project_name}_password,))\n                print(f\"✓ User '{{project_name}_user}' created\")\n            else:\n                print(f\"✓ User '{{project_name}_user}' already exists\")\n\n            cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", ({project_name}_db,))\n            if not cursor.fetchone():\n                print(f\"Creating database '{{project_name}_db}'...\")\n                cursor.execute(f\"CREATE DATABASE {{project_name}_db} OWNER {{project_name}_user}\")\n                print(f\"✓ Database '{{project_name}_db}' created\")\n            else:\n                print(f\"✓ Database '{{project_name}_db}' already exists\")\n\n            print(\"Setting permissions...\")\n            cursor.execute(f\"GRANT ALL PRIVILEGES ON DATABASE {{project_name}_db} TO {{project_name}_user}\")\n            print(f\"✓ Granted all privileges on database '{{project_name}_db}' to user '{{project_name}_user}'\")\n\n        conn.close()\n\n        print(f\"\\\\nConnecting as '{{project_name}_user}' to apply schema...\")\n        {project_name}_conn = psycopg2.connect(\n            host=pg_host,\n            port=pg_port,\n            database={project_name}_db,\n            user={project_name}_user,\n            password={project_name}_password\n        )\n        {project_name}_conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n\n        repo_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n        schema_path = os.path.join(repo_root, 'database', 'schema.sql')\n        if not os.path.exists(schema_path):\n            print(f\"Error: schema file not found at {schema_path}\")\n            sys.exit(1)\n\n        with open(schema_path, 'r', encoding='utf-8') as f:\n            schema_sql = f.read()\n\n        with {project_name}_conn.cursor() as cursor:\n            print(\"Ensuring required extensions...\")\n            cursor.execute(\"CREATE EXTENSION IF NOT EXISTS pgcrypto\")\n            print(f\"Applying schema from {schema_path}...\")\n            cursor.execute(schema_sql)\n            print(\"✓ Schema applied\")\n\n        {project_name}_conn.close()\n        print(\"✓ Database setup complete\")\n        print(f\"Database: {{project_name}_db}\")\n        print(f\"User: {{project_name}_user}\")\n        print(f\"Host: {pg_host}:{pg_port}\")\n\n    except psycopg2.Error as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**CRITICAL**: Replace ALL instances of:\n- `{PROJECT_NAME}` → uppercase project name (e.g., `ARCANA`, `MYAPP`)\n- `{project_name}` → lowercase project name (e.g., `arcana`, `myapp`)\n\n## Step 5: Create Documentation\n\nAdd a section to the project's README.md (or create SETUP.md) documenting:\n\n### Command Line Arguments\n\n**Required:**\n- `--pg-password` - PostgreSQL superuser password\n\n**Optional:**\n- `--pg-user` - PostgreSQL superuser name (default: postgres)\n- `--test-db` - Create test database instead of main database\n\n### Environment Variables\n\n**PostgreSQL connection (optional)**:\n- `POSTGRES_HOST` - PostgreSQL host (default: localhost)\n- `POSTGRES_PORT` - PostgreSQL port (default: 5432)\n\n**Project-specific**:\n- `{PROJECT_NAME}_PG_DB` - Database name (default: {project_name})\n- `{PROJECT_NAME}_PG_USER` - Application user (default: {project_name})\n- `{PROJECT_NAME}_PG_PASSWORD` - Application user password (REQUIRED)\n\n### Setup Instructions\n\n```bash\n# Set required environment variables\nexport {PROJECT_NAME}_PG_PASSWORD=\"your_app_password\"\n\n# Run setup script (pass postgres superuser password as argument)\npython dev_scripts/setup_database.py --pg-password \"your_postgres_password\"\n\n# With custom superuser name\npython dev_scripts/setup_database.py --pg-password \"your_postgres_password\" --pg-user \"admin\"\n\n# For test database\npython dev_scripts/setup_database.py --pg-password \"your_postgres_password\" --test-db\n```\n\n## Step 6: Make Script Executable\n\nRun:\n```bash\nchmod +x dev_scripts/setup_database.py\n```\n\n## Step 7: Create Database Driver (Optional but Recommended)\n\nIf the project needs a database driver/connection manager, create one following this pattern:\n\n### File: `src/{project_name}/driver/database.py`\n\n**Key patterns to follow:**\n\n1. **Connection Pooling**: Use `ThreadedConnectionPool` from psycopg2\n   ```python\n   from psycopg2.pool import ThreadedConnectionPool\n\n   self.pool = ThreadedConnectionPool(\n       min_conn,  # e.g., 2\n       max_conn,  # e.g., 10\n       host=db_host,\n       database=db_name,\n       user=db_user,\n       password=db_passwd\n   )\n   ```\n\n2. **Context Managers**: Provide context managers for connections and cursors\n   ```python\n   @contextmanager\n   def get_cursor(self, commit=True, cursor_factory=None):\n       \"\"\"Context manager for database cursors with automatic commit/rollback\"\"\"\n       with self._get_connection() as conn:\n           cursor = conn.cursor(cursor_factory=cursor_factory)\n           try:\n               yield cursor\n               if commit:\n                   conn.commit()\n           except Exception:\n               conn.rollback()\n               raise\n           finally:\n               cursor.close()\n   ```\n\n3. **Always Use RealDictCursor for Loading Data**: When reading from database, use RealDictCursor\n   ```python\n   from psycopg2.extras import RealDictCursor\n\n   with self.get_cursor(commit=False, cursor_factory=RealDictCursor) as cursor:\n       cursor.execute(\"SELECT * FROM table WHERE id = %s\", (id,))\n       result = cursor.fetchone()\n       return Model.from_dict(dict(result))\n   ```\n\n4. **Unix Timestamps Everywhere**: Convert database timestamps to/from unix timestamps\n   ```python\n   # When saving to DB - store as BIGINT\n   created_at = int(time.time())\n\n   # When loading from DB - already BIGINT, use as-is\n   # In models, store as int (unix timestamp)\n   # Only convert to datetime for display/formatting purposes\n   ```\n\n5. **Proper Cleanup**: Ensure pool is closed on destruction\n   ```python\n   def close(self):\n       if self.pool and not self.pool.closed:\n           self.pool.closeall()\n\n   def __del__(self):\n       if hasattr(self, 'pool'):\n           self.close()\n   ```\n\n### Example Driver Structure:\n```python\nclass {ProjectName}DB:\n    def __init__(self, db_host, db_name, db_user, db_passwd, min_conn=2, max_conn=10):\n        self.pool = ThreadedConnectionPool(...)\n\n    @contextmanager\n    def get_cursor(self, commit=True, cursor_factory=None):\n        # Context manager for cursors\n        pass\n\n    def load_item_by_id(self, item_id: str) -> Item:\n        with self.get_cursor(commit=False, cursor_factory=RealDictCursor) as cursor:\n            cursor.execute(\"SELECT * FROM items WHERE id = %s\", (item_id,))\n            result = cursor.fetchone()\n            if not result:\n                raise Exception(f\"Item {item_id} not found\")\n            return Item.from_dict(dict(result))\n\n    def save_item(self, item: Item) -> str:\n        with self.get_cursor() as cursor:\n            cursor.execute(\n                \"INSERT INTO items (name, created_at) VALUES (%s, %s) RETURNING id\",\n                (item.name, int(time.time()))\n            )\n            return str(cursor.fetchone()[0])\n```\n\n## Design Principles\n\nThis pattern follows these principles:\n\n### Database Schema:\n1. **Separation of concerns** - SQL in .sql files, setup logic in Python\n2. **Idempotency** - Safe to run multiple times\n3. **Test database support** - Easy to create isolated test environments\n4. **Unix timestamps** - Always use BIGINT for dates/times (not TIMESTAMP types)\n5. **UUIDs for keys** - Better for distributed systems\n6. **Environment-based config** - No hardcoded credentials\n\n### Database Driver (if applicable):\n1. **Connection pooling** - Use ThreadedConnectionPool for efficient connection reuse\n2. **Context managers** - Automatic commit/rollback and resource cleanup\n3. **RealDictCursor for reads** - Always use RealDictCursor when loading data for easy dict conversion\n4. **Unix timestamps** - Store as BIGINT, convert only for display\n5. **Proper cleanup** - Close pool on destruction\n\n## Example Usage in Claude Code\n\nUser: \"Set up postgres database for my project\"\nClaude: \"What is your project name?\"\nUser: \"trading-bot\"\nClaude:\n1. Creates database/ and dev_scripts/ directories\n2. Creates database/schema.sql with skeleton\n3. Creates dev_scripts/setup_database.py with:\n   - TRADING_BOT_PG_PASSWORD\n   - trading_bot database and user\n4. Documents environment variables needed\n5. Makes script executable"
              },
              {
                "name": "python-pypi-setup",
                "description": "Set up Python project for PyPI publishing with pyproject.toml, src layout, and build scripts. Use when creating a new Python package, setting up for PyPI distribution, or initializing a Python library project.",
                "path": "skills/python-pypi-setup/SKILL.md",
                "frontmatter": {
                  "name": "python-pypi-setup",
                  "description": "Set up Python project for PyPI publishing with pyproject.toml, src layout, and build scripts. Use when creating a new Python package, setting up for PyPI distribution, or initializing a Python library project."
                },
                "content": "# Python PyPI Project Setup Pattern\n\nThis skill helps you set up a Python project for PyPI publishing following modern best practices with pyproject.toml, src layout, and standardized build/publish scripts.\n\n## When to Use This Skill\n\nUse this skill when:\n- Starting a new Python package for PyPI distribution\n- You want to use modern pyproject.toml-based configuration\n- You need a standardized src/ layout with explicit package discovery\n- You want automated build and publish scripts\n\n## What This Skill Creates\n\n1. **`pyproject.toml`** - Modern Python project configuration\n2. **`src/{package_name}/`** - Source layout with package structure\n3. **`.gitignore`** - Comprehensive Python gitignore\n4. **`dev-requirements.txt`** - Development dependencies (build, twine, testing tools)\n5. **`build-publish.sh`** - Automated build and publish script\n6. **`LICENSE`** - License file (Proprietary, MIT, or O'Saasy)\n7. **`README.md`** - Basic project documentation\n\n## Step 1: Gather Project Information\n\n**IMPORTANT**: Before creating files, ask the user these questions:\n\n1. **\"What is your project name?\"** (e.g., \"pg-podcast-toolkit\", \"mypackage\")\n   - Use this to derive:\n     - PyPI package name: `{project-name}` (with hyphens, e.g., `pg-podcast-toolkit`)\n     - Python package name: `{package_name}` (with underscores, e.g., `pg_podcast_toolkit`)\n     - Module directory: `src/{package_name}/`\n\n2. **\"What is the project description?\"** (brief one-line description for PyPI)\n\n3. **\"What is your name?\"** (for author field)\n\n4. **\"What is your email?\"** (for author field)\n\n5. **\"What is your GitHub username?\"** (for project URLs)\n\n6. **\"What license do you want to use?\"** (options: Proprietary, MIT, O'Saasy)\n   - **Proprietary**: All rights reserved, no open source distribution\n   - **MIT**: Permissive open source, allows commercial use\n   - **O'Saasy**: Modified MIT that reserves commercial SaaS rights for the copyright holder (see https://osaasy.dev/)\n\n7. **\"What Python version should be the minimum requirement?\"** (default: 3.8)\n\n8. **\"What are your initial dependencies?\"** (optional - comma-separated list, can be empty)\n\n9. **\"What keywords describe your project?\"** (optional - for PyPI searchability)\n\n## Step 2: Create Directory Structure\n\nCreate these directories if they don't exist:\n```\n{project_root}/\n├── src/\n│   └── {package_name}/\n└── (other files at root)\n```\n\n## Step 3: Create pyproject.toml\n\nCreate `pyproject.toml` with the following structure, **substituting project-specific values**:\n\n```toml\n[project]\nname = \"{project-name}\"\nversion = \"0.0.1\"\nauthors = [\n  { name=\"{author_name}\", email=\"{author_email}\" },\n]\ndescription = \"{project_description}\"\nkeywords = [{keywords_list}]\nreadme = \"README.md\"\nrequires-python = \">={python_version}\"\nlicense = {text = \"{license_name} License\"}\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"{license_classifier}\",\n    \"Operating System :: OS Independent\",\n]\ndependencies = [\n  {dependencies_list}\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/{package_name}\"]\n\n[project.urls]\nHomepage = \"https://github.com/{github_username}/{project-name}\"\nIssues = \"https://github.com/{github_username}/{project-name}/issues\"\n```\n\n**CRITICAL Substitutions**:\n- `{project-name}` → project name with hyphens (e.g., `pg-podcast-toolkit`)\n- `{package_name}` → package name with underscores (e.g., `pg_podcast_toolkit`)\n- `{author_name}` → author's name\n- `{author_email}` → author's email\n- `{project_description}` → one-line description\n- `{keywords_list}` → comma-separated quoted keywords (e.g., `\"podcasting\", \"rss\", \"parser\"`) or empty\n- `{python_version}` → minimum Python version (e.g., `3.8`)\n- `{license_name}` → license name (e.g., `MIT`, `O'Saasy`, `Proprietary - All Rights Reserved`)\n- `{license_classifier}` → Full classifier string:\n  - MIT: `License :: OSI Approved :: MIT License`\n  - O'Saasy: `License :: Other/Proprietary License`\n  - Proprietary: `License :: Other/Proprietary License`\n- `{dependencies_list}` → comma-separated quoted dependencies (e.g., `'requests', 'beautifulsoup4'`) or empty\n- `{github_username}` → GitHub username\n\n**License Classifiers Mapping**:\n- Proprietary → `Other/Proprietary License`\n- MIT → `MIT License`\n- O'Saasy → `Other/Proprietary License` (modified MIT with SaaS restrictions)\n\n**License Text Handling**:\n- **Proprietary**: Use `license = {text = \"Proprietary - All Rights Reserved\"}`\n- **MIT**: Use `license = {text = \"MIT License\"}`\n- **O'Saasy**: Use `license = {text = \"O'Saasy License\"}` and create LICENSE file from https://osaasy.dev/\n\n## Step 4: Create Comprehensive .gitignore\n\nCreate `.gitignore` with comprehensive Python patterns:\n\n```gitignore\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n*.manifest\n*.spec\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\nPipfile.lock\n\n# PEP 582\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\nbin/\ninclude/\npyvenv.cfg\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# IDEs\n.vscode/\n.idea/\n*.swp\n*.swo\n*~\n.DS_Store\n```\n\n## Step 5: Create dev-requirements.txt\n\nCreate `dev-requirements.txt` with development dependencies:\n\n```\nbuild\ntwine\npytest\nblack\nmypy\n```\n\nThese are the tools needed to build, publish, and develop the package. Add other dev tools as needed (isort, pytest-cov, etc.).\n\n## Step 6: Create build-publish.sh\n\nCreate `build-publish.sh` with venv activation and build/publish commands:\n\n```bash\n#!/bin/bash\n# Build and publish package to PyPI\n# Activates virtual environment before running\n\n# Activate virtual environment\nsource bin/activate\n\n# Clean previous builds\nrm -rf dist/*\n\n# Build package\npython -m build\n\n# Upload to PyPI\npython -m twine upload dist/*\n```\n\n**Note**: This script follows the convention that the virtual environment is in `bin/` at the project root.\n\n## Step 7: Create Package Structure\n\nCreate the basic package structure:\n\n1. **`src/{package_name}/__init__.py`** - Package initialization file:\n   ```python\n   \"\"\"\n   {project_description}\n   \"\"\"\n\n   __version__ = \"0.0.1\"\n   ```\n\n2. **If this is a library package**, you can add:\n   ```python\n   # Export main classes/functions here for easier imports\n   # from .module import ClassName, function_name\n   # __all__ = ['ClassName', 'function_name']\n   ```\n\n## Step 8: Create LICENSE File\n\nCreate the appropriate LICENSE file based on the user's license choice:\n\n### For MIT License:\n```\nMIT License\n\nCopyright (c) {year} {author_name}\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n### For O'Saasy License:\nDownload the license text from https://osaasy.dev/ and replace `<Year>` and `<Copyright Holder>` with appropriate values:\n```\nO'Saasy License\n\nCopyright (c) {year} {author_name}\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to use, copy,\nmodify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\nand to permit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\n1. The above copyright notice and this permission notice shall be included in\n   all copies or substantial portions of the Software.\n\n2. The licensee may not use the Software to directly compete with the original\n   Licensor by offering it to third parties as a hosted, managed, or\n   Software-as-a-Service (SaaS) product or cloud service.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n### For Proprietary License:\n```\nProprietary License\n\nCopyright (c) {year} {author_name}. All rights reserved.\n\nThis software and associated documentation files (the \"Software\") are proprietary\nand confidential. Unauthorized copying, modification, distribution, or use of\nthis Software, via any medium, is strictly prohibited.\n\nThe Software is provided for use only by authorized licensees under the terms\nof a separate written agreement with the copyright holder.\n```\n\n## Step 9: Create README.md\n\nCreate `README.md` with basic project documentation:\n\n```markdown\n# {project-name}\n\n{project_description}\n\n## Installation\n\n```bash\npip install {project-name}\n```\n\n## Usage\n\n```python\nimport {package_name}\n\n# Add usage examples here\n```\n\n## Development\n\n### Setup\n\n```bash\n# Create virtual environment\npython -m venv .\n\n# Activate virtual environment\nsource bin/activate  # On Windows: bin\\Scripts\\activate\n\n# Install dependencies\npip install -r dev-requirements.txt\npip install -e .\n```\n\n### Building and Publishing\n\n```bash\n# Make sure you have PyPI credentials configured\n# Build and publish to PyPI\n./build-publish.sh\n```\n\n## License\n\n{license_name}\n\n## Author\n\n{author_name} ({author_email})\n```\n\n## Step 10: Make Script Executable\n\nRun:\n```bash\nchmod +x build-publish.sh\n```\n\n## Step 11: Create Initial Git Repository (if needed)\n\nIf not already a git repository:\n```bash\ngit init\ngit add .\ngit commit -m \"Initial project structure for PyPI package\"\n```\n\n## Step 12: Document Next Steps\n\nInform the user of the next steps:\n\n1. **Install development dependencies**:\n   ```bash\n   source bin/activate\n   pip install -r dev-requirements.txt\n   ```\n\n2. **Install package in development mode**:\n   ```bash\n   pip install -e .\n   ```\n\n3. **Write your code** in `src/{package_name}/`\n\n4. **Update version** in `pyproject.toml` before publishing\n\n5. **Configure PyPI credentials** (one-time setup):\n   ```bash\n   # Create ~/.pypirc with your PyPI token\n   ```\n\n6. **Build and publish**:\n   ```bash\n   ./build-publish.sh\n   ```\n\n## Design Principles\n\nThis pattern follows these principles:\n\n1. **Modern pyproject.toml** - No setup.py needed, all config in pyproject.toml\n2. **Src Layout** - Source code in `src/` directory for better separation\n3. **Explicit Package Discovery** - Using hatchling with explicit package paths\n4. **Comprehensive .gitignore** - Covers all common Python artifacts\n5. **Virtual Environment Convention** - Uses `bin/` at project root\n6. **Automated Publishing** - Simple script for build and publish\n7. **Best Practices** - Follows PEP 517/518 and modern Python packaging standards\n\n## Example Usage in Claude Code\n\nUser: \"Set up a Python package for PyPI\"\nClaude: \"What is your project name?\"\nUser: \"awesome-lib\"\nClaude: [Asks remaining questions including license: Proprietary, MIT, or O'Saasy]\nClaude:\n1. Creates src/awesome_lib/ directory structure\n2. Creates pyproject.toml with project metadata\n3. Creates comprehensive .gitignore\n4. Creates dev-requirements.txt with build tools and dev dependencies\n5. Creates build-publish.sh script\n6. Creates LICENSE file (based on user's choice)\n7. Creates src/awesome_lib/__init__.py\n8. Creates README.md with instructions\n9. Makes script executable\n10. Documents next steps for the user"
              }
            ]
          }
        ]
      }
    }
  ]
}