{
  "owner": {
    "id": "vinnie357",
    "display_name": "Vinnie Mazza",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/30154163?u=7bd5de49893901f9ccdec6082b244bf492e59fcf&v=4",
    "url": "https://github.com/vinnie357",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 9,
      "total_commands": 9,
      "total_skills": 81,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "vinnie357/claude-skills",
      "url": "https://github.com/vinnie357/claude-skills",
      "description": "claude code skills",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-12-14T00:51:12Z",
        "created_at": "2025-11-15T01:48:14Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2964
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 1374
        },
        {
          "path": ".claude-plugin/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/scripts/README.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": ".claude-plugin/scripts/update-all-skills.nu",
          "type": "blob",
          "size": 4332
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/CODEOWNERS",
          "type": "blob",
          "size": 326
        },
        {
          "path": ".github/actions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/actions/validate-marketplace",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/actions/validate-marketplace/action.yml",
          "type": "blob",
          "size": 1063
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/validate.yml",
          "type": "blob",
          "size": 362
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 23580
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1069
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 13672
        },
        {
          "path": "claude-code",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 620
        },
        {
          "path": "claude-code/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/commands/research-skill.md",
          "type": "blob",
          "size": 3268
        },
        {
          "path": "claude-code/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-agents/SKILL.md",
          "type": "blob",
          "size": 10272
        },
        {
          "path": "claude-code/skills/claude-commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-commands/SKILL.md",
          "type": "blob",
          "size": 7474
        },
        {
          "path": "claude-code/skills/claude-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-hooks/SKILL.md",
          "type": "blob",
          "size": 9916
        },
        {
          "path": "claude-code/skills/claude-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-plugins/SKILL.md",
          "type": "blob",
          "size": 9823
        },
        {
          "path": "claude-code/skills/claude-plugins/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-plugins/references/plugin-schema.md",
          "type": "blob",
          "size": 6045
        },
        {
          "path": "claude-code/skills/claude-plugins/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-plugins/scripts/format-plugin.nu",
          "type": "blob",
          "size": 3385
        },
        {
          "path": "claude-code/skills/claude-plugins/scripts/init-plugin.nu",
          "type": "blob",
          "size": 1609
        },
        {
          "path": "claude-code/skills/claude-plugins/scripts/validate-plugin.nu",
          "type": "blob",
          "size": 13060
        },
        {
          "path": "claude-code/skills/claude-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/claude-skills/SKILL.md",
          "type": "blob",
          "size": 13243
        },
        {
          "path": "claude-code/skills/plugin-marketplace",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/plugin-marketplace/SKILL.md",
          "type": "blob",
          "size": 12060
        },
        {
          "path": "claude-code/skills/plugin-marketplace/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/plugin-marketplace/references/schema-specification.md",
          "type": "blob",
          "size": 5765
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts/analyze-plugins.nu",
          "type": "blob",
          "size": 2321
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts/format-marketplace.nu",
          "type": "blob",
          "size": 1299
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts/init-marketplace.nu",
          "type": "blob",
          "size": 1513
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts/validate-dependencies.nu",
          "type": "blob",
          "size": 3206
        },
        {
          "path": "claude-code/skills/plugin-marketplace/scripts/validate-marketplace.nu",
          "type": "blob",
          "size": 12912
        },
        {
          "path": "claude-code/skills/sources.md",
          "type": "blob",
          "size": 8327
        },
        {
          "path": "core",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 652
        },
        {
          "path": "core/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/commands/gcms.md",
          "type": "blob",
          "size": 290
        },
        {
          "path": "core/commands/research.md",
          "type": "blob",
          "size": 4235
        },
        {
          "path": "core/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/accessibility",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/accessibility/SKILL.md",
          "type": "blob",
          "size": 5861
        },
        {
          "path": "core/skills/anti-fabrication",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/anti-fabrication/SKILL.md",
          "type": "blob",
          "size": 9684
        },
        {
          "path": "core/skills/code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/code-review/SKILL.md",
          "type": "blob",
          "size": 14381
        },
        {
          "path": "core/skills/documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/documentation/SKILL.md",
          "type": "blob",
          "size": 14182
        },
        {
          "path": "core/skills/git",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/git/SKILL.md",
          "type": "blob",
          "size": 14711
        },
        {
          "path": "core/skills/material-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/material-design/SKILL.md",
          "type": "blob",
          "size": 6139
        },
        {
          "path": "core/skills/mise",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/mise/SKILL.md",
          "type": "blob",
          "size": 15435
        },
        {
          "path": "core/skills/nushell",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/nushell/SKILL.md",
          "type": "blob",
          "size": 13229
        },
        {
          "path": "core/skills/sources.md",
          "type": "blob",
          "size": 4424
        },
        {
          "path": "core/skills/twelve-factor",
          "type": "tree",
          "size": null
        },
        {
          "path": "core/skills/twelve-factor/SKILL.md",
          "type": "blob",
          "size": 18928
        },
        {
          "path": "dagu",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 416
        },
        {
          "path": "dagu/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/rest-api",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/rest-api/SKILL.md",
          "type": "blob",
          "size": 4267
        },
        {
          "path": "dagu/skills/rest-api/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/rest-api/references/api-endpoints.md",
          "type": "blob",
          "size": 6328
        },
        {
          "path": "dagu/skills/sources.md",
          "type": "blob",
          "size": 2434
        },
        {
          "path": "dagu/skills/webui",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/webui/SKILL.md",
          "type": "blob",
          "size": 3502
        },
        {
          "path": "dagu/skills/webui/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/webui/references/monitoring.md",
          "type": "blob",
          "size": 4586
        },
        {
          "path": "dagu/skills/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "dagu/skills/workflows/SKILL.md",
          "type": "blob",
          "size": 14926
        },
        {
          "path": "elixir",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 474
        },
        {
          "path": "elixir/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/anti-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/anti-patterns/SKILL.md",
          "type": "blob",
          "size": 15383
        },
        {
          "path": "elixir/skills/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/config/SKILL.md",
          "type": "blob",
          "size": 15155
        },
        {
          "path": "elixir/skills/otp",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/otp/SKILL.md",
          "type": "blob",
          "size": 14570
        },
        {
          "path": "elixir/skills/phoenix",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/phoenix/SKILL.md",
          "type": "blob",
          "size": 11534
        },
        {
          "path": "elixir/skills/sources.md",
          "type": "blob",
          "size": 4160
        },
        {
          "path": "elixir/skills/testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "elixir/skills/testing/SKILL.md",
          "type": "blob",
          "size": 17225
        },
        {
          "path": "github",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 409
        },
        {
          "path": "github/mise.toml",
          "type": "blob",
          "size": 304
        },
        {
          "path": "github/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/act",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/act/SKILL.md",
          "type": "blob",
          "size": 12121
        },
        {
          "path": "github/skills/act/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/act/scripts/install-act.nu",
          "type": "blob",
          "size": 2233
        },
        {
          "path": "github/skills/actions",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/actions/SKILL.md",
          "type": "blob",
          "size": 10347
        },
        {
          "path": "github/skills/sources.md",
          "type": "blob",
          "size": 5883
        },
        {
          "path": "github/skills/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "github/skills/workflows/SKILL.md",
          "type": "blob",
          "size": 16742
        },
        {
          "path": "mise.toml",
          "type": "blob",
          "size": 10193
        },
        {
          "path": "rust",
          "type": "tree",
          "size": null
        },
        {
          "path": "rust/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "rust/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 370
        },
        {
          "path": "rust/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "rust/skills/SKILL.md",
          "type": "blob",
          "size": 17652
        },
        {
          "path": "rust/skills/sources.md",
          "type": "blob",
          "size": 2159
        },
        {
          "path": "test",
          "type": "tree",
          "size": null
        },
        {
          "path": "test/README.md",
          "type": "blob",
          "size": 2766
        },
        {
          "path": "test/validate-all.nu",
          "type": "blob",
          "size": 6200
        },
        {
          "path": "test/validate-plugin.nu",
          "type": "blob",
          "size": 812
        },
        {
          "path": "ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "ui/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "ui/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 369
        },
        {
          "path": "ui/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "ui/skills/daisyui",
          "type": "tree",
          "size": null
        },
        {
          "path": "ui/skills/daisyui/SKILL.md",
          "type": "blob",
          "size": 5157
        },
        {
          "path": "ui/skills/daisyui/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "ui/skills/daisyui/references/components.md",
          "type": "blob",
          "size": 12576
        },
        {
          "path": "ui/skills/sources.md",
          "type": "blob",
          "size": 2102
        }
      ],
      "marketplace": {
        "name": "vinnie357",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "vinnie357",
          "email": "vinnie@example.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "all-skills",
            "description": "Meta-plugin that installs all available skills from all plugins",
            "source": "./",
            "category": "meta",
            "version": "0.3.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install all-skills@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/research-skill",
                "description": "Research topics and create Agent Skills with proper structure and documentation",
                "path": "claude-code/commands/research-skill.md",
                "frontmatter": {
                  "description": "Research topics and create Agent Skills with proper structure and documentation",
                  "argument-hint": "<skill-name> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create a properly structured Agent Skill following the Agent Skills Specification.\n\n**Skill Creation:**\n- **Directory Structure**: Creates `skills/<skill-name>/SKILL.md` with proper frontmatter\n- **Progressive Disclosure**: Generates reference files in `references/` for deep context\n- **Source Documentation**: Updates `promptlog/sources.md` with all research sources\n- **Specification Compliance**: Follows Agent Skills Specification v1.0\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **YAML Frontmatter**: Auto-generates name, description, license, metadata\n- **Reference Management**: Creates separate files for detailed documentation\n- **Source Tracking**: Maintains traceability in promptlog/sources.md\n\n**Examples:**\n```\n/research-skill elixir-genserver\n# Creates: skills/elixir-genserver/SKILL.md\n# Updates: promptlog/sources.md\n\n/research-skill kubernetes-operators --complexity=high\n# Creates: skills/kubernetes-operators/SKILL.md\n#          skills/kubernetes-operators/references/\n# Updates: promptlog/sources.md\n\n/research-skill react-hooks --complexity=medium\n# Creates: skills/react-hooks/SKILL.md with enhanced analysis\n```\n\n**SKILL.md Structure:**\n```markdown\n---\nname: skill-name\ndescription: When Claude should use this skill (concise, activation-focused)\nlicense: MIT\n---\n\n# Skill Name\n\n## When to Use This Skill\n[Activation criteria]\n\n## Core Concepts\n[Essential knowledge]\n\n## Best Practices\n[Guidelines and patterns]\n\n## Examples\n[Concrete usage examples]\n\n## References\n[Links to reference files if needed]\n```\n\n**Workflow:**\n1. **Research**: Gather authoritative sources and best practices\n2. **Structure**: Create skill directory following spec\n3. **Generate SKILL.md**: Write frontmatter and core content\n4. **Create References**: Add detailed docs in references/ for progressive disclosure\n5. **Document Sources**: Update promptlog/sources.md with all sources used\n6. **Validate**: Ensure spec compliance and activation clarity\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. Research the topic thoroughly using web search and authoritative sources\n2. Create the skill directory: `skills/<skill-name>/`\n3. Generate SKILL.md with:\n   - Proper YAML frontmatter (name, description, license)\n   - Clear activation criteria in description\n   - Core procedural knowledge in markdown body\n   - Concrete examples and patterns\n4. Create `skills/<skill-name>/references/` if detailed documentation is needed\n5. Update `promptlog/sources.md` with:\n   - All URLs and documentation sources used\n   - Purpose of each source\n   - Key concepts extracted\n   - Date accessed\n6. Follow the 7-step skill creation workflow from CLAUDE.md\n7. Use imperative/infinitive language (not second-person)\n8. Keep commonly-used context in SKILL.md, detailed references separate\n\nThe agent should produce a complete, spec-compliant skill ready for use."
              },
              {
                "name": "/gcms",
                "description": "Generate brief conventional commit message suggestions",
                "path": "core/commands/gcms.md",
                "frontmatter": {
                  "description": "Generate brief conventional commit message suggestions",
                  "argument-hint": ""
                },
                "content": "Please analyze the current git status and suggest 1-3 brief conventional commit messages based on the staged and unstaged changes. Use the git-commit-message subagent to perform this analysis."
              },
              {
                "name": "/research",
                "description": "Research topics and create comprehensive planning documentation",
                "path": "core/commands/research.md",
                "frontmatter": {
                  "description": "Research topics and create comprehensive planning documentation",
                  "argument-hint": "<category> <topic> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create comprehensive documentation for planning, understanding, and working with the subject.\n\n**Document Creation:**\n- **Directory Structure**: Creates `research/<category>/<topic>/`\n- **File Generation**: Produces `overview.md`, `troubleshooting.md`, and optional guides\n- **Planning Focus**: Helps understand topics before implementation\n- **Reference Material**: Creates searchable knowledge base\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **Structured Content**: Consistent templates for reliability\n- **Authoritative Sources**: Links to official docs and best practices\n- **Practical Examples**: Real-world usage patterns and code samples\n\n**Examples:**\n```\n/research development docker-compose\n# Creates: research/development/docker-compose/\n#          - overview.md\n#          - troubleshooting.md\n\n/research infrastructure kubernetes-networking --complexity=high\n# Creates: research/infrastructure/kubernetes-networking/\n#          - overview.md (with deep analysis)\n#          - troubleshooting.md\n#          - best-practices.md\n\n/research frontend react-state-management --complexity=medium\n# Creates: research/frontend/react-state-management/\n#          - overview.md\n#          - troubleshooting.md\n#          - comparison.md (Redux vs Context vs Zustand)\n```\n\n**Document Structure:**\n\n### overview.md\n- **Purpose & Use Cases**: When and why to use this technology\n- **Core Concepts**: Fundamental principles and architecture\n- **Implementation Patterns**: Common approaches and best practices\n- **Code Examples**: Practical, runnable examples\n- **Integration Guidelines**: How it fits into larger systems\n- **Performance Considerations**: Optimization and scaling\n- **Security**: Common vulnerabilities and protections\n- **Resources**: Official docs, tutorials, community resources\n\n### troubleshooting.md\n- **Common Issues**: Frequently encountered problems\n- **Error Messages**: Interpretation and solutions\n- **Diagnostic Tools**: How to investigate problems\n- **Solutions**: Step-by-step fixes\n- **Prevention**: How to avoid issues\n- **Escalation**: When and where to get help\n\n### Additional Files (generated as needed)\n- `best-practices.md`: Detailed guidelines and patterns\n- `comparison.md`: Technology alternatives and trade-offs\n- `migration.md`: Upgrade paths and migration strategies\n- `quick-start.md`: Fast setup and basic usage\n\n**Quality Standards:**\n- Use authoritative sources (official docs, RFCs, reputable blogs)\n- Include version information where relevant\n- Provide working code examples\n- Link to external resources\n- Note common pitfalls and gotchas\n- Include date of research for freshness tracking\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. **Research Phase**:\n   - Search authoritative sources and documentation\n   - Gather best practices and common patterns\n   - Identify common issues and solutions\n   - Collect practical examples\n\n2. **Structure Phase**:\n   - Create `research/<category>/<topic>/` directory\n   - Determine which documents are needed based on topic\n   - Plan document organization\n\n3. **Content Generation**:\n   - Write comprehensive `overview.md` with:\n     * Clear purpose and use cases\n     * Core concepts and architecture\n     * Implementation patterns\n     * Practical examples\n     * Integration guidance\n   - Write practical `troubleshooting.md` with:\n     * Common issues and solutions\n     * Error message interpretations\n     * Diagnostic approaches\n     * Prevention strategies\n   - Generate additional guides as needed\n\n4. **Quality Assurance**:\n   - Verify all sources are authoritative\n   - Ensure examples are practical and correct\n   - Check for completeness and clarity\n   - Add timestamps and version info\n\nThe agent should produce comprehensive, actionable documentation that helps users understand and work with the topic effectively."
              }
            ],
            "skills": [
              {
                "name": "claude-agents",
                "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                "path": "claude-code/skills/claude-agents/SKILL.md",
                "frontmatter": {
                  "name": "claude-agents",
                  "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Agents\n\nGuide for creating custom agents that provide specialized behaviors and tool access for specific tasks.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating custom agent types for specific workflows\n- Defining agent behaviors and tool permissions\n- Configuring agent capabilities\n- Understanding agent vs skill differences\n- Implementing domain-specific agents\n\n## What Are Agents?\n\nAgents are specialized Claude instances with:\n- **Specific tool access**: Limited or specialized tool sets\n- **Defined behaviors**: Pre-configured instructions and constraints\n- **Task focus**: Optimized for particular workflows\n- **Autonomous operation**: Can execute multi-step tasks independently\n\n## Agents vs Skills\n\n| Feature | Agents | Skills |\n|---------|--------|--------|\n| **Activation** | Explicitly launched via Task tool | Auto-activated based on context |\n| **Tool Access** | Configurable, can be restricted | Inherit from parent context |\n| **State** | Independent, isolated | Share parent context |\n| **Use Case** | Complex multi-step tasks | Knowledge and guidelines |\n| **Persistence** | Single execution | Always available when loaded |\n\n## Agent File Structure\n\n### Location\n\nAgents are defined in markdown files located in:\n- Plugin: `<plugin-root>/agents/`\n- User-level: `.claude/agents/`\n\n### File Naming\n\n- Use kebab-case: `code-reviewer.md`\n- File name becomes the agent type\n- Be descriptive about the agent's purpose\n\n## Basic Agent Format\n\n```markdown\n---\nname: code-reviewer\ndescription: Specialized agent for conducting thorough code reviews\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Code Review Agent\n\nI am a specialized code review agent focused on:\n\n## Responsibilities\n\n- Analyzing code for correctness and style\n- Identifying security vulnerabilities\n- Checking test coverage\n- Ensuring documentation quality\n- Suggesting improvements\n\n## Review Process\n\nWhen reviewing code, I will:\n\n1. Read the changed files\n2. Check for common anti-patterns\n3. Verify error handling\n4. Assess test coverage\n5. Provide actionable feedback\n\n## Guidelines\n\n- Focus on significant issues\n- Provide specific examples\n- Suggest concrete improvements\n- Consider project context\n```\n\n## Agent Configuration\n\n### YAML Frontmatter\n\nRequired and optional fields:\n\n```markdown\n---\nname: agent-name                    # Required: kebab-case identifier\ndescription: Brief description      # Required: What this agent does\ntools:                             # Optional: Tool allowlist\n  - Read\n  - Write\n  - Bash\nmodel: sonnet                      # Optional: Model to use (sonnet, opus, haiku)\nmax_iterations: 10                 # Optional: Maximum task iterations\ntimeout: 300                       # Optional: Timeout in seconds\n---\n```\n\n### Tool Allowlist\n\nRestrict agent to specific tools:\n\n```markdown\n---\ntools:\n  - Read      # Can read files\n  - Grep      # Can search code\n  - Glob      # Can find files\n  # Cannot use Write, Edit, Bash, etc.\n---\n```\n\n**No tool restrictions** (access to all tools):\n\n```markdown\n---\n# Omit tools field entirely\n---\n```\n\n### Model Selection\n\nChoose appropriate model for the task:\n\n```markdown\n---\nmodel: haiku        # Fast, cost-effective for simple tasks\n# model: sonnet     # Balanced (default)\n# model: opus       # Most capable for complex tasks\n---\n```\n\n## Common Agent Patterns\n\n### Read-Only Analysis Agent\n\n```markdown\n---\nname: security-analyzer\ndescription: Analyzes code for security vulnerabilities\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Security Analysis Agent\n\nI perform security analysis on codebases.\n\n## Analysis Areas\n\n- SQL injection vulnerabilities\n- XSS attack vectors\n- Authentication/authorization issues\n- Sensitive data exposure\n- Insecure dependencies\n\n## Process\n\n1. Scan for common vulnerability patterns\n2. Check security best practices\n3. Identify potential risks\n4. Provide remediation guidance\n```\n\n### Test Generation Agent\n\n```markdown\n---\nname: test-generator\ndescription: Generates comprehensive test suites\ntools:\n  - Read\n  - Write\n  - Glob\nmodel: sonnet\n---\n\n# Test Generation Agent\n\nI create comprehensive test suites for your code.\n\n## Test Types\n\n- Unit tests\n- Integration tests\n- Edge case coverage\n- Error scenario tests\n\n## Approach\n\n1. Analyze source code structure\n2. Identify testable units\n3. Generate test cases\n4. Create test files with proper naming\n5. Include setup and teardown logic\n```\n\n### Documentation Agent\n\n```markdown\n---\nname: docs-generator\ndescription: Creates and updates project documentation\ntools:\n  - Read\n  - Write\n  - Glob\n  - Grep\nmodel: sonnet\n---\n\n# Documentation Agent\n\nI create and maintain project documentation.\n\n## Documentation Types\n\n- README files\n- API documentation\n- Code comments\n- Architecture docs\n- User guides\n\n## Standards\n\n- Clear, concise language\n- Practical examples\n- Up-to-date with codebase\n- Proper formatting (Markdown, JSDoc, etc.)\n```\n\n### Refactoring Agent\n\n```markdown\n---\nname: refactorer\ndescription: Safely refactors code while maintaining functionality\ntools:\n  - Read\n  - Write\n  - Edit\n  - Grep\n  - Glob\nmodel: sonnet\nmax_iterations: 20\n---\n\n# Code Refactoring Agent\n\nI refactor code to improve quality while preserving behavior.\n\n## Refactoring Goals\n\n- Improve readability\n- Reduce complexity\n- Eliminate duplication\n- Enhance maintainability\n- Follow best practices\n\n## Safety Measures\n\n- Preserve existing functionality\n- Maintain test coverage\n- Document changes\n- Use safe transformations\n```\n\n## Agent Plugin Configuration\n\n### In plugin.json\n\n```json\n{\n  \"agents\": [\n    \"./agents/code-reviewer.md\",\n    \"./agents/test-generator.md\",\n    \"./agents/security-analyzer.md\"\n  ]\n}\n```\n\n### Directory-Based Loading\n\n```json\n{\n  \"agents\": \"./agents\"\n}\n```\n\nLoads all `.md` files in `agents/` directory.\n\n## Invoking Agents\n\nAgents are launched via the Task tool:\n\n```python\n# In parent Claude conversation\nTask(\n    subagent_type=\"code-reviewer\",\n    description=\"Review authentication module\",\n    prompt=\"\"\"\n    Review the authentication module for:\n    - Security vulnerabilities\n    - Error handling\n    - Input validation\n    - Best practices\n    \"\"\"\n)\n```\n\n## Agent Communication\n\n### Input to Agent\n\n- Task description\n- Detailed prompt\n- Access to conversation history (if configured)\n\n### Output from Agent\n\n- Final report/result\n- No ongoing dialogue\n- One-time execution\n\n## Best Practices\n\n### Clear Purpose\n\nEach agent should have a specific, well-defined purpose:\n\n```markdown\n---\nname: migration-helper\ndescription: Assists with database schema migrations\n---\n\n# Database Migration Agent\n\nSpecialized in creating and validating database migrations.\n```\n\n### Appropriate Tool Access\n\nOnly grant necessary tools:\n\n```markdown\n---\n# Analysis agent - read-only\ntools:\n  - Read\n  - Grep\n  - Glob\n---\n```\n\n```markdown\n---\n# Implementation agent - can modify\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n---\n```\n\n### Model Selection\n\nMatch model to task complexity:\n\n- **haiku**: Simple, repetitive tasks\n- **sonnet**: Standard tasks (default)\n- **opus**: Complex reasoning required\n\n### Iteration Limits\n\nSet appropriate limits for task complexity:\n\n```markdown\n---\nmax_iterations: 5   # Simple, focused task\n# max_iterations: 20  # Complex, multi-step workflow\n---\n```\n\n### Clear Instructions\n\nProvide explicit behavior guidelines:\n\n```markdown\n# Testing Agent\n\n## Mandatory Requirements\n\n- Generate tests for ALL public methods\n- Achieve minimum 80% code coverage\n- Include edge cases and error scenarios\n- Use project's testing framework conventions\n\n## Constraints\n\n- Do not modify source code\n- Follow existing test file naming patterns\n- Use appropriate assertions\n```\n\n## Security Considerations\n\n### Tool Restrictions\n\nLimit dangerous operations:\n\n```markdown\n---\n# Don't give Bash access to untrusted agents\ntools:\n  - Read\n  - Write  # Safer than arbitrary shell commands\n---\n```\n\n### Input Validation\n\nValidate agent inputs:\n\n```markdown\n# Deployment Agent\n\nBefore deploying:\n1. Verify target environment is valid\n2. Check deployment permissions\n3. Validate configuration\n4. Confirm destructive operations\n```\n\n### Sensitive Data\n\nNever hardcode:\n- Credentials\n- API keys\n- Private URLs\n- Access tokens\n\n## Agent Examples\n\n### PR Review Agent\n\n```markdown\n---\nname: pr-reviewer\ndescription: Reviews pull requests for quality and completeness\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Pull Request Review Agent\n\nConducting thorough PR review...\n\n## Checklist\n\n- [ ] Code quality and style\n- [ ] Test coverage\n- [ ] Documentation updates\n- [ ] Breaking changes noted\n- [ ] Security considerations\n- [ ] Performance implications\n\n## Review Process\n\n1. Analyze changed files\n2. Check for common issues\n3. Verify tests exist\n4. Review documentation\n5. Provide constructive feedback\n```\n\n### Migration Agent\n\n```markdown\n---\nname: code-migrator\ndescription: Migrates code from one framework/version to another\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\nmodel: opus\nmax_iterations: 30\n---\n\n# Code Migration Agent\n\nPerforming framework migration...\n\n## Migration Steps\n\n1. Analyze current codebase\n2. Identify migration patterns\n3. Apply transformations\n4. Update dependencies\n5. Verify compatibility\n6. Document changes\n\n## Safety Checks\n\n- Backup original code\n- Incremental changes\n- Validate each step\n- Maintain git history\n```\n\n## Troubleshooting\n\n### Agent Not Found\n\n- Verify agent file location matches plugin.json\n- Check file naming (kebab-case, .md extension)\n- Ensure plugin is properly installed\n\n### Tool Access Denied\n\n- Check tools allowlist in frontmatter\n- Verify tool names match exactly\n- Ensure parent context permits delegation\n\n### Unexpected Behavior\n\n- Review agent instructions for clarity\n- Check model selection appropriateness\n- Verify iteration limits aren't too restrictive\n- Test with verbose output\n\n## References\n\nFor more information:\n- Claude Code Agents Documentation: https://code.claude.com/docs/en/agents\n- Task Tool Documentation: https://code.claude.com/docs/en/tools/task"
              },
              {
                "name": "claude-commands",
                "description": "Guide for creating custom slash commands for Claude Code",
                "path": "claude-code/skills/claude-commands/SKILL.md",
                "frontmatter": {
                  "name": "claude-commands",
                  "description": "Guide for creating custom slash commands for Claude Code",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Commands\n\nGuide for creating custom slash commands that extend Claude Code functionality.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new custom slash commands\n- Understanding command structure and syntax\n- Organizing commands for plugins\n- Implementing command workflows\n- Debugging command execution\n\n## What Are Commands?\n\nCommands are custom slash commands (like `/commit`, `/review`) that users can invoke to trigger specific workflows or expand prompts. They are markdown files that can contain:\n\n- Static prompt text\n- Dynamic content based on arguments\n- Multi-step workflows\n- Integration with tools and scripts\n\n## Command File Structure\n\n### Location\n\nCommands are defined in markdown files located in:\n- Plugin: `<plugin-root>/commands/`\n- User-level: `.claude/commands/`\n\n### File Naming\n\n- Use kebab-case: `my-command.md`\n- File name becomes the command name: `my-command.md` → `/my-command`\n- Avoid conflicts with built-in commands\n\n## Basic Command Format\n\n### Simple Static Command\n\n```markdown\n# /my-command\n\nThis is the prompt that will be expanded when the user types /my-command.\n\nThe entire content of this file will replace the slash command in the conversation.\n```\n\n### Command with Description\n\n```markdown\n<!--\ndescription: Brief description of what this command does\n-->\n\n# /my-command\n\nCommand prompt goes here...\n```\n\n## Command Arguments\n\nCommands can accept arguments that users provide when invoking the command.\n\n### Single Argument\n\n```markdown\n# /greet\n\nHello, {{arg}}! Welcome to the project.\n```\n\nUsage: `/greet Alice` → \"Hello, Alice! Welcome to the project.\"\n\n### Multiple Arguments\n\n```markdown\n# /create-file\n\nCreate a new file at {{arg1}} with the following content:\n\n{{arg2}}\n```\n\nUsage: `/create-file src/main.rs \"fn main() {}\"`\n\n### Named Arguments\n\n```markdown\n# /deploy\n\nDeploy {{environment}} environment to {{region}}.\n\nConfiguration:\n- Environment: {{environment}}\n- Region: {{region}}\n- Branch: {{branch}}\n```\n\nUsage: `/deploy --environment=production --region=us-east-1 --branch=main`\n\n## Advanced Features\n\n### Conditional Content\n\n```markdown\n# /analyze\n\nAnalyze the {{language}} codebase.\n\n{{#if verbose}}\nProvide detailed analysis including:\n- Code complexity metrics\n- Dependency analysis\n- Security vulnerabilities\n{{else}}\nProvide a summary analysis.\n{{/if}}\n```\n\n### Including Files\n\nReference other files or command outputs:\n\n```markdown\n# /context\n\nHere is the current project structure:\n\n{{file:PROJECT_STRUCTURE.md}}\n\nAnd the current git status:\n\n{{shell:git status}}\n```\n\n### Multi-Step Workflows\n\n```markdown\n# /full-review\n\nI'll perform a comprehensive code review:\n\n1. First, let me check the git diff:\n{{shell:git diff}}\n\n2. Now analyzing code quality...\n\n3. Checking for security issues...\n\n4. Final recommendations:\n```\n\n## Best Practices\n\n### Clear Command Names\n\n- Use descriptive, action-oriented names\n- `/analyze-security` not `/sec`\n- `/create-component` not `/comp`\n\n### Provide Context\n\nAlways include what the command will do:\n\n```markdown\n# /commit\n\nI'll analyze the current git changes and create a conventional commit message.\n\nCurrent changes:\n{{shell:git diff --staged}}\n\nBased on these changes, here's my suggested commit message:\n```\n\n### Handle Edge Cases\n\n```markdown\n# /deploy\n\n{{#if staging}}\nDeploying to staging environment (safe for testing)\n{{else if production}}\n⚠️ WARNING: Deploying to PRODUCTION\nAre you sure you want to continue? This will affect live users.\n{{else}}\nError: Unknown environment. Please specify --staging or --production\n{{/if}}\n```\n\n### Document Arguments\n\n```markdown\n<!--\ndescription: Deploy application to specified environment\nusage: /deploy [--environment=<env>] [--region=<region>]\narguments:\n  - environment: Target environment (staging, production)\n  - region: AWS region (us-east-1, eu-west-1, etc.)\n-->\n\n# /deploy\n```\n\n## Command Organization\n\n### Plugin Commands\n\nIn `plugin.json`:\n\n```json\n{\n  \"commands\": [\n    \"./commands/deploy.md\",\n    \"./commands/analyze.md\",\n    \"./commands/review.md\"\n  ]\n}\n```\n\n### Directory-Based Commands\n\n```json\n{\n  \"commands\": [\"./commands\"]\n}\n```\n\nThis loads all `.md` files in the `commands/` directory.\n\n### Namespaced Commands\n\nOrganize related commands in subdirectories:\n\n```\ncommands/\n├── git/\n│   ├── commit.md\n│   ├── review.md\n│   └── cleanup.md\n├── deploy/\n│   ├── staging.md\n│   └── production.md\n```\n\n## Common Command Patterns\n\n### Git Commit Message Generator\n\n```markdown\n# /gcm\n\nI'll analyze the staged changes and generate a conventional commit message.\n\n{{shell:git diff --staged}}\n\nBased on these changes, here's my commit message:\n```\n\n### Code Review Command\n\n```markdown\n# /review-pr\n\nI'll review the pull request changes.\n\nPR Number: {{pr_number}}\n\n{{shell:gh pr diff {{pr_number}}}}\n\nReview checklist:\n- [ ] Code quality and style\n- [ ] Security considerations\n- [ ] Test coverage\n- [ ] Documentation updates\n```\n\n### Project Scaffolding\n\n```markdown\n# /new-component\n\nCreating a new {{component_type}} component named {{name}}.\n\nI'll create:\n1. Component file at src/components/{{name}}.tsx\n2. Test file at src/components/{{name}}.test.tsx\n3. Storybook file at src/components/{{name}}.stories.tsx\n```\n\n## Testing Commands\n\n### Manual Testing\n\n1. Install the plugin locally\n2. Reload Claude Code\n3. Type your command in the chat\n4. Verify the expansion is correct\n\n### Debugging\n\nIf a command doesn't work:\n\n1. Check file location matches plugin.json\n2. Verify markdown syntax\n3. Test argument substitution\n4. Check for conflicts with existing commands\n\n## Command Templates\n\n### Analysis Command Template\n\n```markdown\n<!--\ndescription: Analyze {{target}} for {{criteria}}\n-->\n\n# /analyze-{{target}}\n\nI'll analyze the {{target}} codebase for {{criteria}}.\n\n{{shell:find {{target}} -type f -name \"*.{{extension}}\"}}\n\nAnalysis results:\n```\n\n### Workflow Command Template\n\n```markdown\n<!--\ndescription: Execute {{workflow}} workflow\n-->\n\n# /{{workflow}}\n\nStarting {{workflow}} workflow...\n\nStep 1: {{step1_description}}\n{{step1_action}}\n\nStep 2: {{step2_description}}\n{{step2_action}}\n\nWorkflow complete!\n```\n\n## Integration with Skills\n\nCommands can reference skills:\n\n```markdown\n# /elixir-review\n\nI'll review this Elixir code using my Phoenix and OTP knowledge.\n\nPlease provide the code to review, and I'll check for:\n- Phoenix best practices\n- OTP design patterns\n- Elixir anti-patterns\n- Performance considerations\n```\n\n## Security Considerations\n\n### Avoid Sensitive Data\n\nNever hardcode:\n- API keys\n- Passwords\n- Tokens\n- Private URLs\n\n### Validate Input\n\n```markdown\n# /deploy\n\n{{#unless environment}}\nError: --environment is required\n{{/unless}}\n\n{{#if (validate_environment environment)}}\nProceeding with deployment...\n{{else}}\nError: Invalid environment. Must be staging or production.\n{{/if}}\n```\n\n### Safe Shell Commands\n\nBe cautious with shell command execution:\n\n```markdown\n# /safe-deploy\n\n<!-- Only allow whitelisted commands -->\n{{shell:./scripts/deploy.sh {{environment}}}}\n```\n\n## References\n\nFor more information about Claude Code commands:\n- Claude Code Documentation: https://code.claude.com/docs/en/commands\n- Example Commands: https://github.com/anthropics/claude-code/tree/main/examples/commands"
              },
              {
                "name": "claude-hooks",
                "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                "path": "claude-code/skills/claude-hooks/SKILL.md",
                "frontmatter": {
                  "name": "claude-hooks",
                  "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Hooks\n\nGuide for creating hooks that execute shell commands or scripts in response to Claude Code events and tool calls.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating event-driven automations\n- Implementing custom validation or formatting\n- Integrating with external tools and services\n- Setting up project-specific workflows\n- Responding to tool execution events\n\n## What Are Hooks?\n\nHooks are shell commands that execute automatically in response to specific events:\n\n- **Tool Call Hooks**: Trigger before/after specific tool calls\n- **Lifecycle Hooks**: Trigger on plugin install/uninstall\n- **User Prompt Hooks**: Trigger when users submit prompts\n- **Custom Events**: Application-specific trigger points\n\n## Hook Configuration\n\n### Location\n\nHooks are configured in:\n- Plugin: `<plugin-root>/.claude-plugin/hooks.json`\n- User-level: `.claude/hooks.json`\n- Plugin manifest: Inline in `plugin.json`\n\n### File Structure\n\n**Standalone hooks.json:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"./hooks/format-check.sh\"],\n      \"after\": [\"./hooks/lint.sh\"]\n    },\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh\"]\n    }\n  },\n  \"onInstall\": [\"./hooks/setup.sh\"],\n  \"onUninstall\": [\"./hooks/cleanup.sh\"],\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh\"]\n}\n```\n\n**Inline in plugin.json:**\n```json\n{\n  \"hooks\": {\n    \"onToolCall\": {\n      \"Write\": {\n        \"after\": [\"prettier --write {{file_path}}\"]\n      }\n    }\n  }\n}\n```\n\n## Hook Types\n\n### Tool Call Hooks\n\nExecute before or after specific tool calls.\n\n**Available Tools:**\n- `Read`, `Write`, `Edit`, `MultiEdit`\n- `Bash`, `BashOutput`\n- `Glob`, `Grep`\n- `Task`, `Skill`, `SlashCommand`\n- `TodoWrite`\n- `WebFetch`, `WebSearch`\n- `AskUserQuestion`\n\n**Example:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\n        \"echo 'Writing file: {{file_path}}'\",\n        \"./hooks/backup.sh {{file_path}}\"\n      ],\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"git add {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\"eslint --fix {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Lifecycle Hooks\n\nExecute during plugin installation/uninstallation.\n\n```json\n{\n  \"onInstall\": [\n    \"./hooks/setup-dependencies.sh\",\n    \"npm install\",\n    \"echo 'Plugin installed successfully'\"\n  ],\n  \"onUninstall\": [\n    \"./hooks/cleanup.sh\",\n    \"echo 'Plugin uninstalled'\"\n  ]\n}\n```\n\n### User Prompt Submit Hook\n\nExecute when user submits a prompt:\n\n```json\n{\n  \"onUserPromptSubmit\": [\n    \"./hooks/log-interaction.sh '{{prompt}}'\",\n    \"./hooks/check-context.sh\"\n  ]\n}\n```\n\n## Hook Variables\n\nHooks have access to context-specific variables using `{{variable}}` syntax.\n\n### Tool Call Variables\n\nDifferent tools provide different variables:\n\n**Write Tool:**\n- `{{file_path}}`: Path to file being written\n- `{{content}}`: Content being written (before hooks only)\n\n**Edit Tool:**\n- `{{file_path}}`: Path to file being edited\n- `{{old_string}}`: String being replaced\n- `{{new_string}}`: Replacement string\n\n**Bash Tool:**\n- `{{command}}`: Command being executed\n\n**Read Tool:**\n- `{{file_path}}`: Path to file being read\n\n### Global Variables\n\nAvailable in all hooks:\n- `{{cwd}}`: Current working directory\n- `{{timestamp}}`: Current Unix timestamp\n- `{{user}}`: Current user\n- `{{plugin_root}}`: Plugin installation directory\n\n### User Prompt Variables\n\n- `{{prompt}}`: User's submitted prompt text\n\n## Hook Examples\n\n### Auto-Format on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Pre-Commit Validation\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-git-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-git-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block force push to main/master\nif [[ \"$COMMAND\" =~ \"git push --force\" ]] && [[ \"$COMMAND\" =~ \"main|master\" ]]; then\n  echo \"ERROR: Force push to main/master is not allowed\"\n  exit 1\nfi\n\nexit 0\n```\n\n### Automatic Backups\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    },\n    \"Edit\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    }\n  }\n}\n```\n\n### Logging and Analytics\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/log-file-change.sh {{file_path}}\"]\n    }\n  },\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh '{{prompt}}'\"]\n}\n```\n\n**log-file-change.sh:**\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Modified: $FILE\" >> .claude/file-changes.log\n```\n\n### Integration with External Tools\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"notify-send 'File Updated' 'Modified {{file_path}}'\",\n        \"curl -X POST https://api.example.com/notify -d 'file={{file_path}}'\"\n      ]\n    }\n  }\n}\n```\n\n## Hook Execution\n\n### Execution Order\n\nMultiple hooks execute in array order:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"echo 'Step 1'\",  // Runs first\n        \"echo 'Step 2'\",  // Runs second\n        \"echo 'Step 3'\"   // Runs third\n      ]\n    }\n  }\n}\n```\n\n### Exit Codes\n\n**Before Hooks:**\n- Exit code `0`: Continue with tool execution\n- Exit code non-zero: **Block tool execution**, show error to user\n\n**After Hooks:**\n- Exit codes are logged but don't affect tool execution\n- Tool has already completed\n\n### Error Handling\n\n```bash\n#!/bin/bash\n\n# Before hook - blocks tool on error\nif [[ ! -f \"$1\" ]]; then\n  echo \"ERROR: File does not exist\"\n  exit 1  # Blocks tool execution\nfi\n\n# Validation passed\nexit 0\n```\n\n## Best Practices\n\n### Keep Hooks Fast\n\nHooks block execution - keep them lightweight:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      // ✅ Fast linter\n      \"after\": [\"eslint --fix {{file_path}}\"]\n\n      // ❌ Slow test suite\n      // \"after\": [\"npm test\"]\n    }\n  }\n}\n```\n\n### Use Absolute Paths\n\nReference scripts with paths relative to plugin:\n\n```json\n{\n  \"onInstall\": [\"${CLAUDE_PLUGIN_ROOT}/hooks/setup.sh\"]\n}\n```\n\n### Validate Input\n\nAlways validate hook variables:\n\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\n\nif [[ -z \"$FILE\" ]]; then\n  echo \"ERROR: No file path provided\"\n  exit 1\nfi\n\nif [[ ! -f \"$FILE\" ]]; then\n  echo \"ERROR: File does not exist: $FILE\"\n  exit 1\nfi\n```\n\n### Provide Clear Feedback\n\n```bash\n#!/bin/bash\n\necho \"Running pre-commit checks...\"\n\nif ! npm run lint; then\n  echo \"❌ Linting failed. Please fix errors before committing.\"\n  exit 1\nfi\n\necho \"✅ All checks passed\"\nexit 0\n```\n\n### Handle Edge Cases\n\n```bash\n#!/bin/bash\n\n# Handle files with spaces in names\nFILE=\"$1\"\n\n# Validate file type\nif [[ ! \"$FILE\" =~ \\.(js|ts|jsx|tsx)$ ]]; then\n  # Skip non-JavaScript files silently\n  exit 0\nfi\n\n# Run formatter\nprettier --write \"$FILE\"\n```\n\n## Security Considerations\n\n### Validate Commands\n\nBefore hooks can block dangerous operations:\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block dangerous patterns\nDANGEROUS_PATTERNS=(\n  \"rm -rf /\"\n  \"dd if=\"\n  \"mkfs\"\n  \"> /dev/sda\"\n)\n\nfor pattern in \"${DANGEROUS_PATTERNS[@]}\"; do\n  if [[ \"$COMMAND\" =~ $pattern ]]; then\n    echo \"ERROR: Dangerous command blocked: $pattern\"\n    exit 1\n  fi\ndone\n\nexit 0\n```\n\n### Limit Hook Scope\n\nOnly hook necessary tools:\n\n```json\n{\n  // ✅ Specific tools only\n  \"onToolCall\": {\n    \"Write\": { \"after\": [\"./format.sh {{file_path}}\"] }\n  }\n\n  // ❌ Don't hook everything unnecessarily\n}\n```\n\n### Sanitize Variables\n\n```bash\n#!/bin/bash\n\n# Sanitize file path\nFILE=$(realpath \"$1\")\n\n# Ensure file is within project\nif [[ ! \"$FILE\" =~ ^$(pwd) ]]; then\n  echo \"ERROR: File outside project directory\"\n  exit 1\nfi\n```\n\n## Debugging Hooks\n\n### Enable Verbose Output\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"set -x; ./hooks/debug.sh {{file_path}}; set +x\"]\n    }\n  }\n}\n```\n\n### Log Hook Execution\n\n```bash\n#!/bin/bash\n\nLOG_FILE=\".claude/hooks.log\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Hook: $0, Args: $@\" >> \"$LOG_FILE\"\n\n# Rest of hook logic...\n```\n\n### Test Hooks Manually\n\n```bash\n# Test hook with sample data\n./hooks/format.sh \"src/main.js\"\n\n# Check exit code\necho $?\n```\n\n## Common Hook Patterns\n\n### Auto-Format Pipeline\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Test on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/run-relevant-tests.sh {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Git Integration\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"git add {{file_path}}\"]\n    },\n    \"Edit\": {\n      \"after\": [\"git add {{file_path}}\"]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n### Hook Not Executing\n\n- Check hook file has execute permissions: `chmod +x hooks/script.sh`\n- Verify path is correct relative to plugin root\n- Check JSON syntax in hooks.json\n- Look for errors in Claude Code logs\n\n### Hook Blocking Tool\n\n- Check exit code of before hooks\n- Add debug logging\n- Test hook script manually\n- Verify validation logic\n\n### Variables Not Substituting\n\n- Check variable name spelling: `{{file_path}}` not `{{filepath}}`\n- Verify variable is available for that tool\n- Quote variables in bash: `\"{{file_path}}\"`\n\n## References\n\nFor more information:\n- Claude Code Hooks Documentation: https://code.claude.com/docs/en/hooks\n- Plugin Configuration: https://code.claude.com/docs/en/plugins#hooks"
              },
              {
                "name": "claude-plugins",
                "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                "path": "claude-code/skills/claude-plugins/SKILL.md",
                "frontmatter": {
                  "name": "claude-plugins",
                  "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin\n\nGuide for creating, validating, and managing plugin.json files for Claude Code plugins. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/plugin.json` files\n- Validating plugin.json schema compliance\n- Setting up new plugin directories\n- Troubleshooting plugin configuration issues\n- Understanding plugin manifest structure\n\n## Plugin Manifest Schema\n\n### File Location\n\nAll plugin manifests must be located at `.claude-plugin/plugin.json` within the plugin directory.\n\n### Complete Schema\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Brief plugin description\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"author@example.com\",\n    \"url\": \"https://github.com/author\"\n  },\n  \"homepage\": \"https://docs.example.com/plugin\",\n  \"repository\": \"https://github.com/author/plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"commands\": [\"./custom/commands/special.md\"],\n  \"agents\": \"./custom/agents/\",\n  \"hooks\": \"./config/hooks.json\",\n  \"mcpServers\": \"./mcp-config.json\",\n  \"skills\": [\"./skills/skill-one\", \"./skills/skill-two\"]\n}\n```\n\n### Required Fields\n\n- `name`: Plugin identifier (kebab-case, lowercase alphanumeric and hyphens only)\n\n### Optional Fields\n\n**Metadata:**\n- `version`: Semantic version number (recommended)\n- `description`: Brief explanation of plugin functionality\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of searchability and categorization tags\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n\n**Author Information:**\n- `author.name`: Creator name\n- `author.email`: Contact email\n- `author.url`: Personal or organization website\n\n**Component Paths:**\n- `skills`: Array of skill directory paths (relative to plugin root)\n- `commands`: String path or array of command file/directory paths\n- `agents`: String path or array of agent file paths\n- `hooks`: String path to hooks.json or hooks configuration object\n- `mcpServers`: String path to MCP config or configuration object\n\n## Field Validation Rules\n\n### name\n- **Format**: kebab-case (lowercase alphanumeric and hyphens only)\n- **Pattern**: `^[a-z0-9]+(-[a-z0-9]+)*$`\n- **Examples**:\n  - Valid: `my-plugin`, `core-skills`, `elixir-tools`\n  - Invalid: `myPlugin`, `my_plugin`, `My-Plugin`, `plugin-`\n\n### version\n- **Format**: Semantic versioning\n- **Pattern**: `^[0-9]+\\.[0-9]+\\.[0-9]+(-[a-zA-Z0-9.-]+)?(\\+[a-zA-Z0-9.-]+)?$`\n- **Examples**:\n  - Valid: `1.0.0`, `2.1.3`, `1.0.0-beta.1`, `1.0.0+build.123`\n  - Invalid: `1.0`, `v1.0.0`, `1.0.0.0`\n\n### license\n- **Format**: SPDX license identifier\n- **Common values**: `MIT`, `Apache-2.0`, `GPL-3.0`, `BSD-3-Clause`, `ISC`\n- **Reference**: https://spdx.org/licenses/\n\n### keywords\n- **Format**: Array of strings\n- **Purpose**: Discoverability, searchability, categorization\n- **Recommendations**: Use lowercase, be specific, include domain terms\n\n### Paths (skills, commands, agents, hooks, mcpServers)\n- **Format**: Relative paths from plugin root\n- **Recommendations**: Use `./` prefix for clarity\n- **Skills**: Array of directory paths containing SKILL.md files\n- **Commands**: Can be string (single path) or array of paths\n- **Agents**: Can be string (directory) or array of file paths\n\n## Invalid Fields in plugin.json\n\nThe following fields are **only valid in marketplace.json** entries and must NOT appear in plugin.json:\n\n- `dependencies`: Dependencies belong in marketplace entries, not plugin manifests\n- `category`: Categorization is marketplace-level metadata\n- `strict`: Controls marketplace behavior, not plugin definition\n- `source`: Plugin location is defined in marketplace, not in plugin itself\n- `tags`: Use `keywords` instead\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\nThis validates:\n- JSON syntax\n- Required field presence (name)\n- Kebab-case naming\n- Field type correctness\n- Path accessibility (for relative paths)\n- Invalid field detection\n\n### 2. Path Validation\n\nValidate that referenced paths exist:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-paths.nu .claude-plugin/plugin.json\n```\n\nChecks:\n- Skills directories exist and contain SKILL.md\n- Command files/directories exist\n- Agent files/directories exist\n- Hooks configuration exists\n- MCP server configuration exists\n\n### 3. Initialization Helper\n\nGenerate a template plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nCreates `.claude-plugin/plugin.json` with proper structure.\n\n## Best Practices\n\n### Naming Conventions\n\n- **Plugin name**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Avoid generic names**: Be specific about the plugin's purpose\n- **Match directory name**: Plugin name should match its directory name\n\n### Versioning Strategy\n\n- Use semantic versioning (MAJOR.MINOR.PATCH)\n- Increment MAJOR for breaking changes\n- Increment MINOR for new features (backward compatible)\n- Increment PATCH for bug fixes\n- Use pre-release tags for beta versions (`1.0.0-beta.1`)\n\n### Path Organization\n\n**Recommended structure:**\n```\nplugin-name/\n├── .claude-plugin/\n│   └── plugin.json\n├── skills/\n│   ├── skill-one/\n│   └── skill-two/\n├── commands/\n└── agents/\n```\n\n**In plugin.json:**\n```json\n{\n  \"skills\": [\n    \"./skills/skill-one\",\n    \"./skills/skill-two\"\n  ],\n  \"commands\": [\"./commands\"],\n  \"agents\": [\"./agents\"]\n}\n```\n\n### Metadata Completeness\n\nAlways include:\n- `version`: Track plugin evolution\n- `description`: Help users understand purpose\n- `license`: Clarify usage terms\n- `keywords`: Improve discoverability\n- `repository`: Enable contributions\n\n### Author Information\n\nInclude contact information for:\n- Bug reports\n- Feature requests\n- Contributions\n- Questions\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Invalid field for plugin.json\n\n```json\n// ❌ Invalid (dependencies only in marketplace.json)\n{\n  \"name\": \"my-plugin\",\n  \"dependencies\": [\"other-plugin\"]\n}\n\n// ✅ Valid\n{\n  \"name\": \"my-plugin\",\n  \"keywords\": [\"tool\", \"utility\"]\n}\n```\n\n### Error: Skill path doesn't exist\n\n```json\n// ❌ Invalid (path not found)\n\"skills\": [\"./skills/nonexistent\"]\n\n// ✅ Valid (path exists with SKILL.md)\n\"skills\": [\"./skills/my-skill\"]\n```\n\n### Error: Invalid version format\n\n```json\n// ❌ Invalid\n\"version\": \"1.0\"\n\"version\": \"v1.0.0\"\n\n// ✅ Valid\n\"version\": \"1.0.0\"\n\"version\": \"2.1.3-beta.1\"\n```\n\n## Creating a New Plugin\n\n### Step 1: Initialize Directory Structure\n\n```bash\nmkdir -p my-plugin/.claude-plugin\nmkdir -p my-plugin/skills\n```\n\n### Step 2: Create plugin.json\n\nUse the initialization script:\n\n```bash\ncd my-plugin\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nOr create manually:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"0.1.0\",\n  \"description\": \"My plugin description\",\n  \"author\": {\n    \"name\": \"Your Name\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"skills\": []\n}\n```\n\n### Step 3: Add Skills\n\n1. Create skill directory: `mkdir -p skills/my-skill`\n2. Create SKILL.md in skill directory\n3. Add to plugin.json:\n\n```json\n{\n  \"skills\": [\"./skills/my-skill\"]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\n### Step 5: Test\n\nInstall locally to test:\n\n```bash\nclaude-code install ./\n```\n\n## Hooks Configuration\n\nHooks can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"hooks\": {\n    \"onInstall\": \"./scripts/install.sh\",\n    \"onUninstall\": \"./scripts/uninstall.sh\"\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"hooks\": \"./config/hooks.json\"\n}\n```\n\n## MCP Servers Configuration\n\nMCP servers can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"mcp-server-filesystem\",\n      \"args\": [\"./workspace\"]\n    }\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"mcpServers\": \"./mcp-config.json\"\n}\n```\n\n## Troubleshooting\n\n### Plugin Not Loading\n\n- Verify plugin.json exists at `.claude-plugin/plugin.json`\n- Check JSON syntax is valid\n- Ensure name field is present and kebab-case\n- Validate all path references exist\n\n### Skills Not Found\n\n- Check skill paths in plugin.json match actual directories\n- Ensure each skill directory contains SKILL.md file\n- Verify paths use relative format (`./skills/name`)\n\n### Commands Not Appearing\n\n- Verify command paths exist\n- Check commands are .md files or directories containing .md files\n- Ensure paths are relative to plugin root\n\n### Validation Fails\n\nRun validation with verbose output:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/plugin-schema.md`: Complete JSON schema specification\n- `references/plugin-examples.md`: Real-world plugin.json examples\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-plugin.nu`: Complete plugin.json validation\n- `validate-plugin-paths.nu`: Verify all referenced paths exist\n- `init-plugin.nu`: Generate plugin.json template\n- `format-plugin.nu`: Format and sort plugin.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              },
              {
                "name": "claude-skills",
                "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                "path": "claude-code/skills/claude-skills/SKILL.md",
                "frontmatter": {
                  "name": "claude-skills",
                  "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Agent Skills\n\nComprehensive guide for creating modular, self-contained Agent Skills that extend Claude's capabilities with specialized knowledge.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new Agent Skills\n- Understanding skill structure and organization\n- Implementing progressive disclosure\n- Organizing skill resources (scripts, references, assets)\n- Following Agent Skills best practices\n\n## What Are Agent Skills?\n\nAgent Skills are organized directories containing instructions, scripts, and resources that Claude can dynamically discover and load. They enable a single general-purpose agent to gain domain-specific expertise without requiring separate custom agents for each use case.\n\n### Key Concepts\n\n- **Modularity**: Self-contained packages that can be mixed and matched\n- **Reusability**: Share and distribute expertise across projects and teams\n- **Progressive Disclosure**: Load context only when needed, keeping interactions efficient\n- **Specialization**: Deep domain knowledge without sacrificing generality\n\n## How Skills Work\n\nSkills operate on a principle of progressive disclosure across multiple levels:\n\n### Level 1: Discovery\nAgent system prompts include only skill names and descriptions, allowing Claude to decide when each skill is relevant based on the task at hand.\n\n### Level 2: Activation\nWhen Claude determines a skill applies, it loads the full `SKILL.md` file into context, gaining access to the complete procedural knowledge and guidelines.\n\n### Level 3+: Deep Context\nAdditional bundled files (like references, forms, or documentation) load only when needed for specific scenarios, keeping token usage efficient.\n\nThis tiered approach maintains efficient context windows while supporting potentially unbounded skill complexity.\n\n## Skill Structure\n\n### Minimal Requirements\n\nEvery skill must have:\n\n```\nskill-name/\n└── SKILL.md\n```\n\n### Complete Structure\n\nMore complex skills can include additional resources:\n\n```\nskill-name/\n├── SKILL.md           # Required: Core skill definition\n├── scripts/           # Optional: Executable code for deterministic tasks\n├── references/        # Optional: Documentation loaded on-demand\n└── assets/            # Optional: Templates, images, boilerplate\n```\n\n## SKILL.md Format\n\nEach `SKILL.md` file must begin with YAML frontmatter followed by Markdown content:\n\n```markdown\n---\nname: skill-name\ndescription: Concise explanation of when Claude should use this skill\nlicense: MIT\n---\n\n# Skill Name\n\nMain instructional content goes here...\n```\n\n### Required YAML Properties\n\n- `name`: Hyphen-case identifier matching directory name (lowercase alphanumeric and hyphens only)\n- `description`: Explains the skill's purpose and when Claude should utilize it\n\n### Optional YAML Properties\n\n- `license`: License name or filename reference\n- `allowed-tools`: Pre-approved tools list (Claude Code support only)\n- `metadata`: Key-value string pairs for client-specific properties\n\n### Markdown Body\n\nThe content section has no restrictions and should contain:\n\n- When to activate the skill\n- Core procedural knowledge\n- Best practices and guidelines\n- Examples and patterns\n- References to additional resources (if any)\n\n## Creating Skills: Seven-Step Workflow\n\n### 1. Understanding Through Examples\n\nGather concrete use cases to clarify what the skill should support. Real-world examples reveal actual needs better than theoretical requirements.\n\n**Example:**\n```\nUse Case: Help developers follow Git best practices\nExamples:\n- Creating conventional commit messages\n- Rebasing feature branches\n- Resolving merge conflicts\n- Creating descriptive branch names\n```\n\n### 2. Planning Resources\n\nAnalyze examples to identify needed components:\n\n- **Scripts**: For tasks requiring deterministic reliability or that would need repeated rewriting\n- **References**: Documentation to load into context as needed\n- **Assets**: Output files like templates or boilerplate (not loaded into context)\n\n**Example:**\n```\nGit skill resources:\n- scripts/analyze-commit.sh - Parse git diff for commit message\n- references/conventional-commits.md - Detailed commit format spec\n- assets/gitignore-templates/ - Common .gitignore files\n```\n\n### 3. Initialization\n\nCreate the skill directory structure with the required `SKILL.md` file. Ensure the directory name matches the `name` property exactly.\n\n```bash\nmkdir -p my-skill/{scripts,references,assets}\ntouch my-skill/SKILL.md\n```\n\n### 4. Editing\n\nDevelop resource files and update `SKILL.md` with:\n- Purpose and activation criteria\n- Usage guidelines and best practices\n- Implementation details and examples\n- References to supplementary files\n\n**Use imperative/infinitive form** rather than second-person instruction for clarity.\n\n✅ Good: \"Follow the Conventional Commits specification\"\n✅ Good: \"Use descriptive branch names with type prefixes\"\n❌ Avoid: \"You should try to use descriptive names when possible\"\n\nKeep core procedural information in `SKILL.md` and detailed reference material in separate files.\n\n### 5. Documentation\n\n**Document all sources in the plugin's `sources.md`**. For each skill created, record:\n- URLs of documentation, guides, and references used\n- Purpose of each source\n- Key topics and concepts extracted\n- Date accessed (if relevant)\n\nThis maintains traceability and helps others understand the skill's foundation.\n\n### 6. Validation\n\nTest the skill with representative scenarios to ensure:\n- Claude activates it appropriately\n- Instructions are clear and actionable\n- Progressive disclosure works effectively\n- Token usage remains efficient\n\n### 7. Iteration\n\nRefine based on real-world usage feedback. Monitor how Claude actually uses the skill and adjust the description and content accordingly.\n\n## Best Practices\n\n### Start with Evaluation\n\nIdentify specific capability gaps by testing agents on representative tasks. Build skills incrementally to address actual shortcomings rather than anticipated needs.\n\n### Structure for Scale\n\nSplit unwieldy `SKILL.md` files into separate referenced documents:\n- Keep commonly-used contexts together\n- Separate mutually exclusive information to reduce token usage\n- Use progressive disclosure to load details only when needed\n\n**Example:**\n```markdown\n# Git Skill\n\nFor detailed conventional commit format, see references/conventional-commits.md\nFor rebase workflow, see references/rebasing-guide.md\n```\n\n### Consider Claude's Perspective\n\nThe skill name and description heavily influence when Claude activates it. Pay particular attention to:\n\n- **Name**: Should be clear and reflect the domain (e.g., `git-operations`, `elixir-phoenix`)\n- **Description**: Should specify both what the skill does and when to use it\n\n**Examples:**\n\n✅ Good Description:\n```yaml\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\n```\n\n❌ Too Vague:\n```yaml\ndescription: Helps with Git\n```\n\nMonitor real usage patterns and iterate based on actual behavior.\n\n### Iterate Collaboratively\n\nWork with Claude to capture successful approaches and common mistakes into reusable skill components. Ask Claude to self-reflect on what contextual information actually matters.\n\n### Write for AI Consumption\n\nUse clear, imperative language that Claude can follow:\n\n✅ Good:\n- \"Follow the Conventional Commits specification\"\n- \"Use descriptive branch names with type prefixes\"\n- \"Run tests before committing\"\n\n❌ Avoid:\n- \"You should try to use descriptive names when possible\"\n- \"It might be good to run tests\"\n- \"Consider following best practices\"\n\nInclude concrete examples wherever possible to illustrate patterns and approaches.\n\n### Security Considerations\n\nInstall skills only from trusted sources. When evaluating unfamiliar skills:\n- Thoroughly audit bundled files and scripts\n- Review code dependencies\n- Examine instructions directing Claude to connect with external services\n- Verify the skill doesn't request sensitive information or dangerous operations\n\n## Anti-Fabrication Requirements\n\nAll skills MUST adhere to strict anti-fabrication requirements to ensure factual, measurable content.\n\n### Core Principles\n\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Execute tests before marking tasks complete and report actual results\n- Validate integration recommendations through actual framework detection using tool analysis\n\n### Prohibited Language and Claims\n\n- **Superlatives**: Avoid \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- **Unsubstantiated Metrics**: Never fabricate percentages, success rates, or performance numbers\n- **Assumed Capabilities**: Don't claim features exist without tool verification\n- **Generic Claims**: Replace vague statements with specific, measurable observations\n- **Fabricated Testing**: Never report test results without actual execution\n\n### Time and Effort Estimation Rule\n\n- Never provide time estimates, effort estimates, or completion timelines without actual measurement or analysis\n- If estimates are requested, execute tools to analyze scope (e.g., count files, measure complexity, assess dependencies) before providing data-backed estimates\n- When estimates cannot be measured, explicitly state \"timeline requires analysis of [specific factors]\"\n- Avoid fabricated scheduling language like \"15 minutes\", \"2 hours\", \"quick task\" without factual basis\n\n### Validation Requirements\n\n- **File Claims**: Use Read or Glob tools before claiming files exist or contain specific content\n- **System Integration**: Use Bash or appropriate tools to verify system capabilities\n- **Framework Detection**: Execute actual detection logic before claiming framework presence\n- **Test Results**: Only report test outcomes after actual execution with tool verification\n- **Performance Claims**: Base any performance statements on actual measurement or analysis\n\n## Skill Examples\n\n### Simple Skill (Git)\n\n```markdown\n---\nname: git\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\nlicense: MIT\n---\n\n# Git Operations\n\n## When to Use\n\nActivate when:\n- Creating commit messages\n- Managing branches\n- Resolving conflicts\n- Rebasing or merging\n\n## Conventional Commits\n\nFollow the format: `type(scope): description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n\nExample: `feat(auth): add OAuth2 login support`\n\n## Branch Naming\n\nUse format: `type/description`\n\nExamples:\n- `feature/user-authentication`\n- `fix/memory-leak`\n- `docs/api-reference`\n\n## Rebasing Workflow\n\n1. Update main: `git checkout main && git pull`\n2. Rebase feature: `git checkout feature-branch && git rebase main`\n3. Resolve conflicts if needed\n4. Force push: `git push --force-with-lease`\n```\n\n### Complex Skill (Phoenix)\n\n```markdown\n---\nname: phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and best practices\nlicense: MIT\n---\n\n# Phoenix Framework\n\n## When to Use\n\nActivate for:\n- Phoenix application development\n- LiveView implementations\n- Context design\n- Channel setup\n\n## Project Structure\n\nPhoenix apps follow:\n```\nlib/\n├── my_app/          # Business logic (contexts)\n├── my_app_web/      # Web interface\n└── my_app.ex\n```\n\n## Contexts\n\nGroup related functionality:\n\n```elixir\ndefmodule MyApp.Accounts do\n  def list_users, do: Repo.all(User)\n  def get_user!(id), do: Repo.get!(User, id)\n  def create_user(attrs), do: ...\nend\n```\n\nFor detailed context patterns, see references/contexts.md\n\n## LiveView\n\nFor real-time interfaces, see references/liveview-guide.md\n```\n\n## Common Pitfalls\n\n### Too Generic\n\n❌ Avoid:\n```yaml\nname: programming\ndescription: Helps with programming\n```\n\n✅ Better:\n```yaml\nname: elixir-phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and Elixir best practices\n```\n\n### Too Much in SKILL.md\n\n❌ Avoid putting entire API reference in SKILL.md\n\n✅ Better: Keep core patterns in SKILL.md, detailed reference in `references/`\n\n### Missing Activation Criteria\n\n❌ Avoid:\n```markdown\n# My Skill\n\nThis skill helps with stuff.\n```\n\n✅ Better:\n```markdown\n# My Skill\n\n## When to Use\n\nActivate when:\n- Specific scenario 1\n- Specific scenario 2\n- Specific scenario 3\n```\n\n## References\n\nFor more information:\n- **Agent Skills Blog**: https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\n- **Example Skills**: https://github.com/anthropics/skills\n- **Skills Cookbook**: https://github.com/anthropics/claude-cookbooks/tree/main/skills\n- **Skill Creator Guide**: https://github.com/anthropics/skills/blob/main/skill-creator/SKILL.md\n- **Agent Skills Specification**: https://github.com/anthropics/skills/blob/main/agent_skills_spec.md"
              },
              {
                "name": "plugin-marketplace",
                "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                "path": "claude-code/skills/plugin-marketplace/SKILL.md",
                "frontmatter": {
                  "name": "plugin-marketplace",
                  "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin Marketplace\n\nGuide for creating, validating, and managing plugin marketplaces for Claude Code. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/marketplace.json` files\n- Validating marketplace schema compliance\n- Setting up plugin repositories with marketplaces\n- Troubleshooting marketplace configuration issues\n- Converting plugin structures to marketplace format\n- Creating plugin entries with advanced features\n\n## Marketplace Schema Overview\n\n### Required Structure\n\nAll marketplaces must be located at `.claude-plugin/marketplace.json` in the repository root.\n\n**Required Fields:**\n- `name`: Marketplace identifier (kebab-case, lowercase alphanumeric and hyphens only)\n- `owner`: Object with maintainer details (`name` required, `email` optional)\n- `plugins`: Array of plugin definitions (can be empty)\n\n**Optional Metadata:**\n- `metadata.description`: Summary of marketplace purpose\n- `metadata.version`: Marketplace version tracking (semantic versioning recommended)\n- `metadata.pluginRoot`: Base directory for relative plugin source paths\n\n### Plugin Entry Schema\n\n**IMPORTANT: Schema Relationship**\n\nPlugin entries use the plugin manifest schema with all fields made optional, plus marketplace-specific fields (`source`, `strict`, `category`, `tags`). This means any field valid in a plugin.json file can also be used in a marketplace entry.\n\n- When `strict: false`, the marketplace entry serves as the complete plugin manifest if no plugin.json exists\n- When `strict: true` (default), marketplace fields supplement the plugin's own manifest file\n\nEach plugin entry in the `plugins` array requires:\n\n**Mandatory:**\n- `name`: Plugin identifier (kebab-case)\n- `source`: Location specification (string path or object)\n\n**Standard Metadata:**\n- `description`: Brief explanation of plugin functionality\n- `version`: Semantic version number\n- `author`: Creator information (object with `name`, optional `email`)\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of discovery and categorization tags\n- `category`: Organizational grouping\n- `tags`: Additional searchability terms\n\n**Component Configuration:**\n- `commands`: Custom paths to command files or directories\n- `agents`: Custom paths to agent files\n- `hooks`: Custom hooks configuration or path to hooks file\n- `mcpServers`: MCP server configurations or path to MCP config\n- `skills`: Array of skill directory paths\n\n**Strict Mode Control:**\n- `strict`: Boolean (default: `true`)\n  - `true`: Plugin must include plugin.json; marketplace fields supplement it\n  - `false`: Marketplace entry serves as complete manifest (no plugin.json needed)\n\n**Dependencies:**\n- `dependencies`: Array of plugin names this plugin depends on (format: `\"namespace:plugin-name\"`)\n\n## Plugin Source Formats\n\n### Relative Path\n```json\n\"source\": \"./plugins/my-plugin\"\n```\n\n### Relative Path with pluginRoot\n```json\n// In marketplace metadata\n\"metadata\": {\n  \"pluginRoot\": \"./plugins\"\n}\n\n// In plugin entry\n\"source\": \"my-plugin\"  // Resolves to ./plugins/my-plugin\n```\n\n### GitHub Repository\n```json\n\"source\": {\n  \"source\": \"github\",\n  \"repo\": \"owner/plugin-repo\",\n  \"path\": \"optional/subdirectory\",\n  \"branch\": \"main\"\n}\n```\n\n### Git URL\n```json\n\"source\": {\n  \"source\": \"url\",\n  \"url\": \"https://gitlab.com/team/plugin.git\",\n  \"branch\": \"main\"\n}\n```\n\n## Environment Variables\n\nUse `${CLAUDE_PLUGIN_ROOT}` in paths to reference the plugin's installation directory:\n\n```json\n{\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/my-skill\"\n  ],\n  \"commands\": [\n    \"${CLAUDE_PLUGIN_ROOT}/commands\"\n  ]\n}\n```\n\nThis ensures paths work correctly regardless of installation location.\n\n## Advanced Plugin Entry Features\n\n### Inline Plugin Definitions\n\nUse `strict: false` to define complete plugin manifests inline without requiring plugin.json:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"source\": \"./plugins/my-plugin\",\n  \"strict\": false,\n  \"description\": \"Complete plugin definition inline\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Developer Name\"\n  },\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\",\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-two\"\n  ]\n}\n```\n\n### Component Path Override\n\nCustomize component locations:\n\n```json\n{\n  \"name\": \"custom-paths\",\n  \"source\": \"./plugins/custom\",\n  \"strict\": false,\n  \"commands\": [\"${CLAUDE_PLUGIN_ROOT}/custom-commands\"],\n  \"agents\": [\"${CLAUDE_PLUGIN_ROOT}/custom-agents\"],\n  \"hooks\": {\n    \"onInstall\": \"${CLAUDE_PLUGIN_ROOT}/hooks/install.sh\"\n  },\n  \"mcpServers\": \"${CLAUDE_PLUGIN_ROOT}/mcp-config.json\"\n}\n```\n\n### Metadata Supplementation\n\nWith `strict: true`, marketplace entries can add metadata not in plugin.json:\n\n```json\n{\n  \"name\": \"existing-plugin\",\n  \"source\": \"./plugins/existing\",\n  \"strict\": true,\n  \"category\": \"development\",\n  \"keywords\": [\"added\", \"from\", \"marketplace\"],\n  \"homepage\": \"https://docs.example.com\"\n}\n```\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate marketplace.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\nThis validates:\n- JSON syntax\n- Required fields presence\n- Kebab-case naming\n- Field type correctness\n- Source path accessibility (for relative paths)\n\n### 2. Plugin Entry Validation\n\nValidate individual plugin entries:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-entry.nu .claude-plugin/marketplace.json \"plugin-name\"\n```\n\nChecks:\n- Required fields (name, source)\n- Strict mode consistency\n- Dependency references\n- Path validity\n- Component configuration\n\n### 3. Dependency Graph Validation\n\nCheck for circular dependencies and missing dependencies:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-dependencies.nu .claude-plugin/marketplace.json\n```\n\n## Best Practices\n\n### Naming Conventions\n\n- **Marketplace name**: Use your GitHub username or organization (e.g., `vinnie357`)\n- **Plugin names**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Categories**: Standardize on common categories: `development`, `language`, `tools`, `frontend`, `backend`, `meta`\n\n### Versioning Strategy\n\n- Use semantic versioning for both marketplace and plugins\n- Bump marketplace version when adding/removing plugins\n- Bump plugin versions when updating skills or configuration\n- Document breaking changes in plugin descriptions\n\n### Dependency Management\n\n- Always declare `dependencies` for plugins that require other plugins\n- Keep dependency chains shallow (avoid deep nesting)\n- Consider creating a meta-plugin (like `all-skills`) that bundles related plugins\n- Use namespace prefixes for dependencies (e.g., `all-skills:core`)\n\n### Strict Mode Decision\n\n**Use `strict: false` when:**\n- Creating simple, self-contained plugins\n- All configuration is in marketplace.json\n- You want centralized management\n- Plugin is unlikely to be distributed independently\n\n**Use `strict: true` when:**\n- Plugin has complex configuration\n- Plugin may be distributed separately\n- Plugin has its own versioning lifecycle\n- You want to supplement existing plugin.json with marketplace metadata\n\n### Source Path Organization\n\n```json\n{\n  \"metadata\": {\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"core\",\n      \"source\": \"core\"  // Resolves to ./plugins/core\n    },\n    {\n      \"name\": \"external\",\n      \"source\": {\n        \"source\": \"github\",\n        \"repo\": \"org/repo\"\n      }\n    }\n  ]\n}\n```\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Missing required owner field\n\n```json\n// ❌ Invalid\n{\n  \"name\": \"marketplace\"\n}\n\n// ✅ Valid\n{\n  \"name\": \"marketplace\",\n  \"owner\": {\n    \"name\": \"Developer Name\"\n  }\n}\n```\n\n### Error: Invalid source path\n\n```json\n// ❌ Invalid (path doesn't exist)\n\"source\": \"./plugins/nonexistent\"\n\n// ✅ Valid (path exists)\n\"source\": \"./plugins/core\"\n```\n\n### Error: Circular dependencies\n\n```json\n// ❌ Invalid\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-a\",\n      \"dependencies\": [\"namespace:plugin-b\"]\n    },\n    {\n      \"name\": \"plugin-b\",\n      \"dependencies\": [\"namespace:plugin-a\"]\n    }\n  ]\n}\n```\n\n## Creating a New Marketplace\n\n### Step 1: Initialize Structure\n\n```bash\nmkdir -p .claude-plugin\n```\n\n### Step 2: Create Marketplace File\n\nUse the validation script to generate a template:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-marketplace.nu\n```\n\nThis creates `.claude-plugin/marketplace.json` with required fields.\n\n### Step 3: Add Plugin Entries\n\nFor each plugin, decide on strict mode and add entry:\n\n```json\n{\n  \"name\": \"marketplace-name\",\n  \"owner\": {\n    \"name\": \"Your Name\",\n    \"email\": \"you@example.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Your marketplace description\",\n    \"version\": \"1.0.0\",\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"source\": \"plugin-name\",\n      \"strict\": false,\n      \"description\": \"Plugin description\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Your Name\"\n      },\n      \"license\": \"MIT\",\n      \"category\": \"development\",\n      \"skills\": [\n        \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\"\n      ]\n    }\n  ]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\n### Step 5: Test Installation\n\n```bash\nclaude-code install ./\n```\n\n## Migrating Existing Plugins\n\n### From Individual Plugins to Marketplace\n\n1. **Identify plugins**: List all plugin.json files\n2. **Decide on strict mode**: Choose per plugin based on complexity\n3. **Create marketplace.json**: Add all plugins with appropriate configuration\n4. **Test each plugin**: Verify installation works correctly\n5. **Document dependencies**: Add dependency arrays where needed\n\n### Migration Script\n\nUse the provided script to analyze existing structure:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/analyze-plugins.nu .\n```\n\nThis scans for plugin.json files and suggests marketplace.json structure.\n\n## Troubleshooting\n\n### Plugin Not Found After Installation\n\n- Verify `source` path is correct\n- Check `pluginRoot` in metadata if using relative paths\n- Ensure plugin directory exists at specified location\n\n### Skills Not Loading\n\n- Verify skill paths use `${CLAUDE_PLUGIN_ROOT}` if needed\n- Check that skill directories contain SKILL.md files\n- Validate skill paths in plugin entry or plugin.json\n\n### Dependency Resolution Fails\n\n- Ensure dependency names match exactly (including namespace)\n- Check that all dependencies are listed in marketplace\n- Verify no circular dependencies exist\n\n### Validation Errors\n\nRun validation script with verbose mode:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/schema-specification.md`: Complete JSON schema\n- `references/examples.md`: Real-world marketplace examples\n- `references/migration-guide.md`: Step-by-step migration instructions\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-marketplace.nu`: Full marketplace validation\n- `validate-plugin-entry.nu`: Individual plugin entry validation\n- `validate-dependencies.nu`: Dependency graph validation\n- `init-marketplace.nu`: Generate marketplace template\n- `analyze-plugins.nu`: Analyze existing plugin structure\n- `format-marketplace.nu`: Format and sort marketplace.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              },
              {
                "name": "accessibility",
                "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces",
                "path": "core/skills/accessibility/SKILL.md",
                "frontmatter": {
                  "name": "accessibility",
                  "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces"
                },
                "content": "# Web Accessibility\n\nApply W3C Web Accessibility Initiative (WAI) principles when working on web interfaces to ensure usability for people with disabilities.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing user interfaces\n- Reviewing code for accessibility compliance\n- Creating or editing web content (HTML, CSS, JavaScript)\n- Working with forms, navigation, multimedia, or interactive components\n- Conducting code reviews with accessibility considerations\n- Refactoring existing interfaces for better accessibility\n\n## Core Principles (POUR)\n\nWeb accessibility is organized around four foundational principles:\n\n### 1. Perceivable\n\nInformation must be presentable to users in ways they can perceive.\n\n**Key requirements:**\n- Provide text alternatives for non-text content (images, icons, charts)\n- Provide captions and transcripts for multimedia\n- Create content that can be presented in different ways (responsive, reflow)\n- Make content distinguishable (color contrast, text sizing, audio control)\n\n**Quick example:**\n```html\n<img src=\"chart.png\" alt=\"Sales increased 40% in Q4 2024\">\n<button aria-label=\"Close dialog\">\n  <span class=\"icon-close\" aria-hidden=\"true\"></span>\n</button>\n```\n\nFor detailed guidance on text alternatives, multimedia, and color contrast, see `references/perceivable.md`.\n\n### 2. Operable\n\nUser interface components must be operable by all users.\n\n**Key requirements:**\n- Make all functionality keyboard accessible\n- Provide sufficient time for users to complete tasks\n- Avoid content that causes seizures (no rapid flashing)\n- Help users navigate and find content\n- Support various input modalities (touch, voice, keyboard)\n\n**Quick example:**\n```html\n<button>Click me</button>  <!-- Already keyboard accessible -->\n\n<!-- Custom interactive element needs keyboard support -->\n<div role=\"button\" tabindex=\"0\"\n     onclick=\"handleClick()\"\n     onkeydown=\"handleKeyDown(event)\">\n  Custom Button\n</div>\n```\n\nFor keyboard patterns, focus management, and navigation, see `references/operable.md`.\n\n### 3. Understandable\n\nInformation and UI operation must be understandable.\n\n**Key requirements:**\n- Make text readable and understandable\n- Make web pages appear and operate predictably\n- Help users avoid and correct mistakes\n- Provide clear labels and instructions\n\n**Quick example:**\n```html\n<html lang=\"en\">\n<label for=\"email\">Email address</label>\n<input type=\"email\" id=\"email\"\n       aria-describedby=\"email-help\"\n       required>\n<div id=\"email-help\">We'll never share your email</div>\n```\n\nFor form patterns, error handling, and content clarity, see `references/understandable.md`.\n\n### 4. Robust\n\nContent must work reliably across user agents and assistive technologies.\n\n**Key requirements:**\n- Use valid, well-formed markup\n- Ensure compatibility with assistive technologies\n- Use ARIA correctly for custom components\n- Follow semantic HTML practices\n\n**Quick example:**\n```html\n<!-- Use semantic HTML first -->\n<nav aria-label=\"Main navigation\">\n  <ul>\n    <li><a href=\"/\">Home</a></li>\n  </ul>\n</nav>\n\n<!-- ARIA for custom components when needed -->\n<div role=\"dialog\" aria-labelledby=\"title\" aria-modal=\"true\">\n  <h2 id=\"title\">Dialog Title</h2>\n</div>\n```\n\nFor ARIA patterns and custom components, see `references/robust.md`.\n\n## Common Tasks\n\n### Making Forms Accessible\n\nConsult `references/forms.md` for comprehensive form accessibility including:\n- Label association\n- Error identification and suggestions\n- Required field indication\n- Input validation patterns\n\n### Implementing ARIA\n\nSee `references/aria.md` for:\n- When to use ARIA vs semantic HTML\n- Common ARIA patterns (tabs, accordions, modals)\n- ARIA states and properties\n- Live regions for dynamic content\n\n### Testing for Accessibility\n\nConsult `references/testing.md` for:\n- Keyboard navigation testing\n- Screen reader testing procedures\n- Automated testing tools\n- Color contrast checking\n\n### Common Patterns\n\nSee `references/patterns.md` for accessible implementations of:\n- Modal dialogs\n- Dropdown menus\n- Tabs and accordions\n- Loading states and notifications\n- Skip links and landmarks\n\n## Quick Reference Checklist\n\n**Every page should have:**\n- [ ] Valid HTML structure\n- [ ] Unique, descriptive page title\n- [ ] Proper heading hierarchy (h1, h2, h3...)\n- [ ] Language attribute on `<html>`\n- [ ] Sufficient color contrast (4.5:1 minimum)\n- [ ] Keyboard accessibility for all interactive elements\n- [ ] Visible focus indicators\n- [ ] Text alternatives for images\n- [ ] Form labels associated with inputs\n- [ ] Semantic landmark regions\n\n**For interactive components:**\n- [ ] Keyboard accessible (Tab, Enter, Space, Arrow keys)\n- [ ] Proper ARIA roles, states, and properties\n- [ ] Focus management (modals, dynamic content)\n- [ ] Descriptive labels and instructions\n- [ ] Error messages linked to form controls\n\n## Key Principles\n\n- **Semantic HTML first**: Use native HTML elements before adding ARIA\n- **Keyboard accessibility is fundamental**: If it works with mouse, it must work with keyboard\n- **Test with actual users**: Include people with disabilities in testing\n- **Color is not enough**: Never use color alone to convey information\n- **Provide alternatives**: Text for images, captions for video, transcripts for audio\n- **Make it predictable**: Consistent navigation and behavior across pages\n- **Help users recover**: Clear error messages with suggestions for correction\n\n## Resources\n\n- **WCAG 2.1 Guidelines**: https://www.w3.org/WAI/WCAG21/quickref/\n- **ARIA Authoring Practices**: https://www.w3.org/WAI/ARIA/apg/\n- **WebAIM**: https://webaim.org/\n- **MDN Accessibility**: https://developer.mozilla.org/en-US/docs/Web/Accessibility"
              },
              {
                "name": "anti-fabrication",
                "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                "path": "core/skills/anti-fabrication/SKILL.md",
                "frontmatter": {
                  "name": "anti-fabrication",
                  "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                  "license": "MIT"
                },
                "content": "# Anti-Fabrication\n\nStrict requirements for ensuring factual, measurable, and validated outputs in all work products including documentation, research, reports, and analysis.\n\n## When to Use This Skill\n\nActivate when:\n- Writing documentation or creating research materials\n- Making claims about system capabilities, performance, or features\n- Providing estimates for time, effort, or complexity\n- Reporting test results or analysis outcomes\n- Creating any content that presents factual information\n- Generating metrics, statistics, or performance data\n\n## Core Principles\n\n### Evidence-Based Outputs\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Never assume file existence, system capabilities, or feature presence without verification\n- Validate integration recommendations through actual framework detection\n\n### Explicit Uncertainty\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- State when information cannot be verified: \"Unable to confirm without [specific check]\"\n- Acknowledge knowledge limitations rather than fabricating plausible-sounding content\n- Use conditional language when appropriate: \"may\", \"likely\", \"appears to\"\n\n### Factual Language\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Replace vague statements with specific, measurable observations\n- Report what was actually observed, not what should theoretically be true\n- Distinguish between verified facts and reasonable inferences\n\n## Prohibited Language and Claims\n\n### Superlatives to Avoid\nNever use unverified superlatives:\n- ❌ \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- ❌ \"best practice\", \"industry-leading\", \"cutting-edge\", \"state-of-the-art\"\n- ❌ \"robust\", \"scalable\", \"production-ready\" (without specific evidence)\n\nInstead, use factual descriptions:\n- ✅ \"follows the specification defined in [source]\"\n- ✅ \"implements [specific pattern] as documented in [reference]\"\n- ✅ \"tested with [specific conditions] and produced [specific results]\"\n\n### Unsubstantiated Metrics\nNever fabricate quantitative data:\n- ❌ Percentages without measurement: \"improves performance by 30%\"\n- ❌ Success rates without testing: \"has a 95% success rate\"\n- ❌ Arbitrary scores: \"code quality score of 8/10\"\n- ❌ Made-up statistics: \"reduces memory usage significantly\"\n\nInstead, provide verified measurements:\n- ✅ \"benchmark shows execution time decreased from 150ms to 98ms\"\n- ✅ \"passed 47 of 50 test cases (94%)\"\n- ✅ \"static analysis tool reports complexity score of 12\"\n\n### Assumed Capabilities\nNever claim features exist without verification:\n- ❌ \"This system supports authentication\" (without checking)\n- ❌ \"The API provides rate limiting\" (without reading docs/code)\n- ❌ \"This handles edge cases correctly\" (without testing)\n\nInstead, verify before claiming:\n- ✅ Use Read tool to check configuration files\n- ✅ Use Grep to search for specific implementations\n- ✅ Use Bash to test actual behavior\n- ✅ State \"requires verification\" if tools cannot confirm\n\n## Time and Effort Estimation Rules\n\n### Never Estimate Without Analysis\nDo not provide time estimates without factual basis:\n- ❌ \"This will take 15 minutes\"\n- ❌ \"Should be done in 2 hours\"\n- ❌ \"Quick task, won't take long\"\n- ❌ \"Simple fix\"\n\n### Data-Backed Estimates Only\nIf estimates are requested, execute tools first:\n1. Count files that need modification (using Glob)\n2. Measure code complexity (using Read and analysis)\n3. Assess dependencies (using Grep for imports/references)\n4. Review similar past work (if available)\n\nThen provide estimate with evidence:\n- ✅ \"Requires modifying 12 files based on grep search, estimated X hours\"\n- ✅ \"Analysis shows 3 integration points, complexity suggests Y time\"\n- ✅ \"Timeline requires analysis of [specific factors not yet measured]\"\n\n### When Unable to Estimate\nBe explicit about limitations:\n- ✅ \"Cannot provide time estimate without analyzing [specific aspects]\"\n- ✅ \"Requires investigation of [X, Y, Z] before estimating\"\n- ✅ \"Complexity assessment needed before timeline projection\"\n\n## Validation Requirements\n\n### File Claims\nBefore claiming files exist or contain specific content:\n```\n1. Use Read tool to verify file exists and check contents\n2. Use Glob to find files matching patterns\n3. Use Grep to verify specific code or content is present\n4. Never state \"file X contains Y\" without tool verification\n```\n\n**Example violations:**\n- ❌ \"The config file sets the timeout to 30 seconds\" (without reading it)\n- ❌ \"There are multiple test files for this module\" (without globbing)\n\n**Correct approach:**\n- ✅ Read the config file first, then report actual timeout value\n- ✅ Use Glob to find test files, then report count and names\n\n### System Integration\nBefore claiming system capabilities:\n```\n1. Use Bash to check installed tools/dependencies\n2. Read package.json, requirements.txt, or equivalent\n3. Verify environment variables and configuration\n4. Test actual behavior when possible\n```\n\n### Framework Detection\nBefore claiming framework presence or version:\n```\n1. Read package.json, Gemfile, mix.exs, or dependency file\n2. Search for framework-specific imports or patterns\n3. Check for framework configuration files\n4. Report specific version found, not assumed capabilities\n```\n\n### Test Results\nOnly report test outcomes after actual execution:\n```\n1. Execute tests using Bash tool\n2. Capture and read actual output\n3. Report specific pass/fail counts and error messages\n4. Never claim \"tests pass\" or \"all tests successful\" without execution\n```\n\n### Performance Claims\nOnly make performance statements based on measurement:\n```\n1. Run benchmarks or profiling tools\n2. Capture actual timing/memory data\n3. Report specific measurements with conditions\n4. State testing methodology used\n```\n\n## Anti-Patterns to Avoid\n\n### Fabricated Testing\n❌ \"The code has been thoroughly tested\"\n❌ \"All edge cases are handled\"\n❌ \"Test coverage is good\"\n\n✅ \"Executed test suite: 45 passing, 2 failing\"\n✅ \"Coverage report shows 78% line coverage\"\n✅ \"Tested with inputs [X, Y, Z], observed [specific results]\"\n\n### Unverified Architecture Claims\n❌ \"This follows microservices architecture\"\n❌ \"Uses event-driven design patterns\"\n❌ \"Implements SOLID principles\"\n\n✅ Use Grep to find specific patterns, then describe what exists\n✅ \"Found 12 service definitions in [location]\"\n✅ \"Code shows [specific pattern] in [specific files]\"\n\n### Generic Quality Statements\n❌ \"This is high-quality code\"\n❌ \"Well-structured implementation\"\n❌ \"Follows best practices\"\n\n✅ \"Code follows [specific standard] as verified by linter\"\n✅ \"Matches patterns from [specific reference documentation]\"\n✅ \"Static analysis shows complexity metrics of [specific values]\"\n\n## Validation Workflow\n\nWhen creating any factual content:\n\n1. **Identify Claims**: List all factual assertions being made\n2. **Check Evidence**: For each claim, determine what tool can verify it\n3. **Execute Validation**: Run Read, Grep, Glob, Bash, or other tools\n4. **Report Results**: State only what tools confirmed\n5. **Mark Uncertainty**: Clearly label anything not verified\n\n## Examples\n\n### Documentation Writing\n\n**Bad approach:**\n```markdown\nThis API is highly performant and handles thousands of requests per second.\nIt follows RESTful best practices and includes comprehensive error handling.\n```\n\n**Good approach:**\n```markdown\nThis API implements REST endpoints as defined in [specification link].\nLoad testing with Apache Bench shows handling of 1,200 requests/second\nat 95th percentile latency of 45ms. Error handling covers HTTP status codes\n400, 401, 403, 404, 500 as verified in [source file].\n```\n\n### Research Output\n\n**Bad approach:**\n```markdown\nReact hooks are the modern way to write React components and are much\nbetter than class components. They improve performance and code quality.\n```\n\n**Good approach:**\n```markdown\nReact hooks (introduced in React 16.8 per official changelog) provide\nfunction component state and lifecycle features previously requiring\nclasses. The React documentation at [URL] states hooks reduce component\nnesting and enable logic reuse. Performance impact requires measurement\nfor specific use cases.\n```\n\n### Implementation Planning\n\n**Bad approach:**\n```markdown\nThis should be a quick implementation, probably 2-3 hours.\nWe'll add authentication which is straightforward, then deploy.\n```\n\n**Good approach:**\n```markdown\nImplementation requires:\n- Authentication integration (12 files need modification per grep analysis)\n- Configuration of [specific auth provider]\n- Testing of login/logout flows\n\nComplexity assessment needed before timeline estimation. Requires\ninvestigation of existing auth patterns and deployment requirements.\n```\n\n## Integration with Other Skills\n\nThis skill should be active alongside:\n- **Documentation**: Ensures docs contain verified information\n- **Code Review**: Validates claims about code quality and patterns\n- **Research**: Grounds research in verifiable sources\n- **Git Operations**: Ensures accurate commit messages and PR descriptions\n\n## References\n\n- Agent Skills Specification: Factual, validated skill content\n- Scientific Method: Observation before conclusion\n- Verification Principle: Trust but verify through tool execution"
              },
              {
                "name": "code-review",
                "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices",
                "path": "core/skills/code-review/SKILL.md",
                "frontmatter": {
                  "name": "code-review",
                  "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices"
                },
                "content": "# Code Review Best Practices\n\nThis skill activates when reviewing code for quality, correctness, security, and maintainability.\n\n## When to Use This Skill\n\nActivate when:\n- Reviewing pull requests\n- Conducting code audits\n- Providing feedback on code quality\n- Identifying security vulnerabilities\n- Suggesting refactoring improvements\n- Checking adherence to coding standards\n\n## Code Review Checklist\n\n### 1. Correctness and Functionality\n\n**Does the code do what it's supposed to do?**\n\n- Logic is correct and handles all cases\n- Edge cases are considered\n- Error handling is appropriate\n- No obvious bugs or logical errors\n- Assertions and validations are present\n- Return values are correct\n\n**Questions to ask:**\n- What happens if this receives null/nil?\n- What if the list is empty?\n- What if the number is negative/zero?\n- Are there off-by-one errors?\n- Are comparisons correct (>, >=, <, <=)?\n\n### 2. Security\n\n**Is the code secure?**\n\n- No SQL injection vulnerabilities\n- No XSS (Cross-Site Scripting) vulnerabilities\n- No CSRF vulnerabilities (CSRF protection in place)\n- User input is validated and sanitized\n- Sensitive data is not logged\n- Authentication and authorization are properly implemented\n- No hardcoded secrets or credentials\n- File uploads are validated (type, size, content)\n- External URLs are validated\n- Rate limiting is in place for APIs\n\n**Common security issues:**\n\n```elixir\n# BAD: SQL injection vulnerability\nquery = \"SELECT * FROM users WHERE id = #{user_id}\"\n\n# GOOD: Use parameterized queries\nquery = from u in User, where: u.id == ^user_id\n\n# BAD: XSS vulnerability\nraw(\"<div>#{user_input}</div>\")\n\n# GOOD: Escape user input\n<div><%= user_input %></div>\n\n# BAD: Hardcoded secrets\napi_key = \"sk_live_123456789\"\n\n# GOOD: Use environment variables\napi_key = System.get_env(\"API_KEY\")\n\n# BAD: Mass assignment vulnerability\nUser.changeset(%User{}, params)\n\n# GOOD: Whitelist allowed fields\nUser.changeset(%User{}, params)\n# Where changeset only casts allowed fields:\n# cast(user, attrs, [:name, :email])\n```\n\n### 3. Performance\n\n**Is the code efficient?**\n\n- No N+1 query problems\n- Appropriate data structures chosen\n- Algorithms are efficient\n- Database indexes are used\n- Caching is implemented where appropriate\n- Large datasets are paginated or streamed\n- Unnecessary computations are avoided\n- Resources are cleaned up properly\n\n**Common performance issues:**\n\n```elixir\n# BAD: N+1 query\nposts = Repo.all(Post)\nEnum.map(posts, fn post ->\n  author = Repo.get(User, post.author_id)  # Query for each post!\n  {post, author}\nend)\n\n# GOOD: Preload associations\nposts = Post |> preload(:author) |> Repo.all()\n\n# BAD: Loading entire dataset\nusers = Repo.all(User)  # Loads all millions of users\nEnum.filter(users, & &1.active)\n\n# GOOD: Query in database\nusers = User |> where(active: true) |> Repo.all()\n\n# BAD: Inefficient data structure\nlist = [1, 2, 3, 4, 5]\nif 3 in list do  # O(n) lookup in list\n  # ...\nend\n\n# GOOD: Use set/map for lookups\nset = MapSet.new([1, 2, 3, 4, 5])\nif MapSet.member?(set, 3) do  # O(1) lookup\n  # ...\nend\n```\n\n### 4. Code Quality and Maintainability\n\n**Is the code readable and maintainable?**\n\n- Clear, descriptive variable and function names\n- Functions are small and focused (single responsibility)\n- No code duplication (DRY principle)\n- Comments explain \"why\", not \"what\"\n- Code follows project conventions and style guide\n- Magic numbers are replaced with named constants\n- Complexity is minimized\n- Code is self-documenting\n\n**Code quality issues:**\n\n```elixir\n# BAD: Unclear names\ndef calc(x, y, z) do\n  r = x * y / z\n  r * 1.2\nend\n\n# GOOD: Clear names\ndef calculate_discounted_price(quantity, unit_price, discount_percentage) do\n  subtotal = quantity * unit_price\n  discount_amount = subtotal * (discount_percentage / 100)\n  subtotal - discount_amount\nend\n\n# BAD: Long function with multiple responsibilities\ndef process_order(order) do\n  # Validate order (responsibility 1)\n  # Calculate totals (responsibility 2)\n  # Update inventory (responsibility 3)\n  # Send email (responsibility 4)\n  # Log analytics (responsibility 5)\nend\n\n# GOOD: Single responsibility functions\ndef process_order(order) do\n  with {:ok, order} <- validate_order(order),\n       {:ok, order} <- calculate_totals(order),\n       {:ok, order} <- update_inventory(order),\n       :ok <- send_confirmation_email(order),\n       :ok <- log_order_analytics(order) do\n    {:ok, order}\n  end\nend\n\n# BAD: Magic numbers\nif user.age >= 13 do\n  # ...\nend\n\n# GOOD: Named constants\n@minimum_age_coppa 13\n\nif user.age >= @minimum_age_coppa do\n  # ...\nend\n```\n\n### 5. Error Handling\n\n**Are errors handled properly?**\n\n- Errors don't crash the system unexpectedly\n- Error messages are helpful\n- Errors are logged appropriately\n- Happy path and error paths are both tested\n- No swallowed errors (empty catch blocks)\n- Proper error types are used\n\n**Error handling patterns:**\n\n```elixir\n# BAD: Silent failure\ntry do\n  dangerous_operation()\nrescue\n  _ -> nil  # Error is swallowed!\nend\n\n# GOOD: Handle errors explicitly\ncase dangerous_operation() do\n  {:ok, result} -> result\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n\n# BAD: Generic error message\n{:error, \"failed\"}\n\n# GOOD: Specific error\n{:error, :invalid_email_format}\n{:error, {:validation_failed, errors}}\n\n# BAD: Let it crash when shouldn't\ndef parse_config(path) do\n  File.read!(path)  # Crashes if file missing\n  |> Jason.decode!()  # Crashes if invalid JSON\nend\n\n# GOOD: Handle expected errors\ndef parse_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  else\n    {:error, :enoent} -> {:error, :config_file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_config_format}\n  end\nend\n```\n\n### 6. Testing\n\n**Is the code properly tested?**\n\n- New functionality has tests\n- Edge cases are tested\n- Error conditions are tested\n- Tests are clear and focused\n- Tests are deterministic (no flaky tests)\n- Test names describe what they test\n- Mocks are used appropriately\n- Test coverage is adequate\n\n**Testing concerns:**\n\n```elixir\n# BAD: Unclear test name\ntest \"test1\" do\n  # ...\nend\n\n# GOOD: Descriptive test name\ntest \"create_user/1 returns error when email is invalid\" do\n  # ...\nend\n\n# BAD: Testing too much at once\ntest \"user workflow\" do\n  # Creates user\n  # Updates user\n  # Deletes user\n  # All in one test!\nend\n\n# GOOD: Focused tests\ntest \"create_user/1 creates user with valid attributes\" do\n  # ...\nend\n\ntest \"update_user/2 updates user name\" do\n  # ...\nend\n\ntest \"delete_user/1 removes user from database\" do\n  # ...\nend\n\n# BAD: Non-deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  Process.sleep(100)  # Race condition!\n  assert operation_completed?()\nend\n\n# GOOD: Deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  assert_receive {:completed, _result}, 1000\nend\n```\n\n### 7. Documentation\n\n**Is the code documented?**\n\n- Public APIs have documentation\n- Complex logic has explanatory comments\n- README is updated if needed\n- Changelog is updated for user-facing changes\n- API documentation is accurate\n- Examples are provided\n\n### 8. Dependencies\n\n**Are dependencies handled properly?**\n\n- New dependencies are justified\n- Dependencies are up-to-date and maintained\n- Licenses are compatible with project\n- Security vulnerabilities are checked\n- Dependency versions are pinned or bounded\n\n## Review Process\n\n### Before Reviewing\n\n1. **Understand the context**\n   - Read the PR description\n   - Understand the problem being solved\n   - Check related issues\n\n2. **Build and test locally**\n   - Pull the branch\n   - Run tests\n   - Test the functionality manually\n\n### During Review\n\n1. **Start with the big picture**\n   - Is the approach sound?\n   - Does it fit the architecture?\n   - Is there a better way?\n\n2. **Review for correctness**\n   - Does it work as intended?\n   - Are edge cases handled?\n   - Is error handling appropriate?\n\n3. **Check security and performance**\n   - Are there security vulnerabilities?\n   - Will it perform well at scale?\n\n4. **Review code quality**\n   - Is it readable and maintainable?\n   - Does it follow conventions?\n   - Is it well-tested?\n\n### Providing Feedback\n\n**Be constructive and specific:**\n\n```markdown\n# BAD: Vague criticism\n\"This function is bad.\"\n\n# GOOD: Specific, actionable feedback\n\"This function has three responsibilities: validation, database update, and email sending. Consider splitting it into separate functions for better testability and maintainability:\n\n```elixir\ndef update_user(user, attrs) do\n  with {:ok, changeset} <- validate_user_update(user, attrs),\n       {:ok, user} <- save_user(changeset),\n       :ok <- send_update_notification(user) do\n    {:ok, user}\n  end\nend\n```\n\n# BAD: Demanding\n\"You must change this.\"\n\n# GOOD: Collaborative\n\"What do you think about extracting this into a separate function? It would make the code easier to test.\"\n\n# BAD: Nitpicking without context\n\"Use single quotes instead of double quotes.\"\n\n# GOOD: Explain reasoning\n\"Our style guide prefers single quotes for consistency (see CONTRIBUTING.md section 3.2).\"\n```\n\n**Use labels to categorize feedback:**\n\n- **[blocking]**: Must be fixed before merging\n- **[suggestion]**: Optional improvement\n- **[question]**: Asking for clarification\n- **[nit]**: Very minor, cosmetic issue\n- **[security]**: Security concern\n- **[performance]**: Performance concern\n\n**Example:**\n\n```markdown\n[blocking] This creates a SQL injection vulnerability. Use parameterized queries:\n\n```elixir\n# Instead of:\nquery = \"SELECT * FROM users WHERE name = '#{name}'\"\n\n# Use:\nfrom(u in User, where: u.name == ^name)\n```\n\n[suggestion] Consider extracting this logic into a separate function for reusability.\n\n[question] Why are we using a map here instead of a struct?\n\n[nit] Extra blank line here.\n```\n\n### After Review\n\n1. **Respond to author's questions**\n2. **Re-review after changes**\n3. **Approve when satisfied**\n4. **Celebrate good code**\n\n## Language-Specific Considerations\n\n### Elixir\n\n- Pattern matching is used effectively\n- Functions leverage pipe operator for readability\n- Atoms aren't created dynamically from untrusted input\n- `with` statements handle errors properly\n- Changesets validate all input\n- No direct database queries in controllers/LiveViews (use contexts)\n\n### JavaScript/TypeScript\n\n- Types are properly defined (TypeScript)\n- Promises are handled with .catch() or try/catch\n- == vs === is used correctly\n- Arrays/objects aren't mutated unexpectedly\n- this binding is correct\n- Async operations are properly awaited\n\n### Python\n\n- Type hints are used\n- List comprehensions aren't overly complex\n- Exceptions are specific (not bare except:)\n- Resources are closed (use with statements)\n- Code follows PEP 8\n\n### Rust\n\n- Ownership and borrowing are correct\n- Error handling uses Result/Option properly\n- Unsafe blocks are justified and minimal\n- Clone/copy is used appropriately\n- Lifetimes are correctly specified\n\n## Common Code Smells\n\n### Complexity Smells\n\n- **Long functions** - Function does too much\n- **Long parameter list** - Too many parameters\n- **Deep nesting** - Too many levels of indentation\n- **Complex conditionals** - Hard to understand if statements\n\n### Duplication Smells\n\n- **Copy-paste code** - Same code in multiple places\n- **Similar functions** - Functions that do almost the same thing\n- **Magic numbers** - Repeated literal values\n\n### Naming Smells\n\n- **Unclear names** - Variables like x, tmp, data\n- **Misleading names** - Name doesn't match behavior\n- **Inconsistent names** - Same concept called different things\n\n### Design Smells\n\n- **God object** - Class/module doing everything\n- **Feature envy** - Function using another object's data more than its own\n- **Inappropriate intimacy** - Too much coupling between modules\n\n## Anti-Patterns to Watch For\n\n### Premature Optimization\n\n```elixir\n# BAD: Optimizing before measuring\ndef calculate(data) do\n  # Complex, hard-to-read optimization\n  # that saves 0.1ms\nend\n\n# GOOD: Start simple, optimize if needed\ndef calculate(data) do\n  # Clear, simple code\n  # Optimize later if profiling shows bottleneck\nend\n```\n\n### Premature Abstraction\n\n```elixir\n# BAD: Abstract after one use\ndefmodule AbstractDataProcessorFactoryBuilder do\n  # Complex abstraction for single use case\nend\n\n# GOOD: Wait for second use case\ndef process_user_data(data) do\n  # Simple, direct implementation\n  # Abstract when pattern emerges\nend\n```\n\n### Error Swallowing\n\n```elixir\n# BAD: Hiding errors\ntry do\n  risky_operation()\nrescue\n  _ -> :ok  # What went wrong?\nend\n\n# GOOD: Handle explicitly\ncase risky_operation() do\n  {:ok, result} -> {:ok, result}\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n```\n\n## Review Etiquette\n\n### DO:\n\n- Be respectful and constructive\n- Assume good intent\n- Ask questions instead of making demands\n- Praise good code\n- Explain the \"why\" behind suggestions\n- Offer to pair program on complex issues\n- Respond promptly to author's replies\n\n### DON'T:\n\n- Be sarcastic or condescending\n- Bike-shed on minor style issues\n- Block on personal preferences\n- Review your own code without another reviewer\n- Approve code you don't understand\n- Nitpick excessively\n\n## Self-Review Checklist\n\nBefore submitting code for review:\n\n- [ ] Code compiles and runs\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] No commented-out code\n- [ ] No debug print statements\n- [ ] Documentation is updated\n- [ ] Commit messages are clear\n- [ ] No secrets or sensitive data\n- [ ] Code follows project style guide\n- [ ] Changes are focused (no unrelated changes)\n\n## Key Principles\n\n- **Correctness first**: Code must work correctly\n- **Security matters**: Always consider security implications\n- **Be specific**: Provide actionable, concrete feedback\n- **Be respectful**: Kind, constructive communication\n- **Focus on important issues**: Don't bike-shed\n- **Explain reasoning**: Help author learn, don't just dictate\n- **Approve good code**: Don't let perfect be enemy of good\n- **Collaborate**: You're on the same team"
              },
              {
                "name": "documentation-writing",
                "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation",
                "path": "core/skills/documentation/SKILL.md",
                "frontmatter": {
                  "name": "documentation-writing",
                  "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation"
                },
                "content": "# Technical Documentation Writing\n\nThis skill activates when writing or improving technical documentation, including README files, API documentation, user guides, and inline code documentation.\n\n## When to Use This Skill\n\nActivate when:\n- Writing README files\n- Creating API documentation\n- Writing user guides or tutorials\n- Documenting code with comments or docstrings\n- Creating architecture or design documents\n- Writing changelogs or release notes\n\n## README Files\n\n### Essential README Structure\n\nEvery README should include:\n\n```markdown\n# Project Name\n\nBrief one-liner description of the project.\n\n## Overview\n\n2-3 paragraphs explaining what the project does, why it exists, and who it's for.\n\n## Features\n\n- Key feature 1\n- Key feature 2\n- Key feature 3\n\n## Installation\n\n### Prerequisites\n\n- Requirement 1 (with version)\n- Requirement 2 (with version)\n\n### Install Steps\n\n```bash\n# Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies\nnpm install  # or pip install -r requirements.txt, mix deps.get, etc.\n\n# Configure\ncp .env.example .env\n# Edit .env with your settings\n\n# Run\nnpm start\n```\n\n## Quick Start\n\n```bash\n# Minimal example to get started\nnpm start\n```\n\n## Usage\n\n### Basic Example\n\n```language\n// Clear, runnable example\nconst example = new Project()\nexample.doSomething()\n```\n\n### Advanced Usage\n\nMore complex examples with explanations.\n\n## Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `apiKey` | string | - | API key for authentication |\n| `timeout` | number | 5000 | Request timeout in ms |\n\n## API Reference\n\nLink to detailed API documentation or include core APIs here.\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Development-specific setup\nnpm install --dev\nnpm run setup\n```\n\n### Running Tests\n\n```bash\nnpm test\nnpm run test:coverage\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Credits\n- Inspirations\n- Related projects\n\n## Support\n\n- Documentation: https://docs.example.com\n- Issues: https://github.com/user/project/issues\n- Discussions: https://github.com/user/project/discussions\n```\n\n### README Best Practices\n\n- **Start with a clear one-liner**: Immediately tell readers what the project does\n- **Include badges**: Build status, coverage, version, license\n- **Show, don't tell**: Use code examples liberally\n- **Keep it scannable**: Use headers, lists, and code blocks\n- **Make examples runnable**: Readers should be able to copy-paste and run\n- **Include visual aids**: Screenshots, diagrams, GIFs when appropriate\n- **Update regularly**: Keep documentation in sync with code\n- **Think about newcomers**: Write for someone seeing the project for the first time\n\n## API Documentation\n\n### Documenting Functions\n\n**Elixir (@doc):**\n```elixir\n@doc \"\"\"\nCalculates the sum of two numbers.\n\n## Parameters\n\n- `a` - The first number (integer or float)\n- `b` - The second number (integer or float)\n\n## Returns\n\nThe sum of `a` and `b`.\n\n## Examples\n\n    iex> Math.add(2, 3)\n    5\n\n    iex> Math.add(2.5, 3.7)\n    6.2\n\n\"\"\"\n@spec add(number(), number()) :: number()\ndef add(a, b) do\n  a + b\nend\n```\n\n**JavaScript (JSDoc):**\n```javascript\n/**\n * Calculates the sum of two numbers.\n *\n * @param {number} a - The first number\n * @param {number} b - The second number\n * @returns {number} The sum of a and b\n *\n * @example\n * add(2, 3)\n * // => 5\n */\nfunction add(a, b) {\n  return a + b\n}\n```\n\n**Python (docstring):**\n```python\ndef add(a: float, b: float) -> float:\n    \"\"\"\n    Calculate the sum of two numbers.\n\n    Args:\n        a: The first number\n        b: The second number\n\n    Returns:\n        The sum of a and b\n\n    Examples:\n        >>> add(2, 3)\n        5\n        >>> add(2.5, 3.7)\n        6.2\n\n    Raises:\n        TypeError: If arguments are not numbers\n    \"\"\"\n    return a + b\n```\n\n**Rust (doc comments):**\n```rust\n/// Calculates the sum of two numbers.\n///\n/// # Arguments\n///\n/// * `a` - The first number\n/// * `b` - The second number\n///\n/// # Returns\n///\n/// The sum of `a` and `b`\n///\n/// # Examples\n///\n/// ```\n/// use mylib::add;\n///\n/// assert_eq!(add(2, 3), 5);\n/// assert_eq!(add(2.5, 3.7), 6.2);\n/// ```\npub fn add(a: f64, b: f64) -> f64 {\n    a + b\n}\n```\n\n### Module/Class Documentation\n\nDocument the purpose, usage, and public API:\n\n```elixir\ndefmodule MyApp.UserManager do\n  @moduledoc \"\"\"\n  Manages user accounts and authentication.\n\n  The UserManager provides functions for creating, updating, and authenticating\n  users. It handles password hashing, session management, and user validation.\n\n  ## Usage\n\n      # Create a new user\n      {:ok, user} = UserManager.create_user(%{\n        email: \"alice@example.com\",\n        password: \"secure_password\"\n      })\n\n      # Authenticate\n      {:ok, user} = UserManager.authenticate(\"alice@example.com\", \"secure_password\")\n\n      # Update user\n      {:ok, updated} = UserManager.update_user(user, %{name: \"Alice Smith\"})\n\n  ## Configuration\n\n  Configure in `config/config.exs`:\n\n      config :my_app, MyApp.UserManager,\n        password_min_length: 8,\n        session_timeout: 3600\n\n  \"\"\"\nend\n```\n\n### API Endpoint Documentation\n\nDocument RESTful APIs clearly:\n\n```markdown\n## Endpoints\n\n### Create User\n\nCreates a new user account.\n\n**Endpoint:** `POST /api/users`\n\n**Authentication:** Not required\n\n**Request Body:**\n\n```json\n{\n  \"email\": \"alice@example.com\",\n  \"password\": \"secure_password\",\n  \"name\": \"Alice Smith\"\n}\n```\n\n**Response (201 Created):**\n\n```json\n{\n  \"id\": \"123\",\n  \"email\": \"alice@example.com\",\n  \"name\": \"Alice Smith\",\n  \"created_at\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n**Error Responses:**\n\n- `400 Bad Request` - Invalid input\n  ```json\n  {\n    \"error\": \"validation_error\",\n    \"details\": {\n      \"email\": [\"must be a valid email address\"],\n      \"password\": [\"must be at least 8 characters\"]\n    }\n  }\n  ```\n\n- `409 Conflict` - Email already exists\n  ```json\n  {\n    \"error\": \"email_taken\",\n    \"message\": \"An account with this email already exists\"\n  }\n  ```\n\n**Example:**\n\n```bash\ncurl -X POST https://api.example.com/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"alice@example.com\",\n    \"password\": \"secure_password\",\n    \"name\": \"Alice Smith\"\n  }'\n```\n```\n\n## User Guides and Tutorials\n\n### Tutorial Structure\n\n```markdown\n# Tutorial: Building Your First [Feature]\n\n## What You'll Build\n\nBrief description of the end result.\n\n## Prerequisites\n\n- Knowledge requirement 1\n- Installed tool 1\n- Account/access requirement\n\n## Step 1: [First Major Step]\n\nExplanation of what we're doing and why.\n\n```language\n// Code for this step\n```\n\n**What's happening here:**\n- Explanation of key line 1\n- Explanation of key line 2\n\n## Step 2: [Next Step]\n\nContinue with incremental steps...\n\n## Testing\n\nHow to verify it works.\n\n## Next Steps\n\n- Related tutorial 1\n- Advanced topic 1\n- Further reading\n```\n\n### Tutorial Best Practices\n\n- **Show working code first**: Let readers see the goal before diving into details\n- **Explain the 'why'**: Don't just show what to do, explain reasoning\n- **Incremental steps**: Each step should build on the previous\n- **Include checkpoints**: Ways to verify progress\n- **Provide complete code**: Include a repository or final code snippet\n- **Anticipate problems**: Address common mistakes\n- **Link to references**: Point to relevant API docs and resources\n\n## Inline Code Documentation\n\n### When to Write Comments\n\n**DO write comments for:**\n- Complex algorithms or business logic\n- Non-obvious decisions (\"why\" not \"what\")\n- Workarounds for bugs or limitations\n- Public APIs and exported functions\n- Configuration and constants\n\n**DON'T write comments for:**\n- Obvious code\n- What the code does (prefer clear naming)\n- Outdated information\n- Commented-out code (use version control)\n\n### Good Comment Examples\n\n```elixir\n# Good: Explains WHY\n# Use exponential backoff to avoid overwhelming the API after rate limit errors\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Bad: Explains WHAT (obvious from code)\n# Multiply 2 to the power of attempt and multiply by 1000\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Good: Documents workaround\n# NOTE: Using String.to_existing_atom because the Erlang VM limits atoms to ~1M.\n# All valid status atoms are pre-defined in this module.\ndef parse_status(status_string) do\n  String.to_existing_atom(status_string)\nend\n\n# Good: Explains business rule\n# Users must be at least 13 years old per COPPA regulations\n@minimum_age 13\n```\n\n## Architecture Documentation\n\n### Architecture Decision Records (ADR)\n\nDocument significant architectural decisions:\n\n```markdown\n# ADR 001: Use PostgreSQL for Primary Database\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to choose a database for our application that supports:\n- ACID transactions\n- Complex queries with joins\n- JSON data storage\n- Full-text search\n- Horizontal scalability (future requirement)\n\n## Decision\n\nWe will use PostgreSQL as our primary database.\n\n## Consequences\n\n### Positive\n\n- Mature, stable, well-documented\n- Excellent JSON support with JSONB\n- Built-in full-text search\n- Strong consistency guarantees\n- Large ecosystem of tools and extensions\n- Can scale with read replicas and partitioning\n\n### Negative\n\n- More complex to operate than simpler databases\n- Vertical scaling has limits (though sufficient for our needs)\n- Requires more server resources than lighter alternatives\n\n### Neutral\n\n- Team needs to learn PostgreSQL-specific features\n- May need to hire PostgreSQL expertise as we scale\n\n## Alternatives Considered\n\n- **MySQL**: Weaker JSON support, less feature-rich\n- **MongoDB**: No ACID guarantees, eventual consistency issues\n- **SQLite**: Not suitable for multi-user web applications\n```\n\n## Changelog Documentation\n\nFollow Keep a Changelog format:\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- New feature in development\n\n## [1.2.0] - 2024-01-15\n\n### Added\n- User profile pictures\n- Email notification preferences\n- Dark mode support\n\n### Changed\n- Improved search performance by 40%\n- Updated UI to match new brand guidelines\n\n### Fixed\n- Login redirect loop on Safari\n- Memory leak in background sync process\n\n### Deprecated\n- Old `/v1/users` endpoint (use `/v2/users` instead)\n\n## [1.1.0] - 2024-01-01\n\n### Added\n- Two-factor authentication\n- Export user data to JSON\n\n### Security\n- Fixed XSS vulnerability in comment rendering\n\n## [1.0.0] - 2023-12-15\n\n### Added\n- Initial release\n- User registration and authentication\n- Basic user profiles\n```\n\n## Documentation Tools\n\n### Documentation Generators\n\n- **Elixir**: ExDoc - `mix docs`\n- **JavaScript**: JSDoc, TypeDoc\n- **Python**: Sphinx, MkDocs\n- **Rust**: rustdoc - `cargo doc`\n- **Static sites**: VitePress, Docusaurus, GitBook\n\n### Diagram Tools\n\n- **Mermaid**: Diagrams in Markdown\n  ```markdown\n  ```mermaid\n  graph TD\n      A[User] -->|Requests| B[Load Balancer]\n      B --> C[Web Server 1]\n      B --> D[Web Server 2]\n      C --> E[Database]\n      D --> E\n  ```\n  ```\n\n- **PlantUML**: UML diagrams as code\n- **Excalidraw**: Hand-drawn style diagrams\n- **Draw.io**: Flowcharts and diagrams\n\n## Documentation Style Guide\n\n### Writing Style\n\n- **Use active voice**: \"The function returns\" not \"The value is returned\"\n- **Be concise**: Remove unnecessary words\n- **Use present tense**: \"Returns\" not \"Will return\"\n- **Be specific**: \"Timeout in milliseconds\" not \"Timeout value\"\n- **Avoid jargon**: Or explain it when necessary\n- **Use examples**: Show, don't just tell\n\n### Formatting Conventions\n\n- **Code**: Use `backticks` for inline code\n- **Commands**: Show with `$` prefix or in code blocks\n- **File paths**: Use `code formatting`\n- **Emphasis**: Use **bold** for important points, *italic* for slight emphasis\n- **Lists**: Use bullets for unordered, numbers for sequential steps\n- **Headers**: Use sentence case, not title case\n\n### Code Examples\n\n- **Complete**: Include all necessary imports and setup\n- **Runnable**: Readers should be able to copy and run\n- **Realistic**: Use meaningful variable names and realistic data\n- **Commented**: Explain non-obvious parts\n- **Tested**: Ensure examples actually work\n- **Current**: Keep in sync with latest API\n\n## Documentation Maintenance\n\n### Keeping Docs Updated\n\n- Update documentation in the same PR as code changes\n- Review docs during code review\n- Set up doc linting (broken links, outdated examples)\n- Schedule regular documentation audits\n- Use version tags in examples when API changes\n- Mark deprecated features clearly\n\n### Documentation Testing\n\n```elixir\n# Elixir doctests - examples in docs are actual tests\ndefmodule Math do\n  @doc \"\"\"\n  Adds two numbers.\n\n  ## Examples\n\n      iex> Math.add(2, 3)\n      5\n\n  \"\"\"\n  def add(a, b), do: a + b\nend\n```\n\n```rust\n/// Adds two numbers.\n///\n/// # Examples\n///\n/// ```\n/// assert_eq!(add(2, 3), 5);\n/// ```\npub fn add(a: i32, b: i32) -> i32 {\n    a + b\n}\n```\n\n## Key Principles\n\n- **Write for your audience**: Tailor complexity to reader's experience level\n- **Show examples**: Code examples are worth a thousand words\n- **Keep it current**: Outdated docs are worse than no docs\n- **Make it scannable**: Use headers, lists, code blocks, and white space\n- **Explain the 'why'**: Help readers understand reasoning, not just steps\n- **Start simple**: Begin with quickstart, then go deeper\n- **Test documentation**: Ensure examples run and links work\n- **Iterate based on feedback**: Improve based on user questions and confusion"
              },
              {
                "name": "git-operations",
                "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits",
                "path": "core/skills/git/SKILL.md",
                "frontmatter": {
                  "name": "git-operations",
                  "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits"
                },
                "content": "# Git Operations and Best Practices\n\nThis skill activates when performing Git operations, managing repositories, resolving conflicts, or following Git workflows and conventions.\n\n## When to Use This Skill\n\nActivate when:\n- Creating commits or commit messages\n- Managing branches and merging\n- Resolving merge conflicts\n- Rebasing or rewriting history\n- Creating pull requests\n- Following Git workflows (Git Flow, GitHub Flow, trunk-based)\n- Troubleshooting Git issues\n\n## Commit Message Conventions\n\n### Conventional Commits\n\nFollow the Conventional Commits specification:\n\n```\n<type>(<scope>): <subject>\n\n<body>\n\n<footer>\n```\n\n**Types:**\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation changes\n- `style`: Code style changes (formatting, missing semicolons, etc.)\n- `refactor`: Code refactoring without changing behavior\n- `perf`: Performance improvements\n- `test`: Adding or updating tests\n- `build`: Changes to build system or dependencies\n- `ci`: CI/CD configuration changes\n- `chore`: Other changes that don't modify src or test files\n- `revert`: Revert a previous commit\n\n**Examples:**\n```\nfeat(auth): add JWT authentication\n\nImplement JWT-based authentication with refresh tokens.\n- Add JWT generation and validation\n- Implement refresh token rotation\n- Add authentication middleware\n\nCloses #123\n\nfix(api): handle null values in user response\n\nPreviously, null email addresses would cause the API to crash.\nNow returns empty string for null emails.\n\nFixes #456\n\ndocs: update installation instructions\n\nAdd section on environment variable configuration.\n\ntest(user): add tests for email validation\n\nrefactor(database): simplify query builder\n\nperf(api): add caching for user endpoints\n\nReduces response time by 40% for user list endpoint.\n```\n\n### Writing Good Commit Messages\n\n**Subject line (first line):**\n- Keep under 50 characters\n- Start with lowercase (after type)\n- No period at the end\n- Use imperative mood (\"add\" not \"added\" or \"adds\")\n\n**Body:**\n- Wrap at 72 characters\n- Explain what and why, not how\n- Separate from subject with blank line\n- Can have multiple paragraphs\n\n**Footer:**\n- Reference issues and pull requests\n- Note breaking changes\n- Add co-authors\n\n```\nfeat(api): add user search endpoint\n\nImplement full-text search across user names and emails using\nPostgreSQL's full-text search capabilities. Search results are\nranked by relevance.\n\nPerformance tested with 1M users - average response time < 100ms.\n\nBREAKING CHANGE: API now requires PostgreSQL 12+\n\nCloses #789\nCo-authored-by: Jane Doe <jane@example.com>\n```\n\n## Branch Management\n\n### Branch Naming\n\nUse descriptive, hierarchical branch names:\n\n```\n<type>/<short-description>\n<type>/<issue-number>-<short-description>\n```\n\n**Examples:**\n```\nfeature/user-authentication\nfeature/123-add-search\nfix/456-null-pointer-error\nbugfix/password-reset-email\nhotfix/critical-security-patch\nrefactor/database-queries\ndocs/api-documentation\nchore/update-dependencies\n```\n\n### Creating and Switching Branches\n\n```bash\n# Create and switch to new branch\ngit checkout -b feature/new-feature\n\n# Switch to existing branch\ngit checkout main\ngit switch main  # Modern alternative\n\n# Create branch from specific commit\ngit checkout -b hotfix/bug origin/main\n\n# List branches\ngit branch                    # Local branches\ngit branch -r                 # Remote branches\ngit branch -a                 # All branches\ngit branch -v                 # With last commit\n```\n\n### Deleting Branches\n\n```bash\n# Delete local branch\ngit branch -d feature/completed-feature\n\n# Force delete unmerged branch\ngit branch -D feature/abandoned-feature\n\n# Delete remote branch\ngit push origin --delete feature/old-feature\n```\n\n## Working with Changes\n\n### Staging Changes\n\n```bash\n# Stage specific files\ngit add file1.ex file2.ex\n\n# Stage all changes\ngit add .\ngit add -A\n\n# Stage parts of a file (interactive)\ngit add -p file.ex\n\n# Unstage files\ngit restore --staged file.ex\ngit reset HEAD file.ex  # Old syntax\n```\n\n### Committing\n\n```bash\n# Commit staged changes\ngit commit -m \"feat: add user authentication\"\n\n# Commit with body\ngit commit -m \"feat: add user authentication\" -m \"Implement JWT-based auth with refresh tokens\"\n\n# Amend last commit (change message or add files)\ngit add forgotten-file.ex\ngit commit --amend\n\n# Amend without changing message\ngit commit --amend --no-edit\n```\n\n### Viewing Changes\n\n```bash\n# Show unstaged changes\ngit diff\n\n# Show staged changes\ngit diff --cached\ngit diff --staged\n\n# Show changes in specific file\ngit diff path/to/file.ex\n\n# Show changes between branches\ngit diff main..feature/new-feature\n\n# Show changes between commits\ngit diff abc123..def456\n\n# Show stats only\ngit diff --stat\n```\n\n## Branching Workflows\n\n### Feature Branch Workflow\n\n```bash\n# Start new feature\ngit checkout main\ngit pull origin main\ngit checkout -b feature/new-feature\n\n# Work on feature\ngit add .\ngit commit -m \"feat: implement new feature\"\n\n# Keep feature updated with main\ngit checkout main\ngit pull origin main\ngit checkout feature/new-feature\ngit merge main\n\n# Push feature\ngit push -u origin feature/new-feature\n\n# After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n```\n\n### Rebasing Feature Branch\n\n```bash\n# Keep feature branch up-to-date with clean history\ngit checkout feature/new-feature\ngit fetch origin\ngit rebase origin/main\n\n# If conflicts occur, resolve them, then:\ngit add resolved-file.ex\ngit rebase --continue\n\n# Abort rebase if needed\ngit rebase --abort\n\n# Force push after rebase (careful!)\ngit push --force-with-lease origin feature/new-feature\n```\n\n## Merge Strategies\n\n### Fast-Forward Merge\n\n```bash\n# Default when possible - no merge commit\ngit checkout main\ngit merge feature/simple-feature\n```\n\n### No Fast-Forward\n\n```bash\n# Always create merge commit for history\ngit merge --no-ff feature/important-feature\n```\n\n### Squash Merge\n\n```bash\n# Combine all feature commits into one\ngit merge --squash feature/many-small-commits\ngit commit -m \"feat: add complete feature\"\n```\n\n## Conflict Resolution\n\n### Identifying Conflicts\n\n```bash\n# See conflicted files\ngit status\n\n# See conflict markers in file\n# <<<<<<< HEAD\n# Current branch changes\n# =======\n# Incoming changes\n# >>>>>>> feature/branch\n```\n\n### Resolving Conflicts\n\n```bash\n# Edit files to resolve conflicts, then:\ngit add resolved-file.ex\ngit commit  # Or git rebase --continue if rebasing\n\n# Use merge tools\ngit mergetool\n\n# Choose one side completely\ngit checkout --ours file.ex    # Keep our version\ngit checkout --theirs file.ex  # Keep their version\n```\n\n### Aborting Merge/Rebase\n\n```bash\n# Abort merge\ngit merge --abort\n\n# Abort rebase\ngit rebase --abort\n```\n\n## History Management\n\n### Interactive Rebase\n\nClean up commit history before merging:\n\n```bash\n# Rebase last 3 commits\ngit rebase -i HEAD~3\n\n# Rebase since main\ngit rebase -i main\n\n# Interactive rebase options:\n# pick - keep commit as-is\n# reword - change commit message\n# edit - modify commit\n# squash - combine with previous commit\n# fixup - like squash but discard message\n# drop - remove commit\n```\n\n**Example workflow:**\n```bash\n# You have commits:\n# abc123 fix typo\n# def456 add feature\n# ghi789 fix bug in feature\n# jkl012 add tests\n\ngit rebase -i HEAD~4\n\n# Change to:\n# pick def456 add feature\n# fixup ghi789 fix bug in feature\n# squash jkl012 add tests\n# reword abc123 fix typo\n```\n\n### Viewing History\n\n```bash\n# View commit history\ngit log\n\n# Compact one-line format\ngit log --oneline\n\n# Graph view\ngit log --graph --oneline --all\n\n# With file changes\ngit log --stat\n\n# Search commits\ngit log --grep=\"authentication\"\n\n# Commits by author\ngit log --author=\"John\"\n\n# Commits in date range\ngit log --since=\"2 weeks ago\"\ngit log --after=\"2024-01-01\" --before=\"2024-02-01\"\n\n# Follow file history\ngit log --follow -- path/to/file.ex\n\n# Show specific commit\ngit show abc123\n```\n\n### Undoing Changes\n\n```bash\n# Undo uncommitted changes\ngit restore file.ex\ngit checkout -- file.ex  # Old syntax\n\n# Restore all files\ngit restore .\n\n# Undo commit (keep changes)\ngit reset --soft HEAD~1\n\n# Undo commit (discard changes) - DANGEROUS\ngit reset --hard HEAD~1\n\n# Create new commit that undoes a commit\ngit revert abc123\n\n# Revert merge commit\ngit revert -m 1 abc123\n```\n\n## Stashing\n\nTemporarily save uncommitted changes:\n\n```bash\n# Stash changes\ngit stash\ngit stash push -m \"work in progress on feature\"\n\n# Stash including untracked files\ngit stash -u\n\n# List stashes\ngit stash list\n\n# Apply most recent stash\ngit stash apply\n\n# Apply and remove stash\ngit stash pop\n\n# Apply specific stash\ngit stash apply stash@{2}\n\n# Delete stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n# Create branch from stash\ngit stash branch feature/from-stash\n```\n\n## Remote Operations\n\n### Working with Remotes\n\n```bash\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add origin git@github.com:user/repo.git\n\n# Change remote URL\ngit remote set-url origin git@github.com:user/new-repo.git\n\n# Remove remote\ngit remote remove origin\n\n# Rename remote\ngit remote rename origin upstream\n```\n\n### Fetching and Pulling\n\n```bash\n# Fetch changes from remote\ngit fetch origin\n\n# Fetch all remotes\ngit fetch --all\n\n# Pull changes (fetch + merge)\ngit pull origin main\n\n# Pull with rebase\ngit pull --rebase origin main\n\n# Set upstream branch\ngit push -u origin feature/new-feature\ngit branch --set-upstream-to=origin/feature feature/new-feature\n```\n\n### Pushing\n\n```bash\n# Push to remote\ngit push origin main\n\n# Push and set upstream\ngit push -u origin feature/new-feature\n\n# Force push (CAREFUL!)\ngit push --force origin feature/branch\n\n# Safer force push - fails if remote has new commits\ngit push --force-with-lease origin feature/branch\n\n# Push all branches\ngit push --all origin\n\n# Push tags\ngit push --tags\n```\n\n## Tags\n\n### Creating Tags\n\n```bash\n# Lightweight tag\ngit tag v1.0.0\n\n# Annotated tag (preferred)\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\n\n# Tag specific commit\ngit tag -a v1.0.0 abc123 -m \"Release version 1.0.0\"\n```\n\n### Managing Tags\n\n```bash\n# List tags\ngit tag\ngit tag -l \"v1.*\"\n\n# View tag details\ngit show v1.0.0\n\n# Push tag\ngit push origin v1.0.0\n\n# Push all tags\ngit push origin --tags\n\n# Delete local tag\ngit tag -d v1.0.0\n\n# Delete remote tag\ngit push origin --delete v1.0.0\n```\n\n## Advanced Operations\n\n### Cherry-Picking\n\nApply specific commits to current branch:\n\n```bash\n# Apply single commit\ngit cherry-pick abc123\n\n# Apply multiple commits\ngit cherry-pick abc123 def456\n\n# Cherry-pick without committing\ngit cherry-pick -n abc123\n```\n\n### Bisect\n\nFind which commit introduced a bug:\n\n```bash\n# Start bisect\ngit bisect start\ngit bisect bad                    # Current commit is bad\ngit bisect good abc123            # Known good commit\n\n# Git will checkout commits to test\n# After testing each:\ngit bisect good  # or\ngit bisect bad\n\n# When found, Git shows first bad commit\n# Reset\ngit bisect reset\n```\n\n### Submodules\n\n```bash\n# Add submodule\ngit submodule add git@github.com:user/repo.git path/to/submodule\n\n# Clone with submodules\ngit clone --recurse-submodules git@github.com:user/repo.git\n\n# Update submodules\ngit submodule update --init --recursive\n\n# Pull submodule updates\ngit submodule update --remote\n```\n\n## GitHub Specific\n\n### Pull Requests\n\n```bash\n# Using GitHub CLI (gh)\ngh pr create --title \"feat: add new feature\" --body \"Description of changes\"\n\n# Create draft PR\ngh pr create --draft\n\n# List PRs\ngh pr list\n\n# View PR\ngh pr view 123\n\n# Checkout PR locally\ngh pr checkout 123\n\n# Merge PR\ngh pr merge 123 --squash\n```\n\n### Issues\n\n```bash\n# Create issue\ngh issue create --title \"Bug: authentication fails\" --body \"Description\"\n\n# List issues\ngh issue list\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n```\n\n## Git Aliases\n\nAdd to `.gitconfig`:\n\n```ini\n[alias]\n    co = checkout\n    br = branch\n    ci = commit\n    st = status\n    unstage = restore --staged\n    last = log -1 HEAD\n    lg = log --graph --oneline --all\n    cm = commit -m\n    ca = commit --amend\n    undo = reset --soft HEAD~1\n    sync = !git fetch origin && git rebase origin/main\n    clean-branches = !git branch --merged | grep -v \\\"\\\\*\\\" | xargs -n 1 git branch -d\n```\n\n## Best Practices\n\n### Commits\n\n- Make atomic commits - one logical change per commit\n- Commit often - small, focused commits are better\n- Write clear commit messages following conventions\n- Don't commit sensitive data (API keys, passwords)\n- Don't commit generated files (add to `.gitignore`)\n- Test before committing\n\n### Branches\n\n- Keep branches short-lived\n- Pull main/master frequently to stay updated\n- Delete branches after merging\n- Use descriptive branch names\n- One feature/fix per branch\n\n### History\n\n- Keep history clean with interactive rebase\n- Don't rewrite public history (after pushing)\n- Use `--force-with-lease` instead of `--force`\n- Squash small fixup commits before merging\n\n### Collaboration\n\n- Pull before pushing\n- Resolve conflicts promptly\n- Review changes before committing\n- Communicate about force pushes\n- Use pull requests for code review\n\n## Common Issues and Solutions\n\n### Accidentally Committed to Wrong Branch\n\n```bash\n# Move commit to new branch\ngit branch feature/new-branch\ngit reset --hard HEAD~1\ngit checkout feature/new-branch\n```\n\n### Need to Change Last Commit Message\n\n```bash\ngit commit --amend\n```\n\n### Committed Sensitive Data\n\n```bash\n# Remove from history - CAREFUL!\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/sensitive-file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (faster)\nbfg --delete-files sensitive-file\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n\n# Force push to update remote\ngit push --force --all\n```\n\n### Recover Deleted Branch\n\n```bash\n# Find commit where branch was\ngit reflog\n\n# Recreate branch\ngit checkout -b recovered-branch abc123\n```\n\n### Merge Went Wrong\n\n```bash\n# Undo merge (before pushing)\ngit reset --hard HEAD~1\n\n# Undo merge (after pushing)\ngit revert -m 1 merge-commit-hash\n```\n\n## Key Principles\n\n- **Commit early, commit often**: Small, focused commits\n- **Write clear messages**: Follow conventional commits\n- **Keep history clean**: Rebase and squash before merging\n- **Don't rewrite public history**: Only rebase local commits\n- **Use branches**: Never commit directly to main\n- **Pull before push**: Stay in sync with remote\n- **Review before commit**: Check what you're committing\n- **Use descriptive names**: For branches, commits, and PRs"
              },
              {
                "name": "material-design",
                "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming",
                "path": "core/skills/material-design/SKILL.md",
                "frontmatter": {
                  "name": "material-design",
                  "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming"
                },
                "content": "# Material Design 3 (Material You)\n\nApply Google's Material Design 3 principles when designing and developing user interfaces with emphasis on personalization, accessibility, and cross-platform consistency.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing Android applications\n- Building web applications following Material Design\n- Working with Flutter or Jetpack Compose\n- Implementing dynamic theming and color systems\n- Creating Material components\n- Reviewing designs for Material Design compliance\n\n## What is Material Design 3?\n\nMaterial Design 3 (Material You) represents Google's latest design system with:\n\n- **Personalization**: Dynamic color extraction from user preferences\n- **Expressiveness**: Softer, rounded components with visual hierarchy\n- **Adaptability**: Responsive across devices and platforms\n- **Accessibility**: Built-in inclusive design features\n\n## Key Differences from Material Design 2\n\n| Aspect | MD2 | MD3 |\n|--------|-----|-----|\n| **Colors** | Fixed brand palettes | Dynamic, user-generated schemes |\n| **Customization** | Limited theming | Highly personalized |\n| **Components** | Flat, rigid shapes | Rounded, expressive |\n| **Accessibility** | Basic support | Priority built-in |\n\n## Core Foundations\n\n### 1. Dynamic Color System\n\nMaterial Design 3 uses HCT (Hue, Chroma, Tone) color space for perceptually accurate color generation.\n\n**Key concepts:**\n- Color roles (primary, secondary, tertiary, error, neutral)\n- Tonal palettes (50-99 tones per color)\n- Automatic light/dark theme generation\n- User-driven personalization from wallpaper/system\n\nFor detailed color system implementation, see `references/color-system.md`.\n\n### 2. Typography\n\nType scale with 5 display sizes and 9 text sizes:\n\n**Quick example:**\n- Display Large: 57sp\n- Headline Large: 32sp\n- Body Large: 16sp\n- Label Small: 11sp\n\nFor complete typography system and responsive scaling, see `references/typography.md`.\n\n### 3. Layout\n\nResponsive breakpoints and grid system:\n\n- **Compact**: 0-599dp (phones)\n- **Medium**: 600-839dp (tablets, folded phones)\n- **Expanded**: 840dp+ (desktops, large tablets)\n\nFor layout guidelines and examples, see `references/layout.md`.\n\n## Component Guidelines\n\nMaterial Design 3 provides specifications for:\n\n- **Common Buttons**: Elevated, Filled, Tonal, Outlined, Text\n- **Cards**: Elevated, Filled, Outlined variants\n- **Text Fields**: Filled, Outlined with labels and helper text\n- **Navigation**: Navigation bar, rail, drawer\n- **Chips**: Assist, Filter, Input, Suggestion chips\n- **Dialogs**: Basic, Full-screen dialogs\n\nFor detailed component specifications, consult `references/components.md`.\n\n## Quick Component Examples\n\n### Buttons\n\n```kotlin\n// Jetpack Compose\nButton(onClick = { }) {\n    Text(\"Filled Button\")\n}\n\nOutlinedButton(onClick = { }) {\n    Text(\"Outlined Button\")\n}\n```\n\n### Cards\n\n```kotlin\nCard(\n    modifier = Modifier.fillMaxWidth(),\n    elevation = CardDefaults.cardElevation(defaultElevation = 6.dp)\n) {\n    Column(modifier = Modifier.padding(16.dp)) {\n        Text(\"Card Title\", style = MaterialTheme.typography.headlineSmall)\n        Text(\"Card content\", style = MaterialTheme.typography.bodyMedium)\n    }\n}\n```\n\n### Text Fields\n\n```kotlin\nOutlinedTextField(\n    value = text,\n    onValueChange = { text = it },\n    label = { Text(\"Label\") },\n    supportingText = { Text(\"Helper text\") }\n)\n```\n\nFor more component examples and patterns, see `references/components.md`.\n\n## Implementing Dynamic Color\n\n### Android (Jetpack Compose)\n\n```kotlin\nval dynamicColor = Build.VERSION.SDK_INT >= Build.VERSION_CODES.S\n\nval colorScheme = when {\n    dynamicColor && darkTheme -> dynamicDarkColorScheme(LocalContext.current)\n    dynamicColor && !darkTheme -> dynamicLightColorScheme(LocalContext.current)\n    darkTheme -> darkColorScheme()\n    else -> lightColorScheme()\n}\n\nMaterialTheme(\n    colorScheme = colorScheme,\n    typography = Typography,\n    content = content\n)\n```\n\n### Web\n\nFor web implementation with Material Web Components, see `references/web-implementation.md`.\n\n## Motion and Animation\n\nMaterial Design 3 motion principles:\n- **Easing**: Standard, emphasized, decelerated curves\n- **Duration**: Based on travel distance and complexity\n- **Choreography**: Coordinated element movements\n\nFor motion specifications, see `references/motion.md`.\n\n## Accessibility\n\nMaterial Design 3 prioritizes accessibility:\n- Minimum 4.5:1 contrast ratio (text)\n- 3:1 contrast ratio (UI components)\n- Touch targets minimum 48dp × 48dp\n- Screen reader support\n- Semantic color usage (not color-only indicators)\n\nFor accessibility implementation details, see `references/accessibility.md`.\n\n## When to Consult References\n\n- **Color system implementation**: Read `references/color-system.md`\n- **Typography scales and usage**: Read `references/typography.md`\n- **Layout and responsive design**: Read `references/layout.md`\n- **Component specifications**: Read `references/components.md`\n- **Web implementation**: Read `references/web-implementation.md`\n- **Motion and animation**: Read `references/motion.md`\n- **Accessibility guidelines**: Read `references/accessibility.md`\n\n## Key Principles\n\n- **User-driven personalization**: Colors adapt to user preferences\n- **Expressive and flexible**: Rounded corners, dynamic elevation\n- **Accessible by default**: Built-in contrast, touch targets, semantics\n- **Cross-platform consistency**: Same principles across Android, web, iOS\n- **Design tokens**: Use semantic tokens, not hardcoded values\n- **Responsive**: Adapt to device size and orientation\n\n## Resources\n\n- **Material Design 3**: https://m3.material.io/\n- **Material Theme Builder**: https://m3.material.io/theme-builder\n- **Jetpack Compose**: https://developer.android.com/jetpack/compose/designsystems/material3\n- **Material Web Components**: https://github.com/material-components/material-web\n- **Flutter Material 3**: https://flutter.dev/docs/development/ui/material"
              },
              {
                "name": "mise",
                "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects",
                "path": "core/skills/mise/SKILL.md",
                "frontmatter": {
                  "name": "mise",
                  "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects"
                },
                "content": "# mise - Development Environment Management\n\nThis skill activates when working with mise for managing tool versions, environment variables, and project tasks.\n\n## When to Use This Skill\n\nActivate when:\n- Setting up development environments\n- Managing tool and runtime versions (Node.js, Python, Ruby, Go, etc.)\n- Configuring environment variables and secrets\n- Defining and running project tasks\n- Creating reproducible development setups\n- Working with monorepos or multiple projects\n\n## What is mise?\n\nmise is a polyglot runtime manager and development environment tool that combines:\n- **Tool version management** - Install and manage multiple versions of dev tools\n- **Environment configuration** - Set environment variables per project\n- **Task automation** - Define and run project tasks\n- **Cross-platform** - Works on macOS, Linux, and Windows\n\n## Installation\n\n```bash\n# macOS/Linux (using curl)\ncurl https://mise.run | sh\n\n# macOS (using Homebrew)\nbrew install mise\n\n# Windows\n# See https://mise.jdx.dev for Windows install instructions\n\n# Activate mise in your shell\necho 'eval \"$(mise activate bash)\"' >> ~/.bashrc   # bash\necho 'eval \"$(mise activate zsh)\"' >> ~/.zshrc     # zsh\necho 'mise activate fish | source' >> ~/.config/fish/config.fish  # fish\n```\n\n## Managing Tools\n\n### Tool Backends\n\nmise uses different backends (package managers) to install tools. Understanding backends helps you install tools correctly.\n\n#### Available Backends\n\n- **asdf** - Traditional asdf plugins (default for many tools)\n- **ubi** - Universal Binary Installer (GitHub/GitLab releases)\n- **cargo** - Rust packages (requires Rust installed)\n- **npm** - Node.js packages (requires Node installed)\n- **go** - Go packages (requires Go installed)\n- **aqua** - Package manager\n- **pipx** - Python packages (requires Python installed)\n- **gem** - Ruby packages (requires Ruby installed)\n- **github/gitlab** - Direct from repositories\n- **http** - Direct HTTP downloads\n\n#### Verifying Tool Names\n\nAlways verify tool names using `mise ls-remote` before adding to configuration:\n\n```bash\n# Check if tool exists in registry\nmise ls-remote node\n\n# Check tool with specific backend\nmise ls-remote cargo:ripgrep\nmise ls-remote ubi:sharkdp/fd\n\n# Search the registry\nmise registry | grep <tool-name>\n```\n\n### Installing Tools\n\n```bash\n# List available tools in registry\nmise registry\n\n# Install from default backend\nmise install node@20.10.0\nmise install python@3.12\nmise install ruby@3.3\n\n# Install with specific backend\nmise install cargo:ripgrep        # From Rust crates\nmise install ubi:sharkdp/fd       # From GitHub releases\nmise install npm:typescript       # From npm\n\n# Install latest version\nmise install node@latest\n\n# Install from .mise.toml or .tool-versions\nmise install\n```\n\n### Using Tools with `mise use`\n\nThe `mise use` command is the primary way to add tools to projects. It combines two operations:\n1. **Installs** the tool (if not already installed)\n2. **Adds** the tool to your configuration file\n\n**Key Difference**: `mise install` only installs tools, while `mise use` installs AND configures them.\n\n#### Basic Usage\n\n```bash\n# Interactive selection\nmise use\n\n# Add tool with fuzzy version (default)\nmise use node@20              # Saves as \"20\" in mise.toml\n\n# Add tool with exact version\nmise use --pin node@20.10.0   # Saves as \"20.10.0\"\n\n# Add latest version\nmise use node@latest          # Saves as \"latest\"\n\n# Add with specific backend\nmise use cargo:ripgrep@latest\nmise use ubi:sharkdp/fd\n```\n\n#### Configuration File Selection\n\n`mise use` writes to configuration files in this priority order:\n\n1. **`--global` flag**: `~/.config/mise/config.toml`\n2. **`--path <file>` flag**: Specified file path\n3. **`--env <env>` flag**: `.mise.<env>.toml`\n4. **Default**: `mise.toml` in current directory\n\n```bash\n# Global (all projects)\nmise use --global node@20\n\n# Local (current project)\nmise use node@20              # Creates/updates ./mise.toml\n\n# Environment-specific\nmise use --env local node@20  # Creates .mise.local.toml\n\n# Specific file\nmise use --path ~/.config/mise/custom.toml node@20\n```\n\n#### Important Flags\n\n```bash\n# Pin exact version\nmise use --pin node@20.10.0        # Saves \"20.10.0\"\n\n# Fuzzy version (default)\nmise use --fuzzy node@20           # Saves \"20\"\n\n# Force reinstall\nmise use --force node@20\n\n# Dry run (preview changes)\nmise use --dry-run node@20\n\n# Remove tool from config\nmise use --remove node\n```\n\n#### Version Pinning\n\n```bash\n# Fuzzy (recommended) - auto-updates within major version\nmise use node@20                   # Uses latest 20.x.x\n\n# Exact - locks to specific version\nmise use --pin node@20.10.0        # Always uses 20.10.0\n\n# Latest - always uses newest version\nmise use node@latest               # Always updates to latest\n```\n\n**Best Practice**: Use fuzzy versions for flexibility, `mise.lock` for reproducibility.\n\n### Setting Tool Versions\n\nThe `mise use` command automatically sets tool versions by updating configuration files.\n\n#### .mise.toml Configuration\n\n```toml\n[tools]\nnode = \"20.10.0\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\n\n# Use latest version\nterraform = \"latest\"\n\n# Backends - use quotes for namespaced tools\n\"cargo:ripgrep\" = \"latest\"        # Requires rust installed\n\"ubi:sharkdp/fd\" = \"latest\"       # GitHub releases\n\"npm:typescript\" = \"latest\"       # Requires node installed\n\n# Version from file\nnode = { version = \"lts\", resolve = \"latest-lts\" }\n```\n\n### UBI Backend (Universal Binary Installer)\n\nThe **ubi** backend installs tools directly from GitHub/GitLab releases without requiring plugins. It's built into mise and works cross-platform including Windows.\n\n#### Basic UBI Usage\n\n```bash\n# Install from GitHub releases\nmise use -g ubi:goreleaser/goreleaser\nmise use -g ubi:sharkdp/fd\nmise use -g ubi:BurntSushi/ripgrep\n\n# Specific version\nmise use -g ubi:goreleaser/goreleaser@1.25.1\n\n# In .mise.toml\n[tools]\n\"ubi:goreleaser/goreleaser\" = \"latest\"\n\"ubi:sharkdp/fd\" = \"2.0.0\"\n```\n\n#### UBI Advanced Options\n\nConfigure tool-specific options when binary names differ or filtering is needed:\n\n```toml\n[tools]\n# When executable name differs from repo name\n\"ubi:BurntSushi/ripgrep\" = { version = \"latest\", exe = \"rg\" }\n\n# Filter releases with matching pattern\n\"ubi:some/tool\" = { version = \"latest\", matching = \"linux-gnu\" }\n\n# Use regex for complex filtering\n\"ubi:some/tool\" = { version = \"latest\", matching_regex = \".*-linux-.*\\\\.tar\\\\.gz$\" }\n\n# Extract entire tarball\n\"ubi:some/tool\" = { version = \"latest\", extract_all = true }\n\n# Rename extracted executable\n\"ubi:some/tool\" = { version = \"latest\", rename_exe = \"my-tool\" }\n```\n\n#### UBI Supported Syntax\n\nThree installation formats:\n- **GitHub shorthand (latest)**: `ubi:owner/repo`\n- **GitHub shorthand (version)**: `ubi:owner/repo@1.2.3`\n- **Direct URL**: `ubi:https://github.com/owner/repo/releases/download/v1.2.3/...`\n\n### Cargo Backend\n\nThe **cargo** backend installs Rust packages from crates.io. **Requires Rust to be installed first.**\n\n#### Cargo Prerequisites\n\nInstall Rust before using cargo backend:\n\n```bash\n# Option 1: Install Rust via mise\nmise use -g rust\n\n# Option 2: Install Rust directly\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n#### Cargo Usage\n\n```bash\n# Install from crates.io\nmise use -g cargo:ripgrep\nmise use -g cargo:eza\nmise use -g cargo:bat\n\n# In .mise.toml - requires rust installed first\n[tools]\nrust = \"latest\"              # Install rust first\n\"cargo:ripgrep\" = \"latest\"   # Then cargo tools\n\"cargo:eza\" = \"latest\"\n\"cargo:bat\" = \"latest\"\n```\n\n#### Cargo from Git Repositories\n\n```bash\n# Specific tag\nmise use cargo:https://github.com/username/demo@tag:v1.0.0\n\n# Branch\nmise use cargo:https://github.com/username/demo@branch:main\n\n# Commit hash\nmise use cargo:https://github.com/username/demo@rev:abc123\n```\n\n#### Cargo Settings\n\nConfigure cargo behavior globally:\n\n```toml\n[settings]\n# Use cargo-binstall for faster installs (default: true)\ncargo.binstall = true\n\n# Use alternative cargo registry\ncargo.registry_name = \"my-registry\"\n```\n\n### Managing Installed Tools\n\n```bash\n# List installed tools\nmise list\n\n# List all versions of a tool\nmise list node\n\n# Uninstall a version\nmise uninstall node@18.0.0\n\n# Update all tools to latest\nmise upgrade\n\n# Update specific tool\nmise upgrade node\n```\n\n### Tool Aliases\n\n```bash\n# Create alias for a tool\nmise alias node 20 20.10.0\n\n# Use alias\nmise use node@20\n```\n\n## Environment Variables\n\n### Setting Environment Variables\n\n#### In .mise.toml\n\n```toml\n[env]\nDATABASE_URL = \"postgresql://localhost/myapp\"\nAPI_KEY = \"development-key\"\nNODE_ENV = \"development\"\n\n# Template values\nAPP_ROOT = \"{{ config_root }}\"\nDATA_DIR = \"{{ config_root }}/data\"\n```\n\n#### File-based env vars\n\n```toml\n[env]\n_.file = \".env\"\n_.path = [\"/custom/bin\"]\n```\n\n### Environment Templates\n\nUse Go templates in environment variables:\n\n```toml\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nLOG_FILE = \"{{ config_root }}/logs/app.log\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n```\n\n### Secrets Management\n\n```bash\n# Use with sops\nmise set SECRET_KEY sops://path/to/secret\n\n# Use with age\nmise set API_TOKEN age://path/to/secret\n\n# Use from command\nmise set BUILD_ID \"$(git rev-parse HEAD)\"\n```\n\n## Tasks\n\n### Defining Tasks\n\n#### In .mise.toml\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\n\n[tasks.test]\ndescription = \"Run tests\"\nrun = \"npm test\"\n\n[tasks.lint]\ndescription = \"Run linter\"\nrun = \"npm run lint\"\ndepends = [\"build\"]\n\n[tasks.ci]\ndescription = \"Run CI pipeline\"\ndepends = [\"lint\", \"test\"]\n\n[tasks.dev]\ndescription = \"Start development server\"\nrun = \"npm run dev\"\n```\n\n### Running Tasks\n\n```bash\n# Run a task\nmise run build\nmise run test\n\n# Short form\nmise build\nmise test\n\n# Run multiple tasks\nmise run lint test\n\n# List available tasks\nmise tasks\n\n# Run task with arguments\nmise run script -- arg1 arg2\n```\n\n### Task Dependencies\n\n```toml\n[tasks.deploy]\ndepends = [\"build\", \"test\"]\nrun = \"npm run deploy\"\n\n# Tasks run in order: build, test, then deploy\n```\n\n### Task Options\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\nsources = [\"src/**/*.ts\"]      # Only run if sources changed\noutputs = [\"dist/**/*\"]         # Check outputs for changes\ndir = \"frontend\"                # Run in specific directory\nenv = { NODE_ENV = \"production\" }\n\n[tasks.watch]\nrun = \"npm run watch\"\nraw = true                      # Don't wrap in shell\n```\n\n### Task Files\n\nCreate separate task files:\n\n```bash\n# .mise/tasks/deploy\n#!/bin/bash\n# mise description=\"Deploy to production\"\n# mise depends=[\"build\", \"test\"]\n\necho \"Deploying...\"\nnpm run deploy\n```\n\nMake executable:\n```bash\nchmod +x .mise/tasks/deploy\n```\n\n## Common Workflows\n\n### Node.js Project Setup\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[env]\nNODE_ENV = \"development\"\n\n[tasks.install]\nrun = \"npm install\"\n\n[tasks.dev]\nrun = \"npm run dev\"\ndepends = [\"install\"]\n\n[tasks.build]\nrun = \"npm run build\"\ndepends = [\"install\"]\n\n[tasks.test]\nrun = \"npm test\"\ndepends = [\"install\"]\n```\n\n```bash\n# Setup and run\ncd project\nmise install      # Installs Node 20\nmise dev         # Runs dev server\n```\n\n### Python Project Setup\n\n```toml\n# .mise.toml\n[tools]\npython = \"3.12\"\n\n[env]\nPYTHONPATH = \"{{ config_root }}/src\"\n\n[tasks.venv]\nrun = \"python -m venv .venv\"\n\n[tasks.install]\nrun = \"pip install -r requirements.txt\"\ndepends = [\"venv\"]\n\n[tasks.test]\nrun = \"pytest\"\ndepends = [\"install\"]\n\n[tasks.format]\nrun = \"black src tests\"\n```\n\n### Monorepo Setup\n\n```toml\n# Root .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\n\n[env]\nWORKSPACE_ROOT = \"{{ config_root }}\"\n\n[tasks.install-all]\nrun = \"\"\"\nnpm install\ncd services/api && npm install\ncd services/web && npm install\n\"\"\"\n\n[tasks.test-all]\ndepends = [\"install-all\"]\nrun = \"\"\"\nmise run test --dir services/api\nmise run test --dir services/web\n\"\"\"\n```\n\n### Multi-Tool Project\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\nterraform = \"latest\"\n\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n\n[tasks.setup]\ndescription = \"Setup all dependencies\"\nrun = \"\"\"\nnpm install\npip install -r requirements.txt\nbundle install\ngo mod download\n\"\"\"\n```\n\n## Lock Files\n\nGenerate lock files for reproducible environments:\n\n```bash\n# Generate .mise.lock\nmise lock\n\n# Use locked versions\nmise install --locked\n```\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[settings]\nlockfile = true  # Auto-generate lock file\n```\n\n## Shims\n\nUse shims for tool binaries:\n\n```bash\n# Enable shims\nmise settings set experimental true\nmise reshim\n\n# Now tools are in PATH via shims\nnode --version  # Uses mise-managed node\npython --version  # Uses mise-managed python\n```\n\n## Configuration Locations\n\nmise reads configuration from multiple locations (in order):\n\n1. `.mise.toml` - Project local config\n2. `.mise/config.toml` - Project local config (alternative)\n3. `~/.config/mise/config.toml` - Global config\n4. Environment variables - `MISE_*`\n\n## IDE Integration\n\n### VS Code\n\nAdd to `.vscode/settings.json`:\n\n```json\n{\n  \"terminal.integrated.env.linux\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  },\n  \"terminal.integrated.env.osx\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  }\n}\n```\n\n### JetBrains IDEs\n\nUse mise shims or configure tool paths:\n\n```bash\n# Find tool path\nmise which node\nmise which python\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: CI\n\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: jdx/mise-action@v2\n\n      - name: Run tests\n        run: mise run test\n```\n\n### GitLab CI\n\n```yaml\ntest:\n  image: ubuntu:latest\n  before_script:\n    - curl https://mise.run | sh\n    - eval \"$(mise activate bash)\"\n    - mise install\n  script:\n    - mise run test\n```\n\n## Troubleshooting\n\n### Check mise status\n\n```bash\n# Show configuration\nmise config\n\n# Show environment\nmise env\n\n# Show installed tools\nmise list\n\n# Debug mode\nmise --verbose install node\n```\n\n### Clear cache\n\n```bash\n# Clear tool cache\nmise cache clear\n\n# Remove and reinstall\nmise uninstall node@20\nmise install node@20\n```\n\n### Legacy .tool-versions\n\nmise is compatible with asdf's `.tool-versions`:\n\n```\n# .tool-versions\nnodejs 20.10.0\npython 3.12.0\nruby 3.3.0\n```\n\nConvert to mise:\n\n```bash\n# mise auto-reads .tool-versions\n# Or convert to .mise.toml\nmise config migrate\n```\n\n## Best Practices\n\n- **Use .mise.toml for projects**: Better than .tool-versions (more features)\n- **Pin versions in projects**: Ensure consistency across team\n- **Use tasks for common operations**: Document and standardize workflows\n- **Lock files in production**: Use `mise lock` for reproducibility\n- **Global tools for dev**: Set global defaults, override per project\n- **Environment per project**: Keep secrets and config in .mise.toml\n- **Commit .mise.toml**: Share config with team\n- **Don't commit .mise.lock**: Let mise generate per environment\n\n## Key Principles\n\n- **Reproducible environments**: Lock versions for consistency\n- **Project-specific config**: Each project defines its own tools and env\n- **Task automation**: Centralize common development tasks\n- **Cross-platform**: Same config works on all platforms\n- **Zero setup for team**: Clone and `mise install` to get started"
              },
              {
                "name": "nushell",
                "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features",
                "path": "core/skills/nushell/SKILL.md",
                "frontmatter": {
                  "name": "nushell",
                  "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features"
                },
                "content": "# Nushell - Modern Structured Shell\n\nThis skill activates when working with Nushell (Nu), writing Nu scripts, working with structured data pipelines, or configuring the Nu environment.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Nushell scripts or commands\n- Working with structured data in pipelines\n- Converting from bash/zsh to Nushell\n- Configuring Nushell environment\n- Processing JSON, CSV, YAML, or other structured data\n- Creating custom commands or modules\n\n## What is Nushell?\n\nNushell is a modern shell that:\n- Treats **data as structured** (not just text streams)\n- Works **cross-platform** (Windows, macOS, Linux)\n- Provides **clear error messages** and IDE support\n- Combines **shell and programming language** features\n- Has **built-in data format support** (JSON, CSV, YAML, TOML, XML, etc.)\n\n## Installation\n\n```bash\n# macOS\nbrew install nushell\n\n# Linux (cargo)\ncargo install nu\n\n# Windows\nwinget install nushell\n\n# Or download from https://www.nushell.sh/\n```\n\n## Basic Concepts\n\n### Everything is Data\n\nUnlike traditional shells where everything is text, Nu works with structured data:\n\n```nu\n# Traditional shell (text output)\nls | grep \".txt\"\n\n# Nushell (structured data)\nls | where name =~ \".txt\"\n```\n\n### Pipeline Philosophy\n\nData flows through pipelines as structured tables/records:\n\n```nu\n# Each command outputs structured data\nls | where size > 1kb | sort-by modified | reverse\n```\n\n## Data Types\n\n### Basic Types\n\n```nu\n# Integers\n42\n-10\n\n# Floats\n3.14\n-2.5\n\n# Strings\n\"hello\"\n'world'\n\n# Booleans\ntrue\nfalse\n\n# Null\nnull\n```\n\n### Collections\n\n```nu\n# Lists\n[1 2 3 4 5]\n[\"apple\" \"banana\" \"cherry\"]\n\n# Records (like objects/dicts)\n{name: \"Alice\", age: 30, city: \"NYC\"}\n\n# Tables (list of records)\n[\n  {name: \"Alice\", age: 30}\n  {name: \"Bob\", age: 25}\n]\n```\n\n### Ranges\n\n```nu\n# Number ranges\n1..10\n1..2..10  # Step by 2\n\n# Use in commands\n1..5 | each { |i| $i * 2 }\n```\n\n## Working with Files and Directories\n\n### Navigation\n\n```nu\n# Change directory\ncd /path/to/dir\n\n# List files (returns structured table)\nls\n\n# List with details\nls | select name size modified\n\n# Filter files\nls | where type == file\nls | where size > 1mb\nls | where name =~ \"\\.txt$\"\n```\n\n### File Operations\n\n```nu\n# Create file\n\"hello\" | save hello.txt\n\n# Read file\nopen hello.txt\n\n# Append to file\n\"world\" | save -a hello.txt\n\n# Copy\ncp source.txt dest.txt\n\n# Move/rename\nmv old.txt new.txt\n\n# Remove\nrm file.txt\nrm -r directory/\n\n# Create directory\nmkdir new-dir\n```\n\n### File Content\n\n```nu\n# Read as string\nopen file.txt\n\n# Read structured data\nopen data.json\nopen config.toml\nopen data.csv\n\n# Write structured data\n{name: \"Alice\", age: 30} | to json | save user.json\n[{a: 1} {a: 2}] | to csv | save data.csv\n```\n\n## Pipeline Operations\n\n### Filtering\n\n```nu\n# Filter with where\nls | where size > 1mb\nls | where type == dir\nls | where name =~ \"test\"\n\n# Multiple conditions\nls | where size > 1kb and type == file\n```\n\n### Selecting Columns\n\n```nu\n# Select specific columns\nls | select name size\n\n# Rename columns\nls | select name size | rename file bytes\n```\n\n### Sorting\n\n```nu\n# Sort by column\nls | sort-by size\nls | sort-by modified\n\n# Reverse sort\nls | sort-by size | reverse\n\n# Multiple columns\nls | sort-by type size\n```\n\n### Transforming Data\n\n```nu\n# Map over items with each\n1..5 | each { |i| $i * 2 }\n\n# Update column\nls | update name { |row| $row.name | str upcase }\n\n# Insert column\nls | insert size_kb { |row| $row.size / 1000 }\n\n# Upsert (update or insert)\nls | upsert type_upper { |row| $row.type | str upcase }\n```\n\n### Aggregation\n\n```nu\n# Count items\nls | length\n\n# Sum\n[1 2 3 4 5] | math sum\n\n# Average\n[1 2 3 4 5] | math avg\n\n# Min/Max\nls | get size | math max\nls | get size | math min\n\n# Group by\nls | group-by type\n```\n\n## Variables\n\n### Variable Assignment\n\n```nu\n# Let (immutable by default)\nlet name = \"Alice\"\nlet age = 30\nlet colors = [\"red\" \"green\" \"blue\"]\n\n# Mut (mutable)\nmut counter = 0\n$counter = $counter + 1\n```\n\n### Using Variables\n\n```nu\n# Reference with $\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# In pipelines\nlet threshold = 1mb\nls | where size > $threshold\n```\n\n### Environment Variables\n\n```nu\n# Get environment variable\n$env.PATH\n$env.HOME\n\n# Set environment variable\n$env.MY_VAR = \"value\"\n\n# Load from file\nload-env { API_KEY: \"secret\" }\n```\n\n## String Operations\n\n### String Interpolation\n\n```nu\n# String interpolation with ()\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# With expressions\nlet x = 5\nprint $\"Result: (5 * $x)\"\n```\n\n### String Methods\n\n```nu\n# Case conversion\n\"hello\" | str upcase  # HELLO\n\"WORLD\" | str downcase  # world\n\n# Trimming\n\"  spaces  \" | str trim\n\n# Replace\n\"hello world\" | str replace \"world\" \"nu\"\n\n# Contains\n\"hello world\" | str contains \"world\"  # true\n\n# Split\n\"a,b,c\" | split row \",\"\n```\n\n## Conditionals\n\n### If Expressions\n\n```nu\n# If-else\nif $age >= 18 {\n  print \"Adult\"\n} else {\n  print \"Minor\"\n}\n\n# If-else if-else\nif $score >= 90 {\n  \"A\"\n} else if $score >= 80 {\n  \"B\"\n} else {\n  \"C\"\n}\n\n# Ternary-style with match\nlet status = if $is_active { \"active\" } else { \"inactive\" }\n```\n\n### Match (Pattern Matching)\n\n```nu\n# Match expression\nmatch $value {\n  1 => \"one\"\n  2 => \"two\"\n  _ => \"other\"\n}\n\n# With conditions\nmatch $age {\n  0..17 => \"minor\"\n  18..64 => \"adult\"\n  _ => \"senior\"\n}\n```\n\n## Loops\n\n### For Loop\n\n```nu\n# Loop over range\nfor i in 1..5 {\n  print $i\n}\n\n# Loop over list\nfor name in [\"Alice\" \"Bob\" \"Charlie\"] {\n  print $\"Hello, ($name)\"\n}\n\n# Loop over files\nfor file in (ls | where type == file) {\n  print $file.name\n}\n```\n\n### While Loop\n\n```nu\n# While loop\nmut i = 0\nwhile $i < 5 {\n  print $i\n  $i = $i + 1\n}\n```\n\n### Each (Functional)\n\n```nu\n# Transform each item\n1..5 | each { |i| $i * 2 }\n\n# With index\n[\"a\" \"b\" \"c\"] | enumerate | each { |item|\n  print $\"($item.index): ($item.item)\"\n}\n```\n\n## Custom Commands\n\n### Defining Commands\n\n```nu\n# Simple command\ndef greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\ngreet \"Alice\"\n\n# With return value\ndef add [a: int, b: int] {\n  $a + $b\n}\n\nlet result = add 5 3\n\n# With default values\ndef greet [name: string = \"World\"] {\n  print $\"Hello, ($name)!\"\n}\n```\n\n### Command Parameters\n\n```nu\n# Required parameters\ndef copy [source: path, dest: path] {\n  cp $source $dest\n}\n\n# Optional parameters\ndef greet [\n  name: string\n  --loud (-l)  # Flag\n  --repeat (-r): int = 1  # Named parameter with default\n] {\n  let message = if $loud {\n    $name | str upcase\n  } else {\n    $name\n  }\n\n  1..$repeat | each { print $\"Hello, ($message)!\" }\n}\n\n# Usage\ngreet \"Alice\"\ngreet \"Bob\" --loud\ngreet \"Charlie\" --repeat 3\n```\n\n### Pipeline Commands\n\n```nu\n# Accept pipeline input\ndef filter-large [] {\n  where size > 1mb\n}\n\n# Usage\nls | filter-large\n\n# Accept and transform pipeline\ndef double [] {\n  each { |value| $value * 2 }\n}\n\n[1 2 3] | double\n```\n\n## Working with Structured Data\n\n### JSON\n\n```nu\n# Read JSON\nlet data = open data.json\n\n# Parse JSON string\nlet obj = '{\"name\": \"Alice\", \"age\": 30}' | from json\n\n# Write JSON\n{name: \"Alice\", age: 30} | to json | save user.json\n\n# Pretty print JSON\n{name: \"Alice\", age: 30} | to json -i 2\n```\n\n### CSV\n\n```nu\n# Read CSV\nlet data = open data.csv\n\n# Convert to CSV\n[{a: 1, b: 2} {a: 3, b: 4}] | to csv\n\n# Save CSV\nls | select name size | to csv | save files.csv\n```\n\n### YAML/TOML\n\n```nu\n# Read YAML\nlet config = open config.yaml\n\n# Read TOML\nlet config = open config.toml\n\n# Write YAML\n{key: \"value\"} | to yaml | save config.yaml\n\n# Write TOML\n{key: \"value\"} | to toml | save config.toml\n```\n\n### Working with Tables\n\n```nu\n# Create table\nlet users = [\n  {name: \"Alice\", age: 30, city: \"NYC\"}\n  {name: \"Bob\", age: 25, city: \"LA\"}\n  {name: \"Charlie\", age: 35, city: \"NYC\"}\n]\n\n# Query table\n$users | where age > 25\n$users | where city == \"NYC\"\n$users | select name age\n\n# Add column\n$users | insert country { \"USA\" }\n\n# Group and count\n$users | group-by city | transpose city users\n```\n\n## Modules\n\n### Creating Modules\n\n```nu\n# utils.nu\nexport def greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\nexport def add [a: int, b: int] {\n  $a + $b\n}\n```\n\n### Using Modules\n\n```nu\n# Import module\nuse utils.nu\n\n# Use exported commands\nutils greet \"Alice\"\nutils add 5 3\n\n# Import specific commands\nuse utils.nu [greet add]\n\ngreet \"Alice\"\nadd 5 3\n\n# Import with alias\nuse utils.nu *\n```\n\n## Configuration\n\n### Config File Location\n\n```nu\n# View config\nconfig nu\n\n# Edit config\nconfig nu | open\n\n# Config location\n$nu.config-path\n```\n\n### Common Configurations\n\n```nu\n# config.nu\n$env.config = {\n  show_banner: false\n\n  ls: {\n    use_ls_colors: true\n    clickable_links: true\n  }\n\n  table: {\n    mode: rounded\n    index_mode: auto\n  }\n\n  completions: {\n    quick: true\n    partial: true\n  }\n\n  history: {\n    max_size: 10000\n    sync_on_enter: true\n    file_format: \"sqlite\"\n  }\n}\n```\n\n### Environment Setup\n\n```nu\n# env.nu\n$env.PATH = ($env.PATH | split row (char esep) | append '/custom/bin')\n$env.EDITOR = \"nvim\"\n\n# Load completions\nuse completions/git.nu *\n```\n\n## Common Patterns\n\n### File Processing\n\n```nu\n# Process all JSON files\nls *.json | each { |file|\n  let data = open $file.name\n  print $\"Processing ($file.name): ($data | length) items\"\n}\n\n# Batch rename files\nls *.txt | each { |file|\n  let new_name = ($file.name | str replace \".txt\" \".md\")\n  mv $file.name $new_name\n}\n```\n\n### Data Transformation\n\n```nu\n# CSV to JSON\nopen data.csv | to json | save data.json\n\n# Filter and transform\nopen users.json\n| where active == true\n| select name email\n| to csv\n| save active_users.csv\n\n# Merge data\nlet users = open users.json\nlet orders = open orders.json\n$users | merge $orders\n```\n\n### HTTP Requests\n\n```nu\n# GET request\nhttp get https://api.example.com/users\n\n# POST request\nhttp post https://api.example.com/users {\n  name: \"Alice\"\n  email: \"alice@example.com\"\n}\n\n# With headers\nhttp get -H [Authorization \"Bearer token\"] https://api.example.com/data\n```\n\n### System Commands\n\n```nu\n# Run external command\n^ls -la\n\n# Capture output\nlet output = (^git status)\n\n# Check if command exists\nwhich git\n\n# Get command path\nwhich git | get path\n```\n\n## Error Handling\n\n### Try-Catch\n\n```nu\n# Try expression\ntry {\n  open missing.txt\n} catch {\n  print \"File not found\"\n}\n\n# With error value\ntry {\n  open missing.txt\n} catch { |err|\n  print $\"Error: ($err)\"\n}\n```\n\n### Null Handling\n\n```nu\n# Default value\nlet value = ($env.MY_VAR? | default \"default_value\")\n\n# Null propagation\nlet length = ($value | get name? | str length)\n```\n\n## Scripting\n\n### Script Files\n\n```nu\n#!/usr/bin/env nu\n\n# Script: process_logs.nu\n# Description: Process log files and generate report\n\ndef main [log_dir: path] {\n  let errors = (\n    ls $\"($log_dir)/*.log\"\n    | each { |file| open $file.name | lines }\n    | flatten\n    | where $it =~ \"ERROR\"\n  )\n\n  print $\"Found ($errors | length) errors\"\n  $errors | save error_report.txt\n}\n```\n\nMake executable:\n```bash\nchmod +x process_logs.nu\n./process_logs.nu /var/log\n```\n\n### Script Parameters\n\n```nu\n# With parameters\ndef main [\n  input: path\n  --output (-o): path = \"output.txt\"\n  --verbose (-v)\n] {\n  if $verbose {\n    print $\"Processing ($input)...\"\n  }\n\n  let data = open $input\n  $data | save $output\n\n  if $verbose {\n    print \"Done!\"\n  }\n}\n```\n\n## Comparison with Bash\n\n### Common Operations\n\n```bash\n# Bash\nfind . -name \"*.txt\" | wc -l\n\n# Nushell\nls **/*.txt | length\n```\n\n```bash\n# Bash\ncat file.json | jq '.users[] | select(.age > 25) | .name'\n\n# Nushell\nopen file.json | get users | where age > 25 | get name\n```\n\n```bash\n# Bash\nfor file in *.txt; do\n  mv \"$file\" \"${file%.txt}.md\"\ndone\n\n# Nushell\nls *.txt | each { |f| mv $f.name ($f.name | str replace \".txt\" \".md\") }\n```\n\n## Best Practices\n\n- **Use structured data**: Leverage Nu's strength in handling structured data\n- **Pipeline composition**: Build complex operations from simple pipeline stages\n- **Type annotations**: Add types to custom command parameters for clarity\n- **Error handling**: Use try-catch for operations that might fail\n- **Modules for reuse**: Organize reusable commands in modules\n- **Configuration**: Customize Nu to fit your workflow\n- **External commands**: Use `^` prefix when calling external commands explicitly\n\n## Common Pitfalls\n\n### String vs Bare Words\n\n```nu\n# Bare word (interpreted as string in some contexts)\necho hello\n\n# Explicit string (clearer)\necho \"hello\"\n```\n\n### External Commands\n\n```nu\n# Wrong - Nu tries to parse as Nu command\nls -la\n\n# Right - Explicitly call external command\n^ls -la\n```\n\n### Variable Scope\n\n```nu\n# Variables are scoped to blocks\nif true {\n  let x = 5\n}\n# $x not available here\n\n# Use mut outside for wider scope\nmut x = 0\nif true {\n  $x = 5\n}\nprint $x  # Works\n```\n\n## Key Principles\n\n- **Structured data first**: Think in terms of tables and records, not text\n- **Pipeline composition**: Chain simple operations to build complex workflows\n- **Type safety**: Leverage Nu's type system for reliable scripts\n- **Cross-platform**: Write scripts that work on all platforms\n- **Interactive and scriptable**: Same syntax works in REPL and scripts\n- **Clear errors**: Nu provides helpful error messages for debugging"
              },
              {
                "name": "twelve-factor",
                "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                "path": "core/skills/twelve-factor/SKILL.md",
                "frontmatter": {
                  "name": "twelve-factor",
                  "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                  "license": "MIT"
                },
                "content": "# 12-Factor App Methodology\n\nGuide for building scalable, maintainable, and portable cloud-native applications following the 12-Factor App principles and modern extensions.\n\n## When to Activate\n\nUse this skill when:\n- Designing or refactoring cloud-native applications\n- Building applications for Kubernetes deployment\n- Setting up CI/CD pipelines\n- Implementing microservices architecture\n- Migrating applications to containers\n- Reviewing architecture for cloud readiness\n- Troubleshooting deployment or scaling issues\n- Working with environment configuration\n\n## The 12 Factors\n\n### I. Codebase\n\n**One codebase tracked in revision control, many deploys**\n\n```\nmyapp-repo/\n├── src/\n├── config/\n├── deploy/\n│   ├── staging/\n│   ├── production/\n│   └── development/\n└── Dockerfile\n```\n\n**Key principles:**\n- Single Git repository for the application\n- Multiple environments deploy from same codebase\n- Environment-specific config separate from code\n- Use GitOps (ArgoCD, Flux) for deployment automation\n\n**Anti-patterns:**\n- ❌ Multiple repositories for the same application\n- ❌ Different codebases for different environments\n- ❌ Copying code between repositories\n\n### II. Dependencies\n\n**Explicitly declare and isolate dependencies**\n\nDeclare all dependencies explicitly using package managers:\n\n```dockerfile\n# Multi-stage build for dependency isolation\nFROM node:18-alpine AS dependencies\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nFROM node:18-alpine AS runtime\nWORKDIR /app\nCOPY --from=dependencies /app/node_modules ./node_modules\nCOPY . .\nCMD [\"node\", \"index.js\"]\n```\n\n**Language-specific examples:**\n- Node.js: `package.json` and `package-lock.json`\n- Python: `requirements.txt` or `Pipfile.lock`\n- Java: `pom.xml` or `build.gradle`\n- Go: `go.mod` and `go.sum`\n- Elixir: `mix.exs` and `mix.lock`\n- Rust: `Cargo.toml` and `Cargo.lock`\n\n**Key principles:**\n- Never rely on system-wide packages\n- Use lock files for reproducible builds\n- Vendor dependencies when possible\n- Multi-stage builds for smaller images\n\n### III. Config\n\n**Store config in the environment**\n\nAll configuration should come from environment variables:\n\n```elixir\n# Elixir - config/runtime.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.fetch_env!(\"DATABASE_PASSWORD\"),\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n```\n\n```javascript\n// Node.js\nconst config = {\n  database: {\n    url: process.env.DATABASE_URL,\n    pool: {\n      min: parseInt(process.env.DB_POOL_MIN || '2'),\n      max: parseInt(process.env.DB_POOL_MAX || '10')\n    }\n  },\n  cache: {\n    ttl: parseInt(process.env.CACHE_TTL || '3600')\n  }\n};\n```\n\n**Kubernetes ConfigMaps:**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_HOST: \"postgres-service\"\n  CACHE_TTL: \"3600\"\n  LOG_LEVEL: \"info\"\n```\n\n**Kubernetes Secrets:**\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  DATABASE_PASSWORD: <base64-encoded>\n  JWT_SECRET: <base64-encoded>\n  API_KEY: <base64-encoded>\n```\n\n**Anti-patterns:**\n- ❌ Hardcoded configuration values\n- ❌ Configuration files committed to version control\n- ❌ Different code paths for different environments\n\n### IV. Backing Services\n\n**Treat backing services as attached resources**\n\nConnect to all backing services (databases, queues, caches, APIs) via URLs in environment variables:\n\n```javascript\n// Treat all backing services uniformly\nconst services = {\n  database: createConnection(process.env.DATABASE_URL),\n  cache: createRedisClient(process.env.REDIS_URL),\n  queue: createQueueClient(process.env.RABBITMQ_URL),\n  storage: createS3Client(process.env.S3_ENDPOINT)\n};\n```\n\n**Kubernetes Service Discovery:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n```\n\n**Key principles:**\n- No distinction between local and third-party services\n- Swappable via configuration change only\n- No code changes to swap backing services\n- Connection via URL in environment\n\n### V. Build, Release, Run\n\n**Strictly separate build and run stages**\n\nThree distinct stages:\n1. **Build**: Convert code to executable bundle\n2. **Release**: Combine build with config\n3. **Run**: Execute in target environment\n\n```yaml\n# GitHub Actions CI/CD Pipeline\nname: Build and Deploy\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build Docker image\n        run: docker build -t myapp:${{ github.sha }} .\n      - name: Push to registry\n        run: docker push myapp:${{ github.sha }}\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to Kubernetes\n        run: kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}\n```\n\n**Key principles:**\n- Immutable releases (never modify, only deploy new)\n- Unique release identifiers (git SHA, semver)\n- Rollback by redeploying previous release\n- Separate build artifacts from runtime config\n\n### VI. Processes\n\n**Execute the app as one or more stateless processes**\n\nApplication processes should be stateless and share-nothing. Store persistent state in backing services.\n\n```javascript\n// ❌ Bad: In-memory session store\napp.use(session({\n  secret: process.env.SESSION_SECRET,\n  resave: false\n  // Uses memory store by default\n}));\n\n// ✓ Good: Store session in Redis\napp.use(session({\n  store: new RedisStore({\n    client: redisClient,\n    prefix: 'sess:'\n  }),\n  secret: process.env.SESSION_SECRET,\n  resave: false,\n  saveUninitialized: false\n}));\n```\n\n**Kubernetes Deployment:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3  # Can scale horizontally\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n```\n\n**Key principles:**\n- Stateless processes enable horizontal scaling\n- No sticky sessions\n- No local filesystem for persistent data\n- Shared state goes in databases, caches, or queues\n\n### VII. Port Binding\n\n**Export services via port binding**\n\nApplications are self-contained and bind to a port:\n\n```javascript\nconst express = require('express');\nconst app = express();\nconst port = process.env.PORT || 3000;\n\napp.listen(port, '0.0.0.0', () => {\n  console.log(`Server running on port ${port}`);\n});\n```\n\n```elixir\n# Phoenix endpoint config\nconfig :my_app, MyAppWeb.Endpoint,\n  http: [port: String.to_integer(System.get_env(\"PORT\") || \"4000\")],\n  server: true\n```\n\n**Kubernetes Service:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: LoadBalancer\n```\n\n**Key principles:**\n- Bind to `0.0.0.0`, not `localhost`\n- Port number from environment variable\n- No reliance on runtime injection (e.g., Apache, Nginx)\n- HTTP server library embedded in app\n\n### VIII. Concurrency\n\n**Scale out via the process model**\n\nScale by adding more processes (horizontal scaling), not by making processes larger (vertical scaling):\n\n```yaml\n# Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n**Process types (Procfile concept):**\n\n```\nweb: node server.js\nworker: node worker.js\nscheduler: node scheduler.js\n```\n\n**Key principles:**\n- Different process types for different workloads\n- Processes can scale independently\n- OS process manager handles processes\n- Never daemonize or write PID files\n\n### IX. Disposability\n\n**Maximize robustness with fast startup and graceful shutdown**\n\n```javascript\nconst server = app.listen(port, () => {\n  console.log('Server started');\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', () => {\n  console.log('SIGTERM received, shutting down gracefully');\n\n  server.close(() => {\n    // Close database connections\n    db.close();\n\n    // Close other connections\n    redis.quit();\n\n    console.log('Process terminated');\n    process.exit(0);\n  });\n});\n```\n\n**Kubernetes lifecycle hooks:**\n\n```yaml\nspec:\n  containers:\n  - name: myapp\n    image: myapp:latest\n    lifecycle:\n      preStop:\n        exec:\n          command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n    terminationGracePeriodSeconds: 30\n```\n\n**Key principles:**\n- Minimize startup time (< 10 seconds ideal)\n- Handle SIGTERM for graceful shutdown\n- Finish in-flight requests before shutting down\n- Robust against sudden death\n- Fast startup enables rapid scaling\n\n### X. Dev/Prod Parity\n\n**Keep development, staging, and production as similar as possible**\n\n**Docker Compose for local development:**\n\n```yaml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n\n  redis:\n    image: redis:7-alpine\n```\n\n**Key principles:**\n- Use same backing services in dev and prod\n- Containers ensure environment consistency\n- Infrastructure as code for reproducibility\n- Minimize time gap between dev and production\n- Same deployment process for all environments\n\n### XI. Logs\n\n**Treat logs as event streams**\n\nWrite all logs to stdout/stderr, let the environment handle aggregation:\n\n```javascript\n// Structured logging to stdout\nconst winston = require('winston');\n\nconst logger = winston.createLogger({\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.json()\n  ),\n  transports: [\n    new winston.transports.Console()\n  ]\n});\n\nlogger.info('User logged in', {\n  userId: 123,\n  ip: '192.168.1.1',\n  userAgent: 'Mozilla/5.0...'\n});\n```\n\n```elixir\n# Elixir structured logging\nrequire Logger\n\nLogger.info(\"User logged in\",\n  user_id: 123,\n  ip: \"192.168.1.1\"\n)\n```\n\n**Key principles:**\n- Never manage log files\n- Write unbuffered to stdout\n- Use structured logging (JSON)\n- Let platform route logs (Fluentd, Logstash)\n- Include correlation IDs for tracing\n\n**Anti-patterns:**\n- ❌ Writing to log files\n- ❌ Log rotation within the app\n- ❌ Sending logs directly to aggregation service\n\n### XII. Admin Processes\n\n**Run admin/management tasks as one-off processes**\n\nDatabase migrations, console, one-time scripts:\n\n```yaml\n# Kubernetes Job for database migration\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: myapp:latest\n        command: [\"npm\", \"run\", \"migrate\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: DATABASE_URL\n      restartPolicy: OnFailure\n```\n\n```yaml\n# CronJob for scheduled cleanup\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: data-cleanup\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: myapp:latest\n            command: [\"npm\", \"run\", \"cleanup\"]\n          restartPolicy: OnFailure\n```\n\n**Key principles:**\n- Same environment as regular processes\n- Same codebase and config\n- Run against release, not development code\n- Use scheduler for recurring tasks\n- Ship admin code with application code\n\n## Modern Extensions (Beyond 12)\n\n### XIII. API First\n\nDesign and document APIs before implementation:\n\n```yaml\n# OpenAPI specification\nopenapi: 3.0.0\ninfo:\n  title: My API\n  version: v1\npaths:\n  /users:\n    get:\n      summary: List users\n      responses:\n        '200':\n          description: Success\n```\n\n**Key principles:**\n- OpenAPI/Swagger specifications\n- API versioning (URL or header)\n- API gateway pattern\n- Contract-first development\n\n### XIV. Telemetry\n\nComprehensive observability with metrics, tracing, and monitoring:\n\n```yaml\n# Prometheus ServiceMonitor\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myapp-monitor\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  endpoints:\n  - port: metrics\n    path: /metrics\n```\n\n**Key principles:**\n- Expose /metrics endpoint (Prometheus format)\n- Distributed tracing (OpenTelemetry)\n- Application Performance Monitoring (APM)\n- Custom business metrics\n- Health check endpoints\n\n### XV. Security\n\nAuthentication, authorization, and security by design:\n\n```javascript\n// JWT authentication middleware\nfunction authenticateToken(req, res, next) {\n  const authHeader = req.headers['authorization'];\n  const token = authHeader && authHeader.split(' ')[1];\n\n  if (!token) return res.sendStatus(401);\n\n  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {\n    if (err) return res.sendStatus(403);\n    req.user = user;\n    next();\n  });\n}\n```\n\n**Key principles:**\n- OAuth 2.0 / OpenID Connect\n- RBAC (Role-Based Access Control)\n- Secrets in environment, never in code\n- TLS everywhere\n- Security scanning in CI/CD\n\n## Common Patterns\n\n### Configuration Validation\n\nValidate required configuration at startup:\n\n```javascript\nfunction validateConfig() {\n  const required = ['DATABASE_URL', 'JWT_SECRET', 'REDIS_URL'];\n  const missing = required.filter(key => !process.env[key]);\n\n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n}\n\n// Call before starting server\nvalidateConfig();\n```\n\n### Health Checks\n\nImplement health and readiness endpoints:\n\n```javascript\n// Liveness probe\napp.get('/health', (req, res) => {\n  res.status(200).json({\n    status: 'healthy',\n    timestamp: new Date().toISOString()\n  });\n});\n\n// Readiness probe\napp.get('/ready', async (req, res) => {\n  try {\n    await db.ping();\n    await redis.ping();\n    res.status(200).json({ status: 'ready' });\n  } catch (err) {\n    res.status(503).json({ status: 'not ready', error: err.message });\n  }\n});\n```\n\n**Kubernetes probes:**\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 3000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n### Graceful Degradation\n\nHandle backing service failures gracefully:\n\n```javascript\nasync function getCachedData(key) {\n  try {\n    return await redis.get(key);\n  } catch (err) {\n    logger.warn('Redis unavailable, falling back to database', { error: err.message });\n    return await db.query('SELECT data FROM cache WHERE key = ?', [key]);\n  }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Environment-Specific Code Paths\n\n```javascript\n// DON'T\nif (process.env.NODE_ENV === 'production') {\n  // Different behavior\n} else {\n  // Different behavior\n}\n\n// DO: Use configuration\nconst timeout = parseInt(process.env.TIMEOUT || '5000');\n```\n\n### ❌ Local File Storage\n\n```javascript\n// DON'T: Write to local filesystem\nfs.writeFile('/tmp/uploads/' + filename, data);\n\n// DO: Use object storage\nawait s3.putObject({\n  Bucket: process.env.S3_BUCKET,\n  Key: filename,\n  Body: data\n});\n```\n\n### ❌ In-Memory State\n\n```javascript\n// DON'T: Store state in memory\nconst sessions = new Map();\n\n// DO: Use external store\nconst session = await redis.get(`session:${sessionId}`);\n```\n\n### ❌ Hardcoded Dependencies\n\n```javascript\n// DON'T: Hardcode service locations\nconst db = connect('localhost:5432');\n\n// DO: Use environment variables\nconst db = connect(process.env.DATABASE_URL);\n```\n\n## Troubleshooting Guide\n\n### Application Won't Start\n\n1. Check required environment variables are set\n2. Validate configuration at startup\n3. Check backing service connectivity\n4. Review logs for initialization errors\n\n### Application Won't Scale\n\n1. Identify stateful operations\n2. Move state to backing services\n3. Remove file system dependencies\n4. Eliminate sticky sessions\n\n### Inconsistent Behavior Across Environments\n\n1. Ensure same backing service types (not SQLite in dev, Postgres in prod)\n2. Use containers for dev environment\n3. Check for environment-specific code paths\n4. Verify configuration is environment-only\n\n### Logs Not Appearing\n\n1. Ensure writing to stdout/stderr\n2. Avoid buffering log output\n3. Check log aggregation configuration\n4. Verify Kubernetes logging sidecar/daemonset\n\n## Best Practices Summary\n\n1. **Environment variables for all configuration**\n2. **Stateless processes that can scale horizontally**\n3. **Structured logging to stdout**\n4. **Containers for development parity**\n5. **Automated CI/CD pipelines**\n6. **Health checks for orchestration**\n7. **Graceful shutdown handling**\n8. **Fast startup times (< 10s)**\n9. **Immutable releases with unique IDs**\n10. **Comprehensive monitoring and telemetry**\n\n## Kubernetes-Specific Best Practices\n\n### Resource Limits\n\n```yaml\nresources:\n  requests:\n    memory: \"128Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"256Mi\"\n    cpu: \"200m\"\n```\n\n### Init Containers\n\n```yaml\ninitContainers:\n- name: wait-for-db\n  image: busybox\n  command: ['sh', '-c', 'until nc -z postgres-service 5432; do sleep 1; done']\n```\n\n### Pod Disruption Budgets\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n## Resources\n\n- **12factor.net**: Original methodology\n- **Kubernetes Documentation**: https://kubernetes.io/docs/\n- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n- **OpenTelemetry**: https://opentelemetry.io/\n- **Prometheus**: https://prometheus.io/\n\n## Key Insights\n\n> \"The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\"\n\n> \"A twelve-factor app never relies on implicit existence of state on the filesystem. Even if a process has written something to disk, it must assume that file won't be available on the next request.\"\n\nDesign applications from day one to be cloud-native, scalable, and maintainable. The investment in following these principles pays dividends in operational simplicity and development velocity."
              },
              {
                "name": "dagu-rest-api",
                "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems",
                "path": "dagu/skills/rest-api/SKILL.md",
                "frontmatter": {
                  "name": "dagu-rest-api",
                  "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems"
                },
                "content": "# Dagu REST API\n\nUse this skill when integrating Dagu with external systems, automating workflow operations, or programmatically managing workflows through the API.\n\n## When to Use This Skill\n\nActivate when:\n- Triggering workflows programmatically\n- Querying workflow status from applications\n- Building automation around Dagu\n- Integrating Dagu with CI/CD pipelines\n- Creating custom dashboards or monitoring tools\n- Scheduling workflows dynamically\n- Fetching execution logs programmatically\n\n## Core API Capabilities\n\nThe Dagu REST API provides endpoints for:\n\n1. **Workflow Operations** - Start, stop, retry workflows\n2. **Status Queries** - Get workflow and execution status\n3. **DAG Management** - List and inspect workflow definitions\n4. **Execution History** - Query past executions\n5. **Log Retrieval** - Fetch execution logs\n\n## Base URL\n\nDefault API base URL: `http://localhost:8080/api/v1`\n\nConfigure in Dagu settings if using a different host/port.\n\n## Authentication\n\nConsult `references/authentication.md` for details on:\n- API token configuration\n- Authentication headers\n- Security best practices\n\n## Quick Start Operations\n\n### Start a Workflow\n\n```bash\nPOST /dags/{dagName}/start\n```\n\nBasic example:\n```bash\ncurl -X POST http://localhost:8080/api/v1/dags/my_workflow/start\n```\n\nFor parameter passing and advanced options, see `references/workflow-operations.md`.\n\n### Get Workflow Status\n\n```bash\nGET /dags/{dagName}/status\n```\n\nReturns current status, running steps, and execution details.\n\n### Stop a Workflow\n\n```bash\nPOST /dags/{dagName}/stop\n```\n\nStops currently running execution.\n\n## When to Consult References\n\n- **Detailed endpoint documentation**: Read `references/api-endpoints.md`\n- **Workflow operations (start/stop/retry)**: Read `references/workflow-operations.md`\n- **Status and monitoring queries**: Read `references/status-queries.md`\n- **Authentication setup**: Read `references/authentication.md`\n- **Integration examples**: Read `references/integration-examples.md`\n- **Error handling**: Read `references/error-handling.md`\n\n## Common Use Cases\n\n### CI/CD Integration\n\nTrigger Dagu workflows from your CI/CD pipeline:\n\n```bash\n# In GitHub Actions, GitLab CI, etc.\ncurl -X POST http://dagu-server:8080/api/v1/dags/deploy_production/start \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": \"VERSION=1.2.3 ENVIRONMENT=production\"}'\n```\n\nFor complete CI/CD integration patterns, see `references/integration-examples.md`.\n\n### Monitoring and Alerting\n\nQuery workflow status for external monitoring:\n\n```bash\n# Check if workflow is running\ncurl http://localhost:8080/api/v1/dags/critical_job/status\n```\n\nBuild custom alerts based on status responses. See `references/status-queries.md` for response format details.\n\n### Dynamic Scheduling\n\nTrigger workflows based on external events:\n\n```python\nimport requests\n\ndef trigger_workflow(dag_name, params=None):\n    url = f\"http://localhost:8080/api/v1/dags/{dag_name}/start\"\n    data = {\"params\": params} if params else {}\n    response = requests.post(url, json=data)\n    return response.json()\n```\n\nFor comprehensive examples in multiple languages, see `references/integration-examples.md`.\n\n## Response Formats\n\nAll API responses are JSON. Common response structure:\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": { ... }\n}\n```\n\nError responses:\n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error description\"\n}\n```\n\nFor complete response schemas, consult `references/api-endpoints.md`.\n\n## Key Principles\n\n- **RESTful design**: Standard HTTP methods (GET, POST, DELETE)\n- **JSON responses**: All responses in JSON format\n- **Idempotent operations**: Safe to retry most operations\n- **Error codes**: Standard HTTP status codes\n- **Stateless**: Each request is independent\n\n## Pro Tips\n\n- Use the API for automation, use Web UI for manual operations\n- Implement retry logic for network failures\n- Cache DAG lists if querying frequently\n- Use webhooks for event-driven workflows when possible\n- Monitor API response times for performance issues\n- Validate workflow names before calling API to avoid errors"
              },
              {
                "name": "dagu-webui",
                "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface",
                "path": "dagu/skills/webui/SKILL.md",
                "frontmatter": {
                  "name": "dagu-webui",
                  "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface"
                },
                "content": "# Dagu Web UI\n\nUse this skill when working with Dagu's web interface to manage workflows, view execution history, monitor running workflows, or configure the UI.\n\n## When to Use This Skill\n\nActivate when:\n- Navigating the Dagu web interface\n- Starting, stopping, or retrying workflows via UI\n- Viewing workflow execution logs and status\n- Monitoring running workflows\n- Managing workflow history\n- Configuring workflow schedules through the UI\n- Troubleshooting workflow issues using the UI\n\n## Core Capabilities\n\nThe Dagu Web UI provides:\n\n1. **Workflow Management** - View, start, stop, and manage workflows\n2. **Execution Monitoring** - Real-time status and logs\n3. **History Viewing** - Past execution records and results\n4. **DAG Visualization** - Visual representation of workflow structure\n5. **Log Access** - View detailed execution logs\n6. **Schedule Management** - Configure when workflows run\n\n## Quick Start\n\nAccess Dagu Web UI at `http://localhost:8080` (default) after starting Dagu:\n\n```bash\ndagu server\n```\n\n## Primary Operations\n\n### Start a Workflow\n\nTo manually execute a workflow:\n1. Navigate to workflow list\n2. Click the workflow name\n3. Click \"Start\" button\n4. View real-time execution progress\n\n### Monitor Execution\n\nFor detailed information on a running workflow, consult `references/monitoring.md` which covers:\n- Reading execution logs\n- Understanding status indicators\n- Tracking step progress\n- Identifying failures\n\n### View History\n\nTo review past executions, see `references/history.md` for guidance on:\n- Filtering execution history\n- Analyzing failed runs\n- Comparing execution times\n- Exporting execution data\n\n### Workflow Visualization\n\nThe DAG view shows workflow structure. For detailed visualization features, see `references/visualization.md`.\n\n## When to Consult References\n\n- **Detailed UI navigation**: Read `references/ui-navigation.md`\n- **Advanced monitoring**: Read `references/monitoring.md`\n- **History analysis**: Read `references/history.md`\n- **Workflow editing via UI**: Read `references/workflow-editor.md`\n- **Configuration options**: Read `references/configuration.md`\n\n## Common Tasks\n\n### Restart a Failed Workflow\n\n1. Find the failed execution in history\n2. Click the retry/restart button\n3. Monitor the new execution\n\n### Stop a Running Workflow\n\n1. Navigate to the running workflow\n2. Click \"Stop\" or \"Cancel\"\n3. Confirm the action\n4. View cleanup handlers execution\n\n### View Detailed Logs\n\nWhen you need to debug a workflow:\n1. Click on the specific workflow execution\n2. Select the step with issues\n3. View stdout/stderr logs\n4. Check for error messages\n\nFor advanced log analysis, consult `references/monitoring.md`.\n\n## Key Principles\n\n- **Real-time visibility**: Web UI provides live updates of workflow execution\n- **Click-based operations**: No CLI needed for basic workflow management\n- **History preservation**: All executions are logged and accessible\n- **Visual feedback**: Status indicators show current state at a glance\n- **Log accessibility**: Detailed logs available for debugging\n\n## Pro Tips\n\n- Use the search feature to quickly find workflows by name\n- Filter execution history by date range or status\n- Click on step names in DAG view for step-specific details\n- Use the refresh button if live updates seem delayed\n- Check the scheduler status to verify cron jobs are active"
              },
              {
                "name": "dagu-workflows",
                "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition",
                "path": "dagu/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "dagu-workflows",
                  "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition"
                },
                "content": "# Dagu Workflow Authoring\n\nThis skill activates when creating or modifying Dagu workflow definitions, configuring workflow steps, scheduling, or composing complex workflows.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Dagu workflow YAML files\n- Configuring workflow steps and executors\n- Setting up workflow scheduling with cron\n- Defining step dependencies and data flow\n- Implementing error handling and retries\n- Composing hierarchical workflows\n- Using environment variables and parameters\n\n## Basic Workflow Structure\n\n### Minimal Workflow\n\n```yaml\n# hello.yaml\nsteps:\n  - name: hello\n    command: echo \"Hello from Dagu!\"\n```\n\n### Complete Workflow Structure\n\n```yaml\nname: my_workflow\ndescription: Description of what this workflow does\n\n# Schedule (optional)\nschedule: \"0 2 * * *\"  # Cron format: daily at 2 AM\n\n# Environment variables\nenv:\n  - KEY: value\n  - DB_HOST: localhost\n\n# Parameters\nparams: ENVIRONMENT=production\n\n# Email notifications (optional)\nmailOn:\n  failure: true\n  success: false\n\nsmtp:\n  host: smtp.example.com\n  port: 587\n\nerrorMail:\n  from: dagu@example.com\n  to: alerts@example.com\n\n# Workflow steps\nsteps:\n  - name: step1\n    command: echo \"First step\"\n\n  - name: step2\n    command: echo \"Second step\"\n    depends:\n      - step1\n```\n\n## Steps\n\n### Basic Step\n\n```yaml\nsteps:\n  - name: greet\n    command: echo \"Hello, World!\"\n```\n\n### Step with Script\n\n```yaml\nsteps:\n  - name: process\n    command: |\n      echo \"Starting processing...\"\n      ./scripts/process.sh\n      echo \"Done!\"\n```\n\n### Step with Working Directory\n\n```yaml\nsteps:\n  - name: build\n    dir: /path/to/project\n    command: make build\n```\n\n### Step with Environment Variables\n\n```yaml\nsteps:\n  - name: deploy\n    env:\n      - ENVIRONMENT: production\n      - API_KEY: $API_KEY  # From global env\n    command: ./deploy.sh\n```\n\n## Executors\n\n### Command Executor (Default)\n\n```yaml\nsteps:\n  - name: shell_command\n    command: ./script.sh\n```\n\n### Docker Executor\n\n```yaml\nsteps:\n  - name: run_in_container\n    executor:\n      type: docker\n      config:\n        image: alpine:latest\n    command: echo \"Running in Docker\"\n\n  - name: with_volumes\n    executor:\n      type: docker\n      config:\n        image: node:18\n        volumes:\n          - /host/path:/container/path\n        env:\n          - NODE_ENV=production\n    command: npm run build\n```\n\n### SSH Executor\n\n```yaml\nsteps:\n  - name: remote_execution\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: server.example.com\n        key: /path/to/ssh/key\n    command: ./remote_script.sh\n```\n\n### HTTP Executor\n\n```yaml\nsteps:\n  - name: api_call\n    executor:\n      type: http\n      config:\n        method: POST\n        url: https://api.example.com/webhook\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer $API_TOKEN\n        body: |\n          {\n            \"event\": \"workflow_complete\",\n            \"timestamp\": \"{{.timestamp}}\"\n          }\n```\n\n### Mail Executor\n\n```yaml\nsteps:\n  - name: send_notification\n    executor:\n      type: mail\n      config:\n        to: user@example.com\n        from: dagu@example.com\n        subject: Workflow Complete\n        message: |\n          The workflow has completed successfully.\n          Time: {{.timestamp}}\n```\n\n### JQ Executor\n\n```yaml\nsteps:\n  - name: transform_json\n    executor:\n      type: jq\n      config:\n        query: '.users[] | select(.active == true) | .email'\n    command: cat users.json\n```\n\n## Step Dependencies\n\n### Simple Dependencies\n\n```yaml\nsteps:\n  - name: download\n    command: wget https://example.com/data.zip\n\n  - name: extract\n    depends:\n      - download\n    command: unzip data.zip\n\n  - name: process\n    depends:\n      - extract\n    command: ./process.sh\n```\n\n### Multiple Dependencies\n\n```yaml\nsteps:\n  - name: fetch_data\n    command: ./fetch.sh\n\n  - name: fetch_config\n    command: ./fetch_config.sh\n\n  - name: process\n    depends:\n      - fetch_data\n      - fetch_config\n    command: ./process.sh\n```\n\n### Parallel Execution\n\n```yaml\n# These run in parallel (no dependencies)\nsteps:\n  - name: task1\n    command: ./task1.sh\n\n  - name: task2\n    command: ./task2.sh\n\n  - name: task3\n    command: ./task3.sh\n\n  # This waits for all above to complete\n  - name: finalize\n    depends:\n      - task1\n      - task2\n      - task3\n    command: ./finalize.sh\n```\n\n## Conditional Execution\n\n### Preconditions\n\n```yaml\nsteps:\n  - name: deploy_production\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    command: ./deploy.sh\n```\n\n### Continue On Failure\n\n```yaml\nsteps:\n  - name: optional_step\n    continueOn:\n      failure: true\n    command: ./might_fail.sh\n\n  - name: cleanup\n    depends:\n      - optional_step\n    command: ./cleanup.sh  # Runs even if optional_step fails\n```\n\n## Error Handling and Retries\n\n### Retry Configuration\n\n```yaml\nsteps:\n  - name: flaky_api_call\n    command: curl https://api.example.com/data\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n```\n\n### Exponential Backoff\n\n```yaml\nsteps:\n  - name: with_backoff\n    command: ./external_api.sh\n    retryPolicy:\n      limit: 5\n      intervalSec: 5\n      exponentialBackoff: true  # 5s, 10s, 20s, 40s, 80s\n```\n\n### Signal on Stop\n\n```yaml\nsteps:\n  - name: graceful_shutdown\n    command: ./long_running_process.sh\n    signalOnStop: SIGTERM  # Send SIGTERM instead of SIGKILL\n```\n\n## Data Flow\n\n### Output Variables\n\n```yaml\nsteps:\n  - name: generate_id\n    command: echo \"ID_$(date +%s)\"\n    output: PROCESS_ID\n\n  - name: use_id\n    depends:\n      - generate_id\n    command: echo \"Processing with ID: $PROCESS_ID\"\n```\n\n### Script Output\n\n```yaml\nsteps:\n  - name: get_config\n    script: |\n      #!/bin/bash\n      export DB_HOST=\"localhost\"\n      export DB_PORT=\"5432\"\n    output: DB_CONFIG\n\n  - name: connect\n    depends:\n      - get_config\n    command: ./connect.sh $DB_HOST $DB_PORT\n```\n\n## Scheduling\n\n### Cron Schedule\n\n```yaml\n# Daily at 2 AM\nschedule: \"0 2 * * *\"\n\n# Every Monday at 9 AM\nschedule: \"0 9 * * 1\"\n\n# Every 15 minutes\nschedule: \"*/15 * * * *\"\n\n# First day of month at midnight\nschedule: \"0 0 1 * *\"\n```\n\n### Start/Stop Times\n\n```yaml\n# Only run during business hours\nschedule:\n  start: \"2024-01-01\"\n  end: \"2024-12-31\"\n  cron: \"0 9-17 * * 1-5\"  # Mon-Fri, 9 AM to 5 PM\n```\n\n## Environment Variables\n\n### Global Environment\n\n```yaml\nenv:\n  - ENVIRONMENT: production\n  - LOG_LEVEL: info\n  - API_URL: https://api.example.com\n\nsteps:\n  - name: use_env\n    command: echo \"Environment: $ENVIRONMENT\"\n```\n\n### Step-Level Environment\n\n```yaml\nsteps:\n  - name: with_custom_env\n    env:\n      - CUSTOM_VAR: value\n      - OVERRIDE: step_value\n    command: ./script.sh\n```\n\n### Environment from File\n\n```yaml\nenv:\n  - .env  # Load from .env file\n\nsteps:\n  - name: use_env_file\n    command: echo \"DB_HOST: $DB_HOST\"\n```\n\n## Parameters\n\n### Defining Parameters\n\n```yaml\nparams: ENVIRONMENT=development VERSION=1.0.0\n\nsteps:\n  - name: deploy\n    command: ./deploy.sh $ENVIRONMENT $VERSION\n```\n\n### Using Parameters\n\n```bash\n# Run with default parameters\ndagu start workflow.yaml\n\n# Override parameters\ndagu start workflow.yaml ENVIRONMENT=production VERSION=2.0.0\n```\n\n## Sub-Workflows\n\n### Calling Sub-Workflows\n\n```yaml\n# main.yaml\nsteps:\n  - name: run_sub_workflow\n    run: sub_workflow.yaml\n    params: PARAM=value\n\n  - name: another_sub\n    run: workflows/another.yaml\n```\n\n### Hierarchical Workflows\n\n```yaml\n# orchestrator.yaml\nsteps:\n  - name: data_ingestion\n    run: workflows/ingest.yaml\n\n  - name: data_processing\n    depends:\n      - data_ingestion\n    run: workflows/process.yaml\n\n  - name: data_export\n    depends:\n      - data_processing\n    run: workflows/export.yaml\n```\n\n## Handlers\n\n### Cleanup Handler\n\n```yaml\nhandlerOn:\n  exit:\n    - name: cleanup\n      command: ./cleanup.sh\n\nsteps:\n  - name: main_task\n    command: ./task.sh\n```\n\n### Error Handler\n\n```yaml\nhandlerOn:\n  failure:\n    - name: send_alert\n      executor:\n        type: mail\n        config:\n          to: alerts@example.com\n          subject: \"Workflow Failed\"\n          message: \"Workflow {{.Name}} failed at {{.timestamp}}\"\n\nsteps:\n  - name: risky_operation\n    command: ./operation.sh\n```\n\n### Success Handler\n\n```yaml\nhandlerOn:\n  success:\n    - name: notify_success\n      command: ./notify.sh \"Workflow completed successfully\"\n\nsteps:\n  - name: task\n    command: ./task.sh\n```\n\n## Templates and Variables\n\n### Built-in Variables\n\n```yaml\nsteps:\n  - name: use_variables\n    command: |\n      echo \"Workflow: {{.Name}}\"\n      echo \"Step: {{.Step.Name}}\"\n      echo \"Timestamp: {{.timestamp}}\"\n      echo \"Request ID: {{.requestId}}\"\n```\n\n### Custom Templates\n\n```yaml\nparams: USER=alice\n\nsteps:\n  - name: templated\n    command: echo \"Hello, {{.Params.USER}}!\"\n```\n\n## Common Patterns\n\n### ETL Pipeline\n\n```yaml\nname: etl_pipeline\ndescription: Extract, Transform, Load data pipeline\n\nschedule: \"0 2 * * *\"  # Daily at 2 AM\n\nenv:\n  - DATA_SOURCE: s3://bucket/data\n  - TARGET_DB: postgresql://localhost/warehouse\n\nsteps:\n  - name: extract\n    command: ./extract.sh $DATA_SOURCE\n    output: EXTRACTED_FILE\n\n  - name: transform\n    depends:\n      - extract\n    command: ./transform.sh $EXTRACTED_FILE\n    output: TRANSFORMED_FILE\n\n  - name: load\n    depends:\n      - transform\n    command: ./load.sh $TRANSFORMED_FILE $TARGET_DB\n\n  - name: cleanup\n    depends:\n      - load\n    command: rm -f $EXTRACTED_FILE $TRANSFORMED_FILE\n\nhandlerOn:\n  failure:\n    - name: alert\n      executor:\n        type: mail\n        config:\n          to: data-team@example.com\n          subject: \"ETL Pipeline Failed\"\n```\n\n### Multi-Environment Deployment\n\n```yaml\nname: deploy\ndescription: Deploy application to multiple environments\n\nparams: ENVIRONMENT=staging VERSION=latest\n\nsteps:\n  - name: build\n    command: docker build -t app:$VERSION .\n\n  - name: test\n    depends:\n      - build\n    command: docker run app:$VERSION npm test\n\n  - name: deploy_staging\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"staging\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: staging.example.com\n    command: ./deploy.sh $VERSION\n\n  - name: deploy_production\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: prod.example.com\n    command: ./deploy.sh $VERSION\n```\n\n### Data Backup Workflow\n\n```yaml\nname: database_backup\ndescription: Automated database backup workflow\n\nschedule: \"0 3 * * *\"  # Daily at 3 AM\n\nenv:\n  - DB_HOST: localhost\n  - DB_NAME: myapp\n  - BACKUP_DIR: /backups\n  - S3_BUCKET: s3://backups/db\n\nsteps:\n  - name: create_backup\n    command: |\n      TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n      pg_dump -h $DB_HOST $DB_NAME > $BACKUP_DIR/backup_$TIMESTAMP.sql\n      echo \"backup_$TIMESTAMP.sql\"\n    output: BACKUP_FILE\n\n  - name: compress\n    depends:\n      - create_backup\n    command: gzip $BACKUP_DIR/$BACKUP_FILE\n    output: COMPRESSED_FILE\n\n  - name: upload_to_s3\n    depends:\n      - compress\n    command: aws s3 cp $BACKUP_DIR/$COMPRESSED_FILE.gz $S3_BUCKET/\n\n  - name: cleanup_old_backups\n    depends:\n      - upload_to_s3\n    command: |\n      find $BACKUP_DIR -name \"*.sql.gz\" -mtime +30 -delete\n      aws s3 ls $S3_BUCKET/ | awk '{print $4}' | head -n -30 | xargs -I {} aws s3 rm $S3_BUCKET/{}\n\nhandlerOn:\n  failure:\n    - name: alert_failure\n      executor:\n        type: mail\n        config:\n          to: dba@example.com\n          subject: \"Backup Failed\"\n  success:\n    - name: log_success\n      command: echo \"Backup completed at $(date)\" >> /var/log/backups.log\n```\n\n### Monitoring and Alerts\n\n```yaml\nname: health_check\ndescription: Monitor services and send alerts\n\nschedule: \"*/5 * * * *\"  # Every 5 minutes\n\nsteps:\n  - name: check_web_service\n    command: curl -f https://app.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_api_service\n    command: curl -f https://api.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_database\n    command: pg_isready -h db.example.com\n    continueOn:\n      failure: true\n\nhandlerOn:\n  failure:\n    - name: alert_on_failure\n      executor:\n        type: http\n        config:\n          method: POST\n          url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n          headers:\n            Content-Type: application/json\n          body: |\n            {\n              \"text\": \"⚠️ Service health check failed\",\n              \"attachments\": [{\n                \"color\": \"danger\",\n                \"fields\": [\n                  {\"title\": \"Workflow\", \"value\": \"{{.Name}}\", \"short\": true},\n                  {\"title\": \"Time\", \"value\": \"{{.timestamp}}\", \"short\": true}\n                ]\n              }]\n            }\n```\n\n## Best Practices\n\n### Workflow Organization\n\n```yaml\n# Good: Clear, descriptive names\nname: user_data_sync\ndescription: Synchronize user data from CRM to database\n\n# Good: Logical step names\nsteps:\n  - name: fetch_from_crm\n  - name: validate_data\n  - name: update_database\n\n# Avoid: Generic names\nname: workflow1\nsteps:\n  - name: step1\n  - name: step2\n```\n\n### Error Handling\n\n```yaml\n# Always define error handlers for critical workflows\nhandlerOn:\n  failure:\n    - name: cleanup\n      command: ./cleanup.sh\n    - name: notify\n      executor:\n        type: mail\n        config:\n          to: team@example.com\n\n# Use retries for flaky operations\nsteps:\n  - name: api_call\n    command: curl https://api.example.com\n    retryPolicy:\n      limit: 3\n      intervalSec: 5\n      exponentialBackoff: true\n```\n\n### Environment Management\n\n```yaml\n# Use parameters for environment-specific values\nparams: ENVIRONMENT=development\n\n# Load environment from files\nenv:\n  - config/$ENVIRONMENT.env\n\n# Override in production\n# dagu start workflow.yaml ENVIRONMENT=production\n```\n\n### Modular Workflows\n\n```yaml\n# Break complex workflows into sub-workflows\nsteps:\n  - name: data_ingestion\n    run: workflows/ingestion.yaml\n\n  - name: data_transformation\n    run: workflows/transformation.yaml\n    depends:\n      - data_ingestion\n```\n\n## Key Principles\n\n- **Keep workflows focused**: One workflow per logical task\n- **Use dependencies wisely**: Parallelize when possible\n- **Handle errors explicitly**: Define failure handlers\n- **Use retries for flaky operations**: Network calls, external APIs\n- **Parameterize configurations**: Make workflows reusable\n- **Document workflows**: Add clear names and descriptions\n- **Test workflows**: Start with small, focused workflows\n- **Monitor and alert**: Use handlers to track workflow health"
              },
              {
                "name": "elixir-anti-patterns",
                "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices",
                "path": "elixir/skills/anti-patterns/SKILL.md",
                "frontmatter": {
                  "name": "elixir-anti-patterns",
                  "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices"
                },
                "content": "# Elixir Anti-Patterns Detection and Refactoring\n\nYou are an expert at identifying Elixir anti-patterns and suggesting idiomatic refactorings. Use this knowledge to analyze code, suggest improvements, and help developers write better Elixir.\n\n## Code-Related Anti-Patterns\n\n### 1. Comments Overuse\n**Problem:** Excessive or self-explanatory comments reduce readability rather than enhance it.\n\n**Detection:**\n- Inline comments explaining obvious code\n- Comments for every function line\n- Comments duplicating what code already says clearly\n\n**Refactoring:**\n- Use clear function and variable names instead of explanatory comments\n- Replace inline comments with `@doc` and `@moduledoc` for documentation\n- Use module attributes for configuration values\n\n**Example:**\n```elixir\n# Bad\ndef calculate() do\n  # Get current time\n  now = DateTime.utc_now()\n  # Add 5 minutes\n  DateTime.add(now, 5 * 60, :second)\nend\n\n# Good\n@minutes_to_add 5\n\ndef timestamp_five_minutes_from_now do\n  now = DateTime.utc_now()\n  DateTime.add(now, @minutes_to_add * 60, :second)\nend\n```\n\n### 2. Complex `else` Clauses in `with`\n**Problem:** Flattening all error handling into a single complex `else` block obscures which clause produced which error.\n\n**Detection:**\n- Large `else` blocks with many pattern match clauses\n- Difficulty determining error sources\n- Complex error handling logic in `else`\n\n**Refactoring:**\n- Normalize return types in private functions\n- Handle errors closer to their source\n- Let `with` focus on success paths\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, decoded} <- Jason.decode(content) do\n    {:ok, decoded}\n  else\n    {:error, :enoent} -> {:error, :file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_json}\n    {:error, reason} -> {:error, reason}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- read_file(path),\n       {:ok, config} <- parse_json(content) do\n    {:ok, config}\n  end\nend\n\ndefp read_file(path) do\n  case File.read(path) do\n    {:ok, content} -> {:ok, content}\n    {:error, :enoent} -> {:error, :file_not_found}\n    error -> error\n  end\nend\n\ndefp parse_json(content) do\n  case Jason.decode(content) do\n    {:ok, data} -> {:ok, data}\n    {:error, _} -> {:error, :invalid_json}\n  end\nend\n```\n\n### 3. Complex Extractions in Clauses\n**Problem:** Extracting values across multiple clauses and arguments makes it unclear which variables serve pattern/guard purposes versus function body usage.\n\n**Detection:**\n- Many variable extractions in function heads\n- Mixed guard and body variable usage\n- Unclear variable purposes\n\n**Refactoring:**\n- Extract only pattern/guard-related variables in function signatures\n- Use capture patterns like `%User{age: age} = user`\n- Extract body variables inside the clause\n\n**Example:**\n```elixir\n# Bad\ndef process(%User{age: age, name: name, email: email} = user) when age >= 18 do\n  # Only using name and email in body, not age\n  send_email(email, \"Hello #{name}\")\nend\n\n# Good\ndef process(%User{age: age} = user) when age >= 18 do\n  send_email(user.email, \"Hello #{user.name}\")\nend\n```\n\n### 4. Dynamic Atom Creation\n**Problem:** Atoms aren't garbage-collected and are limited to ~1 million. Uncontrolled dynamic atom creation poses memory and security risks.\n\n**Detection:**\n- `String.to_atom/1` with untrusted input\n- Converting user input directly to atoms\n- Unbounded atom creation in loops\n\n**Refactoring:**\n- Use explicit mappings via pattern-matching\n- Use `String.to_existing_atom/1` with pre-defined atoms\n- Keep strings when atom conversion isn't necessary\n\n**Example:**\n```elixir\n# Bad - Security risk!\ndef set_role(user, role_string) do\n  %{user | role: String.to_atom(role_string)}\nend\n\n# Good\ndef set_role(user, role) when role in [:admin, :editor, :viewer] do\n  %{user | role: role}\nend\n\n# Or with pattern matching\ndef set_role(user, \"admin\"), do: %{user | role: :admin}\ndef set_role(user, \"editor\"), do: %{user | role: :editor}\ndef set_role(user, \"viewer\"), do: %{user | role: :viewer}\ndef set_role(_user, invalid), do: {:error, \"Invalid role: #{invalid}\"}\n```\n\n### 5. Long Parameter List\n**Problem:** Functions with excessive parameters become confusing and error-prone to use.\n\n**Detection:**\n- Functions with 4+ parameters\n- Parameters that are conceptually related\n- Difficult to remember parameter order\n\n**Refactoring:**\n- Group related parameters into maps or structs\n- Use keyword lists for optional parameters\n- Create domain objects\n\n**Example:**\n```elixir\n# Bad\ndef create_loan(user_id, user_name, user_email, book_id, book_title, book_isbn) do\n  # ...\nend\n\n# Good\ndef create_loan(user, book) do\n  # ...\nend\n\n# Or with keyword list for options\ndef create_loan(user, book, opts \\\\ []) do\n  duration = Keyword.get(opts, :duration, 14)\n  renewable = Keyword.get(opts, :renewable, true)\n  # ...\nend\n```\n\n### 6. Namespace Trespassing\n**Problem:** Defining modules outside your library's namespace risks conflicts since the Erlang VM loads only one module instance per name.\n\n**Detection:**\n- Library defining modules in common namespaces (e.g., `Plug.*` when you're not Plug)\n- Modules without library prefix\n- Potential naming conflicts with other libraries\n\n**Refactoring:**\n- Always prefix modules with your library namespace\n- Use clear, unique top-level module names\n\n**Example:**\n```elixir\n# Bad - Library named :plug_auth\ndefmodule Plug.Auth do\n  # This conflicts with the actual Plug library!\nend\n\n# Good\ndefmodule PlugAuth do\n  # ...\nend\n\ndefmodule PlugAuth.Session do\n  # ...\nend\n```\n\n### 7. Non-assertive Map Access\n**Problem:** Using dynamic access (`map[:key]`) for required keys masks missing data, allowing `nil` to propagate instead of failing fast.\n\n**Detection:**\n- `map[:key]` for required/expected keys\n- Nil checks after map access\n- Silent failures from missing keys\n\n**Refactoring:**\n- Use static access (`map.key`) for required keys\n- Pattern-match on struct/map keys\n- Reserve dynamic access for optional fields\n\n**Example:**\n```elixir\n# Bad\ndef distance(point) do\n  x = point[:x]  # Returns nil if :x is missing!\n  y = point[:y]\n  :math.sqrt(x * x + y * y)  # Crashes on nil, but unclear why\nend\n\n# Good\ndef distance(%{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)  # Clear error if keys missing\nend\n\n# Or with structs\ndefmodule Point do\n  defstruct [:x, :y]\nend\n\ndef distance(%Point{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)\nend\n```\n\n### 8. Non-assertive Pattern Matching\n**Problem:** Writing defensive code that returns incorrect values instead of using pattern matching to assert expected structures causes silent failures.\n\n**Detection:**\n- Defensive nil checks instead of pattern matching\n- Functions returning invalid data on unexpected input\n- Avoiding crashes when crashes are appropriate\n\n**Refactoring:**\n- Use pattern matching to assert expected structures\n- Let functions crash on invalid input\n- Trust supervisors to handle failures\n\n**Example:**\n```elixir\n# Bad\ndef parse_query_param(param) do\n  case String.split(param, \"=\") do\n    [key, value] -> {key, value}\n    _ -> {\"\", \"\"}  # Silent failure!\n  end\nend\n\n# Good\ndef parse_query_param(param) do\n  [key, value] = String.split(param, \"=\")\n  {key, value}\nend\n# Crashes with clear error if format is wrong - this is good!\n```\n\n### 9. Non-assertive Truthiness\n**Problem:** Using truthiness operators (`&&`, `||`, `!`) when all operands are boolean is unnecessarily generic and unclear.\n\n**Detection:**\n- `&&`, `||`, `!` with boolean expressions\n- Comparisons like `is_binary(x) && is_integer(y)`\n- Mixing boolean and truthy logic\n\n**Refactoring:**\n- Use `and`, `or`, `not` for boolean-only operations\n- Reserve `&&`, `||`, `!` for truthy/falsy logic\n\n**Example:**\n```elixir\n# Bad\ndef valid_user?(name, age) do\n  is_binary(name) && is_integer(age) && age >= 18\nend\n\n# Good\ndef valid_user?(name, age) do\n  is_binary(name) and is_integer(age) and age >= 18\nend\n\n# Truthy operators are OK for nil/value checks\ndef get_name(user) do\n  user[:name] || \"Anonymous\"\nend\n```\n\n### 10. Structs with 32 Fields or More\n**Problem:** Structs with 32+ fields switch from Erlang's efficient flat-map representation to hash maps, increasing memory usage.\n\n**Detection:**\n- Struct definitions with 32+ fields\n- Large, flat data structures\n- Performance degradation with many fields\n\n**Refactoring:**\n- Nest optional fields into metadata structures\n- Use nested structs for related fields\n- Group frequently-accessed fields separately\n\n**Example:**\n```elixir\n# Bad\ndefmodule User do\n  defstruct [\n    :id, :email, :name, :age, :address, :city, :state, :zip,\n    :phone, :mobile, :fax, :company, :title, :department,\n    :created_at, :updated_at, :last_login, :login_count,\n    :preference1, :preference2, :preference3, :preference4,\n    # ... 15 more fields\n  ]\nend\n\n# Good\ndefmodule User do\n  defstruct [\n    :id,\n    :email,\n    :name,\n    :profile,      # Nested struct\n    :preferences,  # Nested struct\n    :metadata      # Nested struct\n  ]\nend\n\ndefmodule User.Profile do\n  defstruct [:age, :phone, :mobile, :address, :city, :state, :zip]\nend\n\ndefmodule User.Preferences do\n  defstruct [:theme, :notifications, :language]\nend\n```\n\n## Design-Related Anti-Patterns\n\n### 1. Alternative Return Types\n**Problem:** Functions with options that drastically change their return type make it unclear what the function actually returns.\n\n**Detection:**\n- Options that change return type structure\n- Functions returning different types based on flags\n- Unclear function contracts\n\n**Refactoring:**\n- Create separate, specifically-named functions\n- Keep return types consistent within a function\n\n**Example:**\n```elixir\n# Bad\ndef parse(string, opts \\\\ []) do\n  case Integer.parse(string) do\n    {int, rest} ->\n      if opts[:discard_rest], do: int, else: {int, rest}\n    :error ->\n      :error\n  end\nend\n\n# Good\ndef parse(string) do\n  case Integer.parse(string) do\n    {int, rest} -> {int, rest}\n    :error -> :error\n  end\nend\n\ndef parse_discard_rest(string) do\n  case Integer.parse(string) do\n    {int, _rest} -> int\n    :error -> :error\n  end\nend\n```\n\n### 2. Boolean Obsession\n**Problem:** Using multiple booleans with overlapping states instead of atoms or composite types to represent domain concepts.\n\n**Detection:**\n- Multiple boolean parameters\n- Overlapping boolean states\n- Complex boolean logic\n\n**Refactoring:**\n- Replace multiple booleans with a single atom/enum option\n- Prefer atoms over booleans even for single arguments\n- Use domain-specific types\n\n**Example:**\n```elixir\n# Bad\ndef create_user(name, email, admin: false, editor: false, viewer: true) do\n  # What if admin: true, editor: true?\nend\n\n# Good\ndef create_user(name, email, role: :viewer) do\n  # Clear: role can be :admin, :editor, or :viewer\nend\n```\n\n### 3. Exceptions for Control-Flow\n**Problem:** Using `try/rescue` for expected errors instead of pattern matching with case statements and tuple returns.\n\n**Detection:**\n- `try/rescue` blocks for normal operation errors\n- Using `!` functions and rescuing\n- Exceptions in normal business logic\n\n**Refactoring:**\n- Use non-bang functions returning `{:ok, value}` or `{:error, reason}`\n- Reserve exceptions for invalid arguments and programming errors\n- Use pattern matching for error handling\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  try do\n    content = File.read!(path)\n    Jason.decode!(content)\n  rescue\n    e -> {:error, e}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  end\nend\n```\n\n### 4. Primitive Obsession\n**Problem:** Excessively using basic types (strings, integers) instead of creating composite types to represent structured domain concepts.\n\n**Detection:**\n- Passing related primitives separately\n- String/integer parameters representing complex concepts\n- Lack of domain modeling\n\n**Refactoring:**\n- Create domain-specific structs or maps\n- Introduce parser functions converting primitives to structured data\n- Use types to enforce business rules\n\n**Example:**\n```elixir\n# Bad\ndef create_address(street, city, state, zip, country) do\n  # All strings, no validation\n  \"#{street}, #{city}, #{state} #{zip}, #{country}\"\nend\n\n# Good\ndefmodule Address do\n  defstruct [:street, :city, :state, :zip, :country]\n\n  def new(attrs) do\n    struct!(__MODULE__, attrs)\n  end\n\n  def format(%__MODULE__{} = address) do\n    \"#{address.street}, #{address.city}, #{address.state} #{address.zip}, #{address.country}\"\n  end\nend\n```\n\n### 5. Unrelated Multi-Clause Function\n**Problem:** Grouping completely unrelated business logic into one multi-clause function.\n\n**Detection:**\n- Single function handling multiple unrelated types\n- Overly broad type specifications\n- No conceptual relationship between clauses\n\n**Refactoring:**\n- Split into distinct functions with specific names\n- Reserve multi-clause patterns for related functionality variations\n- Use protocols for polymorphism when appropriate\n\n**Example:**\n```elixir\n# Bad\ndef update(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update(%Animal{} = animal) do\n  # Completely different animal logic\nend\n\n# Good\ndef update_product(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update_animal(%Animal{} = animal) do\n  # Animal-specific logic\nend\n\n# Or use a protocol\ndefprotocol Updatable do\n  def update(item)\nend\n\ndefimpl Updatable, for: Product do\n  def update(product), do: # ...\nend\n\ndefimpl Updatable, for: Animal do\n  def update(animal), do: # ...\nend\n```\n\n### 6. Using Application Configuration for Libraries\n**Problem:** Libraries relying on global application environment configuration prevent multiple dependent applications from configuring the library differently.\n\n**Detection:**\n- `Application.get_env/2` or `Application.fetch_env!/2` in library code\n- Global configuration requirements\n- Inability to configure per-consumer\n\n**Refactoring:**\n- Accept configuration via function parameters\n- Use keyword lists with sensible defaults\n- Allow runtime configuration\n\n**Example:**\n```elixir\n# Bad - Library code\ndef split(string) do\n  parts = Application.fetch_env!(:dash_splitter, :parts)\n  String.split(string, \"-\", parts: parts)\nend\n\n# Good\ndef split(string, opts \\\\ []) do\n  parts = Keyword.get(opts, :parts, 2)\n  String.split(string, \"-\", parts: parts)\nend\n```\n\n## Usage Guidelines\n\nWhen reviewing or writing Elixir code:\n\n1. **Scan for anti-patterns** - Check code against the patterns listed above\n2. **Explain the problem** - Help the developer understand why it's an issue\n3. **Suggest refactoring** - Provide concrete, idiomatic alternatives\n4. **Consider context** - Sometimes anti-patterns are acceptable for specific use cases\n5. **Prioritize** - Focus on high-impact issues first (security, performance, maintainability)\n\n## Key Principles\n\n- **Let it crash** - Use pattern matching to assert expectations; don't write defensive code\n- **Fail fast** - Expose errors early rather than propagating nil or invalid data\n- **Be explicit** - Prefer clear, specific code over clever or terse solutions\n- **Model your domain** - Create types that represent business concepts\n- **Design for clarity** - Code should be obvious to read and maintain"
              },
              {
                "name": "elixir-config",
                "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                "path": "elixir/skills/config/SKILL.md",
                "frontmatter": {
                  "name": "elixir-config",
                  "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                  "license": "MIT"
                },
                "content": "# Elixir Configuration\n\nGuide for proper application configuration in Elixir, with emphasis on understanding and correctly using runtime vs compile-time configuration.\n\n## When to Activate\n\nUse this skill when:\n- Setting up or modifying application configuration\n- Choosing between `config.exs` and `runtime.exs`\n- Deciding between `Application.compile_env` and `Application.get_env`\n- Debugging configuration-related issues\n- Working with releases or deployment configuration\n- Migrating from `use Mix.Config` to `import Config`\n- Writing libraries that need configuration\n\n## Critical Principle\n\n> **Runtime configuration is the preferred approach.** Only use compile-time configuration when values must affect compilation itself.\n\n## Configuration Files\n\n### config/config.exs (Compile-Time)\n\nEvaluated during project compilation, before your application starts.\n\n```elixir\nimport Config\n\n# Basic configuration\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  username: \"postgres\",\n  password: \"postgres\",\n  hostname: \"localhost\"\n\n# Environment-specific config\nconfig :my_app,\n  environment: config_env()\n\n# Import environment-specific config files\nimport_config \"#{config_env()}.exs\"\n```\n\n**Key characteristics:**\n- Runs at compile time\n- Uses `import Config` (not `use Mix.Config`)\n- Can use `config_env()` and `config_target()`\n- Can import other config files with `import_config/1`\n- Deep-merges keyword lists\n- **Library config.exs is NOT evaluated when used as a dependency**\n\n### config/runtime.exs (Runtime)\n\nEvaluated right before applications start in both Mix and releases.\n\n```elixir\nimport Config\n\n# Read from environment variables\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.get_env(\"DATABASE_PASSWORD\") || \"postgres\",\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n\n# Conditional runtime configuration\nif config_env() == :prod do\n  config :my_app, MyAppWeb.Endpoint,\n    secret_key_base: System.fetch_env!(\"SECRET_KEY_BASE\"),\n    http: [port: String.to_integer(System.fetch_env!(\"PORT\"))]\nend\n```\n\n**Key characteristics:**\n- Runs at application startup (both dev and prod)\n- Executes in both Mix projects and releases\n- Perfect for environment variables and runtime values\n- **Does NOT support `import_config/1`**\n- Can use `System.get_env` and `System.fetch_env!`\n\n### config/dev.exs, config/test.exs, config/prod.exs\n\nEnvironment-specific compile-time configuration, typically imported from `config.exs`:\n\n```elixir\n# config/config.exs\nimport_config \"#{config_env()}.exs\"\n\n# config/dev.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  show_sensitive_data_on_connection_error: true,\n  pool_size: 10\n\n# config/test.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox,\n  pool_size: 10\n\n# config/prod.exs\nimport Config\n\n# Production-specific compile-time config only\nconfig :my_app, MyAppWeb.Endpoint,\n  cache_static_manifest: \"priv/static/cache_manifest.json\"\n```\n\n## Accessing Configuration\n\n### Runtime Access (Preferred)\n\nUse in function bodies to read configuration at runtime:\n\n#### Application.get_env/3\n\n```elixir\ndefmodule MyApp.Service do\n  def start_link do\n    # Get with default value\n    timeout = Application.get_env(:my_app, :timeout, 5000)\n    GenServer.start_link(__MODULE__, timeout, name: __MODULE__)\n  end\nend\n```\n\n**When to use:**\n- Reading config in function bodies (most common)\n- When a sensible default exists\n- When config might change between environments\n\n#### Application.fetch_env!/2\n\n```elixir\ndefmodule MyApp.Mailer do\n  def deliver(email) do\n    # Raise if not configured (for required config)\n    api_key = Application.fetch_env!(:my_app, :mailgun_api_key)\n    send_email(email, api_key)\n  end\nend\n```\n\n**When to use:**\n- Required configuration that must exist\n- When you want explicit errors for missing config\n- When no sensible default exists\n\n#### Application.fetch_env/2\n\n```elixir\ndefmodule MyApp.Cache do\n  def get(key) do\n    case Application.fetch_env(:my_app, :cache_adapter) do\n      {:ok, adapter} -> adapter.get(key)\n      :error -> nil  # No caching configured\n    end\n  end\nend\n```\n\n**When to use:**\n- Optional configuration\n- When you need pattern matching on result\n- When absence of config is a valid state\n\n### Compile-Time Access (Use Sparingly)\n\nUse only when configuration must affect compilation:\n\n#### Application.compile_env/3\n\n```elixir\ndefmodule MyApp.JSONEncoder do\n  # Only use compile_env when the value affects compilation\n  @json_library Application.compile_env(:my_app, :json_library, Jason)\n\n  def encode(data) do\n    # The specific library is compiled into the module\n    @json_library.encode(data)\n  end\nend\n```\n\n**When to use:**\n- Configuration affects which code gets compiled\n- Performance-critical paths where indirection is costly\n- Compile-time optimizations or code generation\n\n**Warning:** Mix tracks compile-time config and raises errors if values diverge between compile and runtime.\n\n#### Application.compile_env!/2\n\n```elixir\ndefmodule MyApp.Adapter do\n  # Raises at compile time if not configured\n  @adapter Application.compile_env!(:my_app, :storage_adapter)\n\n  def store(data) do\n    @adapter.put(data)\n  end\nend\n```\n\n**When to use:**\n- Required compile-time configuration\n- Adapters or behaviors selected at compile time\n\n## Common Patterns\n\n### Pattern 1: Environment Variables in Runtime\n\n**Correct approach:**\n\n```elixir\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_url: System.get_env(\"API_URL\") || \"http://localhost:4000\",\n  api_key: System.fetch_env!(\"API_KEY\")  # Required in production\n```\n\n**Access in code:**\n\n```elixir\ndefmodule MyApp.Client do\n  def call(endpoint) do\n    api_url = Application.fetch_env!(:my_app, :api_url)\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(\"#{api_url}/#{endpoint}\", [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### Pattern 2: Development vs Production Config\n\n**config/config.exs:**\n\n```elixir\nimport Config\n\n# Shared configuration for all environments\nconfig :my_app, :shared_setting, \"value\"\n\n# Import environment-specific config\nimport_config \"#{config_env()}.exs\"\n```\n\n**config/dev.exs:**\n\n```elixir\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  show_sensitive_data_on_connection_error: true\n```\n\n**config/runtime.exs:**\n\n```elixir\nimport Config\n\n# Runtime config for all environments\nif config_env() == :prod do\n  # Production-specific runtime config\n  database_url = System.fetch_env!(\"DATABASE_URL\")\n\n  config :my_app, MyApp.Repo,\n    url: database_url,\n    pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\nend\n```\n\n### Pattern 3: Storing config_env() for Runtime Access\n\n**Problem:** Can't call `config_env()` at runtime.\n\n**Solution:** Store it in config:\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app, :environment, config_env()\n\n# Then in your code:\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\n\n  def development? do\n    environment() == :dev\n  end\nend\n```\n\n### Pattern 4: Optional Features Based on Config\n\n```elixir\ndefmodule MyApp.Telemetry do\n  def setup do\n    case Application.fetch_env(:my_app, :telemetry_backend) do\n      {:ok, :datadog} -> setup_datadog()\n      {:ok, :prometheus} -> setup_prometheus()\n      :error -> :ok  # Telemetry disabled\n    end\n  end\nend\n```\n\n### Pattern 5: Child Spec with Runtime Config\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      MyApp.Repo,\n      {MyApp.Worker, Application.fetch_env!(:my_app, :worker_opts)},\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Using compile_env for Runtime Values\n\n```elixir\n# DON'T: Using compile_env for environment variables\ndefmodule MyApp.Service do\n  @api_key Application.compile_env(:my_app, :api_key)\n\n  def call do\n    # This won't work correctly in releases!\n    HTTPoison.get(url, [{\"Authorization\", @api_key}])\n  end\nend\n```\n\n**Why it's wrong:** Environment variables aren't available at compile time in releases.\n\n**Correct approach:**\n\n```elixir\ndefmodule MyApp.Service do\n  def call do\n    # Read at runtime\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(url, [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### ❌ Reading Other Application's Config\n\n```elixir\n# DON'T: Directly access other app's configuration\ndefmodule MyApp do\n  def logger_level do\n    Application.get_env(:logger, :level)  # Fragile coupling\n  end\nend\n```\n\n**Why it's wrong:** Creates tight coupling and breaks encapsulation.\n\n**Correct approach:**\n\n```elixir\n# Configure it in your own app\n# config/config.exs\nconfig :my_app, :log_level, :info\n\n# Then read your own config\ndefmodule MyApp do\n  def log_level do\n    Application.get_env(:my_app, :log_level, :info)\n  end\nend\n```\n\n### ❌ Using Application Config in Libraries\n\n```elixir\n# DON'T: In a library\ndefmodule MyLibrary do\n  def process(data) do\n    # Library reading its own application environment\n    timeout = Application.get_env(:my_library, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n```\n\n**Why it's wrong:** Library `config.exs` is not evaluated when used as a dependency.\n\n**Correct approach:**\n\n```elixir\n# DO: Accept options as arguments\ndefmodule MyLibrary do\n  def process(data, opts \\\\ []) do\n    timeout = Keyword.get(opts, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n\n# Users configure in their application\ndefmodule MyApp.Worker do\n  def run do\n    opts = Application.get_env(:my_app, :my_library_opts, [])\n    MyLibrary.process(data, opts)\n  end\nend\n```\n\n### ❌ Using Mix Module in Application Code\n\n```elixir\n# DON'T: Use Mix.env() in application code\ndefmodule MyApp do\n  def environment do\n    Mix.env()  # Won't work in releases!\n  end\nend\n```\n\n**Why it's wrong:** `Mix` is not available in production releases.\n\n**Correct approach:**\n\n```elixir\n# Store it in config\n# config/config.exs\nconfig :my_app, :environment, config_env()\n\n# Access from application environment\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\nend\n```\n\n## Config Functions Reference\n\n### In Configuration Files\n\n| Function | Description | Where to Use |\n|----------|-------------|--------------|\n| `config/2` | Configure app with keyword list | All config files |\n| `config/3` | Configure app key with value | All config files |\n| `config_env/0` | Get current environment (`:dev`, `:test`, `:prod`) | All config files |\n| `config_target/0` | Get build target | All config files |\n| `import_config/1` | Import other config files | Not in `runtime.exs` |\n\n### In Application Code\n\n| Function | Return Type | Use Case |\n|----------|-------------|----------|\n| `Application.get_env/3` | `value \\| default` | Runtime with default |\n| `Application.fetch_env/2` | `{:ok, value} \\| :error` | Runtime with pattern matching |\n| `Application.fetch_env!/2` | `value` (raises if missing) | Required runtime config |\n| `Application.compile_env/3` | `value` | Compile-time with default |\n| `Application.compile_env!/2` | `value` (raises if missing) | Required compile-time config |\n\n## Migration Guide\n\n### From `use Mix.Config` to `import Config`\n\n**Old (deprecated):**\n\n```elixir\nuse Mix.Config\n\nconfig :my_app, :key, \"value\"\n\nif Mix.env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{Mix.env()}.exs\"\n```\n\n**New:**\n\n```elixir\nimport Config\n\nconfig :my_app, :key, \"value\"\n\nif config_env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{config_env()}.exs\"\n```\n\n**Changes:**\n1. Replace `use Mix.Config` with `import Config`\n2. Replace `Mix.env()` with `config_env()`\n3. Remove wildcard imports (not supported)\n\n### Moving Runtime Config to runtime.exs\n\n**Before (all in config.exs):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\"),  # Wrong place!\n  static_value: \"something\"\n```\n\n**After (split correctly):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  static_value: \"something\"\n\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\") || raise(\"API_KEY not set\")\n```\n\n## Best Practices Summary\n\n1. **Default to Runtime Configuration**: Use `Application.get_env/3` in function bodies\n2. **Use runtime.exs for Environment Variables**: Never read env vars in `config.exs`\n3. **Use compile_env Only When Necessary**: Only when config affects compilation\n4. **Libraries Should Not Use Application Config**: Accept options as function arguments\n5. **Never Use Mix in Application Code**: Use `config_env()` in config files, store result\n6. **Validate Required Config Early**: Use `fetch_env!/2` in application start for required values\n7. **Provide Sensible Defaults**: Use `get_env/3` with defaults for optional config\n8. **Document Configuration**: Add comments explaining what each config key does\n9. **Use runtime.exs for Releases**: Essential for Elixir releases and deployments\n10. **Store config_env() for Runtime Use**: Can't call `config_env()` outside config files\n\n## Debugging Configuration\n\n### Check Current Configuration\n\n```elixir\n# In IEx\nApplication.get_all_env(:my_app)\n\n# Check specific key\nApplication.fetch_env(:my_app, :some_key)\n\n# See all applications\nApplication.loaded_applications()\n```\n\n### Common Issues\n\n**Problem:** Config not available in tests\n\n```elixir\n# config/test.exs\nimport Config\n\nconfig :my_app, :test_value, \"configured\"\n```\n\n**Problem:** Different values in dev vs release\n\nCheck that `runtime.exs` is being used and environment variables are set correctly.\n\n**Problem:** Compile-time config not updating\n\n```bash\n# Clean and recompile\nmix clean\nmix compile\n```\n\n## Resources\n\n- **Config Module Docs**: https://hexdocs.pm/elixir/Config.html\n- **Application Module Docs**: https://hexdocs.pm/elixir/Application.html\n- **Runtime Configuration Guide**: https://hexdocs.pm/mix/Mix.Tasks.Release.html#module-runtime-configuration\n\n## Key Insights\n\n> \"Reading the application environment at runtime is the preferred approach.\"\n\n> \"If you are writing a library to be used by other developers, it is generally recommended to avoid the application environment, as the application environment is effectively a global storage.\"\n\n> \"config/config of a library is not evaluated when the library is used as a dependency, as configuration is always meant to configure the current project.\"\n\nConfiguration is a cross-cutting concern. Default to runtime configuration with `Application.get_env/3`, and only reach for compile-time configuration when you have a specific need for it that justifies the trade-offs."
              },
              {
                "name": "elixir-otp-concurrency",
                "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives",
                "path": "elixir/skills/otp/SKILL.md",
                "frontmatter": {
                  "name": "elixir-otp-concurrency",
                  "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives"
                },
                "content": "# Elixir OTP and Concurrency\n\nThis skill activates when working with OTP behaviors, building concurrent systems, managing processes, or implementing fault-tolerant architectures in Elixir.\n\n## When to Use This Skill\n\nActivate when:\n- Implementing GenServer, GenStage, Supervisor, or other OTP behaviors\n- Designing supervision trees and fault-tolerance strategies\n- Working with Tasks, Agents, or process management\n- Building concurrent or distributed systems\n- Managing application state\n- Troubleshooting process-related issues\n\n## OTP Behaviors\n\n### GenServer - Generic Server\n\nUse GenServer for stateful processes:\n\n```elixir\ndefmodule MyApp.Counter do\n  use GenServer\n\n  # Client API\n\n  def start_link(initial_value) do\n    GenServer.start_link(__MODULE__, initial_value, name: __MODULE__)\n  end\n\n  def increment do\n    GenServer.call(__MODULE__, :increment)\n  end\n\n  def get_value do\n    GenServer.call(__MODULE__, :get)\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(initial_value) do\n    {:ok, initial_value}\n  end\n\n  @impl true\n  def handle_call(:increment, _from, state) do\n    {:reply, state + 1, state + 1}\n  end\n\n  @impl true\n  def handle_call(:get, _from, state) do\n    {:reply, state, state}\n  end\nend\n```\n\n#### GenServer Best Practices\n\n- Use `call` for synchronous requests that need a response\n- Use `cast` for asynchronous fire-and-forget messages\n- Use `handle_info` for receiving regular messages\n- Keep server callbacks fast - delegate heavy work to Tasks\n- Name processes with `via` tuples or Registry for dynamic naming\n- Implement timeouts to prevent client processes from hanging\n\n#### GenServer Patterns\n\n**Background Work:**\n```elixir\ndef init(state) do\n  schedule_work()\n  {:ok, state}\nend\n\ndef handle_info(:work, state) do\n  do_work(state)\n  schedule_work()\n  {:noreply, state}\nend\n\ndefp schedule_work do\n  Process.send_after(self(), :work, 5000)\nend\n```\n\n**State Timeouts:**\n```elixir\ndef handle_call(:get, _from, state) do\n  {:reply, state, state, {:state_timeout, 30_000, :cleanup}}\nend\n\ndef handle_state_timeout(:cleanup, state) do\n  {:stop, :normal, state}\nend\n```\n\n### Supervisor - Process Supervision\n\nBuild supervision trees for fault tolerance:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  @impl true\n  def start(_type, _args) do\n    children = [\n      # Database connection pool\n      {MyApp.Repo, []},\n\n      # PubSub system\n      {Phoenix.PubSub, name: MyApp.PubSub},\n\n      # Custom supervisor\n      {MyApp.WorkerSupervisor, []},\n\n      # Individual workers\n      {MyApp.Cache, []},\n      {MyApp.RateLimiter, []},\n\n      # Web endpoint\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n#### Supervision Strategies\n\n**:one_for_one** - If a child dies, only that child is restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_one)\n```\n\n**:one_for_all** - If any child dies, all children are terminated and restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_all)\n```\n\n**:rest_for_one** - If a child dies, it and all children started after it are restarted\n```elixir\nSupervisor.start_link(children, strategy: :rest_for_one)\n```\n\n#### Dynamic Supervisors\n\nFor dynamically creating processes:\n\n```elixir\ndefmodule MyApp.WorkerSupervisor do\n  use DynamicSupervisor\n\n  def start_link(init_arg) do\n    DynamicSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__)\n  end\n\n  def start_worker(args) do\n    spec = {MyApp.Worker, args}\n    DynamicSupervisor.start_child(__MODULE__, spec)\n  end\n\n  @impl true\n  def init(_init_arg) do\n    DynamicSupervisor.init(strategy: :one_for_one)\n  end\nend\n```\n\n#### Restart Strategies\n\nConfigure child restart behavior:\n\n```elixir\nchildren = [\n  # Always restart (default)\n  {MyApp.CriticalWorker, restart: :permanent},\n\n  # Never restart\n  {MyApp.OneTimeTask, restart: :temporary},\n\n  # Only restart on abnormal exit\n  {MyApp.OptionalWorker, restart: :transient}\n]\n```\n\n### Task - Concurrent Work\n\n#### Fire-and-forget Tasks\n\nFor concurrent work without needing results:\n\n```elixir\nTask.start(fn ->\n  send_email(user, \"Welcome!\")\nend)\n```\n\n#### Awaited Tasks\n\nFor concurrent work with results:\n\n```elixir\ntask = Task.async(fn ->\n  expensive_computation()\nend)\n\n# Do other work...\n\nresult = Task.await(task, 5000)  # 5 second timeout\n```\n\n#### Supervised Tasks\n\nFor long-running tasks under supervision:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      {Task.Supervisor, name: MyApp.TaskSupervisor}\n    ]\n\n    Supervisor.start_link(children, strategy: :one_for_one)\n  end\nend\n\n# Use the supervised task\nTask.Supervisor.start_child(MyApp.TaskSupervisor, fn ->\n  long_running_operation()\nend)\n```\n\n#### Concurrent Map\n\nProcess collections concurrently:\n\n```elixir\n# Sequential\nresults = Enum.map(urls, &fetch_url/1)\n\n# Concurrent\nresults = Task.async_stream(urls, &fetch_url/1, max_concurrency: 10)\n         |> Enum.to_list()\n```\n\n### Agent - Simple State Management\n\nUse Agent for simple state:\n\n```elixir\n{:ok, agent} = Agent.start_link(fn -> %{} end, name: MyApp.Cache)\n\n# Get state\nvalue = Agent.get(MyApp.Cache, fn state -> Map.get(state, :key) end)\n\n# Update state\nAgent.update(MyApp.Cache, fn state -> Map.put(state, :key, value) end)\n\n# Get and update atomically\nAgent.get_and_update(MyApp.Cache, fn state ->\n  {Map.get(state, :key), Map.delete(state, :key)}\nend)\n```\n\n**When to use Agent vs GenServer:**\n- Use Agent for simple key-value state\n- Use GenServer when you need complex logic, callbacks, or process lifecycle management\n\n## Process Communication\n\n### send/receive\n\nBasic message passing:\n\n```elixir\n# Send message\nsend(pid, {:hello, \"world\"})\n\n# Receive message\nreceive do\n  {:hello, msg} -> IO.puts(msg)\nafter\n  5000 -> IO.puts(\"Timeout\")\nend\n```\n\n### Process Registration\n\nRegister processes by name:\n\n```elixir\n# Local registration\nProcess.register(self(), :my_process)\nsend(:my_process, :hello)\n\n# Via Registry\n{:ok, _} = Registry.start_link(keys: :unique, name: MyApp.Registry)\n\n{:ok, pid} = GenServer.start_link(MyWorker, nil,\n  name: {:via, Registry, {MyApp.Registry, \"worker_1\"}}\n)\n\n# Look up process\n[{pid, _}] = Registry.lookup(MyApp.Registry, \"worker_1\")\n```\n\n### Process Links and Monitors\n\n**Links** - Bidirectional, propagate exits:\n\n```elixir\n# Link processes\nProcess.link(pid)\n\n# Spawn linked\nspawn_link(fn -> do_work() end)\n```\n\n**Monitors** - Unidirectional, receive DOWN messages:\n\n```elixir\nref = Process.monitor(pid)\n\nreceive do\n  {:DOWN, ^ref, :process, ^pid, reason} ->\n    IO.puts(\"Process died: #{inspect(reason)}\")\nend\n```\n\n## Concurrency Patterns\n\n### Pipeline Pattern\n\nChain operations with concurrency:\n\n```elixir\ndefmodule Pipeline do\n  def process(data) do\n    data\n    |> async(&step1/1)\n    |> async(&step2/1)\n    |> async(&step3/1)\n    |> await_all()\n  end\n\n  defp async(input, fun) do\n    Task.async(fn -> fun.(input) end)\n  end\n\n  defp await_all(tasks) when is_list(tasks) do\n    Enum.map(tasks, &Task.await/1)\n  end\nend\n```\n\n### Worker Pool\n\nImplement a worker pool:\n\n```elixir\ndefmodule MyApp.WorkerPool do\n  use GenServer\n\n  def start_link(opts) do\n    pool_size = Keyword.get(opts, :size, 10)\n    GenServer.start_link(__MODULE__, pool_size, name: __MODULE__)\n  end\n\n  def execute(fun) do\n    GenServer.call(__MODULE__, {:execute, fun})\n  end\n\n  @impl true\n  def init(pool_size) do\n    workers = for _ <- 1..pool_size do\n      {:ok, pid} = Task.Supervisor.start_link()\n      pid\n    end\n\n    {:ok, %{workers: workers, index: 0}}\n  end\n\n  @impl true\n  def handle_call({:execute, fun}, _from, state) do\n    worker = Enum.at(state.workers, state.index)\n    task = Task.Supervisor.async_nolink(worker, fun)\n\n    new_index = rem(state.index + 1, length(state.workers))\n    {:reply, task, %{state | index: new_index}}\n  end\nend\n```\n\n### Backpressure with GenStage\n\nFor producer-consumer pipelines:\n\n```elixir\ndefmodule Producer do\n  use GenStage\n\n  def start_link(initial) do\n    GenStage.start_link(__MODULE__, initial, name: __MODULE__)\n  end\n\n  def init(initial) do\n    {:producer, initial}\n  end\n\n  def handle_demand(demand, state) do\n    events = Enum.to_list(state..state + demand - 1)\n    {:noreply, events, state + demand}\n  end\nend\n\ndefmodule Consumer do\n  use GenStage\n\n  def start_link() do\n    GenStage.start_link(__MODULE__, :ok)\n  end\n\n  def init(:ok) do\n    {:consumer, :ok}\n  end\n\n  def handle_events(events, _from, state) do\n    Enum.each(events, &process_event/1)\n    {:noreply, [], state}\n  end\nend\n```\n\n## ETS - Erlang Term Storage\n\nIn-memory key-value storage:\n\n```elixir\n# Create table\n:ets.new(:my_table, [:named_table, :public, read_concurrency: true])\n\n# Insert\n:ets.insert(:my_table, {:key, \"value\"})\n\n# Lookup\n[{:key, value}] = :ets.lookup(:my_table, :key)\n\n# Delete\n:ets.delete(:my_table, :key)\n\n# Match patterns\n:ets.match(:my_table, {:\"$1\", \"value\"})\n\n# Iterate\n:ets.foldl(fn {k, v}, acc -> [{k, v} | acc] end, [], :my_table)\n```\n\n### ETS Best Practices\n\n- Use `read_concurrency: true` for read-heavy workloads\n- Use `write_concurrency: true` for write-heavy workloads\n- Prefer `:set` (default) for unique keys\n- Use `:bag` or `:duplicate_bag` for multiple values per key\n- Always own ETS tables in a GenServer or Supervisor to prevent data loss\n\n## Error Handling and Fault Tolerance\n\n### Let It Crash Philosophy\n\nDesign for failure:\n\n```elixir\n# Don't do defensive programming\ndef process_order(order_id) do\n  # Let it crash if order doesn't exist\n  order = Repo.get!(Order, order_id)\n\n  # Let it crash if validation fails\n  {:ok, processed} = process(order)\n\n  processed\nend\n```\n\n### Proper Error Handling\n\nWhen to handle errors vs let crash:\n\n```elixir\n# Handle expected errors\ndef fetch_user(id) do\n  case HTTPoison.get(\"#{@api_url}/users/#{id}\") do\n    {:ok, %{status_code: 200, body: body}} ->\n      Jason.decode(body)\n\n    {:ok, %{status_code: 404}} ->\n      {:error, :not_found}\n\n    {:ok, %{status_code: status}} ->\n      {:error, {:unexpected_status, status}}\n\n    {:error, reason} ->\n      {:error, {:network_error, reason}}\n  end\nend\n\n# Let unexpected errors crash\ndef update_user!(id, params) do\n  user = Repo.get!(User, id)  # Crash if not found\n\n  user\n  |> User.changeset(params)\n  |> Repo.update!()  # Crash if invalid\nend\n```\n\n### Circuit Breaker\n\nPrevent cascading failures:\n\n```elixir\ndefmodule CircuitBreaker do\n  use GenServer\n\n  def start_link(_) do\n    GenServer.start_link(__MODULE__, %{status: :closed, failures: 0}, name: __MODULE__)\n  end\n\n  def call(fun) do\n    case GenServer.call(__MODULE__, :status) do\n      :open -> {:error, :circuit_open}\n      :closed -> execute(fun)\n    end\n  end\n\n  defp execute(fun) do\n    try do\n      result = fun.()\n      GenServer.cast(__MODULE__, :success)\n      {:ok, result}\n    rescue\n      e ->\n        GenServer.cast(__MODULE__, :failure)\n        {:error, e}\n    end\n  end\n\n  @impl true\n  def init(state), do: {:ok, state}\n\n  @impl true\n  def handle_call(:status, _from, state) do\n    {:reply, state.status, state}\n  end\n\n  @impl true\n  def handle_cast(:success, state) do\n    {:noreply, %{state | failures: 0, status: :closed}}\n  end\n\n  @impl true\n  def handle_cast(:failure, state) do\n    new_failures = state.failures + 1\n\n    if new_failures >= 5 do\n      Process.send_after(self(), :half_open, 30_000)\n      {:noreply, %{state | failures: new_failures, status: :open}}\n    else\n      {:noreply, %{state | failures: new_failures}}\n    end\n  end\n\n  @impl true\n  def handle_info(:half_open, state) do\n    {:noreply, %{state | status: :closed, failures: 0}}\n  end\nend\n```\n\n## Testing Concurrent Systems\n\n### Testing GenServers\n\n```elixir\ndefmodule MyApp.CounterTest do\n  use ExUnit.Case, async: true\n\n  test \"increments counter\" do\n    {:ok, pid} = MyApp.Counter.start_link(0)\n\n    assert MyApp.Counter.increment(pid) == 1\n    assert MyApp.Counter.increment(pid) == 2\n    assert MyApp.Counter.get_value(pid) == 2\n  end\nend\n```\n\n### Testing Asynchronous Processes\n\n```elixir\ntest \"process receives message\" do\n  parent = self()\n\n  spawn(fn ->\n    receive do\n      :ping -> send(parent, :pong)\n    end\n  end)\n\n  send(pid, :ping)\n\n  assert_receive :pong, 1000\nend\n```\n\n### Testing Supervision\n\n```elixir\ntest \"supervisor restarts crashed worker\" do\n  {:ok, sup} = Supervisor.start_link([MyApp.Worker], strategy: :one_for_one)\n\n  [{_, worker_pid, _, _}] = Supervisor.which_children(sup)\n\n  # Crash the worker\n  Process.exit(worker_pid, :kill)\n\n  # Wait for restart\n  Process.sleep(100)\n\n  # Verify new worker started\n  [{_, new_pid, _, _}] = Supervisor.which_children(sup)\n  assert new_pid != worker_pid\n  assert Process.alive?(new_pid)\nend\n```\n\n## Debugging Concurrent Systems\n\n### Observer\n\nLaunch Observer for visual process inspection:\n\n```elixir\n:observer.start()\n```\n\n### Process Info\n\nInspect running processes:\n\n```elixir\n# List all processes\nProcess.list()\n\n# Process information\nProcess.info(pid)\n\n# Message queue length\n{:message_queue_len, count} = Process.info(pid, :message_queue_len)\n\n# Current function\n{:current_function, {mod, fun, arity}} = Process.info(pid, :current_function)\n```\n\n### Tracing\n\nUse `:sys` module for debugging:\n\n```elixir\n# Enable tracing\n:sys.trace(pid, true)\n\n# Get state\n:sys.get_state(pid)\n\n# Get status\n:sys.get_status(pid)\n```\n\n## Performance Considerations\n\n### Process Spawning\n\n- Processes are lightweight (< 2KB overhead)\n- Spawning thousands/millions of processes is normal\n- Use process pools when spawn rate is very high\n\n### Message Passing\n\n- Messages are copied between processes\n- Large messages are expensive - consider ETS or persistent_term\n- Use binary for efficient large data transfer\n\n### Bottlenecks\n\n- Single GenServer can become bottleneck\n- Solution: shard state across multiple processes\n- Use ETS with `read_concurrency` for read-heavy workloads\n\n## Key Principles\n\n- **Embrace concurrency**: Use processes liberally, they're cheap\n- **Let it crash**: Don't write defensive code, use supervision\n- **Isolate failures**: Design supervision trees to contain failures\n- **Communicate via messages**: Avoid shared state between processes\n- **Use the right tool**: GenServer for state, Task for work, Agent for simple state\n- **Test at boundaries**: Test process APIs, not internal implementation\n- **Monitor and observe**: Use Observer and logging to understand system behavior"
              },
              {
                "name": "phoenix-framework",
                "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices",
                "path": "elixir/skills/phoenix/SKILL.md",
                "frontmatter": {
                  "name": "phoenix-framework",
                  "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices"
                },
                "content": "# Phoenix Framework Development\n\nThis skill activates when working with Phoenix web applications, including setup, development, LiveView, contexts, controllers, and channels.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or modifying Phoenix applications\n- Implementing LiveView components or pages\n- Working with Phoenix contexts and business logic\n- Building real-time features with channels or LiveView\n- Configuring Phoenix routers, plugs, or endpoints\n- Troubleshooting Phoenix-specific issues\n\n## Phoenix Project Structure\n\nFollow Phoenix conventions:\n\n```\nlib/\n  my_app/           # Business logic and contexts\n    accounts/       # Domain contexts\n    repo.ex\n  my_app_web/       # Web interface\n    controllers/\n    live/           # LiveView modules\n    components/     # Function components\n    router.ex\n    endpoint.ex\n```\n\n## Context-Driven Design\n\nOrganize business logic into contexts (bounded domains):\n\n### Creating Contexts\n\nGenerate contexts with related schemas:\n```bash\nmix phx.gen.context Accounts User users email:string name:string\n```\n\nStructure contexts to encapsulate business logic:\n\n```elixir\ndefmodule MyApp.Accounts do\n  @moduledoc \"\"\"\n  The Accounts context - manages user accounts and authentication.\n  \"\"\"\n\n  alias MyApp.Repo\n  alias MyApp.Accounts.User\n\n  def list_users do\n    Repo.all(User)\n  end\n\n  def get_user!(id), do: Repo.get!(User, id)\n\n  def create_user(attrs \\\\ %{}) do\n    %User{}\n    |> User.changeset(attrs)\n    |> Repo.insert()\n  end\n\n  def update_user(%User{} = user, attrs) do\n    user\n    |> User.changeset(attrs)\n    |> Repo.update()\n  end\nend\n```\n\n### Context Best Practices\n\n- Keep contexts focused on a single domain\n- Avoid cross-context dependencies when possible\n- Use public API functions, not direct Repo access in web layer\n- Name contexts after business domains, not technical layers\n\n## LiveView Development\n\nLiveView enables rich, real-time experiences without writing JavaScript.\n\n### LiveView Lifecycle\n\nUnderstand the mount → handle_event → render cycle:\n\n```elixir\ndefmodule MyAppWeb.UserLive.Index do\n  use MyAppWeb, :live_view\n\n  alias MyApp.Accounts\n\n  @impl true\n  def mount(_params, _session, socket) do\n    # Runs on initial page load and live connection\n    {:ok, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def handle_params(params, _url, socket) do\n    # Runs after mount and on live patch\n    {:noreply, apply_action(socket, socket.assigns.live_action, params)}\n  end\n\n  @impl true\n  def handle_event(\"delete\", %{\"id\" => id}, socket) do\n    user = Accounts.get_user!(id)\n    {:ok, _} = Accounts.delete_user(user)\n\n    {:noreply, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def render(assigns) do\n    ~H\"\"\"\n    <div>\n      <.table rows={@users} id=\"users\">\n        <:col :let={user} label=\"Name\"><%= user.name %></:col>\n        <:col :let={user} label=\"Email\"><%= user.email %></:col>\n        <:action :let={user}>\n          <.button phx-click=\"delete\" phx-value-id={user.id}>Delete</.button>\n        </:action>\n      </.table>\n    </div>\n    \"\"\"\n  end\n\n  defp list_users do\n    Accounts.list_users()\n  end\nend\n```\n\n### LiveView Best Practices\n\n- Use `mount/3` for initial data loading\n- Handle route changes in `handle_params/3`\n- Keep renders fast - compute in event handlers, not render\n- Use `assign_new/3` for expensive computations\n- Prefer LiveView over JavaScript for interactive UIs\n- Use `phx-debounce` and `phx-throttle` for frequent events\n\n### Function Components\n\nCreate reusable components:\n\n```elixir\ndefmodule MyAppWeb.Components.UserCard do\n  use Phoenix.Component\n\n  attr :user, :map, required: true\n  attr :class, :string, default: \"\"\n\n  def user_card(assigns) do\n    ~H\"\"\"\n    <div class={\"card \" <> @class}>\n      <h3><%= @user.name %></h3>\n      <p><%= @user.email %></p>\n    </div>\n    \"\"\"\n  end\nend\n```\n\nUse with `<.user_card user={@current_user} />` in templates.\n\n### Form Handling\n\nUse changesets for validation:\n\n```elixir\n@impl true\ndef mount(_params, _session, socket) do\n  changeset = Accounts.change_user(%User{})\n  {:ok, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"validate\", %{\"user\" => user_params}, socket) do\n  changeset =\n    %User{}\n    |> Accounts.change_user(user_params)\n    |> Map.put(:action, :validate)\n\n  {:noreply, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"save\", %{\"user\" => user_params}, socket) do\n  case Accounts.create_user(user_params) do\n    {:ok, user} ->\n      {:noreply,\n       socket\n       |> put_flash(:info, \"User created successfully\")\n       |> push_navigate(to: ~p\"/users/#{user}\")}\n\n    {:error, %Ecto.Changeset{} = changeset} ->\n      {:noreply, assign(socket, form: to_form(changeset))}\n  end\nend\n\ndef render(assigns) do\n  ~H\"\"\"\n  <.form for={@form} phx-change=\"validate\" phx-submit=\"save\">\n    <.input field={@form[:name]} label=\"Name\" />\n    <.input field={@form[:email]} label=\"Email\" type=\"email\" />\n    <.button>Save</.button>\n  </.form>\n  \"\"\"\nend\n```\n\n## Routing\n\n### Route Organization\n\nStructure routes logically:\n\n```elixir\ndefmodule MyAppWeb.Router do\n  use MyAppWeb, :router\n\n  pipeline :browser do\n    plug :accepts, [\"html\"]\n    plug :fetch_session\n    plug :fetch_live_flash\n    plug :put_root_layout, html: {MyAppWeb.Layouts, :root}\n    plug :protect_from_forgery\n    plug :put_secure_browser_headers\n  end\n\n  pipeline :api do\n    plug :accepts, [\"json\"]\n  end\n\n  scope \"/\", MyAppWeb do\n    pipe_through :browser\n\n    live \"/\", HomeLive, :index\n    live \"/users\", UserLive.Index, :index\n    live \"/users/new\", UserLive.Index, :new\n    live \"/users/:id\", UserLive.Show, :show\n  end\n\n  scope \"/api\", MyAppWeb do\n    pipe_through :api\n\n    resources \"/users\", UserController, except: [:new, :edit]\n  end\nend\n```\n\n### LiveView Routes\n\nUse live actions for modal/overlay states:\n\n```elixir\nlive \"/users\", UserLive.Index, :index\nlive \"/users/new\", UserLive.Index, :new\nlive \"/users/:id/edit\", UserLive.Index, :edit\n```\n\nThen handle in `handle_params/3`:\n\n```elixir\ndefp apply_action(socket, :edit, %{\"id\" => id}) do\n  socket\n  |> assign(:page_title, \"Edit User\")\n  |> assign(:user, Accounts.get_user!(id))\nend\n\ndefp apply_action(socket, :new, _params) do\n  socket\n  |> assign(:page_title, \"New User\")\n  |> assign(:user, %User{})\nend\n\ndefp apply_action(socket, :index, _params) do\n  socket\n  |> assign(:page_title, \"Listing Users\")\n  |> assign(:user, nil)\nend\n```\n\n## Channels and PubSub\n\n### Phoenix Channels\n\nFor custom real-time protocols:\n\n```elixir\ndefmodule MyAppWeb.RoomChannel do\n  use MyAppWeb, :channel\n\n  @impl true\n  def join(\"room:\" <> room_id, _payload, socket) do\n    if authorized?(socket, room_id) do\n      {:ok, assign(socket, :room_id, room_id)}\n    else\n      {:error, %{reason: \"unauthorized\"}}\n    end\n  end\n\n  @impl true\n  def handle_in(\"new_msg\", %{\"body\" => body}, socket) do\n    broadcast!(socket, \"new_msg\", %{body: body, user: socket.assigns.user})\n    {:noreply, socket}\n  end\nend\n```\n\n### Phoenix PubSub\n\nFor LiveView updates and process communication:\n\n```elixir\n# Subscribe in mount\ndef mount(_params, _session, socket) do\n  if connected?(socket) do\n    Phoenix.PubSub.subscribe(MyApp.PubSub, \"users\")\n  end\n\n  {:ok, assign(socket, :users, list_users())}\nend\n\n# Handle broadcasts\ndef handle_info({:user_created, user}, socket) do\n  {:noreply, update(socket, :users, fn users -> [user | users] end)}\nend\n\n# Broadcast from context\ndef create_user(attrs) do\n  with {:ok, user} <- do_create_user(attrs) do\n    Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n    {:ok, user}\n  end\nend\n```\n\n## Testing Phoenix Applications\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase, async: true\n\n  test \"GET /users\", %{conn: conn} do\n    conn = get(conn, ~p\"/users\")\n    assert html_response(conn, 200) =~ \"Listing Users\"\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n\n  import Phoenix.LiveViewTest\n\n  test \"displays users\", %{conn: conn} do\n    user = insert(:user)\n\n    {:ok, view, html} = live(conn, ~p\"/users\")\n\n    assert html =~ user.name\n    assert has_element?(view, \"#user-#{user.id}\")\n  end\n\n  test \"creates user\", %{conn: conn} do\n    {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n    assert view\n           |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n           |> render_submit()\n\n    assert_patch(view, ~p\"/users\")\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    {:ok, _, socket} = subscribe_and_join(socket, \"room:lobby\", %{})\n\n    broadcast_from!(socket, \"new_msg\", %{body: \"test\"})\n    assert_broadcast \"new_msg\", %{body: \"test\"}\n  end\nend\n```\n\n## Common Patterns\n\n### Loading Associations\n\nPreload associations efficiently:\n\n```elixir\ndef list_posts do\n  Post\n  |> preload([:author, comments: :author])\n  |> Repo.all()\nend\n```\n\n### Pagination\n\nUse Scrivener or custom pagination:\n\n```elixir\ndef list_users(page \\\\ 1) do\n  User\n  |> order_by(desc: :inserted_at)\n  |> Repo.paginate(page: page, page_size: 20)\nend\n```\n\n### File Uploads\n\nHandle uploads in LiveView:\n\n```elixir\ndef mount(_params, _session, socket) do\n  {:ok,\n   socket\n   |> assign(:uploaded_files, [])\n   |> allow_upload(:avatar, accept: ~w(.jpg .jpeg .png), max_entries: 1)}\nend\n\ndef handle_event(\"save\", _params, socket) do\n  uploaded_files =\n    consume_uploaded_entries(socket, :avatar, fn %{path: path}, _entry ->\n      dest = Path.join(\"priv/static/uploads\", Path.basename(path))\n      File.cp!(path, dest)\n      {:ok, \"/uploads/\" <> Path.basename(dest)}\n    end)\n\n  {:noreply, update(socket, :uploaded_files, &(&1 ++ uploaded_files))}\nend\n```\n\n## Performance Optimization\n\n### Database Query Optimization\n\n- Use `preload/2` to avoid N+1 queries\n- Add database indexes for frequently queried fields\n- Use `select/3` to load only needed fields\n- Consider using `Repo.stream/2` for large datasets\n\n### LiveView Performance\n\n- Move expensive computations to `handle_event` or background jobs\n- Use `assign_new/3` for computed values\n- Implement `handle_continue/2` for async operations after mount\n- Use temporary assigns for large lists: `assign(socket, :items, temporary: true)`\n\n### Caching\n\nUse Cachex or ETS for caching:\n\n```elixir\ndef get_user!(id) do\n  Cachex.fetch(:users, id, fn ->\n    {:commit, Repo.get!(User, id)}\n  end)\nend\n```\n\n## Security Best Practices\n\n- Always validate and sanitize user input through changesets\n- Use CSRF protection (enabled by default)\n- Implement rate limiting for APIs\n- Use `put_secure_browser_headers` plug\n- Validate file uploads (type, size, content)\n- Use prepared statements (Ecto does this automatically)\n- Implement proper authentication and authorization\n\n## Key Principles\n\n- **Context boundaries**: Keep business logic in contexts, not controllers/LiveViews\n- **LiveView first**: Prefer LiveView over JavaScript for interactive features\n- **Changesets for validation**: Always validate through Ecto changesets\n- **Pub/Sub for communication**: Use Phoenix.PubSub for cross-process updates\n- **Test at boundaries**: Test contexts, controllers, and LiveViews separately\n- **Follow conventions**: Use Phoenix generators and follow established patterns"
              },
              {
                "name": "elixir-testing",
                "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices",
                "path": "elixir/skills/testing/SKILL.md",
                "frontmatter": {
                  "name": "elixir-testing",
                  "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices"
                },
                "content": "# Elixir Testing with ExUnit\n\nThis skill activates when writing, organizing, or improving tests for Elixir applications using ExUnit and related testing tools.\n\n## When to Use This Skill\n\nActivate when:\n- Writing unit, integration, or property-based tests\n- Organizing test suites and test files\n- Setting up test fixtures and factories\n- Mocking external dependencies\n- Testing concurrent or asynchronous code\n- Improving test coverage or quality\n- Troubleshooting failing tests\n\n## ExUnit Basics\n\n### Test Module Structure\n\n```elixir\ndefmodule MyApp.MathTest do\n  use ExUnit.Case, async: true\n\n  describe \"add/2\" do\n    test \"adds two positive numbers\" do\n      assert Math.add(2, 3) == 5\n    end\n\n    test \"adds negative numbers\" do\n      assert Math.add(-1, -1) == -2\n    end\n\n    test \"adds zero\" do\n      assert Math.add(5, 0) == 5\n    end\n  end\n\n  describe \"divide/2\" do\n    test \"divides two numbers\" do\n      assert Math.divide(10, 2) == 5.0\n    end\n\n    test \"returns error for division by zero\" do\n      assert Math.divide(10, 0) == {:error, :division_by_zero}\n    end\n  end\nend\n```\n\n### Assertions\n\nCommon assertion patterns:\n\n```elixir\n# Equality\nassert actual == expected\nrefute actual == unexpected\n\n# Boolean\nassert is_binary(value)\nassert is_integer(value)\nrefute is_nil(value)\n\n# Pattern matching\nassert {:ok, result} = function_call()\nassert %User{name: \"Alice\"} = user\n\n# Exceptions\nassert_raise ArgumentError, fn ->\n  String.to_integer(\"not a number\")\nend\n\nassert_raise ArgumentError, \"invalid argument\", fn ->\n  dangerous_function()\nend\n\n# Messages\nsend(self(), :hello)\nassert_received :hello\n\nassert_receive :message, 1000  # With timeout\n\nrefute_received :unwanted\nrefute_receive :unwanted, 100\n```\n\n### Test Organization\n\n#### Using describe blocks\n\nGroup related tests:\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  describe \"create_user/1\" do\n    test \"creates user with valid attributes\" do\n      # ...\n    end\n\n    test \"returns error with invalid email\" do\n      # ...\n    end\n  end\n\n  describe \"update_user/2\" do\n    test \"updates user attributes\" do\n      # ...\n    end\n  end\nend\n```\n\n#### Test tags\n\nCategorize and filter tests:\n\n```elixir\n@moduletag :integration\n\n@tag :slow\ntest \"expensive operation\" do\n  # ...\nend\n\n@tag :external\ntest \"calls external API\" do\n  # ...\nend\n\n# Run only tagged tests\n# mix test --only slow\n# mix test --exclude external\n```\n\n### Setup and Teardown\n\n#### Test context\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  setup do\n    user = %User{name: \"Alice\", email: \"alice@example.com\"}\n    {:ok, user: user}\n  end\n\n  test \"user has name\", %{user: user} do\n    assert user.name == \"Alice\"\n  end\n\n  test \"user has email\", %{user: user} do\n    assert user.email == \"alice@example.com\"\n  end\nend\n```\n\n#### Setup with describe\n\n```elixir\ndescribe \"authenticated user\" do\n  setup do\n    user = insert(:user)\n    token = generate_token(user)\n    {:ok, user: user, token: token}\n  end\n\n  test \"can access protected resource\", %{token: token} do\n    # ...\n  end\nend\n```\n\n#### Module setup\n\n```elixir\nsetup_all do\n  # Runs once before all tests in module\n  start_supervised!(MyApp.Cache)\n  :ok\nend\n\nsetup do\n  # Runs before each test\n  :ok = Ecto.Adapters.SQL.Sandbox.checkout(MyApp.Repo)\nend\n```\n\n#### Conditional setup\n\n```elixir\nsetup context do\n  if context[:integration] do\n    start_external_service()\n    on_exit(fn -> stop_external_service() end)\n  end\n\n  :ok\nend\n\n@tag :integration\ntest \"integration test\" do\n  # ...\nend\n```\n\n## Database Testing\n\n### Sandbox Mode\n\nConfigure for concurrent tests:\n\n```elixir\n# config/test.exs\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox\n\n# test/test_helper.exs\nEcto.Adapters.SQL.Sandbox.mode(MyApp.Repo, :manual)\n\n# test/support/data_case.ex\ndefmodule MyApp.DataCase do\n  use ExUnit.CaseTemplate\n\n  using do\n    quote do\n      alias MyApp.Repo\n      import Ecto\n      import Ecto.Changeset\n      import Ecto.Query\n      import MyApp.DataCase\n    end\n  end\n\n  setup tags do\n    pid = Ecto.Adapters.SQL.Sandbox.start_owner!(MyApp.Repo, shared: not tags[:async])\n    on_exit(fn -> Ecto.Adapters.SQL.Sandbox.stop_owner(pid) end)\n    :ok\n  end\nend\n```\n\n### Test Factories\n\nUse ExMachina for test data:\n\n```elixir\n# test/support/factory.ex\ndefmodule MyApp.Factory do\n  use ExMachina.Ecto, repo: MyApp.Repo\n\n  def user_factory do\n    %MyApp.User{\n      name: \"Jane Smith\",\n      email: sequence(:email, &\"email-#{&1}@example.com\"),\n      age: 25\n    }\n  end\n\n  def admin_factory do\n    struct!(\n      user_factory(),\n      %{role: :admin}\n    )\n  end\n\n  def post_factory do\n    %MyApp.Post{\n      title: \"A title\",\n      body: \"Some content\",\n      author: build(:user)\n    }\n  end\nend\n\n# In tests\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n  import MyApp.Factory\n\n  test \"creates user\" do\n    user = insert(:user)\n    assert user.id\n  end\n\n  test \"creates admin\" do\n    admin = insert(:admin)\n    assert admin.role == :admin\n  end\n\n  test \"builds without inserting\" do\n    user = build(:user, name: \"Custom Name\")\n    assert user.name == \"Custom Name\"\n    refute user.id\n  end\nend\n```\n\n### Testing Changesets\n\n```elixir\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n\n  describe \"changeset/2\" do\n    test \"valid changeset with valid attributes\" do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert changeset.valid?\n    end\n\n    test \"invalid without email\" do\n      attrs = %{name: \"Alice\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      refute changeset.valid?\n      assert \"can't be blank\" in errors_on(changeset).email\n    end\n\n    test \"invalid with short password\" do\n      attrs = %{email: \"test@example.com\", password: \"123\"}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert \"should be at least 8 character(s)\" in errors_on(changeset).password\n    end\n  end\nend\n\n# Helper function\ndef errors_on(changeset) do\n  Ecto.Changeset.traverse_errors(changeset, fn {message, opts} ->\n    Regex.replace(~r\"%{(\\w+)}\", message, fn _, key ->\n      opts |> Keyword.get(String.to_existing_atom(key), key) |> to_string()\n    end)\n  end)\nend\n```\n\n## Phoenix Testing\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase\n  import MyApp.Factory\n\n  describe \"index\" do\n    test \"lists all users\", %{conn: conn} do\n      user = insert(:user)\n\n      conn = get(conn, ~p\"/users\")\n\n      assert html_response(conn, 200) =~ \"Listing Users\"\n      assert html_response(conn, 200) =~ user.name\n    end\n  end\n\n  describe \"create\" do\n    test \"creates user with valid data\", %{conn: conn} do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\"}\n\n      conn = post(conn, ~p\"/users\", user: attrs)\n\n      assert redirected_to(conn) =~ ~p\"/users\"\n\n      conn = get(conn, redirected_to(conn))\n      assert html_response(conn, 200) =~ \"Alice\"\n    end\n\n    test \"renders errors with invalid data\", %{conn: conn} do\n      conn = post(conn, ~p\"/users\", user: %{})\n\n      assert html_response(conn, 200) =~ \"New User\"\n    end\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n  import Phoenix.LiveViewTest\n  import MyApp.Factory\n\n  describe \"Index\" do\n    test \"displays users\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, html} = live(conn, ~p\"/users\")\n\n      assert html =~ \"Listing Users\"\n      assert has_element?(view, \"#user-#{user.id}\")\n      assert render(view) =~ user.name\n    end\n\n    test \"creates new user\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users\")\n\n      html = render(view)\n      assert html =~ \"Alice\"\n    end\n\n    test \"updates user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users/#{user.id}/edit\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Updated Name\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users/#{user.id}\")\n\n      html = render(view)\n      assert html =~ \"Updated Name\"\n    end\n\n    test \"deletes user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      assert view\n             |> element(\"#user-#{user.id} a\", \"Delete\")\n             |> render_click()\n\n      refute has_element?(view, \"#user-#{user.id}\")\n    end\n  end\n\n  describe \"form validation\" do\n    test \"validates on change\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      result =\n        view\n        |> form(\"#user-form\", user: %{email: \"invalid\"})\n        |> render_change()\n\n      assert result =~ \"must have the @ sign\"\n    end\n  end\n\n  describe \"real-time updates\" do\n    test \"receives updates from PubSub\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      user = insert(:user)\n\n      # Trigger PubSub event\n      Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n\n      assert render(view) =~ user.name\n    end\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  setup do\n    {:ok, _, socket} =\n      MyAppWeb.UserSocket\n      |> socket(\"user_id\", %{user_id: 42})\n      |> subscribe_and_join(MyAppWeb.RoomChannel, \"room:lobby\")\n\n    %{socket: socket}\n  end\n\n  test \"ping replies with pong\", %{socket: socket} do\n    ref = push(socket, \"ping\", %{\"hello\" => \"there\"})\n    assert_reply ref, :ok, %{\"hello\" => \"there\"}\n  end\n\n  test \"shout broadcasts to room:lobby\", %{socket: socket} do\n    push(socket, \"shout\", %{\"hello\" => \"all\"})\n    assert_broadcast \"shout\", %{\"hello\" => \"all\"}\n  end\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    broadcast_from!(socket, \"broadcast\", %{\"some\" => \"data\"})\n    assert_push \"broadcast\", %{\"some\" => \"data\"}\n  end\nend\n```\n\n## Mocking and Stubbing\n\n### Using Mox\n\nDefine behaviors and mocks:\n\n```elixir\n# Define behaviour\ndefmodule MyApp.HTTPClient do\n  @callback get(String.t()) :: {:ok, map()} | {:error, term()}\nend\n\n# In config/test.exs\nconfig :my_app, :http_client, MyApp.HTTPClientMock\n\n# In test/test_helper.exs\nMox.defmock(MyApp.HTTPClientMock, for: MyApp.HTTPClient)\n\n# In application code\ndefmodule MyApp.UserFetcher do\n  @http_client Application.compile_env(:my_app, :http_client)\n\n  def fetch_user(id) do\n    @http_client.get(\"/users/#{id}\")\n  end\nend\n\n# In tests\ndefmodule MyApp.UserFetcherTest do\n  use ExUnit.Case, async: true\n  import Mox\n\n  setup :verify_on_exit!\n\n  test \"fetches user successfully\" do\n    expect(MyApp.HTTPClientMock, :get, fn \"/users/1\" ->\n      {:ok, %{\"name\" => \"Alice\"}}\n    end)\n\n    assert {:ok, %{\"name\" => \"Alice\"}} = MyApp.UserFetcher.fetch_user(1)\n  end\n\n  test \"handles error\" do\n    expect(MyApp.HTTPClientMock, :get, fn _ ->\n      {:error, :network_error}\n    end)\n\n    assert {:error, :network_error} = MyApp.UserFetcher.fetch_user(1)\n  end\nend\n```\n\n### Stubbing Multiple Calls\n\n```elixir\ntest \"calls API multiple times\" do\n  MyApp.HTTPClientMock\n  |> expect(:get, 3, fn url ->\n    {:ok, %{\"url\" => url}}\n  end)\n\n  MyApp.batch_fetch([1, 2, 3])\nend\n```\n\n### Global Stubs\n\n```elixir\nsetup do\n  stub(MyApp.HTTPClientMock, :get, fn _ -> {:ok, %{}} end)\n  :ok\nend\n\ntest \"can override stub\" do\n  expect(MyApp.HTTPClientMock, :get, fn _ ->\n    {:error, :timeout}\n  end)\n\n  # ...\nend\n```\n\n## Property-Based Testing\n\nUse StreamData for property-based tests:\n\n```elixir\ndefmodule MyApp.MathPropertyTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  property \"addition is commutative\" do\n    check all a <- integer(),\n              b <- integer() do\n      assert Math.add(a, b) == Math.add(b, a)\n    end\n  end\n\n  property \"list reversal is involutive\" do\n    check all list <- list_of(integer()) do\n      assert Enum.reverse(Enum.reverse(list)) == list\n    end\n  end\n\n  property \"concatenation length\" do\n    check all list1 <- list_of(term()),\n              list2 <- list_of(term()) do\n      concatenated = list1 ++ list2\n      assert length(concatenated) == length(list1) + length(list2)\n    end\n  end\nend\n```\n\n### Custom Generators\n\n```elixir\ndefmodule MyApp.Generators do\n  use ExUnitProperties\n\n  def email do\n    gen all username <- string(:alphanumeric, min_length: 1),\n            domain <- string(:alphanumeric, min_length: 1),\n            tld <- member_of([\"com\", \"org\", \"net\"]) do\n      \"#{username}@#{domain}.#{tld}\"\n    end\n  end\n\n  def user do\n    gen all name <- string(:alphanumeric, min_length: 1),\n            email <- email(),\n            age <- integer(18..100) do\n      %User{name: name, email: email, age: age}\n    end\n  end\nend\n\n# Use in tests\nproperty \"validates email format\" do\n  check all email <- MyApp.Generators.email() do\n    assert User.valid_email?(email)\n  end\nend\n```\n\n## Testing Async and Concurrent Code\n\n### Testing Processes\n\n```elixir\ntest \"GenServer handles messages\" do\n  {:ok, pid} = MyApp.Worker.start_link()\n\n  MyApp.Worker.process(pid, :work)\n\n  assert_receive {:done, :work}, 1000\nend\n```\n\n### Testing Tasks\n\n```elixir\ntest \"async task completes\" do\n  parent = self()\n\n  Task.start(fn ->\n    result = expensive_computation()\n    send(parent, {:result, result})\n  end)\n\n  assert_receive {:result, value}, 5000\n  assert value == expected\nend\n```\n\n### Testing Race Conditions\n\n```elixir\ntest \"concurrent updates are handled correctly\" do\n  {:ok, counter} = Counter.start_link(0)\n\n  tasks = for _ <- 1..100 do\n    Task.async(fn -> Counter.increment(counter) end)\n  end\n\n  Task.await_many(tasks)\n\n  assert Counter.get(counter) == 100\nend\n```\n\n## Test Coverage\n\n### Generate Coverage Reports\n\n```bash\nmix test --cover\n\n# Detailed coverage\nMIX_ENV=test mix coveralls\nMIX_ENV=test mix coveralls.html\n```\n\n### Coverage Configuration\n\n```elixir\n# mix.exs\ndef project do\n  [\n    # ...\n    test_coverage: [tool: ExCoveralls],\n    preferred_cli_env: [\n      coveralls: :test,\n      \"coveralls.detail\": :test,\n      \"coveralls.html\": :test\n    ]\n  ]\nend\n```\n\n## Best Practices\n\n### Test Organization\n\n- One test file per module: `lib/my_app/user.ex` → `test/my_app/user_test.exs`\n- Use `describe` blocks to group related tests\n- Use `test/support` for shared test helpers\n- Keep tests focused on one behavior per test\n\n### Naming\n\n- Use descriptive test names that explain what is being tested\n- Start with the action being tested\n- Include the expected outcome\n\n```elixir\n# Good\ntest \"create_user/1 returns error with invalid email\"\ntest \"add/2 returns sum of two positive integers\"\n\n# Avoid\ntest \"it works\"\ntest \"test1\"\n```\n\n### Setup\n\n- Use `setup` for common test data\n- Keep setup focused - don't create unnecessary data\n- Use context to pass data between setup and tests\n- Use factories for complex data structures\n\n### Assertions\n\n- Prefer pattern matching over multiple assertions\n- Use specific assertions (`assert_receive` vs `assert Process.info(...)`)\n- Test one logical assertion per test when possible\n\n### Async Tests\n\n```elixir\n# Mark tests as async when they don't share state\nuse ExUnit.Case, async: true\n\n# Don't use async when tests:\n# - Modify global state\n# - Use database without sandbox\n# - Access shared resources\n```\n\n### Test Data\n\n- Use factories (ExMachina) for consistent test data\n- Avoid hardcoded IDs - use factories and references\n- Keep test data minimal - only what's needed for the test\n- Use descriptive data that makes tests readable\n\n### External Dependencies\n\n- Mock external APIs and services\n- Use Mox for behavior-based mocking\n- Stub at the boundary - don't mock internal modules\n- Tag tests that require external services\n\n## Debugging Tests\n\n### Running Specific Tests\n\n```bash\n# Run single test file\nmix test test/my_app/user_test.exs\n\n# Run specific line\nmix test test/my_app/user_test.exs:42\n\n# Run tests matching pattern\nmix test --only integration\n\n# Run tests excluding pattern\nmix test --exclude slow\n```\n\n### Test Output\n\n```elixir\n# Add IEx.pry breakpoint\nimport IEx\ntest \"debugging\" do\n  user = build(:user)\n  IEx.pry()  # Stops here\n  # ...\nend\n\n# Print during tests\nIO.inspect(value, label: \"DEBUG\")\n```\n\n### Failed Test Debugging\n\n```bash\n# Re-run only failed tests\nmix test --failed\n\n# Show detailed error traces\nmix test --trace\n\n# Run tests one at a time\nmix test --max-cases 1\n```\n\n## Key Principles\n\n- **Test behavior, not implementation**: Test what the code does, not how it does it\n- **Keep tests fast**: Use async tests, avoid unnecessary setup, mock slow dependencies\n- **Make tests readable**: Use descriptive names, clear assertions, minimal setup\n- **Test at the right level**: Unit tests for logic, integration tests for interactions\n- **Use factories**: Consistent, reusable test data with ExMachina\n- **Mock at boundaries**: Mock external services, not internal modules\n- **Property-based testing**: Use StreamData for algorithmic code\n- **Embrace the database**: Use Ecto sandbox for fast, isolated database tests"
              },
              {
                "name": "act",
                "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution",
                "path": "github/skills/act/SKILL.md",
                "frontmatter": {
                  "name": "act",
                  "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution"
                },
                "content": "# act - Local GitHub Actions Testing\n\nActivate when testing GitHub Actions workflows locally, debugging workflow issues, or developing actions without committing to remote repositories. This skill covers act installation, configuration, and usage patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Testing workflow changes before committing\n- Debugging workflow failures locally\n- Developing new workflows iteratively\n- Validating workflow syntax and logic\n- Testing actions with different events\n- Running workflows without GitHub runners\n- Troubleshooting act-specific issues\n\n## Installation\n\n### Using mise (Recommended for this project)\n\nThe act tool is configured in the github plugin's mise.toml:\n\n```bash\n# Install act via mise\nmise install act\n\n# Verify installation\nact --version\n```\n\n### Alternative Installation Methods\n\n**macOS (Homebrew):**\n```bash\nbrew install act\n```\n\n**Linux (via script):**\n```bash\ncurl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\n```\n\n**From source:**\n```bash\ngit clone https://github.com/nektos/act.git\ncd act\nmake install\n```\n\n**Windows (Chocolatey):**\n```powershell\nchoco install act-cli\n```\n\n## How act Works\n\nact reads workflow files from `.github/workflows/` and:\n1. Determines which actions and jobs to execute\n2. Pulls or builds required Docker images\n3. Creates containers matching GitHub's runner environment\n4. Executes steps in isolated containers\n5. Provides output matching GitHub Actions format\n\n**Key Concept:** act uses Docker to simulate GitHub's runner environment locally.\n\n## Prerequisites\n\n- **Docker**: act requires Docker to run workflows\n- **Workflow files**: Valid `.github/workflows/*.yml` files in repository\n\nVerify Docker is running:\n```bash\ndocker ps\n```\n\n## Basic Usage\n\n### List Available Workflows\n\n```bash\n# List all workflows\nact -l\n\n# Output:\n# Stage  Job ID  Job name  Workflow name  Workflow file  Events\n# 0      build   build     CI             ci.yml         push,pull_request\n# 0      test    test      CI             ci.yml         push,pull_request\n```\n\n### Run Default Event (push)\n\n```bash\n# Run all jobs triggered by push event\nact\n\n# Run specific job\nact -j build\n\n# Run specific workflow\nact -W .github/workflows/ci.yml\n```\n\n### Run Specific Events\n\n```bash\n# Pull request event\nact pull_request\n\n# Manual workflow dispatch\nact workflow_dispatch\n\n# Push to specific branch\nact push -e .github/workflows/push-event.json\n\n# Schedule event\nact schedule\n```\n\n### Dry Run\n\n```bash\n# Show what would run without executing\nact -n\n\n# Show with full details\nact -n -v\n```\n\n## Event Payloads\n\n### Custom Event Data\n\nCreate event JSON file:\n\n```json\n{\n  \"pull_request\": {\n    \"number\": 123,\n    \"head\": {\n      \"ref\": \"feature-branch\"\n    },\n    \"base\": {\n      \"ref\": \"main\"\n    }\n  }\n}\n```\n\nUse with act:\n```bash\nact pull_request -e event.json\n```\n\n### workflow_dispatch Inputs\n\n```json\n{\n  \"inputs\": {\n    \"environment\": \"staging\",\n    \"debug\": true\n  }\n}\n```\n\n```bash\nact workflow_dispatch -e inputs.json\n```\n\n## Secrets Management\n\n### Via Command Line\n\n```bash\n# Single secret\nact -s GITHUB_TOKEN=ghp_xxxxx\n\n# Multiple secrets\nact -s API_KEY=key123 -s DB_PASSWORD=pass456\n```\n\n### Via .secrets File\n\nCreate `.secrets` file (add to .gitignore):\n```\nGITHUB_TOKEN=ghp_xxxxx\nAPI_KEY=key123\nDB_PASSWORD=pass456\n```\n\nRun with secrets file:\n```bash\nact --secret-file .secrets\n```\n\n### Environment Variables\n\n```bash\n# Use existing env var\nact -s GITHUB_TOKEN\n\n# Set from command\nexport MY_SECRET=value\nact -s MY_SECRET\n```\n\n## Configuration\n\n### .actrc File\n\nCreate `.actrc` in repository root or home directory:\n\n```\n# Use specific platform\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Default secrets file\n--secret-file .secrets\n\n# Default environment\n--env-file .env\n\n# Container architecture\n--container-architecture linux/amd64\n\n# Verbose output\n-v\n```\n\n### Custom Runner Images\n\n```bash\n# Use custom image for platform\nact -P ubuntu-latest=my-custom-image:latest\n\n# Use medium size images (recommended)\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Use micro images (faster, less compatible)\nact -P ubuntu-latest=node:16-buster-slim\n```\n\n### Recommended Images\n\nact supports different image sizes:\n\n**Medium images (recommended):**\n- Better compatibility with GitHub Actions\n- More pre-installed tools\n- Slower startup but fewer failures\n\n```bash\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n-P ubuntu-22.04=catthehacker/ubuntu:act-22.04\n```\n\n**Micro images:**\n- Faster startup\n- Minimal pre-installed tools\n- May require additional setup\n\n## Environment Variables\n\n### Via .env File\n\nCreate `.env` file:\n```\nNODE_ENV=test\nAPI_URL=http://localhost:3000\nLOG_LEVEL=debug\n```\n\nUse with act:\n```bash\nact --env-file .env\n```\n\n### Via Command Line\n\n```bash\nact --env NODE_ENV=test --env API_URL=http://localhost:3000\n```\n\n## Advanced Usage\n\n### Bind Workspace\n\nMount local directory into container:\n```bash\nact --bind\n```\n\n### Reuse Containers\n\nKeep containers between runs for faster execution:\n```bash\nact --reuse\n```\n\n### Specific Platforms\n\n```bash\n# Run on specific platform\nact -P ubuntu-latest=ubuntu:latest\n\n# Multiple platforms\nact -P ubuntu-latest=ubuntu:latest \\\n    -P windows-latest=windows:latest\n```\n\n### Container Architecture\n\n```bash\n# Specify architecture (useful for M1/M2 Macs)\nact --container-architecture linux/amd64\n```\n\n### Network Configuration\n\n```bash\n# Use host network\nact --container-daemon-socket -\n\n# Custom network\nact --network my-network\n```\n\n### Artifact Server\n\n```bash\n# Enable artifact server on specific port\nact --artifact-server-path /tmp/artifacts \\\n    --artifact-server-port 34567\n```\n\n## Debugging\n\n### Verbose Output\n\n```bash\n# Verbose logging\nact -v\n\n# Very verbose (debug level)\nact -vv\n```\n\n### Watch Mode\n\n```bash\n# Watch for file changes and re-run\nact --watch\n```\n\n### Interactive Shell\n\n```bash\n# Drop into shell on failure\nact --shell bash\n```\n\n### Container Inspection\n\n```bash\n# List act containers\ndocker ps -a | grep act\n\n# Inspect specific container\ndocker inspect <container-id>\n\n# View logs\ndocker logs <container-id>\n```\n\n## Limitations and Differences\n\n### Not Supported by act\n\n- Some GitHub-hosted runner features\n- GitHub Apps and installations\n- OIDC token generation\n- Some GitHub API interactions\n- Certain cache implementations\n- Job summaries and annotations (limited)\n\n### Workarounds\n\n**Missing tools:**\n```yaml\nsteps:\n  - name: Install missing tool\n    run: |\n      if ! command -v tool &> /dev/null; then\n        apt-get update && apt-get install -y tool\n      fi\n```\n\n**GitHub API calls:**\n```yaml\n# Use GITHUB_TOKEN from secrets\n- env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n  run: gh api repos/${{ github.repository }}/issues\n```\n\n## Common Patterns\n\n### Testing Pull Request Workflow\n\n```bash\n# Create PR event payload\ncat > pr-event.json << EOF\n{\n  \"pull_request\": {\n    \"number\": 1,\n    \"head\": { \"ref\": \"feature\" },\n    \"base\": { \"ref\": \"main\" }\n  }\n}\nEOF\n\n# Run PR workflow\nact pull_request -e pr-event.json -j test\n```\n\n### CI/CD Pipeline Testing\n\n```bash\n# Test entire CI pipeline\nact push\n\n# Test specific stages\nact push -j build\nact push -j test\nact push -j deploy --secret-file .secrets\n```\n\n### Matrix Testing\n\n```bash\n# Run matrix strategy locally\nact -j test\n\n# Test specific matrix combination (modify workflow temporarily)\nact -j test --matrix node-version:20\n```\n\n### Workflow Development Cycle\n\n```bash\n# 1. List jobs\nact -l\n\n# 2. Dry run\nact -n -j build\n\n# 3. Run with verbose output\nact -v -j build\n\n# 4. Iterate and test\nact --reuse -j build\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n**Error: Cannot connect to Docker daemon**\n```bash\n# Start Docker\n# macOS: Start Docker Desktop\n# Linux:\nsudo systemctl start docker\n```\n\n**Error: Permission denied**\n```bash\n# Add user to docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Image Pull Issues\n\n**Error: Failed to pull image**\n```bash\n# Use specific image version\nact -P ubuntu-latest=ubuntu:22.04\n\n# Or use act's recommended images\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n```\n\n### Workflow Not Found\n\n**Error: No workflows found**\n```bash\n# Verify workflow files exist\nls -la .github/workflows/\n\n# Check workflow syntax\nact -n -v\n```\n\n### Secret Issues\n\n**Error: Secret not found**\n```bash\n# List required secrets from workflow\ngrep -r \"secrets\\.\" .github/workflows/\n\n# Provide via command line\nact -s SECRET_NAME=value\n\n# Or use secrets file\nact --secret-file .secrets\n```\n\n### Action Failures\n\n**Error: Action not found or fails**\n```yaml\n# Ensure action versions are compatible\n# Some actions may not work locally\n\n# Use alternative actions if needed\n# Or skip problematic steps locally:\n- name: Problematic step\n  if: github.event_name != 'act'  # Skip in act\n  uses: some/action@v1\n```\n\n### Platform Differences\n\n**Error: Command not found**\n```bash\n# Use medium-sized images with more tools\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Or install tools in workflow\n- run: apt-get update && apt-get install -y <tool>\n```\n\n## Best Practices\n\n### .actrc Configuration\n\nCreate `.actrc` in repository:\n```\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n--secret-file .secrets\n--container-architecture linux/amd64\n--artifact-server-path /tmp/artifacts\n```\n\n### .gitignore Entries\n\n```gitignore\n# act secrets and config\n.secrets\n.env\n\n# act artifacts\n/tmp/artifacts/\n```\n\n### Conditional Logic for Local Testing\n\n```yaml\nsteps:\n  # Skip in local testing\n  - name: Deploy\n    if: github.event_name != 'act'\n    run: ./deploy.sh\n\n  # Run only in local testing\n  - name: Local setup\n    if: github.event_name == 'act'\n    run: ./local-setup.sh\n```\n\n### Fast Feedback Loop\n\n```bash\n# Use reuse flag for faster iterations\nact --reuse -j test\n\n# Run specific job being developed\nact -j my-new-job -v\n\n# Watch mode for continuous testing\nact --watch -j test\n```\n\n## Integration with Development Workflow\n\n### Pre-commit Testing\n\n```bash\n# Test before committing\nact -j test && git commit -m \"message\"\n\n# Git hook (.git/hooks/pre-commit)\n#!/bin/bash\nact -j test --quiet\n```\n\n### Quick Validation\n\n```bash\n# Validate workflow syntax\nact -n\n\n# Test specific changes\nact -j affected-job\n```\n\n### CI Parity\n\n```bash\n# Use same images as CI\nact -P ubuntu-latest=ubuntu:22.04\n\n# Use same secrets structure\nact --secret-file .secrets\n```\n\n## Scripts and Automation\n\n### Installation Script\n\nThe plugin includes an installation script at `scripts/install-act.sh`:\n\n```bash\n#!/usr/bin/env bash\n# Install act via mise or fallback methods\n\nif command -v mise &> /dev/null; then\n  echo \"Installing act via mise...\"\n  mise install act\nelif [[ \"$OSTYPE\" == \"darwin\"* ]] && command -v brew &> /dev/null; then\n  echo \"Installing act via Homebrew...\"\n  brew install act\nelif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n  echo \"Installing act via install script...\"\n  curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\nelse\n  echo \"Please install act manually: https://github.com/nektos/act\"\n  exit 1\nfi\n\nact --version\n```\n\nRun with:\n```bash\nchmod +x scripts/install-act.sh\n./scripts/install-act.sh\n```\n\n## Anti-Fabrication Requirements\n\n- Execute `act --version` before documenting version numbers\n- Use `act -l` to verify actual workflows before claiming their presence\n- Execute `docker ps` to confirm Docker is running before troubleshooting\n- Run `act -n` to validate workflow syntax before claiming correctness\n- Execute actual `act` commands to verify behavior before documenting output format\n- Use `docker images` to verify available images before recommending specific versions\n- Never claim success rates or performance metrics without actual measurement\n- Execute `act -v` to observe actual error messages before documenting troubleshooting steps\n- Use Read tool to verify workflow files exist before testing them with act\n- Run actual event payloads through act before claiming they work correctly"
              },
              {
                "name": "github-actions",
                "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration",
                "path": "github/skills/actions/SKILL.md",
                "frontmatter": {
                  "name": "github-actions",
                  "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration"
                },
                "content": "# GitHub Actions\n\nActivate when creating, modifying, troubleshooting, or optimizing GitHub Actions components. This skill covers action development, marketplace integration, and best practices.\n\n## When to Use This Skill\n\nActivate when:\n- Creating custom GitHub Actions (JavaScript, Docker, or composite)\n- Publishing actions to GitHub Marketplace\n- Configuring action metadata and inputs/outputs\n- Implementing action security and permissions\n- Troubleshooting action execution\n- Selecting or evaluating marketplace actions\n- Optimizing action performance and reliability\n\n## Action Types\n\n### JavaScript Actions\n\nExecute directly on runners with fast startup and cross-platform compatibility.\n\n**Structure:**\n```\nmy-action/\n├── action.yml        # Metadata and interface\n├── index.js          # Entry point\n├── package.json      # Dependencies\n└── node_modules/     # Bundled dependencies\n```\n\n**Key Requirements:**\n- Use `@actions/core` for inputs/outputs\n- Use `@actions/github` for GitHub API access\n- Bundle all dependencies (use @vercel/ncc)\n- Support Node.js LTS versions\n\n**Example action.yml:**\n```yaml\nname: 'My JavaScript Action'\ndescription: 'Performs custom task'\ninputs:\n  token:\n    description: 'GitHub token'\n    required: true\n  config:\n    description: 'Configuration file path'\n    required: false\n    default: 'config.yml'\noutputs:\n  result:\n    description: 'Action result'\nruns:\n  using: 'node20'\n  main: 'dist/index.js'\n```\n\n### Docker Container Actions\n\nProvide consistent execution environment with all dependencies packaged.\n\n**Structure:**\n```\nmy-action/\n├── action.yml\n├── Dockerfile\n├── entrypoint.sh\n└── src/\n```\n\n**Key Requirements:**\n- Use lightweight base images (Alpine when possible)\n- Set proper file permissions\n- Handle signals gracefully\n- Output to STDOUT/STDERR correctly\n\n**Example Dockerfile:**\n```dockerfile\nFROM alpine:3.18\n\nRUN apk add --no-cache bash curl jq\n\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\n### Composite Actions\n\nCombine multiple steps and actions into reusable units.\n\n**Structure:**\n```yaml\nname: 'Setup Environment'\ndescription: 'Configure development environment'\ninputs:\n  node-version:\n    description: 'Node.js version'\n    required: false\n    default: '20'\nruns:\n  using: 'composite'\n  steps:\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - run: npm ci\n      shell: bash\n    - run: npm run build\n      shell: bash\n```\n\n## Action Metadata (action.yml)\n\n### Required Fields\n\n```yaml\nname: 'Action Name'           # Marketplace display name\ndescription: 'What it does'   # Clear, concise purpose\nruns:                         # Execution configuration\n  using: 'node20'            # or 'docker' or 'composite'\n```\n\n### Optional Fields\n\n```yaml\nauthor: 'Your Name'\nbranding:                    # Marketplace icon/color\n  icon: 'activity'\n  color: 'blue'\ninputs:                      # Define all inputs\n  input-name:\n    description: 'Purpose'\n    required: true\n    default: 'value'\noutputs:                     # Define all outputs\n  output-name:\n    description: 'What it contains'\n```\n\n## Inputs and Outputs\n\n### Reading Inputs\n\n**JavaScript:**\n```javascript\nconst core = require('@actions/core');\nconst token = core.getInput('token', { required: true });\nconst config = core.getInput('config') || 'default.yml';\n```\n\n**Shell:**\n```bash\nTOKEN=\"${{ inputs.token }}\"\nCONFIG=\"${{ inputs.config }}\"\n```\n\n### Setting Outputs\n\n**JavaScript:**\n```javascript\ncore.setOutput('result', 'success');\ncore.setOutput('artifact-url', artifactUrl);\n```\n\n**Shell:**\n```bash\necho \"result=success\" >> $GITHUB_OUTPUT\necho \"artifact-url=$ARTIFACT_URL\" >> $GITHUB_OUTPUT\n```\n\n## GitHub Actions Toolkit\n\nEssential npm packages for JavaScript actions:\n\n### @actions/core\n```javascript\nconst core = require('@actions/core');\n\n// Inputs/Outputs\nconst input = core.getInput('name');\ncore.setOutput('name', value);\n\n// Logging\ncore.info('Information message');\ncore.warning('Warning message');\ncore.error('Error message');\ncore.debug('Debug message');\n\n// Grouping\ncore.startGroup('Group name');\n// ... operations\ncore.endGroup();\n\n// Failure\ncore.setFailed('Action failed: reason');\n\n// Secrets\ncore.setSecret('sensitive-value');  // Masks in logs\n\n// Environment\ncore.exportVariable('VAR_NAME', 'value');\n```\n\n### @actions/github\n```javascript\nconst github = require('@actions/github');\n\n// Context\nconst context = github.context;\nconsole.log(context.repo);        // { owner, repo }\nconsole.log(context.sha);         // Commit SHA\nconsole.log(context.ref);         // Branch/tag ref\nconsole.log(context.actor);       // Triggering user\nconsole.log(context.payload);     // Webhook payload\n\n// Octokit client\nconst token = core.getInput('token');\nconst octokit = github.getOctokit(token);\n\n// API operations\nconst { data: issues } = await octokit.rest.issues.listForRepo({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  state: 'open'\n});\n```\n\n### @actions/exec\n```javascript\nconst exec = require('@actions/exec');\n\n// Execute commands\nawait exec.exec('npm', ['install']);\n\n// Capture output\nlet output = '';\nawait exec.exec('git', ['log', '--oneline'], {\n  listeners: {\n    stdout: (data) => { output += data.toString(); }\n  }\n});\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate and sanitize inputs:\n```javascript\nconst core = require('@actions/core');\n\nfunction validateInput(input) {\n  // Check for command injection\n  if (/[;&|`$()]/.test(input)) {\n    throw new Error('Invalid characters in input');\n  }\n  return input;\n}\n\nconst userInput = core.getInput('user-input');\nconst safeInput = validateInput(userInput);\n```\n\n### Token Permissions\n\nRequest minimal required permissions:\n```yaml\npermissions:\n  contents: read           # Read repository\n  pull-requests: write     # Comment on PRs\n  issues: write           # Create issues\n```\n\n### Secret Handling\n\n```javascript\n// Mask secrets in logs\ncore.setSecret(sensitiveValue);\n\n// Never log tokens\ncore.debug(`Token: ${token}`);  // ❌ WRONG\ncore.debug('Token received');   // ✅ CORRECT\n\n// Secure token usage\nconst octokit = github.getOctokit(token);\n// Token automatically included in requests\n```\n\n### Dependency Security\n\n```bash\n# Audit dependencies\nnpm audit\n\n# Use specific versions\nnpm install @actions/core@1.10.0\n\n# Bundle dependencies\nnpm install -g @vercel/ncc\nncc build index.js -o dist\n```\n\n## Marketplace Publishing\n\n### Prerequisites\n\n- Public repository\n- action.yml in repository root\n- README.md with usage examples\n- LICENSE file\n- Repository topics (optional)\n\n### Publishing Process\n\n1. Create release with semantic version tag:\n```bash\ngit tag -a v1.0.0 -m \"Release v1.0.0\"\ngit push origin v1.0.0\n```\n\n2. Create GitHub Release from tag\n3. Check \"Publish this Action to GitHub Marketplace\"\n4. Select primary category\n5. Verify branding icon/color\n\n### Version Management\n\nUse semantic versioning with major version tags:\n```bash\n# Release v1.2.3\ngit tag -a v1.2.3 -m \"Release v1.2.3\"\ngit tag -fa v1 -m \"Update v1 to v1.2.3\"\ngit push origin v1.2.3 v1 --force\n```\n\nUsers reference by major version:\n```yaml\n- uses: owner/action@v1  # Tracks latest v1.x.x\n```\n\n## Testing Actions Locally\n\nUse `act` for local testing (see act skill):\n```bash\n# Test action in current directory\nact -j test\n\n# Test with specific event\nact push\n\n# Test with secrets\nact -s GITHUB_TOKEN=ghp_xxx\n```\n\n## Common Patterns\n\n### Matrix Testing Action\n\n```yaml\n# action.yml\nname: 'Matrix Test Runner'\ndescription: 'Run tests across multiple configurations'\ninputs:\n  matrix-config:\n    description: 'JSON matrix configuration'\n    required: true\nruns:\n  using: 'composite'\n  steps:\n    - run: |\n        echo \"Testing with config: ${{ inputs.matrix-config }}\"\n        # Parse and execute tests\n      shell: bash\n```\n\n### Cache Management Action\n\n```javascript\nconst core = require('@actions/core');\nconst cache = require('@actions/cache');\n\nasync function run() {\n  const paths = [\n    'node_modules',\n    '.npm'\n  ];\n\n  const key = `deps-${process.platform}-${hashFiles('package-lock.json')}`;\n\n  // Restore cache\n  const cacheKey = await cache.restoreCache(paths, key);\n\n  if (!cacheKey) {\n    core.info('Cache miss, installing dependencies');\n    await exec.exec('npm', ['ci']);\n    await cache.saveCache(paths, key);\n  } else {\n    core.info(`Cache hit: ${cacheKey}`);\n  }\n}\n```\n\n### Artifact Upload Action\n\n```javascript\nconst artifact = require('@actions/artifact');\n\nasync function uploadArtifact() {\n  const artifactClient = artifact.create();\n  const files = [\n    'dist/bundle.js',\n    'dist/styles.css'\n  ];\n\n  const rootDirectory = 'dist';\n  const options = {\n    continueOnError: false\n  };\n\n  const uploadResponse = await artifactClient.uploadArtifact(\n    'build-artifacts',\n    files,\n    rootDirectory,\n    options\n  );\n\n  core.setOutput('artifact-id', uploadResponse.artifactId);\n}\n```\n\n## Troubleshooting\n\n### Action Not Found\n\n- Verify repository is public or accessible\n- Check action.yml exists in repository root\n- Confirm version tag exists\n\n### Permission Denied\n\n```yaml\n# Add required permissions to workflow\npermissions:\n  contents: write\n  pull-requests: write\n```\n\n### Node Modules Missing\n\n- Bundle dependencies with ncc\n- Check dist/ folder is committed\n- Verify node_modules excluded from .gitignore for dist/\n\n### Docker Action Fails\n\n- Check Dockerfile syntax\n- Verify entrypoint has execute permissions\n- Test container locally: `docker build -t test . && docker run test`\n\n## Anti-Fabrication Requirements\n\n- Execute Read or Glob tools to verify action files exist before claiming structure\n- Use Bash to test commands before documenting syntax\n- Validate action.yml schema against actual files using tool analysis\n- Execute actual API calls with @actions/github before documenting responses\n- Test permission configurations in real workflows before recommending settings\n- Never claim action capabilities without reading actual implementation code\n- Report actual npm audit results when discussing security, not fabricated vulnerability counts"
              },
              {
                "name": "github-workflows",
                "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns",
                "path": "github/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "github-workflows",
                  "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns"
                },
                "content": "# GitHub Workflows\n\nActivate when creating, modifying, debugging, or optimizing GitHub Actions workflow files. This skill covers workflow syntax, structure, best practices, and common CI/CD patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Writing .github/workflows/*.yml files\n- Configuring workflow triggers and events\n- Defining jobs, steps, and dependencies\n- Using expressions and contexts\n- Managing secrets and environment variables\n- Implementing CI/CD pipelines\n- Optimizing workflow performance\n- Debugging workflow failures\n\n## Workflow File Structure\n\n### Basic Anatomy\n\n```yaml\nname: CI                              # Workflow name (optional)\n\non:                                   # Trigger events\n  push:\n    branches: [main, develop]\n  pull_request:\n\nenv:                                  # Global environment variables\n  NODE_VERSION: '20'\n\njobs:                                 # Job definitions\n  build:\n    name: Build and Test            # Job name (optional)\n    runs-on: ubuntu-latest          # Runner environment\n\n    steps:\n      - name: Checkout code         # Step name (optional)\n        uses: actions/checkout@v4   # Use an action\n\n      - name: Run tests\n        run: npm test               # Run command\n```\n\n### File Location\n\nWorkflows must be in `.github/workflows/` directory:\n```\n.github/\n└── workflows/\n    ├── ci.yml\n    ├── deploy.yml\n    └── release.yml\n```\n\n## Trigger Events (on:)\n\n### Push Events\n\n```yaml\non:\n  push:\n    branches:\n      - main\n      - 'release/**'        # Glob patterns\n    tags:\n      - 'v*'                # Version tags\n    paths:\n      - 'src/**'            # Only when these paths change\n      - '!docs/**'          # Ignore docs changes\n```\n\n### Pull Request Events\n\n```yaml\non:\n  pull_request:\n    types:\n      - opened\n      - synchronize       # New commits pushed\n      - reopened\n    branches:\n      - main\n    paths-ignore:\n      - '**.md'\n```\n\n### Schedule (Cron)\n\n```yaml\non:\n  schedule:\n    # Every day at 2am UTC\n    - cron: '0 2 * * *'\n    # Every Monday at 9am UTC\n    - cron: '0 9 * * 1'\n```\n\n### Manual Trigger (workflow_dispatch)\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Deployment environment'\n        required: true\n        type: choice\n        options:\n          - development\n          - staging\n          - production\n      debug:\n        description: 'Enable debug logging'\n        required: false\n        type: boolean\n        default: false\n```\n\n### Multiple Events\n\n```yaml\non:\n  push:\n    branches: [main]\n  pull_request:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly\n```\n\n## Jobs\n\n### Basic Job Configuration\n\n```yaml\njobs:\n  build:\n    name: Build Application\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm ci\n      - run: npm run build\n```\n\n### Runner Selection\n\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest        # Ubuntu (fastest, most common)\n\n  test-macos:\n    runs-on: macos-latest         # macOS\n\n  test-windows:\n    runs-on: windows-latest       # Windows\n\n  test-specific:\n    runs-on: ubuntu-22.04         # Specific version\n```\n\n### Matrix Strategy\n\n```yaml\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        node: [18, 20, 21]\n        exclude:\n          - os: macos-latest\n            node: 18\n      fail-fast: false            # Continue on failure\n      max-parallel: 4             # Concurrent jobs limit\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n      - run: npm test\n```\n\n### Job Dependencies\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n\n  test:\n    needs: build                  # Wait for build\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  deploy:\n    needs: [build, test]          # Wait for multiple jobs\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n```\n\n### Conditional Execution\n\n```yaml\njobs:\n  deploy:\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n\n  notify:\n    if: failure()                 # Run only if previous jobs failed\n    needs: [build, test]\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Build failed\"\n```\n\n## Steps\n\n### Using Actions\n\n```yaml\nsteps:\n  - name: Checkout repository\n    uses: actions/checkout@v4\n    with:\n      fetch-depth: 0              # Full history\n      submodules: recursive       # Include submodules\n\n  - name: Setup Node.js\n    uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n```\n\n### Running Commands\n\n```yaml\nsteps:\n  - name: Single command\n    run: npm install\n\n  - name: Multi-line script\n    run: |\n      echo \"Installing dependencies\"\n      npm ci\n      npm run build\n\n  - name: Shell selection\n    shell: bash\n    run: echo \"Using bash\"\n```\n\n### Conditional Steps\n\n```yaml\nsteps:\n  - name: Run on main branch only\n    if: github.ref == 'refs/heads/main'\n    run: npm run deploy\n\n  - name: Run on PR only\n    if: github.event_name == 'pull_request'\n    run: npm run test:pr\n```\n\n### Continue on Error\n\n```yaml\nsteps:\n  - name: Lint (optional)\n    continue-on-error: true\n    run: npm run lint\n\n  - name: Test (required)\n    run: npm test\n```\n\n## Environment Variables and Secrets\n\n### Global Variables\n\n```yaml\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo $NODE_ENV\n```\n\n### Job-Level Variables\n\n```yaml\njobs:\n  build:\n    env:\n      BUILD_TYPE: release\n    steps:\n      - run: echo $BUILD_TYPE\n```\n\n### Step-Level Variables\n\n```yaml\nsteps:\n  - name: Configure\n    env:\n      CONFIG_PATH: ./config.json\n    run: cat $CONFIG_PATH\n```\n\n### Using Secrets\n\n```yaml\nsteps:\n  - name: Deploy\n    env:\n      API_KEY: ${{ secrets.API_KEY }}\n      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}\n    run: ./deploy.sh\n```\n\n### Setting Variables Between Steps\n\n```yaml\nsteps:\n  - name: Set version\n    id: version\n    run: echo \"VERSION=$(cat version.txt)\" >> $GITHUB_OUTPUT\n\n  - name: Use version\n    run: echo \"Version is ${{ steps.version.outputs.VERSION }}\"\n```\n\n## Contexts\n\n### github Context\n\n```yaml\nsteps:\n  - name: Context information\n    run: |\n      echo \"Repository: ${{ github.repository }}\"\n      echo \"Branch: ${{ github.ref_name }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Run ID: ${{ github.run_id }}\"\n```\n\n### env Context\n\n```yaml\nenv:\n  MY_VAR: value\n\nsteps:\n  - run: echo \"${{ env.MY_VAR }}\"\n```\n\n### job Context\n\n```yaml\nsteps:\n  - name: Job status\n    if: job.status == 'success'\n    run: echo \"Job succeeded\"\n```\n\n### steps Context\n\n```yaml\nsteps:\n  - id: first-step\n    run: echo \"output=hello\" >> $GITHUB_OUTPUT\n\n  - run: echo \"${{ steps.first-step.outputs.output }}\"\n```\n\n### runner Context\n\n```yaml\nsteps:\n  - run: |\n      echo \"OS: ${{ runner.os }}\"\n      echo \"Arch: ${{ runner.arch }}\"\n      echo \"Temp: ${{ runner.temp }}\"\n```\n\n### matrix Context\n\n```yaml\nstrategy:\n  matrix:\n    version: [18, 20]\n\nsteps:\n  - run: echo \"Node ${{ matrix.version }}\"\n```\n\n## Expressions\n\n### Operators\n\n```yaml\nsteps:\n  # Comparison\n  - if: github.ref == 'refs/heads/main'\n\n  # Logical\n  - if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n  - if: github.event_name == 'pull_request' || github.event_name == 'push'\n\n  # Negation\n  - if: \"!cancelled()\"\n\n  # Contains\n  - if: contains(github.event.head_commit.message, '[skip ci]')\n\n  # StartsWith/EndsWith\n  - if: startsWith(github.ref, 'refs/tags/v')\n  - if: endsWith(github.ref, '-beta')\n```\n\n### Functions\n\n```yaml\nsteps:\n  # Status functions\n  - if: success()        # Previous steps succeeded\n  - if: failure()        # Any previous step failed\n  - if: always()         # Always run\n  - if: cancelled()      # Workflow cancelled\n\n  # String functions\n  - run: echo \"${{ format('Hello {0}', github.actor) }}\"\n  - if: contains(github.event.pull_request.labels.*.name, 'deploy')\n\n  # JSON functions\n  - run: echo '${{ toJSON(github.event) }}'\n  - run: echo '${{ fromJSON(env.CONFIG).database.host }}'\n\n  # Hash function\n  - run: echo \"${{ hashFiles('**/package-lock.json') }}\"\n```\n\n## Artifacts\n\n### Upload Artifacts\n\n```yaml\nsteps:\n  - name: Build\n    run: npm run build\n\n  - name: Upload artifacts\n    uses: actions/upload-artifact@v4\n    with:\n      name: build-files\n      path: |\n        dist/\n        build/\n      retention-days: 7\n      if-no-files-found: error\n```\n\n### Download Artifacts\n\n```yaml\njobs:\n  build:\n    steps:\n      - run: npm run build\n      - uses: actions/upload-artifact@v4\n        with:\n          name: dist\n          path: dist/\n\n  test:\n    needs: build\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: dist\n          path: dist/\n      - run: npm test\n```\n\n## Caching\n\n### npm Cache\n\n```yaml\nsteps:\n  - uses: actions/checkout@v4\n  - uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n  - run: npm ci\n```\n\n### Manual Cache\n\n```yaml\nsteps:\n  - uses: actions/cache@v4\n    with:\n      path: |\n        ~/.npm\n        node_modules\n      key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n      restore-keys: |\n        ${{ runner.os }}-node-\n```\n\n## Permissions\n\n### Repository Token Permissions\n\n```yaml\npermissions:\n  contents: read              # Repository content\n  pull-requests: write        # PR comments\n  issues: write              # Issue creation/comments\n  checks: write              # Check runs\n  statuses: write            # Commit statuses\n  deployments: write         # Deployments\n  packages: write            # Package registry\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n```\n\n### Job-Level Permissions\n\n```yaml\njobs:\n  build:\n    permissions:\n      contents: read\n      pull-requests: write\n    steps:\n      - uses: actions/checkout@v4\n```\n\n## Concurrency\n\n### Prevent Concurrent Runs\n\n```yaml\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true    # Cancel running workflows\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - run: ./deploy.sh\n```\n\n### Job-Level Concurrency\n\n```yaml\njobs:\n  deploy:\n    concurrency:\n      group: deploy-${{ github.ref }}\n      cancel-in-progress: false\n    steps:\n      - run: ./deploy.sh\n```\n\n## Reusable Workflows\n\n### Define Reusable Workflow\n\n```yaml\n# .github/workflows/reusable-test.yml\nname: Reusable Test Workflow\n\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n      coverage:\n        required: false\n        type: boolean\n        default: false\n    outputs:\n      test-result:\n        description: \"Test execution result\"\n        value: ${{ jobs.test.outputs.result }}\n    secrets:\n      token:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    outputs:\n      result: ${{ steps.test.outputs.result }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ inputs.node-version }}\n      - run: npm test\n        id: test\n```\n\n### Call Reusable Workflow\n\n```yaml\njobs:\n  test:\n    uses: ./.github/workflows/reusable-test.yml\n    with:\n      node-version: '20'\n      coverage: true\n    secrets:\n      token: ${{ secrets.GITHUB_TOKEN }}\n```\n\n## Common CI/CD Patterns\n\n### Node.js CI\n\n```yaml\nname: Node.js CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20, 21]\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm test\n      - run: npm run build\n```\n\n### Docker Build and Push\n\n```yaml\nname: Docker\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ghcr.io/${{ github.repository }}\n          tags: |\n            type=ref,event=branch\n            type=semver,pattern={{version}}\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n```\n\n### Deploy on Release\n\n```yaml\nname: Deploy\n\non:\n  release:\n    types: [published]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://example.com\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        env:\n          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\n        run: ./deploy.sh\n```\n\n### Monorepo with Path Filtering\n\n```yaml\nname: Monorepo CI\n\non:\n  pull_request:\n    paths:\n      - 'packages/**'\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      frontend: ${{ steps.filter.outputs.frontend }}\n      backend: ${{ steps.filter.outputs.backend }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          filters: |\n            frontend:\n              - 'packages/frontend/**'\n            backend:\n              - 'packages/backend/**'\n\n  test-frontend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.frontend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=frontend\n\n  test-backend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.backend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=backend\n```\n\n## Debugging Workflows\n\n### Enable Debug Logging\n\nSet repository secrets:\n- `ACTIONS_RUNNER_DEBUG`: true\n- `ACTIONS_STEP_DEBUG`: true\n\n### Debug Steps\n\n```yaml\nsteps:\n  - name: Debug context\n    run: |\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Ref: ${{ github.ref }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n\n  - name: Dump GitHub context\n    run: echo '${{ toJSON(github) }}'\n\n  - name: Dump runner context\n    run: echo '${{ toJSON(runner) }}'\n```\n\n### Tmate Debugging\n\n```yaml\nsteps:\n  - name: Setup tmate session\n    if: failure()\n    uses: mxschmitt/action-tmate@v3\n    timeout-minutes: 30\n```\n\n## Performance Optimization\n\n### Use Caching\n\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}\n```\n\n### Optimize Checkout\n\n```yaml\n- uses: actions/checkout@v4\n  with:\n    fetch-depth: 1              # Shallow clone\n    sparse-checkout: |          # Partial checkout\n      src/\n      tests/\n```\n\n### Concurrent Jobs\n\n```yaml\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run lint\n\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  build:\n    needs: [lint, test]         # Parallel lint and test\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n```\n\n## Anti-Fabrication Requirements\n\n- Execute Read tool to verify workflow files exist before claiming structure\n- Use Bash with `gh workflow list` to confirm actual workflow names before referencing them\n- Execute `gh workflow view <workflow>` to verify trigger configuration before documenting it\n- Use Glob to find actual workflow files before claiming their presence\n- Execute `gh run list` to verify actual workflow runs before discussing execution patterns\n- Never claim workflow success rates without actual run history analysis\n- Validate YAML syntax using yamllint or similar tools via Bash before claiming correctness\n- Report actual permission errors from workflow runs, not fabricated authorization issues\n- Execute actual cache operations before claiming cache hit/miss percentages\n- Use Read tool on action.yml files to verify action inputs/outputs before documenting usage"
              },
              {
                "name": "daisyui",
                "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design",
                "path": "ui/skills/daisyui/SKILL.md",
                "frontmatter": {
                  "name": "daisyui",
                  "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design"
                },
                "content": "# daisyUI Component Library\n\nUse this skill when building user interfaces with daisyUI and Tailwind CSS, implementing UI components, or configuring themes.\n\n## When to Use This Skill\n\nActivate when:\n- Building UI components with daisyUI\n- Choosing appropriate daisyUI components for design needs\n- Implementing responsive layouts with daisyUI\n- Configuring or customizing themes\n- Converting designs to daisyUI components\n- Troubleshooting daisyUI component styling\n\n## What is daisyUI?\n\ndaisyUI is a Tailwind CSS component library providing:\n\n- **Semantic component classes** - High-level abstractions of Tailwind utilities\n- **33+ built-in themes** - Light, dark, and creative theme variants\n- **Framework-agnostic** - Works with any HTML/CSS project\n- **Utility-first compatible** - Combine components with Tailwind utilities\n\n## Installation\n\nAdd daisyUI to your project:\n\n```bash\nnpm install -D daisyui@latest\n```\n\nConfigure `tailwind.config.js`:\n\n```javascript\nmodule.exports = {\n  plugins: [require(\"daisyui\")],\n}\n```\n\nFor detailed installation options and CDN usage, see `references/installation.md`.\n\n## Component Categories\n\ndaisyUI provides components across these categories:\n\n- **Actions**: Buttons, dropdowns, modals, swap\n- **Data Display**: Cards, badges, tables, carousels, stats\n- **Data Input**: Input, textarea, select, checkbox, radio, toggle\n- **Navigation**: Navbar, menu, tabs, breadcrumbs, pagination\n- **Feedback**: Alert, progress, loading, toast, tooltip\n- **Layout**: Drawer, footer, hero, stack, divider\n\nFor component-specific guidance, consult the appropriate reference file.\n\n## Quick Usage\n\n### Basic Button\n\n```html\n<button class=\"btn\">Button</button>\n<button class=\"btn btn-primary\">Primary</button>\n<button class=\"btn btn-secondary\">Secondary</button>\n<button class=\"btn btn-accent\">Accent</button>\n```\n\n### Card Component\n\n```html\n<div class=\"card w-96 bg-base-100 shadow-xl\">\n  <figure><img src=\"image.jpg\" alt=\"Image\" /></figure>\n  <div class=\"card-body\">\n    <h2 class=\"card-title\">Card Title</h2>\n    <p>Card description text</p>\n    <div class=\"card-actions justify-end\">\n      <button class=\"btn btn-primary\">Action</button>\n    </div>\n  </div>\n</div>\n```\n\n### Modal\n\n```html\n<button class=\"btn\" onclick=\"my_modal.showModal()\">Open Modal</button>\n\n<dialog id=\"my_modal\" class=\"modal\">\n  <div class=\"modal-box\">\n    <h3 class=\"font-bold text-lg\">Modal Title</h3>\n    <p class=\"py-4\">Modal content here</p>\n    <div class=\"modal-action\">\n      <form method=\"dialog\">\n        <button class=\"btn\">Close</button>\n      </form>\n    </div>\n  </div>\n</dialog>\n```\n\n## Theming\n\n### Using Built-in Themes\n\nSet theme via HTML attribute:\n\n```html\n<html data-theme=\"cupcake\">\n```\n\nAvailable themes: light, dark, cupcake, bumblebee, emerald, corporate, synthwave, retro, cyberpunk, valentine, halloween, garden, forest, aqua, lofi, pastel, fantasy, wireframe, black, luxury, dracula, cmyk, autumn, business, acid, lemonade, night, coffee, winter, dim, nord, sunset\n\n### Theme Switching\n\n```html\n<select class=\"select\" data-choose-theme>\n  <option value=\"light\">Light</option>\n  <option value=\"dark\">Dark</option>\n  <option value=\"cupcake\">Cupcake</option>\n</select>\n```\n\nFor advanced theming and customization, see `references/theming.md`.\n\n## Responsive Design\n\ndaisyUI components work with Tailwind's responsive prefixes:\n\n```html\n<button class=\"btn btn-sm md:btn-md lg:btn-lg\">\n  Responsive Button\n</button>\n\n<div class=\"card w-full md:w-96\">\n  <!-- Responsive card -->\n</div>\n```\n\n## When to Consult References\n\n- **Installation details**: Read `references/installation.md`\n- **Complete component list**: Read `references/components.md`\n- **Theming and customization**: Read `references/theming.md`\n- **Layout patterns**: Read `references/layouts.md`\n- **Form components**: Read `references/forms.md`\n- **Common patterns**: Read `references/patterns.md`\n\n## Combining with Tailwind Utilities\n\ndaisyUI semantic classes combine with Tailwind utilities:\n\n```html\n<!-- daisyUI component + Tailwind utilities -->\n<button class=\"btn btn-primary shadow-lg hover:shadow-xl transition-all\">\n  Enhanced Button\n</button>\n\n<div class=\"card bg-base-100 border-2 border-primary rounded-lg p-4\">\n  <!-- Card with custom styling -->\n</div>\n```\n\n## Key Principles\n\n- **Semantic over utility**: Use component classes for common patterns\n- **Utility for customization**: Apply Tailwind utilities for unique styling\n- **Theme-aware**: Components adapt to theme colors automatically\n- **Accessible**: Components follow accessibility best practices\n- **Composable**: Combine components to build complex UIs\n\n## Pro Tips\n\n- Use `btn-{size}` modifiers: `btn-xs`, `btn-sm`, `btn-md`, `btn-lg`\n- Add `btn-outline` for outlined button variants\n- Use `badge` component for status indicators\n- Combine `modal` with `modal-backdrop` for better UX\n- Use `drawer` for mobile navigation patterns\n- Leverage `stats` component for dashboard metrics\n- Use `loading` class on buttons for async operations"
              }
            ]
          },
          {
            "name": "claude-code",
            "description": "Claude Code-specific skills: plugin marketplace management and validation",
            "source": "./claude-code",
            "category": "tools",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install claude-code@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/research-skill",
                "description": "Research topics and create Agent Skills with proper structure and documentation",
                "path": "claude-code/commands/research-skill.md",
                "frontmatter": {
                  "description": "Research topics and create Agent Skills with proper structure and documentation",
                  "argument-hint": "<skill-name> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create a properly structured Agent Skill following the Agent Skills Specification.\n\n**Skill Creation:**\n- **Directory Structure**: Creates `skills/<skill-name>/SKILL.md` with proper frontmatter\n- **Progressive Disclosure**: Generates reference files in `references/` for deep context\n- **Source Documentation**: Updates `promptlog/sources.md` with all research sources\n- **Specification Compliance**: Follows Agent Skills Specification v1.0\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **YAML Frontmatter**: Auto-generates name, description, license, metadata\n- **Reference Management**: Creates separate files for detailed documentation\n- **Source Tracking**: Maintains traceability in promptlog/sources.md\n\n**Examples:**\n```\n/research-skill elixir-genserver\n# Creates: skills/elixir-genserver/SKILL.md\n# Updates: promptlog/sources.md\n\n/research-skill kubernetes-operators --complexity=high\n# Creates: skills/kubernetes-operators/SKILL.md\n#          skills/kubernetes-operators/references/\n# Updates: promptlog/sources.md\n\n/research-skill react-hooks --complexity=medium\n# Creates: skills/react-hooks/SKILL.md with enhanced analysis\n```\n\n**SKILL.md Structure:**\n```markdown\n---\nname: skill-name\ndescription: When Claude should use this skill (concise, activation-focused)\nlicense: MIT\n---\n\n# Skill Name\n\n## When to Use This Skill\n[Activation criteria]\n\n## Core Concepts\n[Essential knowledge]\n\n## Best Practices\n[Guidelines and patterns]\n\n## Examples\n[Concrete usage examples]\n\n## References\n[Links to reference files if needed]\n```\n\n**Workflow:**\n1. **Research**: Gather authoritative sources and best practices\n2. **Structure**: Create skill directory following spec\n3. **Generate SKILL.md**: Write frontmatter and core content\n4. **Create References**: Add detailed docs in references/ for progressive disclosure\n5. **Document Sources**: Update promptlog/sources.md with all sources used\n6. **Validate**: Ensure spec compliance and activation clarity\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. Research the topic thoroughly using web search and authoritative sources\n2. Create the skill directory: `skills/<skill-name>/`\n3. Generate SKILL.md with:\n   - Proper YAML frontmatter (name, description, license)\n   - Clear activation criteria in description\n   - Core procedural knowledge in markdown body\n   - Concrete examples and patterns\n4. Create `skills/<skill-name>/references/` if detailed documentation is needed\n5. Update `promptlog/sources.md` with:\n   - All URLs and documentation sources used\n   - Purpose of each source\n   - Key concepts extracted\n   - Date accessed\n6. Follow the 7-step skill creation workflow from CLAUDE.md\n7. Use imperative/infinitive language (not second-person)\n8. Keep commonly-used context in SKILL.md, detailed references separate\n\nThe agent should produce a complete, spec-compliant skill ready for use."
              }
            ],
            "skills": [
              {
                "name": "claude-agents",
                "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                "path": "claude-code/skills/claude-agents/SKILL.md",
                "frontmatter": {
                  "name": "claude-agents",
                  "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Agents\n\nGuide for creating custom agents that provide specialized behaviors and tool access for specific tasks.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating custom agent types for specific workflows\n- Defining agent behaviors and tool permissions\n- Configuring agent capabilities\n- Understanding agent vs skill differences\n- Implementing domain-specific agents\n\n## What Are Agents?\n\nAgents are specialized Claude instances with:\n- **Specific tool access**: Limited or specialized tool sets\n- **Defined behaviors**: Pre-configured instructions and constraints\n- **Task focus**: Optimized for particular workflows\n- **Autonomous operation**: Can execute multi-step tasks independently\n\n## Agents vs Skills\n\n| Feature | Agents | Skills |\n|---------|--------|--------|\n| **Activation** | Explicitly launched via Task tool | Auto-activated based on context |\n| **Tool Access** | Configurable, can be restricted | Inherit from parent context |\n| **State** | Independent, isolated | Share parent context |\n| **Use Case** | Complex multi-step tasks | Knowledge and guidelines |\n| **Persistence** | Single execution | Always available when loaded |\n\n## Agent File Structure\n\n### Location\n\nAgents are defined in markdown files located in:\n- Plugin: `<plugin-root>/agents/`\n- User-level: `.claude/agents/`\n\n### File Naming\n\n- Use kebab-case: `code-reviewer.md`\n- File name becomes the agent type\n- Be descriptive about the agent's purpose\n\n## Basic Agent Format\n\n```markdown\n---\nname: code-reviewer\ndescription: Specialized agent for conducting thorough code reviews\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Code Review Agent\n\nI am a specialized code review agent focused on:\n\n## Responsibilities\n\n- Analyzing code for correctness and style\n- Identifying security vulnerabilities\n- Checking test coverage\n- Ensuring documentation quality\n- Suggesting improvements\n\n## Review Process\n\nWhen reviewing code, I will:\n\n1. Read the changed files\n2. Check for common anti-patterns\n3. Verify error handling\n4. Assess test coverage\n5. Provide actionable feedback\n\n## Guidelines\n\n- Focus on significant issues\n- Provide specific examples\n- Suggest concrete improvements\n- Consider project context\n```\n\n## Agent Configuration\n\n### YAML Frontmatter\n\nRequired and optional fields:\n\n```markdown\n---\nname: agent-name                    # Required: kebab-case identifier\ndescription: Brief description      # Required: What this agent does\ntools:                             # Optional: Tool allowlist\n  - Read\n  - Write\n  - Bash\nmodel: sonnet                      # Optional: Model to use (sonnet, opus, haiku)\nmax_iterations: 10                 # Optional: Maximum task iterations\ntimeout: 300                       # Optional: Timeout in seconds\n---\n```\n\n### Tool Allowlist\n\nRestrict agent to specific tools:\n\n```markdown\n---\ntools:\n  - Read      # Can read files\n  - Grep      # Can search code\n  - Glob      # Can find files\n  # Cannot use Write, Edit, Bash, etc.\n---\n```\n\n**No tool restrictions** (access to all tools):\n\n```markdown\n---\n# Omit tools field entirely\n---\n```\n\n### Model Selection\n\nChoose appropriate model for the task:\n\n```markdown\n---\nmodel: haiku        # Fast, cost-effective for simple tasks\n# model: sonnet     # Balanced (default)\n# model: opus       # Most capable for complex tasks\n---\n```\n\n## Common Agent Patterns\n\n### Read-Only Analysis Agent\n\n```markdown\n---\nname: security-analyzer\ndescription: Analyzes code for security vulnerabilities\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Security Analysis Agent\n\nI perform security analysis on codebases.\n\n## Analysis Areas\n\n- SQL injection vulnerabilities\n- XSS attack vectors\n- Authentication/authorization issues\n- Sensitive data exposure\n- Insecure dependencies\n\n## Process\n\n1. Scan for common vulnerability patterns\n2. Check security best practices\n3. Identify potential risks\n4. Provide remediation guidance\n```\n\n### Test Generation Agent\n\n```markdown\n---\nname: test-generator\ndescription: Generates comprehensive test suites\ntools:\n  - Read\n  - Write\n  - Glob\nmodel: sonnet\n---\n\n# Test Generation Agent\n\nI create comprehensive test suites for your code.\n\n## Test Types\n\n- Unit tests\n- Integration tests\n- Edge case coverage\n- Error scenario tests\n\n## Approach\n\n1. Analyze source code structure\n2. Identify testable units\n3. Generate test cases\n4. Create test files with proper naming\n5. Include setup and teardown logic\n```\n\n### Documentation Agent\n\n```markdown\n---\nname: docs-generator\ndescription: Creates and updates project documentation\ntools:\n  - Read\n  - Write\n  - Glob\n  - Grep\nmodel: sonnet\n---\n\n# Documentation Agent\n\nI create and maintain project documentation.\n\n## Documentation Types\n\n- README files\n- API documentation\n- Code comments\n- Architecture docs\n- User guides\n\n## Standards\n\n- Clear, concise language\n- Practical examples\n- Up-to-date with codebase\n- Proper formatting (Markdown, JSDoc, etc.)\n```\n\n### Refactoring Agent\n\n```markdown\n---\nname: refactorer\ndescription: Safely refactors code while maintaining functionality\ntools:\n  - Read\n  - Write\n  - Edit\n  - Grep\n  - Glob\nmodel: sonnet\nmax_iterations: 20\n---\n\n# Code Refactoring Agent\n\nI refactor code to improve quality while preserving behavior.\n\n## Refactoring Goals\n\n- Improve readability\n- Reduce complexity\n- Eliminate duplication\n- Enhance maintainability\n- Follow best practices\n\n## Safety Measures\n\n- Preserve existing functionality\n- Maintain test coverage\n- Document changes\n- Use safe transformations\n```\n\n## Agent Plugin Configuration\n\n### In plugin.json\n\n```json\n{\n  \"agents\": [\n    \"./agents/code-reviewer.md\",\n    \"./agents/test-generator.md\",\n    \"./agents/security-analyzer.md\"\n  ]\n}\n```\n\n### Directory-Based Loading\n\n```json\n{\n  \"agents\": \"./agents\"\n}\n```\n\nLoads all `.md` files in `agents/` directory.\n\n## Invoking Agents\n\nAgents are launched via the Task tool:\n\n```python\n# In parent Claude conversation\nTask(\n    subagent_type=\"code-reviewer\",\n    description=\"Review authentication module\",\n    prompt=\"\"\"\n    Review the authentication module for:\n    - Security vulnerabilities\n    - Error handling\n    - Input validation\n    - Best practices\n    \"\"\"\n)\n```\n\n## Agent Communication\n\n### Input to Agent\n\n- Task description\n- Detailed prompt\n- Access to conversation history (if configured)\n\n### Output from Agent\n\n- Final report/result\n- No ongoing dialogue\n- One-time execution\n\n## Best Practices\n\n### Clear Purpose\n\nEach agent should have a specific, well-defined purpose:\n\n```markdown\n---\nname: migration-helper\ndescription: Assists with database schema migrations\n---\n\n# Database Migration Agent\n\nSpecialized in creating and validating database migrations.\n```\n\n### Appropriate Tool Access\n\nOnly grant necessary tools:\n\n```markdown\n---\n# Analysis agent - read-only\ntools:\n  - Read\n  - Grep\n  - Glob\n---\n```\n\n```markdown\n---\n# Implementation agent - can modify\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n---\n```\n\n### Model Selection\n\nMatch model to task complexity:\n\n- **haiku**: Simple, repetitive tasks\n- **sonnet**: Standard tasks (default)\n- **opus**: Complex reasoning required\n\n### Iteration Limits\n\nSet appropriate limits for task complexity:\n\n```markdown\n---\nmax_iterations: 5   # Simple, focused task\n# max_iterations: 20  # Complex, multi-step workflow\n---\n```\n\n### Clear Instructions\n\nProvide explicit behavior guidelines:\n\n```markdown\n# Testing Agent\n\n## Mandatory Requirements\n\n- Generate tests for ALL public methods\n- Achieve minimum 80% code coverage\n- Include edge cases and error scenarios\n- Use project's testing framework conventions\n\n## Constraints\n\n- Do not modify source code\n- Follow existing test file naming patterns\n- Use appropriate assertions\n```\n\n## Security Considerations\n\n### Tool Restrictions\n\nLimit dangerous operations:\n\n```markdown\n---\n# Don't give Bash access to untrusted agents\ntools:\n  - Read\n  - Write  # Safer than arbitrary shell commands\n---\n```\n\n### Input Validation\n\nValidate agent inputs:\n\n```markdown\n# Deployment Agent\n\nBefore deploying:\n1. Verify target environment is valid\n2. Check deployment permissions\n3. Validate configuration\n4. Confirm destructive operations\n```\n\n### Sensitive Data\n\nNever hardcode:\n- Credentials\n- API keys\n- Private URLs\n- Access tokens\n\n## Agent Examples\n\n### PR Review Agent\n\n```markdown\n---\nname: pr-reviewer\ndescription: Reviews pull requests for quality and completeness\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Pull Request Review Agent\n\nConducting thorough PR review...\n\n## Checklist\n\n- [ ] Code quality and style\n- [ ] Test coverage\n- [ ] Documentation updates\n- [ ] Breaking changes noted\n- [ ] Security considerations\n- [ ] Performance implications\n\n## Review Process\n\n1. Analyze changed files\n2. Check for common issues\n3. Verify tests exist\n4. Review documentation\n5. Provide constructive feedback\n```\n\n### Migration Agent\n\n```markdown\n---\nname: code-migrator\ndescription: Migrates code from one framework/version to another\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\nmodel: opus\nmax_iterations: 30\n---\n\n# Code Migration Agent\n\nPerforming framework migration...\n\n## Migration Steps\n\n1. Analyze current codebase\n2. Identify migration patterns\n3. Apply transformations\n4. Update dependencies\n5. Verify compatibility\n6. Document changes\n\n## Safety Checks\n\n- Backup original code\n- Incremental changes\n- Validate each step\n- Maintain git history\n```\n\n## Troubleshooting\n\n### Agent Not Found\n\n- Verify agent file location matches plugin.json\n- Check file naming (kebab-case, .md extension)\n- Ensure plugin is properly installed\n\n### Tool Access Denied\n\n- Check tools allowlist in frontmatter\n- Verify tool names match exactly\n- Ensure parent context permits delegation\n\n### Unexpected Behavior\n\n- Review agent instructions for clarity\n- Check model selection appropriateness\n- Verify iteration limits aren't too restrictive\n- Test with verbose output\n\n## References\n\nFor more information:\n- Claude Code Agents Documentation: https://code.claude.com/docs/en/agents\n- Task Tool Documentation: https://code.claude.com/docs/en/tools/task"
              },
              {
                "name": "claude-commands",
                "description": "Guide for creating custom slash commands for Claude Code",
                "path": "claude-code/skills/claude-commands/SKILL.md",
                "frontmatter": {
                  "name": "claude-commands",
                  "description": "Guide for creating custom slash commands for Claude Code",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Commands\n\nGuide for creating custom slash commands that extend Claude Code functionality.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new custom slash commands\n- Understanding command structure and syntax\n- Organizing commands for plugins\n- Implementing command workflows\n- Debugging command execution\n\n## What Are Commands?\n\nCommands are custom slash commands (like `/commit`, `/review`) that users can invoke to trigger specific workflows or expand prompts. They are markdown files that can contain:\n\n- Static prompt text\n- Dynamic content based on arguments\n- Multi-step workflows\n- Integration with tools and scripts\n\n## Command File Structure\n\n### Location\n\nCommands are defined in markdown files located in:\n- Plugin: `<plugin-root>/commands/`\n- User-level: `.claude/commands/`\n\n### File Naming\n\n- Use kebab-case: `my-command.md`\n- File name becomes the command name: `my-command.md` → `/my-command`\n- Avoid conflicts with built-in commands\n\n## Basic Command Format\n\n### Simple Static Command\n\n```markdown\n# /my-command\n\nThis is the prompt that will be expanded when the user types /my-command.\n\nThe entire content of this file will replace the slash command in the conversation.\n```\n\n### Command with Description\n\n```markdown\n<!--\ndescription: Brief description of what this command does\n-->\n\n# /my-command\n\nCommand prompt goes here...\n```\n\n## Command Arguments\n\nCommands can accept arguments that users provide when invoking the command.\n\n### Single Argument\n\n```markdown\n# /greet\n\nHello, {{arg}}! Welcome to the project.\n```\n\nUsage: `/greet Alice` → \"Hello, Alice! Welcome to the project.\"\n\n### Multiple Arguments\n\n```markdown\n# /create-file\n\nCreate a new file at {{arg1}} with the following content:\n\n{{arg2}}\n```\n\nUsage: `/create-file src/main.rs \"fn main() {}\"`\n\n### Named Arguments\n\n```markdown\n# /deploy\n\nDeploy {{environment}} environment to {{region}}.\n\nConfiguration:\n- Environment: {{environment}}\n- Region: {{region}}\n- Branch: {{branch}}\n```\n\nUsage: `/deploy --environment=production --region=us-east-1 --branch=main`\n\n## Advanced Features\n\n### Conditional Content\n\n```markdown\n# /analyze\n\nAnalyze the {{language}} codebase.\n\n{{#if verbose}}\nProvide detailed analysis including:\n- Code complexity metrics\n- Dependency analysis\n- Security vulnerabilities\n{{else}}\nProvide a summary analysis.\n{{/if}}\n```\n\n### Including Files\n\nReference other files or command outputs:\n\n```markdown\n# /context\n\nHere is the current project structure:\n\n{{file:PROJECT_STRUCTURE.md}}\n\nAnd the current git status:\n\n{{shell:git status}}\n```\n\n### Multi-Step Workflows\n\n```markdown\n# /full-review\n\nI'll perform a comprehensive code review:\n\n1. First, let me check the git diff:\n{{shell:git diff}}\n\n2. Now analyzing code quality...\n\n3. Checking for security issues...\n\n4. Final recommendations:\n```\n\n## Best Practices\n\n### Clear Command Names\n\n- Use descriptive, action-oriented names\n- `/analyze-security` not `/sec`\n- `/create-component` not `/comp`\n\n### Provide Context\n\nAlways include what the command will do:\n\n```markdown\n# /commit\n\nI'll analyze the current git changes and create a conventional commit message.\n\nCurrent changes:\n{{shell:git diff --staged}}\n\nBased on these changes, here's my suggested commit message:\n```\n\n### Handle Edge Cases\n\n```markdown\n# /deploy\n\n{{#if staging}}\nDeploying to staging environment (safe for testing)\n{{else if production}}\n⚠️ WARNING: Deploying to PRODUCTION\nAre you sure you want to continue? This will affect live users.\n{{else}}\nError: Unknown environment. Please specify --staging or --production\n{{/if}}\n```\n\n### Document Arguments\n\n```markdown\n<!--\ndescription: Deploy application to specified environment\nusage: /deploy [--environment=<env>] [--region=<region>]\narguments:\n  - environment: Target environment (staging, production)\n  - region: AWS region (us-east-1, eu-west-1, etc.)\n-->\n\n# /deploy\n```\n\n## Command Organization\n\n### Plugin Commands\n\nIn `plugin.json`:\n\n```json\n{\n  \"commands\": [\n    \"./commands/deploy.md\",\n    \"./commands/analyze.md\",\n    \"./commands/review.md\"\n  ]\n}\n```\n\n### Directory-Based Commands\n\n```json\n{\n  \"commands\": [\"./commands\"]\n}\n```\n\nThis loads all `.md` files in the `commands/` directory.\n\n### Namespaced Commands\n\nOrganize related commands in subdirectories:\n\n```\ncommands/\n├── git/\n│   ├── commit.md\n│   ├── review.md\n│   └── cleanup.md\n├── deploy/\n│   ├── staging.md\n│   └── production.md\n```\n\n## Common Command Patterns\n\n### Git Commit Message Generator\n\n```markdown\n# /gcm\n\nI'll analyze the staged changes and generate a conventional commit message.\n\n{{shell:git diff --staged}}\n\nBased on these changes, here's my commit message:\n```\n\n### Code Review Command\n\n```markdown\n# /review-pr\n\nI'll review the pull request changes.\n\nPR Number: {{pr_number}}\n\n{{shell:gh pr diff {{pr_number}}}}\n\nReview checklist:\n- [ ] Code quality and style\n- [ ] Security considerations\n- [ ] Test coverage\n- [ ] Documentation updates\n```\n\n### Project Scaffolding\n\n```markdown\n# /new-component\n\nCreating a new {{component_type}} component named {{name}}.\n\nI'll create:\n1. Component file at src/components/{{name}}.tsx\n2. Test file at src/components/{{name}}.test.tsx\n3. Storybook file at src/components/{{name}}.stories.tsx\n```\n\n## Testing Commands\n\n### Manual Testing\n\n1. Install the plugin locally\n2. Reload Claude Code\n3. Type your command in the chat\n4. Verify the expansion is correct\n\n### Debugging\n\nIf a command doesn't work:\n\n1. Check file location matches plugin.json\n2. Verify markdown syntax\n3. Test argument substitution\n4. Check for conflicts with existing commands\n\n## Command Templates\n\n### Analysis Command Template\n\n```markdown\n<!--\ndescription: Analyze {{target}} for {{criteria}}\n-->\n\n# /analyze-{{target}}\n\nI'll analyze the {{target}} codebase for {{criteria}}.\n\n{{shell:find {{target}} -type f -name \"*.{{extension}}\"}}\n\nAnalysis results:\n```\n\n### Workflow Command Template\n\n```markdown\n<!--\ndescription: Execute {{workflow}} workflow\n-->\n\n# /{{workflow}}\n\nStarting {{workflow}} workflow...\n\nStep 1: {{step1_description}}\n{{step1_action}}\n\nStep 2: {{step2_description}}\n{{step2_action}}\n\nWorkflow complete!\n```\n\n## Integration with Skills\n\nCommands can reference skills:\n\n```markdown\n# /elixir-review\n\nI'll review this Elixir code using my Phoenix and OTP knowledge.\n\nPlease provide the code to review, and I'll check for:\n- Phoenix best practices\n- OTP design patterns\n- Elixir anti-patterns\n- Performance considerations\n```\n\n## Security Considerations\n\n### Avoid Sensitive Data\n\nNever hardcode:\n- API keys\n- Passwords\n- Tokens\n- Private URLs\n\n### Validate Input\n\n```markdown\n# /deploy\n\n{{#unless environment}}\nError: --environment is required\n{{/unless}}\n\n{{#if (validate_environment environment)}}\nProceeding with deployment...\n{{else}}\nError: Invalid environment. Must be staging or production.\n{{/if}}\n```\n\n### Safe Shell Commands\n\nBe cautious with shell command execution:\n\n```markdown\n# /safe-deploy\n\n<!-- Only allow whitelisted commands -->\n{{shell:./scripts/deploy.sh {{environment}}}}\n```\n\n## References\n\nFor more information about Claude Code commands:\n- Claude Code Documentation: https://code.claude.com/docs/en/commands\n- Example Commands: https://github.com/anthropics/claude-code/tree/main/examples/commands"
              },
              {
                "name": "claude-hooks",
                "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                "path": "claude-code/skills/claude-hooks/SKILL.md",
                "frontmatter": {
                  "name": "claude-hooks",
                  "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Hooks\n\nGuide for creating hooks that execute shell commands or scripts in response to Claude Code events and tool calls.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating event-driven automations\n- Implementing custom validation or formatting\n- Integrating with external tools and services\n- Setting up project-specific workflows\n- Responding to tool execution events\n\n## What Are Hooks?\n\nHooks are shell commands that execute automatically in response to specific events:\n\n- **Tool Call Hooks**: Trigger before/after specific tool calls\n- **Lifecycle Hooks**: Trigger on plugin install/uninstall\n- **User Prompt Hooks**: Trigger when users submit prompts\n- **Custom Events**: Application-specific trigger points\n\n## Hook Configuration\n\n### Location\n\nHooks are configured in:\n- Plugin: `<plugin-root>/.claude-plugin/hooks.json`\n- User-level: `.claude/hooks.json`\n- Plugin manifest: Inline in `plugin.json`\n\n### File Structure\n\n**Standalone hooks.json:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"./hooks/format-check.sh\"],\n      \"after\": [\"./hooks/lint.sh\"]\n    },\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh\"]\n    }\n  },\n  \"onInstall\": [\"./hooks/setup.sh\"],\n  \"onUninstall\": [\"./hooks/cleanup.sh\"],\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh\"]\n}\n```\n\n**Inline in plugin.json:**\n```json\n{\n  \"hooks\": {\n    \"onToolCall\": {\n      \"Write\": {\n        \"after\": [\"prettier --write {{file_path}}\"]\n      }\n    }\n  }\n}\n```\n\n## Hook Types\n\n### Tool Call Hooks\n\nExecute before or after specific tool calls.\n\n**Available Tools:**\n- `Read`, `Write`, `Edit`, `MultiEdit`\n- `Bash`, `BashOutput`\n- `Glob`, `Grep`\n- `Task`, `Skill`, `SlashCommand`\n- `TodoWrite`\n- `WebFetch`, `WebSearch`\n- `AskUserQuestion`\n\n**Example:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\n        \"echo 'Writing file: {{file_path}}'\",\n        \"./hooks/backup.sh {{file_path}}\"\n      ],\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"git add {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\"eslint --fix {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Lifecycle Hooks\n\nExecute during plugin installation/uninstallation.\n\n```json\n{\n  \"onInstall\": [\n    \"./hooks/setup-dependencies.sh\",\n    \"npm install\",\n    \"echo 'Plugin installed successfully'\"\n  ],\n  \"onUninstall\": [\n    \"./hooks/cleanup.sh\",\n    \"echo 'Plugin uninstalled'\"\n  ]\n}\n```\n\n### User Prompt Submit Hook\n\nExecute when user submits a prompt:\n\n```json\n{\n  \"onUserPromptSubmit\": [\n    \"./hooks/log-interaction.sh '{{prompt}}'\",\n    \"./hooks/check-context.sh\"\n  ]\n}\n```\n\n## Hook Variables\n\nHooks have access to context-specific variables using `{{variable}}` syntax.\n\n### Tool Call Variables\n\nDifferent tools provide different variables:\n\n**Write Tool:**\n- `{{file_path}}`: Path to file being written\n- `{{content}}`: Content being written (before hooks only)\n\n**Edit Tool:**\n- `{{file_path}}`: Path to file being edited\n- `{{old_string}}`: String being replaced\n- `{{new_string}}`: Replacement string\n\n**Bash Tool:**\n- `{{command}}`: Command being executed\n\n**Read Tool:**\n- `{{file_path}}`: Path to file being read\n\n### Global Variables\n\nAvailable in all hooks:\n- `{{cwd}}`: Current working directory\n- `{{timestamp}}`: Current Unix timestamp\n- `{{user}}`: Current user\n- `{{plugin_root}}`: Plugin installation directory\n\n### User Prompt Variables\n\n- `{{prompt}}`: User's submitted prompt text\n\n## Hook Examples\n\n### Auto-Format on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Pre-Commit Validation\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-git-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-git-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block force push to main/master\nif [[ \"$COMMAND\" =~ \"git push --force\" ]] && [[ \"$COMMAND\" =~ \"main|master\" ]]; then\n  echo \"ERROR: Force push to main/master is not allowed\"\n  exit 1\nfi\n\nexit 0\n```\n\n### Automatic Backups\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    },\n    \"Edit\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    }\n  }\n}\n```\n\n### Logging and Analytics\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/log-file-change.sh {{file_path}}\"]\n    }\n  },\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh '{{prompt}}'\"]\n}\n```\n\n**log-file-change.sh:**\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Modified: $FILE\" >> .claude/file-changes.log\n```\n\n### Integration with External Tools\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"notify-send 'File Updated' 'Modified {{file_path}}'\",\n        \"curl -X POST https://api.example.com/notify -d 'file={{file_path}}'\"\n      ]\n    }\n  }\n}\n```\n\n## Hook Execution\n\n### Execution Order\n\nMultiple hooks execute in array order:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"echo 'Step 1'\",  // Runs first\n        \"echo 'Step 2'\",  // Runs second\n        \"echo 'Step 3'\"   // Runs third\n      ]\n    }\n  }\n}\n```\n\n### Exit Codes\n\n**Before Hooks:**\n- Exit code `0`: Continue with tool execution\n- Exit code non-zero: **Block tool execution**, show error to user\n\n**After Hooks:**\n- Exit codes are logged but don't affect tool execution\n- Tool has already completed\n\n### Error Handling\n\n```bash\n#!/bin/bash\n\n# Before hook - blocks tool on error\nif [[ ! -f \"$1\" ]]; then\n  echo \"ERROR: File does not exist\"\n  exit 1  # Blocks tool execution\nfi\n\n# Validation passed\nexit 0\n```\n\n## Best Practices\n\n### Keep Hooks Fast\n\nHooks block execution - keep them lightweight:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      // ✅ Fast linter\n      \"after\": [\"eslint --fix {{file_path}}\"]\n\n      // ❌ Slow test suite\n      // \"after\": [\"npm test\"]\n    }\n  }\n}\n```\n\n### Use Absolute Paths\n\nReference scripts with paths relative to plugin:\n\n```json\n{\n  \"onInstall\": [\"${CLAUDE_PLUGIN_ROOT}/hooks/setup.sh\"]\n}\n```\n\n### Validate Input\n\nAlways validate hook variables:\n\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\n\nif [[ -z \"$FILE\" ]]; then\n  echo \"ERROR: No file path provided\"\n  exit 1\nfi\n\nif [[ ! -f \"$FILE\" ]]; then\n  echo \"ERROR: File does not exist: $FILE\"\n  exit 1\nfi\n```\n\n### Provide Clear Feedback\n\n```bash\n#!/bin/bash\n\necho \"Running pre-commit checks...\"\n\nif ! npm run lint; then\n  echo \"❌ Linting failed. Please fix errors before committing.\"\n  exit 1\nfi\n\necho \"✅ All checks passed\"\nexit 0\n```\n\n### Handle Edge Cases\n\n```bash\n#!/bin/bash\n\n# Handle files with spaces in names\nFILE=\"$1\"\n\n# Validate file type\nif [[ ! \"$FILE\" =~ \\.(js|ts|jsx|tsx)$ ]]; then\n  # Skip non-JavaScript files silently\n  exit 0\nfi\n\n# Run formatter\nprettier --write \"$FILE\"\n```\n\n## Security Considerations\n\n### Validate Commands\n\nBefore hooks can block dangerous operations:\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block dangerous patterns\nDANGEROUS_PATTERNS=(\n  \"rm -rf /\"\n  \"dd if=\"\n  \"mkfs\"\n  \"> /dev/sda\"\n)\n\nfor pattern in \"${DANGEROUS_PATTERNS[@]}\"; do\n  if [[ \"$COMMAND\" =~ $pattern ]]; then\n    echo \"ERROR: Dangerous command blocked: $pattern\"\n    exit 1\n  fi\ndone\n\nexit 0\n```\n\n### Limit Hook Scope\n\nOnly hook necessary tools:\n\n```json\n{\n  // ✅ Specific tools only\n  \"onToolCall\": {\n    \"Write\": { \"after\": [\"./format.sh {{file_path}}\"] }\n  }\n\n  // ❌ Don't hook everything unnecessarily\n}\n```\n\n### Sanitize Variables\n\n```bash\n#!/bin/bash\n\n# Sanitize file path\nFILE=$(realpath \"$1\")\n\n# Ensure file is within project\nif [[ ! \"$FILE\" =~ ^$(pwd) ]]; then\n  echo \"ERROR: File outside project directory\"\n  exit 1\nfi\n```\n\n## Debugging Hooks\n\n### Enable Verbose Output\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"set -x; ./hooks/debug.sh {{file_path}}; set +x\"]\n    }\n  }\n}\n```\n\n### Log Hook Execution\n\n```bash\n#!/bin/bash\n\nLOG_FILE=\".claude/hooks.log\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Hook: $0, Args: $@\" >> \"$LOG_FILE\"\n\n# Rest of hook logic...\n```\n\n### Test Hooks Manually\n\n```bash\n# Test hook with sample data\n./hooks/format.sh \"src/main.js\"\n\n# Check exit code\necho $?\n```\n\n## Common Hook Patterns\n\n### Auto-Format Pipeline\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Test on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/run-relevant-tests.sh {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Git Integration\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"git add {{file_path}}\"]\n    },\n    \"Edit\": {\n      \"after\": [\"git add {{file_path}}\"]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n### Hook Not Executing\n\n- Check hook file has execute permissions: `chmod +x hooks/script.sh`\n- Verify path is correct relative to plugin root\n- Check JSON syntax in hooks.json\n- Look for errors in Claude Code logs\n\n### Hook Blocking Tool\n\n- Check exit code of before hooks\n- Add debug logging\n- Test hook script manually\n- Verify validation logic\n\n### Variables Not Substituting\n\n- Check variable name spelling: `{{file_path}}` not `{{filepath}}`\n- Verify variable is available for that tool\n- Quote variables in bash: `\"{{file_path}}\"`\n\n## References\n\nFor more information:\n- Claude Code Hooks Documentation: https://code.claude.com/docs/en/hooks\n- Plugin Configuration: https://code.claude.com/docs/en/plugins#hooks"
              },
              {
                "name": "claude-plugins",
                "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                "path": "claude-code/skills/claude-plugins/SKILL.md",
                "frontmatter": {
                  "name": "claude-plugins",
                  "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin\n\nGuide for creating, validating, and managing plugin.json files for Claude Code plugins. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/plugin.json` files\n- Validating plugin.json schema compliance\n- Setting up new plugin directories\n- Troubleshooting plugin configuration issues\n- Understanding plugin manifest structure\n\n## Plugin Manifest Schema\n\n### File Location\n\nAll plugin manifests must be located at `.claude-plugin/plugin.json` within the plugin directory.\n\n### Complete Schema\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Brief plugin description\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"author@example.com\",\n    \"url\": \"https://github.com/author\"\n  },\n  \"homepage\": \"https://docs.example.com/plugin\",\n  \"repository\": \"https://github.com/author/plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"commands\": [\"./custom/commands/special.md\"],\n  \"agents\": \"./custom/agents/\",\n  \"hooks\": \"./config/hooks.json\",\n  \"mcpServers\": \"./mcp-config.json\",\n  \"skills\": [\"./skills/skill-one\", \"./skills/skill-two\"]\n}\n```\n\n### Required Fields\n\n- `name`: Plugin identifier (kebab-case, lowercase alphanumeric and hyphens only)\n\n### Optional Fields\n\n**Metadata:**\n- `version`: Semantic version number (recommended)\n- `description`: Brief explanation of plugin functionality\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of searchability and categorization tags\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n\n**Author Information:**\n- `author.name`: Creator name\n- `author.email`: Contact email\n- `author.url`: Personal or organization website\n\n**Component Paths:**\n- `skills`: Array of skill directory paths (relative to plugin root)\n- `commands`: String path or array of command file/directory paths\n- `agents`: String path or array of agent file paths\n- `hooks`: String path to hooks.json or hooks configuration object\n- `mcpServers`: String path to MCP config or configuration object\n\n## Field Validation Rules\n\n### name\n- **Format**: kebab-case (lowercase alphanumeric and hyphens only)\n- **Pattern**: `^[a-z0-9]+(-[a-z0-9]+)*$`\n- **Examples**:\n  - Valid: `my-plugin`, `core-skills`, `elixir-tools`\n  - Invalid: `myPlugin`, `my_plugin`, `My-Plugin`, `plugin-`\n\n### version\n- **Format**: Semantic versioning\n- **Pattern**: `^[0-9]+\\.[0-9]+\\.[0-9]+(-[a-zA-Z0-9.-]+)?(\\+[a-zA-Z0-9.-]+)?$`\n- **Examples**:\n  - Valid: `1.0.0`, `2.1.3`, `1.0.0-beta.1`, `1.0.0+build.123`\n  - Invalid: `1.0`, `v1.0.0`, `1.0.0.0`\n\n### license\n- **Format**: SPDX license identifier\n- **Common values**: `MIT`, `Apache-2.0`, `GPL-3.0`, `BSD-3-Clause`, `ISC`\n- **Reference**: https://spdx.org/licenses/\n\n### keywords\n- **Format**: Array of strings\n- **Purpose**: Discoverability, searchability, categorization\n- **Recommendations**: Use lowercase, be specific, include domain terms\n\n### Paths (skills, commands, agents, hooks, mcpServers)\n- **Format**: Relative paths from plugin root\n- **Recommendations**: Use `./` prefix for clarity\n- **Skills**: Array of directory paths containing SKILL.md files\n- **Commands**: Can be string (single path) or array of paths\n- **Agents**: Can be string (directory) or array of file paths\n\n## Invalid Fields in plugin.json\n\nThe following fields are **only valid in marketplace.json** entries and must NOT appear in plugin.json:\n\n- `dependencies`: Dependencies belong in marketplace entries, not plugin manifests\n- `category`: Categorization is marketplace-level metadata\n- `strict`: Controls marketplace behavior, not plugin definition\n- `source`: Plugin location is defined in marketplace, not in plugin itself\n- `tags`: Use `keywords` instead\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\nThis validates:\n- JSON syntax\n- Required field presence (name)\n- Kebab-case naming\n- Field type correctness\n- Path accessibility (for relative paths)\n- Invalid field detection\n\n### 2. Path Validation\n\nValidate that referenced paths exist:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-paths.nu .claude-plugin/plugin.json\n```\n\nChecks:\n- Skills directories exist and contain SKILL.md\n- Command files/directories exist\n- Agent files/directories exist\n- Hooks configuration exists\n- MCP server configuration exists\n\n### 3. Initialization Helper\n\nGenerate a template plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nCreates `.claude-plugin/plugin.json` with proper structure.\n\n## Best Practices\n\n### Naming Conventions\n\n- **Plugin name**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Avoid generic names**: Be specific about the plugin's purpose\n- **Match directory name**: Plugin name should match its directory name\n\n### Versioning Strategy\n\n- Use semantic versioning (MAJOR.MINOR.PATCH)\n- Increment MAJOR for breaking changes\n- Increment MINOR for new features (backward compatible)\n- Increment PATCH for bug fixes\n- Use pre-release tags for beta versions (`1.0.0-beta.1`)\n\n### Path Organization\n\n**Recommended structure:**\n```\nplugin-name/\n├── .claude-plugin/\n│   └── plugin.json\n├── skills/\n│   ├── skill-one/\n│   └── skill-two/\n├── commands/\n└── agents/\n```\n\n**In plugin.json:**\n```json\n{\n  \"skills\": [\n    \"./skills/skill-one\",\n    \"./skills/skill-two\"\n  ],\n  \"commands\": [\"./commands\"],\n  \"agents\": [\"./agents\"]\n}\n```\n\n### Metadata Completeness\n\nAlways include:\n- `version`: Track plugin evolution\n- `description`: Help users understand purpose\n- `license`: Clarify usage terms\n- `keywords`: Improve discoverability\n- `repository`: Enable contributions\n\n### Author Information\n\nInclude contact information for:\n- Bug reports\n- Feature requests\n- Contributions\n- Questions\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Invalid field for plugin.json\n\n```json\n// ❌ Invalid (dependencies only in marketplace.json)\n{\n  \"name\": \"my-plugin\",\n  \"dependencies\": [\"other-plugin\"]\n}\n\n// ✅ Valid\n{\n  \"name\": \"my-plugin\",\n  \"keywords\": [\"tool\", \"utility\"]\n}\n```\n\n### Error: Skill path doesn't exist\n\n```json\n// ❌ Invalid (path not found)\n\"skills\": [\"./skills/nonexistent\"]\n\n// ✅ Valid (path exists with SKILL.md)\n\"skills\": [\"./skills/my-skill\"]\n```\n\n### Error: Invalid version format\n\n```json\n// ❌ Invalid\n\"version\": \"1.0\"\n\"version\": \"v1.0.0\"\n\n// ✅ Valid\n\"version\": \"1.0.0\"\n\"version\": \"2.1.3-beta.1\"\n```\n\n## Creating a New Plugin\n\n### Step 1: Initialize Directory Structure\n\n```bash\nmkdir -p my-plugin/.claude-plugin\nmkdir -p my-plugin/skills\n```\n\n### Step 2: Create plugin.json\n\nUse the initialization script:\n\n```bash\ncd my-plugin\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nOr create manually:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"0.1.0\",\n  \"description\": \"My plugin description\",\n  \"author\": {\n    \"name\": \"Your Name\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"skills\": []\n}\n```\n\n### Step 3: Add Skills\n\n1. Create skill directory: `mkdir -p skills/my-skill`\n2. Create SKILL.md in skill directory\n3. Add to plugin.json:\n\n```json\n{\n  \"skills\": [\"./skills/my-skill\"]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\n### Step 5: Test\n\nInstall locally to test:\n\n```bash\nclaude-code install ./\n```\n\n## Hooks Configuration\n\nHooks can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"hooks\": {\n    \"onInstall\": \"./scripts/install.sh\",\n    \"onUninstall\": \"./scripts/uninstall.sh\"\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"hooks\": \"./config/hooks.json\"\n}\n```\n\n## MCP Servers Configuration\n\nMCP servers can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"mcp-server-filesystem\",\n      \"args\": [\"./workspace\"]\n    }\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"mcpServers\": \"./mcp-config.json\"\n}\n```\n\n## Troubleshooting\n\n### Plugin Not Loading\n\n- Verify plugin.json exists at `.claude-plugin/plugin.json`\n- Check JSON syntax is valid\n- Ensure name field is present and kebab-case\n- Validate all path references exist\n\n### Skills Not Found\n\n- Check skill paths in plugin.json match actual directories\n- Ensure each skill directory contains SKILL.md file\n- Verify paths use relative format (`./skills/name`)\n\n### Commands Not Appearing\n\n- Verify command paths exist\n- Check commands are .md files or directories containing .md files\n- Ensure paths are relative to plugin root\n\n### Validation Fails\n\nRun validation with verbose output:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/plugin-schema.md`: Complete JSON schema specification\n- `references/plugin-examples.md`: Real-world plugin.json examples\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-plugin.nu`: Complete plugin.json validation\n- `validate-plugin-paths.nu`: Verify all referenced paths exist\n- `init-plugin.nu`: Generate plugin.json template\n- `format-plugin.nu`: Format and sort plugin.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              },
              {
                "name": "claude-skills",
                "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                "path": "claude-code/skills/claude-skills/SKILL.md",
                "frontmatter": {
                  "name": "claude-skills",
                  "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Agent Skills\n\nComprehensive guide for creating modular, self-contained Agent Skills that extend Claude's capabilities with specialized knowledge.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new Agent Skills\n- Understanding skill structure and organization\n- Implementing progressive disclosure\n- Organizing skill resources (scripts, references, assets)\n- Following Agent Skills best practices\n\n## What Are Agent Skills?\n\nAgent Skills are organized directories containing instructions, scripts, and resources that Claude can dynamically discover and load. They enable a single general-purpose agent to gain domain-specific expertise without requiring separate custom agents for each use case.\n\n### Key Concepts\n\n- **Modularity**: Self-contained packages that can be mixed and matched\n- **Reusability**: Share and distribute expertise across projects and teams\n- **Progressive Disclosure**: Load context only when needed, keeping interactions efficient\n- **Specialization**: Deep domain knowledge without sacrificing generality\n\n## How Skills Work\n\nSkills operate on a principle of progressive disclosure across multiple levels:\n\n### Level 1: Discovery\nAgent system prompts include only skill names and descriptions, allowing Claude to decide when each skill is relevant based on the task at hand.\n\n### Level 2: Activation\nWhen Claude determines a skill applies, it loads the full `SKILL.md` file into context, gaining access to the complete procedural knowledge and guidelines.\n\n### Level 3+: Deep Context\nAdditional bundled files (like references, forms, or documentation) load only when needed for specific scenarios, keeping token usage efficient.\n\nThis tiered approach maintains efficient context windows while supporting potentially unbounded skill complexity.\n\n## Skill Structure\n\n### Minimal Requirements\n\nEvery skill must have:\n\n```\nskill-name/\n└── SKILL.md\n```\n\n### Complete Structure\n\nMore complex skills can include additional resources:\n\n```\nskill-name/\n├── SKILL.md           # Required: Core skill definition\n├── scripts/           # Optional: Executable code for deterministic tasks\n├── references/        # Optional: Documentation loaded on-demand\n└── assets/            # Optional: Templates, images, boilerplate\n```\n\n## SKILL.md Format\n\nEach `SKILL.md` file must begin with YAML frontmatter followed by Markdown content:\n\n```markdown\n---\nname: skill-name\ndescription: Concise explanation of when Claude should use this skill\nlicense: MIT\n---\n\n# Skill Name\n\nMain instructional content goes here...\n```\n\n### Required YAML Properties\n\n- `name`: Hyphen-case identifier matching directory name (lowercase alphanumeric and hyphens only)\n- `description`: Explains the skill's purpose and when Claude should utilize it\n\n### Optional YAML Properties\n\n- `license`: License name or filename reference\n- `allowed-tools`: Pre-approved tools list (Claude Code support only)\n- `metadata`: Key-value string pairs for client-specific properties\n\n### Markdown Body\n\nThe content section has no restrictions and should contain:\n\n- When to activate the skill\n- Core procedural knowledge\n- Best practices and guidelines\n- Examples and patterns\n- References to additional resources (if any)\n\n## Creating Skills: Seven-Step Workflow\n\n### 1. Understanding Through Examples\n\nGather concrete use cases to clarify what the skill should support. Real-world examples reveal actual needs better than theoretical requirements.\n\n**Example:**\n```\nUse Case: Help developers follow Git best practices\nExamples:\n- Creating conventional commit messages\n- Rebasing feature branches\n- Resolving merge conflicts\n- Creating descriptive branch names\n```\n\n### 2. Planning Resources\n\nAnalyze examples to identify needed components:\n\n- **Scripts**: For tasks requiring deterministic reliability or that would need repeated rewriting\n- **References**: Documentation to load into context as needed\n- **Assets**: Output files like templates or boilerplate (not loaded into context)\n\n**Example:**\n```\nGit skill resources:\n- scripts/analyze-commit.sh - Parse git diff for commit message\n- references/conventional-commits.md - Detailed commit format spec\n- assets/gitignore-templates/ - Common .gitignore files\n```\n\n### 3. Initialization\n\nCreate the skill directory structure with the required `SKILL.md` file. Ensure the directory name matches the `name` property exactly.\n\n```bash\nmkdir -p my-skill/{scripts,references,assets}\ntouch my-skill/SKILL.md\n```\n\n### 4. Editing\n\nDevelop resource files and update `SKILL.md` with:\n- Purpose and activation criteria\n- Usage guidelines and best practices\n- Implementation details and examples\n- References to supplementary files\n\n**Use imperative/infinitive form** rather than second-person instruction for clarity.\n\n✅ Good: \"Follow the Conventional Commits specification\"\n✅ Good: \"Use descriptive branch names with type prefixes\"\n❌ Avoid: \"You should try to use descriptive names when possible\"\n\nKeep core procedural information in `SKILL.md` and detailed reference material in separate files.\n\n### 5. Documentation\n\n**Document all sources in the plugin's `sources.md`**. For each skill created, record:\n- URLs of documentation, guides, and references used\n- Purpose of each source\n- Key topics and concepts extracted\n- Date accessed (if relevant)\n\nThis maintains traceability and helps others understand the skill's foundation.\n\n### 6. Validation\n\nTest the skill with representative scenarios to ensure:\n- Claude activates it appropriately\n- Instructions are clear and actionable\n- Progressive disclosure works effectively\n- Token usage remains efficient\n\n### 7. Iteration\n\nRefine based on real-world usage feedback. Monitor how Claude actually uses the skill and adjust the description and content accordingly.\n\n## Best Practices\n\n### Start with Evaluation\n\nIdentify specific capability gaps by testing agents on representative tasks. Build skills incrementally to address actual shortcomings rather than anticipated needs.\n\n### Structure for Scale\n\nSplit unwieldy `SKILL.md` files into separate referenced documents:\n- Keep commonly-used contexts together\n- Separate mutually exclusive information to reduce token usage\n- Use progressive disclosure to load details only when needed\n\n**Example:**\n```markdown\n# Git Skill\n\nFor detailed conventional commit format, see references/conventional-commits.md\nFor rebase workflow, see references/rebasing-guide.md\n```\n\n### Consider Claude's Perspective\n\nThe skill name and description heavily influence when Claude activates it. Pay particular attention to:\n\n- **Name**: Should be clear and reflect the domain (e.g., `git-operations`, `elixir-phoenix`)\n- **Description**: Should specify both what the skill does and when to use it\n\n**Examples:**\n\n✅ Good Description:\n```yaml\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\n```\n\n❌ Too Vague:\n```yaml\ndescription: Helps with Git\n```\n\nMonitor real usage patterns and iterate based on actual behavior.\n\n### Iterate Collaboratively\n\nWork with Claude to capture successful approaches and common mistakes into reusable skill components. Ask Claude to self-reflect on what contextual information actually matters.\n\n### Write for AI Consumption\n\nUse clear, imperative language that Claude can follow:\n\n✅ Good:\n- \"Follow the Conventional Commits specification\"\n- \"Use descriptive branch names with type prefixes\"\n- \"Run tests before committing\"\n\n❌ Avoid:\n- \"You should try to use descriptive names when possible\"\n- \"It might be good to run tests\"\n- \"Consider following best practices\"\n\nInclude concrete examples wherever possible to illustrate patterns and approaches.\n\n### Security Considerations\n\nInstall skills only from trusted sources. When evaluating unfamiliar skills:\n- Thoroughly audit bundled files and scripts\n- Review code dependencies\n- Examine instructions directing Claude to connect with external services\n- Verify the skill doesn't request sensitive information or dangerous operations\n\n## Anti-Fabrication Requirements\n\nAll skills MUST adhere to strict anti-fabrication requirements to ensure factual, measurable content.\n\n### Core Principles\n\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Execute tests before marking tasks complete and report actual results\n- Validate integration recommendations through actual framework detection using tool analysis\n\n### Prohibited Language and Claims\n\n- **Superlatives**: Avoid \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- **Unsubstantiated Metrics**: Never fabricate percentages, success rates, or performance numbers\n- **Assumed Capabilities**: Don't claim features exist without tool verification\n- **Generic Claims**: Replace vague statements with specific, measurable observations\n- **Fabricated Testing**: Never report test results without actual execution\n\n### Time and Effort Estimation Rule\n\n- Never provide time estimates, effort estimates, or completion timelines without actual measurement or analysis\n- If estimates are requested, execute tools to analyze scope (e.g., count files, measure complexity, assess dependencies) before providing data-backed estimates\n- When estimates cannot be measured, explicitly state \"timeline requires analysis of [specific factors]\"\n- Avoid fabricated scheduling language like \"15 minutes\", \"2 hours\", \"quick task\" without factual basis\n\n### Validation Requirements\n\n- **File Claims**: Use Read or Glob tools before claiming files exist or contain specific content\n- **System Integration**: Use Bash or appropriate tools to verify system capabilities\n- **Framework Detection**: Execute actual detection logic before claiming framework presence\n- **Test Results**: Only report test outcomes after actual execution with tool verification\n- **Performance Claims**: Base any performance statements on actual measurement or analysis\n\n## Skill Examples\n\n### Simple Skill (Git)\n\n```markdown\n---\nname: git\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\nlicense: MIT\n---\n\n# Git Operations\n\n## When to Use\n\nActivate when:\n- Creating commit messages\n- Managing branches\n- Resolving conflicts\n- Rebasing or merging\n\n## Conventional Commits\n\nFollow the format: `type(scope): description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n\nExample: `feat(auth): add OAuth2 login support`\n\n## Branch Naming\n\nUse format: `type/description`\n\nExamples:\n- `feature/user-authentication`\n- `fix/memory-leak`\n- `docs/api-reference`\n\n## Rebasing Workflow\n\n1. Update main: `git checkout main && git pull`\n2. Rebase feature: `git checkout feature-branch && git rebase main`\n3. Resolve conflicts if needed\n4. Force push: `git push --force-with-lease`\n```\n\n### Complex Skill (Phoenix)\n\n```markdown\n---\nname: phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and best practices\nlicense: MIT\n---\n\n# Phoenix Framework\n\n## When to Use\n\nActivate for:\n- Phoenix application development\n- LiveView implementations\n- Context design\n- Channel setup\n\n## Project Structure\n\nPhoenix apps follow:\n```\nlib/\n├── my_app/          # Business logic (contexts)\n├── my_app_web/      # Web interface\n└── my_app.ex\n```\n\n## Contexts\n\nGroup related functionality:\n\n```elixir\ndefmodule MyApp.Accounts do\n  def list_users, do: Repo.all(User)\n  def get_user!(id), do: Repo.get!(User, id)\n  def create_user(attrs), do: ...\nend\n```\n\nFor detailed context patterns, see references/contexts.md\n\n## LiveView\n\nFor real-time interfaces, see references/liveview-guide.md\n```\n\n## Common Pitfalls\n\n### Too Generic\n\n❌ Avoid:\n```yaml\nname: programming\ndescription: Helps with programming\n```\n\n✅ Better:\n```yaml\nname: elixir-phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and Elixir best practices\n```\n\n### Too Much in SKILL.md\n\n❌ Avoid putting entire API reference in SKILL.md\n\n✅ Better: Keep core patterns in SKILL.md, detailed reference in `references/`\n\n### Missing Activation Criteria\n\n❌ Avoid:\n```markdown\n# My Skill\n\nThis skill helps with stuff.\n```\n\n✅ Better:\n```markdown\n# My Skill\n\n## When to Use\n\nActivate when:\n- Specific scenario 1\n- Specific scenario 2\n- Specific scenario 3\n```\n\n## References\n\nFor more information:\n- **Agent Skills Blog**: https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\n- **Example Skills**: https://github.com/anthropics/skills\n- **Skills Cookbook**: https://github.com/anthropics/claude-cookbooks/tree/main/skills\n- **Skill Creator Guide**: https://github.com/anthropics/skills/blob/main/skill-creator/SKILL.md\n- **Agent Skills Specification**: https://github.com/anthropics/skills/blob/main/agent_skills_spec.md"
              },
              {
                "name": "plugin-marketplace",
                "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                "path": "claude-code/skills/plugin-marketplace/SKILL.md",
                "frontmatter": {
                  "name": "plugin-marketplace",
                  "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin Marketplace\n\nGuide for creating, validating, and managing plugin marketplaces for Claude Code. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/marketplace.json` files\n- Validating marketplace schema compliance\n- Setting up plugin repositories with marketplaces\n- Troubleshooting marketplace configuration issues\n- Converting plugin structures to marketplace format\n- Creating plugin entries with advanced features\n\n## Marketplace Schema Overview\n\n### Required Structure\n\nAll marketplaces must be located at `.claude-plugin/marketplace.json` in the repository root.\n\n**Required Fields:**\n- `name`: Marketplace identifier (kebab-case, lowercase alphanumeric and hyphens only)\n- `owner`: Object with maintainer details (`name` required, `email` optional)\n- `plugins`: Array of plugin definitions (can be empty)\n\n**Optional Metadata:**\n- `metadata.description`: Summary of marketplace purpose\n- `metadata.version`: Marketplace version tracking (semantic versioning recommended)\n- `metadata.pluginRoot`: Base directory for relative plugin source paths\n\n### Plugin Entry Schema\n\n**IMPORTANT: Schema Relationship**\n\nPlugin entries use the plugin manifest schema with all fields made optional, plus marketplace-specific fields (`source`, `strict`, `category`, `tags`). This means any field valid in a plugin.json file can also be used in a marketplace entry.\n\n- When `strict: false`, the marketplace entry serves as the complete plugin manifest if no plugin.json exists\n- When `strict: true` (default), marketplace fields supplement the plugin's own manifest file\n\nEach plugin entry in the `plugins` array requires:\n\n**Mandatory:**\n- `name`: Plugin identifier (kebab-case)\n- `source`: Location specification (string path or object)\n\n**Standard Metadata:**\n- `description`: Brief explanation of plugin functionality\n- `version`: Semantic version number\n- `author`: Creator information (object with `name`, optional `email`)\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of discovery and categorization tags\n- `category`: Organizational grouping\n- `tags`: Additional searchability terms\n\n**Component Configuration:**\n- `commands`: Custom paths to command files or directories\n- `agents`: Custom paths to agent files\n- `hooks`: Custom hooks configuration or path to hooks file\n- `mcpServers`: MCP server configurations or path to MCP config\n- `skills`: Array of skill directory paths\n\n**Strict Mode Control:**\n- `strict`: Boolean (default: `true`)\n  - `true`: Plugin must include plugin.json; marketplace fields supplement it\n  - `false`: Marketplace entry serves as complete manifest (no plugin.json needed)\n\n**Dependencies:**\n- `dependencies`: Array of plugin names this plugin depends on (format: `\"namespace:plugin-name\"`)\n\n## Plugin Source Formats\n\n### Relative Path\n```json\n\"source\": \"./plugins/my-plugin\"\n```\n\n### Relative Path with pluginRoot\n```json\n// In marketplace metadata\n\"metadata\": {\n  \"pluginRoot\": \"./plugins\"\n}\n\n// In plugin entry\n\"source\": \"my-plugin\"  // Resolves to ./plugins/my-plugin\n```\n\n### GitHub Repository\n```json\n\"source\": {\n  \"source\": \"github\",\n  \"repo\": \"owner/plugin-repo\",\n  \"path\": \"optional/subdirectory\",\n  \"branch\": \"main\"\n}\n```\n\n### Git URL\n```json\n\"source\": {\n  \"source\": \"url\",\n  \"url\": \"https://gitlab.com/team/plugin.git\",\n  \"branch\": \"main\"\n}\n```\n\n## Environment Variables\n\nUse `${CLAUDE_PLUGIN_ROOT}` in paths to reference the plugin's installation directory:\n\n```json\n{\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/my-skill\"\n  ],\n  \"commands\": [\n    \"${CLAUDE_PLUGIN_ROOT}/commands\"\n  ]\n}\n```\n\nThis ensures paths work correctly regardless of installation location.\n\n## Advanced Plugin Entry Features\n\n### Inline Plugin Definitions\n\nUse `strict: false` to define complete plugin manifests inline without requiring plugin.json:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"source\": \"./plugins/my-plugin\",\n  \"strict\": false,\n  \"description\": \"Complete plugin definition inline\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Developer Name\"\n  },\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\",\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-two\"\n  ]\n}\n```\n\n### Component Path Override\n\nCustomize component locations:\n\n```json\n{\n  \"name\": \"custom-paths\",\n  \"source\": \"./plugins/custom\",\n  \"strict\": false,\n  \"commands\": [\"${CLAUDE_PLUGIN_ROOT}/custom-commands\"],\n  \"agents\": [\"${CLAUDE_PLUGIN_ROOT}/custom-agents\"],\n  \"hooks\": {\n    \"onInstall\": \"${CLAUDE_PLUGIN_ROOT}/hooks/install.sh\"\n  },\n  \"mcpServers\": \"${CLAUDE_PLUGIN_ROOT}/mcp-config.json\"\n}\n```\n\n### Metadata Supplementation\n\nWith `strict: true`, marketplace entries can add metadata not in plugin.json:\n\n```json\n{\n  \"name\": \"existing-plugin\",\n  \"source\": \"./plugins/existing\",\n  \"strict\": true,\n  \"category\": \"development\",\n  \"keywords\": [\"added\", \"from\", \"marketplace\"],\n  \"homepage\": \"https://docs.example.com\"\n}\n```\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate marketplace.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\nThis validates:\n- JSON syntax\n- Required fields presence\n- Kebab-case naming\n- Field type correctness\n- Source path accessibility (for relative paths)\n\n### 2. Plugin Entry Validation\n\nValidate individual plugin entries:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-entry.nu .claude-plugin/marketplace.json \"plugin-name\"\n```\n\nChecks:\n- Required fields (name, source)\n- Strict mode consistency\n- Dependency references\n- Path validity\n- Component configuration\n\n### 3. Dependency Graph Validation\n\nCheck for circular dependencies and missing dependencies:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-dependencies.nu .claude-plugin/marketplace.json\n```\n\n## Best Practices\n\n### Naming Conventions\n\n- **Marketplace name**: Use your GitHub username or organization (e.g., `vinnie357`)\n- **Plugin names**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Categories**: Standardize on common categories: `development`, `language`, `tools`, `frontend`, `backend`, `meta`\n\n### Versioning Strategy\n\n- Use semantic versioning for both marketplace and plugins\n- Bump marketplace version when adding/removing plugins\n- Bump plugin versions when updating skills or configuration\n- Document breaking changes in plugin descriptions\n\n### Dependency Management\n\n- Always declare `dependencies` for plugins that require other plugins\n- Keep dependency chains shallow (avoid deep nesting)\n- Consider creating a meta-plugin (like `all-skills`) that bundles related plugins\n- Use namespace prefixes for dependencies (e.g., `all-skills:core`)\n\n### Strict Mode Decision\n\n**Use `strict: false` when:**\n- Creating simple, self-contained plugins\n- All configuration is in marketplace.json\n- You want centralized management\n- Plugin is unlikely to be distributed independently\n\n**Use `strict: true` when:**\n- Plugin has complex configuration\n- Plugin may be distributed separately\n- Plugin has its own versioning lifecycle\n- You want to supplement existing plugin.json with marketplace metadata\n\n### Source Path Organization\n\n```json\n{\n  \"metadata\": {\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"core\",\n      \"source\": \"core\"  // Resolves to ./plugins/core\n    },\n    {\n      \"name\": \"external\",\n      \"source\": {\n        \"source\": \"github\",\n        \"repo\": \"org/repo\"\n      }\n    }\n  ]\n}\n```\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Missing required owner field\n\n```json\n// ❌ Invalid\n{\n  \"name\": \"marketplace\"\n}\n\n// ✅ Valid\n{\n  \"name\": \"marketplace\",\n  \"owner\": {\n    \"name\": \"Developer Name\"\n  }\n}\n```\n\n### Error: Invalid source path\n\n```json\n// ❌ Invalid (path doesn't exist)\n\"source\": \"./plugins/nonexistent\"\n\n// ✅ Valid (path exists)\n\"source\": \"./plugins/core\"\n```\n\n### Error: Circular dependencies\n\n```json\n// ❌ Invalid\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-a\",\n      \"dependencies\": [\"namespace:plugin-b\"]\n    },\n    {\n      \"name\": \"plugin-b\",\n      \"dependencies\": [\"namespace:plugin-a\"]\n    }\n  ]\n}\n```\n\n## Creating a New Marketplace\n\n### Step 1: Initialize Structure\n\n```bash\nmkdir -p .claude-plugin\n```\n\n### Step 2: Create Marketplace File\n\nUse the validation script to generate a template:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-marketplace.nu\n```\n\nThis creates `.claude-plugin/marketplace.json` with required fields.\n\n### Step 3: Add Plugin Entries\n\nFor each plugin, decide on strict mode and add entry:\n\n```json\n{\n  \"name\": \"marketplace-name\",\n  \"owner\": {\n    \"name\": \"Your Name\",\n    \"email\": \"you@example.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Your marketplace description\",\n    \"version\": \"1.0.0\",\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"source\": \"plugin-name\",\n      \"strict\": false,\n      \"description\": \"Plugin description\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Your Name\"\n      },\n      \"license\": \"MIT\",\n      \"category\": \"development\",\n      \"skills\": [\n        \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\"\n      ]\n    }\n  ]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\n### Step 5: Test Installation\n\n```bash\nclaude-code install ./\n```\n\n## Migrating Existing Plugins\n\n### From Individual Plugins to Marketplace\n\n1. **Identify plugins**: List all plugin.json files\n2. **Decide on strict mode**: Choose per plugin based on complexity\n3. **Create marketplace.json**: Add all plugins with appropriate configuration\n4. **Test each plugin**: Verify installation works correctly\n5. **Document dependencies**: Add dependency arrays where needed\n\n### Migration Script\n\nUse the provided script to analyze existing structure:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/analyze-plugins.nu .\n```\n\nThis scans for plugin.json files and suggests marketplace.json structure.\n\n## Troubleshooting\n\n### Plugin Not Found After Installation\n\n- Verify `source` path is correct\n- Check `pluginRoot` in metadata if using relative paths\n- Ensure plugin directory exists at specified location\n\n### Skills Not Loading\n\n- Verify skill paths use `${CLAUDE_PLUGIN_ROOT}` if needed\n- Check that skill directories contain SKILL.md files\n- Validate skill paths in plugin entry or plugin.json\n\n### Dependency Resolution Fails\n\n- Ensure dependency names match exactly (including namespace)\n- Check that all dependencies are listed in marketplace\n- Verify no circular dependencies exist\n\n### Validation Errors\n\nRun validation script with verbose mode:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/schema-specification.md`: Complete JSON schema\n- `references/examples.md`: Real-world marketplace examples\n- `references/migration-guide.md`: Step-by-step migration instructions\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-marketplace.nu`: Full marketplace validation\n- `validate-plugin-entry.nu`: Individual plugin entry validation\n- `validate-dependencies.nu`: Dependency graph validation\n- `init-marketplace.nu`: Generate marketplace template\n- `analyze-plugins.nu`: Analyze existing plugin structure\n- `format-marketplace.nu`: Format and sort marketplace.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              }
            ]
          },
          {
            "name": "core",
            "description": "Essential development skills: Git, documentation, code review, accessibility",
            "source": "./core",
            "category": "development",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install core@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/gcms",
                "description": "Generate brief conventional commit message suggestions",
                "path": "core/commands/gcms.md",
                "frontmatter": {
                  "description": "Generate brief conventional commit message suggestions",
                  "argument-hint": ""
                },
                "content": "Please analyze the current git status and suggest 1-3 brief conventional commit messages based on the staged and unstaged changes. Use the git-commit-message subagent to perform this analysis."
              },
              {
                "name": "/research",
                "description": "Research topics and create comprehensive planning documentation",
                "path": "core/commands/research.md",
                "frontmatter": {
                  "description": "Research topics and create comprehensive planning documentation",
                  "argument-hint": "<category> <topic> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create comprehensive documentation for planning, understanding, and working with the subject.\n\n**Document Creation:**\n- **Directory Structure**: Creates `research/<category>/<topic>/`\n- **File Generation**: Produces `overview.md`, `troubleshooting.md`, and optional guides\n- **Planning Focus**: Helps understand topics before implementation\n- **Reference Material**: Creates searchable knowledge base\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **Structured Content**: Consistent templates for reliability\n- **Authoritative Sources**: Links to official docs and best practices\n- **Practical Examples**: Real-world usage patterns and code samples\n\n**Examples:**\n```\n/research development docker-compose\n# Creates: research/development/docker-compose/\n#          - overview.md\n#          - troubleshooting.md\n\n/research infrastructure kubernetes-networking --complexity=high\n# Creates: research/infrastructure/kubernetes-networking/\n#          - overview.md (with deep analysis)\n#          - troubleshooting.md\n#          - best-practices.md\n\n/research frontend react-state-management --complexity=medium\n# Creates: research/frontend/react-state-management/\n#          - overview.md\n#          - troubleshooting.md\n#          - comparison.md (Redux vs Context vs Zustand)\n```\n\n**Document Structure:**\n\n### overview.md\n- **Purpose & Use Cases**: When and why to use this technology\n- **Core Concepts**: Fundamental principles and architecture\n- **Implementation Patterns**: Common approaches and best practices\n- **Code Examples**: Practical, runnable examples\n- **Integration Guidelines**: How it fits into larger systems\n- **Performance Considerations**: Optimization and scaling\n- **Security**: Common vulnerabilities and protections\n- **Resources**: Official docs, tutorials, community resources\n\n### troubleshooting.md\n- **Common Issues**: Frequently encountered problems\n- **Error Messages**: Interpretation and solutions\n- **Diagnostic Tools**: How to investigate problems\n- **Solutions**: Step-by-step fixes\n- **Prevention**: How to avoid issues\n- **Escalation**: When and where to get help\n\n### Additional Files (generated as needed)\n- `best-practices.md`: Detailed guidelines and patterns\n- `comparison.md`: Technology alternatives and trade-offs\n- `migration.md`: Upgrade paths and migration strategies\n- `quick-start.md`: Fast setup and basic usage\n\n**Quality Standards:**\n- Use authoritative sources (official docs, RFCs, reputable blogs)\n- Include version information where relevant\n- Provide working code examples\n- Link to external resources\n- Note common pitfalls and gotchas\n- Include date of research for freshness tracking\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. **Research Phase**:\n   - Search authoritative sources and documentation\n   - Gather best practices and common patterns\n   - Identify common issues and solutions\n   - Collect practical examples\n\n2. **Structure Phase**:\n   - Create `research/<category>/<topic>/` directory\n   - Determine which documents are needed based on topic\n   - Plan document organization\n\n3. **Content Generation**:\n   - Write comprehensive `overview.md` with:\n     * Clear purpose and use cases\n     * Core concepts and architecture\n     * Implementation patterns\n     * Practical examples\n     * Integration guidance\n   - Write practical `troubleshooting.md` with:\n     * Common issues and solutions\n     * Error message interpretations\n     * Diagnostic approaches\n     * Prevention strategies\n   - Generate additional guides as needed\n\n4. **Quality Assurance**:\n   - Verify all sources are authoritative\n   - Ensure examples are practical and correct\n   - Check for completeness and clarity\n   - Add timestamps and version info\n\nThe agent should produce comprehensive, actionable documentation that helps users understand and work with the topic effectively."
              }
            ],
            "skills": [
              {
                "name": "accessibility",
                "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces",
                "path": "core/skills/accessibility/SKILL.md",
                "frontmatter": {
                  "name": "accessibility",
                  "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces"
                },
                "content": "# Web Accessibility\n\nApply W3C Web Accessibility Initiative (WAI) principles when working on web interfaces to ensure usability for people with disabilities.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing user interfaces\n- Reviewing code for accessibility compliance\n- Creating or editing web content (HTML, CSS, JavaScript)\n- Working with forms, navigation, multimedia, or interactive components\n- Conducting code reviews with accessibility considerations\n- Refactoring existing interfaces for better accessibility\n\n## Core Principles (POUR)\n\nWeb accessibility is organized around four foundational principles:\n\n### 1. Perceivable\n\nInformation must be presentable to users in ways they can perceive.\n\n**Key requirements:**\n- Provide text alternatives for non-text content (images, icons, charts)\n- Provide captions and transcripts for multimedia\n- Create content that can be presented in different ways (responsive, reflow)\n- Make content distinguishable (color contrast, text sizing, audio control)\n\n**Quick example:**\n```html\n<img src=\"chart.png\" alt=\"Sales increased 40% in Q4 2024\">\n<button aria-label=\"Close dialog\">\n  <span class=\"icon-close\" aria-hidden=\"true\"></span>\n</button>\n```\n\nFor detailed guidance on text alternatives, multimedia, and color contrast, see `references/perceivable.md`.\n\n### 2. Operable\n\nUser interface components must be operable by all users.\n\n**Key requirements:**\n- Make all functionality keyboard accessible\n- Provide sufficient time for users to complete tasks\n- Avoid content that causes seizures (no rapid flashing)\n- Help users navigate and find content\n- Support various input modalities (touch, voice, keyboard)\n\n**Quick example:**\n```html\n<button>Click me</button>  <!-- Already keyboard accessible -->\n\n<!-- Custom interactive element needs keyboard support -->\n<div role=\"button\" tabindex=\"0\"\n     onclick=\"handleClick()\"\n     onkeydown=\"handleKeyDown(event)\">\n  Custom Button\n</div>\n```\n\nFor keyboard patterns, focus management, and navigation, see `references/operable.md`.\n\n### 3. Understandable\n\nInformation and UI operation must be understandable.\n\n**Key requirements:**\n- Make text readable and understandable\n- Make web pages appear and operate predictably\n- Help users avoid and correct mistakes\n- Provide clear labels and instructions\n\n**Quick example:**\n```html\n<html lang=\"en\">\n<label for=\"email\">Email address</label>\n<input type=\"email\" id=\"email\"\n       aria-describedby=\"email-help\"\n       required>\n<div id=\"email-help\">We'll never share your email</div>\n```\n\nFor form patterns, error handling, and content clarity, see `references/understandable.md`.\n\n### 4. Robust\n\nContent must work reliably across user agents and assistive technologies.\n\n**Key requirements:**\n- Use valid, well-formed markup\n- Ensure compatibility with assistive technologies\n- Use ARIA correctly for custom components\n- Follow semantic HTML practices\n\n**Quick example:**\n```html\n<!-- Use semantic HTML first -->\n<nav aria-label=\"Main navigation\">\n  <ul>\n    <li><a href=\"/\">Home</a></li>\n  </ul>\n</nav>\n\n<!-- ARIA for custom components when needed -->\n<div role=\"dialog\" aria-labelledby=\"title\" aria-modal=\"true\">\n  <h2 id=\"title\">Dialog Title</h2>\n</div>\n```\n\nFor ARIA patterns and custom components, see `references/robust.md`.\n\n## Common Tasks\n\n### Making Forms Accessible\n\nConsult `references/forms.md` for comprehensive form accessibility including:\n- Label association\n- Error identification and suggestions\n- Required field indication\n- Input validation patterns\n\n### Implementing ARIA\n\nSee `references/aria.md` for:\n- When to use ARIA vs semantic HTML\n- Common ARIA patterns (tabs, accordions, modals)\n- ARIA states and properties\n- Live regions for dynamic content\n\n### Testing for Accessibility\n\nConsult `references/testing.md` for:\n- Keyboard navigation testing\n- Screen reader testing procedures\n- Automated testing tools\n- Color contrast checking\n\n### Common Patterns\n\nSee `references/patterns.md` for accessible implementations of:\n- Modal dialogs\n- Dropdown menus\n- Tabs and accordions\n- Loading states and notifications\n- Skip links and landmarks\n\n## Quick Reference Checklist\n\n**Every page should have:**\n- [ ] Valid HTML structure\n- [ ] Unique, descriptive page title\n- [ ] Proper heading hierarchy (h1, h2, h3...)\n- [ ] Language attribute on `<html>`\n- [ ] Sufficient color contrast (4.5:1 minimum)\n- [ ] Keyboard accessibility for all interactive elements\n- [ ] Visible focus indicators\n- [ ] Text alternatives for images\n- [ ] Form labels associated with inputs\n- [ ] Semantic landmark regions\n\n**For interactive components:**\n- [ ] Keyboard accessible (Tab, Enter, Space, Arrow keys)\n- [ ] Proper ARIA roles, states, and properties\n- [ ] Focus management (modals, dynamic content)\n- [ ] Descriptive labels and instructions\n- [ ] Error messages linked to form controls\n\n## Key Principles\n\n- **Semantic HTML first**: Use native HTML elements before adding ARIA\n- **Keyboard accessibility is fundamental**: If it works with mouse, it must work with keyboard\n- **Test with actual users**: Include people with disabilities in testing\n- **Color is not enough**: Never use color alone to convey information\n- **Provide alternatives**: Text for images, captions for video, transcripts for audio\n- **Make it predictable**: Consistent navigation and behavior across pages\n- **Help users recover**: Clear error messages with suggestions for correction\n\n## Resources\n\n- **WCAG 2.1 Guidelines**: https://www.w3.org/WAI/WCAG21/quickref/\n- **ARIA Authoring Practices**: https://www.w3.org/WAI/ARIA/apg/\n- **WebAIM**: https://webaim.org/\n- **MDN Accessibility**: https://developer.mozilla.org/en-US/docs/Web/Accessibility"
              },
              {
                "name": "anti-fabrication",
                "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                "path": "core/skills/anti-fabrication/SKILL.md",
                "frontmatter": {
                  "name": "anti-fabrication",
                  "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                  "license": "MIT"
                },
                "content": "# Anti-Fabrication\n\nStrict requirements for ensuring factual, measurable, and validated outputs in all work products including documentation, research, reports, and analysis.\n\n## When to Use This Skill\n\nActivate when:\n- Writing documentation or creating research materials\n- Making claims about system capabilities, performance, or features\n- Providing estimates for time, effort, or complexity\n- Reporting test results or analysis outcomes\n- Creating any content that presents factual information\n- Generating metrics, statistics, or performance data\n\n## Core Principles\n\n### Evidence-Based Outputs\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Never assume file existence, system capabilities, or feature presence without verification\n- Validate integration recommendations through actual framework detection\n\n### Explicit Uncertainty\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- State when information cannot be verified: \"Unable to confirm without [specific check]\"\n- Acknowledge knowledge limitations rather than fabricating plausible-sounding content\n- Use conditional language when appropriate: \"may\", \"likely\", \"appears to\"\n\n### Factual Language\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Replace vague statements with specific, measurable observations\n- Report what was actually observed, not what should theoretically be true\n- Distinguish between verified facts and reasonable inferences\n\n## Prohibited Language and Claims\n\n### Superlatives to Avoid\nNever use unverified superlatives:\n- ❌ \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- ❌ \"best practice\", \"industry-leading\", \"cutting-edge\", \"state-of-the-art\"\n- ❌ \"robust\", \"scalable\", \"production-ready\" (without specific evidence)\n\nInstead, use factual descriptions:\n- ✅ \"follows the specification defined in [source]\"\n- ✅ \"implements [specific pattern] as documented in [reference]\"\n- ✅ \"tested with [specific conditions] and produced [specific results]\"\n\n### Unsubstantiated Metrics\nNever fabricate quantitative data:\n- ❌ Percentages without measurement: \"improves performance by 30%\"\n- ❌ Success rates without testing: \"has a 95% success rate\"\n- ❌ Arbitrary scores: \"code quality score of 8/10\"\n- ❌ Made-up statistics: \"reduces memory usage significantly\"\n\nInstead, provide verified measurements:\n- ✅ \"benchmark shows execution time decreased from 150ms to 98ms\"\n- ✅ \"passed 47 of 50 test cases (94%)\"\n- ✅ \"static analysis tool reports complexity score of 12\"\n\n### Assumed Capabilities\nNever claim features exist without verification:\n- ❌ \"This system supports authentication\" (without checking)\n- ❌ \"The API provides rate limiting\" (without reading docs/code)\n- ❌ \"This handles edge cases correctly\" (without testing)\n\nInstead, verify before claiming:\n- ✅ Use Read tool to check configuration files\n- ✅ Use Grep to search for specific implementations\n- ✅ Use Bash to test actual behavior\n- ✅ State \"requires verification\" if tools cannot confirm\n\n## Time and Effort Estimation Rules\n\n### Never Estimate Without Analysis\nDo not provide time estimates without factual basis:\n- ❌ \"This will take 15 minutes\"\n- ❌ \"Should be done in 2 hours\"\n- ❌ \"Quick task, won't take long\"\n- ❌ \"Simple fix\"\n\n### Data-Backed Estimates Only\nIf estimates are requested, execute tools first:\n1. Count files that need modification (using Glob)\n2. Measure code complexity (using Read and analysis)\n3. Assess dependencies (using Grep for imports/references)\n4. Review similar past work (if available)\n\nThen provide estimate with evidence:\n- ✅ \"Requires modifying 12 files based on grep search, estimated X hours\"\n- ✅ \"Analysis shows 3 integration points, complexity suggests Y time\"\n- ✅ \"Timeline requires analysis of [specific factors not yet measured]\"\n\n### When Unable to Estimate\nBe explicit about limitations:\n- ✅ \"Cannot provide time estimate without analyzing [specific aspects]\"\n- ✅ \"Requires investigation of [X, Y, Z] before estimating\"\n- ✅ \"Complexity assessment needed before timeline projection\"\n\n## Validation Requirements\n\n### File Claims\nBefore claiming files exist or contain specific content:\n```\n1. Use Read tool to verify file exists and check contents\n2. Use Glob to find files matching patterns\n3. Use Grep to verify specific code or content is present\n4. Never state \"file X contains Y\" without tool verification\n```\n\n**Example violations:**\n- ❌ \"The config file sets the timeout to 30 seconds\" (without reading it)\n- ❌ \"There are multiple test files for this module\" (without globbing)\n\n**Correct approach:**\n- ✅ Read the config file first, then report actual timeout value\n- ✅ Use Glob to find test files, then report count and names\n\n### System Integration\nBefore claiming system capabilities:\n```\n1. Use Bash to check installed tools/dependencies\n2. Read package.json, requirements.txt, or equivalent\n3. Verify environment variables and configuration\n4. Test actual behavior when possible\n```\n\n### Framework Detection\nBefore claiming framework presence or version:\n```\n1. Read package.json, Gemfile, mix.exs, or dependency file\n2. Search for framework-specific imports or patterns\n3. Check for framework configuration files\n4. Report specific version found, not assumed capabilities\n```\n\n### Test Results\nOnly report test outcomes after actual execution:\n```\n1. Execute tests using Bash tool\n2. Capture and read actual output\n3. Report specific pass/fail counts and error messages\n4. Never claim \"tests pass\" or \"all tests successful\" without execution\n```\n\n### Performance Claims\nOnly make performance statements based on measurement:\n```\n1. Run benchmarks or profiling tools\n2. Capture actual timing/memory data\n3. Report specific measurements with conditions\n4. State testing methodology used\n```\n\n## Anti-Patterns to Avoid\n\n### Fabricated Testing\n❌ \"The code has been thoroughly tested\"\n❌ \"All edge cases are handled\"\n❌ \"Test coverage is good\"\n\n✅ \"Executed test suite: 45 passing, 2 failing\"\n✅ \"Coverage report shows 78% line coverage\"\n✅ \"Tested with inputs [X, Y, Z], observed [specific results]\"\n\n### Unverified Architecture Claims\n❌ \"This follows microservices architecture\"\n❌ \"Uses event-driven design patterns\"\n❌ \"Implements SOLID principles\"\n\n✅ Use Grep to find specific patterns, then describe what exists\n✅ \"Found 12 service definitions in [location]\"\n✅ \"Code shows [specific pattern] in [specific files]\"\n\n### Generic Quality Statements\n❌ \"This is high-quality code\"\n❌ \"Well-structured implementation\"\n❌ \"Follows best practices\"\n\n✅ \"Code follows [specific standard] as verified by linter\"\n✅ \"Matches patterns from [specific reference documentation]\"\n✅ \"Static analysis shows complexity metrics of [specific values]\"\n\n## Validation Workflow\n\nWhen creating any factual content:\n\n1. **Identify Claims**: List all factual assertions being made\n2. **Check Evidence**: For each claim, determine what tool can verify it\n3. **Execute Validation**: Run Read, Grep, Glob, Bash, or other tools\n4. **Report Results**: State only what tools confirmed\n5. **Mark Uncertainty**: Clearly label anything not verified\n\n## Examples\n\n### Documentation Writing\n\n**Bad approach:**\n```markdown\nThis API is highly performant and handles thousands of requests per second.\nIt follows RESTful best practices and includes comprehensive error handling.\n```\n\n**Good approach:**\n```markdown\nThis API implements REST endpoints as defined in [specification link].\nLoad testing with Apache Bench shows handling of 1,200 requests/second\nat 95th percentile latency of 45ms. Error handling covers HTTP status codes\n400, 401, 403, 404, 500 as verified in [source file].\n```\n\n### Research Output\n\n**Bad approach:**\n```markdown\nReact hooks are the modern way to write React components and are much\nbetter than class components. They improve performance and code quality.\n```\n\n**Good approach:**\n```markdown\nReact hooks (introduced in React 16.8 per official changelog) provide\nfunction component state and lifecycle features previously requiring\nclasses. The React documentation at [URL] states hooks reduce component\nnesting and enable logic reuse. Performance impact requires measurement\nfor specific use cases.\n```\n\n### Implementation Planning\n\n**Bad approach:**\n```markdown\nThis should be a quick implementation, probably 2-3 hours.\nWe'll add authentication which is straightforward, then deploy.\n```\n\n**Good approach:**\n```markdown\nImplementation requires:\n- Authentication integration (12 files need modification per grep analysis)\n- Configuration of [specific auth provider]\n- Testing of login/logout flows\n\nComplexity assessment needed before timeline estimation. Requires\ninvestigation of existing auth patterns and deployment requirements.\n```\n\n## Integration with Other Skills\n\nThis skill should be active alongside:\n- **Documentation**: Ensures docs contain verified information\n- **Code Review**: Validates claims about code quality and patterns\n- **Research**: Grounds research in verifiable sources\n- **Git Operations**: Ensures accurate commit messages and PR descriptions\n\n## References\n\n- Agent Skills Specification: Factual, validated skill content\n- Scientific Method: Observation before conclusion\n- Verification Principle: Trust but verify through tool execution"
              },
              {
                "name": "code-review",
                "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices",
                "path": "core/skills/code-review/SKILL.md",
                "frontmatter": {
                  "name": "code-review",
                  "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices"
                },
                "content": "# Code Review Best Practices\n\nThis skill activates when reviewing code for quality, correctness, security, and maintainability.\n\n## When to Use This Skill\n\nActivate when:\n- Reviewing pull requests\n- Conducting code audits\n- Providing feedback on code quality\n- Identifying security vulnerabilities\n- Suggesting refactoring improvements\n- Checking adherence to coding standards\n\n## Code Review Checklist\n\n### 1. Correctness and Functionality\n\n**Does the code do what it's supposed to do?**\n\n- Logic is correct and handles all cases\n- Edge cases are considered\n- Error handling is appropriate\n- No obvious bugs or logical errors\n- Assertions and validations are present\n- Return values are correct\n\n**Questions to ask:**\n- What happens if this receives null/nil?\n- What if the list is empty?\n- What if the number is negative/zero?\n- Are there off-by-one errors?\n- Are comparisons correct (>, >=, <, <=)?\n\n### 2. Security\n\n**Is the code secure?**\n\n- No SQL injection vulnerabilities\n- No XSS (Cross-Site Scripting) vulnerabilities\n- No CSRF vulnerabilities (CSRF protection in place)\n- User input is validated and sanitized\n- Sensitive data is not logged\n- Authentication and authorization are properly implemented\n- No hardcoded secrets or credentials\n- File uploads are validated (type, size, content)\n- External URLs are validated\n- Rate limiting is in place for APIs\n\n**Common security issues:**\n\n```elixir\n# BAD: SQL injection vulnerability\nquery = \"SELECT * FROM users WHERE id = #{user_id}\"\n\n# GOOD: Use parameterized queries\nquery = from u in User, where: u.id == ^user_id\n\n# BAD: XSS vulnerability\nraw(\"<div>#{user_input}</div>\")\n\n# GOOD: Escape user input\n<div><%= user_input %></div>\n\n# BAD: Hardcoded secrets\napi_key = \"sk_live_123456789\"\n\n# GOOD: Use environment variables\napi_key = System.get_env(\"API_KEY\")\n\n# BAD: Mass assignment vulnerability\nUser.changeset(%User{}, params)\n\n# GOOD: Whitelist allowed fields\nUser.changeset(%User{}, params)\n# Where changeset only casts allowed fields:\n# cast(user, attrs, [:name, :email])\n```\n\n### 3. Performance\n\n**Is the code efficient?**\n\n- No N+1 query problems\n- Appropriate data structures chosen\n- Algorithms are efficient\n- Database indexes are used\n- Caching is implemented where appropriate\n- Large datasets are paginated or streamed\n- Unnecessary computations are avoided\n- Resources are cleaned up properly\n\n**Common performance issues:**\n\n```elixir\n# BAD: N+1 query\nposts = Repo.all(Post)\nEnum.map(posts, fn post ->\n  author = Repo.get(User, post.author_id)  # Query for each post!\n  {post, author}\nend)\n\n# GOOD: Preload associations\nposts = Post |> preload(:author) |> Repo.all()\n\n# BAD: Loading entire dataset\nusers = Repo.all(User)  # Loads all millions of users\nEnum.filter(users, & &1.active)\n\n# GOOD: Query in database\nusers = User |> where(active: true) |> Repo.all()\n\n# BAD: Inefficient data structure\nlist = [1, 2, 3, 4, 5]\nif 3 in list do  # O(n) lookup in list\n  # ...\nend\n\n# GOOD: Use set/map for lookups\nset = MapSet.new([1, 2, 3, 4, 5])\nif MapSet.member?(set, 3) do  # O(1) lookup\n  # ...\nend\n```\n\n### 4. Code Quality and Maintainability\n\n**Is the code readable and maintainable?**\n\n- Clear, descriptive variable and function names\n- Functions are small and focused (single responsibility)\n- No code duplication (DRY principle)\n- Comments explain \"why\", not \"what\"\n- Code follows project conventions and style guide\n- Magic numbers are replaced with named constants\n- Complexity is minimized\n- Code is self-documenting\n\n**Code quality issues:**\n\n```elixir\n# BAD: Unclear names\ndef calc(x, y, z) do\n  r = x * y / z\n  r * 1.2\nend\n\n# GOOD: Clear names\ndef calculate_discounted_price(quantity, unit_price, discount_percentage) do\n  subtotal = quantity * unit_price\n  discount_amount = subtotal * (discount_percentage / 100)\n  subtotal - discount_amount\nend\n\n# BAD: Long function with multiple responsibilities\ndef process_order(order) do\n  # Validate order (responsibility 1)\n  # Calculate totals (responsibility 2)\n  # Update inventory (responsibility 3)\n  # Send email (responsibility 4)\n  # Log analytics (responsibility 5)\nend\n\n# GOOD: Single responsibility functions\ndef process_order(order) do\n  with {:ok, order} <- validate_order(order),\n       {:ok, order} <- calculate_totals(order),\n       {:ok, order} <- update_inventory(order),\n       :ok <- send_confirmation_email(order),\n       :ok <- log_order_analytics(order) do\n    {:ok, order}\n  end\nend\n\n# BAD: Magic numbers\nif user.age >= 13 do\n  # ...\nend\n\n# GOOD: Named constants\n@minimum_age_coppa 13\n\nif user.age >= @minimum_age_coppa do\n  # ...\nend\n```\n\n### 5. Error Handling\n\n**Are errors handled properly?**\n\n- Errors don't crash the system unexpectedly\n- Error messages are helpful\n- Errors are logged appropriately\n- Happy path and error paths are both tested\n- No swallowed errors (empty catch blocks)\n- Proper error types are used\n\n**Error handling patterns:**\n\n```elixir\n# BAD: Silent failure\ntry do\n  dangerous_operation()\nrescue\n  _ -> nil  # Error is swallowed!\nend\n\n# GOOD: Handle errors explicitly\ncase dangerous_operation() do\n  {:ok, result} -> result\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n\n# BAD: Generic error message\n{:error, \"failed\"}\n\n# GOOD: Specific error\n{:error, :invalid_email_format}\n{:error, {:validation_failed, errors}}\n\n# BAD: Let it crash when shouldn't\ndef parse_config(path) do\n  File.read!(path)  # Crashes if file missing\n  |> Jason.decode!()  # Crashes if invalid JSON\nend\n\n# GOOD: Handle expected errors\ndef parse_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  else\n    {:error, :enoent} -> {:error, :config_file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_config_format}\n  end\nend\n```\n\n### 6. Testing\n\n**Is the code properly tested?**\n\n- New functionality has tests\n- Edge cases are tested\n- Error conditions are tested\n- Tests are clear and focused\n- Tests are deterministic (no flaky tests)\n- Test names describe what they test\n- Mocks are used appropriately\n- Test coverage is adequate\n\n**Testing concerns:**\n\n```elixir\n# BAD: Unclear test name\ntest \"test1\" do\n  # ...\nend\n\n# GOOD: Descriptive test name\ntest \"create_user/1 returns error when email is invalid\" do\n  # ...\nend\n\n# BAD: Testing too much at once\ntest \"user workflow\" do\n  # Creates user\n  # Updates user\n  # Deletes user\n  # All in one test!\nend\n\n# GOOD: Focused tests\ntest \"create_user/1 creates user with valid attributes\" do\n  # ...\nend\n\ntest \"update_user/2 updates user name\" do\n  # ...\nend\n\ntest \"delete_user/1 removes user from database\" do\n  # ...\nend\n\n# BAD: Non-deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  Process.sleep(100)  # Race condition!\n  assert operation_completed?()\nend\n\n# GOOD: Deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  assert_receive {:completed, _result}, 1000\nend\n```\n\n### 7. Documentation\n\n**Is the code documented?**\n\n- Public APIs have documentation\n- Complex logic has explanatory comments\n- README is updated if needed\n- Changelog is updated for user-facing changes\n- API documentation is accurate\n- Examples are provided\n\n### 8. Dependencies\n\n**Are dependencies handled properly?**\n\n- New dependencies are justified\n- Dependencies are up-to-date and maintained\n- Licenses are compatible with project\n- Security vulnerabilities are checked\n- Dependency versions are pinned or bounded\n\n## Review Process\n\n### Before Reviewing\n\n1. **Understand the context**\n   - Read the PR description\n   - Understand the problem being solved\n   - Check related issues\n\n2. **Build and test locally**\n   - Pull the branch\n   - Run tests\n   - Test the functionality manually\n\n### During Review\n\n1. **Start with the big picture**\n   - Is the approach sound?\n   - Does it fit the architecture?\n   - Is there a better way?\n\n2. **Review for correctness**\n   - Does it work as intended?\n   - Are edge cases handled?\n   - Is error handling appropriate?\n\n3. **Check security and performance**\n   - Are there security vulnerabilities?\n   - Will it perform well at scale?\n\n4. **Review code quality**\n   - Is it readable and maintainable?\n   - Does it follow conventions?\n   - Is it well-tested?\n\n### Providing Feedback\n\n**Be constructive and specific:**\n\n```markdown\n# BAD: Vague criticism\n\"This function is bad.\"\n\n# GOOD: Specific, actionable feedback\n\"This function has three responsibilities: validation, database update, and email sending. Consider splitting it into separate functions for better testability and maintainability:\n\n```elixir\ndef update_user(user, attrs) do\n  with {:ok, changeset} <- validate_user_update(user, attrs),\n       {:ok, user} <- save_user(changeset),\n       :ok <- send_update_notification(user) do\n    {:ok, user}\n  end\nend\n```\n\n# BAD: Demanding\n\"You must change this.\"\n\n# GOOD: Collaborative\n\"What do you think about extracting this into a separate function? It would make the code easier to test.\"\n\n# BAD: Nitpicking without context\n\"Use single quotes instead of double quotes.\"\n\n# GOOD: Explain reasoning\n\"Our style guide prefers single quotes for consistency (see CONTRIBUTING.md section 3.2).\"\n```\n\n**Use labels to categorize feedback:**\n\n- **[blocking]**: Must be fixed before merging\n- **[suggestion]**: Optional improvement\n- **[question]**: Asking for clarification\n- **[nit]**: Very minor, cosmetic issue\n- **[security]**: Security concern\n- **[performance]**: Performance concern\n\n**Example:**\n\n```markdown\n[blocking] This creates a SQL injection vulnerability. Use parameterized queries:\n\n```elixir\n# Instead of:\nquery = \"SELECT * FROM users WHERE name = '#{name}'\"\n\n# Use:\nfrom(u in User, where: u.name == ^name)\n```\n\n[suggestion] Consider extracting this logic into a separate function for reusability.\n\n[question] Why are we using a map here instead of a struct?\n\n[nit] Extra blank line here.\n```\n\n### After Review\n\n1. **Respond to author's questions**\n2. **Re-review after changes**\n3. **Approve when satisfied**\n4. **Celebrate good code**\n\n## Language-Specific Considerations\n\n### Elixir\n\n- Pattern matching is used effectively\n- Functions leverage pipe operator for readability\n- Atoms aren't created dynamically from untrusted input\n- `with` statements handle errors properly\n- Changesets validate all input\n- No direct database queries in controllers/LiveViews (use contexts)\n\n### JavaScript/TypeScript\n\n- Types are properly defined (TypeScript)\n- Promises are handled with .catch() or try/catch\n- == vs === is used correctly\n- Arrays/objects aren't mutated unexpectedly\n- this binding is correct\n- Async operations are properly awaited\n\n### Python\n\n- Type hints are used\n- List comprehensions aren't overly complex\n- Exceptions are specific (not bare except:)\n- Resources are closed (use with statements)\n- Code follows PEP 8\n\n### Rust\n\n- Ownership and borrowing are correct\n- Error handling uses Result/Option properly\n- Unsafe blocks are justified and minimal\n- Clone/copy is used appropriately\n- Lifetimes are correctly specified\n\n## Common Code Smells\n\n### Complexity Smells\n\n- **Long functions** - Function does too much\n- **Long parameter list** - Too many parameters\n- **Deep nesting** - Too many levels of indentation\n- **Complex conditionals** - Hard to understand if statements\n\n### Duplication Smells\n\n- **Copy-paste code** - Same code in multiple places\n- **Similar functions** - Functions that do almost the same thing\n- **Magic numbers** - Repeated literal values\n\n### Naming Smells\n\n- **Unclear names** - Variables like x, tmp, data\n- **Misleading names** - Name doesn't match behavior\n- **Inconsistent names** - Same concept called different things\n\n### Design Smells\n\n- **God object** - Class/module doing everything\n- **Feature envy** - Function using another object's data more than its own\n- **Inappropriate intimacy** - Too much coupling between modules\n\n## Anti-Patterns to Watch For\n\n### Premature Optimization\n\n```elixir\n# BAD: Optimizing before measuring\ndef calculate(data) do\n  # Complex, hard-to-read optimization\n  # that saves 0.1ms\nend\n\n# GOOD: Start simple, optimize if needed\ndef calculate(data) do\n  # Clear, simple code\n  # Optimize later if profiling shows bottleneck\nend\n```\n\n### Premature Abstraction\n\n```elixir\n# BAD: Abstract after one use\ndefmodule AbstractDataProcessorFactoryBuilder do\n  # Complex abstraction for single use case\nend\n\n# GOOD: Wait for second use case\ndef process_user_data(data) do\n  # Simple, direct implementation\n  # Abstract when pattern emerges\nend\n```\n\n### Error Swallowing\n\n```elixir\n# BAD: Hiding errors\ntry do\n  risky_operation()\nrescue\n  _ -> :ok  # What went wrong?\nend\n\n# GOOD: Handle explicitly\ncase risky_operation() do\n  {:ok, result} -> {:ok, result}\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n```\n\n## Review Etiquette\n\n### DO:\n\n- Be respectful and constructive\n- Assume good intent\n- Ask questions instead of making demands\n- Praise good code\n- Explain the \"why\" behind suggestions\n- Offer to pair program on complex issues\n- Respond promptly to author's replies\n\n### DON'T:\n\n- Be sarcastic or condescending\n- Bike-shed on minor style issues\n- Block on personal preferences\n- Review your own code without another reviewer\n- Approve code you don't understand\n- Nitpick excessively\n\n## Self-Review Checklist\n\nBefore submitting code for review:\n\n- [ ] Code compiles and runs\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] No commented-out code\n- [ ] No debug print statements\n- [ ] Documentation is updated\n- [ ] Commit messages are clear\n- [ ] No secrets or sensitive data\n- [ ] Code follows project style guide\n- [ ] Changes are focused (no unrelated changes)\n\n## Key Principles\n\n- **Correctness first**: Code must work correctly\n- **Security matters**: Always consider security implications\n- **Be specific**: Provide actionable, concrete feedback\n- **Be respectful**: Kind, constructive communication\n- **Focus on important issues**: Don't bike-shed\n- **Explain reasoning**: Help author learn, don't just dictate\n- **Approve good code**: Don't let perfect be enemy of good\n- **Collaborate**: You're on the same team"
              },
              {
                "name": "documentation-writing",
                "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation",
                "path": "core/skills/documentation/SKILL.md",
                "frontmatter": {
                  "name": "documentation-writing",
                  "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation"
                },
                "content": "# Technical Documentation Writing\n\nThis skill activates when writing or improving technical documentation, including README files, API documentation, user guides, and inline code documentation.\n\n## When to Use This Skill\n\nActivate when:\n- Writing README files\n- Creating API documentation\n- Writing user guides or tutorials\n- Documenting code with comments or docstrings\n- Creating architecture or design documents\n- Writing changelogs or release notes\n\n## README Files\n\n### Essential README Structure\n\nEvery README should include:\n\n```markdown\n# Project Name\n\nBrief one-liner description of the project.\n\n## Overview\n\n2-3 paragraphs explaining what the project does, why it exists, and who it's for.\n\n## Features\n\n- Key feature 1\n- Key feature 2\n- Key feature 3\n\n## Installation\n\n### Prerequisites\n\n- Requirement 1 (with version)\n- Requirement 2 (with version)\n\n### Install Steps\n\n```bash\n# Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies\nnpm install  # or pip install -r requirements.txt, mix deps.get, etc.\n\n# Configure\ncp .env.example .env\n# Edit .env with your settings\n\n# Run\nnpm start\n```\n\n## Quick Start\n\n```bash\n# Minimal example to get started\nnpm start\n```\n\n## Usage\n\n### Basic Example\n\n```language\n// Clear, runnable example\nconst example = new Project()\nexample.doSomething()\n```\n\n### Advanced Usage\n\nMore complex examples with explanations.\n\n## Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `apiKey` | string | - | API key for authentication |\n| `timeout` | number | 5000 | Request timeout in ms |\n\n## API Reference\n\nLink to detailed API documentation or include core APIs here.\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Development-specific setup\nnpm install --dev\nnpm run setup\n```\n\n### Running Tests\n\n```bash\nnpm test\nnpm run test:coverage\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Credits\n- Inspirations\n- Related projects\n\n## Support\n\n- Documentation: https://docs.example.com\n- Issues: https://github.com/user/project/issues\n- Discussions: https://github.com/user/project/discussions\n```\n\n### README Best Practices\n\n- **Start with a clear one-liner**: Immediately tell readers what the project does\n- **Include badges**: Build status, coverage, version, license\n- **Show, don't tell**: Use code examples liberally\n- **Keep it scannable**: Use headers, lists, and code blocks\n- **Make examples runnable**: Readers should be able to copy-paste and run\n- **Include visual aids**: Screenshots, diagrams, GIFs when appropriate\n- **Update regularly**: Keep documentation in sync with code\n- **Think about newcomers**: Write for someone seeing the project for the first time\n\n## API Documentation\n\n### Documenting Functions\n\n**Elixir (@doc):**\n```elixir\n@doc \"\"\"\nCalculates the sum of two numbers.\n\n## Parameters\n\n- `a` - The first number (integer or float)\n- `b` - The second number (integer or float)\n\n## Returns\n\nThe sum of `a` and `b`.\n\n## Examples\n\n    iex> Math.add(2, 3)\n    5\n\n    iex> Math.add(2.5, 3.7)\n    6.2\n\n\"\"\"\n@spec add(number(), number()) :: number()\ndef add(a, b) do\n  a + b\nend\n```\n\n**JavaScript (JSDoc):**\n```javascript\n/**\n * Calculates the sum of two numbers.\n *\n * @param {number} a - The first number\n * @param {number} b - The second number\n * @returns {number} The sum of a and b\n *\n * @example\n * add(2, 3)\n * // => 5\n */\nfunction add(a, b) {\n  return a + b\n}\n```\n\n**Python (docstring):**\n```python\ndef add(a: float, b: float) -> float:\n    \"\"\"\n    Calculate the sum of two numbers.\n\n    Args:\n        a: The first number\n        b: The second number\n\n    Returns:\n        The sum of a and b\n\n    Examples:\n        >>> add(2, 3)\n        5\n        >>> add(2.5, 3.7)\n        6.2\n\n    Raises:\n        TypeError: If arguments are not numbers\n    \"\"\"\n    return a + b\n```\n\n**Rust (doc comments):**\n```rust\n/// Calculates the sum of two numbers.\n///\n/// # Arguments\n///\n/// * `a` - The first number\n/// * `b` - The second number\n///\n/// # Returns\n///\n/// The sum of `a` and `b`\n///\n/// # Examples\n///\n/// ```\n/// use mylib::add;\n///\n/// assert_eq!(add(2, 3), 5);\n/// assert_eq!(add(2.5, 3.7), 6.2);\n/// ```\npub fn add(a: f64, b: f64) -> f64 {\n    a + b\n}\n```\n\n### Module/Class Documentation\n\nDocument the purpose, usage, and public API:\n\n```elixir\ndefmodule MyApp.UserManager do\n  @moduledoc \"\"\"\n  Manages user accounts and authentication.\n\n  The UserManager provides functions for creating, updating, and authenticating\n  users. It handles password hashing, session management, and user validation.\n\n  ## Usage\n\n      # Create a new user\n      {:ok, user} = UserManager.create_user(%{\n        email: \"alice@example.com\",\n        password: \"secure_password\"\n      })\n\n      # Authenticate\n      {:ok, user} = UserManager.authenticate(\"alice@example.com\", \"secure_password\")\n\n      # Update user\n      {:ok, updated} = UserManager.update_user(user, %{name: \"Alice Smith\"})\n\n  ## Configuration\n\n  Configure in `config/config.exs`:\n\n      config :my_app, MyApp.UserManager,\n        password_min_length: 8,\n        session_timeout: 3600\n\n  \"\"\"\nend\n```\n\n### API Endpoint Documentation\n\nDocument RESTful APIs clearly:\n\n```markdown\n## Endpoints\n\n### Create User\n\nCreates a new user account.\n\n**Endpoint:** `POST /api/users`\n\n**Authentication:** Not required\n\n**Request Body:**\n\n```json\n{\n  \"email\": \"alice@example.com\",\n  \"password\": \"secure_password\",\n  \"name\": \"Alice Smith\"\n}\n```\n\n**Response (201 Created):**\n\n```json\n{\n  \"id\": \"123\",\n  \"email\": \"alice@example.com\",\n  \"name\": \"Alice Smith\",\n  \"created_at\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n**Error Responses:**\n\n- `400 Bad Request` - Invalid input\n  ```json\n  {\n    \"error\": \"validation_error\",\n    \"details\": {\n      \"email\": [\"must be a valid email address\"],\n      \"password\": [\"must be at least 8 characters\"]\n    }\n  }\n  ```\n\n- `409 Conflict` - Email already exists\n  ```json\n  {\n    \"error\": \"email_taken\",\n    \"message\": \"An account with this email already exists\"\n  }\n  ```\n\n**Example:**\n\n```bash\ncurl -X POST https://api.example.com/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"alice@example.com\",\n    \"password\": \"secure_password\",\n    \"name\": \"Alice Smith\"\n  }'\n```\n```\n\n## User Guides and Tutorials\n\n### Tutorial Structure\n\n```markdown\n# Tutorial: Building Your First [Feature]\n\n## What You'll Build\n\nBrief description of the end result.\n\n## Prerequisites\n\n- Knowledge requirement 1\n- Installed tool 1\n- Account/access requirement\n\n## Step 1: [First Major Step]\n\nExplanation of what we're doing and why.\n\n```language\n// Code for this step\n```\n\n**What's happening here:**\n- Explanation of key line 1\n- Explanation of key line 2\n\n## Step 2: [Next Step]\n\nContinue with incremental steps...\n\n## Testing\n\nHow to verify it works.\n\n## Next Steps\n\n- Related tutorial 1\n- Advanced topic 1\n- Further reading\n```\n\n### Tutorial Best Practices\n\n- **Show working code first**: Let readers see the goal before diving into details\n- **Explain the 'why'**: Don't just show what to do, explain reasoning\n- **Incremental steps**: Each step should build on the previous\n- **Include checkpoints**: Ways to verify progress\n- **Provide complete code**: Include a repository or final code snippet\n- **Anticipate problems**: Address common mistakes\n- **Link to references**: Point to relevant API docs and resources\n\n## Inline Code Documentation\n\n### When to Write Comments\n\n**DO write comments for:**\n- Complex algorithms or business logic\n- Non-obvious decisions (\"why\" not \"what\")\n- Workarounds for bugs or limitations\n- Public APIs and exported functions\n- Configuration and constants\n\n**DON'T write comments for:**\n- Obvious code\n- What the code does (prefer clear naming)\n- Outdated information\n- Commented-out code (use version control)\n\n### Good Comment Examples\n\n```elixir\n# Good: Explains WHY\n# Use exponential backoff to avoid overwhelming the API after rate limit errors\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Bad: Explains WHAT (obvious from code)\n# Multiply 2 to the power of attempt and multiply by 1000\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Good: Documents workaround\n# NOTE: Using String.to_existing_atom because the Erlang VM limits atoms to ~1M.\n# All valid status atoms are pre-defined in this module.\ndef parse_status(status_string) do\n  String.to_existing_atom(status_string)\nend\n\n# Good: Explains business rule\n# Users must be at least 13 years old per COPPA regulations\n@minimum_age 13\n```\n\n## Architecture Documentation\n\n### Architecture Decision Records (ADR)\n\nDocument significant architectural decisions:\n\n```markdown\n# ADR 001: Use PostgreSQL for Primary Database\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to choose a database for our application that supports:\n- ACID transactions\n- Complex queries with joins\n- JSON data storage\n- Full-text search\n- Horizontal scalability (future requirement)\n\n## Decision\n\nWe will use PostgreSQL as our primary database.\n\n## Consequences\n\n### Positive\n\n- Mature, stable, well-documented\n- Excellent JSON support with JSONB\n- Built-in full-text search\n- Strong consistency guarantees\n- Large ecosystem of tools and extensions\n- Can scale with read replicas and partitioning\n\n### Negative\n\n- More complex to operate than simpler databases\n- Vertical scaling has limits (though sufficient for our needs)\n- Requires more server resources than lighter alternatives\n\n### Neutral\n\n- Team needs to learn PostgreSQL-specific features\n- May need to hire PostgreSQL expertise as we scale\n\n## Alternatives Considered\n\n- **MySQL**: Weaker JSON support, less feature-rich\n- **MongoDB**: No ACID guarantees, eventual consistency issues\n- **SQLite**: Not suitable for multi-user web applications\n```\n\n## Changelog Documentation\n\nFollow Keep a Changelog format:\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- New feature in development\n\n## [1.2.0] - 2024-01-15\n\n### Added\n- User profile pictures\n- Email notification preferences\n- Dark mode support\n\n### Changed\n- Improved search performance by 40%\n- Updated UI to match new brand guidelines\n\n### Fixed\n- Login redirect loop on Safari\n- Memory leak in background sync process\n\n### Deprecated\n- Old `/v1/users` endpoint (use `/v2/users` instead)\n\n## [1.1.0] - 2024-01-01\n\n### Added\n- Two-factor authentication\n- Export user data to JSON\n\n### Security\n- Fixed XSS vulnerability in comment rendering\n\n## [1.0.0] - 2023-12-15\n\n### Added\n- Initial release\n- User registration and authentication\n- Basic user profiles\n```\n\n## Documentation Tools\n\n### Documentation Generators\n\n- **Elixir**: ExDoc - `mix docs`\n- **JavaScript**: JSDoc, TypeDoc\n- **Python**: Sphinx, MkDocs\n- **Rust**: rustdoc - `cargo doc`\n- **Static sites**: VitePress, Docusaurus, GitBook\n\n### Diagram Tools\n\n- **Mermaid**: Diagrams in Markdown\n  ```markdown\n  ```mermaid\n  graph TD\n      A[User] -->|Requests| B[Load Balancer]\n      B --> C[Web Server 1]\n      B --> D[Web Server 2]\n      C --> E[Database]\n      D --> E\n  ```\n  ```\n\n- **PlantUML**: UML diagrams as code\n- **Excalidraw**: Hand-drawn style diagrams\n- **Draw.io**: Flowcharts and diagrams\n\n## Documentation Style Guide\n\n### Writing Style\n\n- **Use active voice**: \"The function returns\" not \"The value is returned\"\n- **Be concise**: Remove unnecessary words\n- **Use present tense**: \"Returns\" not \"Will return\"\n- **Be specific**: \"Timeout in milliseconds\" not \"Timeout value\"\n- **Avoid jargon**: Or explain it when necessary\n- **Use examples**: Show, don't just tell\n\n### Formatting Conventions\n\n- **Code**: Use `backticks` for inline code\n- **Commands**: Show with `$` prefix or in code blocks\n- **File paths**: Use `code formatting`\n- **Emphasis**: Use **bold** for important points, *italic* for slight emphasis\n- **Lists**: Use bullets for unordered, numbers for sequential steps\n- **Headers**: Use sentence case, not title case\n\n### Code Examples\n\n- **Complete**: Include all necessary imports and setup\n- **Runnable**: Readers should be able to copy and run\n- **Realistic**: Use meaningful variable names and realistic data\n- **Commented**: Explain non-obvious parts\n- **Tested**: Ensure examples actually work\n- **Current**: Keep in sync with latest API\n\n## Documentation Maintenance\n\n### Keeping Docs Updated\n\n- Update documentation in the same PR as code changes\n- Review docs during code review\n- Set up doc linting (broken links, outdated examples)\n- Schedule regular documentation audits\n- Use version tags in examples when API changes\n- Mark deprecated features clearly\n\n### Documentation Testing\n\n```elixir\n# Elixir doctests - examples in docs are actual tests\ndefmodule Math do\n  @doc \"\"\"\n  Adds two numbers.\n\n  ## Examples\n\n      iex> Math.add(2, 3)\n      5\n\n  \"\"\"\n  def add(a, b), do: a + b\nend\n```\n\n```rust\n/// Adds two numbers.\n///\n/// # Examples\n///\n/// ```\n/// assert_eq!(add(2, 3), 5);\n/// ```\npub fn add(a: i32, b: i32) -> i32 {\n    a + b\n}\n```\n\n## Key Principles\n\n- **Write for your audience**: Tailor complexity to reader's experience level\n- **Show examples**: Code examples are worth a thousand words\n- **Keep it current**: Outdated docs are worse than no docs\n- **Make it scannable**: Use headers, lists, code blocks, and white space\n- **Explain the 'why'**: Help readers understand reasoning, not just steps\n- **Start simple**: Begin with quickstart, then go deeper\n- **Test documentation**: Ensure examples run and links work\n- **Iterate based on feedback**: Improve based on user questions and confusion"
              },
              {
                "name": "git-operations",
                "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits",
                "path": "core/skills/git/SKILL.md",
                "frontmatter": {
                  "name": "git-operations",
                  "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits"
                },
                "content": "# Git Operations and Best Practices\n\nThis skill activates when performing Git operations, managing repositories, resolving conflicts, or following Git workflows and conventions.\n\n## When to Use This Skill\n\nActivate when:\n- Creating commits or commit messages\n- Managing branches and merging\n- Resolving merge conflicts\n- Rebasing or rewriting history\n- Creating pull requests\n- Following Git workflows (Git Flow, GitHub Flow, trunk-based)\n- Troubleshooting Git issues\n\n## Commit Message Conventions\n\n### Conventional Commits\n\nFollow the Conventional Commits specification:\n\n```\n<type>(<scope>): <subject>\n\n<body>\n\n<footer>\n```\n\n**Types:**\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation changes\n- `style`: Code style changes (formatting, missing semicolons, etc.)\n- `refactor`: Code refactoring without changing behavior\n- `perf`: Performance improvements\n- `test`: Adding or updating tests\n- `build`: Changes to build system or dependencies\n- `ci`: CI/CD configuration changes\n- `chore`: Other changes that don't modify src or test files\n- `revert`: Revert a previous commit\n\n**Examples:**\n```\nfeat(auth): add JWT authentication\n\nImplement JWT-based authentication with refresh tokens.\n- Add JWT generation and validation\n- Implement refresh token rotation\n- Add authentication middleware\n\nCloses #123\n\nfix(api): handle null values in user response\n\nPreviously, null email addresses would cause the API to crash.\nNow returns empty string for null emails.\n\nFixes #456\n\ndocs: update installation instructions\n\nAdd section on environment variable configuration.\n\ntest(user): add tests for email validation\n\nrefactor(database): simplify query builder\n\nperf(api): add caching for user endpoints\n\nReduces response time by 40% for user list endpoint.\n```\n\n### Writing Good Commit Messages\n\n**Subject line (first line):**\n- Keep under 50 characters\n- Start with lowercase (after type)\n- No period at the end\n- Use imperative mood (\"add\" not \"added\" or \"adds\")\n\n**Body:**\n- Wrap at 72 characters\n- Explain what and why, not how\n- Separate from subject with blank line\n- Can have multiple paragraphs\n\n**Footer:**\n- Reference issues and pull requests\n- Note breaking changes\n- Add co-authors\n\n```\nfeat(api): add user search endpoint\n\nImplement full-text search across user names and emails using\nPostgreSQL's full-text search capabilities. Search results are\nranked by relevance.\n\nPerformance tested with 1M users - average response time < 100ms.\n\nBREAKING CHANGE: API now requires PostgreSQL 12+\n\nCloses #789\nCo-authored-by: Jane Doe <jane@example.com>\n```\n\n## Branch Management\n\n### Branch Naming\n\nUse descriptive, hierarchical branch names:\n\n```\n<type>/<short-description>\n<type>/<issue-number>-<short-description>\n```\n\n**Examples:**\n```\nfeature/user-authentication\nfeature/123-add-search\nfix/456-null-pointer-error\nbugfix/password-reset-email\nhotfix/critical-security-patch\nrefactor/database-queries\ndocs/api-documentation\nchore/update-dependencies\n```\n\n### Creating and Switching Branches\n\n```bash\n# Create and switch to new branch\ngit checkout -b feature/new-feature\n\n# Switch to existing branch\ngit checkout main\ngit switch main  # Modern alternative\n\n# Create branch from specific commit\ngit checkout -b hotfix/bug origin/main\n\n# List branches\ngit branch                    # Local branches\ngit branch -r                 # Remote branches\ngit branch -a                 # All branches\ngit branch -v                 # With last commit\n```\n\n### Deleting Branches\n\n```bash\n# Delete local branch\ngit branch -d feature/completed-feature\n\n# Force delete unmerged branch\ngit branch -D feature/abandoned-feature\n\n# Delete remote branch\ngit push origin --delete feature/old-feature\n```\n\n## Working with Changes\n\n### Staging Changes\n\n```bash\n# Stage specific files\ngit add file1.ex file2.ex\n\n# Stage all changes\ngit add .\ngit add -A\n\n# Stage parts of a file (interactive)\ngit add -p file.ex\n\n# Unstage files\ngit restore --staged file.ex\ngit reset HEAD file.ex  # Old syntax\n```\n\n### Committing\n\n```bash\n# Commit staged changes\ngit commit -m \"feat: add user authentication\"\n\n# Commit with body\ngit commit -m \"feat: add user authentication\" -m \"Implement JWT-based auth with refresh tokens\"\n\n# Amend last commit (change message or add files)\ngit add forgotten-file.ex\ngit commit --amend\n\n# Amend without changing message\ngit commit --amend --no-edit\n```\n\n### Viewing Changes\n\n```bash\n# Show unstaged changes\ngit diff\n\n# Show staged changes\ngit diff --cached\ngit diff --staged\n\n# Show changes in specific file\ngit diff path/to/file.ex\n\n# Show changes between branches\ngit diff main..feature/new-feature\n\n# Show changes between commits\ngit diff abc123..def456\n\n# Show stats only\ngit diff --stat\n```\n\n## Branching Workflows\n\n### Feature Branch Workflow\n\n```bash\n# Start new feature\ngit checkout main\ngit pull origin main\ngit checkout -b feature/new-feature\n\n# Work on feature\ngit add .\ngit commit -m \"feat: implement new feature\"\n\n# Keep feature updated with main\ngit checkout main\ngit pull origin main\ngit checkout feature/new-feature\ngit merge main\n\n# Push feature\ngit push -u origin feature/new-feature\n\n# After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n```\n\n### Rebasing Feature Branch\n\n```bash\n# Keep feature branch up-to-date with clean history\ngit checkout feature/new-feature\ngit fetch origin\ngit rebase origin/main\n\n# If conflicts occur, resolve them, then:\ngit add resolved-file.ex\ngit rebase --continue\n\n# Abort rebase if needed\ngit rebase --abort\n\n# Force push after rebase (careful!)\ngit push --force-with-lease origin feature/new-feature\n```\n\n## Merge Strategies\n\n### Fast-Forward Merge\n\n```bash\n# Default when possible - no merge commit\ngit checkout main\ngit merge feature/simple-feature\n```\n\n### No Fast-Forward\n\n```bash\n# Always create merge commit for history\ngit merge --no-ff feature/important-feature\n```\n\n### Squash Merge\n\n```bash\n# Combine all feature commits into one\ngit merge --squash feature/many-small-commits\ngit commit -m \"feat: add complete feature\"\n```\n\n## Conflict Resolution\n\n### Identifying Conflicts\n\n```bash\n# See conflicted files\ngit status\n\n# See conflict markers in file\n# <<<<<<< HEAD\n# Current branch changes\n# =======\n# Incoming changes\n# >>>>>>> feature/branch\n```\n\n### Resolving Conflicts\n\n```bash\n# Edit files to resolve conflicts, then:\ngit add resolved-file.ex\ngit commit  # Or git rebase --continue if rebasing\n\n# Use merge tools\ngit mergetool\n\n# Choose one side completely\ngit checkout --ours file.ex    # Keep our version\ngit checkout --theirs file.ex  # Keep their version\n```\n\n### Aborting Merge/Rebase\n\n```bash\n# Abort merge\ngit merge --abort\n\n# Abort rebase\ngit rebase --abort\n```\n\n## History Management\n\n### Interactive Rebase\n\nClean up commit history before merging:\n\n```bash\n# Rebase last 3 commits\ngit rebase -i HEAD~3\n\n# Rebase since main\ngit rebase -i main\n\n# Interactive rebase options:\n# pick - keep commit as-is\n# reword - change commit message\n# edit - modify commit\n# squash - combine with previous commit\n# fixup - like squash but discard message\n# drop - remove commit\n```\n\n**Example workflow:**\n```bash\n# You have commits:\n# abc123 fix typo\n# def456 add feature\n# ghi789 fix bug in feature\n# jkl012 add tests\n\ngit rebase -i HEAD~4\n\n# Change to:\n# pick def456 add feature\n# fixup ghi789 fix bug in feature\n# squash jkl012 add tests\n# reword abc123 fix typo\n```\n\n### Viewing History\n\n```bash\n# View commit history\ngit log\n\n# Compact one-line format\ngit log --oneline\n\n# Graph view\ngit log --graph --oneline --all\n\n# With file changes\ngit log --stat\n\n# Search commits\ngit log --grep=\"authentication\"\n\n# Commits by author\ngit log --author=\"John\"\n\n# Commits in date range\ngit log --since=\"2 weeks ago\"\ngit log --after=\"2024-01-01\" --before=\"2024-02-01\"\n\n# Follow file history\ngit log --follow -- path/to/file.ex\n\n# Show specific commit\ngit show abc123\n```\n\n### Undoing Changes\n\n```bash\n# Undo uncommitted changes\ngit restore file.ex\ngit checkout -- file.ex  # Old syntax\n\n# Restore all files\ngit restore .\n\n# Undo commit (keep changes)\ngit reset --soft HEAD~1\n\n# Undo commit (discard changes) - DANGEROUS\ngit reset --hard HEAD~1\n\n# Create new commit that undoes a commit\ngit revert abc123\n\n# Revert merge commit\ngit revert -m 1 abc123\n```\n\n## Stashing\n\nTemporarily save uncommitted changes:\n\n```bash\n# Stash changes\ngit stash\ngit stash push -m \"work in progress on feature\"\n\n# Stash including untracked files\ngit stash -u\n\n# List stashes\ngit stash list\n\n# Apply most recent stash\ngit stash apply\n\n# Apply and remove stash\ngit stash pop\n\n# Apply specific stash\ngit stash apply stash@{2}\n\n# Delete stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n# Create branch from stash\ngit stash branch feature/from-stash\n```\n\n## Remote Operations\n\n### Working with Remotes\n\n```bash\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add origin git@github.com:user/repo.git\n\n# Change remote URL\ngit remote set-url origin git@github.com:user/new-repo.git\n\n# Remove remote\ngit remote remove origin\n\n# Rename remote\ngit remote rename origin upstream\n```\n\n### Fetching and Pulling\n\n```bash\n# Fetch changes from remote\ngit fetch origin\n\n# Fetch all remotes\ngit fetch --all\n\n# Pull changes (fetch + merge)\ngit pull origin main\n\n# Pull with rebase\ngit pull --rebase origin main\n\n# Set upstream branch\ngit push -u origin feature/new-feature\ngit branch --set-upstream-to=origin/feature feature/new-feature\n```\n\n### Pushing\n\n```bash\n# Push to remote\ngit push origin main\n\n# Push and set upstream\ngit push -u origin feature/new-feature\n\n# Force push (CAREFUL!)\ngit push --force origin feature/branch\n\n# Safer force push - fails if remote has new commits\ngit push --force-with-lease origin feature/branch\n\n# Push all branches\ngit push --all origin\n\n# Push tags\ngit push --tags\n```\n\n## Tags\n\n### Creating Tags\n\n```bash\n# Lightweight tag\ngit tag v1.0.0\n\n# Annotated tag (preferred)\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\n\n# Tag specific commit\ngit tag -a v1.0.0 abc123 -m \"Release version 1.0.0\"\n```\n\n### Managing Tags\n\n```bash\n# List tags\ngit tag\ngit tag -l \"v1.*\"\n\n# View tag details\ngit show v1.0.0\n\n# Push tag\ngit push origin v1.0.0\n\n# Push all tags\ngit push origin --tags\n\n# Delete local tag\ngit tag -d v1.0.0\n\n# Delete remote tag\ngit push origin --delete v1.0.0\n```\n\n## Advanced Operations\n\n### Cherry-Picking\n\nApply specific commits to current branch:\n\n```bash\n# Apply single commit\ngit cherry-pick abc123\n\n# Apply multiple commits\ngit cherry-pick abc123 def456\n\n# Cherry-pick without committing\ngit cherry-pick -n abc123\n```\n\n### Bisect\n\nFind which commit introduced a bug:\n\n```bash\n# Start bisect\ngit bisect start\ngit bisect bad                    # Current commit is bad\ngit bisect good abc123            # Known good commit\n\n# Git will checkout commits to test\n# After testing each:\ngit bisect good  # or\ngit bisect bad\n\n# When found, Git shows first bad commit\n# Reset\ngit bisect reset\n```\n\n### Submodules\n\n```bash\n# Add submodule\ngit submodule add git@github.com:user/repo.git path/to/submodule\n\n# Clone with submodules\ngit clone --recurse-submodules git@github.com:user/repo.git\n\n# Update submodules\ngit submodule update --init --recursive\n\n# Pull submodule updates\ngit submodule update --remote\n```\n\n## GitHub Specific\n\n### Pull Requests\n\n```bash\n# Using GitHub CLI (gh)\ngh pr create --title \"feat: add new feature\" --body \"Description of changes\"\n\n# Create draft PR\ngh pr create --draft\n\n# List PRs\ngh pr list\n\n# View PR\ngh pr view 123\n\n# Checkout PR locally\ngh pr checkout 123\n\n# Merge PR\ngh pr merge 123 --squash\n```\n\n### Issues\n\n```bash\n# Create issue\ngh issue create --title \"Bug: authentication fails\" --body \"Description\"\n\n# List issues\ngh issue list\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n```\n\n## Git Aliases\n\nAdd to `.gitconfig`:\n\n```ini\n[alias]\n    co = checkout\n    br = branch\n    ci = commit\n    st = status\n    unstage = restore --staged\n    last = log -1 HEAD\n    lg = log --graph --oneline --all\n    cm = commit -m\n    ca = commit --amend\n    undo = reset --soft HEAD~1\n    sync = !git fetch origin && git rebase origin/main\n    clean-branches = !git branch --merged | grep -v \\\"\\\\*\\\" | xargs -n 1 git branch -d\n```\n\n## Best Practices\n\n### Commits\n\n- Make atomic commits - one logical change per commit\n- Commit often - small, focused commits are better\n- Write clear commit messages following conventions\n- Don't commit sensitive data (API keys, passwords)\n- Don't commit generated files (add to `.gitignore`)\n- Test before committing\n\n### Branches\n\n- Keep branches short-lived\n- Pull main/master frequently to stay updated\n- Delete branches after merging\n- Use descriptive branch names\n- One feature/fix per branch\n\n### History\n\n- Keep history clean with interactive rebase\n- Don't rewrite public history (after pushing)\n- Use `--force-with-lease` instead of `--force`\n- Squash small fixup commits before merging\n\n### Collaboration\n\n- Pull before pushing\n- Resolve conflicts promptly\n- Review changes before committing\n- Communicate about force pushes\n- Use pull requests for code review\n\n## Common Issues and Solutions\n\n### Accidentally Committed to Wrong Branch\n\n```bash\n# Move commit to new branch\ngit branch feature/new-branch\ngit reset --hard HEAD~1\ngit checkout feature/new-branch\n```\n\n### Need to Change Last Commit Message\n\n```bash\ngit commit --amend\n```\n\n### Committed Sensitive Data\n\n```bash\n# Remove from history - CAREFUL!\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/sensitive-file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (faster)\nbfg --delete-files sensitive-file\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n\n# Force push to update remote\ngit push --force --all\n```\n\n### Recover Deleted Branch\n\n```bash\n# Find commit where branch was\ngit reflog\n\n# Recreate branch\ngit checkout -b recovered-branch abc123\n```\n\n### Merge Went Wrong\n\n```bash\n# Undo merge (before pushing)\ngit reset --hard HEAD~1\n\n# Undo merge (after pushing)\ngit revert -m 1 merge-commit-hash\n```\n\n## Key Principles\n\n- **Commit early, commit often**: Small, focused commits\n- **Write clear messages**: Follow conventional commits\n- **Keep history clean**: Rebase and squash before merging\n- **Don't rewrite public history**: Only rebase local commits\n- **Use branches**: Never commit directly to main\n- **Pull before push**: Stay in sync with remote\n- **Review before commit**: Check what you're committing\n- **Use descriptive names**: For branches, commits, and PRs"
              },
              {
                "name": "material-design",
                "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming",
                "path": "core/skills/material-design/SKILL.md",
                "frontmatter": {
                  "name": "material-design",
                  "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming"
                },
                "content": "# Material Design 3 (Material You)\n\nApply Google's Material Design 3 principles when designing and developing user interfaces with emphasis on personalization, accessibility, and cross-platform consistency.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing Android applications\n- Building web applications following Material Design\n- Working with Flutter or Jetpack Compose\n- Implementing dynamic theming and color systems\n- Creating Material components\n- Reviewing designs for Material Design compliance\n\n## What is Material Design 3?\n\nMaterial Design 3 (Material You) represents Google's latest design system with:\n\n- **Personalization**: Dynamic color extraction from user preferences\n- **Expressiveness**: Softer, rounded components with visual hierarchy\n- **Adaptability**: Responsive across devices and platforms\n- **Accessibility**: Built-in inclusive design features\n\n## Key Differences from Material Design 2\n\n| Aspect | MD2 | MD3 |\n|--------|-----|-----|\n| **Colors** | Fixed brand palettes | Dynamic, user-generated schemes |\n| **Customization** | Limited theming | Highly personalized |\n| **Components** | Flat, rigid shapes | Rounded, expressive |\n| **Accessibility** | Basic support | Priority built-in |\n\n## Core Foundations\n\n### 1. Dynamic Color System\n\nMaterial Design 3 uses HCT (Hue, Chroma, Tone) color space for perceptually accurate color generation.\n\n**Key concepts:**\n- Color roles (primary, secondary, tertiary, error, neutral)\n- Tonal palettes (50-99 tones per color)\n- Automatic light/dark theme generation\n- User-driven personalization from wallpaper/system\n\nFor detailed color system implementation, see `references/color-system.md`.\n\n### 2. Typography\n\nType scale with 5 display sizes and 9 text sizes:\n\n**Quick example:**\n- Display Large: 57sp\n- Headline Large: 32sp\n- Body Large: 16sp\n- Label Small: 11sp\n\nFor complete typography system and responsive scaling, see `references/typography.md`.\n\n### 3. Layout\n\nResponsive breakpoints and grid system:\n\n- **Compact**: 0-599dp (phones)\n- **Medium**: 600-839dp (tablets, folded phones)\n- **Expanded**: 840dp+ (desktops, large tablets)\n\nFor layout guidelines and examples, see `references/layout.md`.\n\n## Component Guidelines\n\nMaterial Design 3 provides specifications for:\n\n- **Common Buttons**: Elevated, Filled, Tonal, Outlined, Text\n- **Cards**: Elevated, Filled, Outlined variants\n- **Text Fields**: Filled, Outlined with labels and helper text\n- **Navigation**: Navigation bar, rail, drawer\n- **Chips**: Assist, Filter, Input, Suggestion chips\n- **Dialogs**: Basic, Full-screen dialogs\n\nFor detailed component specifications, consult `references/components.md`.\n\n## Quick Component Examples\n\n### Buttons\n\n```kotlin\n// Jetpack Compose\nButton(onClick = { }) {\n    Text(\"Filled Button\")\n}\n\nOutlinedButton(onClick = { }) {\n    Text(\"Outlined Button\")\n}\n```\n\n### Cards\n\n```kotlin\nCard(\n    modifier = Modifier.fillMaxWidth(),\n    elevation = CardDefaults.cardElevation(defaultElevation = 6.dp)\n) {\n    Column(modifier = Modifier.padding(16.dp)) {\n        Text(\"Card Title\", style = MaterialTheme.typography.headlineSmall)\n        Text(\"Card content\", style = MaterialTheme.typography.bodyMedium)\n    }\n}\n```\n\n### Text Fields\n\n```kotlin\nOutlinedTextField(\n    value = text,\n    onValueChange = { text = it },\n    label = { Text(\"Label\") },\n    supportingText = { Text(\"Helper text\") }\n)\n```\n\nFor more component examples and patterns, see `references/components.md`.\n\n## Implementing Dynamic Color\n\n### Android (Jetpack Compose)\n\n```kotlin\nval dynamicColor = Build.VERSION.SDK_INT >= Build.VERSION_CODES.S\n\nval colorScheme = when {\n    dynamicColor && darkTheme -> dynamicDarkColorScheme(LocalContext.current)\n    dynamicColor && !darkTheme -> dynamicLightColorScheme(LocalContext.current)\n    darkTheme -> darkColorScheme()\n    else -> lightColorScheme()\n}\n\nMaterialTheme(\n    colorScheme = colorScheme,\n    typography = Typography,\n    content = content\n)\n```\n\n### Web\n\nFor web implementation with Material Web Components, see `references/web-implementation.md`.\n\n## Motion and Animation\n\nMaterial Design 3 motion principles:\n- **Easing**: Standard, emphasized, decelerated curves\n- **Duration**: Based on travel distance and complexity\n- **Choreography**: Coordinated element movements\n\nFor motion specifications, see `references/motion.md`.\n\n## Accessibility\n\nMaterial Design 3 prioritizes accessibility:\n- Minimum 4.5:1 contrast ratio (text)\n- 3:1 contrast ratio (UI components)\n- Touch targets minimum 48dp × 48dp\n- Screen reader support\n- Semantic color usage (not color-only indicators)\n\nFor accessibility implementation details, see `references/accessibility.md`.\n\n## When to Consult References\n\n- **Color system implementation**: Read `references/color-system.md`\n- **Typography scales and usage**: Read `references/typography.md`\n- **Layout and responsive design**: Read `references/layout.md`\n- **Component specifications**: Read `references/components.md`\n- **Web implementation**: Read `references/web-implementation.md`\n- **Motion and animation**: Read `references/motion.md`\n- **Accessibility guidelines**: Read `references/accessibility.md`\n\n## Key Principles\n\n- **User-driven personalization**: Colors adapt to user preferences\n- **Expressive and flexible**: Rounded corners, dynamic elevation\n- **Accessible by default**: Built-in contrast, touch targets, semantics\n- **Cross-platform consistency**: Same principles across Android, web, iOS\n- **Design tokens**: Use semantic tokens, not hardcoded values\n- **Responsive**: Adapt to device size and orientation\n\n## Resources\n\n- **Material Design 3**: https://m3.material.io/\n- **Material Theme Builder**: https://m3.material.io/theme-builder\n- **Jetpack Compose**: https://developer.android.com/jetpack/compose/designsystems/material3\n- **Material Web Components**: https://github.com/material-components/material-web\n- **Flutter Material 3**: https://flutter.dev/docs/development/ui/material"
              },
              {
                "name": "mise",
                "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects",
                "path": "core/skills/mise/SKILL.md",
                "frontmatter": {
                  "name": "mise",
                  "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects"
                },
                "content": "# mise - Development Environment Management\n\nThis skill activates when working with mise for managing tool versions, environment variables, and project tasks.\n\n## When to Use This Skill\n\nActivate when:\n- Setting up development environments\n- Managing tool and runtime versions (Node.js, Python, Ruby, Go, etc.)\n- Configuring environment variables and secrets\n- Defining and running project tasks\n- Creating reproducible development setups\n- Working with monorepos or multiple projects\n\n## What is mise?\n\nmise is a polyglot runtime manager and development environment tool that combines:\n- **Tool version management** - Install and manage multiple versions of dev tools\n- **Environment configuration** - Set environment variables per project\n- **Task automation** - Define and run project tasks\n- **Cross-platform** - Works on macOS, Linux, and Windows\n\n## Installation\n\n```bash\n# macOS/Linux (using curl)\ncurl https://mise.run | sh\n\n# macOS (using Homebrew)\nbrew install mise\n\n# Windows\n# See https://mise.jdx.dev for Windows install instructions\n\n# Activate mise in your shell\necho 'eval \"$(mise activate bash)\"' >> ~/.bashrc   # bash\necho 'eval \"$(mise activate zsh)\"' >> ~/.zshrc     # zsh\necho 'mise activate fish | source' >> ~/.config/fish/config.fish  # fish\n```\n\n## Managing Tools\n\n### Tool Backends\n\nmise uses different backends (package managers) to install tools. Understanding backends helps you install tools correctly.\n\n#### Available Backends\n\n- **asdf** - Traditional asdf plugins (default for many tools)\n- **ubi** - Universal Binary Installer (GitHub/GitLab releases)\n- **cargo** - Rust packages (requires Rust installed)\n- **npm** - Node.js packages (requires Node installed)\n- **go** - Go packages (requires Go installed)\n- **aqua** - Package manager\n- **pipx** - Python packages (requires Python installed)\n- **gem** - Ruby packages (requires Ruby installed)\n- **github/gitlab** - Direct from repositories\n- **http** - Direct HTTP downloads\n\n#### Verifying Tool Names\n\nAlways verify tool names using `mise ls-remote` before adding to configuration:\n\n```bash\n# Check if tool exists in registry\nmise ls-remote node\n\n# Check tool with specific backend\nmise ls-remote cargo:ripgrep\nmise ls-remote ubi:sharkdp/fd\n\n# Search the registry\nmise registry | grep <tool-name>\n```\n\n### Installing Tools\n\n```bash\n# List available tools in registry\nmise registry\n\n# Install from default backend\nmise install node@20.10.0\nmise install python@3.12\nmise install ruby@3.3\n\n# Install with specific backend\nmise install cargo:ripgrep        # From Rust crates\nmise install ubi:sharkdp/fd       # From GitHub releases\nmise install npm:typescript       # From npm\n\n# Install latest version\nmise install node@latest\n\n# Install from .mise.toml or .tool-versions\nmise install\n```\n\n### Using Tools with `mise use`\n\nThe `mise use` command is the primary way to add tools to projects. It combines two operations:\n1. **Installs** the tool (if not already installed)\n2. **Adds** the tool to your configuration file\n\n**Key Difference**: `mise install` only installs tools, while `mise use` installs AND configures them.\n\n#### Basic Usage\n\n```bash\n# Interactive selection\nmise use\n\n# Add tool with fuzzy version (default)\nmise use node@20              # Saves as \"20\" in mise.toml\n\n# Add tool with exact version\nmise use --pin node@20.10.0   # Saves as \"20.10.0\"\n\n# Add latest version\nmise use node@latest          # Saves as \"latest\"\n\n# Add with specific backend\nmise use cargo:ripgrep@latest\nmise use ubi:sharkdp/fd\n```\n\n#### Configuration File Selection\n\n`mise use` writes to configuration files in this priority order:\n\n1. **`--global` flag**: `~/.config/mise/config.toml`\n2. **`--path <file>` flag**: Specified file path\n3. **`--env <env>` flag**: `.mise.<env>.toml`\n4. **Default**: `mise.toml` in current directory\n\n```bash\n# Global (all projects)\nmise use --global node@20\n\n# Local (current project)\nmise use node@20              # Creates/updates ./mise.toml\n\n# Environment-specific\nmise use --env local node@20  # Creates .mise.local.toml\n\n# Specific file\nmise use --path ~/.config/mise/custom.toml node@20\n```\n\n#### Important Flags\n\n```bash\n# Pin exact version\nmise use --pin node@20.10.0        # Saves \"20.10.0\"\n\n# Fuzzy version (default)\nmise use --fuzzy node@20           # Saves \"20\"\n\n# Force reinstall\nmise use --force node@20\n\n# Dry run (preview changes)\nmise use --dry-run node@20\n\n# Remove tool from config\nmise use --remove node\n```\n\n#### Version Pinning\n\n```bash\n# Fuzzy (recommended) - auto-updates within major version\nmise use node@20                   # Uses latest 20.x.x\n\n# Exact - locks to specific version\nmise use --pin node@20.10.0        # Always uses 20.10.0\n\n# Latest - always uses newest version\nmise use node@latest               # Always updates to latest\n```\n\n**Best Practice**: Use fuzzy versions for flexibility, `mise.lock` for reproducibility.\n\n### Setting Tool Versions\n\nThe `mise use` command automatically sets tool versions by updating configuration files.\n\n#### .mise.toml Configuration\n\n```toml\n[tools]\nnode = \"20.10.0\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\n\n# Use latest version\nterraform = \"latest\"\n\n# Backends - use quotes for namespaced tools\n\"cargo:ripgrep\" = \"latest\"        # Requires rust installed\n\"ubi:sharkdp/fd\" = \"latest\"       # GitHub releases\n\"npm:typescript\" = \"latest\"       # Requires node installed\n\n# Version from file\nnode = { version = \"lts\", resolve = \"latest-lts\" }\n```\n\n### UBI Backend (Universal Binary Installer)\n\nThe **ubi** backend installs tools directly from GitHub/GitLab releases without requiring plugins. It's built into mise and works cross-platform including Windows.\n\n#### Basic UBI Usage\n\n```bash\n# Install from GitHub releases\nmise use -g ubi:goreleaser/goreleaser\nmise use -g ubi:sharkdp/fd\nmise use -g ubi:BurntSushi/ripgrep\n\n# Specific version\nmise use -g ubi:goreleaser/goreleaser@1.25.1\n\n# In .mise.toml\n[tools]\n\"ubi:goreleaser/goreleaser\" = \"latest\"\n\"ubi:sharkdp/fd\" = \"2.0.0\"\n```\n\n#### UBI Advanced Options\n\nConfigure tool-specific options when binary names differ or filtering is needed:\n\n```toml\n[tools]\n# When executable name differs from repo name\n\"ubi:BurntSushi/ripgrep\" = { version = \"latest\", exe = \"rg\" }\n\n# Filter releases with matching pattern\n\"ubi:some/tool\" = { version = \"latest\", matching = \"linux-gnu\" }\n\n# Use regex for complex filtering\n\"ubi:some/tool\" = { version = \"latest\", matching_regex = \".*-linux-.*\\\\.tar\\\\.gz$\" }\n\n# Extract entire tarball\n\"ubi:some/tool\" = { version = \"latest\", extract_all = true }\n\n# Rename extracted executable\n\"ubi:some/tool\" = { version = \"latest\", rename_exe = \"my-tool\" }\n```\n\n#### UBI Supported Syntax\n\nThree installation formats:\n- **GitHub shorthand (latest)**: `ubi:owner/repo`\n- **GitHub shorthand (version)**: `ubi:owner/repo@1.2.3`\n- **Direct URL**: `ubi:https://github.com/owner/repo/releases/download/v1.2.3/...`\n\n### Cargo Backend\n\nThe **cargo** backend installs Rust packages from crates.io. **Requires Rust to be installed first.**\n\n#### Cargo Prerequisites\n\nInstall Rust before using cargo backend:\n\n```bash\n# Option 1: Install Rust via mise\nmise use -g rust\n\n# Option 2: Install Rust directly\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n#### Cargo Usage\n\n```bash\n# Install from crates.io\nmise use -g cargo:ripgrep\nmise use -g cargo:eza\nmise use -g cargo:bat\n\n# In .mise.toml - requires rust installed first\n[tools]\nrust = \"latest\"              # Install rust first\n\"cargo:ripgrep\" = \"latest\"   # Then cargo tools\n\"cargo:eza\" = \"latest\"\n\"cargo:bat\" = \"latest\"\n```\n\n#### Cargo from Git Repositories\n\n```bash\n# Specific tag\nmise use cargo:https://github.com/username/demo@tag:v1.0.0\n\n# Branch\nmise use cargo:https://github.com/username/demo@branch:main\n\n# Commit hash\nmise use cargo:https://github.com/username/demo@rev:abc123\n```\n\n#### Cargo Settings\n\nConfigure cargo behavior globally:\n\n```toml\n[settings]\n# Use cargo-binstall for faster installs (default: true)\ncargo.binstall = true\n\n# Use alternative cargo registry\ncargo.registry_name = \"my-registry\"\n```\n\n### Managing Installed Tools\n\n```bash\n# List installed tools\nmise list\n\n# List all versions of a tool\nmise list node\n\n# Uninstall a version\nmise uninstall node@18.0.0\n\n# Update all tools to latest\nmise upgrade\n\n# Update specific tool\nmise upgrade node\n```\n\n### Tool Aliases\n\n```bash\n# Create alias for a tool\nmise alias node 20 20.10.0\n\n# Use alias\nmise use node@20\n```\n\n## Environment Variables\n\n### Setting Environment Variables\n\n#### In .mise.toml\n\n```toml\n[env]\nDATABASE_URL = \"postgresql://localhost/myapp\"\nAPI_KEY = \"development-key\"\nNODE_ENV = \"development\"\n\n# Template values\nAPP_ROOT = \"{{ config_root }}\"\nDATA_DIR = \"{{ config_root }}/data\"\n```\n\n#### File-based env vars\n\n```toml\n[env]\n_.file = \".env\"\n_.path = [\"/custom/bin\"]\n```\n\n### Environment Templates\n\nUse Go templates in environment variables:\n\n```toml\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nLOG_FILE = \"{{ config_root }}/logs/app.log\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n```\n\n### Secrets Management\n\n```bash\n# Use with sops\nmise set SECRET_KEY sops://path/to/secret\n\n# Use with age\nmise set API_TOKEN age://path/to/secret\n\n# Use from command\nmise set BUILD_ID \"$(git rev-parse HEAD)\"\n```\n\n## Tasks\n\n### Defining Tasks\n\n#### In .mise.toml\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\n\n[tasks.test]\ndescription = \"Run tests\"\nrun = \"npm test\"\n\n[tasks.lint]\ndescription = \"Run linter\"\nrun = \"npm run lint\"\ndepends = [\"build\"]\n\n[tasks.ci]\ndescription = \"Run CI pipeline\"\ndepends = [\"lint\", \"test\"]\n\n[tasks.dev]\ndescription = \"Start development server\"\nrun = \"npm run dev\"\n```\n\n### Running Tasks\n\n```bash\n# Run a task\nmise run build\nmise run test\n\n# Short form\nmise build\nmise test\n\n# Run multiple tasks\nmise run lint test\n\n# List available tasks\nmise tasks\n\n# Run task with arguments\nmise run script -- arg1 arg2\n```\n\n### Task Dependencies\n\n```toml\n[tasks.deploy]\ndepends = [\"build\", \"test\"]\nrun = \"npm run deploy\"\n\n# Tasks run in order: build, test, then deploy\n```\n\n### Task Options\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\nsources = [\"src/**/*.ts\"]      # Only run if sources changed\noutputs = [\"dist/**/*\"]         # Check outputs for changes\ndir = \"frontend\"                # Run in specific directory\nenv = { NODE_ENV = \"production\" }\n\n[tasks.watch]\nrun = \"npm run watch\"\nraw = true                      # Don't wrap in shell\n```\n\n### Task Files\n\nCreate separate task files:\n\n```bash\n# .mise/tasks/deploy\n#!/bin/bash\n# mise description=\"Deploy to production\"\n# mise depends=[\"build\", \"test\"]\n\necho \"Deploying...\"\nnpm run deploy\n```\n\nMake executable:\n```bash\nchmod +x .mise/tasks/deploy\n```\n\n## Common Workflows\n\n### Node.js Project Setup\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[env]\nNODE_ENV = \"development\"\n\n[tasks.install]\nrun = \"npm install\"\n\n[tasks.dev]\nrun = \"npm run dev\"\ndepends = [\"install\"]\n\n[tasks.build]\nrun = \"npm run build\"\ndepends = [\"install\"]\n\n[tasks.test]\nrun = \"npm test\"\ndepends = [\"install\"]\n```\n\n```bash\n# Setup and run\ncd project\nmise install      # Installs Node 20\nmise dev         # Runs dev server\n```\n\n### Python Project Setup\n\n```toml\n# .mise.toml\n[tools]\npython = \"3.12\"\n\n[env]\nPYTHONPATH = \"{{ config_root }}/src\"\n\n[tasks.venv]\nrun = \"python -m venv .venv\"\n\n[tasks.install]\nrun = \"pip install -r requirements.txt\"\ndepends = [\"venv\"]\n\n[tasks.test]\nrun = \"pytest\"\ndepends = [\"install\"]\n\n[tasks.format]\nrun = \"black src tests\"\n```\n\n### Monorepo Setup\n\n```toml\n# Root .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\n\n[env]\nWORKSPACE_ROOT = \"{{ config_root }}\"\n\n[tasks.install-all]\nrun = \"\"\"\nnpm install\ncd services/api && npm install\ncd services/web && npm install\n\"\"\"\n\n[tasks.test-all]\ndepends = [\"install-all\"]\nrun = \"\"\"\nmise run test --dir services/api\nmise run test --dir services/web\n\"\"\"\n```\n\n### Multi-Tool Project\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\nterraform = \"latest\"\n\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n\n[tasks.setup]\ndescription = \"Setup all dependencies\"\nrun = \"\"\"\nnpm install\npip install -r requirements.txt\nbundle install\ngo mod download\n\"\"\"\n```\n\n## Lock Files\n\nGenerate lock files for reproducible environments:\n\n```bash\n# Generate .mise.lock\nmise lock\n\n# Use locked versions\nmise install --locked\n```\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[settings]\nlockfile = true  # Auto-generate lock file\n```\n\n## Shims\n\nUse shims for tool binaries:\n\n```bash\n# Enable shims\nmise settings set experimental true\nmise reshim\n\n# Now tools are in PATH via shims\nnode --version  # Uses mise-managed node\npython --version  # Uses mise-managed python\n```\n\n## Configuration Locations\n\nmise reads configuration from multiple locations (in order):\n\n1. `.mise.toml` - Project local config\n2. `.mise/config.toml` - Project local config (alternative)\n3. `~/.config/mise/config.toml` - Global config\n4. Environment variables - `MISE_*`\n\n## IDE Integration\n\n### VS Code\n\nAdd to `.vscode/settings.json`:\n\n```json\n{\n  \"terminal.integrated.env.linux\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  },\n  \"terminal.integrated.env.osx\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  }\n}\n```\n\n### JetBrains IDEs\n\nUse mise shims or configure tool paths:\n\n```bash\n# Find tool path\nmise which node\nmise which python\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: CI\n\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: jdx/mise-action@v2\n\n      - name: Run tests\n        run: mise run test\n```\n\n### GitLab CI\n\n```yaml\ntest:\n  image: ubuntu:latest\n  before_script:\n    - curl https://mise.run | sh\n    - eval \"$(mise activate bash)\"\n    - mise install\n  script:\n    - mise run test\n```\n\n## Troubleshooting\n\n### Check mise status\n\n```bash\n# Show configuration\nmise config\n\n# Show environment\nmise env\n\n# Show installed tools\nmise list\n\n# Debug mode\nmise --verbose install node\n```\n\n### Clear cache\n\n```bash\n# Clear tool cache\nmise cache clear\n\n# Remove and reinstall\nmise uninstall node@20\nmise install node@20\n```\n\n### Legacy .tool-versions\n\nmise is compatible with asdf's `.tool-versions`:\n\n```\n# .tool-versions\nnodejs 20.10.0\npython 3.12.0\nruby 3.3.0\n```\n\nConvert to mise:\n\n```bash\n# mise auto-reads .tool-versions\n# Or convert to .mise.toml\nmise config migrate\n```\n\n## Best Practices\n\n- **Use .mise.toml for projects**: Better than .tool-versions (more features)\n- **Pin versions in projects**: Ensure consistency across team\n- **Use tasks for common operations**: Document and standardize workflows\n- **Lock files in production**: Use `mise lock` for reproducibility\n- **Global tools for dev**: Set global defaults, override per project\n- **Environment per project**: Keep secrets and config in .mise.toml\n- **Commit .mise.toml**: Share config with team\n- **Don't commit .mise.lock**: Let mise generate per environment\n\n## Key Principles\n\n- **Reproducible environments**: Lock versions for consistency\n- **Project-specific config**: Each project defines its own tools and env\n- **Task automation**: Centralize common development tasks\n- **Cross-platform**: Same config works on all platforms\n- **Zero setup for team**: Clone and `mise install` to get started"
              },
              {
                "name": "nushell",
                "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features",
                "path": "core/skills/nushell/SKILL.md",
                "frontmatter": {
                  "name": "nushell",
                  "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features"
                },
                "content": "# Nushell - Modern Structured Shell\n\nThis skill activates when working with Nushell (Nu), writing Nu scripts, working with structured data pipelines, or configuring the Nu environment.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Nushell scripts or commands\n- Working with structured data in pipelines\n- Converting from bash/zsh to Nushell\n- Configuring Nushell environment\n- Processing JSON, CSV, YAML, or other structured data\n- Creating custom commands or modules\n\n## What is Nushell?\n\nNushell is a modern shell that:\n- Treats **data as structured** (not just text streams)\n- Works **cross-platform** (Windows, macOS, Linux)\n- Provides **clear error messages** and IDE support\n- Combines **shell and programming language** features\n- Has **built-in data format support** (JSON, CSV, YAML, TOML, XML, etc.)\n\n## Installation\n\n```bash\n# macOS\nbrew install nushell\n\n# Linux (cargo)\ncargo install nu\n\n# Windows\nwinget install nushell\n\n# Or download from https://www.nushell.sh/\n```\n\n## Basic Concepts\n\n### Everything is Data\n\nUnlike traditional shells where everything is text, Nu works with structured data:\n\n```nu\n# Traditional shell (text output)\nls | grep \".txt\"\n\n# Nushell (structured data)\nls | where name =~ \".txt\"\n```\n\n### Pipeline Philosophy\n\nData flows through pipelines as structured tables/records:\n\n```nu\n# Each command outputs structured data\nls | where size > 1kb | sort-by modified | reverse\n```\n\n## Data Types\n\n### Basic Types\n\n```nu\n# Integers\n42\n-10\n\n# Floats\n3.14\n-2.5\n\n# Strings\n\"hello\"\n'world'\n\n# Booleans\ntrue\nfalse\n\n# Null\nnull\n```\n\n### Collections\n\n```nu\n# Lists\n[1 2 3 4 5]\n[\"apple\" \"banana\" \"cherry\"]\n\n# Records (like objects/dicts)\n{name: \"Alice\", age: 30, city: \"NYC\"}\n\n# Tables (list of records)\n[\n  {name: \"Alice\", age: 30}\n  {name: \"Bob\", age: 25}\n]\n```\n\n### Ranges\n\n```nu\n# Number ranges\n1..10\n1..2..10  # Step by 2\n\n# Use in commands\n1..5 | each { |i| $i * 2 }\n```\n\n## Working with Files and Directories\n\n### Navigation\n\n```nu\n# Change directory\ncd /path/to/dir\n\n# List files (returns structured table)\nls\n\n# List with details\nls | select name size modified\n\n# Filter files\nls | where type == file\nls | where size > 1mb\nls | where name =~ \"\\.txt$\"\n```\n\n### File Operations\n\n```nu\n# Create file\n\"hello\" | save hello.txt\n\n# Read file\nopen hello.txt\n\n# Append to file\n\"world\" | save -a hello.txt\n\n# Copy\ncp source.txt dest.txt\n\n# Move/rename\nmv old.txt new.txt\n\n# Remove\nrm file.txt\nrm -r directory/\n\n# Create directory\nmkdir new-dir\n```\n\n### File Content\n\n```nu\n# Read as string\nopen file.txt\n\n# Read structured data\nopen data.json\nopen config.toml\nopen data.csv\n\n# Write structured data\n{name: \"Alice\", age: 30} | to json | save user.json\n[{a: 1} {a: 2}] | to csv | save data.csv\n```\n\n## Pipeline Operations\n\n### Filtering\n\n```nu\n# Filter with where\nls | where size > 1mb\nls | where type == dir\nls | where name =~ \"test\"\n\n# Multiple conditions\nls | where size > 1kb and type == file\n```\n\n### Selecting Columns\n\n```nu\n# Select specific columns\nls | select name size\n\n# Rename columns\nls | select name size | rename file bytes\n```\n\n### Sorting\n\n```nu\n# Sort by column\nls | sort-by size\nls | sort-by modified\n\n# Reverse sort\nls | sort-by size | reverse\n\n# Multiple columns\nls | sort-by type size\n```\n\n### Transforming Data\n\n```nu\n# Map over items with each\n1..5 | each { |i| $i * 2 }\n\n# Update column\nls | update name { |row| $row.name | str upcase }\n\n# Insert column\nls | insert size_kb { |row| $row.size / 1000 }\n\n# Upsert (update or insert)\nls | upsert type_upper { |row| $row.type | str upcase }\n```\n\n### Aggregation\n\n```nu\n# Count items\nls | length\n\n# Sum\n[1 2 3 4 5] | math sum\n\n# Average\n[1 2 3 4 5] | math avg\n\n# Min/Max\nls | get size | math max\nls | get size | math min\n\n# Group by\nls | group-by type\n```\n\n## Variables\n\n### Variable Assignment\n\n```nu\n# Let (immutable by default)\nlet name = \"Alice\"\nlet age = 30\nlet colors = [\"red\" \"green\" \"blue\"]\n\n# Mut (mutable)\nmut counter = 0\n$counter = $counter + 1\n```\n\n### Using Variables\n\n```nu\n# Reference with $\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# In pipelines\nlet threshold = 1mb\nls | where size > $threshold\n```\n\n### Environment Variables\n\n```nu\n# Get environment variable\n$env.PATH\n$env.HOME\n\n# Set environment variable\n$env.MY_VAR = \"value\"\n\n# Load from file\nload-env { API_KEY: \"secret\" }\n```\n\n## String Operations\n\n### String Interpolation\n\n```nu\n# String interpolation with ()\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# With expressions\nlet x = 5\nprint $\"Result: (5 * $x)\"\n```\n\n### String Methods\n\n```nu\n# Case conversion\n\"hello\" | str upcase  # HELLO\n\"WORLD\" | str downcase  # world\n\n# Trimming\n\"  spaces  \" | str trim\n\n# Replace\n\"hello world\" | str replace \"world\" \"nu\"\n\n# Contains\n\"hello world\" | str contains \"world\"  # true\n\n# Split\n\"a,b,c\" | split row \",\"\n```\n\n## Conditionals\n\n### If Expressions\n\n```nu\n# If-else\nif $age >= 18 {\n  print \"Adult\"\n} else {\n  print \"Minor\"\n}\n\n# If-else if-else\nif $score >= 90 {\n  \"A\"\n} else if $score >= 80 {\n  \"B\"\n} else {\n  \"C\"\n}\n\n# Ternary-style with match\nlet status = if $is_active { \"active\" } else { \"inactive\" }\n```\n\n### Match (Pattern Matching)\n\n```nu\n# Match expression\nmatch $value {\n  1 => \"one\"\n  2 => \"two\"\n  _ => \"other\"\n}\n\n# With conditions\nmatch $age {\n  0..17 => \"minor\"\n  18..64 => \"adult\"\n  _ => \"senior\"\n}\n```\n\n## Loops\n\n### For Loop\n\n```nu\n# Loop over range\nfor i in 1..5 {\n  print $i\n}\n\n# Loop over list\nfor name in [\"Alice\" \"Bob\" \"Charlie\"] {\n  print $\"Hello, ($name)\"\n}\n\n# Loop over files\nfor file in (ls | where type == file) {\n  print $file.name\n}\n```\n\n### While Loop\n\n```nu\n# While loop\nmut i = 0\nwhile $i < 5 {\n  print $i\n  $i = $i + 1\n}\n```\n\n### Each (Functional)\n\n```nu\n# Transform each item\n1..5 | each { |i| $i * 2 }\n\n# With index\n[\"a\" \"b\" \"c\"] | enumerate | each { |item|\n  print $\"($item.index): ($item.item)\"\n}\n```\n\n## Custom Commands\n\n### Defining Commands\n\n```nu\n# Simple command\ndef greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\ngreet \"Alice\"\n\n# With return value\ndef add [a: int, b: int] {\n  $a + $b\n}\n\nlet result = add 5 3\n\n# With default values\ndef greet [name: string = \"World\"] {\n  print $\"Hello, ($name)!\"\n}\n```\n\n### Command Parameters\n\n```nu\n# Required parameters\ndef copy [source: path, dest: path] {\n  cp $source $dest\n}\n\n# Optional parameters\ndef greet [\n  name: string\n  --loud (-l)  # Flag\n  --repeat (-r): int = 1  # Named parameter with default\n] {\n  let message = if $loud {\n    $name | str upcase\n  } else {\n    $name\n  }\n\n  1..$repeat | each { print $\"Hello, ($message)!\" }\n}\n\n# Usage\ngreet \"Alice\"\ngreet \"Bob\" --loud\ngreet \"Charlie\" --repeat 3\n```\n\n### Pipeline Commands\n\n```nu\n# Accept pipeline input\ndef filter-large [] {\n  where size > 1mb\n}\n\n# Usage\nls | filter-large\n\n# Accept and transform pipeline\ndef double [] {\n  each { |value| $value * 2 }\n}\n\n[1 2 3] | double\n```\n\n## Working with Structured Data\n\n### JSON\n\n```nu\n# Read JSON\nlet data = open data.json\n\n# Parse JSON string\nlet obj = '{\"name\": \"Alice\", \"age\": 30}' | from json\n\n# Write JSON\n{name: \"Alice\", age: 30} | to json | save user.json\n\n# Pretty print JSON\n{name: \"Alice\", age: 30} | to json -i 2\n```\n\n### CSV\n\n```nu\n# Read CSV\nlet data = open data.csv\n\n# Convert to CSV\n[{a: 1, b: 2} {a: 3, b: 4}] | to csv\n\n# Save CSV\nls | select name size | to csv | save files.csv\n```\n\n### YAML/TOML\n\n```nu\n# Read YAML\nlet config = open config.yaml\n\n# Read TOML\nlet config = open config.toml\n\n# Write YAML\n{key: \"value\"} | to yaml | save config.yaml\n\n# Write TOML\n{key: \"value\"} | to toml | save config.toml\n```\n\n### Working with Tables\n\n```nu\n# Create table\nlet users = [\n  {name: \"Alice\", age: 30, city: \"NYC\"}\n  {name: \"Bob\", age: 25, city: \"LA\"}\n  {name: \"Charlie\", age: 35, city: \"NYC\"}\n]\n\n# Query table\n$users | where age > 25\n$users | where city == \"NYC\"\n$users | select name age\n\n# Add column\n$users | insert country { \"USA\" }\n\n# Group and count\n$users | group-by city | transpose city users\n```\n\n## Modules\n\n### Creating Modules\n\n```nu\n# utils.nu\nexport def greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\nexport def add [a: int, b: int] {\n  $a + $b\n}\n```\n\n### Using Modules\n\n```nu\n# Import module\nuse utils.nu\n\n# Use exported commands\nutils greet \"Alice\"\nutils add 5 3\n\n# Import specific commands\nuse utils.nu [greet add]\n\ngreet \"Alice\"\nadd 5 3\n\n# Import with alias\nuse utils.nu *\n```\n\n## Configuration\n\n### Config File Location\n\n```nu\n# View config\nconfig nu\n\n# Edit config\nconfig nu | open\n\n# Config location\n$nu.config-path\n```\n\n### Common Configurations\n\n```nu\n# config.nu\n$env.config = {\n  show_banner: false\n\n  ls: {\n    use_ls_colors: true\n    clickable_links: true\n  }\n\n  table: {\n    mode: rounded\n    index_mode: auto\n  }\n\n  completions: {\n    quick: true\n    partial: true\n  }\n\n  history: {\n    max_size: 10000\n    sync_on_enter: true\n    file_format: \"sqlite\"\n  }\n}\n```\n\n### Environment Setup\n\n```nu\n# env.nu\n$env.PATH = ($env.PATH | split row (char esep) | append '/custom/bin')\n$env.EDITOR = \"nvim\"\n\n# Load completions\nuse completions/git.nu *\n```\n\n## Common Patterns\n\n### File Processing\n\n```nu\n# Process all JSON files\nls *.json | each { |file|\n  let data = open $file.name\n  print $\"Processing ($file.name): ($data | length) items\"\n}\n\n# Batch rename files\nls *.txt | each { |file|\n  let new_name = ($file.name | str replace \".txt\" \".md\")\n  mv $file.name $new_name\n}\n```\n\n### Data Transformation\n\n```nu\n# CSV to JSON\nopen data.csv | to json | save data.json\n\n# Filter and transform\nopen users.json\n| where active == true\n| select name email\n| to csv\n| save active_users.csv\n\n# Merge data\nlet users = open users.json\nlet orders = open orders.json\n$users | merge $orders\n```\n\n### HTTP Requests\n\n```nu\n# GET request\nhttp get https://api.example.com/users\n\n# POST request\nhttp post https://api.example.com/users {\n  name: \"Alice\"\n  email: \"alice@example.com\"\n}\n\n# With headers\nhttp get -H [Authorization \"Bearer token\"] https://api.example.com/data\n```\n\n### System Commands\n\n```nu\n# Run external command\n^ls -la\n\n# Capture output\nlet output = (^git status)\n\n# Check if command exists\nwhich git\n\n# Get command path\nwhich git | get path\n```\n\n## Error Handling\n\n### Try-Catch\n\n```nu\n# Try expression\ntry {\n  open missing.txt\n} catch {\n  print \"File not found\"\n}\n\n# With error value\ntry {\n  open missing.txt\n} catch { |err|\n  print $\"Error: ($err)\"\n}\n```\n\n### Null Handling\n\n```nu\n# Default value\nlet value = ($env.MY_VAR? | default \"default_value\")\n\n# Null propagation\nlet length = ($value | get name? | str length)\n```\n\n## Scripting\n\n### Script Files\n\n```nu\n#!/usr/bin/env nu\n\n# Script: process_logs.nu\n# Description: Process log files and generate report\n\ndef main [log_dir: path] {\n  let errors = (\n    ls $\"($log_dir)/*.log\"\n    | each { |file| open $file.name | lines }\n    | flatten\n    | where $it =~ \"ERROR\"\n  )\n\n  print $\"Found ($errors | length) errors\"\n  $errors | save error_report.txt\n}\n```\n\nMake executable:\n```bash\nchmod +x process_logs.nu\n./process_logs.nu /var/log\n```\n\n### Script Parameters\n\n```nu\n# With parameters\ndef main [\n  input: path\n  --output (-o): path = \"output.txt\"\n  --verbose (-v)\n] {\n  if $verbose {\n    print $\"Processing ($input)...\"\n  }\n\n  let data = open $input\n  $data | save $output\n\n  if $verbose {\n    print \"Done!\"\n  }\n}\n```\n\n## Comparison with Bash\n\n### Common Operations\n\n```bash\n# Bash\nfind . -name \"*.txt\" | wc -l\n\n# Nushell\nls **/*.txt | length\n```\n\n```bash\n# Bash\ncat file.json | jq '.users[] | select(.age > 25) | .name'\n\n# Nushell\nopen file.json | get users | where age > 25 | get name\n```\n\n```bash\n# Bash\nfor file in *.txt; do\n  mv \"$file\" \"${file%.txt}.md\"\ndone\n\n# Nushell\nls *.txt | each { |f| mv $f.name ($f.name | str replace \".txt\" \".md\") }\n```\n\n## Best Practices\n\n- **Use structured data**: Leverage Nu's strength in handling structured data\n- **Pipeline composition**: Build complex operations from simple pipeline stages\n- **Type annotations**: Add types to custom command parameters for clarity\n- **Error handling**: Use try-catch for operations that might fail\n- **Modules for reuse**: Organize reusable commands in modules\n- **Configuration**: Customize Nu to fit your workflow\n- **External commands**: Use `^` prefix when calling external commands explicitly\n\n## Common Pitfalls\n\n### String vs Bare Words\n\n```nu\n# Bare word (interpreted as string in some contexts)\necho hello\n\n# Explicit string (clearer)\necho \"hello\"\n```\n\n### External Commands\n\n```nu\n# Wrong - Nu tries to parse as Nu command\nls -la\n\n# Right - Explicitly call external command\n^ls -la\n```\n\n### Variable Scope\n\n```nu\n# Variables are scoped to blocks\nif true {\n  let x = 5\n}\n# $x not available here\n\n# Use mut outside for wider scope\nmut x = 0\nif true {\n  $x = 5\n}\nprint $x  # Works\n```\n\n## Key Principles\n\n- **Structured data first**: Think in terms of tables and records, not text\n- **Pipeline composition**: Chain simple operations to build complex workflows\n- **Type safety**: Leverage Nu's type system for reliable scripts\n- **Cross-platform**: Write scripts that work on all platforms\n- **Interactive and scriptable**: Same syntax works in REPL and scripts\n- **Clear errors**: Nu provides helpful error messages for debugging"
              },
              {
                "name": "twelve-factor",
                "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                "path": "core/skills/twelve-factor/SKILL.md",
                "frontmatter": {
                  "name": "twelve-factor",
                  "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                  "license": "MIT"
                },
                "content": "# 12-Factor App Methodology\n\nGuide for building scalable, maintainable, and portable cloud-native applications following the 12-Factor App principles and modern extensions.\n\n## When to Activate\n\nUse this skill when:\n- Designing or refactoring cloud-native applications\n- Building applications for Kubernetes deployment\n- Setting up CI/CD pipelines\n- Implementing microservices architecture\n- Migrating applications to containers\n- Reviewing architecture for cloud readiness\n- Troubleshooting deployment or scaling issues\n- Working with environment configuration\n\n## The 12 Factors\n\n### I. Codebase\n\n**One codebase tracked in revision control, many deploys**\n\n```\nmyapp-repo/\n├── src/\n├── config/\n├── deploy/\n│   ├── staging/\n│   ├── production/\n│   └── development/\n└── Dockerfile\n```\n\n**Key principles:**\n- Single Git repository for the application\n- Multiple environments deploy from same codebase\n- Environment-specific config separate from code\n- Use GitOps (ArgoCD, Flux) for deployment automation\n\n**Anti-patterns:**\n- ❌ Multiple repositories for the same application\n- ❌ Different codebases for different environments\n- ❌ Copying code between repositories\n\n### II. Dependencies\n\n**Explicitly declare and isolate dependencies**\n\nDeclare all dependencies explicitly using package managers:\n\n```dockerfile\n# Multi-stage build for dependency isolation\nFROM node:18-alpine AS dependencies\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nFROM node:18-alpine AS runtime\nWORKDIR /app\nCOPY --from=dependencies /app/node_modules ./node_modules\nCOPY . .\nCMD [\"node\", \"index.js\"]\n```\n\n**Language-specific examples:**\n- Node.js: `package.json` and `package-lock.json`\n- Python: `requirements.txt` or `Pipfile.lock`\n- Java: `pom.xml` or `build.gradle`\n- Go: `go.mod` and `go.sum`\n- Elixir: `mix.exs` and `mix.lock`\n- Rust: `Cargo.toml` and `Cargo.lock`\n\n**Key principles:**\n- Never rely on system-wide packages\n- Use lock files for reproducible builds\n- Vendor dependencies when possible\n- Multi-stage builds for smaller images\n\n### III. Config\n\n**Store config in the environment**\n\nAll configuration should come from environment variables:\n\n```elixir\n# Elixir - config/runtime.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.fetch_env!(\"DATABASE_PASSWORD\"),\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n```\n\n```javascript\n// Node.js\nconst config = {\n  database: {\n    url: process.env.DATABASE_URL,\n    pool: {\n      min: parseInt(process.env.DB_POOL_MIN || '2'),\n      max: parseInt(process.env.DB_POOL_MAX || '10')\n    }\n  },\n  cache: {\n    ttl: parseInt(process.env.CACHE_TTL || '3600')\n  }\n};\n```\n\n**Kubernetes ConfigMaps:**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_HOST: \"postgres-service\"\n  CACHE_TTL: \"3600\"\n  LOG_LEVEL: \"info\"\n```\n\n**Kubernetes Secrets:**\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  DATABASE_PASSWORD: <base64-encoded>\n  JWT_SECRET: <base64-encoded>\n  API_KEY: <base64-encoded>\n```\n\n**Anti-patterns:**\n- ❌ Hardcoded configuration values\n- ❌ Configuration files committed to version control\n- ❌ Different code paths for different environments\n\n### IV. Backing Services\n\n**Treat backing services as attached resources**\n\nConnect to all backing services (databases, queues, caches, APIs) via URLs in environment variables:\n\n```javascript\n// Treat all backing services uniformly\nconst services = {\n  database: createConnection(process.env.DATABASE_URL),\n  cache: createRedisClient(process.env.REDIS_URL),\n  queue: createQueueClient(process.env.RABBITMQ_URL),\n  storage: createS3Client(process.env.S3_ENDPOINT)\n};\n```\n\n**Kubernetes Service Discovery:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n```\n\n**Key principles:**\n- No distinction between local and third-party services\n- Swappable via configuration change only\n- No code changes to swap backing services\n- Connection via URL in environment\n\n### V. Build, Release, Run\n\n**Strictly separate build and run stages**\n\nThree distinct stages:\n1. **Build**: Convert code to executable bundle\n2. **Release**: Combine build with config\n3. **Run**: Execute in target environment\n\n```yaml\n# GitHub Actions CI/CD Pipeline\nname: Build and Deploy\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build Docker image\n        run: docker build -t myapp:${{ github.sha }} .\n      - name: Push to registry\n        run: docker push myapp:${{ github.sha }}\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to Kubernetes\n        run: kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}\n```\n\n**Key principles:**\n- Immutable releases (never modify, only deploy new)\n- Unique release identifiers (git SHA, semver)\n- Rollback by redeploying previous release\n- Separate build artifacts from runtime config\n\n### VI. Processes\n\n**Execute the app as one or more stateless processes**\n\nApplication processes should be stateless and share-nothing. Store persistent state in backing services.\n\n```javascript\n// ❌ Bad: In-memory session store\napp.use(session({\n  secret: process.env.SESSION_SECRET,\n  resave: false\n  // Uses memory store by default\n}));\n\n// ✓ Good: Store session in Redis\napp.use(session({\n  store: new RedisStore({\n    client: redisClient,\n    prefix: 'sess:'\n  }),\n  secret: process.env.SESSION_SECRET,\n  resave: false,\n  saveUninitialized: false\n}));\n```\n\n**Kubernetes Deployment:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3  # Can scale horizontally\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n```\n\n**Key principles:**\n- Stateless processes enable horizontal scaling\n- No sticky sessions\n- No local filesystem for persistent data\n- Shared state goes in databases, caches, or queues\n\n### VII. Port Binding\n\n**Export services via port binding**\n\nApplications are self-contained and bind to a port:\n\n```javascript\nconst express = require('express');\nconst app = express();\nconst port = process.env.PORT || 3000;\n\napp.listen(port, '0.0.0.0', () => {\n  console.log(`Server running on port ${port}`);\n});\n```\n\n```elixir\n# Phoenix endpoint config\nconfig :my_app, MyAppWeb.Endpoint,\n  http: [port: String.to_integer(System.get_env(\"PORT\") || \"4000\")],\n  server: true\n```\n\n**Kubernetes Service:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: LoadBalancer\n```\n\n**Key principles:**\n- Bind to `0.0.0.0`, not `localhost`\n- Port number from environment variable\n- No reliance on runtime injection (e.g., Apache, Nginx)\n- HTTP server library embedded in app\n\n### VIII. Concurrency\n\n**Scale out via the process model**\n\nScale by adding more processes (horizontal scaling), not by making processes larger (vertical scaling):\n\n```yaml\n# Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n**Process types (Procfile concept):**\n\n```\nweb: node server.js\nworker: node worker.js\nscheduler: node scheduler.js\n```\n\n**Key principles:**\n- Different process types for different workloads\n- Processes can scale independently\n- OS process manager handles processes\n- Never daemonize or write PID files\n\n### IX. Disposability\n\n**Maximize robustness with fast startup and graceful shutdown**\n\n```javascript\nconst server = app.listen(port, () => {\n  console.log('Server started');\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', () => {\n  console.log('SIGTERM received, shutting down gracefully');\n\n  server.close(() => {\n    // Close database connections\n    db.close();\n\n    // Close other connections\n    redis.quit();\n\n    console.log('Process terminated');\n    process.exit(0);\n  });\n});\n```\n\n**Kubernetes lifecycle hooks:**\n\n```yaml\nspec:\n  containers:\n  - name: myapp\n    image: myapp:latest\n    lifecycle:\n      preStop:\n        exec:\n          command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n    terminationGracePeriodSeconds: 30\n```\n\n**Key principles:**\n- Minimize startup time (< 10 seconds ideal)\n- Handle SIGTERM for graceful shutdown\n- Finish in-flight requests before shutting down\n- Robust against sudden death\n- Fast startup enables rapid scaling\n\n### X. Dev/Prod Parity\n\n**Keep development, staging, and production as similar as possible**\n\n**Docker Compose for local development:**\n\n```yaml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n\n  redis:\n    image: redis:7-alpine\n```\n\n**Key principles:**\n- Use same backing services in dev and prod\n- Containers ensure environment consistency\n- Infrastructure as code for reproducibility\n- Minimize time gap between dev and production\n- Same deployment process for all environments\n\n### XI. Logs\n\n**Treat logs as event streams**\n\nWrite all logs to stdout/stderr, let the environment handle aggregation:\n\n```javascript\n// Structured logging to stdout\nconst winston = require('winston');\n\nconst logger = winston.createLogger({\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.json()\n  ),\n  transports: [\n    new winston.transports.Console()\n  ]\n});\n\nlogger.info('User logged in', {\n  userId: 123,\n  ip: '192.168.1.1',\n  userAgent: 'Mozilla/5.0...'\n});\n```\n\n```elixir\n# Elixir structured logging\nrequire Logger\n\nLogger.info(\"User logged in\",\n  user_id: 123,\n  ip: \"192.168.1.1\"\n)\n```\n\n**Key principles:**\n- Never manage log files\n- Write unbuffered to stdout\n- Use structured logging (JSON)\n- Let platform route logs (Fluentd, Logstash)\n- Include correlation IDs for tracing\n\n**Anti-patterns:**\n- ❌ Writing to log files\n- ❌ Log rotation within the app\n- ❌ Sending logs directly to aggregation service\n\n### XII. Admin Processes\n\n**Run admin/management tasks as one-off processes**\n\nDatabase migrations, console, one-time scripts:\n\n```yaml\n# Kubernetes Job for database migration\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: myapp:latest\n        command: [\"npm\", \"run\", \"migrate\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: DATABASE_URL\n      restartPolicy: OnFailure\n```\n\n```yaml\n# CronJob for scheduled cleanup\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: data-cleanup\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: myapp:latest\n            command: [\"npm\", \"run\", \"cleanup\"]\n          restartPolicy: OnFailure\n```\n\n**Key principles:**\n- Same environment as regular processes\n- Same codebase and config\n- Run against release, not development code\n- Use scheduler for recurring tasks\n- Ship admin code with application code\n\n## Modern Extensions (Beyond 12)\n\n### XIII. API First\n\nDesign and document APIs before implementation:\n\n```yaml\n# OpenAPI specification\nopenapi: 3.0.0\ninfo:\n  title: My API\n  version: v1\npaths:\n  /users:\n    get:\n      summary: List users\n      responses:\n        '200':\n          description: Success\n```\n\n**Key principles:**\n- OpenAPI/Swagger specifications\n- API versioning (URL or header)\n- API gateway pattern\n- Contract-first development\n\n### XIV. Telemetry\n\nComprehensive observability with metrics, tracing, and monitoring:\n\n```yaml\n# Prometheus ServiceMonitor\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myapp-monitor\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  endpoints:\n  - port: metrics\n    path: /metrics\n```\n\n**Key principles:**\n- Expose /metrics endpoint (Prometheus format)\n- Distributed tracing (OpenTelemetry)\n- Application Performance Monitoring (APM)\n- Custom business metrics\n- Health check endpoints\n\n### XV. Security\n\nAuthentication, authorization, and security by design:\n\n```javascript\n// JWT authentication middleware\nfunction authenticateToken(req, res, next) {\n  const authHeader = req.headers['authorization'];\n  const token = authHeader && authHeader.split(' ')[1];\n\n  if (!token) return res.sendStatus(401);\n\n  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {\n    if (err) return res.sendStatus(403);\n    req.user = user;\n    next();\n  });\n}\n```\n\n**Key principles:**\n- OAuth 2.0 / OpenID Connect\n- RBAC (Role-Based Access Control)\n- Secrets in environment, never in code\n- TLS everywhere\n- Security scanning in CI/CD\n\n## Common Patterns\n\n### Configuration Validation\n\nValidate required configuration at startup:\n\n```javascript\nfunction validateConfig() {\n  const required = ['DATABASE_URL', 'JWT_SECRET', 'REDIS_URL'];\n  const missing = required.filter(key => !process.env[key]);\n\n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n}\n\n// Call before starting server\nvalidateConfig();\n```\n\n### Health Checks\n\nImplement health and readiness endpoints:\n\n```javascript\n// Liveness probe\napp.get('/health', (req, res) => {\n  res.status(200).json({\n    status: 'healthy',\n    timestamp: new Date().toISOString()\n  });\n});\n\n// Readiness probe\napp.get('/ready', async (req, res) => {\n  try {\n    await db.ping();\n    await redis.ping();\n    res.status(200).json({ status: 'ready' });\n  } catch (err) {\n    res.status(503).json({ status: 'not ready', error: err.message });\n  }\n});\n```\n\n**Kubernetes probes:**\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 3000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n### Graceful Degradation\n\nHandle backing service failures gracefully:\n\n```javascript\nasync function getCachedData(key) {\n  try {\n    return await redis.get(key);\n  } catch (err) {\n    logger.warn('Redis unavailable, falling back to database', { error: err.message });\n    return await db.query('SELECT data FROM cache WHERE key = ?', [key]);\n  }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Environment-Specific Code Paths\n\n```javascript\n// DON'T\nif (process.env.NODE_ENV === 'production') {\n  // Different behavior\n} else {\n  // Different behavior\n}\n\n// DO: Use configuration\nconst timeout = parseInt(process.env.TIMEOUT || '5000');\n```\n\n### ❌ Local File Storage\n\n```javascript\n// DON'T: Write to local filesystem\nfs.writeFile('/tmp/uploads/' + filename, data);\n\n// DO: Use object storage\nawait s3.putObject({\n  Bucket: process.env.S3_BUCKET,\n  Key: filename,\n  Body: data\n});\n```\n\n### ❌ In-Memory State\n\n```javascript\n// DON'T: Store state in memory\nconst sessions = new Map();\n\n// DO: Use external store\nconst session = await redis.get(`session:${sessionId}`);\n```\n\n### ❌ Hardcoded Dependencies\n\n```javascript\n// DON'T: Hardcode service locations\nconst db = connect('localhost:5432');\n\n// DO: Use environment variables\nconst db = connect(process.env.DATABASE_URL);\n```\n\n## Troubleshooting Guide\n\n### Application Won't Start\n\n1. Check required environment variables are set\n2. Validate configuration at startup\n3. Check backing service connectivity\n4. Review logs for initialization errors\n\n### Application Won't Scale\n\n1. Identify stateful operations\n2. Move state to backing services\n3. Remove file system dependencies\n4. Eliminate sticky sessions\n\n### Inconsistent Behavior Across Environments\n\n1. Ensure same backing service types (not SQLite in dev, Postgres in prod)\n2. Use containers for dev environment\n3. Check for environment-specific code paths\n4. Verify configuration is environment-only\n\n### Logs Not Appearing\n\n1. Ensure writing to stdout/stderr\n2. Avoid buffering log output\n3. Check log aggregation configuration\n4. Verify Kubernetes logging sidecar/daemonset\n\n## Best Practices Summary\n\n1. **Environment variables for all configuration**\n2. **Stateless processes that can scale horizontally**\n3. **Structured logging to stdout**\n4. **Containers for development parity**\n5. **Automated CI/CD pipelines**\n6. **Health checks for orchestration**\n7. **Graceful shutdown handling**\n8. **Fast startup times (< 10s)**\n9. **Immutable releases with unique IDs**\n10. **Comprehensive monitoring and telemetry**\n\n## Kubernetes-Specific Best Practices\n\n### Resource Limits\n\n```yaml\nresources:\n  requests:\n    memory: \"128Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"256Mi\"\n    cpu: \"200m\"\n```\n\n### Init Containers\n\n```yaml\ninitContainers:\n- name: wait-for-db\n  image: busybox\n  command: ['sh', '-c', 'until nc -z postgres-service 5432; do sleep 1; done']\n```\n\n### Pod Disruption Budgets\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n## Resources\n\n- **12factor.net**: Original methodology\n- **Kubernetes Documentation**: https://kubernetes.io/docs/\n- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n- **OpenTelemetry**: https://opentelemetry.io/\n- **Prometheus**: https://prometheus.io/\n\n## Key Insights\n\n> \"The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\"\n\n> \"A twelve-factor app never relies on implicit existence of state on the filesystem. Even if a process has written something to disk, it must assume that file won't be available on the next request.\"\n\nDesign applications from day one to be cloud-native, scalable, and maintainable. The investment in following these principles pays dividends in operational simplicity and development velocity."
              }
            ]
          },
          {
            "name": "dagu",
            "description": "Dagu workflow orchestration: workflows, web UI, REST API",
            "source": "./dagu",
            "category": "tools",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install dagu@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "dagu-rest-api",
                "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems",
                "path": "dagu/skills/rest-api/SKILL.md",
                "frontmatter": {
                  "name": "dagu-rest-api",
                  "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems"
                },
                "content": "# Dagu REST API\n\nUse this skill when integrating Dagu with external systems, automating workflow operations, or programmatically managing workflows through the API.\n\n## When to Use This Skill\n\nActivate when:\n- Triggering workflows programmatically\n- Querying workflow status from applications\n- Building automation around Dagu\n- Integrating Dagu with CI/CD pipelines\n- Creating custom dashboards or monitoring tools\n- Scheduling workflows dynamically\n- Fetching execution logs programmatically\n\n## Core API Capabilities\n\nThe Dagu REST API provides endpoints for:\n\n1. **Workflow Operations** - Start, stop, retry workflows\n2. **Status Queries** - Get workflow and execution status\n3. **DAG Management** - List and inspect workflow definitions\n4. **Execution History** - Query past executions\n5. **Log Retrieval** - Fetch execution logs\n\n## Base URL\n\nDefault API base URL: `http://localhost:8080/api/v1`\n\nConfigure in Dagu settings if using a different host/port.\n\n## Authentication\n\nConsult `references/authentication.md` for details on:\n- API token configuration\n- Authentication headers\n- Security best practices\n\n## Quick Start Operations\n\n### Start a Workflow\n\n```bash\nPOST /dags/{dagName}/start\n```\n\nBasic example:\n```bash\ncurl -X POST http://localhost:8080/api/v1/dags/my_workflow/start\n```\n\nFor parameter passing and advanced options, see `references/workflow-operations.md`.\n\n### Get Workflow Status\n\n```bash\nGET /dags/{dagName}/status\n```\n\nReturns current status, running steps, and execution details.\n\n### Stop a Workflow\n\n```bash\nPOST /dags/{dagName}/stop\n```\n\nStops currently running execution.\n\n## When to Consult References\n\n- **Detailed endpoint documentation**: Read `references/api-endpoints.md`\n- **Workflow operations (start/stop/retry)**: Read `references/workflow-operations.md`\n- **Status and monitoring queries**: Read `references/status-queries.md`\n- **Authentication setup**: Read `references/authentication.md`\n- **Integration examples**: Read `references/integration-examples.md`\n- **Error handling**: Read `references/error-handling.md`\n\n## Common Use Cases\n\n### CI/CD Integration\n\nTrigger Dagu workflows from your CI/CD pipeline:\n\n```bash\n# In GitHub Actions, GitLab CI, etc.\ncurl -X POST http://dagu-server:8080/api/v1/dags/deploy_production/start \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": \"VERSION=1.2.3 ENVIRONMENT=production\"}'\n```\n\nFor complete CI/CD integration patterns, see `references/integration-examples.md`.\n\n### Monitoring and Alerting\n\nQuery workflow status for external monitoring:\n\n```bash\n# Check if workflow is running\ncurl http://localhost:8080/api/v1/dags/critical_job/status\n```\n\nBuild custom alerts based on status responses. See `references/status-queries.md` for response format details.\n\n### Dynamic Scheduling\n\nTrigger workflows based on external events:\n\n```python\nimport requests\n\ndef trigger_workflow(dag_name, params=None):\n    url = f\"http://localhost:8080/api/v1/dags/{dag_name}/start\"\n    data = {\"params\": params} if params else {}\n    response = requests.post(url, json=data)\n    return response.json()\n```\n\nFor comprehensive examples in multiple languages, see `references/integration-examples.md`.\n\n## Response Formats\n\nAll API responses are JSON. Common response structure:\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": { ... }\n}\n```\n\nError responses:\n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error description\"\n}\n```\n\nFor complete response schemas, consult `references/api-endpoints.md`.\n\n## Key Principles\n\n- **RESTful design**: Standard HTTP methods (GET, POST, DELETE)\n- **JSON responses**: All responses in JSON format\n- **Idempotent operations**: Safe to retry most operations\n- **Error codes**: Standard HTTP status codes\n- **Stateless**: Each request is independent\n\n## Pro Tips\n\n- Use the API for automation, use Web UI for manual operations\n- Implement retry logic for network failures\n- Cache DAG lists if querying frequently\n- Use webhooks for event-driven workflows when possible\n- Monitor API response times for performance issues\n- Validate workflow names before calling API to avoid errors"
              },
              {
                "name": "dagu-webui",
                "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface",
                "path": "dagu/skills/webui/SKILL.md",
                "frontmatter": {
                  "name": "dagu-webui",
                  "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface"
                },
                "content": "# Dagu Web UI\n\nUse this skill when working with Dagu's web interface to manage workflows, view execution history, monitor running workflows, or configure the UI.\n\n## When to Use This Skill\n\nActivate when:\n- Navigating the Dagu web interface\n- Starting, stopping, or retrying workflows via UI\n- Viewing workflow execution logs and status\n- Monitoring running workflows\n- Managing workflow history\n- Configuring workflow schedules through the UI\n- Troubleshooting workflow issues using the UI\n\n## Core Capabilities\n\nThe Dagu Web UI provides:\n\n1. **Workflow Management** - View, start, stop, and manage workflows\n2. **Execution Monitoring** - Real-time status and logs\n3. **History Viewing** - Past execution records and results\n4. **DAG Visualization** - Visual representation of workflow structure\n5. **Log Access** - View detailed execution logs\n6. **Schedule Management** - Configure when workflows run\n\n## Quick Start\n\nAccess Dagu Web UI at `http://localhost:8080` (default) after starting Dagu:\n\n```bash\ndagu server\n```\n\n## Primary Operations\n\n### Start a Workflow\n\nTo manually execute a workflow:\n1. Navigate to workflow list\n2. Click the workflow name\n3. Click \"Start\" button\n4. View real-time execution progress\n\n### Monitor Execution\n\nFor detailed information on a running workflow, consult `references/monitoring.md` which covers:\n- Reading execution logs\n- Understanding status indicators\n- Tracking step progress\n- Identifying failures\n\n### View History\n\nTo review past executions, see `references/history.md` for guidance on:\n- Filtering execution history\n- Analyzing failed runs\n- Comparing execution times\n- Exporting execution data\n\n### Workflow Visualization\n\nThe DAG view shows workflow structure. For detailed visualization features, see `references/visualization.md`.\n\n## When to Consult References\n\n- **Detailed UI navigation**: Read `references/ui-navigation.md`\n- **Advanced monitoring**: Read `references/monitoring.md`\n- **History analysis**: Read `references/history.md`\n- **Workflow editing via UI**: Read `references/workflow-editor.md`\n- **Configuration options**: Read `references/configuration.md`\n\n## Common Tasks\n\n### Restart a Failed Workflow\n\n1. Find the failed execution in history\n2. Click the retry/restart button\n3. Monitor the new execution\n\n### Stop a Running Workflow\n\n1. Navigate to the running workflow\n2. Click \"Stop\" or \"Cancel\"\n3. Confirm the action\n4. View cleanup handlers execution\n\n### View Detailed Logs\n\nWhen you need to debug a workflow:\n1. Click on the specific workflow execution\n2. Select the step with issues\n3. View stdout/stderr logs\n4. Check for error messages\n\nFor advanced log analysis, consult `references/monitoring.md`.\n\n## Key Principles\n\n- **Real-time visibility**: Web UI provides live updates of workflow execution\n- **Click-based operations**: No CLI needed for basic workflow management\n- **History preservation**: All executions are logged and accessible\n- **Visual feedback**: Status indicators show current state at a glance\n- **Log accessibility**: Detailed logs available for debugging\n\n## Pro Tips\n\n- Use the search feature to quickly find workflows by name\n- Filter execution history by date range or status\n- Click on step names in DAG view for step-specific details\n- Use the refresh button if live updates seem delayed\n- Check the scheduler status to verify cron jobs are active"
              },
              {
                "name": "dagu-workflows",
                "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition",
                "path": "dagu/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "dagu-workflows",
                  "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition"
                },
                "content": "# Dagu Workflow Authoring\n\nThis skill activates when creating or modifying Dagu workflow definitions, configuring workflow steps, scheduling, or composing complex workflows.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Dagu workflow YAML files\n- Configuring workflow steps and executors\n- Setting up workflow scheduling with cron\n- Defining step dependencies and data flow\n- Implementing error handling and retries\n- Composing hierarchical workflows\n- Using environment variables and parameters\n\n## Basic Workflow Structure\n\n### Minimal Workflow\n\n```yaml\n# hello.yaml\nsteps:\n  - name: hello\n    command: echo \"Hello from Dagu!\"\n```\n\n### Complete Workflow Structure\n\n```yaml\nname: my_workflow\ndescription: Description of what this workflow does\n\n# Schedule (optional)\nschedule: \"0 2 * * *\"  # Cron format: daily at 2 AM\n\n# Environment variables\nenv:\n  - KEY: value\n  - DB_HOST: localhost\n\n# Parameters\nparams: ENVIRONMENT=production\n\n# Email notifications (optional)\nmailOn:\n  failure: true\n  success: false\n\nsmtp:\n  host: smtp.example.com\n  port: 587\n\nerrorMail:\n  from: dagu@example.com\n  to: alerts@example.com\n\n# Workflow steps\nsteps:\n  - name: step1\n    command: echo \"First step\"\n\n  - name: step2\n    command: echo \"Second step\"\n    depends:\n      - step1\n```\n\n## Steps\n\n### Basic Step\n\n```yaml\nsteps:\n  - name: greet\n    command: echo \"Hello, World!\"\n```\n\n### Step with Script\n\n```yaml\nsteps:\n  - name: process\n    command: |\n      echo \"Starting processing...\"\n      ./scripts/process.sh\n      echo \"Done!\"\n```\n\n### Step with Working Directory\n\n```yaml\nsteps:\n  - name: build\n    dir: /path/to/project\n    command: make build\n```\n\n### Step with Environment Variables\n\n```yaml\nsteps:\n  - name: deploy\n    env:\n      - ENVIRONMENT: production\n      - API_KEY: $API_KEY  # From global env\n    command: ./deploy.sh\n```\n\n## Executors\n\n### Command Executor (Default)\n\n```yaml\nsteps:\n  - name: shell_command\n    command: ./script.sh\n```\n\n### Docker Executor\n\n```yaml\nsteps:\n  - name: run_in_container\n    executor:\n      type: docker\n      config:\n        image: alpine:latest\n    command: echo \"Running in Docker\"\n\n  - name: with_volumes\n    executor:\n      type: docker\n      config:\n        image: node:18\n        volumes:\n          - /host/path:/container/path\n        env:\n          - NODE_ENV=production\n    command: npm run build\n```\n\n### SSH Executor\n\n```yaml\nsteps:\n  - name: remote_execution\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: server.example.com\n        key: /path/to/ssh/key\n    command: ./remote_script.sh\n```\n\n### HTTP Executor\n\n```yaml\nsteps:\n  - name: api_call\n    executor:\n      type: http\n      config:\n        method: POST\n        url: https://api.example.com/webhook\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer $API_TOKEN\n        body: |\n          {\n            \"event\": \"workflow_complete\",\n            \"timestamp\": \"{{.timestamp}}\"\n          }\n```\n\n### Mail Executor\n\n```yaml\nsteps:\n  - name: send_notification\n    executor:\n      type: mail\n      config:\n        to: user@example.com\n        from: dagu@example.com\n        subject: Workflow Complete\n        message: |\n          The workflow has completed successfully.\n          Time: {{.timestamp}}\n```\n\n### JQ Executor\n\n```yaml\nsteps:\n  - name: transform_json\n    executor:\n      type: jq\n      config:\n        query: '.users[] | select(.active == true) | .email'\n    command: cat users.json\n```\n\n## Step Dependencies\n\n### Simple Dependencies\n\n```yaml\nsteps:\n  - name: download\n    command: wget https://example.com/data.zip\n\n  - name: extract\n    depends:\n      - download\n    command: unzip data.zip\n\n  - name: process\n    depends:\n      - extract\n    command: ./process.sh\n```\n\n### Multiple Dependencies\n\n```yaml\nsteps:\n  - name: fetch_data\n    command: ./fetch.sh\n\n  - name: fetch_config\n    command: ./fetch_config.sh\n\n  - name: process\n    depends:\n      - fetch_data\n      - fetch_config\n    command: ./process.sh\n```\n\n### Parallel Execution\n\n```yaml\n# These run in parallel (no dependencies)\nsteps:\n  - name: task1\n    command: ./task1.sh\n\n  - name: task2\n    command: ./task2.sh\n\n  - name: task3\n    command: ./task3.sh\n\n  # This waits for all above to complete\n  - name: finalize\n    depends:\n      - task1\n      - task2\n      - task3\n    command: ./finalize.sh\n```\n\n## Conditional Execution\n\n### Preconditions\n\n```yaml\nsteps:\n  - name: deploy_production\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    command: ./deploy.sh\n```\n\n### Continue On Failure\n\n```yaml\nsteps:\n  - name: optional_step\n    continueOn:\n      failure: true\n    command: ./might_fail.sh\n\n  - name: cleanup\n    depends:\n      - optional_step\n    command: ./cleanup.sh  # Runs even if optional_step fails\n```\n\n## Error Handling and Retries\n\n### Retry Configuration\n\n```yaml\nsteps:\n  - name: flaky_api_call\n    command: curl https://api.example.com/data\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n```\n\n### Exponential Backoff\n\n```yaml\nsteps:\n  - name: with_backoff\n    command: ./external_api.sh\n    retryPolicy:\n      limit: 5\n      intervalSec: 5\n      exponentialBackoff: true  # 5s, 10s, 20s, 40s, 80s\n```\n\n### Signal on Stop\n\n```yaml\nsteps:\n  - name: graceful_shutdown\n    command: ./long_running_process.sh\n    signalOnStop: SIGTERM  # Send SIGTERM instead of SIGKILL\n```\n\n## Data Flow\n\n### Output Variables\n\n```yaml\nsteps:\n  - name: generate_id\n    command: echo \"ID_$(date +%s)\"\n    output: PROCESS_ID\n\n  - name: use_id\n    depends:\n      - generate_id\n    command: echo \"Processing with ID: $PROCESS_ID\"\n```\n\n### Script Output\n\n```yaml\nsteps:\n  - name: get_config\n    script: |\n      #!/bin/bash\n      export DB_HOST=\"localhost\"\n      export DB_PORT=\"5432\"\n    output: DB_CONFIG\n\n  - name: connect\n    depends:\n      - get_config\n    command: ./connect.sh $DB_HOST $DB_PORT\n```\n\n## Scheduling\n\n### Cron Schedule\n\n```yaml\n# Daily at 2 AM\nschedule: \"0 2 * * *\"\n\n# Every Monday at 9 AM\nschedule: \"0 9 * * 1\"\n\n# Every 15 minutes\nschedule: \"*/15 * * * *\"\n\n# First day of month at midnight\nschedule: \"0 0 1 * *\"\n```\n\n### Start/Stop Times\n\n```yaml\n# Only run during business hours\nschedule:\n  start: \"2024-01-01\"\n  end: \"2024-12-31\"\n  cron: \"0 9-17 * * 1-5\"  # Mon-Fri, 9 AM to 5 PM\n```\n\n## Environment Variables\n\n### Global Environment\n\n```yaml\nenv:\n  - ENVIRONMENT: production\n  - LOG_LEVEL: info\n  - API_URL: https://api.example.com\n\nsteps:\n  - name: use_env\n    command: echo \"Environment: $ENVIRONMENT\"\n```\n\n### Step-Level Environment\n\n```yaml\nsteps:\n  - name: with_custom_env\n    env:\n      - CUSTOM_VAR: value\n      - OVERRIDE: step_value\n    command: ./script.sh\n```\n\n### Environment from File\n\n```yaml\nenv:\n  - .env  # Load from .env file\n\nsteps:\n  - name: use_env_file\n    command: echo \"DB_HOST: $DB_HOST\"\n```\n\n## Parameters\n\n### Defining Parameters\n\n```yaml\nparams: ENVIRONMENT=development VERSION=1.0.0\n\nsteps:\n  - name: deploy\n    command: ./deploy.sh $ENVIRONMENT $VERSION\n```\n\n### Using Parameters\n\n```bash\n# Run with default parameters\ndagu start workflow.yaml\n\n# Override parameters\ndagu start workflow.yaml ENVIRONMENT=production VERSION=2.0.0\n```\n\n## Sub-Workflows\n\n### Calling Sub-Workflows\n\n```yaml\n# main.yaml\nsteps:\n  - name: run_sub_workflow\n    run: sub_workflow.yaml\n    params: PARAM=value\n\n  - name: another_sub\n    run: workflows/another.yaml\n```\n\n### Hierarchical Workflows\n\n```yaml\n# orchestrator.yaml\nsteps:\n  - name: data_ingestion\n    run: workflows/ingest.yaml\n\n  - name: data_processing\n    depends:\n      - data_ingestion\n    run: workflows/process.yaml\n\n  - name: data_export\n    depends:\n      - data_processing\n    run: workflows/export.yaml\n```\n\n## Handlers\n\n### Cleanup Handler\n\n```yaml\nhandlerOn:\n  exit:\n    - name: cleanup\n      command: ./cleanup.sh\n\nsteps:\n  - name: main_task\n    command: ./task.sh\n```\n\n### Error Handler\n\n```yaml\nhandlerOn:\n  failure:\n    - name: send_alert\n      executor:\n        type: mail\n        config:\n          to: alerts@example.com\n          subject: \"Workflow Failed\"\n          message: \"Workflow {{.Name}} failed at {{.timestamp}}\"\n\nsteps:\n  - name: risky_operation\n    command: ./operation.sh\n```\n\n### Success Handler\n\n```yaml\nhandlerOn:\n  success:\n    - name: notify_success\n      command: ./notify.sh \"Workflow completed successfully\"\n\nsteps:\n  - name: task\n    command: ./task.sh\n```\n\n## Templates and Variables\n\n### Built-in Variables\n\n```yaml\nsteps:\n  - name: use_variables\n    command: |\n      echo \"Workflow: {{.Name}}\"\n      echo \"Step: {{.Step.Name}}\"\n      echo \"Timestamp: {{.timestamp}}\"\n      echo \"Request ID: {{.requestId}}\"\n```\n\n### Custom Templates\n\n```yaml\nparams: USER=alice\n\nsteps:\n  - name: templated\n    command: echo \"Hello, {{.Params.USER}}!\"\n```\n\n## Common Patterns\n\n### ETL Pipeline\n\n```yaml\nname: etl_pipeline\ndescription: Extract, Transform, Load data pipeline\n\nschedule: \"0 2 * * *\"  # Daily at 2 AM\n\nenv:\n  - DATA_SOURCE: s3://bucket/data\n  - TARGET_DB: postgresql://localhost/warehouse\n\nsteps:\n  - name: extract\n    command: ./extract.sh $DATA_SOURCE\n    output: EXTRACTED_FILE\n\n  - name: transform\n    depends:\n      - extract\n    command: ./transform.sh $EXTRACTED_FILE\n    output: TRANSFORMED_FILE\n\n  - name: load\n    depends:\n      - transform\n    command: ./load.sh $TRANSFORMED_FILE $TARGET_DB\n\n  - name: cleanup\n    depends:\n      - load\n    command: rm -f $EXTRACTED_FILE $TRANSFORMED_FILE\n\nhandlerOn:\n  failure:\n    - name: alert\n      executor:\n        type: mail\n        config:\n          to: data-team@example.com\n          subject: \"ETL Pipeline Failed\"\n```\n\n### Multi-Environment Deployment\n\n```yaml\nname: deploy\ndescription: Deploy application to multiple environments\n\nparams: ENVIRONMENT=staging VERSION=latest\n\nsteps:\n  - name: build\n    command: docker build -t app:$VERSION .\n\n  - name: test\n    depends:\n      - build\n    command: docker run app:$VERSION npm test\n\n  - name: deploy_staging\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"staging\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: staging.example.com\n    command: ./deploy.sh $VERSION\n\n  - name: deploy_production\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: prod.example.com\n    command: ./deploy.sh $VERSION\n```\n\n### Data Backup Workflow\n\n```yaml\nname: database_backup\ndescription: Automated database backup workflow\n\nschedule: \"0 3 * * *\"  # Daily at 3 AM\n\nenv:\n  - DB_HOST: localhost\n  - DB_NAME: myapp\n  - BACKUP_DIR: /backups\n  - S3_BUCKET: s3://backups/db\n\nsteps:\n  - name: create_backup\n    command: |\n      TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n      pg_dump -h $DB_HOST $DB_NAME > $BACKUP_DIR/backup_$TIMESTAMP.sql\n      echo \"backup_$TIMESTAMP.sql\"\n    output: BACKUP_FILE\n\n  - name: compress\n    depends:\n      - create_backup\n    command: gzip $BACKUP_DIR/$BACKUP_FILE\n    output: COMPRESSED_FILE\n\n  - name: upload_to_s3\n    depends:\n      - compress\n    command: aws s3 cp $BACKUP_DIR/$COMPRESSED_FILE.gz $S3_BUCKET/\n\n  - name: cleanup_old_backups\n    depends:\n      - upload_to_s3\n    command: |\n      find $BACKUP_DIR -name \"*.sql.gz\" -mtime +30 -delete\n      aws s3 ls $S3_BUCKET/ | awk '{print $4}' | head -n -30 | xargs -I {} aws s3 rm $S3_BUCKET/{}\n\nhandlerOn:\n  failure:\n    - name: alert_failure\n      executor:\n        type: mail\n        config:\n          to: dba@example.com\n          subject: \"Backup Failed\"\n  success:\n    - name: log_success\n      command: echo \"Backup completed at $(date)\" >> /var/log/backups.log\n```\n\n### Monitoring and Alerts\n\n```yaml\nname: health_check\ndescription: Monitor services and send alerts\n\nschedule: \"*/5 * * * *\"  # Every 5 minutes\n\nsteps:\n  - name: check_web_service\n    command: curl -f https://app.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_api_service\n    command: curl -f https://api.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_database\n    command: pg_isready -h db.example.com\n    continueOn:\n      failure: true\n\nhandlerOn:\n  failure:\n    - name: alert_on_failure\n      executor:\n        type: http\n        config:\n          method: POST\n          url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n          headers:\n            Content-Type: application/json\n          body: |\n            {\n              \"text\": \"⚠️ Service health check failed\",\n              \"attachments\": [{\n                \"color\": \"danger\",\n                \"fields\": [\n                  {\"title\": \"Workflow\", \"value\": \"{{.Name}}\", \"short\": true},\n                  {\"title\": \"Time\", \"value\": \"{{.timestamp}}\", \"short\": true}\n                ]\n              }]\n            }\n```\n\n## Best Practices\n\n### Workflow Organization\n\n```yaml\n# Good: Clear, descriptive names\nname: user_data_sync\ndescription: Synchronize user data from CRM to database\n\n# Good: Logical step names\nsteps:\n  - name: fetch_from_crm\n  - name: validate_data\n  - name: update_database\n\n# Avoid: Generic names\nname: workflow1\nsteps:\n  - name: step1\n  - name: step2\n```\n\n### Error Handling\n\n```yaml\n# Always define error handlers for critical workflows\nhandlerOn:\n  failure:\n    - name: cleanup\n      command: ./cleanup.sh\n    - name: notify\n      executor:\n        type: mail\n        config:\n          to: team@example.com\n\n# Use retries for flaky operations\nsteps:\n  - name: api_call\n    command: curl https://api.example.com\n    retryPolicy:\n      limit: 3\n      intervalSec: 5\n      exponentialBackoff: true\n```\n\n### Environment Management\n\n```yaml\n# Use parameters for environment-specific values\nparams: ENVIRONMENT=development\n\n# Load environment from files\nenv:\n  - config/$ENVIRONMENT.env\n\n# Override in production\n# dagu start workflow.yaml ENVIRONMENT=production\n```\n\n### Modular Workflows\n\n```yaml\n# Break complex workflows into sub-workflows\nsteps:\n  - name: data_ingestion\n    run: workflows/ingestion.yaml\n\n  - name: data_transformation\n    run: workflows/transformation.yaml\n    depends:\n      - data_ingestion\n```\n\n## Key Principles\n\n- **Keep workflows focused**: One workflow per logical task\n- **Use dependencies wisely**: Parallelize when possible\n- **Handle errors explicitly**: Define failure handlers\n- **Use retries for flaky operations**: Network calls, external APIs\n- **Parameterize configurations**: Make workflows reusable\n- **Document workflows**: Add clear names and descriptions\n- **Test workflows**: Start with small, focused workflows\n- **Monitor and alert**: Use handlers to track workflow health"
              }
            ]
          },
          {
            "name": "elixir",
            "description": "Elixir development skills: Phoenix, OTP, testing, configuration",
            "source": "./elixir",
            "category": "language",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install elixir@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "elixir-anti-patterns",
                "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices",
                "path": "elixir/skills/anti-patterns/SKILL.md",
                "frontmatter": {
                  "name": "elixir-anti-patterns",
                  "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices"
                },
                "content": "# Elixir Anti-Patterns Detection and Refactoring\n\nYou are an expert at identifying Elixir anti-patterns and suggesting idiomatic refactorings. Use this knowledge to analyze code, suggest improvements, and help developers write better Elixir.\n\n## Code-Related Anti-Patterns\n\n### 1. Comments Overuse\n**Problem:** Excessive or self-explanatory comments reduce readability rather than enhance it.\n\n**Detection:**\n- Inline comments explaining obvious code\n- Comments for every function line\n- Comments duplicating what code already says clearly\n\n**Refactoring:**\n- Use clear function and variable names instead of explanatory comments\n- Replace inline comments with `@doc` and `@moduledoc` for documentation\n- Use module attributes for configuration values\n\n**Example:**\n```elixir\n# Bad\ndef calculate() do\n  # Get current time\n  now = DateTime.utc_now()\n  # Add 5 minutes\n  DateTime.add(now, 5 * 60, :second)\nend\n\n# Good\n@minutes_to_add 5\n\ndef timestamp_five_minutes_from_now do\n  now = DateTime.utc_now()\n  DateTime.add(now, @minutes_to_add * 60, :second)\nend\n```\n\n### 2. Complex `else` Clauses in `with`\n**Problem:** Flattening all error handling into a single complex `else` block obscures which clause produced which error.\n\n**Detection:**\n- Large `else` blocks with many pattern match clauses\n- Difficulty determining error sources\n- Complex error handling logic in `else`\n\n**Refactoring:**\n- Normalize return types in private functions\n- Handle errors closer to their source\n- Let `with` focus on success paths\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, decoded} <- Jason.decode(content) do\n    {:ok, decoded}\n  else\n    {:error, :enoent} -> {:error, :file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_json}\n    {:error, reason} -> {:error, reason}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- read_file(path),\n       {:ok, config} <- parse_json(content) do\n    {:ok, config}\n  end\nend\n\ndefp read_file(path) do\n  case File.read(path) do\n    {:ok, content} -> {:ok, content}\n    {:error, :enoent} -> {:error, :file_not_found}\n    error -> error\n  end\nend\n\ndefp parse_json(content) do\n  case Jason.decode(content) do\n    {:ok, data} -> {:ok, data}\n    {:error, _} -> {:error, :invalid_json}\n  end\nend\n```\n\n### 3. Complex Extractions in Clauses\n**Problem:** Extracting values across multiple clauses and arguments makes it unclear which variables serve pattern/guard purposes versus function body usage.\n\n**Detection:**\n- Many variable extractions in function heads\n- Mixed guard and body variable usage\n- Unclear variable purposes\n\n**Refactoring:**\n- Extract only pattern/guard-related variables in function signatures\n- Use capture patterns like `%User{age: age} = user`\n- Extract body variables inside the clause\n\n**Example:**\n```elixir\n# Bad\ndef process(%User{age: age, name: name, email: email} = user) when age >= 18 do\n  # Only using name and email in body, not age\n  send_email(email, \"Hello #{name}\")\nend\n\n# Good\ndef process(%User{age: age} = user) when age >= 18 do\n  send_email(user.email, \"Hello #{user.name}\")\nend\n```\n\n### 4. Dynamic Atom Creation\n**Problem:** Atoms aren't garbage-collected and are limited to ~1 million. Uncontrolled dynamic atom creation poses memory and security risks.\n\n**Detection:**\n- `String.to_atom/1` with untrusted input\n- Converting user input directly to atoms\n- Unbounded atom creation in loops\n\n**Refactoring:**\n- Use explicit mappings via pattern-matching\n- Use `String.to_existing_atom/1` with pre-defined atoms\n- Keep strings when atom conversion isn't necessary\n\n**Example:**\n```elixir\n# Bad - Security risk!\ndef set_role(user, role_string) do\n  %{user | role: String.to_atom(role_string)}\nend\n\n# Good\ndef set_role(user, role) when role in [:admin, :editor, :viewer] do\n  %{user | role: role}\nend\n\n# Or with pattern matching\ndef set_role(user, \"admin\"), do: %{user | role: :admin}\ndef set_role(user, \"editor\"), do: %{user | role: :editor}\ndef set_role(user, \"viewer\"), do: %{user | role: :viewer}\ndef set_role(_user, invalid), do: {:error, \"Invalid role: #{invalid}\"}\n```\n\n### 5. Long Parameter List\n**Problem:** Functions with excessive parameters become confusing and error-prone to use.\n\n**Detection:**\n- Functions with 4+ parameters\n- Parameters that are conceptually related\n- Difficult to remember parameter order\n\n**Refactoring:**\n- Group related parameters into maps or structs\n- Use keyword lists for optional parameters\n- Create domain objects\n\n**Example:**\n```elixir\n# Bad\ndef create_loan(user_id, user_name, user_email, book_id, book_title, book_isbn) do\n  # ...\nend\n\n# Good\ndef create_loan(user, book) do\n  # ...\nend\n\n# Or with keyword list for options\ndef create_loan(user, book, opts \\\\ []) do\n  duration = Keyword.get(opts, :duration, 14)\n  renewable = Keyword.get(opts, :renewable, true)\n  # ...\nend\n```\n\n### 6. Namespace Trespassing\n**Problem:** Defining modules outside your library's namespace risks conflicts since the Erlang VM loads only one module instance per name.\n\n**Detection:**\n- Library defining modules in common namespaces (e.g., `Plug.*` when you're not Plug)\n- Modules without library prefix\n- Potential naming conflicts with other libraries\n\n**Refactoring:**\n- Always prefix modules with your library namespace\n- Use clear, unique top-level module names\n\n**Example:**\n```elixir\n# Bad - Library named :plug_auth\ndefmodule Plug.Auth do\n  # This conflicts with the actual Plug library!\nend\n\n# Good\ndefmodule PlugAuth do\n  # ...\nend\n\ndefmodule PlugAuth.Session do\n  # ...\nend\n```\n\n### 7. Non-assertive Map Access\n**Problem:** Using dynamic access (`map[:key]`) for required keys masks missing data, allowing `nil` to propagate instead of failing fast.\n\n**Detection:**\n- `map[:key]` for required/expected keys\n- Nil checks after map access\n- Silent failures from missing keys\n\n**Refactoring:**\n- Use static access (`map.key`) for required keys\n- Pattern-match on struct/map keys\n- Reserve dynamic access for optional fields\n\n**Example:**\n```elixir\n# Bad\ndef distance(point) do\n  x = point[:x]  # Returns nil if :x is missing!\n  y = point[:y]\n  :math.sqrt(x * x + y * y)  # Crashes on nil, but unclear why\nend\n\n# Good\ndef distance(%{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)  # Clear error if keys missing\nend\n\n# Or with structs\ndefmodule Point do\n  defstruct [:x, :y]\nend\n\ndef distance(%Point{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)\nend\n```\n\n### 8. Non-assertive Pattern Matching\n**Problem:** Writing defensive code that returns incorrect values instead of using pattern matching to assert expected structures causes silent failures.\n\n**Detection:**\n- Defensive nil checks instead of pattern matching\n- Functions returning invalid data on unexpected input\n- Avoiding crashes when crashes are appropriate\n\n**Refactoring:**\n- Use pattern matching to assert expected structures\n- Let functions crash on invalid input\n- Trust supervisors to handle failures\n\n**Example:**\n```elixir\n# Bad\ndef parse_query_param(param) do\n  case String.split(param, \"=\") do\n    [key, value] -> {key, value}\n    _ -> {\"\", \"\"}  # Silent failure!\n  end\nend\n\n# Good\ndef parse_query_param(param) do\n  [key, value] = String.split(param, \"=\")\n  {key, value}\nend\n# Crashes with clear error if format is wrong - this is good!\n```\n\n### 9. Non-assertive Truthiness\n**Problem:** Using truthiness operators (`&&`, `||`, `!`) when all operands are boolean is unnecessarily generic and unclear.\n\n**Detection:**\n- `&&`, `||`, `!` with boolean expressions\n- Comparisons like `is_binary(x) && is_integer(y)`\n- Mixing boolean and truthy logic\n\n**Refactoring:**\n- Use `and`, `or`, `not` for boolean-only operations\n- Reserve `&&`, `||`, `!` for truthy/falsy logic\n\n**Example:**\n```elixir\n# Bad\ndef valid_user?(name, age) do\n  is_binary(name) && is_integer(age) && age >= 18\nend\n\n# Good\ndef valid_user?(name, age) do\n  is_binary(name) and is_integer(age) and age >= 18\nend\n\n# Truthy operators are OK for nil/value checks\ndef get_name(user) do\n  user[:name] || \"Anonymous\"\nend\n```\n\n### 10. Structs with 32 Fields or More\n**Problem:** Structs with 32+ fields switch from Erlang's efficient flat-map representation to hash maps, increasing memory usage.\n\n**Detection:**\n- Struct definitions with 32+ fields\n- Large, flat data structures\n- Performance degradation with many fields\n\n**Refactoring:**\n- Nest optional fields into metadata structures\n- Use nested structs for related fields\n- Group frequently-accessed fields separately\n\n**Example:**\n```elixir\n# Bad\ndefmodule User do\n  defstruct [\n    :id, :email, :name, :age, :address, :city, :state, :zip,\n    :phone, :mobile, :fax, :company, :title, :department,\n    :created_at, :updated_at, :last_login, :login_count,\n    :preference1, :preference2, :preference3, :preference4,\n    # ... 15 more fields\n  ]\nend\n\n# Good\ndefmodule User do\n  defstruct [\n    :id,\n    :email,\n    :name,\n    :profile,      # Nested struct\n    :preferences,  # Nested struct\n    :metadata      # Nested struct\n  ]\nend\n\ndefmodule User.Profile do\n  defstruct [:age, :phone, :mobile, :address, :city, :state, :zip]\nend\n\ndefmodule User.Preferences do\n  defstruct [:theme, :notifications, :language]\nend\n```\n\n## Design-Related Anti-Patterns\n\n### 1. Alternative Return Types\n**Problem:** Functions with options that drastically change their return type make it unclear what the function actually returns.\n\n**Detection:**\n- Options that change return type structure\n- Functions returning different types based on flags\n- Unclear function contracts\n\n**Refactoring:**\n- Create separate, specifically-named functions\n- Keep return types consistent within a function\n\n**Example:**\n```elixir\n# Bad\ndef parse(string, opts \\\\ []) do\n  case Integer.parse(string) do\n    {int, rest} ->\n      if opts[:discard_rest], do: int, else: {int, rest}\n    :error ->\n      :error\n  end\nend\n\n# Good\ndef parse(string) do\n  case Integer.parse(string) do\n    {int, rest} -> {int, rest}\n    :error -> :error\n  end\nend\n\ndef parse_discard_rest(string) do\n  case Integer.parse(string) do\n    {int, _rest} -> int\n    :error -> :error\n  end\nend\n```\n\n### 2. Boolean Obsession\n**Problem:** Using multiple booleans with overlapping states instead of atoms or composite types to represent domain concepts.\n\n**Detection:**\n- Multiple boolean parameters\n- Overlapping boolean states\n- Complex boolean logic\n\n**Refactoring:**\n- Replace multiple booleans with a single atom/enum option\n- Prefer atoms over booleans even for single arguments\n- Use domain-specific types\n\n**Example:**\n```elixir\n# Bad\ndef create_user(name, email, admin: false, editor: false, viewer: true) do\n  # What if admin: true, editor: true?\nend\n\n# Good\ndef create_user(name, email, role: :viewer) do\n  # Clear: role can be :admin, :editor, or :viewer\nend\n```\n\n### 3. Exceptions for Control-Flow\n**Problem:** Using `try/rescue` for expected errors instead of pattern matching with case statements and tuple returns.\n\n**Detection:**\n- `try/rescue` blocks for normal operation errors\n- Using `!` functions and rescuing\n- Exceptions in normal business logic\n\n**Refactoring:**\n- Use non-bang functions returning `{:ok, value}` or `{:error, reason}`\n- Reserve exceptions for invalid arguments and programming errors\n- Use pattern matching for error handling\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  try do\n    content = File.read!(path)\n    Jason.decode!(content)\n  rescue\n    e -> {:error, e}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  end\nend\n```\n\n### 4. Primitive Obsession\n**Problem:** Excessively using basic types (strings, integers) instead of creating composite types to represent structured domain concepts.\n\n**Detection:**\n- Passing related primitives separately\n- String/integer parameters representing complex concepts\n- Lack of domain modeling\n\n**Refactoring:**\n- Create domain-specific structs or maps\n- Introduce parser functions converting primitives to structured data\n- Use types to enforce business rules\n\n**Example:**\n```elixir\n# Bad\ndef create_address(street, city, state, zip, country) do\n  # All strings, no validation\n  \"#{street}, #{city}, #{state} #{zip}, #{country}\"\nend\n\n# Good\ndefmodule Address do\n  defstruct [:street, :city, :state, :zip, :country]\n\n  def new(attrs) do\n    struct!(__MODULE__, attrs)\n  end\n\n  def format(%__MODULE__{} = address) do\n    \"#{address.street}, #{address.city}, #{address.state} #{address.zip}, #{address.country}\"\n  end\nend\n```\n\n### 5. Unrelated Multi-Clause Function\n**Problem:** Grouping completely unrelated business logic into one multi-clause function.\n\n**Detection:**\n- Single function handling multiple unrelated types\n- Overly broad type specifications\n- No conceptual relationship between clauses\n\n**Refactoring:**\n- Split into distinct functions with specific names\n- Reserve multi-clause patterns for related functionality variations\n- Use protocols for polymorphism when appropriate\n\n**Example:**\n```elixir\n# Bad\ndef update(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update(%Animal{} = animal) do\n  # Completely different animal logic\nend\n\n# Good\ndef update_product(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update_animal(%Animal{} = animal) do\n  # Animal-specific logic\nend\n\n# Or use a protocol\ndefprotocol Updatable do\n  def update(item)\nend\n\ndefimpl Updatable, for: Product do\n  def update(product), do: # ...\nend\n\ndefimpl Updatable, for: Animal do\n  def update(animal), do: # ...\nend\n```\n\n### 6. Using Application Configuration for Libraries\n**Problem:** Libraries relying on global application environment configuration prevent multiple dependent applications from configuring the library differently.\n\n**Detection:**\n- `Application.get_env/2` or `Application.fetch_env!/2` in library code\n- Global configuration requirements\n- Inability to configure per-consumer\n\n**Refactoring:**\n- Accept configuration via function parameters\n- Use keyword lists with sensible defaults\n- Allow runtime configuration\n\n**Example:**\n```elixir\n# Bad - Library code\ndef split(string) do\n  parts = Application.fetch_env!(:dash_splitter, :parts)\n  String.split(string, \"-\", parts: parts)\nend\n\n# Good\ndef split(string, opts \\\\ []) do\n  parts = Keyword.get(opts, :parts, 2)\n  String.split(string, \"-\", parts: parts)\nend\n```\n\n## Usage Guidelines\n\nWhen reviewing or writing Elixir code:\n\n1. **Scan for anti-patterns** - Check code against the patterns listed above\n2. **Explain the problem** - Help the developer understand why it's an issue\n3. **Suggest refactoring** - Provide concrete, idiomatic alternatives\n4. **Consider context** - Sometimes anti-patterns are acceptable for specific use cases\n5. **Prioritize** - Focus on high-impact issues first (security, performance, maintainability)\n\n## Key Principles\n\n- **Let it crash** - Use pattern matching to assert expectations; don't write defensive code\n- **Fail fast** - Expose errors early rather than propagating nil or invalid data\n- **Be explicit** - Prefer clear, specific code over clever or terse solutions\n- **Model your domain** - Create types that represent business concepts\n- **Design for clarity** - Code should be obvious to read and maintain"
              },
              {
                "name": "elixir-config",
                "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                "path": "elixir/skills/config/SKILL.md",
                "frontmatter": {
                  "name": "elixir-config",
                  "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                  "license": "MIT"
                },
                "content": "# Elixir Configuration\n\nGuide for proper application configuration in Elixir, with emphasis on understanding and correctly using runtime vs compile-time configuration.\n\n## When to Activate\n\nUse this skill when:\n- Setting up or modifying application configuration\n- Choosing between `config.exs` and `runtime.exs`\n- Deciding between `Application.compile_env` and `Application.get_env`\n- Debugging configuration-related issues\n- Working with releases or deployment configuration\n- Migrating from `use Mix.Config` to `import Config`\n- Writing libraries that need configuration\n\n## Critical Principle\n\n> **Runtime configuration is the preferred approach.** Only use compile-time configuration when values must affect compilation itself.\n\n## Configuration Files\n\n### config/config.exs (Compile-Time)\n\nEvaluated during project compilation, before your application starts.\n\n```elixir\nimport Config\n\n# Basic configuration\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  username: \"postgres\",\n  password: \"postgres\",\n  hostname: \"localhost\"\n\n# Environment-specific config\nconfig :my_app,\n  environment: config_env()\n\n# Import environment-specific config files\nimport_config \"#{config_env()}.exs\"\n```\n\n**Key characteristics:**\n- Runs at compile time\n- Uses `import Config` (not `use Mix.Config`)\n- Can use `config_env()` and `config_target()`\n- Can import other config files with `import_config/1`\n- Deep-merges keyword lists\n- **Library config.exs is NOT evaluated when used as a dependency**\n\n### config/runtime.exs (Runtime)\n\nEvaluated right before applications start in both Mix and releases.\n\n```elixir\nimport Config\n\n# Read from environment variables\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.get_env(\"DATABASE_PASSWORD\") || \"postgres\",\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n\n# Conditional runtime configuration\nif config_env() == :prod do\n  config :my_app, MyAppWeb.Endpoint,\n    secret_key_base: System.fetch_env!(\"SECRET_KEY_BASE\"),\n    http: [port: String.to_integer(System.fetch_env!(\"PORT\"))]\nend\n```\n\n**Key characteristics:**\n- Runs at application startup (both dev and prod)\n- Executes in both Mix projects and releases\n- Perfect for environment variables and runtime values\n- **Does NOT support `import_config/1`**\n- Can use `System.get_env` and `System.fetch_env!`\n\n### config/dev.exs, config/test.exs, config/prod.exs\n\nEnvironment-specific compile-time configuration, typically imported from `config.exs`:\n\n```elixir\n# config/config.exs\nimport_config \"#{config_env()}.exs\"\n\n# config/dev.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  show_sensitive_data_on_connection_error: true,\n  pool_size: 10\n\n# config/test.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox,\n  pool_size: 10\n\n# config/prod.exs\nimport Config\n\n# Production-specific compile-time config only\nconfig :my_app, MyAppWeb.Endpoint,\n  cache_static_manifest: \"priv/static/cache_manifest.json\"\n```\n\n## Accessing Configuration\n\n### Runtime Access (Preferred)\n\nUse in function bodies to read configuration at runtime:\n\n#### Application.get_env/3\n\n```elixir\ndefmodule MyApp.Service do\n  def start_link do\n    # Get with default value\n    timeout = Application.get_env(:my_app, :timeout, 5000)\n    GenServer.start_link(__MODULE__, timeout, name: __MODULE__)\n  end\nend\n```\n\n**When to use:**\n- Reading config in function bodies (most common)\n- When a sensible default exists\n- When config might change between environments\n\n#### Application.fetch_env!/2\n\n```elixir\ndefmodule MyApp.Mailer do\n  def deliver(email) do\n    # Raise if not configured (for required config)\n    api_key = Application.fetch_env!(:my_app, :mailgun_api_key)\n    send_email(email, api_key)\n  end\nend\n```\n\n**When to use:**\n- Required configuration that must exist\n- When you want explicit errors for missing config\n- When no sensible default exists\n\n#### Application.fetch_env/2\n\n```elixir\ndefmodule MyApp.Cache do\n  def get(key) do\n    case Application.fetch_env(:my_app, :cache_adapter) do\n      {:ok, adapter} -> adapter.get(key)\n      :error -> nil  # No caching configured\n    end\n  end\nend\n```\n\n**When to use:**\n- Optional configuration\n- When you need pattern matching on result\n- When absence of config is a valid state\n\n### Compile-Time Access (Use Sparingly)\n\nUse only when configuration must affect compilation:\n\n#### Application.compile_env/3\n\n```elixir\ndefmodule MyApp.JSONEncoder do\n  # Only use compile_env when the value affects compilation\n  @json_library Application.compile_env(:my_app, :json_library, Jason)\n\n  def encode(data) do\n    # The specific library is compiled into the module\n    @json_library.encode(data)\n  end\nend\n```\n\n**When to use:**\n- Configuration affects which code gets compiled\n- Performance-critical paths where indirection is costly\n- Compile-time optimizations or code generation\n\n**Warning:** Mix tracks compile-time config and raises errors if values diverge between compile and runtime.\n\n#### Application.compile_env!/2\n\n```elixir\ndefmodule MyApp.Adapter do\n  # Raises at compile time if not configured\n  @adapter Application.compile_env!(:my_app, :storage_adapter)\n\n  def store(data) do\n    @adapter.put(data)\n  end\nend\n```\n\n**When to use:**\n- Required compile-time configuration\n- Adapters or behaviors selected at compile time\n\n## Common Patterns\n\n### Pattern 1: Environment Variables in Runtime\n\n**Correct approach:**\n\n```elixir\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_url: System.get_env(\"API_URL\") || \"http://localhost:4000\",\n  api_key: System.fetch_env!(\"API_KEY\")  # Required in production\n```\n\n**Access in code:**\n\n```elixir\ndefmodule MyApp.Client do\n  def call(endpoint) do\n    api_url = Application.fetch_env!(:my_app, :api_url)\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(\"#{api_url}/#{endpoint}\", [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### Pattern 2: Development vs Production Config\n\n**config/config.exs:**\n\n```elixir\nimport Config\n\n# Shared configuration for all environments\nconfig :my_app, :shared_setting, \"value\"\n\n# Import environment-specific config\nimport_config \"#{config_env()}.exs\"\n```\n\n**config/dev.exs:**\n\n```elixir\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  show_sensitive_data_on_connection_error: true\n```\n\n**config/runtime.exs:**\n\n```elixir\nimport Config\n\n# Runtime config for all environments\nif config_env() == :prod do\n  # Production-specific runtime config\n  database_url = System.fetch_env!(\"DATABASE_URL\")\n\n  config :my_app, MyApp.Repo,\n    url: database_url,\n    pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\nend\n```\n\n### Pattern 3: Storing config_env() for Runtime Access\n\n**Problem:** Can't call `config_env()` at runtime.\n\n**Solution:** Store it in config:\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app, :environment, config_env()\n\n# Then in your code:\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\n\n  def development? do\n    environment() == :dev\n  end\nend\n```\n\n### Pattern 4: Optional Features Based on Config\n\n```elixir\ndefmodule MyApp.Telemetry do\n  def setup do\n    case Application.fetch_env(:my_app, :telemetry_backend) do\n      {:ok, :datadog} -> setup_datadog()\n      {:ok, :prometheus} -> setup_prometheus()\n      :error -> :ok  # Telemetry disabled\n    end\n  end\nend\n```\n\n### Pattern 5: Child Spec with Runtime Config\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      MyApp.Repo,\n      {MyApp.Worker, Application.fetch_env!(:my_app, :worker_opts)},\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Using compile_env for Runtime Values\n\n```elixir\n# DON'T: Using compile_env for environment variables\ndefmodule MyApp.Service do\n  @api_key Application.compile_env(:my_app, :api_key)\n\n  def call do\n    # This won't work correctly in releases!\n    HTTPoison.get(url, [{\"Authorization\", @api_key}])\n  end\nend\n```\n\n**Why it's wrong:** Environment variables aren't available at compile time in releases.\n\n**Correct approach:**\n\n```elixir\ndefmodule MyApp.Service do\n  def call do\n    # Read at runtime\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(url, [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### ❌ Reading Other Application's Config\n\n```elixir\n# DON'T: Directly access other app's configuration\ndefmodule MyApp do\n  def logger_level do\n    Application.get_env(:logger, :level)  # Fragile coupling\n  end\nend\n```\n\n**Why it's wrong:** Creates tight coupling and breaks encapsulation.\n\n**Correct approach:**\n\n```elixir\n# Configure it in your own app\n# config/config.exs\nconfig :my_app, :log_level, :info\n\n# Then read your own config\ndefmodule MyApp do\n  def log_level do\n    Application.get_env(:my_app, :log_level, :info)\n  end\nend\n```\n\n### ❌ Using Application Config in Libraries\n\n```elixir\n# DON'T: In a library\ndefmodule MyLibrary do\n  def process(data) do\n    # Library reading its own application environment\n    timeout = Application.get_env(:my_library, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n```\n\n**Why it's wrong:** Library `config.exs` is not evaluated when used as a dependency.\n\n**Correct approach:**\n\n```elixir\n# DO: Accept options as arguments\ndefmodule MyLibrary do\n  def process(data, opts \\\\ []) do\n    timeout = Keyword.get(opts, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n\n# Users configure in their application\ndefmodule MyApp.Worker do\n  def run do\n    opts = Application.get_env(:my_app, :my_library_opts, [])\n    MyLibrary.process(data, opts)\n  end\nend\n```\n\n### ❌ Using Mix Module in Application Code\n\n```elixir\n# DON'T: Use Mix.env() in application code\ndefmodule MyApp do\n  def environment do\n    Mix.env()  # Won't work in releases!\n  end\nend\n```\n\n**Why it's wrong:** `Mix` is not available in production releases.\n\n**Correct approach:**\n\n```elixir\n# Store it in config\n# config/config.exs\nconfig :my_app, :environment, config_env()\n\n# Access from application environment\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\nend\n```\n\n## Config Functions Reference\n\n### In Configuration Files\n\n| Function | Description | Where to Use |\n|----------|-------------|--------------|\n| `config/2` | Configure app with keyword list | All config files |\n| `config/3` | Configure app key with value | All config files |\n| `config_env/0` | Get current environment (`:dev`, `:test`, `:prod`) | All config files |\n| `config_target/0` | Get build target | All config files |\n| `import_config/1` | Import other config files | Not in `runtime.exs` |\n\n### In Application Code\n\n| Function | Return Type | Use Case |\n|----------|-------------|----------|\n| `Application.get_env/3` | `value \\| default` | Runtime with default |\n| `Application.fetch_env/2` | `{:ok, value} \\| :error` | Runtime with pattern matching |\n| `Application.fetch_env!/2` | `value` (raises if missing) | Required runtime config |\n| `Application.compile_env/3` | `value` | Compile-time with default |\n| `Application.compile_env!/2` | `value` (raises if missing) | Required compile-time config |\n\n## Migration Guide\n\n### From `use Mix.Config` to `import Config`\n\n**Old (deprecated):**\n\n```elixir\nuse Mix.Config\n\nconfig :my_app, :key, \"value\"\n\nif Mix.env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{Mix.env()}.exs\"\n```\n\n**New:**\n\n```elixir\nimport Config\n\nconfig :my_app, :key, \"value\"\n\nif config_env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{config_env()}.exs\"\n```\n\n**Changes:**\n1. Replace `use Mix.Config` with `import Config`\n2. Replace `Mix.env()` with `config_env()`\n3. Remove wildcard imports (not supported)\n\n### Moving Runtime Config to runtime.exs\n\n**Before (all in config.exs):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\"),  # Wrong place!\n  static_value: \"something\"\n```\n\n**After (split correctly):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  static_value: \"something\"\n\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\") || raise(\"API_KEY not set\")\n```\n\n## Best Practices Summary\n\n1. **Default to Runtime Configuration**: Use `Application.get_env/3` in function bodies\n2. **Use runtime.exs for Environment Variables**: Never read env vars in `config.exs`\n3. **Use compile_env Only When Necessary**: Only when config affects compilation\n4. **Libraries Should Not Use Application Config**: Accept options as function arguments\n5. **Never Use Mix in Application Code**: Use `config_env()` in config files, store result\n6. **Validate Required Config Early**: Use `fetch_env!/2` in application start for required values\n7. **Provide Sensible Defaults**: Use `get_env/3` with defaults for optional config\n8. **Document Configuration**: Add comments explaining what each config key does\n9. **Use runtime.exs for Releases**: Essential for Elixir releases and deployments\n10. **Store config_env() for Runtime Use**: Can't call `config_env()` outside config files\n\n## Debugging Configuration\n\n### Check Current Configuration\n\n```elixir\n# In IEx\nApplication.get_all_env(:my_app)\n\n# Check specific key\nApplication.fetch_env(:my_app, :some_key)\n\n# See all applications\nApplication.loaded_applications()\n```\n\n### Common Issues\n\n**Problem:** Config not available in tests\n\n```elixir\n# config/test.exs\nimport Config\n\nconfig :my_app, :test_value, \"configured\"\n```\n\n**Problem:** Different values in dev vs release\n\nCheck that `runtime.exs` is being used and environment variables are set correctly.\n\n**Problem:** Compile-time config not updating\n\n```bash\n# Clean and recompile\nmix clean\nmix compile\n```\n\n## Resources\n\n- **Config Module Docs**: https://hexdocs.pm/elixir/Config.html\n- **Application Module Docs**: https://hexdocs.pm/elixir/Application.html\n- **Runtime Configuration Guide**: https://hexdocs.pm/mix/Mix.Tasks.Release.html#module-runtime-configuration\n\n## Key Insights\n\n> \"Reading the application environment at runtime is the preferred approach.\"\n\n> \"If you are writing a library to be used by other developers, it is generally recommended to avoid the application environment, as the application environment is effectively a global storage.\"\n\n> \"config/config of a library is not evaluated when the library is used as a dependency, as configuration is always meant to configure the current project.\"\n\nConfiguration is a cross-cutting concern. Default to runtime configuration with `Application.get_env/3`, and only reach for compile-time configuration when you have a specific need for it that justifies the trade-offs."
              },
              {
                "name": "elixir-otp-concurrency",
                "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives",
                "path": "elixir/skills/otp/SKILL.md",
                "frontmatter": {
                  "name": "elixir-otp-concurrency",
                  "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives"
                },
                "content": "# Elixir OTP and Concurrency\n\nThis skill activates when working with OTP behaviors, building concurrent systems, managing processes, or implementing fault-tolerant architectures in Elixir.\n\n## When to Use This Skill\n\nActivate when:\n- Implementing GenServer, GenStage, Supervisor, or other OTP behaviors\n- Designing supervision trees and fault-tolerance strategies\n- Working with Tasks, Agents, or process management\n- Building concurrent or distributed systems\n- Managing application state\n- Troubleshooting process-related issues\n\n## OTP Behaviors\n\n### GenServer - Generic Server\n\nUse GenServer for stateful processes:\n\n```elixir\ndefmodule MyApp.Counter do\n  use GenServer\n\n  # Client API\n\n  def start_link(initial_value) do\n    GenServer.start_link(__MODULE__, initial_value, name: __MODULE__)\n  end\n\n  def increment do\n    GenServer.call(__MODULE__, :increment)\n  end\n\n  def get_value do\n    GenServer.call(__MODULE__, :get)\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(initial_value) do\n    {:ok, initial_value}\n  end\n\n  @impl true\n  def handle_call(:increment, _from, state) do\n    {:reply, state + 1, state + 1}\n  end\n\n  @impl true\n  def handle_call(:get, _from, state) do\n    {:reply, state, state}\n  end\nend\n```\n\n#### GenServer Best Practices\n\n- Use `call` for synchronous requests that need a response\n- Use `cast` for asynchronous fire-and-forget messages\n- Use `handle_info` for receiving regular messages\n- Keep server callbacks fast - delegate heavy work to Tasks\n- Name processes with `via` tuples or Registry for dynamic naming\n- Implement timeouts to prevent client processes from hanging\n\n#### GenServer Patterns\n\n**Background Work:**\n```elixir\ndef init(state) do\n  schedule_work()\n  {:ok, state}\nend\n\ndef handle_info(:work, state) do\n  do_work(state)\n  schedule_work()\n  {:noreply, state}\nend\n\ndefp schedule_work do\n  Process.send_after(self(), :work, 5000)\nend\n```\n\n**State Timeouts:**\n```elixir\ndef handle_call(:get, _from, state) do\n  {:reply, state, state, {:state_timeout, 30_000, :cleanup}}\nend\n\ndef handle_state_timeout(:cleanup, state) do\n  {:stop, :normal, state}\nend\n```\n\n### Supervisor - Process Supervision\n\nBuild supervision trees for fault tolerance:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  @impl true\n  def start(_type, _args) do\n    children = [\n      # Database connection pool\n      {MyApp.Repo, []},\n\n      # PubSub system\n      {Phoenix.PubSub, name: MyApp.PubSub},\n\n      # Custom supervisor\n      {MyApp.WorkerSupervisor, []},\n\n      # Individual workers\n      {MyApp.Cache, []},\n      {MyApp.RateLimiter, []},\n\n      # Web endpoint\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n#### Supervision Strategies\n\n**:one_for_one** - If a child dies, only that child is restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_one)\n```\n\n**:one_for_all** - If any child dies, all children are terminated and restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_all)\n```\n\n**:rest_for_one** - If a child dies, it and all children started after it are restarted\n```elixir\nSupervisor.start_link(children, strategy: :rest_for_one)\n```\n\n#### Dynamic Supervisors\n\nFor dynamically creating processes:\n\n```elixir\ndefmodule MyApp.WorkerSupervisor do\n  use DynamicSupervisor\n\n  def start_link(init_arg) do\n    DynamicSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__)\n  end\n\n  def start_worker(args) do\n    spec = {MyApp.Worker, args}\n    DynamicSupervisor.start_child(__MODULE__, spec)\n  end\n\n  @impl true\n  def init(_init_arg) do\n    DynamicSupervisor.init(strategy: :one_for_one)\n  end\nend\n```\n\n#### Restart Strategies\n\nConfigure child restart behavior:\n\n```elixir\nchildren = [\n  # Always restart (default)\n  {MyApp.CriticalWorker, restart: :permanent},\n\n  # Never restart\n  {MyApp.OneTimeTask, restart: :temporary},\n\n  # Only restart on abnormal exit\n  {MyApp.OptionalWorker, restart: :transient}\n]\n```\n\n### Task - Concurrent Work\n\n#### Fire-and-forget Tasks\n\nFor concurrent work without needing results:\n\n```elixir\nTask.start(fn ->\n  send_email(user, \"Welcome!\")\nend)\n```\n\n#### Awaited Tasks\n\nFor concurrent work with results:\n\n```elixir\ntask = Task.async(fn ->\n  expensive_computation()\nend)\n\n# Do other work...\n\nresult = Task.await(task, 5000)  # 5 second timeout\n```\n\n#### Supervised Tasks\n\nFor long-running tasks under supervision:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      {Task.Supervisor, name: MyApp.TaskSupervisor}\n    ]\n\n    Supervisor.start_link(children, strategy: :one_for_one)\n  end\nend\n\n# Use the supervised task\nTask.Supervisor.start_child(MyApp.TaskSupervisor, fn ->\n  long_running_operation()\nend)\n```\n\n#### Concurrent Map\n\nProcess collections concurrently:\n\n```elixir\n# Sequential\nresults = Enum.map(urls, &fetch_url/1)\n\n# Concurrent\nresults = Task.async_stream(urls, &fetch_url/1, max_concurrency: 10)\n         |> Enum.to_list()\n```\n\n### Agent - Simple State Management\n\nUse Agent for simple state:\n\n```elixir\n{:ok, agent} = Agent.start_link(fn -> %{} end, name: MyApp.Cache)\n\n# Get state\nvalue = Agent.get(MyApp.Cache, fn state -> Map.get(state, :key) end)\n\n# Update state\nAgent.update(MyApp.Cache, fn state -> Map.put(state, :key, value) end)\n\n# Get and update atomically\nAgent.get_and_update(MyApp.Cache, fn state ->\n  {Map.get(state, :key), Map.delete(state, :key)}\nend)\n```\n\n**When to use Agent vs GenServer:**\n- Use Agent for simple key-value state\n- Use GenServer when you need complex logic, callbacks, or process lifecycle management\n\n## Process Communication\n\n### send/receive\n\nBasic message passing:\n\n```elixir\n# Send message\nsend(pid, {:hello, \"world\"})\n\n# Receive message\nreceive do\n  {:hello, msg} -> IO.puts(msg)\nafter\n  5000 -> IO.puts(\"Timeout\")\nend\n```\n\n### Process Registration\n\nRegister processes by name:\n\n```elixir\n# Local registration\nProcess.register(self(), :my_process)\nsend(:my_process, :hello)\n\n# Via Registry\n{:ok, _} = Registry.start_link(keys: :unique, name: MyApp.Registry)\n\n{:ok, pid} = GenServer.start_link(MyWorker, nil,\n  name: {:via, Registry, {MyApp.Registry, \"worker_1\"}}\n)\n\n# Look up process\n[{pid, _}] = Registry.lookup(MyApp.Registry, \"worker_1\")\n```\n\n### Process Links and Monitors\n\n**Links** - Bidirectional, propagate exits:\n\n```elixir\n# Link processes\nProcess.link(pid)\n\n# Spawn linked\nspawn_link(fn -> do_work() end)\n```\n\n**Monitors** - Unidirectional, receive DOWN messages:\n\n```elixir\nref = Process.monitor(pid)\n\nreceive do\n  {:DOWN, ^ref, :process, ^pid, reason} ->\n    IO.puts(\"Process died: #{inspect(reason)}\")\nend\n```\n\n## Concurrency Patterns\n\n### Pipeline Pattern\n\nChain operations with concurrency:\n\n```elixir\ndefmodule Pipeline do\n  def process(data) do\n    data\n    |> async(&step1/1)\n    |> async(&step2/1)\n    |> async(&step3/1)\n    |> await_all()\n  end\n\n  defp async(input, fun) do\n    Task.async(fn -> fun.(input) end)\n  end\n\n  defp await_all(tasks) when is_list(tasks) do\n    Enum.map(tasks, &Task.await/1)\n  end\nend\n```\n\n### Worker Pool\n\nImplement a worker pool:\n\n```elixir\ndefmodule MyApp.WorkerPool do\n  use GenServer\n\n  def start_link(opts) do\n    pool_size = Keyword.get(opts, :size, 10)\n    GenServer.start_link(__MODULE__, pool_size, name: __MODULE__)\n  end\n\n  def execute(fun) do\n    GenServer.call(__MODULE__, {:execute, fun})\n  end\n\n  @impl true\n  def init(pool_size) do\n    workers = for _ <- 1..pool_size do\n      {:ok, pid} = Task.Supervisor.start_link()\n      pid\n    end\n\n    {:ok, %{workers: workers, index: 0}}\n  end\n\n  @impl true\n  def handle_call({:execute, fun}, _from, state) do\n    worker = Enum.at(state.workers, state.index)\n    task = Task.Supervisor.async_nolink(worker, fun)\n\n    new_index = rem(state.index + 1, length(state.workers))\n    {:reply, task, %{state | index: new_index}}\n  end\nend\n```\n\n### Backpressure with GenStage\n\nFor producer-consumer pipelines:\n\n```elixir\ndefmodule Producer do\n  use GenStage\n\n  def start_link(initial) do\n    GenStage.start_link(__MODULE__, initial, name: __MODULE__)\n  end\n\n  def init(initial) do\n    {:producer, initial}\n  end\n\n  def handle_demand(demand, state) do\n    events = Enum.to_list(state..state + demand - 1)\n    {:noreply, events, state + demand}\n  end\nend\n\ndefmodule Consumer do\n  use GenStage\n\n  def start_link() do\n    GenStage.start_link(__MODULE__, :ok)\n  end\n\n  def init(:ok) do\n    {:consumer, :ok}\n  end\n\n  def handle_events(events, _from, state) do\n    Enum.each(events, &process_event/1)\n    {:noreply, [], state}\n  end\nend\n```\n\n## ETS - Erlang Term Storage\n\nIn-memory key-value storage:\n\n```elixir\n# Create table\n:ets.new(:my_table, [:named_table, :public, read_concurrency: true])\n\n# Insert\n:ets.insert(:my_table, {:key, \"value\"})\n\n# Lookup\n[{:key, value}] = :ets.lookup(:my_table, :key)\n\n# Delete\n:ets.delete(:my_table, :key)\n\n# Match patterns\n:ets.match(:my_table, {:\"$1\", \"value\"})\n\n# Iterate\n:ets.foldl(fn {k, v}, acc -> [{k, v} | acc] end, [], :my_table)\n```\n\n### ETS Best Practices\n\n- Use `read_concurrency: true` for read-heavy workloads\n- Use `write_concurrency: true` for write-heavy workloads\n- Prefer `:set` (default) for unique keys\n- Use `:bag` or `:duplicate_bag` for multiple values per key\n- Always own ETS tables in a GenServer or Supervisor to prevent data loss\n\n## Error Handling and Fault Tolerance\n\n### Let It Crash Philosophy\n\nDesign for failure:\n\n```elixir\n# Don't do defensive programming\ndef process_order(order_id) do\n  # Let it crash if order doesn't exist\n  order = Repo.get!(Order, order_id)\n\n  # Let it crash if validation fails\n  {:ok, processed} = process(order)\n\n  processed\nend\n```\n\n### Proper Error Handling\n\nWhen to handle errors vs let crash:\n\n```elixir\n# Handle expected errors\ndef fetch_user(id) do\n  case HTTPoison.get(\"#{@api_url}/users/#{id}\") do\n    {:ok, %{status_code: 200, body: body}} ->\n      Jason.decode(body)\n\n    {:ok, %{status_code: 404}} ->\n      {:error, :not_found}\n\n    {:ok, %{status_code: status}} ->\n      {:error, {:unexpected_status, status}}\n\n    {:error, reason} ->\n      {:error, {:network_error, reason}}\n  end\nend\n\n# Let unexpected errors crash\ndef update_user!(id, params) do\n  user = Repo.get!(User, id)  # Crash if not found\n\n  user\n  |> User.changeset(params)\n  |> Repo.update!()  # Crash if invalid\nend\n```\n\n### Circuit Breaker\n\nPrevent cascading failures:\n\n```elixir\ndefmodule CircuitBreaker do\n  use GenServer\n\n  def start_link(_) do\n    GenServer.start_link(__MODULE__, %{status: :closed, failures: 0}, name: __MODULE__)\n  end\n\n  def call(fun) do\n    case GenServer.call(__MODULE__, :status) do\n      :open -> {:error, :circuit_open}\n      :closed -> execute(fun)\n    end\n  end\n\n  defp execute(fun) do\n    try do\n      result = fun.()\n      GenServer.cast(__MODULE__, :success)\n      {:ok, result}\n    rescue\n      e ->\n        GenServer.cast(__MODULE__, :failure)\n        {:error, e}\n    end\n  end\n\n  @impl true\n  def init(state), do: {:ok, state}\n\n  @impl true\n  def handle_call(:status, _from, state) do\n    {:reply, state.status, state}\n  end\n\n  @impl true\n  def handle_cast(:success, state) do\n    {:noreply, %{state | failures: 0, status: :closed}}\n  end\n\n  @impl true\n  def handle_cast(:failure, state) do\n    new_failures = state.failures + 1\n\n    if new_failures >= 5 do\n      Process.send_after(self(), :half_open, 30_000)\n      {:noreply, %{state | failures: new_failures, status: :open}}\n    else\n      {:noreply, %{state | failures: new_failures}}\n    end\n  end\n\n  @impl true\n  def handle_info(:half_open, state) do\n    {:noreply, %{state | status: :closed, failures: 0}}\n  end\nend\n```\n\n## Testing Concurrent Systems\n\n### Testing GenServers\n\n```elixir\ndefmodule MyApp.CounterTest do\n  use ExUnit.Case, async: true\n\n  test \"increments counter\" do\n    {:ok, pid} = MyApp.Counter.start_link(0)\n\n    assert MyApp.Counter.increment(pid) == 1\n    assert MyApp.Counter.increment(pid) == 2\n    assert MyApp.Counter.get_value(pid) == 2\n  end\nend\n```\n\n### Testing Asynchronous Processes\n\n```elixir\ntest \"process receives message\" do\n  parent = self()\n\n  spawn(fn ->\n    receive do\n      :ping -> send(parent, :pong)\n    end\n  end)\n\n  send(pid, :ping)\n\n  assert_receive :pong, 1000\nend\n```\n\n### Testing Supervision\n\n```elixir\ntest \"supervisor restarts crashed worker\" do\n  {:ok, sup} = Supervisor.start_link([MyApp.Worker], strategy: :one_for_one)\n\n  [{_, worker_pid, _, _}] = Supervisor.which_children(sup)\n\n  # Crash the worker\n  Process.exit(worker_pid, :kill)\n\n  # Wait for restart\n  Process.sleep(100)\n\n  # Verify new worker started\n  [{_, new_pid, _, _}] = Supervisor.which_children(sup)\n  assert new_pid != worker_pid\n  assert Process.alive?(new_pid)\nend\n```\n\n## Debugging Concurrent Systems\n\n### Observer\n\nLaunch Observer for visual process inspection:\n\n```elixir\n:observer.start()\n```\n\n### Process Info\n\nInspect running processes:\n\n```elixir\n# List all processes\nProcess.list()\n\n# Process information\nProcess.info(pid)\n\n# Message queue length\n{:message_queue_len, count} = Process.info(pid, :message_queue_len)\n\n# Current function\n{:current_function, {mod, fun, arity}} = Process.info(pid, :current_function)\n```\n\n### Tracing\n\nUse `:sys` module for debugging:\n\n```elixir\n# Enable tracing\n:sys.trace(pid, true)\n\n# Get state\n:sys.get_state(pid)\n\n# Get status\n:sys.get_status(pid)\n```\n\n## Performance Considerations\n\n### Process Spawning\n\n- Processes are lightweight (< 2KB overhead)\n- Spawning thousands/millions of processes is normal\n- Use process pools when spawn rate is very high\n\n### Message Passing\n\n- Messages are copied between processes\n- Large messages are expensive - consider ETS or persistent_term\n- Use binary for efficient large data transfer\n\n### Bottlenecks\n\n- Single GenServer can become bottleneck\n- Solution: shard state across multiple processes\n- Use ETS with `read_concurrency` for read-heavy workloads\n\n## Key Principles\n\n- **Embrace concurrency**: Use processes liberally, they're cheap\n- **Let it crash**: Don't write defensive code, use supervision\n- **Isolate failures**: Design supervision trees to contain failures\n- **Communicate via messages**: Avoid shared state between processes\n- **Use the right tool**: GenServer for state, Task for work, Agent for simple state\n- **Test at boundaries**: Test process APIs, not internal implementation\n- **Monitor and observe**: Use Observer and logging to understand system behavior"
              },
              {
                "name": "phoenix-framework",
                "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices",
                "path": "elixir/skills/phoenix/SKILL.md",
                "frontmatter": {
                  "name": "phoenix-framework",
                  "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices"
                },
                "content": "# Phoenix Framework Development\n\nThis skill activates when working with Phoenix web applications, including setup, development, LiveView, contexts, controllers, and channels.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or modifying Phoenix applications\n- Implementing LiveView components or pages\n- Working with Phoenix contexts and business logic\n- Building real-time features with channels or LiveView\n- Configuring Phoenix routers, plugs, or endpoints\n- Troubleshooting Phoenix-specific issues\n\n## Phoenix Project Structure\n\nFollow Phoenix conventions:\n\n```\nlib/\n  my_app/           # Business logic and contexts\n    accounts/       # Domain contexts\n    repo.ex\n  my_app_web/       # Web interface\n    controllers/\n    live/           # LiveView modules\n    components/     # Function components\n    router.ex\n    endpoint.ex\n```\n\n## Context-Driven Design\n\nOrganize business logic into contexts (bounded domains):\n\n### Creating Contexts\n\nGenerate contexts with related schemas:\n```bash\nmix phx.gen.context Accounts User users email:string name:string\n```\n\nStructure contexts to encapsulate business logic:\n\n```elixir\ndefmodule MyApp.Accounts do\n  @moduledoc \"\"\"\n  The Accounts context - manages user accounts and authentication.\n  \"\"\"\n\n  alias MyApp.Repo\n  alias MyApp.Accounts.User\n\n  def list_users do\n    Repo.all(User)\n  end\n\n  def get_user!(id), do: Repo.get!(User, id)\n\n  def create_user(attrs \\\\ %{}) do\n    %User{}\n    |> User.changeset(attrs)\n    |> Repo.insert()\n  end\n\n  def update_user(%User{} = user, attrs) do\n    user\n    |> User.changeset(attrs)\n    |> Repo.update()\n  end\nend\n```\n\n### Context Best Practices\n\n- Keep contexts focused on a single domain\n- Avoid cross-context dependencies when possible\n- Use public API functions, not direct Repo access in web layer\n- Name contexts after business domains, not technical layers\n\n## LiveView Development\n\nLiveView enables rich, real-time experiences without writing JavaScript.\n\n### LiveView Lifecycle\n\nUnderstand the mount → handle_event → render cycle:\n\n```elixir\ndefmodule MyAppWeb.UserLive.Index do\n  use MyAppWeb, :live_view\n\n  alias MyApp.Accounts\n\n  @impl true\n  def mount(_params, _session, socket) do\n    # Runs on initial page load and live connection\n    {:ok, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def handle_params(params, _url, socket) do\n    # Runs after mount and on live patch\n    {:noreply, apply_action(socket, socket.assigns.live_action, params)}\n  end\n\n  @impl true\n  def handle_event(\"delete\", %{\"id\" => id}, socket) do\n    user = Accounts.get_user!(id)\n    {:ok, _} = Accounts.delete_user(user)\n\n    {:noreply, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def render(assigns) do\n    ~H\"\"\"\n    <div>\n      <.table rows={@users} id=\"users\">\n        <:col :let={user} label=\"Name\"><%= user.name %></:col>\n        <:col :let={user} label=\"Email\"><%= user.email %></:col>\n        <:action :let={user}>\n          <.button phx-click=\"delete\" phx-value-id={user.id}>Delete</.button>\n        </:action>\n      </.table>\n    </div>\n    \"\"\"\n  end\n\n  defp list_users do\n    Accounts.list_users()\n  end\nend\n```\n\n### LiveView Best Practices\n\n- Use `mount/3` for initial data loading\n- Handle route changes in `handle_params/3`\n- Keep renders fast - compute in event handlers, not render\n- Use `assign_new/3` for expensive computations\n- Prefer LiveView over JavaScript for interactive UIs\n- Use `phx-debounce` and `phx-throttle` for frequent events\n\n### Function Components\n\nCreate reusable components:\n\n```elixir\ndefmodule MyAppWeb.Components.UserCard do\n  use Phoenix.Component\n\n  attr :user, :map, required: true\n  attr :class, :string, default: \"\"\n\n  def user_card(assigns) do\n    ~H\"\"\"\n    <div class={\"card \" <> @class}>\n      <h3><%= @user.name %></h3>\n      <p><%= @user.email %></p>\n    </div>\n    \"\"\"\n  end\nend\n```\n\nUse with `<.user_card user={@current_user} />` in templates.\n\n### Form Handling\n\nUse changesets for validation:\n\n```elixir\n@impl true\ndef mount(_params, _session, socket) do\n  changeset = Accounts.change_user(%User{})\n  {:ok, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"validate\", %{\"user\" => user_params}, socket) do\n  changeset =\n    %User{}\n    |> Accounts.change_user(user_params)\n    |> Map.put(:action, :validate)\n\n  {:noreply, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"save\", %{\"user\" => user_params}, socket) do\n  case Accounts.create_user(user_params) do\n    {:ok, user} ->\n      {:noreply,\n       socket\n       |> put_flash(:info, \"User created successfully\")\n       |> push_navigate(to: ~p\"/users/#{user}\")}\n\n    {:error, %Ecto.Changeset{} = changeset} ->\n      {:noreply, assign(socket, form: to_form(changeset))}\n  end\nend\n\ndef render(assigns) do\n  ~H\"\"\"\n  <.form for={@form} phx-change=\"validate\" phx-submit=\"save\">\n    <.input field={@form[:name]} label=\"Name\" />\n    <.input field={@form[:email]} label=\"Email\" type=\"email\" />\n    <.button>Save</.button>\n  </.form>\n  \"\"\"\nend\n```\n\n## Routing\n\n### Route Organization\n\nStructure routes logically:\n\n```elixir\ndefmodule MyAppWeb.Router do\n  use MyAppWeb, :router\n\n  pipeline :browser do\n    plug :accepts, [\"html\"]\n    plug :fetch_session\n    plug :fetch_live_flash\n    plug :put_root_layout, html: {MyAppWeb.Layouts, :root}\n    plug :protect_from_forgery\n    plug :put_secure_browser_headers\n  end\n\n  pipeline :api do\n    plug :accepts, [\"json\"]\n  end\n\n  scope \"/\", MyAppWeb do\n    pipe_through :browser\n\n    live \"/\", HomeLive, :index\n    live \"/users\", UserLive.Index, :index\n    live \"/users/new\", UserLive.Index, :new\n    live \"/users/:id\", UserLive.Show, :show\n  end\n\n  scope \"/api\", MyAppWeb do\n    pipe_through :api\n\n    resources \"/users\", UserController, except: [:new, :edit]\n  end\nend\n```\n\n### LiveView Routes\n\nUse live actions for modal/overlay states:\n\n```elixir\nlive \"/users\", UserLive.Index, :index\nlive \"/users/new\", UserLive.Index, :new\nlive \"/users/:id/edit\", UserLive.Index, :edit\n```\n\nThen handle in `handle_params/3`:\n\n```elixir\ndefp apply_action(socket, :edit, %{\"id\" => id}) do\n  socket\n  |> assign(:page_title, \"Edit User\")\n  |> assign(:user, Accounts.get_user!(id))\nend\n\ndefp apply_action(socket, :new, _params) do\n  socket\n  |> assign(:page_title, \"New User\")\n  |> assign(:user, %User{})\nend\n\ndefp apply_action(socket, :index, _params) do\n  socket\n  |> assign(:page_title, \"Listing Users\")\n  |> assign(:user, nil)\nend\n```\n\n## Channels and PubSub\n\n### Phoenix Channels\n\nFor custom real-time protocols:\n\n```elixir\ndefmodule MyAppWeb.RoomChannel do\n  use MyAppWeb, :channel\n\n  @impl true\n  def join(\"room:\" <> room_id, _payload, socket) do\n    if authorized?(socket, room_id) do\n      {:ok, assign(socket, :room_id, room_id)}\n    else\n      {:error, %{reason: \"unauthorized\"}}\n    end\n  end\n\n  @impl true\n  def handle_in(\"new_msg\", %{\"body\" => body}, socket) do\n    broadcast!(socket, \"new_msg\", %{body: body, user: socket.assigns.user})\n    {:noreply, socket}\n  end\nend\n```\n\n### Phoenix PubSub\n\nFor LiveView updates and process communication:\n\n```elixir\n# Subscribe in mount\ndef mount(_params, _session, socket) do\n  if connected?(socket) do\n    Phoenix.PubSub.subscribe(MyApp.PubSub, \"users\")\n  end\n\n  {:ok, assign(socket, :users, list_users())}\nend\n\n# Handle broadcasts\ndef handle_info({:user_created, user}, socket) do\n  {:noreply, update(socket, :users, fn users -> [user | users] end)}\nend\n\n# Broadcast from context\ndef create_user(attrs) do\n  with {:ok, user} <- do_create_user(attrs) do\n    Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n    {:ok, user}\n  end\nend\n```\n\n## Testing Phoenix Applications\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase, async: true\n\n  test \"GET /users\", %{conn: conn} do\n    conn = get(conn, ~p\"/users\")\n    assert html_response(conn, 200) =~ \"Listing Users\"\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n\n  import Phoenix.LiveViewTest\n\n  test \"displays users\", %{conn: conn} do\n    user = insert(:user)\n\n    {:ok, view, html} = live(conn, ~p\"/users\")\n\n    assert html =~ user.name\n    assert has_element?(view, \"#user-#{user.id}\")\n  end\n\n  test \"creates user\", %{conn: conn} do\n    {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n    assert view\n           |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n           |> render_submit()\n\n    assert_patch(view, ~p\"/users\")\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    {:ok, _, socket} = subscribe_and_join(socket, \"room:lobby\", %{})\n\n    broadcast_from!(socket, \"new_msg\", %{body: \"test\"})\n    assert_broadcast \"new_msg\", %{body: \"test\"}\n  end\nend\n```\n\n## Common Patterns\n\n### Loading Associations\n\nPreload associations efficiently:\n\n```elixir\ndef list_posts do\n  Post\n  |> preload([:author, comments: :author])\n  |> Repo.all()\nend\n```\n\n### Pagination\n\nUse Scrivener or custom pagination:\n\n```elixir\ndef list_users(page \\\\ 1) do\n  User\n  |> order_by(desc: :inserted_at)\n  |> Repo.paginate(page: page, page_size: 20)\nend\n```\n\n### File Uploads\n\nHandle uploads in LiveView:\n\n```elixir\ndef mount(_params, _session, socket) do\n  {:ok,\n   socket\n   |> assign(:uploaded_files, [])\n   |> allow_upload(:avatar, accept: ~w(.jpg .jpeg .png), max_entries: 1)}\nend\n\ndef handle_event(\"save\", _params, socket) do\n  uploaded_files =\n    consume_uploaded_entries(socket, :avatar, fn %{path: path}, _entry ->\n      dest = Path.join(\"priv/static/uploads\", Path.basename(path))\n      File.cp!(path, dest)\n      {:ok, \"/uploads/\" <> Path.basename(dest)}\n    end)\n\n  {:noreply, update(socket, :uploaded_files, &(&1 ++ uploaded_files))}\nend\n```\n\n## Performance Optimization\n\n### Database Query Optimization\n\n- Use `preload/2` to avoid N+1 queries\n- Add database indexes for frequently queried fields\n- Use `select/3` to load only needed fields\n- Consider using `Repo.stream/2` for large datasets\n\n### LiveView Performance\n\n- Move expensive computations to `handle_event` or background jobs\n- Use `assign_new/3` for computed values\n- Implement `handle_continue/2` for async operations after mount\n- Use temporary assigns for large lists: `assign(socket, :items, temporary: true)`\n\n### Caching\n\nUse Cachex or ETS for caching:\n\n```elixir\ndef get_user!(id) do\n  Cachex.fetch(:users, id, fn ->\n    {:commit, Repo.get!(User, id)}\n  end)\nend\n```\n\n## Security Best Practices\n\n- Always validate and sanitize user input through changesets\n- Use CSRF protection (enabled by default)\n- Implement rate limiting for APIs\n- Use `put_secure_browser_headers` plug\n- Validate file uploads (type, size, content)\n- Use prepared statements (Ecto does this automatically)\n- Implement proper authentication and authorization\n\n## Key Principles\n\n- **Context boundaries**: Keep business logic in contexts, not controllers/LiveViews\n- **LiveView first**: Prefer LiveView over JavaScript for interactive features\n- **Changesets for validation**: Always validate through Ecto changesets\n- **Pub/Sub for communication**: Use Phoenix.PubSub for cross-process updates\n- **Test at boundaries**: Test contexts, controllers, and LiveViews separately\n- **Follow conventions**: Use Phoenix generators and follow established patterns"
              },
              {
                "name": "elixir-testing",
                "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices",
                "path": "elixir/skills/testing/SKILL.md",
                "frontmatter": {
                  "name": "elixir-testing",
                  "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices"
                },
                "content": "# Elixir Testing with ExUnit\n\nThis skill activates when writing, organizing, or improving tests for Elixir applications using ExUnit and related testing tools.\n\n## When to Use This Skill\n\nActivate when:\n- Writing unit, integration, or property-based tests\n- Organizing test suites and test files\n- Setting up test fixtures and factories\n- Mocking external dependencies\n- Testing concurrent or asynchronous code\n- Improving test coverage or quality\n- Troubleshooting failing tests\n\n## ExUnit Basics\n\n### Test Module Structure\n\n```elixir\ndefmodule MyApp.MathTest do\n  use ExUnit.Case, async: true\n\n  describe \"add/2\" do\n    test \"adds two positive numbers\" do\n      assert Math.add(2, 3) == 5\n    end\n\n    test \"adds negative numbers\" do\n      assert Math.add(-1, -1) == -2\n    end\n\n    test \"adds zero\" do\n      assert Math.add(5, 0) == 5\n    end\n  end\n\n  describe \"divide/2\" do\n    test \"divides two numbers\" do\n      assert Math.divide(10, 2) == 5.0\n    end\n\n    test \"returns error for division by zero\" do\n      assert Math.divide(10, 0) == {:error, :division_by_zero}\n    end\n  end\nend\n```\n\n### Assertions\n\nCommon assertion patterns:\n\n```elixir\n# Equality\nassert actual == expected\nrefute actual == unexpected\n\n# Boolean\nassert is_binary(value)\nassert is_integer(value)\nrefute is_nil(value)\n\n# Pattern matching\nassert {:ok, result} = function_call()\nassert %User{name: \"Alice\"} = user\n\n# Exceptions\nassert_raise ArgumentError, fn ->\n  String.to_integer(\"not a number\")\nend\n\nassert_raise ArgumentError, \"invalid argument\", fn ->\n  dangerous_function()\nend\n\n# Messages\nsend(self(), :hello)\nassert_received :hello\n\nassert_receive :message, 1000  # With timeout\n\nrefute_received :unwanted\nrefute_receive :unwanted, 100\n```\n\n### Test Organization\n\n#### Using describe blocks\n\nGroup related tests:\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  describe \"create_user/1\" do\n    test \"creates user with valid attributes\" do\n      # ...\n    end\n\n    test \"returns error with invalid email\" do\n      # ...\n    end\n  end\n\n  describe \"update_user/2\" do\n    test \"updates user attributes\" do\n      # ...\n    end\n  end\nend\n```\n\n#### Test tags\n\nCategorize and filter tests:\n\n```elixir\n@moduletag :integration\n\n@tag :slow\ntest \"expensive operation\" do\n  # ...\nend\n\n@tag :external\ntest \"calls external API\" do\n  # ...\nend\n\n# Run only tagged tests\n# mix test --only slow\n# mix test --exclude external\n```\n\n### Setup and Teardown\n\n#### Test context\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  setup do\n    user = %User{name: \"Alice\", email: \"alice@example.com\"}\n    {:ok, user: user}\n  end\n\n  test \"user has name\", %{user: user} do\n    assert user.name == \"Alice\"\n  end\n\n  test \"user has email\", %{user: user} do\n    assert user.email == \"alice@example.com\"\n  end\nend\n```\n\n#### Setup with describe\n\n```elixir\ndescribe \"authenticated user\" do\n  setup do\n    user = insert(:user)\n    token = generate_token(user)\n    {:ok, user: user, token: token}\n  end\n\n  test \"can access protected resource\", %{token: token} do\n    # ...\n  end\nend\n```\n\n#### Module setup\n\n```elixir\nsetup_all do\n  # Runs once before all tests in module\n  start_supervised!(MyApp.Cache)\n  :ok\nend\n\nsetup do\n  # Runs before each test\n  :ok = Ecto.Adapters.SQL.Sandbox.checkout(MyApp.Repo)\nend\n```\n\n#### Conditional setup\n\n```elixir\nsetup context do\n  if context[:integration] do\n    start_external_service()\n    on_exit(fn -> stop_external_service() end)\n  end\n\n  :ok\nend\n\n@tag :integration\ntest \"integration test\" do\n  # ...\nend\n```\n\n## Database Testing\n\n### Sandbox Mode\n\nConfigure for concurrent tests:\n\n```elixir\n# config/test.exs\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox\n\n# test/test_helper.exs\nEcto.Adapters.SQL.Sandbox.mode(MyApp.Repo, :manual)\n\n# test/support/data_case.ex\ndefmodule MyApp.DataCase do\n  use ExUnit.CaseTemplate\n\n  using do\n    quote do\n      alias MyApp.Repo\n      import Ecto\n      import Ecto.Changeset\n      import Ecto.Query\n      import MyApp.DataCase\n    end\n  end\n\n  setup tags do\n    pid = Ecto.Adapters.SQL.Sandbox.start_owner!(MyApp.Repo, shared: not tags[:async])\n    on_exit(fn -> Ecto.Adapters.SQL.Sandbox.stop_owner(pid) end)\n    :ok\n  end\nend\n```\n\n### Test Factories\n\nUse ExMachina for test data:\n\n```elixir\n# test/support/factory.ex\ndefmodule MyApp.Factory do\n  use ExMachina.Ecto, repo: MyApp.Repo\n\n  def user_factory do\n    %MyApp.User{\n      name: \"Jane Smith\",\n      email: sequence(:email, &\"email-#{&1}@example.com\"),\n      age: 25\n    }\n  end\n\n  def admin_factory do\n    struct!(\n      user_factory(),\n      %{role: :admin}\n    )\n  end\n\n  def post_factory do\n    %MyApp.Post{\n      title: \"A title\",\n      body: \"Some content\",\n      author: build(:user)\n    }\n  end\nend\n\n# In tests\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n  import MyApp.Factory\n\n  test \"creates user\" do\n    user = insert(:user)\n    assert user.id\n  end\n\n  test \"creates admin\" do\n    admin = insert(:admin)\n    assert admin.role == :admin\n  end\n\n  test \"builds without inserting\" do\n    user = build(:user, name: \"Custom Name\")\n    assert user.name == \"Custom Name\"\n    refute user.id\n  end\nend\n```\n\n### Testing Changesets\n\n```elixir\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n\n  describe \"changeset/2\" do\n    test \"valid changeset with valid attributes\" do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert changeset.valid?\n    end\n\n    test \"invalid without email\" do\n      attrs = %{name: \"Alice\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      refute changeset.valid?\n      assert \"can't be blank\" in errors_on(changeset).email\n    end\n\n    test \"invalid with short password\" do\n      attrs = %{email: \"test@example.com\", password: \"123\"}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert \"should be at least 8 character(s)\" in errors_on(changeset).password\n    end\n  end\nend\n\n# Helper function\ndef errors_on(changeset) do\n  Ecto.Changeset.traverse_errors(changeset, fn {message, opts} ->\n    Regex.replace(~r\"%{(\\w+)}\", message, fn _, key ->\n      opts |> Keyword.get(String.to_existing_atom(key), key) |> to_string()\n    end)\n  end)\nend\n```\n\n## Phoenix Testing\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase\n  import MyApp.Factory\n\n  describe \"index\" do\n    test \"lists all users\", %{conn: conn} do\n      user = insert(:user)\n\n      conn = get(conn, ~p\"/users\")\n\n      assert html_response(conn, 200) =~ \"Listing Users\"\n      assert html_response(conn, 200) =~ user.name\n    end\n  end\n\n  describe \"create\" do\n    test \"creates user with valid data\", %{conn: conn} do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\"}\n\n      conn = post(conn, ~p\"/users\", user: attrs)\n\n      assert redirected_to(conn) =~ ~p\"/users\"\n\n      conn = get(conn, redirected_to(conn))\n      assert html_response(conn, 200) =~ \"Alice\"\n    end\n\n    test \"renders errors with invalid data\", %{conn: conn} do\n      conn = post(conn, ~p\"/users\", user: %{})\n\n      assert html_response(conn, 200) =~ \"New User\"\n    end\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n  import Phoenix.LiveViewTest\n  import MyApp.Factory\n\n  describe \"Index\" do\n    test \"displays users\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, html} = live(conn, ~p\"/users\")\n\n      assert html =~ \"Listing Users\"\n      assert has_element?(view, \"#user-#{user.id}\")\n      assert render(view) =~ user.name\n    end\n\n    test \"creates new user\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users\")\n\n      html = render(view)\n      assert html =~ \"Alice\"\n    end\n\n    test \"updates user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users/#{user.id}/edit\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Updated Name\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users/#{user.id}\")\n\n      html = render(view)\n      assert html =~ \"Updated Name\"\n    end\n\n    test \"deletes user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      assert view\n             |> element(\"#user-#{user.id} a\", \"Delete\")\n             |> render_click()\n\n      refute has_element?(view, \"#user-#{user.id}\")\n    end\n  end\n\n  describe \"form validation\" do\n    test \"validates on change\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      result =\n        view\n        |> form(\"#user-form\", user: %{email: \"invalid\"})\n        |> render_change()\n\n      assert result =~ \"must have the @ sign\"\n    end\n  end\n\n  describe \"real-time updates\" do\n    test \"receives updates from PubSub\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      user = insert(:user)\n\n      # Trigger PubSub event\n      Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n\n      assert render(view) =~ user.name\n    end\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  setup do\n    {:ok, _, socket} =\n      MyAppWeb.UserSocket\n      |> socket(\"user_id\", %{user_id: 42})\n      |> subscribe_and_join(MyAppWeb.RoomChannel, \"room:lobby\")\n\n    %{socket: socket}\n  end\n\n  test \"ping replies with pong\", %{socket: socket} do\n    ref = push(socket, \"ping\", %{\"hello\" => \"there\"})\n    assert_reply ref, :ok, %{\"hello\" => \"there\"}\n  end\n\n  test \"shout broadcasts to room:lobby\", %{socket: socket} do\n    push(socket, \"shout\", %{\"hello\" => \"all\"})\n    assert_broadcast \"shout\", %{\"hello\" => \"all\"}\n  end\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    broadcast_from!(socket, \"broadcast\", %{\"some\" => \"data\"})\n    assert_push \"broadcast\", %{\"some\" => \"data\"}\n  end\nend\n```\n\n## Mocking and Stubbing\n\n### Using Mox\n\nDefine behaviors and mocks:\n\n```elixir\n# Define behaviour\ndefmodule MyApp.HTTPClient do\n  @callback get(String.t()) :: {:ok, map()} | {:error, term()}\nend\n\n# In config/test.exs\nconfig :my_app, :http_client, MyApp.HTTPClientMock\n\n# In test/test_helper.exs\nMox.defmock(MyApp.HTTPClientMock, for: MyApp.HTTPClient)\n\n# In application code\ndefmodule MyApp.UserFetcher do\n  @http_client Application.compile_env(:my_app, :http_client)\n\n  def fetch_user(id) do\n    @http_client.get(\"/users/#{id}\")\n  end\nend\n\n# In tests\ndefmodule MyApp.UserFetcherTest do\n  use ExUnit.Case, async: true\n  import Mox\n\n  setup :verify_on_exit!\n\n  test \"fetches user successfully\" do\n    expect(MyApp.HTTPClientMock, :get, fn \"/users/1\" ->\n      {:ok, %{\"name\" => \"Alice\"}}\n    end)\n\n    assert {:ok, %{\"name\" => \"Alice\"}} = MyApp.UserFetcher.fetch_user(1)\n  end\n\n  test \"handles error\" do\n    expect(MyApp.HTTPClientMock, :get, fn _ ->\n      {:error, :network_error}\n    end)\n\n    assert {:error, :network_error} = MyApp.UserFetcher.fetch_user(1)\n  end\nend\n```\n\n### Stubbing Multiple Calls\n\n```elixir\ntest \"calls API multiple times\" do\n  MyApp.HTTPClientMock\n  |> expect(:get, 3, fn url ->\n    {:ok, %{\"url\" => url}}\n  end)\n\n  MyApp.batch_fetch([1, 2, 3])\nend\n```\n\n### Global Stubs\n\n```elixir\nsetup do\n  stub(MyApp.HTTPClientMock, :get, fn _ -> {:ok, %{}} end)\n  :ok\nend\n\ntest \"can override stub\" do\n  expect(MyApp.HTTPClientMock, :get, fn _ ->\n    {:error, :timeout}\n  end)\n\n  # ...\nend\n```\n\n## Property-Based Testing\n\nUse StreamData for property-based tests:\n\n```elixir\ndefmodule MyApp.MathPropertyTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  property \"addition is commutative\" do\n    check all a <- integer(),\n              b <- integer() do\n      assert Math.add(a, b) == Math.add(b, a)\n    end\n  end\n\n  property \"list reversal is involutive\" do\n    check all list <- list_of(integer()) do\n      assert Enum.reverse(Enum.reverse(list)) == list\n    end\n  end\n\n  property \"concatenation length\" do\n    check all list1 <- list_of(term()),\n              list2 <- list_of(term()) do\n      concatenated = list1 ++ list2\n      assert length(concatenated) == length(list1) + length(list2)\n    end\n  end\nend\n```\n\n### Custom Generators\n\n```elixir\ndefmodule MyApp.Generators do\n  use ExUnitProperties\n\n  def email do\n    gen all username <- string(:alphanumeric, min_length: 1),\n            domain <- string(:alphanumeric, min_length: 1),\n            tld <- member_of([\"com\", \"org\", \"net\"]) do\n      \"#{username}@#{domain}.#{tld}\"\n    end\n  end\n\n  def user do\n    gen all name <- string(:alphanumeric, min_length: 1),\n            email <- email(),\n            age <- integer(18..100) do\n      %User{name: name, email: email, age: age}\n    end\n  end\nend\n\n# Use in tests\nproperty \"validates email format\" do\n  check all email <- MyApp.Generators.email() do\n    assert User.valid_email?(email)\n  end\nend\n```\n\n## Testing Async and Concurrent Code\n\n### Testing Processes\n\n```elixir\ntest \"GenServer handles messages\" do\n  {:ok, pid} = MyApp.Worker.start_link()\n\n  MyApp.Worker.process(pid, :work)\n\n  assert_receive {:done, :work}, 1000\nend\n```\n\n### Testing Tasks\n\n```elixir\ntest \"async task completes\" do\n  parent = self()\n\n  Task.start(fn ->\n    result = expensive_computation()\n    send(parent, {:result, result})\n  end)\n\n  assert_receive {:result, value}, 5000\n  assert value == expected\nend\n```\n\n### Testing Race Conditions\n\n```elixir\ntest \"concurrent updates are handled correctly\" do\n  {:ok, counter} = Counter.start_link(0)\n\n  tasks = for _ <- 1..100 do\n    Task.async(fn -> Counter.increment(counter) end)\n  end\n\n  Task.await_many(tasks)\n\n  assert Counter.get(counter) == 100\nend\n```\n\n## Test Coverage\n\n### Generate Coverage Reports\n\n```bash\nmix test --cover\n\n# Detailed coverage\nMIX_ENV=test mix coveralls\nMIX_ENV=test mix coveralls.html\n```\n\n### Coverage Configuration\n\n```elixir\n# mix.exs\ndef project do\n  [\n    # ...\n    test_coverage: [tool: ExCoveralls],\n    preferred_cli_env: [\n      coveralls: :test,\n      \"coveralls.detail\": :test,\n      \"coveralls.html\": :test\n    ]\n  ]\nend\n```\n\n## Best Practices\n\n### Test Organization\n\n- One test file per module: `lib/my_app/user.ex` → `test/my_app/user_test.exs`\n- Use `describe` blocks to group related tests\n- Use `test/support` for shared test helpers\n- Keep tests focused on one behavior per test\n\n### Naming\n\n- Use descriptive test names that explain what is being tested\n- Start with the action being tested\n- Include the expected outcome\n\n```elixir\n# Good\ntest \"create_user/1 returns error with invalid email\"\ntest \"add/2 returns sum of two positive integers\"\n\n# Avoid\ntest \"it works\"\ntest \"test1\"\n```\n\n### Setup\n\n- Use `setup` for common test data\n- Keep setup focused - don't create unnecessary data\n- Use context to pass data between setup and tests\n- Use factories for complex data structures\n\n### Assertions\n\n- Prefer pattern matching over multiple assertions\n- Use specific assertions (`assert_receive` vs `assert Process.info(...)`)\n- Test one logical assertion per test when possible\n\n### Async Tests\n\n```elixir\n# Mark tests as async when they don't share state\nuse ExUnit.Case, async: true\n\n# Don't use async when tests:\n# - Modify global state\n# - Use database without sandbox\n# - Access shared resources\n```\n\n### Test Data\n\n- Use factories (ExMachina) for consistent test data\n- Avoid hardcoded IDs - use factories and references\n- Keep test data minimal - only what's needed for the test\n- Use descriptive data that makes tests readable\n\n### External Dependencies\n\n- Mock external APIs and services\n- Use Mox for behavior-based mocking\n- Stub at the boundary - don't mock internal modules\n- Tag tests that require external services\n\n## Debugging Tests\n\n### Running Specific Tests\n\n```bash\n# Run single test file\nmix test test/my_app/user_test.exs\n\n# Run specific line\nmix test test/my_app/user_test.exs:42\n\n# Run tests matching pattern\nmix test --only integration\n\n# Run tests excluding pattern\nmix test --exclude slow\n```\n\n### Test Output\n\n```elixir\n# Add IEx.pry breakpoint\nimport IEx\ntest \"debugging\" do\n  user = build(:user)\n  IEx.pry()  # Stops here\n  # ...\nend\n\n# Print during tests\nIO.inspect(value, label: \"DEBUG\")\n```\n\n### Failed Test Debugging\n\n```bash\n# Re-run only failed tests\nmix test --failed\n\n# Show detailed error traces\nmix test --trace\n\n# Run tests one at a time\nmix test --max-cases 1\n```\n\n## Key Principles\n\n- **Test behavior, not implementation**: Test what the code does, not how it does it\n- **Keep tests fast**: Use async tests, avoid unnecessary setup, mock slow dependencies\n- **Make tests readable**: Use descriptive names, clear assertions, minimal setup\n- **Test at the right level**: Unit tests for logic, integration tests for interactions\n- **Use factories**: Consistent, reusable test data with ExMachina\n- **Mock at boundaries**: Mock external services, not internal modules\n- **Property-based testing**: Use StreamData for algorithmic code\n- **Embrace the database**: Use Ecto sandbox for fast, isolated database tests"
              }
            ]
          },
          {
            "name": "github",
            "description": "GitHub Actions, Workflows, and act local testing tool",
            "source": "./github",
            "category": "tools",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install github@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "act",
                "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution",
                "path": "github/skills/act/SKILL.md",
                "frontmatter": {
                  "name": "act",
                  "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution"
                },
                "content": "# act - Local GitHub Actions Testing\n\nActivate when testing GitHub Actions workflows locally, debugging workflow issues, or developing actions without committing to remote repositories. This skill covers act installation, configuration, and usage patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Testing workflow changes before committing\n- Debugging workflow failures locally\n- Developing new workflows iteratively\n- Validating workflow syntax and logic\n- Testing actions with different events\n- Running workflows without GitHub runners\n- Troubleshooting act-specific issues\n\n## Installation\n\n### Using mise (Recommended for this project)\n\nThe act tool is configured in the github plugin's mise.toml:\n\n```bash\n# Install act via mise\nmise install act\n\n# Verify installation\nact --version\n```\n\n### Alternative Installation Methods\n\n**macOS (Homebrew):**\n```bash\nbrew install act\n```\n\n**Linux (via script):**\n```bash\ncurl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\n```\n\n**From source:**\n```bash\ngit clone https://github.com/nektos/act.git\ncd act\nmake install\n```\n\n**Windows (Chocolatey):**\n```powershell\nchoco install act-cli\n```\n\n## How act Works\n\nact reads workflow files from `.github/workflows/` and:\n1. Determines which actions and jobs to execute\n2. Pulls or builds required Docker images\n3. Creates containers matching GitHub's runner environment\n4. Executes steps in isolated containers\n5. Provides output matching GitHub Actions format\n\n**Key Concept:** act uses Docker to simulate GitHub's runner environment locally.\n\n## Prerequisites\n\n- **Docker**: act requires Docker to run workflows\n- **Workflow files**: Valid `.github/workflows/*.yml` files in repository\n\nVerify Docker is running:\n```bash\ndocker ps\n```\n\n## Basic Usage\n\n### List Available Workflows\n\n```bash\n# List all workflows\nact -l\n\n# Output:\n# Stage  Job ID  Job name  Workflow name  Workflow file  Events\n# 0      build   build     CI             ci.yml         push,pull_request\n# 0      test    test      CI             ci.yml         push,pull_request\n```\n\n### Run Default Event (push)\n\n```bash\n# Run all jobs triggered by push event\nact\n\n# Run specific job\nact -j build\n\n# Run specific workflow\nact -W .github/workflows/ci.yml\n```\n\n### Run Specific Events\n\n```bash\n# Pull request event\nact pull_request\n\n# Manual workflow dispatch\nact workflow_dispatch\n\n# Push to specific branch\nact push -e .github/workflows/push-event.json\n\n# Schedule event\nact schedule\n```\n\n### Dry Run\n\n```bash\n# Show what would run without executing\nact -n\n\n# Show with full details\nact -n -v\n```\n\n## Event Payloads\n\n### Custom Event Data\n\nCreate event JSON file:\n\n```json\n{\n  \"pull_request\": {\n    \"number\": 123,\n    \"head\": {\n      \"ref\": \"feature-branch\"\n    },\n    \"base\": {\n      \"ref\": \"main\"\n    }\n  }\n}\n```\n\nUse with act:\n```bash\nact pull_request -e event.json\n```\n\n### workflow_dispatch Inputs\n\n```json\n{\n  \"inputs\": {\n    \"environment\": \"staging\",\n    \"debug\": true\n  }\n}\n```\n\n```bash\nact workflow_dispatch -e inputs.json\n```\n\n## Secrets Management\n\n### Via Command Line\n\n```bash\n# Single secret\nact -s GITHUB_TOKEN=ghp_xxxxx\n\n# Multiple secrets\nact -s API_KEY=key123 -s DB_PASSWORD=pass456\n```\n\n### Via .secrets File\n\nCreate `.secrets` file (add to .gitignore):\n```\nGITHUB_TOKEN=ghp_xxxxx\nAPI_KEY=key123\nDB_PASSWORD=pass456\n```\n\nRun with secrets file:\n```bash\nact --secret-file .secrets\n```\n\n### Environment Variables\n\n```bash\n# Use existing env var\nact -s GITHUB_TOKEN\n\n# Set from command\nexport MY_SECRET=value\nact -s MY_SECRET\n```\n\n## Configuration\n\n### .actrc File\n\nCreate `.actrc` in repository root or home directory:\n\n```\n# Use specific platform\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Default secrets file\n--secret-file .secrets\n\n# Default environment\n--env-file .env\n\n# Container architecture\n--container-architecture linux/amd64\n\n# Verbose output\n-v\n```\n\n### Custom Runner Images\n\n```bash\n# Use custom image for platform\nact -P ubuntu-latest=my-custom-image:latest\n\n# Use medium size images (recommended)\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Use micro images (faster, less compatible)\nact -P ubuntu-latest=node:16-buster-slim\n```\n\n### Recommended Images\n\nact supports different image sizes:\n\n**Medium images (recommended):**\n- Better compatibility with GitHub Actions\n- More pre-installed tools\n- Slower startup but fewer failures\n\n```bash\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n-P ubuntu-22.04=catthehacker/ubuntu:act-22.04\n```\n\n**Micro images:**\n- Faster startup\n- Minimal pre-installed tools\n- May require additional setup\n\n## Environment Variables\n\n### Via .env File\n\nCreate `.env` file:\n```\nNODE_ENV=test\nAPI_URL=http://localhost:3000\nLOG_LEVEL=debug\n```\n\nUse with act:\n```bash\nact --env-file .env\n```\n\n### Via Command Line\n\n```bash\nact --env NODE_ENV=test --env API_URL=http://localhost:3000\n```\n\n## Advanced Usage\n\n### Bind Workspace\n\nMount local directory into container:\n```bash\nact --bind\n```\n\n### Reuse Containers\n\nKeep containers between runs for faster execution:\n```bash\nact --reuse\n```\n\n### Specific Platforms\n\n```bash\n# Run on specific platform\nact -P ubuntu-latest=ubuntu:latest\n\n# Multiple platforms\nact -P ubuntu-latest=ubuntu:latest \\\n    -P windows-latest=windows:latest\n```\n\n### Container Architecture\n\n```bash\n# Specify architecture (useful for M1/M2 Macs)\nact --container-architecture linux/amd64\n```\n\n### Network Configuration\n\n```bash\n# Use host network\nact --container-daemon-socket -\n\n# Custom network\nact --network my-network\n```\n\n### Artifact Server\n\n```bash\n# Enable artifact server on specific port\nact --artifact-server-path /tmp/artifacts \\\n    --artifact-server-port 34567\n```\n\n## Debugging\n\n### Verbose Output\n\n```bash\n# Verbose logging\nact -v\n\n# Very verbose (debug level)\nact -vv\n```\n\n### Watch Mode\n\n```bash\n# Watch for file changes and re-run\nact --watch\n```\n\n### Interactive Shell\n\n```bash\n# Drop into shell on failure\nact --shell bash\n```\n\n### Container Inspection\n\n```bash\n# List act containers\ndocker ps -a | grep act\n\n# Inspect specific container\ndocker inspect <container-id>\n\n# View logs\ndocker logs <container-id>\n```\n\n## Limitations and Differences\n\n### Not Supported by act\n\n- Some GitHub-hosted runner features\n- GitHub Apps and installations\n- OIDC token generation\n- Some GitHub API interactions\n- Certain cache implementations\n- Job summaries and annotations (limited)\n\n### Workarounds\n\n**Missing tools:**\n```yaml\nsteps:\n  - name: Install missing tool\n    run: |\n      if ! command -v tool &> /dev/null; then\n        apt-get update && apt-get install -y tool\n      fi\n```\n\n**GitHub API calls:**\n```yaml\n# Use GITHUB_TOKEN from secrets\n- env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n  run: gh api repos/${{ github.repository }}/issues\n```\n\n## Common Patterns\n\n### Testing Pull Request Workflow\n\n```bash\n# Create PR event payload\ncat > pr-event.json << EOF\n{\n  \"pull_request\": {\n    \"number\": 1,\n    \"head\": { \"ref\": \"feature\" },\n    \"base\": { \"ref\": \"main\" }\n  }\n}\nEOF\n\n# Run PR workflow\nact pull_request -e pr-event.json -j test\n```\n\n### CI/CD Pipeline Testing\n\n```bash\n# Test entire CI pipeline\nact push\n\n# Test specific stages\nact push -j build\nact push -j test\nact push -j deploy --secret-file .secrets\n```\n\n### Matrix Testing\n\n```bash\n# Run matrix strategy locally\nact -j test\n\n# Test specific matrix combination (modify workflow temporarily)\nact -j test --matrix node-version:20\n```\n\n### Workflow Development Cycle\n\n```bash\n# 1. List jobs\nact -l\n\n# 2. Dry run\nact -n -j build\n\n# 3. Run with verbose output\nact -v -j build\n\n# 4. Iterate and test\nact --reuse -j build\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n**Error: Cannot connect to Docker daemon**\n```bash\n# Start Docker\n# macOS: Start Docker Desktop\n# Linux:\nsudo systemctl start docker\n```\n\n**Error: Permission denied**\n```bash\n# Add user to docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Image Pull Issues\n\n**Error: Failed to pull image**\n```bash\n# Use specific image version\nact -P ubuntu-latest=ubuntu:22.04\n\n# Or use act's recommended images\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n```\n\n### Workflow Not Found\n\n**Error: No workflows found**\n```bash\n# Verify workflow files exist\nls -la .github/workflows/\n\n# Check workflow syntax\nact -n -v\n```\n\n### Secret Issues\n\n**Error: Secret not found**\n```bash\n# List required secrets from workflow\ngrep -r \"secrets\\.\" .github/workflows/\n\n# Provide via command line\nact -s SECRET_NAME=value\n\n# Or use secrets file\nact --secret-file .secrets\n```\n\n### Action Failures\n\n**Error: Action not found or fails**\n```yaml\n# Ensure action versions are compatible\n# Some actions may not work locally\n\n# Use alternative actions if needed\n# Or skip problematic steps locally:\n- name: Problematic step\n  if: github.event_name != 'act'  # Skip in act\n  uses: some/action@v1\n```\n\n### Platform Differences\n\n**Error: Command not found**\n```bash\n# Use medium-sized images with more tools\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Or install tools in workflow\n- run: apt-get update && apt-get install -y <tool>\n```\n\n## Best Practices\n\n### .actrc Configuration\n\nCreate `.actrc` in repository:\n```\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n--secret-file .secrets\n--container-architecture linux/amd64\n--artifact-server-path /tmp/artifacts\n```\n\n### .gitignore Entries\n\n```gitignore\n# act secrets and config\n.secrets\n.env\n\n# act artifacts\n/tmp/artifacts/\n```\n\n### Conditional Logic for Local Testing\n\n```yaml\nsteps:\n  # Skip in local testing\n  - name: Deploy\n    if: github.event_name != 'act'\n    run: ./deploy.sh\n\n  # Run only in local testing\n  - name: Local setup\n    if: github.event_name == 'act'\n    run: ./local-setup.sh\n```\n\n### Fast Feedback Loop\n\n```bash\n# Use reuse flag for faster iterations\nact --reuse -j test\n\n# Run specific job being developed\nact -j my-new-job -v\n\n# Watch mode for continuous testing\nact --watch -j test\n```\n\n## Integration with Development Workflow\n\n### Pre-commit Testing\n\n```bash\n# Test before committing\nact -j test && git commit -m \"message\"\n\n# Git hook (.git/hooks/pre-commit)\n#!/bin/bash\nact -j test --quiet\n```\n\n### Quick Validation\n\n```bash\n# Validate workflow syntax\nact -n\n\n# Test specific changes\nact -j affected-job\n```\n\n### CI Parity\n\n```bash\n# Use same images as CI\nact -P ubuntu-latest=ubuntu:22.04\n\n# Use same secrets structure\nact --secret-file .secrets\n```\n\n## Scripts and Automation\n\n### Installation Script\n\nThe plugin includes an installation script at `scripts/install-act.sh`:\n\n```bash\n#!/usr/bin/env bash\n# Install act via mise or fallback methods\n\nif command -v mise &> /dev/null; then\n  echo \"Installing act via mise...\"\n  mise install act\nelif [[ \"$OSTYPE\" == \"darwin\"* ]] && command -v brew &> /dev/null; then\n  echo \"Installing act via Homebrew...\"\n  brew install act\nelif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n  echo \"Installing act via install script...\"\n  curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\nelse\n  echo \"Please install act manually: https://github.com/nektos/act\"\n  exit 1\nfi\n\nact --version\n```\n\nRun with:\n```bash\nchmod +x scripts/install-act.sh\n./scripts/install-act.sh\n```\n\n## Anti-Fabrication Requirements\n\n- Execute `act --version` before documenting version numbers\n- Use `act -l` to verify actual workflows before claiming their presence\n- Execute `docker ps` to confirm Docker is running before troubleshooting\n- Run `act -n` to validate workflow syntax before claiming correctness\n- Execute actual `act` commands to verify behavior before documenting output format\n- Use `docker images` to verify available images before recommending specific versions\n- Never claim success rates or performance metrics without actual measurement\n- Execute `act -v` to observe actual error messages before documenting troubleshooting steps\n- Use Read tool to verify workflow files exist before testing them with act\n- Run actual event payloads through act before claiming they work correctly"
              },
              {
                "name": "github-actions",
                "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration",
                "path": "github/skills/actions/SKILL.md",
                "frontmatter": {
                  "name": "github-actions",
                  "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration"
                },
                "content": "# GitHub Actions\n\nActivate when creating, modifying, troubleshooting, or optimizing GitHub Actions components. This skill covers action development, marketplace integration, and best practices.\n\n## When to Use This Skill\n\nActivate when:\n- Creating custom GitHub Actions (JavaScript, Docker, or composite)\n- Publishing actions to GitHub Marketplace\n- Configuring action metadata and inputs/outputs\n- Implementing action security and permissions\n- Troubleshooting action execution\n- Selecting or evaluating marketplace actions\n- Optimizing action performance and reliability\n\n## Action Types\n\n### JavaScript Actions\n\nExecute directly on runners with fast startup and cross-platform compatibility.\n\n**Structure:**\n```\nmy-action/\n├── action.yml        # Metadata and interface\n├── index.js          # Entry point\n├── package.json      # Dependencies\n└── node_modules/     # Bundled dependencies\n```\n\n**Key Requirements:**\n- Use `@actions/core` for inputs/outputs\n- Use `@actions/github` for GitHub API access\n- Bundle all dependencies (use @vercel/ncc)\n- Support Node.js LTS versions\n\n**Example action.yml:**\n```yaml\nname: 'My JavaScript Action'\ndescription: 'Performs custom task'\ninputs:\n  token:\n    description: 'GitHub token'\n    required: true\n  config:\n    description: 'Configuration file path'\n    required: false\n    default: 'config.yml'\noutputs:\n  result:\n    description: 'Action result'\nruns:\n  using: 'node20'\n  main: 'dist/index.js'\n```\n\n### Docker Container Actions\n\nProvide consistent execution environment with all dependencies packaged.\n\n**Structure:**\n```\nmy-action/\n├── action.yml\n├── Dockerfile\n├── entrypoint.sh\n└── src/\n```\n\n**Key Requirements:**\n- Use lightweight base images (Alpine when possible)\n- Set proper file permissions\n- Handle signals gracefully\n- Output to STDOUT/STDERR correctly\n\n**Example Dockerfile:**\n```dockerfile\nFROM alpine:3.18\n\nRUN apk add --no-cache bash curl jq\n\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\n### Composite Actions\n\nCombine multiple steps and actions into reusable units.\n\n**Structure:**\n```yaml\nname: 'Setup Environment'\ndescription: 'Configure development environment'\ninputs:\n  node-version:\n    description: 'Node.js version'\n    required: false\n    default: '20'\nruns:\n  using: 'composite'\n  steps:\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - run: npm ci\n      shell: bash\n    - run: npm run build\n      shell: bash\n```\n\n## Action Metadata (action.yml)\n\n### Required Fields\n\n```yaml\nname: 'Action Name'           # Marketplace display name\ndescription: 'What it does'   # Clear, concise purpose\nruns:                         # Execution configuration\n  using: 'node20'            # or 'docker' or 'composite'\n```\n\n### Optional Fields\n\n```yaml\nauthor: 'Your Name'\nbranding:                    # Marketplace icon/color\n  icon: 'activity'\n  color: 'blue'\ninputs:                      # Define all inputs\n  input-name:\n    description: 'Purpose'\n    required: true\n    default: 'value'\noutputs:                     # Define all outputs\n  output-name:\n    description: 'What it contains'\n```\n\n## Inputs and Outputs\n\n### Reading Inputs\n\n**JavaScript:**\n```javascript\nconst core = require('@actions/core');\nconst token = core.getInput('token', { required: true });\nconst config = core.getInput('config') || 'default.yml';\n```\n\n**Shell:**\n```bash\nTOKEN=\"${{ inputs.token }}\"\nCONFIG=\"${{ inputs.config }}\"\n```\n\n### Setting Outputs\n\n**JavaScript:**\n```javascript\ncore.setOutput('result', 'success');\ncore.setOutput('artifact-url', artifactUrl);\n```\n\n**Shell:**\n```bash\necho \"result=success\" >> $GITHUB_OUTPUT\necho \"artifact-url=$ARTIFACT_URL\" >> $GITHUB_OUTPUT\n```\n\n## GitHub Actions Toolkit\n\nEssential npm packages for JavaScript actions:\n\n### @actions/core\n```javascript\nconst core = require('@actions/core');\n\n// Inputs/Outputs\nconst input = core.getInput('name');\ncore.setOutput('name', value);\n\n// Logging\ncore.info('Information message');\ncore.warning('Warning message');\ncore.error('Error message');\ncore.debug('Debug message');\n\n// Grouping\ncore.startGroup('Group name');\n// ... operations\ncore.endGroup();\n\n// Failure\ncore.setFailed('Action failed: reason');\n\n// Secrets\ncore.setSecret('sensitive-value');  // Masks in logs\n\n// Environment\ncore.exportVariable('VAR_NAME', 'value');\n```\n\n### @actions/github\n```javascript\nconst github = require('@actions/github');\n\n// Context\nconst context = github.context;\nconsole.log(context.repo);        // { owner, repo }\nconsole.log(context.sha);         // Commit SHA\nconsole.log(context.ref);         // Branch/tag ref\nconsole.log(context.actor);       // Triggering user\nconsole.log(context.payload);     // Webhook payload\n\n// Octokit client\nconst token = core.getInput('token');\nconst octokit = github.getOctokit(token);\n\n// API operations\nconst { data: issues } = await octokit.rest.issues.listForRepo({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  state: 'open'\n});\n```\n\n### @actions/exec\n```javascript\nconst exec = require('@actions/exec');\n\n// Execute commands\nawait exec.exec('npm', ['install']);\n\n// Capture output\nlet output = '';\nawait exec.exec('git', ['log', '--oneline'], {\n  listeners: {\n    stdout: (data) => { output += data.toString(); }\n  }\n});\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate and sanitize inputs:\n```javascript\nconst core = require('@actions/core');\n\nfunction validateInput(input) {\n  // Check for command injection\n  if (/[;&|`$()]/.test(input)) {\n    throw new Error('Invalid characters in input');\n  }\n  return input;\n}\n\nconst userInput = core.getInput('user-input');\nconst safeInput = validateInput(userInput);\n```\n\n### Token Permissions\n\nRequest minimal required permissions:\n```yaml\npermissions:\n  contents: read           # Read repository\n  pull-requests: write     # Comment on PRs\n  issues: write           # Create issues\n```\n\n### Secret Handling\n\n```javascript\n// Mask secrets in logs\ncore.setSecret(sensitiveValue);\n\n// Never log tokens\ncore.debug(`Token: ${token}`);  // ❌ WRONG\ncore.debug('Token received');   // ✅ CORRECT\n\n// Secure token usage\nconst octokit = github.getOctokit(token);\n// Token automatically included in requests\n```\n\n### Dependency Security\n\n```bash\n# Audit dependencies\nnpm audit\n\n# Use specific versions\nnpm install @actions/core@1.10.0\n\n# Bundle dependencies\nnpm install -g @vercel/ncc\nncc build index.js -o dist\n```\n\n## Marketplace Publishing\n\n### Prerequisites\n\n- Public repository\n- action.yml in repository root\n- README.md with usage examples\n- LICENSE file\n- Repository topics (optional)\n\n### Publishing Process\n\n1. Create release with semantic version tag:\n```bash\ngit tag -a v1.0.0 -m \"Release v1.0.0\"\ngit push origin v1.0.0\n```\n\n2. Create GitHub Release from tag\n3. Check \"Publish this Action to GitHub Marketplace\"\n4. Select primary category\n5. Verify branding icon/color\n\n### Version Management\n\nUse semantic versioning with major version tags:\n```bash\n# Release v1.2.3\ngit tag -a v1.2.3 -m \"Release v1.2.3\"\ngit tag -fa v1 -m \"Update v1 to v1.2.3\"\ngit push origin v1.2.3 v1 --force\n```\n\nUsers reference by major version:\n```yaml\n- uses: owner/action@v1  # Tracks latest v1.x.x\n```\n\n## Testing Actions Locally\n\nUse `act` for local testing (see act skill):\n```bash\n# Test action in current directory\nact -j test\n\n# Test with specific event\nact push\n\n# Test with secrets\nact -s GITHUB_TOKEN=ghp_xxx\n```\n\n## Common Patterns\n\n### Matrix Testing Action\n\n```yaml\n# action.yml\nname: 'Matrix Test Runner'\ndescription: 'Run tests across multiple configurations'\ninputs:\n  matrix-config:\n    description: 'JSON matrix configuration'\n    required: true\nruns:\n  using: 'composite'\n  steps:\n    - run: |\n        echo \"Testing with config: ${{ inputs.matrix-config }}\"\n        # Parse and execute tests\n      shell: bash\n```\n\n### Cache Management Action\n\n```javascript\nconst core = require('@actions/core');\nconst cache = require('@actions/cache');\n\nasync function run() {\n  const paths = [\n    'node_modules',\n    '.npm'\n  ];\n\n  const key = `deps-${process.platform}-${hashFiles('package-lock.json')}`;\n\n  // Restore cache\n  const cacheKey = await cache.restoreCache(paths, key);\n\n  if (!cacheKey) {\n    core.info('Cache miss, installing dependencies');\n    await exec.exec('npm', ['ci']);\n    await cache.saveCache(paths, key);\n  } else {\n    core.info(`Cache hit: ${cacheKey}`);\n  }\n}\n```\n\n### Artifact Upload Action\n\n```javascript\nconst artifact = require('@actions/artifact');\n\nasync function uploadArtifact() {\n  const artifactClient = artifact.create();\n  const files = [\n    'dist/bundle.js',\n    'dist/styles.css'\n  ];\n\n  const rootDirectory = 'dist';\n  const options = {\n    continueOnError: false\n  };\n\n  const uploadResponse = await artifactClient.uploadArtifact(\n    'build-artifacts',\n    files,\n    rootDirectory,\n    options\n  );\n\n  core.setOutput('artifact-id', uploadResponse.artifactId);\n}\n```\n\n## Troubleshooting\n\n### Action Not Found\n\n- Verify repository is public or accessible\n- Check action.yml exists in repository root\n- Confirm version tag exists\n\n### Permission Denied\n\n```yaml\n# Add required permissions to workflow\npermissions:\n  contents: write\n  pull-requests: write\n```\n\n### Node Modules Missing\n\n- Bundle dependencies with ncc\n- Check dist/ folder is committed\n- Verify node_modules excluded from .gitignore for dist/\n\n### Docker Action Fails\n\n- Check Dockerfile syntax\n- Verify entrypoint has execute permissions\n- Test container locally: `docker build -t test . && docker run test`\n\n## Anti-Fabrication Requirements\n\n- Execute Read or Glob tools to verify action files exist before claiming structure\n- Use Bash to test commands before documenting syntax\n- Validate action.yml schema against actual files using tool analysis\n- Execute actual API calls with @actions/github before documenting responses\n- Test permission configurations in real workflows before recommending settings\n- Never claim action capabilities without reading actual implementation code\n- Report actual npm audit results when discussing security, not fabricated vulnerability counts"
              },
              {
                "name": "github-workflows",
                "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns",
                "path": "github/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "github-workflows",
                  "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns"
                },
                "content": "# GitHub Workflows\n\nActivate when creating, modifying, debugging, or optimizing GitHub Actions workflow files. This skill covers workflow syntax, structure, best practices, and common CI/CD patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Writing .github/workflows/*.yml files\n- Configuring workflow triggers and events\n- Defining jobs, steps, and dependencies\n- Using expressions and contexts\n- Managing secrets and environment variables\n- Implementing CI/CD pipelines\n- Optimizing workflow performance\n- Debugging workflow failures\n\n## Workflow File Structure\n\n### Basic Anatomy\n\n```yaml\nname: CI                              # Workflow name (optional)\n\non:                                   # Trigger events\n  push:\n    branches: [main, develop]\n  pull_request:\n\nenv:                                  # Global environment variables\n  NODE_VERSION: '20'\n\njobs:                                 # Job definitions\n  build:\n    name: Build and Test            # Job name (optional)\n    runs-on: ubuntu-latest          # Runner environment\n\n    steps:\n      - name: Checkout code         # Step name (optional)\n        uses: actions/checkout@v4   # Use an action\n\n      - name: Run tests\n        run: npm test               # Run command\n```\n\n### File Location\n\nWorkflows must be in `.github/workflows/` directory:\n```\n.github/\n└── workflows/\n    ├── ci.yml\n    ├── deploy.yml\n    └── release.yml\n```\n\n## Trigger Events (on:)\n\n### Push Events\n\n```yaml\non:\n  push:\n    branches:\n      - main\n      - 'release/**'        # Glob patterns\n    tags:\n      - 'v*'                # Version tags\n    paths:\n      - 'src/**'            # Only when these paths change\n      - '!docs/**'          # Ignore docs changes\n```\n\n### Pull Request Events\n\n```yaml\non:\n  pull_request:\n    types:\n      - opened\n      - synchronize       # New commits pushed\n      - reopened\n    branches:\n      - main\n    paths-ignore:\n      - '**.md'\n```\n\n### Schedule (Cron)\n\n```yaml\non:\n  schedule:\n    # Every day at 2am UTC\n    - cron: '0 2 * * *'\n    # Every Monday at 9am UTC\n    - cron: '0 9 * * 1'\n```\n\n### Manual Trigger (workflow_dispatch)\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Deployment environment'\n        required: true\n        type: choice\n        options:\n          - development\n          - staging\n          - production\n      debug:\n        description: 'Enable debug logging'\n        required: false\n        type: boolean\n        default: false\n```\n\n### Multiple Events\n\n```yaml\non:\n  push:\n    branches: [main]\n  pull_request:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly\n```\n\n## Jobs\n\n### Basic Job Configuration\n\n```yaml\njobs:\n  build:\n    name: Build Application\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm ci\n      - run: npm run build\n```\n\n### Runner Selection\n\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest        # Ubuntu (fastest, most common)\n\n  test-macos:\n    runs-on: macos-latest         # macOS\n\n  test-windows:\n    runs-on: windows-latest       # Windows\n\n  test-specific:\n    runs-on: ubuntu-22.04         # Specific version\n```\n\n### Matrix Strategy\n\n```yaml\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        node: [18, 20, 21]\n        exclude:\n          - os: macos-latest\n            node: 18\n      fail-fast: false            # Continue on failure\n      max-parallel: 4             # Concurrent jobs limit\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n      - run: npm test\n```\n\n### Job Dependencies\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n\n  test:\n    needs: build                  # Wait for build\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  deploy:\n    needs: [build, test]          # Wait for multiple jobs\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n```\n\n### Conditional Execution\n\n```yaml\njobs:\n  deploy:\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n\n  notify:\n    if: failure()                 # Run only if previous jobs failed\n    needs: [build, test]\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Build failed\"\n```\n\n## Steps\n\n### Using Actions\n\n```yaml\nsteps:\n  - name: Checkout repository\n    uses: actions/checkout@v4\n    with:\n      fetch-depth: 0              # Full history\n      submodules: recursive       # Include submodules\n\n  - name: Setup Node.js\n    uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n```\n\n### Running Commands\n\n```yaml\nsteps:\n  - name: Single command\n    run: npm install\n\n  - name: Multi-line script\n    run: |\n      echo \"Installing dependencies\"\n      npm ci\n      npm run build\n\n  - name: Shell selection\n    shell: bash\n    run: echo \"Using bash\"\n```\n\n### Conditional Steps\n\n```yaml\nsteps:\n  - name: Run on main branch only\n    if: github.ref == 'refs/heads/main'\n    run: npm run deploy\n\n  - name: Run on PR only\n    if: github.event_name == 'pull_request'\n    run: npm run test:pr\n```\n\n### Continue on Error\n\n```yaml\nsteps:\n  - name: Lint (optional)\n    continue-on-error: true\n    run: npm run lint\n\n  - name: Test (required)\n    run: npm test\n```\n\n## Environment Variables and Secrets\n\n### Global Variables\n\n```yaml\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo $NODE_ENV\n```\n\n### Job-Level Variables\n\n```yaml\njobs:\n  build:\n    env:\n      BUILD_TYPE: release\n    steps:\n      - run: echo $BUILD_TYPE\n```\n\n### Step-Level Variables\n\n```yaml\nsteps:\n  - name: Configure\n    env:\n      CONFIG_PATH: ./config.json\n    run: cat $CONFIG_PATH\n```\n\n### Using Secrets\n\n```yaml\nsteps:\n  - name: Deploy\n    env:\n      API_KEY: ${{ secrets.API_KEY }}\n      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}\n    run: ./deploy.sh\n```\n\n### Setting Variables Between Steps\n\n```yaml\nsteps:\n  - name: Set version\n    id: version\n    run: echo \"VERSION=$(cat version.txt)\" >> $GITHUB_OUTPUT\n\n  - name: Use version\n    run: echo \"Version is ${{ steps.version.outputs.VERSION }}\"\n```\n\n## Contexts\n\n### github Context\n\n```yaml\nsteps:\n  - name: Context information\n    run: |\n      echo \"Repository: ${{ github.repository }}\"\n      echo \"Branch: ${{ github.ref_name }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Run ID: ${{ github.run_id }}\"\n```\n\n### env Context\n\n```yaml\nenv:\n  MY_VAR: value\n\nsteps:\n  - run: echo \"${{ env.MY_VAR }}\"\n```\n\n### job Context\n\n```yaml\nsteps:\n  - name: Job status\n    if: job.status == 'success'\n    run: echo \"Job succeeded\"\n```\n\n### steps Context\n\n```yaml\nsteps:\n  - id: first-step\n    run: echo \"output=hello\" >> $GITHUB_OUTPUT\n\n  - run: echo \"${{ steps.first-step.outputs.output }}\"\n```\n\n### runner Context\n\n```yaml\nsteps:\n  - run: |\n      echo \"OS: ${{ runner.os }}\"\n      echo \"Arch: ${{ runner.arch }}\"\n      echo \"Temp: ${{ runner.temp }}\"\n```\n\n### matrix Context\n\n```yaml\nstrategy:\n  matrix:\n    version: [18, 20]\n\nsteps:\n  - run: echo \"Node ${{ matrix.version }}\"\n```\n\n## Expressions\n\n### Operators\n\n```yaml\nsteps:\n  # Comparison\n  - if: github.ref == 'refs/heads/main'\n\n  # Logical\n  - if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n  - if: github.event_name == 'pull_request' || github.event_name == 'push'\n\n  # Negation\n  - if: \"!cancelled()\"\n\n  # Contains\n  - if: contains(github.event.head_commit.message, '[skip ci]')\n\n  # StartsWith/EndsWith\n  - if: startsWith(github.ref, 'refs/tags/v')\n  - if: endsWith(github.ref, '-beta')\n```\n\n### Functions\n\n```yaml\nsteps:\n  # Status functions\n  - if: success()        # Previous steps succeeded\n  - if: failure()        # Any previous step failed\n  - if: always()         # Always run\n  - if: cancelled()      # Workflow cancelled\n\n  # String functions\n  - run: echo \"${{ format('Hello {0}', github.actor) }}\"\n  - if: contains(github.event.pull_request.labels.*.name, 'deploy')\n\n  # JSON functions\n  - run: echo '${{ toJSON(github.event) }}'\n  - run: echo '${{ fromJSON(env.CONFIG).database.host }}'\n\n  # Hash function\n  - run: echo \"${{ hashFiles('**/package-lock.json') }}\"\n```\n\n## Artifacts\n\n### Upload Artifacts\n\n```yaml\nsteps:\n  - name: Build\n    run: npm run build\n\n  - name: Upload artifacts\n    uses: actions/upload-artifact@v4\n    with:\n      name: build-files\n      path: |\n        dist/\n        build/\n      retention-days: 7\n      if-no-files-found: error\n```\n\n### Download Artifacts\n\n```yaml\njobs:\n  build:\n    steps:\n      - run: npm run build\n      - uses: actions/upload-artifact@v4\n        with:\n          name: dist\n          path: dist/\n\n  test:\n    needs: build\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: dist\n          path: dist/\n      - run: npm test\n```\n\n## Caching\n\n### npm Cache\n\n```yaml\nsteps:\n  - uses: actions/checkout@v4\n  - uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n  - run: npm ci\n```\n\n### Manual Cache\n\n```yaml\nsteps:\n  - uses: actions/cache@v4\n    with:\n      path: |\n        ~/.npm\n        node_modules\n      key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n      restore-keys: |\n        ${{ runner.os }}-node-\n```\n\n## Permissions\n\n### Repository Token Permissions\n\n```yaml\npermissions:\n  contents: read              # Repository content\n  pull-requests: write        # PR comments\n  issues: write              # Issue creation/comments\n  checks: write              # Check runs\n  statuses: write            # Commit statuses\n  deployments: write         # Deployments\n  packages: write            # Package registry\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n```\n\n### Job-Level Permissions\n\n```yaml\njobs:\n  build:\n    permissions:\n      contents: read\n      pull-requests: write\n    steps:\n      - uses: actions/checkout@v4\n```\n\n## Concurrency\n\n### Prevent Concurrent Runs\n\n```yaml\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true    # Cancel running workflows\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - run: ./deploy.sh\n```\n\n### Job-Level Concurrency\n\n```yaml\njobs:\n  deploy:\n    concurrency:\n      group: deploy-${{ github.ref }}\n      cancel-in-progress: false\n    steps:\n      - run: ./deploy.sh\n```\n\n## Reusable Workflows\n\n### Define Reusable Workflow\n\n```yaml\n# .github/workflows/reusable-test.yml\nname: Reusable Test Workflow\n\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n      coverage:\n        required: false\n        type: boolean\n        default: false\n    outputs:\n      test-result:\n        description: \"Test execution result\"\n        value: ${{ jobs.test.outputs.result }}\n    secrets:\n      token:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    outputs:\n      result: ${{ steps.test.outputs.result }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ inputs.node-version }}\n      - run: npm test\n        id: test\n```\n\n### Call Reusable Workflow\n\n```yaml\njobs:\n  test:\n    uses: ./.github/workflows/reusable-test.yml\n    with:\n      node-version: '20'\n      coverage: true\n    secrets:\n      token: ${{ secrets.GITHUB_TOKEN }}\n```\n\n## Common CI/CD Patterns\n\n### Node.js CI\n\n```yaml\nname: Node.js CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20, 21]\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm test\n      - run: npm run build\n```\n\n### Docker Build and Push\n\n```yaml\nname: Docker\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ghcr.io/${{ github.repository }}\n          tags: |\n            type=ref,event=branch\n            type=semver,pattern={{version}}\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n```\n\n### Deploy on Release\n\n```yaml\nname: Deploy\n\non:\n  release:\n    types: [published]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://example.com\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        env:\n          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\n        run: ./deploy.sh\n```\n\n### Monorepo with Path Filtering\n\n```yaml\nname: Monorepo CI\n\non:\n  pull_request:\n    paths:\n      - 'packages/**'\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      frontend: ${{ steps.filter.outputs.frontend }}\n      backend: ${{ steps.filter.outputs.backend }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          filters: |\n            frontend:\n              - 'packages/frontend/**'\n            backend:\n              - 'packages/backend/**'\n\n  test-frontend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.frontend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=frontend\n\n  test-backend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.backend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=backend\n```\n\n## Debugging Workflows\n\n### Enable Debug Logging\n\nSet repository secrets:\n- `ACTIONS_RUNNER_DEBUG`: true\n- `ACTIONS_STEP_DEBUG`: true\n\n### Debug Steps\n\n```yaml\nsteps:\n  - name: Debug context\n    run: |\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Ref: ${{ github.ref }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n\n  - name: Dump GitHub context\n    run: echo '${{ toJSON(github) }}'\n\n  - name: Dump runner context\n    run: echo '${{ toJSON(runner) }}'\n```\n\n### Tmate Debugging\n\n```yaml\nsteps:\n  - name: Setup tmate session\n    if: failure()\n    uses: mxschmitt/action-tmate@v3\n    timeout-minutes: 30\n```\n\n## Performance Optimization\n\n### Use Caching\n\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}\n```\n\n### Optimize Checkout\n\n```yaml\n- uses: actions/checkout@v4\n  with:\n    fetch-depth: 1              # Shallow clone\n    sparse-checkout: |          # Partial checkout\n      src/\n      tests/\n```\n\n### Concurrent Jobs\n\n```yaml\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run lint\n\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  build:\n    needs: [lint, test]         # Parallel lint and test\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n```\n\n## Anti-Fabrication Requirements\n\n- Execute Read tool to verify workflow files exist before claiming structure\n- Use Bash with `gh workflow list` to confirm actual workflow names before referencing them\n- Execute `gh workflow view <workflow>` to verify trigger configuration before documenting it\n- Use Glob to find actual workflow files before claiming their presence\n- Execute `gh run list` to verify actual workflow runs before discussing execution patterns\n- Never claim workflow success rates without actual run history analysis\n- Validate YAML syntax using yamllint or similar tools via Bash before claiming correctness\n- Report actual permission errors from workflow runs, not fabricated authorization issues\n- Execute actual cache operations before claiming cache hit/miss percentages\n- Use Read tool on action.yml files to verify action inputs/outputs before documenting usage"
              }
            ]
          },
          {
            "name": "rust",
            "description": "Rust programming skills: ownership, borrowing, async, best practices",
            "source": "./rust",
            "category": "language",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install rust@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "ui",
            "description": "UI framework skills: daisyUI components and theming",
            "source": "./ui",
            "category": "frontend",
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install ui@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "daisyui",
                "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design",
                "path": "ui/skills/daisyui/SKILL.md",
                "frontmatter": {
                  "name": "daisyui",
                  "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design"
                },
                "content": "# daisyUI Component Library\n\nUse this skill when building user interfaces with daisyUI and Tailwind CSS, implementing UI components, or configuring themes.\n\n## When to Use This Skill\n\nActivate when:\n- Building UI components with daisyUI\n- Choosing appropriate daisyUI components for design needs\n- Implementing responsive layouts with daisyUI\n- Configuring or customizing themes\n- Converting designs to daisyUI components\n- Troubleshooting daisyUI component styling\n\n## What is daisyUI?\n\ndaisyUI is a Tailwind CSS component library providing:\n\n- **Semantic component classes** - High-level abstractions of Tailwind utilities\n- **33+ built-in themes** - Light, dark, and creative theme variants\n- **Framework-agnostic** - Works with any HTML/CSS project\n- **Utility-first compatible** - Combine components with Tailwind utilities\n\n## Installation\n\nAdd daisyUI to your project:\n\n```bash\nnpm install -D daisyui@latest\n```\n\nConfigure `tailwind.config.js`:\n\n```javascript\nmodule.exports = {\n  plugins: [require(\"daisyui\")],\n}\n```\n\nFor detailed installation options and CDN usage, see `references/installation.md`.\n\n## Component Categories\n\ndaisyUI provides components across these categories:\n\n- **Actions**: Buttons, dropdowns, modals, swap\n- **Data Display**: Cards, badges, tables, carousels, stats\n- **Data Input**: Input, textarea, select, checkbox, radio, toggle\n- **Navigation**: Navbar, menu, tabs, breadcrumbs, pagination\n- **Feedback**: Alert, progress, loading, toast, tooltip\n- **Layout**: Drawer, footer, hero, stack, divider\n\nFor component-specific guidance, consult the appropriate reference file.\n\n## Quick Usage\n\n### Basic Button\n\n```html\n<button class=\"btn\">Button</button>\n<button class=\"btn btn-primary\">Primary</button>\n<button class=\"btn btn-secondary\">Secondary</button>\n<button class=\"btn btn-accent\">Accent</button>\n```\n\n### Card Component\n\n```html\n<div class=\"card w-96 bg-base-100 shadow-xl\">\n  <figure><img src=\"image.jpg\" alt=\"Image\" /></figure>\n  <div class=\"card-body\">\n    <h2 class=\"card-title\">Card Title</h2>\n    <p>Card description text</p>\n    <div class=\"card-actions justify-end\">\n      <button class=\"btn btn-primary\">Action</button>\n    </div>\n  </div>\n</div>\n```\n\n### Modal\n\n```html\n<button class=\"btn\" onclick=\"my_modal.showModal()\">Open Modal</button>\n\n<dialog id=\"my_modal\" class=\"modal\">\n  <div class=\"modal-box\">\n    <h3 class=\"font-bold text-lg\">Modal Title</h3>\n    <p class=\"py-4\">Modal content here</p>\n    <div class=\"modal-action\">\n      <form method=\"dialog\">\n        <button class=\"btn\">Close</button>\n      </form>\n    </div>\n  </div>\n</dialog>\n```\n\n## Theming\n\n### Using Built-in Themes\n\nSet theme via HTML attribute:\n\n```html\n<html data-theme=\"cupcake\">\n```\n\nAvailable themes: light, dark, cupcake, bumblebee, emerald, corporate, synthwave, retro, cyberpunk, valentine, halloween, garden, forest, aqua, lofi, pastel, fantasy, wireframe, black, luxury, dracula, cmyk, autumn, business, acid, lemonade, night, coffee, winter, dim, nord, sunset\n\n### Theme Switching\n\n```html\n<select class=\"select\" data-choose-theme>\n  <option value=\"light\">Light</option>\n  <option value=\"dark\">Dark</option>\n  <option value=\"cupcake\">Cupcake</option>\n</select>\n```\n\nFor advanced theming and customization, see `references/theming.md`.\n\n## Responsive Design\n\ndaisyUI components work with Tailwind's responsive prefixes:\n\n```html\n<button class=\"btn btn-sm md:btn-md lg:btn-lg\">\n  Responsive Button\n</button>\n\n<div class=\"card w-full md:w-96\">\n  <!-- Responsive card -->\n</div>\n```\n\n## When to Consult References\n\n- **Installation details**: Read `references/installation.md`\n- **Complete component list**: Read `references/components.md`\n- **Theming and customization**: Read `references/theming.md`\n- **Layout patterns**: Read `references/layouts.md`\n- **Form components**: Read `references/forms.md`\n- **Common patterns**: Read `references/patterns.md`\n\n## Combining with Tailwind Utilities\n\ndaisyUI semantic classes combine with Tailwind utilities:\n\n```html\n<!-- daisyUI component + Tailwind utilities -->\n<button class=\"btn btn-primary shadow-lg hover:shadow-xl transition-all\">\n  Enhanced Button\n</button>\n\n<div class=\"card bg-base-100 border-2 border-primary rounded-lg p-4\">\n  <!-- Card with custom styling -->\n</div>\n```\n\n## Key Principles\n\n- **Semantic over utility**: Use component classes for common patterns\n- **Utility for customization**: Apply Tailwind utilities for unique styling\n- **Theme-aware**: Components adapt to theme colors automatically\n- **Accessible**: Components follow accessibility best practices\n- **Composable**: Combine components to build complex UIs\n\n## Pro Tips\n\n- Use `btn-{size}` modifiers: `btn-xs`, `btn-sm`, `btn-md`, `btn-lg`\n- Add `btn-outline` for outlined button variants\n- Use `badge` component for status indicators\n- Combine `modal` with `modal-backdrop` for better UX\n- Use `drawer` for mobile navigation patterns\n- Leverage `stats` component for dashboard metrics\n- Use `loading` class on buttons for async operations"
              }
            ]
          },
          {
            "name": "claudio",
            "description": "Comprehensive project analysis and planning system with parallel discovery agents",
            "source": {
              "source": "github",
              "repo": "vinnie357/claudio"
            },
            "category": "tools",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add vinnie357/claude-skills",
              "/plugin install claudio@vinnie357"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-14T00:51:12Z",
              "created_at": "2025-11-15T01:48:14Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/research-skill",
                "description": "Research topics and create Agent Skills with proper structure and documentation",
                "path": "claude-code/commands/research-skill.md",
                "frontmatter": {
                  "description": "Research topics and create Agent Skills with proper structure and documentation",
                  "argument-hint": "<skill-name> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create a properly structured Agent Skill following the Agent Skills Specification.\n\n**Skill Creation:**\n- **Directory Structure**: Creates `skills/<skill-name>/SKILL.md` with proper frontmatter\n- **Progressive Disclosure**: Generates reference files in `references/` for deep context\n- **Source Documentation**: Updates `promptlog/sources.md` with all research sources\n- **Specification Compliance**: Follows Agent Skills Specification v1.0\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **YAML Frontmatter**: Auto-generates name, description, license, metadata\n- **Reference Management**: Creates separate files for detailed documentation\n- **Source Tracking**: Maintains traceability in promptlog/sources.md\n\n**Examples:**\n```\n/research-skill elixir-genserver\n# Creates: skills/elixir-genserver/SKILL.md\n# Updates: promptlog/sources.md\n\n/research-skill kubernetes-operators --complexity=high\n# Creates: skills/kubernetes-operators/SKILL.md\n#          skills/kubernetes-operators/references/\n# Updates: promptlog/sources.md\n\n/research-skill react-hooks --complexity=medium\n# Creates: skills/react-hooks/SKILL.md with enhanced analysis\n```\n\n**SKILL.md Structure:**\n```markdown\n---\nname: skill-name\ndescription: When Claude should use this skill (concise, activation-focused)\nlicense: MIT\n---\n\n# Skill Name\n\n## When to Use This Skill\n[Activation criteria]\n\n## Core Concepts\n[Essential knowledge]\n\n## Best Practices\n[Guidelines and patterns]\n\n## Examples\n[Concrete usage examples]\n\n## References\n[Links to reference files if needed]\n```\n\n**Workflow:**\n1. **Research**: Gather authoritative sources and best practices\n2. **Structure**: Create skill directory following spec\n3. **Generate SKILL.md**: Write frontmatter and core content\n4. **Create References**: Add detailed docs in references/ for progressive disclosure\n5. **Document Sources**: Update promptlog/sources.md with all sources used\n6. **Validate**: Ensure spec compliance and activation clarity\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. Research the topic thoroughly using web search and authoritative sources\n2. Create the skill directory: `skills/<skill-name>/`\n3. Generate SKILL.md with:\n   - Proper YAML frontmatter (name, description, license)\n   - Clear activation criteria in description\n   - Core procedural knowledge in markdown body\n   - Concrete examples and patterns\n4. Create `skills/<skill-name>/references/` if detailed documentation is needed\n5. Update `promptlog/sources.md` with:\n   - All URLs and documentation sources used\n   - Purpose of each source\n   - Key concepts extracted\n   - Date accessed\n6. Follow the 7-step skill creation workflow from CLAUDE.md\n7. Use imperative/infinitive language (not second-person)\n8. Keep commonly-used context in SKILL.md, detailed references separate\n\nThe agent should produce a complete, spec-compliant skill ready for use."
              },
              {
                "name": "/gcms",
                "description": "Generate brief conventional commit message suggestions",
                "path": "core/commands/gcms.md",
                "frontmatter": {
                  "description": "Generate brief conventional commit message suggestions",
                  "argument-hint": ""
                },
                "content": "Please analyze the current git status and suggest 1-3 brief conventional commit messages based on the staged and unstaged changes. Use the git-commit-message subagent to perform this analysis."
              },
              {
                "name": "/research",
                "description": "Research topics and create comprehensive planning documentation",
                "path": "core/commands/research.md",
                "frontmatter": {
                  "description": "Research topics and create comprehensive planning documentation",
                  "argument-hint": "<category> <topic> [--complexity=low|medium|high]"
                },
                "content": "Research a topic and create comprehensive documentation for planning, understanding, and working with the subject.\n\n**Document Creation:**\n- **Directory Structure**: Creates `research/<category>/<topic>/`\n- **File Generation**: Produces `overview.md`, `troubleshooting.md`, and optional guides\n- **Planning Focus**: Helps understand topics before implementation\n- **Reference Material**: Creates searchable knowledge base\n\n**Features:**\n- **Automatic Complexity Assessment**: Evaluates topic complexity (1-10 scale)\n- **Thinking Mode Selection**: Standard/Extended/Deep based on complexity\n- **Manual Override**: Use `--complexity=<level>` to force thinking depth\n- **Structured Content**: Consistent templates for reliability\n- **Authoritative Sources**: Links to official docs and best practices\n- **Practical Examples**: Real-world usage patterns and code samples\n\n**Examples:**\n```\n/research development docker-compose\n# Creates: research/development/docker-compose/\n#          - overview.md\n#          - troubleshooting.md\n\n/research infrastructure kubernetes-networking --complexity=high\n# Creates: research/infrastructure/kubernetes-networking/\n#          - overview.md (with deep analysis)\n#          - troubleshooting.md\n#          - best-practices.md\n\n/research frontend react-state-management --complexity=medium\n# Creates: research/frontend/react-state-management/\n#          - overview.md\n#          - troubleshooting.md\n#          - comparison.md (Redux vs Context vs Zustand)\n```\n\n**Document Structure:**\n\n### overview.md\n- **Purpose & Use Cases**: When and why to use this technology\n- **Core Concepts**: Fundamental principles and architecture\n- **Implementation Patterns**: Common approaches and best practices\n- **Code Examples**: Practical, runnable examples\n- **Integration Guidelines**: How it fits into larger systems\n- **Performance Considerations**: Optimization and scaling\n- **Security**: Common vulnerabilities and protections\n- **Resources**: Official docs, tutorials, community resources\n\n### troubleshooting.md\n- **Common Issues**: Frequently encountered problems\n- **Error Messages**: Interpretation and solutions\n- **Diagnostic Tools**: How to investigate problems\n- **Solutions**: Step-by-step fixes\n- **Prevention**: How to avoid issues\n- **Escalation**: When and where to get help\n\n### Additional Files (generated as needed)\n- `best-practices.md`: Detailed guidelines and patterns\n- `comparison.md`: Technology alternatives and trade-offs\n- `migration.md`: Upgrade paths and migration strategies\n- `quick-start.md`: Fast setup and basic usage\n\n**Quality Standards:**\n- Use authoritative sources (official docs, RFCs, reputable blogs)\n- Include version information where relevant\n- Provide working code examples\n- Link to external resources\n- Note common pitfalls and gotchas\n- Include date of research for freshness tracking\n\n**Task Instructions:**\nUse Task tool with subagent_type: \"general-purpose\" to:\n\n1. **Research Phase**:\n   - Search authoritative sources and documentation\n   - Gather best practices and common patterns\n   - Identify common issues and solutions\n   - Collect practical examples\n\n2. **Structure Phase**:\n   - Create `research/<category>/<topic>/` directory\n   - Determine which documents are needed based on topic\n   - Plan document organization\n\n3. **Content Generation**:\n   - Write comprehensive `overview.md` with:\n     * Clear purpose and use cases\n     * Core concepts and architecture\n     * Implementation patterns\n     * Practical examples\n     * Integration guidance\n   - Write practical `troubleshooting.md` with:\n     * Common issues and solutions\n     * Error message interpretations\n     * Diagnostic approaches\n     * Prevention strategies\n   - Generate additional guides as needed\n\n4. **Quality Assurance**:\n   - Verify all sources are authoritative\n   - Ensure examples are practical and correct\n   - Check for completeness and clarity\n   - Add timestamps and version info\n\nThe agent should produce comprehensive, actionable documentation that helps users understand and work with the topic effectively."
              }
            ],
            "skills": [
              {
                "name": "claude-agents",
                "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                "path": "claude-code/skills/claude-agents/SKILL.md",
                "frontmatter": {
                  "name": "claude-agents",
                  "description": "Guide for creating custom agents for Claude Code with specialized behaviors and tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Agents\n\nGuide for creating custom agents that provide specialized behaviors and tool access for specific tasks.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating custom agent types for specific workflows\n- Defining agent behaviors and tool permissions\n- Configuring agent capabilities\n- Understanding agent vs skill differences\n- Implementing domain-specific agents\n\n## What Are Agents?\n\nAgents are specialized Claude instances with:\n- **Specific tool access**: Limited or specialized tool sets\n- **Defined behaviors**: Pre-configured instructions and constraints\n- **Task focus**: Optimized for particular workflows\n- **Autonomous operation**: Can execute multi-step tasks independently\n\n## Agents vs Skills\n\n| Feature | Agents | Skills |\n|---------|--------|--------|\n| **Activation** | Explicitly launched via Task tool | Auto-activated based on context |\n| **Tool Access** | Configurable, can be restricted | Inherit from parent context |\n| **State** | Independent, isolated | Share parent context |\n| **Use Case** | Complex multi-step tasks | Knowledge and guidelines |\n| **Persistence** | Single execution | Always available when loaded |\n\n## Agent File Structure\n\n### Location\n\nAgents are defined in markdown files located in:\n- Plugin: `<plugin-root>/agents/`\n- User-level: `.claude/agents/`\n\n### File Naming\n\n- Use kebab-case: `code-reviewer.md`\n- File name becomes the agent type\n- Be descriptive about the agent's purpose\n\n## Basic Agent Format\n\n```markdown\n---\nname: code-reviewer\ndescription: Specialized agent for conducting thorough code reviews\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Code Review Agent\n\nI am a specialized code review agent focused on:\n\n## Responsibilities\n\n- Analyzing code for correctness and style\n- Identifying security vulnerabilities\n- Checking test coverage\n- Ensuring documentation quality\n- Suggesting improvements\n\n## Review Process\n\nWhen reviewing code, I will:\n\n1. Read the changed files\n2. Check for common anti-patterns\n3. Verify error handling\n4. Assess test coverage\n5. Provide actionable feedback\n\n## Guidelines\n\n- Focus on significant issues\n- Provide specific examples\n- Suggest concrete improvements\n- Consider project context\n```\n\n## Agent Configuration\n\n### YAML Frontmatter\n\nRequired and optional fields:\n\n```markdown\n---\nname: agent-name                    # Required: kebab-case identifier\ndescription: Brief description      # Required: What this agent does\ntools:                             # Optional: Tool allowlist\n  - Read\n  - Write\n  - Bash\nmodel: sonnet                      # Optional: Model to use (sonnet, opus, haiku)\nmax_iterations: 10                 # Optional: Maximum task iterations\ntimeout: 300                       # Optional: Timeout in seconds\n---\n```\n\n### Tool Allowlist\n\nRestrict agent to specific tools:\n\n```markdown\n---\ntools:\n  - Read      # Can read files\n  - Grep      # Can search code\n  - Glob      # Can find files\n  # Cannot use Write, Edit, Bash, etc.\n---\n```\n\n**No tool restrictions** (access to all tools):\n\n```markdown\n---\n# Omit tools field entirely\n---\n```\n\n### Model Selection\n\nChoose appropriate model for the task:\n\n```markdown\n---\nmodel: haiku        # Fast, cost-effective for simple tasks\n# model: sonnet     # Balanced (default)\n# model: opus       # Most capable for complex tasks\n---\n```\n\n## Common Agent Patterns\n\n### Read-Only Analysis Agent\n\n```markdown\n---\nname: security-analyzer\ndescription: Analyzes code for security vulnerabilities\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Security Analysis Agent\n\nI perform security analysis on codebases.\n\n## Analysis Areas\n\n- SQL injection vulnerabilities\n- XSS attack vectors\n- Authentication/authorization issues\n- Sensitive data exposure\n- Insecure dependencies\n\n## Process\n\n1. Scan for common vulnerability patterns\n2. Check security best practices\n3. Identify potential risks\n4. Provide remediation guidance\n```\n\n### Test Generation Agent\n\n```markdown\n---\nname: test-generator\ndescription: Generates comprehensive test suites\ntools:\n  - Read\n  - Write\n  - Glob\nmodel: sonnet\n---\n\n# Test Generation Agent\n\nI create comprehensive test suites for your code.\n\n## Test Types\n\n- Unit tests\n- Integration tests\n- Edge case coverage\n- Error scenario tests\n\n## Approach\n\n1. Analyze source code structure\n2. Identify testable units\n3. Generate test cases\n4. Create test files with proper naming\n5. Include setup and teardown logic\n```\n\n### Documentation Agent\n\n```markdown\n---\nname: docs-generator\ndescription: Creates and updates project documentation\ntools:\n  - Read\n  - Write\n  - Glob\n  - Grep\nmodel: sonnet\n---\n\n# Documentation Agent\n\nI create and maintain project documentation.\n\n## Documentation Types\n\n- README files\n- API documentation\n- Code comments\n- Architecture docs\n- User guides\n\n## Standards\n\n- Clear, concise language\n- Practical examples\n- Up-to-date with codebase\n- Proper formatting (Markdown, JSDoc, etc.)\n```\n\n### Refactoring Agent\n\n```markdown\n---\nname: refactorer\ndescription: Safely refactors code while maintaining functionality\ntools:\n  - Read\n  - Write\n  - Edit\n  - Grep\n  - Glob\nmodel: sonnet\nmax_iterations: 20\n---\n\n# Code Refactoring Agent\n\nI refactor code to improve quality while preserving behavior.\n\n## Refactoring Goals\n\n- Improve readability\n- Reduce complexity\n- Eliminate duplication\n- Enhance maintainability\n- Follow best practices\n\n## Safety Measures\n\n- Preserve existing functionality\n- Maintain test coverage\n- Document changes\n- Use safe transformations\n```\n\n## Agent Plugin Configuration\n\n### In plugin.json\n\n```json\n{\n  \"agents\": [\n    \"./agents/code-reviewer.md\",\n    \"./agents/test-generator.md\",\n    \"./agents/security-analyzer.md\"\n  ]\n}\n```\n\n### Directory-Based Loading\n\n```json\n{\n  \"agents\": \"./agents\"\n}\n```\n\nLoads all `.md` files in `agents/` directory.\n\n## Invoking Agents\n\nAgents are launched via the Task tool:\n\n```python\n# In parent Claude conversation\nTask(\n    subagent_type=\"code-reviewer\",\n    description=\"Review authentication module\",\n    prompt=\"\"\"\n    Review the authentication module for:\n    - Security vulnerabilities\n    - Error handling\n    - Input validation\n    - Best practices\n    \"\"\"\n)\n```\n\n## Agent Communication\n\n### Input to Agent\n\n- Task description\n- Detailed prompt\n- Access to conversation history (if configured)\n\n### Output from Agent\n\n- Final report/result\n- No ongoing dialogue\n- One-time execution\n\n## Best Practices\n\n### Clear Purpose\n\nEach agent should have a specific, well-defined purpose:\n\n```markdown\n---\nname: migration-helper\ndescription: Assists with database schema migrations\n---\n\n# Database Migration Agent\n\nSpecialized in creating and validating database migrations.\n```\n\n### Appropriate Tool Access\n\nOnly grant necessary tools:\n\n```markdown\n---\n# Analysis agent - read-only\ntools:\n  - Read\n  - Grep\n  - Glob\n---\n```\n\n```markdown\n---\n# Implementation agent - can modify\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n---\n```\n\n### Model Selection\n\nMatch model to task complexity:\n\n- **haiku**: Simple, repetitive tasks\n- **sonnet**: Standard tasks (default)\n- **opus**: Complex reasoning required\n\n### Iteration Limits\n\nSet appropriate limits for task complexity:\n\n```markdown\n---\nmax_iterations: 5   # Simple, focused task\n# max_iterations: 20  # Complex, multi-step workflow\n---\n```\n\n### Clear Instructions\n\nProvide explicit behavior guidelines:\n\n```markdown\n# Testing Agent\n\n## Mandatory Requirements\n\n- Generate tests for ALL public methods\n- Achieve minimum 80% code coverage\n- Include edge cases and error scenarios\n- Use project's testing framework conventions\n\n## Constraints\n\n- Do not modify source code\n- Follow existing test file naming patterns\n- Use appropriate assertions\n```\n\n## Security Considerations\n\n### Tool Restrictions\n\nLimit dangerous operations:\n\n```markdown\n---\n# Don't give Bash access to untrusted agents\ntools:\n  - Read\n  - Write  # Safer than arbitrary shell commands\n---\n```\n\n### Input Validation\n\nValidate agent inputs:\n\n```markdown\n# Deployment Agent\n\nBefore deploying:\n1. Verify target environment is valid\n2. Check deployment permissions\n3. Validate configuration\n4. Confirm destructive operations\n```\n\n### Sensitive Data\n\nNever hardcode:\n- Credentials\n- API keys\n- Private URLs\n- Access tokens\n\n## Agent Examples\n\n### PR Review Agent\n\n```markdown\n---\nname: pr-reviewer\ndescription: Reviews pull requests for quality and completeness\ntools:\n  - Read\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n# Pull Request Review Agent\n\nConducting thorough PR review...\n\n## Checklist\n\n- [ ] Code quality and style\n- [ ] Test coverage\n- [ ] Documentation updates\n- [ ] Breaking changes noted\n- [ ] Security considerations\n- [ ] Performance implications\n\n## Review Process\n\n1. Analyze changed files\n2. Check for common issues\n3. Verify tests exist\n4. Review documentation\n5. Provide constructive feedback\n```\n\n### Migration Agent\n\n```markdown\n---\nname: code-migrator\ndescription: Migrates code from one framework/version to another\ntools:\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\nmodel: opus\nmax_iterations: 30\n---\n\n# Code Migration Agent\n\nPerforming framework migration...\n\n## Migration Steps\n\n1. Analyze current codebase\n2. Identify migration patterns\n3. Apply transformations\n4. Update dependencies\n5. Verify compatibility\n6. Document changes\n\n## Safety Checks\n\n- Backup original code\n- Incremental changes\n- Validate each step\n- Maintain git history\n```\n\n## Troubleshooting\n\n### Agent Not Found\n\n- Verify agent file location matches plugin.json\n- Check file naming (kebab-case, .md extension)\n- Ensure plugin is properly installed\n\n### Tool Access Denied\n\n- Check tools allowlist in frontmatter\n- Verify tool names match exactly\n- Ensure parent context permits delegation\n\n### Unexpected Behavior\n\n- Review agent instructions for clarity\n- Check model selection appropriateness\n- Verify iteration limits aren't too restrictive\n- Test with verbose output\n\n## References\n\nFor more information:\n- Claude Code Agents Documentation: https://code.claude.com/docs/en/agents\n- Task Tool Documentation: https://code.claude.com/docs/en/tools/task"
              },
              {
                "name": "claude-commands",
                "description": "Guide for creating custom slash commands for Claude Code",
                "path": "claude-code/skills/claude-commands/SKILL.md",
                "frontmatter": {
                  "name": "claude-commands",
                  "description": "Guide for creating custom slash commands for Claude Code",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Commands\n\nGuide for creating custom slash commands that extend Claude Code functionality.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new custom slash commands\n- Understanding command structure and syntax\n- Organizing commands for plugins\n- Implementing command workflows\n- Debugging command execution\n\n## What Are Commands?\n\nCommands are custom slash commands (like `/commit`, `/review`) that users can invoke to trigger specific workflows or expand prompts. They are markdown files that can contain:\n\n- Static prompt text\n- Dynamic content based on arguments\n- Multi-step workflows\n- Integration with tools and scripts\n\n## Command File Structure\n\n### Location\n\nCommands are defined in markdown files located in:\n- Plugin: `<plugin-root>/commands/`\n- User-level: `.claude/commands/`\n\n### File Naming\n\n- Use kebab-case: `my-command.md`\n- File name becomes the command name: `my-command.md` → `/my-command`\n- Avoid conflicts with built-in commands\n\n## Basic Command Format\n\n### Simple Static Command\n\n```markdown\n# /my-command\n\nThis is the prompt that will be expanded when the user types /my-command.\n\nThe entire content of this file will replace the slash command in the conversation.\n```\n\n### Command with Description\n\n```markdown\n<!--\ndescription: Brief description of what this command does\n-->\n\n# /my-command\n\nCommand prompt goes here...\n```\n\n## Command Arguments\n\nCommands can accept arguments that users provide when invoking the command.\n\n### Single Argument\n\n```markdown\n# /greet\n\nHello, {{arg}}! Welcome to the project.\n```\n\nUsage: `/greet Alice` → \"Hello, Alice! Welcome to the project.\"\n\n### Multiple Arguments\n\n```markdown\n# /create-file\n\nCreate a new file at {{arg1}} with the following content:\n\n{{arg2}}\n```\n\nUsage: `/create-file src/main.rs \"fn main() {}\"`\n\n### Named Arguments\n\n```markdown\n# /deploy\n\nDeploy {{environment}} environment to {{region}}.\n\nConfiguration:\n- Environment: {{environment}}\n- Region: {{region}}\n- Branch: {{branch}}\n```\n\nUsage: `/deploy --environment=production --region=us-east-1 --branch=main`\n\n## Advanced Features\n\n### Conditional Content\n\n```markdown\n# /analyze\n\nAnalyze the {{language}} codebase.\n\n{{#if verbose}}\nProvide detailed analysis including:\n- Code complexity metrics\n- Dependency analysis\n- Security vulnerabilities\n{{else}}\nProvide a summary analysis.\n{{/if}}\n```\n\n### Including Files\n\nReference other files or command outputs:\n\n```markdown\n# /context\n\nHere is the current project structure:\n\n{{file:PROJECT_STRUCTURE.md}}\n\nAnd the current git status:\n\n{{shell:git status}}\n```\n\n### Multi-Step Workflows\n\n```markdown\n# /full-review\n\nI'll perform a comprehensive code review:\n\n1. First, let me check the git diff:\n{{shell:git diff}}\n\n2. Now analyzing code quality...\n\n3. Checking for security issues...\n\n4. Final recommendations:\n```\n\n## Best Practices\n\n### Clear Command Names\n\n- Use descriptive, action-oriented names\n- `/analyze-security` not `/sec`\n- `/create-component` not `/comp`\n\n### Provide Context\n\nAlways include what the command will do:\n\n```markdown\n# /commit\n\nI'll analyze the current git changes and create a conventional commit message.\n\nCurrent changes:\n{{shell:git diff --staged}}\n\nBased on these changes, here's my suggested commit message:\n```\n\n### Handle Edge Cases\n\n```markdown\n# /deploy\n\n{{#if staging}}\nDeploying to staging environment (safe for testing)\n{{else if production}}\n⚠️ WARNING: Deploying to PRODUCTION\nAre you sure you want to continue? This will affect live users.\n{{else}}\nError: Unknown environment. Please specify --staging or --production\n{{/if}}\n```\n\n### Document Arguments\n\n```markdown\n<!--\ndescription: Deploy application to specified environment\nusage: /deploy [--environment=<env>] [--region=<region>]\narguments:\n  - environment: Target environment (staging, production)\n  - region: AWS region (us-east-1, eu-west-1, etc.)\n-->\n\n# /deploy\n```\n\n## Command Organization\n\n### Plugin Commands\n\nIn `plugin.json`:\n\n```json\n{\n  \"commands\": [\n    \"./commands/deploy.md\",\n    \"./commands/analyze.md\",\n    \"./commands/review.md\"\n  ]\n}\n```\n\n### Directory-Based Commands\n\n```json\n{\n  \"commands\": [\"./commands\"]\n}\n```\n\nThis loads all `.md` files in the `commands/` directory.\n\n### Namespaced Commands\n\nOrganize related commands in subdirectories:\n\n```\ncommands/\n├── git/\n│   ├── commit.md\n│   ├── review.md\n│   └── cleanup.md\n├── deploy/\n│   ├── staging.md\n│   └── production.md\n```\n\n## Common Command Patterns\n\n### Git Commit Message Generator\n\n```markdown\n# /gcm\n\nI'll analyze the staged changes and generate a conventional commit message.\n\n{{shell:git diff --staged}}\n\nBased on these changes, here's my commit message:\n```\n\n### Code Review Command\n\n```markdown\n# /review-pr\n\nI'll review the pull request changes.\n\nPR Number: {{pr_number}}\n\n{{shell:gh pr diff {{pr_number}}}}\n\nReview checklist:\n- [ ] Code quality and style\n- [ ] Security considerations\n- [ ] Test coverage\n- [ ] Documentation updates\n```\n\n### Project Scaffolding\n\n```markdown\n# /new-component\n\nCreating a new {{component_type}} component named {{name}}.\n\nI'll create:\n1. Component file at src/components/{{name}}.tsx\n2. Test file at src/components/{{name}}.test.tsx\n3. Storybook file at src/components/{{name}}.stories.tsx\n```\n\n## Testing Commands\n\n### Manual Testing\n\n1. Install the plugin locally\n2. Reload Claude Code\n3. Type your command in the chat\n4. Verify the expansion is correct\n\n### Debugging\n\nIf a command doesn't work:\n\n1. Check file location matches plugin.json\n2. Verify markdown syntax\n3. Test argument substitution\n4. Check for conflicts with existing commands\n\n## Command Templates\n\n### Analysis Command Template\n\n```markdown\n<!--\ndescription: Analyze {{target}} for {{criteria}}\n-->\n\n# /analyze-{{target}}\n\nI'll analyze the {{target}} codebase for {{criteria}}.\n\n{{shell:find {{target}} -type f -name \"*.{{extension}}\"}}\n\nAnalysis results:\n```\n\n### Workflow Command Template\n\n```markdown\n<!--\ndescription: Execute {{workflow}} workflow\n-->\n\n# /{{workflow}}\n\nStarting {{workflow}} workflow...\n\nStep 1: {{step1_description}}\n{{step1_action}}\n\nStep 2: {{step2_description}}\n{{step2_action}}\n\nWorkflow complete!\n```\n\n## Integration with Skills\n\nCommands can reference skills:\n\n```markdown\n# /elixir-review\n\nI'll review this Elixir code using my Phoenix and OTP knowledge.\n\nPlease provide the code to review, and I'll check for:\n- Phoenix best practices\n- OTP design patterns\n- Elixir anti-patterns\n- Performance considerations\n```\n\n## Security Considerations\n\n### Avoid Sensitive Data\n\nNever hardcode:\n- API keys\n- Passwords\n- Tokens\n- Private URLs\n\n### Validate Input\n\n```markdown\n# /deploy\n\n{{#unless environment}}\nError: --environment is required\n{{/unless}}\n\n{{#if (validate_environment environment)}}\nProceeding with deployment...\n{{else}}\nError: Invalid environment. Must be staging or production.\n{{/if}}\n```\n\n### Safe Shell Commands\n\nBe cautious with shell command execution:\n\n```markdown\n# /safe-deploy\n\n<!-- Only allow whitelisted commands -->\n{{shell:./scripts/deploy.sh {{environment}}}}\n```\n\n## References\n\nFor more information about Claude Code commands:\n- Claude Code Documentation: https://code.claude.com/docs/en/commands\n- Example Commands: https://github.com/anthropics/claude-code/tree/main/examples/commands"
              },
              {
                "name": "claude-hooks",
                "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                "path": "claude-code/skills/claude-hooks/SKILL.md",
                "frontmatter": {
                  "name": "claude-hooks",
                  "description": "Guide for creating hooks that trigger actions in response to Claude Code events",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Hooks\n\nGuide for creating hooks that execute shell commands or scripts in response to Claude Code events and tool calls.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating event-driven automations\n- Implementing custom validation or formatting\n- Integrating with external tools and services\n- Setting up project-specific workflows\n- Responding to tool execution events\n\n## What Are Hooks?\n\nHooks are shell commands that execute automatically in response to specific events:\n\n- **Tool Call Hooks**: Trigger before/after specific tool calls\n- **Lifecycle Hooks**: Trigger on plugin install/uninstall\n- **User Prompt Hooks**: Trigger when users submit prompts\n- **Custom Events**: Application-specific trigger points\n\n## Hook Configuration\n\n### Location\n\nHooks are configured in:\n- Plugin: `<plugin-root>/.claude-plugin/hooks.json`\n- User-level: `.claude/hooks.json`\n- Plugin manifest: Inline in `plugin.json`\n\n### File Structure\n\n**Standalone hooks.json:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"./hooks/format-check.sh\"],\n      \"after\": [\"./hooks/lint.sh\"]\n    },\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh\"]\n    }\n  },\n  \"onInstall\": [\"./hooks/setup.sh\"],\n  \"onUninstall\": [\"./hooks/cleanup.sh\"],\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh\"]\n}\n```\n\n**Inline in plugin.json:**\n```json\n{\n  \"hooks\": {\n    \"onToolCall\": {\n      \"Write\": {\n        \"after\": [\"prettier --write {{file_path}}\"]\n      }\n    }\n  }\n}\n```\n\n## Hook Types\n\n### Tool Call Hooks\n\nExecute before or after specific tool calls.\n\n**Available Tools:**\n- `Read`, `Write`, `Edit`, `MultiEdit`\n- `Bash`, `BashOutput`\n- `Glob`, `Grep`\n- `Task`, `Skill`, `SlashCommand`\n- `TodoWrite`\n- `WebFetch`, `WebSearch`\n- `AskUserQuestion`\n\n**Example:**\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\n        \"echo 'Writing file: {{file_path}}'\",\n        \"./hooks/backup.sh {{file_path}}\"\n      ],\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"git add {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\"eslint --fix {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Lifecycle Hooks\n\nExecute during plugin installation/uninstallation.\n\n```json\n{\n  \"onInstall\": [\n    \"./hooks/setup-dependencies.sh\",\n    \"npm install\",\n    \"echo 'Plugin installed successfully'\"\n  ],\n  \"onUninstall\": [\n    \"./hooks/cleanup.sh\",\n    \"echo 'Plugin uninstalled'\"\n  ]\n}\n```\n\n### User Prompt Submit Hook\n\nExecute when user submits a prompt:\n\n```json\n{\n  \"onUserPromptSubmit\": [\n    \"./hooks/log-interaction.sh '{{prompt}}'\",\n    \"./hooks/check-context.sh\"\n  ]\n}\n```\n\n## Hook Variables\n\nHooks have access to context-specific variables using `{{variable}}` syntax.\n\n### Tool Call Variables\n\nDifferent tools provide different variables:\n\n**Write Tool:**\n- `{{file_path}}`: Path to file being written\n- `{{content}}`: Content being written (before hooks only)\n\n**Edit Tool:**\n- `{{file_path}}`: Path to file being edited\n- `{{old_string}}`: String being replaced\n- `{{new_string}}`: Replacement string\n\n**Bash Tool:**\n- `{{command}}`: Command being executed\n\n**Read Tool:**\n- `{{file_path}}`: Path to file being read\n\n### Global Variables\n\nAvailable in all hooks:\n- `{{cwd}}`: Current working directory\n- `{{timestamp}}`: Current Unix timestamp\n- `{{user}}`: Current user\n- `{{plugin_root}}`: Plugin installation directory\n\n### User Prompt Variables\n\n- `{{prompt}}`: User's submitted prompt text\n\n## Hook Examples\n\n### Auto-Format on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Pre-Commit Validation\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-git-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-git-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block force push to main/master\nif [[ \"$COMMAND\" =~ \"git push --force\" ]] && [[ \"$COMMAND\" =~ \"main|master\" ]]; then\n  echo \"ERROR: Force push to main/master is not allowed\"\n  exit 1\nfi\n\nexit 0\n```\n\n### Automatic Backups\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    },\n    \"Edit\": {\n      \"before\": [\"cp {{file_path}} {{file_path}}.backup\"]\n    }\n  }\n}\n```\n\n### Logging and Analytics\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/log-file-change.sh {{file_path}}\"]\n    }\n  },\n  \"onUserPromptSubmit\": [\"./hooks/log-prompt.sh '{{prompt}}'\"]\n}\n```\n\n**log-file-change.sh:**\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Modified: $FILE\" >> .claude/file-changes.log\n```\n\n### Integration with External Tools\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"notify-send 'File Updated' 'Modified {{file_path}}'\",\n        \"curl -X POST https://api.example.com/notify -d 'file={{file_path}}'\"\n      ]\n    }\n  }\n}\n```\n\n## Hook Execution\n\n### Execution Order\n\nMultiple hooks execute in array order:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"echo 'Step 1'\",  // Runs first\n        \"echo 'Step 2'\",  // Runs second\n        \"echo 'Step 3'\"   // Runs third\n      ]\n    }\n  }\n}\n```\n\n### Exit Codes\n\n**Before Hooks:**\n- Exit code `0`: Continue with tool execution\n- Exit code non-zero: **Block tool execution**, show error to user\n\n**After Hooks:**\n- Exit codes are logged but don't affect tool execution\n- Tool has already completed\n\n### Error Handling\n\n```bash\n#!/bin/bash\n\n# Before hook - blocks tool on error\nif [[ ! -f \"$1\" ]]; then\n  echo \"ERROR: File does not exist\"\n  exit 1  # Blocks tool execution\nfi\n\n# Validation passed\nexit 0\n```\n\n## Best Practices\n\n### Keep Hooks Fast\n\nHooks block execution - keep them lightweight:\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      // ✅ Fast linter\n      \"after\": [\"eslint --fix {{file_path}}\"]\n\n      // ❌ Slow test suite\n      // \"after\": [\"npm test\"]\n    }\n  }\n}\n```\n\n### Use Absolute Paths\n\nReference scripts with paths relative to plugin:\n\n```json\n{\n  \"onInstall\": [\"${CLAUDE_PLUGIN_ROOT}/hooks/setup.sh\"]\n}\n```\n\n### Validate Input\n\nAlways validate hook variables:\n\n```bash\n#!/bin/bash\n\nFILE=\"$1\"\n\nif [[ -z \"$FILE\" ]]; then\n  echo \"ERROR: No file path provided\"\n  exit 1\nfi\n\nif [[ ! -f \"$FILE\" ]]; then\n  echo \"ERROR: File does not exist: $FILE\"\n  exit 1\nfi\n```\n\n### Provide Clear Feedback\n\n```bash\n#!/bin/bash\n\necho \"Running pre-commit checks...\"\n\nif ! npm run lint; then\n  echo \"❌ Linting failed. Please fix errors before committing.\"\n  exit 1\nfi\n\necho \"✅ All checks passed\"\nexit 0\n```\n\n### Handle Edge Cases\n\n```bash\n#!/bin/bash\n\n# Handle files with spaces in names\nFILE=\"$1\"\n\n# Validate file type\nif [[ ! \"$FILE\" =~ \\.(js|ts|jsx|tsx)$ ]]; then\n  # Skip non-JavaScript files silently\n  exit 0\nfi\n\n# Run formatter\nprettier --write \"$FILE\"\n```\n\n## Security Considerations\n\n### Validate Commands\n\nBefore hooks can block dangerous operations:\n\n```json\n{\n  \"onToolCall\": {\n    \"Bash\": {\n      \"before\": [\"./hooks/validate-command.sh '{{command}}'\"]\n    }\n  }\n}\n```\n\n**validate-command.sh:**\n```bash\n#!/bin/bash\n\nCOMMAND=\"$1\"\n\n# Block dangerous patterns\nDANGEROUS_PATTERNS=(\n  \"rm -rf /\"\n  \"dd if=\"\n  \"mkfs\"\n  \"> /dev/sda\"\n)\n\nfor pattern in \"${DANGEROUS_PATTERNS[@]}\"; do\n  if [[ \"$COMMAND\" =~ $pattern ]]; then\n    echo \"ERROR: Dangerous command blocked: $pattern\"\n    exit 1\n  fi\ndone\n\nexit 0\n```\n\n### Limit Hook Scope\n\nOnly hook necessary tools:\n\n```json\n{\n  // ✅ Specific tools only\n  \"onToolCall\": {\n    \"Write\": { \"after\": [\"./format.sh {{file_path}}\"] }\n  }\n\n  // ❌ Don't hook everything unnecessarily\n}\n```\n\n### Sanitize Variables\n\n```bash\n#!/bin/bash\n\n# Sanitize file path\nFILE=$(realpath \"$1\")\n\n# Ensure file is within project\nif [[ ! \"$FILE\" =~ ^$(pwd) ]]; then\n  echo \"ERROR: File outside project directory\"\n  exit 1\nfi\n```\n\n## Debugging Hooks\n\n### Enable Verbose Output\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"before\": [\"set -x; ./hooks/debug.sh {{file_path}}; set +x\"]\n    }\n  }\n}\n```\n\n### Log Hook Execution\n\n```bash\n#!/bin/bash\n\nLOG_FILE=\".claude/hooks.log\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP - Hook: $0, Args: $@\" >> \"$LOG_FILE\"\n\n# Rest of hook logic...\n```\n\n### Test Hooks Manually\n\n```bash\n# Test hook with sample data\n./hooks/format.sh \"src/main.js\"\n\n# Check exit code\necho $?\n```\n\n## Common Hook Patterns\n\n### Auto-Format Pipeline\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    },\n    \"Edit\": {\n      \"after\": [\n        \"prettier --write {{file_path}}\",\n        \"eslint --fix {{file_path}}\"\n      ]\n    }\n  }\n}\n```\n\n### Test on Write\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"./hooks/run-relevant-tests.sh {{file_path}}\"]\n    }\n  }\n}\n```\n\n### Git Integration\n\n```json\n{\n  \"onToolCall\": {\n    \"Write\": {\n      \"after\": [\"git add {{file_path}}\"]\n    },\n    \"Edit\": {\n      \"after\": [\"git add {{file_path}}\"]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n### Hook Not Executing\n\n- Check hook file has execute permissions: `chmod +x hooks/script.sh`\n- Verify path is correct relative to plugin root\n- Check JSON syntax in hooks.json\n- Look for errors in Claude Code logs\n\n### Hook Blocking Tool\n\n- Check exit code of before hooks\n- Add debug logging\n- Test hook script manually\n- Verify validation logic\n\n### Variables Not Substituting\n\n- Check variable name spelling: `{{file_path}}` not `{{filepath}}`\n- Verify variable is available for that tool\n- Quote variables in bash: `\"{{file_path}}\"`\n\n## References\n\nFor more information:\n- Claude Code Hooks Documentation: https://code.claude.com/docs/en/hooks\n- Plugin Configuration: https://code.claude.com/docs/en/plugins#hooks"
              },
              {
                "name": "claude-plugins",
                "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                "path": "claude-code/skills/claude-plugins/SKILL.md",
                "frontmatter": {
                  "name": "claude-plugins",
                  "description": "Guide for creating and validating Claude Code plugin.json files with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin\n\nGuide for creating, validating, and managing plugin.json files for Claude Code plugins. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/plugin.json` files\n- Validating plugin.json schema compliance\n- Setting up new plugin directories\n- Troubleshooting plugin configuration issues\n- Understanding plugin manifest structure\n\n## Plugin Manifest Schema\n\n### File Location\n\nAll plugin manifests must be located at `.claude-plugin/plugin.json` within the plugin directory.\n\n### Complete Schema\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Brief plugin description\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"author@example.com\",\n    \"url\": \"https://github.com/author\"\n  },\n  \"homepage\": \"https://docs.example.com/plugin\",\n  \"repository\": \"https://github.com/author/plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"commands\": [\"./custom/commands/special.md\"],\n  \"agents\": \"./custom/agents/\",\n  \"hooks\": \"./config/hooks.json\",\n  \"mcpServers\": \"./mcp-config.json\",\n  \"skills\": [\"./skills/skill-one\", \"./skills/skill-two\"]\n}\n```\n\n### Required Fields\n\n- `name`: Plugin identifier (kebab-case, lowercase alphanumeric and hyphens only)\n\n### Optional Fields\n\n**Metadata:**\n- `version`: Semantic version number (recommended)\n- `description`: Brief explanation of plugin functionality\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of searchability and categorization tags\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n\n**Author Information:**\n- `author.name`: Creator name\n- `author.email`: Contact email\n- `author.url`: Personal or organization website\n\n**Component Paths:**\n- `skills`: Array of skill directory paths (relative to plugin root)\n- `commands`: String path or array of command file/directory paths\n- `agents`: String path or array of agent file paths\n- `hooks`: String path to hooks.json or hooks configuration object\n- `mcpServers`: String path to MCP config or configuration object\n\n## Field Validation Rules\n\n### name\n- **Format**: kebab-case (lowercase alphanumeric and hyphens only)\n- **Pattern**: `^[a-z0-9]+(-[a-z0-9]+)*$`\n- **Examples**:\n  - Valid: `my-plugin`, `core-skills`, `elixir-tools`\n  - Invalid: `myPlugin`, `my_plugin`, `My-Plugin`, `plugin-`\n\n### version\n- **Format**: Semantic versioning\n- **Pattern**: `^[0-9]+\\.[0-9]+\\.[0-9]+(-[a-zA-Z0-9.-]+)?(\\+[a-zA-Z0-9.-]+)?$`\n- **Examples**:\n  - Valid: `1.0.0`, `2.1.3`, `1.0.0-beta.1`, `1.0.0+build.123`\n  - Invalid: `1.0`, `v1.0.0`, `1.0.0.0`\n\n### license\n- **Format**: SPDX license identifier\n- **Common values**: `MIT`, `Apache-2.0`, `GPL-3.0`, `BSD-3-Clause`, `ISC`\n- **Reference**: https://spdx.org/licenses/\n\n### keywords\n- **Format**: Array of strings\n- **Purpose**: Discoverability, searchability, categorization\n- **Recommendations**: Use lowercase, be specific, include domain terms\n\n### Paths (skills, commands, agents, hooks, mcpServers)\n- **Format**: Relative paths from plugin root\n- **Recommendations**: Use `./` prefix for clarity\n- **Skills**: Array of directory paths containing SKILL.md files\n- **Commands**: Can be string (single path) or array of paths\n- **Agents**: Can be string (directory) or array of file paths\n\n## Invalid Fields in plugin.json\n\nThe following fields are **only valid in marketplace.json** entries and must NOT appear in plugin.json:\n\n- `dependencies`: Dependencies belong in marketplace entries, not plugin manifests\n- `category`: Categorization is marketplace-level metadata\n- `strict`: Controls marketplace behavior, not plugin definition\n- `source`: Plugin location is defined in marketplace, not in plugin itself\n- `tags`: Use `keywords` instead\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\nThis validates:\n- JSON syntax\n- Required field presence (name)\n- Kebab-case naming\n- Field type correctness\n- Path accessibility (for relative paths)\n- Invalid field detection\n\n### 2. Path Validation\n\nValidate that referenced paths exist:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-paths.nu .claude-plugin/plugin.json\n```\n\nChecks:\n- Skills directories exist and contain SKILL.md\n- Command files/directories exist\n- Agent files/directories exist\n- Hooks configuration exists\n- MCP server configuration exists\n\n### 3. Initialization Helper\n\nGenerate a template plugin.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nCreates `.claude-plugin/plugin.json` with proper structure.\n\n## Best Practices\n\n### Naming Conventions\n\n- **Plugin name**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Avoid generic names**: Be specific about the plugin's purpose\n- **Match directory name**: Plugin name should match its directory name\n\n### Versioning Strategy\n\n- Use semantic versioning (MAJOR.MINOR.PATCH)\n- Increment MAJOR for breaking changes\n- Increment MINOR for new features (backward compatible)\n- Increment PATCH for bug fixes\n- Use pre-release tags for beta versions (`1.0.0-beta.1`)\n\n### Path Organization\n\n**Recommended structure:**\n```\nplugin-name/\n├── .claude-plugin/\n│   └── plugin.json\n├── skills/\n│   ├── skill-one/\n│   └── skill-two/\n├── commands/\n└── agents/\n```\n\n**In plugin.json:**\n```json\n{\n  \"skills\": [\n    \"./skills/skill-one\",\n    \"./skills/skill-two\"\n  ],\n  \"commands\": [\"./commands\"],\n  \"agents\": [\"./agents\"]\n}\n```\n\n### Metadata Completeness\n\nAlways include:\n- `version`: Track plugin evolution\n- `description`: Help users understand purpose\n- `license`: Clarify usage terms\n- `keywords`: Improve discoverability\n- `repository`: Enable contributions\n\n### Author Information\n\nInclude contact information for:\n- Bug reports\n- Feature requests\n- Contributions\n- Questions\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Invalid field for plugin.json\n\n```json\n// ❌ Invalid (dependencies only in marketplace.json)\n{\n  \"name\": \"my-plugin\",\n  \"dependencies\": [\"other-plugin\"]\n}\n\n// ✅ Valid\n{\n  \"name\": \"my-plugin\",\n  \"keywords\": [\"tool\", \"utility\"]\n}\n```\n\n### Error: Skill path doesn't exist\n\n```json\n// ❌ Invalid (path not found)\n\"skills\": [\"./skills/nonexistent\"]\n\n// ✅ Valid (path exists with SKILL.md)\n\"skills\": [\"./skills/my-skill\"]\n```\n\n### Error: Invalid version format\n\n```json\n// ❌ Invalid\n\"version\": \"1.0\"\n\"version\": \"v1.0.0\"\n\n// ✅ Valid\n\"version\": \"1.0.0\"\n\"version\": \"2.1.3-beta.1\"\n```\n\n## Creating a New Plugin\n\n### Step 1: Initialize Directory Structure\n\n```bash\nmkdir -p my-plugin/.claude-plugin\nmkdir -p my-plugin/skills\n```\n\n### Step 2: Create plugin.json\n\nUse the initialization script:\n\n```bash\ncd my-plugin\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-plugin.nu\n```\n\nOr create manually:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"0.1.0\",\n  \"description\": \"My plugin description\",\n  \"author\": {\n    \"name\": \"Your Name\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"],\n  \"skills\": []\n}\n```\n\n### Step 3: Add Skills\n\n1. Create skill directory: `mkdir -p skills/my-skill`\n2. Create SKILL.md in skill directory\n3. Add to plugin.json:\n\n```json\n{\n  \"skills\": [\"./skills/my-skill\"]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json\n```\n\n### Step 5: Test\n\nInstall locally to test:\n\n```bash\nclaude-code install ./\n```\n\n## Hooks Configuration\n\nHooks can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"hooks\": {\n    \"onInstall\": \"./scripts/install.sh\",\n    \"onUninstall\": \"./scripts/uninstall.sh\"\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"hooks\": \"./config/hooks.json\"\n}\n```\n\n## MCP Servers Configuration\n\nMCP servers can be inline or referenced:\n\n**Inline:**\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"mcp-server-filesystem\",\n      \"args\": [\"./workspace\"]\n    }\n  }\n}\n```\n\n**Referenced:**\n```json\n{\n  \"mcpServers\": \"./mcp-config.json\"\n}\n```\n\n## Troubleshooting\n\n### Plugin Not Loading\n\n- Verify plugin.json exists at `.claude-plugin/plugin.json`\n- Check JSON syntax is valid\n- Ensure name field is present and kebab-case\n- Validate all path references exist\n\n### Skills Not Found\n\n- Check skill paths in plugin.json match actual directories\n- Ensure each skill directory contains SKILL.md file\n- Verify paths use relative format (`./skills/name`)\n\n### Commands Not Appearing\n\n- Verify command paths exist\n- Check commands are .md files or directories containing .md files\n- Ensure paths are relative to plugin root\n\n### Validation Fails\n\nRun validation with verbose output:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.nu .claude-plugin/plugin.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/plugin-schema.md`: Complete JSON schema specification\n- `references/plugin-examples.md`: Real-world plugin.json examples\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-plugin.nu`: Complete plugin.json validation\n- `validate-plugin-paths.nu`: Verify all referenced paths exist\n- `init-plugin.nu`: Generate plugin.json template\n- `format-plugin.nu`: Format and sort plugin.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              },
              {
                "name": "claude-skills",
                "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                "path": "claude-code/skills/claude-skills/SKILL.md",
                "frontmatter": {
                  "name": "claude-skills",
                  "description": "Guide for creating Agent Skills with progressive disclosure, SKILL.md structure, and best practices",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Agent Skills\n\nComprehensive guide for creating modular, self-contained Agent Skills that extend Claude's capabilities with specialized knowledge.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating new Agent Skills\n- Understanding skill structure and organization\n- Implementing progressive disclosure\n- Organizing skill resources (scripts, references, assets)\n- Following Agent Skills best practices\n\n## What Are Agent Skills?\n\nAgent Skills are organized directories containing instructions, scripts, and resources that Claude can dynamically discover and load. They enable a single general-purpose agent to gain domain-specific expertise without requiring separate custom agents for each use case.\n\n### Key Concepts\n\n- **Modularity**: Self-contained packages that can be mixed and matched\n- **Reusability**: Share and distribute expertise across projects and teams\n- **Progressive Disclosure**: Load context only when needed, keeping interactions efficient\n- **Specialization**: Deep domain knowledge without sacrificing generality\n\n## How Skills Work\n\nSkills operate on a principle of progressive disclosure across multiple levels:\n\n### Level 1: Discovery\nAgent system prompts include only skill names and descriptions, allowing Claude to decide when each skill is relevant based on the task at hand.\n\n### Level 2: Activation\nWhen Claude determines a skill applies, it loads the full `SKILL.md` file into context, gaining access to the complete procedural knowledge and guidelines.\n\n### Level 3+: Deep Context\nAdditional bundled files (like references, forms, or documentation) load only when needed for specific scenarios, keeping token usage efficient.\n\nThis tiered approach maintains efficient context windows while supporting potentially unbounded skill complexity.\n\n## Skill Structure\n\n### Minimal Requirements\n\nEvery skill must have:\n\n```\nskill-name/\n└── SKILL.md\n```\n\n### Complete Structure\n\nMore complex skills can include additional resources:\n\n```\nskill-name/\n├── SKILL.md           # Required: Core skill definition\n├── scripts/           # Optional: Executable code for deterministic tasks\n├── references/        # Optional: Documentation loaded on-demand\n└── assets/            # Optional: Templates, images, boilerplate\n```\n\n## SKILL.md Format\n\nEach `SKILL.md` file must begin with YAML frontmatter followed by Markdown content:\n\n```markdown\n---\nname: skill-name\ndescription: Concise explanation of when Claude should use this skill\nlicense: MIT\n---\n\n# Skill Name\n\nMain instructional content goes here...\n```\n\n### Required YAML Properties\n\n- `name`: Hyphen-case identifier matching directory name (lowercase alphanumeric and hyphens only)\n- `description`: Explains the skill's purpose and when Claude should utilize it\n\n### Optional YAML Properties\n\n- `license`: License name or filename reference\n- `allowed-tools`: Pre-approved tools list (Claude Code support only)\n- `metadata`: Key-value string pairs for client-specific properties\n\n### Markdown Body\n\nThe content section has no restrictions and should contain:\n\n- When to activate the skill\n- Core procedural knowledge\n- Best practices and guidelines\n- Examples and patterns\n- References to additional resources (if any)\n\n## Creating Skills: Seven-Step Workflow\n\n### 1. Understanding Through Examples\n\nGather concrete use cases to clarify what the skill should support. Real-world examples reveal actual needs better than theoretical requirements.\n\n**Example:**\n```\nUse Case: Help developers follow Git best practices\nExamples:\n- Creating conventional commit messages\n- Rebasing feature branches\n- Resolving merge conflicts\n- Creating descriptive branch names\n```\n\n### 2. Planning Resources\n\nAnalyze examples to identify needed components:\n\n- **Scripts**: For tasks requiring deterministic reliability or that would need repeated rewriting\n- **References**: Documentation to load into context as needed\n- **Assets**: Output files like templates or boilerplate (not loaded into context)\n\n**Example:**\n```\nGit skill resources:\n- scripts/analyze-commit.sh - Parse git diff for commit message\n- references/conventional-commits.md - Detailed commit format spec\n- assets/gitignore-templates/ - Common .gitignore files\n```\n\n### 3. Initialization\n\nCreate the skill directory structure with the required `SKILL.md` file. Ensure the directory name matches the `name` property exactly.\n\n```bash\nmkdir -p my-skill/{scripts,references,assets}\ntouch my-skill/SKILL.md\n```\n\n### 4. Editing\n\nDevelop resource files and update `SKILL.md` with:\n- Purpose and activation criteria\n- Usage guidelines and best practices\n- Implementation details and examples\n- References to supplementary files\n\n**Use imperative/infinitive form** rather than second-person instruction for clarity.\n\n✅ Good: \"Follow the Conventional Commits specification\"\n✅ Good: \"Use descriptive branch names with type prefixes\"\n❌ Avoid: \"You should try to use descriptive names when possible\"\n\nKeep core procedural information in `SKILL.md` and detailed reference material in separate files.\n\n### 5. Documentation\n\n**Document all sources in the plugin's `sources.md`**. For each skill created, record:\n- URLs of documentation, guides, and references used\n- Purpose of each source\n- Key topics and concepts extracted\n- Date accessed (if relevant)\n\nThis maintains traceability and helps others understand the skill's foundation.\n\n### 6. Validation\n\nTest the skill with representative scenarios to ensure:\n- Claude activates it appropriately\n- Instructions are clear and actionable\n- Progressive disclosure works effectively\n- Token usage remains efficient\n\n### 7. Iteration\n\nRefine based on real-world usage feedback. Monitor how Claude actually uses the skill and adjust the description and content accordingly.\n\n## Best Practices\n\n### Start with Evaluation\n\nIdentify specific capability gaps by testing agents on representative tasks. Build skills incrementally to address actual shortcomings rather than anticipated needs.\n\n### Structure for Scale\n\nSplit unwieldy `SKILL.md` files into separate referenced documents:\n- Keep commonly-used contexts together\n- Separate mutually exclusive information to reduce token usage\n- Use progressive disclosure to load details only when needed\n\n**Example:**\n```markdown\n# Git Skill\n\nFor detailed conventional commit format, see references/conventional-commits.md\nFor rebase workflow, see references/rebasing-guide.md\n```\n\n### Consider Claude's Perspective\n\nThe skill name and description heavily influence when Claude activates it. Pay particular attention to:\n\n- **Name**: Should be clear and reflect the domain (e.g., `git-operations`, `elixir-phoenix`)\n- **Description**: Should specify both what the skill does and when to use it\n\n**Examples:**\n\n✅ Good Description:\n```yaml\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\n```\n\n❌ Too Vague:\n```yaml\ndescription: Helps with Git\n```\n\nMonitor real usage patterns and iterate based on actual behavior.\n\n### Iterate Collaboratively\n\nWork with Claude to capture successful approaches and common mistakes into reusable skill components. Ask Claude to self-reflect on what contextual information actually matters.\n\n### Write for AI Consumption\n\nUse clear, imperative language that Claude can follow:\n\n✅ Good:\n- \"Follow the Conventional Commits specification\"\n- \"Use descriptive branch names with type prefixes\"\n- \"Run tests before committing\"\n\n❌ Avoid:\n- \"You should try to use descriptive names when possible\"\n- \"It might be good to run tests\"\n- \"Consider following best practices\"\n\nInclude concrete examples wherever possible to illustrate patterns and approaches.\n\n### Security Considerations\n\nInstall skills only from trusted sources. When evaluating unfamiliar skills:\n- Thoroughly audit bundled files and scripts\n- Review code dependencies\n- Examine instructions directing Claude to connect with external services\n- Verify the skill doesn't request sensitive information or dangerous operations\n\n## Anti-Fabrication Requirements\n\nAll skills MUST adhere to strict anti-fabrication requirements to ensure factual, measurable content.\n\n### Core Principles\n\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Execute tests before marking tasks complete and report actual results\n- Validate integration recommendations through actual framework detection using tool analysis\n\n### Prohibited Language and Claims\n\n- **Superlatives**: Avoid \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- **Unsubstantiated Metrics**: Never fabricate percentages, success rates, or performance numbers\n- **Assumed Capabilities**: Don't claim features exist without tool verification\n- **Generic Claims**: Replace vague statements with specific, measurable observations\n- **Fabricated Testing**: Never report test results without actual execution\n\n### Time and Effort Estimation Rule\n\n- Never provide time estimates, effort estimates, or completion timelines without actual measurement or analysis\n- If estimates are requested, execute tools to analyze scope (e.g., count files, measure complexity, assess dependencies) before providing data-backed estimates\n- When estimates cannot be measured, explicitly state \"timeline requires analysis of [specific factors]\"\n- Avoid fabricated scheduling language like \"15 minutes\", \"2 hours\", \"quick task\" without factual basis\n\n### Validation Requirements\n\n- **File Claims**: Use Read or Glob tools before claiming files exist or contain specific content\n- **System Integration**: Use Bash or appropriate tools to verify system capabilities\n- **Framework Detection**: Execute actual detection logic before claiming framework presence\n- **Test Results**: Only report test outcomes after actual execution with tool verification\n- **Performance Claims**: Base any performance statements on actual measurement or analysis\n\n## Skill Examples\n\n### Simple Skill (Git)\n\n```markdown\n---\nname: git\ndescription: Guide for Git operations including commits, branches, rebasing, and conflict resolution\nlicense: MIT\n---\n\n# Git Operations\n\n## When to Use\n\nActivate when:\n- Creating commit messages\n- Managing branches\n- Resolving conflicts\n- Rebasing or merging\n\n## Conventional Commits\n\nFollow the format: `type(scope): description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n\nExample: `feat(auth): add OAuth2 login support`\n\n## Branch Naming\n\nUse format: `type/description`\n\nExamples:\n- `feature/user-authentication`\n- `fix/memory-leak`\n- `docs/api-reference`\n\n## Rebasing Workflow\n\n1. Update main: `git checkout main && git pull`\n2. Rebase feature: `git checkout feature-branch && git rebase main`\n3. Resolve conflicts if needed\n4. Force push: `git push --force-with-lease`\n```\n\n### Complex Skill (Phoenix)\n\n```markdown\n---\nname: phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and best practices\nlicense: MIT\n---\n\n# Phoenix Framework\n\n## When to Use\n\nActivate for:\n- Phoenix application development\n- LiveView implementations\n- Context design\n- Channel setup\n\n## Project Structure\n\nPhoenix apps follow:\n```\nlib/\n├── my_app/          # Business logic (contexts)\n├── my_app_web/      # Web interface\n└── my_app.ex\n```\n\n## Contexts\n\nGroup related functionality:\n\n```elixir\ndefmodule MyApp.Accounts do\n  def list_users, do: Repo.all(User)\n  def get_user!(id), do: Repo.get!(User, id)\n  def create_user(attrs), do: ...\nend\n```\n\nFor detailed context patterns, see references/contexts.md\n\n## LiveView\n\nFor real-time interfaces, see references/liveview-guide.md\n```\n\n## Common Pitfalls\n\n### Too Generic\n\n❌ Avoid:\n```yaml\nname: programming\ndescription: Helps with programming\n```\n\n✅ Better:\n```yaml\nname: elixir-phoenix\ndescription: Guide for building Phoenix web applications with LiveView, contexts, and Elixir best practices\n```\n\n### Too Much in SKILL.md\n\n❌ Avoid putting entire API reference in SKILL.md\n\n✅ Better: Keep core patterns in SKILL.md, detailed reference in `references/`\n\n### Missing Activation Criteria\n\n❌ Avoid:\n```markdown\n# My Skill\n\nThis skill helps with stuff.\n```\n\n✅ Better:\n```markdown\n# My Skill\n\n## When to Use\n\nActivate when:\n- Specific scenario 1\n- Specific scenario 2\n- Specific scenario 3\n```\n\n## References\n\nFor more information:\n- **Agent Skills Blog**: https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\n- **Example Skills**: https://github.com/anthropics/skills\n- **Skills Cookbook**: https://github.com/anthropics/claude-cookbooks/tree/main/skills\n- **Skill Creator Guide**: https://github.com/anthropics/skills/blob/main/skill-creator/SKILL.md\n- **Agent Skills Specification**: https://github.com/anthropics/skills/blob/main/agent_skills_spec.md"
              },
              {
                "name": "plugin-marketplace",
                "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                "path": "claude-code/skills/plugin-marketplace/SKILL.md",
                "frontmatter": {
                  "name": "plugin-marketplace",
                  "description": "Guide for creating, validating, and managing Claude Code plugin marketplaces with schema validation tools",
                  "license": "MIT",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# Claude Code Plugin Marketplace\n\nGuide for creating, validating, and managing plugin marketplaces for Claude Code. Includes schema validation, best practices, and automated tools.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or editing `.claude-plugin/marketplace.json` files\n- Validating marketplace schema compliance\n- Setting up plugin repositories with marketplaces\n- Troubleshooting marketplace configuration issues\n- Converting plugin structures to marketplace format\n- Creating plugin entries with advanced features\n\n## Marketplace Schema Overview\n\n### Required Structure\n\nAll marketplaces must be located at `.claude-plugin/marketplace.json` in the repository root.\n\n**Required Fields:**\n- `name`: Marketplace identifier (kebab-case, lowercase alphanumeric and hyphens only)\n- `owner`: Object with maintainer details (`name` required, `email` optional)\n- `plugins`: Array of plugin definitions (can be empty)\n\n**Optional Metadata:**\n- `metadata.description`: Summary of marketplace purpose\n- `metadata.version`: Marketplace version tracking (semantic versioning recommended)\n- `metadata.pluginRoot`: Base directory for relative plugin source paths\n\n### Plugin Entry Schema\n\n**IMPORTANT: Schema Relationship**\n\nPlugin entries use the plugin manifest schema with all fields made optional, plus marketplace-specific fields (`source`, `strict`, `category`, `tags`). This means any field valid in a plugin.json file can also be used in a marketplace entry.\n\n- When `strict: false`, the marketplace entry serves as the complete plugin manifest if no plugin.json exists\n- When `strict: true` (default), marketplace fields supplement the plugin's own manifest file\n\nEach plugin entry in the `plugins` array requires:\n\n**Mandatory:**\n- `name`: Plugin identifier (kebab-case)\n- `source`: Location specification (string path or object)\n\n**Standard Metadata:**\n- `description`: Brief explanation of plugin functionality\n- `version`: Semantic version number\n- `author`: Creator information (object with `name`, optional `email`)\n- `homepage`: Documentation or project URL\n- `repository`: Source control URL\n- `license`: SPDX license identifier (e.g., MIT, Apache-2.0)\n- `keywords`: Array of discovery and categorization tags\n- `category`: Organizational grouping\n- `tags`: Additional searchability terms\n\n**Component Configuration:**\n- `commands`: Custom paths to command files or directories\n- `agents`: Custom paths to agent files\n- `hooks`: Custom hooks configuration or path to hooks file\n- `mcpServers`: MCP server configurations or path to MCP config\n- `skills`: Array of skill directory paths\n\n**Strict Mode Control:**\n- `strict`: Boolean (default: `true`)\n  - `true`: Plugin must include plugin.json; marketplace fields supplement it\n  - `false`: Marketplace entry serves as complete manifest (no plugin.json needed)\n\n**Dependencies:**\n- `dependencies`: Array of plugin names this plugin depends on (format: `\"namespace:plugin-name\"`)\n\n## Plugin Source Formats\n\n### Relative Path\n```json\n\"source\": \"./plugins/my-plugin\"\n```\n\n### Relative Path with pluginRoot\n```json\n// In marketplace metadata\n\"metadata\": {\n  \"pluginRoot\": \"./plugins\"\n}\n\n// In plugin entry\n\"source\": \"my-plugin\"  // Resolves to ./plugins/my-plugin\n```\n\n### GitHub Repository\n```json\n\"source\": {\n  \"source\": \"github\",\n  \"repo\": \"owner/plugin-repo\",\n  \"path\": \"optional/subdirectory\",\n  \"branch\": \"main\"\n}\n```\n\n### Git URL\n```json\n\"source\": {\n  \"source\": \"url\",\n  \"url\": \"https://gitlab.com/team/plugin.git\",\n  \"branch\": \"main\"\n}\n```\n\n## Environment Variables\n\nUse `${CLAUDE_PLUGIN_ROOT}` in paths to reference the plugin's installation directory:\n\n```json\n{\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/my-skill\"\n  ],\n  \"commands\": [\n    \"${CLAUDE_PLUGIN_ROOT}/commands\"\n  ]\n}\n```\n\nThis ensures paths work correctly regardless of installation location.\n\n## Advanced Plugin Entry Features\n\n### Inline Plugin Definitions\n\nUse `strict: false` to define complete plugin manifests inline without requiring plugin.json:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"source\": \"./plugins/my-plugin\",\n  \"strict\": false,\n  \"description\": \"Complete plugin definition inline\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Developer Name\"\n  },\n  \"skills\": [\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\",\n    \"${CLAUDE_PLUGIN_ROOT}/skills/skill-two\"\n  ]\n}\n```\n\n### Component Path Override\n\nCustomize component locations:\n\n```json\n{\n  \"name\": \"custom-paths\",\n  \"source\": \"./plugins/custom\",\n  \"strict\": false,\n  \"commands\": [\"${CLAUDE_PLUGIN_ROOT}/custom-commands\"],\n  \"agents\": [\"${CLAUDE_PLUGIN_ROOT}/custom-agents\"],\n  \"hooks\": {\n    \"onInstall\": \"${CLAUDE_PLUGIN_ROOT}/hooks/install.sh\"\n  },\n  \"mcpServers\": \"${CLAUDE_PLUGIN_ROOT}/mcp-config.json\"\n}\n```\n\n### Metadata Supplementation\n\nWith `strict: true`, marketplace entries can add metadata not in plugin.json:\n\n```json\n{\n  \"name\": \"existing-plugin\",\n  \"source\": \"./plugins/existing\",\n  \"strict\": true,\n  \"category\": \"development\",\n  \"keywords\": [\"added\", \"from\", \"marketplace\"],\n  \"homepage\": \"https://docs.example.com\"\n}\n```\n\n## Validation Workflow\n\n### 1. Schema Validation\n\nUse the provided Nushell script to validate marketplace.json:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\nThis validates:\n- JSON syntax\n- Required fields presence\n- Kebab-case naming\n- Field type correctness\n- Source path accessibility (for relative paths)\n\n### 2. Plugin Entry Validation\n\nValidate individual plugin entries:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin-entry.nu .claude-plugin/marketplace.json \"plugin-name\"\n```\n\nChecks:\n- Required fields (name, source)\n- Strict mode consistency\n- Dependency references\n- Path validity\n- Component configuration\n\n### 3. Dependency Graph Validation\n\nCheck for circular dependencies and missing dependencies:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-dependencies.nu .claude-plugin/marketplace.json\n```\n\n## Best Practices\n\n### Naming Conventions\n\n- **Marketplace name**: Use your GitHub username or organization (e.g., `vinnie357`)\n- **Plugin names**: Use descriptive kebab-case (e.g., `elixir-phoenix`, `rust-tools`, `core-skills`)\n- **Categories**: Standardize on common categories: `development`, `language`, `tools`, `frontend`, `backend`, `meta`\n\n### Versioning Strategy\n\n- Use semantic versioning for both marketplace and plugins\n- Bump marketplace version when adding/removing plugins\n- Bump plugin versions when updating skills or configuration\n- Document breaking changes in plugin descriptions\n\n### Dependency Management\n\n- Always declare `dependencies` for plugins that require other plugins\n- Keep dependency chains shallow (avoid deep nesting)\n- Consider creating a meta-plugin (like `all-skills`) that bundles related plugins\n- Use namespace prefixes for dependencies (e.g., `all-skills:core`)\n\n### Strict Mode Decision\n\n**Use `strict: false` when:**\n- Creating simple, self-contained plugins\n- All configuration is in marketplace.json\n- You want centralized management\n- Plugin is unlikely to be distributed independently\n\n**Use `strict: true` when:**\n- Plugin has complex configuration\n- Plugin may be distributed separately\n- Plugin has its own versioning lifecycle\n- You want to supplement existing plugin.json with marketplace metadata\n\n### Source Path Organization\n\n```json\n{\n  \"metadata\": {\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"core\",\n      \"source\": \"core\"  // Resolves to ./plugins/core\n    },\n    {\n      \"name\": \"external\",\n      \"source\": {\n        \"source\": \"github\",\n        \"repo\": \"org/repo\"\n      }\n    }\n  ]\n}\n```\n\n## Common Validation Errors\n\n### Error: Invalid kebab-case name\n\n```json\n// ❌ Invalid\n\"name\": \"myPlugin\"\n\"name\": \"my_plugin\"\n\"name\": \"My-Plugin\"\n\n// ✅ Valid\n\"name\": \"my-plugin\"\n\"name\": \"core-skills\"\n```\n\n### Error: Missing required owner field\n\n```json\n// ❌ Invalid\n{\n  \"name\": \"marketplace\"\n}\n\n// ✅ Valid\n{\n  \"name\": \"marketplace\",\n  \"owner\": {\n    \"name\": \"Developer Name\"\n  }\n}\n```\n\n### Error: Invalid source path\n\n```json\n// ❌ Invalid (path doesn't exist)\n\"source\": \"./plugins/nonexistent\"\n\n// ✅ Valid (path exists)\n\"source\": \"./plugins/core\"\n```\n\n### Error: Circular dependencies\n\n```json\n// ❌ Invalid\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-a\",\n      \"dependencies\": [\"namespace:plugin-b\"]\n    },\n    {\n      \"name\": \"plugin-b\",\n      \"dependencies\": [\"namespace:plugin-a\"]\n    }\n  ]\n}\n```\n\n## Creating a New Marketplace\n\n### Step 1: Initialize Structure\n\n```bash\nmkdir -p .claude-plugin\n```\n\n### Step 2: Create Marketplace File\n\nUse the validation script to generate a template:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/init-marketplace.nu\n```\n\nThis creates `.claude-plugin/marketplace.json` with required fields.\n\n### Step 3: Add Plugin Entries\n\nFor each plugin, decide on strict mode and add entry:\n\n```json\n{\n  \"name\": \"marketplace-name\",\n  \"owner\": {\n    \"name\": \"Your Name\",\n    \"email\": \"you@example.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Your marketplace description\",\n    \"version\": \"1.0.0\",\n    \"pluginRoot\": \"./plugins\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"source\": \"plugin-name\",\n      \"strict\": false,\n      \"description\": \"Plugin description\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Your Name\"\n      },\n      \"license\": \"MIT\",\n      \"category\": \"development\",\n      \"skills\": [\n        \"${CLAUDE_PLUGIN_ROOT}/skills/skill-one\"\n      ]\n    }\n  ]\n}\n```\n\n### Step 4: Validate\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json\n```\n\n### Step 5: Test Installation\n\n```bash\nclaude-code install ./\n```\n\n## Migrating Existing Plugins\n\n### From Individual Plugins to Marketplace\n\n1. **Identify plugins**: List all plugin.json files\n2. **Decide on strict mode**: Choose per plugin based on complexity\n3. **Create marketplace.json**: Add all plugins with appropriate configuration\n4. **Test each plugin**: Verify installation works correctly\n5. **Document dependencies**: Add dependency arrays where needed\n\n### Migration Script\n\nUse the provided script to analyze existing structure:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/analyze-plugins.nu .\n```\n\nThis scans for plugin.json files and suggests marketplace.json structure.\n\n## Troubleshooting\n\n### Plugin Not Found After Installation\n\n- Verify `source` path is correct\n- Check `pluginRoot` in metadata if using relative paths\n- Ensure plugin directory exists at specified location\n\n### Skills Not Loading\n\n- Verify skill paths use `${CLAUDE_PLUGIN_ROOT}` if needed\n- Check that skill directories contain SKILL.md files\n- Validate skill paths in plugin entry or plugin.json\n\n### Dependency Resolution Fails\n\n- Ensure dependency names match exactly (including namespace)\n- Check that all dependencies are listed in marketplace\n- Verify no circular dependencies exist\n\n### Validation Errors\n\nRun validation script with verbose mode:\n\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/validate-marketplace.nu .claude-plugin/marketplace.json --verbose\n```\n\n## References\n\nFor detailed schema specifications and examples, see:\n- `references/schema-specification.md`: Complete JSON schema\n- `references/examples.md`: Real-world marketplace examples\n- `references/migration-guide.md`: Step-by-step migration instructions\n\n## Script Usage\n\nAll validation and utility scripts are located in `scripts/`:\n- `validate-marketplace.nu`: Full marketplace validation\n- `validate-plugin-entry.nu`: Individual plugin entry validation\n- `validate-dependencies.nu`: Dependency graph validation\n- `init-marketplace.nu`: Generate marketplace template\n- `analyze-plugins.nu`: Analyze existing plugin structure\n- `format-marketplace.nu`: Format and sort marketplace.json\n\nExecute scripts with:\n```bash\nnu ${CLAUDE_PLUGIN_ROOT}/scripts/[script-name].nu [args]\n```"
              },
              {
                "name": "accessibility",
                "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces",
                "path": "core/skills/accessibility/SKILL.md",
                "frontmatter": {
                  "name": "accessibility",
                  "description": "Guide for implementing web accessibility following W3C WAI principles (WCAG) when designing, developing, or reviewing web interfaces"
                },
                "content": "# Web Accessibility\n\nApply W3C Web Accessibility Initiative (WAI) principles when working on web interfaces to ensure usability for people with disabilities.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing user interfaces\n- Reviewing code for accessibility compliance\n- Creating or editing web content (HTML, CSS, JavaScript)\n- Working with forms, navigation, multimedia, or interactive components\n- Conducting code reviews with accessibility considerations\n- Refactoring existing interfaces for better accessibility\n\n## Core Principles (POUR)\n\nWeb accessibility is organized around four foundational principles:\n\n### 1. Perceivable\n\nInformation must be presentable to users in ways they can perceive.\n\n**Key requirements:**\n- Provide text alternatives for non-text content (images, icons, charts)\n- Provide captions and transcripts for multimedia\n- Create content that can be presented in different ways (responsive, reflow)\n- Make content distinguishable (color contrast, text sizing, audio control)\n\n**Quick example:**\n```html\n<img src=\"chart.png\" alt=\"Sales increased 40% in Q4 2024\">\n<button aria-label=\"Close dialog\">\n  <span class=\"icon-close\" aria-hidden=\"true\"></span>\n</button>\n```\n\nFor detailed guidance on text alternatives, multimedia, and color contrast, see `references/perceivable.md`.\n\n### 2. Operable\n\nUser interface components must be operable by all users.\n\n**Key requirements:**\n- Make all functionality keyboard accessible\n- Provide sufficient time for users to complete tasks\n- Avoid content that causes seizures (no rapid flashing)\n- Help users navigate and find content\n- Support various input modalities (touch, voice, keyboard)\n\n**Quick example:**\n```html\n<button>Click me</button>  <!-- Already keyboard accessible -->\n\n<!-- Custom interactive element needs keyboard support -->\n<div role=\"button\" tabindex=\"0\"\n     onclick=\"handleClick()\"\n     onkeydown=\"handleKeyDown(event)\">\n  Custom Button\n</div>\n```\n\nFor keyboard patterns, focus management, and navigation, see `references/operable.md`.\n\n### 3. Understandable\n\nInformation and UI operation must be understandable.\n\n**Key requirements:**\n- Make text readable and understandable\n- Make web pages appear and operate predictably\n- Help users avoid and correct mistakes\n- Provide clear labels and instructions\n\n**Quick example:**\n```html\n<html lang=\"en\">\n<label for=\"email\">Email address</label>\n<input type=\"email\" id=\"email\"\n       aria-describedby=\"email-help\"\n       required>\n<div id=\"email-help\">We'll never share your email</div>\n```\n\nFor form patterns, error handling, and content clarity, see `references/understandable.md`.\n\n### 4. Robust\n\nContent must work reliably across user agents and assistive technologies.\n\n**Key requirements:**\n- Use valid, well-formed markup\n- Ensure compatibility with assistive technologies\n- Use ARIA correctly for custom components\n- Follow semantic HTML practices\n\n**Quick example:**\n```html\n<!-- Use semantic HTML first -->\n<nav aria-label=\"Main navigation\">\n  <ul>\n    <li><a href=\"/\">Home</a></li>\n  </ul>\n</nav>\n\n<!-- ARIA for custom components when needed -->\n<div role=\"dialog\" aria-labelledby=\"title\" aria-modal=\"true\">\n  <h2 id=\"title\">Dialog Title</h2>\n</div>\n```\n\nFor ARIA patterns and custom components, see `references/robust.md`.\n\n## Common Tasks\n\n### Making Forms Accessible\n\nConsult `references/forms.md` for comprehensive form accessibility including:\n- Label association\n- Error identification and suggestions\n- Required field indication\n- Input validation patterns\n\n### Implementing ARIA\n\nSee `references/aria.md` for:\n- When to use ARIA vs semantic HTML\n- Common ARIA patterns (tabs, accordions, modals)\n- ARIA states and properties\n- Live regions for dynamic content\n\n### Testing for Accessibility\n\nConsult `references/testing.md` for:\n- Keyboard navigation testing\n- Screen reader testing procedures\n- Automated testing tools\n- Color contrast checking\n\n### Common Patterns\n\nSee `references/patterns.md` for accessible implementations of:\n- Modal dialogs\n- Dropdown menus\n- Tabs and accordions\n- Loading states and notifications\n- Skip links and landmarks\n\n## Quick Reference Checklist\n\n**Every page should have:**\n- [ ] Valid HTML structure\n- [ ] Unique, descriptive page title\n- [ ] Proper heading hierarchy (h1, h2, h3...)\n- [ ] Language attribute on `<html>`\n- [ ] Sufficient color contrast (4.5:1 minimum)\n- [ ] Keyboard accessibility for all interactive elements\n- [ ] Visible focus indicators\n- [ ] Text alternatives for images\n- [ ] Form labels associated with inputs\n- [ ] Semantic landmark regions\n\n**For interactive components:**\n- [ ] Keyboard accessible (Tab, Enter, Space, Arrow keys)\n- [ ] Proper ARIA roles, states, and properties\n- [ ] Focus management (modals, dynamic content)\n- [ ] Descriptive labels and instructions\n- [ ] Error messages linked to form controls\n\n## Key Principles\n\n- **Semantic HTML first**: Use native HTML elements before adding ARIA\n- **Keyboard accessibility is fundamental**: If it works with mouse, it must work with keyboard\n- **Test with actual users**: Include people with disabilities in testing\n- **Color is not enough**: Never use color alone to convey information\n- **Provide alternatives**: Text for images, captions for video, transcripts for audio\n- **Make it predictable**: Consistent navigation and behavior across pages\n- **Help users recover**: Clear error messages with suggestions for correction\n\n## Resources\n\n- **WCAG 2.1 Guidelines**: https://www.w3.org/WAI/WCAG21/quickref/\n- **ARIA Authoring Practices**: https://www.w3.org/WAI/ARIA/apg/\n- **WebAIM**: https://webaim.org/\n- **MDN Accessibility**: https://developer.mozilla.org/en-US/docs/Web/Accessibility"
              },
              {
                "name": "anti-fabrication",
                "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                "path": "core/skills/anti-fabrication/SKILL.md",
                "frontmatter": {
                  "name": "anti-fabrication",
                  "description": "Ensure factual accuracy by validating claims through tool execution, avoiding superlatives and unsubstantiated metrics, and marking uncertain information appropriately",
                  "license": "MIT"
                },
                "content": "# Anti-Fabrication\n\nStrict requirements for ensuring factual, measurable, and validated outputs in all work products including documentation, research, reports, and analysis.\n\n## When to Use This Skill\n\nActivate when:\n- Writing documentation or creating research materials\n- Making claims about system capabilities, performance, or features\n- Providing estimates for time, effort, or complexity\n- Reporting test results or analysis outcomes\n- Creating any content that presents factual information\n- Generating metrics, statistics, or performance data\n\n## Core Principles\n\n### Evidence-Based Outputs\n- Base all outputs on actual analysis of real data using tool execution\n- Execute Read, Glob, Bash, or other validation tools before making claims\n- Never assume file existence, system capabilities, or feature presence without verification\n- Validate integration recommendations through actual framework detection\n\n### Explicit Uncertainty\n- Mark uncertain information as \"requires analysis\", \"needs validation\", or \"requires investigation\"\n- State when information cannot be verified: \"Unable to confirm without [specific check]\"\n- Acknowledge knowledge limitations rather than fabricating plausible-sounding content\n- Use conditional language when appropriate: \"may\", \"likely\", \"appears to\"\n\n### Factual Language\n- Use precise, factual language without superlatives or unsubstantiated performance claims\n- Replace vague statements with specific, measurable observations\n- Report what was actually observed, not what should theoretically be true\n- Distinguish between verified facts and reasonable inferences\n\n## Prohibited Language and Claims\n\n### Superlatives to Avoid\nNever use unverified superlatives:\n- ❌ \"excellent\", \"comprehensive\", \"advanced\", \"optimal\", \"perfect\"\n- ❌ \"best practice\", \"industry-leading\", \"cutting-edge\", \"state-of-the-art\"\n- ❌ \"robust\", \"scalable\", \"production-ready\" (without specific evidence)\n\nInstead, use factual descriptions:\n- ✅ \"follows the specification defined in [source]\"\n- ✅ \"implements [specific pattern] as documented in [reference]\"\n- ✅ \"tested with [specific conditions] and produced [specific results]\"\n\n### Unsubstantiated Metrics\nNever fabricate quantitative data:\n- ❌ Percentages without measurement: \"improves performance by 30%\"\n- ❌ Success rates without testing: \"has a 95% success rate\"\n- ❌ Arbitrary scores: \"code quality score of 8/10\"\n- ❌ Made-up statistics: \"reduces memory usage significantly\"\n\nInstead, provide verified measurements:\n- ✅ \"benchmark shows execution time decreased from 150ms to 98ms\"\n- ✅ \"passed 47 of 50 test cases (94%)\"\n- ✅ \"static analysis tool reports complexity score of 12\"\n\n### Assumed Capabilities\nNever claim features exist without verification:\n- ❌ \"This system supports authentication\" (without checking)\n- ❌ \"The API provides rate limiting\" (without reading docs/code)\n- ❌ \"This handles edge cases correctly\" (without testing)\n\nInstead, verify before claiming:\n- ✅ Use Read tool to check configuration files\n- ✅ Use Grep to search for specific implementations\n- ✅ Use Bash to test actual behavior\n- ✅ State \"requires verification\" if tools cannot confirm\n\n## Time and Effort Estimation Rules\n\n### Never Estimate Without Analysis\nDo not provide time estimates without factual basis:\n- ❌ \"This will take 15 minutes\"\n- ❌ \"Should be done in 2 hours\"\n- ❌ \"Quick task, won't take long\"\n- ❌ \"Simple fix\"\n\n### Data-Backed Estimates Only\nIf estimates are requested, execute tools first:\n1. Count files that need modification (using Glob)\n2. Measure code complexity (using Read and analysis)\n3. Assess dependencies (using Grep for imports/references)\n4. Review similar past work (if available)\n\nThen provide estimate with evidence:\n- ✅ \"Requires modifying 12 files based on grep search, estimated X hours\"\n- ✅ \"Analysis shows 3 integration points, complexity suggests Y time\"\n- ✅ \"Timeline requires analysis of [specific factors not yet measured]\"\n\n### When Unable to Estimate\nBe explicit about limitations:\n- ✅ \"Cannot provide time estimate without analyzing [specific aspects]\"\n- ✅ \"Requires investigation of [X, Y, Z] before estimating\"\n- ✅ \"Complexity assessment needed before timeline projection\"\n\n## Validation Requirements\n\n### File Claims\nBefore claiming files exist or contain specific content:\n```\n1. Use Read tool to verify file exists and check contents\n2. Use Glob to find files matching patterns\n3. Use Grep to verify specific code or content is present\n4. Never state \"file X contains Y\" without tool verification\n```\n\n**Example violations:**\n- ❌ \"The config file sets the timeout to 30 seconds\" (without reading it)\n- ❌ \"There are multiple test files for this module\" (without globbing)\n\n**Correct approach:**\n- ✅ Read the config file first, then report actual timeout value\n- ✅ Use Glob to find test files, then report count and names\n\n### System Integration\nBefore claiming system capabilities:\n```\n1. Use Bash to check installed tools/dependencies\n2. Read package.json, requirements.txt, or equivalent\n3. Verify environment variables and configuration\n4. Test actual behavior when possible\n```\n\n### Framework Detection\nBefore claiming framework presence or version:\n```\n1. Read package.json, Gemfile, mix.exs, or dependency file\n2. Search for framework-specific imports or patterns\n3. Check for framework configuration files\n4. Report specific version found, not assumed capabilities\n```\n\n### Test Results\nOnly report test outcomes after actual execution:\n```\n1. Execute tests using Bash tool\n2. Capture and read actual output\n3. Report specific pass/fail counts and error messages\n4. Never claim \"tests pass\" or \"all tests successful\" without execution\n```\n\n### Performance Claims\nOnly make performance statements based on measurement:\n```\n1. Run benchmarks or profiling tools\n2. Capture actual timing/memory data\n3. Report specific measurements with conditions\n4. State testing methodology used\n```\n\n## Anti-Patterns to Avoid\n\n### Fabricated Testing\n❌ \"The code has been thoroughly tested\"\n❌ \"All edge cases are handled\"\n❌ \"Test coverage is good\"\n\n✅ \"Executed test suite: 45 passing, 2 failing\"\n✅ \"Coverage report shows 78% line coverage\"\n✅ \"Tested with inputs [X, Y, Z], observed [specific results]\"\n\n### Unverified Architecture Claims\n❌ \"This follows microservices architecture\"\n❌ \"Uses event-driven design patterns\"\n❌ \"Implements SOLID principles\"\n\n✅ Use Grep to find specific patterns, then describe what exists\n✅ \"Found 12 service definitions in [location]\"\n✅ \"Code shows [specific pattern] in [specific files]\"\n\n### Generic Quality Statements\n❌ \"This is high-quality code\"\n❌ \"Well-structured implementation\"\n❌ \"Follows best practices\"\n\n✅ \"Code follows [specific standard] as verified by linter\"\n✅ \"Matches patterns from [specific reference documentation]\"\n✅ \"Static analysis shows complexity metrics of [specific values]\"\n\n## Validation Workflow\n\nWhen creating any factual content:\n\n1. **Identify Claims**: List all factual assertions being made\n2. **Check Evidence**: For each claim, determine what tool can verify it\n3. **Execute Validation**: Run Read, Grep, Glob, Bash, or other tools\n4. **Report Results**: State only what tools confirmed\n5. **Mark Uncertainty**: Clearly label anything not verified\n\n## Examples\n\n### Documentation Writing\n\n**Bad approach:**\n```markdown\nThis API is highly performant and handles thousands of requests per second.\nIt follows RESTful best practices and includes comprehensive error handling.\n```\n\n**Good approach:**\n```markdown\nThis API implements REST endpoints as defined in [specification link].\nLoad testing with Apache Bench shows handling of 1,200 requests/second\nat 95th percentile latency of 45ms. Error handling covers HTTP status codes\n400, 401, 403, 404, 500 as verified in [source file].\n```\n\n### Research Output\n\n**Bad approach:**\n```markdown\nReact hooks are the modern way to write React components and are much\nbetter than class components. They improve performance and code quality.\n```\n\n**Good approach:**\n```markdown\nReact hooks (introduced in React 16.8 per official changelog) provide\nfunction component state and lifecycle features previously requiring\nclasses. The React documentation at [URL] states hooks reduce component\nnesting and enable logic reuse. Performance impact requires measurement\nfor specific use cases.\n```\n\n### Implementation Planning\n\n**Bad approach:**\n```markdown\nThis should be a quick implementation, probably 2-3 hours.\nWe'll add authentication which is straightforward, then deploy.\n```\n\n**Good approach:**\n```markdown\nImplementation requires:\n- Authentication integration (12 files need modification per grep analysis)\n- Configuration of [specific auth provider]\n- Testing of login/logout flows\n\nComplexity assessment needed before timeline estimation. Requires\ninvestigation of existing auth patterns and deployment requirements.\n```\n\n## Integration with Other Skills\n\nThis skill should be active alongside:\n- **Documentation**: Ensures docs contain verified information\n- **Code Review**: Validates claims about code quality and patterns\n- **Research**: Grounds research in verifiable sources\n- **Git Operations**: Ensures accurate commit messages and PR descriptions\n\n## References\n\n- Agent Skills Specification: Factual, validated skill content\n- Scientific Method: Observation before conclusion\n- Verification Principle: Trust but verify through tool execution"
              },
              {
                "name": "code-review",
                "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices",
                "path": "core/skills/code-review/SKILL.md",
                "frontmatter": {
                  "name": "code-review",
                  "description": "Guide for conducting thorough code reviews focusing on correctness, security, performance, maintainability, and best practices"
                },
                "content": "# Code Review Best Practices\n\nThis skill activates when reviewing code for quality, correctness, security, and maintainability.\n\n## When to Use This Skill\n\nActivate when:\n- Reviewing pull requests\n- Conducting code audits\n- Providing feedback on code quality\n- Identifying security vulnerabilities\n- Suggesting refactoring improvements\n- Checking adherence to coding standards\n\n## Code Review Checklist\n\n### 1. Correctness and Functionality\n\n**Does the code do what it's supposed to do?**\n\n- Logic is correct and handles all cases\n- Edge cases are considered\n- Error handling is appropriate\n- No obvious bugs or logical errors\n- Assertions and validations are present\n- Return values are correct\n\n**Questions to ask:**\n- What happens if this receives null/nil?\n- What if the list is empty?\n- What if the number is negative/zero?\n- Are there off-by-one errors?\n- Are comparisons correct (>, >=, <, <=)?\n\n### 2. Security\n\n**Is the code secure?**\n\n- No SQL injection vulnerabilities\n- No XSS (Cross-Site Scripting) vulnerabilities\n- No CSRF vulnerabilities (CSRF protection in place)\n- User input is validated and sanitized\n- Sensitive data is not logged\n- Authentication and authorization are properly implemented\n- No hardcoded secrets or credentials\n- File uploads are validated (type, size, content)\n- External URLs are validated\n- Rate limiting is in place for APIs\n\n**Common security issues:**\n\n```elixir\n# BAD: SQL injection vulnerability\nquery = \"SELECT * FROM users WHERE id = #{user_id}\"\n\n# GOOD: Use parameterized queries\nquery = from u in User, where: u.id == ^user_id\n\n# BAD: XSS vulnerability\nraw(\"<div>#{user_input}</div>\")\n\n# GOOD: Escape user input\n<div><%= user_input %></div>\n\n# BAD: Hardcoded secrets\napi_key = \"sk_live_123456789\"\n\n# GOOD: Use environment variables\napi_key = System.get_env(\"API_KEY\")\n\n# BAD: Mass assignment vulnerability\nUser.changeset(%User{}, params)\n\n# GOOD: Whitelist allowed fields\nUser.changeset(%User{}, params)\n# Where changeset only casts allowed fields:\n# cast(user, attrs, [:name, :email])\n```\n\n### 3. Performance\n\n**Is the code efficient?**\n\n- No N+1 query problems\n- Appropriate data structures chosen\n- Algorithms are efficient\n- Database indexes are used\n- Caching is implemented where appropriate\n- Large datasets are paginated or streamed\n- Unnecessary computations are avoided\n- Resources are cleaned up properly\n\n**Common performance issues:**\n\n```elixir\n# BAD: N+1 query\nposts = Repo.all(Post)\nEnum.map(posts, fn post ->\n  author = Repo.get(User, post.author_id)  # Query for each post!\n  {post, author}\nend)\n\n# GOOD: Preload associations\nposts = Post |> preload(:author) |> Repo.all()\n\n# BAD: Loading entire dataset\nusers = Repo.all(User)  # Loads all millions of users\nEnum.filter(users, & &1.active)\n\n# GOOD: Query in database\nusers = User |> where(active: true) |> Repo.all()\n\n# BAD: Inefficient data structure\nlist = [1, 2, 3, 4, 5]\nif 3 in list do  # O(n) lookup in list\n  # ...\nend\n\n# GOOD: Use set/map for lookups\nset = MapSet.new([1, 2, 3, 4, 5])\nif MapSet.member?(set, 3) do  # O(1) lookup\n  # ...\nend\n```\n\n### 4. Code Quality and Maintainability\n\n**Is the code readable and maintainable?**\n\n- Clear, descriptive variable and function names\n- Functions are small and focused (single responsibility)\n- No code duplication (DRY principle)\n- Comments explain \"why\", not \"what\"\n- Code follows project conventions and style guide\n- Magic numbers are replaced with named constants\n- Complexity is minimized\n- Code is self-documenting\n\n**Code quality issues:**\n\n```elixir\n# BAD: Unclear names\ndef calc(x, y, z) do\n  r = x * y / z\n  r * 1.2\nend\n\n# GOOD: Clear names\ndef calculate_discounted_price(quantity, unit_price, discount_percentage) do\n  subtotal = quantity * unit_price\n  discount_amount = subtotal * (discount_percentage / 100)\n  subtotal - discount_amount\nend\n\n# BAD: Long function with multiple responsibilities\ndef process_order(order) do\n  # Validate order (responsibility 1)\n  # Calculate totals (responsibility 2)\n  # Update inventory (responsibility 3)\n  # Send email (responsibility 4)\n  # Log analytics (responsibility 5)\nend\n\n# GOOD: Single responsibility functions\ndef process_order(order) do\n  with {:ok, order} <- validate_order(order),\n       {:ok, order} <- calculate_totals(order),\n       {:ok, order} <- update_inventory(order),\n       :ok <- send_confirmation_email(order),\n       :ok <- log_order_analytics(order) do\n    {:ok, order}\n  end\nend\n\n# BAD: Magic numbers\nif user.age >= 13 do\n  # ...\nend\n\n# GOOD: Named constants\n@minimum_age_coppa 13\n\nif user.age >= @minimum_age_coppa do\n  # ...\nend\n```\n\n### 5. Error Handling\n\n**Are errors handled properly?**\n\n- Errors don't crash the system unexpectedly\n- Error messages are helpful\n- Errors are logged appropriately\n- Happy path and error paths are both tested\n- No swallowed errors (empty catch blocks)\n- Proper error types are used\n\n**Error handling patterns:**\n\n```elixir\n# BAD: Silent failure\ntry do\n  dangerous_operation()\nrescue\n  _ -> nil  # Error is swallowed!\nend\n\n# GOOD: Handle errors explicitly\ncase dangerous_operation() do\n  {:ok, result} -> result\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n\n# BAD: Generic error message\n{:error, \"failed\"}\n\n# GOOD: Specific error\n{:error, :invalid_email_format}\n{:error, {:validation_failed, errors}}\n\n# BAD: Let it crash when shouldn't\ndef parse_config(path) do\n  File.read!(path)  # Crashes if file missing\n  |> Jason.decode!()  # Crashes if invalid JSON\nend\n\n# GOOD: Handle expected errors\ndef parse_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  else\n    {:error, :enoent} -> {:error, :config_file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_config_format}\n  end\nend\n```\n\n### 6. Testing\n\n**Is the code properly tested?**\n\n- New functionality has tests\n- Edge cases are tested\n- Error conditions are tested\n- Tests are clear and focused\n- Tests are deterministic (no flaky tests)\n- Test names describe what they test\n- Mocks are used appropriately\n- Test coverage is adequate\n\n**Testing concerns:**\n\n```elixir\n# BAD: Unclear test name\ntest \"test1\" do\n  # ...\nend\n\n# GOOD: Descriptive test name\ntest \"create_user/1 returns error when email is invalid\" do\n  # ...\nend\n\n# BAD: Testing too much at once\ntest \"user workflow\" do\n  # Creates user\n  # Updates user\n  # Deletes user\n  # All in one test!\nend\n\n# GOOD: Focused tests\ntest \"create_user/1 creates user with valid attributes\" do\n  # ...\nend\n\ntest \"update_user/2 updates user name\" do\n  # ...\nend\n\ntest \"delete_user/1 removes user from database\" do\n  # ...\nend\n\n# BAD: Non-deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  Process.sleep(100)  # Race condition!\n  assert operation_completed?()\nend\n\n# GOOD: Deterministic test\ntest \"async operation completes\" do\n  start_async_operation()\n  assert_receive {:completed, _result}, 1000\nend\n```\n\n### 7. Documentation\n\n**Is the code documented?**\n\n- Public APIs have documentation\n- Complex logic has explanatory comments\n- README is updated if needed\n- Changelog is updated for user-facing changes\n- API documentation is accurate\n- Examples are provided\n\n### 8. Dependencies\n\n**Are dependencies handled properly?**\n\n- New dependencies are justified\n- Dependencies are up-to-date and maintained\n- Licenses are compatible with project\n- Security vulnerabilities are checked\n- Dependency versions are pinned or bounded\n\n## Review Process\n\n### Before Reviewing\n\n1. **Understand the context**\n   - Read the PR description\n   - Understand the problem being solved\n   - Check related issues\n\n2. **Build and test locally**\n   - Pull the branch\n   - Run tests\n   - Test the functionality manually\n\n### During Review\n\n1. **Start with the big picture**\n   - Is the approach sound?\n   - Does it fit the architecture?\n   - Is there a better way?\n\n2. **Review for correctness**\n   - Does it work as intended?\n   - Are edge cases handled?\n   - Is error handling appropriate?\n\n3. **Check security and performance**\n   - Are there security vulnerabilities?\n   - Will it perform well at scale?\n\n4. **Review code quality**\n   - Is it readable and maintainable?\n   - Does it follow conventions?\n   - Is it well-tested?\n\n### Providing Feedback\n\n**Be constructive and specific:**\n\n```markdown\n# BAD: Vague criticism\n\"This function is bad.\"\n\n# GOOD: Specific, actionable feedback\n\"This function has three responsibilities: validation, database update, and email sending. Consider splitting it into separate functions for better testability and maintainability:\n\n```elixir\ndef update_user(user, attrs) do\n  with {:ok, changeset} <- validate_user_update(user, attrs),\n       {:ok, user} <- save_user(changeset),\n       :ok <- send_update_notification(user) do\n    {:ok, user}\n  end\nend\n```\n\n# BAD: Demanding\n\"You must change this.\"\n\n# GOOD: Collaborative\n\"What do you think about extracting this into a separate function? It would make the code easier to test.\"\n\n# BAD: Nitpicking without context\n\"Use single quotes instead of double quotes.\"\n\n# GOOD: Explain reasoning\n\"Our style guide prefers single quotes for consistency (see CONTRIBUTING.md section 3.2).\"\n```\n\n**Use labels to categorize feedback:**\n\n- **[blocking]**: Must be fixed before merging\n- **[suggestion]**: Optional improvement\n- **[question]**: Asking for clarification\n- **[nit]**: Very minor, cosmetic issue\n- **[security]**: Security concern\n- **[performance]**: Performance concern\n\n**Example:**\n\n```markdown\n[blocking] This creates a SQL injection vulnerability. Use parameterized queries:\n\n```elixir\n# Instead of:\nquery = \"SELECT * FROM users WHERE name = '#{name}'\"\n\n# Use:\nfrom(u in User, where: u.name == ^name)\n```\n\n[suggestion] Consider extracting this logic into a separate function for reusability.\n\n[question] Why are we using a map here instead of a struct?\n\n[nit] Extra blank line here.\n```\n\n### After Review\n\n1. **Respond to author's questions**\n2. **Re-review after changes**\n3. **Approve when satisfied**\n4. **Celebrate good code**\n\n## Language-Specific Considerations\n\n### Elixir\n\n- Pattern matching is used effectively\n- Functions leverage pipe operator for readability\n- Atoms aren't created dynamically from untrusted input\n- `with` statements handle errors properly\n- Changesets validate all input\n- No direct database queries in controllers/LiveViews (use contexts)\n\n### JavaScript/TypeScript\n\n- Types are properly defined (TypeScript)\n- Promises are handled with .catch() or try/catch\n- == vs === is used correctly\n- Arrays/objects aren't mutated unexpectedly\n- this binding is correct\n- Async operations are properly awaited\n\n### Python\n\n- Type hints are used\n- List comprehensions aren't overly complex\n- Exceptions are specific (not bare except:)\n- Resources are closed (use with statements)\n- Code follows PEP 8\n\n### Rust\n\n- Ownership and borrowing are correct\n- Error handling uses Result/Option properly\n- Unsafe blocks are justified and minimal\n- Clone/copy is used appropriately\n- Lifetimes are correctly specified\n\n## Common Code Smells\n\n### Complexity Smells\n\n- **Long functions** - Function does too much\n- **Long parameter list** - Too many parameters\n- **Deep nesting** - Too many levels of indentation\n- **Complex conditionals** - Hard to understand if statements\n\n### Duplication Smells\n\n- **Copy-paste code** - Same code in multiple places\n- **Similar functions** - Functions that do almost the same thing\n- **Magic numbers** - Repeated literal values\n\n### Naming Smells\n\n- **Unclear names** - Variables like x, tmp, data\n- **Misleading names** - Name doesn't match behavior\n- **Inconsistent names** - Same concept called different things\n\n### Design Smells\n\n- **God object** - Class/module doing everything\n- **Feature envy** - Function using another object's data more than its own\n- **Inappropriate intimacy** - Too much coupling between modules\n\n## Anti-Patterns to Watch For\n\n### Premature Optimization\n\n```elixir\n# BAD: Optimizing before measuring\ndef calculate(data) do\n  # Complex, hard-to-read optimization\n  # that saves 0.1ms\nend\n\n# GOOD: Start simple, optimize if needed\ndef calculate(data) do\n  # Clear, simple code\n  # Optimize later if profiling shows bottleneck\nend\n```\n\n### Premature Abstraction\n\n```elixir\n# BAD: Abstract after one use\ndefmodule AbstractDataProcessorFactoryBuilder do\n  # Complex abstraction for single use case\nend\n\n# GOOD: Wait for second use case\ndef process_user_data(data) do\n  # Simple, direct implementation\n  # Abstract when pattern emerges\nend\n```\n\n### Error Swallowing\n\n```elixir\n# BAD: Hiding errors\ntry do\n  risky_operation()\nrescue\n  _ -> :ok  # What went wrong?\nend\n\n# GOOD: Handle explicitly\ncase risky_operation() do\n  {:ok, result} -> {:ok, result}\n  {:error, reason} ->\n    Logger.error(\"Operation failed: #{inspect(reason)}\")\n    {:error, reason}\nend\n```\n\n## Review Etiquette\n\n### DO:\n\n- Be respectful and constructive\n- Assume good intent\n- Ask questions instead of making demands\n- Praise good code\n- Explain the \"why\" behind suggestions\n- Offer to pair program on complex issues\n- Respond promptly to author's replies\n\n### DON'T:\n\n- Be sarcastic or condescending\n- Bike-shed on minor style issues\n- Block on personal preferences\n- Review your own code without another reviewer\n- Approve code you don't understand\n- Nitpick excessively\n\n## Self-Review Checklist\n\nBefore submitting code for review:\n\n- [ ] Code compiles and runs\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] No commented-out code\n- [ ] No debug print statements\n- [ ] Documentation is updated\n- [ ] Commit messages are clear\n- [ ] No secrets or sensitive data\n- [ ] Code follows project style guide\n- [ ] Changes are focused (no unrelated changes)\n\n## Key Principles\n\n- **Correctness first**: Code must work correctly\n- **Security matters**: Always consider security implications\n- **Be specific**: Provide actionable, concrete feedback\n- **Be respectful**: Kind, constructive communication\n- **Focus on important issues**: Don't bike-shed\n- **Explain reasoning**: Help author learn, don't just dictate\n- **Approve good code**: Don't let perfect be enemy of good\n- **Collaborate**: You're on the same team"
              },
              {
                "name": "documentation-writing",
                "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation",
                "path": "core/skills/documentation/SKILL.md",
                "frontmatter": {
                  "name": "documentation-writing",
                  "description": "Guide for writing clear, comprehensive technical documentation including README files, API docs, guides, and inline documentation"
                },
                "content": "# Technical Documentation Writing\n\nThis skill activates when writing or improving technical documentation, including README files, API documentation, user guides, and inline code documentation.\n\n## When to Use This Skill\n\nActivate when:\n- Writing README files\n- Creating API documentation\n- Writing user guides or tutorials\n- Documenting code with comments or docstrings\n- Creating architecture or design documents\n- Writing changelogs or release notes\n\n## README Files\n\n### Essential README Structure\n\nEvery README should include:\n\n```markdown\n# Project Name\n\nBrief one-liner description of the project.\n\n## Overview\n\n2-3 paragraphs explaining what the project does, why it exists, and who it's for.\n\n## Features\n\n- Key feature 1\n- Key feature 2\n- Key feature 3\n\n## Installation\n\n### Prerequisites\n\n- Requirement 1 (with version)\n- Requirement 2 (with version)\n\n### Install Steps\n\n```bash\n# Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies\nnpm install  # or pip install -r requirements.txt, mix deps.get, etc.\n\n# Configure\ncp .env.example .env\n# Edit .env with your settings\n\n# Run\nnpm start\n```\n\n## Quick Start\n\n```bash\n# Minimal example to get started\nnpm start\n```\n\n## Usage\n\n### Basic Example\n\n```language\n// Clear, runnable example\nconst example = new Project()\nexample.doSomething()\n```\n\n### Advanced Usage\n\nMore complex examples with explanations.\n\n## Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `apiKey` | string | - | API key for authentication |\n| `timeout` | number | 5000 | Request timeout in ms |\n\n## API Reference\n\nLink to detailed API documentation or include core APIs here.\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Development-specific setup\nnpm install --dev\nnpm run setup\n```\n\n### Running Tests\n\n```bash\nnpm test\nnpm run test:coverage\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Credits\n- Inspirations\n- Related projects\n\n## Support\n\n- Documentation: https://docs.example.com\n- Issues: https://github.com/user/project/issues\n- Discussions: https://github.com/user/project/discussions\n```\n\n### README Best Practices\n\n- **Start with a clear one-liner**: Immediately tell readers what the project does\n- **Include badges**: Build status, coverage, version, license\n- **Show, don't tell**: Use code examples liberally\n- **Keep it scannable**: Use headers, lists, and code blocks\n- **Make examples runnable**: Readers should be able to copy-paste and run\n- **Include visual aids**: Screenshots, diagrams, GIFs when appropriate\n- **Update regularly**: Keep documentation in sync with code\n- **Think about newcomers**: Write for someone seeing the project for the first time\n\n## API Documentation\n\n### Documenting Functions\n\n**Elixir (@doc):**\n```elixir\n@doc \"\"\"\nCalculates the sum of two numbers.\n\n## Parameters\n\n- `a` - The first number (integer or float)\n- `b` - The second number (integer or float)\n\n## Returns\n\nThe sum of `a` and `b`.\n\n## Examples\n\n    iex> Math.add(2, 3)\n    5\n\n    iex> Math.add(2.5, 3.7)\n    6.2\n\n\"\"\"\n@spec add(number(), number()) :: number()\ndef add(a, b) do\n  a + b\nend\n```\n\n**JavaScript (JSDoc):**\n```javascript\n/**\n * Calculates the sum of two numbers.\n *\n * @param {number} a - The first number\n * @param {number} b - The second number\n * @returns {number} The sum of a and b\n *\n * @example\n * add(2, 3)\n * // => 5\n */\nfunction add(a, b) {\n  return a + b\n}\n```\n\n**Python (docstring):**\n```python\ndef add(a: float, b: float) -> float:\n    \"\"\"\n    Calculate the sum of two numbers.\n\n    Args:\n        a: The first number\n        b: The second number\n\n    Returns:\n        The sum of a and b\n\n    Examples:\n        >>> add(2, 3)\n        5\n        >>> add(2.5, 3.7)\n        6.2\n\n    Raises:\n        TypeError: If arguments are not numbers\n    \"\"\"\n    return a + b\n```\n\n**Rust (doc comments):**\n```rust\n/// Calculates the sum of two numbers.\n///\n/// # Arguments\n///\n/// * `a` - The first number\n/// * `b` - The second number\n///\n/// # Returns\n///\n/// The sum of `a` and `b`\n///\n/// # Examples\n///\n/// ```\n/// use mylib::add;\n///\n/// assert_eq!(add(2, 3), 5);\n/// assert_eq!(add(2.5, 3.7), 6.2);\n/// ```\npub fn add(a: f64, b: f64) -> f64 {\n    a + b\n}\n```\n\n### Module/Class Documentation\n\nDocument the purpose, usage, and public API:\n\n```elixir\ndefmodule MyApp.UserManager do\n  @moduledoc \"\"\"\n  Manages user accounts and authentication.\n\n  The UserManager provides functions for creating, updating, and authenticating\n  users. It handles password hashing, session management, and user validation.\n\n  ## Usage\n\n      # Create a new user\n      {:ok, user} = UserManager.create_user(%{\n        email: \"alice@example.com\",\n        password: \"secure_password\"\n      })\n\n      # Authenticate\n      {:ok, user} = UserManager.authenticate(\"alice@example.com\", \"secure_password\")\n\n      # Update user\n      {:ok, updated} = UserManager.update_user(user, %{name: \"Alice Smith\"})\n\n  ## Configuration\n\n  Configure in `config/config.exs`:\n\n      config :my_app, MyApp.UserManager,\n        password_min_length: 8,\n        session_timeout: 3600\n\n  \"\"\"\nend\n```\n\n### API Endpoint Documentation\n\nDocument RESTful APIs clearly:\n\n```markdown\n## Endpoints\n\n### Create User\n\nCreates a new user account.\n\n**Endpoint:** `POST /api/users`\n\n**Authentication:** Not required\n\n**Request Body:**\n\n```json\n{\n  \"email\": \"alice@example.com\",\n  \"password\": \"secure_password\",\n  \"name\": \"Alice Smith\"\n}\n```\n\n**Response (201 Created):**\n\n```json\n{\n  \"id\": \"123\",\n  \"email\": \"alice@example.com\",\n  \"name\": \"Alice Smith\",\n  \"created_at\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n**Error Responses:**\n\n- `400 Bad Request` - Invalid input\n  ```json\n  {\n    \"error\": \"validation_error\",\n    \"details\": {\n      \"email\": [\"must be a valid email address\"],\n      \"password\": [\"must be at least 8 characters\"]\n    }\n  }\n  ```\n\n- `409 Conflict` - Email already exists\n  ```json\n  {\n    \"error\": \"email_taken\",\n    \"message\": \"An account with this email already exists\"\n  }\n  ```\n\n**Example:**\n\n```bash\ncurl -X POST https://api.example.com/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"alice@example.com\",\n    \"password\": \"secure_password\",\n    \"name\": \"Alice Smith\"\n  }'\n```\n```\n\n## User Guides and Tutorials\n\n### Tutorial Structure\n\n```markdown\n# Tutorial: Building Your First [Feature]\n\n## What You'll Build\n\nBrief description of the end result.\n\n## Prerequisites\n\n- Knowledge requirement 1\n- Installed tool 1\n- Account/access requirement\n\n## Step 1: [First Major Step]\n\nExplanation of what we're doing and why.\n\n```language\n// Code for this step\n```\n\n**What's happening here:**\n- Explanation of key line 1\n- Explanation of key line 2\n\n## Step 2: [Next Step]\n\nContinue with incremental steps...\n\n## Testing\n\nHow to verify it works.\n\n## Next Steps\n\n- Related tutorial 1\n- Advanced topic 1\n- Further reading\n```\n\n### Tutorial Best Practices\n\n- **Show working code first**: Let readers see the goal before diving into details\n- **Explain the 'why'**: Don't just show what to do, explain reasoning\n- **Incremental steps**: Each step should build on the previous\n- **Include checkpoints**: Ways to verify progress\n- **Provide complete code**: Include a repository or final code snippet\n- **Anticipate problems**: Address common mistakes\n- **Link to references**: Point to relevant API docs and resources\n\n## Inline Code Documentation\n\n### When to Write Comments\n\n**DO write comments for:**\n- Complex algorithms or business logic\n- Non-obvious decisions (\"why\" not \"what\")\n- Workarounds for bugs or limitations\n- Public APIs and exported functions\n- Configuration and constants\n\n**DON'T write comments for:**\n- Obvious code\n- What the code does (prefer clear naming)\n- Outdated information\n- Commented-out code (use version control)\n\n### Good Comment Examples\n\n```elixir\n# Good: Explains WHY\n# Use exponential backoff to avoid overwhelming the API after rate limit errors\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Bad: Explains WHAT (obvious from code)\n# Multiply 2 to the power of attempt and multiply by 1000\ndefp retry_with_backoff(attempt) do\n  :timer.sleep(:math.pow(2, attempt) * 1000)\nend\n\n# Good: Documents workaround\n# NOTE: Using String.to_existing_atom because the Erlang VM limits atoms to ~1M.\n# All valid status atoms are pre-defined in this module.\ndef parse_status(status_string) do\n  String.to_existing_atom(status_string)\nend\n\n# Good: Explains business rule\n# Users must be at least 13 years old per COPPA regulations\n@minimum_age 13\n```\n\n## Architecture Documentation\n\n### Architecture Decision Records (ADR)\n\nDocument significant architectural decisions:\n\n```markdown\n# ADR 001: Use PostgreSQL for Primary Database\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to choose a database for our application that supports:\n- ACID transactions\n- Complex queries with joins\n- JSON data storage\n- Full-text search\n- Horizontal scalability (future requirement)\n\n## Decision\n\nWe will use PostgreSQL as our primary database.\n\n## Consequences\n\n### Positive\n\n- Mature, stable, well-documented\n- Excellent JSON support with JSONB\n- Built-in full-text search\n- Strong consistency guarantees\n- Large ecosystem of tools and extensions\n- Can scale with read replicas and partitioning\n\n### Negative\n\n- More complex to operate than simpler databases\n- Vertical scaling has limits (though sufficient for our needs)\n- Requires more server resources than lighter alternatives\n\n### Neutral\n\n- Team needs to learn PostgreSQL-specific features\n- May need to hire PostgreSQL expertise as we scale\n\n## Alternatives Considered\n\n- **MySQL**: Weaker JSON support, less feature-rich\n- **MongoDB**: No ACID guarantees, eventual consistency issues\n- **SQLite**: Not suitable for multi-user web applications\n```\n\n## Changelog Documentation\n\nFollow Keep a Changelog format:\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- New feature in development\n\n## [1.2.0] - 2024-01-15\n\n### Added\n- User profile pictures\n- Email notification preferences\n- Dark mode support\n\n### Changed\n- Improved search performance by 40%\n- Updated UI to match new brand guidelines\n\n### Fixed\n- Login redirect loop on Safari\n- Memory leak in background sync process\n\n### Deprecated\n- Old `/v1/users` endpoint (use `/v2/users` instead)\n\n## [1.1.0] - 2024-01-01\n\n### Added\n- Two-factor authentication\n- Export user data to JSON\n\n### Security\n- Fixed XSS vulnerability in comment rendering\n\n## [1.0.0] - 2023-12-15\n\n### Added\n- Initial release\n- User registration and authentication\n- Basic user profiles\n```\n\n## Documentation Tools\n\n### Documentation Generators\n\n- **Elixir**: ExDoc - `mix docs`\n- **JavaScript**: JSDoc, TypeDoc\n- **Python**: Sphinx, MkDocs\n- **Rust**: rustdoc - `cargo doc`\n- **Static sites**: VitePress, Docusaurus, GitBook\n\n### Diagram Tools\n\n- **Mermaid**: Diagrams in Markdown\n  ```markdown\n  ```mermaid\n  graph TD\n      A[User] -->|Requests| B[Load Balancer]\n      B --> C[Web Server 1]\n      B --> D[Web Server 2]\n      C --> E[Database]\n      D --> E\n  ```\n  ```\n\n- **PlantUML**: UML diagrams as code\n- **Excalidraw**: Hand-drawn style diagrams\n- **Draw.io**: Flowcharts and diagrams\n\n## Documentation Style Guide\n\n### Writing Style\n\n- **Use active voice**: \"The function returns\" not \"The value is returned\"\n- **Be concise**: Remove unnecessary words\n- **Use present tense**: \"Returns\" not \"Will return\"\n- **Be specific**: \"Timeout in milliseconds\" not \"Timeout value\"\n- **Avoid jargon**: Or explain it when necessary\n- **Use examples**: Show, don't just tell\n\n### Formatting Conventions\n\n- **Code**: Use `backticks` for inline code\n- **Commands**: Show with `$` prefix or in code blocks\n- **File paths**: Use `code formatting`\n- **Emphasis**: Use **bold** for important points, *italic* for slight emphasis\n- **Lists**: Use bullets for unordered, numbers for sequential steps\n- **Headers**: Use sentence case, not title case\n\n### Code Examples\n\n- **Complete**: Include all necessary imports and setup\n- **Runnable**: Readers should be able to copy and run\n- **Realistic**: Use meaningful variable names and realistic data\n- **Commented**: Explain non-obvious parts\n- **Tested**: Ensure examples actually work\n- **Current**: Keep in sync with latest API\n\n## Documentation Maintenance\n\n### Keeping Docs Updated\n\n- Update documentation in the same PR as code changes\n- Review docs during code review\n- Set up doc linting (broken links, outdated examples)\n- Schedule regular documentation audits\n- Use version tags in examples when API changes\n- Mark deprecated features clearly\n\n### Documentation Testing\n\n```elixir\n# Elixir doctests - examples in docs are actual tests\ndefmodule Math do\n  @doc \"\"\"\n  Adds two numbers.\n\n  ## Examples\n\n      iex> Math.add(2, 3)\n      5\n\n  \"\"\"\n  def add(a, b), do: a + b\nend\n```\n\n```rust\n/// Adds two numbers.\n///\n/// # Examples\n///\n/// ```\n/// assert_eq!(add(2, 3), 5);\n/// ```\npub fn add(a: i32, b: i32) -> i32 {\n    a + b\n}\n```\n\n## Key Principles\n\n- **Write for your audience**: Tailor complexity to reader's experience level\n- **Show examples**: Code examples are worth a thousand words\n- **Keep it current**: Outdated docs are worse than no docs\n- **Make it scannable**: Use headers, lists, code blocks, and white space\n- **Explain the 'why'**: Help readers understand reasoning, not just steps\n- **Start simple**: Begin with quickstart, then go deeper\n- **Test documentation**: Ensure examples run and links work\n- **Iterate based on feedback**: Improve based on user questions and confusion"
              },
              {
                "name": "git-operations",
                "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits",
                "path": "core/skills/git/SKILL.md",
                "frontmatter": {
                  "name": "git-operations",
                  "description": "Guide for Git operations including commits, branches, rebasing, conflict resolution, and following Git best practices and conventional commits"
                },
                "content": "# Git Operations and Best Practices\n\nThis skill activates when performing Git operations, managing repositories, resolving conflicts, or following Git workflows and conventions.\n\n## When to Use This Skill\n\nActivate when:\n- Creating commits or commit messages\n- Managing branches and merging\n- Resolving merge conflicts\n- Rebasing or rewriting history\n- Creating pull requests\n- Following Git workflows (Git Flow, GitHub Flow, trunk-based)\n- Troubleshooting Git issues\n\n## Commit Message Conventions\n\n### Conventional Commits\n\nFollow the Conventional Commits specification:\n\n```\n<type>(<scope>): <subject>\n\n<body>\n\n<footer>\n```\n\n**Types:**\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation changes\n- `style`: Code style changes (formatting, missing semicolons, etc.)\n- `refactor`: Code refactoring without changing behavior\n- `perf`: Performance improvements\n- `test`: Adding or updating tests\n- `build`: Changes to build system or dependencies\n- `ci`: CI/CD configuration changes\n- `chore`: Other changes that don't modify src or test files\n- `revert`: Revert a previous commit\n\n**Examples:**\n```\nfeat(auth): add JWT authentication\n\nImplement JWT-based authentication with refresh tokens.\n- Add JWT generation and validation\n- Implement refresh token rotation\n- Add authentication middleware\n\nCloses #123\n\nfix(api): handle null values in user response\n\nPreviously, null email addresses would cause the API to crash.\nNow returns empty string for null emails.\n\nFixes #456\n\ndocs: update installation instructions\n\nAdd section on environment variable configuration.\n\ntest(user): add tests for email validation\n\nrefactor(database): simplify query builder\n\nperf(api): add caching for user endpoints\n\nReduces response time by 40% for user list endpoint.\n```\n\n### Writing Good Commit Messages\n\n**Subject line (first line):**\n- Keep under 50 characters\n- Start with lowercase (after type)\n- No period at the end\n- Use imperative mood (\"add\" not \"added\" or \"adds\")\n\n**Body:**\n- Wrap at 72 characters\n- Explain what and why, not how\n- Separate from subject with blank line\n- Can have multiple paragraphs\n\n**Footer:**\n- Reference issues and pull requests\n- Note breaking changes\n- Add co-authors\n\n```\nfeat(api): add user search endpoint\n\nImplement full-text search across user names and emails using\nPostgreSQL's full-text search capabilities. Search results are\nranked by relevance.\n\nPerformance tested with 1M users - average response time < 100ms.\n\nBREAKING CHANGE: API now requires PostgreSQL 12+\n\nCloses #789\nCo-authored-by: Jane Doe <jane@example.com>\n```\n\n## Branch Management\n\n### Branch Naming\n\nUse descriptive, hierarchical branch names:\n\n```\n<type>/<short-description>\n<type>/<issue-number>-<short-description>\n```\n\n**Examples:**\n```\nfeature/user-authentication\nfeature/123-add-search\nfix/456-null-pointer-error\nbugfix/password-reset-email\nhotfix/critical-security-patch\nrefactor/database-queries\ndocs/api-documentation\nchore/update-dependencies\n```\n\n### Creating and Switching Branches\n\n```bash\n# Create and switch to new branch\ngit checkout -b feature/new-feature\n\n# Switch to existing branch\ngit checkout main\ngit switch main  # Modern alternative\n\n# Create branch from specific commit\ngit checkout -b hotfix/bug origin/main\n\n# List branches\ngit branch                    # Local branches\ngit branch -r                 # Remote branches\ngit branch -a                 # All branches\ngit branch -v                 # With last commit\n```\n\n### Deleting Branches\n\n```bash\n# Delete local branch\ngit branch -d feature/completed-feature\n\n# Force delete unmerged branch\ngit branch -D feature/abandoned-feature\n\n# Delete remote branch\ngit push origin --delete feature/old-feature\n```\n\n## Working with Changes\n\n### Staging Changes\n\n```bash\n# Stage specific files\ngit add file1.ex file2.ex\n\n# Stage all changes\ngit add .\ngit add -A\n\n# Stage parts of a file (interactive)\ngit add -p file.ex\n\n# Unstage files\ngit restore --staged file.ex\ngit reset HEAD file.ex  # Old syntax\n```\n\n### Committing\n\n```bash\n# Commit staged changes\ngit commit -m \"feat: add user authentication\"\n\n# Commit with body\ngit commit -m \"feat: add user authentication\" -m \"Implement JWT-based auth with refresh tokens\"\n\n# Amend last commit (change message or add files)\ngit add forgotten-file.ex\ngit commit --amend\n\n# Amend without changing message\ngit commit --amend --no-edit\n```\n\n### Viewing Changes\n\n```bash\n# Show unstaged changes\ngit diff\n\n# Show staged changes\ngit diff --cached\ngit diff --staged\n\n# Show changes in specific file\ngit diff path/to/file.ex\n\n# Show changes between branches\ngit diff main..feature/new-feature\n\n# Show changes between commits\ngit diff abc123..def456\n\n# Show stats only\ngit diff --stat\n```\n\n## Branching Workflows\n\n### Feature Branch Workflow\n\n```bash\n# Start new feature\ngit checkout main\ngit pull origin main\ngit checkout -b feature/new-feature\n\n# Work on feature\ngit add .\ngit commit -m \"feat: implement new feature\"\n\n# Keep feature updated with main\ngit checkout main\ngit pull origin main\ngit checkout feature/new-feature\ngit merge main\n\n# Push feature\ngit push -u origin feature/new-feature\n\n# After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n```\n\n### Rebasing Feature Branch\n\n```bash\n# Keep feature branch up-to-date with clean history\ngit checkout feature/new-feature\ngit fetch origin\ngit rebase origin/main\n\n# If conflicts occur, resolve them, then:\ngit add resolved-file.ex\ngit rebase --continue\n\n# Abort rebase if needed\ngit rebase --abort\n\n# Force push after rebase (careful!)\ngit push --force-with-lease origin feature/new-feature\n```\n\n## Merge Strategies\n\n### Fast-Forward Merge\n\n```bash\n# Default when possible - no merge commit\ngit checkout main\ngit merge feature/simple-feature\n```\n\n### No Fast-Forward\n\n```bash\n# Always create merge commit for history\ngit merge --no-ff feature/important-feature\n```\n\n### Squash Merge\n\n```bash\n# Combine all feature commits into one\ngit merge --squash feature/many-small-commits\ngit commit -m \"feat: add complete feature\"\n```\n\n## Conflict Resolution\n\n### Identifying Conflicts\n\n```bash\n# See conflicted files\ngit status\n\n# See conflict markers in file\n# <<<<<<< HEAD\n# Current branch changes\n# =======\n# Incoming changes\n# >>>>>>> feature/branch\n```\n\n### Resolving Conflicts\n\n```bash\n# Edit files to resolve conflicts, then:\ngit add resolved-file.ex\ngit commit  # Or git rebase --continue if rebasing\n\n# Use merge tools\ngit mergetool\n\n# Choose one side completely\ngit checkout --ours file.ex    # Keep our version\ngit checkout --theirs file.ex  # Keep their version\n```\n\n### Aborting Merge/Rebase\n\n```bash\n# Abort merge\ngit merge --abort\n\n# Abort rebase\ngit rebase --abort\n```\n\n## History Management\n\n### Interactive Rebase\n\nClean up commit history before merging:\n\n```bash\n# Rebase last 3 commits\ngit rebase -i HEAD~3\n\n# Rebase since main\ngit rebase -i main\n\n# Interactive rebase options:\n# pick - keep commit as-is\n# reword - change commit message\n# edit - modify commit\n# squash - combine with previous commit\n# fixup - like squash but discard message\n# drop - remove commit\n```\n\n**Example workflow:**\n```bash\n# You have commits:\n# abc123 fix typo\n# def456 add feature\n# ghi789 fix bug in feature\n# jkl012 add tests\n\ngit rebase -i HEAD~4\n\n# Change to:\n# pick def456 add feature\n# fixup ghi789 fix bug in feature\n# squash jkl012 add tests\n# reword abc123 fix typo\n```\n\n### Viewing History\n\n```bash\n# View commit history\ngit log\n\n# Compact one-line format\ngit log --oneline\n\n# Graph view\ngit log --graph --oneline --all\n\n# With file changes\ngit log --stat\n\n# Search commits\ngit log --grep=\"authentication\"\n\n# Commits by author\ngit log --author=\"John\"\n\n# Commits in date range\ngit log --since=\"2 weeks ago\"\ngit log --after=\"2024-01-01\" --before=\"2024-02-01\"\n\n# Follow file history\ngit log --follow -- path/to/file.ex\n\n# Show specific commit\ngit show abc123\n```\n\n### Undoing Changes\n\n```bash\n# Undo uncommitted changes\ngit restore file.ex\ngit checkout -- file.ex  # Old syntax\n\n# Restore all files\ngit restore .\n\n# Undo commit (keep changes)\ngit reset --soft HEAD~1\n\n# Undo commit (discard changes) - DANGEROUS\ngit reset --hard HEAD~1\n\n# Create new commit that undoes a commit\ngit revert abc123\n\n# Revert merge commit\ngit revert -m 1 abc123\n```\n\n## Stashing\n\nTemporarily save uncommitted changes:\n\n```bash\n# Stash changes\ngit stash\ngit stash push -m \"work in progress on feature\"\n\n# Stash including untracked files\ngit stash -u\n\n# List stashes\ngit stash list\n\n# Apply most recent stash\ngit stash apply\n\n# Apply and remove stash\ngit stash pop\n\n# Apply specific stash\ngit stash apply stash@{2}\n\n# Delete stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n# Create branch from stash\ngit stash branch feature/from-stash\n```\n\n## Remote Operations\n\n### Working with Remotes\n\n```bash\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add origin git@github.com:user/repo.git\n\n# Change remote URL\ngit remote set-url origin git@github.com:user/new-repo.git\n\n# Remove remote\ngit remote remove origin\n\n# Rename remote\ngit remote rename origin upstream\n```\n\n### Fetching and Pulling\n\n```bash\n# Fetch changes from remote\ngit fetch origin\n\n# Fetch all remotes\ngit fetch --all\n\n# Pull changes (fetch + merge)\ngit pull origin main\n\n# Pull with rebase\ngit pull --rebase origin main\n\n# Set upstream branch\ngit push -u origin feature/new-feature\ngit branch --set-upstream-to=origin/feature feature/new-feature\n```\n\n### Pushing\n\n```bash\n# Push to remote\ngit push origin main\n\n# Push and set upstream\ngit push -u origin feature/new-feature\n\n# Force push (CAREFUL!)\ngit push --force origin feature/branch\n\n# Safer force push - fails if remote has new commits\ngit push --force-with-lease origin feature/branch\n\n# Push all branches\ngit push --all origin\n\n# Push tags\ngit push --tags\n```\n\n## Tags\n\n### Creating Tags\n\n```bash\n# Lightweight tag\ngit tag v1.0.0\n\n# Annotated tag (preferred)\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\n\n# Tag specific commit\ngit tag -a v1.0.0 abc123 -m \"Release version 1.0.0\"\n```\n\n### Managing Tags\n\n```bash\n# List tags\ngit tag\ngit tag -l \"v1.*\"\n\n# View tag details\ngit show v1.0.0\n\n# Push tag\ngit push origin v1.0.0\n\n# Push all tags\ngit push origin --tags\n\n# Delete local tag\ngit tag -d v1.0.0\n\n# Delete remote tag\ngit push origin --delete v1.0.0\n```\n\n## Advanced Operations\n\n### Cherry-Picking\n\nApply specific commits to current branch:\n\n```bash\n# Apply single commit\ngit cherry-pick abc123\n\n# Apply multiple commits\ngit cherry-pick abc123 def456\n\n# Cherry-pick without committing\ngit cherry-pick -n abc123\n```\n\n### Bisect\n\nFind which commit introduced a bug:\n\n```bash\n# Start bisect\ngit bisect start\ngit bisect bad                    # Current commit is bad\ngit bisect good abc123            # Known good commit\n\n# Git will checkout commits to test\n# After testing each:\ngit bisect good  # or\ngit bisect bad\n\n# When found, Git shows first bad commit\n# Reset\ngit bisect reset\n```\n\n### Submodules\n\n```bash\n# Add submodule\ngit submodule add git@github.com:user/repo.git path/to/submodule\n\n# Clone with submodules\ngit clone --recurse-submodules git@github.com:user/repo.git\n\n# Update submodules\ngit submodule update --init --recursive\n\n# Pull submodule updates\ngit submodule update --remote\n```\n\n## GitHub Specific\n\n### Pull Requests\n\n```bash\n# Using GitHub CLI (gh)\ngh pr create --title \"feat: add new feature\" --body \"Description of changes\"\n\n# Create draft PR\ngh pr create --draft\n\n# List PRs\ngh pr list\n\n# View PR\ngh pr view 123\n\n# Checkout PR locally\ngh pr checkout 123\n\n# Merge PR\ngh pr merge 123 --squash\n```\n\n### Issues\n\n```bash\n# Create issue\ngh issue create --title \"Bug: authentication fails\" --body \"Description\"\n\n# List issues\ngh issue list\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n```\n\n## Git Aliases\n\nAdd to `.gitconfig`:\n\n```ini\n[alias]\n    co = checkout\n    br = branch\n    ci = commit\n    st = status\n    unstage = restore --staged\n    last = log -1 HEAD\n    lg = log --graph --oneline --all\n    cm = commit -m\n    ca = commit --amend\n    undo = reset --soft HEAD~1\n    sync = !git fetch origin && git rebase origin/main\n    clean-branches = !git branch --merged | grep -v \\\"\\\\*\\\" | xargs -n 1 git branch -d\n```\n\n## Best Practices\n\n### Commits\n\n- Make atomic commits - one logical change per commit\n- Commit often - small, focused commits are better\n- Write clear commit messages following conventions\n- Don't commit sensitive data (API keys, passwords)\n- Don't commit generated files (add to `.gitignore`)\n- Test before committing\n\n### Branches\n\n- Keep branches short-lived\n- Pull main/master frequently to stay updated\n- Delete branches after merging\n- Use descriptive branch names\n- One feature/fix per branch\n\n### History\n\n- Keep history clean with interactive rebase\n- Don't rewrite public history (after pushing)\n- Use `--force-with-lease` instead of `--force`\n- Squash small fixup commits before merging\n\n### Collaboration\n\n- Pull before pushing\n- Resolve conflicts promptly\n- Review changes before committing\n- Communicate about force pushes\n- Use pull requests for code review\n\n## Common Issues and Solutions\n\n### Accidentally Committed to Wrong Branch\n\n```bash\n# Move commit to new branch\ngit branch feature/new-branch\ngit reset --hard HEAD~1\ngit checkout feature/new-branch\n```\n\n### Need to Change Last Commit Message\n\n```bash\ngit commit --amend\n```\n\n### Committed Sensitive Data\n\n```bash\n# Remove from history - CAREFUL!\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/sensitive-file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (faster)\nbfg --delete-files sensitive-file\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n\n# Force push to update remote\ngit push --force --all\n```\n\n### Recover Deleted Branch\n\n```bash\n# Find commit where branch was\ngit reflog\n\n# Recreate branch\ngit checkout -b recovered-branch abc123\n```\n\n### Merge Went Wrong\n\n```bash\n# Undo merge (before pushing)\ngit reset --hard HEAD~1\n\n# Undo merge (after pushing)\ngit revert -m 1 merge-commit-hash\n```\n\n## Key Principles\n\n- **Commit early, commit often**: Small, focused commits\n- **Write clear messages**: Follow conventional commits\n- **Keep history clean**: Rebase and squash before merging\n- **Don't rewrite public history**: Only rebase local commits\n- **Use branches**: Never commit directly to main\n- **Pull before push**: Stay in sync with remote\n- **Review before commit**: Check what you're committing\n- **Use descriptive names**: For branches, commits, and PRs"
              },
              {
                "name": "material-design",
                "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming",
                "path": "core/skills/material-design/SKILL.md",
                "frontmatter": {
                  "name": "material-design",
                  "description": "Guide for implementing Material Design 3 (Material You) principles for Android, web, and cross-platform applications with dynamic theming"
                },
                "content": "# Material Design 3 (Material You)\n\nApply Google's Material Design 3 principles when designing and developing user interfaces with emphasis on personalization, accessibility, and cross-platform consistency.\n\n## When to Activate\n\nUse this skill when:\n- Designing or implementing Android applications\n- Building web applications following Material Design\n- Working with Flutter or Jetpack Compose\n- Implementing dynamic theming and color systems\n- Creating Material components\n- Reviewing designs for Material Design compliance\n\n## What is Material Design 3?\n\nMaterial Design 3 (Material You) represents Google's latest design system with:\n\n- **Personalization**: Dynamic color extraction from user preferences\n- **Expressiveness**: Softer, rounded components with visual hierarchy\n- **Adaptability**: Responsive across devices and platforms\n- **Accessibility**: Built-in inclusive design features\n\n## Key Differences from Material Design 2\n\n| Aspect | MD2 | MD3 |\n|--------|-----|-----|\n| **Colors** | Fixed brand palettes | Dynamic, user-generated schemes |\n| **Customization** | Limited theming | Highly personalized |\n| **Components** | Flat, rigid shapes | Rounded, expressive |\n| **Accessibility** | Basic support | Priority built-in |\n\n## Core Foundations\n\n### 1. Dynamic Color System\n\nMaterial Design 3 uses HCT (Hue, Chroma, Tone) color space for perceptually accurate color generation.\n\n**Key concepts:**\n- Color roles (primary, secondary, tertiary, error, neutral)\n- Tonal palettes (50-99 tones per color)\n- Automatic light/dark theme generation\n- User-driven personalization from wallpaper/system\n\nFor detailed color system implementation, see `references/color-system.md`.\n\n### 2. Typography\n\nType scale with 5 display sizes and 9 text sizes:\n\n**Quick example:**\n- Display Large: 57sp\n- Headline Large: 32sp\n- Body Large: 16sp\n- Label Small: 11sp\n\nFor complete typography system and responsive scaling, see `references/typography.md`.\n\n### 3. Layout\n\nResponsive breakpoints and grid system:\n\n- **Compact**: 0-599dp (phones)\n- **Medium**: 600-839dp (tablets, folded phones)\n- **Expanded**: 840dp+ (desktops, large tablets)\n\nFor layout guidelines and examples, see `references/layout.md`.\n\n## Component Guidelines\n\nMaterial Design 3 provides specifications for:\n\n- **Common Buttons**: Elevated, Filled, Tonal, Outlined, Text\n- **Cards**: Elevated, Filled, Outlined variants\n- **Text Fields**: Filled, Outlined with labels and helper text\n- **Navigation**: Navigation bar, rail, drawer\n- **Chips**: Assist, Filter, Input, Suggestion chips\n- **Dialogs**: Basic, Full-screen dialogs\n\nFor detailed component specifications, consult `references/components.md`.\n\n## Quick Component Examples\n\n### Buttons\n\n```kotlin\n// Jetpack Compose\nButton(onClick = { }) {\n    Text(\"Filled Button\")\n}\n\nOutlinedButton(onClick = { }) {\n    Text(\"Outlined Button\")\n}\n```\n\n### Cards\n\n```kotlin\nCard(\n    modifier = Modifier.fillMaxWidth(),\n    elevation = CardDefaults.cardElevation(defaultElevation = 6.dp)\n) {\n    Column(modifier = Modifier.padding(16.dp)) {\n        Text(\"Card Title\", style = MaterialTheme.typography.headlineSmall)\n        Text(\"Card content\", style = MaterialTheme.typography.bodyMedium)\n    }\n}\n```\n\n### Text Fields\n\n```kotlin\nOutlinedTextField(\n    value = text,\n    onValueChange = { text = it },\n    label = { Text(\"Label\") },\n    supportingText = { Text(\"Helper text\") }\n)\n```\n\nFor more component examples and patterns, see `references/components.md`.\n\n## Implementing Dynamic Color\n\n### Android (Jetpack Compose)\n\n```kotlin\nval dynamicColor = Build.VERSION.SDK_INT >= Build.VERSION_CODES.S\n\nval colorScheme = when {\n    dynamicColor && darkTheme -> dynamicDarkColorScheme(LocalContext.current)\n    dynamicColor && !darkTheme -> dynamicLightColorScheme(LocalContext.current)\n    darkTheme -> darkColorScheme()\n    else -> lightColorScheme()\n}\n\nMaterialTheme(\n    colorScheme = colorScheme,\n    typography = Typography,\n    content = content\n)\n```\n\n### Web\n\nFor web implementation with Material Web Components, see `references/web-implementation.md`.\n\n## Motion and Animation\n\nMaterial Design 3 motion principles:\n- **Easing**: Standard, emphasized, decelerated curves\n- **Duration**: Based on travel distance and complexity\n- **Choreography**: Coordinated element movements\n\nFor motion specifications, see `references/motion.md`.\n\n## Accessibility\n\nMaterial Design 3 prioritizes accessibility:\n- Minimum 4.5:1 contrast ratio (text)\n- 3:1 contrast ratio (UI components)\n- Touch targets minimum 48dp × 48dp\n- Screen reader support\n- Semantic color usage (not color-only indicators)\n\nFor accessibility implementation details, see `references/accessibility.md`.\n\n## When to Consult References\n\n- **Color system implementation**: Read `references/color-system.md`\n- **Typography scales and usage**: Read `references/typography.md`\n- **Layout and responsive design**: Read `references/layout.md`\n- **Component specifications**: Read `references/components.md`\n- **Web implementation**: Read `references/web-implementation.md`\n- **Motion and animation**: Read `references/motion.md`\n- **Accessibility guidelines**: Read `references/accessibility.md`\n\n## Key Principles\n\n- **User-driven personalization**: Colors adapt to user preferences\n- **Expressive and flexible**: Rounded corners, dynamic elevation\n- **Accessible by default**: Built-in contrast, touch targets, semantics\n- **Cross-platform consistency**: Same principles across Android, web, iOS\n- **Design tokens**: Use semantic tokens, not hardcoded values\n- **Responsive**: Adapt to device size and orientation\n\n## Resources\n\n- **Material Design 3**: https://m3.material.io/\n- **Material Theme Builder**: https://m3.material.io/theme-builder\n- **Jetpack Compose**: https://developer.android.com/jetpack/compose/designsystems/material3\n- **Material Web Components**: https://github.com/material-components/material-web\n- **Flutter Material 3**: https://flutter.dev/docs/development/ui/material"
              },
              {
                "name": "mise",
                "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects",
                "path": "core/skills/mise/SKILL.md",
                "frontmatter": {
                  "name": "mise",
                  "description": "Guide for using mise (mise-en-place) to manage development tools, runtime versions, environment variables, and tasks across projects"
                },
                "content": "# mise - Development Environment Management\n\nThis skill activates when working with mise for managing tool versions, environment variables, and project tasks.\n\n## When to Use This Skill\n\nActivate when:\n- Setting up development environments\n- Managing tool and runtime versions (Node.js, Python, Ruby, Go, etc.)\n- Configuring environment variables and secrets\n- Defining and running project tasks\n- Creating reproducible development setups\n- Working with monorepos or multiple projects\n\n## What is mise?\n\nmise is a polyglot runtime manager and development environment tool that combines:\n- **Tool version management** - Install and manage multiple versions of dev tools\n- **Environment configuration** - Set environment variables per project\n- **Task automation** - Define and run project tasks\n- **Cross-platform** - Works on macOS, Linux, and Windows\n\n## Installation\n\n```bash\n# macOS/Linux (using curl)\ncurl https://mise.run | sh\n\n# macOS (using Homebrew)\nbrew install mise\n\n# Windows\n# See https://mise.jdx.dev for Windows install instructions\n\n# Activate mise in your shell\necho 'eval \"$(mise activate bash)\"' >> ~/.bashrc   # bash\necho 'eval \"$(mise activate zsh)\"' >> ~/.zshrc     # zsh\necho 'mise activate fish | source' >> ~/.config/fish/config.fish  # fish\n```\n\n## Managing Tools\n\n### Tool Backends\n\nmise uses different backends (package managers) to install tools. Understanding backends helps you install tools correctly.\n\n#### Available Backends\n\n- **asdf** - Traditional asdf plugins (default for many tools)\n- **ubi** - Universal Binary Installer (GitHub/GitLab releases)\n- **cargo** - Rust packages (requires Rust installed)\n- **npm** - Node.js packages (requires Node installed)\n- **go** - Go packages (requires Go installed)\n- **aqua** - Package manager\n- **pipx** - Python packages (requires Python installed)\n- **gem** - Ruby packages (requires Ruby installed)\n- **github/gitlab** - Direct from repositories\n- **http** - Direct HTTP downloads\n\n#### Verifying Tool Names\n\nAlways verify tool names using `mise ls-remote` before adding to configuration:\n\n```bash\n# Check if tool exists in registry\nmise ls-remote node\n\n# Check tool with specific backend\nmise ls-remote cargo:ripgrep\nmise ls-remote ubi:sharkdp/fd\n\n# Search the registry\nmise registry | grep <tool-name>\n```\n\n### Installing Tools\n\n```bash\n# List available tools in registry\nmise registry\n\n# Install from default backend\nmise install node@20.10.0\nmise install python@3.12\nmise install ruby@3.3\n\n# Install with specific backend\nmise install cargo:ripgrep        # From Rust crates\nmise install ubi:sharkdp/fd       # From GitHub releases\nmise install npm:typescript       # From npm\n\n# Install latest version\nmise install node@latest\n\n# Install from .mise.toml or .tool-versions\nmise install\n```\n\n### Using Tools with `mise use`\n\nThe `mise use` command is the primary way to add tools to projects. It combines two operations:\n1. **Installs** the tool (if not already installed)\n2. **Adds** the tool to your configuration file\n\n**Key Difference**: `mise install` only installs tools, while `mise use` installs AND configures them.\n\n#### Basic Usage\n\n```bash\n# Interactive selection\nmise use\n\n# Add tool with fuzzy version (default)\nmise use node@20              # Saves as \"20\" in mise.toml\n\n# Add tool with exact version\nmise use --pin node@20.10.0   # Saves as \"20.10.0\"\n\n# Add latest version\nmise use node@latest          # Saves as \"latest\"\n\n# Add with specific backend\nmise use cargo:ripgrep@latest\nmise use ubi:sharkdp/fd\n```\n\n#### Configuration File Selection\n\n`mise use` writes to configuration files in this priority order:\n\n1. **`--global` flag**: `~/.config/mise/config.toml`\n2. **`--path <file>` flag**: Specified file path\n3. **`--env <env>` flag**: `.mise.<env>.toml`\n4. **Default**: `mise.toml` in current directory\n\n```bash\n# Global (all projects)\nmise use --global node@20\n\n# Local (current project)\nmise use node@20              # Creates/updates ./mise.toml\n\n# Environment-specific\nmise use --env local node@20  # Creates .mise.local.toml\n\n# Specific file\nmise use --path ~/.config/mise/custom.toml node@20\n```\n\n#### Important Flags\n\n```bash\n# Pin exact version\nmise use --pin node@20.10.0        # Saves \"20.10.0\"\n\n# Fuzzy version (default)\nmise use --fuzzy node@20           # Saves \"20\"\n\n# Force reinstall\nmise use --force node@20\n\n# Dry run (preview changes)\nmise use --dry-run node@20\n\n# Remove tool from config\nmise use --remove node\n```\n\n#### Version Pinning\n\n```bash\n# Fuzzy (recommended) - auto-updates within major version\nmise use node@20                   # Uses latest 20.x.x\n\n# Exact - locks to specific version\nmise use --pin node@20.10.0        # Always uses 20.10.0\n\n# Latest - always uses newest version\nmise use node@latest               # Always updates to latest\n```\n\n**Best Practice**: Use fuzzy versions for flexibility, `mise.lock` for reproducibility.\n\n### Setting Tool Versions\n\nThe `mise use` command automatically sets tool versions by updating configuration files.\n\n#### .mise.toml Configuration\n\n```toml\n[tools]\nnode = \"20.10.0\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\n\n# Use latest version\nterraform = \"latest\"\n\n# Backends - use quotes for namespaced tools\n\"cargo:ripgrep\" = \"latest\"        # Requires rust installed\n\"ubi:sharkdp/fd\" = \"latest\"       # GitHub releases\n\"npm:typescript\" = \"latest\"       # Requires node installed\n\n# Version from file\nnode = { version = \"lts\", resolve = \"latest-lts\" }\n```\n\n### UBI Backend (Universal Binary Installer)\n\nThe **ubi** backend installs tools directly from GitHub/GitLab releases without requiring plugins. It's built into mise and works cross-platform including Windows.\n\n#### Basic UBI Usage\n\n```bash\n# Install from GitHub releases\nmise use -g ubi:goreleaser/goreleaser\nmise use -g ubi:sharkdp/fd\nmise use -g ubi:BurntSushi/ripgrep\n\n# Specific version\nmise use -g ubi:goreleaser/goreleaser@1.25.1\n\n# In .mise.toml\n[tools]\n\"ubi:goreleaser/goreleaser\" = \"latest\"\n\"ubi:sharkdp/fd\" = \"2.0.0\"\n```\n\n#### UBI Advanced Options\n\nConfigure tool-specific options when binary names differ or filtering is needed:\n\n```toml\n[tools]\n# When executable name differs from repo name\n\"ubi:BurntSushi/ripgrep\" = { version = \"latest\", exe = \"rg\" }\n\n# Filter releases with matching pattern\n\"ubi:some/tool\" = { version = \"latest\", matching = \"linux-gnu\" }\n\n# Use regex for complex filtering\n\"ubi:some/tool\" = { version = \"latest\", matching_regex = \".*-linux-.*\\\\.tar\\\\.gz$\" }\n\n# Extract entire tarball\n\"ubi:some/tool\" = { version = \"latest\", extract_all = true }\n\n# Rename extracted executable\n\"ubi:some/tool\" = { version = \"latest\", rename_exe = \"my-tool\" }\n```\n\n#### UBI Supported Syntax\n\nThree installation formats:\n- **GitHub shorthand (latest)**: `ubi:owner/repo`\n- **GitHub shorthand (version)**: `ubi:owner/repo@1.2.3`\n- **Direct URL**: `ubi:https://github.com/owner/repo/releases/download/v1.2.3/...`\n\n### Cargo Backend\n\nThe **cargo** backend installs Rust packages from crates.io. **Requires Rust to be installed first.**\n\n#### Cargo Prerequisites\n\nInstall Rust before using cargo backend:\n\n```bash\n# Option 1: Install Rust via mise\nmise use -g rust\n\n# Option 2: Install Rust directly\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n#### Cargo Usage\n\n```bash\n# Install from crates.io\nmise use -g cargo:ripgrep\nmise use -g cargo:eza\nmise use -g cargo:bat\n\n# In .mise.toml - requires rust installed first\n[tools]\nrust = \"latest\"              # Install rust first\n\"cargo:ripgrep\" = \"latest\"   # Then cargo tools\n\"cargo:eza\" = \"latest\"\n\"cargo:bat\" = \"latest\"\n```\n\n#### Cargo from Git Repositories\n\n```bash\n# Specific tag\nmise use cargo:https://github.com/username/demo@tag:v1.0.0\n\n# Branch\nmise use cargo:https://github.com/username/demo@branch:main\n\n# Commit hash\nmise use cargo:https://github.com/username/demo@rev:abc123\n```\n\n#### Cargo Settings\n\nConfigure cargo behavior globally:\n\n```toml\n[settings]\n# Use cargo-binstall for faster installs (default: true)\ncargo.binstall = true\n\n# Use alternative cargo registry\ncargo.registry_name = \"my-registry\"\n```\n\n### Managing Installed Tools\n\n```bash\n# List installed tools\nmise list\n\n# List all versions of a tool\nmise list node\n\n# Uninstall a version\nmise uninstall node@18.0.0\n\n# Update all tools to latest\nmise upgrade\n\n# Update specific tool\nmise upgrade node\n```\n\n### Tool Aliases\n\n```bash\n# Create alias for a tool\nmise alias node 20 20.10.0\n\n# Use alias\nmise use node@20\n```\n\n## Environment Variables\n\n### Setting Environment Variables\n\n#### In .mise.toml\n\n```toml\n[env]\nDATABASE_URL = \"postgresql://localhost/myapp\"\nAPI_KEY = \"development-key\"\nNODE_ENV = \"development\"\n\n# Template values\nAPP_ROOT = \"{{ config_root }}\"\nDATA_DIR = \"{{ config_root }}/data\"\n```\n\n#### File-based env vars\n\n```toml\n[env]\n_.file = \".env\"\n_.path = [\"/custom/bin\"]\n```\n\n### Environment Templates\n\nUse Go templates in environment variables:\n\n```toml\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nLOG_FILE = \"{{ config_root }}/logs/app.log\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n```\n\n### Secrets Management\n\n```bash\n# Use with sops\nmise set SECRET_KEY sops://path/to/secret\n\n# Use with age\nmise set API_TOKEN age://path/to/secret\n\n# Use from command\nmise set BUILD_ID \"$(git rev-parse HEAD)\"\n```\n\n## Tasks\n\n### Defining Tasks\n\n#### In .mise.toml\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\n\n[tasks.test]\ndescription = \"Run tests\"\nrun = \"npm test\"\n\n[tasks.lint]\ndescription = \"Run linter\"\nrun = \"npm run lint\"\ndepends = [\"build\"]\n\n[tasks.ci]\ndescription = \"Run CI pipeline\"\ndepends = [\"lint\", \"test\"]\n\n[tasks.dev]\ndescription = \"Start development server\"\nrun = \"npm run dev\"\n```\n\n### Running Tasks\n\n```bash\n# Run a task\nmise run build\nmise run test\n\n# Short form\nmise build\nmise test\n\n# Run multiple tasks\nmise run lint test\n\n# List available tasks\nmise tasks\n\n# Run task with arguments\nmise run script -- arg1 arg2\n```\n\n### Task Dependencies\n\n```toml\n[tasks.deploy]\ndepends = [\"build\", \"test\"]\nrun = \"npm run deploy\"\n\n# Tasks run in order: build, test, then deploy\n```\n\n### Task Options\n\n```toml\n[tasks.build]\ndescription = \"Build the project\"\nrun = \"npm run build\"\nsources = [\"src/**/*.ts\"]      # Only run if sources changed\noutputs = [\"dist/**/*\"]         # Check outputs for changes\ndir = \"frontend\"                # Run in specific directory\nenv = { NODE_ENV = \"production\" }\n\n[tasks.watch]\nrun = \"npm run watch\"\nraw = true                      # Don't wrap in shell\n```\n\n### Task Files\n\nCreate separate task files:\n\n```bash\n# .mise/tasks/deploy\n#!/bin/bash\n# mise description=\"Deploy to production\"\n# mise depends=[\"build\", \"test\"]\n\necho \"Deploying...\"\nnpm run deploy\n```\n\nMake executable:\n```bash\nchmod +x .mise/tasks/deploy\n```\n\n## Common Workflows\n\n### Node.js Project Setup\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[env]\nNODE_ENV = \"development\"\n\n[tasks.install]\nrun = \"npm install\"\n\n[tasks.dev]\nrun = \"npm run dev\"\ndepends = [\"install\"]\n\n[tasks.build]\nrun = \"npm run build\"\ndepends = [\"install\"]\n\n[tasks.test]\nrun = \"npm test\"\ndepends = [\"install\"]\n```\n\n```bash\n# Setup and run\ncd project\nmise install      # Installs Node 20\nmise dev         # Runs dev server\n```\n\n### Python Project Setup\n\n```toml\n# .mise.toml\n[tools]\npython = \"3.12\"\n\n[env]\nPYTHONPATH = \"{{ config_root }}/src\"\n\n[tasks.venv]\nrun = \"python -m venv .venv\"\n\n[tasks.install]\nrun = \"pip install -r requirements.txt\"\ndepends = [\"venv\"]\n\n[tasks.test]\nrun = \"pytest\"\ndepends = [\"install\"]\n\n[tasks.format]\nrun = \"black src tests\"\n```\n\n### Monorepo Setup\n\n```toml\n# Root .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\n\n[env]\nWORKSPACE_ROOT = \"{{ config_root }}\"\n\n[tasks.install-all]\nrun = \"\"\"\nnpm install\ncd services/api && npm install\ncd services/web && npm install\n\"\"\"\n\n[tasks.test-all]\ndepends = [\"install-all\"]\nrun = \"\"\"\nmise run test --dir services/api\nmise run test --dir services/web\n\"\"\"\n```\n\n### Multi-Tool Project\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\npython = \"3.12\"\nruby = \"3.3\"\ngo = \"1.21\"\nterraform = \"latest\"\n\n[env]\nPROJECT_ROOT = \"{{ config_root }}\"\nPATH = [\"{{ config_root }}/bin\", \"$PATH\"]\n\n[tasks.setup]\ndescription = \"Setup all dependencies\"\nrun = \"\"\"\nnpm install\npip install -r requirements.txt\nbundle install\ngo mod download\n\"\"\"\n```\n\n## Lock Files\n\nGenerate lock files for reproducible environments:\n\n```bash\n# Generate .mise.lock\nmise lock\n\n# Use locked versions\nmise install --locked\n```\n\n```toml\n# .mise.toml\n[tools]\nnode = \"20\"\n\n[settings]\nlockfile = true  # Auto-generate lock file\n```\n\n## Shims\n\nUse shims for tool binaries:\n\n```bash\n# Enable shims\nmise settings set experimental true\nmise reshim\n\n# Now tools are in PATH via shims\nnode --version  # Uses mise-managed node\npython --version  # Uses mise-managed python\n```\n\n## Configuration Locations\n\nmise reads configuration from multiple locations (in order):\n\n1. `.mise.toml` - Project local config\n2. `.mise/config.toml` - Project local config (alternative)\n3. `~/.config/mise/config.toml` - Global config\n4. Environment variables - `MISE_*`\n\n## IDE Integration\n\n### VS Code\n\nAdd to `.vscode/settings.json`:\n\n```json\n{\n  \"terminal.integrated.env.linux\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  },\n  \"terminal.integrated.env.osx\": {\n    \"PATH\": \"${env:HOME}/.local/share/mise/shims:${env:PATH}\"\n  }\n}\n```\n\n### JetBrains IDEs\n\nUse mise shims or configure tool paths:\n\n```bash\n# Find tool path\nmise which node\nmise which python\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: CI\n\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: jdx/mise-action@v2\n\n      - name: Run tests\n        run: mise run test\n```\n\n### GitLab CI\n\n```yaml\ntest:\n  image: ubuntu:latest\n  before_script:\n    - curl https://mise.run | sh\n    - eval \"$(mise activate bash)\"\n    - mise install\n  script:\n    - mise run test\n```\n\n## Troubleshooting\n\n### Check mise status\n\n```bash\n# Show configuration\nmise config\n\n# Show environment\nmise env\n\n# Show installed tools\nmise list\n\n# Debug mode\nmise --verbose install node\n```\n\n### Clear cache\n\n```bash\n# Clear tool cache\nmise cache clear\n\n# Remove and reinstall\nmise uninstall node@20\nmise install node@20\n```\n\n### Legacy .tool-versions\n\nmise is compatible with asdf's `.tool-versions`:\n\n```\n# .tool-versions\nnodejs 20.10.0\npython 3.12.0\nruby 3.3.0\n```\n\nConvert to mise:\n\n```bash\n# mise auto-reads .tool-versions\n# Or convert to .mise.toml\nmise config migrate\n```\n\n## Best Practices\n\n- **Use .mise.toml for projects**: Better than .tool-versions (more features)\n- **Pin versions in projects**: Ensure consistency across team\n- **Use tasks for common operations**: Document and standardize workflows\n- **Lock files in production**: Use `mise lock` for reproducibility\n- **Global tools for dev**: Set global defaults, override per project\n- **Environment per project**: Keep secrets and config in .mise.toml\n- **Commit .mise.toml**: Share config with team\n- **Don't commit .mise.lock**: Let mise generate per environment\n\n## Key Principles\n\n- **Reproducible environments**: Lock versions for consistency\n- **Project-specific config**: Each project defines its own tools and env\n- **Task automation**: Centralize common development tasks\n- **Cross-platform**: Same config works on all platforms\n- **Zero setup for team**: Clone and `mise install` to get started"
              },
              {
                "name": "nushell",
                "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features",
                "path": "core/skills/nushell/SKILL.md",
                "frontmatter": {
                  "name": "nushell",
                  "description": "Guide for using Nushell (Nu), a modern shell with structured data pipelines, cross-platform compatibility, and programming language features"
                },
                "content": "# Nushell - Modern Structured Shell\n\nThis skill activates when working with Nushell (Nu), writing Nu scripts, working with structured data pipelines, or configuring the Nu environment.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Nushell scripts or commands\n- Working with structured data in pipelines\n- Converting from bash/zsh to Nushell\n- Configuring Nushell environment\n- Processing JSON, CSV, YAML, or other structured data\n- Creating custom commands or modules\n\n## What is Nushell?\n\nNushell is a modern shell that:\n- Treats **data as structured** (not just text streams)\n- Works **cross-platform** (Windows, macOS, Linux)\n- Provides **clear error messages** and IDE support\n- Combines **shell and programming language** features\n- Has **built-in data format support** (JSON, CSV, YAML, TOML, XML, etc.)\n\n## Installation\n\n```bash\n# macOS\nbrew install nushell\n\n# Linux (cargo)\ncargo install nu\n\n# Windows\nwinget install nushell\n\n# Or download from https://www.nushell.sh/\n```\n\n## Basic Concepts\n\n### Everything is Data\n\nUnlike traditional shells where everything is text, Nu works with structured data:\n\n```nu\n# Traditional shell (text output)\nls | grep \".txt\"\n\n# Nushell (structured data)\nls | where name =~ \".txt\"\n```\n\n### Pipeline Philosophy\n\nData flows through pipelines as structured tables/records:\n\n```nu\n# Each command outputs structured data\nls | where size > 1kb | sort-by modified | reverse\n```\n\n## Data Types\n\n### Basic Types\n\n```nu\n# Integers\n42\n-10\n\n# Floats\n3.14\n-2.5\n\n# Strings\n\"hello\"\n'world'\n\n# Booleans\ntrue\nfalse\n\n# Null\nnull\n```\n\n### Collections\n\n```nu\n# Lists\n[1 2 3 4 5]\n[\"apple\" \"banana\" \"cherry\"]\n\n# Records (like objects/dicts)\n{name: \"Alice\", age: 30, city: \"NYC\"}\n\n# Tables (list of records)\n[\n  {name: \"Alice\", age: 30}\n  {name: \"Bob\", age: 25}\n]\n```\n\n### Ranges\n\n```nu\n# Number ranges\n1..10\n1..2..10  # Step by 2\n\n# Use in commands\n1..5 | each { |i| $i * 2 }\n```\n\n## Working with Files and Directories\n\n### Navigation\n\n```nu\n# Change directory\ncd /path/to/dir\n\n# List files (returns structured table)\nls\n\n# List with details\nls | select name size modified\n\n# Filter files\nls | where type == file\nls | where size > 1mb\nls | where name =~ \"\\.txt$\"\n```\n\n### File Operations\n\n```nu\n# Create file\n\"hello\" | save hello.txt\n\n# Read file\nopen hello.txt\n\n# Append to file\n\"world\" | save -a hello.txt\n\n# Copy\ncp source.txt dest.txt\n\n# Move/rename\nmv old.txt new.txt\n\n# Remove\nrm file.txt\nrm -r directory/\n\n# Create directory\nmkdir new-dir\n```\n\n### File Content\n\n```nu\n# Read as string\nopen file.txt\n\n# Read structured data\nopen data.json\nopen config.toml\nopen data.csv\n\n# Write structured data\n{name: \"Alice\", age: 30} | to json | save user.json\n[{a: 1} {a: 2}] | to csv | save data.csv\n```\n\n## Pipeline Operations\n\n### Filtering\n\n```nu\n# Filter with where\nls | where size > 1mb\nls | where type == dir\nls | where name =~ \"test\"\n\n# Multiple conditions\nls | where size > 1kb and type == file\n```\n\n### Selecting Columns\n\n```nu\n# Select specific columns\nls | select name size\n\n# Rename columns\nls | select name size | rename file bytes\n```\n\n### Sorting\n\n```nu\n# Sort by column\nls | sort-by size\nls | sort-by modified\n\n# Reverse sort\nls | sort-by size | reverse\n\n# Multiple columns\nls | sort-by type size\n```\n\n### Transforming Data\n\n```nu\n# Map over items with each\n1..5 | each { |i| $i * 2 }\n\n# Update column\nls | update name { |row| $row.name | str upcase }\n\n# Insert column\nls | insert size_kb { |row| $row.size / 1000 }\n\n# Upsert (update or insert)\nls | upsert type_upper { |row| $row.type | str upcase }\n```\n\n### Aggregation\n\n```nu\n# Count items\nls | length\n\n# Sum\n[1 2 3 4 5] | math sum\n\n# Average\n[1 2 3 4 5] | math avg\n\n# Min/Max\nls | get size | math max\nls | get size | math min\n\n# Group by\nls | group-by type\n```\n\n## Variables\n\n### Variable Assignment\n\n```nu\n# Let (immutable by default)\nlet name = \"Alice\"\nlet age = 30\nlet colors = [\"red\" \"green\" \"blue\"]\n\n# Mut (mutable)\nmut counter = 0\n$counter = $counter + 1\n```\n\n### Using Variables\n\n```nu\n# Reference with $\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# In pipelines\nlet threshold = 1mb\nls | where size > $threshold\n```\n\n### Environment Variables\n\n```nu\n# Get environment variable\n$env.PATH\n$env.HOME\n\n# Set environment variable\n$env.MY_VAR = \"value\"\n\n# Load from file\nload-env { API_KEY: \"secret\" }\n```\n\n## String Operations\n\n### String Interpolation\n\n```nu\n# String interpolation with ()\nlet name = \"Alice\"\nprint $\"Hello, ($name)!\"\n\n# With expressions\nlet x = 5\nprint $\"Result: (5 * $x)\"\n```\n\n### String Methods\n\n```nu\n# Case conversion\n\"hello\" | str upcase  # HELLO\n\"WORLD\" | str downcase  # world\n\n# Trimming\n\"  spaces  \" | str trim\n\n# Replace\n\"hello world\" | str replace \"world\" \"nu\"\n\n# Contains\n\"hello world\" | str contains \"world\"  # true\n\n# Split\n\"a,b,c\" | split row \",\"\n```\n\n## Conditionals\n\n### If Expressions\n\n```nu\n# If-else\nif $age >= 18 {\n  print \"Adult\"\n} else {\n  print \"Minor\"\n}\n\n# If-else if-else\nif $score >= 90 {\n  \"A\"\n} else if $score >= 80 {\n  \"B\"\n} else {\n  \"C\"\n}\n\n# Ternary-style with match\nlet status = if $is_active { \"active\" } else { \"inactive\" }\n```\n\n### Match (Pattern Matching)\n\n```nu\n# Match expression\nmatch $value {\n  1 => \"one\"\n  2 => \"two\"\n  _ => \"other\"\n}\n\n# With conditions\nmatch $age {\n  0..17 => \"minor\"\n  18..64 => \"adult\"\n  _ => \"senior\"\n}\n```\n\n## Loops\n\n### For Loop\n\n```nu\n# Loop over range\nfor i in 1..5 {\n  print $i\n}\n\n# Loop over list\nfor name in [\"Alice\" \"Bob\" \"Charlie\"] {\n  print $\"Hello, ($name)\"\n}\n\n# Loop over files\nfor file in (ls | where type == file) {\n  print $file.name\n}\n```\n\n### While Loop\n\n```nu\n# While loop\nmut i = 0\nwhile $i < 5 {\n  print $i\n  $i = $i + 1\n}\n```\n\n### Each (Functional)\n\n```nu\n# Transform each item\n1..5 | each { |i| $i * 2 }\n\n# With index\n[\"a\" \"b\" \"c\"] | enumerate | each { |item|\n  print $\"($item.index): ($item.item)\"\n}\n```\n\n## Custom Commands\n\n### Defining Commands\n\n```nu\n# Simple command\ndef greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\ngreet \"Alice\"\n\n# With return value\ndef add [a: int, b: int] {\n  $a + $b\n}\n\nlet result = add 5 3\n\n# With default values\ndef greet [name: string = \"World\"] {\n  print $\"Hello, ($name)!\"\n}\n```\n\n### Command Parameters\n\n```nu\n# Required parameters\ndef copy [source: path, dest: path] {\n  cp $source $dest\n}\n\n# Optional parameters\ndef greet [\n  name: string\n  --loud (-l)  # Flag\n  --repeat (-r): int = 1  # Named parameter with default\n] {\n  let message = if $loud {\n    $name | str upcase\n  } else {\n    $name\n  }\n\n  1..$repeat | each { print $\"Hello, ($message)!\" }\n}\n\n# Usage\ngreet \"Alice\"\ngreet \"Bob\" --loud\ngreet \"Charlie\" --repeat 3\n```\n\n### Pipeline Commands\n\n```nu\n# Accept pipeline input\ndef filter-large [] {\n  where size > 1mb\n}\n\n# Usage\nls | filter-large\n\n# Accept and transform pipeline\ndef double [] {\n  each { |value| $value * 2 }\n}\n\n[1 2 3] | double\n```\n\n## Working with Structured Data\n\n### JSON\n\n```nu\n# Read JSON\nlet data = open data.json\n\n# Parse JSON string\nlet obj = '{\"name\": \"Alice\", \"age\": 30}' | from json\n\n# Write JSON\n{name: \"Alice\", age: 30} | to json | save user.json\n\n# Pretty print JSON\n{name: \"Alice\", age: 30} | to json -i 2\n```\n\n### CSV\n\n```nu\n# Read CSV\nlet data = open data.csv\n\n# Convert to CSV\n[{a: 1, b: 2} {a: 3, b: 4}] | to csv\n\n# Save CSV\nls | select name size | to csv | save files.csv\n```\n\n### YAML/TOML\n\n```nu\n# Read YAML\nlet config = open config.yaml\n\n# Read TOML\nlet config = open config.toml\n\n# Write YAML\n{key: \"value\"} | to yaml | save config.yaml\n\n# Write TOML\n{key: \"value\"} | to toml | save config.toml\n```\n\n### Working with Tables\n\n```nu\n# Create table\nlet users = [\n  {name: \"Alice\", age: 30, city: \"NYC\"}\n  {name: \"Bob\", age: 25, city: \"LA\"}\n  {name: \"Charlie\", age: 35, city: \"NYC\"}\n]\n\n# Query table\n$users | where age > 25\n$users | where city == \"NYC\"\n$users | select name age\n\n# Add column\n$users | insert country { \"USA\" }\n\n# Group and count\n$users | group-by city | transpose city users\n```\n\n## Modules\n\n### Creating Modules\n\n```nu\n# utils.nu\nexport def greet [name: string] {\n  print $\"Hello, ($name)!\"\n}\n\nexport def add [a: int, b: int] {\n  $a + $b\n}\n```\n\n### Using Modules\n\n```nu\n# Import module\nuse utils.nu\n\n# Use exported commands\nutils greet \"Alice\"\nutils add 5 3\n\n# Import specific commands\nuse utils.nu [greet add]\n\ngreet \"Alice\"\nadd 5 3\n\n# Import with alias\nuse utils.nu *\n```\n\n## Configuration\n\n### Config File Location\n\n```nu\n# View config\nconfig nu\n\n# Edit config\nconfig nu | open\n\n# Config location\n$nu.config-path\n```\n\n### Common Configurations\n\n```nu\n# config.nu\n$env.config = {\n  show_banner: false\n\n  ls: {\n    use_ls_colors: true\n    clickable_links: true\n  }\n\n  table: {\n    mode: rounded\n    index_mode: auto\n  }\n\n  completions: {\n    quick: true\n    partial: true\n  }\n\n  history: {\n    max_size: 10000\n    sync_on_enter: true\n    file_format: \"sqlite\"\n  }\n}\n```\n\n### Environment Setup\n\n```nu\n# env.nu\n$env.PATH = ($env.PATH | split row (char esep) | append '/custom/bin')\n$env.EDITOR = \"nvim\"\n\n# Load completions\nuse completions/git.nu *\n```\n\n## Common Patterns\n\n### File Processing\n\n```nu\n# Process all JSON files\nls *.json | each { |file|\n  let data = open $file.name\n  print $\"Processing ($file.name): ($data | length) items\"\n}\n\n# Batch rename files\nls *.txt | each { |file|\n  let new_name = ($file.name | str replace \".txt\" \".md\")\n  mv $file.name $new_name\n}\n```\n\n### Data Transformation\n\n```nu\n# CSV to JSON\nopen data.csv | to json | save data.json\n\n# Filter and transform\nopen users.json\n| where active == true\n| select name email\n| to csv\n| save active_users.csv\n\n# Merge data\nlet users = open users.json\nlet orders = open orders.json\n$users | merge $orders\n```\n\n### HTTP Requests\n\n```nu\n# GET request\nhttp get https://api.example.com/users\n\n# POST request\nhttp post https://api.example.com/users {\n  name: \"Alice\"\n  email: \"alice@example.com\"\n}\n\n# With headers\nhttp get -H [Authorization \"Bearer token\"] https://api.example.com/data\n```\n\n### System Commands\n\n```nu\n# Run external command\n^ls -la\n\n# Capture output\nlet output = (^git status)\n\n# Check if command exists\nwhich git\n\n# Get command path\nwhich git | get path\n```\n\n## Error Handling\n\n### Try-Catch\n\n```nu\n# Try expression\ntry {\n  open missing.txt\n} catch {\n  print \"File not found\"\n}\n\n# With error value\ntry {\n  open missing.txt\n} catch { |err|\n  print $\"Error: ($err)\"\n}\n```\n\n### Null Handling\n\n```nu\n# Default value\nlet value = ($env.MY_VAR? | default \"default_value\")\n\n# Null propagation\nlet length = ($value | get name? | str length)\n```\n\n## Scripting\n\n### Script Files\n\n```nu\n#!/usr/bin/env nu\n\n# Script: process_logs.nu\n# Description: Process log files and generate report\n\ndef main [log_dir: path] {\n  let errors = (\n    ls $\"($log_dir)/*.log\"\n    | each { |file| open $file.name | lines }\n    | flatten\n    | where $it =~ \"ERROR\"\n  )\n\n  print $\"Found ($errors | length) errors\"\n  $errors | save error_report.txt\n}\n```\n\nMake executable:\n```bash\nchmod +x process_logs.nu\n./process_logs.nu /var/log\n```\n\n### Script Parameters\n\n```nu\n# With parameters\ndef main [\n  input: path\n  --output (-o): path = \"output.txt\"\n  --verbose (-v)\n] {\n  if $verbose {\n    print $\"Processing ($input)...\"\n  }\n\n  let data = open $input\n  $data | save $output\n\n  if $verbose {\n    print \"Done!\"\n  }\n}\n```\n\n## Comparison with Bash\n\n### Common Operations\n\n```bash\n# Bash\nfind . -name \"*.txt\" | wc -l\n\n# Nushell\nls **/*.txt | length\n```\n\n```bash\n# Bash\ncat file.json | jq '.users[] | select(.age > 25) | .name'\n\n# Nushell\nopen file.json | get users | where age > 25 | get name\n```\n\n```bash\n# Bash\nfor file in *.txt; do\n  mv \"$file\" \"${file%.txt}.md\"\ndone\n\n# Nushell\nls *.txt | each { |f| mv $f.name ($f.name | str replace \".txt\" \".md\") }\n```\n\n## Best Practices\n\n- **Use structured data**: Leverage Nu's strength in handling structured data\n- **Pipeline composition**: Build complex operations from simple pipeline stages\n- **Type annotations**: Add types to custom command parameters for clarity\n- **Error handling**: Use try-catch for operations that might fail\n- **Modules for reuse**: Organize reusable commands in modules\n- **Configuration**: Customize Nu to fit your workflow\n- **External commands**: Use `^` prefix when calling external commands explicitly\n\n## Common Pitfalls\n\n### String vs Bare Words\n\n```nu\n# Bare word (interpreted as string in some contexts)\necho hello\n\n# Explicit string (clearer)\necho \"hello\"\n```\n\n### External Commands\n\n```nu\n# Wrong - Nu tries to parse as Nu command\nls -la\n\n# Right - Explicitly call external command\n^ls -la\n```\n\n### Variable Scope\n\n```nu\n# Variables are scoped to blocks\nif true {\n  let x = 5\n}\n# $x not available here\n\n# Use mut outside for wider scope\nmut x = 0\nif true {\n  $x = 5\n}\nprint $x  # Works\n```\n\n## Key Principles\n\n- **Structured data first**: Think in terms of tables and records, not text\n- **Pipeline composition**: Chain simple operations to build complex workflows\n- **Type safety**: Leverage Nu's type system for reliable scripts\n- **Cross-platform**: Write scripts that work on all platforms\n- **Interactive and scriptable**: Same syntax works in REPL and scripts\n- **Clear errors**: Nu provides helpful error messages for debugging"
              },
              {
                "name": "twelve-factor",
                "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                "path": "core/skills/twelve-factor/SKILL.md",
                "frontmatter": {
                  "name": "twelve-factor",
                  "description": "Guide for building cloud-native applications following the 12-Factor App methodology with Kubernetes, containers, and modern deployment practices",
                  "license": "MIT"
                },
                "content": "# 12-Factor App Methodology\n\nGuide for building scalable, maintainable, and portable cloud-native applications following the 12-Factor App principles and modern extensions.\n\n## When to Activate\n\nUse this skill when:\n- Designing or refactoring cloud-native applications\n- Building applications for Kubernetes deployment\n- Setting up CI/CD pipelines\n- Implementing microservices architecture\n- Migrating applications to containers\n- Reviewing architecture for cloud readiness\n- Troubleshooting deployment or scaling issues\n- Working with environment configuration\n\n## The 12 Factors\n\n### I. Codebase\n\n**One codebase tracked in revision control, many deploys**\n\n```\nmyapp-repo/\n├── src/\n├── config/\n├── deploy/\n│   ├── staging/\n│   ├── production/\n│   └── development/\n└── Dockerfile\n```\n\n**Key principles:**\n- Single Git repository for the application\n- Multiple environments deploy from same codebase\n- Environment-specific config separate from code\n- Use GitOps (ArgoCD, Flux) for deployment automation\n\n**Anti-patterns:**\n- ❌ Multiple repositories for the same application\n- ❌ Different codebases for different environments\n- ❌ Copying code between repositories\n\n### II. Dependencies\n\n**Explicitly declare and isolate dependencies**\n\nDeclare all dependencies explicitly using package managers:\n\n```dockerfile\n# Multi-stage build for dependency isolation\nFROM node:18-alpine AS dependencies\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nFROM node:18-alpine AS runtime\nWORKDIR /app\nCOPY --from=dependencies /app/node_modules ./node_modules\nCOPY . .\nCMD [\"node\", \"index.js\"]\n```\n\n**Language-specific examples:**\n- Node.js: `package.json` and `package-lock.json`\n- Python: `requirements.txt` or `Pipfile.lock`\n- Java: `pom.xml` or `build.gradle`\n- Go: `go.mod` and `go.sum`\n- Elixir: `mix.exs` and `mix.lock`\n- Rust: `Cargo.toml` and `Cargo.lock`\n\n**Key principles:**\n- Never rely on system-wide packages\n- Use lock files for reproducible builds\n- Vendor dependencies when possible\n- Multi-stage builds for smaller images\n\n### III. Config\n\n**Store config in the environment**\n\nAll configuration should come from environment variables:\n\n```elixir\n# Elixir - config/runtime.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.fetch_env!(\"DATABASE_PASSWORD\"),\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n```\n\n```javascript\n// Node.js\nconst config = {\n  database: {\n    url: process.env.DATABASE_URL,\n    pool: {\n      min: parseInt(process.env.DB_POOL_MIN || '2'),\n      max: parseInt(process.env.DB_POOL_MAX || '10')\n    }\n  },\n  cache: {\n    ttl: parseInt(process.env.CACHE_TTL || '3600')\n  }\n};\n```\n\n**Kubernetes ConfigMaps:**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_HOST: \"postgres-service\"\n  CACHE_TTL: \"3600\"\n  LOG_LEVEL: \"info\"\n```\n\n**Kubernetes Secrets:**\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  DATABASE_PASSWORD: <base64-encoded>\n  JWT_SECRET: <base64-encoded>\n  API_KEY: <base64-encoded>\n```\n\n**Anti-patterns:**\n- ❌ Hardcoded configuration values\n- ❌ Configuration files committed to version control\n- ❌ Different code paths for different environments\n\n### IV. Backing Services\n\n**Treat backing services as attached resources**\n\nConnect to all backing services (databases, queues, caches, APIs) via URLs in environment variables:\n\n```javascript\n// Treat all backing services uniformly\nconst services = {\n  database: createConnection(process.env.DATABASE_URL),\n  cache: createRedisClient(process.env.REDIS_URL),\n  queue: createQueueClient(process.env.RABBITMQ_URL),\n  storage: createS3Client(process.env.S3_ENDPOINT)\n};\n```\n\n**Kubernetes Service Discovery:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n```\n\n**Key principles:**\n- No distinction between local and third-party services\n- Swappable via configuration change only\n- No code changes to swap backing services\n- Connection via URL in environment\n\n### V. Build, Release, Run\n\n**Strictly separate build and run stages**\n\nThree distinct stages:\n1. **Build**: Convert code to executable bundle\n2. **Release**: Combine build with config\n3. **Run**: Execute in target environment\n\n```yaml\n# GitHub Actions CI/CD Pipeline\nname: Build and Deploy\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build Docker image\n        run: docker build -t myapp:${{ github.sha }} .\n      - name: Push to registry\n        run: docker push myapp:${{ github.sha }}\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to Kubernetes\n        run: kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}\n```\n\n**Key principles:**\n- Immutable releases (never modify, only deploy new)\n- Unique release identifiers (git SHA, semver)\n- Rollback by redeploying previous release\n- Separate build artifacts from runtime config\n\n### VI. Processes\n\n**Execute the app as one or more stateless processes**\n\nApplication processes should be stateless and share-nothing. Store persistent state in backing services.\n\n```javascript\n// ❌ Bad: In-memory session store\napp.use(session({\n  secret: process.env.SESSION_SECRET,\n  resave: false\n  // Uses memory store by default\n}));\n\n// ✓ Good: Store session in Redis\napp.use(session({\n  store: new RedisStore({\n    client: redisClient,\n    prefix: 'sess:'\n  }),\n  secret: process.env.SESSION_SECRET,\n  resave: false,\n  saveUninitialized: false\n}));\n```\n\n**Kubernetes Deployment:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3  # Can scale horizontally\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n```\n\n**Key principles:**\n- Stateless processes enable horizontal scaling\n- No sticky sessions\n- No local filesystem for persistent data\n- Shared state goes in databases, caches, or queues\n\n### VII. Port Binding\n\n**Export services via port binding**\n\nApplications are self-contained and bind to a port:\n\n```javascript\nconst express = require('express');\nconst app = express();\nconst port = process.env.PORT || 3000;\n\napp.listen(port, '0.0.0.0', () => {\n  console.log(`Server running on port ${port}`);\n});\n```\n\n```elixir\n# Phoenix endpoint config\nconfig :my_app, MyAppWeb.Endpoint,\n  http: [port: String.to_integer(System.get_env(\"PORT\") || \"4000\")],\n  server: true\n```\n\n**Kubernetes Service:**\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: LoadBalancer\n```\n\n**Key principles:**\n- Bind to `0.0.0.0`, not `localhost`\n- Port number from environment variable\n- No reliance on runtime injection (e.g., Apache, Nginx)\n- HTTP server library embedded in app\n\n### VIII. Concurrency\n\n**Scale out via the process model**\n\nScale by adding more processes (horizontal scaling), not by making processes larger (vertical scaling):\n\n```yaml\n# Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n**Process types (Procfile concept):**\n\n```\nweb: node server.js\nworker: node worker.js\nscheduler: node scheduler.js\n```\n\n**Key principles:**\n- Different process types for different workloads\n- Processes can scale independently\n- OS process manager handles processes\n- Never daemonize or write PID files\n\n### IX. Disposability\n\n**Maximize robustness with fast startup and graceful shutdown**\n\n```javascript\nconst server = app.listen(port, () => {\n  console.log('Server started');\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', () => {\n  console.log('SIGTERM received, shutting down gracefully');\n\n  server.close(() => {\n    // Close database connections\n    db.close();\n\n    // Close other connections\n    redis.quit();\n\n    console.log('Process terminated');\n    process.exit(0);\n  });\n});\n```\n\n**Kubernetes lifecycle hooks:**\n\n```yaml\nspec:\n  containers:\n  - name: myapp\n    image: myapp:latest\n    lifecycle:\n      preStop:\n        exec:\n          command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n    terminationGracePeriodSeconds: 30\n```\n\n**Key principles:**\n- Minimize startup time (< 10 seconds ideal)\n- Handle SIGTERM for graceful shutdown\n- Finish in-flight requests before shutting down\n- Robust against sudden death\n- Fast startup enables rapid scaling\n\n### X. Dev/Prod Parity\n\n**Keep development, staging, and production as similar as possible**\n\n**Docker Compose for local development:**\n\n```yaml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n\n  redis:\n    image: redis:7-alpine\n```\n\n**Key principles:**\n- Use same backing services in dev and prod\n- Containers ensure environment consistency\n- Infrastructure as code for reproducibility\n- Minimize time gap between dev and production\n- Same deployment process for all environments\n\n### XI. Logs\n\n**Treat logs as event streams**\n\nWrite all logs to stdout/stderr, let the environment handle aggregation:\n\n```javascript\n// Structured logging to stdout\nconst winston = require('winston');\n\nconst logger = winston.createLogger({\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.json()\n  ),\n  transports: [\n    new winston.transports.Console()\n  ]\n});\n\nlogger.info('User logged in', {\n  userId: 123,\n  ip: '192.168.1.1',\n  userAgent: 'Mozilla/5.0...'\n});\n```\n\n```elixir\n# Elixir structured logging\nrequire Logger\n\nLogger.info(\"User logged in\",\n  user_id: 123,\n  ip: \"192.168.1.1\"\n)\n```\n\n**Key principles:**\n- Never manage log files\n- Write unbuffered to stdout\n- Use structured logging (JSON)\n- Let platform route logs (Fluentd, Logstash)\n- Include correlation IDs for tracing\n\n**Anti-patterns:**\n- ❌ Writing to log files\n- ❌ Log rotation within the app\n- ❌ Sending logs directly to aggregation service\n\n### XII. Admin Processes\n\n**Run admin/management tasks as one-off processes**\n\nDatabase migrations, console, one-time scripts:\n\n```yaml\n# Kubernetes Job for database migration\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: myapp:latest\n        command: [\"npm\", \"run\", \"migrate\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: DATABASE_URL\n      restartPolicy: OnFailure\n```\n\n```yaml\n# CronJob for scheduled cleanup\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: data-cleanup\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: myapp:latest\n            command: [\"npm\", \"run\", \"cleanup\"]\n          restartPolicy: OnFailure\n```\n\n**Key principles:**\n- Same environment as regular processes\n- Same codebase and config\n- Run against release, not development code\n- Use scheduler for recurring tasks\n- Ship admin code with application code\n\n## Modern Extensions (Beyond 12)\n\n### XIII. API First\n\nDesign and document APIs before implementation:\n\n```yaml\n# OpenAPI specification\nopenapi: 3.0.0\ninfo:\n  title: My API\n  version: v1\npaths:\n  /users:\n    get:\n      summary: List users\n      responses:\n        '200':\n          description: Success\n```\n\n**Key principles:**\n- OpenAPI/Swagger specifications\n- API versioning (URL or header)\n- API gateway pattern\n- Contract-first development\n\n### XIV. Telemetry\n\nComprehensive observability with metrics, tracing, and monitoring:\n\n```yaml\n# Prometheus ServiceMonitor\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myapp-monitor\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  endpoints:\n  - port: metrics\n    path: /metrics\n```\n\n**Key principles:**\n- Expose /metrics endpoint (Prometheus format)\n- Distributed tracing (OpenTelemetry)\n- Application Performance Monitoring (APM)\n- Custom business metrics\n- Health check endpoints\n\n### XV. Security\n\nAuthentication, authorization, and security by design:\n\n```javascript\n// JWT authentication middleware\nfunction authenticateToken(req, res, next) {\n  const authHeader = req.headers['authorization'];\n  const token = authHeader && authHeader.split(' ')[1];\n\n  if (!token) return res.sendStatus(401);\n\n  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {\n    if (err) return res.sendStatus(403);\n    req.user = user;\n    next();\n  });\n}\n```\n\n**Key principles:**\n- OAuth 2.0 / OpenID Connect\n- RBAC (Role-Based Access Control)\n- Secrets in environment, never in code\n- TLS everywhere\n- Security scanning in CI/CD\n\n## Common Patterns\n\n### Configuration Validation\n\nValidate required configuration at startup:\n\n```javascript\nfunction validateConfig() {\n  const required = ['DATABASE_URL', 'JWT_SECRET', 'REDIS_URL'];\n  const missing = required.filter(key => !process.env[key]);\n\n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n}\n\n// Call before starting server\nvalidateConfig();\n```\n\n### Health Checks\n\nImplement health and readiness endpoints:\n\n```javascript\n// Liveness probe\napp.get('/health', (req, res) => {\n  res.status(200).json({\n    status: 'healthy',\n    timestamp: new Date().toISOString()\n  });\n});\n\n// Readiness probe\napp.get('/ready', async (req, res) => {\n  try {\n    await db.ping();\n    await redis.ping();\n    res.status(200).json({ status: 'ready' });\n  } catch (err) {\n    res.status(503).json({ status: 'not ready', error: err.message });\n  }\n});\n```\n\n**Kubernetes probes:**\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 3000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n### Graceful Degradation\n\nHandle backing service failures gracefully:\n\n```javascript\nasync function getCachedData(key) {\n  try {\n    return await redis.get(key);\n  } catch (err) {\n    logger.warn('Redis unavailable, falling back to database', { error: err.message });\n    return await db.query('SELECT data FROM cache WHERE key = ?', [key]);\n  }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Environment-Specific Code Paths\n\n```javascript\n// DON'T\nif (process.env.NODE_ENV === 'production') {\n  // Different behavior\n} else {\n  // Different behavior\n}\n\n// DO: Use configuration\nconst timeout = parseInt(process.env.TIMEOUT || '5000');\n```\n\n### ❌ Local File Storage\n\n```javascript\n// DON'T: Write to local filesystem\nfs.writeFile('/tmp/uploads/' + filename, data);\n\n// DO: Use object storage\nawait s3.putObject({\n  Bucket: process.env.S3_BUCKET,\n  Key: filename,\n  Body: data\n});\n```\n\n### ❌ In-Memory State\n\n```javascript\n// DON'T: Store state in memory\nconst sessions = new Map();\n\n// DO: Use external store\nconst session = await redis.get(`session:${sessionId}`);\n```\n\n### ❌ Hardcoded Dependencies\n\n```javascript\n// DON'T: Hardcode service locations\nconst db = connect('localhost:5432');\n\n// DO: Use environment variables\nconst db = connect(process.env.DATABASE_URL);\n```\n\n## Troubleshooting Guide\n\n### Application Won't Start\n\n1. Check required environment variables are set\n2. Validate configuration at startup\n3. Check backing service connectivity\n4. Review logs for initialization errors\n\n### Application Won't Scale\n\n1. Identify stateful operations\n2. Move state to backing services\n3. Remove file system dependencies\n4. Eliminate sticky sessions\n\n### Inconsistent Behavior Across Environments\n\n1. Ensure same backing service types (not SQLite in dev, Postgres in prod)\n2. Use containers for dev environment\n3. Check for environment-specific code paths\n4. Verify configuration is environment-only\n\n### Logs Not Appearing\n\n1. Ensure writing to stdout/stderr\n2. Avoid buffering log output\n3. Check log aggregation configuration\n4. Verify Kubernetes logging sidecar/daemonset\n\n## Best Practices Summary\n\n1. **Environment variables for all configuration**\n2. **Stateless processes that can scale horizontally**\n3. **Structured logging to stdout**\n4. **Containers for development parity**\n5. **Automated CI/CD pipelines**\n6. **Health checks for orchestration**\n7. **Graceful shutdown handling**\n8. **Fast startup times (< 10s)**\n9. **Immutable releases with unique IDs**\n10. **Comprehensive monitoring and telemetry**\n\n## Kubernetes-Specific Best Practices\n\n### Resource Limits\n\n```yaml\nresources:\n  requests:\n    memory: \"128Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"256Mi\"\n    cpu: \"200m\"\n```\n\n### Init Containers\n\n```yaml\ninitContainers:\n- name: wait-for-db\n  image: busybox\n  command: ['sh', '-c', 'until nc -z postgres-service 5432; do sleep 1; done']\n```\n\n### Pod Disruption Budgets\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n## Resources\n\n- **12factor.net**: Original methodology\n- **Kubernetes Documentation**: https://kubernetes.io/docs/\n- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n- **OpenTelemetry**: https://opentelemetry.io/\n- **Prometheus**: https://prometheus.io/\n\n## Key Insights\n\n> \"The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\"\n\n> \"A twelve-factor app never relies on implicit existence of state on the filesystem. Even if a process has written something to disk, it must assume that file won't be available on the next request.\"\n\nDesign applications from day one to be cloud-native, scalable, and maintainable. The investment in following these principles pays dividends in operational simplicity and development velocity."
              },
              {
                "name": "dagu-rest-api",
                "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems",
                "path": "dagu/skills/rest-api/SKILL.md",
                "frontmatter": {
                  "name": "dagu-rest-api",
                  "description": "Guide for using the Dagu REST API to programmatically manage and execute workflows, query status, and integrate with external systems"
                },
                "content": "# Dagu REST API\n\nUse this skill when integrating Dagu with external systems, automating workflow operations, or programmatically managing workflows through the API.\n\n## When to Use This Skill\n\nActivate when:\n- Triggering workflows programmatically\n- Querying workflow status from applications\n- Building automation around Dagu\n- Integrating Dagu with CI/CD pipelines\n- Creating custom dashboards or monitoring tools\n- Scheduling workflows dynamically\n- Fetching execution logs programmatically\n\n## Core API Capabilities\n\nThe Dagu REST API provides endpoints for:\n\n1. **Workflow Operations** - Start, stop, retry workflows\n2. **Status Queries** - Get workflow and execution status\n3. **DAG Management** - List and inspect workflow definitions\n4. **Execution History** - Query past executions\n5. **Log Retrieval** - Fetch execution logs\n\n## Base URL\n\nDefault API base URL: `http://localhost:8080/api/v1`\n\nConfigure in Dagu settings if using a different host/port.\n\n## Authentication\n\nConsult `references/authentication.md` for details on:\n- API token configuration\n- Authentication headers\n- Security best practices\n\n## Quick Start Operations\n\n### Start a Workflow\n\n```bash\nPOST /dags/{dagName}/start\n```\n\nBasic example:\n```bash\ncurl -X POST http://localhost:8080/api/v1/dags/my_workflow/start\n```\n\nFor parameter passing and advanced options, see `references/workflow-operations.md`.\n\n### Get Workflow Status\n\n```bash\nGET /dags/{dagName}/status\n```\n\nReturns current status, running steps, and execution details.\n\n### Stop a Workflow\n\n```bash\nPOST /dags/{dagName}/stop\n```\n\nStops currently running execution.\n\n## When to Consult References\n\n- **Detailed endpoint documentation**: Read `references/api-endpoints.md`\n- **Workflow operations (start/stop/retry)**: Read `references/workflow-operations.md`\n- **Status and monitoring queries**: Read `references/status-queries.md`\n- **Authentication setup**: Read `references/authentication.md`\n- **Integration examples**: Read `references/integration-examples.md`\n- **Error handling**: Read `references/error-handling.md`\n\n## Common Use Cases\n\n### CI/CD Integration\n\nTrigger Dagu workflows from your CI/CD pipeline:\n\n```bash\n# In GitHub Actions, GitLab CI, etc.\ncurl -X POST http://dagu-server:8080/api/v1/dags/deploy_production/start \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": \"VERSION=1.2.3 ENVIRONMENT=production\"}'\n```\n\nFor complete CI/CD integration patterns, see `references/integration-examples.md`.\n\n### Monitoring and Alerting\n\nQuery workflow status for external monitoring:\n\n```bash\n# Check if workflow is running\ncurl http://localhost:8080/api/v1/dags/critical_job/status\n```\n\nBuild custom alerts based on status responses. See `references/status-queries.md` for response format details.\n\n### Dynamic Scheduling\n\nTrigger workflows based on external events:\n\n```python\nimport requests\n\ndef trigger_workflow(dag_name, params=None):\n    url = f\"http://localhost:8080/api/v1/dags/{dag_name}/start\"\n    data = {\"params\": params} if params else {}\n    response = requests.post(url, json=data)\n    return response.json()\n```\n\nFor comprehensive examples in multiple languages, see `references/integration-examples.md`.\n\n## Response Formats\n\nAll API responses are JSON. Common response structure:\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": { ... }\n}\n```\n\nError responses:\n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error description\"\n}\n```\n\nFor complete response schemas, consult `references/api-endpoints.md`.\n\n## Key Principles\n\n- **RESTful design**: Standard HTTP methods (GET, POST, DELETE)\n- **JSON responses**: All responses in JSON format\n- **Idempotent operations**: Safe to retry most operations\n- **Error codes**: Standard HTTP status codes\n- **Stateless**: Each request is independent\n\n## Pro Tips\n\n- Use the API for automation, use Web UI for manual operations\n- Implement retry logic for network failures\n- Cache DAG lists if querying frequently\n- Use webhooks for event-driven workflows when possible\n- Monitor API response times for performance issues\n- Validate workflow names before calling API to avoid errors"
              },
              {
                "name": "dagu-webui",
                "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface",
                "path": "dagu/skills/webui/SKILL.md",
                "frontmatter": {
                  "name": "dagu-webui",
                  "description": "Guide for using the Dagu Web UI to manage, monitor, and execute workflows through the browser interface"
                },
                "content": "# Dagu Web UI\n\nUse this skill when working with Dagu's web interface to manage workflows, view execution history, monitor running workflows, or configure the UI.\n\n## When to Use This Skill\n\nActivate when:\n- Navigating the Dagu web interface\n- Starting, stopping, or retrying workflows via UI\n- Viewing workflow execution logs and status\n- Monitoring running workflows\n- Managing workflow history\n- Configuring workflow schedules through the UI\n- Troubleshooting workflow issues using the UI\n\n## Core Capabilities\n\nThe Dagu Web UI provides:\n\n1. **Workflow Management** - View, start, stop, and manage workflows\n2. **Execution Monitoring** - Real-time status and logs\n3. **History Viewing** - Past execution records and results\n4. **DAG Visualization** - Visual representation of workflow structure\n5. **Log Access** - View detailed execution logs\n6. **Schedule Management** - Configure when workflows run\n\n## Quick Start\n\nAccess Dagu Web UI at `http://localhost:8080` (default) after starting Dagu:\n\n```bash\ndagu server\n```\n\n## Primary Operations\n\n### Start a Workflow\n\nTo manually execute a workflow:\n1. Navigate to workflow list\n2. Click the workflow name\n3. Click \"Start\" button\n4. View real-time execution progress\n\n### Monitor Execution\n\nFor detailed information on a running workflow, consult `references/monitoring.md` which covers:\n- Reading execution logs\n- Understanding status indicators\n- Tracking step progress\n- Identifying failures\n\n### View History\n\nTo review past executions, see `references/history.md` for guidance on:\n- Filtering execution history\n- Analyzing failed runs\n- Comparing execution times\n- Exporting execution data\n\n### Workflow Visualization\n\nThe DAG view shows workflow structure. For detailed visualization features, see `references/visualization.md`.\n\n## When to Consult References\n\n- **Detailed UI navigation**: Read `references/ui-navigation.md`\n- **Advanced monitoring**: Read `references/monitoring.md`\n- **History analysis**: Read `references/history.md`\n- **Workflow editing via UI**: Read `references/workflow-editor.md`\n- **Configuration options**: Read `references/configuration.md`\n\n## Common Tasks\n\n### Restart a Failed Workflow\n\n1. Find the failed execution in history\n2. Click the retry/restart button\n3. Monitor the new execution\n\n### Stop a Running Workflow\n\n1. Navigate to the running workflow\n2. Click \"Stop\" or \"Cancel\"\n3. Confirm the action\n4. View cleanup handlers execution\n\n### View Detailed Logs\n\nWhen you need to debug a workflow:\n1. Click on the specific workflow execution\n2. Select the step with issues\n3. View stdout/stderr logs\n4. Check for error messages\n\nFor advanced log analysis, consult `references/monitoring.md`.\n\n## Key Principles\n\n- **Real-time visibility**: Web UI provides live updates of workflow execution\n- **Click-based operations**: No CLI needed for basic workflow management\n- **History preservation**: All executions are logged and accessible\n- **Visual feedback**: Status indicators show current state at a glance\n- **Log accessibility**: Detailed logs available for debugging\n\n## Pro Tips\n\n- Use the search feature to quickly find workflows by name\n- Filter execution history by date range or status\n- Click on step names in DAG view for step-specific details\n- Use the refresh button if live updates seem delayed\n- Check the scheduler status to verify cron jobs are active"
              },
              {
                "name": "dagu-workflows",
                "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition",
                "path": "dagu/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "dagu-workflows",
                  "description": "Guide for authoring Dagu workflows including YAML syntax, steps, executors, scheduling, dependencies, and workflow composition"
                },
                "content": "# Dagu Workflow Authoring\n\nThis skill activates when creating or modifying Dagu workflow definitions, configuring workflow steps, scheduling, or composing complex workflows.\n\n## When to Use This Skill\n\nActivate when:\n- Writing Dagu workflow YAML files\n- Configuring workflow steps and executors\n- Setting up workflow scheduling with cron\n- Defining step dependencies and data flow\n- Implementing error handling and retries\n- Composing hierarchical workflows\n- Using environment variables and parameters\n\n## Basic Workflow Structure\n\n### Minimal Workflow\n\n```yaml\n# hello.yaml\nsteps:\n  - name: hello\n    command: echo \"Hello from Dagu!\"\n```\n\n### Complete Workflow Structure\n\n```yaml\nname: my_workflow\ndescription: Description of what this workflow does\n\n# Schedule (optional)\nschedule: \"0 2 * * *\"  # Cron format: daily at 2 AM\n\n# Environment variables\nenv:\n  - KEY: value\n  - DB_HOST: localhost\n\n# Parameters\nparams: ENVIRONMENT=production\n\n# Email notifications (optional)\nmailOn:\n  failure: true\n  success: false\n\nsmtp:\n  host: smtp.example.com\n  port: 587\n\nerrorMail:\n  from: dagu@example.com\n  to: alerts@example.com\n\n# Workflow steps\nsteps:\n  - name: step1\n    command: echo \"First step\"\n\n  - name: step2\n    command: echo \"Second step\"\n    depends:\n      - step1\n```\n\n## Steps\n\n### Basic Step\n\n```yaml\nsteps:\n  - name: greet\n    command: echo \"Hello, World!\"\n```\n\n### Step with Script\n\n```yaml\nsteps:\n  - name: process\n    command: |\n      echo \"Starting processing...\"\n      ./scripts/process.sh\n      echo \"Done!\"\n```\n\n### Step with Working Directory\n\n```yaml\nsteps:\n  - name: build\n    dir: /path/to/project\n    command: make build\n```\n\n### Step with Environment Variables\n\n```yaml\nsteps:\n  - name: deploy\n    env:\n      - ENVIRONMENT: production\n      - API_KEY: $API_KEY  # From global env\n    command: ./deploy.sh\n```\n\n## Executors\n\n### Command Executor (Default)\n\n```yaml\nsteps:\n  - name: shell_command\n    command: ./script.sh\n```\n\n### Docker Executor\n\n```yaml\nsteps:\n  - name: run_in_container\n    executor:\n      type: docker\n      config:\n        image: alpine:latest\n    command: echo \"Running in Docker\"\n\n  - name: with_volumes\n    executor:\n      type: docker\n      config:\n        image: node:18\n        volumes:\n          - /host/path:/container/path\n        env:\n          - NODE_ENV=production\n    command: npm run build\n```\n\n### SSH Executor\n\n```yaml\nsteps:\n  - name: remote_execution\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: server.example.com\n        key: /path/to/ssh/key\n    command: ./remote_script.sh\n```\n\n### HTTP Executor\n\n```yaml\nsteps:\n  - name: api_call\n    executor:\n      type: http\n      config:\n        method: POST\n        url: https://api.example.com/webhook\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer $API_TOKEN\n        body: |\n          {\n            \"event\": \"workflow_complete\",\n            \"timestamp\": \"{{.timestamp}}\"\n          }\n```\n\n### Mail Executor\n\n```yaml\nsteps:\n  - name: send_notification\n    executor:\n      type: mail\n      config:\n        to: user@example.com\n        from: dagu@example.com\n        subject: Workflow Complete\n        message: |\n          The workflow has completed successfully.\n          Time: {{.timestamp}}\n```\n\n### JQ Executor\n\n```yaml\nsteps:\n  - name: transform_json\n    executor:\n      type: jq\n      config:\n        query: '.users[] | select(.active == true) | .email'\n    command: cat users.json\n```\n\n## Step Dependencies\n\n### Simple Dependencies\n\n```yaml\nsteps:\n  - name: download\n    command: wget https://example.com/data.zip\n\n  - name: extract\n    depends:\n      - download\n    command: unzip data.zip\n\n  - name: process\n    depends:\n      - extract\n    command: ./process.sh\n```\n\n### Multiple Dependencies\n\n```yaml\nsteps:\n  - name: fetch_data\n    command: ./fetch.sh\n\n  - name: fetch_config\n    command: ./fetch_config.sh\n\n  - name: process\n    depends:\n      - fetch_data\n      - fetch_config\n    command: ./process.sh\n```\n\n### Parallel Execution\n\n```yaml\n# These run in parallel (no dependencies)\nsteps:\n  - name: task1\n    command: ./task1.sh\n\n  - name: task2\n    command: ./task2.sh\n\n  - name: task3\n    command: ./task3.sh\n\n  # This waits for all above to complete\n  - name: finalize\n    depends:\n      - task1\n      - task2\n      - task3\n    command: ./finalize.sh\n```\n\n## Conditional Execution\n\n### Preconditions\n\n```yaml\nsteps:\n  - name: deploy_production\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    command: ./deploy.sh\n```\n\n### Continue On Failure\n\n```yaml\nsteps:\n  - name: optional_step\n    continueOn:\n      failure: true\n    command: ./might_fail.sh\n\n  - name: cleanup\n    depends:\n      - optional_step\n    command: ./cleanup.sh  # Runs even if optional_step fails\n```\n\n## Error Handling and Retries\n\n### Retry Configuration\n\n```yaml\nsteps:\n  - name: flaky_api_call\n    command: curl https://api.example.com/data\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n```\n\n### Exponential Backoff\n\n```yaml\nsteps:\n  - name: with_backoff\n    command: ./external_api.sh\n    retryPolicy:\n      limit: 5\n      intervalSec: 5\n      exponentialBackoff: true  # 5s, 10s, 20s, 40s, 80s\n```\n\n### Signal on Stop\n\n```yaml\nsteps:\n  - name: graceful_shutdown\n    command: ./long_running_process.sh\n    signalOnStop: SIGTERM  # Send SIGTERM instead of SIGKILL\n```\n\n## Data Flow\n\n### Output Variables\n\n```yaml\nsteps:\n  - name: generate_id\n    command: echo \"ID_$(date +%s)\"\n    output: PROCESS_ID\n\n  - name: use_id\n    depends:\n      - generate_id\n    command: echo \"Processing with ID: $PROCESS_ID\"\n```\n\n### Script Output\n\n```yaml\nsteps:\n  - name: get_config\n    script: |\n      #!/bin/bash\n      export DB_HOST=\"localhost\"\n      export DB_PORT=\"5432\"\n    output: DB_CONFIG\n\n  - name: connect\n    depends:\n      - get_config\n    command: ./connect.sh $DB_HOST $DB_PORT\n```\n\n## Scheduling\n\n### Cron Schedule\n\n```yaml\n# Daily at 2 AM\nschedule: \"0 2 * * *\"\n\n# Every Monday at 9 AM\nschedule: \"0 9 * * 1\"\n\n# Every 15 minutes\nschedule: \"*/15 * * * *\"\n\n# First day of month at midnight\nschedule: \"0 0 1 * *\"\n```\n\n### Start/Stop Times\n\n```yaml\n# Only run during business hours\nschedule:\n  start: \"2024-01-01\"\n  end: \"2024-12-31\"\n  cron: \"0 9-17 * * 1-5\"  # Mon-Fri, 9 AM to 5 PM\n```\n\n## Environment Variables\n\n### Global Environment\n\n```yaml\nenv:\n  - ENVIRONMENT: production\n  - LOG_LEVEL: info\n  - API_URL: https://api.example.com\n\nsteps:\n  - name: use_env\n    command: echo \"Environment: $ENVIRONMENT\"\n```\n\n### Step-Level Environment\n\n```yaml\nsteps:\n  - name: with_custom_env\n    env:\n      - CUSTOM_VAR: value\n      - OVERRIDE: step_value\n    command: ./script.sh\n```\n\n### Environment from File\n\n```yaml\nenv:\n  - .env  # Load from .env file\n\nsteps:\n  - name: use_env_file\n    command: echo \"DB_HOST: $DB_HOST\"\n```\n\n## Parameters\n\n### Defining Parameters\n\n```yaml\nparams: ENVIRONMENT=development VERSION=1.0.0\n\nsteps:\n  - name: deploy\n    command: ./deploy.sh $ENVIRONMENT $VERSION\n```\n\n### Using Parameters\n\n```bash\n# Run with default parameters\ndagu start workflow.yaml\n\n# Override parameters\ndagu start workflow.yaml ENVIRONMENT=production VERSION=2.0.0\n```\n\n## Sub-Workflows\n\n### Calling Sub-Workflows\n\n```yaml\n# main.yaml\nsteps:\n  - name: run_sub_workflow\n    run: sub_workflow.yaml\n    params: PARAM=value\n\n  - name: another_sub\n    run: workflows/another.yaml\n```\n\n### Hierarchical Workflows\n\n```yaml\n# orchestrator.yaml\nsteps:\n  - name: data_ingestion\n    run: workflows/ingest.yaml\n\n  - name: data_processing\n    depends:\n      - data_ingestion\n    run: workflows/process.yaml\n\n  - name: data_export\n    depends:\n      - data_processing\n    run: workflows/export.yaml\n```\n\n## Handlers\n\n### Cleanup Handler\n\n```yaml\nhandlerOn:\n  exit:\n    - name: cleanup\n      command: ./cleanup.sh\n\nsteps:\n  - name: main_task\n    command: ./task.sh\n```\n\n### Error Handler\n\n```yaml\nhandlerOn:\n  failure:\n    - name: send_alert\n      executor:\n        type: mail\n        config:\n          to: alerts@example.com\n          subject: \"Workflow Failed\"\n          message: \"Workflow {{.Name}} failed at {{.timestamp}}\"\n\nsteps:\n  - name: risky_operation\n    command: ./operation.sh\n```\n\n### Success Handler\n\n```yaml\nhandlerOn:\n  success:\n    - name: notify_success\n      command: ./notify.sh \"Workflow completed successfully\"\n\nsteps:\n  - name: task\n    command: ./task.sh\n```\n\n## Templates and Variables\n\n### Built-in Variables\n\n```yaml\nsteps:\n  - name: use_variables\n    command: |\n      echo \"Workflow: {{.Name}}\"\n      echo \"Step: {{.Step.Name}}\"\n      echo \"Timestamp: {{.timestamp}}\"\n      echo \"Request ID: {{.requestId}}\"\n```\n\n### Custom Templates\n\n```yaml\nparams: USER=alice\n\nsteps:\n  - name: templated\n    command: echo \"Hello, {{.Params.USER}}!\"\n```\n\n## Common Patterns\n\n### ETL Pipeline\n\n```yaml\nname: etl_pipeline\ndescription: Extract, Transform, Load data pipeline\n\nschedule: \"0 2 * * *\"  # Daily at 2 AM\n\nenv:\n  - DATA_SOURCE: s3://bucket/data\n  - TARGET_DB: postgresql://localhost/warehouse\n\nsteps:\n  - name: extract\n    command: ./extract.sh $DATA_SOURCE\n    output: EXTRACTED_FILE\n\n  - name: transform\n    depends:\n      - extract\n    command: ./transform.sh $EXTRACTED_FILE\n    output: TRANSFORMED_FILE\n\n  - name: load\n    depends:\n      - transform\n    command: ./load.sh $TRANSFORMED_FILE $TARGET_DB\n\n  - name: cleanup\n    depends:\n      - load\n    command: rm -f $EXTRACTED_FILE $TRANSFORMED_FILE\n\nhandlerOn:\n  failure:\n    - name: alert\n      executor:\n        type: mail\n        config:\n          to: data-team@example.com\n          subject: \"ETL Pipeline Failed\"\n```\n\n### Multi-Environment Deployment\n\n```yaml\nname: deploy\ndescription: Deploy application to multiple environments\n\nparams: ENVIRONMENT=staging VERSION=latest\n\nsteps:\n  - name: build\n    command: docker build -t app:$VERSION .\n\n  - name: test\n    depends:\n      - build\n    command: docker run app:$VERSION npm test\n\n  - name: deploy_staging\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"staging\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: staging.example.com\n    command: ./deploy.sh $VERSION\n\n  - name: deploy_production\n    depends:\n      - test\n    preconditions:\n      - condition: \"`echo $ENVIRONMENT`\"\n        expected: \"production\"\n    executor:\n      type: ssh\n      config:\n        user: deploy\n        host: prod.example.com\n    command: ./deploy.sh $VERSION\n```\n\n### Data Backup Workflow\n\n```yaml\nname: database_backup\ndescription: Automated database backup workflow\n\nschedule: \"0 3 * * *\"  # Daily at 3 AM\n\nenv:\n  - DB_HOST: localhost\n  - DB_NAME: myapp\n  - BACKUP_DIR: /backups\n  - S3_BUCKET: s3://backups/db\n\nsteps:\n  - name: create_backup\n    command: |\n      TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n      pg_dump -h $DB_HOST $DB_NAME > $BACKUP_DIR/backup_$TIMESTAMP.sql\n      echo \"backup_$TIMESTAMP.sql\"\n    output: BACKUP_FILE\n\n  - name: compress\n    depends:\n      - create_backup\n    command: gzip $BACKUP_DIR/$BACKUP_FILE\n    output: COMPRESSED_FILE\n\n  - name: upload_to_s3\n    depends:\n      - compress\n    command: aws s3 cp $BACKUP_DIR/$COMPRESSED_FILE.gz $S3_BUCKET/\n\n  - name: cleanup_old_backups\n    depends:\n      - upload_to_s3\n    command: |\n      find $BACKUP_DIR -name \"*.sql.gz\" -mtime +30 -delete\n      aws s3 ls $S3_BUCKET/ | awk '{print $4}' | head -n -30 | xargs -I {} aws s3 rm $S3_BUCKET/{}\n\nhandlerOn:\n  failure:\n    - name: alert_failure\n      executor:\n        type: mail\n        config:\n          to: dba@example.com\n          subject: \"Backup Failed\"\n  success:\n    - name: log_success\n      command: echo \"Backup completed at $(date)\" >> /var/log/backups.log\n```\n\n### Monitoring and Alerts\n\n```yaml\nname: health_check\ndescription: Monitor services and send alerts\n\nschedule: \"*/5 * * * *\"  # Every 5 minutes\n\nsteps:\n  - name: check_web_service\n    command: curl -f https://app.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_api_service\n    command: curl -f https://api.example.com/health\n    retryPolicy:\n      limit: 3\n      intervalSec: 10\n    continueOn:\n      failure: true\n\n  - name: check_database\n    command: pg_isready -h db.example.com\n    continueOn:\n      failure: true\n\nhandlerOn:\n  failure:\n    - name: alert_on_failure\n      executor:\n        type: http\n        config:\n          method: POST\n          url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n          headers:\n            Content-Type: application/json\n          body: |\n            {\n              \"text\": \"⚠️ Service health check failed\",\n              \"attachments\": [{\n                \"color\": \"danger\",\n                \"fields\": [\n                  {\"title\": \"Workflow\", \"value\": \"{{.Name}}\", \"short\": true},\n                  {\"title\": \"Time\", \"value\": \"{{.timestamp}}\", \"short\": true}\n                ]\n              }]\n            }\n```\n\n## Best Practices\n\n### Workflow Organization\n\n```yaml\n# Good: Clear, descriptive names\nname: user_data_sync\ndescription: Synchronize user data from CRM to database\n\n# Good: Logical step names\nsteps:\n  - name: fetch_from_crm\n  - name: validate_data\n  - name: update_database\n\n# Avoid: Generic names\nname: workflow1\nsteps:\n  - name: step1\n  - name: step2\n```\n\n### Error Handling\n\n```yaml\n# Always define error handlers for critical workflows\nhandlerOn:\n  failure:\n    - name: cleanup\n      command: ./cleanup.sh\n    - name: notify\n      executor:\n        type: mail\n        config:\n          to: team@example.com\n\n# Use retries for flaky operations\nsteps:\n  - name: api_call\n    command: curl https://api.example.com\n    retryPolicy:\n      limit: 3\n      intervalSec: 5\n      exponentialBackoff: true\n```\n\n### Environment Management\n\n```yaml\n# Use parameters for environment-specific values\nparams: ENVIRONMENT=development\n\n# Load environment from files\nenv:\n  - config/$ENVIRONMENT.env\n\n# Override in production\n# dagu start workflow.yaml ENVIRONMENT=production\n```\n\n### Modular Workflows\n\n```yaml\n# Break complex workflows into sub-workflows\nsteps:\n  - name: data_ingestion\n    run: workflows/ingestion.yaml\n\n  - name: data_transformation\n    run: workflows/transformation.yaml\n    depends:\n      - data_ingestion\n```\n\n## Key Principles\n\n- **Keep workflows focused**: One workflow per logical task\n- **Use dependencies wisely**: Parallelize when possible\n- **Handle errors explicitly**: Define failure handlers\n- **Use retries for flaky operations**: Network calls, external APIs\n- **Parameterize configurations**: Make workflows reusable\n- **Document workflows**: Add clear names and descriptions\n- **Test workflows**: Start with small, focused workflows\n- **Monitor and alert**: Use handlers to track workflow health"
              },
              {
                "name": "elixir-anti-patterns",
                "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices",
                "path": "elixir/skills/anti-patterns/SKILL.md",
                "frontmatter": {
                  "name": "elixir-anti-patterns",
                  "description": "Identifies and helps refactor Elixir anti-patterns including code smells, design issues, and bad practices"
                },
                "content": "# Elixir Anti-Patterns Detection and Refactoring\n\nYou are an expert at identifying Elixir anti-patterns and suggesting idiomatic refactorings. Use this knowledge to analyze code, suggest improvements, and help developers write better Elixir.\n\n## Code-Related Anti-Patterns\n\n### 1. Comments Overuse\n**Problem:** Excessive or self-explanatory comments reduce readability rather than enhance it.\n\n**Detection:**\n- Inline comments explaining obvious code\n- Comments for every function line\n- Comments duplicating what code already says clearly\n\n**Refactoring:**\n- Use clear function and variable names instead of explanatory comments\n- Replace inline comments with `@doc` and `@moduledoc` for documentation\n- Use module attributes for configuration values\n\n**Example:**\n```elixir\n# Bad\ndef calculate() do\n  # Get current time\n  now = DateTime.utc_now()\n  # Add 5 minutes\n  DateTime.add(now, 5 * 60, :second)\nend\n\n# Good\n@minutes_to_add 5\n\ndef timestamp_five_minutes_from_now do\n  now = DateTime.utc_now()\n  DateTime.add(now, @minutes_to_add * 60, :second)\nend\n```\n\n### 2. Complex `else` Clauses in `with`\n**Problem:** Flattening all error handling into a single complex `else` block obscures which clause produced which error.\n\n**Detection:**\n- Large `else` blocks with many pattern match clauses\n- Difficulty determining error sources\n- Complex error handling logic in `else`\n\n**Refactoring:**\n- Normalize return types in private functions\n- Handle errors closer to their source\n- Let `with` focus on success paths\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, decoded} <- Jason.decode(content) do\n    {:ok, decoded}\n  else\n    {:error, :enoent} -> {:error, :file_not_found}\n    {:error, %Jason.DecodeError{}} -> {:error, :invalid_json}\n    {:error, reason} -> {:error, reason}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- read_file(path),\n       {:ok, config} <- parse_json(content) do\n    {:ok, config}\n  end\nend\n\ndefp read_file(path) do\n  case File.read(path) do\n    {:ok, content} -> {:ok, content}\n    {:error, :enoent} -> {:error, :file_not_found}\n    error -> error\n  end\nend\n\ndefp parse_json(content) do\n  case Jason.decode(content) do\n    {:ok, data} -> {:ok, data}\n    {:error, _} -> {:error, :invalid_json}\n  end\nend\n```\n\n### 3. Complex Extractions in Clauses\n**Problem:** Extracting values across multiple clauses and arguments makes it unclear which variables serve pattern/guard purposes versus function body usage.\n\n**Detection:**\n- Many variable extractions in function heads\n- Mixed guard and body variable usage\n- Unclear variable purposes\n\n**Refactoring:**\n- Extract only pattern/guard-related variables in function signatures\n- Use capture patterns like `%User{age: age} = user`\n- Extract body variables inside the clause\n\n**Example:**\n```elixir\n# Bad\ndef process(%User{age: age, name: name, email: email} = user) when age >= 18 do\n  # Only using name and email in body, not age\n  send_email(email, \"Hello #{name}\")\nend\n\n# Good\ndef process(%User{age: age} = user) when age >= 18 do\n  send_email(user.email, \"Hello #{user.name}\")\nend\n```\n\n### 4. Dynamic Atom Creation\n**Problem:** Atoms aren't garbage-collected and are limited to ~1 million. Uncontrolled dynamic atom creation poses memory and security risks.\n\n**Detection:**\n- `String.to_atom/1` with untrusted input\n- Converting user input directly to atoms\n- Unbounded atom creation in loops\n\n**Refactoring:**\n- Use explicit mappings via pattern-matching\n- Use `String.to_existing_atom/1` with pre-defined atoms\n- Keep strings when atom conversion isn't necessary\n\n**Example:**\n```elixir\n# Bad - Security risk!\ndef set_role(user, role_string) do\n  %{user | role: String.to_atom(role_string)}\nend\n\n# Good\ndef set_role(user, role) when role in [:admin, :editor, :viewer] do\n  %{user | role: role}\nend\n\n# Or with pattern matching\ndef set_role(user, \"admin\"), do: %{user | role: :admin}\ndef set_role(user, \"editor\"), do: %{user | role: :editor}\ndef set_role(user, \"viewer\"), do: %{user | role: :viewer}\ndef set_role(_user, invalid), do: {:error, \"Invalid role: #{invalid}\"}\n```\n\n### 5. Long Parameter List\n**Problem:** Functions with excessive parameters become confusing and error-prone to use.\n\n**Detection:**\n- Functions with 4+ parameters\n- Parameters that are conceptually related\n- Difficult to remember parameter order\n\n**Refactoring:**\n- Group related parameters into maps or structs\n- Use keyword lists for optional parameters\n- Create domain objects\n\n**Example:**\n```elixir\n# Bad\ndef create_loan(user_id, user_name, user_email, book_id, book_title, book_isbn) do\n  # ...\nend\n\n# Good\ndef create_loan(user, book) do\n  # ...\nend\n\n# Or with keyword list for options\ndef create_loan(user, book, opts \\\\ []) do\n  duration = Keyword.get(opts, :duration, 14)\n  renewable = Keyword.get(opts, :renewable, true)\n  # ...\nend\n```\n\n### 6. Namespace Trespassing\n**Problem:** Defining modules outside your library's namespace risks conflicts since the Erlang VM loads only one module instance per name.\n\n**Detection:**\n- Library defining modules in common namespaces (e.g., `Plug.*` when you're not Plug)\n- Modules without library prefix\n- Potential naming conflicts with other libraries\n\n**Refactoring:**\n- Always prefix modules with your library namespace\n- Use clear, unique top-level module names\n\n**Example:**\n```elixir\n# Bad - Library named :plug_auth\ndefmodule Plug.Auth do\n  # This conflicts with the actual Plug library!\nend\n\n# Good\ndefmodule PlugAuth do\n  # ...\nend\n\ndefmodule PlugAuth.Session do\n  # ...\nend\n```\n\n### 7. Non-assertive Map Access\n**Problem:** Using dynamic access (`map[:key]`) for required keys masks missing data, allowing `nil` to propagate instead of failing fast.\n\n**Detection:**\n- `map[:key]` for required/expected keys\n- Nil checks after map access\n- Silent failures from missing keys\n\n**Refactoring:**\n- Use static access (`map.key`) for required keys\n- Pattern-match on struct/map keys\n- Reserve dynamic access for optional fields\n\n**Example:**\n```elixir\n# Bad\ndef distance(point) do\n  x = point[:x]  # Returns nil if :x is missing!\n  y = point[:y]\n  :math.sqrt(x * x + y * y)  # Crashes on nil, but unclear why\nend\n\n# Good\ndef distance(%{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)  # Clear error if keys missing\nend\n\n# Or with structs\ndefmodule Point do\n  defstruct [:x, :y]\nend\n\ndef distance(%Point{x: x, y: y}) do\n  :math.sqrt(x * x + y * y)\nend\n```\n\n### 8. Non-assertive Pattern Matching\n**Problem:** Writing defensive code that returns incorrect values instead of using pattern matching to assert expected structures causes silent failures.\n\n**Detection:**\n- Defensive nil checks instead of pattern matching\n- Functions returning invalid data on unexpected input\n- Avoiding crashes when crashes are appropriate\n\n**Refactoring:**\n- Use pattern matching to assert expected structures\n- Let functions crash on invalid input\n- Trust supervisors to handle failures\n\n**Example:**\n```elixir\n# Bad\ndef parse_query_param(param) do\n  case String.split(param, \"=\") do\n    [key, value] -> {key, value}\n    _ -> {\"\", \"\"}  # Silent failure!\n  end\nend\n\n# Good\ndef parse_query_param(param) do\n  [key, value] = String.split(param, \"=\")\n  {key, value}\nend\n# Crashes with clear error if format is wrong - this is good!\n```\n\n### 9. Non-assertive Truthiness\n**Problem:** Using truthiness operators (`&&`, `||`, `!`) when all operands are boolean is unnecessarily generic and unclear.\n\n**Detection:**\n- `&&`, `||`, `!` with boolean expressions\n- Comparisons like `is_binary(x) && is_integer(y)`\n- Mixing boolean and truthy logic\n\n**Refactoring:**\n- Use `and`, `or`, `not` for boolean-only operations\n- Reserve `&&`, `||`, `!` for truthy/falsy logic\n\n**Example:**\n```elixir\n# Bad\ndef valid_user?(name, age) do\n  is_binary(name) && is_integer(age) && age >= 18\nend\n\n# Good\ndef valid_user?(name, age) do\n  is_binary(name) and is_integer(age) and age >= 18\nend\n\n# Truthy operators are OK for nil/value checks\ndef get_name(user) do\n  user[:name] || \"Anonymous\"\nend\n```\n\n### 10. Structs with 32 Fields or More\n**Problem:** Structs with 32+ fields switch from Erlang's efficient flat-map representation to hash maps, increasing memory usage.\n\n**Detection:**\n- Struct definitions with 32+ fields\n- Large, flat data structures\n- Performance degradation with many fields\n\n**Refactoring:**\n- Nest optional fields into metadata structures\n- Use nested structs for related fields\n- Group frequently-accessed fields separately\n\n**Example:**\n```elixir\n# Bad\ndefmodule User do\n  defstruct [\n    :id, :email, :name, :age, :address, :city, :state, :zip,\n    :phone, :mobile, :fax, :company, :title, :department,\n    :created_at, :updated_at, :last_login, :login_count,\n    :preference1, :preference2, :preference3, :preference4,\n    # ... 15 more fields\n  ]\nend\n\n# Good\ndefmodule User do\n  defstruct [\n    :id,\n    :email,\n    :name,\n    :profile,      # Nested struct\n    :preferences,  # Nested struct\n    :metadata      # Nested struct\n  ]\nend\n\ndefmodule User.Profile do\n  defstruct [:age, :phone, :mobile, :address, :city, :state, :zip]\nend\n\ndefmodule User.Preferences do\n  defstruct [:theme, :notifications, :language]\nend\n```\n\n## Design-Related Anti-Patterns\n\n### 1. Alternative Return Types\n**Problem:** Functions with options that drastically change their return type make it unclear what the function actually returns.\n\n**Detection:**\n- Options that change return type structure\n- Functions returning different types based on flags\n- Unclear function contracts\n\n**Refactoring:**\n- Create separate, specifically-named functions\n- Keep return types consistent within a function\n\n**Example:**\n```elixir\n# Bad\ndef parse(string, opts \\\\ []) do\n  case Integer.parse(string) do\n    {int, rest} ->\n      if opts[:discard_rest], do: int, else: {int, rest}\n    :error ->\n      :error\n  end\nend\n\n# Good\ndef parse(string) do\n  case Integer.parse(string) do\n    {int, rest} -> {int, rest}\n    :error -> :error\n  end\nend\n\ndef parse_discard_rest(string) do\n  case Integer.parse(string) do\n    {int, _rest} -> int\n    :error -> :error\n  end\nend\n```\n\n### 2. Boolean Obsession\n**Problem:** Using multiple booleans with overlapping states instead of atoms or composite types to represent domain concepts.\n\n**Detection:**\n- Multiple boolean parameters\n- Overlapping boolean states\n- Complex boolean logic\n\n**Refactoring:**\n- Replace multiple booleans with a single atom/enum option\n- Prefer atoms over booleans even for single arguments\n- Use domain-specific types\n\n**Example:**\n```elixir\n# Bad\ndef create_user(name, email, admin: false, editor: false, viewer: true) do\n  # What if admin: true, editor: true?\nend\n\n# Good\ndef create_user(name, email, role: :viewer) do\n  # Clear: role can be :admin, :editor, or :viewer\nend\n```\n\n### 3. Exceptions for Control-Flow\n**Problem:** Using `try/rescue` for expected errors instead of pattern matching with case statements and tuple returns.\n\n**Detection:**\n- `try/rescue` blocks for normal operation errors\n- Using `!` functions and rescuing\n- Exceptions in normal business logic\n\n**Refactoring:**\n- Use non-bang functions returning `{:ok, value}` or `{:error, reason}`\n- Reserve exceptions for invalid arguments and programming errors\n- Use pattern matching for error handling\n\n**Example:**\n```elixir\n# Bad\ndef read_config(path) do\n  try do\n    content = File.read!(path)\n    Jason.decode!(content)\n  rescue\n    e -> {:error, e}\n  end\nend\n\n# Good\ndef read_config(path) do\n  with {:ok, content} <- File.read(path),\n       {:ok, config} <- Jason.decode(content) do\n    {:ok, config}\n  end\nend\n```\n\n### 4. Primitive Obsession\n**Problem:** Excessively using basic types (strings, integers) instead of creating composite types to represent structured domain concepts.\n\n**Detection:**\n- Passing related primitives separately\n- String/integer parameters representing complex concepts\n- Lack of domain modeling\n\n**Refactoring:**\n- Create domain-specific structs or maps\n- Introduce parser functions converting primitives to structured data\n- Use types to enforce business rules\n\n**Example:**\n```elixir\n# Bad\ndef create_address(street, city, state, zip, country) do\n  # All strings, no validation\n  \"#{street}, #{city}, #{state} #{zip}, #{country}\"\nend\n\n# Good\ndefmodule Address do\n  defstruct [:street, :city, :state, :zip, :country]\n\n  def new(attrs) do\n    struct!(__MODULE__, attrs)\n  end\n\n  def format(%__MODULE__{} = address) do\n    \"#{address.street}, #{address.city}, #{address.state} #{address.zip}, #{address.country}\"\n  end\nend\n```\n\n### 5. Unrelated Multi-Clause Function\n**Problem:** Grouping completely unrelated business logic into one multi-clause function.\n\n**Detection:**\n- Single function handling multiple unrelated types\n- Overly broad type specifications\n- No conceptual relationship between clauses\n\n**Refactoring:**\n- Split into distinct functions with specific names\n- Reserve multi-clause patterns for related functionality variations\n- Use protocols for polymorphism when appropriate\n\n**Example:**\n```elixir\n# Bad\ndef update(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update(%Animal{} = animal) do\n  # Completely different animal logic\nend\n\n# Good\ndef update_product(%Product{} = product) do\n  # Product-specific logic\nend\n\ndef update_animal(%Animal{} = animal) do\n  # Animal-specific logic\nend\n\n# Or use a protocol\ndefprotocol Updatable do\n  def update(item)\nend\n\ndefimpl Updatable, for: Product do\n  def update(product), do: # ...\nend\n\ndefimpl Updatable, for: Animal do\n  def update(animal), do: # ...\nend\n```\n\n### 6. Using Application Configuration for Libraries\n**Problem:** Libraries relying on global application environment configuration prevent multiple dependent applications from configuring the library differently.\n\n**Detection:**\n- `Application.get_env/2` or `Application.fetch_env!/2` in library code\n- Global configuration requirements\n- Inability to configure per-consumer\n\n**Refactoring:**\n- Accept configuration via function parameters\n- Use keyword lists with sensible defaults\n- Allow runtime configuration\n\n**Example:**\n```elixir\n# Bad - Library code\ndef split(string) do\n  parts = Application.fetch_env!(:dash_splitter, :parts)\n  String.split(string, \"-\", parts: parts)\nend\n\n# Good\ndef split(string, opts \\\\ []) do\n  parts = Keyword.get(opts, :parts, 2)\n  String.split(string, \"-\", parts: parts)\nend\n```\n\n## Usage Guidelines\n\nWhen reviewing or writing Elixir code:\n\n1. **Scan for anti-patterns** - Check code against the patterns listed above\n2. **Explain the problem** - Help the developer understand why it's an issue\n3. **Suggest refactoring** - Provide concrete, idiomatic alternatives\n4. **Consider context** - Sometimes anti-patterns are acceptable for specific use cases\n5. **Prioritize** - Focus on high-impact issues first (security, performance, maintainability)\n\n## Key Principles\n\n- **Let it crash** - Use pattern matching to assert expectations; don't write defensive code\n- **Fail fast** - Expose errors early rather than propagating nil or invalid data\n- **Be explicit** - Prefer clear, specific code over clever or terse solutions\n- **Model your domain** - Create types that represent business concepts\n- **Design for clarity** - Code should be obvious to read and maintain"
              },
              {
                "name": "elixir-config",
                "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                "path": "elixir/skills/config/SKILL.md",
                "frontmatter": {
                  "name": "elixir-config",
                  "description": "Guide for Elixir application configuration focusing on runtime vs compile-time config, config.exs, runtime.exs, Application.compile_env, and Application.get_env best practices",
                  "license": "MIT"
                },
                "content": "# Elixir Configuration\n\nGuide for proper application configuration in Elixir, with emphasis on understanding and correctly using runtime vs compile-time configuration.\n\n## When to Activate\n\nUse this skill when:\n- Setting up or modifying application configuration\n- Choosing between `config.exs` and `runtime.exs`\n- Deciding between `Application.compile_env` and `Application.get_env`\n- Debugging configuration-related issues\n- Working with releases or deployment configuration\n- Migrating from `use Mix.Config` to `import Config`\n- Writing libraries that need configuration\n\n## Critical Principle\n\n> **Runtime configuration is the preferred approach.** Only use compile-time configuration when values must affect compilation itself.\n\n## Configuration Files\n\n### config/config.exs (Compile-Time)\n\nEvaluated during project compilation, before your application starts.\n\n```elixir\nimport Config\n\n# Basic configuration\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  username: \"postgres\",\n  password: \"postgres\",\n  hostname: \"localhost\"\n\n# Environment-specific config\nconfig :my_app,\n  environment: config_env()\n\n# Import environment-specific config files\nimport_config \"#{config_env()}.exs\"\n```\n\n**Key characteristics:**\n- Runs at compile time\n- Uses `import Config` (not `use Mix.Config`)\n- Can use `config_env()` and `config_target()`\n- Can import other config files with `import_config/1`\n- Deep-merges keyword lists\n- **Library config.exs is NOT evaluated when used as a dependency**\n\n### config/runtime.exs (Runtime)\n\nEvaluated right before applications start in both Mix and releases.\n\n```elixir\nimport Config\n\n# Read from environment variables\nconfig :my_app, MyApp.Repo,\n  database: System.get_env(\"DATABASE_NAME\") || \"my_app_dev\",\n  username: System.get_env(\"DATABASE_USER\") || \"postgres\",\n  password: System.get_env(\"DATABASE_PASSWORD\") || \"postgres\",\n  hostname: System.get_env(\"DATABASE_HOST\") || \"localhost\",\n  pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\n\n# Conditional runtime configuration\nif config_env() == :prod do\n  config :my_app, MyAppWeb.Endpoint,\n    secret_key_base: System.fetch_env!(\"SECRET_KEY_BASE\"),\n    http: [port: String.to_integer(System.fetch_env!(\"PORT\"))]\nend\n```\n\n**Key characteristics:**\n- Runs at application startup (both dev and prod)\n- Executes in both Mix projects and releases\n- Perfect for environment variables and runtime values\n- **Does NOT support `import_config/1`**\n- Can use `System.get_env` and `System.fetch_env!`\n\n### config/dev.exs, config/test.exs, config/prod.exs\n\nEnvironment-specific compile-time configuration, typically imported from `config.exs`:\n\n```elixir\n# config/config.exs\nimport_config \"#{config_env()}.exs\"\n\n# config/dev.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  show_sensitive_data_on_connection_error: true,\n  pool_size: 10\n\n# config/test.exs\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox,\n  pool_size: 10\n\n# config/prod.exs\nimport Config\n\n# Production-specific compile-time config only\nconfig :my_app, MyAppWeb.Endpoint,\n  cache_static_manifest: \"priv/static/cache_manifest.json\"\n```\n\n## Accessing Configuration\n\n### Runtime Access (Preferred)\n\nUse in function bodies to read configuration at runtime:\n\n#### Application.get_env/3\n\n```elixir\ndefmodule MyApp.Service do\n  def start_link do\n    # Get with default value\n    timeout = Application.get_env(:my_app, :timeout, 5000)\n    GenServer.start_link(__MODULE__, timeout, name: __MODULE__)\n  end\nend\n```\n\n**When to use:**\n- Reading config in function bodies (most common)\n- When a sensible default exists\n- When config might change between environments\n\n#### Application.fetch_env!/2\n\n```elixir\ndefmodule MyApp.Mailer do\n  def deliver(email) do\n    # Raise if not configured (for required config)\n    api_key = Application.fetch_env!(:my_app, :mailgun_api_key)\n    send_email(email, api_key)\n  end\nend\n```\n\n**When to use:**\n- Required configuration that must exist\n- When you want explicit errors for missing config\n- When no sensible default exists\n\n#### Application.fetch_env/2\n\n```elixir\ndefmodule MyApp.Cache do\n  def get(key) do\n    case Application.fetch_env(:my_app, :cache_adapter) do\n      {:ok, adapter} -> adapter.get(key)\n      :error -> nil  # No caching configured\n    end\n  end\nend\n```\n\n**When to use:**\n- Optional configuration\n- When you need pattern matching on result\n- When absence of config is a valid state\n\n### Compile-Time Access (Use Sparingly)\n\nUse only when configuration must affect compilation:\n\n#### Application.compile_env/3\n\n```elixir\ndefmodule MyApp.JSONEncoder do\n  # Only use compile_env when the value affects compilation\n  @json_library Application.compile_env(:my_app, :json_library, Jason)\n\n  def encode(data) do\n    # The specific library is compiled into the module\n    @json_library.encode(data)\n  end\nend\n```\n\n**When to use:**\n- Configuration affects which code gets compiled\n- Performance-critical paths where indirection is costly\n- Compile-time optimizations or code generation\n\n**Warning:** Mix tracks compile-time config and raises errors if values diverge between compile and runtime.\n\n#### Application.compile_env!/2\n\n```elixir\ndefmodule MyApp.Adapter do\n  # Raises at compile time if not configured\n  @adapter Application.compile_env!(:my_app, :storage_adapter)\n\n  def store(data) do\n    @adapter.put(data)\n  end\nend\n```\n\n**When to use:**\n- Required compile-time configuration\n- Adapters or behaviors selected at compile time\n\n## Common Patterns\n\n### Pattern 1: Environment Variables in Runtime\n\n**Correct approach:**\n\n```elixir\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_url: System.get_env(\"API_URL\") || \"http://localhost:4000\",\n  api_key: System.fetch_env!(\"API_KEY\")  # Required in production\n```\n\n**Access in code:**\n\n```elixir\ndefmodule MyApp.Client do\n  def call(endpoint) do\n    api_url = Application.fetch_env!(:my_app, :api_url)\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(\"#{api_url}/#{endpoint}\", [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### Pattern 2: Development vs Production Config\n\n**config/config.exs:**\n\n```elixir\nimport Config\n\n# Shared configuration for all environments\nconfig :my_app, :shared_setting, \"value\"\n\n# Import environment-specific config\nimport_config \"#{config_env()}.exs\"\n```\n\n**config/dev.exs:**\n\n```elixir\nimport Config\n\nconfig :my_app, MyApp.Repo,\n  database: \"my_app_dev\",\n  show_sensitive_data_on_connection_error: true\n```\n\n**config/runtime.exs:**\n\n```elixir\nimport Config\n\n# Runtime config for all environments\nif config_env() == :prod do\n  # Production-specific runtime config\n  database_url = System.fetch_env!(\"DATABASE_URL\")\n\n  config :my_app, MyApp.Repo,\n    url: database_url,\n    pool_size: String.to_integer(System.get_env(\"POOL_SIZE\") || \"10\")\nend\n```\n\n### Pattern 3: Storing config_env() for Runtime Access\n\n**Problem:** Can't call `config_env()` at runtime.\n\n**Solution:** Store it in config:\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app, :environment, config_env()\n\n# Then in your code:\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\n\n  def development? do\n    environment() == :dev\n  end\nend\n```\n\n### Pattern 4: Optional Features Based on Config\n\n```elixir\ndefmodule MyApp.Telemetry do\n  def setup do\n    case Application.fetch_env(:my_app, :telemetry_backend) do\n      {:ok, :datadog} -> setup_datadog()\n      {:ok, :prometheus} -> setup_prometheus()\n      :error -> :ok  # Telemetry disabled\n    end\n  end\nend\n```\n\n### Pattern 5: Child Spec with Runtime Config\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      MyApp.Repo,\n      {MyApp.Worker, Application.fetch_env!(:my_app, :worker_opts)},\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n## Anti-Patterns to Avoid\n\n### ❌ Using compile_env for Runtime Values\n\n```elixir\n# DON'T: Using compile_env for environment variables\ndefmodule MyApp.Service do\n  @api_key Application.compile_env(:my_app, :api_key)\n\n  def call do\n    # This won't work correctly in releases!\n    HTTPoison.get(url, [{\"Authorization\", @api_key}])\n  end\nend\n```\n\n**Why it's wrong:** Environment variables aren't available at compile time in releases.\n\n**Correct approach:**\n\n```elixir\ndefmodule MyApp.Service do\n  def call do\n    # Read at runtime\n    api_key = Application.fetch_env!(:my_app, :api_key)\n    HTTPoison.get(url, [{\"Authorization\", api_key}])\n  end\nend\n```\n\n### ❌ Reading Other Application's Config\n\n```elixir\n# DON'T: Directly access other app's configuration\ndefmodule MyApp do\n  def logger_level do\n    Application.get_env(:logger, :level)  # Fragile coupling\n  end\nend\n```\n\n**Why it's wrong:** Creates tight coupling and breaks encapsulation.\n\n**Correct approach:**\n\n```elixir\n# Configure it in your own app\n# config/config.exs\nconfig :my_app, :log_level, :info\n\n# Then read your own config\ndefmodule MyApp do\n  def log_level do\n    Application.get_env(:my_app, :log_level, :info)\n  end\nend\n```\n\n### ❌ Using Application Config in Libraries\n\n```elixir\n# DON'T: In a library\ndefmodule MyLibrary do\n  def process(data) do\n    # Library reading its own application environment\n    timeout = Application.get_env(:my_library, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n```\n\n**Why it's wrong:** Library `config.exs` is not evaluated when used as a dependency.\n\n**Correct approach:**\n\n```elixir\n# DO: Accept options as arguments\ndefmodule MyLibrary do\n  def process(data, opts \\\\ []) do\n    timeout = Keyword.get(opts, :timeout, 5000)\n    do_work(data, timeout)\n  end\nend\n\n# Users configure in their application\ndefmodule MyApp.Worker do\n  def run do\n    opts = Application.get_env(:my_app, :my_library_opts, [])\n    MyLibrary.process(data, opts)\n  end\nend\n```\n\n### ❌ Using Mix Module in Application Code\n\n```elixir\n# DON'T: Use Mix.env() in application code\ndefmodule MyApp do\n  def environment do\n    Mix.env()  # Won't work in releases!\n  end\nend\n```\n\n**Why it's wrong:** `Mix` is not available in production releases.\n\n**Correct approach:**\n\n```elixir\n# Store it in config\n# config/config.exs\nconfig :my_app, :environment, config_env()\n\n# Access from application environment\ndefmodule MyApp do\n  def environment do\n    Application.fetch_env!(:my_app, :environment)\n  end\nend\n```\n\n## Config Functions Reference\n\n### In Configuration Files\n\n| Function | Description | Where to Use |\n|----------|-------------|--------------|\n| `config/2` | Configure app with keyword list | All config files |\n| `config/3` | Configure app key with value | All config files |\n| `config_env/0` | Get current environment (`:dev`, `:test`, `:prod`) | All config files |\n| `config_target/0` | Get build target | All config files |\n| `import_config/1` | Import other config files | Not in `runtime.exs` |\n\n### In Application Code\n\n| Function | Return Type | Use Case |\n|----------|-------------|----------|\n| `Application.get_env/3` | `value \\| default` | Runtime with default |\n| `Application.fetch_env/2` | `{:ok, value} \\| :error` | Runtime with pattern matching |\n| `Application.fetch_env!/2` | `value` (raises if missing) | Required runtime config |\n| `Application.compile_env/3` | `value` | Compile-time with default |\n| `Application.compile_env!/2` | `value` (raises if missing) | Required compile-time config |\n\n## Migration Guide\n\n### From `use Mix.Config` to `import Config`\n\n**Old (deprecated):**\n\n```elixir\nuse Mix.Config\n\nconfig :my_app, :key, \"value\"\n\nif Mix.env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{Mix.env()}.exs\"\n```\n\n**New:**\n\n```elixir\nimport Config\n\nconfig :my_app, :key, \"value\"\n\nif config_env() == :prod do\n  config :my_app, :production, true\nend\n\nimport_config \"#{config_env()}.exs\"\n```\n\n**Changes:**\n1. Replace `use Mix.Config` with `import Config`\n2. Replace `Mix.env()` with `config_env()`\n3. Remove wildcard imports (not supported)\n\n### Moving Runtime Config to runtime.exs\n\n**Before (all in config.exs):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\"),  # Wrong place!\n  static_value: \"something\"\n```\n\n**After (split correctly):**\n\n```elixir\n# config/config.exs\nimport Config\n\nconfig :my_app,\n  static_value: \"something\"\n\n# config/runtime.exs\nimport Config\n\nconfig :my_app,\n  api_key: System.get_env(\"API_KEY\") || raise(\"API_KEY not set\")\n```\n\n## Best Practices Summary\n\n1. **Default to Runtime Configuration**: Use `Application.get_env/3` in function bodies\n2. **Use runtime.exs for Environment Variables**: Never read env vars in `config.exs`\n3. **Use compile_env Only When Necessary**: Only when config affects compilation\n4. **Libraries Should Not Use Application Config**: Accept options as function arguments\n5. **Never Use Mix in Application Code**: Use `config_env()` in config files, store result\n6. **Validate Required Config Early**: Use `fetch_env!/2` in application start for required values\n7. **Provide Sensible Defaults**: Use `get_env/3` with defaults for optional config\n8. **Document Configuration**: Add comments explaining what each config key does\n9. **Use runtime.exs for Releases**: Essential for Elixir releases and deployments\n10. **Store config_env() for Runtime Use**: Can't call `config_env()` outside config files\n\n## Debugging Configuration\n\n### Check Current Configuration\n\n```elixir\n# In IEx\nApplication.get_all_env(:my_app)\n\n# Check specific key\nApplication.fetch_env(:my_app, :some_key)\n\n# See all applications\nApplication.loaded_applications()\n```\n\n### Common Issues\n\n**Problem:** Config not available in tests\n\n```elixir\n# config/test.exs\nimport Config\n\nconfig :my_app, :test_value, \"configured\"\n```\n\n**Problem:** Different values in dev vs release\n\nCheck that `runtime.exs` is being used and environment variables are set correctly.\n\n**Problem:** Compile-time config not updating\n\n```bash\n# Clean and recompile\nmix clean\nmix compile\n```\n\n## Resources\n\n- **Config Module Docs**: https://hexdocs.pm/elixir/Config.html\n- **Application Module Docs**: https://hexdocs.pm/elixir/Application.html\n- **Runtime Configuration Guide**: https://hexdocs.pm/mix/Mix.Tasks.Release.html#module-runtime-configuration\n\n## Key Insights\n\n> \"Reading the application environment at runtime is the preferred approach.\"\n\n> \"If you are writing a library to be used by other developers, it is generally recommended to avoid the application environment, as the application environment is effectively a global storage.\"\n\n> \"config/config of a library is not evaluated when the library is used as a dependency, as configuration is always meant to configure the current project.\"\n\nConfiguration is a cross-cutting concern. Default to runtime configuration with `Application.get_env/3`, and only reach for compile-time configuration when you have a specific need for it that justifies the trade-offs."
              },
              {
                "name": "elixir-otp-concurrency",
                "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives",
                "path": "elixir/skills/otp/SKILL.md",
                "frontmatter": {
                  "name": "elixir-otp-concurrency",
                  "description": "Guide for building concurrent, fault-tolerant systems using OTP (GenServer, Supervisor, Task, Agent) and Elixir concurrency primitives"
                },
                "content": "# Elixir OTP and Concurrency\n\nThis skill activates when working with OTP behaviors, building concurrent systems, managing processes, or implementing fault-tolerant architectures in Elixir.\n\n## When to Use This Skill\n\nActivate when:\n- Implementing GenServer, GenStage, Supervisor, or other OTP behaviors\n- Designing supervision trees and fault-tolerance strategies\n- Working with Tasks, Agents, or process management\n- Building concurrent or distributed systems\n- Managing application state\n- Troubleshooting process-related issues\n\n## OTP Behaviors\n\n### GenServer - Generic Server\n\nUse GenServer for stateful processes:\n\n```elixir\ndefmodule MyApp.Counter do\n  use GenServer\n\n  # Client API\n\n  def start_link(initial_value) do\n    GenServer.start_link(__MODULE__, initial_value, name: __MODULE__)\n  end\n\n  def increment do\n    GenServer.call(__MODULE__, :increment)\n  end\n\n  def get_value do\n    GenServer.call(__MODULE__, :get)\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(initial_value) do\n    {:ok, initial_value}\n  end\n\n  @impl true\n  def handle_call(:increment, _from, state) do\n    {:reply, state + 1, state + 1}\n  end\n\n  @impl true\n  def handle_call(:get, _from, state) do\n    {:reply, state, state}\n  end\nend\n```\n\n#### GenServer Best Practices\n\n- Use `call` for synchronous requests that need a response\n- Use `cast` for asynchronous fire-and-forget messages\n- Use `handle_info` for receiving regular messages\n- Keep server callbacks fast - delegate heavy work to Tasks\n- Name processes with `via` tuples or Registry for dynamic naming\n- Implement timeouts to prevent client processes from hanging\n\n#### GenServer Patterns\n\n**Background Work:**\n```elixir\ndef init(state) do\n  schedule_work()\n  {:ok, state}\nend\n\ndef handle_info(:work, state) do\n  do_work(state)\n  schedule_work()\n  {:noreply, state}\nend\n\ndefp schedule_work do\n  Process.send_after(self(), :work, 5000)\nend\n```\n\n**State Timeouts:**\n```elixir\ndef handle_call(:get, _from, state) do\n  {:reply, state, state, {:state_timeout, 30_000, :cleanup}}\nend\n\ndef handle_state_timeout(:cleanup, state) do\n  {:stop, :normal, state}\nend\n```\n\n### Supervisor - Process Supervision\n\nBuild supervision trees for fault tolerance:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  @impl true\n  def start(_type, _args) do\n    children = [\n      # Database connection pool\n      {MyApp.Repo, []},\n\n      # PubSub system\n      {Phoenix.PubSub, name: MyApp.PubSub},\n\n      # Custom supervisor\n      {MyApp.WorkerSupervisor, []},\n\n      # Individual workers\n      {MyApp.Cache, []},\n      {MyApp.RateLimiter, []},\n\n      # Web endpoint\n      MyAppWeb.Endpoint\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\nend\n```\n\n#### Supervision Strategies\n\n**:one_for_one** - If a child dies, only that child is restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_one)\n```\n\n**:one_for_all** - If any child dies, all children are terminated and restarted\n```elixir\nSupervisor.start_link(children, strategy: :one_for_all)\n```\n\n**:rest_for_one** - If a child dies, it and all children started after it are restarted\n```elixir\nSupervisor.start_link(children, strategy: :rest_for_one)\n```\n\n#### Dynamic Supervisors\n\nFor dynamically creating processes:\n\n```elixir\ndefmodule MyApp.WorkerSupervisor do\n  use DynamicSupervisor\n\n  def start_link(init_arg) do\n    DynamicSupervisor.start_link(__MODULE__, init_arg, name: __MODULE__)\n  end\n\n  def start_worker(args) do\n    spec = {MyApp.Worker, args}\n    DynamicSupervisor.start_child(__MODULE__, spec)\n  end\n\n  @impl true\n  def init(_init_arg) do\n    DynamicSupervisor.init(strategy: :one_for_one)\n  end\nend\n```\n\n#### Restart Strategies\n\nConfigure child restart behavior:\n\n```elixir\nchildren = [\n  # Always restart (default)\n  {MyApp.CriticalWorker, restart: :permanent},\n\n  # Never restart\n  {MyApp.OneTimeTask, restart: :temporary},\n\n  # Only restart on abnormal exit\n  {MyApp.OptionalWorker, restart: :transient}\n]\n```\n\n### Task - Concurrent Work\n\n#### Fire-and-forget Tasks\n\nFor concurrent work without needing results:\n\n```elixir\nTask.start(fn ->\n  send_email(user, \"Welcome!\")\nend)\n```\n\n#### Awaited Tasks\n\nFor concurrent work with results:\n\n```elixir\ntask = Task.async(fn ->\n  expensive_computation()\nend)\n\n# Do other work...\n\nresult = Task.await(task, 5000)  # 5 second timeout\n```\n\n#### Supervised Tasks\n\nFor long-running tasks under supervision:\n\n```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  def start(_type, _args) do\n    children = [\n      {Task.Supervisor, name: MyApp.TaskSupervisor}\n    ]\n\n    Supervisor.start_link(children, strategy: :one_for_one)\n  end\nend\n\n# Use the supervised task\nTask.Supervisor.start_child(MyApp.TaskSupervisor, fn ->\n  long_running_operation()\nend)\n```\n\n#### Concurrent Map\n\nProcess collections concurrently:\n\n```elixir\n# Sequential\nresults = Enum.map(urls, &fetch_url/1)\n\n# Concurrent\nresults = Task.async_stream(urls, &fetch_url/1, max_concurrency: 10)\n         |> Enum.to_list()\n```\n\n### Agent - Simple State Management\n\nUse Agent for simple state:\n\n```elixir\n{:ok, agent} = Agent.start_link(fn -> %{} end, name: MyApp.Cache)\n\n# Get state\nvalue = Agent.get(MyApp.Cache, fn state -> Map.get(state, :key) end)\n\n# Update state\nAgent.update(MyApp.Cache, fn state -> Map.put(state, :key, value) end)\n\n# Get and update atomically\nAgent.get_and_update(MyApp.Cache, fn state ->\n  {Map.get(state, :key), Map.delete(state, :key)}\nend)\n```\n\n**When to use Agent vs GenServer:**\n- Use Agent for simple key-value state\n- Use GenServer when you need complex logic, callbacks, or process lifecycle management\n\n## Process Communication\n\n### send/receive\n\nBasic message passing:\n\n```elixir\n# Send message\nsend(pid, {:hello, \"world\"})\n\n# Receive message\nreceive do\n  {:hello, msg} -> IO.puts(msg)\nafter\n  5000 -> IO.puts(\"Timeout\")\nend\n```\n\n### Process Registration\n\nRegister processes by name:\n\n```elixir\n# Local registration\nProcess.register(self(), :my_process)\nsend(:my_process, :hello)\n\n# Via Registry\n{:ok, _} = Registry.start_link(keys: :unique, name: MyApp.Registry)\n\n{:ok, pid} = GenServer.start_link(MyWorker, nil,\n  name: {:via, Registry, {MyApp.Registry, \"worker_1\"}}\n)\n\n# Look up process\n[{pid, _}] = Registry.lookup(MyApp.Registry, \"worker_1\")\n```\n\n### Process Links and Monitors\n\n**Links** - Bidirectional, propagate exits:\n\n```elixir\n# Link processes\nProcess.link(pid)\n\n# Spawn linked\nspawn_link(fn -> do_work() end)\n```\n\n**Monitors** - Unidirectional, receive DOWN messages:\n\n```elixir\nref = Process.monitor(pid)\n\nreceive do\n  {:DOWN, ^ref, :process, ^pid, reason} ->\n    IO.puts(\"Process died: #{inspect(reason)}\")\nend\n```\n\n## Concurrency Patterns\n\n### Pipeline Pattern\n\nChain operations with concurrency:\n\n```elixir\ndefmodule Pipeline do\n  def process(data) do\n    data\n    |> async(&step1/1)\n    |> async(&step2/1)\n    |> async(&step3/1)\n    |> await_all()\n  end\n\n  defp async(input, fun) do\n    Task.async(fn -> fun.(input) end)\n  end\n\n  defp await_all(tasks) when is_list(tasks) do\n    Enum.map(tasks, &Task.await/1)\n  end\nend\n```\n\n### Worker Pool\n\nImplement a worker pool:\n\n```elixir\ndefmodule MyApp.WorkerPool do\n  use GenServer\n\n  def start_link(opts) do\n    pool_size = Keyword.get(opts, :size, 10)\n    GenServer.start_link(__MODULE__, pool_size, name: __MODULE__)\n  end\n\n  def execute(fun) do\n    GenServer.call(__MODULE__, {:execute, fun})\n  end\n\n  @impl true\n  def init(pool_size) do\n    workers = for _ <- 1..pool_size do\n      {:ok, pid} = Task.Supervisor.start_link()\n      pid\n    end\n\n    {:ok, %{workers: workers, index: 0}}\n  end\n\n  @impl true\n  def handle_call({:execute, fun}, _from, state) do\n    worker = Enum.at(state.workers, state.index)\n    task = Task.Supervisor.async_nolink(worker, fun)\n\n    new_index = rem(state.index + 1, length(state.workers))\n    {:reply, task, %{state | index: new_index}}\n  end\nend\n```\n\n### Backpressure with GenStage\n\nFor producer-consumer pipelines:\n\n```elixir\ndefmodule Producer do\n  use GenStage\n\n  def start_link(initial) do\n    GenStage.start_link(__MODULE__, initial, name: __MODULE__)\n  end\n\n  def init(initial) do\n    {:producer, initial}\n  end\n\n  def handle_demand(demand, state) do\n    events = Enum.to_list(state..state + demand - 1)\n    {:noreply, events, state + demand}\n  end\nend\n\ndefmodule Consumer do\n  use GenStage\n\n  def start_link() do\n    GenStage.start_link(__MODULE__, :ok)\n  end\n\n  def init(:ok) do\n    {:consumer, :ok}\n  end\n\n  def handle_events(events, _from, state) do\n    Enum.each(events, &process_event/1)\n    {:noreply, [], state}\n  end\nend\n```\n\n## ETS - Erlang Term Storage\n\nIn-memory key-value storage:\n\n```elixir\n# Create table\n:ets.new(:my_table, [:named_table, :public, read_concurrency: true])\n\n# Insert\n:ets.insert(:my_table, {:key, \"value\"})\n\n# Lookup\n[{:key, value}] = :ets.lookup(:my_table, :key)\n\n# Delete\n:ets.delete(:my_table, :key)\n\n# Match patterns\n:ets.match(:my_table, {:\"$1\", \"value\"})\n\n# Iterate\n:ets.foldl(fn {k, v}, acc -> [{k, v} | acc] end, [], :my_table)\n```\n\n### ETS Best Practices\n\n- Use `read_concurrency: true` for read-heavy workloads\n- Use `write_concurrency: true` for write-heavy workloads\n- Prefer `:set` (default) for unique keys\n- Use `:bag` or `:duplicate_bag` for multiple values per key\n- Always own ETS tables in a GenServer or Supervisor to prevent data loss\n\n## Error Handling and Fault Tolerance\n\n### Let It Crash Philosophy\n\nDesign for failure:\n\n```elixir\n# Don't do defensive programming\ndef process_order(order_id) do\n  # Let it crash if order doesn't exist\n  order = Repo.get!(Order, order_id)\n\n  # Let it crash if validation fails\n  {:ok, processed} = process(order)\n\n  processed\nend\n```\n\n### Proper Error Handling\n\nWhen to handle errors vs let crash:\n\n```elixir\n# Handle expected errors\ndef fetch_user(id) do\n  case HTTPoison.get(\"#{@api_url}/users/#{id}\") do\n    {:ok, %{status_code: 200, body: body}} ->\n      Jason.decode(body)\n\n    {:ok, %{status_code: 404}} ->\n      {:error, :not_found}\n\n    {:ok, %{status_code: status}} ->\n      {:error, {:unexpected_status, status}}\n\n    {:error, reason} ->\n      {:error, {:network_error, reason}}\n  end\nend\n\n# Let unexpected errors crash\ndef update_user!(id, params) do\n  user = Repo.get!(User, id)  # Crash if not found\n\n  user\n  |> User.changeset(params)\n  |> Repo.update!()  # Crash if invalid\nend\n```\n\n### Circuit Breaker\n\nPrevent cascading failures:\n\n```elixir\ndefmodule CircuitBreaker do\n  use GenServer\n\n  def start_link(_) do\n    GenServer.start_link(__MODULE__, %{status: :closed, failures: 0}, name: __MODULE__)\n  end\n\n  def call(fun) do\n    case GenServer.call(__MODULE__, :status) do\n      :open -> {:error, :circuit_open}\n      :closed -> execute(fun)\n    end\n  end\n\n  defp execute(fun) do\n    try do\n      result = fun.()\n      GenServer.cast(__MODULE__, :success)\n      {:ok, result}\n    rescue\n      e ->\n        GenServer.cast(__MODULE__, :failure)\n        {:error, e}\n    end\n  end\n\n  @impl true\n  def init(state), do: {:ok, state}\n\n  @impl true\n  def handle_call(:status, _from, state) do\n    {:reply, state.status, state}\n  end\n\n  @impl true\n  def handle_cast(:success, state) do\n    {:noreply, %{state | failures: 0, status: :closed}}\n  end\n\n  @impl true\n  def handle_cast(:failure, state) do\n    new_failures = state.failures + 1\n\n    if new_failures >= 5 do\n      Process.send_after(self(), :half_open, 30_000)\n      {:noreply, %{state | failures: new_failures, status: :open}}\n    else\n      {:noreply, %{state | failures: new_failures}}\n    end\n  end\n\n  @impl true\n  def handle_info(:half_open, state) do\n    {:noreply, %{state | status: :closed, failures: 0}}\n  end\nend\n```\n\n## Testing Concurrent Systems\n\n### Testing GenServers\n\n```elixir\ndefmodule MyApp.CounterTest do\n  use ExUnit.Case, async: true\n\n  test \"increments counter\" do\n    {:ok, pid} = MyApp.Counter.start_link(0)\n\n    assert MyApp.Counter.increment(pid) == 1\n    assert MyApp.Counter.increment(pid) == 2\n    assert MyApp.Counter.get_value(pid) == 2\n  end\nend\n```\n\n### Testing Asynchronous Processes\n\n```elixir\ntest \"process receives message\" do\n  parent = self()\n\n  spawn(fn ->\n    receive do\n      :ping -> send(parent, :pong)\n    end\n  end)\n\n  send(pid, :ping)\n\n  assert_receive :pong, 1000\nend\n```\n\n### Testing Supervision\n\n```elixir\ntest \"supervisor restarts crashed worker\" do\n  {:ok, sup} = Supervisor.start_link([MyApp.Worker], strategy: :one_for_one)\n\n  [{_, worker_pid, _, _}] = Supervisor.which_children(sup)\n\n  # Crash the worker\n  Process.exit(worker_pid, :kill)\n\n  # Wait for restart\n  Process.sleep(100)\n\n  # Verify new worker started\n  [{_, new_pid, _, _}] = Supervisor.which_children(sup)\n  assert new_pid != worker_pid\n  assert Process.alive?(new_pid)\nend\n```\n\n## Debugging Concurrent Systems\n\n### Observer\n\nLaunch Observer for visual process inspection:\n\n```elixir\n:observer.start()\n```\n\n### Process Info\n\nInspect running processes:\n\n```elixir\n# List all processes\nProcess.list()\n\n# Process information\nProcess.info(pid)\n\n# Message queue length\n{:message_queue_len, count} = Process.info(pid, :message_queue_len)\n\n# Current function\n{:current_function, {mod, fun, arity}} = Process.info(pid, :current_function)\n```\n\n### Tracing\n\nUse `:sys` module for debugging:\n\n```elixir\n# Enable tracing\n:sys.trace(pid, true)\n\n# Get state\n:sys.get_state(pid)\n\n# Get status\n:sys.get_status(pid)\n```\n\n## Performance Considerations\n\n### Process Spawning\n\n- Processes are lightweight (< 2KB overhead)\n- Spawning thousands/millions of processes is normal\n- Use process pools when spawn rate is very high\n\n### Message Passing\n\n- Messages are copied between processes\n- Large messages are expensive - consider ETS or persistent_term\n- Use binary for efficient large data transfer\n\n### Bottlenecks\n\n- Single GenServer can become bottleneck\n- Solution: shard state across multiple processes\n- Use ETS with `read_concurrency` for read-heavy workloads\n\n## Key Principles\n\n- **Embrace concurrency**: Use processes liberally, they're cheap\n- **Let it crash**: Don't write defensive code, use supervision\n- **Isolate failures**: Design supervision trees to contain failures\n- **Communicate via messages**: Avoid shared state between processes\n- **Use the right tool**: GenServer for state, Task for work, Agent for simple state\n- **Test at boundaries**: Test process APIs, not internal implementation\n- **Monitor and observe**: Use Observer and logging to understand system behavior"
              },
              {
                "name": "phoenix-framework",
                "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices",
                "path": "elixir/skills/phoenix/SKILL.md",
                "frontmatter": {
                  "name": "phoenix-framework",
                  "description": "Guide for building Phoenix web applications with LiveView, contexts, channels, and following Phoenix best practices"
                },
                "content": "# Phoenix Framework Development\n\nThis skill activates when working with Phoenix web applications, including setup, development, LiveView, contexts, controllers, and channels.\n\n## When to Use This Skill\n\nActivate this skill when:\n- Creating or modifying Phoenix applications\n- Implementing LiveView components or pages\n- Working with Phoenix contexts and business logic\n- Building real-time features with channels or LiveView\n- Configuring Phoenix routers, plugs, or endpoints\n- Troubleshooting Phoenix-specific issues\n\n## Phoenix Project Structure\n\nFollow Phoenix conventions:\n\n```\nlib/\n  my_app/           # Business logic and contexts\n    accounts/       # Domain contexts\n    repo.ex\n  my_app_web/       # Web interface\n    controllers/\n    live/           # LiveView modules\n    components/     # Function components\n    router.ex\n    endpoint.ex\n```\n\n## Context-Driven Design\n\nOrganize business logic into contexts (bounded domains):\n\n### Creating Contexts\n\nGenerate contexts with related schemas:\n```bash\nmix phx.gen.context Accounts User users email:string name:string\n```\n\nStructure contexts to encapsulate business logic:\n\n```elixir\ndefmodule MyApp.Accounts do\n  @moduledoc \"\"\"\n  The Accounts context - manages user accounts and authentication.\n  \"\"\"\n\n  alias MyApp.Repo\n  alias MyApp.Accounts.User\n\n  def list_users do\n    Repo.all(User)\n  end\n\n  def get_user!(id), do: Repo.get!(User, id)\n\n  def create_user(attrs \\\\ %{}) do\n    %User{}\n    |> User.changeset(attrs)\n    |> Repo.insert()\n  end\n\n  def update_user(%User{} = user, attrs) do\n    user\n    |> User.changeset(attrs)\n    |> Repo.update()\n  end\nend\n```\n\n### Context Best Practices\n\n- Keep contexts focused on a single domain\n- Avoid cross-context dependencies when possible\n- Use public API functions, not direct Repo access in web layer\n- Name contexts after business domains, not technical layers\n\n## LiveView Development\n\nLiveView enables rich, real-time experiences without writing JavaScript.\n\n### LiveView Lifecycle\n\nUnderstand the mount → handle_event → render cycle:\n\n```elixir\ndefmodule MyAppWeb.UserLive.Index do\n  use MyAppWeb, :live_view\n\n  alias MyApp.Accounts\n\n  @impl true\n  def mount(_params, _session, socket) do\n    # Runs on initial page load and live connection\n    {:ok, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def handle_params(params, _url, socket) do\n    # Runs after mount and on live patch\n    {:noreply, apply_action(socket, socket.assigns.live_action, params)}\n  end\n\n  @impl true\n  def handle_event(\"delete\", %{\"id\" => id}, socket) do\n    user = Accounts.get_user!(id)\n    {:ok, _} = Accounts.delete_user(user)\n\n    {:noreply, assign(socket, :users, list_users())}\n  end\n\n  @impl true\n  def render(assigns) do\n    ~H\"\"\"\n    <div>\n      <.table rows={@users} id=\"users\">\n        <:col :let={user} label=\"Name\"><%= user.name %></:col>\n        <:col :let={user} label=\"Email\"><%= user.email %></:col>\n        <:action :let={user}>\n          <.button phx-click=\"delete\" phx-value-id={user.id}>Delete</.button>\n        </:action>\n      </.table>\n    </div>\n    \"\"\"\n  end\n\n  defp list_users do\n    Accounts.list_users()\n  end\nend\n```\n\n### LiveView Best Practices\n\n- Use `mount/3` for initial data loading\n- Handle route changes in `handle_params/3`\n- Keep renders fast - compute in event handlers, not render\n- Use `assign_new/3` for expensive computations\n- Prefer LiveView over JavaScript for interactive UIs\n- Use `phx-debounce` and `phx-throttle` for frequent events\n\n### Function Components\n\nCreate reusable components:\n\n```elixir\ndefmodule MyAppWeb.Components.UserCard do\n  use Phoenix.Component\n\n  attr :user, :map, required: true\n  attr :class, :string, default: \"\"\n\n  def user_card(assigns) do\n    ~H\"\"\"\n    <div class={\"card \" <> @class}>\n      <h3><%= @user.name %></h3>\n      <p><%= @user.email %></p>\n    </div>\n    \"\"\"\n  end\nend\n```\n\nUse with `<.user_card user={@current_user} />` in templates.\n\n### Form Handling\n\nUse changesets for validation:\n\n```elixir\n@impl true\ndef mount(_params, _session, socket) do\n  changeset = Accounts.change_user(%User{})\n  {:ok, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"validate\", %{\"user\" => user_params}, socket) do\n  changeset =\n    %User{}\n    |> Accounts.change_user(user_params)\n    |> Map.put(:action, :validate)\n\n  {:noreply, assign(socket, form: to_form(changeset))}\nend\n\n@impl true\ndef handle_event(\"save\", %{\"user\" => user_params}, socket) do\n  case Accounts.create_user(user_params) do\n    {:ok, user} ->\n      {:noreply,\n       socket\n       |> put_flash(:info, \"User created successfully\")\n       |> push_navigate(to: ~p\"/users/#{user}\")}\n\n    {:error, %Ecto.Changeset{} = changeset} ->\n      {:noreply, assign(socket, form: to_form(changeset))}\n  end\nend\n\ndef render(assigns) do\n  ~H\"\"\"\n  <.form for={@form} phx-change=\"validate\" phx-submit=\"save\">\n    <.input field={@form[:name]} label=\"Name\" />\n    <.input field={@form[:email]} label=\"Email\" type=\"email\" />\n    <.button>Save</.button>\n  </.form>\n  \"\"\"\nend\n```\n\n## Routing\n\n### Route Organization\n\nStructure routes logically:\n\n```elixir\ndefmodule MyAppWeb.Router do\n  use MyAppWeb, :router\n\n  pipeline :browser do\n    plug :accepts, [\"html\"]\n    plug :fetch_session\n    plug :fetch_live_flash\n    plug :put_root_layout, html: {MyAppWeb.Layouts, :root}\n    plug :protect_from_forgery\n    plug :put_secure_browser_headers\n  end\n\n  pipeline :api do\n    plug :accepts, [\"json\"]\n  end\n\n  scope \"/\", MyAppWeb do\n    pipe_through :browser\n\n    live \"/\", HomeLive, :index\n    live \"/users\", UserLive.Index, :index\n    live \"/users/new\", UserLive.Index, :new\n    live \"/users/:id\", UserLive.Show, :show\n  end\n\n  scope \"/api\", MyAppWeb do\n    pipe_through :api\n\n    resources \"/users\", UserController, except: [:new, :edit]\n  end\nend\n```\n\n### LiveView Routes\n\nUse live actions for modal/overlay states:\n\n```elixir\nlive \"/users\", UserLive.Index, :index\nlive \"/users/new\", UserLive.Index, :new\nlive \"/users/:id/edit\", UserLive.Index, :edit\n```\n\nThen handle in `handle_params/3`:\n\n```elixir\ndefp apply_action(socket, :edit, %{\"id\" => id}) do\n  socket\n  |> assign(:page_title, \"Edit User\")\n  |> assign(:user, Accounts.get_user!(id))\nend\n\ndefp apply_action(socket, :new, _params) do\n  socket\n  |> assign(:page_title, \"New User\")\n  |> assign(:user, %User{})\nend\n\ndefp apply_action(socket, :index, _params) do\n  socket\n  |> assign(:page_title, \"Listing Users\")\n  |> assign(:user, nil)\nend\n```\n\n## Channels and PubSub\n\n### Phoenix Channels\n\nFor custom real-time protocols:\n\n```elixir\ndefmodule MyAppWeb.RoomChannel do\n  use MyAppWeb, :channel\n\n  @impl true\n  def join(\"room:\" <> room_id, _payload, socket) do\n    if authorized?(socket, room_id) do\n      {:ok, assign(socket, :room_id, room_id)}\n    else\n      {:error, %{reason: \"unauthorized\"}}\n    end\n  end\n\n  @impl true\n  def handle_in(\"new_msg\", %{\"body\" => body}, socket) do\n    broadcast!(socket, \"new_msg\", %{body: body, user: socket.assigns.user})\n    {:noreply, socket}\n  end\nend\n```\n\n### Phoenix PubSub\n\nFor LiveView updates and process communication:\n\n```elixir\n# Subscribe in mount\ndef mount(_params, _session, socket) do\n  if connected?(socket) do\n    Phoenix.PubSub.subscribe(MyApp.PubSub, \"users\")\n  end\n\n  {:ok, assign(socket, :users, list_users())}\nend\n\n# Handle broadcasts\ndef handle_info({:user_created, user}, socket) do\n  {:noreply, update(socket, :users, fn users -> [user | users] end)}\nend\n\n# Broadcast from context\ndef create_user(attrs) do\n  with {:ok, user} <- do_create_user(attrs) do\n    Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n    {:ok, user}\n  end\nend\n```\n\n## Testing Phoenix Applications\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase, async: true\n\n  test \"GET /users\", %{conn: conn} do\n    conn = get(conn, ~p\"/users\")\n    assert html_response(conn, 200) =~ \"Listing Users\"\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n\n  import Phoenix.LiveViewTest\n\n  test \"displays users\", %{conn: conn} do\n    user = insert(:user)\n\n    {:ok, view, html} = live(conn, ~p\"/users\")\n\n    assert html =~ user.name\n    assert has_element?(view, \"#user-#{user.id}\")\n  end\n\n  test \"creates user\", %{conn: conn} do\n    {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n    assert view\n           |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n           |> render_submit()\n\n    assert_patch(view, ~p\"/users\")\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    {:ok, _, socket} = subscribe_and_join(socket, \"room:lobby\", %{})\n\n    broadcast_from!(socket, \"new_msg\", %{body: \"test\"})\n    assert_broadcast \"new_msg\", %{body: \"test\"}\n  end\nend\n```\n\n## Common Patterns\n\n### Loading Associations\n\nPreload associations efficiently:\n\n```elixir\ndef list_posts do\n  Post\n  |> preload([:author, comments: :author])\n  |> Repo.all()\nend\n```\n\n### Pagination\n\nUse Scrivener or custom pagination:\n\n```elixir\ndef list_users(page \\\\ 1) do\n  User\n  |> order_by(desc: :inserted_at)\n  |> Repo.paginate(page: page, page_size: 20)\nend\n```\n\n### File Uploads\n\nHandle uploads in LiveView:\n\n```elixir\ndef mount(_params, _session, socket) do\n  {:ok,\n   socket\n   |> assign(:uploaded_files, [])\n   |> allow_upload(:avatar, accept: ~w(.jpg .jpeg .png), max_entries: 1)}\nend\n\ndef handle_event(\"save\", _params, socket) do\n  uploaded_files =\n    consume_uploaded_entries(socket, :avatar, fn %{path: path}, _entry ->\n      dest = Path.join(\"priv/static/uploads\", Path.basename(path))\n      File.cp!(path, dest)\n      {:ok, \"/uploads/\" <> Path.basename(dest)}\n    end)\n\n  {:noreply, update(socket, :uploaded_files, &(&1 ++ uploaded_files))}\nend\n```\n\n## Performance Optimization\n\n### Database Query Optimization\n\n- Use `preload/2` to avoid N+1 queries\n- Add database indexes for frequently queried fields\n- Use `select/3` to load only needed fields\n- Consider using `Repo.stream/2` for large datasets\n\n### LiveView Performance\n\n- Move expensive computations to `handle_event` or background jobs\n- Use `assign_new/3` for computed values\n- Implement `handle_continue/2` for async operations after mount\n- Use temporary assigns for large lists: `assign(socket, :items, temporary: true)`\n\n### Caching\n\nUse Cachex or ETS for caching:\n\n```elixir\ndef get_user!(id) do\n  Cachex.fetch(:users, id, fn ->\n    {:commit, Repo.get!(User, id)}\n  end)\nend\n```\n\n## Security Best Practices\n\n- Always validate and sanitize user input through changesets\n- Use CSRF protection (enabled by default)\n- Implement rate limiting for APIs\n- Use `put_secure_browser_headers` plug\n- Validate file uploads (type, size, content)\n- Use prepared statements (Ecto does this automatically)\n- Implement proper authentication and authorization\n\n## Key Principles\n\n- **Context boundaries**: Keep business logic in contexts, not controllers/LiveViews\n- **LiveView first**: Prefer LiveView over JavaScript for interactive features\n- **Changesets for validation**: Always validate through Ecto changesets\n- **Pub/Sub for communication**: Use Phoenix.PubSub for cross-process updates\n- **Test at boundaries**: Test contexts, controllers, and LiveViews separately\n- **Follow conventions**: Use Phoenix generators and follow established patterns"
              },
              {
                "name": "elixir-testing",
                "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices",
                "path": "elixir/skills/testing/SKILL.md",
                "frontmatter": {
                  "name": "elixir-testing",
                  "description": "Guide for writing comprehensive tests in Elixir using ExUnit, property-based testing, mocks, and test organization best practices"
                },
                "content": "# Elixir Testing with ExUnit\n\nThis skill activates when writing, organizing, or improving tests for Elixir applications using ExUnit and related testing tools.\n\n## When to Use This Skill\n\nActivate when:\n- Writing unit, integration, or property-based tests\n- Organizing test suites and test files\n- Setting up test fixtures and factories\n- Mocking external dependencies\n- Testing concurrent or asynchronous code\n- Improving test coverage or quality\n- Troubleshooting failing tests\n\n## ExUnit Basics\n\n### Test Module Structure\n\n```elixir\ndefmodule MyApp.MathTest do\n  use ExUnit.Case, async: true\n\n  describe \"add/2\" do\n    test \"adds two positive numbers\" do\n      assert Math.add(2, 3) == 5\n    end\n\n    test \"adds negative numbers\" do\n      assert Math.add(-1, -1) == -2\n    end\n\n    test \"adds zero\" do\n      assert Math.add(5, 0) == 5\n    end\n  end\n\n  describe \"divide/2\" do\n    test \"divides two numbers\" do\n      assert Math.divide(10, 2) == 5.0\n    end\n\n    test \"returns error for division by zero\" do\n      assert Math.divide(10, 0) == {:error, :division_by_zero}\n    end\n  end\nend\n```\n\n### Assertions\n\nCommon assertion patterns:\n\n```elixir\n# Equality\nassert actual == expected\nrefute actual == unexpected\n\n# Boolean\nassert is_binary(value)\nassert is_integer(value)\nrefute is_nil(value)\n\n# Pattern matching\nassert {:ok, result} = function_call()\nassert %User{name: \"Alice\"} = user\n\n# Exceptions\nassert_raise ArgumentError, fn ->\n  String.to_integer(\"not a number\")\nend\n\nassert_raise ArgumentError, \"invalid argument\", fn ->\n  dangerous_function()\nend\n\n# Messages\nsend(self(), :hello)\nassert_received :hello\n\nassert_receive :message, 1000  # With timeout\n\nrefute_received :unwanted\nrefute_receive :unwanted, 100\n```\n\n### Test Organization\n\n#### Using describe blocks\n\nGroup related tests:\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  describe \"create_user/1\" do\n    test \"creates user with valid attributes\" do\n      # ...\n    end\n\n    test \"returns error with invalid email\" do\n      # ...\n    end\n  end\n\n  describe \"update_user/2\" do\n    test \"updates user attributes\" do\n      # ...\n    end\n  end\nend\n```\n\n#### Test tags\n\nCategorize and filter tests:\n\n```elixir\n@moduletag :integration\n\n@tag :slow\ntest \"expensive operation\" do\n  # ...\nend\n\n@tag :external\ntest \"calls external API\" do\n  # ...\nend\n\n# Run only tagged tests\n# mix test --only slow\n# mix test --exclude external\n```\n\n### Setup and Teardown\n\n#### Test context\n\n```elixir\ndefmodule MyApp.UserTest do\n  use ExUnit.Case\n\n  setup do\n    user = %User{name: \"Alice\", email: \"alice@example.com\"}\n    {:ok, user: user}\n  end\n\n  test \"user has name\", %{user: user} do\n    assert user.name == \"Alice\"\n  end\n\n  test \"user has email\", %{user: user} do\n    assert user.email == \"alice@example.com\"\n  end\nend\n```\n\n#### Setup with describe\n\n```elixir\ndescribe \"authenticated user\" do\n  setup do\n    user = insert(:user)\n    token = generate_token(user)\n    {:ok, user: user, token: token}\n  end\n\n  test \"can access protected resource\", %{token: token} do\n    # ...\n  end\nend\n```\n\n#### Module setup\n\n```elixir\nsetup_all do\n  # Runs once before all tests in module\n  start_supervised!(MyApp.Cache)\n  :ok\nend\n\nsetup do\n  # Runs before each test\n  :ok = Ecto.Adapters.SQL.Sandbox.checkout(MyApp.Repo)\nend\n```\n\n#### Conditional setup\n\n```elixir\nsetup context do\n  if context[:integration] do\n    start_external_service()\n    on_exit(fn -> stop_external_service() end)\n  end\n\n  :ok\nend\n\n@tag :integration\ntest \"integration test\" do\n  # ...\nend\n```\n\n## Database Testing\n\n### Sandbox Mode\n\nConfigure for concurrent tests:\n\n```elixir\n# config/test.exs\nconfig :my_app, MyApp.Repo,\n  pool: Ecto.Adapters.SQL.Sandbox\n\n# test/test_helper.exs\nEcto.Adapters.SQL.Sandbox.mode(MyApp.Repo, :manual)\n\n# test/support/data_case.ex\ndefmodule MyApp.DataCase do\n  use ExUnit.CaseTemplate\n\n  using do\n    quote do\n      alias MyApp.Repo\n      import Ecto\n      import Ecto.Changeset\n      import Ecto.Query\n      import MyApp.DataCase\n    end\n  end\n\n  setup tags do\n    pid = Ecto.Adapters.SQL.Sandbox.start_owner!(MyApp.Repo, shared: not tags[:async])\n    on_exit(fn -> Ecto.Adapters.SQL.Sandbox.stop_owner(pid) end)\n    :ok\n  end\nend\n```\n\n### Test Factories\n\nUse ExMachina for test data:\n\n```elixir\n# test/support/factory.ex\ndefmodule MyApp.Factory do\n  use ExMachina.Ecto, repo: MyApp.Repo\n\n  def user_factory do\n    %MyApp.User{\n      name: \"Jane Smith\",\n      email: sequence(:email, &\"email-#{&1}@example.com\"),\n      age: 25\n    }\n  end\n\n  def admin_factory do\n    struct!(\n      user_factory(),\n      %{role: :admin}\n    )\n  end\n\n  def post_factory do\n    %MyApp.Post{\n      title: \"A title\",\n      body: \"Some content\",\n      author: build(:user)\n    }\n  end\nend\n\n# In tests\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n  import MyApp.Factory\n\n  test \"creates user\" do\n    user = insert(:user)\n    assert user.id\n  end\n\n  test \"creates admin\" do\n    admin = insert(:admin)\n    assert admin.role == :admin\n  end\n\n  test \"builds without inserting\" do\n    user = build(:user, name: \"Custom Name\")\n    assert user.name == \"Custom Name\"\n    refute user.id\n  end\nend\n```\n\n### Testing Changesets\n\n```elixir\ndefmodule MyApp.UserTest do\n  use MyApp.DataCase\n\n  describe \"changeset/2\" do\n    test \"valid changeset with valid attributes\" do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert changeset.valid?\n    end\n\n    test \"invalid without email\" do\n      attrs = %{name: \"Alice\", age: 25}\n      changeset = User.changeset(%User{}, attrs)\n\n      refute changeset.valid?\n      assert \"can't be blank\" in errors_on(changeset).email\n    end\n\n    test \"invalid with short password\" do\n      attrs = %{email: \"test@example.com\", password: \"123\"}\n      changeset = User.changeset(%User{}, attrs)\n\n      assert \"should be at least 8 character(s)\" in errors_on(changeset).password\n    end\n  end\nend\n\n# Helper function\ndef errors_on(changeset) do\n  Ecto.Changeset.traverse_errors(changeset, fn {message, opts} ->\n    Regex.replace(~r\"%{(\\w+)}\", message, fn _, key ->\n      opts |> Keyword.get(String.to_existing_atom(key), key) |> to_string()\n    end)\n  end)\nend\n```\n\n## Phoenix Testing\n\n### Controller Tests\n\n```elixir\ndefmodule MyAppWeb.UserControllerTest do\n  use MyAppWeb.ConnCase\n  import MyApp.Factory\n\n  describe \"index\" do\n    test \"lists all users\", %{conn: conn} do\n      user = insert(:user)\n\n      conn = get(conn, ~p\"/users\")\n\n      assert html_response(conn, 200) =~ \"Listing Users\"\n      assert html_response(conn, 200) =~ user.name\n    end\n  end\n\n  describe \"create\" do\n    test \"creates user with valid data\", %{conn: conn} do\n      attrs = %{name: \"Alice\", email: \"alice@example.com\"}\n\n      conn = post(conn, ~p\"/users\", user: attrs)\n\n      assert redirected_to(conn) =~ ~p\"/users\"\n\n      conn = get(conn, redirected_to(conn))\n      assert html_response(conn, 200) =~ \"Alice\"\n    end\n\n    test \"renders errors with invalid data\", %{conn: conn} do\n      conn = post(conn, ~p\"/users\", user: %{})\n\n      assert html_response(conn, 200) =~ \"New User\"\n    end\n  end\nend\n```\n\n### LiveView Tests\n\n```elixir\ndefmodule MyAppWeb.UserLiveTest do\n  use MyAppWeb.ConnCase\n  import Phoenix.LiveViewTest\n  import MyApp.Factory\n\n  describe \"Index\" do\n    test \"displays users\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, html} = live(conn, ~p\"/users\")\n\n      assert html =~ \"Listing Users\"\n      assert has_element?(view, \"#user-#{user.id}\")\n      assert render(view) =~ user.name\n    end\n\n    test \"creates new user\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Alice\", email: \"alice@example.com\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users\")\n\n      html = render(view)\n      assert html =~ \"Alice\"\n    end\n\n    test \"updates user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users/#{user.id}/edit\")\n\n      assert view\n             |> form(\"#user-form\", user: %{name: \"Updated Name\"})\n             |> render_submit()\n\n      assert_patch(view, ~p\"/users/#{user.id}\")\n\n      html = render(view)\n      assert html =~ \"Updated Name\"\n    end\n\n    test \"deletes user\", %{conn: conn} do\n      user = insert(:user)\n\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      assert view\n             |> element(\"#user-#{user.id} a\", \"Delete\")\n             |> render_click()\n\n      refute has_element?(view, \"#user-#{user.id}\")\n    end\n  end\n\n  describe \"form validation\" do\n    test \"validates on change\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users/new\")\n\n      result =\n        view\n        |> form(\"#user-form\", user: %{email: \"invalid\"})\n        |> render_change()\n\n      assert result =~ \"must have the @ sign\"\n    end\n  end\n\n  describe \"real-time updates\" do\n    test \"receives updates from PubSub\", %{conn: conn} do\n      {:ok, view, _html} = live(conn, ~p\"/users\")\n\n      user = insert(:user)\n\n      # Trigger PubSub event\n      Phoenix.PubSub.broadcast(MyApp.PubSub, \"users\", {:user_created, user})\n\n      assert render(view) =~ user.name\n    end\n  end\nend\n```\n\n### Channel Tests\n\n```elixir\ndefmodule MyAppWeb.RoomChannelTest do\n  use MyAppWeb.ChannelCase\n\n  setup do\n    {:ok, _, socket} =\n      MyAppWeb.UserSocket\n      |> socket(\"user_id\", %{user_id: 42})\n      |> subscribe_and_join(MyAppWeb.RoomChannel, \"room:lobby\")\n\n    %{socket: socket}\n  end\n\n  test \"ping replies with pong\", %{socket: socket} do\n    ref = push(socket, \"ping\", %{\"hello\" => \"there\"})\n    assert_reply ref, :ok, %{\"hello\" => \"there\"}\n  end\n\n  test \"shout broadcasts to room:lobby\", %{socket: socket} do\n    push(socket, \"shout\", %{\"hello\" => \"all\"})\n    assert_broadcast \"shout\", %{\"hello\" => \"all\"}\n  end\n\n  test \"broadcasts are pushed to the client\", %{socket: socket} do\n    broadcast_from!(socket, \"broadcast\", %{\"some\" => \"data\"})\n    assert_push \"broadcast\", %{\"some\" => \"data\"}\n  end\nend\n```\n\n## Mocking and Stubbing\n\n### Using Mox\n\nDefine behaviors and mocks:\n\n```elixir\n# Define behaviour\ndefmodule MyApp.HTTPClient do\n  @callback get(String.t()) :: {:ok, map()} | {:error, term()}\nend\n\n# In config/test.exs\nconfig :my_app, :http_client, MyApp.HTTPClientMock\n\n# In test/test_helper.exs\nMox.defmock(MyApp.HTTPClientMock, for: MyApp.HTTPClient)\n\n# In application code\ndefmodule MyApp.UserFetcher do\n  @http_client Application.compile_env(:my_app, :http_client)\n\n  def fetch_user(id) do\n    @http_client.get(\"/users/#{id}\")\n  end\nend\n\n# In tests\ndefmodule MyApp.UserFetcherTest do\n  use ExUnit.Case, async: true\n  import Mox\n\n  setup :verify_on_exit!\n\n  test \"fetches user successfully\" do\n    expect(MyApp.HTTPClientMock, :get, fn \"/users/1\" ->\n      {:ok, %{\"name\" => \"Alice\"}}\n    end)\n\n    assert {:ok, %{\"name\" => \"Alice\"}} = MyApp.UserFetcher.fetch_user(1)\n  end\n\n  test \"handles error\" do\n    expect(MyApp.HTTPClientMock, :get, fn _ ->\n      {:error, :network_error}\n    end)\n\n    assert {:error, :network_error} = MyApp.UserFetcher.fetch_user(1)\n  end\nend\n```\n\n### Stubbing Multiple Calls\n\n```elixir\ntest \"calls API multiple times\" do\n  MyApp.HTTPClientMock\n  |> expect(:get, 3, fn url ->\n    {:ok, %{\"url\" => url}}\n  end)\n\n  MyApp.batch_fetch([1, 2, 3])\nend\n```\n\n### Global Stubs\n\n```elixir\nsetup do\n  stub(MyApp.HTTPClientMock, :get, fn _ -> {:ok, %{}} end)\n  :ok\nend\n\ntest \"can override stub\" do\n  expect(MyApp.HTTPClientMock, :get, fn _ ->\n    {:error, :timeout}\n  end)\n\n  # ...\nend\n```\n\n## Property-Based Testing\n\nUse StreamData for property-based tests:\n\n```elixir\ndefmodule MyApp.MathPropertyTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  property \"addition is commutative\" do\n    check all a <- integer(),\n              b <- integer() do\n      assert Math.add(a, b) == Math.add(b, a)\n    end\n  end\n\n  property \"list reversal is involutive\" do\n    check all list <- list_of(integer()) do\n      assert Enum.reverse(Enum.reverse(list)) == list\n    end\n  end\n\n  property \"concatenation length\" do\n    check all list1 <- list_of(term()),\n              list2 <- list_of(term()) do\n      concatenated = list1 ++ list2\n      assert length(concatenated) == length(list1) + length(list2)\n    end\n  end\nend\n```\n\n### Custom Generators\n\n```elixir\ndefmodule MyApp.Generators do\n  use ExUnitProperties\n\n  def email do\n    gen all username <- string(:alphanumeric, min_length: 1),\n            domain <- string(:alphanumeric, min_length: 1),\n            tld <- member_of([\"com\", \"org\", \"net\"]) do\n      \"#{username}@#{domain}.#{tld}\"\n    end\n  end\n\n  def user do\n    gen all name <- string(:alphanumeric, min_length: 1),\n            email <- email(),\n            age <- integer(18..100) do\n      %User{name: name, email: email, age: age}\n    end\n  end\nend\n\n# Use in tests\nproperty \"validates email format\" do\n  check all email <- MyApp.Generators.email() do\n    assert User.valid_email?(email)\n  end\nend\n```\n\n## Testing Async and Concurrent Code\n\n### Testing Processes\n\n```elixir\ntest \"GenServer handles messages\" do\n  {:ok, pid} = MyApp.Worker.start_link()\n\n  MyApp.Worker.process(pid, :work)\n\n  assert_receive {:done, :work}, 1000\nend\n```\n\n### Testing Tasks\n\n```elixir\ntest \"async task completes\" do\n  parent = self()\n\n  Task.start(fn ->\n    result = expensive_computation()\n    send(parent, {:result, result})\n  end)\n\n  assert_receive {:result, value}, 5000\n  assert value == expected\nend\n```\n\n### Testing Race Conditions\n\n```elixir\ntest \"concurrent updates are handled correctly\" do\n  {:ok, counter} = Counter.start_link(0)\n\n  tasks = for _ <- 1..100 do\n    Task.async(fn -> Counter.increment(counter) end)\n  end\n\n  Task.await_many(tasks)\n\n  assert Counter.get(counter) == 100\nend\n```\n\n## Test Coverage\n\n### Generate Coverage Reports\n\n```bash\nmix test --cover\n\n# Detailed coverage\nMIX_ENV=test mix coveralls\nMIX_ENV=test mix coveralls.html\n```\n\n### Coverage Configuration\n\n```elixir\n# mix.exs\ndef project do\n  [\n    # ...\n    test_coverage: [tool: ExCoveralls],\n    preferred_cli_env: [\n      coveralls: :test,\n      \"coveralls.detail\": :test,\n      \"coveralls.html\": :test\n    ]\n  ]\nend\n```\n\n## Best Practices\n\n### Test Organization\n\n- One test file per module: `lib/my_app/user.ex` → `test/my_app/user_test.exs`\n- Use `describe` blocks to group related tests\n- Use `test/support` for shared test helpers\n- Keep tests focused on one behavior per test\n\n### Naming\n\n- Use descriptive test names that explain what is being tested\n- Start with the action being tested\n- Include the expected outcome\n\n```elixir\n# Good\ntest \"create_user/1 returns error with invalid email\"\ntest \"add/2 returns sum of two positive integers\"\n\n# Avoid\ntest \"it works\"\ntest \"test1\"\n```\n\n### Setup\n\n- Use `setup` for common test data\n- Keep setup focused - don't create unnecessary data\n- Use context to pass data between setup and tests\n- Use factories for complex data structures\n\n### Assertions\n\n- Prefer pattern matching over multiple assertions\n- Use specific assertions (`assert_receive` vs `assert Process.info(...)`)\n- Test one logical assertion per test when possible\n\n### Async Tests\n\n```elixir\n# Mark tests as async when they don't share state\nuse ExUnit.Case, async: true\n\n# Don't use async when tests:\n# - Modify global state\n# - Use database without sandbox\n# - Access shared resources\n```\n\n### Test Data\n\n- Use factories (ExMachina) for consistent test data\n- Avoid hardcoded IDs - use factories and references\n- Keep test data minimal - only what's needed for the test\n- Use descriptive data that makes tests readable\n\n### External Dependencies\n\n- Mock external APIs and services\n- Use Mox for behavior-based mocking\n- Stub at the boundary - don't mock internal modules\n- Tag tests that require external services\n\n## Debugging Tests\n\n### Running Specific Tests\n\n```bash\n# Run single test file\nmix test test/my_app/user_test.exs\n\n# Run specific line\nmix test test/my_app/user_test.exs:42\n\n# Run tests matching pattern\nmix test --only integration\n\n# Run tests excluding pattern\nmix test --exclude slow\n```\n\n### Test Output\n\n```elixir\n# Add IEx.pry breakpoint\nimport IEx\ntest \"debugging\" do\n  user = build(:user)\n  IEx.pry()  # Stops here\n  # ...\nend\n\n# Print during tests\nIO.inspect(value, label: \"DEBUG\")\n```\n\n### Failed Test Debugging\n\n```bash\n# Re-run only failed tests\nmix test --failed\n\n# Show detailed error traces\nmix test --trace\n\n# Run tests one at a time\nmix test --max-cases 1\n```\n\n## Key Principles\n\n- **Test behavior, not implementation**: Test what the code does, not how it does it\n- **Keep tests fast**: Use async tests, avoid unnecessary setup, mock slow dependencies\n- **Make tests readable**: Use descriptive names, clear assertions, minimal setup\n- **Test at the right level**: Unit tests for logic, integration tests for interactions\n- **Use factories**: Consistent, reusable test data with ExMachina\n- **Mock at boundaries**: Mock external services, not internal modules\n- **Property-based testing**: Use StreamData for algorithmic code\n- **Embrace the database**: Use Ecto sandbox for fast, isolated database tests"
              },
              {
                "name": "act",
                "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution",
                "path": "github/skills/act/SKILL.md",
                "frontmatter": {
                  "name": "act",
                  "description": "Test GitHub Actions workflows locally using act, including installation, configuration, debugging, and troubleshooting local workflow execution"
                },
                "content": "# act - Local GitHub Actions Testing\n\nActivate when testing GitHub Actions workflows locally, debugging workflow issues, or developing actions without committing to remote repositories. This skill covers act installation, configuration, and usage patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Testing workflow changes before committing\n- Debugging workflow failures locally\n- Developing new workflows iteratively\n- Validating workflow syntax and logic\n- Testing actions with different events\n- Running workflows without GitHub runners\n- Troubleshooting act-specific issues\n\n## Installation\n\n### Using mise (Recommended for this project)\n\nThe act tool is configured in the github plugin's mise.toml:\n\n```bash\n# Install act via mise\nmise install act\n\n# Verify installation\nact --version\n```\n\n### Alternative Installation Methods\n\n**macOS (Homebrew):**\n```bash\nbrew install act\n```\n\n**Linux (via script):**\n```bash\ncurl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\n```\n\n**From source:**\n```bash\ngit clone https://github.com/nektos/act.git\ncd act\nmake install\n```\n\n**Windows (Chocolatey):**\n```powershell\nchoco install act-cli\n```\n\n## How act Works\n\nact reads workflow files from `.github/workflows/` and:\n1. Determines which actions and jobs to execute\n2. Pulls or builds required Docker images\n3. Creates containers matching GitHub's runner environment\n4. Executes steps in isolated containers\n5. Provides output matching GitHub Actions format\n\n**Key Concept:** act uses Docker to simulate GitHub's runner environment locally.\n\n## Prerequisites\n\n- **Docker**: act requires Docker to run workflows\n- **Workflow files**: Valid `.github/workflows/*.yml` files in repository\n\nVerify Docker is running:\n```bash\ndocker ps\n```\n\n## Basic Usage\n\n### List Available Workflows\n\n```bash\n# List all workflows\nact -l\n\n# Output:\n# Stage  Job ID  Job name  Workflow name  Workflow file  Events\n# 0      build   build     CI             ci.yml         push,pull_request\n# 0      test    test      CI             ci.yml         push,pull_request\n```\n\n### Run Default Event (push)\n\n```bash\n# Run all jobs triggered by push event\nact\n\n# Run specific job\nact -j build\n\n# Run specific workflow\nact -W .github/workflows/ci.yml\n```\n\n### Run Specific Events\n\n```bash\n# Pull request event\nact pull_request\n\n# Manual workflow dispatch\nact workflow_dispatch\n\n# Push to specific branch\nact push -e .github/workflows/push-event.json\n\n# Schedule event\nact schedule\n```\n\n### Dry Run\n\n```bash\n# Show what would run without executing\nact -n\n\n# Show with full details\nact -n -v\n```\n\n## Event Payloads\n\n### Custom Event Data\n\nCreate event JSON file:\n\n```json\n{\n  \"pull_request\": {\n    \"number\": 123,\n    \"head\": {\n      \"ref\": \"feature-branch\"\n    },\n    \"base\": {\n      \"ref\": \"main\"\n    }\n  }\n}\n```\n\nUse with act:\n```bash\nact pull_request -e event.json\n```\n\n### workflow_dispatch Inputs\n\n```json\n{\n  \"inputs\": {\n    \"environment\": \"staging\",\n    \"debug\": true\n  }\n}\n```\n\n```bash\nact workflow_dispatch -e inputs.json\n```\n\n## Secrets Management\n\n### Via Command Line\n\n```bash\n# Single secret\nact -s GITHUB_TOKEN=ghp_xxxxx\n\n# Multiple secrets\nact -s API_KEY=key123 -s DB_PASSWORD=pass456\n```\n\n### Via .secrets File\n\nCreate `.secrets` file (add to .gitignore):\n```\nGITHUB_TOKEN=ghp_xxxxx\nAPI_KEY=key123\nDB_PASSWORD=pass456\n```\n\nRun with secrets file:\n```bash\nact --secret-file .secrets\n```\n\n### Environment Variables\n\n```bash\n# Use existing env var\nact -s GITHUB_TOKEN\n\n# Set from command\nexport MY_SECRET=value\nact -s MY_SECRET\n```\n\n## Configuration\n\n### .actrc File\n\nCreate `.actrc` in repository root or home directory:\n\n```\n# Use specific platform\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Default secrets file\n--secret-file .secrets\n\n# Default environment\n--env-file .env\n\n# Container architecture\n--container-architecture linux/amd64\n\n# Verbose output\n-v\n```\n\n### Custom Runner Images\n\n```bash\n# Use custom image for platform\nact -P ubuntu-latest=my-custom-image:latest\n\n# Use medium size images (recommended)\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Use micro images (faster, less compatible)\nact -P ubuntu-latest=node:16-buster-slim\n```\n\n### Recommended Images\n\nact supports different image sizes:\n\n**Medium images (recommended):**\n- Better compatibility with GitHub Actions\n- More pre-installed tools\n- Slower startup but fewer failures\n\n```bash\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n-P ubuntu-22.04=catthehacker/ubuntu:act-22.04\n```\n\n**Micro images:**\n- Faster startup\n- Minimal pre-installed tools\n- May require additional setup\n\n## Environment Variables\n\n### Via .env File\n\nCreate `.env` file:\n```\nNODE_ENV=test\nAPI_URL=http://localhost:3000\nLOG_LEVEL=debug\n```\n\nUse with act:\n```bash\nact --env-file .env\n```\n\n### Via Command Line\n\n```bash\nact --env NODE_ENV=test --env API_URL=http://localhost:3000\n```\n\n## Advanced Usage\n\n### Bind Workspace\n\nMount local directory into container:\n```bash\nact --bind\n```\n\n### Reuse Containers\n\nKeep containers between runs for faster execution:\n```bash\nact --reuse\n```\n\n### Specific Platforms\n\n```bash\n# Run on specific platform\nact -P ubuntu-latest=ubuntu:latest\n\n# Multiple platforms\nact -P ubuntu-latest=ubuntu:latest \\\n    -P windows-latest=windows:latest\n```\n\n### Container Architecture\n\n```bash\n# Specify architecture (useful for M1/M2 Macs)\nact --container-architecture linux/amd64\n```\n\n### Network Configuration\n\n```bash\n# Use host network\nact --container-daemon-socket -\n\n# Custom network\nact --network my-network\n```\n\n### Artifact Server\n\n```bash\n# Enable artifact server on specific port\nact --artifact-server-path /tmp/artifacts \\\n    --artifact-server-port 34567\n```\n\n## Debugging\n\n### Verbose Output\n\n```bash\n# Verbose logging\nact -v\n\n# Very verbose (debug level)\nact -vv\n```\n\n### Watch Mode\n\n```bash\n# Watch for file changes and re-run\nact --watch\n```\n\n### Interactive Shell\n\n```bash\n# Drop into shell on failure\nact --shell bash\n```\n\n### Container Inspection\n\n```bash\n# List act containers\ndocker ps -a | grep act\n\n# Inspect specific container\ndocker inspect <container-id>\n\n# View logs\ndocker logs <container-id>\n```\n\n## Limitations and Differences\n\n### Not Supported by act\n\n- Some GitHub-hosted runner features\n- GitHub Apps and installations\n- OIDC token generation\n- Some GitHub API interactions\n- Certain cache implementations\n- Job summaries and annotations (limited)\n\n### Workarounds\n\n**Missing tools:**\n```yaml\nsteps:\n  - name: Install missing tool\n    run: |\n      if ! command -v tool &> /dev/null; then\n        apt-get update && apt-get install -y tool\n      fi\n```\n\n**GitHub API calls:**\n```yaml\n# Use GITHUB_TOKEN from secrets\n- env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n  run: gh api repos/${{ github.repository }}/issues\n```\n\n## Common Patterns\n\n### Testing Pull Request Workflow\n\n```bash\n# Create PR event payload\ncat > pr-event.json << EOF\n{\n  \"pull_request\": {\n    \"number\": 1,\n    \"head\": { \"ref\": \"feature\" },\n    \"base\": { \"ref\": \"main\" }\n  }\n}\nEOF\n\n# Run PR workflow\nact pull_request -e pr-event.json -j test\n```\n\n### CI/CD Pipeline Testing\n\n```bash\n# Test entire CI pipeline\nact push\n\n# Test specific stages\nact push -j build\nact push -j test\nact push -j deploy --secret-file .secrets\n```\n\n### Matrix Testing\n\n```bash\n# Run matrix strategy locally\nact -j test\n\n# Test specific matrix combination (modify workflow temporarily)\nact -j test --matrix node-version:20\n```\n\n### Workflow Development Cycle\n\n```bash\n# 1. List jobs\nact -l\n\n# 2. Dry run\nact -n -j build\n\n# 3. Run with verbose output\nact -v -j build\n\n# 4. Iterate and test\nact --reuse -j build\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n**Error: Cannot connect to Docker daemon**\n```bash\n# Start Docker\n# macOS: Start Docker Desktop\n# Linux:\nsudo systemctl start docker\n```\n\n**Error: Permission denied**\n```bash\n# Add user to docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Image Pull Issues\n\n**Error: Failed to pull image**\n```bash\n# Use specific image version\nact -P ubuntu-latest=ubuntu:22.04\n\n# Or use act's recommended images\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n```\n\n### Workflow Not Found\n\n**Error: No workflows found**\n```bash\n# Verify workflow files exist\nls -la .github/workflows/\n\n# Check workflow syntax\nact -n -v\n```\n\n### Secret Issues\n\n**Error: Secret not found**\n```bash\n# List required secrets from workflow\ngrep -r \"secrets\\.\" .github/workflows/\n\n# Provide via command line\nact -s SECRET_NAME=value\n\n# Or use secrets file\nact --secret-file .secrets\n```\n\n### Action Failures\n\n**Error: Action not found or fails**\n```yaml\n# Ensure action versions are compatible\n# Some actions may not work locally\n\n# Use alternative actions if needed\n# Or skip problematic steps locally:\n- name: Problematic step\n  if: github.event_name != 'act'  # Skip in act\n  uses: some/action@v1\n```\n\n### Platform Differences\n\n**Error: Command not found**\n```bash\n# Use medium-sized images with more tools\nact -P ubuntu-latest=catthehacker/ubuntu:act-latest\n\n# Or install tools in workflow\n- run: apt-get update && apt-get install -y <tool>\n```\n\n## Best Practices\n\n### .actrc Configuration\n\nCreate `.actrc` in repository:\n```\n-P ubuntu-latest=catthehacker/ubuntu:act-latest\n--secret-file .secrets\n--container-architecture linux/amd64\n--artifact-server-path /tmp/artifacts\n```\n\n### .gitignore Entries\n\n```gitignore\n# act secrets and config\n.secrets\n.env\n\n# act artifacts\n/tmp/artifacts/\n```\n\n### Conditional Logic for Local Testing\n\n```yaml\nsteps:\n  # Skip in local testing\n  - name: Deploy\n    if: github.event_name != 'act'\n    run: ./deploy.sh\n\n  # Run only in local testing\n  - name: Local setup\n    if: github.event_name == 'act'\n    run: ./local-setup.sh\n```\n\n### Fast Feedback Loop\n\n```bash\n# Use reuse flag for faster iterations\nact --reuse -j test\n\n# Run specific job being developed\nact -j my-new-job -v\n\n# Watch mode for continuous testing\nact --watch -j test\n```\n\n## Integration with Development Workflow\n\n### Pre-commit Testing\n\n```bash\n# Test before committing\nact -j test && git commit -m \"message\"\n\n# Git hook (.git/hooks/pre-commit)\n#!/bin/bash\nact -j test --quiet\n```\n\n### Quick Validation\n\n```bash\n# Validate workflow syntax\nact -n\n\n# Test specific changes\nact -j affected-job\n```\n\n### CI Parity\n\n```bash\n# Use same images as CI\nact -P ubuntu-latest=ubuntu:22.04\n\n# Use same secrets structure\nact --secret-file .secrets\n```\n\n## Scripts and Automation\n\n### Installation Script\n\nThe plugin includes an installation script at `scripts/install-act.sh`:\n\n```bash\n#!/usr/bin/env bash\n# Install act via mise or fallback methods\n\nif command -v mise &> /dev/null; then\n  echo \"Installing act via mise...\"\n  mise install act\nelif [[ \"$OSTYPE\" == \"darwin\"* ]] && command -v brew &> /dev/null; then\n  echo \"Installing act via Homebrew...\"\n  brew install act\nelif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n  echo \"Installing act via install script...\"\n  curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash\nelse\n  echo \"Please install act manually: https://github.com/nektos/act\"\n  exit 1\nfi\n\nact --version\n```\n\nRun with:\n```bash\nchmod +x scripts/install-act.sh\n./scripts/install-act.sh\n```\n\n## Anti-Fabrication Requirements\n\n- Execute `act --version` before documenting version numbers\n- Use `act -l` to verify actual workflows before claiming their presence\n- Execute `docker ps` to confirm Docker is running before troubleshooting\n- Run `act -n` to validate workflow syntax before claiming correctness\n- Execute actual `act` commands to verify behavior before documenting output format\n- Use `docker images` to verify available images before recommending specific versions\n- Never claim success rates or performance metrics without actual measurement\n- Execute `act -v` to observe actual error messages before documenting troubleshooting steps\n- Use Read tool to verify workflow files exist before testing them with act\n- Run actual event payloads through act before claiming they work correctly"
              },
              {
                "name": "github-actions",
                "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration",
                "path": "github/skills/actions/SKILL.md",
                "frontmatter": {
                  "name": "github-actions",
                  "description": "Create, configure, and optimize GitHub Actions including action types, triggers, runners, security practices, and marketplace integration"
                },
                "content": "# GitHub Actions\n\nActivate when creating, modifying, troubleshooting, or optimizing GitHub Actions components. This skill covers action development, marketplace integration, and best practices.\n\n## When to Use This Skill\n\nActivate when:\n- Creating custom GitHub Actions (JavaScript, Docker, or composite)\n- Publishing actions to GitHub Marketplace\n- Configuring action metadata and inputs/outputs\n- Implementing action security and permissions\n- Troubleshooting action execution\n- Selecting or evaluating marketplace actions\n- Optimizing action performance and reliability\n\n## Action Types\n\n### JavaScript Actions\n\nExecute directly on runners with fast startup and cross-platform compatibility.\n\n**Structure:**\n```\nmy-action/\n├── action.yml        # Metadata and interface\n├── index.js          # Entry point\n├── package.json      # Dependencies\n└── node_modules/     # Bundled dependencies\n```\n\n**Key Requirements:**\n- Use `@actions/core` for inputs/outputs\n- Use `@actions/github` for GitHub API access\n- Bundle all dependencies (use @vercel/ncc)\n- Support Node.js LTS versions\n\n**Example action.yml:**\n```yaml\nname: 'My JavaScript Action'\ndescription: 'Performs custom task'\ninputs:\n  token:\n    description: 'GitHub token'\n    required: true\n  config:\n    description: 'Configuration file path'\n    required: false\n    default: 'config.yml'\noutputs:\n  result:\n    description: 'Action result'\nruns:\n  using: 'node20'\n  main: 'dist/index.js'\n```\n\n### Docker Container Actions\n\nProvide consistent execution environment with all dependencies packaged.\n\n**Structure:**\n```\nmy-action/\n├── action.yml\n├── Dockerfile\n├── entrypoint.sh\n└── src/\n```\n\n**Key Requirements:**\n- Use lightweight base images (Alpine when possible)\n- Set proper file permissions\n- Handle signals gracefully\n- Output to STDOUT/STDERR correctly\n\n**Example Dockerfile:**\n```dockerfile\nFROM alpine:3.18\n\nRUN apk add --no-cache bash curl jq\n\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\n### Composite Actions\n\nCombine multiple steps and actions into reusable units.\n\n**Structure:**\n```yaml\nname: 'Setup Environment'\ndescription: 'Configure development environment'\ninputs:\n  node-version:\n    description: 'Node.js version'\n    required: false\n    default: '20'\nruns:\n  using: 'composite'\n  steps:\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - run: npm ci\n      shell: bash\n    - run: npm run build\n      shell: bash\n```\n\n## Action Metadata (action.yml)\n\n### Required Fields\n\n```yaml\nname: 'Action Name'           # Marketplace display name\ndescription: 'What it does'   # Clear, concise purpose\nruns:                         # Execution configuration\n  using: 'node20'            # or 'docker' or 'composite'\n```\n\n### Optional Fields\n\n```yaml\nauthor: 'Your Name'\nbranding:                    # Marketplace icon/color\n  icon: 'activity'\n  color: 'blue'\ninputs:                      # Define all inputs\n  input-name:\n    description: 'Purpose'\n    required: true\n    default: 'value'\noutputs:                     # Define all outputs\n  output-name:\n    description: 'What it contains'\n```\n\n## Inputs and Outputs\n\n### Reading Inputs\n\n**JavaScript:**\n```javascript\nconst core = require('@actions/core');\nconst token = core.getInput('token', { required: true });\nconst config = core.getInput('config') || 'default.yml';\n```\n\n**Shell:**\n```bash\nTOKEN=\"${{ inputs.token }}\"\nCONFIG=\"${{ inputs.config }}\"\n```\n\n### Setting Outputs\n\n**JavaScript:**\n```javascript\ncore.setOutput('result', 'success');\ncore.setOutput('artifact-url', artifactUrl);\n```\n\n**Shell:**\n```bash\necho \"result=success\" >> $GITHUB_OUTPUT\necho \"artifact-url=$ARTIFACT_URL\" >> $GITHUB_OUTPUT\n```\n\n## GitHub Actions Toolkit\n\nEssential npm packages for JavaScript actions:\n\n### @actions/core\n```javascript\nconst core = require('@actions/core');\n\n// Inputs/Outputs\nconst input = core.getInput('name');\ncore.setOutput('name', value);\n\n// Logging\ncore.info('Information message');\ncore.warning('Warning message');\ncore.error('Error message');\ncore.debug('Debug message');\n\n// Grouping\ncore.startGroup('Group name');\n// ... operations\ncore.endGroup();\n\n// Failure\ncore.setFailed('Action failed: reason');\n\n// Secrets\ncore.setSecret('sensitive-value');  // Masks in logs\n\n// Environment\ncore.exportVariable('VAR_NAME', 'value');\n```\n\n### @actions/github\n```javascript\nconst github = require('@actions/github');\n\n// Context\nconst context = github.context;\nconsole.log(context.repo);        // { owner, repo }\nconsole.log(context.sha);         // Commit SHA\nconsole.log(context.ref);         // Branch/tag ref\nconsole.log(context.actor);       // Triggering user\nconsole.log(context.payload);     // Webhook payload\n\n// Octokit client\nconst token = core.getInput('token');\nconst octokit = github.getOctokit(token);\n\n// API operations\nconst { data: issues } = await octokit.rest.issues.listForRepo({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  state: 'open'\n});\n```\n\n### @actions/exec\n```javascript\nconst exec = require('@actions/exec');\n\n// Execute commands\nawait exec.exec('npm', ['install']);\n\n// Capture output\nlet output = '';\nawait exec.exec('git', ['log', '--oneline'], {\n  listeners: {\n    stdout: (data) => { output += data.toString(); }\n  }\n});\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate and sanitize inputs:\n```javascript\nconst core = require('@actions/core');\n\nfunction validateInput(input) {\n  // Check for command injection\n  if (/[;&|`$()]/.test(input)) {\n    throw new Error('Invalid characters in input');\n  }\n  return input;\n}\n\nconst userInput = core.getInput('user-input');\nconst safeInput = validateInput(userInput);\n```\n\n### Token Permissions\n\nRequest minimal required permissions:\n```yaml\npermissions:\n  contents: read           # Read repository\n  pull-requests: write     # Comment on PRs\n  issues: write           # Create issues\n```\n\n### Secret Handling\n\n```javascript\n// Mask secrets in logs\ncore.setSecret(sensitiveValue);\n\n// Never log tokens\ncore.debug(`Token: ${token}`);  // ❌ WRONG\ncore.debug('Token received');   // ✅ CORRECT\n\n// Secure token usage\nconst octokit = github.getOctokit(token);\n// Token automatically included in requests\n```\n\n### Dependency Security\n\n```bash\n# Audit dependencies\nnpm audit\n\n# Use specific versions\nnpm install @actions/core@1.10.0\n\n# Bundle dependencies\nnpm install -g @vercel/ncc\nncc build index.js -o dist\n```\n\n## Marketplace Publishing\n\n### Prerequisites\n\n- Public repository\n- action.yml in repository root\n- README.md with usage examples\n- LICENSE file\n- Repository topics (optional)\n\n### Publishing Process\n\n1. Create release with semantic version tag:\n```bash\ngit tag -a v1.0.0 -m \"Release v1.0.0\"\ngit push origin v1.0.0\n```\n\n2. Create GitHub Release from tag\n3. Check \"Publish this Action to GitHub Marketplace\"\n4. Select primary category\n5. Verify branding icon/color\n\n### Version Management\n\nUse semantic versioning with major version tags:\n```bash\n# Release v1.2.3\ngit tag -a v1.2.3 -m \"Release v1.2.3\"\ngit tag -fa v1 -m \"Update v1 to v1.2.3\"\ngit push origin v1.2.3 v1 --force\n```\n\nUsers reference by major version:\n```yaml\n- uses: owner/action@v1  # Tracks latest v1.x.x\n```\n\n## Testing Actions Locally\n\nUse `act` for local testing (see act skill):\n```bash\n# Test action in current directory\nact -j test\n\n# Test with specific event\nact push\n\n# Test with secrets\nact -s GITHUB_TOKEN=ghp_xxx\n```\n\n## Common Patterns\n\n### Matrix Testing Action\n\n```yaml\n# action.yml\nname: 'Matrix Test Runner'\ndescription: 'Run tests across multiple configurations'\ninputs:\n  matrix-config:\n    description: 'JSON matrix configuration'\n    required: true\nruns:\n  using: 'composite'\n  steps:\n    - run: |\n        echo \"Testing with config: ${{ inputs.matrix-config }}\"\n        # Parse and execute tests\n      shell: bash\n```\n\n### Cache Management Action\n\n```javascript\nconst core = require('@actions/core');\nconst cache = require('@actions/cache');\n\nasync function run() {\n  const paths = [\n    'node_modules',\n    '.npm'\n  ];\n\n  const key = `deps-${process.platform}-${hashFiles('package-lock.json')}`;\n\n  // Restore cache\n  const cacheKey = await cache.restoreCache(paths, key);\n\n  if (!cacheKey) {\n    core.info('Cache miss, installing dependencies');\n    await exec.exec('npm', ['ci']);\n    await cache.saveCache(paths, key);\n  } else {\n    core.info(`Cache hit: ${cacheKey}`);\n  }\n}\n```\n\n### Artifact Upload Action\n\n```javascript\nconst artifact = require('@actions/artifact');\n\nasync function uploadArtifact() {\n  const artifactClient = artifact.create();\n  const files = [\n    'dist/bundle.js',\n    'dist/styles.css'\n  ];\n\n  const rootDirectory = 'dist';\n  const options = {\n    continueOnError: false\n  };\n\n  const uploadResponse = await artifactClient.uploadArtifact(\n    'build-artifacts',\n    files,\n    rootDirectory,\n    options\n  );\n\n  core.setOutput('artifact-id', uploadResponse.artifactId);\n}\n```\n\n## Troubleshooting\n\n### Action Not Found\n\n- Verify repository is public or accessible\n- Check action.yml exists in repository root\n- Confirm version tag exists\n\n### Permission Denied\n\n```yaml\n# Add required permissions to workflow\npermissions:\n  contents: write\n  pull-requests: write\n```\n\n### Node Modules Missing\n\n- Bundle dependencies with ncc\n- Check dist/ folder is committed\n- Verify node_modules excluded from .gitignore for dist/\n\n### Docker Action Fails\n\n- Check Dockerfile syntax\n- Verify entrypoint has execute permissions\n- Test container locally: `docker build -t test . && docker run test`\n\n## Anti-Fabrication Requirements\n\n- Execute Read or Glob tools to verify action files exist before claiming structure\n- Use Bash to test commands before documenting syntax\n- Validate action.yml schema against actual files using tool analysis\n- Execute actual API calls with @actions/github before documenting responses\n- Test permission configurations in real workflows before recommending settings\n- Never claim action capabilities without reading actual implementation code\n- Report actual npm audit results when discussing security, not fabricated vulnerability counts"
              },
              {
                "name": "github-workflows",
                "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns",
                "path": "github/skills/workflows/SKILL.md",
                "frontmatter": {
                  "name": "github-workflows",
                  "description": "Write, configure, and optimize GitHub Actions workflows including syntax, triggers, jobs, contexts, expressions, artifacts, and CI/CD patterns"
                },
                "content": "# GitHub Workflows\n\nActivate when creating, modifying, debugging, or optimizing GitHub Actions workflow files. This skill covers workflow syntax, structure, best practices, and common CI/CD patterns.\n\n## When to Use This Skill\n\nActivate when:\n- Writing .github/workflows/*.yml files\n- Configuring workflow triggers and events\n- Defining jobs, steps, and dependencies\n- Using expressions and contexts\n- Managing secrets and environment variables\n- Implementing CI/CD pipelines\n- Optimizing workflow performance\n- Debugging workflow failures\n\n## Workflow File Structure\n\n### Basic Anatomy\n\n```yaml\nname: CI                              # Workflow name (optional)\n\non:                                   # Trigger events\n  push:\n    branches: [main, develop]\n  pull_request:\n\nenv:                                  # Global environment variables\n  NODE_VERSION: '20'\n\njobs:                                 # Job definitions\n  build:\n    name: Build and Test            # Job name (optional)\n    runs-on: ubuntu-latest          # Runner environment\n\n    steps:\n      - name: Checkout code         # Step name (optional)\n        uses: actions/checkout@v4   # Use an action\n\n      - name: Run tests\n        run: npm test               # Run command\n```\n\n### File Location\n\nWorkflows must be in `.github/workflows/` directory:\n```\n.github/\n└── workflows/\n    ├── ci.yml\n    ├── deploy.yml\n    └── release.yml\n```\n\n## Trigger Events (on:)\n\n### Push Events\n\n```yaml\non:\n  push:\n    branches:\n      - main\n      - 'release/**'        # Glob patterns\n    tags:\n      - 'v*'                # Version tags\n    paths:\n      - 'src/**'            # Only when these paths change\n      - '!docs/**'          # Ignore docs changes\n```\n\n### Pull Request Events\n\n```yaml\non:\n  pull_request:\n    types:\n      - opened\n      - synchronize       # New commits pushed\n      - reopened\n    branches:\n      - main\n    paths-ignore:\n      - '**.md'\n```\n\n### Schedule (Cron)\n\n```yaml\non:\n  schedule:\n    # Every day at 2am UTC\n    - cron: '0 2 * * *'\n    # Every Monday at 9am UTC\n    - cron: '0 9 * * 1'\n```\n\n### Manual Trigger (workflow_dispatch)\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Deployment environment'\n        required: true\n        type: choice\n        options:\n          - development\n          - staging\n          - production\n      debug:\n        description: 'Enable debug logging'\n        required: false\n        type: boolean\n        default: false\n```\n\n### Multiple Events\n\n```yaml\non:\n  push:\n    branches: [main]\n  pull_request:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly\n```\n\n## Jobs\n\n### Basic Job Configuration\n\n```yaml\njobs:\n  build:\n    name: Build Application\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm ci\n      - run: npm run build\n```\n\n### Runner Selection\n\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest        # Ubuntu (fastest, most common)\n\n  test-macos:\n    runs-on: macos-latest         # macOS\n\n  test-windows:\n    runs-on: windows-latest       # Windows\n\n  test-specific:\n    runs-on: ubuntu-22.04         # Specific version\n```\n\n### Matrix Strategy\n\n```yaml\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        node: [18, 20, 21]\n        exclude:\n          - os: macos-latest\n            node: 18\n      fail-fast: false            # Continue on failure\n      max-parallel: 4             # Concurrent jobs limit\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n      - run: npm test\n```\n\n### Job Dependencies\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n\n  test:\n    needs: build                  # Wait for build\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  deploy:\n    needs: [build, test]          # Wait for multiple jobs\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n```\n\n### Conditional Execution\n\n```yaml\njobs:\n  deploy:\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run deploy\n\n  notify:\n    if: failure()                 # Run only if previous jobs failed\n    needs: [build, test]\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Build failed\"\n```\n\n## Steps\n\n### Using Actions\n\n```yaml\nsteps:\n  - name: Checkout repository\n    uses: actions/checkout@v4\n    with:\n      fetch-depth: 0              # Full history\n      submodules: recursive       # Include submodules\n\n  - name: Setup Node.js\n    uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n```\n\n### Running Commands\n\n```yaml\nsteps:\n  - name: Single command\n    run: npm install\n\n  - name: Multi-line script\n    run: |\n      echo \"Installing dependencies\"\n      npm ci\n      npm run build\n\n  - name: Shell selection\n    shell: bash\n    run: echo \"Using bash\"\n```\n\n### Conditional Steps\n\n```yaml\nsteps:\n  - name: Run on main branch only\n    if: github.ref == 'refs/heads/main'\n    run: npm run deploy\n\n  - name: Run on PR only\n    if: github.event_name == 'pull_request'\n    run: npm run test:pr\n```\n\n### Continue on Error\n\n```yaml\nsteps:\n  - name: Lint (optional)\n    continue-on-error: true\n    run: npm run lint\n\n  - name: Test (required)\n    run: npm test\n```\n\n## Environment Variables and Secrets\n\n### Global Variables\n\n```yaml\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo $NODE_ENV\n```\n\n### Job-Level Variables\n\n```yaml\njobs:\n  build:\n    env:\n      BUILD_TYPE: release\n    steps:\n      - run: echo $BUILD_TYPE\n```\n\n### Step-Level Variables\n\n```yaml\nsteps:\n  - name: Configure\n    env:\n      CONFIG_PATH: ./config.json\n    run: cat $CONFIG_PATH\n```\n\n### Using Secrets\n\n```yaml\nsteps:\n  - name: Deploy\n    env:\n      API_KEY: ${{ secrets.API_KEY }}\n      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}\n    run: ./deploy.sh\n```\n\n### Setting Variables Between Steps\n\n```yaml\nsteps:\n  - name: Set version\n    id: version\n    run: echo \"VERSION=$(cat version.txt)\" >> $GITHUB_OUTPUT\n\n  - name: Use version\n    run: echo \"Version is ${{ steps.version.outputs.VERSION }}\"\n```\n\n## Contexts\n\n### github Context\n\n```yaml\nsteps:\n  - name: Context information\n    run: |\n      echo \"Repository: ${{ github.repository }}\"\n      echo \"Branch: ${{ github.ref_name }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Run ID: ${{ github.run_id }}\"\n```\n\n### env Context\n\n```yaml\nenv:\n  MY_VAR: value\n\nsteps:\n  - run: echo \"${{ env.MY_VAR }}\"\n```\n\n### job Context\n\n```yaml\nsteps:\n  - name: Job status\n    if: job.status == 'success'\n    run: echo \"Job succeeded\"\n```\n\n### steps Context\n\n```yaml\nsteps:\n  - id: first-step\n    run: echo \"output=hello\" >> $GITHUB_OUTPUT\n\n  - run: echo \"${{ steps.first-step.outputs.output }}\"\n```\n\n### runner Context\n\n```yaml\nsteps:\n  - run: |\n      echo \"OS: ${{ runner.os }}\"\n      echo \"Arch: ${{ runner.arch }}\"\n      echo \"Temp: ${{ runner.temp }}\"\n```\n\n### matrix Context\n\n```yaml\nstrategy:\n  matrix:\n    version: [18, 20]\n\nsteps:\n  - run: echo \"Node ${{ matrix.version }}\"\n```\n\n## Expressions\n\n### Operators\n\n```yaml\nsteps:\n  # Comparison\n  - if: github.ref == 'refs/heads/main'\n\n  # Logical\n  - if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n  - if: github.event_name == 'pull_request' || github.event_name == 'push'\n\n  # Negation\n  - if: \"!cancelled()\"\n\n  # Contains\n  - if: contains(github.event.head_commit.message, '[skip ci]')\n\n  # StartsWith/EndsWith\n  - if: startsWith(github.ref, 'refs/tags/v')\n  - if: endsWith(github.ref, '-beta')\n```\n\n### Functions\n\n```yaml\nsteps:\n  # Status functions\n  - if: success()        # Previous steps succeeded\n  - if: failure()        # Any previous step failed\n  - if: always()         # Always run\n  - if: cancelled()      # Workflow cancelled\n\n  # String functions\n  - run: echo \"${{ format('Hello {0}', github.actor) }}\"\n  - if: contains(github.event.pull_request.labels.*.name, 'deploy')\n\n  # JSON functions\n  - run: echo '${{ toJSON(github.event) }}'\n  - run: echo '${{ fromJSON(env.CONFIG).database.host }}'\n\n  # Hash function\n  - run: echo \"${{ hashFiles('**/package-lock.json') }}\"\n```\n\n## Artifacts\n\n### Upload Artifacts\n\n```yaml\nsteps:\n  - name: Build\n    run: npm run build\n\n  - name: Upload artifacts\n    uses: actions/upload-artifact@v4\n    with:\n      name: build-files\n      path: |\n        dist/\n        build/\n      retention-days: 7\n      if-no-files-found: error\n```\n\n### Download Artifacts\n\n```yaml\njobs:\n  build:\n    steps:\n      - run: npm run build\n      - uses: actions/upload-artifact@v4\n        with:\n          name: dist\n          path: dist/\n\n  test:\n    needs: build\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: dist\n          path: dist/\n      - run: npm test\n```\n\n## Caching\n\n### npm Cache\n\n```yaml\nsteps:\n  - uses: actions/checkout@v4\n  - uses: actions/setup-node@v4\n    with:\n      node-version: '20'\n      cache: 'npm'\n  - run: npm ci\n```\n\n### Manual Cache\n\n```yaml\nsteps:\n  - uses: actions/cache@v4\n    with:\n      path: |\n        ~/.npm\n        node_modules\n      key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n      restore-keys: |\n        ${{ runner.os }}-node-\n```\n\n## Permissions\n\n### Repository Token Permissions\n\n```yaml\npermissions:\n  contents: read              # Repository content\n  pull-requests: write        # PR comments\n  issues: write              # Issue creation/comments\n  checks: write              # Check runs\n  statuses: write            # Commit statuses\n  deployments: write         # Deployments\n  packages: write            # Package registry\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n```\n\n### Job-Level Permissions\n\n```yaml\njobs:\n  build:\n    permissions:\n      contents: read\n      pull-requests: write\n    steps:\n      - uses: actions/checkout@v4\n```\n\n## Concurrency\n\n### Prevent Concurrent Runs\n\n```yaml\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true    # Cancel running workflows\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - run: ./deploy.sh\n```\n\n### Job-Level Concurrency\n\n```yaml\njobs:\n  deploy:\n    concurrency:\n      group: deploy-${{ github.ref }}\n      cancel-in-progress: false\n    steps:\n      - run: ./deploy.sh\n```\n\n## Reusable Workflows\n\n### Define Reusable Workflow\n\n```yaml\n# .github/workflows/reusable-test.yml\nname: Reusable Test Workflow\n\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n      coverage:\n        required: false\n        type: boolean\n        default: false\n    outputs:\n      test-result:\n        description: \"Test execution result\"\n        value: ${{ jobs.test.outputs.result }}\n    secrets:\n      token:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    outputs:\n      result: ${{ steps.test.outputs.result }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ inputs.node-version }}\n      - run: npm test\n        id: test\n```\n\n### Call Reusable Workflow\n\n```yaml\njobs:\n  test:\n    uses: ./.github/workflows/reusable-test.yml\n    with:\n      node-version: '20'\n      coverage: true\n    secrets:\n      token: ${{ secrets.GITHUB_TOKEN }}\n```\n\n## Common CI/CD Patterns\n\n### Node.js CI\n\n```yaml\nname: Node.js CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20, 21]\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm test\n      - run: npm run build\n```\n\n### Docker Build and Push\n\n```yaml\nname: Docker\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ghcr.io/${{ github.repository }}\n          tags: |\n            type=ref,event=branch\n            type=semver,pattern={{version}}\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n```\n\n### Deploy on Release\n\n```yaml\nname: Deploy\n\non:\n  release:\n    types: [published]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://example.com\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        env:\n          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\n        run: ./deploy.sh\n```\n\n### Monorepo with Path Filtering\n\n```yaml\nname: Monorepo CI\n\non:\n  pull_request:\n    paths:\n      - 'packages/**'\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      frontend: ${{ steps.filter.outputs.frontend }}\n      backend: ${{ steps.filter.outputs.backend }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          filters: |\n            frontend:\n              - 'packages/frontend/**'\n            backend:\n              - 'packages/backend/**'\n\n  test-frontend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.frontend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=frontend\n\n  test-backend:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.backend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test --workspace=backend\n```\n\n## Debugging Workflows\n\n### Enable Debug Logging\n\nSet repository secrets:\n- `ACTIONS_RUNNER_DEBUG`: true\n- `ACTIONS_STEP_DEBUG`: true\n\n### Debug Steps\n\n```yaml\nsteps:\n  - name: Debug context\n    run: |\n      echo \"Event: ${{ github.event_name }}\"\n      echo \"Ref: ${{ github.ref }}\"\n      echo \"SHA: ${{ github.sha }}\"\n      echo \"Actor: ${{ github.actor }}\"\n\n  - name: Dump GitHub context\n    run: echo '${{ toJSON(github) }}'\n\n  - name: Dump runner context\n    run: echo '${{ toJSON(runner) }}'\n```\n\n### Tmate Debugging\n\n```yaml\nsteps:\n  - name: Setup tmate session\n    if: failure()\n    uses: mxschmitt/action-tmate@v3\n    timeout-minutes: 30\n```\n\n## Performance Optimization\n\n### Use Caching\n\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}\n```\n\n### Optimize Checkout\n\n```yaml\n- uses: actions/checkout@v4\n  with:\n    fetch-depth: 1              # Shallow clone\n    sparse-checkout: |          # Partial checkout\n      src/\n      tests/\n```\n\n### Concurrent Jobs\n\n```yaml\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run lint\n\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  build:\n    needs: [lint, test]         # Parallel lint and test\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm run build\n```\n\n## Anti-Fabrication Requirements\n\n- Execute Read tool to verify workflow files exist before claiming structure\n- Use Bash with `gh workflow list` to confirm actual workflow names before referencing them\n- Execute `gh workflow view <workflow>` to verify trigger configuration before documenting it\n- Use Glob to find actual workflow files before claiming their presence\n- Execute `gh run list` to verify actual workflow runs before discussing execution patterns\n- Never claim workflow success rates without actual run history analysis\n- Validate YAML syntax using yamllint or similar tools via Bash before claiming correctness\n- Report actual permission errors from workflow runs, not fabricated authorization issues\n- Execute actual cache operations before claiming cache hit/miss percentages\n- Use Read tool on action.yml files to verify action inputs/outputs before documenting usage"
              },
              {
                "name": "daisyui",
                "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design",
                "path": "ui/skills/daisyui/SKILL.md",
                "frontmatter": {
                  "name": "daisyui",
                  "description": "Guide for using daisyUI component library with Tailwind CSS for building UI components, theming, and responsive design"
                },
                "content": "# daisyUI Component Library\n\nUse this skill when building user interfaces with daisyUI and Tailwind CSS, implementing UI components, or configuring themes.\n\n## When to Use This Skill\n\nActivate when:\n- Building UI components with daisyUI\n- Choosing appropriate daisyUI components for design needs\n- Implementing responsive layouts with daisyUI\n- Configuring or customizing themes\n- Converting designs to daisyUI components\n- Troubleshooting daisyUI component styling\n\n## What is daisyUI?\n\ndaisyUI is a Tailwind CSS component library providing:\n\n- **Semantic component classes** - High-level abstractions of Tailwind utilities\n- **33+ built-in themes** - Light, dark, and creative theme variants\n- **Framework-agnostic** - Works with any HTML/CSS project\n- **Utility-first compatible** - Combine components with Tailwind utilities\n\n## Installation\n\nAdd daisyUI to your project:\n\n```bash\nnpm install -D daisyui@latest\n```\n\nConfigure `tailwind.config.js`:\n\n```javascript\nmodule.exports = {\n  plugins: [require(\"daisyui\")],\n}\n```\n\nFor detailed installation options and CDN usage, see `references/installation.md`.\n\n## Component Categories\n\ndaisyUI provides components across these categories:\n\n- **Actions**: Buttons, dropdowns, modals, swap\n- **Data Display**: Cards, badges, tables, carousels, stats\n- **Data Input**: Input, textarea, select, checkbox, radio, toggle\n- **Navigation**: Navbar, menu, tabs, breadcrumbs, pagination\n- **Feedback**: Alert, progress, loading, toast, tooltip\n- **Layout**: Drawer, footer, hero, stack, divider\n\nFor component-specific guidance, consult the appropriate reference file.\n\n## Quick Usage\n\n### Basic Button\n\n```html\n<button class=\"btn\">Button</button>\n<button class=\"btn btn-primary\">Primary</button>\n<button class=\"btn btn-secondary\">Secondary</button>\n<button class=\"btn btn-accent\">Accent</button>\n```\n\n### Card Component\n\n```html\n<div class=\"card w-96 bg-base-100 shadow-xl\">\n  <figure><img src=\"image.jpg\" alt=\"Image\" /></figure>\n  <div class=\"card-body\">\n    <h2 class=\"card-title\">Card Title</h2>\n    <p>Card description text</p>\n    <div class=\"card-actions justify-end\">\n      <button class=\"btn btn-primary\">Action</button>\n    </div>\n  </div>\n</div>\n```\n\n### Modal\n\n```html\n<button class=\"btn\" onclick=\"my_modal.showModal()\">Open Modal</button>\n\n<dialog id=\"my_modal\" class=\"modal\">\n  <div class=\"modal-box\">\n    <h3 class=\"font-bold text-lg\">Modal Title</h3>\n    <p class=\"py-4\">Modal content here</p>\n    <div class=\"modal-action\">\n      <form method=\"dialog\">\n        <button class=\"btn\">Close</button>\n      </form>\n    </div>\n  </div>\n</dialog>\n```\n\n## Theming\n\n### Using Built-in Themes\n\nSet theme via HTML attribute:\n\n```html\n<html data-theme=\"cupcake\">\n```\n\nAvailable themes: light, dark, cupcake, bumblebee, emerald, corporate, synthwave, retro, cyberpunk, valentine, halloween, garden, forest, aqua, lofi, pastel, fantasy, wireframe, black, luxury, dracula, cmyk, autumn, business, acid, lemonade, night, coffee, winter, dim, nord, sunset\n\n### Theme Switching\n\n```html\n<select class=\"select\" data-choose-theme>\n  <option value=\"light\">Light</option>\n  <option value=\"dark\">Dark</option>\n  <option value=\"cupcake\">Cupcake</option>\n</select>\n```\n\nFor advanced theming and customization, see `references/theming.md`.\n\n## Responsive Design\n\ndaisyUI components work with Tailwind's responsive prefixes:\n\n```html\n<button class=\"btn btn-sm md:btn-md lg:btn-lg\">\n  Responsive Button\n</button>\n\n<div class=\"card w-full md:w-96\">\n  <!-- Responsive card -->\n</div>\n```\n\n## When to Consult References\n\n- **Installation details**: Read `references/installation.md`\n- **Complete component list**: Read `references/components.md`\n- **Theming and customization**: Read `references/theming.md`\n- **Layout patterns**: Read `references/layouts.md`\n- **Form components**: Read `references/forms.md`\n- **Common patterns**: Read `references/patterns.md`\n\n## Combining with Tailwind Utilities\n\ndaisyUI semantic classes combine with Tailwind utilities:\n\n```html\n<!-- daisyUI component + Tailwind utilities -->\n<button class=\"btn btn-primary shadow-lg hover:shadow-xl transition-all\">\n  Enhanced Button\n</button>\n\n<div class=\"card bg-base-100 border-2 border-primary rounded-lg p-4\">\n  <!-- Card with custom styling -->\n</div>\n```\n\n## Key Principles\n\n- **Semantic over utility**: Use component classes for common patterns\n- **Utility for customization**: Apply Tailwind utilities for unique styling\n- **Theme-aware**: Components adapt to theme colors automatically\n- **Accessible**: Components follow accessibility best practices\n- **Composable**: Combine components to build complex UIs\n\n## Pro Tips\n\n- Use `btn-{size}` modifiers: `btn-xs`, `btn-sm`, `btn-md`, `btn-lg`\n- Add `btn-outline` for outlined button variants\n- Use `badge` component for status indicators\n- Combine `modal` with `modal-backdrop` for better UX\n- Use `drawer` for mobile navigation patterns\n- Leverage `stats` component for dashboard metrics\n- Use `loading` class on buttons for async operations"
              }
            ]
          }
        ]
      }
    }
  ]
}