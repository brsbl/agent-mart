{
  "owner": {
    "id": "timescale",
    "display_name": "Tiger Data",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/8986001?v=4",
    "url": "https://github.com/timescale",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 4,
      "total_stars": 1362,
      "total_forks": 74
    }
  },
  "repos": [
    {
      "full_name": "timescale/pg-aiguide",
      "url": "https://github.com/timescale/pg-aiguide",
      "description": "MCP server and Claude plugin for Postgres skills and documentation. Helps AI coding tools generate better PostgreSQL code.",
      "homepage": "",
      "signals": {
        "stars": 1362,
        "forks": 74,
        "pushed_at": "2026-01-12T22:38:32Z",
        "created_at": "2025-07-23T16:40:10Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1260
        },
        {
          "path": ".dockerignore",
          "type": "blob",
          "size": 36
        },
        {
          "path": ".env.sample",
          "type": "blob",
          "size": 149
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/build-and-deploy-on-merge.yaml",
          "type": "blob",
          "size": 1328
        },
        {
          "path": ".github/workflows/build-on-feature-branch.yaml",
          "type": "blob",
          "size": 500
        },
        {
          "path": ".github/workflows/deploy-feature-branch.yaml",
          "type": "blob",
          "size": 468
        },
        {
          "path": ".github/workflows/ingest-postgres-docs.yaml",
          "type": "blob",
          "size": 1799
        },
        {
          "path": ".github/workflows/ingest-tiger-docs.yaml",
          "type": "blob",
          "size": 1362
        },
        {
          "path": ".github/workflows/lint.yml",
          "type": "blob",
          "size": 320
        },
        {
          "path": ".github/workflows/publish.yml",
          "type": "blob",
          "size": 3999
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 138
        },
        {
          "path": "API.md",
          "type": "blob",
          "size": 1940
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 1134
        },
        {
          "path": "DEVELOPMENT.md",
          "type": "blob",
          "size": 4063
        },
        {
          "path": "Dockerfile",
          "type": "blob",
          "size": 352
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 9585
        },
        {
          "path": "NOTICE",
          "type": "blob",
          "size": 573
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 9326
        },
        {
          "path": "biome.json",
          "type": "blob",
          "size": 554
        },
        {
          "path": "bun",
          "type": "blob",
          "size": 381
        },
        {
          "path": "bun.lock",
          "type": "blob",
          "size": 195743
        },
        {
          "path": "docker-compose.yml",
          "type": "blob",
          "size": 966
        },
        {
          "path": "docker",
          "type": "tree",
          "size": null
        },
        {
          "path": "docker/tsdb",
          "type": "tree",
          "size": null
        },
        {
          "path": "docker/tsdb/100_setup_db.sql",
          "type": "blob",
          "size": 356
        },
        {
          "path": "generate-server.json.ts",
          "type": "blob",
          "size": 2233
        },
        {
          "path": "ingest",
          "type": "tree",
          "size": null
        },
        {
          "path": "ingest/.python-version",
          "type": "blob",
          "size": 5
        },
        {
          "path": "ingest/README.md",
          "type": "blob",
          "size": 2944
        },
        {
          "path": "ingest/postgres_docs.py",
          "type": "blob",
          "size": 17239
        },
        {
          "path": "ingest/pyproject.toml",
          "type": "blob",
          "size": 391
        },
        {
          "path": "ingest/tiger_docs.py",
          "type": "blob",
          "size": 47828
        },
        {
          "path": "ingest/tiger_docs_config.toml",
          "type": "blob",
          "size": 406
        },
        {
          "path": "ingest/uv.lock",
          "type": "blob",
          "size": 135661
        },
        {
          "path": "migrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "migrations/1756387543053-initial.js",
          "type": "blob",
          "size": 2664
        },
        {
          "path": "migrations/1759241172003-add-hnsw-indexes.js",
          "type": "blob",
          "size": 1112
        },
        {
          "path": "migrations/1759241361471-add-version-index.js",
          "type": "blob",
          "size": 737
        },
        {
          "path": "migrations/1759851009030-add-tiger-indexes.js",
          "type": "blob",
          "size": 1603
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 1345
        },
        {
          "path": "skills.yaml",
          "type": "blob",
          "size": 53
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/design-postgres-tables",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/design-postgres-tables/SKILL.md",
          "type": "blob",
          "size": 16144
        },
        {
          "path": "skills/find-hypertable-candidates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/find-hypertable-candidates/SKILL.md",
          "type": "blob",
          "size": 9650
        },
        {
          "path": "skills/migrate-postgres-tables-to-hypertables",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/migrate-postgres-tables-to-hypertables/SKILL.md",
          "type": "blob",
          "size": 14180
        },
        {
          "path": "skills/setup-timescaledb-hypertables",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/setup-timescaledb-hypertables/SKILL.md",
          "type": "blob",
          "size": 16795
        },
        {
          "path": "src",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/apis",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/apis/index.ts",
          "type": "blob",
          "size": 804
        },
        {
          "path": "src/apis/kewordSearchTigerDocs.ts",
          "type": "blob",
          "size": 2391
        },
        {
          "path": "src/apis/semanticSearchPostgresDocs.ts",
          "type": "blob",
          "size": 2795
        },
        {
          "path": "src/apis/semanticSearchTigerDocs.ts",
          "type": "blob",
          "size": 2522
        },
        {
          "path": "src/config.ts",
          "type": "blob",
          "size": 55
        },
        {
          "path": "src/httpServer.ts",
          "type": "blob",
          "size": 704
        },
        {
          "path": "src/index.ts",
          "type": "blob",
          "size": 357
        },
        {
          "path": "src/migrate.ts",
          "type": "blob",
          "size": 3195
        },
        {
          "path": "src/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/prompts/index.ts",
          "type": "blob",
          "size": 150
        },
        {
          "path": "src/serverInfo.ts",
          "type": "blob",
          "size": 285
        },
        {
          "path": "src/stdio.ts",
          "type": "blob",
          "size": 337
        },
        {
          "path": "src/types.ts",
          "type": "blob",
          "size": 134
        },
        {
          "path": "src/util",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/util/featureFlags.ts",
          "type": "blob",
          "size": 956
        },
        {
          "path": "tsconfig.json",
          "type": "blob",
          "size": 530
        }
      ],
      "marketplace": {
        "name": "aiguide",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "TigerData",
          "url": "https://tigerdata.com",
          "email": "support@tigerdata.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "pg",
            "description": "Comprehensive PostgreSQL documentation and best practices through semantic search and curated skills, including ecosystem tools like TimescaleDB and Tiger Cloud",
            "source": "./",
            "category": "database",
            "version": "0.1.0",
            "author": {
              "name": "TigerData",
              "url": "https://tigerdata.com"
            },
            "install_commands": [
              "/plugin marketplace add timescale/pg-aiguide",
              "/plugin install pg@aiguide"
            ],
            "signals": {
              "stars": 1362,
              "forks": 74,
              "pushed_at": "2026-01-12T22:38:32Z",
              "created_at": "2025-07-23T16:40:10Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": [
              {
                "name": "design-postgres-tables",
                "description": "Comprehensive PostgreSQL-specific table design reference covering data types, indexing, constraints, performance patterns, and advanced features",
                "path": "skills/design-postgres-tables/SKILL.md",
                "frontmatter": {
                  "name": "design-postgres-tables",
                  "description": "Comprehensive PostgreSQL-specific table design reference covering data types, indexing, constraints, performance patterns, and advanced features"
                },
                "content": "# PostgreSQL Table Design\n\n## Core Rules\n\n- Define a **PRIMARY KEY** for reference tables (users, orders, etc.). Not always needed for time-series/event/log data. When used, prefer `BIGINT GENERATED ALWAYS AS IDENTITY`; use `UUID` only when global uniqueness/opacity is needed.\n- **Normalize first (to 3NF)** to eliminate data redundancy and update anomalies; denormalize **only** for measured, high-ROI reads where join performance is proven problematic. Premature denormalization creates maintenance burden.\n- Add **NOT NULL** everywhere it’s semantically required; use **DEFAULT**s for common values.\n- Create **indexes for access paths you actually query**: PK/unique (auto), **FK columns (manual!)**, frequent filters/sorts, and join keys.\n- Prefer **TIMESTAMPTZ** for event time; **NUMERIC** for money; **TEXT** for strings; **BIGINT** for integer values, **DOUBLE PRECISION** for floats (or `NUMERIC` for exact decimal arithmetic).\n\n## PostgreSQL “Gotchas”\n\n- **Identifiers**: unquoted → lowercased. Avoid quoted/mixed-case names. Convention: use `snake_case` for table/column names.\n- **Unique + NULLs**: UNIQUE allows multiple NULLs. Use `UNIQUE (...) NULLS NOT DISTINCT` (PG15+) to restrict to one NULL.\n- **FK indexes**: PostgreSQL **does not** auto-index FK columns. Add them.\n- **No silent coercions**: length/precision overflows error out (no truncation). Example: inserting 999 into `NUMERIC(2,0)` fails with error, unlike some databases that silently truncate or round.\n- **Sequences/identity have gaps** (normal; don't \"fix\"). Rollbacks, crashes, and concurrent transactions create gaps in ID sequences (1, 2, 5, 6...). This is expected behavior—don't try to make IDs consecutive.\n- **Heap storage**: no clustered PK by default (unlike SQL Server/MySQL InnoDB); `CLUSTER` is one-off reorganization, not maintained on subsequent inserts. Row order on disk is insertion order unless explicitly clustered.\n- **MVCC**: updates/deletes leave dead tuples; vacuum handles them—design to avoid hot wide-row churn.\n\n## Data Types\n\n- **IDs**: `BIGINT GENERATED ALWAYS AS IDENTITY` preferred (`GENERATED BY DEFAULT` also fine); `UUID` when merging/federating/used in a distributed system or for opaque IDs. Generate with `uuidv7()` (preferred if using PG18+) or `gen_random_uuid()` (if using an older PG version).\n- **Integers**: prefer `BIGINT` unless storage space is critical; `INTEGER` for smaller ranges; avoid `SMALLINT` unless constrained.\n- **Floats**: prefer `DOUBLE PRECISION` over `REAL` unless storage space is critical. Use `NUMERIC` for exact decimal arithmetic.\n- **Strings**: prefer `TEXT`; if length limits needed, use `CHECK (LENGTH(col) <= n)` instead of `VARCHAR(n)`; avoid `CHAR(n)`. Use `BYTEA` for binary data. Large strings/binary (>2KB default threshold) automatically stored in TOAST with compression. TOAST storage: `PLAIN` (no TOAST), `EXTENDED` (compress + out-of-line), `EXTERNAL` (out-of-line, no compress), `MAIN` (compress, keep in-line if possible). Default `EXTENDED` usually optimal. Control with `ALTER TABLE tbl ALTER COLUMN col SET STORAGE strategy` and `ALTER TABLE tbl SET (toast_tuple_target = 4096)` for threshold. Case-insensitive: for locale/accent handling use non-deterministic collations; for plain ASCII use expression indexes on `LOWER(col)` (preferred unless column needs case-insensitive PK/FK/UNIQUE) or `CITEXT`.\n- **Money**: `NUMERIC(p,s)` (never float).\n- **Time**: `TIMESTAMPTZ` for timestamps; `DATE` for date-only; `INTERVAL` for durations. Avoid `TIMESTAMP` (without timezone). Use `now()` for transaction start time, `clock_timestamp()` for current wall-clock time.\n- **Booleans**: `BOOLEAN` with `NOT NULL` constraint unless tri-state values are required.\n- **Enums**: `CREATE TYPE ... AS ENUM` for small, stable sets (e.g. US states, days of week). For business-logic-driven and evolving values (e.g. order statuses) → use TEXT (or INT) + CHECK or lookup table.\n- **Arrays**: `TEXT[]`, `INTEGER[]`, etc. Use for ordered lists where you query elements. Index with **GIN** for containment (`@>`, `<@`) and overlap (`&&`) queries. Access: `arr[1]` (1-indexed), `arr[1:3]` (slicing). Good for tags, categories; avoid for relations—use junction tables instead. Literal syntax: `'{val1,val2}'` or `ARRAY[val1,val2]`.\n- **Range types**: `daterange`, `numrange`, `tstzrange` for intervals. Support overlap (`&&`), containment (`@>`), operators. Index with **GiST**. Good for scheduling, versioning, numeric ranges. Pick a bounds scheme and use it consistently; prefer `[)` (inclusive/exclusive) by default.\n- **Network types**: `INET` for IP addresses, `CIDR` for network ranges, `MACADDR` for MAC addresses. Support network operators (`<<`, `>>`, `&&`).\n- **Geometric types**: avoid `POINT`, `LINE`, `POLYGON`, `CIRCLE`. Index with **GiST**. Consider **PostGIS** for spatial features.\n- **Text search**: `TSVECTOR` for full-text search documents, `TSQUERY` for search queries. Index `tsvector` with **GIN**. Always specify language: `to_tsvector('english', col)` and `to_tsquery('english', 'query')`. Never use single-argument versions. This applies to both index expressions and queries.\n- **Domain types**: `CREATE DOMAIN email AS TEXT CHECK (VALUE ~ '^[^@]+@[^@]+$')` for reusable custom types with validation. Enforces constraints across tables.\n- **Composite types**: `CREATE TYPE address AS (street TEXT, city TEXT, zip TEXT)` for structured data within columns. Access with `(col).field` syntax.\n- **JSONB**: preferred over JSON; index with **GIN**. Use only for optional/semi-structured attrs. ONLY use JSON if the original ordering of the contents MUST be preserved.\n- **Vector types**: `vector` type by `pgvector` for vector similarity search for embeddings.\n\n### Do not use the following data types\n\n- DO NOT use `timestamp` (without time zone); DO use `timestamptz` instead.\n- DO NOT use `char(n)` or `varchar(n)`; DO use `text` instead.\n- DO NOT use `money` type; DO use `numeric` instead.\n- DO NOT use `timetz` type; DO use `timestamptz` instead.\n- DO NOT use `timestamptz(0)` or any other precision specification; DO use `timestamptz` instead\n- DO NOT use `serial` type; DO use `generated always as identity` instead.\n- DO NOT use `POINT`, `LINE`, `POLYGON`, `CIRCLE` built-in types, DO use `geometry` from postgis extension instead.\n\n## Table Types\n\n- **Regular**: default; fully durable, logged.\n- **TEMPORARY**: session-scoped, auto-dropped, not logged. Faster for scratch work.\n- **UNLOGGED**: persistent but not crash-safe. Faster writes; good for caches/staging.\n\n## Row-Level Security\n\nEnable with `ALTER TABLE tbl ENABLE ROW LEVEL SECURITY`. Create policies: `CREATE POLICY user_access ON orders FOR SELECT TO app_users USING (user_id = current_user_id())`. Built-in user-based access control at the row level.\n\n## Constraints\n\n- **PK**: implicit UNIQUE + NOT NULL; creates a B-tree index.\n- **FK**: specify `ON DELETE/UPDATE` action (`CASCADE`, `RESTRICT`, `SET NULL`, `SET DEFAULT`). Add explicit index on referencing column—speeds up joins and prevents locking issues on parent deletes/updates. Use `DEFERRABLE INITIALLY DEFERRED` for circular FK dependencies checked at transaction end.\n- **UNIQUE**: creates a B-tree index; allows multiple NULLs unless `NULLS NOT DISTINCT` (PG15+). Standard behavior: `(1, NULL)` and `(1, NULL)` are allowed. With `NULLS NOT DISTINCT`: only one `(1, NULL)` allowed. Prefer `NULLS NOT DISTINCT` unless you specifically need duplicate NULLs.\n- **CHECK**: row-local constraints; NULL values pass the check (three-valued logic). Example: `CHECK (price > 0)` allows NULL prices. Combine with `NOT NULL` to enforce: `price NUMERIC NOT NULL CHECK (price > 0)`.\n- **EXCLUDE**: prevents overlapping values using operators. `EXCLUDE USING gist (room_id WITH =, booking_period WITH &&)` prevents double-booking rooms. Requires appropriate index type (often GiST).\n\n## Indexing\n\n- **B-tree**: default for equality/range queries (`=`, `<`, `>`, `BETWEEN`, `ORDER BY`)\n- **Composite**: order matters—index used if equality on leftmost prefix (`WHERE a = ? AND b > ?` uses index on `(a,b)`, but `WHERE b = ?` does not). Put most selective/frequently filtered columns first.\n- **Covering**: `CREATE INDEX ON tbl (id) INCLUDE (name, email)` - includes non-key columns for index-only scans without visiting table.\n- **Partial**: for hot subsets (`WHERE status = 'active'` → `CREATE INDEX ON tbl (user_id) WHERE status = 'active'`). Any query with `status = 'active'` can use this index.\n- **Expression**: for computed search keys (`CREATE INDEX ON tbl (LOWER(email))`). Expression must match exactly in WHERE clause: `WHERE LOWER(email) = 'user@example.com'`.\n- **GIN**: JSONB containment/existence, arrays (`@>`, `?`), full-text search (`@@`)\n- **GiST**: ranges, geometry, exclusion constraints\n- **BRIN**: very large, naturally ordered data (time-series)—minimal storage overhead. Effective when row order on disk correlates with indexed column (insertion order or after `CLUSTER`).\n\n## Partitioning\n\n- Use for very large tables (>100M rows) where queries consistently filter on partition key (often time/date).\n- Alternate use: use for tables where data maintenance tasks dictates e.g. data pruned or bulk replaced periodically\n- **RANGE**: common for time-series (`PARTITION BY RANGE (created_at)`). Create partitions: `CREATE TABLE logs_2024_01 PARTITION OF logs FOR VALUES FROM ('2024-01-01') TO ('2024-02-01')`. **TimescaleDB** automates time-based or ID-based partitioning with retention policies and compression.\n- **LIST**: for discrete values (`PARTITION BY LIST (region)`). Example: `FOR VALUES IN ('us-east', 'us-west')`.\n- **HASH**: for even distribution when no natural key (`PARTITION BY HASH (user_id)`). Creates N partitions with modulus.\n- **Constraint exclusion**: requires `CHECK` constraints on partitions for query planner to prune. Auto-created for declarative partitioning (PG10+).\n- Prefer declarative partitioning or hypertables. Do NOT use table inheritance.\n- **Limitations**: no global UNIQUE constraints—include partition key in PK/UNIQUE. FKs from partitioned tables not supported; use triggers.\n\n## Special Considerations\n\n### Update-Heavy Tables\n\n- **Separate hot/cold columns**—put frequently updated columns in separate table to minimize bloat.\n- **Use `fillfactor=90`** to leave space for HOT updates that avoid index maintenance.\n- **Avoid updating indexed columns**—prevents beneficial HOT updates.\n- **Partition by update patterns**—separate frequently updated rows in a different partition from stable data.\n\n### Insert-Heavy Workloads\n\n- **Minimize indexes**—only create what you query; every index slows inserts.\n- **Use `COPY` or multi-row `INSERT`** instead of single-row inserts.\n- **UNLOGGED tables** for rebuildable staging data—much faster writes.\n- **Defer index creation** for bulk loads—>drop index, load data, recreate indexes.\n- **Partition by time/hash** to distribute load. **TimescaleDB** automates partitioning and compression of insert-heavy data.\n- **Use a natural key for primary key** such as a (timestamp, device_id) if enforcing global uniqueness is important many insert-heavy tables don't need a primary key at all.\n- If you do need a surrogate key, **Prefer `BIGINT GENERATED ALWAYS AS IDENTITY` over `UUID`**.\n\n### Upsert-Friendly Design\n\n- **Requires UNIQUE index** on conflict target columns—`ON CONFLICT (col1, col2)` needs exact matching unique index (partial indexes don't work).\n- **Use `EXCLUDED.column`** to reference would-be-inserted values; only update columns that actually changed to reduce write overhead.\n- **`DO NOTHING` faster** than `DO UPDATE` when no actual update needed.\n\n### Safe Schema Evolution\n\n- **Transactional DDL**: most DDL operations can run in transactions and be rolled back—`BEGIN; ALTER TABLE...; ROLLBACK;` for safe testing.\n- **Concurrent index creation**: `CREATE INDEX CONCURRENTLY` avoids blocking writes but can't run in transactions.\n- **Volatile defaults cause rewrites**: adding `NOT NULL` columns with volatile defaults (e.g., `now()`, `gen_random_uuid()`) rewrites entire table. Non-volatile defaults are fast.\n- **Drop constraints before columns**: `ALTER TABLE DROP CONSTRAINT` then `DROP COLUMN` to avoid dependency issues.\n- **Function signature changes**: `CREATE OR REPLACE` with different arguments creates overloads, not replacements. DROP old version if no overload desired.\n\n## Generated Columns\n\n- `... GENERATED ALWAYS AS (<expr>) STORED` for computed, indexable fields. PG18+ adds `VIRTUAL` columns (computed on read, not stored).\n\n## Extensions\n\n- **`pgcrypto`**: `crypt()` for password hashing.\n- **`uuid-ossp`**: alternative UUID functions; prefer `pgcrypto` for new projects.\n- **`pg_trgm`**: fuzzy text search with `%` operator, `similarity()` function. Index with GIN for `LIKE '%pattern%'` acceleration.\n- **`citext`**: case-insensitive text type. Prefer expression indexes on `LOWER(col)` unless you need case-insensitive constraints.\n- **`btree_gin`/`btree_gist`**: enable mixed-type indexes (e.g., GIN index on both JSONB and text columns).\n- **`hstore`**: key-value pairs; mostly superseded by JSONB but useful for simple string mappings.\n- **`timescaledb`**: essential for time-series—automated partitioning, retention, compression, continuous aggregates.\n- **`postgis`**: comprehensive geospatial support beyond basic geometric types—essential for location-based applications.\n- **`pgvector`**: vector similarity search for embeddings.\n- **`pgaudit`**: audit logging for all database activity.\n\n## JSONB Guidance\n\n- Prefer `JSONB` with **GIN** index.\n- Default: `CREATE INDEX ON tbl USING GIN (jsonb_col);` → accelerates:\n  - **Containment** `jsonb_col @> '{\"k\":\"v\"}'`\n  - **Key existence** `jsonb_col ? 'k'`, **any/all keys** `?\\|`, `?&`\n  - **Path containment** on nested docs\n  - **Disjunction** `jsonb_col @> ANY(ARRAY['{\"status\":\"active\"}', '{\"status\":\"pending\"}'])`\n- Heavy `@>` workloads: consider opclass `jsonb_path_ops` for smaller/faster containment-only indexes:\n  - `CREATE INDEX ON tbl USING GIN (jsonb_col jsonb_path_ops);`\n  - **Trade-off**: loses support for key existence (`?`, `?|`, `?&`) queries—only supports containment (`@>`)\n- Equality/range on a specific scalar field: extract and index with B-tree (generated column or expression):\n  - `ALTER TABLE tbl ADD COLUMN price INT GENERATED ALWAYS AS ((jsonb_col->>'price')::INT) STORED;`\n  - `CREATE INDEX ON tbl (price);`\n  - Prefer queries like `WHERE price BETWEEN 100 AND 500` (uses B-tree) over `WHERE (jsonb_col->>'price')::INT BETWEEN 100 AND 500` without index.\n- Arrays inside JSONB: use GIN + `@>` for containment (e.g., tags). Consider `jsonb_path_ops` if only doing containment.\n- Keep core relations in tables; use JSONB for optional/variable attributes.\n- Use constraints to limit allowed JSONB values in a column e.g. `config JSONB NOT NULL CHECK(jsonb_typeof(config) = 'object')`\n\n## Examples\n\n### Users\n\n```sql\nCREATE TABLE users (\n  user_id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n  email TEXT NOT NULL UNIQUE,\n  name TEXT NOT NULL,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n);\nCREATE UNIQUE INDEX ON users (LOWER(email));\nCREATE INDEX ON users (created_at);\n```\n\n### Orders\n\n```sql\nCREATE TABLE orders (\n  order_id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n  user_id BIGINT NOT NULL REFERENCES users(user_id),\n  status TEXT NOT NULL DEFAULT 'PENDING' CHECK (status IN ('PENDING','PAID','CANCELED')),\n  total NUMERIC(10,2) NOT NULL CHECK (total > 0),\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n);\nCREATE INDEX ON orders (user_id);\nCREATE INDEX ON orders (created_at);\n```\n\n### JSONB\n\n```sql\nCREATE TABLE profiles (\n  user_id BIGINT PRIMARY KEY REFERENCES users(user_id),\n  attrs JSONB NOT NULL DEFAULT '{}',\n  theme TEXT GENERATED ALWAYS AS (attrs->>'theme') STORED\n);\nCREATE INDEX profiles_attrs_gin ON profiles USING GIN (attrs);\n```"
              },
              {
                "name": "find-hypertable-candidates",
                "description": "Analyze an existing PostgreSQL database to identify tables that would benefit from conversion to TimescaleDB hypertables",
                "path": "skills/find-hypertable-candidates/SKILL.md",
                "frontmatter": {
                  "name": "find-hypertable-candidates",
                  "description": "Analyze an existing PostgreSQL database to identify tables that would benefit from conversion to TimescaleDB hypertables"
                },
                "content": "# PostgreSQL Hypertable Candidate Analysis\n\nIdentify tables that would benefit from TimescaleDB hypertable conversion. After identification, use the companion \"migrate-postgres-tables-to-hypertables\" skill for configuration and migration.\n\n## TimescaleDB Benefits\n\n**Performance gains:** 90%+ compression, fast time-based queries, improved insert performance, efficient aggregations, continuous aggregates for materialization (dashboards, reports, analytics), automatic data management (retention, compression).\n\n**Best for insert-heavy patterns:**\n\n- Time-series data (sensors, metrics, monitoring)\n- Event logs (user events, audit trails, application logs)\n- Transaction records (orders, payments, financial)\n- Sequential data (auto-incrementing IDs with timestamps)\n- Append-only datasets (immutable records, historical)\n\n**Requirements:** Large volumes (1M+ rows), time-based queries, infrequent updates\n\n## Step 1: Database Schema Analysis\n\n### Option A: From Database Connection\n\n#### Table statistics and size\n\n```sql\n-- Get all tables with row counts and insert/update patterns\nWITH table_stats AS (\n    SELECT\n        schemaname, tablename,\n        n_tup_ins as total_inserts,\n        n_tup_upd as total_updates,\n        n_tup_del as total_deletes,\n        n_live_tup as live_rows,\n        n_dead_tup as dead_rows\n    FROM pg_stat_user_tables\n),\ntable_sizes AS (\n    SELECT\n        schemaname, tablename,\n        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,\n        pg_total_relation_size(schemaname||'.'||tablename) as total_size_bytes\n    FROM pg_tables\n    WHERE schemaname NOT IN ('information_schema', 'pg_catalog')\n)\nSELECT\n    ts.schemaname, ts.tablename, ts.live_rows,\n    tsize.total_size, tsize.total_size_bytes,\n    ts.total_inserts, ts.total_updates, ts.total_deletes,\n    ROUND(CASE WHEN ts.live_rows > 0\n          THEN (ts.total_inserts::float / ts.live_rows) * 100\n          ELSE 0 END, 2) as insert_ratio_pct\nFROM table_stats ts\nJOIN table_sizes tsize ON ts.schemaname = tsize.schemaname AND ts.tablename = tsize.tablename\nORDER BY tsize.total_size_bytes DESC;\n```\n\n**Look for:**\n\n- mostly insert-heavy patterns (less updates/deletes)\n- big tables (1M+ rows or 100MB+)\n\n#### Index patterns\n\n```sql\n-- Identify common query dimensions\nSELECT schemaname, tablename, indexname, indexdef\nFROM pg_indexes\nWHERE schemaname NOT IN ('information_schema', 'pg_catalog')\nORDER BY tablename, indexname;\n```\n\n**Look for:**\n\n- Multiple indexes with timestamp/created_at columns → time-based queries\n- Composite (entity_id, timestamp) indexes → good candidates\n- Time-only indexes → time range filtering common\n\n#### Query patterns (if pg_stat_statements available)\n\n```sql\n-- Check availability\nSELECT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements');\n\n-- Analyze expensive queries for candidate tables\nSELECT query, calls, mean_exec_time, total_exec_time\nFROM pg_stat_statements\nWHERE query ILIKE '%your_table_name%'\nORDER BY total_exec_time DESC LIMIT 20;\n```\n\n**✅ Good patterns:** Time-based WHERE, entity filtering combined with time-based qualifiers, GROUP BY time_bucket, range queries over time\n**❌ Poor patterns:** Non-time lookups with no time-based qualifiers in same query (WHERE email = ...)\n\n#### Constraints\n\n```sql\n-- Check migration compatibility\nSELECT conname, contype, pg_get_constraintdef(oid) as definition\nFROM pg_constraint\nWHERE conrelid = 'your_table_name'::regclass;\n```\n\n**Compatibility:**\n\n- Primary keys (p): Must include partition column or ask user if can be modified\n- Foreign keys (f): Plain→Hypertable and Hypertable→Plain OK, Hypertable→Hypertable NOT supported\n- Unique constraints (u): Must include partition column or ask user if can be modified\n- Check constraints (c): Usually OK\n\n### Option B: From Code Analysis\n\n#### ✅ GOOD Patterns\n\n```python\n# Append-only logging\nINSERT INTO events (user_id, event_time, data) VALUES (...);\n# Time-series collection\nINSERT INTO metrics (device_id, timestamp, value) VALUES (...);\n# Time-based queries\nSELECT * FROM metrics WHERE timestamp >= NOW() - INTERVAL '24 hours';\n# Time aggregations\nSELECT DATE_TRUNC('day', timestamp), COUNT(*) GROUP BY 1;\n```\n\n#### ❌ POOR Patterns\n\n```python\n# Frequent updates to historical records\nUPDATE users SET email = ..., updated_at = NOW() WHERE id = ...;\n# Non-time lookups\nSELECT * FROM users WHERE email = ...;\n# Small reference tables\nSELECT * FROM countries ORDER BY name;\n```\n\n#### Schema Indicators\n\n**✅ GOOD:**\n\n- Has timestamp/timestamptz column\n- Multiple indexes with timestamp-based columns\n- Composite (entity_id, timestamp) indexes\n\n**❌ POOR:**\n\n- Mostly indexes with non-time-based columns (on columns like email, name, status, etc.)\n- Columns that you expect to be updated over time (updated_at, updated_by, status, etc.)\n- Unique constraints on non-time fields\n- Frequent updated_at modifications\n- Small static tables\n\n#### Special Case: ID-Based Tables\n\nSequential ID tables can be candidates if:\n\n- Insert-mostly pattern / updates are either infrequent or only on recent records.\n- If updates do happen, they occur on recent records (such as an order status being updated orderered->processing->delivered. Note once an order is delivered, it is unlikely to be updated again.)\n- IDs correlate with time (as is the case for serial/auto-incrementing IDs/GENERATED ALWAYS AS IDENTITY)\n- ID is the primary query dimension\n- Recent data accessed more often (frequently the case in ecommerce, finance, etc.)\n- Time-based reporting common (e.g. monthly, daily summaries/analytics)\n\n```sql\nCREATE TABLE orders (\n    id BIGSERIAL PRIMARY KEY,           -- Can partition by ID\n    user_id BIGINT,\n    created_at TIMESTAMPTZ DEFAULT NOW() -- For sparse indexes\n);\n```\n\nNote: For ID-based tables where there is also a time column (created_at, ordered_at, etc.),\nyou can partition by ID and use sparse indexes on the time column.\nSee the `migrate-postgres-tables-to-hypertables` skill for details.\n\n## Step 2: Candidacy Scoring (8+ points = good candidate)\n\n### Time-Series Characteristics (5+ points needed)\n\n- Has timestamp/timestamptz column: **3 points**\n- Data inserted chronologically: **2 points**\n- Queries filter by time: **2 points**\n- Time aggregations common: **2 points**\n\n### Scale & Performance (3+ points recommended)\n\n- Large table (1M+ rows or 100MB+): **2 points**\n- High insert volume: **1 point**\n- Infrequent updates to historical: **1 point**\n- Range queries common: **1 point**\n- Aggregation queries: **2 points**\n\n### Data Patterns (bonus)\n\n- Contains entity ID for segmentation (device_id, user_id, product_id, symbol, etc.): **1 point**\n- Numeric measurements: **1 point**\n- Log/event structure: **1 point**\n\n## Common Patterns\n\n### ✅ GOOD Candidates\n\n**✅ Event/Log Tables** (user_events, audit_logs)\n\n```sql\nCREATE TABLE user_events (\n    id BIGSERIAL PRIMARY KEY,\n    user_id BIGINT,\n    event_type TEXT,\n    event_time TIMESTAMPTZ DEFAULT NOW(),\n    metadata JSONB\n);\n-- Partition by id, segment by user_id, enable minmax sparse_index on event_time\n```\n\n**✅ Sensor/IoT Data** (sensor_readings, telemetry)\n\n```sql\nCREATE TABLE sensor_readings (\n    device_id TEXT,\n    timestamp TIMESTAMPTZ,\n    temperature DOUBLE PRECISION,\n    humidity DOUBLE PRECISION\n);\n-- Partition by timestamp, segment by device_id, minmax sparse indexes on temperature and humidity\n```\n\n**✅ Financial/Trading** (stock_prices, transactions)\n\n```sql\nCREATE TABLE stock_prices (\n    symbol VARCHAR(10),\n    price_time TIMESTAMPTZ,\n    open_price DECIMAL,\n    close_price DECIMAL,\n    volume BIGINT\n);\n-- Partition by price_time, segment by symbol, minmax sparse indexes on open_price and close_price and volume\n```\n\n**✅ System Metrics** (monitoring_data)\n\n```sql\nCREATE TABLE system_metrics (\n    hostname TEXT,\n    metric_time TIMESTAMPTZ,\n    cpu_usage DOUBLE PRECISION,\n    memory_usage BIGINT\n);\n-- Partition by metric_time, segment by hostname, minmax sparse indexes on cpu_usage and memory_usage\n```\n\n### ❌ POOR Candidates\n\n**❌ Reference Tables** (countries, categories)\n\n```sql\nCREATE TABLE countries (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100),\n    code CHAR(2)\n);\n-- Static data, no time component\n```\n\n**❌ User Profiles** (users, accounts)\n\n```sql\nCREATE TABLE users (\n    id BIGSERIAL PRIMARY KEY,\n    email VARCHAR(255),\n    created_at TIMESTAMPTZ,\n    updated_at TIMESTAMPTZ\n);\n-- Accessed by ID, frequently updated, has timestamp but it's not the primary query dimension (the primary query dimension is id or email)\n```\n\n**❌ Settings/Config** (user_settings)\n\n```sql\nCREATE TABLE user_settings (\n    user_id BIGINT PRIMARY KEY,\n    theme VARCHAR(20),       -- Changes: light -> dark -> auto\n    language VARCHAR(10),    -- Changes: en -> es -> fr\n    notifications JSONB,     -- Frequent preference updates\n    updated_at TIMESTAMPTZ\n);\n-- Accessed by user_id, frequently updated, has timestamp but it's not the primary query dimension (the primary query dimension is user_id)\n```\n\n## Analysis Output Requirements\n\nFor each candidate table provide:\n\n- **Score:** Based on criteria (8+ = strong candidate)\n- **Pattern:** Insert vs update ratio\n- **Access:** Time-based vs entity lookups\n- **Size:** Current size and growth rate\n- **Queries:** Time-range, aggregations, point lookups\n\nFocus on insert-heavy patterns with time-based or sequential access. Tables scoring 8+ points are strong candidates for conversion."
              },
              {
                "name": "migrate-postgres-tables-to-hypertables",
                "description": "Comprehensive guide for migrating PostgreSQL tables to TimescaleDB hypertables with optimal configuration and performance validation",
                "path": "skills/migrate-postgres-tables-to-hypertables/SKILL.md",
                "frontmatter": {
                  "name": "migrate-postgres-tables-to-hypertables",
                  "description": "Comprehensive guide for migrating PostgreSQL tables to TimescaleDB hypertables with optimal configuration and performance validation"
                },
                "content": "# PostgreSQL to TimescaleDB Hypertable Migration\n\nMigrate identified PostgreSQL tables to TimescaleDB hypertables with optimal configuration, migration planning and validation.\n\n**Prerequisites**: Tables already identified as hypertable candidates (use companion \"find-hypertable-candidates\" skill if needed).\n\n## Step 1: Optimal Configuration\n\n### Partition Column Selection\n\n```sql\n-- Find potential partition columns\nSELECT column_name, data_type, is_nullable\nFROM information_schema.columns\nWHERE table_name = 'your_table_name'\n  AND data_type IN ('timestamp', 'timestamptz', 'bigint', 'integer', 'date')\nORDER BY ordinal_position;\n```\n\n**Requirements:** Time-based (TIMESTAMP/TIMESTAMPTZ/DATE) or sequential integer (INT/BIGINT)\n\nShould represent when the event actually occurred or sequential ordering.\n\n**Common choices:**\n\n- `timestamp`, `created_at`, `event_time` - when event occurred\n- `id`, `sequence_number` - auto-increment (for sequential data without timestamps)\n- `ingested_at` - less ideal, only if primary query dimension\n- `updated_at` - AVOID (records updated out of order, breaks chunk distribution) unless primary query dimension\n\n#### Special Case: table with BOTH ID AND Timestamp\n\nWhen table has sequential ID (PK) AND timestamp that correlate:\n\n```sql\n-- Partition by ID, enable minmax sparse indexes on timestamp\nSELECT create_hypertable('orders', 'id', chunk_time_interval => 1000000);\nALTER TABLE orders SET (\n    timescaledb.sparse_index = 'minmax(created_at),...'\n);\n```\n\nSparse indexes on time column enable skipping compressed blocks outside queried time ranges.\n\nUse when: ID correlates with time (newer records have higher IDs), need ID-based lookups, time queries also common\n\n### Chunk Interval Selection\n\n```sql\n-- Ensure statistics are current\nANALYZE your_table_name;\n\n-- Estimate index size per time unit\nWITH time_range AS (\n    SELECT\n        MIN(timestamp_column) as min_time,\n        MAX(timestamp_column) as max_time,\n        EXTRACT(EPOCH FROM (MAX(timestamp_column) - MIN(timestamp_column)))/3600 as total_hours\n    FROM your_table_name\n),\ntotal_index_size AS (\n    SELECT SUM(pg_relation_size(indexname::regclass)) as total_index_bytes\n    FROM pg_stat_user_indexes\n    WHERE schemaname||'.'||tablename = 'your_schema.your_table_name'\n)\nSELECT\n    pg_size_pretty(tis.total_index_bytes / tr.total_hours) as index_size_per_hour\nFROM time_range tr, total_index_size tis;\n```\n\n**Target:** Indexes of recent chunks < 25% of RAM\n**Default:** IMPORTANT: Keep default of 7 days if unsure\n**Range:** 1 hour minimum, 30 days maximum\n\n**Example:** 32GB RAM → target 8GB for recent indexes. If index_size_per_hour = 200MB:\n\n- 1 hour chunks: 200MB chunk index size × 40 recent = 8GB ✓\n- 6 hour chunks: 1.2GB chunk index size × 7 recent = 8.4GB ✓\n- 1 day chunks: 4.8GB chunk index size × 2 recent = 9.6GB ⚠️\n  Choose largest interval keeping 2+ recent chunk indexes under target.\n\n### Primary Key/ Unique Constraints Compatibility\n\n```sql\n-- Check existing primary key/ unique constraints\nSELECT conname, pg_get_constraintdef(oid) as definition\nFROM pg_constraint\nWHERE conrelid = 'your_table_name'::regclass AND contype = 'p' OR contype = 'u';\n```\n\n**Rules:** PK/UNIQUE must include partition column\n\n**Actions:**\n\n1. **No PK/UNIQUE:** No changes needed\n2. **PK/UNIQUE includes partition column:** No changes needed\n3. **PK/UNIQUE excludes partition column:** ⚠️ **ASK USER PERMISSION** to modify PK/UNIQUE\n\n**Example: user prompt if needed:**\n\n> \"Primary key (id) doesn't include partition column (timestamp). Must modify to PRIMARY KEY (id, timestamp) to convert to hypertable. This may break application code. Is this acceptable?\"\n> \"Unique constraint (id) doesn't include partition column (timestamp). Must modify to UNIQUE (id, timestamp) to convert to hypertable. This may break application code. Is this acceptable?\"\n\nIf the user accepts, modify the constraint:\n\n```sql\nBEGIN;\nALTER TABLE your_table_name DROP CONSTRAINT existing_pk_name;\nALTER TABLE your_table_name ADD PRIMARY KEY (existing_columns, partition_column);\nCOMMIT;\n```\n\nIf the user does not accept, you should NOT migrate the table.\n\nIMPORTANT: DO NOT modify the primary key/unique constraint without user permission.\n\n### Compression Configuration\n\nFor detailed segment_by and order_by selection, see \"setup-timescaledb-hypertables\" skill. Quick reference:\n\n**segment_by:** Most common WHERE filter with >100 rows per value per chunk\n\n- IoT: `device_id`\n- Finance: `symbol`\n- Analytics: `user_id` or `session_id`\n\n```sql\n-- Analyze cardinality for segment_by selection\nSELECT column_name, COUNT(DISTINCT column_name) as unique_values,\n       ROUND(COUNT(*)::float / COUNT(DISTINCT column_name), 2) as avg_rows_per_value\nFROM your_table_name GROUP BY column_name;\n```\n\n**order_by:** Usually `timestamp DESC`. The (segment_by, order_by) combination should form a natural time-series progression.\n\n- If column has <100 rows/chunk (too low for segment_by), prepend to order_by: `order_by='low_density_col, timestamp DESC'`\n\n**sparse indexes:** add minmax on the columns that are used in the WHERE clauses but are not in the segment_by or order_by. Use minmax for columns used in range queries.\n\n```sql\nALTER TABLE your_table_name SET (\n    timescaledb.enable_columnstore,\n    timescaledb.segmentby = 'entity_id',\n    timescaledb.orderby = 'timestamp DESC'\n    timescaledb.sparse_index = 'minmax(value_1),...'\n);\n\n-- Compress after data unlikely to change (adjust `after` parameter based on update patterns)\nCALL add_columnstore_policy('your_table_name', after => INTERVAL '7 days');\n```\n\n## Step 2: Migration Planning\n\n### Pre-Migration Checklist\n\n- [ ] Partition column selected\n- [ ] Chunk interval calculated (or using default)\n- [ ] PK includes partition column OR user approved modification\n- [ ] No Hypertable→Hypertable foreign keys\n- [ ] Unique constraints include partition column\n- [ ] Created compression configuration (segment_by, order_by, sparse indexes, compression policy)\n- [ ] Maintenance window scheduled / backup created.\n\n### Migration Options\n\n#### Option 1: In-Place (Tables < 1GB)\n\n```sql\n-- Enable extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Convert to hypertable (locks table)\nSELECT create_hypertable(\n    'your_table_name',\n    'timestamp_column',\n    chunk_time_interval => INTERVAL '7 days',\n    if_not_exists => TRUE\n);\n\n-- Configure compression\nALTER TABLE your_table_name SET (\n    timescaledb.enable_columnstore,\n    timescaledb.segmentby = 'entity_id',\n    timescaledb.orderby = 'timestamp DESC',\n    timescaledb.sparse_index = 'minmax(value_1),...'\n);\n\n-- Adjust `after` parameter based on update patterns\nCALL add_columnstore_policy('your_table_name', after => INTERVAL '7 days');\n```\n\n#### Option 2: Blue-Green (Tables > 1GB)\n\n```sql\n-- 1. Create new hypertable\nCREATE TABLE your_table_name_new (LIKE your_table_name INCLUDING ALL);\n\n-- 2. Convert to hypertable\nSELECT create_hypertable('your_table_name_new', 'timestamp_column');\n\n-- 3. Configure compression\nALTER TABLE your_table_name_new SET (\n    timescaledb.enable_columnstore,\n    timescaledb.segmentby = 'entity_id',\n    timescaledb.orderby = 'timestamp DESC'\n);\n\n-- 4. Migrate data in batches\nINSERT INTO your_table_name_new\nSELECT * FROM your_table_name\nWHERE timestamp_column >= '2024-01-01' AND timestamp_column < '2024-02-01';\n-- Repeat for each time range\n\n-- 4. Enter maintenance window and do the following:\n\n-- 5. Pause modification of the old table.\n\n-- 6. Copy over the most recent data from the old table to the new table.\n\n-- 7. Swap tables\nBEGIN;\nALTER TABLE your_table_name RENAME TO your_table_name_old;\nALTER TABLE your_table_name_new RENAME TO your_table_name;\nCOMMIT;\n\n-- 8. Exit maintenance window.\n\n-- 9. (sometime much later) Drop old table after validation\n-- DROP TABLE your_table_name_old;\n```\n\n### Common Issues\n\n#### Foreign Keys\n\n```sql\n-- Check foreign keys\nSELECT conname, confrelid::regclass as referenced_table\nFROM pg_constraint\nWHERE (conrelid = 'your_table_name'::regclass\n    OR confrelid = 'your_table_name'::regclass)\n  AND contype = 'f';\n```\n\n**Supported:** Plain→Hypertable, Hypertable→Plain\n**NOT supported:** Hypertable→Hypertable\n\n⚠️ **CRITICAL:** Hypertable→Hypertable FKs must be dropped (enforce in application). **ASK USER PERMISSION**. If no, **STOP MIGRATION**.\n\n#### Large Table Migration Time\n\n```sql\n-- Rough estimate: ~75k rows/second\nSELECT\n    pg_size_pretty(pg_total_relation_size(tablename)) as size,\n    n_live_tup as rows,\n    ROUND(n_live_tup / 75000.0 / 60, 1) as estimated_minutes\nFROM pg_stat_user_tables\nWHERE tablename = 'your_table_name';\n```\n\n**Solutions for large tables (>1GB/10M rows):** Use blue-green migration, migrate during off-peak, test on subset first\n\n## Step 3: Performance Validation\n\n### Chunk & Compression Analysis\n\n```sql\n-- View chunks and compression\nSELECT\n    chunk_name,\n    pg_size_pretty(total_bytes) as size,\n    pg_size_pretty(compressed_total_bytes) as compressed_size,\n    ROUND((total_bytes - compressed_total_bytes::numeric) / total_bytes * 100, 1) as compression_pct,\n    range_start,\n    range_end\nFROM timescaledb_information.chunks\nWHERE hypertable_name = 'your_table_name'\nORDER BY range_start DESC;\n```\n\n**Look for:**\n\n- Consistent chunk sizes (within 2x)\n- Compression >90% for time-series\n- Recent chunks uncompressed\n- Chunk indexes < 25% RAM\n\n### Query Performance Tests\n\n```sql\n-- 1. Time-range query (should show chunk exclusion)\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT COUNT(*), AVG(value)\nFROM your_table_name\nWHERE timestamp >= NOW() - INTERVAL '1 day';\n\n-- 2. Entity + time query (benefits from segment_by)\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT * FROM your_table_name\nWHERE entity_id = 'X' AND timestamp >= NOW() - INTERVAL '1 week';\n\n-- 3. Aggregation (benefits from columnstore)\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT DATE_TRUNC('hour', timestamp), entity_id, COUNT(*), AVG(value)\nFROM your_table_name\nWHERE timestamp >= NOW() - INTERVAL '1 month'\nGROUP BY 1, 2;\n```\n\n**✅ Good signs:**\n\n- \"Chunks excluded during startup: X\" in EXPLAIN plan\n- \"Custom Scan (ColumnarScan)\" for compressed data\n- Lower \"Buffers: shared read\" in EXPLAIN ANALYZE plan than pre-migration\n- Faster execution times\n\n**❌ Bad signs:**\n\n- \"Seq Scan\" on large chunks\n- No chunk exclusion messages\n- Slower than before migration\n\n### Storage Metrics\n\n```sql\n-- Monitor compression effectiveness\nSELECT\n    hypertable_name,\n    pg_size_pretty(total_bytes) as total_size,\n    pg_size_pretty(compressed_total_bytes) as compressed_size,\n    ROUND(compressed_total_bytes::numeric / total_bytes * 100, 1) as compressed_pct_of_total,\n    ROUND((uncompressed_total_bytes - compressed_total_bytes::numeric) /\n          uncompressed_total_bytes * 100, 1) as compression_ratio_pct\nFROM timescaledb_information.hypertables\nWHERE hypertable_name = 'your_table_name';\n```\n\n**Monitor:**\n\n- compression_ratio_pct >90% (typical time-series)\n- compressed_pct_of_total growing as data ages\n- Size growth slowing significantly vs pre-hypertable\n- Decreasing compression_ratio_pct = poor segment_by\n\n### Troubleshooting\n\n#### Poor Chunk Exclusion\n\n```sql\n-- Verify chunks are being excluded\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT * FROM your_table_name\nWHERE timestamp >= '2024-01-01' AND timestamp < '2024-01-02';\n-- Look for \"Chunks excluded during startup: X\"\n```\n\n#### Poor Compression\n\n```sql\n-- Get newest compressed chunk name\nSELECT chunk_name FROM timescaledb_information.chunks\nWHERE hypertable_name = 'your_table_name'\n  AND compressed_total_bytes IS NOT NULL\nORDER BY range_start DESC LIMIT 1;\n\n-- Analyze segment distribution\nSELECT segment_by_column, COUNT(*) as rows_per_segment\nFROM _timescaledb_internal._hyper_X_Y_chunk  -- Use actual chunk name\nGROUP BY 1 ORDER BY 2 DESC;\n```\n\n**Look for:** <20 rows per segment: Poor segment_by choice (should be >100) => Low compression potential.\n\n#### Poor insert performance\n\nCheck that you don't have too many indexes. Unused indexes hurt insert performance and should be dropped.\n\n```sql\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_tup_read,\n    idx_tup_fetch,\n    idx_scan\nFROM pg_stat_user_indexes\nWHERE tablename LIKE '%your_table_name%'\nORDER BY idx_scan DESC;\n```\n\n**Look for:** Unused indexes via a low idx_scan value. Drop such indexes (but ask user permission).\n\n### Ongoing Monitoring\n\n```sql\n-- Monitor chunk compression status\nCREATE OR REPLACE VIEW hypertable_compression_status AS\nSELECT\n    h.hypertable_name,\n    COUNT(c.chunk_name) as total_chunks,\n    COUNT(c.chunk_name) FILTER (WHERE c.compressed_total_bytes IS NOT NULL) as compressed_chunks,\n    ROUND(\n        COUNT(c.chunk_name) FILTER (WHERE c.compressed_total_bytes IS NOT NULL)::numeric /\n        COUNT(c.chunk_name) * 100, 1\n    ) as compression_coverage_pct,\n    pg_size_pretty(SUM(c.total_bytes)) as total_size,\n    pg_size_pretty(SUM(c.compressed_total_bytes)) as compressed_size\nFROM timescaledb_information.hypertables h\nLEFT JOIN timescaledb_information.chunks c ON h.hypertable_name = c.hypertable_name\nGROUP BY h.hypertable_name;\n\n-- Query this view regularly to monitor compression progress\nSELECT * FROM hypertable_compression_status\nWHERE hypertable_name = 'your_table_name';\n```\n\n**Look for:**\n\n- compression_coverage_pct should increase over time as data ages and gets compressed.\n- total_chunks should not grow too quickly (more than 10000 becomes a problem).\n- You should not see unexpected spikes in total_size or compressed_size.\n\n## Success Criteria\n\n**✅ Migration successful when:**\n\n- All queries return correct results\n- Query performance equal or better\n- Compression >90% for older data\n- Chunk exclusion working for time queries\n- Insert performance acceptable\n\n**❌ Investigate if:**\n\n- Query performance >20% worse\n- Compression <80%\n- No chunk exclusion\n- Insert performance degraded\n- Increased error rates\n\nFocus on high-volume, insert-heavy workloads with time-based access patterns for best ROI."
              },
              {
                "name": "setup-timescaledb-hypertables",
                "description": "Step-by-step instructions for designing table schemas and setting up TimescaleDB with hypertables, indexes, compression, retention policies, and continuous aggregates. Instructions for selecting: partition columns, segment_by columns, order_by columns, chunk time interval, real-time aggregation.",
                "path": "skills/setup-timescaledb-hypertables/SKILL.md",
                "frontmatter": {
                  "name": "setup-timescaledb-hypertables",
                  "description": "Step-by-step instructions for designing table schemas and setting up TimescaleDB with hypertables, indexes, compression, retention policies, and continuous aggregates. Instructions for selecting: partition columns, segment_by columns, order_by columns, chunk time interval, real-time aggregation."
                },
                "content": "# TimescaleDB Complete Setup\n\nInstructions for insert-heavy data patterns where data is inserted but rarely changed:\n\n- **Time-series data** (sensors, metrics, system monitoring)\n- **Event logs** (user events, audit trails, application logs)\n- **Transaction records** (orders, payments, financial transactions)\n- **Sequential data** (records with auto-incrementing IDs and timestamps)\n- **Append-only datasets** (immutable records, historical data)\n\n## Step 1: Create Hypertable\n\n```sql\nCREATE TABLE your_table_name (\n    timestamp TIMESTAMPTZ NOT NULL,\n    entity_id TEXT NOT NULL,          -- device_id, user_id, symbol, etc.\n    category TEXT,                    -- sensor_type, event_type, asset_class, etc.\n    value_1 DOUBLE PRECISION,         -- price, temperature, latency, etc.\n    value_2 DOUBLE PRECISION,         -- volume, humidity, throughput, etc.\n    value_3 INTEGER,                  -- count, status, level, etc.\n    metadata JSONB                    -- flexible additional data\n) WITH (\n    tsdb.hypertable,\n    tsdb.partition_column='timestamp',\n    tsdb.enable_columnstore=true,     -- Disable if table has vector columns\n    tsdb.segmentby='entity_id',       -- See selection guide below\n    tsdb.orderby='timestamp DESC',     -- See selection guide below\n    tsdb.sparse_index='minmax(value_1),minmax(value_2),minmax(value_3)' -- see selection guide below\n);\n```\n\n### Compression Decision\n\n- **Enable by default** for insert-heavy patterns\n- **Disable** if table has vector type columns (pgvector) - indexes on vector columns incompatible with columnstore\n\n### Partition Column Selection\n\nMust be time-based (TIMESTAMP/TIMESTAMPTZ/DATE) or integer (INT/BIGINT) with good temporal/sequential distribution.\n\n**Common patterns:**\n\n- TIME-SERIES: `timestamp`, `event_time`, `measured_at`\n- EVENT LOGS: `event_time`, `created_at`, `logged_at`\n- TRANSACTIONS: `created_at`, `transaction_time`, `processed_at`\n- SEQUENTIAL: `id` (auto-increment when no timestamp), `sequence_number`\n- APPEND-ONLY: `created_at`, `inserted_at`, `id`\n\n**Less ideal:** `ingested_at` (when data entered system - use only if it's your primary query dimension)\n**Avoid:** `updated_at` (breaks time ordering unless it's primary query dimension)\n\n### Segment_By Column Selection\n\n**PREFER SINGLE COLUMN** - multi-column rarely optimal. Multi-column can only work for highly correlated columns (e.g., metric_name + metric_type) with sufficient row density.\n\n**Requirements:**\n\n- Frequently used in WHERE clauses (most common filter)\n- Good row density (>100 rows per value per chunk)\n- Primary logical partition/grouping\n\n**Examples:**\n\n- IoT: `device_id`\n- Finance: `symbol`\n- Metrics: `service_name`, `service_name, metric_type` (if sufficient row density), `metric_name, metric_type` (if sufficient row density)\n- Analytics: `user_id` if sufficient row density, otherwise `session_id`\n- E-commerce: `product_id` if sufficient row density, otherwise `category_id`\n\n**Row density guidelines:**\n\n- Target: >100 rows per segment_by value within each chunk.\n- Poor: <10 rows per segment_by value per chunk → choose less granular column\n- What to do with low-density columns: prepend to order_by column list.\n\n**Query pattern drives choice:**\n\n```sql\nSELECT * FROM table WHERE entity_id = 'X' AND timestamp > ...\n-- ↳ segment_by: entity_id (if >100 rows per chunk)\n```\n\n**Avoid:** timestamps, unique IDs, low-density columns (<100 rows/value/chunk), columns rarely used in filtering\n\n### Order_By Column Selection\n\nCreates natural time-series progression when combined with segment_by for optimal compression.\n\n**Most common:** `timestamp DESC`\n\n**Examples:**\n\n- IoT/Finance/E-commerce: `timestamp DESC`\n- Metrics: `metric_name, timestamp DESC` (if metric_name has too low density for segment_by)\n- Analytics: `user_id, timestamp DESC` (user_id has too low density for segment_by)\n\n**Alternative patterns:**\n\n- `sequence_id DESC` for event streams with sequence numbers\n- `timestamp DESC, event_order DESC` for sub-ordering within same timestamp\n\n**Low-density column handling:**\nIf a column has <100 rows per chunk (too low for segment_by), prepend it to order_by:\n\n- Example: `metric_name` has 20 rows/chunk → use `segment_by='service_name'`, `order_by='metric_name, timestamp DESC'`\n- Groups similar values together (all temperature readings, then pressure readings) for better compression\n\n**Good test:** ordering created by `(segment_by_column, order_by_column)` should form a natural time-series progression. Values close to each other in the progression should be similar.\n\n**Avoid in order_by:** random columns, columns with high variance between adjacent rows, columns unrelated to segment_by\n\n### Compression Sparse Index Selection\n\n**Sparse indexes** enable query filtering on compressed data without decompression. Store metadata per batch (~1000 rows) to eliminate batches that don't match query predicates.\n\n**Types:**\n\n- **minmax:** Min/max values per batch - for range queries (>, <, BETWEEN) on numeric/temporal columns\n\n**Use minmax for:** price, temperature, measurement, timestamp (range filtering)\n\n**Use for:**\n\n- minmax for outlier detection (temperature > 90).\n- minmax for fields that are highly correlated with segmentby and orderby columns (e.g. if orderby includes `created_at`, minmax on `updated_at` is useful).\n\n**Avoid:** rarely filtered columns.\n\nIMPORTANT: NEVER index columns in segmentby or orderby. Orderby columns will always have minmax indexes without any configuration.\n\n**Configuration:**\nThe format is a comma-separated list of type_of_index(column_name).\n\n```sql\nALTER TABLE table_name SET (\n    timescaledb.sparse_index = 'minmax(value_1),minmax(value_2)'\n);\n```\n\nExplicit configuration available since v2.22.0 (was auto-created since v2.16.0).\n\n### Chunk Time Interval (Optional)\n\nDefault: 7 days (use if volume unknown, or ask user). Adjust based on volume:\n\n- High frequency: 1 hour - 1 day\n- Medium: 1 day - 1 week\n- Low: 1 week - 1 month\n\n```sql\nSELECT set_chunk_time_interval('your_table_name', INTERVAL '1 day');\n```\n\n**Good test:** recent chunk indexes should fit in less than 25% of RAM.\n\n### Indexes & Primary Keys\n\nCommon index patterns - composite indexes on an id and timestamp:\n\n```sql\nCREATE INDEX idx_entity_timestamp ON your_table_name (entity_id, timestamp DESC);\n```\n\n**Important:** Only create indexes you'll actually use - each has maintenance overhead.\n\n**Primary key and unique constraints rules:** Must include partition column.\n\n**Option 1: Composite PK with partition column**\n\n```sql\nALTER TABLE your_table_name ADD PRIMARY KEY (entity_id, timestamp);\n```\n\n**Option 2: Single-column PK (only if it's the partition column)**\n\n```sql\nCREATE TABLE ... (id BIGINT PRIMARY KEY, ...) WITH (tsdb.partition_column='id');\n```\n\n**Option 3: No PK**: strict uniqueness is often not required for insert-heavy patterns.\n\n## Step 2: Compression Policy\n\nSet `after` interval for when: data becomes mostly immutable (some updates/backfill OK) AND B-tree indexes aren't needed for queries (less common criterion).\n\n```sql\n-- Adjust 'after' based on update patterns\nCALL add_columnstore_policy('your_table_name', after => INTERVAL '1 day');\n```\n\n## Step 3: Retention Policy\n\nIMPORTANT: Don't guess - ask user or comment out if unknown.\n\n```sql\n-- Example - replace with requirements or comment out\nSELECT add_retention_policy('your_table_name', INTERVAL '365 days');\n```\n\n## Step 4: Create Continuous Aggregates\n\nUse different aggregation intervals for different uses.\n\n### Short-term (Minutes/Hours)\n\nFor up-to-the-minute dashboards on high-frequency data.\n\n```sql\nCREATE MATERIALIZED VIEW your_table_hourly\nWITH (timescaledb.continuous) AS\nSELECT\n    time_bucket(INTERVAL '1 hour', timestamp) AS bucket,\n    entity_id,\n    category,\n    COUNT(*) as record_count,\n    AVG(value_1) as avg_value_1,\n    MIN(value_1) as min_value_1,\n    MAX(value_1) as max_value_1,\n    SUM(value_2) as sum_value_2\nFROM your_table_name\nGROUP BY bucket, entity_id, category;\n```\n\n### Long-term (Days/Weeks/Months)\n\nFor long-term reporting and analytics.\n\n```sql\nCREATE MATERIALIZED VIEW your_table_daily\nWITH (timescaledb.continuous) AS\nSELECT\n    time_bucket(INTERVAL '1 day', timestamp) AS bucket,\n    entity_id,\n    category,\n    COUNT(*) as record_count,\n    AVG(value_1) as avg_value_1,\n    MIN(value_1) as min_value_1,\n    MAX(value_1) as max_value_1,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY value_1) as median_value_1,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY value_1) as p95_value_1,\n    SUM(value_2) as sum_value_2\nFROM your_table_name\nGROUP BY bucket, entity_id, category;\n```\n\n## Step 5: Aggregate Refresh Policies\n\nSet up refresh policies based on your data freshness requirements.\n\n**start_offset:** Usually omit (refreshes all). Exception: If you don't care about refreshing data older than X (see below). With retention policy on raw data: match the retention policy.\n\n**end_offset:** Set beyond active update window (e.g., 15 min if data usually arrives within 10 min). Data newer than end_offset won't appear in queries without real-time aggregation. If you don't know your update window, use the size of the time_bucket in the query, but not less than 5 minutes.\n\n**schedule_interval:** Set to the same value as the end_offset but not more than 1 hour.\n\n**Hourly - frequent refresh for dashboards:**\n\n```sql\nSELECT add_continuous_aggregate_policy('your_table_hourly',\n    end_offset => INTERVAL '15 minutes',\n    schedule_interval => INTERVAL '15 minutes');\n```\n\n**Daily - less frequent for reports:**\n\n```sql\nSELECT add_continuous_aggregate_policy('your_table_daily',\n    end_offset => INTERVAL '1 hour',\n    schedule_interval => INTERVAL '1 hour');\n```\n\n**Use start_offset only if you don't care about refreshing old data**\nUse for high-volume systems where query accuracy on older data doesn't matter:\n\n```sql\n-- the following aggregate can be stale for data older than 7 days\n-- SELECT add_continuous_aggregate_policy('aggregate_for_last_7_days',\n--     start_offset => INTERVAL '7 days',    -- only refresh last 7 days\n--     end_offset => INTERVAL '15 minutes',\n--     schedule_interval => INTERVAL '15 minutes');\n```\n\nIMPORTANT: you MUST set a start_offset to be less than the retention policy on raw data. By default, set the start_offset equal to the retention policy.\nIf the retention policy is commented out, comment out the start_offset as well. like this:\n\n```sql\nSELECT add_continuous_aggregate_policy('your_table_daily',\n--  start_offset => INTERVAL '<retention period here>',    -- uncomment if retention policy is enabled on the raw data table\n    end_offset => INTERVAL '1 hour',\n    schedule_interval => INTERVAL '1 hour');\n```\n\n## Step 6: Real-Time Aggregation (Optional)\n\nReal-time combines materialized + recent raw data at query time. Provides up-to-date results at the cost of higher query latency.\n\nMore useful for fine-grained aggregates (e.g., minutely) than coarse ones (e.g., daily/monthly) since large buckets will be mostly incomplete with recent data anyway.\n\nDisabled by default in v2.13+, before that it was enabled by default.\n\n**Use when:** Need data newer than end_offset, up-to-minute dashboards, can tolerate higher query latency\n**Disable when:** Performance critical, refresh policies sufficient, high query volume, missing and stale data for recent data is acceptable\n\n**Enable for current results (higher query cost):**\n\n```sql\nALTER MATERIALIZED VIEW your_table_hourly SET (timescaledb.materialized_only = false);\n```\n\n**Disable for performance (but with stale results):**\n\n```sql\nALTER MATERIALIZED VIEW your_table_hourly SET (timescaledb.materialized_only = true);\n```\n\n## Step 7: Compress Aggregates\n\nRule: segment_by = ALL GROUP BY columns except time_bucket, order_by = time_bucket DESC\n\n```sql\n-- Hourly\nALTER MATERIALIZED VIEW your_table_hourly SET (\n    timescaledb.enable_columnstore,\n    timescaledb.segmentby = 'entity_id, category',\n    timescaledb.orderby = 'bucket DESC'\n);\nCALL add_columnstore_policy('your_table_hourly', after => INTERVAL '3 days');\n\n-- Daily\nALTER MATERIALIZED VIEW your_table_daily SET (\n    timescaledb.enable_columnstore,\n    timescaledb.segmentby = 'entity_id, category',\n    timescaledb.orderby = 'bucket DESC'\n);\nCALL add_columnstore_policy('your_table_daily', after => INTERVAL '7 days');\n```\n\n## Step 8: Aggregate Retention\n\nAggregates are typically kept longer than raw data.\nIMPORTANT: Don't guess - ask user or you **MUST comment out if unknown**.\n\n```sql\n-- Example - replace or comment out\nSELECT add_retention_policy('your_table_hourly', INTERVAL '2 years');\nSELECT add_retention_policy('your_table_daily', INTERVAL '5 years');\n```\n\n## Step 9: Performance Indexes on Continuous Aggregates\n\n**Index strategy:** Analyze WHERE clauses in common queries → Create indexes matching filter columns + time ordering\n\n**Pattern:** `(filter_column, bucket DESC)` supports `WHERE filter_column = X AND bucket >= Y ORDER BY bucket DESC`\n\nExamples:\n\n```sql\nCREATE INDEX idx_hourly_entity_bucket ON your_table_hourly (entity_id, bucket DESC);\nCREATE INDEX idx_hourly_category_bucket ON your_table_hourly (category, bucket DESC);\n```\n\n**Multi-column filters:** Create composite indexes for `WHERE entity_id = X AND category = Y`:\n\n```sql\nCREATE INDEX idx_hourly_entity_category_bucket ON your_table_hourly (entity_id, category, bucket DESC);\n```\n\n**Important:** Only create indexes you'll actually use - each has maintenance overhead.\n\n## Step 10: Optional Enhancements\n\n### Space Partitioning (NOT RECOMMENDED)\n\nOnly for query patterns where you ALWAYS filter by the space-partition column with expert knowledge and extensive benchmarking. STRONGLY prefer time-only partitioning.\n\n## Step 11: Verify Configuration\n\n```sql\n-- Check hypertable\nSELECT * FROM timescaledb_information.hypertables\nWHERE hypertable_name = 'your_table_name';\n\n-- Check compression\nSELECT * FROM timescaledb_information.columnstore_settings\nWHERE hypertable_name LIKE 'your_table_name';\n\n-- Check aggregates\nSELECT * FROM timescaledb_information.continuous_aggregates;\n\n-- Check policies\nSELECT * FROM timescaledb_information.jobs ORDER BY job_id;\n\n-- Monitor chunk information\nSELECT chunk_name, table_size, compressed_heap_size, compressed_index_size\nFROM timescaledb_information.chunks\nWHERE hypertable_name = 'your_table_name';\n```\n\n## Performance Guidelines\n\n- **Chunk size:** Recent chunk indexes should fit in less than 25% of RAM\n- **Compression:** Expect 90%+ reduction (10x) with proper columnstore config\n- **Query optimization:** Use continuous aggregates for historical queries and dashboards\n- **Memory:** Run `timescaledb-tune` for self-hosting (auto-configured on cloud)\n\n## Schema Best Practices\n\n### Do's and Don'ts\n\n- ✅ Use `TIMESTAMPTZ` NOT `timestamp`\n- ✅ Use `>=` and `<` NOT `BETWEEN` for timestamps\n- ✅ Use `TEXT` with constraints NOT `char(n)`/`varchar(n)`\n- ✅ Use `snake_case` NOT `CamelCase`\n- ✅ Use `BIGINT GENERATED ALWAYS AS IDENTITY` NOT `SERIAL`\n- ✅ Use `BIGINT` for IDs by default over `INTEGER` or `SMALLINT`\n- ✅ Use `DOUBLE PRECISION` by default over `REAL`/`FLOAT`\n- ✅ Use `NUMERIC` NOT `MONEY`\n- ✅ Use `NOT EXISTS` NOT `NOT IN`\n- ✅ Use `time_bucket()` or `date_trunc()` NOT `timestamp(0)` for truncation\n\n## API Reference (Current vs Deprecated)\n\n**Deprecated Parameters → New Parameters:**\n\n- `timescaledb.compress` → `timescaledb.enable_columnstore`\n- `timescaledb.compress_segmentby` → `timescaledb.segmentby`\n- `timescaledb.compress_orderby` → `timescaledb.orderby`\n\n**Deprecated Functions → New Functions:**\n\n- `add_compression_policy()` → `add_columnstore_policy()`\n- `remove_compression_policy()` → `remove_columnstore_policy()`\n- `compress_chunk()` → `convert_to_columnstore()`\n- `decompress_chunk()` → `convert_to_rowstore()`\n\n**Deprecated Views → New Views:**\n\n- `compression_settings` → `columnstore_settings`\n- `hypertable_compression_settings` → `hypertable_columnstore_settings`\n- `chunk_compression_settings` → `chunk_columnstore_settings`\n\n**Deprecated Stats Functions → New Stats Functions:**\n\n- `hypertable_compression_stats()` → `hypertable_columnstore_stats()`\n- `chunk_compression_stats()` → `chunk_columnstore_stats()`\n\n## Questions to Ask User\n\n1. What kind of data will you be storing?\n2. How do you expect to use the data?\n3. What queries will you run?\n4. How long to keep the data?\n5. Column types if unclear"
              }
            ]
          }
        ]
      }
    }
  ]
}