{
  "owner": {
    "id": "drillan",
    "display_name": "driller",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/10865659?u=0673ee51a163a29c5ca2875037eb1c37310388de&v=4",
    "url": "https://github.com/drillan",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 3,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "drillan/amplifier-skills-plugin",
      "url": "https://github.com/drillan/amplifier-skills-plugin",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-09T06:14:53Z",
        "created_at": "2026-01-08T06:20:35Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 639
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 257
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/sync-amplifier-core.yml",
          "type": "blob",
          "size": 1313
        },
        {
          "path": ".github/workflows/sync-upstream.yml",
          "type": "blob",
          "size": 1323
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 18
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2784
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/ddd-1-plan.md",
          "type": "blob",
          "size": 5771
        },
        {
          "path": "commands/ddd-2-docs.md",
          "type": "blob",
          "size": 24923
        },
        {
          "path": "commands/ddd-3-code-plan.md",
          "type": "blob",
          "size": 23200
        },
        {
          "path": "commands/ddd-4-code.md",
          "type": "blob",
          "size": 18319
        },
        {
          "path": "commands/ddd-5-finish.md",
          "type": "blob",
          "size": 14836
        },
        {
          "path": "pyproject.toml",
          "type": "blob",
          "size": 200
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/sync_amplifier_core.py",
          "type": "blob",
          "size": 6761
        },
        {
          "path": "scripts/sync_ddd.py",
          "type": "blob",
          "size": 8897
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/amplifier-philosophy",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/amplifier-philosophy/SKILL.md",
          "type": "blob",
          "size": 13319
        },
        {
          "path": "skills/ddd-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/SKILL.md",
          "type": "blob",
          "size": 2364
        },
        {
          "path": "skills/ddd-guide/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/core-concepts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/context-poisoning.md",
          "type": "blob",
          "size": 11416
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/file-crawling.md",
          "type": "blob",
          "size": 7469
        },
        {
          "path": "skills/ddd-guide/references/core-concepts/retcon-writing.md",
          "type": "blob",
          "size": 9315
        },
        {
          "path": "skills/ddd-guide/references/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/phases/00-planning-and-alignment.md",
          "type": "blob",
          "size": 2674
        },
        {
          "path": "skills/ddd-guide/references/phases/01-documentation-retcon.md",
          "type": "blob",
          "size": 15335
        },
        {
          "path": "skills/ddd-guide/references/phases/02-approval-gate.md",
          "type": "blob",
          "size": 3473
        },
        {
          "path": "skills/ddd-guide/references/phases/03-implementation-planning.md",
          "type": "blob",
          "size": 3008
        },
        {
          "path": "skills/ddd-guide/references/phases/04-code-implementation.md",
          "type": "blob",
          "size": 1226
        },
        {
          "path": "skills/ddd-guide/references/phases/05-testing-and-verification.md",
          "type": "blob",
          "size": 18170
        },
        {
          "path": "skills/ddd-guide/references/phases/06-cleanup-and-push.md",
          "type": "blob",
          "size": 3228
        },
        {
          "path": "skills/ddd-guide/references/philosophy",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ddd-guide/references/philosophy/ddd-principles.md",
          "type": "blob",
          "size": 9056
        },
        {
          "path": "skills/ddd-guide/references/philosophy/links-to-foundation.md",
          "type": "blob",
          "size": 4258
        },
        {
          "path": "skills/module-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/module-development/SKILL.md",
          "type": "blob",
          "size": 62002
        }
      ],
      "marketplace": {
        "name": "amplifier-skills-marketplace",
        "version": null,
        "description": "Amplifier design philosophy and DDD workflow plugins",
        "owner_info": {
          "name": "driller"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "amplifier-skills",
            "description": "Amplifier design philosophy, development skills, and Document-Driven Development (DDD) workflow for Claude Code",
            "source": "./",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "driller"
            },
            "install_commands": [
              "/plugin marketplace add drillan/amplifier-skills-plugin",
              "/plugin install amplifier-skills@amplifier-skills-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-09T06:14:53Z",
              "created_at": "2026-01-08T06:20:35Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "amplifier-philosophy",
                "description": "Amplifier design philosophy using Linux kernel metaphor. Covers mechanism vs policy, module architecture, event-driven design, and kernel principles. Use when designing new modules or making architectural decisions.",
                "path": "skills/amplifier-philosophy/SKILL.md",
                "frontmatter": {
                  "name": "amplifier-philosophy",
                  "description": "Amplifier design philosophy using Linux kernel metaphor. Covers mechanism vs policy, module architecture, event-driven design, and kernel principles. Use when designing new modules or making architectural decisions.",
                  "version": "1.0.0",
                  "license": "MIT",
                  "metadata": {
                    "category": "architecture",
                    "complexity": "medium",
                    "original_source": "https://github.com/microsoft/amplifier-core/blob/main/docs/DESIGN_PHILOSOPHY.md"
                  }
                },
                "content": "<!--\n  Source: https://github.com/microsoft/amplifier-core\n  License: MIT\n  Auto-synced for Claude Code Plugin format\n-->\n\n# Amplifier Design Philosophy\n\n**Purpose**: Complete design philosophy for Amplifier - principles, patterns, and decision frameworks that guide all development.\n\n**Use**: When requirements are unclear, use this document and the codebase to make correct, aligned decisions.\n\n---\n\n## Core Principles\n\n**1. Mechanism, Not Policy**\n- Kernel provides capabilities and stable contracts\n- Modules decide behavior (which provider, how to orchestrate, what to log)\n- If two teams might want different behavior → module, not kernel\n\n**2. Ruthless Simplicity**\n- KISS taken to heart: as simple as possible, but no simpler\n- Minimize abstractions - every layer must justify existence\n- Start minimal, grow as needed (avoid future-proofing)\n- Code you don't write has no bugs\n\n**3. Small, Stable, Boring Kernel**\n- Kernel changes rarely, maintains backward compatibility always\n- Easy to reason about by single maintainer\n- Favor deletion over accretion in kernel\n- Innovation happens at edges (modules)\n\n**4. Modular Design (Bricks & Studs)**\n- Each module = self-contained \"brick\" with clear responsibility\n- Interfaces = \"studs\" that allow independent regeneration\n- Prefer regeneration over editing (rebuild from spec, don't line-edit)\n- Stable contracts enable parallel development\n\n**5. Event-First Observability**\n- If it's important → emit a canonical event\n- If it's not observable → it didn't happen\n- One JSONL stream = single source of truth\n- Hooks observe without blocking\n\n**6. Text-First, Inspectable**\n- Human-readable, diffable, versionable representations\n- JSON schemas for validation\n- No hidden state, no magic globals\n- Explicit > implicit\n\n---\n\n## The Linux Kernel Decision Framework\n\nUse Linux kernel as a metaphor when decisions are unclear.\n\n### Metaphor Mapping\n\n| Linux Concept | Amplifier Analog | Decision Guidance |\n|---------------|------------------|-------------------|\n| **Ring 0 kernel** | `amplifier-core` | Export mechanisms (mount, emit), never policy. Keep tiny & boring. |\n| **Syscalls** | Session operations | Few and sharp: `create_session()`, `mount()`, `emit()`. Stable ABI. |\n| **Loadable drivers** | Modules (providers, tools, hooks, orchestrators) | Compete at edges; comply with protocols; regeneratable. |\n| **Signals/Netlink** | Event bus / hooks | Kernel emits lifecycle events; hooks observe; non-blocking. |\n| **/proc & dmesg** | Unified JSONL log | One canonical stream; redaction before logging. |\n| **Capabilities/LSM** | Approval & capability checks | Least privilege; deny-by-default; policy at edges. |\n| **Scheduler** | Orchestrator modules | Swap strategies by replacing module, not changing kernel. |\n| **VM/Memory** | Context manager | Deterministic compaction; emit `context:*` events. |\n\n### Decision Playbook\n\nWhen requirements are vague:\n\n1. **Is this kernel work?**\n   - If it selects, optimizes, formats, routes, plans → **module** (policy)\n   - Kernel only adds mechanisms many policies could use\n   - **Litmus test**: Could two teams want different behavior? → Module\n\n2. **Do we have two implementations?**\n   - Prototype at edges first\n   - Extract to kernel only after ≥2 modules converge on the need\n\n3. **Prefer regeneration**\n   - Keep contracts stable\n   - Regenerate modules to new spec (don't line-edit)\n\n4. **Event-first**\n   - Important actions → emit canonical event\n   - Hooks observe without blocking\n\n5. **Text-first**\n   - All diagnostics → JSONL\n   - External views derive from canonical stream\n\n6. **Ruthless simplicity**\n   - Fewer moving parts wins\n   - Clearer failure modes wins\n\n---\n\n## Kernel vs Module Boundaries\n\n### Kernel Responsibilities (Mechanisms)\n\n**What kernel does**:\n- Stable contracts (protocols, schemas)\n- Module loading/unloading (mount/unmount)\n- Event dispatch (emit lifecycle events)\n- Capability checks (enforcement mechanism)\n- Minimal context plumbing (session_id, request_id)\n\n**What kernel NEVER does**:\n- Select providers or models (policy)\n- Decide orchestration strategy (policy)\n- Choose tool behavior (policy)\n- Format output or pick logging destination (policy)\n- Make product decisions (policy)\n\n### Module Responsibilities (Policies)\n\n**Providers**: Which model, what parameters\n**Tools**: How to execute capabilities\n**Orchestrators**: Execution strategy (basic, streaming, planning)\n**Hooks**: What to log, where to log, what to redact\n**Context**: Compaction strategy, summarization approach\n**Agents**: Configuration overlays for sub-sessions\n\n### Evolution Without Breaking Modules\n\n**Additive evolution**:\n- Add optional capabilities (feature negotiation)\n- Extend schemas with optional fields\n- Provide deprecation windows for removals\n\n**Two-implementation rule**:\n- Need from ≥2 independent modules before adding to kernel\n- Proves the mechanism is actually general\n\n**Backward compatibility is sacred**:\n- Kernel changes must not break existing modules\n- Clear deprecation notices + dual-path support\n- Long sunset periods\n\n---\n\n## Module Design: Bricks & Studs\n\n### The LEGO Model\n\nThink of software as LEGO bricks:\n\n**Brick** = Self-contained module with clear responsibility\n**Stud** = Interface/protocol where bricks connect\n**Blueprint** = Specification (docs define target state)\n**Builder** = AI generates code from spec\n\n### Key Practices\n\n1. **Start with the contract** (the \"stud\")\n   - Define: purpose, inputs, outputs, side-effects, dependencies\n   - Document in README or top-level docstring\n   - Keep small enough to hold in one prompt\n\n2. **Build in isolation**\n   - Code, tests, fixtures inside module directory\n   - Only expose contract via `__all__` or interface file\n   - No other module imports internals\n\n3. **Regenerate, don't patch**\n   - When change needed inside brick → rewrite whole brick from spec\n   - Contract change → locate consumers, regenerate them too\n   - Prefer clean regeneration over scattered line edits\n\n4. **Human as architect, AI as builder**\n   - Human: Write spec, review behavior, make decisions\n   - AI: Generate brick, run tests, report results\n   - Human rarely reads code unless tests fail\n\n### Benefits\n\n- Each module independently regeneratable\n- Parallel development (different bricks simultaneously)\n- Multiple variants possible (try different approaches)\n- AI-native workflow (specify → generate → test)\n\n---\n\n## Implementation Patterns\n\n### Vertical Slices\n\n- Implement complete end-to-end paths first\n- Start with core user journeys\n- Get data flowing through all layers early\n- Add features horizontally only after core flows work\n\n### Iterative Development\n\n- 80/20 principle: high-value, low-effort first\n- One working feature > multiple partial features\n- Validate with real usage before enhancing\n- Be willing to refactor early work as patterns emerge\n\n### Testing Strategy\n\n- Emphasis on integration tests (full flow)\n- Manual testability as design goal\n- Critical path testing initially\n- Unit tests for complex logic and edge cases\n- Testing pyramid: 60% unit, 30% integration, 10% end-to-end\n\n**What to test**:\n- ✅ Runtime invariants (catch real bugs)\n- ✅ Edge cases (boundary conditions)\n- ✅ Integration behavior (full flow)\n- ❌ Things obvious from reading code (constant values)\n- ❌ Redundant with code inspection\n\n### Error Handling\n\n- Handle common errors robustly\n- Log detailed information for debugging\n- Provide clear error messages to users\n- Fail fast and visibly during development\n- Never silent fallbacks that hide bugs\n\n### Simplicity Guidelines\n\n**Start minimal**:\n- Begin with simplest implementation meeting current needs\n- Don't build for hypothetical future requirements\n- Add complexity only when requirements demand it\n\n**Question everything**:\n- Does this abstraction justify its existence?\n- Can we solve this more directly?\n- What's the maintenance cost?\n\n**Areas to embrace complexity**:\n- Security (never compromise fundamentals)\n- Data integrity (consistency and reliability)\n- Core user experience (smooth primary flows)\n- Error visibility (make problems diagnosable)\n\n**Areas to aggressively simplify**:\n- Internal abstractions (minimize layers)\n- Generic \"future-proof\" code (YAGNI)\n- Edge case handling (common cases first)\n- Framework usage (only what you need)\n- State management (keep simple and explicit)\n\n---\n\n## Decision Framework\n\nWhen faced with implementation decisions, ask:\n\n1. **Necessity**: \"Do we actually need this right now?\"\n2. **Simplicity**: \"What's the simplest way to solve this?\"\n3. **Directness**: \"Can we solve this more directly?\"\n4. **Value**: \"Does the complexity add proportional value?\"\n5. **Maintenance**: \"How easy will this be to understand later?\"\n\n### Library vs Custom Code\n\n**Evolution pattern** (both valid):\n- Start simple → Custom code for basic needs\n- Growing complexity → Switch to library when requirements expand\n- Hitting limits → Back to custom when outgrowing library\n\n**Questions to ask**:\n- How well does this library align with actual needs?\n- Are we fighting the library or working with it?\n- Is integration clean or requiring workarounds?\n- Will future requirements stay within library's capabilities?\n\n**Stay flexible**: Keep library integration minimal and isolated so you can switch approaches when needs change.\n\n---\n\n## Anti-Patterns (What to Resist)\n\n### In Kernel Development\n\n❌ Adding defaults or config resolution inside kernel\n❌ File I/O or search paths in kernel (app layer responsibility)\n❌ Provider selection, orchestration strategy, tool routing (policies)\n❌ Logging to stdout or private files (use unified JSONL only)\n❌ Breaking backward compatibility without migration path\n\n### In Module Development\n\n❌ Depending on kernel internals (use protocols only)\n❌ Inventing ad-hoc event names (use canonical taxonomy)\n❌ Private log files (write via `context.log` only)\n❌ Failing to emit events for observable actions\n❌ Crashing kernel on module failure (non-interference)\n\n### In Design\n\n❌ Over-general modules trying to do everything\n❌ Copying patterns without understanding rationale\n❌ Optimizing the wrong thing (looks over function)\n❌ Over-engineering for hypothetical futures\n❌ Clever code over clear code\n\n### In Process\n\n❌ \"Let's add a flag in kernel for this use case\" → Module instead\n❌ \"We'll break the API now; adoption is small\" → Backward compat sacred\n❌ \"We'll add it to kernel and figure out policy later\" → Policy first at edges\n❌ \"This needs to be in kernel for speed\" → Prove with benchmarks first\n\n---\n\n## Governance & Evolution\n\n### Kernel Changes\n\n**High bar, low velocity**:\n- Kernel PRs: tiny diff, invariant review, tests, docs, rollback plan\n- Releases are small and boring\n- Large ideas prove themselves at edges first\n\n**Acceptance criteria**:\n- ✅ Implements mechanism (not policy)\n- ✅ Evidence from ≥2 modules needing it\n- ✅ Preserves invariants (non-interference, backward compat)\n- ✅ Interface small, explicit, text-first\n- ✅ Tests and docs included\n- ✅ Retires equivalent complexity elsewhere\n\n### Module Changes\n\n**Fast lanes at the edges**:\n- Modules iterate rapidly\n- Kernel doesn't chase module changes\n- Modules adapt to kernel (not vice versa)\n- Compete through better policies\n\n---\n\n## Quick Reference\n\n### For Kernel Work\n\n**Questions before adding to kernel**:\n1. Is this a mechanism many policies could use?\n2. Do ≥2 modules need this?\n3. Does it preserve backward compatibility?\n4. Is the interface minimal and stable?\n\n**If any \"no\"** → Prototype as module first\n\n### For Module Work\n\n**Module author checklist**:\n- [ ] Implements protocol only (no kernel internals)\n- [ ] Emits canonical events where appropriate\n- [ ] Uses `context.log` (no private logging)\n- [ ] Handles own failures (non-interference)\n- [ ] Tests include isolation verification\n\n### For Design Decisions\n\n**Use the Linux kernel lens**:\n- Scheduling strategy? → Orchestrator module (userspace)\n- Provider selection? → App layer policy\n- Tool behavior? → Tool module\n- Security policy? → Hook module\n- Logging destination? → Hook module\n\n**Remember**: If two teams might want different behavior → Module, not kernel.\n\n---\n\n## Summary\n\n**Amplifier succeeds by**:\n- Keeping kernel tiny, stable, boring\n- Pushing innovation to competing modules\n- Maintaining strong, text-first contracts\n- Enabling observability without opinion\n- Trusting emergence over central planning\n\n**The center stays still so the edges can move fast.**\n\nBuild mechanisms in kernel. Build policies in modules. Use Linux kernel as your decision metaphor. Keep it simple, keep it observable, keep it regeneratable.\n\n**When in doubt**: Could another team want different behavior? If yes → Module. If no → Maybe kernel, but prove with ≥2 implementations first."
              },
              {
                "name": "ddd-guide",
                "description": "Document-Driven Development workflow for existing codebases. Provides systematic planning, documentation-first design, and implementation verification.",
                "path": "skills/ddd-guide/SKILL.md",
                "frontmatter": {
                  "name": "ddd-guide",
                  "description": "Document-Driven Development workflow for existing codebases. Provides systematic planning, documentation-first design, and implementation verification.",
                  "version": "1.0.0",
                  "license": "MIT",
                  "metadata": {
                    "category": "workflow",
                    "complexity": "high",
                    "original_source": "https://github.com/robotdad/amplifier-collection-ddd"
                  }
                },
                "content": "# Document-Driven Development (DDD) Guide\n\n## Core Principle\n\n**Documentation IS the specification. Code implements what documentation describes.**\n\nDDD inverts traditional development: update documentation first, then implement code to match.\n\n## Why DDD?\n\n- **Catches design flaws early** - Before expensive code changes\n- **Prevents documentation drift** - Docs and code stay synchronized\n- **Enables human review** - Humans approve specs, not code\n- **AI-friendly** - Clear specifications reduce hallucination\n\n## Six-Phase Workflow\n\n| Phase | Name | Command | Deliverable |\n|-------|------|---------|-------------|\n| 0-1 | Planning | /ddd 1-plan | plan.md |\n| 2 | Documentation | /ddd 2-docs | Updated docs |\n| 3 | Code Planning | /ddd 3-code-plan | code_plan.md |\n| 4 | Implementation | /ddd 4-code | Working code |\n| 5-6 | Finalization | /ddd 5-finish | Tested, committed |\n\n## Core Techniques\n\n### Retcon Writing\n\nDocument features as if they already exist. No future tense.\n\n- Bad: \"This feature will add...\"\n- Good: \"This feature provides...\"\n\n### File Crawling\n\nProcess files one at a time to avoid context overflow:\n\n1. Generate index with `[ ]` checkboxes\n2. Process one file per iteration\n3. Mark `[x]` when complete\n\n### Context Poisoning Prevention\n\nEliminate contradictions:\n\n- One authoritative location per concept\n- Delete duplicates, don't update\n- Resolve conflicts before proceeding\n\n## When to Use DDD\n\n**Use DDD for:**\n- Multi-file changes\n- New features in existing codebases\n- Complex integrations\n\n**Skip DDD for:**\n- Typo fixes\n- Single-file changes\n- Emergency hotfixes\n\n## References\n\nSee `@skills/ddd-guide/references/` for detailed documentation:\n\n- `core-concepts/` - Techniques and methodologies\n- `phases/` - Step-by-step phase guides\n- `philosophy/` - Underlying principles\n\n## Remember\n\n**Documentation first.** If it's not documented, it doesn't exist.\n\n**Retcon, don't predict.** Write as if the feature already exists.\n\n**One source of truth.** Delete duplicates, don't update them."
              },
              {
                "name": "module-development",
                "description": "Guide for creating new Amplifier modules including protocol implementation, entry points, mount functions, and testing patterns. Use when creating new modules or understanding module architecture.",
                "path": "skills/module-development/SKILL.md",
                "frontmatter": {
                  "name": "module-development",
                  "description": "Guide for creating new Amplifier modules including protocol implementation, entry points, mount functions, and testing patterns. Use when creating new modules or understanding module architecture.",
                  "version": "1.0.0",
                  "license": "MIT",
                  "metadata": {
                    "category": "development",
                    "complexity": "medium",
                    "original_source": "https://github.com/microsoft/amplifier-core/tree/main/docs/contracts"
                  }
                },
                "content": "<!--\n  Source: https://github.com/microsoft/amplifier-core\n  License: MIT\n  Auto-synced for Claude Code Plugin format\n-->\n\n# Module Contracts\n\n**Start here for building Amplifier modules.**\n\nThis directory contains the authoritative guidance for building each type of Amplifier module. Each contract document explains:\n\n1. **What it is** - Purpose and responsibilities\n2. **Protocol reference** - Link to interfaces.py with exact line numbers\n3. **Entry point pattern** - How modules are discovered and loaded\n4. **Configuration** - Mount Plan integration\n5. **Canonical example** - Reference implementation\n6. **Validation** - How to verify your module works\n\n---\n\n## Module Types\n\n| Module Type | Contract | Purpose |\n|-------------|----------|---------|\n| **Provider** | [PROVIDER_CONTRACT.md](PROVIDER_CONTRACT.md) | LLM backend integration |\n| **Tool** | [TOOL_CONTRACT.md](TOOL_CONTRACT.md) | Agent capabilities |\n| **Hook** | [HOOK_CONTRACT.md](HOOK_CONTRACT.md) | Lifecycle observation and control |\n| **Orchestrator** | [ORCHESTRATOR_CONTRACT.md](ORCHESTRATOR_CONTRACT.md) | Agent loop execution strategy |\n| **Context** | [CONTEXT_CONTRACT.md](CONTEXT_CONTRACT.md) | Conversation memory management |\n\n---\n\n## Quick Start Pattern\n\nAll modules follow this pattern:\n\n```python\n# 1. Implement the Protocol from interfaces.py\nclass MyModule:\n    # ... implement required methods\n    pass\n\n# 2. Provide mount() function\nasync def mount(coordinator, config):\n    \"\"\"Initialize and register module.\"\"\"\n    instance = MyModule(config)\n    await coordinator.mount(\"category\", instance, name=\"my-module\")\n    return instance  # or cleanup function\n\n# 3. Register entry point in pyproject.toml\n# [project.entry-points.\"amplifier.modules\"]\n# my-module = \"my_package:mount\"\n```\n\n---\n\n## Source of Truth\n\n**Protocols are in code**, not docs:\n\n- **Protocol definitions**: `amplifier_core/interfaces.py`\n- **Data models**: `amplifier_core/models.py`\n- **Message models**: `amplifier_core/message_models.py` (Pydantic models for request/response envelopes)\n- **Content models**: `amplifier_core/content_models.py` (dataclass types for events and streaming)\n\nThese contract documents provide **guidance** that code cannot express. Always read the code docstrings first.\n\n---\n\n## Related Documentation\n\n- [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) - Configuration contract\n- [MODULE_SOURCE_PROTOCOL.md](https://github.com/microsoft/amplifier-core/blob/main/MODULE_SOURCE_PROTOCOL.md) - Module loading mechanism\n- [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) - Module contribution pattern\n- [DESIGN_PHILOSOPHY.md](https://github.com/microsoft/amplifier-core/blob/main/DESIGN_PHILOSOPHY.md) - Kernel design principles\n\n---\n\n## Validation\n\nVerify your module before release:\n\n```bash\n# Structural validation\namplifier module validate ./my-module\n```\n\nSee individual contract documents for type-specific validation requirements.\n\n---\n\n**For ecosystem overview**: [amplifier](https://github.com/microsoft/amplifier)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: tool\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Tool\n    relationship: protocol_definition\n    lines: 121-146\n  - path: amplifier_core/models.py#ToolResult\n    relationship: result_model\n  - path: amplifier_core/message_models.py#ToolCall\n    relationship: invocation_model\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: amplifier_core/testing.py#MockTool\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-tool-filesystem\n---\n\n# Tool Contract\n\nTools provide capabilities that agents can invoke during execution.\n\n---\n\n## Purpose\n\nTools extend agent capabilities beyond pure conversation:\n- **Filesystem operations** - Read, write, edit files\n- **Command execution** - Run shell commands\n- **Web access** - Fetch URLs, search\n- **Task delegation** - Spawn sub-agents\n- **Custom capabilities** - Domain-specific operations\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 121-146\n\n```python\n@runtime_checkable\nclass Tool(Protocol):\n    @property\n    def name(self) -> str:\n        \"\"\"Tool name for invocation.\"\"\"\n        ...\n\n    @property\n    def description(self) -> str:\n        \"\"\"Human-readable tool description.\"\"\"\n        ...\n\n    async def execute(self, input: dict[str, Any]) -> ToolResult:\n        \"\"\"\n        Execute tool with given input.\n\n        Args:\n            input: Tool-specific input parameters\n\n        Returns:\n            Tool execution result\n        \"\"\"\n        ...\n```\n\n---\n\n## Data Models\n\n### ToolCall (Input)\n\n**Source**: `amplifier_core/message_models.py`\n\n```python\nclass ToolCall(BaseModel):\n    id: str                    # Unique ID for correlation\n    name: str                  # Tool name to invoke\n    arguments: dict[str, Any]  # Tool-specific parameters\n```\n\n### ToolResult (Output)\n\n**Source**: `amplifier_core/models.py`\n\n```python\nclass ToolResult(BaseModel):\n    success: bool = True              # Whether execution succeeded\n    output: Any | None = None         # Tool output (typically str or dict)\n    error: dict[str, Any] | None = None  # Error details if failed\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Tool | Callable | None:\n    \"\"\"\n    Initialize and register tool.\n\n    Returns:\n        - Tool instance\n        - Cleanup callable (for resource cleanup)\n        - None for graceful degradation\n    \"\"\"\n    tool = MyTool(config=config)\n    await coordinator.mount(\"tools\", tool, name=\"my-tool\")\n    return tool\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-tool = \"my_tool:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### Name and Description\n\nTools must provide clear identification:\n\n```python\nclass MyTool:\n    @property\n    def name(self) -> str:\n        return \"my_tool\"  # Used for invocation\n\n    @property\n    def description(self) -> str:\n        return \"Performs specific action with given parameters.\"\n```\n\n**Best practices**:\n- `name`: Short, snake_case, unique across mounted tools\n- `description`: Clear explanation of what the tool does and expects\n\n### execute() Method\n\nHandle inputs and return structured results:\n\n```python\nasync def execute(self, input: dict[str, Any]) -> ToolResult:\n    try:\n        # Validate input\n        required_param = input.get(\"required_param\")\n        if not required_param:\n            return ToolResult(\n                success=False,\n                error={\"message\": \"required_param is required\"}\n            )\n\n        # Do the work\n        result = await self._do_work(required_param)\n\n        return ToolResult(\n            success=True,\n            output=result\n        )\n\n    except Exception as e:\n        return ToolResult(\n            success=False,\n            error={\"message\": str(e), \"type\": type(e).__name__}\n        )\n```\n\n### Tool Schema (Optional but Recommended)\n\nProvide JSON schema for input validation:\n\n```python\ndef get_schema(self) -> dict:\n    \"\"\"Return JSON schema for tool input.\"\"\"\n    return {\n        \"type\": \"object\",\n        \"properties\": {\n            \"required_param\": {\n                \"type\": \"string\",\n                \"description\": \"Description of parameter\"\n            },\n            \"optional_param\": {\n                \"type\": \"integer\",\n                \"default\": 10\n            }\n        },\n        \"required\": [\"required_param\"]\n    }\n```\n\n---\n\n## Configuration\n\nTools receive configuration via Mount Plan:\n\n```yaml\ntools:\n  - module: my-tool\n    source: git+https://github.com/org/my-tool@main\n    config:\n      max_size: 1048576\n      allowed_paths:\n        - /home/user/projects\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister lifecycle events:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-tool\",\n    lambda: [\"my-tool:started\", \"my-tool:completed\", \"my-tool:error\"]\n)\n```\n\nStandard tool events emitted by orchestrators:\n- `tool:pre` - Before tool execution\n- `tool:post` - After successful execution\n- `tool:error` - On execution failure\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-tool-filesystem](https://github.com/microsoft/amplifier-module-tool-filesystem)\n\nStudy this module for:\n- Tool protocol implementation\n- Input validation patterns\n- Error handling and result formatting\n- Configuration integration\n\nAdditional examples:\n- [amplifier-module-tool-bash](https://github.com/microsoft/amplifier-module-tool-bash) - Command execution\n- [amplifier-module-tool-web](https://github.com/microsoft/amplifier-module-tool-web) - Web access\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements Tool protocol (name, description, execute)\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Returns `ToolResult` from execute()\n- [ ] Handles errors gracefully (returns success=False, doesn't crash)\n\n### Recommended\n\n- [ ] Provides JSON schema via `get_schema()`\n- [ ] Validates input before processing\n- [ ] Logs operations at appropriate levels\n- [ ] Registers observability events\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, MockTool\n\n@pytest.mark.asyncio\nasync def test_tool_execution():\n    tool = MyTool(config={})\n\n    result = await tool.execute({\n        \"required_param\": \"value\"\n    })\n\n    assert result.success\n    assert result.error is None\n```\n\n### MockTool for Testing Orchestrators\n\n```python\nfrom amplifier_core.testing import MockTool\n\nmock_tool = MockTool(\n    name=\"test_tool\",\n    description=\"Test tool\",\n    return_value=\"mock result\"\n)\n\n# After use\nassert mock_tool.call_count == 1\nassert mock_tool.last_input == {...}\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-tool --type tool\n```\n\n---\n\n**Related**: [README.md](README.md) | [HOOK_CONTRACT.md](HOOK_CONTRACT.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: provider\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Provider\n    relationship: protocol_definition\n    lines: 54-119\n  - path: amplifier_core/message_models.py\n    relationship: request_response_models\n  - path: amplifier_core/content_models.py\n    relationship: event_content_types\n  - path: amplifier_core/models.py#ProviderInfo\n    relationship: metadata_models\n  - path: ../specs/PROVIDER_SPECIFICATION.md\n    relationship: detailed_spec\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-provider-anthropic\n---\n\n# Provider Contract\n\nProviders translate between Amplifier's unified message format and vendor-specific LLM APIs.\n\n---\n\n## Detailed Specification\n\n**See [PROVIDER_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md)** for complete implementation guidance including:\n\n- Protocol summary and method signatures\n- Content block preservation requirements\n- Role conversion patterns\n- Auto-continuation handling\n- Debug levels and observability\n\nThis contract document provides the quick-reference essentials. The specification contains the full details.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 54-119\n\n```python\n@runtime_checkable\nclass Provider(Protocol):\n    @property\n    def name(self) -> str: ...\n\n    def get_info(self) -> ProviderInfo: ...\n\n    async def list_models(self) -> list[ModelInfo]: ...\n\n    async def complete(self, request: ChatRequest, **kwargs) -> ChatResponse: ...\n\n    def parse_tool_calls(self, response: ChatResponse) -> list[ToolCall]: ...\n```\n\n**Note**: `ToolCall` is from `amplifier_core.message_models` (see [REQUEST_ENVELOPE_V1](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md) for details)\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Provider | Callable | None:\n    \"\"\"\n    Initialize and return provider instance.\n\n    Returns:\n        - Provider instance (registered automatically)\n        - Cleanup callable (for resource cleanup on unmount)\n        - None for graceful degradation (e.g., missing API key)\n    \"\"\"\n    api_key = config.get(\"api_key\") or os.environ.get(\"MY_API_KEY\")\n    if not api_key:\n        logger.warning(\"No API key - provider not mounted\")\n        return None\n\n    provider = MyProvider(api_key=api_key, config=config)\n    await coordinator.mount(\"providers\", provider, name=\"my-provider\")\n\n    async def cleanup():\n        await provider.client.close()\n\n    return cleanup\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-provider = \"my_provider:mount\"\n```\n\n---\n\n## Configuration\n\nProviders receive configuration via Mount Plan:\n\n```yaml\nproviders:\n  - module: my-provider\n    source: git+https://github.com/org/my-provider@main\n    config:\n      api_key: \"${MY_API_KEY}\"\n      default_model: model-v1\n      debug: true\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events via contribution channels:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-provider\",\n    lambda: [\"my-provider:rate_limit\", \"my-provider:retry\"]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-provider-anthropic](https://github.com/microsoft/amplifier-module-provider-anthropic)\n\nStudy this module for:\n- Complete Provider protocol implementation\n- Content block handling patterns\n- Configuration and credential management\n- Debug logging integration\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements all 5 Provider protocol methods\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Preserves all content block types (especially `signature` in ThinkingBlock)\n- [ ] Reports `Usage` (input/output/total tokens)\n- [ ] Returns `ChatResponse` from `complete()`\n\n### Recommended\n\n- [ ] Graceful degradation on missing config (return None from mount)\n- [ ] Validates tool call/result sequences\n- [ ] Supports debug configuration flags\n- [ ] Registers cleanup function for resource management\n- [ ] Registers observability events via contribution channels\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, create_test_coordinator\n\n@pytest.mark.asyncio\nasync def test_provider_mount():\n    coordinator = create_test_coordinator()\n    cleanup = await mount(coordinator, {\"api_key\": \"test-key\"})\n\n    assert \"my-provider\" in coordinator.get_mounted(\"providers\")\n\n    if cleanup:\n        await cleanup()\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-provider --type provider\n```\n\n---\n\n**Related**: [PROVIDER_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/PROVIDER_SPECIFICATION.md) | [README.md](README.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: hook\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#HookHandler\n    relationship: protocol_definition\n    lines: 205-220\n  - path: amplifier_core/models.py#HookResult\n    relationship: result_model\n  - path: ../HOOKS_API.md\n    relationship: detailed_api\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#EventRecorder\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-hooks-logging\n---\n\n# Hook Contract\n\nHooks observe, validate, and control agent lifecycle events.\n\n---\n\n## Purpose\n\nHooks enable:\n- **Observation** - Logging, metrics, audit trails\n- **Validation** - Security checks, input validation\n- **Feedback injection** - Automated correction loops\n- **Approval gates** - Dynamic permission requests\n- **Output control** - Clean user experience\n\n---\n\n## Detailed API Reference\n\n**See [HOOKS_API.md](https://github.com/microsoft/amplifier-core/blob/main/HOOKS_API.md)** for complete documentation including:\n\n- HookResult actions and fields\n- Registration patterns\n- Common patterns with examples\n- Best practices\n\nThis contract provides the essentials. The API reference contains full details.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 205-220\n\n```python\n@runtime_checkable\nclass HookHandler(Protocol):\n    async def __call__(self, event: str, data: dict[str, Any]) -> HookResult:\n        \"\"\"\n        Handle a lifecycle event.\n\n        Args:\n            event: Event name (e.g., \"tool:pre\", \"session:start\")\n            data: Event-specific data\n\n        Returns:\n            HookResult indicating action to take\n        \"\"\"\n        ...\n```\n\n---\n\n## HookResult Actions\n\n**Source**: `amplifier_core/models.py`\n\n| Action | Behavior | Use Case |\n|--------|----------|----------|\n| `continue` | Proceed normally | Default, observation only |\n| `deny` | Block operation | Validation failure, security |\n| `modify` | Transform data | Preprocessing, enrichment |\n| `inject_context` | Add to agent's context | Feedback loops, corrections |\n| `ask_user` | Request approval | High-risk operations |\n\n```python\nfrom amplifier_core.models import HookResult\n\n# Simple observation\nHookResult(action=\"continue\")\n\n# Block with reason\nHookResult(action=\"deny\", reason=\"Access denied\")\n\n# Inject feedback\nHookResult(\n    action=\"inject_context\",\n    context_injection=\"Found 3 linting errors...\",\n    user_message=\"Linting issues detected\"\n)\n\n# Request approval\nHookResult(\n    action=\"ask_user\",\n    approval_prompt=\"Allow write to production file?\",\n    approval_default=\"deny\"\n)\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Callable | None:\n    \"\"\"\n    Initialize and register hook handlers.\n\n    Returns:\n        Cleanup callable to unregister handlers\n    \"\"\"\n    handlers = []\n\n    # Register handlers for specific events\n    handlers.append(\n        coordinator.hooks.register(\"tool:pre\", my_validation_hook, priority=10)\n    )\n    handlers.append(\n        coordinator.hooks.register(\"tool:post\", my_feedback_hook, priority=20)\n    )\n\n    # Return cleanup function\n    def cleanup():\n        for unregister in handlers:\n            unregister()\n\n    return cleanup\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-hook = \"my_hook:mount\"\n```\n\n---\n\n## Event Registration\n\nRegister handlers during mount():\n\n```python\nfrom amplifier_core.hooks import HookRegistry\n\n# Get registry from coordinator\nregistry: HookRegistry = coordinator.hooks\n\n# Register with priority (lower = earlier)\nunregister = registry.register(\n    event=\"tool:post\",\n    handler=my_handler,\n    priority=10,\n    name=\"my_handler\"\n)\n\n# Later: unregister()\n```\n\n---\n\n## Common Events\n\n| Event | Trigger | Data Includes |\n|-------|---------|---------------|\n| `session:start` | Session created | session_id, config |\n| `session:end` | Session ending | session_id, stats |\n| `prompt:submit` | User input | prompt text |\n| `tool:pre` | Before tool execution | tool_name, tool_input |\n| `tool:post` | After tool execution | tool_name, tool_result |\n| `tool:error` | Tool failed | tool_name, error |\n| `provider:request` | LLM call starting | provider, messages |\n| `provider:response` | LLM call complete | provider, response, usage |\n\n---\n\n## Configuration\n\nHooks receive configuration via Mount Plan:\n\n```yaml\nhooks:\n  - module: my-hook\n    source: git+https://github.com/org/my-hook@main\n    config:\n      enabled_events:\n        - \"tool:pre\"\n        - \"tool:post\"\n      log_level: \"info\"\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events your hook emits:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-hook\",\n    lambda: [\"my-hook:validation_failed\", \"my-hook:approved\"]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-hooks-logging](https://github.com/microsoft/amplifier-module-hooks-logging)\n\nStudy this module for:\n- Hook registration patterns\n- Event handling\n- Configuration integration\n- Observability contribution\n\nAdditional examples:\n- [amplifier-module-hooks-approval](https://github.com/microsoft/amplifier-module-hooks-approval) - Approval gates\n- [amplifier-module-hooks-redaction](https://github.com/microsoft/amplifier-module-hooks-redaction) - Security redaction\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Handler implements `async def __call__(event, data) -> HookResult`\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Returns valid `HookResult` for all code paths\n- [ ] Handles exceptions gracefully (don't crash kernel)\n\n### Recommended\n\n- [ ] Register cleanup function to unregister handlers\n- [ ] Use appropriate priority (10-90, lower = earlier)\n- [ ] Log handler registration for debugging\n- [ ] Support configuration for enabled events\n- [ ] Register custom events via contribution channels\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import TestCoordinator, EventRecorder\nfrom amplifier_core.models import HookResult\n\n@pytest.mark.asyncio\nasync def test_hook_handler():\n    # Test handler directly\n    result = await my_validation_hook(\"tool:pre\", {\n        \"tool_name\": \"Write\",\n        \"tool_input\": {\"file_path\": \"/etc/passwd\"}\n    })\n\n    assert result.action == \"deny\"\n    assert \"denied\" in result.reason.lower()\n\n@pytest.mark.asyncio\nasync def test_hook_registration():\n    coordinator = TestCoordinator()\n    cleanup = await mount(coordinator, {})\n\n    # Verify handlers registered\n    # ... test event emission\n\n    cleanup()\n```\n\n### EventRecorder for Testing\n\n```python\nfrom amplifier_core.testing import EventRecorder\n\nrecorder = EventRecorder()\n\n# Use in tests\nawait recorder.record(\"tool:pre\", {\"tool_name\": \"Write\"})\n\n# Assert\nevents = recorder.get_events()\nassert len(events) == 1\nassert events[0][0] == \"tool:pre\"  # events are (event_name, data) tuples\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-hook --type hook\n```\n\n---\n\n**Related**: [HOOKS_API.md](https://github.com/microsoft/amplifier-core/blob/main/HOOKS_API.md) | [README.md](README.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: orchestrator\ncontract_version: 1.0.0\nlast_modified: 2025-01-29\nrelated_files:\n  - path: amplifier_core/interfaces.py#Orchestrator\n    relationship: protocol_definition\n    lines: 26-52\n  - path: amplifier_core/content_models.py\n    relationship: event_content_types\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#ScriptedOrchestrator\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-loop-basic\n---\n\n# Orchestrator Contract\n\nOrchestrators implement the agent execution loop strategy.\n\n---\n\n## Purpose\n\nOrchestrators control **how** agents execute:\n- **Basic loops** - Simple prompt → response → tool → response cycles\n- **Streaming** - Real-time response delivery\n- **Event-driven** - Complex multi-step workflows\n- **Custom strategies** - Domain-specific execution patterns\n\n**Key principle**: The orchestrator is **policy**, not mechanism. Swap orchestrators to change agent behavior without modifying the kernel.\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 26-52\n\n```python\n@runtime_checkable\nclass Orchestrator(Protocol):\n    async def execute(\n        self,\n        prompt: str,\n        context: ContextManager,\n        providers: dict[str, Provider],\n        tools: dict[str, Tool],\n        hooks: HookRegistry,\n    ) -> str:\n        \"\"\"\n        Execute the agent loop with given prompt.\n\n        Args:\n            prompt: User input prompt\n            context: Context manager for conversation state\n            providers: Available LLM providers (keyed by name)\n            tools: Available tools (keyed by name)\n            hooks: Hook registry for lifecycle events\n\n        Returns:\n            Final response string\n        \"\"\"\n        ...\n```\n\n---\n\n## Execution Flow\n\nA typical orchestrator implements this flow:\n\n```\nUser Prompt\n    ↓\nAdd to Context\n    ↓\n┌─────────────────────────────────────┐\n│  LOOP until response has no tools   │\n│                                     │\n│  1. emit(\"provider:request\")        │\n│  2. provider.complete(messages)     │\n│  3. emit(\"provider:response\")       │\n│  4. Add response to context         │\n│                                     │\n│  If tool_calls:                     │\n│    for each tool_call:              │\n│      5. emit(\"tool:pre\")            │\n│      6. tool.execute(input)         │\n│      7. emit(\"tool:post\")           │\n│      8. Add result to context       │\n│                                     │\n│  Continue loop...                   │\n└─────────────────────────────────────┘\n    ↓\nReturn final text response\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> Orchestrator | Callable | None:\n    \"\"\"\n    Initialize and return orchestrator instance.\n\n    Returns:\n        - Orchestrator instance\n        - Cleanup callable\n        - None for graceful degradation\n    \"\"\"\n    orchestrator = MyOrchestrator(config=config)\n    await coordinator.mount(\"session\", orchestrator, name=\"orchestrator\")\n    return orchestrator\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-orchestrator = \"my_orchestrator:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### Event Emission\n\nOrchestrators must emit lifecycle events for observability:\n\n```python\nasync def execute(self, prompt, context, providers, tools, hooks):\n    # Before LLM call\n    await hooks.emit(\"provider:request\", {\n        \"provider\": provider_name,\n        \"messages\": messages,\n        \"model\": model_name\n    })\n\n    response = await provider.complete(request)\n\n    # After LLM call\n    await hooks.emit(\"provider:response\", {\n        \"provider\": provider_name,\n        \"response\": response,\n        \"usage\": response.usage\n    })\n\n    # Before tool execution\n    await hooks.emit(\"tool:pre\", {\n        \"tool_name\": tool_call.name,\n        \"tool_input\": tool_call.input\n    })\n\n    result = await tool.execute(tool_call.input)\n\n    # After tool execution\n    await hooks.emit(\"tool:post\", {\n        \"tool_name\": tool_call.name,\n        \"tool_input\": tool_call.input,\n        \"tool_result\": result\n    })\n```\n\n### Hook Processing\n\nHandle HookResult actions:\n\n```python\n# Before tool execution\npre_result = await hooks.emit(\"tool:pre\", data)\n\nif pre_result.action == \"deny\":\n    # Don't execute tool\n    return ToolResult(is_error=True, output=pre_result.reason)\n\nif pre_result.action == \"modify\":\n    # Use modified data\n    data = pre_result.data\n\nif pre_result.action == \"inject_context\":\n    # Add feedback to context\n    await context.add_message({\n        \"role\": pre_result.context_injection_role,\n        \"content\": pre_result.context_injection\n    })\n\nif pre_result.action == \"ask_user\":\n    # Request approval (requires approval provider)\n    approved = await request_approval(pre_result)\n    if not approved:\n        return ToolResult(is_error=True, output=\"User denied\")\n```\n\n### Context Management\n\nManage conversation state:\n\n```python\n# Add user message\nawait context.add_message({\"role\": \"user\", \"content\": prompt})\n\n# Add assistant response\nawait context.add_message({\"role\": \"assistant\", \"content\": response.content})\n\n# Add tool result\nawait context.add_message({\n    \"role\": \"tool\",\n    \"tool_call_id\": tool_call.id,\n    \"content\": result.output\n})\n\n# Get messages for LLM request (context handles compaction internally)\nmessages = await context.get_messages_for_request()\n```\n\n### Provider Selection\n\nHandle multiple providers:\n\n```python\n# Get default or configured provider\nprovider_name = config.get(\"default_provider\", list(providers.keys())[0])\nprovider = providers[provider_name]\n\n# Or allow per-request provider selection\nprovider_name = request_options.get(\"provider\", default_provider_name)\n```\n\n---\n\n## Configuration\n\nOrchestrators receive configuration via Mount Plan:\n\n```yaml\nsession:\n  orchestrator: my-orchestrator\n  context: context-simple\n\n# Orchestrator-specific config can be passed via providers/tools config\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister custom events your orchestrator emits:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-orchestrator\",\n    lambda: [\n        \"my-orchestrator:loop_started\",\n        \"my-orchestrator:loop_iteration\",\n        \"my-orchestrator:loop_completed\"\n    ]\n)\n```\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-loop-basic](https://github.com/microsoft/amplifier-module-loop-basic)\n\nStudy this module for:\n- Complete execute() implementation\n- Event emission patterns\n- Hook result handling\n- Context management\n\nAdditional examples:\n- [amplifier-module-loop-streaming](https://github.com/microsoft/amplifier-module-loop-streaming) - Real-time streaming\n- [amplifier-module-loop-events](https://github.com/microsoft/amplifier-module-loop-events) - Event-driven patterns\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements `execute(prompt, context, providers, tools, hooks) -> str`\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] Emits standard events (provider:request/response, tool:pre/post)\n- [ ] Handles HookResult actions appropriately\n- [ ] Manages context (add messages, check compaction)\n\n### Recommended\n\n- [ ] Supports multiple providers\n- [ ] Implements max iterations limit (prevent infinite loops)\n- [ ] Handles provider errors gracefully\n- [ ] Registers custom observability events\n- [ ] Supports streaming via async generators\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import (\n    TestCoordinator,\n    MockTool,\n    MockContextManager,\n    ScriptedOrchestrator,\n    EventRecorder\n)\n\n@pytest.mark.asyncio\nasync def test_orchestrator_basic():\n    orchestrator = MyOrchestrator(config={})\n    context = MockContextManager()\n    providers = {\"test\": MockProvider()}\n    tools = {\"test_tool\": MockTool()}\n    hooks = HookRegistry()\n\n    result = await orchestrator.execute(\n        prompt=\"Test prompt\",\n        context=context,\n        providers=providers,\n        tools=tools,\n        hooks=hooks\n    )\n\n    assert isinstance(result, str)\n    assert len(context.messages) > 0\n```\n\n### ScriptedOrchestrator for Testing\n\n```python\nfrom amplifier_core.testing import ScriptedOrchestrator\n\n# For testing components that use orchestrators\norchestrator = ScriptedOrchestrator(responses=[\"Response 1\", \"Response 2\"])\n\nresult = await orchestrator.execute(...)\nassert result == \"Response 1\"\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-orchestrator --type orchestrator\n```\n\n---\n\n**Related**: [README.md](README.md) | [CONTEXT_CONTRACT.md](CONTEXT_CONTRACT.md)\n\n\n---\n\n---\ncontract_type: module_specification\nmodule_type: context\ncontract_version: 2.1.0\nlast_modified: 2026-01-01\nrelated_files:\n  - path: amplifier_core/interfaces.py#ContextManager\n    relationship: protocol_definition\n    lines: 148-180\n  - path: ../specs/MOUNT_PLAN_SPECIFICATION.md\n    relationship: configuration\n  - path: ../specs/CONTRIBUTION_CHANNELS.md\n    relationship: observability\n  - path: amplifier_core/testing.py#MockContextManager\n    relationship: test_utilities\ncanonical_example: https://github.com/microsoft/amplifier-module-context-simple\n---\n\n# Context Contract\n\nContext managers handle conversation memory and message storage.\n\n---\n\n## Purpose\n\nContext managers control **what the agent remembers**:\n- **Message storage** - Store conversation history\n- **Request preparation** - Return messages that fit within token limits\n- **Persistence** - Optionally persist across sessions\n- **Memory strategies** - Implement various memory patterns\n\n**Key principle**: The context manager owns **policy** for memory. The orchestrator asks for messages; the context manager decides **how** to fit them within limits. Swap context managers to change memory behavior without modifying orchestrators.\n\n**Mechanism vs Policy**: Orchestrators provide the mechanism (request messages, make LLM calls). Context managers provide the policy (what to return, when to compact, how to fit within limits).\n\n---\n\n## Protocol Definition\n\n**Source**: `amplifier_core/interfaces.py` lines 148-180\n\n```python\n@runtime_checkable\nclass ContextManager(Protocol):\n    async def add_message(self, message: dict[str, Any]) -> None:\n        \"\"\"Add a message to the context.\"\"\"\n        ...\n\n    async def get_messages_for_request(\n        self,\n        token_budget: int | None = None,\n        provider: Any | None = None,\n    ) -> list[dict[str, Any]]:\n        \"\"\"\n        Get messages ready for an LLM request.\n\n        The context manager handles any compaction needed internally.\n        Returns messages that fit within the token budget.\n\n        Args:\n            token_budget: Optional explicit token limit (deprecated, prefer provider).\n            provider: Optional provider instance for dynamic budget calculation.\n                If provided, budget = context_window - max_output_tokens - safety_margin.\n\n        Returns:\n            Messages ready for LLM request, compacted if necessary.\n        \"\"\"\n        ...\n\n    async def get_messages(self) -> list[dict[str, Any]]:\n        \"\"\"Get all messages (raw, uncompacted) for transcripts/debugging.\"\"\"\n        ...\n\n    async def set_messages(self, messages: list[dict[str, Any]]) -> None:\n        \"\"\"Set messages directly (for session resume).\"\"\"\n        ...\n\n    async def clear(self) -> None:\n        \"\"\"Clear all messages.\"\"\"\n        ...\n```\n\n---\n\n## Message Format\n\nMessages follow a standard structure:\n\n```python\n# User message\n{\n    \"role\": \"user\",\n    \"content\": \"User's input text\"\n}\n\n# Assistant message\n{\n    \"role\": \"assistant\",\n    \"content\": \"Assistant's response\"\n}\n\n# Assistant message with tool calls\n{\n    \"role\": \"assistant\",\n    \"content\": None,\n    \"tool_calls\": [\n        {\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"read_file\", \"arguments\": \"{...}\"}\n        }\n    ]\n}\n\n# System message\n{\n    \"role\": \"system\",\n    \"content\": \"System instructions\"\n}\n\n# Tool result\n{\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_123\",\n    \"content\": \"Tool output\"\n}\n```\n\n---\n\n## Entry Point Pattern\n\n### mount() Function\n\n```python\nasync def mount(coordinator: ModuleCoordinator, config: dict) -> ContextManager | Callable | None:\n    \"\"\"\n    Initialize and return context manager instance.\n\n    Returns:\n        - ContextManager instance\n        - Cleanup callable\n        - None for graceful degradation\n    \"\"\"\n    context = MyContextManager(\n        max_tokens=config.get(\"max_tokens\", 100000),\n        compaction_threshold=config.get(\"compaction_threshold\", 0.8)\n    )\n    await coordinator.mount(\"session\", context, name=\"context\")\n    return context\n```\n\n### pyproject.toml\n\n```toml\n[project.entry-points.\"amplifier.modules\"]\nmy-context = \"my_context:mount\"\n```\n\n---\n\n## Implementation Requirements\n\n### add_message()\n\nStore messages with proper validation:\n\n```python\nasync def add_message(self, message: dict[str, Any]) -> None:\n    \"\"\"Add a message to the context.\"\"\"\n    # Validate required fields\n    if \"role\" not in message:\n        raise ValueError(\"Message must have 'role' field\")\n\n    # Store message\n    self._messages.append(message)\n\n    # Track token count (approximate)\n    self._token_count += self._estimate_tokens(message)\n```\n\n### get_messages_for_request()\n\nReturn messages ready for LLM request, handling compaction internally:\n\n```python\nasync def get_messages_for_request(\n    self,\n    token_budget: int | None = None,\n    provider: Any | None = None,\n) -> list[dict[str, Any]]:\n    \"\"\"\n    Get messages ready for an LLM request.\n\n    Handles compaction internally if needed. Orchestrators call this\n    before every LLM request and trust the context manager to return\n    messages that fit within limits.\n\n    Args:\n        token_budget: Optional explicit token limit (deprecated, prefer provider).\n        provider: Optional provider instance for dynamic budget calculation.\n            If provided, budget = context_window - max_output_tokens - safety_margin.\n    \"\"\"\n    budget = self._calculate_budget(token_budget, provider)\n\n    # Check if compaction needed\n    if self._token_count > (budget * self._compaction_threshold):\n        await self._compact_internal()\n\n    return list(self._messages)  # Return copy to prevent mutation\n\ndef _calculate_budget(self, token_budget: int | None, provider: Any | None) -> int:\n    \"\"\"Calculate effective token budget from provider or fallback to config.\"\"\"\n    # Explicit budget takes precedence (for backward compatibility)\n    if token_budget is not None:\n        return token_budget\n\n    # Try provider-based dynamic budget\n    if provider is not None:\n        try:\n            info = provider.get_info()\n            defaults = info.defaults or {}\n            context_window = defaults.get(\"context_window\")\n            max_output_tokens = defaults.get(\"max_output_tokens\")\n\n            if context_window and max_output_tokens:\n                safety_margin = 1000  # Buffer to avoid hitting hard limits\n                return context_window - max_output_tokens - safety_margin\n        except Exception:\n            pass  # Fall back to configured max_tokens\n\n    return self._max_tokens\n```\n\n### get_messages()\n\nReturn all messages for transcripts/debugging (no compaction):\n\n```python\nasync def get_messages(self) -> list[dict[str, Any]]:\n    \"\"\"Get all messages (raw, uncompacted) for transcripts/debugging.\"\"\"\n    return list(self._messages)  # Return copy to prevent mutation\n```\n\n### set_messages()\n\nSet messages directly for session resume:\n\n```python\nasync def set_messages(self, messages: list[dict[str, Any]]) -> None:\n    \"\"\"Set messages directly (for session resume).\"\"\"\n    self._messages = list(messages)\n    self._token_count = sum(self._estimate_tokens(m) for m in self._messages)\n```\n\n**File-Based Context Managers - Special Behavior**:\n\nFor context managers with persistent file storage (like `context-persistent`), the behavior on session resume is different:\n\n```python\nasync def set_messages(self, messages: list[dict[str, Any]]) -> None:\n    \"\"\"\n    Set messages - behavior depends on whether we loaded from file.\n    \n    If we already loaded from our own file (session resume):\n      - IGNORE this call to preserve our complete history\n      - CLI's filtered transcript would lose system/developer messages\n    \n    If this is a fresh session or migration:\n      - Accept the messages and write to our file\n    \"\"\"\n    if self._loaded_from_file:\n        # Already have complete history - ignore CLI's filtered transcript\n        logger.info(\"Ignoring set_messages - loaded from persistent file\")\n        return\n    \n    # Fresh session: accept messages\n    self._messages = list(messages)\n    self._write_to_file()\n```\n\n**Why This Pattern?**:\n- CLI's `SessionStore` saves a **filtered** transcript (no system/developer messages)\n- File-based context managers save the **complete** history\n- On resume, the context manager's file is authoritative\n- Prevents loss of system context during session resume\n\n### clear()\n\nReset context state:\n\n```python\nasync def clear(self) -> None:\n    \"\"\"Clear all messages.\"\"\"\n    self._messages = []\n    self._token_count = 0\n```\n\n---\n\n## Internal Compaction\n\nCompaction is an **internal implementation detail** of the context manager. It happens automatically when `get_messages_for_request()` is called and the context exceeds thresholds.\n\n### Non-Destructive Compaction (REQUIRED)\n\n**Critical Design Principle**: Compaction MUST be **ephemeral** - it returns a compacted VIEW without modifying the stored history.\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    NON-DESTRUCTIVE COMPACTION                   │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  messages[]                    get_messages_for_request()       │\n│  ┌──────────┐                  ┌──────────┐                     │\n│  │ msg 1    │                  │ msg 1    │  (compacted view)   │\n│  │ msg 2    │   ──────────▶    │ [summ]   │                     │\n│  │ msg 3    │   ephemeral      │ msg N    │                     │\n│  │ ...      │   compaction     └──────────┘                     │\n│  │ msg N    │                                                   │\n│  └──────────┘                  get_messages()                   │\n│       │                        ┌──────────┐                     │\n│       │                        │ msg 1    │  (FULL history)     │\n│       └───────────────────▶    │ msg 2    │                     │\n│         unchanged              │ msg 3    │                     │\n│                                │ ...      │                     │\n│                                │ msg N    │                     │\n│                                └──────────┘                     │\n│                                                                 │\n│  Key: Internal state is NEVER modified by compaction.           │\n│       Compaction produces temporary views for LLM requests.     │\n│       Full history is always available via get_messages().      │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**Why Non-Destructive?**:\n- **Transcript integrity**: Full conversation history is preserved for replay/debugging\n- **Session resume**: Can resume from any point with complete context\n- **Reproducibility**: Same inputs produce same outputs\n- **Observability**: Hook systems can observe the full conversation\n\n**Implementation Pattern**:\n```python\nasync def get_messages_for_request(self, token_budget=None, provider=None):\n    \"\"\"Return compacted VIEW without modifying internal state.\"\"\"\n    budget = self._calculate_budget(token_budget, provider)\n    \n    # Read current messages (don't modify)\n    messages = list(self._messages)  # Copy!\n    \n    # Check if compaction needed\n    token_count = self._count_tokens(messages)\n    if not self._should_compact(token_count, budget):\n        return messages\n    \n    # Compact EPHEMERALLY - return compacted copy\n    return self._compact_messages(messages, budget)  # Returns NEW list\n\nasync def get_messages(self):\n    \"\"\"Return FULL history (never compacted).\"\"\"\n    return list(self._messages)  # Always complete\n```\n\n### Tool Pair Preservation\n\n**Critical**: During compaction, tool_use and tool_result messages must be kept together. Separating them causes LLM API errors.\n\n```python\nasync def _compact_internal(self) -> None:\n    \"\"\"Internal compaction - preserves tool pairs.\"\"\"\n    # Emit pre-compaction event\n    await self._hooks.emit(\"context:pre_compact\", {\n        \"message_count\": len(self._messages),\n        \"token_count\": self._token_count\n    })\n\n    # Build tool_call_id -> tool_use index map\n    tool_use_ids = set()\n    for msg in self._messages:\n        if msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n            for tc in msg[\"tool_calls\"]:\n                tool_use_ids.add(tc.get(\"id\"))\n\n    # Identify which tool results have matching tool_use\n    orphan_result_indices = []\n    for i, msg in enumerate(self._messages):\n        if msg.get(\"role\") == \"tool\":\n            if msg.get(\"tool_call_id\") not in tool_use_ids:\n                orphan_result_indices.append(i)\n\n    # Strategy: Keep system messages + recent messages\n    # But ensure we don't split tool pairs\n    system_messages = [m for m in self._messages if m[\"role\"] == \"system\"]\n\n    # Find safe truncation point (not in middle of tool sequence)\n    keep_count = self._keep_recent\n    recent_start = max(0, len(self._messages) - keep_count)\n\n    # Adjust start to not split tool sequences\n    while recent_start > 0:\n        msg = self._messages[recent_start]\n        if msg.get(\"role\") == \"tool\":\n            # This is a tool result - need to include the tool_use before it\n            recent_start -= 1\n        else:\n            break\n\n    recent_messages = self._messages[recent_start:]\n\n    self._messages = system_messages + recent_messages\n    self._token_count = sum(self._estimate_tokens(m) for m in self._messages)\n\n    # Emit post-compaction event\n    await self._hooks.emit(\"context:post_compact\", {\n        \"message_count\": len(self._messages),\n        \"token_count\": self._token_count\n    })\n```\n\n### Compaction Strategies\n\nDifferent strategies for different use cases:\n\n#### Simple Truncation\n\nKeep N most recent messages (with tool pair preservation):\n\n```python\n# Find safe truncation point\nkeep_from = len(self._messages) - keep_count\n# Adjust to not split tool pairs\nwhile keep_from > 0 and self._messages[keep_from].get(\"role\") == \"tool\":\n    keep_from -= 1\nself._messages = self._messages[keep_from:]\n```\n\n#### Summarization\n\nUse LLM to summarize older messages:\n\n```python\n# Summarize old messages\nold_messages = self._messages[:-keep_recent]\nsummary = await summarize(old_messages)\n\n# Replace with summary\nself._messages = [\n    {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"},\n    *self._messages[-keep_recent:]\n]\n```\n\n#### Importance-Based\n\nKeep messages based on importance score:\n\n```python\nscored = [(m, self._score_importance(m)) for m in self._messages]\nscored.sort(key=lambda x: x[1], reverse=True)\n# Keep high-importance messages, but preserve tool pairs\nself._messages = self._reorder_preserving_tool_pairs(\n    [m for m, _ in scored[:keep_count]]\n)\n```\n\n---\n\n## Configuration\n\nContext managers receive configuration via Mount Plan:\n\n```yaml\nsession:\n  orchestrator: loop-basic\n  context: my-context\n\n# Context config can be passed via top-level config\n```\n\nSee [MOUNT_PLAN_SPECIFICATION.md](https://github.com/microsoft/amplifier-core/blob/main/specs/MOUNT_PLAN_SPECIFICATION.md) for full schema.\n\n---\n\n## Observability\n\nRegister compaction events:\n\n```python\ncoordinator.register_contributor(\n    \"observability.events\",\n    \"my-context\",\n    lambda: [\"context:pre_compact\", \"context:post_compact\"]\n)\n```\n\nStandard events to emit:\n- `context:pre_compact` - Before compaction (include message_count, token_count)\n- `context:post_compact` - After compaction (include new counts)\n\nSee [CONTRIBUTION_CHANNELS.md](https://github.com/microsoft/amplifier-core/blob/main/specs/CONTRIBUTION_CHANNELS.md) for the pattern.\n\n---\n\n## Canonical Example\n\n**Reference implementation**: [amplifier-module-context-simple](https://github.com/microsoft/amplifier-module-context-simple)\n\nStudy this module for:\n- Basic ContextManager implementation\n- Token counting approach\n- Internal compaction with tool pair preservation\n\nAdditional examples:\n- [amplifier-module-context-persistent](https://github.com/microsoft/amplifier-module-context-persistent) - File-based persistence\n\n---\n\n## Validation Checklist\n\n### Required\n\n- [ ] Implements all 5 ContextManager protocol methods\n- [ ] `mount()` function with entry point in pyproject.toml\n- [ ] `get_messages_for_request()` handles compaction internally\n- [ ] Compaction preserves tool_use/tool_result pairs\n- [ ] Messages returned in conversation order\n\n### Recommended\n\n- [ ] Token counting for accurate compaction triggers\n- [ ] Emits context:pre_compact and context:post_compact events\n- [ ] Preserves system messages during compaction\n- [ ] Thread-safe for concurrent access\n- [ ] Configurable thresholds\n\n---\n\n## Testing\n\nUse test utilities from `amplifier_core/testing.py`:\n\n```python\nfrom amplifier_core.testing import MockContextManager\n\n@pytest.mark.asyncio\nasync def test_context_manager():\n    context = MyContextManager(max_tokens=1000)\n\n    # Add messages\n    await context.add_message({\"role\": \"user\", \"content\": \"Hello\"})\n    await context.add_message({\"role\": \"assistant\", \"content\": \"Hi there!\"})\n\n    # Get messages for request (may compact)\n    messages = await context.get_messages_for_request()\n    assert len(messages) == 2\n    assert messages[0][\"role\"] == \"user\"\n\n    # Get raw messages (no compaction)\n    raw_messages = await context.get_messages()\n    assert len(raw_messages) == 2\n\n    # Test clear\n    await context.clear()\n    assert len(await context.get_messages()) == 0\n\n\n@pytest.mark.asyncio\nasync def test_compaction_preserves_tool_pairs():\n    \"\"\"Verify tool_use and tool_result stay together during compaction.\"\"\"\n    context = MyContextManager(max_tokens=100, compaction_threshold=0.5)\n\n    # Add messages including tool sequence\n    await context.add_message({\"role\": \"user\", \"content\": \"Read file.txt\"})\n    await context.add_message({\n        \"role\": \"assistant\",\n        \"content\": None,\n        \"tool_calls\": [{\"id\": \"call_123\", \"type\": \"function\", \"function\": {...}}]\n    })\n    await context.add_message({\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_123\",\n        \"content\": \"File contents...\"\n    })\n\n    # Force compaction by adding more messages\n    for i in range(50):\n        await context.add_message({\"role\": \"user\", \"content\": f\"Message {i}\"})\n\n    # Get messages for request (triggers compaction)\n    messages = await context.get_messages_for_request()\n\n    # Verify tool pairs are preserved\n    tool_use_ids = set()\n    tool_result_ids = set()\n    for msg in messages:\n        if msg.get(\"tool_calls\"):\n            for tc in msg[\"tool_calls\"]:\n                tool_use_ids.add(tc.get(\"id\"))\n        if msg.get(\"role\") == \"tool\":\n            tool_result_ids.add(msg.get(\"tool_call_id\"))\n\n    # Every tool result should have matching tool use\n    assert tool_result_ids.issubset(tool_use_ids), \"Orphaned tool results found!\"\n\n\n@pytest.mark.asyncio\nasync def test_session_resume():\n    \"\"\"Verify set_messages works for session resume.\"\"\"\n    context = MyContextManager(max_tokens=1000)\n\n    saved_messages = [\n        {\"role\": \"user\", \"content\": \"Previous conversation\"},\n        {\"role\": \"assistant\", \"content\": \"Previous response\"}\n    ]\n\n    await context.set_messages(saved_messages)\n\n    messages = await context.get_messages()\n    assert len(messages) == 2\n    assert messages[0][\"content\"] == \"Previous conversation\"\n```\n\n### MockContextManager for Testing\n\n```python\nfrom amplifier_core.testing import MockContextManager\n\n# For testing orchestrators\ncontext = MockContextManager()\n\nawait context.add_message({\"role\": \"user\", \"content\": \"Test\"})\nmessages = await context.get_messages_for_request()\n\n# Access internal state for assertions\nassert len(context.messages) == 1\n```\n\n---\n\n## Quick Validation Command\n\n```bash\n# Structural validation\namplifier module validate ./my-context --type context\n```\n\n---\n\n**Related**: [README.md](README.md) | [ORCHESTRATOR_CONTRACT.md](ORCHESTRATOR_CONTRACT.md)\n\n\n---\n\n# Appendix: Module Source Protocol\n\n# Module Source Protocol\n\n_Version: 1.0.0_\n_Layer: Kernel Mechanism_\n_Status: Specification_\n\n---\n\n## Purpose\n\nThe kernel provides a mechanism for custom module source resolution. The loader accepts an optional `ModuleSourceResolver` via mount point injection. If no resolver is provided, the kernel falls back to standard Python entry point discovery.\n\n**How modules are discovered and from where is app-layer policy.**\n\n---\n\n## Kernel Contracts\n\n### ModuleSource Protocol\n\n```python\nclass ModuleSource(Protocol):\n    \"\"\"Contract for module sources.\n\n    Implementations must resolve to a filesystem path where a Python module\n    can be imported.\n    \"\"\"\n\n    def resolve(self) -> Path:\n        \"\"\"\n        Resolve source to filesystem path.\n\n        Returns:\n            Path: Directory containing importable Python module\n\n        Raises:\n            ModuleNotFoundError: Source cannot be resolved\n            OSError: Filesystem access error\n        \"\"\"\n```\n\n**Examples of conforming implementations (app-layer):**\n\n- FileSource: Resolves local filesystem paths\n- GitSource: Clones git repos, caches, returns cache path\n- PackageSource: Finds installed Python packages\n\n**Kernel does NOT define these implementations.** They are app-layer policy.\n\n### ModuleSourceResolver Protocol\n\n```python\nclass ModuleSourceResolver(Protocol):\n    \"\"\"Contract for module source resolution strategies.\n\n    Implementations decide WHERE to find modules based on module ID and\n    optional profile hints.\n    \"\"\"\n\n    def resolve(self, module_id: str, profile_hint: Any = None) -> ModuleSource:\n        \"\"\"\n        Resolve module ID to a source.\n\n        Args:\n            module_id: Module identifier (e.g., \"tool-bash\")\n            profile_hint: Optional hint from profile configuration\n                         (format defined by app layer)\n\n        Returns:\n            ModuleSource that can be resolved to a path\n\n        Raises:\n            ModuleNotFoundError: Module cannot be found by this resolver\n        \"\"\"\n```\n\n**The resolver is app-layer policy.** Different apps may use different resolution strategies:\n\n- Development app: Check workspace, then configs, then packages\n- Production app: Only use verified packages\n- Testing app: Use mock implementations\n\n**Kernel does NOT define resolution strategy.** It only provides the injection mechanism.\n\n---\n\n## Loader Injection Contract\n\n### Module Loader API\n\n```python\nclass ModuleLoader:\n    \"\"\"Kernel mechanism for loading modules.\n\n    Accepts optional ModuleSourceResolver via coordinator mount point.\n    Falls back to direct entry-point discovery if no resolver provided.\n    \"\"\"\n\n    def __init__(self, coordinator):\n        \"\"\"Initialize loader with coordinator.\"\"\"\n        self.coordinator = coordinator\n\n    async def load(self, module_id: str, config: dict = None, profile_source = None):\n        \"\"\"\n        Load module using resolver or fallback to direct discovery.\n\n        Args:\n            module_id: Module identifier\n            config: Optional module configuration\n            profile_source: Optional source hint from profile/config\n\n        Raises:\n            ModuleNotFoundError: Module not found\n            ModuleLoadError: Module found but failed to load\n        \"\"\"\n        # Try to get resolver from mount point\n        source_resolver = None\n        if self.coordinator:\n            try:\n                source_resolver = self.coordinator.get(\"module-source-resolver\")\n            except ValueError:\n                pass  # No resolver mounted\n\n        if source_resolver is None:\n            # No resolver - use direct entry-point discovery\n            return await self._load_direct(module_id, config)\n\n        # Use resolver\n        source = source_resolver.resolve(module_id, profile_source)\n        module_path = source.resolve()\n\n        # Load from resolved path\n        # ... import and mount logic ...\n```\n\n### Mounting a Custom Resolver (App-Layer)\n\n```python\n# App layer creates resolver (policy)\nresolver = CustomModuleSourceResolver()\n\n# Mount it before creating loader\ncoordinator.mount(\"module-source-resolver\", resolver)\n\n# Loader will use custom resolver\nloader = AmplifierModuleLoader(coordinator)\n```\n\n**Kernel provides the mount point and fallback. App layer provides the resolver.**\n\n---\n\n## Kernel Responsibilities\n\n**The kernel:**\n\n- ✅ Defines ModuleSource and ModuleSourceResolver protocols\n- ✅ Accepts resolver via \"module-source-resolver\" mount point\n- ✅ Falls back to entry point discovery if no resolver\n- ✅ Loads module from resolved path\n- ✅ Handles module import and mounting\n\n**The kernel does NOT:**\n\n- ❌ Define specific resolution strategies (6-layer, configs, etc.)\n- ❌ Parse configuration files (YAML, TOML, JSON, etc.)\n- ❌ Know about workspace conventions, git caching, or URIs\n- ❌ Provide CLI commands for source management\n- ❌ Define profile schemas or source field formats\n\n---\n\n## Error Contracts\n\n### ModuleNotFoundError\n\n```python\nclass ModuleNotFoundError(Exception):\n    \"\"\"Raised when a module cannot be found.\n\n    Resolvers MUST raise this when all resolution attempts fail.\n    Loaders MUST propagate this to callers.\n\n    Message SHOULD be helpful, indicating:\n    - What module was requested\n    - What resolution attempts were made (if applicable)\n    - Suggestions for resolution (if applicable)\n    \"\"\"\n```\n\n### ModuleLoadError\n\n```python\nclass ModuleLoadError(Exception):\n    \"\"\"Raised when a module is found but cannot be loaded.\n\n    Examples:\n    - Module path exists but isn't valid Python\n    - Import fails due to missing dependencies\n    - Module doesn't implement required protocol\n    \"\"\"\n```\n\n---\n\n## Fallback Behavior\n\n### Direct Entry Point Discovery (Kernel Default)\n\nWhen no ModuleSourceResolver is mounted, the kernel falls back to direct entry point discovery via the `_load_direct()` method:\n\n1. Searches Python entry points (group=\"amplifier.modules\")\n2. Falls back to filesystem discovery (if search paths configured)\n3. Uses standard Python import mechanisms\n\n**Implementation**: The `_load_direct()` method directly calls `_load_entry_point()` and `_load_filesystem()` without creating a resolver wrapper object.\n\n**This ensures the kernel works without any app-layer resolver.**\n\n---\n\n## Example: Custom Resolver (App-Layer)\n\n**Not in kernel, but shown for clarity:**\n\n```python\n# App layer defines custom resolution strategy\nclass MyCustomResolver:\n    \"\"\"Example custom resolver (app-layer policy).\"\"\"\n\n    def resolve(self, module_id: str, profile_hint: Any = None) -> ModuleSource:\n        # App-specific logic\n        if module_id in self.overrides:\n            return FileSource(self.overrides[module_id])\n\n        # Fall back to profile hint\n        if profile_hint:\n            return self.parse_profile_hint(profile_hint)\n\n        # Fall back to some default\n        return PackageSource(f\"myapp-module-{module_id}\")\n```\n\nThis is **policy, not kernel.** Different apps can implement different strategies.\n\n---\n\n## Kernel Invariants\n\nWhen implementing custom resolvers:\n\n1. **Must return ModuleSource**: Conforming to protocol\n2. **Must raise ModuleNotFoundError**: On failure\n3. **Must not interfere with kernel**: No side effects beyond resolution\n4. **Must be deterministic**: Same inputs → same output\n\n---\n\n## Related Documentation\n\n**Kernel specifications:**\n\n- [SESSION_FORK_SPECIFICATION.md](./SESSION_FORK_SPECIFICATION.md) - Session forking contracts\n- [COORDINATOR_INFRASTRUCTURE_CONTEXT.md](./COORDINATOR_INFRASTRUCTURE_CONTEXT.md) - Mount point system\n\n**Related Specifications:**\n\n- [DESIGN_PHILOSOPHY.md](./DESIGN_PHILOSOPHY.md) - Kernel design principles\n- [MOUNT_PLAN_SPECIFICATION.md](./specs/MOUNT_PLAN_SPECIFICATION.md) - Mount plan format\n\n**Note**: Module source resolution implementation is application-layer policy. Applications may implement custom resolution strategies using the protocols defined above."
              }
            ]
          }
        ]
      }
    }
  ]
}