{
  "owner": {
    "id": "tommymorgan",
    "display_name": "Tommy Morgan",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/347420?v=4",
    "url": "https://github.com/tommymorgan",
    "bio": "Big ol' nerd",
    "stats": {
      "total_repos": 1,
      "total_plugins": 2,
      "total_commands": 9,
      "total_skills": 7,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "tommymorgan/claude-plugins",
      "url": "https://github.com/tommymorgan/claude-plugins",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-03T03:46:50Z",
        "created_at": "2025-12-10T20:10:41Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 932
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1069
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2707
        },
        {
          "path": "plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plans/2024-12-15-tommymorgan-plugin.md",
          "type": "blob",
          "size": 5816
        },
        {
          "path": "publish.sh",
          "type": "blob",
          "size": 6324
        },
        {
          "path": "test",
          "type": "tree",
          "size": null
        },
        {
          "path": "test/publish.bats",
          "type": "blob",
          "size": 10475
        },
        {
          "path": "tommymorgan",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1780
        },
        {
          "path": "tommymorgan/README.md",
          "type": "blob",
          "size": 11571
        },
        {
          "path": "tommymorgan/debugging",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/debugging/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/debugging/agents/root-cause-analyzer.md",
          "type": "blob",
          "size": 12407
        },
        {
          "path": "tommymorgan/debugging/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/debugging/commands/root-cause.md",
          "type": "blob",
          "size": 1310
        },
        {
          "path": "tommymorgan/debugging/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/debugging/skills/five-whys-methodology",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/debugging/skills/five-whys-methodology/SKILL.md",
          "type": "blob",
          "size": 8233
        },
        {
          "path": "tommymorgan/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/docs/technical-spec.md",
          "type": "blob",
          "size": 7871
        },
        {
          "path": "tommymorgan/docs/user-guide.md",
          "type": "blob",
          "size": 3415
        },
        {
          "path": "tommymorgan/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/hooks/hooks.json",
          "type": "blob",
          "size": 694
        },
        {
          "path": "tommymorgan/hooks/post-push-cleanup.py",
          "type": "blob",
          "size": 2262
        },
        {
          "path": "tommymorgan/hooks/pre_push_squash.py",
          "type": "blob",
          "size": 19177
        },
        {
          "path": "tommymorgan/hooks/resize_images.py",
          "type": "blob",
          "size": 9367
        },
        {
          "path": "tommymorgan/hooks/stop_if_incomplete.py",
          "type": "blob",
          "size": 7005
        },
        {
          "path": "tommymorgan/hooks/test_pre_push_squash.py",
          "type": "blob",
          "size": 2710
        },
        {
          "path": "tommymorgan/hooks/test_resize_images.py",
          "type": "blob",
          "size": 24835
        },
        {
          "path": "tommymorgan/hooks/test_stop_if_incomplete.py",
          "type": "blob",
          "size": 8263
        },
        {
          "path": "tommymorgan/migration",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/migration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/migration/commands/coverage-analysis.md",
          "type": "blob",
          "size": 2599
        },
        {
          "path": "tommymorgan/migration/commands/doc_generator.py",
          "type": "blob",
          "size": 11708
        },
        {
          "path": "tommymorgan/migration/commands/feature_parser.py",
          "type": "blob",
          "size": 4897
        },
        {
          "path": "tommymorgan/migration/commands/generate-docs.md",
          "type": "blob",
          "size": 2665
        },
        {
          "path": "tommymorgan/migration/commands/migrate-to-living-specs.md",
          "type": "blob",
          "size": 2456
        },
        {
          "path": "tommymorgan/migration/commands/migrate.py",
          "type": "blob",
          "size": 10359
        },
        {
          "path": "tommymorgan/migration/commands/spec_updater.py",
          "type": "blob",
          "size": 4276
        },
        {
          "path": "tommymorgan/migration/commands/test_doc_generator.py",
          "type": "blob",
          "size": 8300
        },
        {
          "path": "tommymorgan/migration/commands/test_feature_parser.py",
          "type": "blob",
          "size": 4007
        },
        {
          "path": "tommymorgan/migration/commands/test_migrate.py",
          "type": "blob",
          "size": 14466
        },
        {
          "path": "tommymorgan/migration/commands/test_spec_updater.py",
          "type": "blob",
          "size": 4148
        },
        {
          "path": "tommymorgan/planning",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/commands/plan.md",
          "type": "blob",
          "size": 9076
        },
        {
          "path": "tommymorgan/planning/commands/review-plan.md",
          "type": "blob",
          "size": 7373
        },
        {
          "path": "tommymorgan/planning/commands/review_plan.py",
          "type": "blob",
          "size": 4059
        },
        {
          "path": "tommymorgan/planning/commands/status.md",
          "type": "blob",
          "size": 3018
        },
        {
          "path": "tommymorgan/planning/commands/test_review_plan.py",
          "type": "blob",
          "size": 7524
        },
        {
          "path": "tommymorgan/planning/commands/work.md",
          "type": "blob",
          "size": 16234
        },
        {
          "path": "tommymorgan/planning/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/skills/plan-format",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/skills/plan-format/SKILL.md",
          "type": "blob",
          "size": 5553
        },
        {
          "path": "tommymorgan/planning/skills/tdd-execution",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/skills/tdd-execution/SKILL.md",
          "type": "blob",
          "size": 10856
        },
        {
          "path": "tommymorgan/planning/skills/verification-sweep",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/planning/skills/verification-sweep/SKILL.md",
          "type": "blob",
          "size": 6294
        },
        {
          "path": "tommymorgan/specs",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/specs/living-gherkin-documentation-system.feature",
          "type": "blob",
          "size": 15309
        },
        {
          "path": "tommymorgan/testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/agents/api-explorer.md",
          "type": "blob",
          "size": 7241
        },
        {
          "path": "tommymorgan/testing/agents/browser-explorer.md",
          "type": "blob",
          "size": 6869
        },
        {
          "path": "tommymorgan/testing/agents/cli-tester.md",
          "type": "blob",
          "size": 7145
        },
        {
          "path": "tommymorgan/testing/agents/exploratory-tester.md",
          "type": "blob",
          "size": 3621
        },
        {
          "path": "tommymorgan/testing/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/commands/test.md",
          "type": "blob",
          "size": 4229
        },
        {
          "path": "tommymorgan/testing/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 8066
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns/references/authentication-patterns.md",
          "type": "blob",
          "size": 4108
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns/references/openapi-parsing.md",
          "type": "blob",
          "size": 6868
        },
        {
          "path": "tommymorgan/testing/skills/api-testing-patterns/references/security-testing.md",
          "type": "blob",
          "size": 4940
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 8391
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns/references/accessibility-testing.md",
          "type": "blob",
          "size": 6758
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns/references/performance-metrics.md",
          "type": "blob",
          "size": 6169
        },
        {
          "path": "tommymorgan/testing/skills/browser-testing-patterns/references/playwright-mcp-tools.md",
          "type": "blob",
          "size": 8391
        },
        {
          "path": "tommymorgan/testing/skills/cli-testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/cli-testing-patterns/SKILL.md",
          "type": "blob",
          "size": 8528
        },
        {
          "path": "tommymorgan/testing/skills/cli-testing-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "tommymorgan/testing/skills/cli-testing-patterns/references/exit-code-standards.md",
          "type": "blob",
          "size": 5046
        },
        {
          "path": "tommymorgan/testing/skills/cli-testing-patterns/references/output-validation.md",
          "type": "blob",
          "size": 5581
        }
      ],
      "marketplace": {
        "name": "tommymorgan",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Tommy Morgan",
          "email": "tommy@tommymorgan.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "tommymorgan",
            "description": "Comprehensive development workflow: planning with expert review, autonomous TDD execution, smart testing, and root-cause debugging",
            "source": "./tommymorgan",
            "category": "workflow",
            "version": "0.3.3",
            "author": null,
            "install_commands": [
              "/plugin marketplace add tommymorgan/claude-plugins",
              "/plugin install tommymorgan@tommymorgan"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T03:46:50Z",
              "created_at": "2025-12-10T20:10:41Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/root-cause",
                "description": "Perform systematic root cause analysis using five whys methodology",
                "path": "tommymorgan/debugging/commands/root-cause.md",
                "frontmatter": {
                  "name": "tommymorgan:root-cause",
                  "description": "Perform systematic root cause analysis using five whys methodology",
                  "argument-hint": [
                    "problem-description"
                  ],
                  "allowed-tools": "Task"
                },
                "content": "Perform systematic root cause analysis on the reported problem using the root-cause-analyzer agent.\n\n**Problem Context:** $ARGUMENTS\n\n**Instructions:**\n\n1. Launch the root-cause-analyzer agent to investigate this problem\n2. The agent will autonomously perform five whys analysis without user prompts\n3. Each investigation level will be shown progressively as the agent works\n4. The agent will continue until identifying an actionable root cause\n5. After root cause is identified, proceed to implementing the fix\n\n**Agent Behavior:**\n- Gathers evidence from code, logs, and outputs\n- Asks \"why?\" iteratively based on concrete findings\n- Shows structured output for each investigation level\n- Determines when root cause criteria are met (actionable, explanatory, terminal)\n- Completes automatically when root cause is found\n\n**Your Role:**\n- Use the Task tool to launch the root-cause-analyzer agent\n- Provide the problem context to the agent\n- Wait for agent to complete root cause analysis\n- Once agent identifies root cause, proceed with the recommended fix approach\n\nDo not attempt speculative fixes before root cause is identified."
              },
              {
                "name": "/coverage-analysis",
                "description": "Analyze test coverage of living specification scenarios",
                "path": "tommymorgan/migration/commands/coverage-analysis.md",
                "frontmatter": {
                  "name": "tommymorgan:coverage-analysis",
                  "description": "Analyze test coverage of living specification scenarios",
                  "argument-hint": "[project-path]",
                  "allowed-tools": [
                    "Bash",
                    "Read",
                    "Glob"
                  ]
                },
                "content": "# Analyze Test Coverage of Living Specifications\n\nAnalyzes which scenarios in living .feature files have corresponding tests, identifying coverage gaps.\n\n## Usage\n\n```\n/tommymorgan:coverage-analysis [project-path]\n```\n\n## What This Does\n\n1. **Loads living specs** from specs/ directory\n2. **Discovers test files** in project (test_*.py, *.test.js, etc.)\n3. **Matches scenarios to tests** using fuzzy name matching\n4. **Reports coverage percentage**\n5. **Lists untested scenarios**\n\n## Process\n\n```bash\ncd $PROJECT_PATH\n\npython3 tools/claude-plugins/tommymorgan/migration/commands/doc_generator.py coverage specs/ .\n```\n\n## Output\n\n```\nTest Coverage Analysis\n\nTotal scenarios: 15\nTested: 10\nCoverage: 66.7%\n\nScenarios without tests:\n  ‚úó User resets password (authentication.feature)\n  ‚úó Admin manages permissions (user-management.feature)\n  ‚úó Export analytics to CSV (analytics.feature)\n  ‚úó Bulk import test cases (test-management.feature)\n  ‚úó API rate limiting (api.feature)\n\nScenarios with tests:\n  ‚úì User logs in successfully (test_auth.py::test_user_login)\n  ‚úì User logs out (test_auth.py::test_user_logout)\n  ‚úì JWT token generation (test_auth.py::test_jwt_generation)\n  ...\n```\n\n## Matching Algorithm\n\n**Fuzzy matching** finds tests that likely correspond to scenarios:\n\n- Converts scenario names to test naming conventions\n- Searches test file content for scenario names\n- Reports matches with test file names\n- Flags scenarios with no matches\n\n**Example matches**:\n- \"User logs in successfully\" ‚Üí test_user_login_successfully()\n- \"JWT token generation\" ‚Üí test_jwt_token_generation()\n- \"API returns 404 for missing resource\" ‚Üí test_api_404_missing_resource()\n\n## Benefits\n\n- **Identify gaps**: See which scenarios lack test coverage\n- **Track progress**: Monitor coverage as features are implemented\n- **Quality metric**: Quantify test completeness\n- **Guide testing**: Prioritize writing tests for untested scenarios\n\n## When to Use\n\n- After implementing features (verify new scenarios are tested)\n- Before releases (ensure adequate coverage)\n- During code review (check test completeness)\n- Sprint retrospectives (discuss coverage trends)\n\n## Important Notes\n\n- Uses fuzzy matching (may have false positives/negatives)\n- Only analyzes scenario name matching (not step-level coverage)\n- Requires test files follow naming conventions\n- Manual review recommended for ambiguous matches"
              },
              {
                "name": "/generate-docs",
                "description": "Generate user or developer documentation from living .feature files",
                "path": "tommymorgan/migration/commands/generate-docs.md",
                "frontmatter": {
                  "name": "tommymorgan:generate-docs",
                  "description": "Generate user or developer documentation from living .feature files",
                  "argument-hint": "[user|dev|both] [project-path]",
                  "allowed-tools": [
                    "Bash",
                    "Read",
                    "Write",
                    "Glob"
                  ]
                },
                "content": "# Generate Documentation from Living Specifications\n\nGenerate user documentation, developer documentation, or both from living .feature files.\n\n## Usage\n\n```\n/tommymorgan:generate-docs user\n/tommymorgan:generate-docs dev\n/tommymorgan:generate-docs both [project-path]\n```\n\n## What This Does\n\n**User Documentation** (@user scenarios only):\n- Filters scenarios tagged @user\n- Converts Gherkin to readable prose\n- Excludes all technical implementation details\n- Creates user-friendly guide\n\n**Developer Documentation** (@technical scenarios):\n- Filters scenarios tagged @technical\n- Includes implementation requirements\n- Shows expected system behavior\n- Documents technical specifications\n\n**Evolutionary Updates**:\n- Preserves existing introduction and non-scenario content\n- Updates only changed scenarios\n- Maintains documentation voice and style\n- Keeps custom examples and diagrams\n\n## Process\n\n```bash\ncd $PROJECT_PATH\n\n# Generate user docs\npython3 tools/claude-plugins/tommymorgan/migration/commands/doc_generator.py generate specs/ docs/user-guide.md --tags @user\n\n# Generate developer docs\npython3 tools/claude-plugins/tommymorgan/migration/commands/doc_generator.py generate specs/ docs/technical-spec.md --tags @technical\n```\n\n## Output\n\n**User Documentation** (docs/user-guide.md):\n```markdown\n# User Guide\n\n## Introduction\n(Preserved from existing docs)\n\n## Authentication\n\n### Logging In\nUser logs in successfully: i am on the login page, i enter valid credentials, i am redirected to dashboard.\n\n### Resetting Password\nUser resets forgotten password: i click forgot password, i enter my email, i receive reset link.\n```\n\n**Developer Documentation** (docs/technical-spec.md):\n```markdown\n# Technical Specification\n\n## Authentication API\n\n### JWT Token Generation\nJWT token generation: user authenticates successfully, auth service processes request, jwt token is issued with user claims, token expires in 24 hours.\n```\n\n## Benefits\n\n- **Always accurate**: Generated from living specs (source of truth)\n- **No duplication**: One source for both user and dev docs\n- **Test-aligned**: Docs match what tests verify\n- **Evolutionary**: Preserves manual content while updating scenarios\n- **Accessible**: Generates semantic HTML when needed\n\n## When to Use\n\n- After implementing new features (living specs updated)\n- Before releases (refresh documentation)\n- When onboarding new team members\n- To verify docs match current functionality"
              },
              {
                "name": "/migrate-to-living-specs",
                "description": "One-time migration to create living .feature files from historical plans",
                "path": "tommymorgan/migration/commands/migrate-to-living-specs.md",
                "frontmatter": {
                  "name": "tommymorgan:migrate-to-living-specs",
                  "description": "One-time migration to create living .feature files from historical plans",
                  "argument-hint": "[project-path]",
                  "allowed-tools": [
                    "Bash",
                    "Read",
                    "Write",
                    "Glob"
                  ]
                },
                "content": "# Migrate Historical Plans to Living Specifications\n\nOne-time migration that creates living .feature files from historical plan files.\n\n## Usage\n\n```\n/tommymorgan:migrate-to-living-specs [project-path]\n```\n\nIf no path provided, uses current project (inferred from working directory).\n\n## What This Does\n\n1. **Scans plans/** directory for historical plan files\n2. **Extracts Gherkin scenarios** from User Requirements and Technical Specifications\n3. **Groups scenarios by feature** area using semantic similarity\n4. **Tags scenarios** appropriately (@user, @technical)\n5. **Creates .feature files** in specs/ directory\n6. **Reports progress** with success/failure counts\n\n## Process\n\n```bash\ncd $PROJECT_PATH\n\n# Run migration tool\npython3 tools/claude-plugins/tommymorgan/migration/commands/migrate.py .\n\n# Results will be in specs/ directory\nls specs/*.feature\n```\n\n## Output\n\nCreates `.feature` files in `<project>/specs/` directory:\n\n```\nspecs/\n  authentication.feature\n  user-management.feature\n  analytics.feature\n```\n\nEach .feature file contains:\n```gherkin\nFeature: <Feature Name>\n\n  @user\n  Scenario: User-facing behavior\n    Given user context\n    When user action\n    Then user outcome\n\n  @technical\n  Scenario: Technical requirement\n    Given system state\n    When technical action\n    Then technical outcome\n```\n\n## Summary Report\n\nAfter completion, shows:\n```\nMigration complete!\n\nProcessed: 15 plans\nSuccesses: 12\nFailures: 3\nCreated: 5 .feature files\n\nErrors:\n- plan-old.md: No scenarios found\n- plan-broken.md: Parse error\n- plan-incomplete.md: No Gherkin blocks\n\nCheck specs/ directory for living specifications.\n```\n\n## When to Use\n\n**Run this ONCE** to bootstrap living documentation from historical plans.\n\nAfter migration:\n- New plans will reference existing living specs (interactive reconciliation)\n- Work tool will automatically update living specs\n- Never run migration again (living specs are maintained going forward)\n\n## Important Notes\n\n- Historical plans remain unchanged (valuable record)\n- Semantic similarity groups related scenarios (0.8 threshold)\n- Processes in batches for memory efficiency\n- Continues on errors, reports all issues at end\n- Scenarios below 0.8 similarity flagged for manual review"
              },
              {
                "name": "/plan",
                "description": "Create a new feature plan via brainstorming with Gherkin requirements",
                "path": "tommymorgan/planning/commands/plan.md",
                "frontmatter": {
                  "name": "tommymorgan:plan",
                  "description": "Create a new feature plan via brainstorming with Gherkin requirements",
                  "argument-hint": "feature description",
                  "allowed-tools": [
                    "Task",
                    "Skill",
                    "Read",
                    "Write",
                    "Bash",
                    "Glob",
                    "Grep",
                    "AskUserQuestion"
                  ]
                },
                "content": "# Create Feature Plan\n\nCreate a comprehensive feature plan through collaborative brainstorming, generating Gherkin scenarios with expert review and quality assurance.\n\n## Workflow\n\n### Step 1: Invoke Brainstorming\n\nUse the Skill tool to invoke the brainstorming skill:\n\n```\nSkill(\"superpowers:brainstorming\")\n```\n\nBrainstorm the feature described by the user: $ARGUMENTS\n\nAsk questions one at a time to understand:\n- What problem does this solve?\n- Who are the users?\n- What are the success criteria?\n- What are the constraints?\n\nExplore 2-3 approaches and recommend one.\n\n### Step 2: Determine Project Location\n\nInfer the project location from context:\n- Current working directory\n- Recently accessed files\n- Conversation context\n\nProjects live in `apps/`, `libs/`, or `tools/` directories.\n\nIf ambiguous, use AskUserQuestion to ask:\n\"Which project does this feature belong to?\"\nOptions: List detected projects from apps/, libs/, tools/\n\n### Step 3: Generate Gherkin Requirements\n\nOnce the design is clear, write Gherkin scenarios separated into two sections:\n\n**User Requirements** (language/framework agnostic):\n```gherkin\nScenario: <user-focused behavior>\n  Given <user context>\n  When <user action>\n  Then <user outcome>\n```\n\n**Technical Specifications** (implementation details):\n```gherkin\nScenario: <technical requirement>\n  Given <system state>\n  When <technical action>\n  Then <technical outcome>\n```\n\nCover:\n- Happy paths\n- Error cases\n- Edge cases\n- Security considerations\n- Accessibility (table stakes)\n- Performance (table stakes)\n\n### Step 4: Interactive Reconciliation with Living Specs\n\n**Check for existing living specs:**\n\n```bash\n# Check if project has specs/ directory\nif [ -d \"<project>/specs\" ]; then\n  # Load existing .feature files\n  ls <project>/specs/*.feature 2>/dev/null\nfi\n```\n\n**If living specs exist:**\n\nFor each drafted scenario (both User Requirements and Technical Specifications):\n\n1. **Load existing scenarios** from all .feature files in specs/\n2. **Find similar scenarios** using semantic similarity (from migration tool):\n   - Compare scenario names and steps\n   - Use 0.8 similarity threshold\n   - Show matches to user\n\n3. **Present reconciliation options** using AskUserQuestion:\n   ```\n   Your drafted scenario: \"User logs in successfully\"\n\n   Found similar scenario in specs/authentication.feature:\n     \"User authenticates with valid credentials\"\n     Similarity: 0.85\n\n   How should this scenario relate to the living spec?\n\n   Options:\n   - creates: This is a new, independent scenario\n   - replaces: This completely replaces the existing scenario\n   - extends: This adds to the existing scenario\n   - deprecates: This marks the old scenario as obsolete\n   - none: These are not related (use different name)\n   ```\n\n4. **Record metadata** as comments before each scenario:\n   ```gherkin\n   # Living: <project>/specs/<file>.feature::<scenario-name>\n   # Action: creates|replaces|extends|removes|deprecates\n   # Status: TODO\n   # Living updated: NO\n   Scenario: <scenario-text>\n   ```\n\n**If no living specs exist:**\n\nSkip reconciliation - all scenarios will have:\n```gherkin\n# Living: none (initial implementation)\n# Action: creates\n# Status: TODO\n# Living updated: NO\nScenario: <scenario-text>\n```\n\n**Benefits:**\n- Maintains consistent naming across plans\n- Makes relationships explicit\n- Work tool knows exactly what to update\n- Prevents duplicate/conflicting scenarios\n\n### Step 5: Automated Quality Checklist\n\nBefore expert review, validate plan against checklist:\n\n1. **Language/Framework Agnostic User Scenarios**\n   - No mention of React, SQL, specific frameworks in User Requirements\n   - User scenarios describe outcomes, not implementation\n\n2. **Version Verification**\n   - Check actual versions from project files:\n     - package.json for Node/npm versions\n     - pyproject.toml/requirements.txt for Python\n     - docker-compose.yml for database versions\n   - Use correct versions in Technical Specifications\n\n3. **No Optional Items**\n   - Everything in plan meets quality bar\n   - No \"nice to have\" or \"optional\" markers\n\n4. **Table Stakes Included**\n   - Accessibility considerations present\n   - Performance considerations present\n\n### Step 6: Expert Review Panel\n\nInvoke expert reviewers to validate and improve the plan. Experts debate conflicts autonomously and reach consensus.\n\n**Panel Members:**\n\n1. **Marty Cagan** (Product Management)\n   - Reviews: User Requirements only\n   - Focus: Product value, user outcomes, completeness of user paths\n   - Pushback: Technical details in user scenarios\n\n2. **Dave Farley** (Continuous Delivery)\n   - Reviews: User Requirements AND Technical Specifications\n   - Focus: Testability, automation, CD practices, deployability\n\n3. **OWASP Security Expert**\n   - Reviews: Technical Specifications\n   - Focus: Auth/authz, input validation, data protection, threat modeling\n\n4. **Jakob Nielsen** (UX/Accessibility)\n   - Reviews: User Requirements\n   - Focus: Usability depth, accessibility beyond compliance, interaction patterns\n\n5. **Martin Kleppmann** (Data-Intensive Systems)\n   - Reviews: Technical Specifications\n   - Focus: Scalability, data architecture, performance design\n\n6. **Eric Evans** (Domain-Driven Design)\n   - Reviews: User Requirements AND Technical Specifications\n   - Focus: Domain modeling, business logic, bounded contexts\n\n7. **Google SRE**\n   - Reviews: Technical Specifications\n   - Focus: Observability, monitoring, failure modes, operational concerns\n\n**Review Process:**\n\n1. Each expert reviews their assigned sections\n2. Experts identify issues and suggest improvements\n3. When experts disagree, they debate and reach consensus\n4. No user intervention - experts resolve conflicts autonomously\n5. Plan is refined based on consensus\n\n**Implementation:**\n```\nReview each section with appropriate experts\nCollect feedback\nResolve conflicts through expert debate\nRefine plan based on consensus\n```\n\nInstead of separate task lists, use inline comments to track scenario progress:\n\n```gherkin\n<!-- TODO -->\nScenario: User logs in successfully\n  Given I am on the login page\n  When I enter valid credentials\n  Then I am redirected to dashboard\n\n<!-- DONE -->\nScenario: Invalid credentials show error\n  Given I am on the login page\n  When I enter invalid credentials\n  Then I see an error message\n```\n\nScenarios ARE the plan. Tests prove scenarios are satisfied. No duplicate tracking needed.\n\n### Step 8: Local Development Environment Check\n\nBefore finalizing plan, verify local dev environment exists or add setup scenarios:\n\n1. **Intelligently detect local dev** (apply good judgment):\n   - Container orchestration (Docker, Podman, compose files)\n   - Dev server configs (package.json scripts, Makefile, justfile)\n   - Environment config patterns (.env files, config templates)\n   - Database setup (connection configs, migration scripts)\n   - Project-specific tooling\n\n2. **If local dev missing/incomplete**, add setup scenarios:\n   - Generate appropriate scenarios for the specific tech stack\n   - Not prescriptive - adapt to project needs\n   - Must be comprehensive - broken local dev blocks all work\n\n3. **Validation**:\n   - Local dev scenarios come first\n   - Work command will verify local dev works before starting\n\n### Step 9: Write Plan File\n\nCreate the plan file at:\n`<project>/plans/YYYY-MM-DD-<slug>.md`\n\nWhere:\n- `<project>` is the inferred project path (e.g., `apps/web`, `libs/shared`)\n- `YYYY-MM-DD` is today's date\n- `<slug>` is a kebab-case version of the feature name\n\nPlan file format:\n```markdown\n# Feature: <title>\n\n**Created**: YYYY-MM-DD\n**Goal**: <one-sentence user-facing outcome>\n\n## User Requirements\n\n<!-- TODO -->\nScenario: <user-focused behavior>\n  Given <user context>\n  When <user action>\n  Then <user outcome>\n\n## Technical Specifications\n\n<!-- TODO -->\nScenario: <technical requirement>\n  Given <system state>\n  When <technical action>\n  Then <technical outcome>\n\n## Notes\n\n<design decisions, architectural choices, constraints, context>\n```\n\nCreate the plans directory if it doesn't exist:\n```bash\nmkdir -p <project>/plans\n```\n\n### Step 10: Report Completion\n\nAfter creating the plan, report:\n\n```\nPlan created: <path to plan file>\n\nExpert Review Summary:\n- All 7 experts reviewed and approved\n- [List any major changes from review]\n\nReady to work. Run /tommymorgan:work to begin.\n```\n\n## Important Notes\n\n- Never skip brainstorming, even for \"simple\" features\n- Expert review is mandatory - all conflicts must be resolved\n- Automated checklist must pass before expert review\n- User Requirements must be language/framework agnostic\n- Technical Specifications use actual project versions\n- Scenarios ARE the plan - no separate task lists\n- Local dev must be verified/planned before implementation\n- The plan file is the single source of truth for all future sessions"
              },
              {
                "name": "/review-plan",
                "description": "Independent plan review with context-aware domain experts providing prioritized recommendations",
                "path": "tommymorgan/planning/commands/review-plan.md",
                "frontmatter": {
                  "name": "tommymorgan:review-plan",
                  "description": "Independent plan review with context-aware domain experts providing prioritized recommendations",
                  "argument-hint": "<path/to/plan.md>",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Bash"
                  ]
                },
                "content": "# Review Plan with Expert Panel\n\nConduct comprehensive plan review using 7 domain experts. Experts debate recommendations and provide context-aware, prioritized feedback.\n\n## Workflow\n\n### Step 1: Load and Analyze Plan\n\nRead the plan file from the argument:\n```\n$ARGUMENTS\n```\n\nIf no argument provided, find plan using same logic as `/tommymorgan:status`.\n\nParse the plan to extract:\n- Goal and Created date\n- User Requirements section\n- Technical Specifications section\n- TODO/DONE scenario counts\n- Plan context (detect keywords: API, hook, CLI, database, UI, etc.)\n\n### Step 2: Detect Plan Context\n\nScan plan content for keywords to categorize the plan type:\n\n**Keywords ‚Üí Categories:**\n- `api, endpoint, http, rest, jwt` ‚Üí backend_service\n- `hook, bash, cli, git push, pre-push` ‚Üí hook\n- `web, ui, page, component, dashboard, form` ‚Üí ui_component\n- `database, schema, migration, table, postgres` ‚Üí database_migration\n\nPlans may have multiple categories (e.g., full-stack = backend + ui + database).\n\nStore detected categories - experts will use this to filter relevance.\n\n### Step 3: Simulate Expert Reviews\n\nInvoke 7 domain experts to review the plan. Each expert:\n1. Reviews their relevant section (User Requirements or Technical Specifications)\n2. Considers detected plan context\n3. Provides recommendations with priority (Critical/High/Medium)\n4. Self-filters based on relevance to context\n\n**Expert Panel:**\n\n#### 1. Marty Cagan (Product Strategy)\n- **Focus**: User Requirements section\n- **Reviews for**: User outcomes vs implementation details, product value, user scenarios\n- **Flags**: Technical details leaking into user scenarios\n- **Output**: Product-focused recommendations\n\n#### 2. Dave Farley (Continuous Delivery)\n- **Focus**: Both sections\n- **Reviews for**: Testability, automation, CD anti-patterns (manual gates, approval steps)\n- **Flags**: Deployment blockers, manual processes\n- **Output**: Automation and deployability recommendations\n\n#### 3. OWASP Security Expert\n- **Focus**: Technical Specifications\n- **Context-aware filtering**:\n  - If context = \"hook\" (not \"api\"): Focus on command injection, input validation, path validation\n  - If context = \"hook\" (not \"api\"): Skip API auth/authz, rate limiting, CORS\n  - Other experts validate relevance\n- **Output**: Security recommendations relevant to plan context\n\n#### 4. Jakob Nielsen (Usability)\n- **Focus**: User Requirements\n- **Reviews for**: User experience, clarity, error messaging\n- **Flags**: Multiple prompts, unclear errors, UX anti-patterns\n- **Output**: Usability improvements\n\n#### 5. Martin Kleppmann (Distributed Systems)\n- **Focus**: Technical Specifications\n- **Context-aware filtering**:\n  - If context = \"cli_tool\" or \"hook\" (not \"data_intensive_system\"): Acknowledge limited applicability\n  - Other experts flag if scalability recommendations are overkill\n  - Adjust to relevant performance concerns only\n- **Output**: Performance recommendations appropriate to scale\n\n#### 6. Eric Evans (Domain-Driven Design)\n- **Focus**: Both sections\n- **Reviews for**: Domain modeling, ubiquitous language consistency\n- **Flags**: Mismatched terminology, unclear domain concepts\n- **Output**: Domain model improvements\n\n#### 7. Google SRE Expert\n- **Focus**: Technical Specifications\n- **Context-aware filtering**:\n  - If context = \"hook\" (not \"production_service\"): Minimal observability needs\n  - Other experts flag when monitoring/alerting is excessive\n  - Adjust to: basic logging, error handling only\n  - Skip: metrics, dashboards, alerting, SLOs\n- **Output**: Operational recommendations appropriate to context\n\n### Step 4: Simulate Expert Debates\n\nWhen experts have conflicting recommendations:\n\n1. **Present disagreement**: Expert A states position with reasoning\n2. **Counter-argument**: Expert B presents alternative view\n3. **Other experts weigh in**: Additional perspectives from relevant experts\n4. **Reach consensus**: Document agreed-upon approach with rationale\n\n**Example Debate Structure:**\n```\n**Debate: Stop Hook Blocking**\n\nFarley: \"Blocking stop violates CD principles - creates manual gates\"\nCagan: \"This is internal quality control, not deployment blocking\"\nEvans: \"The domain is work session completion, not deployment pipeline\"\n\n**Consensus**: Acceptable with override mechanism (TOMMYMORGAN_ALLOW_INCOMPLETE_STOP)\n**Reasoning**: Internal quality gate with escape hatch balances discipline and flexibility\n```\n\n### Step 5: Generate Prioritized Recommendations\n\nGroup all expert recommendations by priority:\n\n**Critical Issues**: Must address before implementation\n**High Priority**: Strongly recommended improvements\n**Medium Priority**: Nice-to-have enhancements\n\nEach recommendation includes:\n- Expert name\n- Specific scenario references\n- Reasoning\n- Suggested improvement\n- Priority level\n\n### Step 6: Output Structured Review\n\nGenerate markdown-formatted review:\n\n```markdown\n# Plan Review: <plan filename>\n\n## Summary\n- **Plan Type**: <detected contexts>\n- **Goal**: <extracted goal>\n- **Scenarios**: <total> total (<user_req> User Requirements, <tech_spec> Technical Specifications)\n- **Completion**: <done_count>/<total> (<percentage>% DONE)\n\n## Critical Issues\n\n### [Expert Name] <Issue Title>\n**Scenario Reference**: <which scenario(s)>\n\n<Description of issue>\n\n**Recommendation**: <specific suggestion>\n\n**Reasoning**: <why this matters>\n\n---\n\n## High Priority\n\n### [Expert Name] <Issue Title>\n...\n\n---\n\n## Medium Priority\n\n### [Expert Name] <Issue Title>\n...\n\n---\n\n## Expert Debates & Consensus\n\n### <Debate Topic>\n\n**Disagreement**:\n- [Expert A]: <position>\n- [Expert B]: <counter-position>\n- [Expert C]: <additional perspective>\n\n**Consensus Reached**: <agreed approach>\n\n**Reasoning**: <why this approach was chosen>\n\n---\n\n## Recommendations Summary\n\n**Total Recommendations**: <count>\n- Critical: <count>\n- High Priority: <count>\n- Medium Priority: <count>\n\n**Next Steps**:\n1. Address Critical issues before implementation\n2. Consider High Priority improvements\n3. Evaluate Medium Priority enhancements based on time/scope\n4. Re-run `/tommymorgan:review-plan` after updates to verify improvements\n```\n\n### Step 7: Performance Budget\n\nComplete all expert reviews and output within **30 seconds**.\n\nProvide progress feedback during execution:\n```\nAnalyzing plan context...\nInvoking Marty Cagan review...\nInvoking Dave Farley review...\nSimulating expert debates...\nGenerating prioritized recommendations...\n```\n\nIf timeout approached, gracefully complete with partial review and note which experts didn't finish.\n\n## Important Notes\n\n- **Context-awareness is critical**: Experts must self-filter based on plan type\n- **Other experts validate**: If one expert suggests something irrelevant, others flag it\n- **Debates add value**: Show reasoning when experts disagree, not just final answer\n- **Prioritization helps**: User should know what to tackle first\n- **This is advisory**: User manually applies recommendations, not automatic\n- **Re-runnable**: User can update plan and re-run to verify improvements\n- **Focus on outcomes**: Experts should reference specific scenarios and suggest concrete improvements"
              },
              {
                "name": "/status",
                "description": "Check scenario completion status in a plan and report progress",
                "path": "tommymorgan/planning/commands/status.md",
                "frontmatter": {
                  "name": "tommymorgan:status",
                  "description": "Check scenario completion status in a plan and report progress",
                  "argument-hint": "[optional: path/to/plan.md]",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# Check Plan Status\n\nParse plan file scenarios and report completion status. Shows TODO vs DONE scenarios and overall progress.\n\n## Workflow\n\n### Step 1: Find Plan File\n\nIf a path is provided as argument, use it:\n```\n$ARGUMENTS\n```\n\nIf no path provided, find the plan file:\n\n1. Check current directory for `plans/*.md` or `docs/plans/*.md`\n2. Check parent directories\n3. Look for most recent plan file by date in filename (YYYY-MM-DD pattern)\n\nIf multiple plans found and ambiguous, list them and ask user to specify.\n\n### Step 2: Read and Parse Plan\n\nRead the plan file and extract scenarios from both sections:\n\n**User Requirements section:**\n```gherkin\n<!-- TODO -->\nScenario: <description>\n  Given <context>\n  When <action>\n  Then <outcome>\n\n<!-- DONE -->\nScenario: <description>\n  Given <context>\n  When <action>\n  Then <outcome>\n```\n\n**Technical Specifications section:**\nSame format as User Requirements.\n\n### Step 3: Count Scenarios\n\nParse the plan and count:\n- Total TODO scenarios (in both sections)\n- Total DONE scenarios (in both sections)\n- User Requirements TODO/DONE\n- Technical Specifications TODO/DONE\n\nExtract scenario titles (the \"Scenario: <title>\" line).\n\n### Step 4: Report Status\n\nReport current progress:\n\n```\n## Plan Status: <plan filename>\n\n**Progress**: X/Y scenarios complete (Z% done)\n\n### User Requirements\n- ‚úÖ <DONE scenario title>\n- ‚úÖ <DONE scenario title>\n- ‚è≥ <TODO scenario title>\n\n### Technical Specifications\n- ‚úÖ <DONE scenario title>\n- ‚è≥ <TODO scenario title>\n- ‚è≥ <TODO scenario title>\n\n**Next scenario**: <first TODO scenario title>\n```\n\nIf all scenarios DONE:\n```\n## Plan Complete! üéâ\n\nAll Y scenarios implemented and marked DONE.\n\nUser Requirements: X/X complete\nTechnical Specifications: Y/Y complete\n\nFeature ready for deployment!\n```\n\nIf no TODO scenarios remain but work seems incomplete:\n```\n## Status Check\n\nAll scenarios marked DONE.\n\nConsider:\n- Has exploratory testing been run?\n- Have all quality gates passed?\n- Is documentation up to date?\n- Ready to squash commits and finalize?\n```\n\n### Step 5: Show Next Steps\n\nBased on status, suggest next action:\n\n**If TODO scenarios exist:**\n```\n**Next action**: Run /tommymorgan:work to implement next scenario\n```\n\n**If all DONE but not finalized:**\n```\n**Next action**: Ensure quality gates passed, then squash commits\n```\n\n**If complete:**\n```\n**Next action**: Feature ready! Consider running /tommymorgan:test for final validation\n```\n\n## Important Notes\n\n- Scenarios ARE the plan - no separate verification commands\n- TODO/DONE comments are the source of truth\n- Status command is read-only - doesn't change plan file\n- Work command updates TODO ‚Üí DONE as scenarios are implemented\n- Use this command frequently to check progress\n- Parsing looks for exact comment format: `<!-- TODO -->` or `<!-- DONE -->`"
              },
              {
                "name": "/work",
                "description": "Execute plan autonomously with TDD, code review, and exploratory testing gates until complete",
                "path": "tommymorgan/planning/commands/work.md",
                "frontmatter": {
                  "name": "tommymorgan:work",
                  "description": "Execute plan autonomously with TDD, code review, and exploratory testing gates until complete",
                  "argument-hint": "[optional: path/to/plan.md]",
                  "allowed-tools": [
                    "Task",
                    "Read",
                    "Write",
                    "Edit",
                    "Bash",
                    "Glob",
                    "Grep",
                    "TodoWrite",
                    "AskUserQuestion"
                  ]
                },
                "content": "# Execute Plan Autonomously with Quality Gates\n\nWork through all scenarios in a plan using TDD with comprehensive quality gates. Continue autonomously until complete or blocked.\n\n## Core Principles\n\n- **Bulldog Persistence**: Don't give up or take shortcuts that create tech debt\n- **Border Collie Intelligence**: Apply excellent judgment and autonomy\n- **Root-Cause Driven**: Use `/tommymorgan:root-cause` instead of speculation\n- **Quality Gates**: Code review and exploratory testing before completion\n- **Documentation First-Class**: Update docs alongside code\n\n## Completion Criteria\n\n**The work command ONLY stops when ALL three conditions are met:**\n\n1. **All scenarios DONE**: Every scenario in plan marked `<!-- DONE -->`\n2. **All tests passing**: Full test suite passes in local development\n3. **All layers functional**: Every application layer validated and working\n\n**Before ANY potential stopping point, check completion criteria:**\n```\nfunction checkCompletionCriteria():\n  scenarios_done = all scenarios marked DONE in plan\n  tests_passing = test suite exits 0 in local dev\n  layers_functional = all detected layers validated successfully\n\n  return scenarios_done AND tests_passing AND layers_functional\n```\n\n**If `checkCompletionCriteria()` returns false ‚Üí continue execution**\n**If `checkCompletionCriteria()` returns true ‚Üí report completion and stop**\n\n**NEVER:**\n- Ask \"should I continue?\"\n- Mention token usage as stopping reason\n- Ask \"what should I do next?\"\n- Say \"this is separate work\"\n- Rationalize incomplete work as \"done\"\n\n## Workflow\n\n### Step 1: Initialization\n\n**Ask about workspace:**\n\nUse AskUserQuestion to ask:\n```\n\"Should I work in a worktree or directly on trunk?\"\nOptions:\n- \"Create worktree at ~/src/worktrees/<repo>-<branch>\" (Recommended for isolation)\n- \"Work directly on main/master/trunk\" (Faster for small changes)\n```\n\nIf worktree selected:\n- Create at `~/src/worktrees/<repo-name>-<branch-name>/`\n- Example: `~/src/worktrees/homelab-feat-plugin-consolidation/`\n- Clean, short paths to avoid context window issues\n\n**Read and understand plan:**\n- Load plan file\n- Parse User Requirements section\n- Parse Technical Specifications section\n- Understand all scenarios\n\n**Detect application architecture:**\n```\nfunction detectArchitecture():\n  layers = []\n\n  // Check project files\n  if exists(package.json) ‚Üí examine dependencies for: databases, APIs, UI frameworks, build tools\n  if exists(pyproject.toml or requirements.txt) ‚Üí examine for: databases, web frameworks, CLI tools\n\n  // Check directory structure\n  if exists(src/db or migrations or schema) ‚Üí add 'database' layer\n  if exists(src/api or routes or controllers) ‚Üí add 'api' layer\n  if exists(src/ui or components or views) ‚Üí add 'ui' layer\n  if exists(src/cli or __main__.py or bin/) ‚Üí add 'cli' layer\n\n  // Check plan scenarios for mentioned layers\n  scan scenarios for keywords: \"database\", \"API\", \"UI\", \"CLI\", \"build\", \"library\"\n\n  // Determine build vs library\n  if has build process (vite, webpack, tsc, etc.) ‚Üí add 'build-output' layer\n  if exports modules (library) ‚Üí add 'library' layer\n\n  return layers\n```\n\nStore detected layers for validation later.\n\n### Step 2: Verify Local Development Environment\n\n**CRITICAL**: All work must be done in local dev environment.\n\n**Intelligently verify based on project type:**\n- Container orchestration running (Podman, Docker)\n- Databases accessible (PostgreSQL, Redis, etc.)\n- Dev server can start\n- Environment variables configured\n- Dependencies installed\n\n**If verification fails:**\n```\nSTOP: Local dev environment not ready\n\nIssues found:\n- <specific problems>\n\nSee plan for local dev setup scenarios.\nRun those first, then resume work.\n```\n\n**Only proceed if local dev works.**\n\n### Step 3: Check Scenario Status\n\nRead plan file and identify scenarios:\n- Count TODO scenarios\n- Count DONE scenarios\n- Identify next TODO scenario\n\nIf all scenarios DONE:\n```\nPlan complete! All scenarios implemented and tested.\n```\nStop here.\n\nIf scenarios remain, continue to Step 4.\n\n### Step 4: Select Next Scenario\n\nPick the first TODO scenario (in order: User Requirements first, then Technical Specifications).\n\nUse TodoWrite to track implementation of this scenario.\n\n### Step 5: TDD Implementation\n\n**Red-Green-Refactor for this scenario:**\n\n1. **Write failing tests** that prove scenario is satisfied\n   - Tests describe behavior from Gherkin Given/When/Then\n   - Meaningful test titles\n   - Run tests to confirm they fail\n\n2. **Implement to make tests pass**\n   - Write minimal code\n   - Follow Gherkin as specification\n   - **Update documentation** (user, developer, API docs as needed)\n   - Commit incrementally (each green cycle)\n\n3. **Update living specification** (automatic):\n   - Parse scenario metadata from plan:\n     ```gherkin\n     # Living: <project>/specs/<file>.feature::<scenario-name>\n     # Action: creates|replaces|extends|removes|deprecates\n     # Status: TODO\n     # Living updated: NO\n     ```\n   - If `Living:` is not \"none\":\n     - Load the .feature file\n     - Apply the action:\n       - **creates**: Append new scenario to file (preserve @user/@technical tag)\n       - **replaces**: Find and replace existing scenario completely\n       - **extends**: Add new Given/When/Then steps to existing scenario\n       - **removes**: Delete scenario from file\n       - **deprecates**: Add @deprecated tag and comment\n     - Update plan metadata:\n       ```gherkin\n       # Status: DONE\n       # Living updated: YES\n       ```\n     - Commit living spec update\n   - Enforce strict sequential:\n     - Before starting next scenario, verify current has `Living updated: YES`\n     - If NO, halt with error\n\n4. **If blocked after reasonable attempts:**\n   - **NEVER guess at fixes**\n   - **ALWAYS invoke `/tommymorgan:root-cause`:**\n     ```typescript\n     Task({\n       subagent_type: \"root-cause-analysis:root-cause-analyzer\",\n       description: \"Analyze failure: <scenario>\",\n       prompt: `Root cause analysis:\n\n       Scenario: <Gherkin scenario>\n       Error: <what's failing>\n       Attempts: <what was tried>\n\n       Use five-whys methodology.\n       Provide evidence-based analysis.`\n     })\n     ```\n   - Apply fix based on root cause\n   - If unfixable, mark scenario blocked\n\n5. **Refactor** if needed\n\n### Step 6: Code Review Gate\n\nBefore marking scenario complete, get code review:\n\n```typescript\nTask({\n  subagent_type: \"pr-review-toolkit:code-reviewer\",\n  description: \"Review scenario implementation\",\n  prompt: `Review implementation:\n\nScenario: <Gherkin scenario text>\n\nFocus on:\n- Does implementation satisfy the scenario?\n- Code quality and best practices\n- Security considerations\n- Documentation accuracy and completeness\n- Test quality\n\nChanged files: <list changed files>\n\nRespond: APPROVED or NEEDS_CHANGES with details.`\n})\n```\n\n**If NEEDS_CHANGES:**\n1. Invoke `/tommymorgan:root-cause` to analyze feedback\n2. Address feedback based on root cause analysis\n3. Re-run tests\n4. Request another review\n5. If still NEEDS_CHANGES, repeat steps 1-4\n6. After 3 root-cause-driven iterations: Block and require user intervention\n\n**Hooks will run automatically:**\n- Linting\n- Type checking\n- Formatting\n\n### Step 7: Check Progress\n\n**After code review approves:**\n\nRun `checkCompletionCriteria()`:\n- All scenarios DONE?\n- All tests passing?\n- All layers functional?\n\n**If ANY condition is false:**\n- Identify what's incomplete\n- **More scenarios TODO?** ‚Üí Go to Step 4 (next scenario)\n- **Tests not run/failing?** ‚Üí Run tests, fix failures, retry\n- **Layers not validated?** ‚Üí Continue to Step 8\n\n**If ALL conditions true:**\n- Skip to Step 10 (already complete)\n\n**DO NOT ask user - continue autonomously**\n\n### Step 8: Layer Validation Gate\n\n**Before exploratory testing**, validate all detected layers:\n\n```\nfunction validateLayers(detected_layers):\n  for each layer in detected_layers:\n    validateLayer(layer)\n\n  validateIntegration(detected_layers)\n\nfunction validateLayer(layer_type):\n  switch layer_type:\n    case 'database':\n      - Verify migrations applied (check migration status)\n      - Execute sample queries (SELECT, INSERT if appropriate)\n      - Confirm schema matches expected structure\n\n    case 'api':\n      - Start API server in local dev\n      - Test endpoints respond with correct status codes\n      - Verify response data structure and content\n      - Test error cases return appropriate errors\n\n    case 'ui':\n      - Start UI dev server\n      - Verify pages/components render\n      - Test interactivity (clicks, forms, navigation)\n      - Verify data displays correctly\n\n    case 'cli':\n      - Execute command with valid inputs\n      - Verify expected output\n      - Test error cases\n      - Confirm exit codes correct\n\n    case 'library':\n      - Verify exports are accessible\n      - Test imports in sample code\n      - Confirm types/interfaces work\n\n    case 'build-output':\n      - Run build process\n      - Verify build succeeds (exit 0)\n      - Confirm artifacts generated\n      - Check artifacts are valid\n\nfunction validateIntegration(layers):\n  // Identify integration points and verify with concrete methods\n\n  if 'database' in layers and 'api' in layers:\n    - Start API server in local dev\n    - Execute API endpoint that queries database\n    - Verify API response contains expected database data\n    - Check database query logs show correct SQL execution\n    - Confirm data transformation from DB format to API format\n\n  if 'api' in layers and 'ui' in layers:\n    - Start both API and UI servers in local dev\n    - Navigate UI to page that fetches from API\n    - Verify network requests show API calls (browser DevTools or logs)\n    - Verify UI displays correct API response data\n    - Test error case: API returns error, UI shows error message\n\n  if 'database' in layers and 'ui' in layers:\n    - Execute end-to-end scenario: UI interaction ‚Üí API call ‚Üí DB operation ‚Üí API response ‚Üí UI update\n    - Verify each layer processed the operation (check logs, network, DB)\n    - Verify final UI state reflects the database change\n    - Test reverse flow: DB change ‚Üí UI reflects change (if applicable)\n\n  // Test realistic user scenarios that span multiple layers\n  - Extract key user flows from plan scenarios\n  - For each flow: Execute as if user performed it\n  - Verify data flows correctly through all layers\n  - Verify outcome matches scenario expectation\n  - Confirm no layer is bypassed or mocked\n```\n\n**If any layer validation fails:**\n- **NEVER guess at fixes**\n- **ALWAYS invoke `/tommymorgan:root-cause`:**\n  ```typescript\n  Task({\n    subagent_type: \"tommymorgan:root-cause-analyzer\",\n    description: \"Analyze layer validation failure\",\n    prompt: `Root cause analysis:\n\n    Layer: <layer type>\n    Validation failure: <what failed>\n    Evidence: <error messages, logs>\n\n    Use five-whys methodology.\n    Provide evidence-based analysis.`\n  })\n  ```\n- Apply fix based on root cause\n- Retry validation\n- Apply retry logic (max 3 attempts with different root cause approaches)\n- After 3 failed attempts: Block and require user intervention\n\n**Only proceed when all layers validated.**\n\n### Step 9: Exploratory Testing Gate\n\n**After layer validation**, validate with exploratory testing:\n\n```typescript\nTask({\n  subagent_type: \"tommymorgan:exploratory-tester\",\n  description: \"Validate implementation against plan\",\n  prompt: `Run plan-aware exploratory testing.\n\nPlan file: <plan path>\n\nValidate all scenarios (User Requirements + Technical Specifications).\nReport any failures.`\n})\n```\n\n**If exploratory tests PASS:**\n- Check completion criteria\n- If complete: Continue to Step 10\n- If incomplete: Return to appropriate step\n\n**If exploratory tests FAIL:**\n- Go back to Step 4\n- Fix failing scenarios\n- Retry review, layer validation, and testing gates\n\n### Step 10: Check Completion Criteria\n\n**Before proceeding to final commit:**\n\nRun `checkCompletionCriteria()`:\n- All scenarios DONE?\n- All tests passing?\n- All layers functional?\n\n**If ANY condition is false:**\n- Identify what's incomplete\n- Return to appropriate step to fix\n- DO NOT ask user - continue autonomously\n\n**If ALL conditions true:**\n- Continue to Step 11\n\n### Step 11: Squash & Final Commit\n\n**After all scenarios complete and tested:**\n\n1. **Squash incremental commits** into single, clean commit:\n   ```bash\n   git rebase -i main  # or appropriate base branch\n   ```\n\n2. **Create final conventional commit:**\n   ```bash\n   git commit -m \"<type>(<scope>): <description>\n\n   <body describing what was delivered, not how>\n\n   Implements: <list scenarios delivered>\n\n   ü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\n   Co-Authored-By: Claude Sonnet 4.5 (1M context) <noreply@anthropic.com>\"\n   ```\n\n### Step 12: Update Plan & Report\n\n1. **Mark all scenarios as DONE** in plan file (change `<!-- TODO -->` to `<!-- DONE -->`)\n\n2. **Report completion:**\n   ```\n   ## Work Session Complete\n\n   **Plan**: <plan file>\n   **Scenarios**: All X scenarios implemented\n   **Quality Gates**: All passed (code review ‚úì, exploratory testing ‚úì)\n   **Commits**: Squashed to 1 clean commit\n\n   ### Completed Scenarios\n   <list all DONE scenarios>\n\n   ### Quality Summary\n   - Code review: APPROVED\n   - Exploratory testing: PASSED\n   - Documentation: Updated\n   - Local dev: Verified\n\n   Ready for deployment!\n   ```\n\n## Progress Feedback\n\n**Log progress messages at key milestones:**\n- \"Starting scenario X of Y: [title]\"\n- \"Writing tests for scenario...\"\n- \"Tests passing, implementing code...\"\n- \"Running code review...\"\n- \"Code review approved, continuing...\"\n- \"Checking completion criteria...\"\n- \"Completion check: X/3 criteria met (scenarios: X, tests: X, layers: X)\"\n- \"Validating layers: [list]\"\n- \"Validating [layer_type] layer...\"\n- \"Layer [layer_type]: PASSED\"\n- \"Testing integration between [layer1] and [layer2]...\"\n- \"Integration validation complete\"\n- \"Running exploratory tests...\"\n- \"All tests passed, all layers functional\"\n\n**Messages are informational only - never wait for user response.**\n\n## Error Handling\n\n**When any validation or test fails:**\n- Log: \"Failure detected: [description]\"\n- Invoke `/tommymorgan:root-cause` (not speculation!)\n- Apply evidence-based fix\n- Retry validation\n- Continue until fixed\n\n**Retry logic - \"exhausting options\":**\n- Attempt 1: Root cause analysis ‚Üí fix ‚Üí retry\n- Attempt 2: Different root cause analysis ‚Üí different fix ‚Üí retry\n- Attempt 3: Third root cause analysis ‚Üí third fix ‚Üí retry\n- After 3 attempts: Block and require user intervention\n\n**NEVER:**\n- Ask \"is this in scope?\"\n- Say \"this is separate work\"\n- Ask \"should I fix this?\"\n- Mention token usage\n- Ask \"what should I do?\"\n\n**ALWAYS:**\n- Fix problems immediately\n- Continue autonomously\n- Apply bulldog persistence\n\n## Important Notes\n\n- **Scenarios ARE the plan** - no separate task lists\n- **Commit incrementally** during TDD, squash at end\n- **Documentation updates** are mandatory, not optional\n- **Local dev verification** is mandatory before starting\n- **Code review gate** is mandatory before continuing\n- **Layer validation gate** is mandatory after all scenarios\n- **Exploratory testing gate** is mandatory before completion\n- **Completion criteria check** is mandatory before stopping\n- **Root-cause analysis** instead of guessing\n- **Bulldog persistence** - no shortcuts that create tech debt\n- **Border Collie intelligence** - excellent autonomous judgment\n- **Never stop for token usage** - execute to completion\n- **Fix all problems** - never ask if in scope\n\n## Local Development Definition\n\n**Local development means:**\n- Application running on developer machine (not production, not CI)\n- Local database instances (containers or host processes)\n- Locally running services (API servers, dev servers)\n- Access to localhost ports and local filesystems\n\n**Local development excludes:**\n- Remote servers\n- Production environments\n- CI/CD pipelines"
              },
              {
                "name": "/test",
                "description": "Smart testing with plan-awareness - automatically selects appropriate testing strategy based on context",
                "path": "tommymorgan/testing/commands/test.md",
                "frontmatter": {
                  "name": "tommymorgan:test",
                  "description": "Smart testing with plan-awareness - automatically selects appropriate testing strategy based on context",
                  "argument-hint": "[test description]",
                  "allowed-tools": [
                    "Task",
                    "Read",
                    "Glob",
                    "Grep",
                    "Bash"
                  ]
                },
                "content": "# Smart Test Command\n\nIntelligent test command that adapts to context:\n- **Plan-aware mode**: Reads Gherkin scenarios from plan and validates implementation\n- **Standalone mode**: Parses description to determine appropriate test type\n\n## Workflow\n\n### Step 1: Detect Plan File\n\nSearch for plan file in current project:\n\n```bash\n# Look in project plans directories\nfind . -path \"*/plans/*.md\" -type f | grep -E \"(apps|libs|tools)\" | sort -r | head -1\n```\n\nIf plan file found, go to **Step 2: Plan-Aware Mode**\nIf no plan file, go to **Step 3: Standalone Mode**\n\n### Step 2: Plan-Aware Mode\n\n**When plan file exists**, use Gherkin scenarios to guide testing:\n\n1. **Read the plan file**:\n   ```\n   Read(<plan_file_path>)\n   ```\n\n2. **Parse Gherkin sections**:\n   - Extract User Requirements section ‚Üí Browser testing scenarios\n   - Extract Technical Specifications section ‚Üí API/CLI testing scenarios\n\n3. **Determine test types based on Gherkin content**:\n\n   **For User Requirements** (always browser testing):\n   - User-focused scenarios test UI flows\n   - Launch browser-explorer agent\n\n   **For Technical Specifications** (analyze keywords):\n   - API indicators: \"endpoint\", \"POST\", \"GET\", \"REST\", \"GraphQL\", \"status code\"\n     ‚Üí Launch api-explorer agent\n   - CLI indicators: \"command\", \"script\", \"shell\", \"exit code\", \"stdout\"\n     ‚Üí Launch cli-tester agent\n\n4. **Launch appropriate agents**:\n\n   For browser testing:\n   ```typescript\n   Task({\n     subagent_type: \"exploratory-tester:browser-explorer\",\n     description: \"Test User Requirements scenarios\",\n     prompt: `Validate implementation against User Requirements:\n\n${user_requirements_gherkin}\n\nTest each scenario systematically. Report findings.`\n   })\n   ```\n\n   For API testing:\n   ```typescript\n   Task({\n     subagent_type: \"exploratory-tester:api-explorer\",\n     description: \"Test Technical Specifications\",\n     prompt: `Validate API implementation against Technical Specifications:\n\n${technical_specs_gherkin}\n\nTest endpoints, authentication, error handling. Report findings.`\n   })\n   ```\n\n   For CLI testing:\n   ```typescript\n   Task({\n     subagent_type: \"exploratory-tester:cli-tester\",\n     description: \"Test CLI specifications\",\n     prompt: `Validate CLI implementation against Technical Specifications:\n\n${technical_specs_gherkin}\n\nTest commands, output, exit codes. Report findings.`\n   })\n   ```\n\n5. **Report results**:\n   - Scenarios validated ‚úì\n   - Failures found ‚úó\n   - Return to implementation if failures\n\n### Step 3: Standalone Mode\n\n**When no plan file**, parse user's test description:\n\n1. **Analyze description for test type signals**:\n\n   **Browser testing signals**:\n   - Keywords: \"UI\", \"page\", \"form\", \"button\", \"login\", \"navigate\", \"click\"\n   - Default choice if ambiguous\n\n   **API testing signals**:\n   - Keywords: \"API\", \"endpoint\", \"/api/\", \"POST\", \"GET\", \"REST\", \"GraphQL\"\n   - URL patterns\n\n   **CLI testing signals**:\n   - Keywords: \"command\", \"script\", \"CLI\", \"binary\", \"shell\"\n\n2. **Launch appropriate agent** with user's description:\n\n   ```typescript\n   Task({\n     subagent_type: \"exploratory-tester:<type>-explorer\",\n     description: \"Exploratory testing\",\n     prompt: `$ARGUMENTS\n\nConduct comprehensive exploratory testing. Report findings.`\n   })\n   ```\n\n3. **Default to browser** if ambiguous\n\n### Step 4: Report Results\n\nSummarize testing session:\n```\n## Testing Complete\n\n**Mode**: Plan-aware / Standalone\n**Type**: Browser / API / CLI\n**Scenarios tested**: N\n**Passed**: X\n**Failed**: Y\n\n<details of failures>\n```\n\n## Usage Examples\n\n**With plan**:\n```\n/tommymorgan:test\n```\n‚Üí Automatically reads plan, tests all scenarios\n\n**Without plan** (browser):\n```\n/tommymorgan:test the login flow\n```\n‚Üí Browser testing of login functionality\n\n**Without plan** (API):\n```\n/tommymorgan:test POST /api/users endpoint\n```\n‚Üí API testing of users endpoint\n\n**Without plan** (CLI):\n```\n/tommymorgan:test the deploy script\n```\n‚Üí CLI testing of deployment script"
              }
            ],
            "skills": [
              {
                "name": "Five Whys Root Cause Analysis",
                "description": "This skill should be used when the user reports \"error\", \"bug\", \"problem\", needs to \"fix\" something, mentions \"debug\", \"failing\", \"broken\", \"issue\", \"not working\", \"crash\", or any problem-solving scenario. Enforces systematic root cause identification using five whys methodology before attempting solutions.",
                "path": "tommymorgan/debugging/skills/five-whys-methodology/SKILL.md",
                "frontmatter": {
                  "name": "Five Whys Root Cause Analysis",
                  "description": "This skill should be used when the user reports \"error\", \"bug\", \"problem\", needs to \"fix\" something, mentions \"debug\", \"failing\", \"broken\", \"issue\", \"not working\", \"crash\", or any problem-solving scenario. Enforces systematic root cause identification using five whys methodology before attempting solutions.",
                  "version": "0.1.0"
                },
                "content": "# Five Whys Root Cause Analysis\n\n## Purpose\n\nThis skill enforces systematic root cause identification before solution attempts. Apply the five whys methodology to prevent speculation-driven debugging that wastes effort, creates harmful changes, and results in never-ending fix loops.\n\n## Core Problem\n\nClaude Code's default behavior when encountering problems:\n1. Observe symptom\n2. Speculate about cause\n3. Implement speculative fix\n4. Fix doesn't work or creates new problems\n5. Repeat indefinitely\n\n**Result:** Wasted effort, potentially harmful changes, user frustration.\n\n## Five Whys Methodology\n\n### The Process\n\nStart with the observed symptom and iteratively ask \"why?\" until reaching an actionable root cause.\n\n**Example progression:**\n1. **Symptom:** Login returns 500 error\n2. **Why #1:** Why does login return 500? ‚Üí Database connection timeout\n3. **Why #2:** Why does database timeout? ‚Üí Connection pool exhausted\n4. **Why #3:** Why is pool exhausted? ‚Üí Connections not being released\n5. **Why #4:** Why aren't connections released? ‚Üí Missing connection.release() call in session handler\n\n**Root cause identified:** Missing connection.release() call - this is actionable.\n\n### Iteration Guidelines\n\n**Continue asking \"why\" until:**\n- The cause is **actionable** (can be fixed with code/config changes)\n- The cause **directly explains** the observed symptom\n- Next \"why\" would be **outside our control** (external system, architectural decision)\n\n**Don't stop at:**\n- Symptom restatements (\"it's broken because it doesn't work\")\n- Vague causes (\"there's a configuration issue\")\n- First plausible explanation without verification\n- Standard iteration counts (five is just a name - continue until root cause found)\n\n### Evidence-Based Investigation\n\n**For each \"why\", gather evidence:**\n\n1. **Read relevant code:**\n   - Files mentioned in error messages\n   - Call stack locations\n   - Related modules and dependencies\n\n2. **Examine outputs:**\n   - Error messages and stack traces\n   - Log files\n   - Console output\n   - Test results\n\n3. **Check configurations:**\n   - Environment variables\n   - Config files\n   - Build settings\n   - Dependency versions\n\n4. **Verify assumptions:**\n   - Don't assume - check actual behavior\n   - Read the code that's actually running\n   - Check what values are actually present\n\n**Base each \"why\" answer on concrete evidence, not speculation.**\n\n## Root Cause Determination\n\n### Criteria for Root Cause\n\nA root cause is identified when ALL these conditions are met:\n\n1. **Actionable:**\n   - Can be fixed with code, configuration, or environment changes\n   - Within our control to modify\n   - Clear fix approach exists\n\n2. **Directly Explanatory:**\n   - Cause fully explains the observed symptom\n   - No gaps in the causal chain\n   - Removing this cause would eliminate the symptom\n\n3. **Terminal:**\n   - Next \"why\" would go outside our control (system architecture, external dependencies, business requirements)\n   - OR: Next \"why\" would not lead to a more actionable fix\n   - OR: This is the deepest layer we can address\n\n### Example: Root Cause vs Symptom\n\n**Symptom:** \"Tests are failing\"\n- ‚ùå NOT root cause: \"The test expectations are wrong\" (what made them wrong?)\n- ‚ùå NOT root cause: \"The code doesn't match tests\" (why doesn't it match?)\n- ‚úÖ ROOT CAUSE: \"Function returns null when user.profile is undefined, but tests expect empty object\"\n\n**Symptom:** \"API returns 404\"\n- ‚ùå NOT root cause: \"Route doesn't exist\" (why doesn't it exist?)\n- ‚ùå NOT root cause: \"Router not configured\" (what's the actual misconfiguration?)\n- ‚úÖ ROOT CAUSE: \"Route path is '/api/v1/users' but code defines '/api/users' - missing '/v1' prefix\"\n\n### When to Stop\n\n**Stop investigating when:**\n- Identified an actionable fix with clear implementation\n- Cause is a concrete code/config/data issue\n- Evidence confirms this cause produces the symptom\n\n**Continue investigating when:**\n- Cause is vague or speculative\n- No clear fix path exists\n- Haven't verified the cause with evidence\n- Fixing this wouldn't fully resolve symptom\n\n## Anti-Patterns to Avoid\n\n### Speculation Without Evidence\n\n‚ùå **Bad:**\n```\nWhy is login failing?\n‚Üí \"Probably a timeout issue\" [no evidence]\n‚Üí \"Let me increase the timeout\" [speculative fix]\n```\n\n‚úÖ **Good:**\n```\nWhy is login failing?\n‚Üí [Read error logs] ‚Üí Database connection timeout after 5 seconds\n‚Üí [Read database config] ‚Üí Connection pool size is 5\n‚Üí [Read application metrics] ‚Üí 50 concurrent requests\n‚Üí Root cause: Connection pool too small for request volume\n```\n\n### Stopping Too Early\n\n‚ùå **Bad:**\n```\nWhy are tests failing?\n‚Üí \"Tests are outdated\"\n‚Üí [Starts rewriting tests without understanding why they're outdated]\n```\n\n‚úÖ **Good:**\n```\nWhy are tests failing?\n‚Üí Tests expect user.email, code returns user.emailAddress\nWhy the field name difference?\n‚Üí API v2 migration changed field names\nWhy weren't tests updated?\n‚Üí Tests live in separate repo, not updated in migration PR\nRoot cause: Migration process doesn't include cross-repo test updates\n```\n\n### Treating Symptoms\n\n‚ùå **Bad:**\n```\nError: \"Cannot read property 'name' of undefined\"\n‚Üí \"Let me add null check\" [treats symptom]\n```\n\n‚úÖ **Good:**\n```\nWhy is object undefined?\n‚Üí API call returns undefined when user not found\nWhy is API call made for non-existent user?\n‚Üí User ID comes from URL parameter without validation\nWhy no validation?\n‚Üí Route handler assumes ID is always valid\nRoot cause: Missing user ID validation in route handler\n```\n\n## Progressive Output Format\n\nShow each investigation level as it completes:\n\n```markdown\n## Root Cause Analysis\n\n### Investigation Level 1\n**Observation:** [What was observed]\n**Evidence:** [What was checked - files, logs, outputs]\n**Finding:** [What this reveals]\n**Next Question:** Why [finding]?\n\n### Investigation Level 2\n**Observation:** [Deeper observation]\n**Evidence:** [More specific evidence]\n**Finding:** [What this reveals]\n**Next Question:** Why [finding]?\n\n[Continue until root cause identified]\n\n### Root Cause Identified\n**Cause:** [Specific, actionable cause]\n**Evidence:** [Concrete evidence supporting this]\n**Fix Approach:** [How this can be addressed]\n**Why This is Root:** [Meets actionability, explanatory, and terminal criteria]\n```\n\n## Integration with Problem Solving\n\n### When Skill Activates\n\nThis skill triggers automatically when:\n- User message contains problem keywords (error, bug, fix, debug, etc.)\n- Tool output contains errors or exceptions\n- User is clearly trying to solve a problem\n\n### Agent Invocation\n\nWhen this skill triggers, it should invoke the root-cause-analyzer agent:\n- Agent runs autonomously through five whys\n- Shows progressive output for each iteration\n- Determines when root cause is reached\n- Completes automatically\n\n### User Override\n\nUser can bypass analysis by:\n- Interrupting the agent manually\n- Explicitly stating \"skip root cause analysis\"\n- Requesting immediate fix attempts\n\n**But default behavior is always root cause first.**\n\n## Workflow\n\n1. **Detect problem:** Skill triggers on keywords or errors\n2. **Launch analysis:** Invoke root-cause-analyzer agent\n3. **Autonomous iteration:** Agent works through whys without user prompts\n4. **Progressive display:** Show each iteration as investigation proceeds\n5. **Root cause identified:** Agent determines when criteria met\n6. **Proceed to solution:** Now fix the identified root cause\n\n## Success Criteria\n\nRoot cause analysis is complete when:\n- ‚úÖ Specific, actionable cause identified\n- ‚úÖ Evidence gathered confirms the cause\n- ‚úÖ Cause directly explains observed symptom\n- ‚úÖ Clear fix approach exists\n- ‚úÖ Fixing this cause will resolve the problem\n\n**Output statement:**\n\"Root cause identified: [specific cause]. This is actionable and directly explains [symptom]. Proceeding to solution...\""
              },
              {
                "name": "plan-format",
                "description": "This skill should be used when creating plan files, writing Gherkin requirements, generating tasks with verification commands, or parsing plan file structure. Provides the plan file template and format specifications.",
                "path": "tommymorgan/planning/skills/plan-format/SKILL.md",
                "frontmatter": {
                  "name": "plan-format",
                  "description": "This skill should be used when creating plan files, writing Gherkin requirements, generating tasks with verification commands, or parsing plan file structure. Provides the plan file template and format specifications.",
                  "version": "0.1.0"
                },
                "content": "# Plan File Format\n\nSpecification for plan files used by the tommymorgan plugin. Plan files capture feature requirements as Gherkin scenarios and tasks with verification commands.\n\n## Plan File Purpose\n\nPlan files serve as the single source of truth for feature development:\n- Capture requirements as executable Gherkin scenarios\n- Define tasks with verification commands that prove completion\n- Track progress across Claude sessions\n- Enable autonomous TDD execution\n\n## File Location\n\nPlan files live in the project's plans directory:\n\n```\n<project>/plans/YYYY-MM-DD-<slug>.md\n```\n\nWhere:\n- `<project>` is the project path (e.g., `apps/web`, `libs/shared`, `tools/cli`)\n- `YYYY-MM-DD` is the creation date\n- `<slug>` is a kebab-case version of the feature name\n\nExample: `apps/web/plans/2024-12-15-user-authentication.md`\n\n## Plan File Template\n\n```markdown\n# Feature: <Title>\n\n**Created**: YYYY-MM-DD\n**Branch**: feat/<slug>\n**Goal**: <One-sentence description of user-facing outcome>\n\n## Requirements\n\nFeature: <Feature Name>\n  Scenario: <Scenario name>\n    Given <precondition>\n    When <action>\n    Then <expected outcome>\n    And <additional outcome>\n\n  Scenario: <Another scenario>\n    Given <precondition>\n    When <action>\n    Then <expected outcome>\n\n## Tasks\n\n### 1. <Task description>\n**Verify**: `<command that exits 0 on success>`\n**Status**: pending\n\n### 2. <Task description>\n**Verify**: `<command>`\n**Status**: pending\n\n### N. Refactor tests for maintainability\n**Verify**: `<full test suite command>`\n**Status**: pending\n**Notes**: Reorganize from implementation-coupled to feature-focused tests\n\n## Notes\n\n<Design decisions, constraints, and context for future sessions>\n```\n\n## Section Specifications\n\n### Header Section\n\n**Required fields:**\n- `**Created**`: Date in YYYY-MM-DD format\n- `**Branch**`: Git branch name, typically `feat/<slug>`\n- `**Goal**`: Single sentence describing user-facing outcome\n\n### Requirements Section\n\nWrite Gherkin scenarios that fully capture the feature requirements.\n\n**Gherkin syntax:**\n- `Feature:` - Groups related scenarios\n- `Scenario:` - Specific behavior to test\n- `Given` - Preconditions/setup\n- `When` - Action being tested\n- `Then` - Expected outcome\n- `And` - Additional conditions\n\n**Coverage requirements:**\n- Happy path scenarios (successful operations)\n- Error cases (invalid input, failures)\n- Edge cases (boundaries, empty states)\n- Security considerations (authorization, validation)\n\n**Writing tips:**\n- Use concrete values in examples (\"user@example.com\" not \"an email\")\n- One behavior per scenario\n- Focus on user-observable outcomes\n- Avoid implementation details in scenarios\n\n### Tasks Section\n\nEach task maps 1:1 with a test file. Task format:\n\n```markdown\n### N. <Task description>\n**Verify**: `<verification command>`\n**Status**: pending|complete|blocked\n```\n\n**Verification command requirements:**\n- Must exit 0 on success, non-zero on failure\n- Typically runs a specific test file\n- Must be deterministic and repeatable\n\n**Common verification patterns:**\n- TypeScript/JavaScript: `pnpm test src/path/to/test.test.ts`\n- Python: `pytest src/path/to/test_file.py`\n- Go: `go test ./path/to/package -run TestName`\n- Rust: `cargo test test_name`\n\n**Status values:**\n- `pending` - Task not yet completed\n- `complete` - Verification command passes\n- `blocked` - Cannot proceed, includes root cause\n\n**For blocked tasks, add root cause:**\n```markdown\n### 3. Add authentication middleware\n**Verify**: `pnpm test src/middleware/auth.test.ts`\n**Status**: blocked\n**Root Cause**: Requires JWT library upgrade in shared-lib, out of scope for this feature.\n```\n\n**Final task requirement:**\nEvery plan must end with a test refactoring task:\n```markdown\n### N. Refactor tests for maintainability\n**Verify**: `<full test suite command>`\n**Status**: pending\n**Notes**: Reorganize from implementation-coupled to feature-focused tests\n```\n\n### Notes Section\n\nPreserve context for future sessions:\n- Design decisions and rationale\n- Constraints and dependencies\n- Implementation approach chosen\n- Rejected alternatives and why\n- Links to relevant documentation\n\n## Parsing Plan Files\n\nTo extract tasks from a plan file:\n\n1. Find lines matching `### \\d+\\. `\n2. Extract description (text after number and period)\n3. Find next line matching `**Verify**: \\`(.+)\\``\n4. Extract command from backticks\n5. Find next line matching `**Status**: (.+)`\n6. Extract status value\n\nTo update task status:\n\n1. Locate the specific task by number\n2. Find the `**Status**: ` line\n3. Replace status value (pending ‚Üí complete)\n4. Preserve all other content\n\n## Validation Rules\n\n**Plan file validation:**\n- [ ] Has title starting with `# Feature:`\n- [ ] Has Created, Branch, Goal header fields\n- [ ] Has Requirements section with Gherkin scenarios\n- [ ] Has Tasks section with numbered tasks\n- [ ] Each task has Verify and Status fields\n- [ ] Verification commands are in backticks\n- [ ] Status values are valid (pending|complete|blocked)\n- [ ] Final task is test refactoring\n\n**Gherkin validation:**\n- [ ] At least one Feature block\n- [ ] Each Scenario has Given/When/Then\n- [ ] Scenarios are independent\n- [ ] No implementation details in scenarios\n\n## Example Plan File\n\nSee `references/example-plan.md` for a complete working example."
              },
              {
                "name": "tdd-execution",
                "description": "This skill should be used when executing plan tasks autonomously, implementing features using TDD, running the red-green-refactor cycle, handling code review loops, or dealing with blocked tasks. Provides the autonomous TDD execution workflow.",
                "path": "tommymorgan/planning/skills/tdd-execution/SKILL.md",
                "frontmatter": {
                  "name": "tdd-execution",
                  "description": "This skill should be used when executing plan tasks autonomously, implementing features using TDD, running the red-green-refactor cycle, handling code review loops, or dealing with blocked tasks. Provides the autonomous TDD execution workflow.",
                  "version": "0.1.0"
                },
                "content": "# TDD Execution\n\nWorkflow for executing plan tasks autonomously using Test-Driven Development, with code review gates and root cause analysis for failures.\n\n## Core Principles\n\n**Test-First:**\nWrite the failing test before any implementation. The test defines what \"done\" looks like.\n\n**Atomic Tasks:**\nEach task = one test file = one commit. Keep changes small and reviewable.\n\n**Evidence-Based:**\nNever guess at failures. Use root-cause-analysis before declaring anything blocked.\n\n**Continuous Progress:**\nUpdate the plan file after each task. Progress survives session interruption.\n\n## Execution Loop\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  VERIFICATION SWEEP                      ‚îÇ\n‚îÇ              (establish ground truth)                    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  SELECT NEXT TASK                        ‚îÇ\n‚îÇ           (first pending task by number)                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                   RED PHASE                              ‚îÇ\n‚îÇ         Write failing test from Gherkin                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  GREEN PHASE                             ‚îÇ\n‚îÇ      Implement minimum code to pass test                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  CODE REVIEW                             ‚îÇ\n‚îÇ            Loop until approved                           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    COMMIT                                ‚îÇ\n‚îÇ          Conventional commit message                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                  UPDATE PLAN                             ‚îÇ\n‚îÇ          Status: pending ‚Üí complete                      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                          ‚îÇ\n                          ‚ñº\n                   More pending? ‚îÄ‚îÄyes‚îÄ‚îÄ‚ñ∫ SELECT NEXT TASK\n                          ‚îÇ\n                         no\n                          ‚îÇ\n                          ‚ñº\n                       DONE\n```\n\n## Red Phase: Write Failing Test\n\n### Purpose\n\nDefine the expected behavior before writing any implementation. The test is the specification.\n\n### Process\n\n1. Read the task description and related Gherkin scenarios\n2. Determine test file location from verification command\n3. Write test that captures the Gherkin behavior\n4. Run verification command to confirm test fails\n\n### Test Quality Criteria\n\n**Good tests:**\n- Title describes outcome, not implementation\n- Test validates the Gherkin scenario\n- Could replace implementation without changing test\n- Single responsibility per test\n\n**From Gherkin to test:**\n\nGherkin:\n```gherkin\nScenario: Successful registration with valid email\n  Given I am a new user\n  When I register with email \"user@example.com\" and password \"SecurePass123\"\n  Then my account should be created\n  And I should receive a confirmation message\n```\n\nTest:\n```typescript\ndescribe('User Registration', () => {\n  it('should create account and return confirmation for valid registration', async () => {\n    // Given: new user (no setup needed)\n\n    // When: register with valid credentials\n    const result = await register({\n      email: 'user@example.com',\n      password: 'SecurePass123'\n    });\n\n    // Then: account created and confirmation returned\n    expect(result.success).toBe(true);\n    expect(result.message).toContain('confirmation');\n  });\n});\n```\n\n### Unexpected Pass\n\nIf the test passes immediately:\n- The task might already be done\n- Run verification sweep to confirm\n- If truly complete, update status and move on\n\n## Green Phase: Implement\n\n### Purpose\n\nWrite the minimum code necessary to make the test pass.\n\n### Process\n\n1. Implement just enough to satisfy the test\n2. Run verification command after each change\n3. Continue until test passes\n\n### Implementation Guidelines\n\n**Minimum viable implementation:**\n- Don't add features not tested\n- Don't optimize prematurely\n- Don't refactor during green phase\n\n**Stuck after 3-5 attempts:**\n- Stop trying random fixes\n- Invoke root-cause-analysis\n- Address actual root cause\n\n## Code Review\n\n### Purpose\n\nEnsure implementation quality before committing. Fresh perspective catches issues.\n\n### Process\n\nLaunch code-reviewer subagent:\n\n```typescript\nTask({\n  subagent_type: \"pr-review-toolkit:code-reviewer\",\n  description: \"Review task implementation\",\n  prompt: `Review the implementation:\n\nTask: <task description>\nGherkin: <related scenarios>\n\nCheck:\n- Does implementation satisfy Gherkin scenarios?\n- Code quality and best practices\n- Security considerations\n- Test quality\n\nFiles changed: <list>\n\nRespond: APPROVED or NEEDS_CHANGES with specifics.`\n})\n```\n\n### Review Loop\n\nIf NEEDS_CHANGES:\n1. Address feedback\n2. Verify tests still pass\n3. Request another review\n4. Maximum 3 iterations\n\nIf still not approved after 3 iterations:\n- Investigate with root-cause-analysis\n- Identify fundamental issue\n- Mark blocked if necessary\n\n## Commit\n\n### Purpose\n\nCreate atomic, reviewable commits with clear history.\n\n### Commit Message Format\n\n```\n<type>(<scope>): <subject>\n\n<body>\n\nTask: <number> - <description>\n```\n\n**Type selection:**\n- `feat` - New functionality\n- `fix` - Bug fix\n- `refactor` - Code improvement\n- `test` - Test changes only\n- `docs` - Documentation\n\n**Example:**\n```\nfeat(auth): add user registration endpoint\n\nImplement registration with email validation and password hashing.\nReturns confirmation message on successful account creation.\n\nTask: 2 - Add registration endpoint\n```\n\n### Commit Process\n\n```bash\ngit add -A\ngit commit -m \"<message>\"\n```\n\nOne task = one commit. No batching.\n\n## Update Plan\n\n### Purpose\n\nReflect actual completion status in the plan file.\n\n### Process\n\nAfter successful commit, update the task status:\n\nFind:\n```markdown\n**Status**: pending\n```\n\nReplace:\n```markdown\n**Status**: complete\n```\n\nThis ensures:\n- Progress survives session interruption\n- Next session knows what's done\n- Plan file is always accurate\n\n## Handling Failures\n\n### Test Won't Pass\n\nAfter 3-5 implementation attempts:\n\n1. **Stop trying random fixes**\n2. **Invoke root-cause-analysis:**\n```typescript\nTask({\n  subagent_type: \"root-cause-analysis:root-cause-analyzer\",\n  prompt: `Analyze failure:\n\nError: <what's failing>\nAttempts: <what was tried>\n\nUse five-whys methodology.\nIdentify actual root cause.`\n})\n```\n3. **Apply fix based on root cause**\n4. **If still failing, mark blocked:**\n```markdown\n**Status**: blocked\n**Root Cause**: <identified root cause>\n```\n\n### Code Review Loop Won't Converge\n\nAfter 3 review iterations:\n\n1. **Invoke root-cause-analysis** on the review feedback\n2. **Identify why fixes aren't addressing core issue**\n3. **Either:** Apply targeted fix **or** mark blocked\n\n### Environment Issues\n\nIf tests fail due to environment:\n- Document the issue\n- Report to user\n- Do not mark task as blocked for environment issues\n- Environment issues need human intervention\n\n## Session Interruption\n\nIf work is interrupted:\n\n**Plan file reflects accurate state:**\n- Completed tasks marked complete\n- Current task stays pending\n- No partial commits\n\n**Next session:**\n1. Run verification sweep\n2. Continue from first pending task\n3. No duplicate work\n\n## Progress Tracking\n\nUse TodoWrite for real-time progress:\n\n```typescript\nTodoWrite({\n  todos: [\n    { content: \"Task 1: Create user model\", status: \"completed\" },\n    { content: \"Task 2: Add registration endpoint\", status: \"in_progress\" },\n    { content: \"Task 3: Add login endpoint\", status: \"pending\" }\n  ]\n})\n```\n\nUpdate after each task completion.\n\n## Completion\n\nWhen all tasks complete:\n\n```\n## Work Complete\n\n**Plan**: <filename>\n**Tasks**: X/X complete\n**Commits**: N commits\n\nAll tasks verified and complete.\nFeature ready for final review.\n```\n\nWhen blocked:\n\n```\n## Work Blocked\n\n**Plan**: <filename>\n**Progress**: X/Y complete\n**Blocked on**: Task N\n\nRoot Cause: <explanation>\n\nHuman intervention required.\n```"
              },
              {
                "name": "verification-sweep",
                "description": "This skill should be used when checking plan status, running verification commands, updating task statuses, or determining what work remains. Provides the verification sweep algorithm and status update procedures.",
                "path": "tommymorgan/planning/skills/verification-sweep/SKILL.md",
                "frontmatter": {
                  "name": "verification-sweep",
                  "description": "This skill should be used when checking plan status, running verification commands, updating task statuses, or determining what work remains. Provides the verification sweep algorithm and status update procedures.",
                  "version": "0.1.0"
                },
                "content": "# Verification Sweep\n\nAlgorithm for verifying task completion by running verification commands and updating plan status.\n\n## Purpose\n\nVerification sweep establishes ground truth about task completion:\n- Run verification commands to prove tasks are done\n- Update plan file with accurate statuses\n- Detect regressions in previously complete tasks\n- Report what work remains\n\n## When to Use\n\nPerform verification sweep:\n- At the start of `/tommymorgan:work` (before doing any work)\n- When running `/tommymorgan:status`\n- After returning from a break or new session\n- Before reporting completion status\n\n## Algorithm\n\n### Step 1: Load Plan File\n\nRead the plan file and extract all tasks:\n\n```\nFor each line matching \"### \\d+\\. \":\n  - task_number = extracted number\n  - task_description = text after number\n  - verify_command = content of next **Verify**: `...`\n  - current_status = value after **Status**:\n```\n\n### Step 2: Run Verification Commands\n\nFor each task, execute its verification command:\n\n```bash\n<verify_command>\necho \"Exit code: $?\"\n```\n\nCapture:\n- Exit code (0 = success)\n- Stdout/stderr (for debugging)\n\n### Step 3: Determine New Status\n\nApply status transition rules:\n\n| Current Status | Exit Code | New Status | Action |\n|----------------|-----------|------------|--------|\n| pending | 0 | complete | Update plan |\n| pending | non-zero | pending | No change |\n| complete | 0 | complete | No change |\n| complete | non-zero | complete | Warn: regression |\n| blocked | 0 | complete | Update plan, clear block |\n| blocked | non-zero | blocked | No change |\n\n**Regression handling:**\nIf a previously complete task now fails, warn but do not change status:\n```\nWARNING: Regression detected in task N: <description>\nVerification command failed but task marked complete.\nInvestigate before continuing.\n```\n\n### Step 4: Update Plan File\n\nFor each status change, update the plan file:\n\nFind:\n```markdown\n**Status**: pending\n```\n\nReplace with:\n```markdown\n**Status**: complete\n```\n\nUse precise edits - only change the status value, preserve everything else.\n\n### Step 5: Generate Report\n\nAfter verification sweep, report results:\n\n```\n## Verification Sweep: <plan filename>\n\n**Run at**: <timestamp>\n**Progress**: X/Y tasks complete\n\n### Results\n- Task 1: <description> - PASS (complete)\n- Task 2: <description> - PASS (complete)\n- Task 3: <description> - FAIL (pending)\n- Task 4: <description> - SKIP (blocked)\n\n### Summary\n- Complete: X\n- Pending: Y\n- Blocked: Z\n- Regressions: N\n\n**Next task**: <first pending task description>\n```\n\n## Verification Command Best Practices\n\n**Good verification commands:**\n```bash\n# Run specific test file\npnpm test src/models/user.test.ts\n\n# Run test with specific pattern\npytest -k test_user_registration\n\n# Check file exists with content\ngrep -q \"export class User\" src/models/user.ts\n\n# Verify build succeeds\npnpm build && test -f dist/index.js\n```\n\n**Bad verification commands:**\n```bash\n# Too broad - might pass for wrong reasons\npnpm test\n\n# Non-deterministic\ncurl http://localhost:3000/health\n\n# Requires manual inspection\ncat src/models/user.ts\n```\n\n## Handling Edge Cases\n\n### Missing Verification Command\n\nIf a task lacks a verification command:\n```\nERROR: Task N has no verification command.\nCannot determine completion status.\n```\nMark as pending and flag for human attention.\n\n### Verification Command Timeout\n\nSet reasonable timeout (30 seconds default):\n```bash\ntimeout 30 <verify_command>\n```\n\nIf timeout:\n- Treat as failure\n- Log timeout warning\n- Consider if command is appropriate\n\n### Environment Dependencies\n\nBefore running verification commands:\n- Ensure test environment is ready\n- Check required services are running\n- Verify dependencies are installed\n\nIf environment issue detected, report clearly:\n```\nERROR: Verification environment not ready.\nMissing: <dependency>\nRun: <setup command>\n```\n\n## Status Transitions\n\n```\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   pending   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n        ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n        ‚îÇ                  ‚îÇ                  ‚îÇ\n   (verify fails)    (verify passes)    (unblock)\n        ‚îÇ                  ‚îÇ                  ‚îÇ\n        ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  complete   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n                           ‚îÇ             ‚îÇ\n                    (manual block)       ‚îÇ\n                           ‚îÇ             ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n                    ‚îÇ   blocked   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Transitions:**\n- `pending` ‚Üí `complete`: Verification passes\n- `pending` ‚Üí `blocked`: Manual intervention (with root cause)\n- `complete` ‚Üí `blocked`: Manual intervention (regression)\n- `blocked` ‚Üí `complete`: Verification passes after unblock\n- `blocked` ‚Üí `pending`: Manual unblock\n\n## Integration with Commands\n\n### /tommymorgan:status\n\nRuns verification sweep and reports results. Does not perform any work.\n\n### /tommymorgan:work\n\nRuns verification sweep first, then works on pending tasks:\n1. Verification sweep\n2. If all complete ‚Üí done\n3. If blocked and no pending ‚Üí report and stop\n4. If pending ‚Üí work on next task\n5. After each task completion ‚Üí mini-sweep (verify just that task)\n\n## Troubleshooting\n\n**All tasks show as pending:**\n- Check verification commands are correct\n- Ensure test files exist\n- Verify test runner is configured\n\n**Complete tasks regressing:**\n- Check for test pollution between runs\n- Verify test isolation\n- Look for environment-dependent tests\n\n**Verification commands hang:**\n- Add timeout handling\n- Check for interactive prompts\n- Ensure commands are non-blocking"
              },
              {
                "name": "API Testing Patterns",
                "description": "This skill should be used when performing exploratory testing of REST or GraphQL APIs, including endpoint discovery, request/response validation, authentication testing, and error handling checks. Triggers when testing APIs, validating OpenAPI specs, checking endpoint security, or verifying API responses.",
                "path": "tommymorgan/testing/skills/api-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "API Testing Patterns",
                  "description": "This skill should be used when performing exploratory testing of REST or GraphQL APIs, including endpoint discovery, request/response validation, authentication testing, and error handling checks. Triggers when testing APIs, validating OpenAPI specs, checking endpoint security, or verifying API responses.",
                  "version": "0.1.0"
                },
                "content": "# API Testing Patterns\n\n## Purpose\n\nProvide systematic patterns for autonomous API exploratory testing. Guide agents through comprehensive API testing including endpoint discovery, request/response validation, authentication verification, error handling checks, and security testing.\n\n## Core Testing Methodology\n\n### API Discovery Strategy\n\nDiscover API endpoints through multiple methods:\n\n**1. OpenAPI/Swagger Specification**\n- Parse spec to extract all endpoints\n- Understand request/response schemas\n- Identify authentication requirements\n- Note documented error codes\n\n**2. Common Pattern Discovery**\n- Probe common API paths (/api/*, /v1/*, /graphql)\n- Try standard REST conventions (GET /users, POST /users, etc.)\n- Check for API documentation endpoints (/docs, /swagger, /openapi.json)\n\n**3. Undocumented Endpoint Detection**\n- Compare discovered endpoints with spec\n- Report endpoints found but not documented (security concern)\n- Note missing endpoints that should exist\n\n### Comprehensive Testing Coverage\n\nTest each endpoint across multiple dimensions:\n\n**HTTP Methods:**\n- GET: Read operations\n- POST: Create operations\n- PUT/PATCH: Update operations\n- DELETE: Delete operations\n- OPTIONS: CORS preflight\n\n**Response Codes:**\n- 200/201: Success scenarios\n- 400: Bad request (invalid input)\n- 401: Unauthorized (missing/invalid auth)\n- 403: Forbidden (insufficient permissions)\n- 404: Not found (non-existent resources)\n- 500: Server errors\n\n**Input Variations:**\n- Valid data (happy path)\n- Invalid data types\n- Missing required fields\n- Boundary values (empty, very long, negative)\n- Malicious inputs (SQL injection, XSS attempts)\n\n## Authentication Testing\n\n### Common Authentication Patterns\n\n**Bearer Token:**\n```bash\ncurl -H \"Authorization: Bearer <token>\" https://api.example.com/endpoint\n```\n\n**API Key:**\n```bash\ncurl -H \"X-API-Key: <key>\" https://api.example.com/endpoint\n```\n\n**Basic Auth:**\n```bash\ncurl -u username:password https://api.example.com/endpoint\n```\n\n### Authentication Tests\n\nTest authentication enforcement:\n\n1. **No auth**: Request without credentials (should return 401)\n2. **Invalid auth**: Request with malformed credentials (should return 401)\n3. **Expired auth**: Request with expired token (should return 401)\n4. **Valid auth**: Request with proper credentials (should succeed)\n\n### Authorization Tests\n\nTest permission boundaries:\n\n1. **Own resources**: User can access their own data (should succeed)\n2. **Other resources**: User cannot access other users' data (should return 403)\n3. **Admin resources**: Non-admin cannot access admin endpoints (should return 403)\n4. **Role boundaries**: Test each role's access limits\n\n## Input Validation Testing\n\n### Test Data Generation\n\nGenerate test data based on schemas:\n\n**From OpenAPI Schema:**\n```json\n{\n  \"email\": \"test@example.com\",\n  \"age\": 25,\n  \"name\": \"Test User\"\n}\n```\n\n**Invalid Variations:**\n```json\n// Wrong type\n{\"email\": 123, \"age\": \"not a number\"}\n\n// Missing required\n{\"email\": \"test@example.com\"}\n\n// Boundary values\n{\"email\": \"\", \"age\": -1}\n\n// Too long\n{\"name\": \"A\".repeat(1000)}\n```\n\n### SQL Injection Testing\n\nTest for SQL injection vulnerabilities:\n\n```markdown\nSend inputs designed to break SQL queries:\n- `' OR '1'='1`\n- `; DROP TABLE users; --`\n- `1' UNION SELECT * FROM users --`\n\nExpected behavior:\n- API rejects malicious input (400/422)\n- Error message doesn't reveal SQL structure\n- No database errors in response\n```\n\n### XSS Testing\n\nTest for cross-site scripting vulnerabilities:\n\n```markdown\nSend HTML/JavaScript in inputs:\n- `<script>alert('XSS')</script>`\n- `<img src=x onerror=alert('XSS')>`\n- `javascript:alert('XSS')`\n\nExpected behavior:\n- Input is sanitized or rejected\n- Response escapes HTML properly\n- No executable code in responses\n```\n\n## Response Validation\n\n### Schema Validation\n\nValidate responses match documented schemas:\n\n```markdown\nFor each endpoint:\n1. Parse response JSON\n2. Check all required fields present\n3. Verify field types match schema\n4. Validate nested object structures\n5. Check enum values are from allowed set\n6. Verify array items match schema\n```\n\n### Error Response Format\n\nValidate error responses are consistent:\n\n```markdown\nExpected error format:\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Email is required\",\n    \"details\": {...}\n  }\n}\n\nCheck:\n- Errors don't expose sensitive data\n- Error codes are consistent\n- Messages are helpful\n- Stack traces not exposed\n```\n\n## Performance Testing\n\n### Response Time Measurement\n\nMeasure API response times:\n\n```markdown\nUse WebFetch or Bash (curl with timing):\n- Simple queries: <200ms\n- Complex queries: <500ms\n- Search operations: <400ms\n- Bulk operations: <2s\n\nReport slow endpoints with:\n- Actual response time\n- Endpoint and method\n- Request parameters\n- Threshold exceeded\n```\n\n### Load Testing Indicators\n\nCheck for performance under load:\n\n```markdown\nLook for:\n- Rate limiting headers (X-RateLimit-*)\n- Retry-After headers\n- 429 Too Many Requests responses\n- Increasing response times with repeated requests\n```\n\n## GraphQL Testing\n\n### Introspection Query\n\nDiscover GraphQL schema:\n\n```graphql\n{\n  __schema {\n    types {\n      name\n      fields {\n        name\n        type {\n          name\n        }\n      }\n    }\n  }\n}\n```\n\n### GraphQL-Specific Tests\n\nTest GraphQL endpoints:\n\n1. **Valid queries**: Test documented queries\n2. **Invalid queries**: Malformed GraphQL syntax\n3. **Deep nesting**: Excessive query depth (DoS risk)\n4. **Field suggestions**: Check error messages for typos\n5. **Mutations**: Test data modification operations\n6. **Subscriptions**: Test real-time subscriptions if supported\n\n## Reporting Patterns\n\n### Issue Categorization\n\n**Critical Issues ‚ùå:**\n- Authentication bypass\n- Authorization failures\n- Data exposure\n- SQL injection vulnerabilities\n- Crashes or 500 errors on normal inputs\n\n**Security Concerns üîí:**\n- Undocumented endpoints\n- Weak input validation\n- Sensitive data in errors\n- Missing CORS headers\n- No rate limiting\n\n**Warnings ‚ö†Ô∏è:**\n- Inconsistent error formats\n- Poor error messages\n- Slow response times\n- Missing validation\n- Deprecated endpoints\n\n**Tests Passed ‚úÖ:**\n- Proper authentication enforcement\n- Strong input validation\n- Consistent response schemas\n- Good performance\n- Helpful error messages\n\n### Reproduction Examples\n\nProvide cURL examples for every issue:\n\n```markdown\n### Issue: Missing Email Validation\n\n**Reproduction:**\n```bash\ncurl -X POST https://api.example.com/users \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer <token>\" \\\n  -d '{\"email\": \"not-an-email\", \"name\": \"Test\"}'\n```\n\n**Expected**: 400 Bad Request with validation error\n**Actual**: 201 Created (accepted invalid email)\n**Impact**: Invalid data in database\n**Severity**: High\n```\n\n## Test Data Sources\n\n### Priority Order\n\n1. **Application test data generator**: Use /api/test/seed or similar\n2. **OpenAPI schema**: Generate from documented schemas\n3. **Infer from responses**: Learn from successful responses\n4. **Simple defaults**: Use basic valid test data\n\n### Realistic Test Data\n\nGenerate realistic data:\n\n```markdown\nGood:\n- Emails: \"test@example.com\", \"user@test.org\"\n- Names: \"Test User\", \"Jane Doe\"\n- Dates: Current date ¬± reasonable range\n- IDs: Realistic integers or UUIDs\n\nAvoid:\n- Obvious fakes: \"foo\", \"bar\", \"test\"\n- Invalid formats: \"asdf@asdf\"\n- Destructive: very large values\n```\n\n## Additional Resources\n\nFor complete API testing procedures, see:\n- **`references/openapi-parsing.md`** - Parsing and using OpenAPI specs\n- **`references/authentication-patterns.md`** - Authentication testing strategies\n- **`references/security-testing.md`** - Security vulnerability testing"
              },
              {
                "name": "Browser Testing Patterns",
                "description": "This skill should be used when performing browser-based exploratory testing of web applications, including functional testing, visual inspection, accessibility checks, and performance analysis. Triggers when testing web UIs, checking for browser bugs, validating WCAG compliance, or measuring Core Web Vitals.",
                "path": "tommymorgan/testing/skills/browser-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "Browser Testing Patterns",
                  "description": "This skill should be used when performing browser-based exploratory testing of web applications, including functional testing, visual inspection, accessibility checks, and performance analysis. Triggers when testing web UIs, checking for browser bugs, validating WCAG compliance, or measuring Core Web Vitals.",
                  "version": "0.1.0"
                },
                "content": "# Browser Testing Patterns\n\n## Purpose\n\nProvide systematic patterns and best practices for autonomous browser-based exploratory testing using Playwright MCP. Guide agents through comprehensive web application testing including functional validation, visual inspection, accessibility compliance, and performance measurement.\n\n## Core Testing Methodology\n\n### Systematic Exploration Approach\n\nTest web applications using a structured layered approach:\n\n1. **Initial reconnaissance**: Navigate to target, capture state\n2. **Functional testing**: Interact with elements, test workflows\n3. **Accessibility validation**: Check WCAG compliance\n4. **Performance measurement**: Analyze Core Web Vitals\n5. **Visual inspection**: Detect layout issues\n6. **Reporting**: Categorize and document findings\n\n### Prioritization Framework\n\nFocus testing effort based on user impact:\n\n**Critical (test thoroughly):**\n- User authentication and authorization\n- Payment and transaction flows\n- Data submission and persistence\n- Primary user workflows\n\n**Important (test systematically):**\n- Navigation and routing\n- Form validation and error handling\n- Search and filtering\n- Content display\n\n**Secondary (spot check):**\n- Footer links and secondary pages\n- Help documentation\n- Administrative features\n\n## Functional Testing Patterns\n\n### Element Interaction Strategy\n\nTest interactive elements systematically:\n\n1. **Identify interactive elements**: Use browser_snapshot to find buttons, links, inputs\n2. **Click testing**: Click each button/link, verify expected behavior\n3. **Form testing**: Fill forms with valid and invalid data, submit and verify\n4. **Keyboard testing**: Tab through forms, test Enter/Escape keys\n5. **Error states**: Trigger validation errors, check messages\n\n### Console Error Detection\n\nCheck for JavaScript errors:\n\n```markdown\nUse browser_console_messages with level \"error\" to detect:\n- Uncaught exceptions\n- Failed network requests\n- Resource loading failures\n- Runtime errors\n\nCategorize console errors:\n- **Critical**: Breaks functionality (TypeError, ReferenceError)\n- **Warning**: Degraded experience (404 for non-critical resources)\n- **Info**: Non-blocking issues (deprecation warnings)\n```\n\n### Network Request Validation\n\nAnalyze network traffic:\n\n```markdown\nUse browser_network_requests to check:\n- Failed requests (404, 500 errors)\n- Slow requests (>2 seconds)\n- Large responses (>1MB)\n- Excessive request count (>50 per page)\n\nReport network issues clearly:\n- URL and method\n- Status code and response time\n- Whether it blocks functionality\n```\n\n## Accessibility Testing Patterns\n\n### WCAG 2.1 Level AA Checklist\n\nTest for common accessibility violations:\n\n**Perceivable:**\n- All images have alt text\n- Video/audio has captions or transcripts\n- Color contrast meets 4.5:1 ratio\n- Content is not conveyed by color alone\n\n**Operable:**\n- All functionality available via keyboard\n- No keyboard traps\n- Sufficient time for interactions\n- Focus visible on interactive elements\n\n**Understandable:**\n- Form labels are clear and associated\n- Error messages are helpful\n- Consistent navigation\n- Predictable behavior\n\n**Robust:**\n- Valid HTML structure\n- Proper heading hierarchy (h1 ‚Üí h2 ‚Üí h3)\n- ARIA labels where needed\n- Form controls have accessible names\n\n### Keyboard Navigation Testing\n\nTest keyboard usability:\n\n```markdown\nUse browser_press_key to test:\n1. Tab - Focus moves logically through page\n2. Shift+Tab - Reverse focus order works\n3. Enter - Activates buttons and links\n4. Escape - Closes modals and dropdowns\n5. Arrow keys - Navigate within components\n\nVerify:\n- Focus is visible (outline/highlight)\n- Tab order is logical\n- All interactive elements reachable\n- No keyboard traps\n```\n\n## Performance Testing Patterns\n\n### Core Web Vitals Measurement\n\nMeasure key performance metrics using browser_evaluate:\n\n```javascript\n// Largest Contentful Paint (LCP)\nconst lcp = performance.getEntriesByType('largest-contentful-paint')[0]?.renderTime;\n\n// First Input Delay (FID)\nconst fid = performance.getEntriesByType('first-input')[0]?.processingStart;\n\n// Cumulative Layout Shift (CLS)\nconst cls = performance.getEntriesByType('layout-shift').reduce((sum, entry) => sum + entry.value, 0);\n```\n\n**Thresholds (Google standards):**\n- LCP: <2.5s (good), 2.5-4s (needs improvement), >4s (poor)\n- FID: <100ms (good), 100-300ms (needs improvement), >300ms (poor)\n- CLS: <0.1 (good), 0.1-0.25 (needs improvement), >0.25 (poor)\n\n### Network Performance Analysis\n\nAnalyze request patterns:\n\n```markdown\nCheck network_requests for:\n- Total request count (>50 = potential issue)\n- Total data transferred (>2MB = optimization needed)\n- Slow requests (>2s)\n- Failed requests (404, 500)\n- Unnecessary requests (duplicate fetches)\n```\n\n## Visual Testing Patterns\n\n### Layout Issue Detection\n\nIdentify visual problems:\n\n**Look for:**\n- Overlapping elements (z-index issues)\n- Cut-off text or content (overflow problems)\n- Elements positioned outside viewport\n- Missing spacing (elements touching)\n- Inconsistent styling\n\n**When to screenshot:**\n- Visual issue detected (not routinely)\n- Layout appears broken\n- Content is cut off or overlapping\n- Responsive design fails\n\n### Screenshot Strategy\n\nTake screenshots efficiently:\n\n```markdown\nUse browser_take_screenshot:\n- ONLY when visual issue detected\n- Include context (full page or specific element)\n- Use descriptive filenames\n- Reference in report clearly\n\nAvoid:\n- Screenshots of every action\n- Screenshots with no issues\n- Redundant screenshots\n```\n\n## Testing Depth Management\n\n### Single Page Testing (depth=1)\n\nTest only the provided URL:\n- Complete functional testing\n- Full accessibility audit\n- Performance measurement\n- Visual inspection\n- Console error check\n\n### Shallow Crawl (depth=2)\n\nTest main page + linked pages:\n- Identify all links on main page\n- Follow internal links (same domain)\n- Apply smoke tests to linked pages\n- Report critical issues on any page\n\n### Deep Crawl (depth=3+)\n\nRecursive exploration:\n- Track visited URLs (avoid loops)\n- Follow links up to specified depth\n- Prioritize critical flows over exhaustive crawling\n- Respect time limits (don't test forever)\n\n## Reporting Best Practices\n\n### Categorize by Severity\n\n**Critical ‚ùå:**\n- Prevents core functionality\n- Blocks users from completing tasks\n- Exposes security vulnerabilities\n- Causes data loss\n\n**Warning ‚ö†Ô∏è:**\n- Degrades user experience\n- Inconsistent behavior\n- Poor error messages\n- Performance issues\n\n**Info ‚ÑπÔ∏è:**\n- Best practice violations\n- Accessibility improvements\n- Performance optimizations\n- UX enhancements\n\n### Provide Actionable Details\n\nFor each issue include:\n- **Location**: Specific selector or URL\n- **Reproduction**: Steps to reproduce\n- **Expected vs Actual**: What should happen vs what happens\n- **Impact**: How this affects users\n- **Priority**: Critical/High/Medium/Low\n\n### Balance Thoroughness with Clarity\n\nReport both problems and successes:\n- Don't only report failures (show what works)\n- Don't report every minor issue (focus on impactful findings)\n- Group related issues (don't duplicate similar problems)\n- Prioritize by user impact, not alphabetically\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed testing techniques, consult:\n- **`references/playwright-mcp-tools.md`** - Complete Playwright MCP tool reference\n- **`references/accessibility-testing.md`** - Comprehensive WCAG testing guide\n- **`references/performance-metrics.md`** - Detailed performance analysis patterns\n\nThese references provide deeper technical details while keeping this core skill focused on essential patterns and workflows.\n\n## Quick Reference\n\n**Testing checklist:**\n- [ ] Navigate to target URL\n- [ ] Capture initial state (snapshot, console, network)\n- [ ] Test interactive elements\n- [ ] Validate accessibility\n- [ ] Measure performance\n- [ ] Inspect for visual issues\n- [ ] Report findings by severity\n\n**Remember:** Focus on finding real bugs autonomously. Be thorough but scoped to context. Report actionably."
              },
              {
                "name": "CLI Testing Patterns",
                "description": "This skill should be used when performing exploratory testing of command-line tools and scripts, including help text validation, option testing, error handling verification, and output validation. Triggers when testing CLI commands, scripts, build tools, or command-line interfaces.",
                "path": "tommymorgan/testing/skills/cli-testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "CLI Testing Patterns",
                  "description": "This skill should be used when performing exploratory testing of command-line tools and scripts, including help text validation, option testing, error handling verification, and output validation. Triggers when testing CLI commands, scripts, build tools, or command-line interfaces.",
                  "version": "0.1.0"
                },
                "content": "# CLI Testing Patterns\n\n## Purpose\n\nProvide systematic patterns for autonomous command-line tool exploratory testing. Guide agents through comprehensive CLI testing including help discovery, option validation, error handling checks, exit code verification, and output correctness testing.\n\n## Core Testing Methodology\n\n### Systematic CLI Testing Approach\n\nTest command-line tools using a structured approach:\n\n1. **Discovery**: Find command and understand its options\n2. **Basic smoke test**: Verify command runs\n3. **Help validation**: Check help text completeness\n4. **Option testing**: Test all flags and arguments\n5. **Error handling**: Test edge cases and invalid inputs\n6. **Output validation**: Verify correctness and formatting\n7. **Scoped comprehensive testing**: Focus on changed areas\n\n### CLI Discovery Methods\n\nIdentify command to test from context:\n\n**Script path**: `./scripts/build.sh`\n**Binary name**: `mycommand`\n**Package.json script**: `npm run test`\n**System command**: `git status`\n\nVerify command exists:\n```bash\nwhich mycommand\ncommand -v mycommand\n[ -f ./script.sh ] && echo \"exists\"\n```\n\n## Help Text Discovery\n\n### Common Help Patterns\n\nTry these flags to discover usage:\n\n```bash\ncommand --help\ncommand -h\ncommand help\ncommand\n```\n\n### Parse Help Output\n\nExtract from help text:\n- Available options/flags\n- Required vs optional arguments\n- Expected input formats\n- Subcommands (if applicable)\n- Usage examples\n\nExample help parsing:\n```markdown\nUsage: deploy [options] <environment>\n\nOptions:\n  -v, --verbose    Enable verbose output\n  -d, --dry-run    Show what would be deployed\n  --config <file>  Use custom config\n\nIdentifies:\n- Required: <environment>\n- Optional flags: -v, -d, --config\n- Config file option\n```\n\n## Smoke Testing\n\n### Basic Execution Tests\n\nTest command works at all:\n\n```bash\n# Version check\ncommand --version\ncommand -v\n\n# No arguments (if allowed)\ncommand\n\n# Help (should never error)\ncommand --help\n```\n\nVerify:\n- Command executes without crashing\n- Returns appropriate exit code\n- Produces expected output or error\n\n## Comprehensive Option Testing\n\n### Test All Flags\n\nFor each discovered flag:\n\n```markdown\n1. Test flag alone: `command --flag`\n2. Test with valid value: `command --flag value`\n3. Test with invalid value: `command --flag invalid`\n4. Test flag combinations: `command --flag1 --flag2`\n5. Test conflicting flags: `command --yes --no`\n```\n\n### Argument Testing\n\nTest required and optional arguments:\n\n```markdown\nValid inputs:\n- Correct type and format\n- Boundary values (min/max)\n- Typical use cases\n\nInvalid inputs:\n- Wrong type (string instead of number)\n- Out of range (negative when positive required)\n- Missing required arguments\n- Too many arguments\n- Special characters\n```\n\n### Scoped Testing Strategy\n\nFocus testing based on context:\n\n```markdown\nIf context says \"I updated the --config flag\":\n- Comprehensive tests on --config\n  - Valid config files\n  - Invalid config files\n  - Missing config files\n  - Malformed config data\n- Smoke tests on other flags (ensure still work)\n- Quick validation on core functionality\n\nBalance depth with scope:\n- Changed areas: Comprehensive testing\n- Related areas: Thorough testing\n- Unrelated areas: Smoke testing only\n```\n\n## Error Handling Validation\n\n### Exit Codes\n\nVerify proper exit codes:\n\n```markdown\nSuccess: Exit 0\nGeneral error: Exit 1\nUsage error: Exit 2\nPermission denied: Exit 126\nCommand not found: Exit 127\n```\n\nTest:\n```bash\ncommand; echo \"Exit code: $?\"\n```\n\n### Error Messages\n\nValidate error messages are:\n- **Clear**: Explain what went wrong\n- **Actionable**: Suggest how to fix\n- **Specific**: Not generic \"error occurred\"\n- **Helpful**: Include relevant details\n\n**Good error:**\n```\nError: Config file 'config.json' not found.\nPlease create a config file or specify path with --config\n```\n\n**Bad error:**\n```\nError\n```\n\n### stderr vs stdout\n\nVerify output streams used correctly:\n\n```bash\n# Errors should go to stderr\ncommand 2>&1 | grep \"Error\" # Should find errors here\n\n# Normal output should go to stdout\ncommand 2>/dev/null # Should show normal output\n```\n\n## Output Validation\n\n### Format Validation\n\nCheck output is correctly formatted:\n\n**JSON output:**\n```bash\ncommand --json | jq . # Should parse without error\n```\n\n**CSV output:**\n```bash\ncommand --csv | head -1 # Check headers present\n```\n\n**Table output:**\n```bash\ncommand --table | column # Check column alignment\n```\n\n### Correctness Validation\n\nVerify output matches expected:\n\n```markdown\nKnown inputs ‚Üí Verify outputs:\n1. Use predictable test data\n2. Run command\n3. Parse output\n4. Verify values match expected\n5. Report discrepancies\n```\n\n## Test Data Management\n\n### Using Test Data Generators\n\nCheck for test data support:\n\n```bash\n# Common patterns\ncommand test generate\ncommand fixtures create\ncommand seed --test\n```\n\nIf available:\n- Use for realistic testing\n- More comprehensive coverage\n- Matches app's data model\n\n### Creating Temporary Data\n\nWhen generating test data:\n\n```bash\n# Create temp directory\nTMPDIR=$(mktemp -d)\n\n# Create test files\necho \"test data\" > \"$TMPDIR/test.txt\"\n\n# Run command with test data\ncommand --input \"$TMPDIR/test.txt\"\n\n# Cleanup\nrm -rf \"$TMPDIR\"\n```\n\n### Safe Test Data\n\nGenerate data that:\n- Doesn't interfere with real data\n- Uses temporary locations (/tmp)\n- Avoids destructive operations\n- Is properly cleaned up after testing\n\n## Common CLI Issues\n\n### Missing Help Text\n\n**Issue**: No --help flag or unclear help\n**Impact**: Users don't know how to use command\n**Test**: Try `command --help`\n**Report**: Missing or inadequate help text\n\n### Poor Error Messages\n\n**Issue**: Cryptic or missing error messages\n**Impact**: Users can't diagnose problems\n**Test**: Trigger errors, check messages\n**Report**: Unhelpful error messages\n\n### Inconsistent Exit Codes\n\n**Issue**: Command always exits 0 even on errors\n**Impact**: Scripts can't detect failures\n**Test**: Cause error, check exit code\n**Report**: Exit code should be non-zero on error\n\n### Destructive Defaults\n\n**Issue**: Dangerous operations without confirmation\n**Impact**: Accidental data loss\n**Test**: Run potentially destructive commands\n**Report**: Should require --force flag or confirmation\n\n### Missing Input Validation\n\n**Issue**: Accepts invalid inputs without error\n**Impact**: Silent failures or unexpected behavior\n**Test**: Provide malformed inputs\n**Report**: Should validate and reject bad inputs\n\n## Testing Workflow\n\n### Complete CLI Test Flow\n\n```markdown\n1. Discover command and help text\n2. Run smoke tests (version, help, basic execution)\n3. Test each flag individually\n4. Test flag combinations\n5. Test with valid arguments\n6. Test with invalid arguments\n7. Test edge cases (empty, very long, special chars)\n8. Verify exit codes\n9. Validate error messages\n10. Check output formatting\n11. Report findings by severity\n```\n\n## Reporting Template\n\n```markdown\n## Critical Issues ‚ùå\n\n### Command Crashes on Empty Input - Critical\n- **Command**: `deploy`\n- **Issue**: Crashes when no environment specified\n- **Test**:\n  ```bash\n  ./deploy.sh\n  ```\n- **Exit Code**: 139 (Segmentation fault)\n- **Expected**: Exit 2 with usage message\n- **Impact**: Confusing user experience, possibly destructive\n\n## Error Handling Issues ‚ö†Ô∏è\n\n### Unhelpful Error Message - Medium\n- **Command**: `deploy --config invalid.json`\n- **Issue**: Error message doesn't explain problem\n- **Output**: `Error: Invalid config`\n- **Better**: `Error: Config file 'invalid.json' not found. Expected JSON file with keys: ...`\n\n## Documentation Issues üìù\n\n### Missing --help Flag - Medium\n- **Command**: `deploy --help`\n- **Issue**: No help text available\n- **Output**: `Unknown option: --help`\n- **Expected**: Usage information and option descriptions\n\n## Tests Passed ‚úÖ\n\n- Command executes successfully with valid inputs\n- Exit codes are correct (0 for success, 1 for errors)\n- Verbose flag produces additional output\n- Version flag shows current version\n```\n\n## Additional Resources\n\nFor complete CLI testing procedures, see:\n- **`references/exit-code-standards.md`** - Exit code conventions and validation\n- **`references/output-validation.md`** - Output format testing patterns"
              }
            ]
          },
          {
            "name": "auto-resize-images",
            "description": "Automatically resize oversized images before Claude API submission to prevent 2000px dimension limit errors",
            "source": "./auto-resize-images",
            "category": "productivity",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add tommymorgan/claude-plugins",
              "/plugin install auto-resize-images@tommymorgan"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T03:46:50Z",
              "created_at": "2025-12-10T20:10:41Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}