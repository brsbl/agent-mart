{
  "owner": {
    "id": "claude-market",
    "display_name": "Claude Market",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/241502931?v=4",
    "url": "https://github.com/claude-market",
    "bio": "Open source, curated marketplace for Claude Code.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 3,
      "total_commands": 12,
      "total_skills": 0,
      "total_stars": 4,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "claude-market/marketplace",
      "url": "https://github.com/claude-market/marketplace",
      "description": "Open source, hand-curated marketplace for Claude Code tools, agents and skills.",
      "homepage": null,
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2025-11-04T00:09:55Z",
        "created_at": "2025-11-01T20:24:32Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3830
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE/bug_report.md",
          "type": "blob",
          "size": 739
        },
        {
          "path": ".github/ISSUE_TEMPLATE/feature_request.md",
          "type": "blob",
          "size": 1042
        },
        {
          "path": ".github/ISSUE_TEMPLATE/plugin_submission.md",
          "type": "blob",
          "size": 1332
        },
        {
          "path": ".github/pull_request_template.md",
          "type": "blob",
          "size": 2174
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 352
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 6147
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 10572
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "Makefile",
          "type": "blob",
          "size": 140
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 6206
        },
        {
          "path": "claude-market-logo.png",
          "type": "blob",
          "size": 48562
        },
        {
          "path": "plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plans/SPECFORGE_BACKEND_PLUGIN.md",
          "type": "blob",
          "size": 58007
        },
        {
          "path": "plans/SPECFORGE_CLIENT_PLUGIN.md",
          "type": "blob",
          "size": 29312
        },
        {
          "path": "plans/SPECFORGE_DATABASE_PLUGIN.md",
          "type": "blob",
          "size": 31846
        },
        {
          "path": "plans/SPECFORGE_GENERATE_PLUGIN.md",
          "type": "blob",
          "size": 29652
        },
        {
          "path": "plans/SPECFORGE_PLAN.md",
          "type": "blob",
          "size": 56281
        },
        {
          "path": "plugin-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 841
        },
        {
          "path": "plugin-builder/CODEOWNERS",
          "type": "blob",
          "size": 73
        },
        {
          "path": "plugin-builder/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugin-builder/README.md",
          "type": "blob",
          "size": 13614
        },
        {
          "path": "plugin-builder/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/commands/add.md",
          "type": "blob",
          "size": 5612
        },
        {
          "path": "plugin-builder/commands/edit.md",
          "type": "blob",
          "size": 8237
        },
        {
          "path": "plugin-builder/commands/init.md",
          "type": "blob",
          "size": 9876
        },
        {
          "path": "plugin-builder/commands/publish.md",
          "type": "blob",
          "size": 6498
        },
        {
          "path": "plugin-builder/commands/validate.md",
          "type": "blob",
          "size": 6701
        },
        {
          "path": "plugin-builder/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/skills/cc-agent-builder.md",
          "type": "blob",
          "size": 14691
        },
        {
          "path": "plugin-builder/skills/cc-command-builder.md",
          "type": "blob",
          "size": 11398
        },
        {
          "path": "plugin-builder/skills/cc-hook-builder.md",
          "type": "blob",
          "size": 14632
        },
        {
          "path": "plugin-builder/skills/cc-mcp-builder.md",
          "type": "blob",
          "size": 15466
        },
        {
          "path": "plugin-builder/skills/cc-skill-builder.md",
          "type": "blob",
          "size": 10700
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/generate-marketplace-json.sh",
          "type": "blob",
          "size": 5973
        },
        {
          "path": "specforge-backend-rust-axum",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1011
        },
        {
          "path": "specforge-backend-rust-axum/CODEOWNERS",
          "type": "blob",
          "size": 62
        },
        {
          "path": "specforge-backend-rust-axum/LICENSE",
          "type": "blob",
          "size": 1074
        },
        {
          "path": "specforge-backend-rust-axum/README.md",
          "type": "blob",
          "size": 5765
        },
        {
          "path": "specforge-backend-rust-axum/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/agents/handler-implementer.md",
          "type": "blob",
          "size": 2646
        },
        {
          "path": "specforge-backend-rust-axum/agents/router-builder.md",
          "type": "blob",
          "size": 2194
        },
        {
          "path": "specforge-backend-rust-axum/agents/test-generator.md",
          "type": "blob",
          "size": 2623
        },
        {
          "path": "specforge-backend-rust-axum/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-error-mapping.md",
          "type": "blob",
          "size": 3142
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-handler-implementation.md",
          "type": "blob",
          "size": 3170
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-health-shutdown.md",
          "type": "blob",
          "size": 3355
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-idempotency.md",
          "type": "blob",
          "size": 3086
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-middleware.md",
          "type": "blob",
          "size": 4462
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-patterns.md",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-rate-limiting.md",
          "type": "blob",
          "size": 1930
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-resilience.md",
          "type": "blob",
          "size": 2591
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-timeouts.md",
          "type": "blob",
          "size": 2127
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-tracing.md",
          "type": "blob",
          "size": 2864
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-dev-setup.md",
          "type": "blob",
          "size": 1125
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-openapi-integration.md",
          "type": "blob",
          "size": 1532
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-testing.md",
          "type": "blob",
          "size": 5512
        },
        {
          "path": "specforge",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1099
        },
        {
          "path": "specforge/CODEOWNERS",
          "type": "blob",
          "size": 130
        },
        {
          "path": "specforge/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "specforge/README.md",
          "type": "blob",
          "size": 13888
        },
        {
          "path": "specforge/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/agents/captain-orchestrator.md",
          "type": "blob",
          "size": 10193
        },
        {
          "path": "specforge/agents/planning-agent.md",
          "type": "blob",
          "size": 10233
        },
        {
          "path": "specforge/agents/validation-agent.md",
          "type": "blob",
          "size": 19431
        },
        {
          "path": "specforge/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/commands/build.md",
          "type": "blob",
          "size": 15809
        },
        {
          "path": "specforge/commands/init.md",
          "type": "blob",
          "size": 10922
        },
        {
          "path": "specforge/commands/plan.md",
          "type": "blob",
          "size": 15685
        },
        {
          "path": "specforge/commands/ship.md",
          "type": "blob",
          "size": 19761
        },
        {
          "path": "specforge/commands/sync.md",
          "type": "blob",
          "size": 13032
        },
        {
          "path": "specforge/commands/test.md",
          "type": "blob",
          "size": 14707
        },
        {
          "path": "specforge/commands/validate.md",
          "type": "blob",
          "size": 10971
        },
        {
          "path": "specforge/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/skills/integration-expert.md",
          "type": "blob",
          "size": 15877
        },
        {
          "path": "specforge/skills/openapi-expert.md",
          "type": "blob",
          "size": 12523
        },
        {
          "path": "specforge/skills/stack-advisor.md",
          "type": "blob",
          "size": 12426
        }
      ],
      "marketplace": {
        "name": "claude-market",
        "version": "1.0.6",
        "description": "Hand-curated open source repository for Claude Code tools, agents and skills",
        "owner_info": {
          "name": "Claude Market",
          "url": "https://github.com/claude-market/marketplace"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "plugin-builder",
            "description": "Interactive plugin builder for Claude Code - serves as both an example plugin and a tool to create new plugins through guided prompts with specialized builder skills for each component type",
            "source": "./plugin-builder",
            "category": null,
            "version": "1.3.2",
            "author": {
              "name": "Daniel Kovacs",
              "email": "kovacsemod@gmail.com",
              "url": "https://github.com/danielkov"
            },
            "install_commands": [
              "/plugin marketplace add claude-market/marketplace",
              "/plugin install plugin-builder@claude-market"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2025-11-04T00:09:55Z",
              "created_at": "2025-11-01T20:24:32Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add",
                "description": "Add a new component to an existing plugin",
                "path": "plugin-builder/commands/add.md",
                "frontmatter": {
                  "description": "Add a new component to an existing plugin"
                },
                "content": "You are helping a user add a new component to an existing Claude Code plugin.\n\n## Step 1: Select Plugin\n\nUse Glob to find all plugins in `./*/.claude-plugin/plugin.json` and ask the user which plugin they want to add to (or let them specify a path).\n\n## Step 2: Read Current Plugin Configuration\n\nRead the plugin.json to understand what components already exist.\n\n## Step 3: Select Component Type to Add\n\nUse AskUserQuestion to ask what type of component they want to add:\n\n- **Slash Command** - Reusable prompt template for frequent operations\n- **Agent (Subagent)** - Specialized AI assistant for specific tasks\n- **Hook** - Automated workflow trigger at lifecycle events\n- **Skill** - Domain-specific expertise invoked when needed\n- **MCP Server** - External tool/data source via Model Context Protocol\n\nPresent these as clear options explaining what each type does.\n\n## Step 4: Collect Basic Information\n\nBased on the selected component type, collect the essential information:\n\n### For Slash Command:\n\nUse AskUserQuestion to collect:\n\n1. **Command name** (kebab-case, must not conflict with existing commands)\n2. **Brief description** (what does this command do?)\n\n### For Agent (Subagent):\n\nUse AskUserQuestion to collect:\n\n1. **Agent name** (kebab-case, must not conflict with existing agents)\n2. **Brief description** (when should this agent be invoked?)\n\n### For Hook:\n\nUse AskUserQuestion to collect:\n\n1. **Hook name** (kebab-case identifier)\n2. **Brief description** (what workflow does this automate?)\n\n### For Skill:\n\nUse AskUserQuestion to collect:\n\n1. **Skill name** (kebab-case, must not conflict with existing skills)\n2. **Brief description** (what domain expertise does this provide?)\n\n### For MCP Server:\n\nUse AskUserQuestion to collect:\n\n1. **Server name** (kebab-case identifier)\n2. **Brief description** (what tools/data does this provide?)\n\n## Step 5: Invoke Appropriate Builder Skill\n\nBased on the component type selected, invoke the corresponding builder skill to handle the detailed generation:\n\n### For Slash Command:\n\nInvoke the `cc-command-builder` skill:\n\n```\nCreate a new Claude Code slash command with the following details:\n- Plugin: [plugin-name]\n- Command name: [name]\n- Description: [description]\n- File path: [plugin-path]/commands/[command-name].md\n\nPlease guide the user through creating this command following best practices.\n```\n\n### For Agent (Subagent):\n\nInvoke the `cc-agent-builder` skill:\n\n```\nCreate a new Claude Code subagent with the following details:\n- Plugin: [plugin-name]\n- Agent name: [name]\n- Description: [description]\n- File path: [plugin-path]/agents/[agent-name].md\n\nPlease guide the user through creating this subagent following best practices.\n```\n\n### For Hook:\n\nInvoke the `cc-hook-builder` skill:\n\n```\nCreate a new Claude Code hook with the following details:\n- Plugin: [plugin-name]\n- Hook name: [name]\n- Description: [description]\n- File path: [plugin-path]/hooks/[hook-name].json\n\nPlease guide the user through creating this hook following best practices.\n```\n\n### For Skill:\n\nInvoke the `cc-skill-builder` skill:\n\n```\nCreate a new Claude Code skill with the following details:\n- Plugin: [plugin-name]\n- Skill name: [name]\n- Description: [description]\n- File path: [plugin-path]/skills/[skill-name].md\n\nPlease guide the user through creating this skill following best practices.\n```\n\n### For MCP Server:\n\nInvoke the `cc-mcp-builder` skill:\n\n```\nConfigure a new MCP server with the following details:\n- Plugin: [plugin-name]\n- Server name: [name]\n- Description: [description]\n- File path: [plugin-path]/mcp-servers/[server-name].json\n\nPlease guide the user through configuring this MCP server following best practices.\n```\n\n## Step 6: Update Plugin Manifest (Post-Generation)\n\n**IMPORTANT:** After the builder skill completes and creates the component file, you must update plugin.json to include the new component.\n\nAdd the new component file path to the appropriate array in plugin.json:\n\n- **commands**: Add to the `commands` array (e.g., `\"./commands/my-command.md\"`)\n- **agents**: Add to the `agents` array (e.g., `\"./agents/my-agent.md\"`)\n- **hooks**: Add path or inline config to `hooks` field\n- **skills**: Add to the `skills` array (e.g., `\"./skills/my-skill.md\"`)\n- **mcpServers**: Add path or inline config to `mcpServers` field\n\nAll paths must be relative to plugin root and begin with `./`\n\n**Example:**\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"commands\": [\"./commands/existing-command.md\", \"./commands/new-command.md\"],\n  \"agents\": [\"./agents/existing-agent.md\"],\n  \"skills\": [\"./skills/new-skill.md\"]\n}\n```\n\n## Step 7: Update README (Post-Generation)\n\nUpdate the plugin's README.md to document the new component with usage examples.\n\n## Step 8: Summary\n\nShow the user:\n\n- What was added (component type and name)\n- File path of the new component\n- Updated plugin.json showing the new component\n- How to use the new component\n- How to test it\n\n## Important Notes:\n\n- **Modular approach**: Each component type has its own specialized builder skill that handles the detailed generation\n- **Ensure no conflicts**: Check that component names don't conflict with existing ones before invoking builder skills\n- **Create directories**: Create component directories (commands/, agents/, hooks/, skills/, mcp-servers/) if they don't exist\n- **Update manifest**: Always update plugin.json after component creation\n- **Maintain consistency**: Ensure new components follow the style of existing ones\n- **Version bump**: Increment patch version in plugin.json (e.g., 1.0.0 â†’ 1.0.1)"
              },
              {
                "name": "/edit",
                "description": "Edit an existing component in your plugin",
                "path": "plugin-builder/commands/edit.md",
                "frontmatter": {
                  "description": "Edit an existing component in your plugin"
                },
                "content": "You are helping a user edit an existing component in a Claude Code plugin using natural language descriptions.\n\n## Step 1: Select Plugin\n\nUse Glob to find all plugins in `./*/.claude-plugin/plugin.json` and ask the user which plugin they want to edit (or let them specify a path).\n\n## Step 2: Read Current Plugin Configuration\n\nRead the plugin.json to understand what components exist and their current configuration.\n\n## Step 3: List Available Components\n\nDisplay all available components organized by type:\n\n- **Commands**: List all command files with their descriptions (from frontmatter if available)\n- **Agents**: List all agent files with their descriptions\n- **Hooks**: List all hook configurations\n- **Skills**: List all skill files with their descriptions\n- **MCP Servers**: List all MCP server configurations\n\nUse AskUserQuestion to ask which component they want to edit. Present options based on what exists in the plugin.\n\n## Step 4: Read the Selected Component\n\nRead the full content of the selected component file to understand its current implementation.\n\nDisplay a summary of the component to the user:\n\n- Component name and type\n- Current description\n- Key functionality (summarized)\n- File path\n\n## Step 5: Understand the Desired Edit\n\nAsk the user to describe what changes they want to make in natural language.\n\n**Prompt**: \"Please describe the changes you want to make to this component. Be as specific or general as you like.\"\n\nThe user might say things like:\n\n- \"Add validation for email addresses\"\n- \"Make it ask for confirmation before deleting\"\n- \"Change the default model from haiku to sonnet\"\n- \"Add better error handling\"\n- \"Include examples in the documentation\"\n- \"Make it work with TypeScript files too\"\n- \"Update the description to be clearer\"\n- \"Add support for multiple arguments\"\n\n## Step 6: Route to Appropriate Builder Skill\n\nBased on the component type, route the editing task to the corresponding builder skill with context about the existing component and desired changes:\n\n### For Slash Command:\n\nInvoke the `cc-command-builder` skill:\n\n```\nEdit an existing Claude Code slash command with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Command name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for slash commands.\n```\n\n### For Agent (Subagent):\n\nInvoke the `cc-agent-builder` skill:\n\n```\nEdit an existing Claude Code subagent with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Agent name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for subagents.\n```\n\n### For Hook:\n\nInvoke the `cc-hook-builder` skill:\n\n```\nEdit an existing Claude Code hook with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Hook name: [name]\n- File path: [path]\n- Current configuration:\n[paste current JSON]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for hooks.\n```\n\n### For Skill:\n\nInvoke the `cc-skill-builder` skill:\n\n```\nEdit an existing Claude Code skill with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Skill name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for skills.\n```\n\n### For MCP Server:\n\nInvoke the `cc-mcp-builder` skill:\n\n```\nEdit an existing MCP server configuration with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Server name: [name]\n- File path: [path]\n- Current configuration:\n[paste current JSON]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for MCP servers.\n```\n\n## Step 7: Update Plugin Metadata (If Needed)\n\nAfter the builder skill completes the edits, determine if other files need updating:\n\n- **If component description changed significantly**: Offer to update the README.md\n- **If functionality changed**: Offer to update the plugin.json description or keywords\n- **If the edit is substantial**: Offer to increment the version number (patch bump)\n\nUse AskUserQuestion to ask if they want to update these related files.\n\n## Step 8: Apply Additional Updates\n\nIf the user agreed to update related files:\n\n### Update README:\n\n- Find the section documenting this component\n- Update the usage examples or description\n- Ensure it reflects the new behavior\n\n### Update Plugin.json:\n\n- Increment version (e.g., 1.0.0 â†’ 1.0.1 for patch changes)\n- Update keywords if new functionality was added\n- Update description if plugin's overall purpose expanded\n\n## Step 9: Summary\n\nProvide the user with a clear summary:\n\n- **Component edited**: Name and file path\n- **Changes made**: Concise summary of what was modified\n- **Files updated**: List all files that were changed (component + README/manifest if applicable)\n- **New version**: If version was bumped\n- **Testing recommendation**: Suggest how to test the changes\n\n## Important Guidelines:\n\n### Modular Approach:\n\n- Each component type has its own specialized builder skill\n- Route editing tasks to the appropriate skill based on component type\n- Builder skills understand best practices for their component type\n- This ensures consistent, high-quality edits\n\n### Interpreting Natural Language:\n\n- **Be intelligent about intent**: If user says \"make it faster\", interpret based on context (use haiku model, optimize steps, etc.)\n- **Ask for clarification** if the request is genuinely ambiguous\n- **Make reasonable assumptions** for small details, but confirm major changes\n- **Preserve existing functionality** unless explicitly asked to remove it\n\n### Edit Best Practices:\n\n- **Make surgical changes**: Only modify what's necessary\n- **Preserve formatting**: Maintain the existing markdown/JSON style\n- **Keep consistency**: Match the style of the original component\n- **Test mentally**: Think through whether edits will work as intended\n- **Respect the component's purpose**: Don't change what the component fundamentally does unless explicitly asked\n\n### Validation:\n\n- **For .md files**: Ensure frontmatter is valid, markdown is well-formed\n- **For .json files**: Validate JSON syntax, ensure required fields are present\n- **For paths**: Ensure any file paths referenced still exist and are correct\n- **For tool usage**: Ensure tools referenced in prompts actually exist\n\n### Communication:\n\n- **Show before/after** for significant changes (builder skill handles this)\n- **Explain interpretation** of their natural language request\n- **Highlight assumptions** made during editing\n- **Offer to refine** if the edit wasn't quite what they wanted\n\n## Example Interaction Flow:\n\n1. Find plugins â†’ user selects \"plugin-builder\"\n2. Read plugin.json â†’ show components: init, add, validate, edit commands\n3. User selects â†’ \"add command\"\n4. Read component â†’ display current add.md implementation summary\n5. User describes â†’ \"Make it use the new modular builder skill approach\"\n6. Route to `cc-command-builder` skill with context\n7. Builder skill helps apply changes following best practices\n8. Offer to update README with new capability\n9. Update README if accepted\n10. Version bump â†’ Increment to next version\n11. Summary â†’ List all changes and testing instructions\n\n## Edge Cases to Handle:\n\n- **Component doesn't exist**: Guide user back to component selection\n- **Invalid edit request**: Ask for clarification if request doesn't make sense for this component type\n- **Conflicting changes**: Warn if edit might break existing functionality\n- **Syntax errors**: Builder skill should catch and fix syntax issues\n- **Multiple files**: If component spans multiple files (like hooks with shell scripts), edit all relevant ones\n\nBegin by finding available plugins and asking which one to edit!"
              },
              {
                "name": "/init",
                "description": "Initialize a new plugin with guided prompts",
                "path": "plugin-builder/commands/init.md",
                "frontmatter": {
                  "description": "Initialize a new plugin with guided prompts"
                },
                "content": "You are helping a user create a new Claude Code plugin. Follow this workflow step by step:\n\n## Step 1: Get GitHub Username and Fetch Profile\n\nUse the AskUserQuestion tool to ask for:\n- **GitHub username** - Will be used in CODEOWNERS and to fetch author information\n\nOnce you have the GitHub username, use WebFetch to fetch their profile:\n- URL: `https://api.github.com/users/{username}`\n- Extract the `name` field from the response\n- If `name` is present and not null, use it as the author name\n- If `name` is null or missing, use the GitHub username as the author name\n\n## Step 2: Collect Plugin Metadata\n\nAsk the user for:\n\n- **Plugin name** (kebab-case identifier, unique)\n- **Description** (what does this plugin do?)\n- **Version** (default: \"1.0.0\" if not specified)\n- **License** (default: \"MIT\" if unsure)\n- **Homepage** (optional - documentation URL)\n- **Repository** (optional - source code URL, can default to GitHub repo if they want)\n- **Keywords** (optional - array of tags for discoverability)\n- **Author email** (optional - contact email)\n- **Author URL** (optional - personal website/profile)\n\n## Step 3: Create Plugin Directory\n\nCreate the directory structure at the top level:\n\n```\n./{plugin-name}/\n./{plugin-name}/.claude-plugin/ # required\n./{plugin-name}/commands/ # optional - only if 1 or more commands added\n./{plugin-name}/agents/ # optional - only if 1 or more agents added\n./{plugin-name}/hooks/ # optional - only if 1 or more hooks added\n./{plugin-name}/skills/ # optional - only if 1 or more skills added\n./{plugin-name}/mcp-servers/ # optional - only if 1 or more MCP servers added\n```\n\n## Step 4: Select Components to Create\n\nUse AskUserQuestion with multiSelect=true to ask what components they want to create:\n\n- Slash Command\n- Agent (Subagent)\n- Hook\n- Skill\n- MCP Server\n\n## Step 5: For Each Component Type, Collect Details\n\n### If they selected \"Slash Command\":\n\nAsk these questions (you can ask multiple in one AskUserQuestion call):\n\n1. **Command name** (kebab-case, will be invoked as /plugin-name:command-name)\n2. **Command description** (one-line summary)\n3. **What should this command do?** (detailed explanation of the command's purpose and behavior)\n4. **What files/resources will it need to read or modify?**\n5. **Should it use any specific tools?** (e.g., Bash, Read, Edit, Grep, etc.)\n\nThen create:\n\n- `./{plugin-name}/commands/{command-name}.md` with a comprehensive prompt that:\n  - Clearly defines the command's purpose\n  - Provides step-by-step instructions\n  - Specifies which tools to use\n  - Includes examples if relevant\n  - Handles edge cases\n\n### If they selected \"Agent\":\n\nAsk these questions:\n\n1. **Agent name** (kebab-case)\n2. **Agent description** (what specialized task does it perform?)\n3. **What problem does this agent solve?**\n4. **What tools should it have access to?** (all tools, or specific subset?)\n5. **What should be the default model?** (haiku for quick tasks, sonnet for complex)\n6. **Any specific workflow or steps it should follow?**\n\nThen create:\n\n- `./{plugin-name}/agents/{agent-name}.md` with a detailed agent prompt that:\n  - Defines the agent's specialized role\n  - Specifies available tools\n  - Provides clear workflow steps\n  - Includes examples and best practices\n  - Defines success criteria\n\n### If they selected \"Hook\":\n\nAsk these questions:\n\n1. **Hook type** (choose one):\n   - user-prompt-submit (runs before user input is sent)\n   - tool-call (runs before/after tool execution)\n   - agent-start (runs when agent starts)\n   - agent-end (runs when agent completes)\n2. **Hook name** (descriptive name)\n3. **What behavior should this hook add/modify?**\n4. **Should it block certain actions or just add information?**\n5. **What command should it run?** (shell command)\n\nThen create:\n\n- `./{plugin-name}/hooks/{hook-name}.json` with proper hook configuration including:\n  - Hook type\n  - Trigger conditions\n  - Command to execute\n  - Whether it should block on failure\n\n### If they selected \"Skill\":\n\nAsk these questions:\n\n1. **Skill name** (kebab-case)\n2. **What domain/technology does this skill cover?**\n3. **What specialized knowledge or capabilities should it provide?**\n4. **What tools does it need access to?**\n5. **What specific tasks should users invoke it for?**\n\nThen create:\n\n- `./{plugin-name}/skills/{skill-name}.md` with a comprehensive skill definition that:\n  - Defines the domain expertise\n  - Lists specific capabilities\n  - Provides usage patterns\n  - Includes domain-specific best practices\n  - Specifies when to use this skill\n\n### If they selected \"MCP Server\":\n\nAsk these questions:\n\n1. **Server name** (kebab-case)\n2. **What tools/resources does this MCP server provide?**\n3. **Connection details** (stdio command, or SSE URL)\n4. **Any environment variables needed?**\n5. **What Claude Code features should it enable?**\n\nThen create:\n\n- `./{plugin-name}/mcp-servers/{server-name}.json` with MCP server configuration\n\n## Step 6: Create Plugin Manifest\n\nCreate `./{plugin-name}/.claude-plugin/plugin.json` following the complete schema:\n\n### Required Fields\n\n- **name** (string): Plugin identifier in kebab-case, unique across all plugins\n\n### Metadata Fields (All Optional)\n\n- **version** (string): Semantic versioning (e.g., \"1.0.0\", \"2.1.0\")\n- **description** (string): Brief explanation of plugin purpose\n- **author** (object): Author information with these properties:\n  - `name` (string): Author or organization name (from GitHub profile or username)\n  - `email` (string, optional): Contact email address\n  - `url` (string, optional): Author's website or profile URL\n- **homepage** (string): Documentation URL link\n- **repository** (string): Source code repository URL\n- **license** (string): License identifier (MIT, Apache-2.0, etc.)\n- **keywords** (array of strings): Discovery and categorization tags\n\n### Component Path Fields (Optional)\n\nList the components you want to include in your plugin:\n\n- **commands** (string or array): Command markdown files (e.g., `[\"./commands/init.md\", \"./commands/add.md\"]`)\n- **agents** (string or array): Agent markdown files (e.g., `[\"./agents/optimizer.md\"]`)\n- **hooks** (string or object): Hook configuration file path or inline JSON config\n- **mcpServers** (string or object): MCP server configuration file path or inline config\n- **skills** (string or array): Skill markdown files (e.g., `[\"./skills/react.md\"]`)\n\nAll paths must be relative to plugin root and begin with `./`\n\n### Example Plugin Manifest\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A helpful plugin for productivity\",\n  \"author\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\",\n    \"url\": \"https://github.com/johndoe\"\n  },\n  \"homepage\": \"https://github.com/johndoe/my-plugin\",\n  \"repository\": \"https://github.com/johndoe/my-plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"helper\", \"productivity\", \"automation\"],\n  \"commands\": [\n    \"./commands/helper.md\"\n  ],\n  \"agents\": [\n    \"./agents/assistant.md\"\n  ]\n}\n```\n\n**Schema Notes:**\n- Only `name` is required; all other fields are optional\n- `author` must be an object (not a string) if provided\n- Components must be explicitly listed in their respective arrays (e.g., commands, agents, skills)\n- Include only fields that have values; omit empty/null fields\n\n## Step 7: Create CODEOWNERS\n\nCreate `./{plugin-name}/CODEOWNERS` file with the following format:\n\n```\n# Plugin maintainers and reviewers\n* @claude-market @{github-username}\n```\n\nReplace:\n- `{github-username}` with the GitHub username from Step 1\n\nThis ensures that:\n- The Claude Market organization is always notified\n- The plugin creator's GitHub account is tagged for review\n\n## Step 8: Create README\n\nCreate a README.md in the plugin directory that includes:\n\n- Plugin name and description\n- Installation instructions (`/plugin install ./{plugin-name}`)\n- List of components with usage examples\n- Requirements (if any)\n- License information\n\n## Step 9: Summary\n\nProvide the user with:\n\n- Path to their new plugin\n- List of all created files\n- Next steps (how to test it, how to install it locally, how to submit to marketplace)\n- Command to install locally: `/plugin install ./{plugin-name}`\n\n## Important Guidelines:\n\n- **Write comprehensive, detailed prompts** for commands/agents/skills. The quality of the plugin depends on clear, actionable instructions.\n- **Include examples** wherever possible to illustrate expected behavior.\n- **Think about edge cases** and include handling for them.\n- **Use proper markdown formatting** including code blocks, lists, and sections.\n- **Follow Claude Code best practices**:\n  - Commands should use appropriate tools (Read, Edit, Grep, Glob, Bash)\n  - Agents should have clear, focused purposes\n  - Hooks should be non-intrusive and helpful\n- **Validate inputs**: ensure names are in kebab-case, descriptions are clear, etc.\n\n## Example Interaction Flow:\n\n1. Ask for GitHub username â†’ \"awesome-dev\"\n2. Fetch GitHub profile â†’ Extract name \"Awesome Developer\" (or use \"awesome-dev\" if no name)\n3. Ask for plugin metadata â†’ name: \"react-helpers\", description: \"Helpers for React development\", email (optional), etc.\n4. Create `./react-helpers/` directory\n5. Ask what to create â†’ [Slash Command, Agent]\n6. For command â†’ name: \"add-component\", description: \"Add a new React component with tests\"\n7. Collect detailed requirements for the command\n8. Generate well-structured command file\n9. For agent â†’ name: \"react-optimizer\", description: \"Optimize React components for performance\"\n10. Collect agent requirements\n11. Generate agent file\n12. Create plugin.json with complete metadata (name from GitHub, optional email/url if provided)\n13. Create CODEOWNERS with @claude-market @awesome-dev\n14. Create README.md\n15. Show summary and next steps\n\nBegin by asking for the GitHub username!"
              },
              {
                "name": "/publish",
                "description": null,
                "path": "plugin-builder/commands/publish.md",
                "frontmatter": null,
                "content": "# Publish Command\n\nYou are helping publish a Claude Code plugin to the Claude Market marketplace.\n\n## Overview\n\nThis command validates changed plugins, generates the marketplace manifest, creates a semantic commit, and opens a pull request to the marketplace repository.\n\n## Step 1: Identify Changed Plugins\n\nCheck the current branch with `git branch --show-current`.\n\nIf on `main` branch:\n- Run `git status --porcelain` to find uncommitted/staged changes\n- Parse the output to identify which plugins have been modified\n\nIf on a different branch:\n- Run `git diff main --name-only` to find all changed files compared to main\n- Parse the output to identify which plugins have been modified\n\nLook for paths matching `{plugin-name}/.claude-plugin/plugin.json` or `{plugin-name}/*`.\n\nExtract the unique plugin names from the changed paths.\n\nIf no plugin changes are detected, inform the user and exit.\n\n## Step 2: Branch Management\n\nCheck the current branch (already checked in Step 1).\n\nIf on `main` branch:\n\n- Attempt to get GitHub username with `git config user.github || git config user.name`\n- Create branch name in format: `{github-user}/{plugin-affected}/{very-short-description}`\n  - Use kebab-case for all parts\n  - Keep description to 3-4 words max\n  - Example: `danielkov/browser-tools/add-chromium-mcp`\n- Run `git checkout -b {branch-name}` to create and switch to new branch\n- **Note**: Do not stage or commit yet - changes will be committed after validation passes\n\nIf on a different branch (not main):\n\n- Continue with the existing branch\n- **Note**: Do not stage or commit yet - changes will be committed after validation passes\n\n## Step 3: Validate Changed Plugins in Parallel\n\nFor each changed plugin identified in Step 1, spawn a sub-agent using the Task tool to validate the plugin.\n\nUse a single message with multiple Task tool calls to validate all plugins simultaneously:\n\n```\nFor each plugin, spawn an agent with:\n- subagent_type: \"general-purpose\"\n- model: \"haiku\" (for speed)\n- description: \"Validate {plugin-name}\"\n- prompt: \"Run the command `/plugin-builder:validate {plugin-name} --minimal` and return ONLY the exit code and output. The output should start with either '0' (success) or '1' (failure) followed by any error details.\"\n```\n\nWait for all validation agents to complete and collect their outputs.\n\n## Step 4: Check Validation Results\n\nParse each agent's output:\n\n- If output starts with `0`: Plugin passed validation\n- If output starts with `1`: Plugin failed validation\n\nIf **all** plugins pass validation (all outputs start with `0`):\n\n- Proceed to Step 5\n\nIf **any** plugin fails validation (any output starts with `1`):\n\n- For each failed plugin, display:\n  - Plugin name\n  - The full validation output (excluding the leading `1`)\n- Exit the command with an error message\n- Do not proceed further\n\n## Step 5: Generate Marketplace Manifest\n\nRun `make generate-marketplace-json` in the project root directory to update `.claude-plugin/marketplace.json`.\n\nThis script automatically reads all plugin manifests and generates the marketplace listing.\n\n## Step 6: Create Semantic Commit Message\n\nAnalyze the changes using `git diff` to understand what was modified in each plugin.\n\nCreate a short, concise semantic commit message following this format:\n\n- `feat({plugin-name}): {short description}` - for new features\n- `fix({plugin-name}): {short description}` - for bug fixes\n- `docs({plugin-name}): {short description}` - for documentation changes\n- `chore({plugin-name}): {short description}` - for maintenance tasks\n\nIf multiple plugins are affected, use the most relevant plugin name or use `marketplace` as the scope.\n\nExamples:\n\n- `feat(browser-tools): added chromium mcp server`\n- `fix(plugin-builder): corrected validation logic`\n- `docs(security-toolkit): updated usage examples`\n\nThe description should be lowercase, concise, and explain **what** changed, not **how** it changed.\n\n## Step 7: Commit and Push Changes\n\nStage all changes: `git add .`\n\nCommit with semantic message: `git commit -m \"{semantic-commit-message}\"`\n\nPush the branch:\n- If on a newly created branch (from Step 2): `git push -u origin {branch-name}`\n- If on an existing branch: `git push`\n\n## Step 8: Create Pull Request\n\nCheck if `gh` CLI is available by running `gh --version`.\n\n### If `gh` CLI is available:\n\nRead `.github/pull_request_template.md` to understand the PR template structure.\n\nFor each changed plugin, read its `.claude-plugin/plugin.json` to extract:\n\n- Plugin name\n- Author\n- Version\n- License\n- Keywords\n- Component counts (commands, agents, hooks, skills, mcpServers)\n\nAlso read the plugin's README.md for the overview and usage examples.\n\nFill in the PR template with:\n\n- **Title**: Use the semantic commit message\n- **Overview**: Extract from plugin README.md or describe the changes\n- **Plugin Information**: Fill from plugin.json\n- **Components Included**: Check the appropriate boxes and counts\n- **What does this plugin do?**: Extract from README.md\n- **Example Usage**: Extract from README.md or create based on components\n- **Testing Checklist**: Mark relevant items as checked based on what was done\n- **Documentation**: Mark relevant items as checked\n- **Code Quality**: Mark relevant items as checked\n\nCreate the PR using:\n\n```bash\ngh pr create --title \"{semantic-commit-message}\" --body \"{filled-template}\"\n```\n\nUse a HEREDOC to properly format the body:\n\n```bash\ngh pr create --title \"{semantic-commit-message}\" --body \"$(cat <<'EOF'\n{filled-template-content}\nEOF\n)\"\n```\n\nDisplay the PR URL to the user.\n\n### If `gh` CLI is NOT available:\n\nDisplay a message to the user:\n\n```\nâœ“ Changes committed and pushed successfully!\n\nTo create a pull request:\n1. Visit: https://github.com/claude-market/marketplace/compare/{branch-name}\n2. Fill in the PR template with the changes you made\n\nðŸ’¡ Tip: Install the GitHub CLI for automated PR creation:\n   https://cli.github.com/manual/installation\n\nBranch: {branch-name}\nCommit: {semantic-commit-message}\n```\n\n## Important Notes\n\n- Always validate plugins before publishing\n- Use parallel validation for efficiency\n- Generate semantic commit messages automatically\n- Follow kebab-case naming for branches\n- Fill PR template comprehensively\n- Handle both scenarios: with/without gh CLI\n- Be clear and concise in all communications\n\n## Error Handling\n\nIf any step fails:\n\n- Display the error clearly to the user\n- Indicate which step failed\n- Suggest remediation if possible\n- Do not proceed to subsequent steps\n"
              },
              {
                "name": "/validate",
                "description": "Validate plugin structure and configuration",
                "path": "plugin-builder/commands/validate.md",
                "frontmatter": {
                  "description": "Validate plugin structure and configuration"
                },
                "content": "You are validating a Claude Code plugin's structure and configuration.\n\n## Step 1: Select Plugin\n\nAsk the user which plugin to validate, or use Glob to find all plugins in `./*/.claude-plugin/plugin.json` and let them choose.\n\n## Step 2: Validation Checks\n\nPerform these validation checks:\n\n### Structure Validation\n\n1. **Required files exist:**\n\n   - `.claude-plugin/plugin.json` must exist\n   - `CODEOWNERS` must exist\n   - At least one component directory should have content\n\n2. **Directory structure:**\n\n   - `commands/` for commands (optional - only if commands exist)\n   - `agents/` for agents (optional - only if agents exist)\n   - `hooks/` for hooks (optional - only if hooks exist)\n   - `skills/` for skills (optional - only if skills exist)\n   - `mcp-servers/` for MCP servers (optional - only if MCP servers exist)\n\n3. **CODEOWNERS validation:**\n   - File exists at plugin root\n   - Contains @claude-market\n   - Contains at least one GitHub username\n   - Format is valid (pattern: \\* @org @user name)\n\n### Plugin Manifest Validation\n\nRead and validate `.claude-plugin/plugin.json`:\n\n1. **Required fields:**\n\n   - `name` (string, kebab-case, unique identifier)\n   - No other fields are strictly required\n\n2. **Optional metadata fields:**\n\n   - `version` (string, semantic versioning like \"1.0.0\")\n   - `description` (string, clear and concise)\n   - `author` (object with `name`, optional `email` and `url`)\n   - `homepage` (string, documentation URL)\n   - `repository` (string, source code URL)\n   - `license` (string, license identifier like \"MIT\")\n   - `keywords` (array of strings for discoverability)\n\n3. **Author field format:**\n\n   - If present, `author` must be an object with a `name` property\n   - Optional properties: `email` (string), `url` (string)\n   - **Correct format:** `\"author\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"url\": \"https://example.com\"}`\n   - **Incorrect format:** `\"author\": \"John Doe\"`\n\n4. **Component path fields (optional):**\n\n   List the components your plugin provides:\n\n   - `commands` (string or array): Command markdown file paths (e.g., `[\"./commands/init.md\"]`)\n   - `agents` (string or array): Agent markdown file paths (e.g., `[\"./agents/helper.md\"]`)\n   - `skills` (string or array): Skill markdown file paths (e.g., `[\"./skills/expert.md\"]`)\n   - `hooks` (string or object): Hook configuration file path or inline config\n   - `mcpServers` (string or object): MCP server configuration file path or inline config\n\n   All paths must:\n\n   - Be relative to plugin root\n   - Begin with `./`\n\n5. **JSON validity:**\n   - Properly formatted JSON\n   - No syntax errors\n\n### Component File Validation\n\nCheck that all components listed in plugin.json exist and are valid:\n\n1. **Commands** (check files listed in `commands` array):\n\n   - Each listed file path exists\n   - Each file should contain frontmatter with description\n   - Has meaningful content (not empty)\n   - Uses proper markdown formatting\n   - File names should be in kebab-case\n\n2. **Agents** (check files listed in `agents` array):\n\n   - Each listed file path exists\n   - Contains clear instructions\n   - Defines specialized purpose\n   - Has meaningful content\n   - File names should be in kebab-case\n\n3. **Hooks** (check path in `hooks` field):\n\n   - File path exists if specified\n   - Valid JSON format\n   - Contains required hook configuration\n   - Hook type is valid\n   - File name should be in kebab-case\n\n4. **Skills** (check files listed in `skills` array):\n\n   - Each listed file path exists\n   - Defines domain expertise\n   - Contains clear usage instructions\n   - Has meaningful content\n   - File names should be in kebab-case\n\n5. **MCP Servers** (check path in `mcpServers` field):\n   - File path exists if specified\n   - Valid JSON format\n   - Contains connection configuration\n   - File name should be in kebab-case\n\n**Additional checks:**\n\n- Verify all component file paths are relative and begin with `./`\n- Check that listed components actually exist at their specified paths\n- Warn about component files that exist but aren't listed in plugin.json\n\n### Content Quality Checks\n\n1. **Names are kebab-case:** Check all component names\n2. **Descriptions are present and clear:** Not empty or placeholder text\n3. **No duplicate names:** Across all components\n4. **Files have substantial content:** Not just placeholders\n\n### Documentation Validation\n\n1. **README.md exists**\n2. **README contains:**\n   - Plugin name and description\n   - Installation instructions\n   - Usage examples for components\n   - License information\n3. **LICENSE file exists**\n\n## Step 3: Report Results\n\nCreate a structured validation report:\n\n### âœ“ Passed Checks\n\nList all checks that passed\n\n### âœ— Failed Checks\n\nList all checks that failed with:\n\n- What failed\n- Why it's a problem\n- How to fix it\n\n### âš  Warnings\n\nList recommendations for improvement:\n\n- Missing optional fields\n- Content that could be more detailed\n- Best practices not followed\n\n## Step 4: Recommendations\n\nProvide actionable recommendations:\n\n- Critical fixes needed before the plugin can work\n- Suggested improvements for better user experience\n- Best practices to follow\n\n## Example Output:\n\n```\nValidating plugin: awesome-plugin\n\nâœ“ Passed Checks:\n  - Plugin manifest exists\n  - All required fields present\n  - JSON is valid\n  - All listed commands have corresponding files\n  - Directory structure is correct\n  - CODEOWNERS file exists and is valid\n  - README.md exists\n  - LICENSE file exists\n\nâœ— Failed Checks:\n  - File commands/broken-cmd.md found but contains no content\n    Fix: Add meaningful content to the command file or remove it\n\nâš  Warnings:\n  - No version specified in plugin.json (recommended for tracking)\n  - Command \"init\" frontmatter description is minimal (could be more detailed)\n  - No keywords specified (helps with discoverability)\n  - No author information provided (recommended for attribution)\n  - CODEOWNERS doesn't include @claude-market (required for marketplace submissions)\n\nRecommendations:\n  1. Add version field to plugin.json (suggest starting at \"1.0.0\")\n  2. Expand command descriptions to be more informative\n  3. Add relevant keywords for marketplace discoverability\n  4. Consider adding usage examples to README\n```\n\nBe thorough but constructive in validation feedback.\n\n## Example Output (with `--minimal` flag)\n\nIf `--minimal` flag is present in input, the following output structure MUST BE FOLLOWED:\n\n### On Success\n\n```\n0\n```\n\n### On Failure\n\n```\n1\nâœ— Failed Checks:\n  - File commands/broken-cmd.md found but contains no content\n    Fix: Add meaningful content to the command file or remove it\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "specforge",
            "description": "Schema-first development ecosystem with dual-spec workflows (OpenAPI + DB schema) for building production-ready applications through deterministic code generation and intelligent orchestration",
            "source": "./specforge",
            "category": null,
            "version": "0.1.2",
            "author": {
              "name": "Daniel Emod Kovacs",
              "url": "https://github.com/danielkov"
            },
            "install_commands": [
              "/plugin marketplace add claude-market/marketplace",
              "/plugin install specforge@claude-market"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2025-11-04T00:09:55Z",
              "created_at": "2025-11-01T20:24:32Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/build",
                "description": "Apply specs, generate code, implement handlers with test-driven iteration",
                "path": "specforge/commands/build.md",
                "frontmatter": {
                  "name": "build",
                  "description": "Apply specs, generate code, implement handlers with test-driven iteration"
                },
                "content": "# SpecForge Build Orchestrator\n\nApply spec changes, run code generation, implement handlers, test and iterate until everything works.\n\n## Overview\n\nThe build process is orchestrated in phases:\n\n1. **Apply Spec Changes** - Update OpenAPI and database schemas\n2. **Code Generation** - Generate type-safe code from schemas\n3. **Handler Implementation** - Implement business logic in parallel\n4. **Test & Iterate** - Run tests, observe behavior, fix issues\n5. **Integration** - Wire everything together and verify\n\n## Phase 1: Apply Spec Changes\n\n### Apply Database Migrations\n\nRead the SpecForge configuration from CLAUDE.md to identify the database plugin:\n\n```bash\n# Extract database plugin from CLAUDE.md\ngrep \"database:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nUse the database plugin's migration skill:\n\n```\nInvoke the {database-plugin}/migrations-expert skill with:\n- action: \"apply-migrations\"\n- migrations_dir: \"migrations/\"\n- database_url: from environment or docker-compose\n```\n\nSteps:\n\n1. List pending migrations\n2. Backup database (if production)\n3. Apply migrations in order\n4. Verify schema matches expected state\n5. Handle errors (rollback if needed)\n\n### Verify OpenAPI Spec\n\nUse the **openapi-expert** skill to validate the spec:\n\n```\nInvoke openapi-expert skill with:\n- action: \"validate-spec\"\n- spec_path: \"spec/openapi.yaml\"\n```\n\nValidation checks:\n\n- Valid OpenAPI 3.x syntax\n- All $ref references resolve\n- Schemas are well-formed\n- Examples match schemas\n- No duplicate operation IDs\n\n## Phase 2: Code Generation\n\n### Run Codegen Pipeline (DB Schema â†’ Types)\n\nIdentify the codegen plugin from CLAUDE.md:\n\n```bash\ngrep \"codegen:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nInvoke the codegen plugin's generation skill:\n\n```\nUse {codegen-plugin}/codegen-expert skill with:\n- action: \"generate-from-schema\"\n- schema_dir: \"migrations/\"\n- output_dir: \"{backend-path}/src/generated/db\"\n- database_url: from environment\n```\n\nThis generates:\n\n- Type-safe database models (structs, classes, types)\n- Query functions with compile-time verification\n- Database client code\n- Schema types and enums\n\n**Example for rust-sql codegen:**\n\n```rust\n// Generated: backend/src/generated/db/models.rs\n#[derive(Debug, Clone, sqlx::FromRow)]\npub struct Order {\n    pub id: i64,\n    pub user_id: i64,\n    pub total_cents: i64,\n    pub status: OrderStatus,\n    pub created_at: chrono::DateTime<Utc>,\n}\n\n#[derive(Debug, Clone, sqlx::Type)]\n#[sqlx(type_name = \"TEXT\")]\npub enum OrderStatus {\n    Pending,\n    Completed,\n    Cancelled,\n}\n\n// Generated: backend/src/generated/db/queries.rs\npub async fn get_orders_by_user_id(\n    pool: &SqlitePool,\n    user_id: i64,\n    limit: i64,\n    offset: i64,\n) -> Result<Vec<Order>, sqlx::Error> {\n    sqlx::query_as!(\n        Order,\n        r#\"\n        SELECT id, user_id, total_cents, status as \"status: OrderStatus\",\n               created_at\n        FROM orders\n        WHERE user_id = ?\n        ORDER BY created_at DESC\n        LIMIT ? OFFSET ?\n        \"#,\n        user_id,\n        limit,\n        offset\n    )\n    .fetch_all(pool)\n    .await\n}\n```\n\n### Generate OpenAPI Types\n\nIdentify the backend plugin:\n\n```bash\ngrep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nInvoke backend plugin's OpenAPI type generation:\n\n```\nUse {backend-plugin}/openapi-types-expert skill with:\n- action: \"generate-types\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{backend-path}/src/generated/api\"\n```\n\nThis generates:\n\n- Request/Response DTOs\n- Validation schemas\n- Serialization/Deserialization code\n\n### Frontend Client Generation (Optional)\n\nIf frontend plugin is configured:\n\n```\nUse {frontend-plugin}/codegen-expert skill with:\n- action: \"generate-client\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{frontend-path}/src/api\"\n```\n\n## Phase 3: Handler Implementation\n\n### Parse OpenAPI Spec\n\nRead the OpenAPI spec and extract all endpoints:\n\n```javascript\nconst spec = parseYAML(\"spec/openapi.yaml\");\nconst endpoints = [];\n\nfor (const [path, methods] of Object.entries(spec.paths)) {\n  for (const [method, operation] of Object.entries(methods)) {\n    if ([\"get\", \"post\", \"put\", \"patch\", \"delete\"].includes(method)) {\n      endpoints.push({\n        path,\n        method: method.toUpperCase(),\n        operationId: operation.operationId,\n        summary: operation.summary,\n        description: operation.description,\n        parameters: operation.parameters || [],\n        requestBody: operation.requestBody,\n        responses: operation.responses,\n        complexity: assessComplexity(operation),\n      });\n    }\n  }\n}\n```\n\n### Assess Endpoint Complexity\n\nFor each endpoint, determine complexity to select appropriate agent model:\n\n**Simple CRUD (Haiku)**:\n\n- Single database query\n- Basic validation\n- Standard CRUD operation\n- No transactions\n- No external API calls\n\n**Complex (Sonnet)**:\n\n- Multiple database queries\n- Transactions required\n- Complex business logic\n- External API integration\n- Advanced error handling\n\n### Parallel Handler Implementation\n\nGroup endpoints by complexity and spawn handler agents in parallel (max 5 concurrent):\n\n```javascript\nconst simpleEndpoints = endpoints.filter((e) => e.complexity === \"simple\");\nconst complexEndpoints = endpoints.filter((e) => e.complexity === \"complex\");\n\n// Process in batches of 5\nconst batches = chunk([...simpleEndpoints, ...complexEndpoints], 5);\n\nfor (const batch of batches) {\n  const results = await Promise.all(\n    batch.map((endpoint) => {\n      const agentModel = endpoint.complexity === \"simple\" ? \"haiku\" : \"sonnet\";\n      const backendPlugin = getBackendPlugin(); // from CLAUDE.md\n\n      return invokeAgent(`${backendPlugin}/handler-agent`, {\n        model: agentModel,\n        endpoint: endpoint,\n        generated_db_types: `${backendPath}/src/generated/db`,\n        generated_api_types: `${backendPath}/src/generated/api`,\n        patterns: getBackendPatterns(backendPlugin),\n      });\n    })\n  );\n\n  // Track results for Phase 4\n  handlerResults.push(...results);\n}\n```\n\n### Handler Agent Instructions\n\nEach handler agent receives:\n\n**Context**:\n\n```json\n{\n  \"endpoint\": {\n    \"path\": \"/api/users/{id}/orders\",\n    \"method\": \"GET\",\n    \"description\": \"...\",\n    \"parameters\": [...],\n    \"responses\": {...}\n  },\n  \"generated_db_types\": \"backend/src/generated/db\",\n  \"generated_api_types\": \"backend/src/generated/api\",\n  \"patterns\": \"Framework-specific patterns documentation\"\n}\n```\n\n**Instructions**:\n\n```\n1. Import generated database models and queries\n2. Import generated API request/response types\n3. Implement handler following framework patterns\n4. Use ONLY generated queries - NO manual SQL\n5. Handle all error cases from OpenAPI spec\n6. Add logging for debugging\n7. Return handler file path when complete\n```\n\n**Example Handler Implementation (Rust/Axum)**:\n\n```rust\n// backend/src/handlers/orders.rs\n\nuse axum::{\n    extract::{Path, Query, State},\n    http::StatusCode,\n    response::IntoResponse,\n    Json,\n};\n\n// Import generated types\nuse crate::generated::db::{get_orders_by_user_id, get_user_by_id, count_orders_by_user_id};\nuse crate::generated::api::{OrdersResponse, Pagination, PaginationParams};\nuse crate::error::ApiError;\nuse crate::AppState;\n\npub async fn get_user_orders(\n    State(state): State<AppState>,\n    Path(user_id): Path<i64>,\n    Query(params): Query<PaginationParams>,\n) -> Result<impl IntoResponse, ApiError> {\n    // 1. Validate user exists\n    let _user = get_user_by_id(&state.db, user_id)\n        .await?\n        .ok_or(ApiError::NotFound(\"User not found\".to_string()))?;\n\n    // 2. Calculate offset from page\n    let limit = params.limit.min(100); // Enforce max\n    let offset = (params.page - 1) * limit;\n\n    // 3. Get orders using generated query\n    let orders = get_orders_by_user_id(&state.db, user_id, limit, offset).await?;\n\n    // 4. Get total count for pagination\n    let total = count_orders_by_user_id(&state.db, user_id).await?;\n\n    // 5. Build response\n    Ok(Json(OrdersResponse {\n        data: orders,\n        pagination: Pagination {\n            page: params.page,\n            limit,\n            total,\n            total_pages: (total + limit - 1) / limit,\n        },\n    }))\n}\n```\n\n## Phase 4: Test & Iterate\n\nFor each implemented handler, run a test-driven iteration loop:\n\n### Test Generation & Execution\n\n```javascript\nfor (const handler of handlerResults) {\n  let success = false;\n  let iterationCount = 0;\n  const MAX_ITERATIONS = 3;\n\n  while (!success && iterationCount < MAX_ITERATIONS) {\n    // 1. Generate and run tests\n    const testResult = await invokeAgent(`${backendPlugin}/test-agent`, {\n      model: \"haiku\",\n      handler_path: handler.path,\n      endpoint: handler.endpoint,\n      generated_types: handler.generated_types,\n    });\n\n    if (testResult.status === \"passed\") {\n      success = true;\n      console.log(`âœ“ ${handler.endpoint.path} tests passed`);\n      break;\n    }\n\n    // 2. Diagnose issues\n    const diagnostic = await invokeAgent(`${codegenPlugin}/diagnostics-agent`, {\n      model: \"sonnet\",\n      handler_path: handler.path,\n      errors: testResult.errors,\n      compile_errors: testResult.compile_errors,\n      test_failures: testResult.test_failures,\n      generated_code_path: handler.generated_types,\n    });\n\n    // 3. Apply fixes\n    await applyFixes(diagnostic.fixes);\n\n    iterationCount++;\n  }\n\n  if (!success) {\n    console.error(\n      `âœ— Failed to get ${handler.endpoint.path} working after ${MAX_ITERATIONS} iterations`\n    );\n    console.error(\"Last errors:\", testResult.errors);\n\n    // Ask user for guidance\n    const userDecision = await askUser({\n      question: `Handler for ${handler.endpoint.path} failed tests. What should I do?`,\n      options: [\n        \"Skip for now and continue\",\n        \"Try again with more iterations\",\n        \"Show me the errors and let me fix it\",\n        \"Abort the build\",\n      ],\n    });\n\n    handleUserDecision(userDecision);\n  }\n}\n```\n\n### Test Agent Responsibilities\n\nThe test agent should:\n\n1. **Generate Unit Tests**:\n\n   - Test handler logic with mocked database\n   - Test validation rules\n   - Test error cases from OpenAPI\n\n2. **Generate Integration Tests**:\n\n   - Full HTTP request â†’ database â†’ response flow\n   - Test with real database (test fixtures)\n   - Verify response format matches OpenAPI\n\n3. **Run Tests**:\n\n   - Execute test suite\n   - Capture compile errors, test failures, runtime errors\n   - Return detailed error information\n\n4. **Behavior Observation**:\n   - Log SQL queries executed\n   - Verify response schema matches OpenAPI\n   - Check error responses\n\n### Diagnostics Agent Responsibilities\n\nWhen tests fail, the diagnostics agent analyzes:\n\n1. **Compile Errors**:\n\n   - Type mismatches between generated code and handler\n   - Missing imports\n   - Incorrect function signatures\n\n2. **Test Failures**:\n\n   - Logic errors in handler implementation\n   - Incorrect query parameters\n   - Response format issues\n\n3. **Runtime Errors**:\n   - Database connection issues\n   - Query errors (schema mismatches)\n   - Serialization failures\n\n**Diagnostic Output**:\n\n```json\n{\n  \"status\": \"errors_found\",\n  \"issues\": [\n    {\n      \"type\": \"compile_error\",\n      \"location\": \"backend/src/handlers/orders.rs:45\",\n      \"message\": \"expected i64, found i32\",\n      \"fix\": \"Change parameter type to i64 to match generated query signature\"\n    },\n    {\n      \"type\": \"test_failure\",\n      \"test\": \"test_get_user_orders_pagination\",\n      \"message\": \"assertion failed: total_pages == 3\",\n      \"fix\": \"Pagination calculation is incorrect. Use (total + limit - 1) / limit\"\n    }\n  ],\n  \"fixes\": [\n    {\n      \"file\": \"backend/src/handlers/orders.rs\",\n      \"changes\": [...]\n    }\n  ]\n}\n```\n\n## Phase 5: Integration & Verification\n\n### Wire Up Handlers to Router\n\nOnce all handlers pass tests, wire them to the router:\n\n```\nUse {backend-plugin}/integration-expert skill with:\n- action: \"wire-handlers\"\n- handlers: [list of handler file paths]\n- output: \"{backend-path}/src/main\" or router file\n```\n\n**Example Router Integration (Rust/Axum)**:\n\n```rust\n// backend/src/main.rs\n\nmod handlers;\nmod generated;\nmod error;\n\nuse axum::{\n    routing::{get, post, patch},\n    Router,\n};\n\n#[tokio::main]\nasync fn main() {\n    // Database connection\n    let db = setup_database().await;\n\n    let app_state = AppState { db };\n\n    // Router with all handlers\n    let app = Router::new()\n        .route(\"/api/users/:id/orders\", get(handlers::orders::get_user_orders))\n        .route(\"/api/orders\", post(handlers::orders::create_order))\n        .route(\"/api/orders/:id\", get(handlers::orders::get_order))\n        .route(\"/api/orders/:id\", patch(handlers::orders::update_order_status))\n        .with_state(app_state);\n\n    // Start server\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\n### Run Full Integration Tests\n\n```bash\n# Start services\ndocker-compose up -d\n\n# Wait for health checks\ndocker-compose exec api curl http://localhost:3000/health\n\n# Run integration test suite\ndocker-compose exec api cargo test --test integration\n```\n\n### Verify Against OpenAPI Spec\n\nUse validation tools to ensure API matches spec:\n\n```bash\n# Generate Prism mock server from OpenAPI spec\nnpx @stoplight/prism-cli mock spec/openapi.yaml\n\n# Run contract tests\ndocker-compose exec api cargo test --test contract_tests\n```\n\n## Build Summary\n\nProvide comprehensive build report:\n\n```\nâœ“ Build Complete!\n\nPhase 1: Spec Changes\n  âœ“ Applied 1 database migration (003_add_orders.sql)\n  âœ“ Validated OpenAPI spec\n\nPhase 2: Code Generation\n  âœ“ Generated database models (Order, OrderItem)\n  âœ“ Generated query functions (5 functions)\n  âœ“ Generated API types (OrdersResponse, CreateOrderRequest)\n\nPhase 3: Handler Implementation\n  âœ“ Implemented 4 handlers (2 simple, 2 complex)\n  - GET /api/users/{id}/orders (Haiku)\n  - GET /api/orders/{id} (Haiku)\n  - POST /api/orders (Sonnet)\n  - PATCH /api/orders/{id} (Sonnet)\n\nPhase 4: Testing\n  âœ“ Generated 12 unit tests\n  âœ“ Generated 4 integration tests\n  âœ“ All tests passing\n\nPhase 5: Integration\n  âœ“ Wired handlers to router\n  âœ“ Verified against OpenAPI spec\n\nBuild Statistics:\n- Time: 45 minutes\n- Tokens used: 48,000\n- Cost: ~$0.45\n- Files modified: 8\n- Tests created: 16\n\nNext Steps:\n1. Run `/specforge:test` for full test suite\n2. Run `/specforge:validate` to validate everything\n3. Start services: docker-compose up -d\n4. Test manually: curl http://localhost:3000/api/users/1/orders\n```\n\n## Error Handling\n\n### Migration Failures\n\nIf migration fails:\n\n1. Show error details\n2. Offer to rollback\n3. Let user fix migration file\n4. Retry\n\n### Code Generation Failures\n\nIf codegen fails:\n\n1. Check schema syntax\n2. Verify database connection\n3. Check codegen tool installation\n4. Show tool-specific error messages\n\n### Handler Implementation Failures\n\nIf handler agent fails:\n\n1. Show implementation errors\n2. Offer to retry with Sonnet (if was Haiku)\n3. Let user implement manually\n4. Skip and continue with other handlers\n\n### Test Failures Beyond Max Iterations\n\nIf tests still fail after 3 iterations:\n\n1. Show detailed error report\n2. Ask user to review implementation\n3. Offer to continue with other handlers\n4. Create TODO for manual fix\n\n## Implementation Notes\n\n- Track state in `.specforge/build-state.json`\n- Log all agent interactions for debugging\n- Create backups before applying changes\n- Use transactions where possible\n- Provide detailed error messages\n- Allow resuming from failed phase\n\n## Resources\n\n- **Parallel Processing**: Max 5 concurrent agents\n- **Context Budget**: <5K per Haiku, <15K per Sonnet\n- **Retry Logic**: Max 3 iterations per handler\n- **State Persistence**: `.specforge/build-state.json`"
              },
              {
                "name": "/init",
                "description": "Initialize a SpecForge project with tech stack selection",
                "path": "specforge/commands/init.md",
                "frontmatter": {
                  "name": "init",
                  "description": "Initialize a SpecForge project with tech stack selection"
                },
                "content": "# SpecForge Initialization\n\nInitialize a new SpecForge project by selecting your tech stack and configuring the development environment.\n\n## Overview\n\nSpecForge uses a **three-plugin architecture**:\n\n1. **Backend Plugin** - Framework patterns and handlers\n2. **Database Plugin** - Database tooling and migrations\n3. **Codegen Pipeline Plugin** - Type-safe code generation bridging backend + database\n\nThis initialization command will guide you through selecting compatible plugins for your stack.\n\n## Step 1: Discover Available Tech Stack Options\n\nSearch the Claude Market marketplace for available SpecForge plugins:\n\n```bash\ncurl -s https://api.github.com/repos/claude-market/marketplace/contents/ | jq -r '.[] | select(.type == \"dir\" and (.name | startswith(\"specforge-\"))) | .name'\n```\n\nParse the results to categorize plugins:\n\n- `specforge-backend-*` - Backend framework plugins\n- `specforge-db-*` - Database plugins\n- `specforge-generate-*` - Codegen pipeline plugins\n- `specforge-frontend-*` - Frontend plugins (optional)\n\n## Step 2: Check for Existing Project Structure\n\nLook for existing OpenAPI specification in common locations:\n\n```bash\n# Check for OpenAPI spec\nfind . -maxdepth 3 -name \"openapi.yaml\" -o -name \"openapi.json\" -o -name \"api.yaml\"\n```\n\nCommon locations:\n\n- `spec/openapi.yaml`\n- `spec/openapi.json`\n- `api/openapi.yaml`\n- `openapi.yaml`\n\nIf not found, offer to:\n\n1. Create a new minimal OpenAPI spec\n2. Provide example specs they can customize\n3. Skip for now (they'll add it later)\n\n## Step 3: Present Stack Selection\n\nUse the **AskUserQuestion** tool to present available stacks in an interactive way.\n\n### Backend Selection\n\nPresent available backend plugins discovered in Step 1. For each option, show:\n\n- Technology and framework name\n- Brief description\n- Compatible databases\n\nExample options:\n\n- **Rust + Axum** - High-performance async web framework\n- **Node + Express** - Popular, mature JavaScript framework\n- **Python + FastAPI** - Modern Python with automatic OpenAPI\n- **Go + Gin** - Fast, minimalist Go web framework\n\n### Database Selection\n\nFilter database plugins based on backend selection compatibility. Show:\n\n- Database type\n- Migration strategy\n- Compatible with chosen backend\n\nExample options:\n\n- **PostgreSQL** - Robust relational database\n- **SQLite** - Lightweight embedded database\n- **MySQL** - Popular open-source database\n- **MongoDB** - NoSQL document database\n\n### Codegen Pipeline Selection\n\nFilter codegen plugins compatible with both backend and database selections. Show:\n\n- Tool name (e.g., sql-gen, Prisma, sqlc)\n- Type safety features\n- Build integration\n\nExample options:\n\n- **rust-sql** - Uses sql-gen for compile-time verified queries\n- **ts-prisma** - Prisma ORM for TypeScript\n- **go-sqlc** - Generates type-safe Go from SQL\n- **python-sqlalchemy** - SQLAlchemy ORM\n\n### Frontend Selection (Optional)\n\nPresent frontend plugins if user wants full-stack:\n\n- **React + TanStack Query** - Modern React with data fetching\n- **Vue + Pinia** - Vue 3 with state management\n- **Next.js** - React with SSR/SSG\n- **Svelte** - Compiled reactive framework\n\n## Step 4: Generate Project Structure\n\nCreate the standard SpecForge directory structure:\n\n```bash\nmkdir -p spec backend frontend docker tests migrations\n```\n\nDirectory structure:\n\n```\nproject/\nâ”œâ”€â”€ spec/\nâ”‚   â””â”€â”€ openapi.yaml              # OpenAPI specification\nâ”œâ”€â”€ migrations/                    # Database migrations\nâ”‚   â””â”€â”€ 001_initial.sql\nâ”œâ”€â”€ backend/                       # Backend code\nâ”œâ”€â”€ frontend/                      # Frontend code (if selected)\nâ”œâ”€â”€ docker/                        # Docker configs\nâ”‚   â””â”€â”€ docker-compose.yml\nâ”œâ”€â”€ tests/                         # Integration tests\nâ”œâ”€â”€ CLAUDE.md                      # SpecForge configuration\nâ””â”€â”€ README.md\n```\n\n## Step 5: Install Selected Plugins\n\nFor each selected plugin, use the `/plugin install` command:\n\n```bash\n# Install the three required plugins\n/plugin install specforge-backend-{technology}-{framework}\n/plugin install specforge-db-{database}\n/plugin install specforge-generate-{technology}-{database}\n\n# Optional: Install frontend plugin\n/plugin install specforge-frontend-{framework}-{variant}\n```\n\n**Note**: These commands should be executed by Claude Code, not in bash.\n\n## Step 6: Invoke Plugin Setup Skills\n\nDelegate to each plugin's setup skill to initialize project files:\n\n### Backend Plugin Setup\n\nInvoke the backend plugin's initialization skill:\n\n```\nUse the {backend-plugin}/setup skill to:\n- Generate project scaffold (Cargo.toml, package.json, etc.)\n- Create main application entry point\n- Set up basic routing structure\n- Configure build tools\n- Create Dockerfile template\n```\n\n### Database Plugin Setup\n\nInvoke the database plugin's initialization skill:\n\n```\nUse the {database-plugin}/setup skill to:\n- Create initial migration file\n- Set up migration tooling\n- Configure database connection\n- Add to docker-compose.yml\n- Create health check scripts\n```\n\n### Codegen Plugin Setup\n\nInvoke the codegen plugin's initialization skill:\n\n```\nUse the {codegen-plugin}/setup skill to:\n- Create codegen configuration (sql-gen.toml, prisma.schema, etc.)\n- Set up build integration\n- Configure output directories\n- Add compile-time verification\n```\n\n### Frontend Plugin Setup (Optional)\n\nIf frontend was selected:\n\n```\nUse the {frontend-plugin}/setup skill to:\n- Initialize frontend project\n- Generate API client from OpenAPI spec\n- Set up routing and state management\n- Configure build tools\n- Add to docker-compose.yml\n```\n\n## Step 7: Generate Docker Compose Configuration\n\nAggregate Docker configurations from all plugins into a unified docker-compose.yml:\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  # From database plugin\n  db:\n    image: { database-image }\n    environment:\n      # Database-specific env vars\n    volumes:\n      - db-data:/var/lib/db\n    healthcheck:\n      test: { database-health-check }\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  # From backend plugin\n  api:\n    build: ./backend\n    ports:\n      - \"3000:3000\"\n    environment:\n      DATABASE_URL: { database-connection-string }\n    depends_on:\n      db:\n        condition: service_healthy\n    volumes:\n      - ./backend:/app\n      - /app/target # For build caching\n\n  # From frontend plugin (if selected)\n  web:\n    build: ./frontend\n    ports:\n      - \"5173:5173\"\n    environment:\n      VITE_API_URL: http://localhost:3000\n    volumes:\n      - ./frontend:/app\n      - /app/node_modules\n\nvolumes:\n  db-data:\n```\n\n## Step 8: Create Initial OpenAPI Spec (if needed)\n\nIf no OpenAPI spec exists, create a minimal starter:\n\n```yaml\nopenapi: 3.1.0\ninfo:\n  title: My API\n  version: 1.0.0\n  description: API built with SpecForge\n\nservers:\n  - url: http://localhost:3000\n    description: Local development\n\npaths:\n  /health:\n    get:\n      summary: Health check\n      description: Returns API health status\n      responses:\n        \"200\":\n          description: API is healthy\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    example: ok\n\ncomponents:\n  schemas:\n    Error:\n      type: object\n      required:\n        - error\n        - message\n      properties:\n        error:\n          type: string\n          description: Error code\n        message:\n          type: string\n          description: Human-readable error message\n```\n\n## Step 9: Save Configuration to CLAUDE.md\n\nUpdate or create `CLAUDE.md` with SpecForge configuration:\n\n```markdown\n## SpecForge Configuration\n\n**Stack Selection:**\n\n- Backend: {technology}-{framework}\n- Database: {database}\n- Codegen: {technology}-{database}\n- Frontend: {framework}-{variant} (optional)\n\n**Installed Plugins:**\n\n- specforge-backend-{technology}-{framework}\n- specforge-db-{database}\n- specforge-generate-{technology}-{database}\n- specforge-frontend-{framework}-{variant} (optional)\n\n**Project Structure:**\n\n- OpenAPI Spec: `spec/openapi.yaml`\n- Database Migrations: `migrations/`\n- Backend Code: `backend/`\n- Frontend Code: `frontend/` (optional)\n\n**Development:**\n\n- Start all services: `docker-compose up -d`\n- View logs: `docker-compose logs -f`\n- Stop services: `docker-compose down`\n\n**SpecForge Workflow:**\n\n1. Edit `spec/openapi.yaml` to define API endpoints\n2. Run `/specforge:plan` to generate implementation plan\n3. Run `/specforge:build` to generate code and implement handlers\n4. Run `/specforge:test` to run test suite\n5. Run `/specforge:validate` to validate everything works\n6. Run `/specforge:ship` to prepare for deployment\n```\n\n## Step 10: Summary and Next Steps\n\nProvide the user with:\n\n1. **Installation Summary:**\n\n   - Backend: {selected backend plugin}\n   - Database: {selected database plugin}\n   - Codegen: {selected codegen plugin}\n   - Frontend: {selected frontend plugin} (if applicable)\n\n2. **Files Created:**\n\n   - Project structure\n   - OpenAPI spec location\n   - Docker configuration\n   - Plugin configuration\n\n3. **Next Steps:**\n\n   ```\n   1. Review/edit your OpenAPI spec: spec/openapi.yaml\n   2. Define your database schema: migrations/001_initial.sql\n   3. Start development environment: docker-compose up -d\n   4. Generate implementation plan: /specforge:plan\n   5. Build your application: /specforge:build\n   ```\n\n4. **Quick Start Commands:**\n\n   ```bash\n   # Start all services\n   docker-compose up -d\n\n   # Check service health\n   docker-compose ps\n\n   # View API logs\n   docker-compose logs -f api\n\n   # Run migrations\n   docker-compose exec api {migration-command}\n   ```\n\n## Resources\n\n- **OpenAPI Specification**: https://spec.openapis.org/oas/latest.html\n- **OpenAPI Examples**: https://github.com/OAI/OpenAPI-Specification/tree/main/examples\n- **OpenAPI Best Practices**: https://learn.openapis.org/best-practices.html\n- **SpecForge Documentation**: https://github.com/claude-market/marketplace/tree/main/specforge\n\n## Error Handling\n\n### Plugin Not Found\n\nIf a plugin is not available in the marketplace:\n\n- Suggest alternative plugins\n- Provide instructions for requesting new plugins\n- Link to plugin contribution guide\n\n### Incompatible Plugins\n\nIf user selects incompatible plugins:\n\n- Show compatibility matrix\n- Suggest compatible alternatives\n- Explain why plugins are incompatible\n\n### Existing Project Detected\n\nIf project files already exist:\n\n- Warn user about potential overwrites\n- Offer to backup existing files\n- Allow selective initialization\n\n## Implementation Notes\n\n- Use **AskUserQuestion** for all user selections\n- Validate plugin compatibility before installation\n- Create backups before modifying existing files\n- Provide clear error messages with actionable solutions\n- Test Docker configuration before finalizing"
              },
              {
                "name": "/plan",
                "description": "Generate implementation plan with dual-spec changes (OpenAPI + DB schema)",
                "path": "specforge/commands/plan.md",
                "frontmatter": {
                  "name": "plan",
                  "description": "Generate implementation plan with dual-spec changes (OpenAPI + DB schema)"
                },
                "content": "# SpecForge Planning Agent\n\nAnalyze feature requirements and propose coordinated changes to both OpenAPI spec and database schema.\n\n## Overview\n\nSpecForge's core innovation is the **dual-spec approach**: both OpenAPI and database schemas drive development. This planning command helps you:\n\n1. Analyze feature requirements\n2. Propose OpenAPI spec changes\n3. Propose database schema changes\n4. Generate implementation plan\n5. Estimate complexity and agent requirements\n\n## Planning Process\n\n### Step 1: Gather Feature Requirements\n\nAsk the user what feature they want to implement. Use **AskUserQuestion** to collect:\n\n1. **Feature description**: What functionality should this add?\n2. **User-facing behavior**: What does the user see/experience?\n3. **Data requirements**: What data needs to be stored/retrieved?\n4. **Integration points**: Does this interact with existing features?\n\n### Step 2: Analyze Current State\n\nRead and analyze existing specifications:\n\n```bash\n# Read current OpenAPI spec\ncat spec/openapi.yaml\n\n# Read existing database migrations\nls -la migrations/\ncat migrations/*.sql\n\n# Check existing backend code structure\nfind backend -type f -name \"*.rs\" -o -name \"*.ts\" -o -name \"*.py\" -o -name \"*.go\" | head -20\n```\n\nUnderstand:\n\n- Current API endpoints\n- Existing database schema\n- Current data models\n- Existing patterns and conventions\n\n### Step 3: Propose OpenAPI Spec Changes\n\nBased on the feature requirements, draft new OpenAPI endpoints with:\n\n#### Detailed Endpoint Specifications\n\n```yaml\npaths:\n  /api/users/{id}/orders:\n    get:\n      summary: Get user's orders\n      description: |\n        Retrieve all orders for a specific user.\n\n        Business Logic:\n        1. Authenticate the requesting user\n        2. Verify user has permission to view these orders\n        3. Fetch user from database by ID\n        4. Query orders with user_id foreign key\n        5. Include order items with product details via JOIN\n        6. Calculate order totals (sum of item prices)\n        7. Sort by created_at DESC (most recent first)\n        8. Return paginated results\n\n        Edge Cases:\n        - User not found: Return 404\n        - No orders: Return empty array with 200\n        - Permission denied: Return 403\n        - Invalid pagination params: Return 400\n\n        Performance Considerations:\n        - Use database indexes on user_id and created_at\n        - Limit JOIN depth to avoid N+1 queries\n        - Default page size: 20, max: 100\n        - Consider caching for frequently accessed users\n\n      parameters:\n        - name: id\n          in: path\n          required: true\n          schema:\n            type: integer\n            format: int64\n          description: User ID\n        - name: page\n          in: query\n          schema:\n            type: integer\n            default: 1\n            minimum: 1\n          description: Page number for pagination\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            default: 20\n            minimum: 1\n            maximum: 100\n          description: Number of orders per page\n\n      responses:\n        \"200\":\n          description: User's orders retrieved successfully\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: \"#/components/schemas/Order\"\n                  pagination:\n                    type: object\n                    properties:\n                      page:\n                        type: integer\n                      limit:\n                        type: integer\n                      total:\n                        type: integer\n                      totalPages:\n                        type: integer\n              examples:\n                success:\n                  value:\n                    data:\n                      - id: 1\n                        userId: 123\n                        total: 4999\n                        status: completed\n                        createdAt: \"2025-01-15T10:30:00Z\"\n                        items:\n                          - productId: 456\n                            quantity: 2\n                            price: 2499\n                    pagination:\n                      page: 1\n                      limit: 20\n                      total: 45\n                      totalPages: 3\n        \"404\":\n          description: User not found\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Error\"\n        \"403\":\n          description: Permission denied\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Error\"\n\ncomponents:\n  schemas:\n    Order:\n      type: object\n      required:\n        - id\n        - userId\n        - total\n        - status\n        - createdAt\n      properties:\n        id:\n          type: integer\n          format: int64\n          description: Unique order identifier\n        userId:\n          type: integer\n          format: int64\n          description: ID of user who placed the order\n        total:\n          type: integer\n          description: Total price in cents\n          example: 4999\n        status:\n          type: string\n          enum: [pending, completed, cancelled]\n          description: Current order status\n        createdAt:\n          type: string\n          format: date-time\n          description: When order was created\n        items:\n          type: array\n          items:\n            $ref: \"#/components/schemas/OrderItem\"\n```\n\n### Step 4: Propose Database Schema Changes\n\nDraft SQL migration files that align with the OpenAPI changes:\n\n```sql\n-- migrations/003_add_orders.sql\n\n-- Orders table\nCREATE TABLE orders (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    user_id INTEGER NOT NULL,\n    total_cents INTEGER NOT NULL,\n    status TEXT NOT NULL CHECK(status IN ('pending', 'completed', 'cancelled')),\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\n    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n);\n\n-- Indexes for performance\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_orders_status ON orders(status);\nCREATE INDEX idx_orders_created_at ON orders(created_at DESC);\n\n-- Composite index for common query pattern\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Order items table\nCREATE TABLE order_items (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    order_id INTEGER NOT NULL,\n    product_id INTEGER NOT NULL,\n    quantity INTEGER NOT NULL CHECK(quantity > 0),\n    price_cents INTEGER NOT NULL,\n\n    FOREIGN KEY (order_id) REFERENCES orders(id) ON DELETE CASCADE,\n    FOREIGN KEY (product_id) REFERENCES products(id)\n);\n\n-- Index for order items queries\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\n\n-- Trigger to update order total when items change\nCREATE TRIGGER update_order_total\nAFTER INSERT ON order_items\nBEGIN\n    UPDATE orders\n    SET total_cents = (\n        SELECT SUM(quantity * price_cents)\n        FROM order_items\n        WHERE order_id = NEW.order_id\n    )\n    WHERE id = NEW.order_id;\nEND;\n```\n\n**Schema Design Considerations:**\n\n1. **Foreign Keys**: Maintain referential integrity\n2. **Indexes**: Add indexes for common query patterns\n3. **Constraints**: Use CHECK constraints for data validation\n4. **Triggers**: Consider triggers for computed values\n5. **Normalization**: Follow appropriate normal form\n6. **Timestamps**: Add created_at/updated_at for auditing\n\n### Step 5: Generate Implementation Plan\n\nCreate a comprehensive plan document:\n\n````markdown\n## Implementation Plan: User Orders Feature\n\n### Overview\n\nAdd ability to create and retrieve orders for users.\n\n### Spec Changes Summary\n\n#### OpenAPI Changes\n\n1. **New Endpoints**:\n\n   - GET /api/users/{id}/orders - Retrieve user's orders\n   - POST /api/orders - Create new order\n   - GET /api/orders/{id} - Get order details\n   - PATCH /api/orders/{id} - Update order status\n\n2. **New Schemas**:\n   - Order - Order information\n   - OrderItem - Individual order line items\n   - CreateOrderRequest - Request payload for creating orders\n\n#### Database Schema Changes\n\n1. **New Tables**:\n\n   - orders (id, user_id, total_cents, status, created_at, updated_at)\n   - order_items (id, order_id, product_id, quantity, price_cents)\n\n2. **New Indexes**:\n\n   - idx_orders_user_id\n   - idx_orders_status\n   - idx_orders_created_at\n   - idx_orders_user_status (composite)\n   - idx_order_items_order_id\n\n3. **Triggers**:\n   - update_order_total - Automatically calculate order total\n\n### Code Generation Required\n\nThe **codegen plugin** (e.g., `specforge-generate-rust-sql`) will generate:\n\n1. **Database Models** (from schema):\n\n   - `Order` struct with all fields\n   - `OrderItem` struct with all fields\n   - Type-safe enums for OrderStatus\n\n2. **Type-Safe Queries** (from schema):\n\n   - `get_orders_by_user_id(pool, user_id, page, limit)` -> `Vec<Order>`\n   - `create_order(pool, user_id)` -> `Order`\n   - `add_order_item(pool, order_id, product_id, quantity, price)` -> `OrderItem`\n   - `get_order_with_items(pool, order_id)` -> `OrderWithItems`\n\n3. **API Types** (from OpenAPI):\n   - Request/Response DTOs\n   - Validation schemas\n   - Serialization code\n\n### Handler Implementation\n\nUsing the **backend plugin** (e.g., `specforge-backend-rust-axum`), implement handlers:\n\n#### 1. GET /api/users/{id}/orders\n\n```rust\n// Pseudocode - actual implementation by backend plugin agent\n\npub async fn get_user_orders(\n    State(db): State<DatabasePool>,\n    Path(user_id): Path<i64>,\n    Query(params): Query<PaginationParams>,\n) -> Result<Json<OrdersResponse>, ApiError> {\n    // 1. Validate user exists (use generated query)\n    let user = get_user_by_id(&db, user_id)\n        .await?\n        .ok_or(ApiError::NotFound)?;\n\n    // 2. Get orders with pagination (use generated query)\n    let orders = get_orders_by_user_id(&db, user_id, params.page, params.limit)\n        .await?;\n\n    // 3. Get total count for pagination (use generated query)\n    let total = count_orders_by_user_id(&db, user_id).await?;\n\n    // 4. Build response\n    Ok(Json(OrdersResponse {\n        data: orders,\n        pagination: Pagination {\n            page: params.page,\n            limit: params.limit,\n            total,\n            total_pages: (total + params.limit - 1) / params.limit,\n        },\n    }))\n}\n```\n````\n\n**Complexity**: Simple CRUD with pagination\n**Agent Model**: Haiku (sufficient for generated types + straightforward logic)\n**Estimated Time**: 5 minutes\n\n#### 2. POST /api/orders\n\n```rust\n// More complex - involves transaction\n\npub async fn create_order(\n    State(db): State<DatabasePool>,\n    Json(payload): Json<CreateOrderRequest>,\n) -> Result<Json<Order>, ApiError> {\n    // 1. Start transaction\n    let mut tx = db.begin().await?;\n\n    // 2. Validate user and products exist\n    // 3. Check inventory availability\n    // 4. Create order\n    // 5. Create order items\n    // 6. Commit transaction\n\n    // Implementation by backend handler agent\n}\n```\n\n**Complexity**: Complex - transaction, validation, multiple tables\n**Agent Model**: Sonnet (complex business logic, error handling)\n**Estimated Time**: 15 minutes\n\n### Testing Strategy\n\nGenerate tests using **backend test agent**:\n\n1. **Unit Tests**:\n\n   - Test handler logic with mocked database\n   - Test validation rules\n   - Test error cases\n\n2. **Integration Tests**:\n\n   - Full flow from HTTP request to database\n   - Test transaction rollback\n   - Test pagination logic\n\n3. **Behavior Observation**:\n   - Verify correct SQL queries generated\n   - Check response format matches OpenAPI\n   - Validate error responses\n\n### Migration Strategy\n\n1. **Development**:\n\n   - Run migration locally: `./migrate.sh migrations/003_add_orders.sql`\n   - Verify schema with: `sqlite3 dev.db .schema`\n\n2. **Testing**:\n\n   - Apply migration to test database\n   - Run codegen to verify types\n   - Compile backend to catch errors\n\n3. **Production**:\n   - Backup database before migration\n   - Apply migration with transaction\n   - Verify with health check\n\n### Estimated Complexity\n\n**Overall**: Medium complexity\n\n- 4 new endpoints (2 simple, 2 complex)\n- 2 new database tables\n- Multiple generated types and queries\n- Transaction handling required\n\n**Timeline**:\n\n- Spec updates: 10 minutes\n- Schema migration: 5 minutes\n- Code generation: 2 minutes (automated)\n- Handler implementation: 30 minutes (2 Haiku + 2 Sonnet agents)\n- Testing: 15 minutes\n- **Total**: ~60 minutes\n\n**Cost Estimate**:\n\n- Planning (Sonnet): ~5K tokens\n- Handler agents (2 Haiku): ~10K tokens\n- Handler agents (2 Sonnet): ~30K tokens\n- Test agents (Haiku): ~5K tokens\n- **Total**: ~50K tokens â‰ˆ $0.50\n\n### Dependencies\n\n- Requires `users` table to exist\n- Requires `products` table to exist\n- No external API dependencies\n\n### Risks & Mitigation\n\n1. **Risk**: Race condition in order total calculation\n   **Mitigation**: Use database trigger, test with concurrent requests\n\n2. **Risk**: Inventory overselling\n   **Mitigation**: Use optimistic locking or row-level locks\n\n3. **Risk**: Large order results causing performance issues\n   **Mitigation**: Enforce maximum page size, add query timeouts\n\n```\n\n### Step 6: Present Plan to User\n\nFormat the plan clearly and ask for approval:\n\n```\n\nI've analyzed your feature requirements and created an implementation plan.\n\n**Summary:**\n\n- 4 new API endpoints\n- 2 new database tables with indexes\n- Type-safe code generation from schemas\n- Estimated time: 60 minutes\n- Estimated cost: $0.50 in AI inference\n\n**Next Steps:**\n\n1. Review the proposed spec changes above\n2. If approved, run `/specforge:build` to implement\n3. Run `/specforge:test` to verify everything works\n\nWould you like me to proceed with the build, or would you like to modify the plan first?\n\n```\n\nUse **AskUserQuestion** to get approval:\n- Proceed with build\n- Modify OpenAPI spec\n- Modify database schema\n- Cancel and revise\n\n## Planning Best Practices\n\n### 1. Detailed Business Logic\n\nAlways document business logic in OpenAPI descriptions:\n- Step-by-step algorithm\n- Edge cases and error handling\n- Performance considerations\n- Security requirements\n\n### 2. Schema Design Principles\n\nFollow database best practices:\n- Proper normalization\n- Foreign key constraints\n- Appropriate indexes\n- Data validation constraints\n- Audit timestamps\n\n### 3. Type Safety\n\nEnsure alignment between OpenAPI and database schemas:\n- Matching field names (snake_case DB, camelCase API)\n- Compatible data types\n- Consistent validation rules\n\n### 4. Performance Planning\n\nConsider performance early:\n- Index common query patterns\n- Avoid N+1 queries with JOINs\n- Plan for pagination\n- Set reasonable limits\n\n### 5. Error Handling\n\nDefine comprehensive error responses:\n- HTTP status codes\n- Error message format\n- Recovery suggestions\n- Logging strategy\n\n## Resources\n\n- **OpenAPI Best Practices**: https://learn.openapis.org/best-practices.html\n- **Database Design**: https://www.postgresql.org/docs/current/ddl.html\n- **SQL Indexing**: https://use-the-index-luke.com/\n- **API Design Guide**: https://cloud.google.com/apis/design\n\n## Implementation Notes\n\n- Use the **openapi-expert** skill for spec validation\n- Use the **stack-advisor** skill for technology-specific patterns\n- Consult plugin documentation for framework-specific conventions\n- Validate compatibility between proposed changes and existing code\n```"
              },
              {
                "name": "/ship",
                "description": "Deployment preparation - build production artifacts, run security checks, and generate deployment configs",
                "path": "specforge/commands/ship.md",
                "frontmatter": {
                  "name": "ship",
                  "description": "Deployment preparation - build production artifacts, run security checks, and generate deployment configs"
                },
                "content": "# SpecForge Ship Command\n\nPrepare your SpecForge project for deployment by building production artifacts, running security checks, optimizing Docker images, and generating deployment configurations.\n\n## Overview\n\nThe ship command orchestrates the final steps before deployment:\n\n1. **Pre-deployment Validation** - Ensure everything passes validation and tests\n2. **Production Build** - Build optimized production artifacts\n3. **Security Scanning** - Scan for vulnerabilities\n4. **Docker Optimization** - Build optimized production Docker images\n5. **Deployment Config Generation** - Generate deployment manifests\n6. **Deployment Readiness Report** - Final checklist and next steps\n\n## Ship Orchestration Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  /specforge:ship                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â–¼             â–¼             â–¼              â–¼               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Pre-   â”‚  â”‚ Build    â”‚  â”‚ Security â”‚  â”‚ Docker   â”‚  â”‚ Deploy      â”‚\nâ”‚ flight â”‚  â”‚ Prod     â”‚  â”‚ Scan     â”‚  â”‚ Optimize â”‚  â”‚ Config      â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n     â”‚            â”‚             â”‚             â”‚              â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                 â”‚\n                                 â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ Readiness Report   â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Step 1: Pre-deployment Validation\n\nRun full validation and tests to ensure everything is ready:\n\n```bash\n# Run validation\n/specforge:validate --full\n\n# Run full test suite\n/specforge:test --full\n```\n\nIf validation or tests fail, abort the ship process and report errors to the user.\n\n**Checks:**\n\n- âœ“ All validation levels pass (spec, schema, codegen, compile, runtime)\n- âœ“ All tests pass (unit, integration, contract)\n- âœ“ No pending migrations\n- âœ“ No uncommitted changes (warn if dirty git status)\n- âœ“ Version is tagged (recommend if not)\n\n## Step 2: Production Build\n\nBuild optimized production artifacts using backend plugin:\n\n```bash\n# Read backend plugin from CLAUDE.md\nBACKEND_PLUGIN=$(grep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' ')\n```\n\nInvoke backend plugin's production build:\n\n```\nUse {backend-plugin}/docker-expert skill with:\n- action: \"build-production\"\n- project_path: \"backend/\"\n- optimization: \"release\"\n- output_path: \"dist/\"\n```\n\n**Technology-Specific Builds:**\n\n**Rust:**\n\n```bash\ncd backend && cargo build --release\nstrip target/release/api  # Strip debug symbols\n```\n\n**TypeScript/Node:**\n\n```bash\ncd backend && npm run build\nnpm prune --production  # Remove dev dependencies\n```\n\n**Python:**\n\n```bash\ncd backend && pip install --no-dev\npython -m compileall .  # Compile to bytecode\n```\n\n**Go:**\n\n```bash\ncd backend && CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-s -w' -o dist/api\n```\n\n**Optimization Checks:**\n\n- âœ“ Debug symbols removed\n- âœ“ Dev dependencies excluded\n- âœ“ Build artifacts optimized\n- âœ“ Static files bundled\n- âœ“ Environment variables externalized\n\n## Step 3: Security Scanning\n\nRun security scans on dependencies and container images:\n\n### Dependency Scanning\n\nScan project dependencies for known vulnerabilities:\n\n**Rust:**\n\n```bash\ncargo audit\n```\n\n**Node/TypeScript:**\n\n```bash\nnpm audit --production\n# Or use snyk\nnpx snyk test\n```\n\n**Python:**\n\n```bash\npip-audit\n# Or use safety\nsafety check\n```\n\n**Go:**\n\n```bash\ngo list -json -m all | nancy sleuth\n```\n\n### Container Image Scanning\n\nUse [Trivy](https://aquasecurity.github.io/trivy/) to scan Docker images:\n\n```bash\n# Build production image first\ndocker build -t my-api:latest -f backend/Dockerfile.prod backend/\n\n# Scan for vulnerabilities\ntrivy image --severity HIGH,CRITICAL my-api:latest\n\n# Scan for misconfigurations\ntrivy config docker/docker-compose.yml\n```\n\n**Security Checks:**\n\n- âœ“ No critical vulnerabilities in dependencies\n- âœ“ No high-severity CVEs in base images\n- âœ“ No secrets in Docker images\n- âœ“ Non-root user in containers\n- âœ“ Minimal base image used\n- âœ“ No unnecessary packages\n\nIf critical vulnerabilities found, report and recommend fixes:\n\n```\nâš  Security Issues Found:\n\n1. Critical: CVE-2024-1234 in openssl 1.1.1\n   Fix: Update Dockerfile to use openssl 3.0\n\n2. High: Secrets detected in .env file\n   Fix: Remove .env from Docker context, use environment variables\n\nRun `trivy image --severity CRITICAL my-api:latest` for details.\n```\n\n## Step 4: Docker Optimization\n\nBuild optimized multi-stage Docker images:\n\n```\nUse {backend-plugin}/docker-expert skill with:\n- action: \"build-optimized-image\"\n- dockerfile: \"backend/Dockerfile.prod\"\n- image_tag: \"{project-name}:{version}\"\n- optimization: \"multi-stage\"\n```\n\n**Optimized Dockerfile Pattern (Rust Example):**\n\n```dockerfile\n# Stage 1: Build\nFROM rust:1.75 AS builder\nWORKDIR /app\nCOPY Cargo.toml Cargo.lock ./\nCOPY src ./src\nRUN cargo build --release\nRUN strip target/release/api\n\n# Stage 2: Runtime\nFROM debian:bookworm-slim\nRUN apt-get update && apt-get install -y \\\n    ca-certificates \\\n    libssl3 \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN useradd -m -u 1000 app\nUSER app\nWORKDIR /app\n\n# Copy binary from builder\nCOPY --from=builder /app/target/release/api ./\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:3000/health || exit 1\n\nEXPOSE 3000\nCMD [\"./api\"]\n```\n\n**Optimization Metrics:**\n\n- Image size reduction (target: <100MB for compiled languages)\n- Build time\n- Layer caching efficiency\n- Security score (no vulnerabilities)\n\n### Database Docker Optimization\n\nPrepare production database configuration:\n\n```\nUse {database-plugin}/docker-expert skill with:\n- action: \"build-production-config\"\n- compose_file: \"docker/docker-compose.prod.yml\"\n```\n\n**Production Database Config (PostgreSQL Example):**\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  db:\n    image: postgres:15-alpine\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD_FILE: /run/secrets/db_password\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    secrets:\n      - db_password\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - backend\n\n  api:\n    image: my-api:${VERSION:-latest}\n    restart: unless-stopped\n    depends_on:\n      db:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}\n      RUST_LOG: info\n    ports:\n      - \"3000:3000\"\n    networks:\n      - backend\n      - frontend\n\nsecrets:\n  db_password:\n    external: true\n\nvolumes:\n  postgres-data:\n\nnetworks:\n  backend:\n  frontend:\n```\n\n## Step 5: Deployment Configuration Generation\n\nGenerate deployment manifests for various platforms:\n\n### Ask User for Deployment Target\n\nUse AskUserQuestion to ask where they're deploying:\n\n- **Docker Compose** - Simple VPS deployment\n- **Kubernetes** - Scalable cloud deployment\n- **AWS ECS/Fargate** - Managed containers on AWS\n- **Google Cloud Run** - Serverless containers on GCP\n- **Azure Container Instances** - Containers on Azure\n- **Fly.io** - Edge deployment platform\n- **Railway** - Simple cloud platform\n\n### Generate Platform-Specific Configs\n\n#### Docker Compose (VPS Deployment)\n\n```yaml\n# docker-compose.prod.yml\nversion: \"3.8\"\n\nservices:\n  db:\n    image: postgres:15-alpine\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - ./data/postgres:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n\n  api:\n    image: ${REGISTRY_URL}/my-api:${VERSION}\n    restart: unless-stopped\n    depends_on:\n      db:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}\n    ports:\n      - \"80:3000\"\n\n  nginx:\n    image: nginx:alpine\n    restart: unless-stopped\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./ssl:/etc/nginx/ssl\n    depends_on:\n      - api\n```\n\n#### Kubernetes Deployment\n\nGenerate Kubernetes manifests:\n\n```yaml\n# k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\n  labels:\n    app: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n        - name: api\n          image: ${REGISTRY_URL}/my-api:${VERSION}\n          ports:\n            - containerPort: 3000\n          env:\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: url\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 10\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          resources:\n            requests:\n              memory: \"128Mi\"\n              cpu: \"100m\"\n            limits:\n              memory: \"512Mi\"\n              cpu: \"500m\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 3000\n  type: LoadBalancer\n```\n\n#### AWS ECS Task Definition\n\n```json\n{\n  \"family\": \"my-api\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"256\",\n  \"memory\": \"512\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"api\",\n      \"image\": \"${REGISTRY_URL}/my-api:${VERSION}\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 3000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"DATABASE_URL\",\n          \"value\": \"${DATABASE_URL}\"\n        }\n      ],\n      \"healthCheck\": {\n        \"command\": [\n          \"CMD-SHELL\",\n          \"curl -f http://localhost:3000/health || exit 1\"\n        ],\n        \"interval\": 30,\n        \"timeout\": 5,\n        \"retries\": 3\n      },\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/my-api\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n```\n\n#### Fly.io Configuration\n\n```toml\n# fly.toml\napp = \"my-api\"\nprimary_region = \"iad\"\n\n[build]\n  image = \"${REGISTRY_URL}/my-api:${VERSION}\"\n\n[env]\n  PORT = \"3000\"\n\n[http_service]\n  internal_port = 3000\n  force_https = true\n  auto_stop_machines = true\n  auto_start_machines = true\n  min_machines_running = 0\n\n[[http_service.checks]]\n  interval = \"10s\"\n  timeout = \"2s\"\n  grace_period = \"5s\"\n  method = \"GET\"\n  path = \"/health\"\n```\n\n### Generate Environment Variable Template\n\nCreate `.env.production.example`:\n\n```bash\n# Database Configuration\nDATABASE_URL=postgresql://user:password@db:5432/dbname\n\n# API Configuration\nAPI_PORT=3000\nRUST_LOG=info\n\n# Security\nJWT_SECRET=<generate-secure-secret>\nAPI_KEY=<generate-api-key>\n\n# External Services (if needed)\nREDIS_URL=redis://redis:6379\nS3_BUCKET=my-bucket\nAWS_REGION=us-east-1\n\n# Monitoring (optional)\nSENTRY_DSN=<your-sentry-dsn>\n```\n\n## Step 6: Generate Deployment Readiness Report\n\nCreate a comprehensive readiness report:\n\n````markdown\n# Deployment Readiness Report\n\n**Project**: my-api\n**Version**: 1.0.0\n**Date**: 2025-01-15\n**Target**: Production\n\n---\n\n## âœ“ Pre-flight Checks\n\n- âœ“ All validation levels passed\n- âœ“ All tests passed (65/65)\n- âœ“ No pending migrations\n- âœ“ Git tag: v1.0.0\n- âœ“ No uncommitted changes\n\n## âœ“ Production Build\n\n- âœ“ Backend built (release mode)\n- âœ“ Binary size: 12.5 MB\n- âœ“ Build time: 2m 34s\n- âœ“ Static files bundled\n- âœ“ Dependencies optimized\n\n## âœ“ Security Scan\n\n- âœ“ No critical vulnerabilities\n- âœ“ No high-severity CVEs\n- âœ“ Container scan passed\n- âš  1 medium-severity issue (non-blocking)\n  - Update recommended: rust 1.75 â†’ 1.76\n\n## âœ“ Docker Images\n\n**Backend Image**: my-api:1.0.0\n\n- Base: debian:bookworm-slim\n- Size: 45 MB (reduced from 850 MB)\n- Layers: 8\n- Non-root user: âœ“\n- Health check: âœ“\n- Security scan: PASS\n\n**Database Image**: postgres:15-alpine\n\n- Size: 238 MB\n- Security scan: PASS\n\n## âœ“ Deployment Configuration\n\nGenerated configs for:\n\n- âœ“ Docker Compose (docker-compose.prod.yml)\n- âœ“ Kubernetes (k8s/\\*.yaml)\n- âœ“ Environment variables (.env.production.example)\n\n## Next Steps\n\n### 1. Push Docker Images\n\n```bash\n# Tag image\ndocker tag my-api:1.0.0 ${REGISTRY_URL}/my-api:1.0.0\ndocker tag my-api:1.0.0 ${REGISTRY_URL}/my-api:latest\n\n# Push to registry\ndocker push ${REGISTRY_URL}/my-api:1.0.0\ndocker push ${REGISTRY_URL}/my-api:latest\n```\n````\n\n### 2. Set Environment Variables\n\nCopy `.env.production.example` to `.env.production` and fill in:\n\n- Database credentials\n- JWT secret (generate with: `openssl rand -base64 32`)\n- API keys\n- External service credentials\n\n### 3. Deploy\n\n**Docker Compose (VPS):**\n\n```bash\n# Copy files to server\nscp docker-compose.prod.yml .env.production user@server:/app/\n\n# SSH to server\nssh user@server\n\n# Start services\ncd /app\ndocker-compose -f docker-compose.prod.yml up -d\n\n# Run migrations\ndocker-compose exec api ./api migrate\n\n# Check health\ncurl https://your-domain.com/health\n```\n\n**Kubernetes:**\n\n```bash\n# Apply manifests\nkubectl apply -f k8s/\n\n# Check deployment\nkubectl rollout status deployment/api\n\n# Check pods\nkubectl get pods\n\n# Check logs\nkubectl logs -f deployment/api\n```\n\n### 4. Post-Deployment Verification\n\n- [ ] Health check endpoint responds: `curl https://your-domain.com/health`\n- [ ] API endpoints respond correctly\n- [ ] Database migrations applied\n- [ ] Monitoring/logging configured\n- [ ] SSL/TLS certificates valid\n- [ ] DNS configured correctly\n\n### 5. Monitoring & Observability (Recommended)\n\nSet up monitoring:\n\n- [ ] Health check monitoring (UptimeRobot, Pingdom)\n- [ ] Error tracking (Sentry, Rollbar)\n- [ ] Performance monitoring (New Relic, DataDog)\n- [ ] Log aggregation (CloudWatch, Papertrail)\n- [ ] Metrics (Prometheus + Grafana)\n\n## Rollback Plan\n\nIf deployment fails:\n\n```bash\n# Docker Compose\ndocker-compose -f docker-compose.prod.yml down\ndocker-compose -f docker-compose.prod.yml up -d my-api:0.9.0\n\n# Kubernetes\nkubectl rollout undo deployment/api\n```\n\n## Support\n\n- **Documentation**: https://github.com/your-org/my-api\n- **Issues**: https://github.com/your-org/my-api/issues\n- **OpenAPI Spec**: https://your-domain.com/api/docs\n\n---\n\n**Status**: âœ… READY TO SHIP\n\nAll checks passed. Your application is ready for production deployment!\n\n```\n\n## Ship Summary\n\nDisplay concise summary to user:\n\n```\n\nâœ… Ship Complete!\n\nPre-flight: âœ“ All checks passed\nBuild: âœ“ Optimized production artifacts\nSecurity: âœ“ No critical vulnerabilities\nDocker: âœ“ Images built and optimized (45 MB)\nDeploy: âœ“ Configs generated\n\nðŸ“¦ Docker Image: my-api:1.0.0 (45 MB)\nðŸ“ Deployment configs generated in ./deploy/\n\nNext Steps:\n\n1. Push images: docker push ${REGISTRY_URL}/my-api:1.0.0\n2. Configure environment: cp .env.production.example .env.production\n3. Deploy: docker-compose -f docker-compose.prod.yml up -d\n4. Verify: curl https://your-domain.com/health\n\nFull report: ./deploy/DEPLOYMENT_REPORT.md\n\n```\n\n## Error Handling\n\n### Pre-flight Validation Fails\n\nIf validation or tests fail, abort ship and show errors:\n\n```\n\nâŒ Ship Aborted\n\nPre-flight validation failed:\n\nTests: âœ— 3 tests failing\n\n- test_create_order_validation\n- test_get_user_not_found\n- test_pagination\n\nFix these issues and run /specforge:ship again.\n\nRun /specforge:test for details.\n\n```\n\n### Security Vulnerabilities Found\n\nIf critical vulnerabilities found, warn and recommend fixes:\n\n```\n\nâš ï¸ Critical Security Issues Found\n\n2 critical vulnerabilities detected:\n\n1. CVE-2024-1234 in openssl 1.1.1 (CRITICAL)\n   Fix: Update base image to use openssl 3.0\n\n2. Secrets detected in Docker image (HIGH)\n   Fix: Remove .env from Docker context\n\nRecommendation: Fix these issues before deploying to production.\n\nContinue anyway? (not recommended)\n\n```\n\n### Build Failures\n\nIf production build fails:\n\n```\n\nâŒ Production Build Failed\n\nError: Compilation failed in release mode\n--> src/handlers/orders.rs:45:12\n|\n45 | let total = order.items.sum();\n| ^^^^^^ method not found\n\nFix the compilation error and run /specforge:ship again.\n\n````\n\n## Platform-Specific Guidance\n\nProvide additional guidance based on deployment target:\n\n### Docker Compose (VPS)\n\n```markdown\n## VPS Deployment Guide\n\n1. **Provision a VPS** (DigitalOcean, Linode, Vultr)\n   - Recommended: 2 vCPU, 4GB RAM, 50GB SSD\n   - OS: Ubuntu 22.04 LTS\n\n2. **Install Docker & Docker Compose**\n   ```bash\n   curl -fsSL https://get.docker.com | sh\n   sudo usermod -aG docker $USER\n````\n\n3. **Set up reverse proxy (Nginx)**\n\n   - SSL with Let's Encrypt (Certbot)\n   - Rate limiting\n   - Gzip compression\n\n4. **Configure firewall**\n   ```bash\n   ufw allow 22/tcp\n   ufw allow 80/tcp\n   ufw allow 443/tcp\n   ufw enable\n   ```\n\n````\n\n### Kubernetes\n\n```markdown\n## Kubernetes Deployment Guide\n\n1. **Set up cluster** (EKS, GKE, AKS, or self-hosted)\n\n2. **Configure kubectl**\n   ```bash\n   kubectl config use-context my-cluster\n````\n\n3. **Create namespace**\n\n   ```bash\n   kubectl create namespace my-api\n   ```\n\n4. **Create secrets**\n\n   ```bash\n   kubectl create secret generic db-credentials \\\n     --from-literal=url='postgresql://...' \\\n     -n my-api\n   ```\n\n5. **Apply manifests**\n\n   ```bash\n   kubectl apply -f k8s/ -n my-api\n   ```\n\n6. **Set up ingress** (Nginx Ingress, Traefik, or cloud load balancer)\n\n````\n\n## Cleanup\n\nOptionally clean up development artifacts:\n\n```bash\n# Remove development containers\ndocker-compose down -v\n\n# Clean build artifacts\ncd backend && cargo clean\n\n# Remove node_modules (if applicable)\nrm -rf node_modules\n\n# Prune Docker system\ndocker system prune -af\n````\n\n## Resources\n\n- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n- **Kubernetes Best Practices**: https://kubernetes.io/docs/concepts/configuration/overview/\n- **Trivy Security Scanner**: https://aquasecurity.github.io/trivy/\n- **Container Security**: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html\n- **Zero-Downtime Deployments**: https://martinfowler.com/bliki/BlueGreenDeployment.html\n\n---\n\n**The ship command ensures your SpecForge project is production-ready, secure, and optimized for deployment.**"
              },
              {
                "name": "/sync",
                "description": "Sync codebase after spec changes (OpenAPI + DB schema)",
                "path": "specforge/commands/sync.md",
                "frontmatter": {
                  "name": "sync",
                  "description": "Sync codebase after spec changes (OpenAPI + DB schema)"
                },
                "content": "# SpecForge Sync Command\n\nSynchronize your codebase after manual changes to OpenAPI spec or database schema. This command detects changes and regenerates code to keep everything in sync.\n\n## Overview\n\nThe sync command is useful when:\n\n1. You've manually edited `spec/openapi.yaml`\n2. You've added/modified database migrations\n3. You've pulled changes from version control that include spec updates\n4. You need to regenerate code after resolving merge conflicts\n\n**Sync orchestrates:**\n\n- Detecting spec changes since last sync\n- Validating updated specifications\n- Regenerating type-safe code from schemas\n- Identifying affected handlers that need updates\n- Running tests to verify everything still works\n\n## Sync Process\n\n### Step 1: Detect Changes\n\nIdentify what has changed since the last successful build:\n\n```bash\n# Check for OpenAPI spec changes\nif [ -f spec/openapi.yaml ]; then\n    echo \"Checking OpenAPI spec for changes...\"\n    # Compare with last known state (stored in .specforge/last-sync.json)\nfi\n\n# Check for new database migrations\necho \"Checking for new migrations...\"\nls -t migrations/*.sql | head -5\n\n# Check for schema changes in existing migrations\ngit diff HEAD~1 migrations/ 2>/dev/null || echo \"Not a git repository or no previous commits\"\n```\n\nCategorize changes:\n\n- **OpenAPI only**: New/modified endpoints, schemas, examples\n- **Database only**: New migrations, schema alterations\n- **Both**: Coordinated changes to API and database\n\n### Step 2: Validate Updated Specifications\n\n#### Validate OpenAPI Spec\n\nUse the **openapi-expert** skill:\n\n```\nInvoke openapi-expert skill with:\n- action: \"validate-spec\"\n- spec_path: \"spec/openapi.yaml\"\n```\n\nValidation checks:\n\n- Valid OpenAPI 3.x syntax\n- All $ref references resolve\n- Request/response schemas are valid\n- Examples match schemas\n- No duplicate operation IDs\n- Descriptions include business logic details\n\n#### Validate Database Schema\n\nRead SpecForge configuration to identify database plugin:\n\n```bash\ngrep \"database:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nUse database plugin's validation skill:\n\n```\nInvoke {database-plugin}/migrations-expert skill with:\n- action: \"validate-migrations\"\n- migrations_dir: \"migrations/\"\n```\n\nValidation checks:\n\n- Migration files are numbered sequentially\n- No syntax errors in SQL\n- Foreign keys reference existing tables\n- Indexes are properly defined\n- No breaking changes (unless acknowledged)\n\n### Step 3: Apply Database Migrations\n\nIf new migrations are detected, apply them:\n\n```\nInvoke {database-plugin}/migrations-expert skill with:\n- action: \"apply-migrations\"\n- migrations_dir: \"migrations/\"\n- database_url: from environment or docker-compose\n```\n\nSteps:\n\n1. List pending migrations\n2. Show what will be applied\n3. Ask user for confirmation\n4. Backup database (if production)\n5. Apply migrations in transaction\n6. Verify schema state\n7. Handle errors with rollback\n\n**Interactive Confirmation:**\n\nUse **AskUserQuestion** to confirm migration:\n\n```\nFound 2 new migrations:\n  - 003_add_orders_table.sql\n  - 004_add_order_items_table.sql\n\nThese migrations will:\n  - Create orders table with indexes\n  - Create order_items table with foreign keys\n  - Add trigger for order total calculation\n\nApply these migrations?\n```\n\nOptions:\n\n- Apply migrations now\n- Review migrations first\n- Skip migrations (code generation only)\n- Cancel sync\n\n### Step 4: Regenerate Code from Schemas\n\n#### Database Code Generation\n\nIdentify codegen plugin from CLAUDE.md:\n\n```bash\ngrep \"codegen:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nRun codegen pipeline:\n\n```\nInvoke {codegen-plugin}/codegen-expert skill with:\n- action: \"generate-from-schema\"\n- schema_dir: \"migrations/\"\n- output_dir: \"{backend-path}/src/generated/db\"\n- database_url: from environment\n- force: true  # Regenerate even if files exist\n```\n\nThis regenerates:\n\n- Database models (structs, classes, types)\n- Type-safe query functions\n- Schema types and enums\n- Database client code\n\n#### OpenAPI Type Generation\n\nIdentify backend plugin from CLAUDE.md:\n\n```bash\ngrep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nGenerate API types:\n\n```\nInvoke {backend-plugin}/openapi-types-expert skill with:\n- action: \"generate-types\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{backend-path}/src/generated/api\"\n- force: true  # Regenerate even if files exist\n```\n\nThis regenerates:\n\n- Request/response DTOs\n- Validation schemas\n- Serialization/deserialization code\n- API client types\n\n#### Frontend Client Generation (if applicable)\n\nIf frontend plugin is configured:\n\n```bash\ngrep \"frontend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\n```\nInvoke {frontend-plugin}/codegen-expert skill with:\n- action: \"generate-client\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{frontend-path}/src/api\"\n- force: true\n```\n\n### Step 5: Identify Affected Handlers\n\nAnalyze which handlers are affected by spec changes:\n\n```bash\n# Compare current spec with previous version\n# Identify new, modified, or deleted endpoints\n\n# For each modified endpoint:\n# - Check if handler exists\n# - Verify handler signature matches new types\n# - Flag for review/reimplementation\n```\n\n**Report to user:**\n\n```markdown\n## Sync Summary\n\n### Code Regenerated\n\n- âœ“ Database models: 15 types regenerated\n- âœ“ Database queries: 32 functions regenerated\n- âœ“ API types: 8 request/response types regenerated\n- âœ“ Frontend client: API client regenerated\n\n### Handlers Affected\n\n**Needs Update:**\n\n- `GET /api/users/{id}/orders` - Response type changed\n- `POST /api/orders` - Request validation rules changed\n\n**Still Valid:**\n\n- `GET /api/users` - No changes\n- `POST /api/users` - No changes\n\n**New (Not Implemented):**\n\n- `PATCH /api/orders/{id}` - New endpoint, needs implementation\n\n### Next Steps\n\n1. Review affected handlers above\n2. Update handlers with new types: `/specforge:build` (will only rebuild affected handlers)\n3. Run tests: `/specforge:test`\n```\n\n### Step 6: Compile and Test\n\nVerify that generated code compiles:\n\n```bash\n# Backend compilation (example for Rust)\ncd backend && cargo check 2>&1\n\n# If TypeScript backend\ncd backend && npm run type-check 2>&1\n\n# If Python backend\ncd backend && mypy . 2>&1\n```\n\nIf compilation fails:\n\n1. Collect error messages\n2. Invoke **{codegen-plugin}/diagnostics-expert** skill\n3. Fix schema/type mismatches\n4. Regenerate code\n5. Retry compilation\n\n**Run existing tests:**\n\n```bash\n# Run backend tests\ncd backend && cargo test 2>&1  # or npm test, pytest, etc.\n```\n\nReport test results:\n\n```\n## Test Results\n\n- âœ“ 127 tests passed\n- âœ— 3 tests failed (affected by schema changes)\n- âš  2 tests skipped (handlers not implemented)\n\nFailed tests:\n  - test_get_user_orders - Response type mismatch\n  - test_create_order - Request validation changed\n  - test_order_total_calculation - New trigger behavior\n```\n\n### Step 7: Save Sync State\n\nUpdate sync state file:\n\n```json\n// .specforge/last-sync.json\n{\n  \"timestamp\": \"2025-01-15T10:30:00Z\",\n  \"openapi_spec_hash\": \"sha256:abc123...\",\n  \"migrations_applied\": [\n    \"001_init.sql\",\n    \"002_add_users.sql\",\n    \"003_add_orders.sql\"\n  ],\n  \"generated_files\": [\n    \"backend/src/generated/db/models.rs\",\n    \"backend/src/generated/db/queries.rs\",\n    \"backend/src/generated/api/types.rs\",\n    \"frontend/src/api/client.ts\"\n  ],\n  \"handlers_affected\": [\"GET /api/users/{id}/orders\", \"POST /api/orders\"],\n  \"tests_status\": {\n    \"passed\": 127,\n    \"failed\": 3,\n    \"skipped\": 2\n  }\n}\n```\n\n### Step 8: Present Summary and Next Steps\n\nShow user what was synced and what needs attention:\n\n```markdown\n## SpecForge Sync Complete\n\n**Changes Detected:**\n\n- 1 new database migration applied\n- OpenAPI spec updated (3 endpoints modified)\n\n**Code Regenerated:**\n\n- âœ“ Database models and queries\n- âœ“ API request/response types\n- âœ“ Frontend API client\n\n**Action Required:**\n\n1. **Update Handlers** (3 affected):\n\n   - GET /api/users/{id}/orders - Response type changed\n   - POST /api/orders - Request validation changed\n   - PATCH /api/orders/{id} - New endpoint\n\n   Run: `/specforge:build` to update/implement handlers\n\n2. **Fix Failing Tests** (3 tests):\n\n   Run: `/specforge:test` after handler updates\n\n**Next Command:**\n\n`/specforge:build` - Implement/update affected handlers\n```\n\nUse **AskUserQuestion** to get next action:\n\n- Update affected handlers now (`/specforge:build`)\n- Run validation first (`/specforge:validate`)\n- Review changes manually\n- Skip for now\n\n## Use Cases\n\n### Use Case 1: After Pulling Changes\n\n```bash\n# Teammate added new endpoints to OpenAPI spec\ngit pull origin main\n\n# Sync to regenerate code\n/specforge:sync\n```\n\n### Use Case 2: After Manual Spec Editing\n\n```yaml\n# You manually edited spec/openapi.yaml\n# Added new endpoint: GET /api/products\npaths:\n  /api/products:\n    get:\n      summary: List products\n      # ... endpoint definition\n\n# Sync to generate types and scaffold handler\n/specforge:sync\n```\n\n### Use Case 3: After Database Migration\n\n```bash\n# You created a new migration file\ncat > migrations/005_add_products.sql <<EOF\nCREATE TABLE products (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    price_cents INTEGER NOT NULL\n);\nEOF\n\n# Sync to apply migration and generate types\n/specforge:sync\n```\n\n### Use Case 4: After Merge Conflict Resolution\n\n```bash\n# Resolved conflicts in spec/openapi.yaml and migrations/\ngit merge feature-branch\n# ... resolved conflicts ...\ngit add spec/openapi.yaml migrations/\ngit commit -m \"Merge feature-branch\"\n\n# Sync to regenerate everything\n/specforge:sync\n```\n\n## Sync vs Build\n\n**When to use `/specforge:sync`:**\n\n- Specs changed externally (git pull, manual edit, merge)\n- Need to regenerate code from updated schemas\n- Want to detect what changed without full rebuild\n\n**When to use `/specforge:build`:**\n\n- Implementing new features from scratch\n- Following a `/specforge:plan`\n- Applying spec changes AND implementing handlers in one go\n\n**Relationship:**\n\n```\n/specforge:plan   â†’  Propose spec changes\n                  â†“\n/specforge:build  â†’  Apply specs + implement handlers + test\n                  â†“\n/specforge:sync   â†’  Regenerate after external spec changes\n```\n\n## Configuration\n\nSync behavior can be customized in CLAUDE.md:\n\n```markdown\n## SpecForge Configuration\n\n- backend: rust-axum\n- database: sqlite\n- codegen: rust-sql\n- frontend: react-tanstack\n\n### Sync Options\n\n- auto_apply_migrations: false # Require confirmation\n- regenerate_on_conflict: true # Auto-regenerate on file conflicts\n- run_tests_after_sync: true # Run tests automatically\n- backup_before_migration: true # Backup DB before applying migrations\n```\n\n## Safety Features\n\n### 1. Backup Before Migration\n\nAlways backup database before applying migrations (unless in development mode):\n\n```bash\n# Production/staging: backup first\nif [ \"$ENV\" != \"development\" ]; then\n    echo \"Backing up database...\"\n    # Use database plugin backup skill\nfi\n```\n\n### 2. Dry Run Mode\n\nShow what would happen without making changes:\n\n```\n/specforge:sync --dry-run\n\nWould apply:\n  - migrations/003_add_orders.sql\n  - migrations/004_add_order_items.sql\n\nWould regenerate:\n  - backend/src/generated/db/*\n  - backend/src/generated/api/*\n  - frontend/src/api/*\n\nNo changes made (dry run).\n```\n\n### 3. Rollback on Error\n\nIf migration or code generation fails, rollback changes:\n\n```\nError applying migration 003_add_orders.sql:\n  - Foreign key constraint failed\n\nRolling back migration...\nDatabase restored to previous state.\n\nSync failed. Please fix migration and try again.\n```\n\n## Troubleshooting\n\n### Issue: \"Migration out of order\"\n\n```\nError: Migration 005_xxx.sql has timestamp before 004_xxx.sql\n```\n\n**Solution:** Rename migration file with correct sequential number.\n\n### Issue: \"Generated code conflicts with manual changes\"\n\n```\nWarning: backend/src/generated/db/models.rs has been manually modified\n```\n\n**Solution:**\n\n- Move custom code out of `generated/` directory\n- Use partial classes/extensions (if language supports it)\n- Accept regeneration and reapply manual changes\n\n### Issue: \"OpenAPI spec invalid after sync\"\n\n```\nError: Invalid $ref in spec/openapi.yaml:\n  - #/components/schemas/Order not found\n```\n\n**Solution:** Fix OpenAPI spec, then re-run sync.\n\n### Issue: \"Compilation fails after sync\"\n\n```\nError: Type mismatch in handler\n  - Expected: OrderResponse\n  - Found: Order\n```\n\n**Solution:** Run `/specforge:build` to update handlers with new types.\n\n## Resources\n\n- **OpenAPI Spec Validation**: https://redocly.com/docs/cli/\n- **Database Migrations Best Practices**: https://www.postgresql.org/docs/current/ddl-alter.html\n- **Git Conflict Resolution**: https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging\n- **Code Generation Patterns**: https://openapi-generator.tech/\n\n## Related Commands\n\n- `/specforge:plan` - Generate implementation plan for new features\n- `/specforge:build` - Full build pipeline (specs + handlers + tests)\n- `/specforge:validate` - Validate specs and code without regenerating\n- `/specforge:test` - Run test suite"
              },
              {
                "name": "/test",
                "description": "Test orchestration with behavior observation and iterative fixing",
                "path": "specforge/commands/test.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Test orchestration with behavior observation and iterative fixing"
                },
                "content": "# SpecForge Test Orchestrator\n\nRun comprehensive tests with behavior observation, diagnose failures, and iterate until all tests pass.\n\n## Overview\n\nThe test orchestrator runs tests at multiple levels and uses behavior observation to diagnose and fix issues:\n\n1. **Unit Tests** - Test individual handlers and functions\n2. **Integration Tests** - Test API endpoints end-to-end\n3. **Contract Tests** - Verify API matches OpenAPI spec\n4. **Behavior Observation** - Monitor test execution to identify issues\n5. **Iterative Fixing** - Diagnose and fix failures automatically\n\n## Test Orchestration Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  /specforge:test                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â–¼             â–¼             â–¼              â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Unit   â”‚  â”‚ Integrationâ”‚ â”‚ Contract â”‚  â”‚ E2E Tests   â”‚\nâ”‚ Tests  â”‚  â”‚ Tests     â”‚  â”‚ Tests    â”‚  â”‚ (optional)  â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n     â”‚            â”‚             â”‚              â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚ Behavior Observation â”‚\n       â”‚ & Failure Diagnosis  â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚ Iterate & Fix        â”‚\n       â”‚ (Max 3 iterations)   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Step 1: Read Configuration\n\nExtract the tech stack configuration from CLAUDE.md:\n\n```bash\n# Read SpecForge configuration\ngrep \"SpecForge configuration\" -A 10 CLAUDE.md\n```\n\nExtract:\n\n- Backend plugin\n- Database plugin\n- Codegen plugin\n- Frontend plugin (if applicable)\n\n## Step 2: Unit Tests\n\nRun unit tests for the backend using the backend plugin's testing expert:\n\n```bash\n# Get backend plugin\nBACKEND_PLUGIN=$(grep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' ')\n```\n\nInvoke the backend plugin's test agent:\n\n```\nUse {backend-plugin}/test-agent skill with:\n- action: \"run-unit-tests\"\n- test_dir: \"backend/tests/unit\"\n- coverage: true\n```\n\n**Checks:**\n\n- All unit tests pass\n- Code coverage meets threshold (default: 80%)\n- No test failures or errors\n- Mock setup correct\n- Edge cases covered\n\n**Technology-Specific Commands:**\n\n**Rust:**\n\n```bash\ncd backend && cargo test --lib\ncargo tarpaulin --out Html --output-dir coverage/\n```\n\n**TypeScript/Node:**\n\n```bash\ncd backend && npm test -- --coverage\n```\n\n**Python:**\n\n```bash\ncd backend && pytest tests/unit --cov=src --cov-report=html\n```\n\n**Go:**\n\n```bash\ncd backend && go test ./... -cover -coverprofile=coverage.out\n```\n\n## Step 3: Integration Tests\n\nRun integration tests that test full request-to-response flows:\n\n```\nUse {backend-plugin}/test-agent skill with:\n- action: \"run-integration-tests\"\n- test_dir: \"tests/integration\"\n- database_url: test database URL\n```\n\n**Test Pattern:**\n\nEach endpoint should have integration tests:\n\n```rust\n// Example Rust integration test\n#[tokio::test]\nasync fn test_create_order_success() {\n    // Setup test database\n    let db = setup_test_db().await;\n    let app = create_test_app(db.clone()).await;\n\n    // Create test user\n    let user = create_test_user(&db).await;\n\n    // Test request\n    let response = app\n        .post(\"/api/orders\")\n        .json(&json!({\n            \"items\": [{\"product_id\": 1, \"quantity\": 2}],\n            \"user_id\": user.id\n        }))\n        .send()\n        .await;\n\n    // Assertions\n    assert_eq!(response.status(), StatusCode::CREATED);\n    let order: Order = response.json().await;\n    assert_eq!(order.user_id, user.id);\n    assert_eq!(order.status, OrderStatus::Pending);\n}\n```\n\n**Checks:**\n\n- HTTP status codes correct\n- Response bodies match schemas\n- Database state updated correctly\n- Error handling works\n- Authentication/authorization enforced\n- Edge cases handled\n\n## Step 4: Contract Testing\n\nVerify that the running API matches the OpenAPI specification:\n\n```\nUse openapi-expert skill with:\n- action: \"contract-test\"\n- spec_path: \"spec/openapi.yaml\"\n- api_url: \"http://localhost:3000\"\n- test_report_path: \"test-results/contract-tests.json\"\n```\n\n**Tools:**\n\n- **Schemathesis**: Property-based API testing from OpenAPI spec\n- **Dredd**: HTTP API testing framework\n- **Prism**: Mock server and contract testing\n\n**Schemathesis Example:**\n\n```bash\n# Install schemathesis\npip install schemathesis\n\n# Run contract tests\nschemathesis run spec/openapi.yaml \\\n    --base-url http://localhost:3000 \\\n    --checks all \\\n    --hypothesis-max-examples=50\n```\n\n**Checks:**\n\n- All endpoints respond correctly\n- Response schemas match OpenAPI spec\n- Status codes match spec\n- Required fields present\n- Type validation correct\n- Examples in spec are valid\n\n## Step 5: Behavior Observation & Diagnosis\n\nMonitor test execution to identify patterns of failure:\n\n```javascript\nconst testResults = {\n  unit: { passed: [], failed: [] },\n  integration: { passed: [], failed: [] },\n  contract: { passed: [], failed: [] },\n};\n\n// Collect all failures\nconst allFailures = [\n  ...testResults.unit.failed,\n  ...testResults.integration.failed,\n  ...testResults.contract.failed,\n];\n\nif (allFailures.length > 0) {\n  // Analyze failure patterns\n  const patterns = analyzeFailurePatterns(allFailures);\n\n  // Categorize failures\n  const categories = {\n    typeErrors: [],\n    databaseErrors: [],\n    validationErrors: [],\n    businessLogicErrors: [],\n    networkErrors: [],\n  };\n\n  // Group by category for targeted fixing\n  for (const failure of allFailures) {\n    const category = categorizeFailure(failure);\n    categories[category].push(failure);\n  }\n}\n```\n\n**Common Failure Patterns:**\n\n1. **Type Mismatches**: Generated types don't match database schema\n2. **Query Errors**: SQL queries fail or return wrong data\n3. **Validation Errors**: Request validation fails unexpectedly\n4. **Business Logic Errors**: Handler logic incorrect\n5. **Schema Mismatches**: API responses don't match OpenAPI spec\n\n## Step 6: Iterative Fixing\n\nFor each category of failures, invoke the appropriate diagnostics agent:\n\n```javascript\nconst MAX_ITERATIONS = 3;\nlet iteration = 0;\nlet allTestsPassed = false;\n\nwhile (!allTestsPassed && iteration < MAX_ITERATIONS) {\n  iteration++;\n\n  // Run tests\n  const results = await runAllTests();\n\n  if (results.allPassed) {\n    allTestsPassed = true;\n    break;\n  }\n\n  // Diagnose type/codegen issues\n  if (results.typeErrors.length > 0) {\n    await invokeAgent(`${codegenPlugin}/diagnostics-agent`, {\n      model: \"sonnet\",\n      errors: results.typeErrors,\n      generated_code_path: \"backend/src/generated\",\n      schema_path: \"migrations/\",\n    });\n  }\n\n  // Diagnose handler/business logic issues\n  if (results.businessLogicErrors.length > 0) {\n    await invokeAgent(`${backendPlugin}/handler-agent`, {\n      model: \"sonnet\",\n      errors: results.businessLogicErrors,\n      handlers_path: \"backend/src/handlers\",\n    });\n  }\n\n  // Diagnose database/query issues\n  if (results.databaseErrors.length > 0) {\n    await invokeAgent(`${databasePlugin}/migration-agent`, {\n      model: \"haiku\",\n      errors: results.databaseErrors,\n      migrations_path: \"migrations/\",\n    });\n  }\n\n  // Re-run code generation if schema changed\n  if (schemaChanged) {\n    await regenerateCode();\n  }\n}\n\nif (!allTestsPassed) {\n  throw new Error(\n    `Tests still failing after ${MAX_ITERATIONS} iterations. Manual intervention required.`\n  );\n}\n```\n\n## Step 7: Test Report Generation\n\nGenerate a comprehensive test report:\n\n```markdown\n# SpecForge Test Report\n\n**Project**: my-api\n**Date**: 2025-01-15\n**Duration**: 45s\n**Iterations**: 2\n\n## Summary\n\n- âœ“ Unit Tests: 45/45 passed (100%)\n- âœ“ Integration Tests: 12/12 passed (100%)\n- âœ“ Contract Tests: 8/8 passed (100%)\n- **Total**: 65/65 tests passed\n\n## Coverage\n\n- Line Coverage: 87%\n- Branch Coverage: 82%\n- Function Coverage: 95%\n\n## Test Details\n\n### Unit Tests (45 passed)\n\n- âœ“ handlers::users::create_user\n- âœ“ handlers::users::get_user\n- âœ“ handlers::orders::create_order\n- âœ“ handlers::orders::get_orders_by_user\n- ... (41 more)\n\n### Integration Tests (12 passed)\n\n- âœ“ POST /api/users - creates user successfully\n- âœ“ GET /api/users/:id - returns user\n- âœ“ POST /api/orders - creates order\n- âœ“ GET /api/users/:id/orders - returns user's orders\n- ... (8 more)\n\n### Contract Tests (8 passed)\n\n- âœ“ POST /api/users matches schema\n- âœ“ GET /api/users/:id matches schema\n- âœ“ POST /api/orders matches schema\n- ... (5 more)\n\n## Iterations\n\n### Iteration 1\n\n- 3 test failures\n- Issues: Type mismatch in Order.created_at (expected DateTime, got String)\n- Fix: Updated sql-gen type overrides\n- Result: Re-ran codegen, 3 tests now pass\n\n### Iteration 2\n\n- All tests passed âœ“\n\n## Performance\n\n- Average response time: 45ms\n- Slowest endpoint: GET /api/users/:id/orders (120ms)\n- Database queries: Average 15ms\n\n## Recommendations\n\n1. Add tests for error cases (400, 404, 500)\n2. Add pagination tests for list endpoints\n3. Add authentication/authorization tests\n4. Consider adding load tests for critical endpoints\n```\n\n## Test Modes\n\n### Quick Test (Default)\n\nRun unit and integration tests only (skip contract tests):\n\n```bash\n/specforge:test\n```\n\n### Full Test (CI Mode)\n\nRun all test suites including contract tests:\n\n```bash\n/specforge:test --full\n```\n\n### Specific Test Suite\n\nRun a specific test suite:\n\n```bash\n/specforge:test --suite unit\n/specforge:test --suite integration\n/specforge:test --suite contract\n```\n\n### Watch Mode\n\nRun tests continuously on file changes (development mode):\n\n```bash\n/specforge:test --watch\n```\n\n## Integration with CI/CD\n\nExample GitHub Actions workflow:\n\n```yaml\nname: SpecForge Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: test\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install SpecForge\n        run: |\n          /plugin install specforge\n          /plugin install specforge-backend-rust-axum\n          /plugin install specforge-db-postgresql\n          /plugin install specforge-generate-rust-sql\n\n      - name: Run Tests\n        run: /specforge:test --full\n        env:\n          DATABASE_URL: postgresql://postgres:test@localhost:5432/test\n\n      - name: Upload Coverage\n        uses: codecov/codecov-action@v3\n        with:\n          files: ./coverage/lcov.info\n```\n\n## Testing Best Practices\n\n1. **Write tests during development** - Don't wait until the end\n2. **Test edge cases** - Empty lists, null values, boundary conditions\n3. **Use behavior observation** - Let agents learn from test failures\n4. **Iterate automatically** - Fix common issues without manual intervention\n5. **Maintain high coverage** - Aim for >80% code coverage\n6. **Test error handling** - Ensure proper error responses\n7. **Use contract testing** - Verify API matches OpenAPI spec\n8. **Test database constraints** - Ensure foreign keys, uniqueness work\n9. **Clean test data** - Reset database between tests\n10. **Mock external services** - Don't depend on third-party APIs\n\n## Common Test Failures & Fixes\n\n### Type Mismatch Errors\n\n```\nError: Type mismatch - expected i64, found String\n```\n\n**Fix**: Update codegen type overrides:\n\n```toml\n# sql-gen.toml\n[sql-gen.settings]\ntype_overrides = { \"user_id\" = \"i64\" }\n```\n\nRe-run: `/specforge:build`\n\n### Database Constraint Violations\n\n```\nError: FOREIGN KEY constraint failed\n```\n\n**Fix**: Ensure foreign key references exist in test setup:\n\n```rust\n// Create parent record first\nlet user = create_test_user(&db).await;\n\n// Then create child record\nlet order = create_test_order(&db, user.id).await;\n```\n\n### Schema Validation Failures\n\n```\nError: Response does not match schema - missing required field 'created_at'\n```\n\n**Fix**: Update handler to include all required fields:\n\n```rust\n// Ensure all schema fields are returned\nJson(OrderResponse {\n    id: order.id,\n    user_id: order.user_id,\n    total_cents: order.total_cents,\n    status: order.status,\n    created_at: order.created_at, // Don't forget this!\n})\n```\n\n### Contract Test Failures\n\n```\nError: Endpoint GET /api/users/:id returned 500, expected 200\n```\n\n**Fix**: Check handler error handling:\n\n```rust\n// Add proper error handling\nlet user = get_user_by_id(&db, id)\n    .await?\n    .ok_or(ApiError::NotFound)?;\n\nOk(Json(user))\n```\n\n## Resources\n\n- **Schemathesis**: https://schemathesis.readthedocs.io/\n- **Dredd**: https://dredd.org/\n- **Prism**: https://stoplight.io/open-source/prism\n- **Property-Based Testing**: https://hypothesis.readthedocs.io/\n- **Test Coverage Tools**: https://github.com/marketplace/actions/codecov\n\n## Frontend Testing (Optional)\n\nIf a frontend plugin is configured, run frontend tests:\n\n```\nUse {frontend-plugin}/test-expert skill with:\n- action: \"run-tests\"\n- test_dir: \"frontend/tests\"\n```\n\n**Frontend Test Types:**\n\n- Component tests (unit)\n- Integration tests (React Testing Library, etc.)\n- E2E tests (Playwright, Cypress)\n- Visual regression tests (optional)\n\n**Example E2E Test (Playwright):**\n\n```typescript\ntest(\"create order flow\", async ({ page }) => {\n  // Navigate to app\n  await page.goto(\"http://localhost:5173\");\n\n  // Create order\n  await page.click(\"text=New Order\");\n  await page.fill(\"[name=quantity]\", \"2\");\n  await page.click(\"text=Submit\");\n\n  // Verify order created\n  await expect(page.locator(\".order-list\")).toContainText(\"Order #1\");\n});\n```\n\n---\n\n**Test orchestration ensures your SpecForge project works correctly at all levels - from unit tests to end-to-end workflows.**"
              },
              {
                "name": "/validate",
                "description": "Multi-level validation (spec, schema, code, runtime) for SpecForge projects",
                "path": "specforge/commands/validate.md",
                "frontmatter": {
                  "name": "validate",
                  "description": "Multi-level validation (spec, schema, code, runtime) for SpecForge projects"
                },
                "content": "# SpecForge Validation Orchestrator\n\nPerform comprehensive validation across all levels of your SpecForge project: OpenAPI spec, database schema, generated code, and runtime behavior.\n\n## Validation Levels\n\n### Level 1: Specification Validation\n\nValidate the OpenAPI specification for correctness and best practices.\n\n```javascript\n// Validate OpenAPI spec\nawait invokeSkill(\"openapi-expert\", {\n  action: \"validate-spec\",\n  spec_path: \"spec/openapi.yaml\",\n});\n```\n\n**Checks:**\n\n- OpenAPI 3.x compliance\n- Schema definitions are complete\n- Required fields present\n- No circular references\n- Examples match schemas\n- Response codes appropriate\n- Security schemes defined\n- Operation IDs unique\n\n**Tools:**\n\n- [Redocly CLI](https://redocly.com/docs/cli/) - `redocly lint spec/openapi.yaml`\n- [Spectral](https://stoplight.io/open-source/spectral) - Custom linting rules\n\n### Level 2: Database Schema Validation\n\nValidate database schema for consistency and best practices.\n\n```javascript\n// Read CLAUDE.md to get database plugin\nconst config = await readCLAUDEmd();\nconst dbPlugin = config.specforge.database;\n\n// Validate database schema\nawait invokeSkill(`specforge-db-${dbPlugin}/migrations-expert`, {\n  action: \"validate-schema\",\n  migrations_dir: \"migrations/\",\n  database_url: process.env.DATABASE_URL,\n});\n```\n\n**Checks:**\n\n- Migration files are sequential\n- No conflicting migrations\n- Foreign key constraints valid\n- Indexes defined appropriately\n- Column types consistent\n- No missing NOT NULL constraints on required fields\n- Timestamps have defaults\n\n**Tools:**\n\n- Database-specific schema validators\n- Migration consistency checks\n\n### Level 3: Code Generation Validation\n\nValidate that code generation succeeded and generated code is correct.\n\n```javascript\n// Read CLAUDE.md to get tech stack\nconst config = await readCLAUDEmd();\nconst backendPlugin = config.specforge.backend;\nconst codegenPlugin = config.specforge.codegen;\n\n// Validate generated code exists\nawait invokeSkill(`${codegenPlugin}/diagnostics-expert`, {\n  action: \"validate-generated-code\",\n  generated_db_path: `${backendPlugin.path}/src/generated/db`,\n  generated_api_path: `${backendPlugin.path}/src/generated/api`,\n  database_schema_path: \"migrations/\",\n});\n```\n\n**Checks:**\n\n- Generated type files exist\n- Generated query functions exist\n- Types match database schema\n- API types match OpenAPI spec\n- No compilation errors\n- No type mismatches\n\n### Level 4: Compilation Validation\n\nEnsure the project compiles successfully.\n\n```javascript\n// Run compilation\nconst config = await readCLAUDEmd();\nconst backendPlugin = config.specforge.backend;\n\nawait invokeSkill(`${backendPlugin}/testing-expert`, {\n  action: \"compile-check\",\n  project_path: backendPlugin.path,\n});\n```\n\n**Checks (Technology-Specific):**\n\n**Rust:**\n\n```bash\ncd backend && cargo check\ncargo clippy -- -D warnings\n```\n\n**TypeScript:**\n\n```bash\ncd backend && npm run type-check\nnpm run lint\n```\n\n**Python:**\n\n```bash\ncd backend && mypy .\npylint src/\n```\n\n**Go:**\n\n```bash\ncd backend && go build ./...\ngo vet ./...\n```\n\n### Level 5: Runtime Validation\n\nValidate runtime behavior through tests and contract testing.\n\n```javascript\n// Run test suite\nawait invokeSkill(\"test-orchestrator\", {\n  action: \"run-all-tests\",\n  project_path: \".\",\n});\n\n// Contract testing against OpenAPI spec\nawait invokeSkill(\"openapi-expert\", {\n  action: \"contract-test\",\n  spec_path: \"spec/openapi.yaml\",\n  api_url: \"http://localhost:3000\",\n});\n```\n\n**Checks:**\n\n- Unit tests pass\n- Integration tests pass\n- API responses match OpenAPI spec\n- Database constraints enforced\n- Error handling correct\n- Performance acceptable\n\n**Tools:**\n\n- [Schemathesis](https://schemathesis.readthedocs.io/) - Property-based API testing\n- [Dredd](https://dredd.org/) - HTTP API testing against spec\n- [Prism](https://stoplight.io/open-source/prism) - Mock server validation\n\n## Validation Orchestration Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  /specforge:validate                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â–¼             â–¼             â–¼              â–¼               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Spec   â”‚  â”‚ Schema   â”‚  â”‚ Codegen  â”‚  â”‚ Compile  â”‚  â”‚ Runtime     â”‚\nâ”‚ Valid. â”‚  â”‚ Valid.   â”‚  â”‚ Valid.   â”‚  â”‚ Valid.   â”‚  â”‚ Valid.      â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n     â”‚            â”‚             â”‚             â”‚              â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                 â”‚\n                                 â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ Validation Report  â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Implementation\n\n### Step 1: Read Configuration\n\n```bash\n# Read CLAUDE.md to get tech stack configuration\ncat CLAUDE.md | grep \"SpecForge configuration\" -A 10\n```\n\nExtract:\n\n- Backend plugin\n- Database plugin\n- Codegen plugin\n- Frontend plugin (if applicable)\n\n### Step 2: Run Validation Levels Sequentially\n\nRun each validation level and collect results:\n\n```javascript\nconst results = {\n  spec: { status: \"pending\", errors: [] },\n  schema: { status: \"pending\", errors: [] },\n  codegen: { status: \"pending\", errors: [] },\n  compile: { status: \"pending\", errors: [] },\n  runtime: { status: \"pending\", errors: [] },\n};\n\n// Level 1: Spec Validation\ntry {\n  await validateSpec();\n  results.spec.status = \"passed\";\n} catch (error) {\n  results.spec.status = \"failed\";\n  results.spec.errors.push(error);\n}\n\n// Level 2: Schema Validation\ntry {\n  await validateSchema();\n  results.schema.status = \"passed\";\n} catch (error) {\n  results.schema.status = \"failed\";\n  results.schema.errors.push(error);\n}\n\n// Continue for other levels...\n```\n\n### Step 3: Invoke Validation Agent for Deep Debugging\n\nIf any validation fails, invoke the validation agent (Sonnet) for deep debugging:\n\n```javascript\nif (\n  results.spec.status === \"failed\" ||\n  results.schema.status === \"failed\" ||\n  results.codegen.status === \"failed\" ||\n  results.compile.status === \"failed\" ||\n  results.runtime.status === \"failed\"\n) {\n  await invokeAgent(\"validation-agent\", {\n    model: \"sonnet\",\n    results: results,\n    project_path: \".\",\n  });\n}\n```\n\n### Step 4: Generate Validation Report\n\n```markdown\n# SpecForge Validation Report\n\n## Summary\n\n- âœ“ Spec Validation: PASSED\n- âœ“ Schema Validation: PASSED\n- âœ— Codegen Validation: FAILED\n- - Compile Validation: SKIPPED\n- - Runtime Validation: SKIPPED\n\n## Details\n\n### Codegen Validation Errors\n\n1. **Missing generated type**: `Order` type not found in `src/generated/db/models.rs`\n\n   - Expected: `pub struct Order { ... }`\n   - Actual: File does not exist\n   - Fix: Run `/specforge:build` to regenerate code\n\n2. **Type mismatch**: `user_id` field type mismatch\n   - Database schema: `INTEGER`\n   - Generated type: `String`\n   - Fix: Update sql-gen.toml type overrides\n\n## Recommendations\n\n1. Run code generation: `/specforge:build`\n2. Fix type overrides in sql-gen.toml\n3. Re-run validation: `/specforge:validate`\n```\n\n## Validation Modes\n\n### Quick Validation (Default)\n\nRun spec, schema, and compile validation (skip runtime tests):\n\n```bash\n/specforge:validate\n```\n\n### Full Validation (CI Mode)\n\nRun all validation levels including runtime tests:\n\n```bash\n/specforge:validate --full\n```\n\n### Specific Level Validation\n\nRun a specific validation level:\n\n```bash\n/specforge:validate --level spec\n/specforge:validate --level schema\n/specforge:validate --level codegen\n/specforge:validate --level compile\n/specforge:validate --level runtime\n```\n\n## Exit Codes\n\n- `0`: All validations passed\n- `1`: Spec validation failed\n- `2`: Schema validation failed\n- `3`: Codegen validation failed\n- `4`: Compilation failed\n- `5`: Runtime validation failed\n\n## Integration with CI/CD\n\nExample GitHub Actions workflow:\n\n```yaml\nname: SpecForge Validation\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install SpecForge\n        run: |\n          /plugin install specforge\n          /plugin install specforge-backend-rust-axum\n          /plugin install specforge-db-sqlite\n          /plugin install specforge-generate-rust-sql\n\n      - name: Run Full Validation\n        run: /specforge:validate --full\n```\n\n## Validation Best Practices\n\n1. **Run validation after every spec change**\n2. **Fix validation errors before committing**\n3. **Use validation in CI/CD pipeline**\n4. **Review validation reports thoroughly**\n5. **Keep specs and schemas in sync**\n\n## Resources\n\n- **Redocly CLI**: https://redocly.com/docs/cli/\n- **Schemathesis**: https://schemathesis.readthedocs.io/\n- **Dredd**: https://dredd.org/\n- **OpenAPI Validation**: https://learn.openapis.org/validation.html\n\n## Troubleshooting\n\n### Common Validation Errors\n\n#### Spec Validation Fails\n\n```\nError: Missing required field 'description' for operation POST /users\n```\n\n**Fix**: Add description to OpenAPI spec:\n\n```yaml\npaths:\n  /users:\n    post:\n      description: Create a new user\n      # ...\n```\n\n#### Schema Validation Fails\n\n```\nError: Migration 002 references non-existent table 'users'\n```\n\n**Fix**: Ensure migrations are sequential and applied in order.\n\n#### Codegen Validation Fails\n\n```\nError: Generated type 'User' does not match database schema\n```\n\n**Fix**: Re-run code generation with updated schema:\n\n```bash\n/specforge:build\n```\n\n#### Compilation Fails\n\n```\nError: Type mismatch in handler - expected i64, found String\n```\n\n**Fix**: Update type overrides in codegen config or fix handler implementation.\n\n#### Runtime Validation Fails\n\n```\nError: API response does not match OpenAPI spec for GET /users/:id\nExpected type: object\nActual type: array\n```\n\n**Fix**: Update handler implementation or OpenAPI spec to match.\n\n---\n\n**Validation ensures your SpecForge project maintains integrity across all layers - from specs to runtime.**"
              }
            ],
            "skills": []
          },
          {
            "name": "specforge-backend-rust-axum",
            "description": "Rust + Axum backend framework expertise - implements handlers using OpenAPI and DB-generated types",
            "source": "./specforge-backend-rust-axum",
            "category": null,
            "version": "1.0.0",
            "author": {
              "name": "Daniel Emod Kovacs"
            },
            "install_commands": [
              "/plugin marketplace add claude-market/marketplace",
              "/plugin install specforge-backend-rust-axum@claude-market"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2025-11-04T00:09:55Z",
              "created_at": "2025-11-01T20:24:32Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}