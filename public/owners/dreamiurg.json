{
  "owner": {
    "id": "dreamiurg",
    "display_name": "Dmytro Gaivoronsky",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/369468?v=4",
    "url": "https://github.com/dreamiurg",
    "bio": "üá∫üá¶ üá∫üá∏ Engineering leader & builder. 25+ yrs in tech including Google, AWS, and Meta.\r\n",
    "stats": {
      "total_repos": 3,
      "total_plugins": 3,
      "total_commands": 0,
      "total_skills": 2,
      "total_stars": 8,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "dreamiurg/claude-mountaineering-skills",
      "url": "https://github.com/dreamiurg/claude-mountaineering-skills",
      "description": "Automates mountain route research for North American peaks. Aggregates data from 10+ mountaineering sources to generate detailed route beta reports with weather, avalanche conditions, and trip reports.",
      "homepage": "",
      "signals": {
        "stars": 7,
        "forks": 0,
        "pushed_at": "2025-11-14T05:12:57Z",
        "created_at": "2025-10-20T16:18:12Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/hooks.json",
          "type": "blob",
          "size": 197
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 539
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 752
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/pull_request_template.md",
          "type": "blob",
          "size": 1328
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/release.yml",
          "type": "blob",
          "size": 797
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 438
        },
        {
          "path": ".gitmessage",
          "type": "blob",
          "size": 1940
        },
        {
          "path": ".releaserc.json",
          "type": "blob",
          "size": 926
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 11835
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3842
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1066
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 10623
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 634
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/sync-changelog-to-readme.js",
          "type": "blob",
          "size": 4926
        },
        {
          "path": "scripts/update-versions.js",
          "type": "blob",
          "size": 1577
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/route-researcher",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/route-researcher/SKILL.md",
          "type": "blob",
          "size": 42064
        },
        {
          "path": "skills/route-researcher/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/route-researcher/assets/report-template.md",
          "type": "blob",
          "size": 15359
        },
        {
          "path": "skills/route-researcher/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/route-researcher/examples/2025-10-23-mount-si.md",
          "type": "blob",
          "size": 14011
        },
        {
          "path": "skills/route-researcher/examples/2025-11-06-mount-adams.md",
          "type": "blob",
          "size": 17338
        },
        {
          "path": "skills/route-researcher/examples/2025-11-06-wolf-peak.md",
          "type": "blob",
          "size": 13242
        },
        {
          "path": "skills/route-researcher/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/route-researcher/tools/.python-version",
          "type": "blob",
          "size": 5
        },
        {
          "path": "skills/route-researcher/tools/README.md",
          "type": "blob",
          "size": 11136
        },
        {
          "path": "skills/route-researcher/tools/calculate_daylight.py",
          "type": "blob",
          "size": 1621
        },
        {
          "path": "skills/route-researcher/tools/cloudscrape.py",
          "type": "blob",
          "size": 1005
        },
        {
          "path": "skills/route-researcher/tools/fetch_avalanche.py",
          "type": "blob",
          "size": 3014
        },
        {
          "path": "skills/route-researcher/tools/fetch_weather.py",
          "type": "blob",
          "size": 3837
        },
        {
          "path": "skills/route-researcher/tools/pyproject.toml",
          "type": "blob",
          "size": 512
        },
        {
          "path": "skills/route-researcher/tools/test_calculate_daylight.py",
          "type": "blob",
          "size": 473
        },
        {
          "path": "skills/route-researcher/tools/test_fetch_avalanche.py",
          "type": "blob",
          "size": 391
        },
        {
          "path": "skills/route-researcher/tools/test_fetch_weather.py",
          "type": "blob",
          "size": 435
        },
        {
          "path": "skills/route-researcher/tools/uv.lock",
          "type": "blob",
          "size": 103632
        }
      ],
      "marketplace": {
        "name": "mountaineering-skills-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "dreamiurg",
          "url": "https://github.com/dreamiurg"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "mountaineering-skills",
            "description": "Automated mountain route research combining PeakBagger data, weather forecasts, avalanche conditions, and trip reports into comprehensive beta documents",
            "source": {
              "source": "url",
              "url": "https://github.com/dreamiurg/claude-mountaineering-skills.git"
            },
            "category": null,
            "version": "3.5.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add dreamiurg/claude-mountaineering-skills",
              "/plugin install mountaineering-skills@mountaineering-skills-marketplace"
            ],
            "signals": {
              "stars": 7,
              "forks": 0,
              "pushed_at": "2025-11-14T05:12:57Z",
              "created_at": "2025-10-20T16:18:12Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "route-researcher",
                "description": "Research North American mountain peaks and generate comprehensive route beta reports",
                "path": "skills/route-researcher/SKILL.md",
                "frontmatter": {
                  "name": "route-researcher",
                  "description": "Research North American mountain peaks and generate comprehensive route beta reports"
                },
                "content": "# Route Researcher\n\nResearch mountain peaks across North America and generate comprehensive route beta reports combining data from multiple sources including PeakBagger, SummitPost, WTA, AllTrails, weather forecasts, avalanche conditions, and trip reports.\n\n**Data Sources:** This skill aggregates information from specialized mountaineering websites (PeakBagger, SummitPost, Washington Trails Association, AllTrails, The Mountaineers, and regional avalanche centers). The quality of the generated report depends on the availability of information on these sources. If your target peak lacks coverage on these websites, the report may contain limited details. The skill works best for well-documented peaks in North America.\n\n## When to Use This Skill\n\nUse this skill when the user requests:\n- Research on a specific mountain peak\n- Route beta or climbing information\n- Trip planning information for peaks\n- Current conditions for mountaineering objectives\n\nExamples:\n- \"Research Mt Baker\"\n- \"I'm planning to climb Sahale Peak next month, can you research the route?\"\n- \"Generate route beta for Forbidden Peak\"\n\n## Progress Checklist\n\nResearch Progress:\n- [ ] Phase 1: Peak Identification (peak validated, ID obtained)\n- [ ] Phase 2: Peak Information Retrieval (coordinates and details obtained)\n- [ ] Phase 3: Data Gathering (route descriptions, trip reports, weather, conditions collected)\n  - [ ] Phase 3 Stage 1: Parallel data gathering (Steps 3A-3H)\n  - [ ] Phase 3 Stage 2: Fetch trip report content (Step 3I - 10-15 reports for representative sample)\n- [ ] Phase 4: Route Analysis (synthesize route, crux, hazards from all sources including trip reports)\n- [ ] Phase 5: Report Generation (markdown file created)\n- [ ] Phase 6: Report Review & Validation (check for inconsistencies and errors)\n- [ ] Phase 7: Completion (user notified, next steps provided)\n\n## Orchestration Workflow\n\n### Phase 1: Peak Identification\n\n**Goal:** Identify and validate the specific peak to research.\n\n1. **Extract Peak Name** from user message\n   - Look for peak names, mountain names, or climbing objectives\n   - Common patterns: \"Mt Baker\", \"Mount Rainier\", \"Sahale Peak\", etc.\n\n2. **Search PeakBagger** using peakbagger-cli:\n   ```bash\n   uvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak search \"{peak_name}\" --format json\n   ```\n   - Parse JSON output to extract peak matches\n   - Each result includes: peak_id, name, elevation (feet/meters), location, url\n\n3. **Handle Multiple Matches:**\n   - If **multiple peaks** found: Use AskUserQuestion to present options\n     - For each option, show: peak name, elevation, location, AND PeakBagger URL\n     - Format each option description as: \"[Peak Name] ([Elevation], [Location]) - [PeakBagger URL]\"\n     - This allows user to click through and verify the correct peak\n     - Let user select the correct peak\n     - Provide \"Other\" option if none match\n\n   - If **single match** found: Confirm with user\n     - Present confirmation message with peak details and PeakBagger link\n     - Show: \"Found: [Peak Name] ([Elevation], [Location])\"\n     - Include PeakBagger URL in the message so user can verify: \"[PeakBagger URL]\"\n     - Use AskUserQuestion: \"Is this the correct peak? You can verify at [PeakBagger URL]\"\n\n   - If **no matches** found:\n     - Try peak name variations systematically (see \"Peak Name Variations\" section):\n       - **Word order reversal:** \"Mountain Pratt\" ‚Üí \"Pratt Mountain\"\n       - **Title variations:** Mt/Mount, St/Saint\n       - **Add location:** Include state or range name\n       - **Remove titles:** Try just the core name\n     - Run multiple searches in parallel with different variations\n     - Combine results and present best matches to user\n     - If still no results, use AskUserQuestion to ask for:\n       - A different peak name variation\n       - Direct PeakBagger peak ID or URL\n       - General PeakBagger search\n\n4. **Extract Peak ID:**\n   - From search results JSON, extract the `peak_id` field\n   - Store for use in subsequent peakbagger-cli commands\n   - Also store the PeakBagger URL for reference links\n\n### Phase 2: Peak Information Retrieval\n\n**Goal:** Get detailed peak information and coordinates needed for location-based data gathering.\n\nThis phase must complete before Phase 3, as coordinates are required for weather, daylight, and avalanche data.\n\nRetrieve detailed peak information using the peak ID from Phase 1:\n\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak show {peak_id} --format json\n```\n\nThis returns structured JSON with:\n- Peak name and alternate names\n- Elevation (feet and meters)\n- Prominence (feet and meters)\n- Isolation (miles and kilometers)\n- Coordinates (latitude, longitude in decimal degrees)\n- Location (county, state, country)\n- Routes (if available): trailhead, distance, vertical gain\n- Peak list memberships and rankings\n- Standard route description (if available in routes data)\n\n**Error Handling:**\n- If peakbagger-cli fails: Fall back to WebSearch/WebFetch and note in \"Information Gaps\"\n- If specific fields missing in JSON: Mark as \"Not available\" in gaps section\n- Rate limiting: Built into peakbagger-cli (default 2 second delay)\n\n**Once coordinates are obtained from this step, immediately proceed to Phase 3.**\n\n### Phase 3: Data Gathering\n\n**Goal:** Gather comprehensive route information from all available sources.\n\n**Execution Strategy:** Execute ALL steps in parallel to minimize total execution time.\n\nAll Phase 3 steps run simultaneously. Do not wait for any step to complete before starting others.\n\n#### Step 3A: Route Description Research (WebSearch + WebFetch)\n\n**Step 1:** Search for route descriptions:\n```\nWebSearch queries (run in parallel):\n1. \"{peak_name} route description climbing\"\n2. \"{peak_name} summit post route\"\n3. \"{peak_name} mountain project\"\n4. \"{peak_name} site:mountaineers.org route\"\n5. \"{peak_name} site:alltrails.com\"\n6. \"{peak_name} standard route\"\n```\n\n**Step 2:** Fetch top relevant pages:\n\n**Universal Fetching Strategy:**\n\nFor ANY website, use this two-tier approach:\n\n1. **Try WebFetch first** with appropriate extraction prompt\n2. **If WebFetch fails or returns incomplete data,** use cloudscrape.py as fallback:\n   ```bash\n   cd skills/route-researcher/tools\n   uv run python cloudscrape.py \"{url}\"\n   ```\n   Then parse the returned HTML to extract needed information.\n\n**Common sites and their extraction prompts:**\n\n**AllTrails (try WebFetch, fallback to cloudscrape.py):**\n```\nWebFetch Prompt: \"Extract route information including:\n- Trail name\n- Route description and key features\n- Difficulty rating\n- Distance and elevation gain\n- Estimated time\n- Route type (loop, out & back, point to point)\n- Best season\n- Known hazards or warnings\n- Current conditions if mentioned in recent reviews\"\n```\n\n**Save AllTrails URL for Phase 4:**\n- Overview sources section (primary route information sources)\n- Trip reports \"Browse All\" section (for reviews)\n\n**SummitPost, Mountaineers.org, PeakBagger (try WebFetch, fallback to cloudscrape.py):**\n```\nWebFetch Prompt: \"Extract route information including:\n- Route name and difficulty rating\n- Approach details and trailhead\n- Route description and key sections\n- Technical difficulty (YDS class, scramble grade, etc.)\n- Crux description\n- Distance and elevation gain\n- Estimated time\n- Known hazards and conditions\"\n```\n\n**Mountain Project, WTA (try WebFetch, fallback to cloudscrape.py):**\n```\nWebFetch Prompt: \"Extract route information including:\n- Approach details\n- Route description and key sections\n- Technical difficulty (YDS class, scramble grade, etc.)\n- Crux description\n- Distance and elevation gain\n- Estimated time\n- Known hazards\"\n```\n\n**Error Handling:**\n- If WebFetch fails or returns incomplete data: Automatically retry with cloudscrape.py\n- If cloudscrape.py also fails: Note in \"Information Gaps\" section with URL for manual checking\n- If no route descriptions found from any source: Note in gaps and provide general guidance\n- If conflicting information between sources: Note discrepancies in report\n\n#### Step 3B: Peak Ascent Statistics (peakbagger-cli)\n\nRetrieve ascent data and patterns using the peak ID:\n\n**Step 1: Get overall ascent statistics**\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak stats {peak_id} --format json\n```\n\nThis returns:\n- Total ascent count (all-time)\n- Seasonal distribution (by month)\n- Count of ascents with GPX tracks\n- Count of ascents with trip reports\n\n**Step 2: Get detailed ascent list based on activity level**\n\nBased on the total count from Step 1, adaptively retrieve ascents:\n\n**For popular peaks (>50 ascents total):**\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak ascents {peak_id} --format json --within 1y\n```\nRecent data (1 year) is sufficient for active peaks.\n\n**For moderate peaks (10-50 ascents total):**\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak ascents {peak_id} --format json --within 5y\n```\nExpand to 5 years to get meaningful sample size.\n\n**For rarely-climbed peaks (<10 ascents total):**\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger peak ascents {peak_id} --format json\n```\nGet all available ascent data regardless of age.\n\n**Additional filters (apply as needed):**\n- `--with-gpx`: Focus on ascents with GPS tracks (useful for route finding)\n- `--with-tr`: Focus on ascents with trip reports (useful for conditions)\n\n**Extract and save for Phase 4 (Report Generation):**\n- Total ascent statistics (total count, temporal breakdown, monthly distribution)\n- **All ascents from JSON with the following data:**\n  - Date (`date` field)\n  - Climber name (`climber.name` field)\n  - Trip report word count (`trip_report.word_count` field)\n  - GPX availability (`has_gpx` field)\n  - Ascent URL (`url` field)\n- Seasonal patterns (monthly distribution data)\n- Timeframe used for the query (1y, 5y, or all)\n\n**Error Handling:**\n- If peakbagger-cli fails: Fall back to WebSearch for trip reports\n- If no ascents found: Note in report and continue with other sources\n\n#### Step 3C: Trip Report Sources Discovery (WebSearch)\n\nSystematically search for trip report pages across major platforms:\n\n```\nWebSearch queries (run in parallel):\n1. \"{peak_name} site:wta.org\" - WTA hike page with trip reports\n2. \"{peak_name} site:alltrails.com\" - AllTrails page with reviews\n3. \"{peak_name} site:summitpost.org\" - SummitPost route page\n4. \"{peak_name} site:mountaineers.org\" - Mountaineers route information\n5. \"{peak_name} trip report site:cascadeclimbers.com\" - Forum discussions\n```\n\n**Extract and save URLs for Phase 4 (Report Generation):**\n- WTA trip reports URL (if found)\n- AllTrails trail page URL (if found)\n- SummitPost route/trip reports URL (if found)\n- Mountaineers.org route page URL (if found)\n- CascadeClimbers forum search URL or relevant thread URLs (if found)\n\n**Optional WebFetch for conditions data:**\n- If specific high-value trip reports identified, fetch 1-2 for detailed conditions\n- Extract recent dates and conditions mentioned for \"Recent Conditions\" section\n\n#### Step 3D: Weather Forecast (Open-Meteo API + NOAA)\n\n**Requires coordinates from Phase 2 (latitude, longitude, elevation):**\n\nGather weather data from multiple sources in parallel:\n\n**Source 1: Open-Meteo Weather API (Primary)**\n\nUse WebFetch to get detailed mountain weather forecast:\n```\nURL: https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&elevation={peak_elevation_m}&hourly=temperature_2m,precipitation,freezing_level_height,snow_depth,wind_speed_10m,wind_gusts_10m,weather_code&daily=temperature_2m_max,temperature_2m_min,precipitation_sum,precipitation_probability_max,wind_speed_10m_max&timezone=auto&forecast_days=7\n\nPrompt: \"Parse the JSON response and extract:\n- Daily weather summary for 6-7 days (date, conditions based on weather_code, temps, precip probability)\n- Freezing level height in feet for each day (convert from meters)\n- Snow depth if applicable\n- Wind speeds and gusts\n- Organize by calendar date with day-of-week\n- **IMPORTANT:** The timezone parameter is set to 'auto', so dates are in local time. Calculate day-of-week from the actual date strings in the JSON response (YYYY-MM-DD format). Today's date in local time is {current_date}.\n- Map weather_code to descriptive conditions (0=clear, 1-3=partly cloudy, 45-48=fog, 51-67=rain, 71-77=snow, 80-82=showers, 95-99=thunderstorms)\"\n```\n\n**Weather Code to Icon/Description mapping:**\n- 0: ‚òÄÔ∏è Clear\n- 1-3: ‚õÖ Partly cloudy\n- 45-48: üå´Ô∏è Fog\n- 51-67: üåßÔ∏è Rain\n- 71-77: ‚ùÑÔ∏è Snow\n- 80-82: üåßÔ∏è Showers\n- 95-99: ‚õàÔ∏è Thunderstorms\n\n**Source 2: Open-Meteo Air Quality API**\n\nUse WebFetch to get air quality forecast:\n```\nURL: https://air-quality-api.open-meteo.com/v1/air-quality?latitude={lat}&longitude={lon}&hourly=pm2_5,pm10,us_aqi&timezone=auto&forecast_days=7\n\nPrompt: \"Parse the JSON and determine air quality for the forecast period:\n- Check US AQI values: 0-50 (good), 51-100 (moderate), 101-150 (unhealthy for sensitive), 151-200 (unhealthy), 201-300 (very unhealthy), 301+ (hazardous)\n- Check PM2.5 and PM10 levels\n- Identify any days with AQI >100 (concerning for outdoor activities)\n- Return overall assessment and any days to be cautious\"\n```\n\n**Source 3: NOAA/NWS Point Forecast (Supplemental)**\n\nUse WebFetch for detailed text forecast and warnings:\n```\nURL: https://forecast.weather.gov/MapClick.php?textField1={lat}&textField2={lon}\nPrompt: \"Extract:\n- Detailed text forecasts for context\n- Any weather warnings or alerts\n- Hazardous weather outlook\"\n```\n\n**Source 4: NWAC Mountain Weather (if applicable)**\n\nIf in avalanche season (roughly Nov-Apr), check NWAC mountain weather:\n```\nWebFetch: https://nwac.us/mountain-weather-forecast/\nPrompt: \"Extract general mountain weather patterns for the Cascades region including synoptic pattern and multi-day trend\"\n```\n\n**Data to extract and save for Phase 4:**\n- 6-7 day forecast with conditions, temps, precipitation, wind\n- **Freezing level height for each day** (from Open-Meteo)\n- Snow depth changes (from Open-Meteo)\n- **Air quality assessment** (good/moderate/poor, note any concerning days)\n- Weather warnings or alerts (from NOAA)\n- Mountain-Forecast.com URL for manual checking (find via WebSearch, don't scrape)\n- **Open-Meteo Weather Link:** Construct from coordinates and elevation:\n  `https://open-meteo.com/en/docs#latitude={lat}&longitude={lon}&elevation={peak_elevation_m}&hourly=&daily=temperature_2m_max,temperature_2m_min,precipitation_sum&timezone=auto`\n- **Open-Meteo Air Quality Link:** Construct from coordinates:\n  `https://open-meteo.com/en/docs/air-quality-api#latitude={lat}&longitude={lon}&hourly=&daily=&timezone=auto`\n\n**Error Handling:**\n- If Open-Meteo API fails: Fall back to NOAA only, note reduced data quality in gaps\n- If Air Quality API fails: Note in gaps, continue without AQ data\n- If NOAA WebFetch fails: Continue with Open-Meteo data only\n- If NWAC not in season or fails: Skip this source\n- **Always provide manual check links** for Mountain-Forecast.com and NOAA even when API data retrieved\n\n#### Step 3E: Avalanche Forecast (Python Script)\n\n**Requires coordinates from Phase 2. Only for peaks with glaciers or avalanche terrain (elevation >6000ft in winter months):**\n\n```bash\ncd skills/route-researcher/tools\nuv run python fetch_avalanche.py --region \"North Cascades\" --coordinates \"{lat},{lon}\"\n```\n\n**Expected Output:** JSON with NWAC avalanche forecast\n\n**Error Handling:**\n- Script not yet implemented: Note \"Avalanche script pending - check NWAC.us manually\"\n- Script fails: Note in gaps with link to NWAC\n- Not applicable (low elevation, summer): Skip this step\n\n#### Step 3F: Daylight Calculations (Sunrise-Sunset.org API)\n\n**Requires coordinates from Phase 2 (latitude, longitude):**\n\nUse WebFetch to get sunrise/sunset data from Sunrise-Sunset.org API:\n\n```\nURL: https://api.sunrise-sunset.org/json?lat={latitude}&lng={longitude}&date={YYYY-MM-DD}&formatted=0\nPrompt: \"Extract the following data from the JSON response:\n- Sunrise time (convert from UTC to local time if needed)\n- Sunset time (convert from UTC to local time if needed)\n- Day length (convert seconds to hours and minutes)\n- Civil twilight begin/end (useful for alpine starts)\n- Solar noon\nFormat times in a user-friendly way (e.g., '6:45 AM', '8:30 PM')\"\n```\n\n**Data to extract and save for Phase 4:**\n- Sunrise time (local)\n- Sunset time (local)\n- Day length in hours and minutes\n- Civil twilight begin (useful for alpine starts)\n\n**Error Handling:**\n- If API call fails: Note in gaps section with link to timeanddate.com or sunrise-sunset.org\n- If no coordinates available: Skip this step and note in gaps\n- If date is far in future: API should still work, but note that times are calculated\n\n#### Step 3G: Access and Permits (WebSearch)\n\n```\nWebSearch queries:\n1. \"{peak_name} trailhead access\"\n2. \"{peak_name} permit requirements\"\n3. \"{peak_name} forest service road conditions\"\n```\n\n**Extract:**\n- Trailhead names and locations\n- Required permits (if any)\n- Access notes (road conditions, seasonal closures)\n\n#### Step 3H: Trip Report Identification\n\n**Goal:** Identify trip reports across all sources for comprehensive route beta coverage.\n\nThis step synthesizes information from:\n- PeakBagger ascent data (from Step 3B)\n- Trip report source URLs (from Step 3C)\n\n**Selection Strategy:**\n\nGather a representative sample of reports covering different perspectives:\n- **Recency:** Recent reports (last 1-2 years) for current conditions\n- **Variety:** Mix of sources (PeakBagger, WTA, Mountaineers) for diverse experiences\n- **Temporal spread:** Include older reports if they provide unique insights\n- **Sample size:** Aim for 10-15 reports total to capture range of conditions and perspectives\n\n**Note:** Report length/word count is NOT a quality indicator. A concise 50-word report with specific route beta is more valuable than a 500-word narrative about the drive. Focus on reports that have substantive content regardless of length.\n\n**PeakBagger Trip Reports (uses data from Step 3B):**\n\nFrom the ascent data already retrieved in Step 3B:\n\n1. **Identify reports with trip report content:**\n   - Filter: Only ascents where `trip_report.word_count > 0`\n   - Sort by date (most recent first) to identify recent reports\n   - Also identify reports from various time periods (not just recent)\n\n2. **Extract for each report:**\n   - Date (`date` field)\n   - Climber name (`climber.name` field)\n   - Word count (`trip_report.word_count` field)\n   - Ascent URL (`url` field)\n\n3. **Select diverse sample:**\n   - Take 5-10 recent reports (last 1-2 years)\n   - Include 2-5 older reports if they provide unique insights\n   - Include reports with GPX tracks when available (useful for users to download and verify route)\n   - Mix of seasons if available\n\n**WTA Trip Reports (if WTA URL found in Step 3C):**\n\nIf WTA URL was found, extract trip reports using the AJAX endpoint:\n\n```bash\n# Construct endpoint: {wta_url}/@@related_tripreport_listing\ncd skills/route-researcher/tools\nuv run python cloudscrape.py \"{wta_url}/@@related_tripreport_listing\"\n```\n\nParse HTML to extract for each report: date, author, trip report URL. Target 10-15 individual URLs, prioritize recent but include variety of dates.\n\n**Error Handling:**\n- If extraction yields <5 reports: Note in \"Information Gaps\"\n- If cloudscrape.py fails: Note with WTA browse link\n\n**Mountaineers.org Trip Reports (if Mountaineers URL found in Step 3C):**\n\nIf Mountaineers URL was found, extract trip reports from the trip-reports endpoint:\n\n```bash\n# Construct endpoint: {mountaineers_url}/trip-reports\ncd skills/route-researcher/tools\nuv run python cloudscrape.py \"{mountaineers_url}/trip-reports\"\n```\n\nParse HTML to extract for each report: date, title, author, trip report URL. Select top 3-5 most recent reports.\n\n**Error Handling:**\n- If cloudscrape.py fails: Note in \"Information Gaps\"\n- If no trip reports found: Note in \"Information Gaps\"\n\n**Extract and save for Phase 4 (Report Generation):**\n\n**High-Quality PeakBagger Reports:**\n- List of top 5-10 reports by word count (regardless of date)\n- Each with: date, climber name, word count, URL\n\n**Recent PeakBagger Reports:**\n- List of top 3-5 most recent reports with trip reports\n- Each with: date, climber name, word count, URL\n\n**WTA Reports:**\n- List of top 3-5 reports (recent or detailed)\n- Each with: date, author/title, URL\n\n**Mountaineers Reports:**\n- List of top 3-5 reports (recent or detailed)\n- Each with: date, title, URL\n\n**Error Handling:**\n- **WTA:** If cloudscrape.py fails for AJAX endpoint: Note in gaps, include WTA browse link only\n- **WTA:** If HTML parsing yields <5 reports: Note as parsing failure in gaps\n- **Mountaineers:** Do not attempt extraction - note limitation in gaps, include browse link\n- **AllTrails:** Do not attempt trip report extraction - note limitation in gaps if AllTrails URL was found\n- If no trip reports found on successfully fetched pages: Note in gaps, include browse link\n- If WTA/Mountaineers URLs not found in Step 3C: Skip those sources\n- PeakBagger data already available from Step 3B, no additional fetch needed\n\n#### Step 3I: Fetch Trip Report Content\n\n**Goal:** Fetch 10-15 trip reports to get representative sample of conditions and experiences (runs after Step 3H identifies candidates).\n\n**Selection from Step 3H results:**\n- Recent reports (last 1-2 years) for current conditions\n- Mix of sources (PeakBagger, WTA, Mountaineers)\n- Variety of dates/seasons to capture different conditions\n- Include some reports with GPX tracks when available (provides users with downloadable route data for verification)\n\n**Fetching:**\n\n**PeakBagger:** Use CLI to fetch full trip report content:\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger ascent show {ascent_id} --format json\n```\n\n**WTA/Mountaineers:** Use cloudscrape.py to fetch individual trip report pages:\n```bash\ncd skills/route-researcher/tools\nuv run python cloudscrape.py \"{trip_report_url}\"\n```\n\n**Extract and organize by theme:**\n- **Route:** Landmarks, variations, navigation details, actual times/distances\n- **Crux:** Difficulty assessments, technical requirements, conditions impact\n- **Hazards:** Rockfall, exposure, route-finding challenges, seasonal hazards, approach hazards\n- **Gear:** What people used/needed, seasonal variations\n- **Conditions:** Snow/ice timing, trail conditions, best months\n\n**Error Handling:**\n- If CLI/cloudscrape fails: Note which reports failed, continue with others\n- If report appears to have no substantive content: Note and continue\n- Minimum target: Successfully fetch at least 5-8 reports with useful content\n\n**Phase 3 Execution Summary:**\n\nPhase 3 has two execution stages:\n\n**Stage 1 - Parallel Execution (Steps 3A through 3H):**\n- Step 3A: Route descriptions (WebSearch + WebFetch)\n- Step 3B: Ascent statistics (peakbagger-cli)\n- Step 3C: Trip report sources (WebSearch)\n- Step 3D: Weather forecast (Open-Meteo + NOAA)\n- Step 3E: Avalanche forecast (Python script)\n- Step 3F: Daylight calculations (Sunrise-Sunset API)\n- Step 3G: Access and permits (WebSearch)\n- Step 3H: High-quality trip report identification\n\n**Stage 2 - Sequential Execution (Step 3I):**\n- Step 3I: Fetch high-quality trip report content (MUST run after 3H completes)\n\n**Performance Benefit:** Stage 1 total time = max(time(3A), time(3B), ..., time(3H)) instead of summing all step times. Stage 2 runs after Stage 1 completes to use identified reports from Step 3H.\n\n### Phase 4: Route Analysis\n\n**Goal:** Analyze gathered data to determine route characteristics and synthesize information.\n\n#### Step 4A: Determine Route Type\n\nBased on route descriptions, elevation, and gear mentions, classify as:\n- **Glacier:** Crevasses mentioned, glacier travel, typically >8000ft\n- **Rock:** Technical climbing, YDS ratings (5.x), protection mentioned\n- **Scramble:** Class 2-4, exposed but non-technical\n- **Hike:** Class 1-2, trail-based, minimal exposure\n\n#### Step 4B: Synthesize Route Information from Multiple Sources\n\n**Goal:** Combine trip reports (Step 3I), route descriptions (Step 3A), and other sources into comprehensive route beta.\n\n**Source Priority:**\n1. Trip reports (Step 3I) - first-hand experiences\n2. Route descriptions (Step 3A) - published beta baseline\n3. PeakBagger/ascent data (Steps 2 & 3B) - basic info, patterns\n\n**Synthesis Pattern for Route, Crux, and Hazards:**\n\n1. **Start with baseline** from route descriptions (standard route name, published difficulty)\n2. **Enrich with trip report details** (landmarks, specific conditions, actual experiences)\n3. **Note conflicts** when trip reports disagree with published info\n4. **Highlight consensus** (\"Multiple reports mention...\")\n5. **Include specifics** (elevations, locations, quotes)\n\n**Example (Route Description):**\n> \"The standard route follows the East Ridge (Class 3). Multiple trip reports mention a well-cairned use trail branching right at 4,800 ft‚Äîthis is the correct turn. The use trail climbs through talus (described as 'tedious' and 'ankle-rolling'). In early season, this section may be snow-covered, requiring microspikes.\"\n\n**Apply this pattern to:**\n- **Route:** Use baseline structure, add landmarks/navigation from trip reports, include actual times\n- **Crux:** Describe location/difficulty, add trip report assessments, note conditions-dependent variations\n- **Hazards:** Extract ALL hazards from trip reports (rockfall, exposure, route-finding, seasonal), organize by type, include specific locations and mitigation strategies. Be comprehensive‚Äîsafety-critical.\n\n**Extract Key Information:**\n\nFrom all synthesized data, identify:\n- **Difficulty Rating:** YDS class, scramble grade, or general difficulty (validated by trip reports)\n- **Crux:** Hardest/most technical section of route (synthesized above)\n- **Hazards:** All identified hazards (synthesized above)\n- **Notable Gear:** Any unusual or important gear mentioned in trip reports or beta (to be included in relevant sections, not as standalone section)\n- **Trailhead:** Name and approximate location\n- **Distance/Gain:** Round-trip distance and elevation gain (compare published vs actual trip report data)\n- **Time Estimates:** Calculate three-tier pacing based on distance and gain:\n  - **Fast pace:** Calculate based on 2+ mph and 1000+ ft/hr gain rate\n  - **Moderate pace:** Calculate based on 1.5-2 mph and 700-900 ft/hr gain rate\n  - **Leisurely pace:** Calculate based on 1-1.5 mph and 500-700 ft/hr gain rate\n  - Use the **slower** of distance-based or gain-based calculations for each tier\n  - Example: For 4 miles, 2700 ft gain:\n    - Fast: max(4mi/2mph, 2700ft/1000ft/hr) = max(2hr, 2.7hr) = ~2.5-3 hours\n    - Moderate: max(4mi/1.5mph, 2700ft/800ft/hr) = max(2.7hr, 3.4hr) = ~3-4 hours\n    - Leisurely: max(4mi/1mph, 2700ft/600ft/hr) = max(4hr, 4.5hr) = ~4-5 hours\n- **Freezing Level Analysis:** Compare peak elevation with forecasted freezing levels:\n  - **Include Freezing Level Alert if:** Any day in forecast has freezing level within 2000 ft of peak elevation\n  - **Omit if:** Freezing level stays >2000 ft above peak throughout forecast (typical summer conditions)\n  - Example: 5,469 ft peak with 5,000-8,000 ft freezing levels ‚Üí Include alert (marginal conditions)\n  - Example: 4,000 ft peak with 10,000+ ft freezing levels ‚Üí Omit alert (well above summit)\n\n#### Step 4C: Identify Information Gaps\n\nExplicitly document what data was **not found or unreliable:**\n- Missing trip reports\n- No GPS tracks available\n- Script failures (weather, avalanche, daylight)\n- Conflicting information between sources\n- Limited seasonal data\n\n### Phase 5: Report Generation\n\n**Goal:** Create comprehensive Markdown document following the template.\n\n#### Step 5A: Generate Report Content\n\nCreate report in the current working directory: `{YYYY-MM-DD}-{peak-name-lowercase-hyphenated}.md`\n\n**Filename Examples:**\n- `2025-10-20-mount-baker.md`\n- `2025-10-20-sahale-peak.md`\n\n**Location:** Reports are generated in the user's current working directory, not in the plugin installation directory.\n\n**Structure and Formatting:**\n\nRead `assets/report-template.md` and follow it exactly for:\n- Section structure and headings\n- Content formatting (how to present ascent data, trip report links, etc.)\n- Conditional sections (when to include/exclude sections based on available data)\n- All layout and presentation decisions\n\nThe template is the **single source of truth** for report formatting. Phase 3 (Data Gathering) specifies **what data to extract**. This phase (Phase 5) uses the template to determine **how to present that data**.\n\n#### Step 5B: Markdown Formatting Rules\n\nFollow these formatting rules to ensure proper Markdown rendering:\n\n1. **Blank lines before lists:** ALWAYS add a blank line before starting a bullet or numbered list\n   ```markdown\n   ‚úÖ CORRECT:\n   This is a paragraph.\n\n   - First bullet\n   - Second bullet\n\n   ‚ùå INCORRECT:\n   This is a paragraph.\n   - First bullet  (missing blank line)\n   ```\n\n2. **Blank lines after section headers:** Always have a blank line after ## or ### headers\n\n3. **Consistent list formatting:**\n   - Use `-` for unordered lists (not `*` or `+`)\n   - Indent sub-items with 2 spaces\n   - Keep list items concise (if >2 sentences, consider paragraphs instead)\n\n4. **Break up long text blocks:**\n   - Paragraphs >4 sentences should be split or bulleted\n   - Sequential steps should use numbered lists (1. 2. 3.)\n   - Related items should use bullet lists\n\n5. **Bold formatting:** Use `**text**` for emphasis, not for list item headers without bullets\n\n6. **Hazards and Safety:** Use bullet lists with sub-items:\n   ```markdown\n   **Known Hazards:**\n\n   - **Route-finding:** Orange markers help but can be missed\n   - **Slippery conditions:** Boulders treacherous when wet/icy\n   - **Weather exposure:** Above treeline sections exposed to elements\n   ```\n\n7. **Information that continues after colon:** Must have blank line before list:\n   ```markdown\n   ‚úÖ CORRECT:\n   Winter access adds the following:\n\n   - **Additional Distance:** 5.6 miles\n   - **Additional Elevation:** 1,700 ft\n\n   ‚ùå INCORRECT:\n   Winter access adds the following:\n   - **Additional Distance:** 5.6 miles  (missing blank line)\n   ```\n\n#### Step 5C: Write Report File\n\nUse the Write tool to create the file in the current working directory.\n\n**Verification:**\n- Use proper filename format (YYYY-MM-DD-peak-name.md)\n- Save file in user's current working directory\n- Validate Markdown syntax per formatting rules above\n- Check that all lists have blank lines before them\n\n### Phase 6: Report Review & Validation\n\n**Goal:** Systematically review the generated report for inconsistencies, errors, and quality issues before presenting to the user.\n\nThis phase ensures report quality by catching common issues that may occur during automated generation.\n\n#### Step 6A: Read Generated Report\n\nRead the complete report file that was just created in Phase 5.\n\n#### Step 6B: Systematic Quality Checks\n\nPerform the following checks in order:\n\n**1. Factual Consistency:**\n- Verify dates match their stated day-of-week (e.g., \"Thu Nov 6, 2025\" is actually a Thursday)\n- Verify narrative day-of-week references match the actual date\n- Check coordinates, elevations, and distances are consistent across all mentions\n- Verify weather forecasts align logically (freezing levels match precipitation types)\n- Check difficulty ratings are consistent between sections\n\n**2. Mathematical Accuracy:**\n- Verify elevation gains add up correctly\n- Check time estimates are reasonable given distance and elevation gain\n- Verify pace calculations match stated mph/ft per hour rates\n- Check unit conversions are correct (feet to meters, etc.)\n\n**3. Internal Logic:**\n- Verify hazard warnings align with route descriptions\n- Check recommendations match current conditions (not recommending a route when hazards are extreme)\n- Verify seasonal considerations are consistent with forecast data\n- Check crux descriptions match the overall difficulty rating\n\n**4. Completeness:**\n- Check for placeholder texts that weren't replaced (e.g., {peak_name}, {YYYY-MM-DD})\n- Verify all referenced links are actually provided\n- Check mandatory sections are present (Overview, Route, Current Conditions, Trip Reports, Information Gaps, Data Sources)\n- Verify trip report sections have actual URLs or proper placeholders\n\n**5. Formatting Issues:**\n- Check markdown headers are properly structured\n- Verify lists have proper blank lines before them (per Phase 5B formatting rules)\n- Check tables are properly formatted\n- Verify bold/emphasis markers are used correctly and not overdone\n\n**6. Source Consistency:**\n- Verify quoted or paraphrased details are accurate to sources (if in doubt, re-check)\n- Check conflicting information from different sources is acknowledged\n- Verify URLs are correct and complete\n\n**7. Safety & Responsibility:**\n- Verify critical hazards are properly emphasized\n- Check AI disclaimer is present and prominent\n- Verify users are directed to verify information from primary sources\n- Check limitations are explicitly stated in Information Gaps\n\n#### Step 6C: Fix Issues\n\nFor each issue found:\n1. **Document the issue** mentally (what's wrong, where it is, severity)\n2. **Fix the issue** immediately by editing the report file\n3. **Verify the fix** doesn't create new issues\n\n**Priority for fixes:**\n- **Critical:** Safety errors, factual errors, missing disclaimers (MUST fix)\n- **Important:** Completeness, usability, consistency issues (SHOULD fix)\n- **Minor:** Formatting, polish issues (FIX if quick, otherwise acceptable)\n\n**Common issues to watch for:**\n- Day-of-week mismatches (e.g., report dated Thursday but says \"today (Wednesday)\")\n- Missing blank lines before lists (violates Phase 5B rules)\n- Placeholder text not replaced\n- Inconsistent elevation or distance values\n- Weather data that doesn't make sense (e.g., snow at 12,000 ft freezing level)\n\n#### Step 6D: Save Corrected Report\n\nIf any issues were found and fixed:\n1. Use Edit or Write tool to save the corrected report\n2. Verify the file is saved in the correct location\n3. Proceed to Phase 7\n\nIf no issues were found:\n1. Proceed directly to Phase 7\n\n### Phase 7: Completion\n\n**Goal:** Inform user of completion and next steps.\n\nReport to user:\n1. **Success message:** \"Route research complete for {Peak Name}\"\n2. **File location:** Full absolute path to generated report\n3. **Summary:** Brief 2-3 sentence overview:\n   - Route type and difficulty\n   - Key hazards or considerations\n   - Any significant information gaps\n4. **Next steps:** Encourage user to:\n   - Review the report\n   - Verify critical information from primary sources\n   - Check current conditions before attempting route\n\n**Example completion message:**\n```\nRoute research complete for Mount Baker!\n\nReport saved to: 2025-10-20-mount-baker.md\n\nSummary: Mount Baker via Coleman-Deming route is a moderate glacier climb (Class 3) with significant crevasse hazards. The route involves 5,000+ ft elevation gain and typically requires an alpine start. Weather and avalanche forecasts are included.\n\nNext steps: Review the report and verify current conditions before your climb. Remember that mountain conditions change rapidly - check recent trip reports and weather forecasts immediately before your trip.\n```\n\n## Error Handling Principles\n\nThroughout execution, follow these error handling guidelines:\n\n### Script Failures\n- **Don't block:** If a Python script fails, note in \"Information Gaps\" and continue\n- **Provide alternatives:** Include manual check links (Mountain-Forecast.com, NWAC.us)\n- **One retry:** Retry once on network timeouts, then continue\n\n### Missing Data\n- **Be explicit:** Always document what wasn't found\n- **Be helpful:** Provide links for manual checking\n- **Don't guess:** Never fabricate data to fill gaps\n\n### Search Failures\n- **Try variations:** If peak not found, try alternate names (Mt vs Mount)\n- **Ask user:** If still not found, ask user for clarification or direct URL\n- **Provide guidance:** Suggest how to search PeakBagger manually\n\n### WebFetch/WebSearch Issues\n- **Universal fallback pattern:** Always try WebFetch first, then cloudscrape.py if it fails\n- **Automatic retry:** If WebFetch fails or returns incomplete data, immediately retry with cloudscrape.py\n- **Graceful degradation:** Missing one source shouldn't stop entire research\n- **Document gaps:** Note which sources were unavailable (both WebFetch AND cloudscrape.py failed)\n- **Prioritize safety:** If critical safety info (avalanche, hazards) unavailable, emphasize in gaps section\n\n## Execution Timeouts\n\n- **Individual Python scripts:** 30 seconds each\n- **WebFetch operations:** Use default timeout\n- **WebSearch operations:** Use default timeout\n- **Total skill execution:** Target 3-5 minutes, acceptable up to 10 minutes for comprehensive research\n\n## Quality Principles\n\nEvery generated report must:\n1. ‚úÖ **Include safety disclaimer** prominently at top\n2. ‚úÖ **Document all information gaps** explicitly\n3. ‚úÖ **Cite sources** with links\n4. ‚úÖ **Use current date** in filename and metadata\n5. ‚úÖ **Follow template structure** exactly\n6. ‚úÖ **Provide actionable information** (distances, times, gear)\n7. ‚úÖ **Emphasize verification** - this is research, not gospel\n\n## Implementation Notes\n\n### Current Status (as of 2025-10-21)\n\n**Implemented:**\n- **peakbagger-cli** integration for peak search, info, and ascent data\n- Python tools directory structure\n- Report generation in user's current working directory\n- **cloudscrape.py** - Universal fallback for WebFetch failures, works with ANY website including:\n  - Cloudflare-protected sites (SummitPost, PeakBagger, Mountaineers.org)\n  - AllTrails (when WebFetch fails)\n  - WTA (when WebFetch fails)\n  - Any other site that blocks or limits WebFetch access\n- **Two-tier fetching strategy:** WebFetch first, cloudscrape.py as automatic fallback\n- **Open-Meteo Weather API** for mountain weather forecasts (temperature, precipitation, freezing level, wind)\n- **Open-Meteo Air Quality API** for AQI forecasting (US AQI scale with conditional alerts)\n- Multi-source weather gathering (Open-Meteo, NOAA/NWS, NWAC)\n- Adaptive ascent data retrieval based on peak popularity\n- **Sunrise-Sunset.org API** for daylight calculations (sunrise, sunset, civil twilight, day length)\n- **High-quality trip report identification** across PeakBagger and WTA sources\n- **WTA AJAX endpoint** for trip report extraction (`{wta_url}/@@related_tripreport_listing`)\n\n**Pending Implementation:**\n- `fetch_avalanche.py` - NWAC avalanche data (currently using WebSearch/WebFetch as fallback)\n- **Browser automation** for Mountaineers.org and AllTrails trip report extraction (requires Playwright/Chrome)\n  - Current: Both sites load content via JavaScript, cloudscrape.py cannot extract\n  - Future: Add browser automation as 3rd-tier fallback\n\n**When Python scripts are not yet implemented:**\n- Note in \"Information Gaps\" section\n- Provide manual check links\n- Continue with available data\n- Don't block report generation\n\n### peakbagger-cli Command Reference (v1.7.0)\n\nAll commands use `--format json` for structured output. Run via:\n```bash\nuvx --from git+https://github.com/dreamiurg/peakbagger-cli.git@v1.7.0 peakbagger <command> --format json\n```\n\n**Available Commands:**\n- `peak search <query>` - Search for peaks by name\n- `peak show <peak_id>` - Get detailed peak information (coordinates, elevation, routes)\n- `peak stats <peak_id>` - Get ascent statistics and temporal patterns\n  - `--within <period>` - Filter by period (e.g., '1y', '5y')\n  - `--after <YYYY-MM-DD>` / `--before <YYYY-MM-DD>` - Date filters\n- `peak ascents <peak_id>` - List individual ascents with trip report links\n  - `--within <period>` - Filter by period (e.g., '1y', '5y')\n  - `--with-gpx` - Only ascents with GPS tracks\n  - `--with-tr` - Only ascents with trip reports\n  - `--limit <n>` - Max ascents to return (default: 100)\n- `ascent show <ascent_id>` - Get detailed ascent information\n\n**Note:** For comprehensive command options, run `peakbagger --help` or `peakbagger <command> --help`\n\n### Peak Name Variations\n\nCommon variations to try if initial search fails:\n- **Word order reversal:** \"Mountain Pratt\" ‚Üí \"Pratt Mountain\", \"Peak Sahale\" ‚Üí \"Sahale Peak\"\n- **Title expansion:** \"Mt\" ‚Üí \"Mount\", \"St\" ‚Üí \"Saint\"\n- **Add location:** \"Baker, WA\" or \"Baker, North Cascades\"\n- **Remove title:** \"Baker\" instead of \"Mt Baker\"\n- **Combine variations:** Try reversed order with title expansion (e.g., \"Mountain Pratt\" ‚Üí \"Pratt Mount\" + \"Pratt Mountain\")\n\n### Google Maps and USGS Links\n\n#### Summit Coordinates Links\n\n**Google Maps (for summit coordinates):**\n```\nhttps://www.google.com/maps/search/?api=1&query={latitude},{longitude}\n```\nExample: `https://www.google.com/maps/search/?api=1&query=48.7768,-121.8144`\n\n**USGS TopoView (for summit coordinates):**\n```\nhttps://ngmdb.usgs.gov/topoview/viewer/#{{latitude}}/{longitude}/15\n```\nExample: `https://ngmdb.usgs.gov/topoview/viewer/#17/48.7768/-121.8144`\n\n**Note:** Use decimal degree format for coordinates. TopoView uses zoom level in URL (15-17 works well for peaks).\n\n#### Trailhead Google Maps Links\n\n**If coordinates available** (e.g., from Mountaineers.org place information):\n```\nhttps://www.google.com/maps/search/?api=1&query={latitude},{longitude}\n```\nExample: `https://www.google.com/maps/search/?api=1&query=48.5123,-121.0456`\n\n**If only trailhead name available:**\n```\nhttps://www.google.com/maps/search/?api=1&query={trailhead_name}+{state}\n```\nExample: `https://www.google.com/maps/search/?api=1&query=Cascade+Pass+Trailhead+WA`\n\n**Note:** Prefer coordinates when available for more precise location."
              }
            ]
          }
        ]
      }
    },
    {
      "full_name": "dreamiurg/shipmate",
      "url": "https://github.com/dreamiurg/shipmate",
      "description": "Your daily dev sidekick for tracking what you ship",
      "homepage": "",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-11-09T02:12:02Z",
        "created_at": "2025-11-05T01:03:20Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/hooks.json",
          "type": "blob",
          "size": 3
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 460
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 791
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/pull_request_template.md",
          "type": "blob",
          "size": 871
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/ci.yml",
          "type": "blob",
          "size": 2740
        },
        {
          "path": ".github/workflows/release.yml",
          "type": "blob",
          "size": 797
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 238
        },
        {
          "path": ".gitleaks.toml",
          "type": "blob",
          "size": 633
        },
        {
          "path": ".gitmessage",
          "type": "blob",
          "size": 595
        },
        {
          "path": ".markdownlint-cli2.yaml",
          "type": "blob",
          "size": 1199
        },
        {
          "path": ".mise.toml",
          "type": "blob",
          "size": 63
        },
        {
          "path": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1080
        },
        {
          "path": ".releaserc.json",
          "type": "blob",
          "size": 812
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 7152
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3756
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1066
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 11309
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/claude-analyzer-agent.md",
          "type": "blob",
          "size": 2514
        },
        {
          "path": "agents/correlation-agent.md",
          "type": "blob",
          "size": 4831
        },
        {
          "path": "agents/github-analyzer-agent.md",
          "type": "blob",
          "size": 3358
        },
        {
          "path": "agents/summarizer-agent.md",
          "type": "blob",
          "size": 7946
        },
        {
          "path": "config.example.yaml",
          "type": "blob",
          "size": 1550
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/AGENT_OUTPUT_PROTOCOL.md",
          "type": "blob",
          "size": 4146
        },
        {
          "path": "docs/CLAUDE_SESSIONS.md",
          "type": "blob",
          "size": 1927
        },
        {
          "path": "integrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "integrations/notion",
          "type": "tree",
          "size": null
        },
        {
          "path": "integrations/notion/README.md",
          "type": "blob",
          "size": 1916
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 585
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/fetch-github-activity.sh",
          "type": "blob",
          "size": 4352
        },
        {
          "path": "scripts/parse-claude-sessions.js",
          "type": "blob",
          "size": 4720
        },
        {
          "path": "scripts/update-versions.js",
          "type": "blob",
          "size": 1853
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/eod",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/eod/SKILL.md",
          "type": "blob",
          "size": 18500
        }
      ],
      "marketplace": {
        "name": "shipmate-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "dreamiurg",
          "url": "https://github.com/dreamiurg"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "shipmate",
            "description": "Your daily dev sidekick for tracking what you ship - generates conversational end-of-day summaries from GitHub activity",
            "source": {
              "source": "url",
              "url": "https://github.com/dreamiurg/shipmate.git"
            },
            "category": null,
            "version": "1.3.3",
            "author": null,
            "install_commands": [
              "/plugin marketplace add dreamiurg/shipmate",
              "/plugin install shipmate@shipmate-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-11-09T02:12:02Z",
              "created_at": "2025-11-05T01:03:20Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "eod",
                "description": "Generate a concise end-of-day summary from GitHub activity in the last 24 hours - commits, issues, and PRs",
                "path": "skills/eod/SKILL.md",
                "frontmatter": {
                  "name": "eod",
                  "description": "Generate a concise end-of-day summary from GitHub activity in the last 24 hours - commits, issues, and PRs"
                },
                "content": "# End-of-Day Summary\n\nThis skill orchestrates two specialized agents to generate a concise summary of your GitHub activity for the current day.\n\n## What This Skill Does\n\n1. **Extracts GitHub activity data** using the `github-analyzer-agent` agent\n2. **Generates a beautiful summary** using the `summarizer-agent` agent\n\nThe result is a conversational, scannable update perfect for sharing with teammates.\n\n## Usage\n\nAsk Claude naturally: \"Generate my end-of-day summary\"\n\nOr invoke directly with the Skill tool: `shipmate:eod`\n\n## Configuration\n\nThis skill can be configured via `shipmate.yaml` in either:\n\n- `~/.claude/shipmate.yaml` (global)\n- `<project>/.claude/shipmate.yaml` (project-specific, overrides global)\n\nSee `config.example.yaml` for configuration options.\n\n## MANDATORY FIRST STEP: Create Todo List\n\n**BEFORE DOING ANYTHING ELSE**, you MUST use the TodoWrite tool to create a task list with these EXACT step names:\n\n1. \"Detect GitHub organizations and username\"\n2. \"Ask user to select activity scope\"\n3. \"Extract GitHub activity data\"\n4. \"Extract Claude Code sessions (if enabled)\"\n5. \"Analyze activity and identify themes\"\n6. \"Ask user to select main topics\"\n7. \"Generate conversational summary\"\n8. \"Present summary to user\"\n9. \"Post to enabled integrations (if configured)\"\n\nMark the first todo as `in_progress` immediately after creating the list.\n\n**Why:** This provides visibility to the user and ensures consistent progress tracking across all skill runs.\n\n## Process\n\n### Step 1: Detect GitHub Organizations (MANDATORY - DO NOT SKIP)\n\n**CRITICAL:** You MUST complete this step before proceeding. Do NOT jump to Step 3.\n\nRun these commands to detect organizations and get the username:\n\n```bash\ngh api user/orgs --jq '.[].login'\n```\n\n```bash\ngh api user --jq '.login'\n```\n\nStore both the list of organizations and the username.\n\n### Step 2: Ask User for Scope (MANDATORY - DO NOT SKIP)\n\n**CRITICAL:** You MUST ask the user which scope they want. Do NOT assume or skip this step.\n\nUse the AskUserQuestion tool with this EXACT question:\n\n**Question:** \"Which GitHub activity would you like to include in your end-of-day summary?\"\n\n**Options** (create one option for each scenario):\n\n- **Personal account only** ({username}) - \"Only activity from your personal GitHub account\"\n- **Organization: {org_name}** - \"Only activity from the {org_name} organization\" (one option per org discovered)\n- **All accounts** - \"Activity from your personal account and all organizations\"\n\n**IMPORTANT:**\n\n- If the user belongs to multiple organizations, include one option for EACH organization\n- Always include the \"Personal account only\" option\n- Always include the \"All accounts\" option\n- Do NOT proceed to Step 3 until the user has made a selection\n\nStore the user's selection for use in Step 3.\n\n### Step 2.5: Read Configuration and Detect Script Paths\n\n**A. Read configuration from `shipmate.yaml`:**\n\nCheck for configuration in these locations (project-specific overrides global):\n\n- `<project>/.claude/shipmate.yaml`\n- `~/.claude/shipmate.yaml`\n\nExtract these values from the `claude_sessions` section:\n\n- `enabled` (default: true)\n- `time_window_hours` (default: 24)\n- `correlation_window_hours` (default: 2)\n- `min_duration_minutes` (default: 2)\n\nExtract these values from the `integrations` section:\n\n- `notion.enabled` (default: false)\n- `notion.daily_log_url` (required if notion.enabled is true)\n\nSet `HAS_INTEGRATIONS` flag:\n- `true` if any integration is enabled (e.g., `notion.enabled` is true)\n- `false` if no integrations are enabled\n\n**B. Detect script paths for agents:**\n\nRun these commands separately to find the shipmate plugin directory and set script paths:\n\n```bash\nfind ~/.claude/plugins -name \"shipmate\" -type d 2>/dev/null | head -1\n```\n\nIf the command returns a path (plugin is installed):\n- Set `GITHUB_SCRIPT` to `{plugin_dir}/scripts/fetch-github-activity.sh`\n- Set `CLAUDE_SCRIPT` to `{plugin_dir}/scripts/parse-claude-sessions.js`\n\nIf the command returns nothing (running locally):\n- Set `GITHUB_SCRIPT` to `./scripts/fetch-github-activity.sh`\n- Set `CLAUDE_SCRIPT` to `./scripts/parse-claude-sessions.js`\n\nStore these paths and flags for use in later steps.\n\n### Step 3: Invoke Data Extraction Agents in Parallel\n\n**Mark todo #2 as completed, todos #3 and #4 as in_progress.**\n\n**IMPORTANT: Run both agents in PARALLEL using a single message with TWO Task tool calls.**\n\n**Agent 1 - GitHub Analyzer:**\n\nUse the Task tool to invoke the `shipmate:github-analyzer-agent` agent (subagent_type=\"shipmate:github-analyzer-agent\"):\n\n```text\nPlease extract GitHub activity data for the last 24 hours with the following scope: [user's selection from Step 2]\n\nUse the bundled script at this path: {GITHUB_SCRIPT from Step 2.5}\n\nRun it as:\n{GITHUB_SCRIPT} [scope] {username} [{org_name}]\n\nWhere:\n- scope: \"personal\", \"org\", or \"all\" based on user selection\n- username: {username from Step 1}\n- org_name: {org_name from Step 1} (only if scope is \"org\")\n\nThe script returns consolidated JSON with commits, issues, and PRs.\n```\n\n**Agent 2 - Claude Analyzer (Conditional):**\n\n**Check if Claude sessions integration is enabled** from the configuration read in Step 2.5.\n\n**If `claude_sessions.enabled` is true:**\n\nUse the Task tool to invoke the `shipmate:claude-analyzer-agent` agent (subagent_type=\"shipmate:claude-analyzer-agent\", model=\"haiku\") **IN THE SAME MESSAGE as Agent 1**:\n\n```text\nExtract Claude Code sessions from the last {time_window_hours} hours with minimum duration {min_duration_minutes} minutes.\n\nUse the bundled script at this path: {CLAUDE_SCRIPT from Step 2.5}\n\nRun it as:\nnode {CLAUDE_SCRIPT} {time_window_hours} {min_duration_minutes}\n\nThe script returns JSON with session metadata including:\n- session_id, project_path\n- start_time, end_time, duration_minutes\n- message_count, summary\n- tool_usage (file_edits, bash_commands, reads)\n- metadata (time_window_hours, min_duration_minutes, total_sessions)\n```\n\n**If `claude_sessions.enabled` is false:**\n\nSkip the Claude analyzer agent. Set `CLAUDE_SESSIONS` to empty result:\n\n```json\n{\n  \"sessions\": [],\n  \"metadata\": {\n    \"time_window_hours\": 24,\n    \"min_duration_minutes\": 2,\n    \"total_sessions\": 0\n  }\n}\n```\n\n**IMPORTANT:**\n\n- Launch both agents in parallel (single message with 2 Task calls) when sessions are enabled\n- Use Haiku model for Claude analyzer (fast, cheap execution)\n- Both agents run independently and return results simultaneously\n- Store GitHub data in `GITHUB_ACTIVITIES` variable\n- Store Claude data in `CLAUDE_SESSIONS` variable\n\nOnce both agents return data (or Claude sessions disabled), mark todos #3 and #4 as completed.\n\n### Step 4: Analyze Activity and Identify Key Themes\n\n**Mark todo #5 as in_progress.**\n\nReview the data from Step 3 and identify distinct themes/topics based on:\n\n- Issue bodies and titles (what questions were being answered?)\n- Commit messages and patterns\n- Repositories affected\n- Type of work (investigation, feature, bugfix, tooling, etc.)\n\nFor each theme, create a clear, descriptive label like:\n\n- \"Investigated AWS infrastructure\"\n- \"Investigated Railway deployments\"\n- \"Investigated Auth0 setup\"\n- \"Set up secret detection\"\n- \"Added markdown linting\"\n- \"Documentation improvements\"\n\n**IMPORTANT**: Identify ALL distinct themes, not just major ones. Include both substantial investigations and smaller tasks.\n\n### Step 4.5: Correlate Claude Sessions with GitHub Activity (Conditional)\n\n**Before marking todo #5 as completed**, check if correlation is needed.\n\n**If `CLAUDE_SESSIONS.metadata.total_sessions` is 0:**\n\n- Skip correlation entirely (no sessions to correlate)\n- Set `ENRICHED_ACTIVITIES` to `GITHUB_ACTIVITIES` (pass through unchanged)\n- Set `ORPHANED_SESSIONS` to empty array\n- Mark todo #5 as completed and proceed to Step 5\n\n**If `CLAUDE_SESSIONS.metadata.total_sessions` > 0:**\n\nUse the Task tool to invoke the `shipmate:correlation-agent` agent (subagent_type=\"shipmate:correlation-agent\", model=\"haiku\"):\n\n```text\nCorrelate Claude Code sessions with GitHub activities using time proximity and path matching.\n\nInput:\n- GitHub Activities: {JSON from Step 3}\n- Claude Sessions: {JSON from Step 3}\n- Correlation Window: {correlation_window_hours from config}\n\nMatch sessions to activities by:\n1. Path matching (normalized repo names)\n2. Time proximity (¬±{correlation_window_hours} hours)\n\nReturn enriched activities with related_sessions and identify orphaned sessions (investigation work without commits).\n```\n\n**IMPORTANT:**\n\n- Use Haiku model for fast, cheap correlation\n- The agent will return enriched activities with `related_sessions` arrays\n- Orphaned sessions represent investigation/exploration work\n- Store the agent's output in `ENRICHED_ACTIVITIES` and `ORPHANED_SESSIONS` for use in Step 6\n\nOnce the agent returns data (or correlation was skipped), mark todo #5 as completed.\n\n### Step 5: Ask User to Select Main Topics\n\n**Mark todo #6 as in_progress.**\n\nUse the AskUserQuestion tool with multiSelect enabled:\n\n**Question:** \"Which topics should be highlighted as main accomplishments? (Select 2-4. Everything else will be grouped as 'Housekeeping')\"\n\n**Options**: Create one option for each theme identified in Step 4, ordered by estimated importance/time spent (most significant first)\n\nExample:\n\n```text\n- \"Investigated AWS infrastructure\" - \"Documented IAM setup, S3 buckets, VPC config, and admin access\"\n- \"Investigated Railway deployments\" - \"Found two projects, identified active vs inactive deployments\"\n- \"Set up secret detection\" - \"Added pre-commit hooks with gitleaks to prevent credential leaks\"\n- \"Added markdown linting\" - \"Configured markdownlint-cli2 with auto-fix\"\n```\n\nStore the user's selections (2-4 topics).\n\nOnce the user has made their selections, mark todo #6 as completed.\n\n### Step 6: Invoke Activity Summarizer Agent\n\n**Mark todo #7 as in_progress.**\n\nUse the Task tool to invoke the `shipmate:summarizer-agent` agent (subagent_type=\"shipmate:summarizer-agent\"):\n\n```text\nPlease create a team standup summary from this GitHub activity data:\n\n[Paste the enriched GitHub activities from Step 4.5, including any related_sessions]\n\nOrphaned Claude sessions (investigation work without commits):\n[Paste orphaned sessions from Step 4.5, if any]\n\nThe user has selected these topics to highlight as main accomplishments:\n[List the topics selected in Step 5]\n\nGenerate a conversational summary following the format with:\n- Selected topics as separate bullets with detailed findings and insights\n- All other activities grouped as \"Housekeeping\"\n- Plain URLs to documentation artifacts\n- Past tense, casual tone\n- Weave in session insights naturally where related_sessions exist (see agent instructions for guidance)\n```\n\n**IMPORTANT**:\n\n- Pass the enriched GitHub activities with related_sessions (from Step 4.5)\n- Pass orphaned sessions separately\n- Include correlation window hours in context\n- Clearly indicate which topics the user selected to highlight\n- Everything NOT selected should be grouped into \"Housekeeping\"\n- The agent will return the formatted summary\n\nOnce the agent returns the summary, mark todo #7 as completed.\n\n### Step 7: Present Summary to User\n\n**Mark todo #8 as in_progress.**\n\nDisplay the summary returned by the summarizer agent.\n\n**Optional Polish**: If the `elements-of-style:writing-clearly-and-concisely` skill is available, you may optionally use it to polish the summary further, but the summarizer agent already applies these principles.\n\nOnce the summary is displayed, mark todo #8 as completed.\n\n### Step 8: Post to Enabled Integrations (if configured)\n\n**Mark todo #9 as in_progress.**\n\nCheck the `HAS_INTEGRATIONS` flag set in Step 2.5.\n\n**If `HAS_INTEGRATIONS` is false:**\n\n- Skip integration posting entirely\n- Mark todo #9 as completed\n- You're done!\n\n**If `HAS_INTEGRATIONS` is true:**\n\nProceed to Step 9 to post to enabled integrations.\n\n### Step 9: Post to Notion Daily Log (if enabled)\n\nIf Notion integration is enabled in config:\n\n1. Get today's date in \"Month Day, Year\" format (e.g., \"November 4, 2025\")\n2. Fetch the Daily Log page URL from config: `integrations.notion.daily_log_url`\n3. Format the summary using the user's preferred bullet format:\n\n```markdown\n## {Date}\n\n### What I accomplished today\n\n- **{Topic 1}** - {Description from summary}\n - [{Link text}]({{URL}})\n- **{Topic 2}** - {Description from summary}\n - [{Link text}]({{URL}})\n- **{Topic 3}** - {Description from summary}\n - [{Link text}]({{URL}})\n- **Housekeeping** - {Description from summary}\n```\n\n**IMPORTANT Formatting Rules**:\n\n- Each main accomplishment is a top-level bullet with bold topic name\n- Key artifacts (1-3 per topic) are nested bullets (indented with tab) under each accomplishment\n- Include the most relevant links: documentation files, PRs, and issues that capture the work\n- Use plain markdown links like `[Auth0 Documentation]({{URL}})` or `[PR #123]({{URL}})`\n- Extract actual GitHub URLs from the summary text and raw activity data\n- Prioritize comprehensive documentation links over minor commits\n- The Housekeeping item does NOT have nested links unless there are specific artifacts to link\n\n1. If the page already has content, prepend the new entry at the top (most recent first)\n2. If the page is blank, just add the new entry\n3. Use the `mcp__notion__notion-update-page` tool with appropriate command (`replace_content` or `insert_content_after`)\n\nOnce posted to Notion (or if Notion is not enabled), mark todo #8 as completed.\n\n**Example of final Notion format**:\n\n```markdown\n## November 4, 2025\n\n### What I accomplished today\n\n- **Investigated Auth0 setup** - Dug through the whole Auth0 configuration to see what we're actually using. Found we have one production tenant with three main applications. Turns out we're using Google and Microsoft for social logins, have MFA set up but not enforced, and have a bunch of roles configured. Documented all the apps, APIs, connections, rules, hooks, and who has admin access.\n - [Auth0 Documentation]({{https://github.com/example-org/docs/blob/main/infrastructure/auth0.md}})\n - [Issue #7: Auth0 Configuration Inventory]({{https://github.com/example-org/docs/issues/7}})\n- **Investigated AWS infrastructure** - Went through the AWS account to figure out what we have. Found IAM users and roles, data in S3 buckets, VPC configuration, and mapped out who has admin access. Got it all documented so we can plan the migration.\n - [AWS Documentation]({{https://github.com/example-org/docs/blob/main/infrastructure/aws.md}})\n- **Investigated deployment process** - Figured out how we actually ship code to production. Frontend deploys manually, backend auto-deploys from GitHub.\n - [Deployment Process Documentation]({{https://github.com/example-org/docs/blob/main/infrastructure/deployment-process.md}})\n - [Issue #5]({{https://github.com/example-org/docs/issues/5}})\n- **Housekeeping** - Set up pre-commit hooks with gitleaks to prevent secrets from leaking into the repo, added markdownlint for documentation quality.\n```\n\n## Agent Roles\n\n### github-analyzer-agent\n\n- **Specialty**: GitHub CLI (`gh`) expertise\n- **Tools**: Bash\n- **Model**: Haiku (fast, cost-effective for data extraction)\n- **Output**: Structured JSON data about commits, issues, PRs\n\n### summarizer-agent\n\n- **Specialty**: Writing conversational, scannable summaries\n- **Tools**: None (pure analysis and writing)\n- **Model**: Sonnet (better at nuanced writing)\n- **Output**: Formatted markdown summary ready to share\n\n## Why Two Agents?\n\n**Separation of Concerns**:\n\n- **Analyzer** focuses on technical data gathering (parallel queries, timezone handling, error handling)\n- **Summarizer** focuses on human communication (tone, grouping, insights)\n\n**Optimized Models**:\n\n- Use fast/cheap Haiku for repetitive data extraction\n- Use smart Sonnet for nuanced writing and analysis\n\n**Maintainability**:\n\n- Each agent has a single, clear responsibility\n- Can improve or swap agents independently\n- Easier to test and debug\n\n## Important Notes\n\n- **Performance**: The analyzer agent runs all GitHub queries in parallel (~2-3 seconds total)\n- **Time Range**: Last 24 hours from current time (not calendar day)\n- **Scope**: Supports personal, organization-specific, or all-account views\n- **Artifacts**: Includes links to documentation produced\n- **Tone**: Conversational and scannable, optimized for team standups\n\n## Error Handling\n\nIf the github-analyzer-agent fails:\n\n- Check `gh auth status`\n- Verify `gh` CLI is installed (requires 2.23.0+)\n- Verify organization membership for org-scoped queries\n\nIf the summarizer-agent produces unexpected format:\n\n- Ensure you passed complete raw data from analyzer\n- Verify the analyzer included issue bodies for closed issues\n\n## Customization\n\nTo customize:\n\n- Edit `agents/github-analyzer-agent.md` for different data sources\n- Edit `agents/summarizer-agent.md` for different summary formats\n- Modify this skill to change orchestration logic\n- Add new integrations in `integrations/` directory\n\n## Example Flow\n\n```text\nUser: /shipmate:eod\n\nSkill: [Detects orgs: example-org]\nSkill: [Asks user: \"Personal, example-org, or all?\"]\nUser: \"example-org\"\n\nSkill: [Invokes github-analyzer-agent]\nAnalyzer: [Runs 5 parallel gh queries, returns JSON]\n\nSkill: [Analyzes data, identifies themes]\nSkill: [Presents themes to user with multiSelect question]\nThemes identified:\n  - Investigated AWS infrastructure\n  - Investigated Railway deployments\n  - Investigated Auth0 setup\n  - Investigated deployment process\n  - Set up secret detection\n  - Added markdown linting\n  - Documentation improvements\n\nUser: [Selects: AWS, Railway, Auth0, Deployment process]\n\nSkill: [Invokes summarizer-agent with data + selected topics]\nSummarizer: [Creates summary with 4 main bullets + Housekeeping group]\n\nSkill: [Presents summary to user]\nSkill: [Checks config for enabled integrations]\nSkill: [If Notion enabled, posts to Notion Daily Log]\nSkill: \"Posted to Notion Daily Log!\" (if applicable)\n```\n\n## Files\n\nThis skill uses:\n\n- `agents/github-analyzer-agent.md` - Data extraction agent\n- `agents/summarizer-agent.md` - Summary writing agent\n- `skills/shipmate:end-of-day-summary/SKILL.md` - This orchestration skill\n- `commands/shipmate:eod.md` - Slash command shortcut\n- `integrations/notion/` - Optional Notion integration\n\n---\n\n**Version:** 1.1.5  \n**Last Updated:** 2025-11-05"
              }
            ]
          }
        ]
      }
    },
    {
      "full_name": "dreamiurg/claude-session-topics",
      "url": "https://github.com/dreamiurg/claude-session-topics",
      "description": "Claude Code plugin for automatic AI-powered session topic generation",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T21:04:15Z",
        "created_at": "2026-01-09T00:23:39Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 484
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 566
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/dependabot.yml",
          "type": "blob",
          "size": 282
        },
        {
          "path": ".github/pull_request_template.md",
          "type": "blob",
          "size": 648
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/ci.yml",
          "type": "blob",
          "size": 1524
        },
        {
          "path": ".github/workflows/release.yml",
          "type": "blob",
          "size": 386
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 355
        },
        {
          "path": ".gitleaks.toml",
          "type": "blob",
          "size": 547
        },
        {
          "path": ".gitmessage",
          "type": "blob",
          "size": 1096
        },
        {
          "path": ".markdownlint-cli2.yaml",
          "type": "blob",
          "size": 776
        },
        {
          "path": ".mise.toml",
          "type": "blob",
          "size": 21
        },
        {
          "path": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1989
        },
        {
          "path": ".release-please-manifest.json",
          "type": "blob",
          "size": 19
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 10791
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1066
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2534
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/regenerate-topic.md",
          "type": "blob",
          "size": 796
        },
        {
          "path": "commands/setup-statusline.md",
          "type": "blob",
          "size": 2277
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 490
        },
        {
          "path": "lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "lib/common.sh",
          "type": "blob",
          "size": 4815
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 484
        },
        {
          "path": "release-please-config.json",
          "type": "blob",
          "size": 659
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/force-topic",
          "type": "blob",
          "size": 5626
        },
        {
          "path": "scripts/session-cleanup",
          "type": "blob",
          "size": 759
        },
        {
          "path": "scripts/topic-display",
          "type": "blob",
          "size": 3377
        },
        {
          "path": "scripts/topic-generator",
          "type": "blob",
          "size": 11735
        },
        {
          "path": "scripts/update-versions.js",
          "type": "blob",
          "size": 1607
        }
      ],
      "marketplace": {
        "name": "claude-session-topics-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "dreamiurg",
          "url": "https://github.com/dreamiurg"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "claude-session-topics",
            "description": "Know what each session is about. Displays topics like '# OAuth debug: fixing schema' in your status line",
            "source": {
              "source": "url",
              "url": "https://github.com/dreamiurg/claude-session-topics.git"
            },
            "category": null,
            "version": "1.7.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add dreamiurg/claude-session-topics",
              "/plugin install claude-session-topics@claude-session-topics-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T21:04:15Z",
              "created_at": "2026-01-09T00:23:39Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}