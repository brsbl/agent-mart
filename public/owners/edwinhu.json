{
  "owner": {
    "id": "edwinhu",
    "display_name": "Edwin Hu",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/174409?v=4",
    "url": "https://github.com/edwinhu",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 2,
      "total_skills": 39,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "edwinhu/workflows",
      "url": "https://github.com/edwinhu/workflows",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T05:31:43Z",
        "created_at": "2026-01-04T02:46:52Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 581
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 216
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/CLAUDE.md",
          "type": "blob",
          "size": 3193
        },
        {
          "path": ".claude/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/commands/dev.md",
          "type": "blob",
          "size": 312
        },
        {
          "path": ".claude/commands/ds.md",
          "type": "blob",
          "size": 309
        },
        {
          "path": ".claude/skill-gate.lock",
          "type": "blob",
          "size": null
        },
        {
          "path": ".copilot",
          "type": "tree",
          "size": null
        },
        {
          "path": ".copilot/COMPATIBILITY.md",
          "type": "blob",
          "size": 7661
        },
        {
          "path": ".copilot/INSTALL.md",
          "type": "blob",
          "size": 12470
        },
        {
          "path": ".copilot/QUICK_START.md",
          "type": "blob",
          "size": 1872
        },
        {
          "path": ".copilot/README.md",
          "type": "blob",
          "size": 5024
        },
        {
          "path": ".copilot/SUMMARY.md",
          "type": "blob",
          "size": 3767
        },
        {
          "path": ".copilot/install.sh",
          "type": "blob",
          "size": 2381
        },
        {
          "path": ".copilot/workflows.instructions.md",
          "type": "blob",
          "size": 7930
        },
        {
          "path": ".copilot/workflows.instructions.md.new",
          "type": "blob",
          "size": 7930
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 342
        },
        {
          "path": ".gitmodules",
          "type": "blob",
          "size": 122
        },
        {
          "path": ".opencode",
          "type": "tree",
          "size": null
        },
        {
          "path": ".opencode/INSTALL.md",
          "type": "blob",
          "size": 5747
        },
        {
          "path": ".opencode/QUICK_START.md",
          "type": "blob",
          "size": 2137
        },
        {
          "path": ".opencode/README.md",
          "type": "blob",
          "size": 8770
        },
        {
          "path": ".opencode/install.sh",
          "type": "blob",
          "size": 6993
        },
        {
          "path": ".opencode/plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".opencode/plugin/package.json",
          "type": "blob",
          "size": 353
        },
        {
          "path": ".opencode/plugin/workflows.js",
          "type": "blob",
          "size": 8269
        },
        {
          "path": ".opencode/quick-install.sh",
          "type": "blob",
          "size": 363
        },
        {
          "path": ".serena",
          "type": "tree",
          "size": null
        },
        {
          "path": ".serena/.gitignore",
          "type": "blob",
          "size": 7
        },
        {
          "path": ".serena/project.yml",
          "type": "blob",
          "size": 5603
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 11141
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/dev.md",
          "type": "blob",
          "size": 312
        },
        {
          "path": "commands/ds.md",
          "type": "blob",
          "size": 313
        },
        {
          "path": "commands/exit.md",
          "type": "blob",
          "size": 358
        },
        {
          "path": "commands/writing.md",
          "type": "blob",
          "size": 295
        },
        {
          "path": "common",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/OH-MY-OPENCODE-PATTERNS.md",
          "type": "blob",
          "size": 15567
        },
        {
          "path": "common/helpers",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/helpers/skill-description-patterns.md",
          "type": "blob",
          "size": 6085
        },
        {
          "path": "common/helpers/tool-restrictions.md",
          "type": "blob",
          "size": 4546
        },
        {
          "path": "common/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/hooks/hooks.json",
          "type": "blob",
          "size": 1021
        },
        {
          "path": "common/hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/hooks/scripts/boulder.py",
          "type": "blob",
          "size": 7910
        },
        {
          "path": "common/hooks/scripts/cleanup-session.py",
          "type": "blob",
          "size": 572
        },
        {
          "path": "common/hooks/scripts/context_collector.py",
          "type": "blob",
          "size": 5464
        },
        {
          "path": "common/hooks/scripts/dispatcher.py",
          "type": "blob",
          "size": 12780
        },
        {
          "path": "common/hooks/scripts/main-chat-sandbox.py",
          "type": "blob",
          "size": 4430
        },
        {
          "path": "common/hooks/scripts/markdown_validators.py",
          "type": "blob",
          "size": 4120
        },
        {
          "path": "common/hooks/scripts/orchestration.py",
          "type": "blob",
          "size": 10948
        },
        {
          "path": "common/hooks/scripts/post-dispatcher.py",
          "type": "blob",
          "size": 2618
        },
        {
          "path": "common/hooks/scripts/rules_matcher.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "common/hooks/scripts/rules_parser.py",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "common/hooks/scripts/sandbox-check.py",
          "type": "blob",
          "size": 2339
        },
        {
          "path": "common/hooks/scripts/session-start.py",
          "type": "blob",
          "size": 15020
        },
        {
          "path": "common/hooks/scripts/session.py",
          "type": "blob",
          "size": 4268
        },
        {
          "path": "common/hooks/scripts/skill-activator.py",
          "type": "blob",
          "size": 2568
        },
        {
          "path": "common/hooks/scripts/transcript.py",
          "type": "blob",
          "size": 5020
        },
        {
          "path": "common/metadata",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/metadata/skill-metadata.py",
          "type": "blob",
          "size": 7395
        },
        {
          "path": "common/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/skills/docx",
          "type": "blob",
          "size": 43
        },
        {
          "path": "common/skills/pdf",
          "type": "blob",
          "size": 42
        },
        {
          "path": "common/skills/pptx",
          "type": "blob",
          "size": 43
        },
        {
          "path": "common/skills/using-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/skills/using-skills/SKILL.md",
          "type": "blob",
          "size": 2056
        },
        {
          "path": "common/skills/xlsx",
          "type": "blob",
          "size": 43
        },
        {
          "path": "common/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "common/templates/delegation-template.md",
          "type": "blob",
          "size": 8246
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/AUDIT-2026-01-09.md",
          "type": "blob",
          "size": 20335
        },
        {
          "path": "docs/investigations",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/investigations/2026-01-09_oh-my-claude-sisyphus-comparison.md",
          "type": "blob",
          "size": 14481
        },
        {
          "path": "docs/investigations/2026-01-09_proposed-changes-explained.md",
          "type": "blob",
          "size": 27212
        },
        {
          "path": "docs/investigations/2026-01-09_qa-on-sisyphus-features.md",
          "type": "blob",
          "size": 26593
        },
        {
          "path": "external",
          "type": "tree",
          "size": null
        },
        {
          "path": "external/anthropic-skills",
          "type": "commit",
          "size": null
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 1727
        },
        {
          "path": "hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/README.md",
          "type": "blob",
          "size": 4965
        },
        {
          "path": "hooks/scripts/common",
          "type": "blob",
          "size": 26
        },
        {
          "path": "hooks/scripts/jupytext",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/jupytext/README.md",
          "type": "blob",
          "size": 4486
        },
        {
          "path": "hooks/scripts/jupytext/jupytext-sync.py",
          "type": "blob",
          "size": 12234
        },
        {
          "path": "hooks/scripts/jupytext/test_python.ipynb",
          "type": "blob",
          "size": 1060
        },
        {
          "path": "hooks/scripts/jupytext/test_python.py",
          "type": "blob",
          "size": 546
        },
        {
          "path": "hooks/scripts/jupytext/test_r.R",
          "type": "blob",
          "size": 327
        },
        {
          "path": "hooks/scripts/jupytext/test_stata.do",
          "type": "blob",
          "size": 217
        },
        {
          "path": "hooks/scripts/marimo",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/marimo/README.md",
          "type": "blob",
          "size": 3107
        },
        {
          "path": "hooks/scripts/marimo/marimo-check.py",
          "type": "blob",
          "size": 5622
        },
        {
          "path": "hooks/scripts/marimo/test_multiple_def.py",
          "type": "blob",
          "size": 278
        },
        {
          "path": "hooks/scripts/marimo/test_notebook.py",
          "type": "blob",
          "size": 387
        },
        {
          "path": "hooks/scripts/markdown",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/markdown/README.md",
          "type": "blob",
          "size": 3382
        },
        {
          "path": "hooks/scripts/markdown/markdown-check.py",
          "type": "blob",
          "size": 2203
        },
        {
          "path": "hooks/scripts/ralph-loop",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/ralph-loop/enforce-loop.py",
          "type": "blob",
          "size": 3338
        },
        {
          "path": "hooks/scripts/rules",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/scripts/rules/rules-injector.py",
          "type": "blob",
          "size": 4572
        },
        {
          "path": "lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "lib/skills-core.js",
          "type": "blob",
          "size": 6461
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/common",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/common/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/common/hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/common/hooks/scripts/boulder.py",
          "type": "blob",
          "size": 4314
        },
        {
          "path": "plugins/workflows/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/hooks/hooks.json",
          "type": "blob",
          "size": 265
        },
        {
          "path": "plugins/workflows/hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/hooks/scripts/common",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflows/hooks/scripts/common/session-start.py",
          "type": "blob",
          "size": 4336
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ai-anti-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ai-anti-patterns/SKILL.md",
          "type": "blob",
          "size": 6368
        },
        {
          "path": "skills/ai-anti-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ai-anti-patterns/references/00-introduction.md",
          "type": "blob",
          "size": 5166
        },
        {
          "path": "skills/ai-anti-patterns/references/01-puffery-and-exaggeration.md",
          "type": "blob",
          "size": 6461
        },
        {
          "path": "skills/ai-anti-patterns/references/02-promotional-language.md",
          "type": "blob",
          "size": 4238
        },
        {
          "path": "skills/ai-anti-patterns/references/03-structural-patterns.md",
          "type": "blob",
          "size": 10898
        },
        {
          "path": "skills/ai-anti-patterns/references/04-stylistic-quirks.md",
          "type": "blob",
          "size": 6923
        },
        {
          "path": "skills/ai-anti-patterns/references/05-formatting-and-typography.md",
          "type": "blob",
          "size": 9061
        },
        {
          "path": "skills/ai-anti-patterns/references/06-communication-patterns.md",
          "type": "blob",
          "size": 11478
        },
        {
          "path": "skills/ai-anti-patterns/references/07-template-artifacts.md",
          "type": "blob",
          "size": 3992
        },
        {
          "path": "skills/ai-anti-patterns/references/08-markup-issues.md",
          "type": "blob",
          "size": 8782
        },
        {
          "path": "skills/ai-anti-patterns/references/09-chatgpt-specific-artifacts.md",
          "type": "blob",
          "size": 5991
        },
        {
          "path": "skills/ai-anti-patterns/references/10-citation-problems.md",
          "type": "blob",
          "size": 10161
        },
        {
          "path": "skills/ai-anti-patterns/references/11-meta-indicators.md",
          "type": "blob",
          "size": 11890
        },
        {
          "path": "skills/ai-anti-patterns/references/_index.md",
          "type": "blob",
          "size": 4306
        },
        {
          "path": "skills/dev-brainstorm",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-brainstorm/SKILL.md",
          "type": "blob",
          "size": 6678
        },
        {
          "path": "skills/dev-clarify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-clarify/SKILL.md",
          "type": "blob",
          "size": 6513
        },
        {
          "path": "skills/dev-debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-debug/SKILL.md",
          "type": "blob",
          "size": 12156
        },
        {
          "path": "skills/dev-delegate",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-delegate/SKILL.md",
          "type": "blob",
          "size": 10427
        },
        {
          "path": "skills/dev-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-design/SKILL.md",
          "type": "blob",
          "size": 13023
        },
        {
          "path": "skills/dev-explore",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-explore/SKILL.md",
          "type": "blob",
          "size": 11734
        },
        {
          "path": "skills/dev-explore/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-explore/references/ast-grep-patterns.md",
          "type": "blob",
          "size": 1377
        },
        {
          "path": "skills/dev-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-implement/SKILL.md",
          "type": "blob",
          "size": 9924
        },
        {
          "path": "skills/dev-ralph-loop",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-ralph-loop/SKILL.md",
          "type": "blob",
          "size": 6786
        },
        {
          "path": "skills/dev-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-review/SKILL.md",
          "type": "blob",
          "size": 8841
        },
        {
          "path": "skills/dev-tdd",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-tdd/SKILL.md",
          "type": "blob",
          "size": 10856
        },
        {
          "path": "skills/dev-tdd/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-tdd/references/execution-gates.md",
          "type": "blob",
          "size": 12366
        },
        {
          "path": "skills/dev-tdd/references/logging-requirements.md",
          "type": "blob",
          "size": 2654
        },
        {
          "path": "skills/dev-test-chrome",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-test-chrome/SKILL.md",
          "type": "blob",
          "size": 14557
        },
        {
          "path": "skills/dev-test-hammerspoon",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-test-hammerspoon/SKILL.md",
          "type": "blob",
          "size": 11769
        },
        {
          "path": "skills/dev-test-linux",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-test-linux/SKILL.md",
          "type": "blob",
          "size": 15357
        },
        {
          "path": "skills/dev-test-playwright",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-test-playwright/SKILL.md",
          "type": "blob",
          "size": 13330
        },
        {
          "path": "skills/dev-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-test/SKILL.md",
          "type": "blob",
          "size": 16272
        },
        {
          "path": "skills/dev-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-tools/SKILL.md",
          "type": "blob",
          "size": 2177
        },
        {
          "path": "skills/dev-verify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-verify/SKILL.md",
          "type": "blob",
          "size": 11622
        },
        {
          "path": "skills/dev-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dev-worktree/SKILL.md",
          "type": "blob",
          "size": 4243
        },
        {
          "path": "skills/ds-brainstorm",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-brainstorm/SKILL.md",
          "type": "blob",
          "size": 6904
        },
        {
          "path": "skills/ds-delegate",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-delegate/SKILL.md",
          "type": "blob",
          "size": 9192
        },
        {
          "path": "skills/ds-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-implement/SKILL.md",
          "type": "blob",
          "size": 9557
        },
        {
          "path": "skills/ds-implement/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-implement/references/verification-patterns.md",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "skills/ds-plan",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-plan/SKILL.md",
          "type": "blob",
          "size": 9118
        },
        {
          "path": "skills/ds-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-review/SKILL.md",
          "type": "blob",
          "size": 7850
        },
        {
          "path": "skills/ds-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-tools/SKILL.md",
          "type": "blob",
          "size": 3004
        },
        {
          "path": "skills/ds-verify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ds-verify/SKILL.md",
          "type": "blob",
          "size": 7962
        },
        {
          "path": "skills/exit",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/exit/SKILL.md",
          "type": "blob",
          "size": 1264
        },
        {
          "path": "skills/gemini-batch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/gemini-batch/SKILL.md",
          "type": "blob",
          "size": 11983
        },
        {
          "path": "skills/gemini-batch/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/gemini-batch/examples/batch_processor.py",
          "type": "blob",
          "size": 14372
        },
        {
          "path": "skills/gemini-batch/examples/icon_batch_vision.py",
          "type": "blob",
          "size": 11750
        },
        {
          "path": "skills/gemini-batch/examples/pipeline_template.py",
          "type": "blob",
          "size": 2218
        },
        {
          "path": "skills/gemini-batch/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/gemini-batch/references/best-practices.md",
          "type": "blob",
          "size": 7126
        },
        {
          "path": "skills/gemini-batch/references/cli-reference.md",
          "type": "blob",
          "size": 2080
        },
        {
          "path": "skills/gemini-batch/references/gcs-setup.md",
          "type": "blob",
          "size": 9105
        },
        {
          "path": "skills/gemini-batch/references/gotchas.md",
          "type": "blob",
          "size": 19574
        },
        {
          "path": "skills/gemini-batch/references/troubleshooting.md",
          "type": "blob",
          "size": 6852
        },
        {
          "path": "skills/gemini-batch/references/vertex-ai.md",
          "type": "blob",
          "size": 2152
        },
        {
          "path": "skills/gemini-batch/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/gemini-batch/scripts/test_single.py",
          "type": "blob",
          "size": 2439
        },
        {
          "path": "skills/gemini-batch/scripts/validate_jsonl.py",
          "type": "blob",
          "size": 3704
        },
        {
          "path": "skills/jupytext",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/jupytext/SKILL.md",
          "type": "blob",
          "size": 10671
        },
        {
          "path": "skills/jupytext/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/jupytext/examples/cross_kernel_pipeline.py",
          "type": "blob",
          "size": 3914
        },
        {
          "path": "skills/jupytext/examples/python_analysis.py",
          "type": "blob",
          "size": 1984
        },
        {
          "path": "skills/jupytext/examples/r_analysis.R",
          "type": "blob",
          "size": 1916
        },
        {
          "path": "skills/jupytext/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/jupytext/references/data-sharing.md",
          "type": "blob",
          "size": 10370
        },
        {
          "path": "skills/jupytext/references/formats.md",
          "type": "blob",
          "size": 6561
        },
        {
          "path": "skills/jupytext/references/kernels.md",
          "type": "blob",
          "size": 6746
        },
        {
          "path": "skills/jupytext/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/jupytext/scripts/init_project.sh",
          "type": "blob",
          "size": 3010
        },
        {
          "path": "skills/jupytext/scripts/sync_all.sh",
          "type": "blob",
          "size": 1438
        },
        {
          "path": "skills/look-at",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/look-at/README.md",
          "type": "blob",
          "size": 5690
        },
        {
          "path": "skills/look-at/SKILL.md",
          "type": "blob",
          "size": 8241
        },
        {
          "path": "skills/look-at/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/look-at/examples/analyze_pdf.sh",
          "type": "blob",
          "size": 848
        },
        {
          "path": "skills/look-at/examples/describe_image.sh",
          "type": "blob",
          "size": 820
        },
        {
          "path": "skills/look-at/examples/extract_table.sh",
          "type": "blob",
          "size": 874
        },
        {
          "path": "skills/look-at/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/look-at/references/api-details.md",
          "type": "blob",
          "size": 4555
        },
        {
          "path": "skills/look-at/references/use-cases.md",
          "type": "blob",
          "size": 8800
        },
        {
          "path": "skills/look-at/requirements.txt",
          "type": "blob",
          "size": 121
        },
        {
          "path": "skills/look-at/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/look-at/scripts/look_at.py",
          "type": "blob",
          "size": 6761
        },
        {
          "path": "skills/lseg-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lseg-data/SKILL.md",
          "type": "blob",
          "size": 7973
        },
        {
          "path": "skills/lseg-data/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lseg-data/examples/fundamentals_query.py",
          "type": "blob",
          "size": 1985
        },
        {
          "path": "skills/lseg-data/examples/historical_pricing.ipynb",
          "type": "blob",
          "size": 246878
        },
        {
          "path": "skills/lseg-data/examples/stock_screener.ipynb",
          "type": "blob",
          "size": 8331
        },
        {
          "path": "skills/lseg-data/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lseg-data/references/esg.md",
          "type": "blob",
          "size": 9853
        },
        {
          "path": "skills/lseg-data/references/fundamentals.md",
          "type": "blob",
          "size": 8031
        },
        {
          "path": "skills/lseg-data/references/pricing.md",
          "type": "blob",
          "size": 3791
        },
        {
          "path": "skills/lseg-data/references/screening.md",
          "type": "blob",
          "size": 4823
        },
        {
          "path": "skills/lseg-data/references/symbology.md",
          "type": "blob",
          "size": 8952
        },
        {
          "path": "skills/lseg-data/references/troubleshooting.md",
          "type": "blob",
          "size": 9900
        },
        {
          "path": "skills/lseg-data/references/wrds-comparison.md",
          "type": "blob",
          "size": 7536
        },
        {
          "path": "skills/lseg-data/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lseg-data/scripts/test_connection.py",
          "type": "blob",
          "size": 1947
        },
        {
          "path": "skills/marimo",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/marimo/SKILL.md",
          "type": "blob",
          "size": 9516
        },
        {
          "path": "skills/marimo/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/marimo/examples/basic_notebook.py",
          "type": "blob",
          "size": 1303
        },
        {
          "path": "skills/marimo/examples/data_analysis.py",
          "type": "blob",
          "size": 2532
        },
        {
          "path": "skills/marimo/examples/interactive_widgets.py",
          "type": "blob",
          "size": 2884
        },
        {
          "path": "skills/marimo/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/marimo/references/debugging.md",
          "type": "blob",
          "size": 5224
        },
        {
          "path": "skills/marimo/references/reactivity.md",
          "type": "blob",
          "size": 3071
        },
        {
          "path": "skills/marimo/references/sql.md",
          "type": "blob",
          "size": 7845
        },
        {
          "path": "skills/marimo/references/widgets.md",
          "type": "blob",
          "size": 7040
        },
        {
          "path": "skills/marimo/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/marimo/scripts/check_notebook.sh",
          "type": "blob",
          "size": 1587
        },
        {
          "path": "skills/marimo/scripts/get_cell_map.py",
          "type": "blob",
          "size": 3239
        },
        {
          "path": "skills/notebook-debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/notebook-debug/SKILL.md",
          "type": "blob",
          "size": 7596
        },
        {
          "path": "skills/using-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/using-skills/SKILL.md",
          "type": "blob",
          "size": 11579
        },
        {
          "path": "skills/using-skills/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/using-skills/references/agent-harnessing.md",
          "type": "blob",
          "size": 3946
        },
        {
          "path": "skills/wrds",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/wrds/SKILL.md",
          "type": "blob",
          "size": 7883
        },
        {
          "path": "skills/wrds/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/wrds/examples/form4_disposals.py",
          "type": "blob",
          "size": 20646
        },
        {
          "path": "skills/wrds/examples/wrds_connector.py",
          "type": "blob",
          "size": 24628
        },
        {
          "path": "skills/wrds/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/wrds/references/compustat.md",
          "type": "blob",
          "size": 10324
        },
        {
          "path": "skills/wrds/references/connection.md",
          "type": "blob",
          "size": 10831
        },
        {
          "path": "skills/wrds/references/crsp.md",
          "type": "blob",
          "size": 4575
        },
        {
          "path": "skills/wrds/references/edgar.md",
          "type": "blob",
          "size": 16014
        },
        {
          "path": "skills/wrds/references/insider-form4.md",
          "type": "blob",
          "size": 4986
        },
        {
          "path": "skills/wrds/references/iss-compensation.md",
          "type": "blob",
          "size": 3514
        },
        {
          "path": "skills/wrds/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/wrds/scripts/test_connection.py",
          "type": "blob",
          "size": 2119
        },
        {
          "path": "skills/writing-brainstorm",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-brainstorm/SKILL.md",
          "type": "blob",
          "size": 9931
        },
        {
          "path": "skills/writing-econ",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-econ/SKILL.md",
          "type": "blob",
          "size": 7208
        },
        {
          "path": "skills/writing-econ/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-econ/references/economical-writing-full.md",
          "type": "blob",
          "size": 5635
        },
        {
          "path": "skills/writing-legal",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-legal/SKILL.md",
          "type": "blob",
          "size": 9449
        },
        {
          "path": "skills/writing-legal/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing-legal/references/volokh-distilled.md",
          "type": "blob",
          "size": 18340
        },
        {
          "path": "skills/writing",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing/SKILL.md",
          "type": "blob",
          "size": 5070
        },
        {
          "path": "skills/writing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/writing/references/elements-of-style.md",
          "type": "blob",
          "size": 71183
        }
      ],
      "marketplace": {
        "name": "edwinhu-plugins",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Edwin Hu"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "workflows",
            "description": "Unified development, data science, and writing workflows with sandbox mode, TDD enforcement, and output-first verification",
            "source": "./",
            "category": "development",
            "version": "2.11.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add edwinhu/workflows",
              "/plugin install workflows@edwinhu-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T05:31:43Z",
              "created_at": "2026-01-04T02:46:52Z",
              "license": null
            },
            "commands": [
              {
                "name": "/dev",
                "description": "Start the 7-phase feature development workflow with TDD enforcement",
                "path": ".claude/commands/dev.md",
                "frontmatter": {
                  "description": "Start the 7-phase feature development workflow with TDD enforcement",
                  "allowed-tools": "Skill"
                },
                "content": "Start the dev workflow by invoking Phase 1 (brainstorming):\n\nSkill(skill=\"workflows:dev-brainstorm\")\n\nThe brainstorm phase will handle workflow activation and guide you through requirements gathering."
              },
              {
                "name": "/ds",
                "description": "Start the 5-phase data analysis workflow with output-first verification",
                "path": ".claude/commands/ds.md",
                "frontmatter": {
                  "description": "Start the 5-phase data analysis workflow with output-first verification",
                  "allowed-tools": "Skill"
                },
                "content": "Start the ds workflow by invoking Phase 1 (brainstorming):\n\nSkill(skill=\"workflows:ds-brainstorm\")\n\nThe brainstorm phase will handle workflow activation and guide you through analysis planning."
              }
            ],
            "skills": [
              {
                "name": "ai-anti-patterns",
                "description": "This skill should be used when reviewing AI-generated text, checking for AI writing patterns, detecting undisclosed AI content, or before finalizing any written content. Covers 12 categories of AI writing indicators from Wikipedia's comprehensive guide.",
                "path": "skills/ai-anti-patterns/SKILL.md",
                "frontmatter": {
                  "name": "ai-anti-patterns",
                  "description": "This skill should be used when reviewing AI-generated text, checking for AI writing patterns, detecting undisclosed AI content, or before finalizing any written content. Covers 12 categories of AI writing indicators from Wikipedia's comprehensive guide."
                },
                "content": "# AI Writing Anti-Patterns\n\nField guide for detecting and revising AI-generated content indicators based on Wikipedia's \"Signs of AI writing\" guide.\n\n## When to Use\n\nInvoke this skill:\n- Before finalizing ANY AI-assisted writing\n- When reviewing text for AI writing indicators\n- When editing content to sound more natural\n- After completing writing tasks (automatic via hooks)\n\n## The Iron Law\n\n**Check every piece of AI-assisted writing against these patterns before submission.**\n\nThis is not optional. AI writing patterns are detectable and undermine credibility.\n\n## Quick Screening Order\n\nStart with the most objective indicators:\n\n| Priority | Section | What to Check |\n|----------|---------|---------------|\n| 1 | ChatGPT Artifacts | `turn0search0`, `oaicite`, `contentReference` |\n| 2 | Citation Problems | Hallucinated DOIs, dead links, non-existent sources |\n| 3 | Prompt Refusals | \"As an AI language model...\", \"I hope this helps\" |\n| 4 | Puffery | \"stands as\", \"plays a vital role\", \"rich tapestry\" |\n| 5 | Structure | Section summaries, \"Despite challenges\", rule of three |\n\n## Critical Patterns to Avoid\n\n### CRITICAL Severity (Immediate Revision Required)\n\nThese patterns are unambiguous AI artifacts:\n\n**ChatGPT-Specific Artifacts:**\n- `turn0search0`, `turn1search2` (internal search references)\n- `oaicite:X` (citation placeholders)\n- `contentReference[oaicite:X]` (unresolved references)\n- JSON attribution blocks in output\n\n**Prompt Refusals:**\n- \"As an AI language model...\"\n- \"I cannot provide...\"\n- \"I hope this helps!\"\n- \"I hope this email finds you well\"\n\n### HIGH Severity (Strong Revision Recommended)\n\n**Puffery and Exaggeration:**\n- \"stands as\" (a testament/example/beacon)\n- \"plays a vital/crucial/pivotal role\"\n- \"rich tapestry of\"\n- \"nestled in/among\"\n- \"it's important to note that\"\n- \"delves into\"\n- \"the landscape of\"\n\n**Promotional Language:**\n- \"groundbreaking\", \"transformative\", \"revolutionary\"\n- \"unparalleled\", \"unprecedented\"\n- \"cutting-edge\", \"state-of-the-art\"\n\n### MEDIUM Severity (Review and Consider)\n\n**Structural Patterns:**\n- Section summaries that repeat the heading\n- \"Despite [challenge], [positive outcome]\" formula\n- Negative parallelisms: \"However... Nevertheless...\"\n- Rule of three: exactly three examples every time\n- Weasel wording: \"some experts say\", \"it is believed\"\n\n**Stylistic Quirks:**\n- Elegant variation (synonym cycling to avoid repetition)\n- False ranges (\"from X to Y\" without real data)\n- Title Case In All Headings\n- Em dash overuse (—)\n- Excessive boldface for emphasis\n\n## How to Revise\n\n### For Puffery\n\n| AI Pattern | Human Alternative |\n|------------|-------------------|\n| \"stands as a testament to\" | \"shows\" or \"demonstrates\" |\n| \"plays a vital role in\" | \"affects\" or just state the effect |\n| \"rich tapestry of\" | describe specifically what it contains |\n| \"nestled in the heart of\" | \"in\" or \"located in\" |\n| \"delves into\" | \"examines\" or \"covers\" |\n\n### For Structure\n\n| AI Pattern | Human Alternative |\n|------------|-------------------|\n| Section summary of heading | Start with substance, not meta-commentary |\n| \"Despite challenges...\" | State the reality directly without formula |\n| Exactly three examples | Use the number that fits: 2, 4, 5, or just 1 |\n| \"It's important to note\" | Just state the important thing |\n\n### For Promotional Language\n\n| AI Pattern | Human Alternative |\n|------------|-------------------|\n| \"groundbreaking\" | describe what it actually does |\n| \"revolutionary\" | compare to what came before |\n| \"cutting-edge\" | specify the technology |\n| \"transformative\" | show the transformation with evidence |\n\n## Reference Files\n\nFor detailed patterns and extensive examples, consult:\n\n| File | Contents |\n|------|----------|\n| `references/_index.md` | Overview and quick screening guide |\n| `references/01-puffery-and-exaggeration.md` | \"Stands as\", superficial analyses |\n| `references/02-promotional-language.md` | \"Rich tapestry\", disclaimers |\n| `references/03-structural-patterns.md` | Section summaries, negative parallelisms |\n| `references/04-stylistic-quirks.md` | Elegant variation, false ranges |\n| `references/05-formatting-and-typography.md` | Boldface, em dashes, emojis |\n| `references/06-communication-patterns.md` | Subject lines, \"I hope this helps\" |\n| `references/07-template-artifacts.md` | Mad Libs patterns, placeholders |\n| `references/08-markup-issues.md` | Markdown vs wikitext confusion |\n| `references/09-chatgpt-specific-artifacts.md` | turn0search, oaicite |\n| `references/10-citation-problems.md` | Hallucinated DOIs, dead links |\n| `references/11-meta-indicators.md` | Abrupt cutoffs, style discrepancies |\n\n## Automatic Detection\n\nThis plugin includes PostToolUse hooks that automatically scan Write/Edit output for anti-patterns. When patterns are detected:\n\n1. Hook emits a warning with specific patterns found\n2. Claude immediately revises the content\n3. Revision removes or replaces flagged patterns\n\nThe hook checks for all CRITICAL and HIGH severity patterns automatically.\n\n## Red Flags - Stop If You Think\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"This sounds professional\" | AI puffery sounds generic, not professional | Use concrete, specific language |\n| \"I'll add emphasis\" | \"Very important\" and bold are AI tells | Let content speak for itself |\n| \"Let me summarize the section\" | Section summaries are formulaic | Start with substance |\n| \"Three examples is a good number\" | Rule of three is an AI pattern | Use the right number for the content |\n\n## Key Principles\n\nFrom Wikipedia's guide:\n\n1. **These are signs, not proof** - Multiple indicators strengthen the case\n2. **Context matters** - Some patterns appear in human writing too\n3. **Focus on deeper issues** - Surface defects point to synthesis and quality problems\n4. **Don't rely on detection tools** - Human judgment required\n\n## Related Skills\n\n- `/writing` - Core writing principles from Elements of Style\n- `/writing-legal` - Legal writing (Phase 2)\n- `/writing-econ` - Economics writing (Phase 2)"
              },
              {
                "name": "dev-brainstorm",
                "description": "REQUIRED Phase 1 of /dev workflow. Uses Socratic questioning to understand requirements before exploration.",
                "path": "skills/dev-brainstorm/SKILL.md",
                "frontmatter": {
                  "name": "dev-brainstorm",
                  "description": "REQUIRED Phase 1 of /dev workflow. Uses Socratic questioning to understand requirements before exploration."
                },
                "content": "**Announce:** \"I'm using dev-brainstorm (Phase 1) to gather requirements.\"\n\n## First: Activate Workflow\n\nBefore anything else, activate the dev workflow. This enables workflow-specific hooks (sandbox enforcement, TDD checks, etc.).\n\n**Activate dev workflow by creating timestamp file:**\n\n```bash\nmkdir -p /tmp/claude-workflow-$PPID && touch /tmp/claude-workflow-$PPID/dev_mode && echo \"✓ Dev workflow activated\"\n```\n\n## Contents\n\n- [The Iron Law of Brainstorming](#the-iron-law-of-brainstorming)\n- [What Brainstorm Does](#what-brainstorm-does)\n- [Process](#process)\n- [Red Flags - STOP If You're About To](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Brainstorming (Questions Only)\n\nRefine vague ideas into clear requirements through Socratic questioning.\n**NO exploration, NO approaches** - just questions and requirements.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Brainstorming\n\n**ASK QUESTIONS BEFORE ANYTHING ELSE. This is not negotiable.**\n\nBefore exploring codebase, before proposing approaches, follow these requirements:\n1. Ask clarifying questions using AskUserQuestion\n2. Understand what the user actually wants\n3. Define success criteria\n\nApproaches come later (in /dev-design) after exploring the codebase.\n\n**If YOU catch YOURSELF about to explore the codebase before asking questions, STOP.**\n</EXTREMELY-IMPORTANT>\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"The requirements seem obvious\" | Your assumptions are often wrong | ASK questions to confirm |\n| \"Let me just look at the code to understand\" | Code tells HOW, not WHY | ASK what user wants first |\n| \"I can gather requirements while exploring\" | You'll waste time on distraction and miss critical questions | QUESTIONS FIRST, exploration later |\n| \"User already explained everything\" | You'll find users always leave out critical details | ASK clarifying questions anyway |\n| \"I'll ask if I need more info\" | You cannot know unknown unknowns without asking | ASK questions NOW, not later |\n| \"Quick peek at the code won't hurt\" | You'll let codebases bias your thinking | STAY IGNORANT until requirements clear |\n| \"I can propose approaches based on description\" | You need exploration to precede design | WAIT for dev-design phase |\n\n### Honesty Framing\n\n**Guessing user requirements is LYING about what they want.**\n\nAsking questions is cheap. Building the wrong thing is expensive. Every minute spent clarifying requirements saves hours of wasted implementation.\n\n### No Pause After Completion\n\nAfter writing `.claude/SPEC.md` and completing brainstorm, immediately invoke the next phase:\n\n**Invoke the explore phase:**\n\n```bash\nSkill(skill=\"workflows:dev-explore\")\n```\n\nDO NOT:\n- Summarize what was learned\n- Ask \"should I proceed?\"\n- Wait for user confirmation\n- Write status updates\n\nThe workflow phases are SEQUENTIAL. Complete brainstorm → immediately start explore.\n\n## What Brainstorm Does\n\n| DO | DON'T |\n|----|-------|\n| Ask clarifying questions | Explore codebase |\n| Understand requirements | Spawn explore agents |\n| Define success criteria | Look at existing code |\n| Write draft SPEC.md | Propose approaches (that's design) |\n| Identify unknowns | Create implementation tasks |\n\n**Brainstorm answers: WHAT do we need and WHY**\n**Explore answers: WHERE is the code** (next phase)\n**Design answers: HOW to build it** (after exploration)\n\n## Process\n\n### 1. Ask Questions First\n\nUse `AskUserQuestion` immediately with these principles:\n- **One question at a time** - never batch\n- **Multiple-choice preferred** - easier to answer\n- Focus on: purpose, constraints, success criteria\n\nExample questions to ask:\n- \"What problem does this solve?\"\n- \"Who will use this feature?\"\n- \"What's the most important requirement?\"\n- \"Any constraints (performance, compatibility)?\"\n\n### 2. Define Success Criteria\n\nAfter understanding requirements, define measurable success criteria:\n- Turn requirements into measurable criteria\n- Use checkboxes for clear pass/fail\n- Confirm criteria with user\n\n### 3. Write Draft SPEC.md\n\nWrite the initial spec to `.claude/SPEC.md`:\n\n```markdown\n# Spec: [Feature Name]\n\n> **For Claude:** After writing this spec, use `Skill(skill=\"workflows:dev-explore\")` for Phase 2.\n\n## Problem\n[What problem this solves]\n\n## Requirements\n- [Requirement 1]\n- [Requirement 2]\n\n## Success Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n## Constraints\n- [Any limitations or boundaries]\n\n## Automated Testing (MANDATORY)\n\n> **For Claude:** Use `Skill(skill=\"workflows:dev-test\")` for automation options.\n\n- **Framework:** [pytest / playwright / jest / etc.]\n- **Command:** [e.g., `pytest tests/ -v`]\n- **Core functionality to verify:** [what MUST be tested automatically]\n\n### What Counts as a Real Automated Test\n\n| ✅ REAL TEST (execute + verify) | ❌ NOT A TEST (never acceptable) |\n|---------------------------------|----------------------------------|\n| pytest calls function, checks return value | grep/ast-grep finds function exists |\n| Playwright clicks button, verifies result | Check logs say \"success\" |\n| ydotool simulates user, screenshot verifies | Read source code structure |\n| API call returns expected response | \"Code looks correct\" |\n| CLI invocation produces expected output | Structural pattern matching |\n\n**THE TEST MUST EXECUTE THE CODE AND VERIFY RUNTIME BEHAVIOR.**\n\nGrepping is not testing. Log checking is not testing. Code review is not testing.\n\n## Open Questions\n- [Questions to resolve during exploration]\n```\n\n**Note:** No \"Chosen Approach\" yet - that comes after exploration and design phases.\n\n## Red Flags - STOP If You're About To:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Spawn explore agent | You're exploring before understanding | Ask questions first |\n| Read source files | You're looking at code before requirements are clear | Ask what user wants |\n| Propose approaches | You're jumping ahead - you need exploration first | Save for /dev-design |\n| Create task list | You're planning before you understand the requirements | Finish brainstorm first |\n\n## Output\n\nBrainstorm complete when:\n- Problem is clearly understood\n- Requirements defined\n- Success criteria defined\n- `.claude/SPEC.md` written (draft)\n- Open questions identified for exploration\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After completing brainstorm, immediately invoke the explore phase:\n\n**Start explore phase - Phase 2:**\n\n```bash\nSkill(skill=\"workflows:dev-explore\")\n```"
              },
              {
                "name": "dev-clarify",
                "description": "REQUIRED Phase 3 of /dev workflow. Asks targeted questions based on codebase exploration findings.",
                "path": "skills/dev-clarify/SKILL.md",
                "frontmatter": {
                  "name": "dev-clarify",
                  "description": "REQUIRED Phase 3 of /dev workflow. Asks targeted questions based on codebase exploration findings."
                },
                "content": "**Announce:** \"I'm using dev-clarify (Phase 3) to resolve ambiguities.\"\n\n## Contents\n\n- [The Iron Law of Clarification](#the-iron-law-of-clarification)\n- [What Clarify Does](#what-clarify-does)\n- [Process](#process)\n- [Question Categories](#question-categories)\n- [Red Flags](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Post-Exploration Clarification\n\nAsk targeted questions based on what exploration revealed.\n**Prerequisite:** Exploration phase complete, key files read.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Clarification\n\n**ASK BEFORE DESIGNING. This is not negotiable.**\n\nAfter exploration, you now know:\n- What exists in the codebase\n- What patterns are used\n- What integrations are needed\n\nUse this knowledge to ask **informed questions** about:\n- Edge cases the code will need to handle\n- Integration points with existing systems\n- Behavior in ambiguous scenarios\n\n**If you catch yourself about to design without resolving ambiguities, STOP.**\n</EXTREMELY-IMPORTANT>\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"The pattern choice is obvious\" | Multiple patterns exist for a reason | ASK which to follow |\n| \"I can decide edge cases myself\" | Your assumptions don't match user expectations | ASK for clarification |\n| \"This is a small detail\" | Small details cause big bugs | ASK about edge cases now |\n| \"I'll handle integration points during implementation\" | Wrong integration breaks everything | CLARIFY integration NOW |\n| \"The exploration gave me enough info\" | Code tells you HOW, not WHAT SHOULD happen | ASK for requirements, not just patterns |\n| \"I can make a reasonable assumption\" | Reasonable != correct | ASK, don't assume |\n| \"Asking too many questions annoys users\" | Building wrong thing annoys users more | ASK clarifying questions |\n\n### Honesty Framing\n\n**Assuming user requirements without asking is LYING about what they want.**\n\nYou explored the codebase and found patterns. But patterns show HOW things work, not WHAT the user wants. Clarification bridges this gap.\n\nAsking costs minutes. Wrong assumptions cost hours of rework.\n\n### No Pause After Completion\n\nAfter updating `.claude/SPEC.md` with all clarified requirements, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:dev-design\")\n```\n\nDO NOT:\n- Summarize what you learned\n- Ask \"should I proceed to design?\"\n- Wait for user confirmation\n- Write status updates\n\nThe workflow phases are SEQUENTIAL. Complete clarify → immediately start design.\n\n## What Clarify Does\n\n| DO | DON'T |\n|----|-------|\n| Ask questions based on exploration | Ask vague/generic questions |\n| Reference specific code patterns found | Repeat questions from brainstorm |\n| Clarify integration points | Propose approaches (that's design) |\n| Resolve edge cases | Make assumptions |\n| Update SPEC.md with answers | Skip to implementation |\n\n**Clarify answers: WHAT EXACTLY should happen in specific scenarios**\n**Design answers: HOW to build it** (next phase)\n\n## Process\n\n### 1. Review Exploration Findings\n\nBefore asking questions, review:\n- Key files you read\n- Patterns discovered\n- Architecture insights\n- Integration points identified\n\n### 2. Identify Ambiguities\n\nCommon areas needing clarification after exploration:\n\n**Integration Points:**\n- \"The existing auth system uses JWT. Should the new feature use the same token or create a new session type?\"\n\n**Edge Cases:**\n- \"What happens if [condition discovered in code]?\"\n\n**Scope Boundaries:**\n- \"The existing feature handles X. Should the new feature also handle X or is that out of scope?\"\n\n**Behavior Choices:**\n- \"I found two patterns in the codebase for this. Pattern A in `file.ts:23` and Pattern B in `other.ts:45`. Which should we follow?\"\n\n### 3. Ask Questions with AskUserQuestion\n\nPresent questions with context from exploration:\n\n```\nAskUserQuestion(questions=[{\n  \"question\": \"The auth middleware at src/middleware/auth.ts:78 validates tokens synchronously. The new endpoint needs user data. Should we: validate synchronously (faster, simpler) or fetch fresh user data (slower, always current)?\",\n  \"header\": \"Auth pattern\",\n  \"options\": [\n    {\"label\": \"Sync validation (Recommended)\", \"description\": \"Faster, uses cached token claims, matches existing patterns\"},\n    {\"label\": \"Fresh fetch\", \"description\": \"Slower, always current, needed if user data changes frequently\"}\n  ],\n  \"multiSelect\": false\n}])\n```\n\n**Key principles:**\n- Reference specific files/lines from exploration\n- Lead with recommendation based on codebase patterns\n- Explain trade-offs clearly\n- One question at a time for complex topics\n\n### 4. Update SPEC.md\n\nAfter each answer, update `.claude/SPEC.md`:\n- Add clarified requirements\n- Document decisions made\n- Note trade-offs accepted\n\n```markdown\n## Clarified Requirements\n\n### Auth Pattern\n- Decision: Sync validation\n- Rationale: Matches existing patterns, user data changes infrequently\n- Reference: src/middleware/auth.ts:78\n\n### Edge Case: Expired Token\n- Decision: Return 401, let client refresh\n- Rationale: Consistent with other endpoints\n```\n\n## Question Categories\n\n### Must Ask (based on exploration)\n- Integration points with existing systems\n- Patterns to follow (when multiple exist)\n- Edge cases revealed by code reading\n\n### Optional (if unclear)\n- Performance requirements\n- Error handling preferences\n- Backward compatibility needs\n\n### Don't Ask (already decided)\n- What the feature does (that's brainstorm)\n- Whether to build it (user already decided)\n- Architecture approach (that's design)\n\n## Red Flags - STOP If You're About To:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Ask without exploration context | Questions will be generic | Reference specific code findings |\n| Propose architecture | Too early, still clarifying | Ask questions, save design for next phase |\n| Make assumptions | Leads to rework | Ask and get explicit answer |\n| Skip to design | Ambiguities cause bugs | Resolve all questions first |\n\n## Output\n\nClarification complete when:\n- All integration points clarified\n- Edge cases resolved\n- Pattern choices made\n- `.claude/SPEC.md` updated with final requirements\n- No remaining ambiguities\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After completing clarification, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:dev-design\")\n```"
              },
              {
                "name": "dev-debug",
                "description": "This skill should be used when the user asks to 'debug', 'fix bug', 'investigate error', 'why is it broken', 'trace root cause', 'find the bug', or needs systematic bug investigation and fixing with verification-driven methodology using ralph loops.",
                "path": "skills/dev-debug/SKILL.md",
                "frontmatter": {
                  "name": "dev-debug",
                  "version": 1,
                  "description": "This skill should be used when the user asks to 'debug', 'fix bug', 'investigate error', 'why is it broken', 'trace root cause', 'find the bug', or needs systematic bug investigation and fixing with verification-driven methodology using ralph loops."
                },
                "content": "**Announce:** \"I'm using dev-debug for systematic bug investigation.\"\n\n<EXTREMELY-IMPORTANT>\n## GUI Application Debugging Gate\n\nWhen debugging GUI applications, you MUST complete the execution gates from dev-tdd during REPRODUCE and VERIFY phases:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN: Test reproduction or verification\n```\n\n**Critical phases requiring gates:**\n\n**REPRODUCE phase:**\n- Build → Launch with logs → Wait → Check running → **READ LOGS** → Verify bug appears in logs\n- Only after reading logs can you claim \"bug reproduced\"\n\n**VERIFY phase:**\n- Build → Launch with logs → Wait → Check running → **READ LOGS** → Verify bug is gone from logs\n- Only after reading logs can you claim \"bug fixed\"\n\n**You loaded dev-tdd via ralph-loop. Follow the gates for GUI debugging.**\n</EXTREMELY-IMPORTANT>\n\n## Where This Fits\n\n```\nMain Chat (you)                    Task Agent\n─────────────────────────────────────────────────────\ndev-debug (this skill)\n  → ralph loop (one per bug)\n    → dev-delegate (spawn agents)\n      → Task agent ──────────────→ investigates\n                                   writes regression test\n                                   implements fix\n```\n\n**Main chat orchestrates.** Task agents investigate and fix.\n\n## Contents\n\n- [The Iron Law of Debugging](#the-iron-law-of-debugging)\n- [The Iron Law of Delegation](#the-iron-law-of-delegation)\n- [The Process](#the-process)\n- [The Four Phases](#the-four-phases)\n- [If Max Iterations Reached](#if-max-iterations-reached)\n\n# Systematic Debugging\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Debugging\n\n**NO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST. This is not negotiable.**\n\nBefore writing ANY fix, you MUST:\n1. Reproduce the bug (with a test)\n2. Trace the data flow\n3. Form a specific hypothesis\n4. Test that hypothesis\n5. Only THEN write a fix (with a regression test first!)\n\n**If you catch yourself about to write a fix without investigation, STOP.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Delegation\n\n**MAIN CHAT MUST NOT WRITE CODE. This is not negotiable.**\n\nMain chat orchestrates the ralph loop. Task agents do the work:\n- **Investigation**: Task agents read code, run tests, gather evidence\n- **Fixes**: Task agents write regression tests and fixes\n\n| Main Chat Does | Task Agents Do |\n|----------------|----------------|\n| Start ralph loop | Investigate root cause |\n| Spawn Task agents | Run tests, read code |\n| Review findings | Write regression tests |\n| Verify fix | Implement fixes |\n\n**If you're about to edit code directly, STOP and delegate instead.**\n</EXTREMELY-IMPORTANT>\n\n## The Process\n\nUnlike implementation (per-task loops), debugging uses **ONE loop per bug**:\n\n```\n1. Start ralph loop for the bug\n   Skill(skill=\"ralph-loop:ralph-loop\", args=\"Debug: [SYMPTOM] --max-iterations 15 --completion-promise FIXED\")\n\n2. Inside loop: spawn Task agent for investigation/fix\n   → Skill(skill=\"workflows:dev-delegate\")\n\n3. Task agent follows 4-phase debug protocol\n\n4. When regression test passes → output promise\n   <promise>FIXED</promise>\n\n5. Bug fixed, loop ends\n```\n\n### Step 1: Start Ralph Loop\n\n**IMPORTANT:** Avoid parentheses `()` in the prompt.\n\n```\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Debug: [SYMPTOM] --max-iterations 15 --completion-promise FIXED\")\n```\n\n### Step 2: Spawn Task Agent\n\nUse dev-delegate, but with debug-specific instructions:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nDebug [SYMPTOM] following systematic protocol.\n\n## Context\n- Read .claude/LEARNINGS.md for prior hypotheses\n- Read .claude/SPEC.md for expected behavior\n\n## Debug Protocol (4 Phases)\n\n### Phase 1: Investigate\n- Add debug logging to suspected code path\n- Reproduce the bug with a test\n- Document: \"Reproduced with [test], output: [error]\"\n\n### Phase 2: Analyze\n- Trace data flow through the code\n- Compare to working code paths\n- Document findings in LEARNINGS.md\n\n### Phase 3: Hypothesize\n- Form ONE specific hypothesis\n- Test it with minimal change\n- If wrong: document what was ruled out\n- If right: proceed to fix\n\n### Phase 4: Fix\n- Write regression test FIRST (must fail before fix)\n- Implement minimal fix\n- Run test, see it PASS\n- Run full test suite\n\n## Output\nReport:\n- Hypothesis tested\n- Root cause (if found)\n- Regression test written\n- Fix applied (or blockers)\n\"\"\")\n```\n\n### Step 3: Verify and Complete\n\nAfter Task agent returns, verify:\n- [ ] Regression test FAILS before fix\n- [ ] Regression test PASSES after fix\n- [ ] Root cause documented in LEARNINGS.md\n- [ ] All existing tests still pass\n\n**If ALL pass → output the promise:**\n```\n<promise>FIXED</promise>\n```\n\n**If ANY fail → iterate (don't output promise yet).**\n\n## The Four Phases\n\n| Phase | Purpose | Output |\n|-------|---------|--------|\n| **Investigate** | Reproduce, trace data flow | Bug reproduction |\n| **Analyze** | Compare working vs broken | Findings documented |\n| **Hypothesize** | ONE specific hypothesis | Hypothesis tested |\n| **Fix** | Regression test → fix | Tests pass |\n\n## The Gate Function\n\nBefore claiming ANY bug is fixed:\n\n```\n1. REPRODUCE → Run test, see bug manifest\n2. INVESTIGATE → Trace data flow, form hypothesis\n3. TEST → Verify hypothesis with minimal change\n4. FIX → Write regression test FIRST (see it FAIL)\n5. VERIFY → Run fix, see regression test PASS\n6. CONFIRM → Run full test suite, no regressions\n7. CLAIM → Only after steps 1-6\n```\n\n**Skipping any step is guessing, not debugging.**\n\n## Rationalization Prevention\n\nThese thoughts mean STOP—you're about to skip the protocol:\n\n| Thought | Reality |\n|---------|---------|\n| \"I know exactly what this is\" | Knowing ≠ verified. Investigate anyway. |\n| \"Let me just try this fix\" | Guessing. Form hypothesis first. |\n| \"The fix is obvious\" | Obvious fixes often mask deeper issues. |\n| \"I've seen this before\" | This instance may be different. Verify. |\n| \"No need for regression test\" | Every fix needs a regression test. Period. |\n| \"It works now\" | \"Works now\" ≠ \"fixed correctly\". Run full suite. |\n| \"I'll add the test later\" | You won't. Write it BEFORE the fix. |\n| **\"Log checking proves fix works\"** | **Logs prove code ran, not that output is correct. Verify actual results.** |\n| **\"It stopped failing\"** | **Stopped failing ≠ fixed. Could be hiding the symptom. Need E2E.** |\n| **\"The error is gone\"** | **No error ≠ correct behavior. Verify expected output.** |\n| **\"Regression test is too complex\"** | **If too complex to test, too complex to know it's fixed.** |\n\n### Fake Fix Verification - STOP\n\n**These do NOT prove a bug is fixed:**\n\n| ❌ Fake Verification | ✅ Real Verification |\n|----------------------|----------------------|\n| \"Error message is gone\" | \"Regression test passes + output matches spec\" |\n| \"Logs show correct path taken\" | \"E2E test verifies user-visible behavior\" |\n| \"No exception thrown\" | \"Test asserts expected data returned\" |\n| \"Process exits 0\" | \"Functional test confirms correct side effects\" |\n| \"Changed one line, seems fine\" | \"Regression test failed before, passes after\" |\n| \"Can't reproduce anymore\" | \"Regression test reproduces it, fix makes it pass\" |\n\n**Red Flag:** If you're claiming \"fixed\" based on absence of errors rather than presence of correct behavior - STOP. That's symptom suppression, not bug fixing.\n\n### Red Flags - STOP If You Think:\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"Let's just try this fix\" | You're guessing | Investigate first |\n| \"I'm pretty sure it's this\" | \"Pretty sure\" ≠ root cause | Gather evidence |\n| \"This should work\" | Hope is not debugging | Test your hypothesis |\n| \"Let me change a few things\" | Multiple changes = can't learn | ONE hypothesis at a time |\n\n## If Max Iterations Reached\n\nRalph exits after max iterations. **Still do NOT ask user to manually verify.**\n\nMain chat should:\n1. **Summarize** hypotheses tested (from LEARNINGS.md)\n2. **Report** what was ruled out and what remains unclear\n3. **Ask user** for direction:\n   - A) Start new loop with different investigation angle\n   - B) Add more logging to specific code path\n   - C) User provides additional context\n   - D) User explicitly requests manual verification\n\n**Never default to \"please verify manually\".** Always exhaust automation first.\n\n## When Fix Requires Substantial Changes\n\nIf root cause reveals need for significant refactoring:\n\n1. Document root cause in LEARNINGS.md\n2. Complete debug loop with `<promise>FIXED</promise>` for the investigation\n3. Use `Skill(skill=\"workflows:dev\")` for the implementation work\n\nDebug finds the problem. The dev workflow implements the solution.\n\n## Failure Recovery Protocol\n\n**Pattern from oh-my-opencode: After 3 consecutive failures, escalate.**\n\n### 3-Failure Trigger\n\nIf you attempt 3 hypotheses and ALL fail:\n\n```\nFailure 1: Hypothesis A tested → still broken\nFailure 2: Hypothesis B tested → still broken\nFailure 3: Hypothesis C tested → still broken\n→ TRIGGER RECOVERY PROTOCOL\n```\n\n### Recovery Steps\n\n1. **STOP** all further debugging attempts\n   - No more \"let me try one more thing\"\n   - No guessing or throwing fixes at the wall\n\n2. **REVERT** to last known working state\n   - `git checkout <last-working-commit>`\n   - Or revert specific files: `git checkout HEAD~N -- file.ts`\n   - Document what was attempted in `.claude/RECOVERY.md`\n\n3. **DOCUMENT** what was attempted\n   - All 3 hypotheses tested\n   - Evidence gathered\n   - Why each failed\n   - What this rules out\n\n4. **CONSULT** with user\n   - \"I've tested 3 hypotheses. All failed. Here's what I've ruled out...\"\n   - Present evidence from investigation\n   - Request: additional context, different investigation angle, or pair debugging\n\n5. **ASK USER** before proceeding\n   - Option A: Start new ralph loop with different approach\n   - Option B: User provides domain knowledge/context\n   - Option C: Escalate to more experienced reviewer\n   - Option D: Accept this as a blocker and document\n\n**NO EVIDENCE = NOT FIXED** (hard rule)\n\n### Recovery Checklist\n\nBefore claiming a bug is fixed after multiple failures:\n\n- [ ] At least 1 hypothesis succeeded (not just \"stopped failing\")\n- [ ] Regression test exists and PASSES\n- [ ] Full test suite passes (no new failures)\n- [ ] Changes are minimal and targeted\n- [ ] Root cause is understood (not just symptom suppressed)\n\n### Anti-Patterns After Failures\n\n**DON'T:**\n- Keep trying random fixes (\"maybe if I change this...\")\n- Expand scope to \"related\" issues\n- Make multiple changes at once\n- Skip the regression test \"this time\"\n- Claim fix without evidence\n\n**DO:**\n- Stop and document what failed\n- Revert to clean state\n- Consult before continuing\n- Follow recovery protocol exactly\n- Require evidence for completion\n\n### Example Recovery Flow\n\n```\nAttempt 1: \"Bug is in parser\" → Added logging → Still broken\nAttempt 2: \"Bug is in validator\" → Fixed validation → Still broken\nAttempt 3: \"Bug is in transformer\" → Rewrote transform → Still broken\n\n→ RECOVERY PROTOCOL:\n1. STOP (no attempt 4)\n2. REVERT all changes: git checkout HEAD -- src/\n3. DOCUMENT in .claude/RECOVERY.md:\n   - Ruled out: parser, validator, transformer\n   - Evidence: logs show data correct at each stage\n   - Hypothesis: Bug might be in consumer, not producer\n4. ASK USER:\n   \"I've ruled out the parser/validator/transformer chain.\n    Logs show data is correct when it leaves our system.\n    Next investigation angle: check the consumer.\n    Should I:\n    A) Start new loop investigating consumer\n    B) Pause for your input on where else to look\"\n```"
              },
              {
                "name": "dev-delegate",
                "description": "Internal skill used by dev-implement during Phase 5 of /dev workflow. NOT user-facing - should only be invoked by dev-ralph-loop inside each implementation iteration. Handles Task agent spawning with TDD enforcement and two-stage review (spec compliance + code quality).",
                "path": "skills/dev-delegate/SKILL.md",
                "frontmatter": {
                  "name": "dev-delegate",
                  "version": 1,
                  "description": "Internal skill used by dev-implement during Phase 5 of /dev workflow. NOT user-facing - should only be invoked by dev-ralph-loop inside each implementation iteration. Handles Task agent spawning with TDD enforcement and two-stage review (spec compliance + code quality)."
                },
                "content": "**Announce:** \"I'm using dev-delegate to dispatch implementation subagents.\"\n\n## Contents\n\n- [The Iron Law of Delegation](#the-iron-law-of-delegation)\n- [Where This Fits](#where-this-fits)\n- [The Process](#the-process)\n- [Honesty Requirement](#honesty-requirement)\n- [Rationalization Prevention](#rationalization-prevention)\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Delegation\n\n**EVERY IMPLEMENTATION MUST GO THROUGH A TASK AGENT. This is not negotiable.**\n\nMain chat MUST NOT:\n- Write code directly\n- Make \"quick fixes\"\n- Edit implementation files\n- \"Just do this one thing\"\n\n**If you're about to write code in main chat, STOP. Spawn a Task agent instead.**\n</EXTREMELY-IMPORTANT>\n\n## Where This Fits\n\n```\nMain Chat                          Task Agent\n─────────────────────────────────────────────────────\ndev-implement (orchestrates)\n  → dev-ralph-loop (per-task loops)\n    → dev-delegate (this skill)\n      → spawns Task agent ──────→ follows dev-tdd\n                                  uses dev-test tools\n```\n\n**Main chat** uses this skill to spawn Task agents.\n**Task agents** follow `dev-tdd` (TDD protocol) and use `dev-test` (testing tools).\n\n## Core Principle\n\n**Fresh subagent per task + two-stage review = high quality, fast iteration**\n\n- Implementer subagent does the work (following dev-tdd)\n- Spec reviewer confirms it matches requirements\n- Quality reviewer checks code quality\n- Loop until both approve\n\n## When to Use\n\nCalled by `dev-implement` inside each ralph loop iteration. Don't invoke directly.\n\n## The Process\n\n```\nFor each task:\n    1. Dispatch implementer subagent\n       - If questions → answer, re-dispatch\n       - Implements, tests, commits\n    2. Dispatch spec reviewer subagent\n       - If issues → implementer fixes → re-review\n    3. Dispatch quality reviewer subagent\n       - If issues → implementer fixes → re-review\n    4. Mark task complete\n```\n\n## Step 1: Dispatch Implementer\n\n**Pattern:** Use structured delegation template from `common/templates/delegation-template.md`\n\nEvery delegation MUST include:\n1. TASK - What to do\n2. EXPECTED OUTCOME - Success criteria\n3. REQUIRED SKILLS - Why this agent\n4. REQUIRED TOOLS - What they'll need\n5. MUST DO - Non-negotiable constraints\n6. MUST NOT DO - Hard blocks\n7. CONTEXT - Parent session state\n8. VERIFICATION - How to confirm completion\n\nUse this Task invocation (fill in brackets):\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\n# TASK\n\nImplement: [TASK NAME]\n\n## EXPECTED OUTCOME\n\nYou will have successfully completed this task when:\n- [ ] [Success criterion 1]\n- [ ] [Success criterion 2]\n- [ ] Tests pass (TDD enforced)\n- [ ] No regressions in existing tests\n\n## REQUIRED SKILLS\n\nThis task requires:\n- [Language/framework]: [Why]\n- Testing: TDD (test-first mandatory)\n- [Other skills as needed]\n\n## REQUIRED TOOLS\n\nYou will need:\n- Read: Examine existing code\n- Write: Create new files\n- Edit: Modify existing files\n- Bash: Run tests and verify\n\n**Tools denied:** None (full implementation access)\n\n## MUST DO\n\n- [ ] Write test FIRST (TDD RED-GREEN-REFACTOR)\n- [ ] Run test suite after each change\n- [ ] Follow existing code patterns in [file]\n- [ ] [Other non-negotiable requirements]\n\n## MUST NOT DO\n\n- ❌ Write code before test\n- ❌ Skip test execution\n- ❌ Use `any` / `@ts-ignore` / type suppression\n- ❌ Commit broken code\n\n## CONTEXT\n\n### Task Description\n[PASTE FULL TASK TEXT FROM PLAN.md - don't make subagent read file]\n\n### Project Context\n- Project: [brief description]\n- Related files: [list from exploration]\n- Test command: [from SPEC.md]\n\n## TDD Protocol (MANDATORY)\n\n<EXTREMELY-IMPORTANT>\n**LOAD THIS SKILL FIRST:**\n\nBefore writing any code, you MUST load the TDD skill:\n\n```\nSkill(skill=\"workflows:dev-tdd\")\n```\n\nThis loads:\n- Task reframing (your job is writing tests, not features)\n- The Execution Gate (6 mandatory gates before E2E testing)\n- GATE 5: READ LOGS (mandatory - cannot skip)\n- The Iron Law of TDD (test-first approach)\n\n**Load dev-tdd now before proceeding.**\n</EXTREMELY-IMPORTANT>\n\nFollow the RED-GREEN-REFACTOR cycle from dev-tdd:\n\n1. **RED**: Write a failing test FIRST\n   - Run it, SEE IT FAIL\n   - Document: \"RED: [test] fails with [error]\"\n\n2. **GREEN**: Write MINIMAL code to pass\n   - Run test, SEE IT PASS\n   - Document: \"GREEN: [test] passes\"\n\n3. **REFACTOR**: Clean up while staying green\n\n**If you write code before seeing RED, you're not doing TDD. Stop and restart.**\n\n## Testing Tools\n\nFor test options (pytest, Playwright, ydotool), load dev-test skill after dev-tdd.\n\nTests must EXECUTE code and VERIFY behavior. Grepping is NOT testing.\n\n## If Unclear\nAsk questions BEFORE implementing. Don't guess.\n\n## Output\nReport:\n- RED: What test failed and how\n- GREEN: What made it pass\n- Test command and output\n- Commit SHA\n- Any concerns\n\"\"\")\n```\n\n**If implementer asks questions:** Answer clearly, then re-dispatch with answers included.\n\n**If implementer finishes:** Proceed to spec review.\n\n## Step 2: Dispatch Spec Reviewer\n\nUse this Task invocation:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nReview spec compliance for: [TASK NAME]\n\n## Original Requirements\n[PASTE TASK TEXT FROM PLAN.md]\n\n## Success Criteria (from SPEC.md)\n[PASTE RELEVANT CRITERIA]\n\n## CRITICAL: Do Not Trust the Report\n\nThe implementer finished suspiciously quickly. Their report may be incomplete,\ninaccurate, or optimistic. You MUST verify everything independently.\n\n**DO NOT:**\n- Take their word for what they implemented\n- Trust their claims about completeness\n- Accept their interpretation of requirements\n\n**DO:**\n- Read the actual code they wrote\n- Compare actual implementation to requirements line by line\n- Check for missing pieces they claimed to implement\n- Look for extra features they didn't mention\n\n## Review Checklist\n1. Does implementation meet ALL requirements?\n2. Is anything MISSING from the spec?\n3. Is anything EXTRA not in the spec?\n\n## Output Format\n- COMPLIANT: All requirements met, nothing extra (after verifying code yourself)\n- ISSUES: List what's missing or extra with file:line references\n\nBe strict. \"Close enough\" is not compliant. Verify by reading code.\n\"\"\")\n```\n\n**If COMPLIANT:** Proceed to quality review.\n\n**If ISSUES:** Have implementer fix, then re-run spec review.\n\n## Step 3: Dispatch Quality Reviewer\n\nUse this Task invocation:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nReview code quality for: [TASK NAME]\n\n## Changes to Review\nFiles modified: [list files]\nCommit range: [BASE_SHA]..[HEAD_SHA]\n\n## Review Focus\n1. Code correctness (logic errors, edge cases)\n2. Test coverage (are tests meaningful?)\n3. Code style (matches project conventions?)\n4. No regressions introduced\n\n## Confidence Scoring\nRate each issue 0-100. Only report issues >= 80 confidence.\n\n## Output Format\n### Strengths\n- [what's good]\n\n### Issues (Confidence >= 80)\n#### [Issue Title] (Confidence: XX)\n- Location: [file:line]\n- Problem: [description]\n- Fix: [suggestion]\n\n### Verdict\nAPPROVED or CHANGES REQUIRED\n\"\"\")\n```\n\n**If APPROVED:** Mark task complete, move to next task.\n\n**If CHANGES REQUIRED:** Have implementer fix, then re-run quality review.\n\n## Honesty Requirement\n\n<EXTREMELY-IMPORTANT>\n**Claiming \"done\" without subagent verification is LYING.**\n\nWhen you say \"Task complete\", you are asserting:\n- A Task agent implemented the change\n- Spec reviewer confirmed compliance\n- Quality reviewer approved\n\nIf ANY of these didn't happen, you are not \"summarizing\" or \"moving on\" - you are LYING about the state of the work.\n\n**Dishonest claims destroy trust. Honest \"still working\" builds trust.**\n</EXTREMELY-IMPORTANT>\n\n## Rationalization Prevention\n\nThese thoughts mean STOP—you're about to skip delegation:\n\n| Thought | Reality |\n|---------|---------|\n| \"I'll just fix this quickly\" | Quick = sloppy = bugs. Delegate. |\n| \"The subagent will be slower\" | Subagent time is cheap. Your context is expensive. |\n| \"I already know what to do\" | Knowing ≠ doing correctly. Delegate. |\n| \"It's just one line\" | One line can break everything. Delegate. |\n| \"I'm already looking at the code\" | Looking ≠ editing. Delegate. |\n| \"User is waiting\" | User wants CORRECT, not fast. Delegate. |\n| \"Skip review, it's obviously right\" | \"Obviously\" is how bugs ship. Review. |\n| \"Spec review passed, skip quality\" | Both reviews exist for a reason. Do both. |\n| \"Quality review found nothing, done\" | Did spec review pass first? Check. |\n\n## Red Flags\n\n**Never:**\n- Skip either review (spec OR quality)\n- Proceed with unfixed issues\n- Let implementer self-review replace actual review\n- Start quality review before spec compliance passes\n- Make subagent read plan file (provide full text)\n- Rush past subagent questions\n\n**If subagent fails:**\n- Dispatch fix subagent with specific instructions\n- Don't fix manually in main chat (context pollution)\n\n## Example Flow\n\n```\nMe: Implementing Task 1: Add user validation\n\n[Dispatch implementer with full task text]\n\nImplementer: \"Should validation happen client-side or server-side?\"\n\nMe: \"Server-side only, in the API layer\"\n\n[Re-dispatch implementer with answer]\n\nImplementer:\n- Added validateUser() in api/users.ts\n- Tests: 5/5 passing\n- Committed: abc123\n\n[Dispatch spec reviewer]\n\nSpec Reviewer: ISSUES\n- Missing: Email format validation (spec line 12)\n\n[Tell implementer to fix]\n\nImplementer:\n- Added email regex validation\n- Tests: 6/6 passing\n- Committed: def456\n\n[Re-dispatch spec reviewer]\n\nSpec Reviewer: COMPLIANT\n\n[Dispatch quality reviewer with commit range]\n\nQuality Reviewer: APPROVED\n- Strengths: Good test coverage, clear naming\n- No issues >= 80 confidence\n\n[Mark Task 1 complete, move to Task 2]\n```\n\n## Integration\n\n**Main chat invokes:**\n- `dev-implement` → `dev-ralph-loop` → `dev-delegate` (this skill)\n\n**Task agents follow:**\n- `dev-tdd` - TDD protocol (RED-GREEN-REFACTOR)\n- `dev-test` - Testing tools (pytest, Playwright, ydotool)\n\nAfter all tasks complete with passing tests, `dev-implement` proceeds to `dev-review`."
              },
              {
                "name": "dev-design",
                "description": "This skill should be used when the user asks to \"propose architecture\", \"design implementation approach\", \"choose between approaches\", or in Phase 4 of /dev workflow after exploration is complete. Proposes 2-3 architecture approaches with clear trade-offs, decomposes features into independent tasks, and obtains explicit user approval before implementation begins.",
                "path": "skills/dev-design/SKILL.md",
                "frontmatter": {
                  "name": "dev-design",
                  "description": "This skill should be used when the user asks to \"propose architecture\", \"design implementation approach\", \"choose between approaches\", or in Phase 4 of /dev workflow after exploration is complete. Proposes 2-3 architecture approaches with clear trade-offs, decomposes features into independent tasks, and obtains explicit user approval before implementation begins.",
                  "version": "0.1.0"
                },
                "content": "**Announce:** \"Using dev-design (Phase 4) to propose implementation approaches and obtain user approval.\"\n\n## Contents\n\n- [The Iron Law of Design](#the-iron-law-of-design)\n- [What Design Does](#what-design-does)\n- [Process](#process)\n- [Approach Categories](#approach-categories)\n- [PLAN.md Format](#planmd-format)\n- [Red Flags](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Architecture Design with User Gate\n\nPropose implementation approaches, explain trade-offs, get user approval.\n**Prerequisites:** SPEC.md finalized, exploration complete, clarifications resolved.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Design\n\n**YOU MUST GET USER APPROVAL BEFORE IMPLEMENTATION. This is not negotiable.**\n\nAfter presenting approaches:\n1. Show 2-3 options with trade-offs\n2. Lead with your recommendation\n3. **Ask user which approach**\n4. **Wait for explicit approval**\n\nImplementation CANNOT start without user saying \"Yes\" or choosing an approach.\n\n**STOP - you're about to implement without user approval.**\n</EXTREMELY-IMPORTANT>\n\n## What Design Does\n\n| DO | DON'T |\n|----|-------|\n| Propose 2-3 approaches | Implement anything |\n| Explain trade-offs clearly | Make the choice for user |\n| Lead with recommendation | Present without opinion |\n| Get explicit approval | Assume approval |\n| Write PLAN.md | Skip the user gate |\n\n**Design answers: HOW to build it and WHY this approach**\n**Implement executes: the approved approach** (next phase, after gate)\n\n## Process\n\n### 1. Review Inputs\n\nBefore designing, ensure the following exist:\n- `.claude/SPEC.md` - final requirements\n- Exploration findings - key files, patterns\n- Clarified decisions - edge cases, integrations\n\n### 2. Propose 2-3 Approaches\n\nEach approach should address the same requirements differently:\n\n**Approach A: Minimal Changes**\n- Smallest diff, maximum reuse\n- Trade-off: May be less clean, tech debt\n\n**Approach B: Clean Architecture**\n- Best patterns, maintainability\n- Trade-off: More changes, longer implementation\n\n**Approach C: Pragmatic Balance**\n- Balance of speed and quality\n- Trade-off: Compromise on both\n\n### 3. Present with Trade-offs\n\nUse the AskUserQuestion tool to present approaches:\n\n```python\n# AskUserQuestion: Present 2-3 architecture approaches with trade-offs for user selection\nAskUserQuestion(questions=[{\n  \"question\": \"Which architecture approach should we use?\",\n  \"header\": \"Architecture\",\n  \"options\": [\n    {\n      \"label\": \"Pragmatic Balance (Recommended)\",\n      \"description\": \"Extend existing AuthService with new method. ~150 lines changed. Balances reuse with clean separation.\"\n    },\n    {\n      \"label\": \"Minimal Changes\",\n      \"description\": \"Add logic to existing endpoint. ~50 lines changed. Fast but increases coupling.\"\n    },\n    {\n      \"label\": \"Clean Architecture\",\n      \"description\": \"New service with full abstraction. ~300 lines. Most maintainable but longest to build.\"\n    }\n  ],\n  \"multiSelect\": false\n}])\n```\n\n**Key principles:**\n- Lead with recommendation (first option + \"Recommended\")\n- Concrete numbers (lines changed, files affected)\n- Clear trade-offs for each\n- Reference specific files from exploration\n\n### 4. Feature Decomposition Check\n\n**CRITICAL:** Before writing PLAN.md, check if this is actually multiple features.\n\nReview the scope and ask:\n\n```python\n# AskUserQuestion: Determine if feature should be split into independent tasks\nAskUserQuestion(questions=[{\n  \"question\": \"Is this one cohesive feature or multiple independent features?\",\n  \"header\": \"Scope\",\n  \"options\": [\n    {\n      \"label\": \"One feature\",\n      \"description\": \"Implement everything together in one branch/worktree\"\n    },\n    {\n      \"label\": \"Multiple features\",\n      \"description\": \"Break into separate features, each with own branch/worktree/PR\"\n    }\n  ],\n  \"multiSelect\": false\n}])\n```\n\n**If \"Multiple features\":**\n\n1. **List the independent features** identified from SPEC.md:\n   ```\n   Based on the requirements, this breaks into:\n   1. Theme infrastructure (color system, theme provider)\n   2. Settings UI (theme selector component)\n   3. Component updates (update 20+ components to use theme)\n   4. Persistence layer (save user preference)\n\n   Each can be implemented and PR'd independently.\n   ```\n\n2. **Ask which to tackle first:**\n   ```python\n   # AskUserQuestion: Prioritize which feature to implement first\n   AskUserQuestion(questions=[{\n     \"question\": \"Which feature should we implement first?\",\n     \"header\": \"Priority\",\n     \"options\": [\n       {\"label\": \"Theme infrastructure (Recommended)\", \"description\": \"Foundation that others depend on\"},\n       {\"label\": \"Settings UI\", \"description\": \"UI for theme selection\"},\n       {\"label\": \"Component updates\", \"description\": \"Apply themes to components\"},\n       {\"label\": \"Persistence layer\", \"description\": \"Save user preference\"}\n     ],\n     \"multiSelect\": false\n   }])\n   ```\n\n3. **Write PLAN.md for ONLY the chosen feature**\n\n4. **Document remaining features** in `.claude/BACKLOG.md`:\n   ```markdown\n   # Feature Backlog\n\n   ## Dark Mode Implementation\n\n   ### Completed\n   - [ ] None yet\n\n   ### Next Up\n   - [ ] Theme infrastructure\n   - [ ] Settings UI\n   - [ ] Component updates\n   - [ ] Persistence layer\n\n   **Current Focus:** Theme infrastructure\n   ```\n\n**If \"One feature\":**\n\nProceed to write PLAN.md for the entire scope (step 5 below).\n\n**Why this matters:**\n\n- Multiple features in one branch = massive PR, review hell, merge conflicts\n- Separate features = clean PRs, incremental progress, easier reviews\n- After first feature PR merges, come back and tackle next feature\n\n### 5. Write PLAN.md\n\nAfter user chooses approach AND confirms scope, write `.claude/PLAN.md`:\n\n```markdown\n# Implementation Plan: [Feature]\n\n> **For Claude:** REQUIRED SUB-SKILL: Invoke `Skill(skill=\"workflows:dev-implement\")` to implement this plan.\n>\n> **Per-Task Ralph Loops:** Assign each task its OWN ralph loop. Do NOT combine multiple tasks into one loop.\n>\n> **Delegation:** Main chat orchestrates, Task agents implement. Use `Skill(skill=\"workflows:dev-delegate\")` for subagent templates.\n\n## Chosen Approach\n[Name]: [Brief description]\n\n## Rationale\n- [Why this approach fits]\n- [Trade-offs accepted]\n\n## Files to Modify\n| File | Change |\n|------|--------|\n| `src/auth/service.ts` | Add `validateSession()` method |\n| `src/routes/api.ts` | Add new endpoint |\n\n## New Files\n| File | Purpose |\n|------|---------|\n| `src/auth/types.ts` | Session type definitions |\n\n## Implementation Order (with Per-Task Ralph Loops)\n\n> **For Claude:** Each task = one ralph loop. Complete task N before starting task N+1.\n>\n> Pattern: `Skill(skill=\"ralph-loop:ralph-loop\", args=\"Task N: [name] --max-iterations 10 --completion-promise TASKN_DONE\")`\n\n| Task | Ralph Loop | Core Test (MUST EXECUTE CODE) | Verify Command |\n|------|------------|-------------------------------|----------------|\n| 1. Add types | `\"Task 1: Add types\" → TASK1_DONE` | N/A (types only) | `tsc --noEmit` |\n| 2. Service method | `\"Task 2: Service method\" → TASK2_DONE` | `test_validate_session()` calls method, checks return | `pytest tests/test_auth.py -v` |\n| 3. Route handler | `\"Task 3: Route handler\" → TASK3_DONE` | Integration test hits endpoint, checks response | `pytest tests/test_api.py -v` |\n\n### What Counts as a REAL Test\n\n| ✅ REAL (execute + verify) | ❌ NOT A TEST (never do this) |\n|----------------------------|-------------------------------|\n| pytest calls function | grep for function exists |\n| Playwright clicks button | ast-grep finds pattern |\n| API request checks response | Log says \"success\" |\n| Screenshot comparison | \"Code looks correct\" |\n\n**Every task MUST have a test that EXECUTES the code and VERIFIES behavior.**\n```\n\n### 6. User Gate - Final Approval\n\nAfter writing PLAN.md, get explicit approval:\n\n```\nAskUserQuestion(questions=[{\n  \"question\": \"Ready to start implementation?\",\n  \"header\": \"Approval\",\n  \"options\": [\n    {\"label\": \"Yes, proceed\", \"description\": \"Start implementation with TDD\"},\n    {\"label\": \"No, discuss changes\", \"description\": \"Modify the plan first\"}\n  ],\n  \"multiSelect\": false\n}])\n```\n\n**If \"No\":** Wait for user feedback, modify plan, ask again.\n\n**If \"Yes\":** Proceed to workspace setup question in Step 7 below.\n\n### 7. Workspace Setup Question\n\nAfter user approves implementation, ask about worktree isolation:\n\n```\nAskUserQuestion(questions=[{\n  \"question\": \"Create isolated worktree for this feature?\",\n  \"header\": \"Workspace\",\n  \"options\": [\n    {\"label\": \"Yes (Recommended)\", \"description\": \"Work in isolated .worktrees/ directory - keeps main workspace clean\"},\n    {\"label\": \"No\", \"description\": \"Work in current directory\"}\n  ],\n  \"multiSelect\": false\n}])\n```\n\n**If \"Yes (Recommended)\":**\n\nInvoke the dev-worktree skill:\n```bash\n# dev-worktree: Create isolated git worktree for feature development\nSkill(skill=\"workflows:dev-worktree\")\n```\n\nThen after worktree is created, invoke dev-implement.\n\n**If \"No\":**\n\nDirectly invoke dev-implement in current directory without worktree isolation.\n\n## Approach Categories\n\n| Category | When to Use | Trade-off |\n|----------|-------------|-----------|\n| Minimal | Bug fixes, small features | Speed vs cleanliness |\n| Clean | New systems, core features | Quality vs time |\n| Pragmatic | Most features | Balance |\n\n## PLAN.md Format\n\nRequired sections:\n- **Chosen Approach** - What was selected and why\n- **Files to Modify** - Specific paths with change descriptions\n- **New Files** - If any, with purposes\n- **Implementation Order** - Ordered task list with dependencies\n- **Testing Strategy** - How to verify\n\n## The Gate Function\n\nComplete all steps before starting implementation:\n\n```\n1. REVIEW → Read SPEC.md and exploration findings\n2. PROPOSE → Present 2-3 approaches with trade-offs\n3. ASK → Use AskUserQuestion with clear options\n4. DECOMPOSE → Ask \"One feature or multiple?\" (CRITICAL)\n   └─ If multiple → List features, ask which first, write BACKLOG.md\n5. WAIT → Do NOT proceed until user responds\n6. DOCUMENT → Write chosen approach to PLAN.md (for chosen feature only if decomposed)\n7. CONFIRM → Ask \"Ready to proceed?\"\n8. WORKSPACE → Ask \"Create worktree?\" (Yes recommended / No)\n9. SETUP → If worktree Yes, invoke dev-worktree\n10. GATE → Only start /dev-implement after all approvals\n```\n\n**Mandatory steps (NEVER skip):** DECOMPOSE, WAIT, WORKSPACE, and GATE.\n\n## Rationalization Prevention\n\nRecognize these thoughts as red flags—they signal attempts to bypass the user gate:\n\n| Thought | Reality |\n|---------|---------|\n| \"User will approve this\" | Your assumption ≠ approval. Ask and wait. |\n| \"It's the obvious choice\" | User decides what's obvious. Present options. |\n| \"Let me just start\" | NO. Gate exists for a reason. Wait. |\n| \"User said they trust me\" | Trust doesn't mean skip approval. Ask. |\n| \"Time pressure\" | You'll waste more time with the wrong approach. Wait for approval. |\n| \"Only one viable option\" | Present it anyway. User may see alternatives. |\n| \"Ask forgiveness later\" | No. Ask permission now. |\n\n## Red Flags - STOP If You're About To:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Present only one approach | You're removing user choice | Always show 2-3 options |\n| Skip trade-offs | You're making decision for user | Explain pros/cons clearly |\n| Start implementing | You don't have approval yet | Wait for explicit \"Yes\" |\n| Assume recommendation accepted | You're guessing at user preference | Ask and wait for answer |\n\n## Output\n\nDesign complete when:\n- 2-3 approaches presented with trade-offs\n- User chose an approach\n- `.claude/PLAN.md` written with chosen approach\n- **User explicitly approved** (\"Yes, proceed\")\n\n## Phase Complete\n\n**After user approves (\"Yes, proceed\"):**\n\n1. **Ask about worktree** (Step 7 above)\n2. **If worktree chosen:**\n   - Invoke `Skill(skill=\"workflows:dev-worktree\")`\n   - After worktree created, invoke `Skill(skill=\"workflows:dev-implement\")`\n3. **If no worktree:**\n   - Directly invoke `Skill(skill=\"workflows:dev-implement\")`\n\n**Required before proceeding:**\n- Explicit user approval for implementation\n- Feature scope decision (one feature vs multiple)\n- User choice on worktree (Yes/No)\n\n**After this feature is implemented and PR'd:**\n\nIf multiple features were identified in step 4, check `.claude/BACKLOG.md` for remaining features:\n1. View remaining features in BACKLOG.md\n2. Invoke `/dev` again to tackle the next feature\n3. Repeat until all features are complete\n\nThis enables incremental development: one feature → PR → merge → next feature."
              },
              {
                "name": "dev-explore",
                "description": "REQUIRED Phase 2 of /dev workflow after dev-brainstorm. This skill should be used when the user asks to 'explore the codebase', 'map architecture', 'find similar features', 'discover test infrastructure', 'trace execution paths', 'identify code patterns', or needs to understand WHERE code lives and HOW it works before implementation. Launches parallel explore agents and returns prioritized key files list.",
                "path": "skills/dev-explore/SKILL.md",
                "frontmatter": {
                  "name": "dev-explore",
                  "version": 1,
                  "description": "REQUIRED Phase 2 of /dev workflow after dev-brainstorm. This skill should be used when the user asks to 'explore the codebase', 'map architecture', 'find similar features', 'discover test infrastructure', 'trace execution paths', 'identify code patterns', or needs to understand WHERE code lives and HOW it works before implementation. Launches parallel explore agents and returns prioritized key files list."
                },
                "content": "**Announce:** \"I'm using dev-explore (Phase 2) to map the codebase.\"\n\n## Contents\n\n- [The Iron Law of Exploration](#the-iron-law-of-exploration)\n- [What Explore Does](#what-explore-does)\n- [Process](#process)\n- [Test Infrastructure Discovery](#test-infrastructure-discovery)\n- [Key Files List Format](#key-files-list-format)\n- [Red Flags](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Codebase Exploration\n\nMap relevant code, trace execution paths, and return prioritized files for reading.\n**Prerequisite:** `.claude/SPEC.md` must exist with draft requirements.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Exploration\n\n**RETURN KEY FILES LIST. This is not negotiable.**\n\nEvery exploration, you MUST return:\n1. Summary of findings\n2. **5-10 key files** with line numbers and purpose\n3. Patterns discovered\n\nAfter agents return, **you MUST read all key files** before proceeding.\n\n**STOP if you're about to move on without reading all key files.**\n</EXTREMELY-IMPORTANT>\n\n### Rationalization Table - STOP If Thinking:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I can design without reading all key files\" | You'll miss critical patterns | READ every file on the list |\n| \"The file names tell me enough\" | File names hide implementation details | READ the actual code |\n| \"I'll read them if I need more info\" | You cannot know what is missing | READ all key files NOW |\n| \"Exploration summary is enough\" | Summaries miss crucial nuances | READ original files |\n| \"Reading files will take too long\" | You'll waste days later by skipping them | READ now, save time later |\n| \"I already understand the architecture\" | Your assumptions remain incomplete | READ to confirm understanding |\n| \"I can grep for specific details later\" | You'll miss context and relationships | READ to understand connections |\n\n### Honesty Framing\n\n**Returning key files without reading them is LYING about understanding the codebase.**\n\nExploration agents find the files. Main chat MUST read them to understand the codebase. Skipping reads means proceeding with incomplete knowledge, which guarantees wrong implementation choices.\n\nReading costs minutes. Wrong architecture costs days of rework.\n\n### No Pause After Completion\n\nAfter reading all key files and updating `.claude/SPEC.md` with findings, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:dev-clarify\")\n```\n\nDO NOT:\n- Summarize findings (proceed directly)\n- Ask \"should I proceed to clarify?\"\n- Wait for user confirmation\n- Write status updates\n\nThe workflow phases are SEQUENTIAL. Complete explore → immediately start clarify.\n\n## What Explore Does\n\n| DO | DON'T |\n|----|-------|\n| Trace execution paths | Ask user questions (that's clarify) |\n| Map architecture layers | Design approaches (that's design) |\n| Find similar features | Write implementation tasks |\n| Identify patterns and conventions | Make architecture decisions |\n| Return key files list | Skip reading key files |\n\n**Explore answers: WHERE is the code and HOW does it work**\n**Design answers: WHAT approach to take** (separate skill)\n\n## Process\n\n### 1. Launch 3-5 Explore Agents in Parallel + Background\n\n<EXTREMELY-IMPORTANT>\n**Launch ALL agents in a SINGLE message with multiple Task calls.**\n\n**Use `run_in_background: true` for ALL explore agents.**\n\nThis enables true parallel execution:\n- All agents start immediately\n- Main conversation continues without blocking\n- Results collected asynchronously with TaskOutput\n\nPattern from oh-my-opencode: Default to background + parallel for exploratory work.\n</EXTREMELY-IMPORTANT>\n\nBased on `.claude/SPEC.md`, spawn 3-5 agents with different focuses:\n\n```\n# PARALLEL + BACKGROUND: All Task calls in ONE message\n\nTask(\n    subagent_type=\"Explore\",\n    description=\"Find similar features\",\n    run_in_background=true,\n    prompt=\"\"\"\nExplore the codebase for [FEATURE AREA].\n\nFocus: Find similar features to [SPEC REQUIREMENT]\n\nUse ast-grep for semantic search:\n- sg -p 'function_name($$$)' --lang [language]\n- sg -p 'class $NAME { $$$ }' --lang [language]\n\nTasks:\n- Trace execution paths from entry point to data storage\n- Find similar implementations to follow\n- Identify patterns used\n- Return 5-10 key files with line numbers\n\nContext from SPEC.md:\n[paste relevant requirements]\n\"\"\")\n\nTask(\n    subagent_type=\"Explore\",\n    description=\"Map architecture layers\",\n    run_in_background=true,\n    prompt=\"\"\"\nExplore the codebase for [FEATURE AREA].\n\nFocus: Map architecture and abstractions for [AREA]\n\nUse ast-grep for semantic search:\n- sg -p 'class $NAME($BASE):' --lang [language]\n- sg -p 'interface $NAME { $$$ }' --lang [language]\n\nTasks:\n- Identify abstraction layers\n- Find cross-cutting concerns (logging, auth, errors)\n- Map module dependencies\n- Return 5-10 key files with line numbers\n\nContext from SPEC.md:\n[paste relevant requirements]\n\"\"\")\n\nTask(\n    subagent_type=\"Explore\",\n    description=\"Find test infrastructure\",\n    run_in_background=true,\n    prompt=\"\"\"\nExplore the codebase for [FEATURE AREA].\n\nFocus: Test infrastructure and patterns\n\nUse ast-grep for test discovery:\n- sg -p 'def test_$NAME($$$):' --lang python\n- sg -p 'it($DESC, $$$)' --lang javascript\n- sg -p '@pytest.fixture' --lang python\n\nTasks:\n- Find test directory and framework\n- Identify existing test patterns\n- Check for fixtures, mocks, helpers\n- Return 5-10 key test files with line numbers\n\nContext from SPEC.md:\n[paste relevant requirements]\n\"\"\")\n```\n\n**After launching all agents in parallel:**\n- Continue immediately to other work (don't wait)\n- Check agent status with `/tasks` command\n- Collect results when ready with TaskOutput tool\n\n### 1b. Collect Background Results\n\nOnce agents complete, collect their findings:\n\n```\n# Check running tasks\n/tasks\n\n# Get results from completed agents\nTaskOutput(task_id=\"task-abc123\", block=true, timeout=30000)\nTaskOutput(task_id=\"task-def456\", block=true, timeout=30000)\nTaskOutput(task_id=\"task-ghi789\", block=true, timeout=30000)\n```\n\n**Stop Conditions** (from oh-my-opencode):\n- Enough context to proceed confidently\n- Same info appearing across multiple agents\n- 2 search iterations yielded nothing new\n- Direct answer found\n\n**DO NOT over-explore. Time is precious.**\n\n### 2. Consolidate Key Files\n\nAfter all agents return, consolidate their key files lists:\n- Remove duplicates\n- Prioritize by relevance to requirements\n- Create master list of 10-15 files\n\n### 3. Read All Key Files\n\n**CRITICAL: Main chat must read every file on the key files list.**\n\n```\nRead(file_path=\"src/auth/login.ts\")\nRead(file_path=\"src/services/session.ts\")\n...\n```\n\nThis builds deep understanding before asking clarifying questions.\n\n### 4. Document Findings\n\nWrite exploration summary (can be verbal or in `.claude/EXPLORATION.md`):\n- Patterns discovered\n- Architecture insights\n- Dependencies identified\n- Questions raised for clarify phase\n\n## Code Search Tools\n\n**Prefer semantic search over text search when exploring code.**\n\nUse ast-grep (`sg`) for precise AST-based pattern matching and ripgrep-all (`rga`) for searching non-code files.\n\n**For detailed patterns and usage, see:** `references/ast-grep-patterns.md`\n\n## Test Infrastructure Discovery\n\n<EXTREMELY-IMPORTANT>\n**CRITICAL: You MUST discover how to run REAL automated tests.**\n\nREAL automated tests EXECUTE code and verify RUNTIME behavior.\nGrepping source files is NOT testing. Log checking is NOT testing.\n\n| ✅ REAL TEST INFRASTRUCTURE | ❌ NOT TESTING (never acceptable) |\n|-----------------------------|-----------------------------------|\n| pytest that calls functions | grep/ast-grep to find code |\n| Playwright that clicks buttons | Reading logs for \"success\" |\n| ydotool that simulates user input | Code review / structure check |\n| API calls that verify responses | \"It looks correct\" |\n\n**If no way to EXECUTE and VERIFY exists, flag this as a blocker.**\n</EXTREMELY-IMPORTANT>\n\n### Project Test Framework\n\n```bash\n# Find test directories across common locations\nls -d tests/ test/ spec/ __tests__/ 2>/dev/null\n\n# Find test frameworks in build configuration\ncat meson.build 2>/dev/null | grep -i test\n\n# Find test frameworks in Node package manifest\ncat package.json 2>/dev/null | grep -E \"(test|jest|mocha|vitest)\"\n\n# Find pytest configuration in Python projects\ncat pyproject.toml 2>/dev/null | grep -i pytest\n\n# Find dev dependencies in Rust projects\ncat Cargo.toml 2>/dev/null | grep -i \"\\[dev-dependencies\\]\"\n\n# Find and list existing test files\nfind . -name \"*test*\" -type f | head -20\n```\n\n### Available Tools for REAL Testing\n\n| What to Test | Tool | How It's a REAL Test |\n|--------------|------|----------------------|\n| Functions | pytest, jest, cargo test | Calls function, checks return value |\n| CLI | subprocess, execa | Runs binary, checks output |\n| Web UI | Playwright MCP | Clicks button, verifies DOM |\n| Desktop UI | ydotool + grim | Simulates input, screenshots result |\n| API | requests, fetch | Sends request, checks response |\n| D-Bus apps | dbus-send | Invokes method, checks return |\n\n```bash\n# Check for desktop automation tools\nwhich ydotool grim dbus-send 2>/dev/null\n\n# List available D-Bus services for desktop app automation\ndbus-send --session --print-reply --dest=org.freedesktop.DBus \\\n  /org/freedesktop/DBus org.freedesktop.DBus.ListNames 2>/dev/null | grep -i appname\n```\n\n### Document in Exploration Output\n\n**REQUIRED findings for SPEC.md:**\n- **Test framework:** meson test / pytest / jest / etc.\n- **Test command:** Exact command to run tests\n- **How to verify core functionality:** What EXECUTES the code\n- **Available automation:** Playwright MCP, ydotool, D-Bus interfaces\n- **Blocker:** If no way to run REAL tests, flag immediately\n\n## Key Files List Format\n\nEach agent MUST return files in this format:\n\n```markdown\n## Key Files to Read\n\n| Priority | File:Line | Purpose |\n|----------|-----------|---------|\n| 1 | `src/auth/login.ts:45` | Entry point for auth flow |\n| 2 | `src/services/session.ts:12` | Session management |\n| 3 | `src/middleware/auth.ts:78` | Auth middleware |\n| 4 | `src/types/user.ts:1` | User type definitions |\n| 5 | `tests/auth/login.test.ts:1` | Existing test patterns |\n```\n\n## Red Flags - STOP If You're About To:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Skip reading key files | You'll miss crucial context | Read every file on the list |\n| Ask design questions | You're conflating exploration with design | Save for clarify/design phases |\n| Propose approaches | You're jumping to decisions too early | Just document what exists |\n| Start implementing | You must understand first | Complete exploration fully |\n\n## Output\n\nExploration complete when:\n- 2-3 explore agents returned findings\n- Key files list consolidated (10-15 files)\n- **All key files read by main chat**\n- Patterns and architecture documented\n- **Test infrastructure documented**\n- Questions for clarification identified\n\n### Required Output Sections\n\n1. **Key Files** - 10-15 files with line numbers\n2. **Architecture** - Layers, patterns, conventions\n3. **Test Infrastructure** - Framework, tools, patterns\n4. **Questions** - For clarify phase\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After completing exploration, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:dev-clarify\")\n```"
              },
              {
                "name": "dev-implement",
                "description": "REQUIRED Phase 5 of /dev workflow. Orchestrates per-task ralph loops with delegated TDD implementation.",
                "path": "skills/dev-implement/SKILL.md",
                "frontmatter": {
                  "name": "dev-implement",
                  "version": 1,
                  "description": "REQUIRED Phase 5 of /dev workflow. Orchestrates per-task ralph loops with delegated TDD implementation."
                },
                "content": "**Announce:** \"I'm using dev-implement (Phase 5) to orchestrate implementation.\"\n\n## Where This Fits\n\n```\nMain Chat (you)                    Task Agent\n─────────────────────────────────────────────────────\ndev-implement (this skill)\n  → dev-ralph-loop (per-task loops)\n    → dev-delegate (spawn agents)\n      → Task agent ──────────────→ follows dev-tdd\n                                   uses dev-test tools\n```\n\n**Main chat orchestrates.** Task agents implement.\n\n## Contents\n\n- [Prerequisites](#prerequisites)\n- [The Iron Law of Delegation](#the-iron-law-of-delegation)\n- [The Process](#the-process)\n- [Sub-Skills Reference](#sub-skills-reference)\n- [If Max Iterations Reached](#if-max-iterations-reached)\n- [Phase Complete](#phase-complete)\n\n# Implementation (Orchestration)\n\n<EXTREMELY-IMPORTANT>\n## Prerequisites\n\n**Do NOT start implementation without these:**\n\n1. `.claude/SPEC.md` exists with final requirements\n2. `.claude/PLAN.md` exists with chosen approach\n3. **User explicitly approved** in /dev-design phase\n\nIf any prerequisite is missing, STOP and complete the earlier phases.\n\n**Check PLAN.md for:** files to modify, implementation order, testing strategy.\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Delegation\n\n**MAIN CHAT MUST NOT WRITE CODE. This is not negotiable.**\n\nMain chat orchestrates. Subagents implement. If you catch yourself about to use Write or Edit on a code file, STOP.\n\n| Allowed in Main Chat | NOT Allowed in Main Chat |\n|---------------------|--------------------------|\n| Spawn Task agents | Write/Edit code files |\n| Review Task agent output | Direct implementation |\n| Write to .claude/*.md files | \"Quick fixes\" |\n| Run git commands | Any code editing |\n| Start ralph loops | Bypassing delegation |\n\n**If you're about to edit code directly, STOP and spawn a Task agent instead.**\n\n### Rationalization Prevention\n\nThese thoughts mean STOP—you're rationalizing:\n\n| Thought | Reality |\n|---------|---------|\n| \"It's just a small fix\" | Small fixes become big mistakes. Delegate. |\n| \"I'll be quick\" | Quick means sloppy. Delegate. |\n| \"The subagent will take too long\" | Subagent time is cheap. Your context is expensive. |\n| \"I already know what to do\" | Knowing ≠ doing it well. Delegate. |\n| \"Let me just do this one thing\" | One thing leads to another. Delegate. |\n| \"This is too simple for a subagent\" | Simple is exactly when delegation works best. |\n| \"I'm already here in the code\" | Being there ≠ writing there. Delegate. |\n| \"The user is waiting\" | User wants DONE, not fast. They won't debug your shortcuts. |\n| \"This is just porting/adapting code\" | Porting = writing = code. Delegate. |\n| \"I already have context loaded\" | Fresh context per task is the point. Delegate. |\n| \"It's config, not real code\" | JSON/YAML/TOML = code. Delegate. |\n| \"I need to set things up first\" | Setup IS implementation. Delegate. |\n| \"This is boilerplate\" | Boilerplate = code = delegate. |\n| \"PLAN.md is detailed, just executing\" | Execution IS implementation. Delegate. |\n\n### The Meta-Rationalization\n\n**If you're treating these rules as \"guidelines for complex work\" rather than \"invariants for ALL work\", you've already failed.**\n\nSimple work is EXACTLY when discipline matters most—because that's when you're most tempted to skip it.\n</EXTREMELY-IMPORTANT>\n\n## The Process\n\n```\nFor each task N in PLAN.md:\n    1. Start ralph loop for task N\n       → Skill(skill=\"workflows:dev-ralph-loop\")\n\n    2. Inside loop: spawn Task agent\n       → Skill(skill=\"workflows:dev-delegate\")\n\n    3. Task agent follows TDD (dev-tdd) using testing tools (dev-test)\n\n    4. Verify tests pass, output promise\n\n    5. Move to task N+1, start NEW ralph loop\n```\n\n### Step 1: Start Ralph Loop for Each Task\n\n**REQUIRED SUB-SKILL:**\n```\nSkill(skill=\"workflows:dev-ralph-loop\")\n```\n\nKey points from dev-ralph-loop:\n- ONE loop PER TASK (not one loop for feature)\n- Each task gets its own completion promise\n- Don't move to task N+1 until task N's loop completes\n\n### Step 2: Inside Loop - Spawn Task Agent\n\n**REQUIRED SUB-SKILL:**\n```\nSkill(skill=\"workflows:dev-delegate\")\n```\n\nKey points from dev-delegate:\n- Implementer → Spec reviewer → Quality reviewer\n- Task agent follows dev-tdd protocol\n- Task agent uses dev-test tools\n\n### Step 3: Verify and Complete\n\nAfter Task agent returns, verify:\n- [ ] Tests EXECUTE code (not grep)\n- [ ] Tests PASS (SKIP ≠ PASS)\n- [ ] LEARNINGS.md has actual output\n- [ ] Build succeeds\n\n**If ALL pass → output the promise.** If ANY fail → iterate.\n\n## Sub-Skills Reference\n\n| Skill | Purpose | Used By |\n|-------|---------|---------|\n| `dev-ralph-loop` | Per-task loop pattern | Main chat |\n| `dev-delegate` | Task agent templates | Main chat |\n| `dev-tdd` | TDD protocol (RED-GREEN-REFACTOR) | Task agent |\n| `dev-test` | Testing tools (pytest, Playwright, etc.) | Task agent |\n\n## Failure Recovery Protocol\n\n**Pattern from oh-my-opencode: After 3 consecutive implementation failures, escalate.**\n\n### 3-Failure Trigger\n\nIf you attempt 3 implementations and ALL fail tests:\n\n```\nIteration 1: Implement approach A → tests fail\nIteration 2: Implement approach B → tests fail\nIteration 3: Implement approach C → tests fail\n→ TRIGGER RECOVERY PROTOCOL\n```\n\n### Recovery Steps\n\n1. **STOP** all further implementation attempts\n   - No more \"let me try a different approach\"\n   - No guessing or throwing code at the problem\n\n2. **REVERT** to last known working state\n   - `git checkout <last-passing-commit>`\n   - Or revert specific files\n   - Document what was attempted in `.claude/RECOVERY.md`\n\n3. **DOCUMENT** what was attempted\n   - All 3 approaches tried\n   - Test failures for each\n   - Why each approach failed\n   - What this reveals about the problem\n\n4. **CONSULT** with user BEFORE continuing\n   - \"I've tried 3 approaches. All fail tests. Here's what I've learned...\"\n   - Present test failure patterns\n   - Request: requirements clarification, design input, or different strategy\n\n5. **ASK USER** for direction\n   - Option A: Re-examine requirements (may need /dev-clarify)\n   - Option B: Try completely different design (may need /dev-design)\n   - Option C: Investigate why tests fail (may need /dev-debug)\n   - Option D: User provides domain knowledge\n\n**NO PASSING TESTS = NOT COMPLETE** (hard rule)\n\n### Recovery Checklist\n\nBefore continuing after multiple failures:\n\n- [ ] All 3 approaches documented with test failures\n- [ ] Pattern in failures identified (same tests? different errors?)\n- [ ] Current code reverted to clean state\n- [ ] User consulted with specific question\n- [ ] Clear direction from user before proceeding\n\n### Anti-Patterns After Failures\n\n**DON'T:**\n- Keep trying \"just one more thing\"\n- Make larger and larger changes\n- Skip TDD \"to get it working first\"\n- Suppress test failures (\"I'll fix them later\")\n- Blame the tests (\"tests are wrong\")\n\n**DO:**\n- Stop and analyze the failure pattern\n- Revert to clean state\n- Document what each approach revealed\n- Consult user with specific findings\n- Get clear direction before continuing\n\n### Example Recovery Flow\n\n```\nLoop 1: Implement with synchronous approach → Tests timeout\nLoop 2: Implement with async/await → Tests hang\nLoop 3: Implement with promises → Tests fail assertion\n\n→ RECOVERY PROTOCOL:\n1. STOP (no loop 4)\n2. REVERT: git checkout HEAD -- src/feature.ts tests/\n3. DOCUMENT in .claude/RECOVERY.md:\n   - Pattern: All async implementations cause timing issues\n   - Tests expect synchronous behavior\n   - Hypothesis: Requirements may need async, tests don't handle it\n4. ASK USER:\n   \"I've tried 3 async implementations. All cause timing issues.\n    Tests expect synchronous behavior.\n\n    This suggests either:\n    A) Feature should actually be synchronous (simpler)\n    B) Tests need updating for async behavior\n\n    Which direction should I take?\"\n```\n\n### When to Trigger Recovery\n\nTrigger after 3 failures when:\n- Same test keeps failing despite different approaches\n- Different tests fail in pattern (suggests wrong approach)\n- Tests pass locally but fail in CI\n- Implementation works but breaks unrelated tests\n\nDon't wait for max iterations - trigger early when pattern emerges.\n\n## If Max Iterations Reached\n\nRalph exits after max iterations. **Still do NOT ask user to manually test.**\n\nMain chat should:\n1. **Summarize** what's failing (from LEARNINGS.md)\n2. **Report** which automated tests fail and why\n3. **Ask user** for direction:\n   - A) Start new loop with different approach\n   - B) Add more logging to debug\n   - C) User provides guidance\n   - D) User explicitly requests manual testing\n\n**Never default to \"please test manually\".** Always exhaust automation first.\n\n## No Pause Between Tasks\n\n<EXTREMELY-IMPORTANT>\n**After completing task N, IMMEDIATELY start task N+1. Do NOT pause.**\n\n| Thought | Reality |\n|---------|---------|\n| \"Task done, let me check in with user\" | NO. User wants ALL tasks done. Keep going. |\n| \"User might want to review\" | User will review at the END. Continue. |\n| \"Natural pause point\" | Only pause when ALL tasks complete or blocked. |\n| \"Let me summarize progress\" | Summarize AFTER all tasks. Keep moving. |\n| \"User has been waiting\" | User is waiting for COMPLETION, not updates. |\n\nThe promise signals task completion. After outputting promise, IMMEDIATELY start next task's loop.\n\n**Pausing between tasks is procrastination disguised as courtesy.**\n</EXTREMELY-IMPORTANT>\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After ALL tasks complete with passing tests:\n```\nSkill(skill=\"workflows:dev-review\")\n```\n\nDo NOT proceed until automated tests pass for every task."
              },
              {
                "name": "dev-ralph-loop",
                "description": "Per-task ralph loop pattern for implementation and debugging. One loop per task, not one loop per feature.",
                "path": "skills/dev-ralph-loop/SKILL.md",
                "frontmatter": {
                  "name": "dev-ralph-loop",
                  "description": "Per-task ralph loop pattern for implementation and debugging. One loop per task, not one loop per feature."
                },
                "content": "**Announce:** \"I'm using dev-ralph-loop to set up verification loops.\"\n\n<EXTREMELY-IMPORTANT>\n## Load TDD Enforcement (REQUIRED)\n\nBefore starting ANY ralph loop, you MUST load the TDD skill to remember the testing gates and task reframing:\n\n```\nSkill(skill=\"workflows:dev-tdd\")\n```\n\nThis loads:\n- Task reframing (your job is writing tests, not features)\n- The Execution Gate (6 mandatory gates before E2E testing)\n- GATE 5: READ LOGS (mandatory - cannot skip)\n- The Iron Law of TDD (test-first approach)\n\n**Read dev-tdd skill content now before proceeding with ralph loops.**\n</EXTREMELY-IMPORTANT>\n\n## Contents\n\n- [The Iron Law](#the-iron-law-of-ralph-loops)\n- [Per-Task Pattern](#the-per-task-pattern)\n- [Starting a Loop](#starting-a-loop)\n- [Inside the Loop](#inside-the-loop)\n- [Completing a Loop](#completing-a-loop)\n- [Example: Multi-Task Feature](#example-multi-task-feature)\n- [Rationalizations](#rationalization-prevention)\n\n# Ralph Loop Pattern\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Ralph Loops\n\n**ONE LOOP PER TASK. NOT ONE LOOP PER FEATURE. This is not negotiable.**\n\nA single feature-level loop provides ZERO per-task enforcement. You can just move to the next task without the loop actually gating anything.\n\nEach task in PLAN.md gets its own ralph loop with its own completion promise.\n</EXTREMELY-IMPORTANT>\n\n## The Per-Task Pattern\n\n```\nFor task N in PLAN.md (1, 2, 3, ...):\n    1. Start ralph loop for task N\n    2. Inside loop: spawn Task agents, iterate until done\n    3. Output promise → loop ends\n    4. Move to task N+1, start NEW ralph loop\n```\n\n**Why per-task loops?**\n- Feature loops don't enforce task completion\n- Without a loop per task, you can just... move on\n- Each task needs its own completion gate\n- The promise is your proof that the task is done\n\n## Starting a Loop\n\n**IMPORTANT:** Avoid parentheses `()` in the prompt - they break zsh argument parsing.\nUse dashes or brackets instead.\n\n### For Implementation Tasks\n\n```\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Task N: [TASK NAME] --max-iterations 10 --completion-promise TASKN_DONE\")\n```\n\n### For Debug Tasks\n\n```\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Debug: [SYMPTOM] --max-iterations 15 --completion-promise FIXED\")\n```\n\n### Parameters\n\n| Parameter | Purpose | Recommendation |\n|-----------|---------|----------------|\n| Prompt | What this loop is for | Be specific: \"Task 2: Add auth service\" |\n| `--max-iterations` | Safety limit | 10 for implementation, 15 for debugging |\n| `--completion-promise` | The completion gate | Unique per task: TASK1_DONE, TASK2_DONE, etc. |\n\n## Inside the Loop\n\nEach iteration follows this pattern:\n\n### 1. Spawn Task Agent\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\n[TASK-SPECIFIC INSTRUCTIONS]\n\nContext:\n- Read .claude/LEARNINGS.md for prior attempts\n- Read .claude/SPEC.md for requirements\n- Read .claude/PLAN.md for approach\n\nReport back: what was done, results, any blockers.\n\"\"\")\n```\n\n### 2. Verify Results\n\nAfter Task agent returns:\n- Check if the work is actually complete\n- Verify tests pass (for implementation)\n- Verify bug is fixed (for debugging)\n\n### 3. Decide: Promise or Iterate\n\n**If complete:** Output the promise\n```\n<promise>TASKN_DONE</promise>\n```\n\n**If incomplete:** Do NOT output promise. Spawn another Task agent to continue.\n\n<EXTREMELY-IMPORTANT>\n## Promise Rules\n\n**You may ONLY output the promise when the statement is COMPLETELY AND UNEQUIVOCALLY TRUE.**\n\nThe promise is a claim that:\n- For implementation: \"This task's tests pass. The implementation is complete.\"\n- For debugging: \"The bug is fixed. Regression test passes.\"\n\nYou may NOT output the promise to:\n- \"Move on\" to the next task\n- \"Try something else\"\n- Skip verification\n\n**If the promise isn't true, don't output it. Keep iterating.**\n</EXTREMELY-IMPORTANT>\n\n## Completing a Loop\n\nWhen you output the promise, the ralph loop ends. Then:\n\n1. Document completion in LEARNINGS.md\n2. Move to the next task\n3. Start a NEW ralph loop for that task\n\n## Example: Multi-Task Feature\n\n```\n## Task 1: Create types\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Task 1: Create types --max-iterations 5 --completion-promise TASK1_DONE\")\n\n[Spawn Task agent → implements types]\n[Verify: tsc --noEmit passes]\n\n<promise>TASK1_DONE</promise>\n\n## Task 2: Add service method\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Task 2: Add service method --max-iterations 10 --completion-promise TASK2_DONE\")\n\n[Spawn Task agent → implements method]\n[Verify: tests fail → iterate]\n[Spawn Task agent → fixes tests]\n[Verify: tests pass]\n\n<promise>TASK2_DONE</promise>\n\n## Task 3: Add route handler\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Task 3: Add route handler --max-iterations 10 --completion-promise TASK3_DONE\")\n\n[Spawn Task agent → implements route]\n[Verify: integration test passes]\n\n<promise>TASK3_DONE</promise>\n\n## All tasks complete\n```\n\n## Rationalization Prevention\n\nThese thoughts mean STOP—you're about to skip enforcement:\n\n| Thought | Reality |\n|---------|---------|\n| \"One loop for the whole feature\" | NO. One loop PER TASK. Feature loops don't enforce. |\n| \"I'll just move to the next task\" | Did the current task's loop complete? If no loop, no gate. |\n| \"Per-task loops are overhead\" | Per-task loops are the ONLY enforcement. |\n| \"Ralph is for hard problems\" | Ralph is for ALL tasks. Simple tasks need gates too. |\n| \"I'll iterate without the loop\" | Without ralph, you'll declare done prematurely. |\n| \"The ceremony isn't worth it\" | The ceremony IS the value. It prevents shortcuts. |\n| \"I'll cherry-pick the parts I need\" | Skills are protocols, not menus. Follow all of it. |\n| \"Tests passed on first try, skip loop\" | Still need the loop structure. Lucky ≠ verified. |\n| \"Task done, let me check in\" | NO. Start next task's loop immediately. |\n| \"User might want to review\" | User wants ALL tasks done. Keep going. |\n| \"Natural pause point\" | Only pause when ALL tasks complete. |\n\n**Each task needs its own ralph loop. One feature loop provides ZERO per-task enforcement.**\n\n**After outputting a promise, IMMEDIATELY start the next task's loop. Do NOT pause for user review.**\n\n## When NOT to Use Ralph Loops\n\nRalph loops are for:\n- Implementation tasks (dev-implement)\n- Bug fixes (dev-debug)\n\nRalph loops are NOT for:\n- Exploration (dev-explore)\n- Design (dev-design)\n- Data science (ds uses output-first verification instead)\n- Review phases (dev-review, ds-review)\n\n## Integration\n\nThis skill is invoked by:\n- `dev-implement` - for implementation tasks\n- `dev-debug` - for bug investigation and fixes\n\nAfter all tasks complete, proceed to the next phase of the parent workflow."
              },
              {
                "name": "dev-review",
                "description": "This skill should be used as REQUIRED Phase 6 of /dev workflow when the implementation is complete and needs code review. Combines spec compliance and code quality checks with confidence-based filtering.",
                "path": "skills/dev-review/SKILL.md",
                "frontmatter": {
                  "name": "dev-review",
                  "description": "This skill should be used as REQUIRED Phase 6 of /dev workflow when the implementation is complete and needs code review. Combines spec compliance and code quality checks with confidence-based filtering."
                },
                "content": "## Contents\n\n- [Prerequisites - Test Output Gate](#prerequisites---test-output-gate)\n- [The Iron Law of Review](#the-iron-law-of-review)\n- [Red Flags - STOP Immediately If You Think](#red-flags---stop-immediately-if-you-think)\n- [Review Focus Areas](#review-focus-areas)\n- [Confidence Scoring](#confidence-scoring)\n- [Required Output Structure](#required-output-structure)\n- [Agent Invocation](#agent-invocation)\n- [Quality Standards](#quality-standards)\n\n# Code Review\n\nSingle-pass code review combining spec compliance and quality checks. Uses confidence-based filtering to report only high-priority issues.\n\n<EXTREMELY-IMPORTANT>\n## Prerequisites - Test Output Gate\n\n**Do NOT start review without test evidence.**\n\nBefore reviewing, verify these preconditions:\n1. `.claude/LEARNINGS.md` contains **actual test output**\n2. Tests were **run** (not just written)\n3. Test output shows **PASS** (not SKIP, not assumed)\n\n### What Counts as Test Evidence\n\n| Valid Evidence | NOT Valid |\n|----------------|-----------|\n| `meson test` output with results | \"Tests should pass\" |\n| `pytest` output showing PASS | \"I wrote tests\" |\n| Screenshot of working UI | \"It looks correct\" |\n| Playwright snapshot showing expected state | \"User can verify\" |\n| D-Bus command output | \"The feature works\" |\n| **E2E test output with user flow verified** | **\"Unit tests pass\" (for UI changes)** |\n\n<EXTREMELY-IMPORTANT>\n### The E2E Evidence Requirement\n\n**FOR USER-FACING CHANGES: Unit test evidence is INSUFFICIENT.**\n\nBefore approving user-facing changes, verify:\n1. Unit tests pass (necessary but not sufficient)\n2. **E2E tests pass** (required for approval)\n3. Visual evidence exists (screenshots/snapshots for UI)\n\n| Change Type | Unit Evidence | E2E Evidence | Approval? |\n|-------------|---------------|--------------|------------|\n| Internal refactor | ✅ | N/A | ✅ APPROVE |\n| API change | ✅ | ❌ Missing | ❌ BLOCKED |\n| UI change | ✅ | ❌ Missing | ❌ BLOCKED |\n| User workflow | ✅ | ❌ Missing | ❌ BLOCKED |\n\nReturn BLOCKED if E2E evidence is missing for user-facing changes.\n\n\"Unit tests pass\" without E2E for UI changes is NOT approvable.\n</EXTREMELY-IMPORTANT>\n\n### Gate Check\n\nCheck LEARNINGS.md for test output:\n\n```bash\nrg -E \"(PASS|OK|SUCCESS|\\d+ passed)\" .claude/LEARNINGS.md\n```\n\nIf no test output is found, STOP and return to /dev-implement.\n\n\"It should work\" is NOT evidence. Test output IS evidence.\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Review\n\n**You MUST report only issues with >= 80% confidence. This is not negotiable.**\n\nBefore reporting ANY issue, complete these verification steps:\n1. Verify it's not a false positive\n2. Verify it's not a pre-existing issue\n3. Assign a confidence score\n4. Report only if score >= 80\n\nYou MUST apply this rule even when encountering:\n- \"This looks suspicious\"\n- \"I think this might be wrong\"\n- \"The style seems inconsistent\"\n- \"I would have done it differently\"\n\nYou MUST discard any low-confidence issue found during review.\n</EXTREMELY-IMPORTANT>\n\n## Red Flags - STOP Immediately If You Think:\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"Tests probably pass\" | You don't have evidence - absence of evidence is not evidence | Check LEARNINGS.md for actual output |\n| \"This looks wrong\" | Your vague suspicion ≠ evidence | Find concrete proof or discard |\n| \"I would do it differently\" | Your style preference ≠ bug | Check if it violates project guidelines |\n| \"This might cause problems\" | Your \"might\" = < 80% confidence | Find proof or discard |\n| \"Pre-existing but should be fixed\" | You're out of scope | Score it 0 and discard |\n| \"User can test it\" | Your manual testing is less reliable than automation | Return to implement phase |\n\n## Review Focus Areas\n\n### Test Evidence (Check First!)\n- [ ] LEARNINGS.md contains actual test command output\n- [ ] Tests show PASS/OK (not SKIP, FAIL, or missing)\n- [ ] UI changes have screenshot/snapshot evidence\n- [ ] All test types run (unit, integration, UI as applicable)\n- [ ] E2E tests exist and pass for user-facing changes\n- [ ] E2E test simulates actual user flow, not just component render\n\n### Spec Compliance\n- [ ] All requirements from .claude/SPEC.md are implemented\n- [ ] Acceptance criteria are met\n- [ ] No requirements were skipped or partially implemented\n- [ ] Edge cases mentioned in spec are handled\n\n### Code Quality\n- [ ] Code is simple and DRY (no unnecessary duplication)\n- [ ] Logic is correct (no bugs, handles edge cases)\n- [ ] Codebase conventions followed (naming, patterns, structure)\n- [ ] Error handling is complete\n- [ ] No security vulnerabilities detected\n\n## Confidence Scoring\n\nRate each potential issue from 0-100:\n\n| Score | Meaning |\n|-------|---------|\n| 0 | False positive or pre-existing issue |\n| 25 | Might be real, might not. Stylistic without guideline backing |\n| 50 | Real issue but nitpick or rare in practice |\n| 75 | Verified real issue, impacts functionality |\n| 100 | Absolutely certain, confirmed with direct evidence |\n\n**CRITICAL: Only report issues with confidence >= 80.**\n\n## Required Output Structure\n\n```markdown\n## Code Review: [Feature/Change Name]\nReviewing: [files/scope being reviewed]\n\n### Test Evidence Verified\n- Unit tests: [PASS/FAIL/MISSING] - [paste key output line]\n- Integration: [PASS/FAIL/N/A]\n- UI/Visual: [Screenshot taken / Snapshot verified / N/A]\n\n### Critical Issues (Confidence >= 90)\n\n#### [Issue Title] (Confidence: XX)\n\n**Location:** `file/path.ext:line_number`\n\n**Problem:** Clear description of the issue\n\n**Fix:**\n```[language]\n// Specific code fix\n```\n\n### Important Issues (Confidence 80-89)\n\n[Same format as Critical Issues]\n\n### Summary\n\n**Verdict:** APPROVED | CHANGES REQUIRED | BLOCKED (no test evidence)\n\n[If APPROVED]\nThe reviewed code meets project standards. Tests pass. No issues with confidence >= 80 detected.\n\n[If CHANGES REQUIRED]\nX critical issues and Y important issues must be addressed before proceeding.\n\n[If BLOCKED]\nCannot approve without test evidence. Return to /dev-implement and run tests.\n```\n\n## Agent Invocation\n\nSpawn Task agent for review execution:\n\n```\nTask(subagent_type=\"general-purpose\"):\n\"Review implementation against .claude/SPEC.md.\n\nFIRST: Check .claude/LEARNINGS.md for test output.\nReturn BLOCKED immediately if no test output is found.\n\nComplete single-pass review covering:\n1. Test evidence - tests actually run and pass?\n2. Spec compliance - all requirements met?\n3. Code quality - simple, correct, follows conventions?\n\nConfidence score each issue (0-100).\nReport only issues with >= 80 confidence.\nReturn structured output per /dev-review format.\"\n```\n\n## Honesty Requirement\n\n<EXTREMELY-IMPORTANT>\n**You approving without test evidence is LYING.**\n\nAn \"APPROVED\" verdict means YOU assert:\n- Tests actually ran (not \"should work\")\n- Test output shows PASS (not SKIP, not assumed)\n- Evidence exists and YOU verified it (not trusted reports)\n\nYou approving without test evidence is not \"efficiency\" - it is LYING about code quality.\n\n**BLOCKED is honest. Your fake APPROVED is fraud.**\n</EXTREMELY-IMPORTANT>\n\n## Rationalization Prevention\n\nSTOP - you're about to rationalize if these thoughts arise—they indicate dishonest approval:\n\n| Thought | Reality |\n|---------|---------|\n| \"Tests probably pass\" | Your probably ≠ evidence. Check LEARNINGS.md. |\n| \"I saw the code, it looks right\" | Your looking ≠ running. Find test output. |\n| \"User is waiting for approval\" | They want honest approval. You return BLOCKED if needed. |\n| \"It's a small change\" | Your size estimate doesn't matter. Small changes break things. Require evidence. |\n| \"I trust the implementer\" | Your trust doesn't replace verification. You verify evidence. |\n| \"I'll approve and they can fix later\" | You block now or bugs ship to users. |\n| \"Review is just a formality\" | Review is the LAST GATE before bugs ship. You execute seriously. |\n\n## Quality Standards\n\n- **Test evidence is mandatory** - do not approve without test output\n- Do not report style preferences lacking project guideline backing\n- Do not report pre-existing issues (confidence = 0)\n- Make each reported issue immediately actionable\n- Use absolute file paths with line numbers in reports\n- Treat uncertainty as below 80 confidence\n\n## Phase Complete\n\nAfter review completes:\n\n**If APPROVED:** Immediately invoke the dev-verify skill:\n```\nSkill(skill=\"workflows:dev-verify\")\n```\n\n**If CHANGES REQUIRED:** Return to `/dev-implement` to fix reported issues.\n\n**If BLOCKED:** Return to `/dev-implement` to collect test evidence."
              },
              {
                "name": "dev-tdd",
                "description": "This skill should be used when the user asks to \"implement using TDD\", \"test-driven development\", \"RED-GREEN-REFACTOR\", or \"write failing test first\". Enforces test-first approach with RED-GREEN-REFACTOR cycle and execution-based verification.",
                "path": "skills/dev-tdd/SKILL.md",
                "frontmatter": {
                  "name": "dev-tdd",
                  "description": "This skill should be used when the user asks to \"implement using TDD\", \"test-driven development\", \"RED-GREEN-REFACTOR\", or \"write failing test first\". Enforces test-first approach with RED-GREEN-REFACTOR cycle and execution-based verification.",
                  "version": "0.1.0"
                },
                "content": "## Contents\n\n- [The Iron Law](#the-iron-law-of-tdd)\n- [The TDD Cycle](#the-tdd-cycle)\n- [What Counts as a Test](#what-counts-as-a-test)\n- [Logging TDD Progress](#logging-tdd-progress)\n- [Red Flags - Thoughts That Mean STOP](#red-flags---thoughts-that-mean-stop)\n- [Delete & Restart](#delete--restart)\n- [E2E Test Requirement](#e2e-test-requirement)\n\n# Test-Driven Development\n\n<EXTREMELY-IMPORTANT>\n## Task Reframing: What Your Job Actually Is\n\n**Your job is NOT to implement features. Your job is to write tests that prove features work.**\n\nReframe every task:\n- ❌ \"Implement user login\"\n- ✅ \"Write a test that proves user login works. Then make it pass.\"\n\n- ❌ \"Fix the icon rendering bug\"\n- ✅ \"Write a test that fails when icons render wrong. Then fix it.\"\n\n**The test IS your deliverable. The implementation just makes the test pass.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## File-Based Logging (MANDATORY)\n\n**ALL CODE MUST USE FILE-BASED LOGGING.**\n\nEvery application you write MUST redirect output to a log file:\n- CLI apps: `./app > /tmp/app.log 2>&1 &`\n- GUI apps: `./app --log-file=/tmp/app.log 2>&1 &`\n- Test runners: `pytest -v > /tmp/test.log 2>&1`\n\n**Why:** Without log files, you have NO EVIDENCE of what happened. \"I saw it in terminal\" is not verification.\n\n**Read the full requirements:** `@references/logging-requirements.md`\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Execution Gate (MANDATORY)\n\n**NO E2E TESTS WITHOUT PASSING THE EXECUTION GATE FIRST.**\n\nBefore running E2E tests or taking screenshots, you MUST complete all 6 gates in order:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN AND ONLY THEN: E2E tests/screenshots\n```\n\n**Key enforcement:**\n- If you catch yourself thinking \"let me take a screenshot\" → STOP, you skipped gates 1-6\n- If process is running → READ LOGS (GATE 5) before testing\n- Logs come BEFORE screenshots, not after\n\n**For GUI applications:**\n- Screenshot WINDOW ONLY (not whole screen)\n- When testing specific feature (toolbar icons), crop to THAT REGION only\n- Whole screen = false conclusions from other apps\n\n**Read the complete gate sequence:** `@references/execution-gates.md`\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of TDD\n\n**YOU MUST WRITE THE FAILING TEST FIRST. YOU MUST SEE IT FAIL. This is not negotiable.**\n\nBefore writing ANY implementation code:\n1. You write a test that will fail (because the feature doesn't exist yet)\n2. You run the test and **SEE THE FAILURE OUTPUT** (RED)\n3. You document in LEARNINGS.md: \"RED: [test name] fails with [error message]\"\n4. Only THEN you write implementation code\n5. You run the test again, **SEE IT PASS** (GREEN)\n6. You document: \"GREEN: [test name] now passes\"\n\n**The RED step is not optional. If the test hasn't failed, you haven't practiced TDD.**\n</EXTREMELY-IMPORTANT>\n\n## The TDD Cycle\n\n```\nRED → Write test → Run through GATES → See failure → Read logs → Document\nGREEN → Minimal code → Run through GATES → See pass → Read logs → Document\nREFACTOR → Clean up while staying green\n```\n\n### Step 1: RED - Write Failing Test\n\n```python\n# Write the test FIRST\ndef test_user_can_login():\n    result = login(\"user@example.com\", \"password123\")\n    assert result.success == True\n    assert result.token is not None\n```\n\nRun the test through the execution gates:\n\n```bash\n# For unit tests, minimum gates are: EXECUTE + READ OUTPUT\npytest tests/test_auth.py::test_user_can_login -v 2>&1 | tee /tmp/test.log\n# pytest: run specific test and see RED failure\n\n# READ the output (MANDATORY)\ncat /tmp/test.log\n```\n\nOutput will show:\n```\nFAILED - NameError: name 'login' is not defined\n```\n\n**Log to LEARNINGS.md:**\n```markdown\n## RED: test_user_can_login\n- Test written\n- Ran through gates (pytest executed, output read)\n- Fails with: NameError: name 'login' is not defined\n- Expected: function doesn't exist yet\n```\n\n### Step 2: GREEN - Minimal Implementation\n\nWrite the **minimum code** to make the test pass:\n\n```python\ndef login(email: str, password: str) -> LoginResult:\n    # Minimal implementation\n    return LoginResult(success=True, token=\"dummy-token\")\n```\n\nRun the test through gates again:\n\n```bash\npytest tests/test_auth.py::test_user_can_login -v 2>&1 | tee /tmp/test.log\n# pytest: run test again and see GREEN success\n\n# READ the output (MANDATORY)\ncat /tmp/test.log\n```\n\nOutput will show:\n```\nPASSED\n```\n\n**Log to LEARNINGS.md:**\n```markdown\n## GREEN: test_user_can_login\n- Minimal login() implemented\n- Ran through gates (pytest executed, output read)\n- Test passes\n- No errors in output\n- Ready for refactor\n```\n\n### Step 3: REFACTOR - Improve While Green\n\nClean up the code while keeping tests passing:\n\n```python\ndef login(email: str, password: str) -> LoginResult:\n    user = User.find_by_email(email)\n    if user and user.check_password(password):\n        return LoginResult(success=True, token=generate_token(user))\n    return LoginResult(success=False, token=None)\n```\n\nVerify tests remain green after refactoring:\n\n```bash\npytest tests/test_auth.py -v\n# pytest: run all tests and verify GREEN after refactor\n```\n\nOutput will show:\n```\nAll tests PASSED\n```\n\n## What Counts as a Test\n\n<EXTREMELY-IMPORTANT>\n### REAL Tests vs FAKE \"Tests\"\n\n| REAL TEST (execute + verify) | FAKE \"TEST\" (NEVER ACCEPTABLE) |\n|------------------------------|--------------------------------|\n| pytest calls function, asserts return | grep for function exists |\n| Playwright clicks button, checks DOM | ast-grep finds pattern |\n| ydotool types input, screenshot verifies | Log says \"success\" |\n| CLI invocation checks stdout | \"Code looks correct\" |\n| API request verifies response body | \"I'm confident it works\" |\n\n**THE TEST MUST EXECUTE THE CODE AND VERIFY RUNTIME BEHAVIOR.**\n\nGrepping is NOT testing. Log reading is NOT testing. Code review is NOT testing.\n</EXTREMELY-IMPORTANT>\n\n### Why Grepping is Not Testing\n\n| Fake Approach | Why It's Worthless | What Happens |\n|---------------|-------------------|--------------|\n| `grep \"function_name\"` | Proves function exists, not that it works | Bug ships |\n| `ast-grep pattern` | Proves structure matches, not behavior | Runtime crash |\n| \"Log says success\" | Log was written, code might not run | Silent failure |\n| \"Code review passed\" | Human opinion, not execution | Edge cases missed |\n\n## Logging TDD Progress\n\nDocument every TDD cycle in `.claude/LEARNINGS.md`:\n\n```markdown\n## TDD Cycle: [Feature/Test Name]\n\n### RED\n- **Test:** `test_feature_works()`\n- **Command:**\n\n```bash\npytest tests/test_feature.py::test_feature_works -v\n# pytest: run test and observe RED failure\n```\n\n- **Output:**\n```\nFAILED - AssertionError: expected True, got None\n```\n- **Expected:** Feature not implemented yet\n\n### GREEN\n- **Implementation:** Added `feature_works()` function\n- **Command:**\n\n```bash\npytest tests/test_feature.py::test_feature_works -v\n# pytest: run test and verify GREEN success\n```\n\n- **Output:**\n```\nPASSED\n```\n\n### REFACTOR\n- Extracted helper function\n- Added type hints\n- Verify tests still pass:\n\n```bash\npytest tests/test_feature.py -v\n# pytest: run all tests and confirm GREEN after refactor\n```\n```\n\n## Red Flags - Thoughts That Mean STOP\n\nIf you catch yourself thinking these thoughts—STOP. They're indicators you're about to skip TDD:\n\n| Thought | Reality |\n|---------|---------|\n| \"Write the test after\" | You're about to do verification, not TDD. You MUST test FIRST. |\n| \"This is too simple for TDD\" | Your simple code benefits MOST from TDD. |\n| \"Just fix this quickly\" | Your speed isn't the goal. Your correctness is. |\n| \"Know the test will fail\" | You knowing isn't the same as you seeing it fail. You MUST RUN it, see RED. |\n| \"Grep confirms it exists\" | Your existence check ≠ working code. You MUST execute the code. |\n| \"Already have the code\" | You MUST DELETE IT. You write test first, then reimplement. |\n| \"Test passed on first run\" | Suspicious. Did you actually see RED first? |\n\n**If your test doesn't fail first, you haven't practiced TDD.**\n\n## Delete & Restart\n\n<EXTREMELY-IMPORTANT>\n**Wrote implementation code before test? You MUST DELETE IT. No exceptions.**\n\nWhen you discover implementation code that wasn't driven by a test:\n1. **DELETE** your implementation code\n2. **WRITE** the test first\n3. **RUN** it, **SEE RED**\n4. **REWRITE** the implementation\n\n\"But it works\" is not an excuse. \"But it would waste your time\" is not an excuse.\n\n**Code you wrote without TDD is UNTRUSTED code. You delete it and do it right.**\n</EXTREMELY-IMPORTANT>\n\n## E2E Test Requirement\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of E2E in TDD\n\n**USER-FACING FEATURES REQUIRE E2E TESTS IN ADDITION TO UNIT TESTS.**\n\nTDD cycle for user-facing changes:\n\n```\nUnit TDD:     RED → GREEN → REFACTOR\n                    ↓\nE2E TDD:      RED → GREEN → REFACTOR\n```\n\n**Both cycles must complete. Unit GREEN does not mean DONE.**\n\n### When E2E is Required\n\n| Change Type | Unit Tests | E2E Required? |\n|-------------|------------|---------------|\n| Internal logic | Yes | No |\n| API endpoint | Yes | Yes (test full request/response) |\n| UI component | Yes | **Yes** (Playwright/automation) |\n| CLI command | Yes | Yes (test actual invocation) |\n| User workflow | Yes | **Yes** (simulate user actions) |\n| Visual change | Yes | **Yes** (screenshot comparison) |\n\n### E2E TDD Cycle\n\n1. **RED**: Write E2E test simulating user action\n   - Run through ALL 6 GATES (BUILD → LAUNCH → WAIT → CHECK → READ LOGS → VERIFY LOGS)\n   - Only after GATE 6: Run the E2E test\n   - Observe the failure (feature doesn't exist)\n   - Document: \"E2E RED: [test] fails with [error]. All gates passed, logs clean.\"\n\n2. **GREEN**: Implement to make E2E pass (unit tests already green)\n   - Run through ALL 6 GATES again\n   - Only after GATE 6: Run the E2E test\n   - Verify the pass\n   - Document: \"E2E GREEN: [test] passes. All gates passed, logs clean.\"\n\n3. **REFACTOR**: Ensure both unit and E2E stay green\n   - Continue running through gates for each test run\n\n### Delete & Restart (E2E)\n\n**You shipped user-facing code without E2E test? You MUST WRITE ONE NOW.**\n\nRetroactive E2E is better than no E2E. But next time: You write E2E FIRST.\n</EXTREMELY-IMPORTANT>\n\n## Integration\n\nThis skill is invoked by:\n- `dev-implement` - for TDD during implementation\n- `dev-debug` - for regression tests during debugging\n\nFor testing tool options (Playwright, ydotool, etc.), see:\n```\nSkill(skill=\"workflows:dev-test\")\n```"
              },
              {
                "name": "dev-test-chrome",
                "description": "Chrome MCP browser testing. Console/network debugging, JS execution, GIF recording.",
                "path": "skills/dev-test-chrome/SKILL.md",
                "frontmatter": {
                  "name": "dev-test-chrome",
                  "description": "Chrome MCP browser testing. Console/network debugging, JS execution, GIF recording."
                },
                "content": "**Announce:** \"I'm using dev-test-chrome for Chrome browser automation with debugging.\"\n\n<EXTREMELY-IMPORTANT>\n## Gate Reminder\n\nBefore taking screenshots or running E2E tests, you MUST complete all 6 gates from dev-tdd:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN: E2E tests/screenshots\n```\n\n**You loaded dev-tdd earlier. Follow the gates now.**\n</EXTREMELY-IMPORTANT>\n\n## Contents\n\n- [Tool Availability Gate](#tool-availability-gate)\n- [When to Use Chrome MCP](#when-to-use-chrome-mcp)\n- [MCP Tools Overview](#mcp-tools-overview)\n- [Console Debugging](#console-debugging)\n- [Network Request Inspection](#network-request-inspection)\n- [JavaScript Execution](#javascript-execution)\n- [Navigation & Interaction](#navigation--interaction)\n- [GIF Recording](#gif-recording)\n- [Complete E2E Examples](#complete-e2e-examples)\n\n# Chrome MCP Browser Automation\n\n<EXTREMELY-IMPORTANT>\n## Tool Availability Gate\n\n**Verify Chrome MCP tools are available before proceeding.**\n\nCheck for these MCP functions:\n- `mcp__claude-in-chrome__read_page`\n- `mcp__claude-in-chrome__navigate`\n- `mcp__claude-in-chrome__read_console_messages`\n- `mcp__claude-in-chrome__read_network_requests`\n\n**If MCP tools are not available:**\n```\nSTOP: Cannot proceed with Chrome MCP automation.\n\nMissing: Chrome MCP server (claude-in-chrome extension)\n\nThe Chrome MCP requires:\n1. Chrome browser with claude-in-chrome extension installed\n2. Extension connected to Claude Code\n3. Browser window visible (not headless)\n\nCheck your Claude Code MCP configuration.\n\nReply when configured and I'll continue testing.\n```\n\n**This gate is non-negotiable. Missing tools = full stop.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## When to Use Chrome MCP\n\n**USE Chrome MCP when you need:**\n- Console message debugging (`console.log`, `console.error`)\n- Network request inspection (API calls, XHR, Fetch)\n- JavaScript execution in page context\n- GIF recording of interactions\n- Interactive debugging with real browser\n- Natural language element finding\n\n**DO NOT use Chrome MCP when:**\n- Running in CI/CD (requires visible browser)\n- Cross-browser testing needed (Chrome only)\n- Headless automation required\n\n**For CI/CD and headless, use:** `Skill(skill=\"workflows:dev-test-playwright\")`\n\n### Rationalization Prevention\n\n| Thought | Reality |\n|---------|---------|\n| \"I'll check the console manually\" | NO. Use `read_console_messages` |\n| \"I can infer what the API returns\" | NO. Use `read_network_requests` |\n| \"I'll just look at DevTools\" | AUTOMATE IT. Chrome MCP captures the same data |\n| \"Chrome MCP works for CI\" | NO. It requires visible browser. Use Playwright. |\n| \"Recording a GIF is overkill\" | GIFs prove interactions worked. Record them. |\n</EXTREMELY-IMPORTANT>\n\n## MCP Tools Overview\n\n| Tool | Purpose |\n|------|---------|\n| `navigate` | Navigate to URL |\n| `read_page` | Get accessibility tree (page state) |\n| `find` | Natural language element search |\n| `computer` | Mouse/keyboard (click, type, scroll, screenshot) |\n| `form_input` | Set form values |\n| `javascript_tool` | Execute JS in page context |\n| `read_console_messages` | Read browser console |\n| `read_network_requests` | Read HTTP requests |\n| `get_page_text` | Extract page text content |\n| `gif_creator` | Record interactions as GIF |\n| `tabs_context_mcp` | Get tab context |\n| `tabs_create_mcp` | Create new tab |\n\n## Console Debugging\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of Console Debugging\n\n**DO NOT manually check console. Use `read_console_messages`.**\n\nIf JavaScript errors exist, you MUST capture them automatically.\n</EXTREMELY-IMPORTANT>\n\n### Reading Console Messages\n\n```\nmcp__claude-in-chrome__read_console_messages(\n    tabId=TAB_ID,\n    pattern=\"error|warning\"  # Filter by regex pattern\n)\n```\n\n### Pattern Filtering (Required)\n\n**Always provide a pattern** to avoid verbose output:\n\n| Pattern | Captures |\n|---------|----------|\n| `\"error\"` | All error messages |\n| `\"error\\|warning\"` | Errors and warnings |\n| `\"MyApp\"` | Application-specific logs |\n| `\"API\"` | API-related messages |\n| `\"fetch\\|xhr\"` | Network-related logs |\n\n### Example: Debug JavaScript Error\n\n```\n# 1. Navigate to page\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://app.example.com\")\n\n# 2. Trigger the action\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, coordinate=[500, 300])\n\n# 3. Check for errors\nmcp__claude-in-chrome__read_console_messages(\n    tabId=TAB_ID,\n    pattern=\"error|Error|ERROR\",\n    onlyErrors=true\n)\n```\n\n### Clearing Console Between Tests\n\n```\nmcp__claude-in-chrome__read_console_messages(\n    tabId=TAB_ID,\n    pattern=\".*\",\n    clear=true  # Clear after reading\n)\n```\n\n## Network Request Inspection\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of API Debugging\n\n**DO NOT guess API responses. Use `read_network_requests`.**\n\nIf debugging API calls, you MUST capture actual requests and responses.\n</EXTREMELY-IMPORTANT>\n\n### Reading Network Requests\n\n```\nmcp__claude-in-chrome__read_network_requests(\n    tabId=TAB_ID,\n    urlPattern=\"/api/\"  # Filter by URL pattern\n)\n```\n\n### URL Pattern Filtering\n\n| Pattern | Captures |\n|---------|----------|\n| `\"/api/\"` | All API calls |\n| `\"graphql\"` | GraphQL requests |\n| `\"auth\"` | Authentication requests |\n| `\"example.com\"` | Requests to specific domain |\n\n### Example: Debug API Call\n\n```\n# 1. Navigate and trigger action\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://app.example.com\")\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, ref=\"submit-button\")\n\n# 2. Wait for network activity\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=2)\n\n# 3. Inspect API calls\nmcp__claude-in-chrome__read_network_requests(\n    tabId=TAB_ID,\n    urlPattern=\"/api/submit\"\n)\n```\n\n### Clearing Network Log Between Tests\n\n```\nmcp__claude-in-chrome__read_network_requests(\n    tabId=TAB_ID,\n    clear=true\n)\n```\n\n## JavaScript Execution\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of JS Execution\n\n**DO NOT assume page state. Execute JS to verify.**\n\nIf you need to check page variables, DOM state, or run custom logic, use `javascript_tool`.\n</EXTREMELY-IMPORTANT>\n\n### Executing JavaScript\n\n```\nmcp__claude-in-chrome__javascript_tool(\n    action=\"javascript_exec\",\n    tabId=TAB_ID,\n    text=\"document.querySelector('#my-element').innerText\"\n)\n```\n\n### Common Use Cases\n\n**Get element text:**\n```\ntext=\"document.querySelector('.status').innerText\"\n```\n\n**Check if element exists:**\n```\ntext=\"document.querySelector('#login-button') !== null\"\n```\n\n**Get form values:**\n```\ntext=\"document.querySelector('input[name=email]').value\"\n```\n\n**Check localStorage:**\n```\ntext=\"localStorage.getItem('authToken')\"\n```\n\n**Get page data:**\n```\ntext=\"window.__APP_STATE__\"\n```\n\n**Trigger event:**\n```\ntext=\"document.querySelector('#btn').dispatchEvent(new Event('click'))\"\n```\n\n### Important Notes\n\n- Do NOT use `return` statements - just write the expression\n- Result of the last expression is returned automatically\n- Code runs in page context with access to DOM, window, etc.\n\n## Navigation & Interaction\n\n### Get Tab Context First\n\n```\n# Always start by getting available tabs\nmcp__claude-in-chrome__tabs_context_mcp(createIfEmpty=true)\n```\n\n### Navigation\n\n```\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://example.com\")\n```\n\n### Reading Page Structure\n\n```\n# Get accessibility tree\nmcp__claude-in-chrome__read_page(tabId=TAB_ID)\n\n# Get interactive elements only\nmcp__claude-in-chrome__read_page(tabId=TAB_ID, filter=\"interactive\")\n\n# Get specific element by ref\nmcp__claude-in-chrome__read_page(tabId=TAB_ID, ref_id=\"ref_123\")\n```\n\n### Finding Elements (Natural Language)\n\n```\nmcp__claude-in-chrome__find(\n    tabId=TAB_ID,\n    query=\"login button\"\n)\n```\n\n### Clicking Elements\n\n```\n# By coordinates\nmcp__claude-in-chrome__computer(\n    action=\"left_click\",\n    tabId=TAB_ID,\n    coordinate=[500, 300]\n)\n\n# By element ref (from read_page or find)\nmcp__claude-in-chrome__computer(\n    action=\"left_click\",\n    tabId=TAB_ID,\n    ref=\"ref_1\"\n)\n```\n\n### Typing Text\n\n```\nmcp__claude-in-chrome__computer(\n    action=\"type\",\n    tabId=TAB_ID,\n    text=\"hello@example.com\"\n)\n```\n\n### Form Input\n\n```\nmcp__claude-in-chrome__form_input(\n    tabId=TAB_ID,\n    ref=\"ref_1\",\n    value=\"user@example.com\"\n)\n```\n\n### Screenshots\n\n```\nmcp__claude-in-chrome__computer(\n    action=\"screenshot\",\n    tabId=TAB_ID\n)\n```\n\n### Waiting\n\n```\nmcp__claude-in-chrome__computer(\n    action=\"wait\",\n    tabId=TAB_ID,\n    duration=2  # seconds\n)\n```\n\n## GIF Recording\n\n<EXTREMELY-IMPORTANT>\n### When to Record GIFs\n\n**Record GIFs for multi-step interactions that need visual verification.**\n\nGIFs prove interactions worked. Screenshots only show end state.\n</EXTREMELY-IMPORTANT>\n\n### Recording Workflow\n\n```\n# 1. Start recording\nmcp__claude-in-chrome__gif_creator(action=\"start_recording\", tabId=TAB_ID)\n\n# 2. Take initial screenshot (first frame)\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# 3. Perform interactions\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, coordinate=[500, 300])\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=1)\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# ... more interactions with screenshots between ...\n\n# 4. Take final screenshot (last frame)\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# 5. Stop recording\nmcp__claude-in-chrome__gif_creator(action=\"stop_recording\", tabId=TAB_ID)\n\n# 6. Export GIF\nmcp__claude-in-chrome__gif_creator(\n    action=\"export\",\n    tabId=TAB_ID,\n    download=true,\n    filename=\"login_flow.gif\"\n)\n```\n\n### GIF Best Practices\n\n1. **Name meaningfully** - Use descriptive filenames like `checkout_flow.gif`\n2. **Capture extra frames** - Take screenshots before and after actions\n3. **Include wait time** - Allow animations to complete between screenshots\n\n## Complete E2E Examples\n\n### Login Flow with Console/Network Debugging\n\n```\n# 1. Get tab context\nmcp__claude-in-chrome__tabs_context_mcp(createIfEmpty=true)\n\n# 2. Create new tab\nmcp__claude-in-chrome__tabs_create_mcp()\n# Returns tabId\n\n# 3. Navigate to login\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://app.example.com/login\")\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=2)\n\n# 4. Clear console and network for clean test\nmcp__claude-in-chrome__read_console_messages(tabId=TAB_ID, pattern=\".*\", clear=true)\nmcp__claude-in-chrome__read_network_requests(tabId=TAB_ID, clear=true)\n\n# 5. Get page structure\nmcp__claude-in-chrome__read_page(tabId=TAB_ID, filter=\"interactive\")\n\n# 6. Fill login form\nmcp__claude-in-chrome__find(tabId=TAB_ID, query=\"email input\")\nmcp__claude-in-chrome__form_input(tabId=TAB_ID, ref=\"ref_1\", value=\"user@example.com\")\n\nmcp__claude-in-chrome__find(tabId=TAB_ID, query=\"password input\")\nmcp__claude-in-chrome__form_input(tabId=TAB_ID, ref=\"ref_2\", value=\"password123\")\n\n# 7. Submit\nmcp__claude-in-chrome__find(tabId=TAB_ID, query=\"sign in button\")\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, ref=\"ref_3\")\n\n# 8. Wait for response\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=3)\n\n# 9. Check for errors in console\nmcp__claude-in-chrome__read_console_messages(\n    tabId=TAB_ID,\n    pattern=\"error|Error|failed\",\n    onlyErrors=true\n)\n\n# 10. Verify API call succeeded\nmcp__claude-in-chrome__read_network_requests(\n    tabId=TAB_ID,\n    urlPattern=\"/api/login\"\n)\n\n# 11. Verify logged in state via JS\nmcp__claude-in-chrome__javascript_tool(\n    action=\"javascript_exec\",\n    tabId=TAB_ID,\n    text=\"localStorage.getItem('authToken') !== null\"\n)\n\n# 12. Screenshot for evidence\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n```\n\n### API Debugging Workflow\n\n```\n# 1. Setup\nmcp__claude-in-chrome__tabs_context_mcp(createIfEmpty=true)\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://api-dashboard.example.com\")\n\n# 2. Clear previous network data\nmcp__claude-in-chrome__read_network_requests(tabId=TAB_ID, clear=true)\n\n# 3. Trigger the problematic action\nmcp__claude-in-chrome__find(tabId=TAB_ID, query=\"refresh data button\")\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, ref=\"ref_1\")\n\n# 4. Wait for network activity\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=3)\n\n# 5. Inspect all API calls\nmcp__claude-in-chrome__read_network_requests(\n    tabId=TAB_ID,\n    urlPattern=\"/api/\"\n)\n\n# 6. Check console for related errors\nmcp__claude-in-chrome__read_console_messages(\n    tabId=TAB_ID,\n    pattern=\"API|fetch|error\"\n)\n\n# 7. Verify page state after API call\nmcp__claude-in-chrome__javascript_tool(\n    action=\"javascript_exec\",\n    tabId=TAB_ID,\n    text=\"document.querySelector('.data-table tbody tr').length\"\n)\n```\n\n### Form Submission with Validation\n\n```\n# 1. Navigate to form\nmcp__claude-in-chrome__navigate(tabId=TAB_ID, url=\"https://app.example.com/contact\")\n\n# 2. Start GIF recording\nmcp__claude-in-chrome__gif_creator(action=\"start_recording\", tabId=TAB_ID)\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# 3. Fill form with invalid data to test validation\nmcp__claude-in-chrome__form_input(tabId=TAB_ID, ref=\"email-input\", value=\"invalid-email\")\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# 4. Submit\nmcp__claude-in-chrome__computer(action=\"left_click\", tabId=TAB_ID, ref=\"submit-btn\")\nmcp__claude-in-chrome__computer(action=\"wait\", tabId=TAB_ID, duration=1)\nmcp__claude-in-chrome__computer(action=\"screenshot\", tabId=TAB_ID)\n\n# 5. Check for validation errors in console\nmcp__claude-in-chrome__read_console_messages(tabId=TAB_ID, pattern=\"validation|error\")\n\n# 6. Verify error message appears\nmcp__claude-in-chrome__javascript_tool(\n    action=\"javascript_exec\",\n    tabId=TAB_ID,\n    text=\"document.querySelector('.error-message').innerText\"\n)\n\n# 7. Stop and export GIF\nmcp__claude-in-chrome__gif_creator(action=\"stop_recording\", tabId=TAB_ID)\nmcp__claude-in-chrome__gif_creator(\n    action=\"export\",\n    tabId=TAB_ID,\n    download=true,\n    filename=\"form_validation.gif\"\n)\n```\n\n## Integration\n\nThis skill is referenced by `dev-test` for Chrome MCP browser automation.\n\n**For headless/CI testing, use:** `Skill(skill=\"workflows:dev-test-playwright\")`\n\nFor TDD protocol, see: `Skill(skill=\"workflows:dev-tdd\")`"
              },
              {
                "name": "dev-test-hammerspoon",
                "description": "This skill should be used when the user asks to \"debug macOS app\", \"test native app\", \"automate macOS workflow\", \"test native macOS application\", or needs desktop automation for testing macOS applications with Hammerspoon. Use for application launch/control, window management, keyboard/mouse simulation, and visual verification.",
                "path": "skills/dev-test-hammerspoon/SKILL.md",
                "frontmatter": {
                  "name": "dev-test-hammerspoon",
                  "description": "This skill should be used when the user asks to \"debug macOS app\", \"test native app\", \"automate macOS workflow\", \"test native macOS application\", or needs desktop automation for testing macOS applications with Hammerspoon. Use for application launch/control, window management, keyboard/mouse simulation, and visual verification."
                },
                "content": "**Announce:** \"I'm using dev-test-hammerspoon for macOS desktop automation.\"\n\n<EXTREMELY-IMPORTANT>\n## Gate Reminder\n\nBefore taking screenshots or running E2E tests, you MUST complete all 6 gates from dev-tdd:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN: E2E tests/screenshots\n```\n\n**You loaded dev-tdd earlier. Follow the gates now.**\n</EXTREMELY-IMPORTANT>\n\n## Contents\n\n- [Tool Availability Gate](#tool-availability-gate)\n- [When to Use Hammerspoon](#when-to-use-hammerspoon)\n- [Hammerspoon Setup](#hammerspoon-setup)\n- [Input Simulation](#input-simulation)\n- [Application Control](#application-control)\n- [Window Management](#window-management)\n- [Screenshots](#screenshots)\n- [Complete E2E Example](#complete-e2e-example)\n- [Alternative: cliclick](#alternative-cliclick)\n\n# macOS Desktop Automation\n\n<EXTREMELY-IMPORTANT>\n## Tool Availability Gate\n\n**Verify Hammerspoon is installed before proceeding.**\n\n```bash\n# Check Hammerspoon installation (both CLI and app)\nwhich hs || echo \"MISSING: hs CLI\"\nls /Applications/Hammerspoon.app 2>/dev/null || echo \"MISSING: Hammerspoon.app\"\n```\n\n**If missing:**\n```\nSTOP: Cannot proceed with macOS automation.\n\nMissing tool: Hammerspoon (required for macOS E2E testing)\n\nInstall with:\n  brew install --cask hammerspoon\n\nAfter installing:\n  1. Open Hammerspoon.app\n  2. Grant Accessibility permissions in System Preferences\n  3. In Hammerspoon console, run: hs.ipc.cliInstall()\n  4. Add to ~/.hammerspoon/init.lua: require(\"hs.ipc\")\n\nReply when installed and I'll continue testing.\n```\n\n**This gate is non-negotiable. Missing tools = full stop.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## When to Use Hammerspoon\n\n**Use Hammerspoon for:**\n- macOS native application automation\n- System-wide keyboard shortcuts\n- Window management and positioning\n- Menu item automation\n- Clipboard verification\n- Multi-app workflows on macOS\n\n**Do not use Hammerspoon for:**\n- Testing web applications (use Chrome MCP or Playwright)\n- Cross-platform testing needed\n- Linux desktop automation (use dev-test-linux)\n\n**For web testing, use:**\n- `Skill(skill=\"workflows:dev-test-chrome\")` - debugging\n- `Skill(skill=\"workflows:dev-test-playwright\")` - CI/CD\n\n### Rationalization Prevention\n\n| Thought | Reality |\n|---------|---------|\n| \"I can use AppleScript instead\" | Hammerspoon is more reliable for automation |\n| \"I'll test the app manually\" | AUTOMATE IT with Hammerspoon |\n| \"Web testing tools work for desktop apps\" | NO. Use Hammerspoon for native apps |\n| \"Accessibility permissions are too hard\" | One-time setup. Do it. |\n| \"The app is too complex to automate\" | Break it into testable steps |\n</EXTREMELY-IMPORTANT>\n\n## Hammerspoon Setup\n\n**One-time setup in `~/.hammerspoon/init.lua`:**\n```lua\nrequire(\"hs.ipc\")  -- Enables CLI\n```\n\n**Reload config after changes:**\n```bash\nhs -c 'hs.reload()'  # Reload Hammerspoon configuration\n```\n\n## Input Simulation\n\n### hs.eventtap - Keyboard/Mouse\n\n```lua\n-- Type text (simulates keystrokes)\nhs.eventtap.keyStrokes(\"hello world\")\n\n-- Key press with modifiers\nhs.eventtap.keyStroke({\"cmd\"}, \"c\")           -- Cmd+C\nhs.eventtap.keyStroke({\"cmd\", \"shift\"}, \"s\")  -- Cmd+Shift+S\nhs.eventtap.keyStroke({\"ctrl\", \"alt\"}, \"t\")   -- Ctrl+Alt+T\nhs.eventtap.keyStroke({}, \"return\")           -- Enter key\nhs.eventtap.keyStroke({}, \"escape\")           -- Escape key\n\n-- Function keys\nhs.eventtap.keyStroke({}, \"f1\")\nhs.eventtap.keyStroke({\"cmd\"}, \"f5\")\n\n-- Mouse clicks\nhs.eventtap.leftClick({x=100, y=200})\nhs.eventtap.rightClick({x=100, y=200})\nhs.eventtap.middleClick({x=100, y=200})\nhs.eventtap.doubleClick({x=100, y=200})\n\n-- Mouse movement\nhs.mouse.absolutePosition({x=500, y=300})\n\n-- Scroll\nhs.eventtap.scrollWheel({0, -5}, {})  -- Scroll down\nhs.eventtap.scrollWheel({0, 5}, {})   -- Scroll up\n```\n\n### Running from CLI\n\n```bash\n# Execute Lua code directly\nhs -c 'hs.eventtap.keyStroke({\"cmd\"}, \"c\")'  # Run inline Lua code via CLI\n\n# Execute a script file\nhs /path/to/test_script.lua  # Run Hammerspoon script from file\n\n# Pipe script via stdin\necho 'hs.eventtap.keyStrokes(\"test\")' | hs -s  # Run script piped through stdin\n```\n\n## Application Control\n\n### hs.application\n\n```lua\n-- Launch or focus app by name\nlocal app = hs.application.launchOrFocus(\"Safari\")\n\n-- Launch app by bundle ID\nhs.application.launchOrFocusByBundleID(\"com.apple.Safari\")\n\n-- Get running app\nlocal app = hs.application.get(\"Safari\")\nif app then\n    app:activate()       -- Bring to front\n    app:hide()           -- Hide\n    app:unhide()         -- Unhide\n    app:kill()           -- Terminate gracefully\n    app:kill9()          -- Force kill\nend\n\n-- Get frontmost app\nlocal front = hs.application.frontmostApplication()\nprint(front:name())\nprint(front:bundleID())\n\n-- List all running apps\nfor _, app in ipairs(hs.application.runningApplications()) do\n    print(app:name())\nend\n\n-- Wait for app to launch\nhs.timer.waitUntil(\n    function() return hs.application.get(\"MyApp\") ~= nil end,\n    function() print(\"App launched\") end,\n    0.5  -- Check every 0.5 seconds\n)\n```\n\n### Menu Items\n\n```lua\n-- Click menu item\nlocal app = hs.application.get(\"Safari\")\napp:selectMenuItem({\"File\", \"New Window\"})\napp:selectMenuItem({\"Edit\", \"Paste\"})\n\n-- Check if menu item exists\nlocal menuItem = app:findMenuItem({\"File\", \"Save\"})\nif menuItem then\n    print(\"Save is available, enabled:\", menuItem.enabled)\nend\n```\n\n## Window Management\n\n### hs.window\n\n```lua\n-- Get focused window\nlocal win = hs.window.focusedWindow()\nprint(win:title())\nprint(win:frame())  -- {x, y, w, h}\n\n-- Get app's windows\nlocal app = hs.application.get(\"Safari\")\nlocal wins = app:allWindows()\nfor _, win in ipairs(wins) do\n    print(win:title())\nend\n\n-- Get window by title (partial match)\nlocal win = hs.window.get(\"My Document\")\n\n-- Window actions\nwin:focus()           -- Focus window\nwin:maximize()        -- Maximize\nwin:minimize()        -- Minimize to dock\nwin:close()           -- Close window\n\n-- Move/resize\nwin:setFrame({x=100, y=100, w=800, h=600})\nwin:move({100, 0})    -- Move relative\nwin:setSize({800, 600})\nwin:centerOnScreen()\n\n-- Get window position and size\nlocal frame = win:frame()\nprint(\"Position:\", frame.x, frame.y)\nprint(\"Size:\", frame.w, frame.h)\n```\n\n## Screenshots\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of Visual Verification\n\n**Every E2E test MUST include screenshot evidence.**\n\nAfter completing a workflow, capture a screenshot to prove success.\n</EXTREMELY-IMPORTANT>\n\n### screencapture (CLI)\n\n```bash\n# Full screen (all displays)\nscreencapture /tmp/screenshot.png  # Capture entire screen to file\n\n# Main screen only\nscreencapture -m /tmp/main_screen.png  # Capture primary screen only\n\n# Specific window (interactive - click to select)\nscreencapture -w /tmp/window.png  # Interactively select window to capture\n\n# Specific region\nscreencapture -R 100,200,800,600 /tmp/region.png  # Capture rectangular region (x,y,w,h)\n\n# Without window shadow\nscreencapture -o /tmp/no_shadow.png  # Capture without window shadows\n\n# Silent (no camera sound)\nscreencapture -x /tmp/silent.png  # Capture silently without shutter sound\n\n# To clipboard instead of file\nscreencapture -c  # Capture to clipboard\n\n# Combined: silent, no shadow, specific region\nscreencapture -x -o -R 0,0,1920,1080 /tmp/clean.png  # Capture region silently without shadows\n```\n\n### hs.screen (Hammerspoon)\n\n```lua\n-- Capture focused window\nlocal win = hs.window.focusedWindow()\nif win then\n    local img = win:snapshot()\n    img:saveToFile(\"/tmp/window.png\")\nend\n\n-- Capture entire screen\nlocal screen = hs.screen.mainScreen()\nlocal img = screen:snapshot()\nimg:saveToFile(\"/tmp/screen.png\")\n\n-- Capture specific region\nlocal img = hs.screen.mainScreen():snapshot({x=0, y=0, w=800, h=600})\nimg:saveToFile(\"/tmp/region.png\")\n```\n\n## Complete E2E Example\n\n<EXTREMELY-IMPORTANT>\n### E2E Test Structure\n\nEvery Hammerspoon E2E test MUST:\n1. **Launch** - Start the application\n2. **Verify launch** - Assert app is running\n3. **Interact** - Perform user actions\n4. **Verify state** - Check expected state (clipboard, window, etc.)\n5. **Screenshot** - Capture visual evidence\n6. **Cleanup** - Close app, restore state\n</EXTREMELY-IMPORTANT>\n\n```lua\n-- test_workflow.lua\n-- Run with: hs /path/to/test_workflow.lua\n\nlocal function test_app_workflow()\n    -- 1. Launch app\n    print(\"Launching app...\")\n    hs.application.launchOrFocus(\"TextEdit\")\n    hs.timer.usleep(1000000)  -- Wait 1 second\n\n    -- 2. Verify app launched\n    local app = hs.application.get(\"TextEdit\")\n    assert(app, \"FAIL: TextEdit did not launch\")\n    print(\"App launched: \" .. app:name())\n\n    -- 3. Create new document\n    hs.eventtap.keyStroke({\"cmd\"}, \"n\")\n    hs.timer.usleep(500000)\n\n    -- 4. Type content\n    hs.eventtap.keyStrokes(\"Hello, this is an automated test!\")\n    hs.timer.usleep(300000)\n\n    -- 5. Select all and copy\n    hs.eventtap.keyStroke({\"cmd\"}, \"a\")\n    hs.timer.usleep(100000)\n    hs.eventtap.keyStroke({\"cmd\"}, \"c\")\n\n    -- 6. Verify clipboard\n    local clipboard = hs.pasteboard.getContents()\n    assert(clipboard:find(\"automated test\"), \"FAIL: Clipboard doesn't match\")\n    print(\"Clipboard verified: \" .. clipboard)\n\n    -- 7. Take screenshot\n    local win = hs.window.focusedWindow()\n    local img = win:snapshot()\n    img:saveToFile(\"/tmp/test_result.png\")\n    print(\"Screenshot saved to /tmp/test_result.png\")\n\n    -- 8. Close without saving\n    hs.eventtap.keyStroke({\"cmd\"}, \"w\")\n    hs.timer.usleep(500000)\n    hs.eventtap.keyStroke({}, \"d\")  -- \"Don't Save\" button\n\n    print(\"PASS: Workflow completed successfully\")\nend\n\n-- Run the test\nlocal status, err = pcall(test_app_workflow)\nif not status then\n    print(\"FAIL: \" .. tostring(err))\n    os.exit(1)\nend\nos.exit(0)\n```\n\n**Run from CLI:**\n```bash\nhs /path/to/test_workflow.lua && echo \"TEST PASSED\" || echo \"TEST FAILED\"  # Execute test script and report result\n```\n\n## Alternative: cliclick\n\nFor simpler needs, `cliclick` provides CLI-based mouse/keyboard control:\n\n```bash\n# Install cliclick tool\nbrew install cliclick\n\n# Mouse click at coordinates\ncliclick c:100,200       # Left-click at coordinates\ncliclick rc:100,200      # Right-click at coordinates\ncliclick dc:100,200      # Double-click at coordinates\n\n# Move mouse\ncliclick m:500,300  # Move mouse to coordinates\n\n# Type text\ncliclick t:\"Hello world\"  # Type text at current cursor position\n\n# Key press\ncliclick kp:return  # Press return key\ncliclick kp:escape  # Press escape key\ncliclick kd:cmd kp:c ku:cmd  # Press Cmd+C (key down, press, key up)\n\n# Wait (milliseconds)\ncliclick w:500  # Wait for 500 milliseconds\n```\n\n**cliclick is useful for simple scripts but lacks app control - prefer Hammerspoon for complex E2E tests.**\n\n## Output Requirements\n\n**Every test run MUST be documented in LEARNINGS.md:**\n\n```markdown\n## macOS E2E Test: [Description]\n\n**Tool:** Hammerspoon\n\n**Script:**\n```bash\nhs /path/to/test_script.lua\n```\n\n**Output:**\n```\nLaunching app...\nApp launched: TextEdit\nClipboard verified: Hello, this is an automated test!\nScreenshot saved to /tmp/test_result.png\nPASS: Workflow completed successfully\n```\n\n**Result:** PASS\n\n**Screenshot:** /tmp/test_result.png\n```\n\n## Integration\n\nThis skill is referenced by `dev-test` for macOS desktop automation.\n\nFor TDD protocol, see: `Skill(skill=\"workflows:dev-tdd\")`"
              },
              {
                "name": "dev-test-linux",
                "description": "This skill should be used when the user asks to \"test Linux desktop apps\", \"automate GTK/Qt applications\", \"test with ydotool\", \"test with xdotool\", \"verify Linux UI interactions\", \"capture screenshots on Linux\", \"control D-Bus services\", \"test Wayland applications\", \"test X11 applications\", or needs Linux desktop E2E testing. Provides comprehensive guidance for Linux automation with ydotool (Wayland), xdotool (X11), grim, and D-Bus.",
                "path": "skills/dev-test-linux/SKILL.md",
                "frontmatter": {
                  "name": "dev-test-linux",
                  "description": "This skill should be used when the user asks to \"test Linux desktop apps\", \"automate GTK/Qt applications\", \"test with ydotool\", \"test with xdotool\", \"verify Linux UI interactions\", \"capture screenshots on Linux\", \"control D-Bus services\", \"test Wayland applications\", \"test X11 applications\", or needs Linux desktop E2E testing. Provides comprehensive guidance for Linux automation with ydotool (Wayland), xdotool (X11), grim, and D-Bus.",
                  "version": "0.1.0"
                },
                "content": "<EXTREMELY-IMPORTANT>\n## Gate Reminder\n\nBefore taking screenshots or running E2E tests, you MUST complete all 6 gates from dev-tdd:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN: E2E tests/screenshots\n```\n\n**You loaded dev-tdd earlier. Follow the gates now.**\n</EXTREMELY-IMPORTANT>\n\n## Contents\n\n- [Tool Availability Gate](#tool-availability-gate)\n- [When to Use Linux Automation](#when-to-use-linux-automation)\n- [Detect Display Server](#detect-display-server)\n- [Wayland: ydotool](#wayland-ydotool)\n- [X11: xdotool](#x11-xdotool)\n- [Screenshots](#screenshots)\n- [D-Bus Control](#d-bus-control)\n- [Accessibility (AT-SPI)](#accessibility-at-spi)\n- [Complete E2E Examples](#complete-e2e-examples)\n\n# Linux Desktop Automation\n\n<EXTREMELY-IMPORTANT>\n## Tool Availability Gate\n\nVerify automation tools are installed before proceeding.\n\n```bash\n# Detect display server (check for Wayland vs X11)\necho $XDG_SESSION_TYPE  # \"wayland\" or \"x11\"\n\n# Wayland tools check (verify ydotool, wtype, grim, slurp)\nwhich ydotool || echo \"MISSING: ydotool\"\nwhich wtype || echo \"MISSING: wtype\"\nwhich grim || echo \"MISSING: grim\"\nwhich slurp || echo \"MISSING: slurp\"\n\n# X11 tools check (verify xdotool, xclip, scrot)\nwhich xdotool || echo \"MISSING: xdotool\"\nwhich xclip || echo \"MISSING: xclip\"\nwhich scrot || echo \"MISSING: scrot\"\n\n# D-Bus check (verify dbus-send availability)\nwhich dbus-send || echo \"MISSING: dbus-send\"\n```\n\n**If missing (Wayland):**\n```\nSTOP: Cannot proceed with Wayland automation.\n\nMissing tools for Wayland E2E testing.\n\nInstall with:\n  # Arch\n  sudo pacman -S ydotool wtype grim slurp\n\n  # Debian/Ubuntu\n  sudo apt install ydotool wtype grim slurp\n\n  # Nix\n  nix-env -iA nixpkgs.ydotool nixpkgs.wtype nixpkgs.grim nixpkgs.slurp\n\nStart ydotool daemon:\n  sudo systemctl enable --now ydotool\n  # Or for user service:\n  systemctl --user enable --now ydotool\n\nReply when installed and I'll continue testing.\n```\n\n**This gate is non-negotiable. Missing tools = full stop.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## When to Use Linux Automation\n\nUse Linux automation (ydotool/xdotool) for:\n- Linux native application automation\n- GTK/Qt application testing\n- System-wide keyboard/mouse control\n- Window management testing\n- D-Bus service interaction\n- Accessibility testing (AT-SPI)\n\nDo NOT use Linux automation for:\n- Testing web applications (use Chrome MCP or Playwright)\n- macOS desktop automation (use dev-test-hammerspoon)\n- Cross-platform testing\n\nFor web testing, use:\n- `Skill(skill=\"workflows:dev-test-chrome\")` - debugging\n- `Skill(skill=\"workflows:dev-test-playwright\")` - CI/CD\n\n### Rationalization Prevention\n\n| Thought | Reality |\n|---------|---------|\n| \"I can test the app manually\" | AUTOMATE IT with ydotool/xdotool |\n| \"Web testing tools work for desktop apps\" | NO. Use native Linux tools |\n| \"ydotool daemon is hard to set up\" | One-time setup. Do it. |\n| \"X11 is deprecated, skip xdotool\" | Many systems still use X11. Support both. |\n| \"D-Bus is too complex\" | D-Bus gives precise control. Learn it. |\n\n### Display Server Detection\n\n```bash\n# Detect display server and choose appropriate tools\nif [ \"$XDG_SESSION_TYPE\" = \"wayland\" ]; then\n    # Use ydotool, wtype, grim\nelse\n    # Use xdotool, xclip, scrot\nfi\n```\n\nAlways detect display server before choosing tools.\n</EXTREMELY-IMPORTANT>\n\n## Detect Display Server\n\n```bash\n# Check display server type (Wayland or X11)\nif [ \"$XDG_SESSION_TYPE\" = \"wayland\" ]; then\n    echo \"Using Wayland tools (ydotool, wtype, grim)\"\nelse\n    echo \"Using X11 tools (xdotool, xclip, scrot)\"\nfi\n```\n\n## Wayland: ydotool\n\nRequires ydotoold daemon running.\n\n### Keyboard Input\n\n```bash\n# Type text (simple text input to focused window)\nydotool type \"hello world\"\n\n# Type with delay (type text with microsecond delay between keys)\nydotool type --delay 50 \"slow typing\"\n\n# Press Enter key (send Enter key using keycode format)\nydotool key 28:1 28:0\n\n# Press Escape key (send Escape key)\nydotool key 1:1 1:0\n\n# Press Ctrl+C (send Ctrl+C combination)\nydotool key 29:1 46:1 46:0 29:0\n\n# Press Ctrl+V (send Ctrl+V combination)\nydotool key 29:1 47:1 47:0 29:0\n\n# Press Alt+Tab (send Alt+Tab combination)\nydotool key 56:1 15:1 15:0 56:0\n\n# Common keycodes reference\n# 1=Escape, 14=Backspace, 15=Tab, 28=Enter, 29=Ctrl, 42=LShift\n# 56=Alt, 57=Space, 100=RightAlt, 125=Super/Win\n```\n\n### Alternative: wtype (Wayland-native)\n\n```bash\n# Type text (simple text input to focused window)\nwtype \"hello world\"\n\n# Press Ctrl+C (send Ctrl+C combination)\nwtype -M ctrl -k c\n\n# Press Ctrl+Shift+S (send Ctrl+Shift+S combination)\nwtype -M ctrl -M shift -k s\n\n# Press Enter (send Enter key)\nwtype -k Return\n\n# Press Escape (send Escape key)\nwtype -k Escape\n```\n\nAvailable modifiers: shift, ctrl, alt, logo (super)\n\n### Mouse Input\n\n```bash\n# Move mouse to absolute position (move cursor to screen coordinates)\nydotool mousemove --absolute 100 200\n\n# Move mouse relative (move cursor by relative offset)\nydotool mousemove 50 -30\n\n# Click left button (send left mouse click)\nydotool click 1\n\n# Click right button (send right mouse click)\nydotool click 3\n\n# Double click (send double click)\nydotool click 1 1\n\n# Click at position (move and click in one operation)\nydotool mousemove --absolute 500 300 && ydotool click 1\n\n# Drag operation (move mouse while holding button)\nydotool mousemove --absolute 100 100\nydotool mousedown 1\nydotool mousemove --absolute 200 200\nydotool mouseup 1\n```\n\n## X11: xdotool\n\n### Keyboard Input\n\n```bash\n# Type text (simple text input to focused window)\nxdotool type \"hello world\"\n\n# Press Return (send Return key)\nxdotool key Return\n\n# Press Escape (send Escape key)\nxdotool key Escape\n\n# Press Ctrl+C (send Ctrl+C combination)\nxdotool key ctrl+c\n\n# Press Ctrl+Shift+S (send Ctrl+Shift+S combination)\nxdotool key ctrl+shift+s\n\n# Press Alt+Tab (send Alt+Tab combination)\nxdotool key alt+Tab\n\n# Press Super+D (send Super+D combination)\nxdotool key super+d\n\n# Type with delay (type text with millisecond delay between keys)\nxdotool type --delay 50 \"slow typing\"\n\n# Hold key down (press and hold Ctrl)\nxdotool keydown ctrl\n\n# Press C (send C key)\nxdotool key c\n\n# Release key (release Ctrl)\nxdotool keyup ctrl\n```\n\n### Mouse Input\n\n```bash\n# Move mouse absolute (move cursor to screen coordinates)\nxdotool mousemove 100 200\n\n# Move mouse relative (move cursor by relative offset)\nxdotool mousemove --relative 50 30\n\n# Click left button (send left mouse click)\nxdotool click 1\n\n# Click middle button (send middle mouse click)\nxdotool click 2\n\n# Click right button (send right mouse click)\nxdotool click 3\n\n# Double click (send double click)\nxdotool click --repeat 2 1\n\n# Click at position (move and click in one operation)\nxdotool mousemove 500 300 click 1\n\n# Drag operation (move mouse while holding button)\nxdotool mousemove 100 100 mousedown 1 mousemove 200 200 mouseup 1\n```\n\n### Window Control (X11)\n\n```bash\n# Get active window ID (get numeric window identifier)\nxdotool getactivewindow\n\n# Focus window by name (find and focus window matching name)\nxdotool search --name \"Firefox\" windowactivate\n\n# Focus window by class (find and focus window matching class)\nxdotool search --class \"firefox\" windowactivate\n\n# Get window title (get title of active window)\nxdotool getactivewindow getwindowname\n\n# Move window (move active window to coordinates)\nxdotool getactivewindow windowmove 100 100\n\n# Resize window (resize active window to dimensions)\nxdotool getactivewindow windowsize 800 600\n\n# Minimize window (minimize active window)\nxdotool getactivewindow windowminimize\n\n# Focus window and wait (find, focus, and synchronize with window)\nxdotool search --name \"Firefox\" windowactivate --sync\n```\n\n## Screenshots\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of Visual Verification\n\nEvery E2E test MUST include screenshot evidence.\n\nCapture a screenshot after completing a workflow to prove success.\n</EXTREMELY-IMPORTANT>\n\n### Wayland: grim + slurp\n\n```bash\n# Capture full screen (capture all outputs)\ngrim /tmp/screenshot.png\n\n# Capture specific output (capture single monitor/output)\ngrim -o DP-1 /tmp/screen.png\n\n# Capture region interactively (select region with slurp then capture)\ngrim -g \"$(slurp)\" /tmp/region.png\n\n# Capture specific region (capture region by coordinates and size)\ngrim -g \"100,200 800x600\" /tmp/region.png\n\n# Capture Hyprland window (get window geometry and capture)\nhyprctl clients -j | jq '.[] | select(.class==\"firefox\")'\ngrim -g \"X,Y WxH\" /tmp/window.png\n\n# Capture Sway focused window (get focused window geometry and capture)\ngrim -g \"$(swaymsg -t get_tree | jq -r '.. | select(.focused?) | .rect | \"\\(.x),\\(.y) \\(.width)x\\(.height)\"')\" /tmp/window.png\n```\n\n### X11: scrot / import\n\n```bash\n# Capture full screen (screenshot of entire display)\nscrot /tmp/screenshot.png\n\n# Capture active window (screenshot of focused window)\nscrot -u /tmp/window.png\n\n# Capture interactive selection (select region with mouse then capture)\nscrot -s /tmp/selection.png\n\n# Capture with delay (wait before capturing)\nscrot -d 3 /tmp/delayed.png\n\n# Capture root window (screenshot using ImageMagick)\nimport -window root /tmp/screenshot.png\n\n# Capture active window (screenshot of focused window using ImageMagick)\nimport -window \"$(xdotool getactivewindow)\" /tmp/window.png\n```\n\n### Image Comparison\n\n```bash\n# Compare screenshots (count different pixels using ImageMagick)\ncompare -metric AE baseline.png current.png diff.png\n\n# Threshold comparison (allow 5% fuzz when comparing)\ncompare -metric AE -fuzz 5% baseline.png current.png diff.png\n```\n\n## D-Bus Control\n\nPreferred for apps that expose D-Bus interfaces.\n\n```bash\n# List available services (enumerate all D-Bus services)\ndbus-send --session --print-reply --dest=org.freedesktop.DBus \\\n  /org/freedesktop/DBus org.freedesktop.DBus.ListNames\n\n# Open document in Zathura (get PID first, then use org.pwmt.zathura.PID-XXXX)\ndbus-send --print-reply --dest=org.pwmt.zathura.PID-12345 \\\n  /org/pwmt/zathura org.pwmt.zathura.OpenDocument string:\"/path/to/file.pdf\"\n\n# Go to page in Zathura (navigate to specific page)\ndbus-send --print-reply --dest=org.pwmt.zathura.PID-12345 \\\n  /org/pwmt/zathura org.pwmt.zathura.GotoPage uint32:5\n\n# Open file in GNOME Nautilus (open folder via D-Bus)\ndbus-send --session --dest=org.gnome.Nautilus \\\n  /org/gnome/Nautilus org.freedesktop.Application.Open \\\n  array:string:\"file:///home/user\" dict:string:string:\"\"\n\n# Introspect D-Bus service (discover available methods and properties)\ndbus-send --session --print-reply --dest=org.example.App \\\n  /org/example/App org.freedesktop.DBus.Introspectable.Introspect\n```\n\n## Accessibility (AT-SPI)\n\nUse AT-SPI for UI element discovery and verification.\n\n```python\n#!/usr/bin/env python3\nimport pyatspi\n\n# Find application (get desktop and search for app by name)\ndesktop = pyatspi.Registry.getDesktop(0)\nfor app in desktop:\n    if \"firefox\" in app.name.lower():\n        print(f\"Found: {app.name}\")\n\n        # Traverse accessibility tree (recursively dump accessibility tree)\n        def dump_tree(node, indent=0):\n            print(\"  \" * indent + f\"{node.getRole()}: {node.name}\")\n            for child in node:\n                dump_tree(child, indent + 1)\n\n        dump_tree(app)\n\n# Find specific element (search for button by name in tree)\ndef find_button(app, name):\n    for child in app:\n        if child.getRole() == pyatspi.ROLE_PUSH_BUTTON:\n            if name.lower() in child.name.lower():\n                return child\n        found = find_button(child, name)\n        if found:\n            return found\n    return None\n\n# Click button via AT-SPI (trigger button action via accessibility interface)\nbutton = find_button(app, \"Submit\")\nif button:\n    button.queryAction().doAction(0)\n```\n\n## Complete E2E Examples\n\n<EXTREMELY-IMPORTANT>\n### E2E Test Structure\n\nEvery Linux E2E test MUST:\n1. Detect - Check display server (Wayland vs X11)\n2. Launch - Start the application\n3. Wait - Allow app to fully initialize\n4. Interact - Perform user actions\n5. Verify - Check expected state\n6. Screenshot - Capture visual evidence\n7. Cleanup - Close app, restore state\n</EXTREMELY-IMPORTANT>\n\n### Wayland E2E Test\n\n```bash\n#!/bin/bash\n# test_workflow.sh - Wayland E2E test\n\nset -e  # Exit on error\n\necho \"Starting E2E test...\"\n\n# Launch Firefox\nfirefox &\nsleep 3\n\n# Focus address bar and navigate (focus address bar with Ctrl+L)\nwtype -M ctrl -k l\nsleep 0.2\n\n# Type URL (type example.com URL)\nwtype \"https://example.com\"\n\n# Press Enter (send Return key)\nwtype -k Return\nsleep 2\n\n# Capture initial screenshot (screenshot before interaction)\ngrim /tmp/test_before.png\n\n# Move mouse and click (move to element and click)\nydotool mousemove --absolute 500 400\nydotool click 1\nsleep 0.5\n\n# Capture final screenshot (screenshot after interaction)\ngrim /tmp/test_after.png\n\n# Compare screenshots (compare file sizes to detect changes)\nSIZE_BEFORE=$(stat -c%s /tmp/test_before.png)\nSIZE_AFTER=$(stat -c%s /tmp/test_after.png)\n\nif [ \"$SIZE_BEFORE\" -ne \"$SIZE_AFTER\" ]; then\n    echo \"PASS: Screenshots differ (interaction worked)\"\nelse\n    echo \"WARN: Screenshots identical\"\nfi\n\necho \"Test complete\"\n```\n\n### X11 E2E Test\n\n```bash\n#!/bin/bash\n# test_workflow_x11.sh - X11 E2E test\n\nset -e\n\necho \"Starting X11 E2E test...\"\n\n# Launch gedit (start text editor application)\ngedit &\nsleep 2\n\n# Focus gedit window (find and focus window by name)\nxdotool search --name \"gedit\" windowactivate --sync\n\n# Type test content (type test text into editor)\nxdotool type \"Hello, this is an automated test!\"\nsleep 0.5\n\n# Select all text (select all with Ctrl+A)\nxdotool key ctrl+a\n\n# Copy to clipboard (copy selected text with Ctrl+C)\nxdotool key ctrl+c\n\n# Verify clipboard content (get clipboard and verify content)\nCLIPBOARD=$(xclip -selection clipboard -o)\nif [[ \"$CLIPBOARD\" == *\"automated test\"* ]]; then\n    echo \"PASS: Clipboard contains expected text\"\nelse\n    echo \"FAIL: Clipboard mismatch\"\n    exit 1\nfi\n\n# Capture window screenshot (screenshot of active window)\nscrot -u /tmp/test_result.png\necho \"Screenshot saved\"\n\n# Close without saving (close window with Ctrl+W)\nxdotool key ctrl+w\nsleep 0.5\n\n# Dismiss save dialog (press Tab and Return to skip save)\nxdotool key Tab key Return\n\necho \"Test complete\"\n```\n\n## Output Requirements\n\nDocument every test run in LEARNINGS.md using this template:\n\n```markdown\n## Linux E2E Test: [Description]\n\n**Display Server:** Wayland / X11\n\n**Tool:** ydotool / xdotool\n\n**Script:**\n```bash\n./test_workflow.sh\n```\n\n**Output:**\n```\nStarting E2E test...\nPASS: Screenshots differ (interaction worked)\nTest complete\n```\n\n**Result:** PASS\n\n**Screenshot:** /tmp/test_result.png\n```\n\n## Integration\n\nThis skill integrates with `dev-test` for Linux desktop automation.\n\nFor TDD protocol, see: `Skill(skill=\"workflows:dev-tdd\")`"
              },
              {
                "name": "dev-test-playwright",
                "description": "Playwright MCP browser testing. Headless E2E, cross-browser, CI/CD automation.",
                "path": "skills/dev-test-playwright/SKILL.md",
                "frontmatter": {
                  "name": "dev-test-playwright",
                  "description": "Playwright MCP browser testing. Headless E2E, cross-browser, CI/CD automation."
                },
                "content": "**Announce:** \"I'm using dev-test-playwright for headless browser automation.\"\n\n<EXTREMELY-IMPORTANT>\n## Gate Reminder\n\nBefore taking screenshots or running E2E tests, you MUST complete all 6 gates from dev-tdd:\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN: E2E tests/screenshots\n```\n\n**You loaded dev-tdd earlier. Follow the gates now.**\n</EXTREMELY-IMPORTANT>\n\n## Contents\n\n- [Tool Availability Gate](#tool-availability-gate)\n- [When to Use Playwright MCP](#when-to-use-playwright-mcp)\n- [MCP Tools Overview](#mcp-tools-overview)\n- [Navigation](#navigation)\n- [Element Interaction](#element-interaction)\n- [Verification](#verification)\n- [Form Handling](#form-handling)\n- [Advanced Patterns](#advanced-patterns)\n- [Complete E2E Examples](#complete-e2e-examples)\n\n# Playwright MCP Browser Automation\n\n<EXTREMELY-IMPORTANT>\n## Tool Availability Gate\n\n**Verify Playwright MCP tools are available before proceeding.**\n\nCheck for these MCP functions:\n- `mcp__playwright__browser_navigate`\n- `mcp__playwright__browser_snapshot`\n- `mcp__playwright__browser_click`\n\n**If MCP tools are not available:**\n```\nSTOP: Cannot proceed with Playwright automation.\n\nMissing: Playwright MCP server\n\nThe Playwright MCP server must be configured and running.\nCheck your Claude Code MCP configuration.\n\nReply when configured and I'll continue testing.\n```\n\n**This gate is non-negotiable. Missing tools = full stop.**\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## When to Use Playwright MCP\n\n**USE Playwright MCP when you need:**\n- Headless browser automation (CI/CD)\n- Cross-browser testing (Chromium, Firefox, WebKit)\n- Test isolation (fresh browser state per test)\n- Standard E2E test suite automation\n- Network mocking/interception\n- Parallel test execution\n\n**DO NOT use Playwright MCP when:**\n- Debugging console messages (use Chrome MCP)\n- Inspecting network requests/responses (use Chrome MCP)\n- Executing custom JavaScript in page (use Chrome MCP)\n- Recording GIFs of interactions (use Chrome MCP)\n- Interactive debugging with real browser (use Chrome MCP)\n\n**For debugging, use:** `Skill(skill=\"workflows:dev-test-chrome\")`\n\n### Rationalization Prevention\n\n| Thought | Reality |\n|---------|---------|\n| \"Playwright can do everything\" | NO. It cannot read console or network requests. |\n| \"I don't need console debugging\" | You will. Start with Chrome MCP if unsure. |\n| \"I'll add console checks later\" | You can't with Playwright. Choose the right tool now. |\n| \"Headless mode doesn't matter\" | YES IT DOES for CI/CD. |\n| \"Chrome MCP works for CI\" | NO. It requires visible browser. |\n\n### Capability Comparison\n\n| Capability | Playwright MCP | Chrome MCP |\n|------------|---------------|------------|\n| Navigate/click/type | ✅ | ✅ |\n| Accessibility tree | ✅ `browser_snapshot` | ✅ `read_page` |\n| Screenshots | ✅ | ✅ |\n| **Headless mode** | ✅ | ❌ |\n| **Cross-browser** | ✅ | ❌ |\n| Console messages | ❌ | ✅ |\n| Network requests | ❌ | ✅ |\n| JavaScript execution | ❌ | ✅ |\n| GIF recording | ❌ | ✅ |\n</EXTREMELY-IMPORTANT>\n\n## MCP Tools Overview\n\n| Tool | Purpose |\n|------|---------|\n| `browser_navigate` | Navigate to URL |\n| `browser_snapshot` | Get accessibility tree (page state) |\n| `browser_click` | Click elements |\n| `browser_type` | Type into inputs |\n| `browser_select_option` | Select dropdown options |\n| `browser_hover` | Hover over elements |\n| `browser_wait_for` | Wait for conditions |\n| `browser_take_screenshot` | Visual capture |\n| `browser_press` | Press keys |\n\n## Navigation\n\n### Basic Navigation\n\n```\nmcp__playwright__browser_navigate(url=\"https://example.com\")\n```\n\n### Wait for Page Load\n\n```\nmcp__playwright__browser_navigate(url=\"https://example.com\")\nmcp__playwright__browser_wait_for(state=\"networkidle\")\n```\n\n### Get Current State\n\n```\nmcp__playwright__browser_snapshot()\n```\n\nThe snapshot returns the **accessibility tree** - a structured representation of all interactive elements on the page.\n\n## Element Interaction\n\n### Clicking Elements\n\n```\n# By visible text\nmcp__playwright__browser_click(element=\"Submit button\")\n\n# By ref (from snapshot)\nmcp__playwright__browser_click(ref=\"button[type=submit]\")\n\n# By role and name\nmcp__playwright__browser_click(element=\"Login\", role=\"button\")\n```\n\n### Typing Text\n\n```\n# Into focused element\nmcp__playwright__browser_type(text=\"hello world\")\n\n# Into specific element\nmcp__playwright__browser_click(element=\"Email input\")\nmcp__playwright__browser_type(text=\"user@example.com\")\n\n# Clear and type\nmcp__playwright__browser_click(element=\"Search box\")\nmcp__playwright__browser_type(text=\"new search\", clear=true)\n```\n\n### Keyboard Shortcuts\n\n```\n# Press Enter\nmcp__playwright__browser_press(key=\"Enter\")\n\n# Keyboard shortcuts\nmcp__playwright__browser_press(key=\"Control+a\")\nmcp__playwright__browser_press(key=\"Control+c\")\n```\n\n## Verification\n\n<EXTREMELY-IMPORTANT>\n### The Iron Law of Verification\n\n**EVERY action must be VERIFIED. Taking action is not enough.**\n\nAfter clicking, typing, or navigating, you MUST:\n1. Wait for the expected result\n2. Take a snapshot to verify state\n3. Document the verification in LEARNINGS.md\n\n| Action | Verification |\n|--------|--------------|\n| Click submit | `wait_for(text=\"Success\")` + snapshot |\n| Navigate | `wait_for(state=\"networkidle\")` + snapshot |\n| Fill form | Snapshot shows filled values |\n| Login | Snapshot shows dashboard/logged-in state |\n\n**\"I clicked it\" is not verification. Prove the click worked.**\n</EXTREMELY-IMPORTANT>\n\n### Snapshot Verification\n\n```\n# 1. Perform action\nmcp__playwright__browser_click(element=\"Submit\")\n\n# 2. Wait for result\nmcp__playwright__browser_wait_for(text=\"Success\")\n\n# 3. Take snapshot to verify\nmcp__playwright__browser_snapshot()\n# Check snapshot contains expected elements\n```\n\n### Wait Conditions\n\n```\n# Wait for text to appear\nmcp__playwright__browser_wait_for(text=\"Welcome back\")\n\n# Wait for element\nmcp__playwright__browser_wait_for(selector=\"#success-message\")\n\n# Wait for network idle\nmcp__playwright__browser_wait_for(state=\"networkidle\")\n\n# Wait for navigation\nmcp__playwright__browser_wait_for(state=\"load\")\n```\n\n### Screenshots\n\n```\n# Full page\nmcp__playwright__browser_take_screenshot(path=\"/tmp/screenshot.png\", fullPage=true)\n\n# Viewport only\nmcp__playwright__browser_take_screenshot(path=\"/tmp/viewport.png\")\n\n# Specific element\nmcp__playwright__browser_take_screenshot(\n    path=\"/tmp/element.png\",\n    selector=\"#main-content\"\n)\n```\n\n## Form Handling\n\n### Text Inputs\n\n```\nmcp__playwright__browser_click(element=\"Username\")\nmcp__playwright__browser_type(text=\"john_doe\")\n\nmcp__playwright__browser_click(element=\"Password\")\nmcp__playwright__browser_type(text=\"secret123\")\n```\n\n### Dropdowns\n\n```\nmcp__playwright__browser_select_option(\n    element=\"Country dropdown\",\n    value=\"US\"\n)\n\n# Or by label\nmcp__playwright__browser_select_option(\n    element=\"Country\",\n    label=\"United States\"\n)\n```\n\n### Checkboxes and Radio Buttons\n\n```\n# Check checkbox\nmcp__playwright__browser_click(element=\"Accept terms checkbox\")\n\n# Verify checked state (via snapshot)\nmcp__playwright__browser_snapshot()\n# Look for checked=\"true\" in accessibility tree\n```\n\n### File Upload\n\n```\nmcp__playwright__browser_set_input_files(\n    selector=\"input[type=file]\",\n    files=[\"/path/to/file.pdf\"]\n)\n```\n\n## Advanced Patterns\n\n### Multi-Step Form\n\n```\n# Step 1\nmcp__playwright__browser_click(element=\"Name input\")\nmcp__playwright__browser_type(text=\"John Doe\")\nmcp__playwright__browser_click(element=\"Next button\")\nmcp__playwright__browser_wait_for(text=\"Step 2\")\n\n# Step 2\nmcp__playwright__browser_click(element=\"Email input\")\nmcp__playwright__browser_type(text=\"john@example.com\")\nmcp__playwright__browser_click(element=\"Next button\")\nmcp__playwright__browser_wait_for(text=\"Step 3\")\n\n# Step 3 - Submit\nmcp__playwright__browser_click(element=\"Submit button\")\nmcp__playwright__browser_wait_for(text=\"Success\")\n```\n\n### Handling Modals\n\n```\n# Click to open modal\nmcp__playwright__browser_click(element=\"Open Dialog\")\nmcp__playwright__browser_wait_for(text=\"Dialog Title\")\n\n# Interact with modal\nmcp__playwright__browser_click(element=\"Confirm button\")\nmcp__playwright__browser_wait_for(state=\"hidden\", selector=\".modal\")\n```\n\n### Iframes\n\n```\n# Switch to iframe\nmcp__playwright__browser_frame(name=\"payment-iframe\")\n\n# Interact within iframe\nmcp__playwright__browser_click(element=\"Card number\")\nmcp__playwright__browser_type(text=\"4111111111111111\")\n\n# Switch back to main\nmcp__playwright__browser_main_frame()\n```\n\n### Hover and Tooltips\n\n```\nmcp__playwright__browser_hover(element=\"Help icon\")\nmcp__playwright__browser_wait_for(text=\"This is the tooltip text\")\nmcp__playwright__browser_snapshot()\n```\n\n## Complete E2E Examples\n\n### Login Flow\n\n```\n# 1. Navigate to login page\nmcp__playwright__browser_navigate(url=\"https://app.example.com/login\")\nmcp__playwright__browser_wait_for(state=\"networkidle\")\n\n# 2. Take initial snapshot\nmcp__playwright__browser_snapshot()\n# Verify: Login form is visible\n\n# 3. Fill credentials\nmcp__playwright__browser_click(element=\"Email\")\nmcp__playwright__browser_type(text=\"user@example.com\")\n\nmcp__playwright__browser_click(element=\"Password\")\nmcp__playwright__browser_type(text=\"password123\")\n\n# 4. Submit\nmcp__playwright__browser_click(element=\"Sign In\")\nmcp__playwright__browser_wait_for(text=\"Dashboard\")\n\n# 5. Verify success\nmcp__playwright__browser_snapshot()\n# Verify: Dashboard is visible, user name shown\n\n# 6. Screenshot for evidence\nmcp__playwright__browser_take_screenshot(path=\"/tmp/login_success.png\")\n```\n\n### E-Commerce Checkout\n\n```\n# 1. Navigate to product\nmcp__playwright__browser_navigate(url=\"https://shop.example.com/product/123\")\nmcp__playwright__browser_wait_for(state=\"networkidle\")\n\n# 2. Add to cart\nmcp__playwright__browser_click(element=\"Add to Cart\")\nmcp__playwright__browser_wait_for(text=\"Added to cart\")\n\n# 3. Go to cart\nmcp__playwright__browser_click(element=\"Cart icon\")\nmcp__playwright__browser_wait_for(text=\"Your Cart\")\n\n# 4. Verify cart\nmcp__playwright__browser_snapshot()\n# Verify: Product in cart, correct price\n\n# 5. Proceed to checkout\nmcp__playwright__browser_click(element=\"Checkout\")\nmcp__playwright__browser_wait_for(text=\"Shipping Address\")\n\n# 6. Fill shipping\nmcp__playwright__browser_click(element=\"Address\")\nmcp__playwright__browser_type(text=\"123 Main St\")\n\nmcp__playwright__browser_click(element=\"City\")\nmcp__playwright__browser_type(text=\"New York\")\n\nmcp__playwright__browser_select_option(element=\"State\", value=\"NY\")\n\nmcp__playwright__browser_click(element=\"Zip\")\nmcp__playwright__browser_type(text=\"10001\")\n\n# 7. Continue to payment\nmcp__playwright__browser_click(element=\"Continue to Payment\")\nmcp__playwright__browser_wait_for(text=\"Payment Method\")\n\n# 8. Verify order summary\nmcp__playwright__browser_snapshot()\n# Verify: Correct items, shipping address, total\n\nmcp__playwright__browser_take_screenshot(path=\"/tmp/checkout_complete.png\")\n```\n\n### Search and Filter\n\n```\n# 1. Navigate\nmcp__playwright__browser_navigate(url=\"https://search.example.com\")\n\n# 2. Search\nmcp__playwright__browser_click(element=\"Search box\")\nmcp__playwright__browser_type(text=\"laptop\")\nmcp__playwright__browser_press(key=\"Enter\")\nmcp__playwright__browser_wait_for(text=\"results\")\n\n# 3. Apply filter\nmcp__playwright__browser_click(element=\"Price filter\")\nmcp__playwright__browser_click(element=\"Under $1000\")\nmcp__playwright__browser_wait_for(state=\"networkidle\")\n\n# 4. Verify filtered results\nmcp__playwright__browser_snapshot()\n# Verify: Results shown, filter applied\n\n# 5. Click first result\nmcp__playwright__browser_click(element=\"First product link\")\nmcp__playwright__browser_wait_for(text=\"Product Details\")\n\nmcp__playwright__browser_take_screenshot(path=\"/tmp/search_result.png\")\n```\n\n## Error Handling\n\n### Retry Pattern\n\n```\n# Attempt action with retry\nfor attempt in range(3):\n    try:\n        mcp__playwright__browser_click(element=\"Flaky Button\")\n        mcp__playwright__browser_wait_for(text=\"Success\", timeout=5000)\n        break  # Success\n    except:\n        if attempt == 2:\n            raise  # Give up after 3 attempts\n        time.sleep(1)  # Wait before retry\n```\n\n### Timeout Handling\n\n```\n# Set explicit timeout\nmcp__playwright__browser_wait_for(\n    text=\"Slow loading content\",\n    timeout=30000  # 30 seconds\n)\n```\n\n## Limitations\n\n<EXTREMELY-IMPORTANT>\n### What Playwright MCP Cannot Do\n\n| Need | Why Playwright Fails | Use Instead |\n|------|---------------------|-------------|\n| Read console.log | No console access | Chrome MCP `read_console_messages` |\n| Inspect API responses | No network access | Chrome MCP `read_network_requests` |\n| Execute page JavaScript | No JS execution | Chrome MCP `javascript_tool` |\n| Record GIF | No recording capability | Chrome MCP `gif_creator` |\n\n**If you need debugging capabilities, switch to Chrome MCP.**\n</EXTREMELY-IMPORTANT>\n\n## Integration\n\nThis skill is referenced by `dev-test` for Playwright browser automation.\n\n**For debugging (console/network), use:** `Skill(skill=\"workflows:dev-test-chrome\")`\n\nFor TDD protocol, see: `Skill(skill=\"workflows:dev-tdd\")`"
              },
              {
                "name": "dev-test",
                "description": "This skill should be used when the user needs to 'debug web applications', 'test UI interactions', 'capture screenshots or network requests', 'test desktop automation', or needs to select between testing tools. Routes to platform-specific E2E testing skills: Chrome MCP for debugging, Playwright for CI/CD, Hammerspoon for macOS, Linux for X11/Wayland.",
                "path": "skills/dev-test/SKILL.md",
                "frontmatter": {
                  "name": "dev-test",
                  "description": "This skill should be used when the user needs to 'debug web applications', 'test UI interactions', 'capture screenshots or network requests', 'test desktop automation', or needs to select between testing tools. Routes to platform-specific E2E testing skills: Chrome MCP for debugging, Playwright for CI/CD, Hammerspoon for macOS, Linux for X11/Wayland."
                },
                "content": "## Where This Fits\n\n```\nMain Chat                          Task Agent\n─────────────────────────────────────────────────────\ndev-implement\n  → dev-ralph-loop (loads dev-tdd)\n    → dev-delegate\n      → Task agent ──────────────→ uses dev-test (this skill)\n                                     ↓ loads dev-tdd again\n                                   has TDD protocol + gates\n                                     → routes to specific tool\n```\n\n<EXTREMELY-IMPORTANT>\n## Load TDD Enforcement (REQUIRED)\n\nBefore choosing testing tools, you MUST load the TDD skill to ensure gate compliance:\n\n```\nSkill(skill=\"workflows:dev-tdd\")\n```\n\nThis loads:\n- Task reframing (your job is writing tests, not features)\n- **The Execution Gate** (6 mandatory gates before E2E testing)\n- **GATE 5: READ LOGS** (mandatory - cannot skip)\n- The Iron Law of TDD (test-first approach)\n\n**Read dev-tdd skill content now before selecting testing tools.**\n</EXTREMELY-IMPORTANT>\n\n**This skill routes to the right testing tool.** The loaded `dev-tdd` skill provides TDD protocol details.\n\n## Contents\n\n- [The Iron Law](#the-iron-law-of-testing)\n- [Browser Testing Decision Tree](#browser-testing-decision-tree)\n- [Platform Detection](#platform-detection)\n- [Sub-Skills Reference](#sub-skills-reference)\n- [Unit & Integration Tests](#unit--integration-tests)\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Testing\n\n**YOU MUST WRITE E2E TESTS FOR USER-FACING FEATURES. This is not negotiable.**\n\nWhen your changes affect what users see or interact with, you MUST:\n1. Write an E2E test that simulates user behavior\n2. Run it and verify it PASSES (not just unit tests)\n3. Document: \"E2E: [test name] passes with [evidence]\"\n4. Include screenshot/snapshot for visual changes\n\n**Unit tests prove components work. E2E tests prove YOUR feature works for users.**\n\n### Rationalization Prevention\n\nWhen you catch yourself thinking these rationalizations, STOP—you're about to skip E2E tests:\n\n| Thought | Why You're Wrong | Do Instead |\n|---------|-----------------|-----------|\n| \"Unit tests are enough\" | Your unit tests don't test user flows. | Write E2E. |\n| \"E2E is too slow\" | You're choosing slow tests < shipped bugs. | Write E2E. |\n| \"I'll add E2E later\" | You won't. Your future self won't either. | Write it NOW. |\n| \"This is just backend\" | Does it affect user output? Then YOU need E2E. | Write E2E. |\n| \"The tool setup is complex\" | Your complexity = complex failure modes. E2E finds them. | Write E2E. |\n| \"The UI is unchanged\" | Your assumption isn't proven. | Prove it with a visual snapshot. |\n| \"Manual testing is faster\" | You're LYING about coverage to yourself. | Write E2E. |\n| \"It's just a small change\" | Your small change breaks UIs. E2E proves it doesn't. | Write E2E. |\n| \"User can verify\" | NO. You don't trust users with QA. | Automated verification or it didn't happen. |\n| **\"Log checking is my E2E test\"** | **You're confusing observability with verification.** | **Verify your actual outputs.** |\n| **\"Screenshots are too hard to capture\"** | **Your avoidance = hard to debug in production later.** | **Automate it.** |\n\n### Fake E2E Detection - STOP\n\n**If your \"E2E test\" does any of these, it's NOT E2E:**\n\n| Pattern | Why It's Fake | Real E2E Alternative |\n|---------|---------------|----------------------|\n| `grep \"success\" logs.txt` | Only proves code ran | Verify actual output file/UI/API response |\n| `assert mock.called` | Tests mock, not real system | Use real integration, verify real data |\n| `cat output.txt \\| wc -l` | File exists ≠ correct content | Read file, assert exact expected content |\n| \"I ran it manually\" | No automation = no evidence | Capture manual test as automated test |\n| Check log for icon name | Observability, not verification | Screenshot + visual diff of rendered icon |\n| Exit code 0 | Process succeeded ≠ output correct | Verify the actual output data |\n\n**The test:** If removing the actual implementation still passes your \"E2E test\", it's fake.\n\n**Example of fake E2E that caught nothing:**\n```python\n# FAKE E2E - only checks logs\ndef test_icon_theme_change():\n    run_command(\"set-theme papirus\")\n    logs = read_logs()\n    assert \"papirus\" in logs  # ❌ FAKE - only proves code ran\n    # BUG: 89% of icons weren't changed, test still passed!\n```\n\n**Real E2E that would have caught the bug:**\n```python\n# REAL E2E - verifies actual output\ndef test_icon_theme_change():\n    run_command(\"set-theme papirus\")\n    screenshot = capture_desktop()\n    assert visual_diff(screenshot, \"expected_papirus.png\") < threshold  # ✅ REAL\n    # This would have shown 89% of icons were wrong\n```\n\n### Red Flags - STOP If Thinking:\n\nIf you catch yourself thinking these patterns, STOP—you're about to skip E2E:\n\n| Thought | Why You're Wrong | Do Instead |\n|---------|-----------------|-----------|\n| \"Tests pass\" (only unit) | Your unit tests ≠ E2E | Write E2E test |\n| \"Code looks correct\" | You're only looking ≠ running user flow | Run E2E |\n| \"It worked when I tried it\" | Your manual testing ≠ automated | Capture as E2E |\n| \"Screenshot shows it works\" | Your static screenshot ≠ interaction test | Add automation |\n</EXTREMELY-IMPORTANT>\n\n## Browser Testing Decision Tree\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    BROWSER TESTING REQUIRED?                     │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n        ┌─────────────────────────────────────────────┐\n        │  Need to debug JS errors or API calls?       │\n        │  (console.log, network requests, XHR)        │\n        └─────────────────────────────────────────────┘\n                    │                    │\n                   YES                   NO\n                    │                    │\n                    ▼                    ▼\n        ┌───────────────────┐  ┌──────────────────────────┐\n        │   CHROME MCP      │  │  Running in CI/CD?        │\n        │   (debugging)     │  │  (headless, automated)    │\n        └───────────────────┘  └──────────────────────────┘\n                                      │           │\n                                     YES          NO\n                                      │           │\n                                      ▼           ▼\n                        ┌──────────────┐  ┌───────────────────┐\n                        │ PLAYWRIGHT   │  │ Cross-browser     │\n                        │ MCP          │  │ needed?           │\n                        └──────────────┘  └───────────────────┘\n                                                │          │\n                                               YES         NO\n                                                │          │\n                                                ▼          ▼\n                                    ┌──────────────┐ ┌────────────┐\n                                    │ PLAYWRIGHT   │ │ Either OK  │\n                                    │ MCP          │ │ (prefer    │\n                                    └──────────────┘ │ Playwright)│\n                                                     └────────────┘\n```\n\n<EXTREMELY-IMPORTANT>\n### Iron Laws: Browser MCP Selection\n\n**YOU MUST USE CHROME MCP FOR API/CONSOLE DEBUGGING. NO EXCEPTIONS.**\n**YOU MUST USE PLAYWRIGHT MCP FOR CI/CD TESTING. NO EXCEPTIONS.**\n\n### Quick Decision Table\n\n| Need | Tool | Why |\n|------|------|-----|\n| Debug console errors | **Chrome MCP** | `read_console_messages` |\n| Inspect API calls/responses | **Chrome MCP** | `read_network_requests` |\n| Execute custom JS in page | **Chrome MCP** | `javascript_tool` |\n| Record interaction as GIF | **Chrome MCP** | `gif_creator` |\n| Headless/CI automation | **Playwright MCP** | Headless mode |\n| Cross-browser testing | **Playwright MCP** | Firefox/WebKit support |\n| Standard E2E suite | **Playwright MCP** | Test isolation, maturity |\n| Interactive debugging | **Chrome MCP** | Real browser, console access |\n\n### Capability Comparison\n\n| Capability | Playwright MCP | Chrome MCP |\n|------------|---------------|------------|\n| Navigate/click/type | ✅ | ✅ |\n| Accessibility tree | ✅ `browser_snapshot` | ✅ `read_page` |\n| Screenshots | ✅ | ✅ |\n| **Console messages** | ❌ | ✅ `read_console_messages` |\n| **Network requests** | ❌ | ✅ `read_network_requests` |\n| **JavaScript execution** | ❌ | ✅ `javascript_tool` |\n| **GIF recording** | ❌ | ✅ `gif_creator` |\n| **Headless mode** | ✅ | ❌ (requires visible browser) |\n| **Cross-browser** | ✅ (Chromium/Firefox/WebKit) | ❌ (Chrome only) |\n| Natural language find | ❌ | ✅ `find` |\n\n### Rationalization Prevention (Browser MCP)\n\n| Thought | Why You're Wrong | Do Instead |\n|---------|-----------------|-----------|\n| \"I'll check the console manually\" | You can't capture all edge cases manually. | Use Chrome MCP `read_console_messages` |\n| \"I can infer the API response\" | Your inference is wrong. Real data differs. | Use Chrome MCP `read_network_requests` |\n| \"Playwright can do everything\" | You're wrong. It cannot read console or network. | Use Chrome MCP for debugging |\n| \"Chrome MCP is enough for CI\" | You're ignoring constraints—it requires visible browser. | Use Playwright MCP for CI/CD |\n| \"I'll just look at DevTools\" | Your manual inspection is not automated. | Automate with Chrome MCP |\n| \"Headless doesn't matter\" | You're wrong. Your CI/CD requires headless. | Use Playwright MCP |\n</EXTREMELY-IMPORTANT>\n\n## Platform Detection\n\nDetect the operating system and display server to select the appropriate testing tool:\n\n```bash\n# Detect platform for desktop automation\ncase \"$(uname -s)\" in\n    Darwin) echo \"macOS - use dev-test-hammerspoon\" ;;\n    Linux)\n        if [ \"$XDG_SESSION_TYPE\" = \"wayland\" ]; then\n            echo \"Linux/Wayland - use dev-test-linux (ydotool)\"\n        else\n            echo \"Linux/X11 - use dev-test-linux (xdotool)\"\n        fi\n        ;;\nesac\n```\n\n### Desktop Automation Decision Tree\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                   DESKTOP AUTOMATION REQUIRED?                   │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n                    ┌─────────────────┐\n                    │   Platform?      │\n                    └─────────────────┘\n                    /        |         \\\n                 macOS    Linux      Windows\n                   │         │           │\n                   ▼         ▼           ▼\n        ┌──────────────┐ ┌─────────┐ ┌─────────┐\n        │ HAMMERSPOON  │ │ LINUX   │ │ NOT     │\n        │ (dev-test-   │ │ (dev-   │ │ SUPPORTED│\n        │ hammerspoon) │ │ test-   │ └─────────┘\n        └──────────────┘ │ linux)  │\n                         └─────────┘\n                              │\n                    ┌─────────┴─────────┐\n                    │   Display Server?  │\n                    └───────────────────┘\n                         /         \\\n                    Wayland        X11\n                       │            │\n                       ▼            ▼\n                 ┌──────────┐ ┌──────────┐\n                 │ ydotool  │ │ xdotool  │\n                 └──────────┘ └──────────┘\n```\n\n## Sub-Skills Reference\n\n<EXTREMELY-IMPORTANT>\n### Tool Availability Gate\n\n**Verify tools are available BEFORE proceeding. Missing tools = FULL STOP.**\n\nEach sub-skill has its own availability gate. Load the appropriate skill and follow its gate.\n</EXTREMELY-IMPORTANT>\n\n### Browser Testing\n\n| Skill | Use Case | Key Capabilities |\n|-------|----------|------------------|\n| `Skill(skill=\"workflows:dev-test-chrome\")` | Debugging, console/network inspection | `read_console_messages`, `read_network_requests`, `javascript_tool` |\n| `Skill(skill=\"workflows:dev-test-playwright\")` | CI/CD, headless, cross-browser E2E | Headless mode, Firefox/WebKit, test isolation |\n\n### Desktop Automation\n\n| Skill | Platform | Primary Tool |\n|-------|----------|--------------|\n| `Skill(skill=\"workflows:dev-test-hammerspoon\")` | macOS | Hammerspoon (`hs`) |\n| `Skill(skill=\"workflows:dev-test-linux\")` | Linux | ydotool (Wayland) / xdotool (X11) |\n\n## Unit & Integration Tests\n\n### Test Discovery\n\nLocate test directories and identify the test framework used in the project:\n\n```bash\n# Find test directory\nls -d tests/ test/ spec/ __tests__/ 2>/dev/null\n\n# Find test framework\ncat package.json 2>/dev/null | grep -E \"(test|jest)\"\ncat pyproject.toml 2>/dev/null | grep -i pytest\ncat Cargo.toml 2>/dev/null | grep -i \"\\[dev-dependencies\\]\"\ncat meson.build 2>/dev/null | grep -i test\n```\n\n### Common Test Frameworks\n\n| Language | Framework | Command |\n|----------|-----------|---------|\n| Python | pytest | `pytest tests/ -v` |\n| JavaScript | jest | `npm test` |\n| TypeScript | vitest | `npx vitest` |\n| Rust | cargo | `cargo test` |\n| C/C++ | meson | `meson test -C build -v` |\n| Go | go test | `go test ./...` |\n\n### CLI Application Testing\n\nExecute CLI applications with test inputs and verify outputs against expected results:\n\n```bash\n# Run with test inputs\n./app --test-mode input.txt > output.txt\n\n# Compare to expected\ndiff expected.txt output.txt\n\n# Check exit code\n./app --validate file && echo \"PASS\" || echo \"FAIL\"\n```\n\n## Output Requirements\n\n**Every test run MUST be documented in LEARNINGS.md:**\n\n```markdown\n## Test Run: [Description]\n\n**Tool:** [Chrome MCP / Playwright / Hammerspoon / ydotool / pytest / etc.]\n\n**Command:**\n```bash\npytest tests/ -v\n```\n\n**Output:**\n```\ntests/test_feature.py::test_basic PASSED\ntests/test_feature.py::test_edge_case PASSED\ntests/test_feature.py::test_error FAILED\n\n1 failed, 2 passed\n```\n\n**Result:** 2/3 PASS, 1 FAIL\n\n**Next:** Fix test_error failure\n```\n\n## Integration\n\nFor TDD protocol (RED-GREEN-REFACTOR), see:\n```\nSkill(skill=\"workflows:dev-tdd\")\n```\n\nThis skill is invoked by Task agents during `dev-implement` phase."
              },
              {
                "name": "dev-tools",
                "description": "This skill should be used when the user asks to \"what plugins are available\", \"list dev tools\", \"what MCP servers can I use\", \"enable code intelligence\", or needs to discover available development plugins like serena, context7, or playwright.",
                "path": "skills/dev-tools/SKILL.md",
                "frontmatter": {
                  "name": "dev-tools",
                  "description": "This skill should be used when the user asks to \"what plugins are available\", \"list dev tools\", \"what MCP servers can I use\", \"enable code intelligence\", or needs to discover available development plugins like serena, context7, or playwright."
                },
                "content": "# Available Development Plugins\n\nThese plugins extend Claude Code capabilities for development workflows. Enable when needed for specific tasks.\n\n## Code Intelligence\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `serena` | Semantic code analysis, refactoring, symbol navigation | `claude --enable-plugin serena@claude-plugins-official` |\n| `pyright-lsp` | Python type checking and diagnostics | `claude --enable-plugin pyright-lsp@claude-plugins-official` |\n| `clangd-lsp` | C/C++ code intelligence | Already enabled |\n\n## Documentation\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `context7` | Up-to-date library/framework docs lookup | `claude --enable-plugin context7@claude-plugins-official` |\n\n## Testing & Automation\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `playwright` | Browser automation, E2E testing, screenshots | `claude --enable-plugin playwright@claude-plugins-official` |\n\n## Workflow\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `ralph-loop` | Self-referential iteration loops | Already enabled |\n| `hookify` | Create custom hooks from conversation patterns | Already enabled |\n\n## When to Enable\n\n- **serena**: Complex refactoring, finding symbol references, understanding large codebases\n- **context7**: Need current docs for React, pandas, FastAPI, etc.\n- **playwright**: Testing web UIs, scraping, taking screenshots\n- **pyright-lsp**: Python projects needing strict type checking\n\n## Usage\n\nEnable a plugin for the current session by running:\n```bash\n# Enable a plugin: claude --enable-plugin <plugin-name>\nclaude --enable-plugin <plugin-name>\n```\n\nEnable a plugin for a project by adding to `.claude/settings.json`:\n```json\n{\n  \"enabledPlugins\": {\n    \"serena@claude-plugins-official\": true\n  }\n}\n```"
              },
              {
                "name": "dev-verify",
                "description": "This skill should be used when the user asks to 'verify completion', 'check that tests pass', 'confirm feature works', or REQUIRED Phase 7 of /dev workflow (final). Enforces fresh runtime evidence before claiming completion.",
                "path": "skills/dev-verify/SKILL.md",
                "frontmatter": {
                  "name": "dev-verify",
                  "description": "This skill should be used when the user asks to 'verify completion', 'check that tests pass', 'confirm feature works', or REQUIRED Phase 7 of /dev workflow (final). Enforces fresh runtime evidence before claiming completion.",
                  "version": "1.0.0"
                },
                "content": "Announce: \"Using dev-verify (Phase 7) to confirm completion with fresh evidence.\"\n\n## Contents\n\n- [The Iron Law of Verification](#the-iron-law-of-verification)\n- [Red Flags - STOP Immediately If You Think](#red-flags---stop-immediately-if-you-think)\n- [The Gate Function](#the-gate-function)\n- [Claims Requiring Evidence](#claims-requiring-evidence)\n- [Insufficient Evidence](#insufficient-evidence)\n- [Verification Patterns](#verification-patterns)\n- [User Acceptance (Final Step)](#user-acceptance-final-step)\n- [Bottom Line](#bottom-line)\n\n# Verification Gate\n\n<EXTREMELY-IMPORTANT>\n## Your Job is to Write Automated Tests\n\n**The automated test IS your deliverable. The implementation just makes the test pass.**\n\nReframe your task:\n- ❌ \"Implement feature X, and test it\"\n- ✅ \"Write an automated test that proves feature X works. Then make it pass.\"\n\nThe test proves value. The implementation is a means to an end.\n\nWithout a REAL automated test (executes code, verifies behavior), you have delivered NOTHING.\n</EXTREMELY-IMPORTANT>\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Verification\n\n**NO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE. This is not negotiable.**\n\nBefore claiming ANYTHING is complete, you MUST:\n1. IDENTIFY - Which command proves your assertion?\n2. RUN - Execute the command fresh (not from cache/memory)\n3. READ - Review full output and exit codes\n4. VERIFY - Confirm output supports your claim\n5. Only THEN make the claim\n\nThis applies even when:\n- \"I just ran it a moment ago\"\n- \"The agent said it passed\"\n- \"It should work\"\n- \"I'm confident it's fine\"\n\n**If you catch yourself about to claim completion without fresh evidence, STOP.**\n</EXTREMELY-IMPORTANT>\n\n## Red Flags - STOP Immediately If You Catch Yourself Thinking:\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"It should work\" | \"Should\" isn't evidence | Run the command |\n| \"I'm pretty sure it passes\" | Confidence isn't verification | Run the command |\n| \"The agent reported success\" | Agent reports need confirmation | Run it yourself |\n| \"I ran it earlier\" | Earlier isn't fresh | Run it again |\n| \"The code exists\" | Existing ≠ working | Run and check output |\n| \"Grep shows the function\" | Pattern match ≠ runtime test | Run the function |\n\n## The Gate Function\n\nBefore making ANY status claim:\n\n```\n1. IDENTIFY → Which command proves your assertion?\n2. RUN     → Execute the command fresh\n3. READ    → Review full output and exit codes\n4. VERIFY  → Confirm output supports your claim\n5. CLAIM   → Only after steps 1-4\n```\n\n**Skipping any step is dishonest, not verification.**\n\n## Claims Requiring Evidence\n\n| Claim | Required Evidence |\n|-------|-------------------|\n| \"Tests pass\" | Test output showing 0 failures |\n| \"Build succeeds\" | Exit code 0 from build command |\n| \"Linter clean\" | Linter output showing 0 errors |\n| \"Bug fixed\" | Test that failed now passes |\n| \"Feature complete\" | All acceptance criteria verified |\n| **\"User-facing feature works\"** | **E2E test output showing PASS** |\n\n<EXTREMELY-IMPORTANT>\n## The E2E Evidence Gate\n\n**USER-FACING CLAIMS REQUIRE E2E EVIDENCE. Unit tests are insufficient.**\n\n| Claim | Unit Test Evidence | E2E Evidence Required |\n|-------|--------------------|-----------------------|\n| \"API works\" | ❌ Insufficient | ✅ Full request/response test |\n| \"UI renders\" | ❌ Insufficient | ✅ Playwright snapshot/interaction |\n| \"Feature complete\" | ❌ Insufficient | ✅ User flow simulation |\n| \"No regressions\" | ❌ Insufficient | ✅ E2E suite passes |\n\n### Fake E2E Patterns - STOP\n\n**These are NOT E2E tests. They are observability, not verification.**\n\n| ❌ Fake E2E | ✅ Real E2E |\n|-------------|-------------|\n| \"Log shows function was called\" | \"Screenshot shows correct UI rendered\" |\n| \"grep papirus in logs\" | \"grim screenshot + visual diff confirms icon changed\" |\n| \"Console output contains 'success'\" | \"Playwright assertion: element.textContent === 'Success'\" |\n| \"File was created\" | \"E2E test opens file and verifies contents\" |\n| \"Process exited 0\" | \"Functional test verifies actual output matches spec\" |\n| \"Mock returned expected value\" | \"Real integration returns expected value\" |\n\n**Red Flag:** If you catch yourself thinking \"logs prove it works\" - STOP, you're about to claim false verification. Logs prove code executed, not that it produced correct results. E2E means verifying the actual output users see.\n\n### Rationalization Prevention (E2E)\n\n| Thought | Reality |\n|---------|---------|\n| \"Unit tests cover it\" | Unit tests don't simulate users. Where's YOUR E2E? |\n| \"E2E would be redundant\" | YOU'LL catch bugs with redundancy. Write E2E. |\n| \"No time for E2E\" | YOU don't have time to fix production bugs? Write E2E. |\n| \"Feature is internal\" | Does it affect user output? Then YOU need E2E. |\n| \"I manually tested\" | YOU provided no evidence. Automate it. |\n| **\"Log checking verifies it works\"** | **YOUR log checking only verifies code executed, not results. Not E2E.** |\n| **\"E2E with screenshots is too complex\"** | **If YOU can't verify it simply, your feature isn't done. Complexity = bugs hiding.** |\n| **\"Implementation is done, testing is just verification\"** | **Testing IS YOUR implementation. Untested code is unfinished code.** |\n\n### The E2E Gate Function\n\nFor user-facing changes, add to verification:\n\n```\n1. IDENTIFY → Which E2E test proves user-facing behavior?\n2. RUN     → Execute E2E test fresh\n3. READ    → Review full output (screenshots if visual)\n4. VERIFY  → User flow works as specified\n5. CLAIM   → Only after E2E evidence exists\n```\n\n**\"Unit tests pass\" is not \"feature complete\" for user-facing changes.**\n\n### GUI Application Gate (CRITICAL)\n\n<EXTREMELY-IMPORTANT>\n**For GUI applications, you MUST complete the 6-gate sequence from dev-tdd BEFORE E2E testing:**\n\n```\nGATE 1: BUILD\nGATE 2: LAUNCH (with file-based logging)\nGATE 3: WAIT\nGATE 4: CHECK PROCESS\nGATE 5: READ LOGS ← MANDATORY, CANNOT SKIP\nGATE 6: VERIFY LOGS\nTHEN AND ONLY THEN: E2E tests/screenshots\n```\n\n**You cannot skip GATE 5 (READ LOGS).** If you catch yourself about to take screenshots without reading logs first, STOP.\n\nSee `Skill(skill=\"workflows:dev-tdd\")` for the full gate sequence with examples.\n</EXTREMELY-IMPORTANT>\n</EXTREMELY-IMPORTANT>\n\n## Insufficient Evidence\n\nThese do NOT count as verification:\n\n- Previous runs (must be fresh)\n- Assumptions (\"it should work\")\n- Partial checks (ran some tests, not all)\n- Agent reports without independent confirmation\n- \"I think...\" / \"It seems...\" / \"Probably...\"\n\n## Honesty Requirement\n\n<EXTREMELY-IMPORTANT>\n**Claiming completion without fresh evidence is LYING.**\n\nWhen you say \"Feature complete\", you are asserting:\n- You ran the verification commands yourself (fresh)\n- You saw the output with your own tokens\n- The output confirms the claim\n\nSaying \"complete\" based on stale data or agent reports is not \"summarizing\" - it is LYING about project state.\n\n**\"Still verifying\" is honest. \"Complete\" without evidence is fraud.**\n</EXTREMELY-IMPORTANT>\n\n## Rationalization Prevention\n\nThese thoughts mean STOP—you're about to claim falsely:\n\n| Thought | Reality |\n|---------|---------|\n| \"I just ran it\" | \"Just\" = stale. YOU must run it AGAIN. |\n| \"The agent said it passed\" | Agent reports need YOUR confirmation. YOU run it. |\n| \"It should work\" | \"Should\" is hope. YOU run and see output. |\n| \"I'm confident\" | YOUR confidence ≠ verification. YOU run the command. |\n| \"We already verified earlier\" | Earlier ≠ now. YOU need fresh evidence only. |\n| \"User will verify it\" | NO. YOU verify before claiming. User trusts YOUR claim. |\n| \"Close enough\" | Close ≠ complete. YOU verify fully. |\n| \"Time to move on\" | YOU only move on after FRESH verification. |\n\n**STRUCTURAL VERIFICATION IS NOT RUNTIME VERIFICATION:**\n\n| ❌ NOT Verification | ✅ IS Verification |\n|---------------------|-------------------|\n| \"Code exists in file\" | \"Code ran and produced output X\" |\n| \"Function is defined\" | \"Function was called and returned Y\" |\n| \"Grep found the pattern\" | \"Program output shows expected behavior\" |\n| \"ast-grep found the code\" | \"Test executed and passed with output\" |\n| \"Diff shows the change\" | \"Change tested with actual input/output\" |\n| \"Implementation looks correct\" | \"Ran test, saw PASS in logs\" |\n\n**The key difference:**\n- Structural: \"The code IS THERE\" (useless)\n- Runtime: \"The code WORKS\" (valid)\n\nIf you find yourself saying \"the code exists\" or \"I verified the implementation\" without running it, **STOP** - you're doing structural analysis, not verification.\n\n## Verification Patterns\n\n### Tests\n```bash\n# Run tests (e.g., npm test, pytest, cargo test)\nnpm test\n\n# Check results: \"34/34 pass\" = can claim tests pass\n# \"33/34 pass\" = cannot claim success (partial fail)\n```\n\n**Tool description:** Run automated test suite to verify all tests pass\n\n### Regression Test\n```bash\n# 1. Write test → run (should fail initially)\n# 2. Apply fix → run (should pass)\n# 3. Revert fix → run (must fail again to confirm fix)\n# 4. Restore fix → run (must pass to confirm success)\n```\n\n**Tool description:** Execute regression test cycle to validate bug fix reproducibility\n\n### Build\n```bash\nnpm run build && echo \"Exit code: $?\"\n# Must see \"Exit code: 0\" to claim success\n```\n\n**Tool description:** Build application and verify exit code is 0\n\n## User Acceptance (Final Step)\n\nAfter technical verification passes, confirm with user. Use the AskUserQuestion pattern:\n\n**Tool description:** Request user confirmation that implementation meets specified requirements\n\n```yaml\nquestion: \"Does this implementation meet your requirements?\"\noptions:\n  - label: \"Yes, requirements met\"\n    description: \"Feature works as designed, ready to merge\"\n  - label: \"Partially\"\n    description: \"Core works but missing some requirements\"\n  - label: \"No\"\n    description: \"Does not meet requirements, needs more work\"\n```\n\nReference `.claude/SPEC.md` when asking—remind user of the success criteria they defined.\n\nIf user responds \"Partially\" or \"No\":\n1. Ask which specific requirement is not met\n2. Return to `/dev-implement` to address gaps\n3. Re-run verification\n\n**Only claim COMPLETE when:**\n- [ ] All technical tests pass (automated)\n- [ ] User confirms requirements met (manual)\n\n## Bottom Line\n\n**Two types of verification required:**\n\n1. **Technical** - Run commands, see output, confirm no errors\n2. **Requirements** - Ask user if it does what they wanted\n\nBoth must pass. No shortcuts exist.\n\n## Workflow Complete\n\nWhen user confirms \"Yes, requirements met\":\n\nAnnounce: \"Dev workflow complete. All 7 phases passed.\"\n\nThe `/dev` workflow is now finished. Offer to:\n- Commit the changes\n- Clean up `.claude/` files\n- Start a new feature with `/dev`\n\n---\n\n## Key Principles\n\n**Fresh Evidence Always:** Every claim requires proof from a fresh command execution, not cached results or agent reports.\n\n**Runtime Over Structural:** Verify code works by running it, not by checking if code exists. Structural analysis cannot prove behavior.\n\n**E2E for User-Facing:** User-visible features require end-to-end evidence (screenshots, user flow tests), not unit tests alone.\n\n**Honesty Requirement:** Claiming completion without fresh evidence is misrepresenting project state. Only advance when fully verified."
              },
              {
                "name": "dev-worktree",
                "description": "This skill should be used when the user asks to \"create an isolated worktree\", \"set up worktree for feature\", \"create a feature branch worktree\", or needs workspace isolation with automatic dependency setup and test verification.",
                "path": "skills/dev-worktree/SKILL.md",
                "frontmatter": {
                  "name": "dev-worktree",
                  "description": "This skill should be used when the user asks to \"create an isolated worktree\", \"set up worktree for feature\", \"create a feature branch worktree\", or needs workspace isolation with automatic dependency setup and test verification."
                },
                "content": "# Create Development Worktree\n\nCreate an isolated git worktree for feature work, ensuring workspace isolation and clean baseline.\n\n## The Process\n\n### Step 1: Ensure .worktrees/ is Gitignored\n\n**CRITICAL:** Verify worktree directory is gitignored to prevent accidental commits.\n\n**Run:**\n```bash\nif ! git check-ignore -q .worktrees 2>/dev/null; then\n  echo \"Adding .worktrees/ to .gitignore\"\n  echo \".worktrees/\" >> .gitignore\n  git add .gitignore\n  git commit -m \"chore: add .worktrees/ to gitignore\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\"\nfi\n```\n\n**Description:** dev-worktree: check if .worktrees is gitignored and add if missing\n\n### Step 2: Determine Branch Name\n\nExtract from `.claude/PLAN.md` first line or infer from feature name:\n\n**Run:**\n```bash\n# Extract from PLAN.md if exists\nfeature_name=$(grep -m1 '^# ' .claude/PLAN.md 2>/dev/null | sed 's/^# //' | tr '[:upper:] ' '[:lower:]-' | sed 's/[^a-z0-9-]//g')\n\n# Or ask user if needed\n```\n\n**Description:** dev-worktree: extract or prompt for feature name\n\nBranch name format: `feature/${feature_name}`\n\n### Step 3: Create Worktree\n\n**Run:**\n```bash\n# Create worktree with new branch\ngit worktree add .worktrees/${feature_name} -b feature/${feature_name}\n\n# Change to worktree directory\ncd .worktrees/${feature_name}\n```\n\n**Description:** dev-worktree: create isolated git worktree with feature branch\n\n### Step 4: Run Project Setup\n\nAuto-detect and run setup based on project files:\n\n**Run:**\n```bash\n# Node.js\nif [ -f package.json ]; then\n  npm install\nfi\n\n# Python\nif [ -f requirements.txt ]; then\n  pip install -r requirements.txt\nfi\nif [ -f pyproject.toml ]; then\n  poetry install || pip install -e .\nfi\nif [ -f pixi.toml ]; then\n  pixi install\nfi\n\n# Rust\nif [ -f Cargo.toml ]; then\n  cargo build\nfi\n\n# Go\nif [ -f go.mod ]; then\n  go mod download\nfi\n```\n\n**Description:** dev-worktree: auto-detect project type and install dependencies\n\n### Step 5: Verify Clean Baseline (Optional)\n\nRun tests to verify baseline if project has test suite:\n\n**Run:**\n```bash\n# Examples - auto-detect test command\nif [ -f package.json ] && grep -q '\"test\"' package.json; then\n  npm test\nelif [ -f Cargo.toml ]; then\n  cargo test\nelif [ -f pytest.ini ] || [ -f pyproject.toml ]; then\n  pytest\nelif [ -f go.mod ]; then\n  go test ./...\nfi\n```\n\n**Description:** dev-worktree: auto-detect and run project test suite\n\n**If tests fail:** Report failures and note that baseline has issues.\n**If tests pass:** Report clean baseline.\n\n### Step 6: Report Ready\n\nReport completion status:\n\n```\n✓ Worktree created: .worktrees/${feature_name}\n✓ Branch: feature/${feature_name}\n✓ Dependencies installed\n✓ Tests passing (or note if failed)\n\nReady for implementation.\n```\n\n## Safety Checks\n\n**Execute before creating worktree:**\n- Verify .worktrees/ is gitignored\n- Add to .gitignore if missing\n- Commit gitignore change\n\n**Execute after creating worktree:**\n- Run project setup (npm install, etc.)\n- Verify clean baseline with tests\n- Report status\n\n## Red Flags\n\n**Critical - Never deviate from these rules:**\n- Do not create worktree without verifying gitignore\n- Do not skip project setup commands\n- Do not proceed without reporting test status\n\n**Critical - Always follow these rules:**\n- Verify .worktrees/ is ignored before creating\n- Auto-detect project type and run appropriate setup\n- Report test baseline status\n\n## Common Patterns\n\n### Node.js Project\n```bash\n# .worktrees/ gitignored → create worktree → npm install → npm test\n```\n\n### Python Project\n```bash\n# .worktrees/ gitignored → create worktree → pixi install → pytest\n```\n\n### Rust Project\n```bash\n# .worktrees/ gitignored → create worktree → cargo build → cargo test\n```\n\n## Workflow Transition\n\nAfter worktree creation, the workspace is ready. Proceed to dev-implement to start implementing tasks.\n\nCurrent working directory: `.worktrees/${feature_name}`\n\nAll implementation work happens here, keeping main workspace clean."
              },
              {
                "name": "ds-brainstorm",
                "description": "This skill should be used when the user asks to \"start a data science project\", \"brainstorm analysis\", \"plan a data analysis\", or wants to clarify analysis requirements. REQUIRED Phase 1 of /ds workflow. Uses Socratic questioning to clarify goals, data sources, and constraints.",
                "path": "skills/ds-brainstorm/SKILL.md",
                "frontmatter": {
                  "name": "ds-brainstorm",
                  "description": "This skill should be used when the user asks to \"start a data science project\", \"brainstorm analysis\", \"plan a data analysis\", or wants to clarify analysis requirements. REQUIRED Phase 1 of /ds workflow. Uses Socratic questioning to clarify goals, data sources, and constraints."
                },
                "content": "## First: Activate Workflow\n\nActivate the ds workflow to enable workflow-specific hooks (data quality checks, output verification):\n\n```bash\n# Activate ds workflow and enable development mode for hook verification\npython3 -c \"\nimport sys\nsys.path.insert(0, '\\${CLAUDE_PLUGIN_ROOT}/hooks/scripts/common')\nfrom session import activate_workflow, activate_dev_mode\nactivate_workflow('ds')\nactivate_dev_mode()\nprint('✓ DS workflow activated')\n\"\n```\n\n## Contents\n\n- [The Iron Law of DS Brainstorming](#the-iron-law-of-ds-brainstorming)\n- [What Brainstorm Does](#what-brainstorm-does)\n- [Critical Questions to Ask](#critical-questions-to-ask)\n- [Process](#process)\n- [Red Flags - STOP If You're About To](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Brainstorming (Questions Only)\n\nRefine vague analysis requests into clear objectives through Socratic questioning.\n**NO data exploration, NO coding** - just questions and objectives.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of DS Brainstorming\n\n**ASK QUESTIONS BEFORE ANYTHING ELSE. This is not negotiable.**\n\nBefore loading data, before exploring, before proposing approaches, you MUST:\n1. Ask clarifying questions using AskUserQuestion\n2. Understand what the user actually wants to learn\n3. Identify data sources and constraints\n4. Define success criteria\n5. Only THEN propose analysis approaches\n\n**STOP - You're about to load data or explore before asking questions. Don't do this.**\n</EXTREMELY-IMPORTANT>\n\n## What Brainstorm Does\n\n| DO | DON'T |\n|-------|----------|\n| Ask clarifying questions | Load or explore data |\n| Understand analysis objectives | Run queries |\n| Identify data sources | Profile data (that's /ds-plan) |\n| Define success criteria | Create visualizations |\n| Ask about constraints | Write analysis code |\n| Check if replicating existing analysis | Propose specific methodology |\n\n**Brainstorm answers: WHAT and WHY**\n**Plan answers: HOW (data profile + tasks)** (separate skill)\n\n## Critical Questions to Ask\n\n### Data Source Questions\n- What data sources are available?\n- Where is the data located (files, database, API)?\n- What time period does the data cover?\n- How frequently is the data updated?\n\n### Objective Questions\n- What question are you trying to answer?\n- Who is the audience for this analysis?\n- What decisions will be made based on results?\n- What would a successful outcome look like?\n\n### Constraint Questions\n- **Are you replicating an existing analysis?** (Critical for methodology)\n- Are there specific methodologies required?\n- What is the timeline for this analysis?\n- Are there computational resource constraints?\n\n### Output Questions\n- What format should results be in (report, dashboard, model)?\n- What visualizations are expected?\n- How will results be validated?\n\n## Process\n\n### 1. Ask Questions First\n\nEmploy `AskUserQuestion` immediately:\n- **One question at a time** - never batch\n- **Multiple-choice preferred** - easier to answer\n- Focus on: objectives, data sources, constraints, replication requirements\n\n### 2. Identify Replication Requirements\n\n**CRITICAL:** Ask early if replicating existing work:\n\n```\nAskUserQuestion:\n  question: \"Are you replicating or extending existing analysis?\"\n  options:\n    - label: \"Replicating existing\"\n      description: \"Must match specific methodology/results\"\n    - label: \"Extending existing\"\n      description: \"Building on prior work with modifications\"\n    - label: \"New analysis\"\n      description: \"Fresh analysis, methodology flexible\"\n```\n\nWhen replicating:\n- Obtain reference to original (paper, code, report)\n- Document exact methodology requirements\n- Define acceptable deviation from original results\n\n### 3. Propose Approaches\n\nAfter objectives are clear:\n- Propose **2-3 different approaches** with trade-offs\n- **Lead with recommendation** (mark as \"Recommended\")\n- Use `AskUserQuestion` for the user to select the preferred approach\n\n### 4. Write Spec Doc\n\nAfter selecting an approach:\n- Write to `.claude/SPEC.md`\n- Include: objectives, data sources, success criteria, constraints\n- **NO implementation details** - reserve those for /ds-plan\n\n```markdown\n# Spec: [Analysis Name]\n\n> **For Claude:** After writing this spec, use `Skill(skill=\"workflows:ds-plan\")` for Phase 2.\n\n## Objective\n[What question this analysis answers]\n\n## Data Sources\n- [Source 1]: [location, format, time period]\n- [Source 2]: [location, format, time period]\n\n## Success Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n## Constraints\n- Replication: [yes/no - if yes, reference source]\n- Timeline: [deadline]\n- Methodology: [required approaches]\n\n## Chosen Approach\n[Description of selected approach]\n\n## Rejected Alternatives\n- Option B: [why rejected]\n- Option C: [why rejected]\n```\n\n## Red Flags - STOP If You Catch Yourself Doing This:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Loading data | You're exploring before understanding goals | Ask what the user wants to learn |\n| Running describe() | You're profiling data when that's for /ds-plan | Finish defining objectives first |\n| Proposing specific models | You're jumping to HOW before clarifying WHAT | Define success criteria first |\n| Creating task lists | You're planning before objectives are clear | Complete brainstorm first |\n| Skipping replication question | You might miss critical methodology constraints | Always ask about replication upfront |\n\n## Output\n\nDeclare brainstorm complete when:\n- Analysis objectives clearly understood\n- Data sources identified\n- Success criteria defined\n- Constraints documented (especially replication requirements)\n- Approach chosen from alternatives\n- `.claude/SPEC.md` written\n- User confirms ready for data exploration\n\n## Workflow Context\n\nThis skill is Phase 1 of the 5-phase `/ds` workflow:\n\n1. **Phase 1: ds-brainstorm** (current) - Clarify objectives through Socratic questioning\n2. **Phase 2: ds-plan** - Profile data and break analysis into tasks\n3. **Phase 3: ds-implement** - Execute analysis tasks with output-first verification\n4. **Phase 4: ds-review** - Review methodology, data quality, and statistical validity\n5. **Phase 5: ds-verify** - Check reproducibility and obtain user acceptance\n\n## Phase Complete\n\nAfter completing brainstorm, IMMEDIATELY invoke the next phase:\n\n```bash\n# Invoke Phase 2: Data profiling and task breakdown\n/ds-plan\n```\n\nOr use the Skill tool directly:\n\n```\nSkill(skill=\"workflows:ds-plan\")\n```\n\n**CRITICAL:** Do not skip to analysis implementation. Phase 2 profiles data and breaks down the analysis into discrete, manageable tasks."
              },
              {
                "name": "ds-delegate",
                "description": "Subagent delegation for data analysis. Dispatches fresh Task agents with output-first verification.",
                "path": "skills/ds-delegate/SKILL.md",
                "frontmatter": {
                  "name": "ds-delegate",
                  "version": 1,
                  "description": "Subagent delegation for data analysis. Dispatches fresh Task agents with output-first verification."
                },
                "content": "## Contents\n\n- [The Iron Law of Delegation](#the-iron-law-of-delegation)\n- [Core Principle](#core-principle)\n- [The Process](#the-process)\n- [Honesty Requirement](#honesty-requirement)\n- [Rationalization Prevention](#rationalization-prevention)\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Delegation\n\n**YOU MUST route EVERY ANALYSIS STEP THROUGH A TASK AGENT. This is not negotiable.**\n\nYou MUST NOT:\n- Write analysis code directly\n- Run \"quick\" data checks\n- Edit notebooks or scripts\n- Make \"just this one plot\"\n\n**If you're about to write analysis code in main chat, STOP. Spawn a Task agent instead.**\n</EXTREMELY-IMPORTANT>\n\n## Core Principle\n\n**Fresh subagent per task + output-first verification = reliable analysis**\n\n- Analyst subagent does the work\n- Must produce visible output at each step\n- Methodology reviewer checks approach\n- Loop until output verified\n\n## When to Use\n\nCalled by `ds-implement` for each task in PLAN.md. Don't invoke directly.\n\n## The Process\n\n```\nFor each task:\n    1. Dispatch analyst subagent\n       - If questions → answer, re-dispatch\n       - Implements with output-first protocol\n    2. Verify outputs are present and reasonable\n    3. Dispatch methodology reviewer (if complex)\n    4. Mark task complete, log to LEARNINGS.md\n```\n\n## Step 1: Dispatch Analyst\n\n**Pattern:** Use structured delegation template from `common/templates/delegation-template.md`\n\nEvery delegation MUST include:\n1. TASK - What to analyze\n2. EXPECTED OUTCOME - Success criteria\n3. REQUIRED SKILLS - Statistical/ML methods needed\n4. REQUIRED TOOLS - Data access and analysis tools\n5. MUST DO - Output-first verification\n6. MUST NOT DO - Methodology violations\n7. CONTEXT - Data sources and previous work\n8. VERIFICATION - Output requirements\n\nUse this Task invocation (fill in brackets):\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\n# TASK\n\nAnalyze: [TASK NAME]\n\n## EXPECTED OUTCOME\n\nYou will have successfully completed this task when:\n- [ ] [Specific analysis output 1]\n- [ ] [Specific analysis output 2]\n- [ ] Output-first verification at each step\n- [ ] Results documented with evidence\n\n## REQUIRED SKILLS\n\nThis task requires:\n- [Statistical method]: [Why needed]\n- [Programming language]: Data manipulation\n- Output-first verification (mandatory)\n\n## REQUIRED TOOLS\n\nYou will need:\n- Read: Load datasets and existing code\n- Write: Create analysis scripts/notebooks\n- Bash: Run analysis and verify outputs\n\n**Tools denied:** None (full analysis access)\n\n## MUST DO\n\n- [ ] Print state BEFORE each operation (shape, head)\n- [ ] Print state AFTER each operation (nulls, sample)\n- [ ] Verify outputs are reasonable at each step\n- [ ] Document methodology decisions\n\n## MUST NOT DO\n\n- ❌ Skip verification outputs\n- ❌ Proceed with questionable data without flagging\n- ❌ Guess on methodology (ask if unclear)\n- ❌ Claim completion without visible outputs\n\n## CONTEXT\n\n### Task Description\n[PASTE FULL TASK TEXT FROM PLAN.md]\n\n### Analysis Context\n- Analysis objective: [from SPEC.md]\n- Data sources: [list with paths]\n- Previous steps: [summary from LEARNINGS.md]\n\n## Output-First Protocol (MANDATORY)\nFor EVERY operation:\n1. Print state BEFORE (shape, head)\n2. Execute operation\n3. Print state AFTER (shape, nulls, sample)\n4. Verify output is reasonable\n\nExample:\n```python\nprint(f\"Before: {df.shape}\")\ndf = df.merge(other, on='key')\nprint(f\"After: {df.shape}\")\nprint(f\"Nulls introduced: {df.isnull().sum().sum()}\")\ndf.head()\n```\n\n## Required Outputs by Operation\n| Operation | Required Output |\n|-----------|-----------------|\n| Load data | shape, dtypes, head() |\n| Filter | shape before/after, % removed |\n| Merge/Join | shape, null check, sample |\n| Groupby | result shape, sample groups |\n| Model fit | metrics, convergence |\n\n## If Unclear\nAsk questions BEFORE implementing. Don't guess on methodology.\n\n## Output\nReport: what you did, key outputs observed, any data quality issues found.\n\"\"\")\n```\n\n**If analyst asks questions:** Answer clearly, especially about methodology choices.\n\n**If analyst completes task:** Verify outputs, then proceed or review.\n\n## Step 2: Verify Outputs\n\nConfirm before proceeding:\n- [ ] Output files/variables exist\n- [ ] Shapes are reasonable (no unexpected row loss)\n- [ ] No silent null introduction\n- [ ] Sample output matches expectations\n\nUpon verification failure, re-dispatch analyst with specific fix instructions.\n\n## Step 3: Dispatch Methodology Reviewer (Complex Tasks)\n\nFor statistical analysis, modeling, or methodology-sensitive tasks, dispatch a methodology reviewer:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nReview methodology for: [TASK NAME]\n\n## What Was Done\n[SUMMARY FROM ANALYST OUTPUT]\n\n## Original Requirements\n[FROM SPEC.md - especially any replication requirements]\n\n## CRITICAL: Do Not Trust the Report\n\nThe analyst may have:\n- Reported success without actually running the code\n- Cherry-picked output that looks correct\n- Glossed over data quality issues\n- Made methodology choices without justification\n\n**DO:**\n- Read the actual code or notebook cells\n- Verify outputs exist and match claims\n- Check for silent failures (empty DataFrames, all nulls)\n- Confirm statistical assumptions were checked\n\n## Review Checklist\n1. Is the statistical method appropriate for the data type?\n2. Are assumptions documented and checked?\n3. Is sample size adequate for conclusions?\n4. Are there data leakage concerns?\n5. Is the approach reproducible (seeds, versions)?\n\n## Confidence Scoring\nRate each issue 0-100. Only report issues >= 80 confidence.\n\n## Output Format\n- APPROVED: Methodology sound (after verifying code/outputs yourself)\n- ISSUES: List concerns with confidence scores and file:line references\n\"\"\")\n```\n\n## Step 4: Log to LEARNINGS.md\n\nAppend to `.claude/LEARNINGS.md` after each task:\n\n```markdown\n## Task N: [Name] - COMPLETE\n\n**Input:** [describe input state]\n\n**Operation:** [what was done]\n\n**Output:**\n- Shape: [final shape]\n- Key findings: [observations]\n\n**Verification:**\n- [how you confirmed it worked]\n\n**Next:** [what comes next]\n```\n\n## Honesty Requirement\n\n<EXTREMELY-IMPORTANT>\n**Claiming \"analysis done\" without output verification is LYING.**\n\nWhen you say \"Step complete\", you are asserting:\n- A Task agent ran the analysis\n- Output was visible and verified by you\n- You personally checked it (not just trusting the agent's word)\n- Methodology reviewer approved (for statistical tasks)\n\nIf ANY of these didn't happen, you are not \"summarizing\" - you are LYING about the state of the analysis.\n\n**Your dishonest claims corrupt research. Your honest \"investigating\" maintains integrity.**\n</EXTREMELY-IMPORTANT>\n\n## Rationalization Prevention\n\nRecognize these thoughts as signals to stop and delegate instead:\n\n| Thought | Reality |\n|---------|---------|\n| \"I'll just check the shape quickly\" | You'll skip the output-first protocol. Delegate instead. |\n| \"It's just a simple merge\" | Your merges fail silently. Delegate with verification. |\n| \"I already know this data\" | Your knowing ≠ verified. Delegate anyway. |\n| \"The subagent will be slower\" | You're wrong—wrong results are slower than slow results. Delegate. |\n| \"Just this one plot\" | You're hiding data issues with one plot. Delegate. |\n| \"User wants results fast\" | They want CORRECT results. You're optimizing for wrong metric. Delegate. |\n| \"Skip methodology review, it's standard\" | Your \"standard\" assumptions often fail. Review anyway. |\n| \"Output looked reasonable\" | You didn't verify—\"looked reasonable\" ≠ verified. Check numbers. |\n\n## Red Flags\n\n**If you catch yourself thinking these, STOP immediately:**\n\n- \"I can skip output verification this time\"\n- \"I'll chain operations together, it's fine\"\n- \"Unexpected nulls are probably okay\"\n- \"Methodology review takes too long, skip it\"\n- \"The merge probably worked\"\n- \"Output-first protocol is overkill here\"\n- \"I'll just summarize PLAN.md for the analyst\" (STOP—provide full text)\n\n**When analyst produces no visible output:**\n- You must re-dispatch with explicit output requirements\n- Treat this as a hard failure, not something to work around\n\n**When analyst fails a task:**\n- You must dispatch a fix subagent with specific instructions\n- Don't fix it yourself in main chat—you'll pollute context and hide the real issue\n\n## Example Flow\n\n```\nMe: Implementing Task 1: Load and clean transaction data\n\n[Dispatch analyst with full task text]\n\nAnalyst:\n- Loaded transactions.csv: (50000, 12)\n- Found 5% nulls in amount column\n- \"Should I drop or impute nulls?\"\n\nMe: \"Impute with median, flag imputed rows\"\n\n[Re-dispatch with answer]\n\nAnalyst:\n- Imputed 2,500 rows with median ($45.50)\n- Added is_imputed flag column\n- Final shape: (50000, 13)\n- Sample output: [shows head with flag]\n\n[Verify: shapes match, flag exists, no unexpected changes]\n\n[Log to LEARNINGS.md]\n\n[Mark Task 1 complete, move to Task 2]\n```\n\n## Integration\n\nThis skill is invoked by `ds-implement` during the output-first implementation phase.\nAfter all tasks complete, `ds-implement` proceeds to `ds-review`."
              },
              {
                "name": "ds-implement",
                "description": "REQUIRED Phase 3 of /ds workflow. Enforces output-first verification at each step.",
                "path": "skills/ds-implement/SKILL.md",
                "frontmatter": {
                  "name": "ds-implement",
                  "description": "REQUIRED Phase 3 of /ds workflow. Enforces output-first verification at each step."
                },
                "content": "## Overview\n\nApply output-first verification at every step of analysis implementation. This is Phase 3 of the `/ds` workflow.\n\n## Contents\n\n- [Delegation Pattern](#delegation-pattern) - Main chat orchestrates, subagents analyze\n- [The Iron Law](#the-iron-law-of-ds-implementation) - EVERY step MUST produce visible output\n- [Output-First Protocol](#output-first-protocol) - Required outputs by operation type\n- [Implementation Process](#implementation-process) - Step-by-step workflow\n- [Task Agent Invocation](#task-agent-invocation) - Spawning sub-agents\n- [Verification Patterns](#verification-patterns) - See `references/verification-patterns.md`\n- [Common Failures](#common-failures-to-avoid) - Silent data loss, hidden nulls\n\n# Implementation (Output-First Verification)\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of Delegation\n\n**YOU MUST NOT WRITE ANALYSIS CODE. This is not negotiable.**\n\nYou orchestrate. Subagents analyze. STOP if you're about to write Python/R code.\n\nAllowed in main chat:\n- Spawn Task agents\n- Review Task agent output\n- Verify outputs exist and are reasonable\n- Write to .claude/*.md files\n\nNOT allowed in main chat:\n- Write/Edit code files (.py, .R, .ipynb, etc.)\n- Direct data manipulation\n- \"Quick analysis\"\n\n**If you're about to write analysis code directly, STOP and spawn a Task agent instead.**\n\n### Rationalization Prevention\n\nStop immediately when you encounter these rationalizations:\n\n| Rationalization | Reality |\n|---------|---------|\n| \"It's just a quick plot\" | You'll hide data issues. Delegate instead. |\n| \"I'll just check the shape\" | Your shape checks need output-first protocol. Delegate. |\n| \"The subagent will take too long\" | Your impatience costs more in context than subagent time. Delegate. |\n| \"I already know this data\" | Your knowledge ≠ verified output. Delegate and see. |\n| \"Let me just run this merge\" | Your merges will silently fail. Delegate with verification. |\n| \"This is too simple for a subagent\" | Your simple code hides errors. Delegate. |\n| \"I'm already looking at the data\" | Your looking ≠ analyzing. Delegate. |\n| \"Results are needed fast\" | Your wrong results are worse than slow right results. Delegate. |\n</EXTREMELY-IMPORTANT>\n\n## Delegation Pattern\n\nFor each task in PLAN.md:\n1. Dispatch analyst subagent (does the work with output-first)\n2. Verify outputs are present and reasonable\n3. Dispatch methodology reviewer (for statistical tasks)\n4. Log findings to LEARNINGS.md\n\n**Why delegate?**\n- Fresh context per task (no pollution from previous analysis)\n- Enforced output verification (can't skip)\n- Error isolation (bad analysis doesn't corrupt main context)\n\n**REQUIRED SUB-SKILL:** For Task templates and detailed flow:\n```\nSkill(skill=\"workflows:ds-delegate\")\n```\n\n---\n\nImplement analysis with mandatory visible output at every step.\n**NO TDD** - instead, every code step MUST produce and verify output.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of DS Implementation\n\n**EVERY CODE STEP YOU WRITE MUST PRODUCE VISIBLE OUTPUT. This is not negotiable.**\n\nBefore moving to the next step, you MUST execute the following:\n1. Run the code\n2. See the output (print, display, plot)\n3. Verify output is correct/reasonable\n4. Document in LEARNINGS.md\n5. Only THEN proceed to next step\n\nThis applies even when YOU think:\n- \"I know this works\"\n- \"It's just a simple transformation\"\n- \"I'll check results at the end\"\n- \"The code is straightforward\"\n\n**If you're about to write code without outputting results, STOP.**\n</EXTREMELY-IMPORTANT>\n\n## What Output-First Means\n\n| DO | DON'T |\n|-------|----------|\n| Print shape after each transform | Chain operations silently |\n| Display sample rows | Trust transformations work |\n| Show summary stats | Wait until end to check |\n| Verify row counts | Assume merges worked |\n| Check for unexpected nulls | Skip intermediate checks |\n| Plot distributions | Move on without looking |\n\n**The Mantra:** If not visible, it cannot be trusted.\n\n## Red Flags - STOP Immediately\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"I'll check at the end\" | STOP - you're letting errors compound silently | Check after every step |\n| \"This transform is simple\" | STOP - simple code can still be wrong | Output and verify |\n| \"I know merge worked\" | STOP - you've assumed this before and been wrong | Check row counts |\n| \"Data looks fine\" | STOP - you're confusing \"looks\" with verification | Print stats, show samples |\n| \"I'll batch the outputs\" | STOP - you're about to lose your ability to isolate issues | Output per operation |\n\n## Output-First Protocol\n\n### For Every Data Operation:\n\n```python\n# BEFORE\nprint(f\"Before: {df.shape}\")\n\n# OPERATION\ndf = df.merge(other, on='key')\n\n# AFTER - MANDATORY\nprint(f\"After: {df.shape}\")\nprint(f\"Nulls introduced: {df.isnull().sum().sum()}\")\ndf.head()\n```\n\n### Required Outputs by Operation Type\n\n| Operation | Required Output |\n|-----------|-----------------|\n| Load data | shape, dtypes, head() |\n| Filter | shape before/after, % removed |\n| Merge/Join | shape, null check, sample |\n| Groupby | result shape, sample groups |\n| Transform | before/after comparison, sample |\n| Model fit | metrics, convergence info |\n| Prediction | distribution, sample predictions |\n\n## Implementation Process\n\n### Step 1: Read Plan\n\nRead the plan to understand task order:\n\n```bash\ncat .claude/PLAN.md  # View analysis plan and task sequence\n```\n\nFollow the task order defined in the plan.\n\n### Step 2: Implement with Output\n\nFor each task:\n\n```python\n# Task N: [Description]\nprint(\"=\" * 50)\nprint(\"Task N: [Description]\")\nprint(\"=\" * 50)\n\n# Before state\nprint(f\"Input shape: {df.shape}\")\n\n# Operation\nresult = do_operation(df)\n\n# After state - MANDATORY\nprint(f\"Output shape: {result.shape}\")\nprint(f\"Sample output:\")\ndisplay(result.head())\n\n# Verification\nassert result.shape[0] > 0, \"No rows returned!\"\nprint(\"Task N complete\")\n```\n\n### Step 3: Log to LEARNINGS.md\n\nDocument every significant step:\n\n```markdown\n## Step N: [Task Description]\n\n**Input:** DataFrame with shape (10000, 15)\n\n**Operation:** Merged with reference table on 'id'\n\n**Output:**\n- Shape: (9500, 20)\n- 500 rows dropped (no match)\n- 5 new columns added\n- No new nulls introduced\n\n**Verification:**\n- Row count reasonable (5% drop expected due to filtering)\n- Sample output matches expected format\n- Key columns preserved\n\n**Notes:** [Any observations, issues, or decisions]\n```\n\n## Task Agent Invocation\n\nMain chat spawns Task agent:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nImplement [TASK] following output-first protocol.\n\nContext:\n- Read .claude/LEARNINGS.md for prior steps\n- Read .claude/PLAN.md for task details\n- Read .claude/SPEC.md for objectives\n\nOutput-First Protocol:\n1. Print state BEFORE each operation\n2. Execute the operation\n3. Print state AFTER with verification\n4. Display sample output\n5. Document in LEARNINGS.md\n\nRequired outputs per operation:\n- Shape before/after\n- Null counts\n- Sample rows (head)\n- Sanity checks (row counts, value ranges)\n\nDO NOT proceed to next task without:\n- Visible output showing operation worked\n- LEARNINGS.md entry documenting the step\n\nReport back: what was done, output observed, any issues.\n\"\"\")\n```\n\n## Verification Patterns\n\nSee [references/verification-patterns.md](references/verification-patterns.md) for detailed code patterns for:\n- Data loading, filtering, merging\n- Aggregation and model training\n- Quick reference table by operation type\n\n## Common Failures to Avoid\n\n| Failure | Why It Happens | Prevention |\n|---------|----------------|------------|\n| Silent data loss | Merge drops rows | Print row counts before/after |\n| Hidden nulls | Join introduces nulls | Check null counts after joins |\n| Wrong aggregation | Groupby logic error | Display sample groups |\n| Type coercion | Pandas silent conversion | Verify dtypes after load |\n| Off-by-one | Date filtering edge cases | Print min/max dates |\n\n## Logging\n\nAppend each step to `.claude/LEARNINGS.md`:\n\n```markdown\n## Step N: [Description] - [STATUS]\n\n**Input:** [Describe input state]\n\n**Operation:** [What was done]\n\n**Output:** [Shape, stats, sample]\n```\n[Paste actual output here]\n```\n\n**Verification:** [How you confirmed it worked]\n\n**Next:** [What comes next]\n```\n\n## If Output Looks Wrong\n\n1. **STOP** - do not proceed\n2. **Investigate** - print more details\n3. **Document** - log the issue in LEARNINGS.md\n4. **Ask** - if unclear, ask user for guidance\n5. **Fix** - only proceed after output verified\n\n**Never hide failures.** Bad output documented is better than silent failure.\n\n## No Pause Between Tasks\n\n<EXTREMELY-IMPORTANT>\n**After completing task N, IMMEDIATELY start task N+1. You MUST NOT pause.**\n\n| Thought | Reality |\n|---------|---------|\n| \"Task done, should check in with user\" | You're wasting context. User wants ALL tasks done. Keep going. |\n| \"User might want to see intermediate results\" | You're assuming wrong. User will see results at the END. Continue. |\n| \"Natural pause point\" | You're making excuses. Only pause when ALL tasks complete or you're blocked. |\n| \"Should summarize this step\" | You're procrastinating. Summarize AFTER all tasks. Keep moving. |\n\n**Your pausing between tasks is procrastination disguised as courtesy.**\n</EXTREMELY-IMPORTANT>\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After all analysis steps complete with verified output, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:ds-review\")\n```"
              },
              {
                "name": "ds-plan",
                "description": "REQUIRED Phase 2 of /ds workflow. Profiles data and creates analysis task breakdown.",
                "path": "skills/ds-plan/SKILL.md",
                "frontmatter": {
                  "name": "ds-plan",
                  "version": 1,
                  "description": "REQUIRED Phase 2 of /ds workflow. Profiles data and creates analysis task breakdown."
                },
                "content": "Announce: \"Using ds-plan (Phase 2) to profile data and create task breakdown.\"\n\n## Contents\n\n- [The Iron Law of DS Planning](#the-iron-law-of-ds-planning)\n- [What Plan Does](#what-plan-does)\n- [Process](#process)\n- [Red Flags - STOP If You're About To](#red-flags---stop-if-youre-about-to)\n- [Output](#output)\n\n# Planning (Data Profiling + Task Breakdown)\n\nProfile the data and create an analysis plan based on the spec.\n**Requires `.claude/SPEC.md` from /ds-brainstorm first.**\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of DS Planning\n\n**SPEC MUST EXIST BEFORE PLANNING. This is not negotiable.**\n\nBefore exploring data or creating tasks, you MUST have:\n1. `.claude/SPEC.md` with objectives and constraints\n2. Clear success criteria\n3. User-approved spec\n\n**If `.claude/SPEC.md` doesn't exist, run /ds-brainstorm first.**\n</EXTREMELY-IMPORTANT>\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"Data looks clean, profiling unnecessary\" | Your data is never clean | PROFILE to discover issues |\n| \"I can profile as I go\" | You'll miss systemic issues | PROFILE comprehensively NOW |\n| \"Quick .head() is enough\" | Your head hides tail problems | RUN full profiling checklist |\n| \"Missing values won't affect my analysis\" | They always do | DOCUMENT and plan handling |\n| \"I'll handle data issues during analysis\" | Your issues will derail your analysis | FIX data issues FIRST |\n| \"User didn't mention data quality\" | They assume YOU'LL check | QUALITY check is YOUR job |\n| \"Profiling takes too long\" | Your skipping it costs days later | INVEST time now |\n\n### Honesty Framing\n\n**Creating an analysis plan without profiling the data is LYING about understanding the data.**\n\nYou cannot plan analysis steps without knowing:\n- Your data's shape and types\n- Your missing value patterns\n- Your data quality issues\n- Your cleaning requirements\n\nProfiling costs you minutes. Your wrong plan costs hours of rework and incorrect results.\n\n### No Pause After Completion\n\nAfter writing `.claude/PLAN.md`, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:ds-implement\")\n```\n\nDO NOT:\n- Ask \"should I proceed with implementation?\"\n- Summarize the plan\n- Wait for user confirmation (they approved SPEC already)\n- Write status updates\n\nThe workflow phases are SEQUENTIAL. Complete plan → immediately start implement.\n\n## What Plan Does\n\n| DO | DON'T |\n|-------|----------|\n| Read .claude/SPEC.md | Skip brainstorm phase |\n| Profile data (shape, types, stats) | Skip to analysis |\n| Identify data quality issues | Ignore missing/duplicate data |\n| Create ordered task list | Write final analysis code |\n| Write .claude/PLAN.md | Make completion claims |\n\n**Brainstorm answers: WHAT and WHY**\n**Plan answers: HOW and DATA QUALITY**\n\n## Process\n\n### 1. Verify Spec Exists\n\n```bash\ncat .claude/SPEC.md  # verify-spec: read SPEC file to confirm it exists\n```\n\nIf missing, stop and run `/ds-brainstorm` first.\n\n### 2. Data Profiling\n\n**For multiple data sources:** Profile in parallel using background Task agents.\n\n#### Single Data Source (Direct Profiling)\n\n**MANDATORY profiling steps:**\n\n```python\nimport pandas as pd\n\n# Basic structure\ndf.shape                    # (rows, columns)\ndf.dtypes                   # Column types\ndf.head(10)                 # Sample data\ndf.tail(5)                  # End of data\n\n# Summary statistics\ndf.describe()               # Numeric summaries\ndf.describe(include='object')  # Categorical summaries\ndf.info()                   # Memory, non-null counts\n\n# Data quality checks\ndf.isnull().sum()           # Missing values per column\ndf.duplicated().sum()       # Duplicate rows\ndf[col].value_counts()      # Distribution of categories\n\n# For time series\ndf[date_col].min(), df[date_col].max()  # Date range\ndf.groupby(date_col).size()              # Records per period\n```\n\n#### Multiple Data Sources (Parallel Profiling)\n\n<EXTREMELY-IMPORTANT>\n**Pattern from oh-my-opencode: Launch ALL profiling agents in a SINGLE message.**\n\n**Use `run_in_background: true` for parallel execution.**\n\nWhen profiling 2+ data sources, launch agents in parallel:\n</EXTREMELY-IMPORTANT>\n\n```\n# PARALLEL + BACKGROUND: All Task calls in ONE message\n\nTask(\n    subagent_type=\"general-purpose\",\n    description=\"Profile dataset 1\",\n    run_in_background=true,\n    prompt=\"\"\"\nProfile this dataset and return a data quality report.\n\nDataset: /path/to/dataset1.csv\n\nRequired checks:\n1. Shape: rows x columns\n2. Data types: df.dtypes\n3. Missing values: df.isnull().sum()\n4. Duplicates: df.duplicated().sum()\n5. Summary statistics: df.describe()\n6. Unique value counts for categorical columns\n7. Date range if time series\n8. Memory usage: df.info()\n\nOutput format:\n- Markdown table with column summary\n- List of data quality issues found\n- Recommendations for cleaning\n\nTools denied: Write, Edit, NotebookEdit (read-only profiling)\n\"\"\")\n\nTask(\n    subagent_type=\"general-purpose\",\n    description=\"Profile dataset 2\",\n    run_in_background=true,\n    prompt=\"\"\"\n[Same template for dataset 2]\n\"\"\")\n\nTask(\n    subagent_type=\"general-purpose\",\n    description=\"Profile dataset 3\",\n    run_in_background=true,\n    prompt=\"\"\"\n[Same template for dataset 3]\n\"\"\")\n```\n\n**After launching agents:**\n- Continue to other work (don't wait)\n- Check status with `/tasks` command\n- Collect results with TaskOutput when ready\n\n```\n# Collect profiling results\nTaskOutput(task_id=\"task-abc123\", block=true, timeout=30000)\nTaskOutput(task_id=\"task-def456\", block=true, timeout=30000)\nTaskOutput(task_id=\"task-ghi789\", block=true, timeout=30000)\n```\n\n**Benefits:**\n- 3x faster profiling for 3 datasets\n- Each agent focused on single source\n- Results consolidated in main chat\n\n### 3. Identify Data Quality Issues\n\n**CRITICAL:** Document ALL issues before proceeding:\n\n| Check | What to Look For |\n|-------|------------------|\n| Missing values | Null counts, patterns of missingness |\n| Duplicates | Exact duplicates, key-based duplicates |\n| Outliers | Extreme values, impossible values |\n| Type issues | Strings in numeric columns, date parsing |\n| Cardinality | Unexpected unique values |\n| Distribution | Skewness, unexpected patterns |\n\n### 4. Create Task Breakdown\n\nBreak analysis into ordered tasks:\n- Each task should produce **visible output**\n- Order by data dependencies\n- Include data cleaning tasks FIRST\n\n### 5. Write Plan Doc\n\nWrite to `.claude/PLAN.md`:\n\n```markdown\n# Analysis Plan: [Analysis Name]\n\n> **For Claude:** REQUIRED SUB-SKILL: Use `Skill(skill=\"workflows:ds-implement\")` to implement this plan with output-first verification.\n>\n> **Delegation:** Main chat orchestrates, Task agents implement. Use `Skill(skill=\"workflows:ds-delegate\")` for subagent templates.\n\n## Spec Reference\nSee: .claude/SPEC.md\n\n## Data Profile\n\n### Source 1: [name]\n- Location: [path/connection]\n- Shape: [rows] x [columns]\n- Date range: [start] to [end]\n- Key columns: [list]\n\n#### Column Summary\n| Column | Type | Non-null | Unique | Notes |\n|--------|------|----------|--------|-------|\n| col1 | int64 | 100% | 50 | Primary key |\n| col2 | object | 95% | 10 | Category |\n\n#### Data Quality Issues\n- [ ] Missing: col2 has 5% nulls - [strategy: drop/impute/flag]\n- [ ] Duplicates: 100 duplicate rows on [key] - [strategy]\n- [ ] Outliers: col3 has values > 1000 - [strategy]\n\n### Source 2: [name]\n[Same structure]\n\n## Task Breakdown\n\n### Task 1: Data Cleaning (required first)\n- Handle missing values in col2\n- Remove duplicates\n- Fix data types\n- Output: Clean DataFrame, log of rows removed\n\n### Task 2: [Analysis Step]\n- Input: Clean DataFrame\n- Process: [description]\n- Output: [specific output to verify]\n- Dependencies: Task 1\n\n### Task 3: [Next Step]\n[Same structure]\n\n## Output Verification Plan\nFor each task, define what output proves completion:\n- Task 1: \"X rows cleaned, Y rows dropped\"\n- Task 2: \"Visualization showing [pattern]\"\n- Task 3: \"Model accuracy >= 0.8\"\n\n## Reproducibility Requirements\n- Random seed: [value if needed]\n- Package versions: [key packages]\n- Data snapshot: [date/version]\n```\n\n## Red Flags - STOP If You're About To:\n\n| Action | Why It's Wrong | Do Instead |\n|--------|----------------|------------|\n| Skip data profiling | Your data issues will break your analysis | Always profile first |\n| Ignore missing values | You'll corrupt your results | Document and plan handling |\n| Start analysis immediately | You haven't characterized your data | Complete profiling |\n| Assume your data is clean | Never assume, you must verify | Run quality checks |\n\n## Output\n\nComplete the plan when:\n- Read and understand `.claude/SPEC.md`\n- Profile all data sources (shape, types, stats)\n- Document data quality issues\n- Define cleaning strategy for each issue\n- Order tasks by dependency\n- Define output verification criteria\n- Write `.claude/PLAN.md`\n- Confirm ready for implementation\n\n## Phase Complete\n\n**REQUIRED SUB-SKILL:** After completing plan, IMMEDIATELY invoke:\n```\nSkill(skill=\"workflows:ds-implement\")\n```"
              },
              {
                "name": "ds-review",
                "description": "This skill should be used when running Phase 4 of the /ds workflow to review methodology, data quality, and statistical validity. Provides structured review checklists, confidence scoring, and issue identification for data analysis validation.",
                "path": "skills/ds-review/SKILL.md",
                "frontmatter": {
                  "name": "ds-review",
                  "description": "This skill should be used when running Phase 4 of the /ds workflow to review methodology, data quality, and statistical validity. Provides structured review checklists, confidence scoring, and issue identification for data analysis validation.",
                  "version": "1.0.0"
                },
                "content": "Announce: \"Using ds-review (Phase 4) to check methodology and quality.\"\n\n## Contents\n\n- [The Iron Law of DS Review](#the-iron-law-of-ds-review)\n- [Red Flags - STOP Immediately If You Think](#red-flags---stop-immediately-if-you-think)\n- [Review Focus Areas](#review-focus-areas)\n- [Confidence Scoring](#confidence-scoring)\n- [Common DS Issues to Check](#common-ds-issues-to-check)\n- [Required Output Structure](#required-output-structure)\n- [Agent Invocation](#agent-invocation)\n- [Quality Standards](#quality-standards)\n\n# Analysis Review\n\nSingle-pass review combining methodology correctness, data quality handling, and reproducibility checks. Uses confidence-based filtering.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of DS Review\n\n**You MUST only report issues with >= 80% confidence. This is not negotiable.**\n\nBefore reporting ANY issue, you MUST:\n1. Verify it's not a false positive\n2. Verify it impacts results or reproducibility\n3. Assign a confidence score\n4. Only report if score >= 80\n\nThis applies even when:\n- \"This methodology looks suspicious\"\n- \"I think this might introduce bias\"\n- \"The approach seems unusual\"\n- \"I would have done it differently\"\n\n**STOP - If you catch yourself about to report a low-confidence issue, DISCARD IT. You're about to compromise the review's integrity.**\n</EXTREMELY-IMPORTANT>\n\n## Red Flags - STOP Immediately If You Think:\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"This looks wrong\" | Your vague suspicion isn't evidence | Find concrete proof or discard |\n| \"I would do it differently\" | Your style preference isn't a methodology error | Check if the approach is valid |\n| \"This might cause problems\" | Your \"might\" means < 80% confidence | Find proof or discard |\n| \"Unusual approach\" | Unusual isn't wrong—your bias toward familiar methods is clouding judgment | Verify the methodology is sound |\n\n## Review Focus Areas\n\n### Spec Compliance\n- [ ] Verify all objectives from .claude/SPEC.md are addressed\n- [ ] Confirm success criteria can be verified\n- [ ] Check constraints were respected (especially replication requirements)\n- [ ] Verify analysis answers the original question\n\n### Data Quality Handling\n- [ ] Confirm missing values handled appropriately (not ignored)\n- [ ] Verify duplicates addressed (documented if kept)\n- [ ] Check outliers considered (handled or justified)\n- [ ] Verify data types correct (dates parsed, numerics not strings)\n- [ ] Confirm filtering logic documented with counts\n\n### Methodology Appropriateness\n- [ ] Verify statistical methods appropriate for data type\n- [ ] Check assumptions documented and verified (normality, independence, etc.)\n- [ ] Confirm sample sizes adequate for conclusions\n- [ ] Check multiple comparisons addressed if applicable\n- [ ] Verify causality claims justified (or appropriately limited)\n\n### Reproducibility\n- [ ] Verify random seeds set where needed\n- [ ] Check package versions documented\n- [ ] Verify data source/version documented\n- [ ] Confirm all transformations traceable\n- [ ] Verify results can be regenerated\n\n### Output Quality\n- [ ] Verify visualizations labeled (title, axes, legend)\n- [ ] Check numbers formatted appropriately (sig figs, units)\n- [ ] Verify conclusions supported by evidence shown\n- [ ] Confirm limitations acknowledged\n\n## Confidence Scoring\n\nRate each potential issue from 0-100:\n\n| Score | Meaning |\n|-------|---------|\n| 0 | False positive or style preference |\n| 25 | Might be real, methodology is unusual but valid |\n| 50 | Real issue but minor impact on conclusions |\n| 75 | Verified issue, impacts result interpretation |\n| 100 | Certain error that invalidates conclusions |\n\n**CRITICAL: You MUST only report issues with confidence >= 80. If you report below this threshold, you're misrepresenting your certainty.**\n\n## Common DS Issues to Check\n\n### Data Leakage\n- Training data contains information from future\n- Test data used in feature engineering\n- Target variable used directly or indirectly in features\n\n### Selection Bias\n- Filtering introduced systematic bias\n- Survivorship bias in longitudinal data\n- Non-random sampling not addressed\n\n### Statistical Errors\n- Multiple testing without correction\n- p-hacking or selective reporting\n- Correlation interpreted as causation\n- Inadequate sample size for claimed precision\n\n### Reproducibility Failures\n- Random operations without seeds\n- Undocumented data preprocessing\n- Hard-coded paths or environment dependencies\n- Missing package versions\n\n## Required Output Structure\n\nmarkdown-output-structure: Template for review results with confidence-scored issues\n\n```markdown\n## Analysis Review: [Analysis Name]\nReviewing: [files/notebooks being reviewed]\n\n### Critical Issues (Confidence >= 90)\n\n#### [Issue Title] (Confidence: XX)\n\n**Location:** `file/path.py:line` or `notebook.ipynb cell N`\n\n**Problem:** Clear description of the issue\n\n**Impact:** How this affects results/conclusions\n\n**Fix:**\n```python\n# Specific fix\n```\n\n### Important Issues (Confidence 80-89)\n\n[Same format as Critical Issues]\n\n### Data Quality Checklist\n\n| Check | Status | Notes |\n|-------|--------|-------|\n| Missing values | PASS/FAIL | [details] |\n| Duplicates | PASS/FAIL | [details] |\n| Outliers | PASS/FAIL | [details] |\n| Type correctness | PASS/FAIL | [details] |\n\n### Methodology Checklist\n\n| Check | Status | Notes |\n|-------|--------|-------|\n| Appropriate for data | PASS/FAIL | [details] |\n| Assumptions checked | PASS/FAIL | [details] |\n| Sample size adequate | PASS/FAIL | [details] |\n\n### Reproducibility Checklist\n\n| Check | Status | Notes |\n|-------|--------|-------|\n| Seeds set | PASS/FAIL | [details] |\n| Versions documented | PASS/FAIL | [details] |\n| Data versioned | PASS/FAIL | [details] |\n\n### Summary\n\n**Verdict:** APPROVED | CHANGES REQUIRED\n\n[If APPROVED]\nThe analysis meets quality standards. No methodology issues with confidence >= 80 detected.\n\n[If CHANGES REQUIRED]\nX critical issues and Y important issues must be addressed before proceeding.\n```\n\n## Agent Invocation\n\ntask-agent-spawn: Spawn Task agent for structured analysis review\n\nSpawn a Task agent to review the analysis:\n\n```\nTask(subagent_type=\"general-purpose\"):\n\"Review analysis against .claude/SPEC.md.\n\nExecute single-pass review covering:\n1. Spec compliance - verify objectives met\n2. Data quality - confirm nulls, dupes, outliers handled\n3. Methodology - verify appropriate, assumptions checked\n4. Reproducibility - confirm seeds, versions, documentation\n\nConfidence score each issue (0-100).\nReport only issues with >= 80 confidence.\nReturn structured output per /ds-review format.\"\n```\n\n## Quality Standards\n\n- **You must NOT report methodology preferences not backed by statistical principles.** Your opinion about how code should be written is not a review issue.\n- **You must treat alternative valid approaches as non-issues (confidence = 0).** If the approach works correctly, don't report it.\n- Ensure each reported issue is immediately actionable\n- **If you're unsure, rate it below 80 confidence.** Uncertainty is not a reason to report—it's a reason to investigate more.\n- Focus on what affects conclusions, not style. **STOP if you catch yourself criticizing coding style—that's not your role here.**\n\n## Phase Complete\n\nphase-transition: Invoke ds-verify after APPROVED review\n\nAfter review is APPROVED, immediately invoke:\n\nds-verify: Verify analysis reproducibility and user acceptance\n\n```\nSkill(skill=\"workflows:ds-verify\")\n```\n\nIf CHANGES REQUIRED, return to `/ds-implement` to fix issues first."
              },
              {
                "name": "ds-tools",
                "description": "This skill should be used when the user asks \"what plugins are available\", \"list data science tools\", \"what MCP servers can I use\", \"enable code intelligence\", or needs to discover available plugins like serena, context7, or data access skills like wrds and lseg-data.",
                "path": "skills/ds-tools/SKILL.md",
                "frontmatter": {
                  "name": "ds-tools",
                  "description": "This skill should be used when the user asks \"what plugins are available\", \"list data science tools\", \"what MCP servers can I use\", \"enable code intelligence\", or needs to discover available plugins like serena, context7, or data access skills like wrds and lseg-data."
                },
                "content": "# Available Data Science Plugins\n\nThese plugins extend Claude Code capabilities for data science workflows. Enable when needed.\n\n## Code Intelligence\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `serena` | Semantic code analysis, refactoring, symbol navigation | `claude --enable-plugin serena@claude-plugins-official` |\n| `pyright-lsp` | Python type checking and diagnostics | `claude --enable-plugin pyright-lsp@claude-plugins-official` |\n\n## Documentation\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `context7` | Up-to-date library docs (pandas, numpy, sklearn, etc.) | `claude --enable-plugin context7@claude-plugins-official` |\n\n## Web & Automation\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `playwright` | Web scraping, browser automation, screenshots | `claude --enable-plugin playwright@claude-plugins-official` |\n\n## Workflow\n\n| Plugin | Description | Enable Command |\n|--------|-------------|----------------|\n| `ralph-loop` | Self-referential iteration loops | Already enabled |\n| `hookify` | Create custom hooks from conversation patterns | Already enabled |\n\n## Data Access Skills (Built-in)\n\nThese are skills, not plugins - already available via `/ds`:\n\n| Skill | Description |\n|-------|-------------|\n| `/wrds` | WRDS (Wharton Research Data Services) queries |\n| `/lseg-data` | LSEG Data Library (formerly Refinitiv) |\n| `/gemini-batch` | Gemini Batch API for large-scale LLM processing |\n| `/jupytext` | Jupyter notebooks as text files |\n| `/marimo` | Marimo reactive Python notebooks |\n\n## File Format Skills (Bundled)\n\nOffice document and PDF skills from Anthropic's official skills repo (bundled via submodule):\n\n| Skill | Use For |\n|-------|---------|\n| `/xlsx` | Spreadsheets, formulas, CSV→Excel conversion |\n| `/pdf` | PDF extraction, creation, form filling |\n| `/pptx` | Presentation creation and editing |\n| `/docx` | Word docs, tracked changes, reports |\n\nThese skills are available via the `shared` plugin - no separate installation needed.\n\n## When to Enable\n\n- **serena**: Understanding complex analysis codebases, refactoring pipelines\n- **context7**: Access current docs for pandas, scikit-learn, statsmodels, and other libraries\n- **playwright**: Scrape web data sources, automate data collection\n- **pyright-lsp**: Type check data pipelines\n\n## Usage\n\nEnable a plugin for the current session:\n```bash\nclaude --enable-plugin <plugin-name>  # Enable a plugin by name and source\n```\n\nEnable a plugin for a project by adding to `.claude/settings.json`:\n```json\n{\n  \"enabledPlugins\": {\n    \"serena@claude-plugins-official\": true\n  }\n}\n```"
              },
              {
                "name": "ds-verify",
                "description": "This skill should be used when the user asks to 'verify analysis results', 'check reproducibility', 'validate data science output', 'confirm completion', or as Phase 5 of the /ds workflow (final). Enforces reproducibility demonstration and user acceptance before completion claims.",
                "path": "skills/ds-verify/SKILL.md",
                "frontmatter": {
                  "name": "ds-verify",
                  "description": "This skill should be used when the user asks to 'verify analysis results', 'check reproducibility', 'validate data science output', 'confirm completion', or as Phase 5 of the /ds workflow (final). Enforces reproducibility demonstration and user acceptance before completion claims."
                },
                "content": "Announce: \"Using ds-verify (Phase 5) to confirm reproducibility and completion.\"\n\n## Contents\n\n- [The Iron Law of DS Verification](#the-iron-law-of-ds-verification)\n- [Red Flags - STOP Immediately If You Think](#red-flags---stop-immediately-if-you-think)\n- [The Verification Gate](#the-verification-gate)\n- [Verification Checklist](#verification-checklist)\n- [Reproducibility Demonstration](#reproducibility-demonstration)\n- [Claims Requiring Evidence](#claims-requiring-evidence)\n- [Insufficient Evidence](#insufficient-evidence)\n- [Required Output Structure](#required-output-structure)\n- [Completion Criteria](#completion-criteria)\n\n# Verification Gate\n\nFinal verification with reproducibility checks and user acceptance interview.\n\n<EXTREMELY-IMPORTANT>\n## The Iron Law of DS Verification\n\n**NO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION. This is not negotiable.**\n\nBefore claiming analysis is complete, you MUST:\n1. RE-RUN - Execute analysis fresh (not cached results)\n2. CHECK - Verify outputs match expectations\n3. REPRODUCE - Confirm results are reproducible\n4. ASK - Interview user about constraints and acceptance\n5. Only THEN claim completion\n\nThis applies even when:\n- \"I just ran it\"\n- \"Results look the same\"\n- \"It should reproduce\"\n- \"User seemed happy earlier\"\n\n**If you catch yourself thinking \"I can skip verification,\" STOP - you're about to lie.**\n</EXTREMELY-IMPORTANT>\n\n## Red Flags - STOP Immediately If You Think:\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"Results should be the same\" | Your \"should\" isn't verification | Re-run and compare |\n| \"I ran it earlier\" | Your earlier run isn't fresh | Run it again now |\n| \"It's reproducible\" | Your claim requires evidence | Demonstrate reproducibility |\n| \"User will be happy\" | Your assumption isn't their acceptance | Ask explicitly |\n| \"Outputs look right\" | Your visual inspection isn't verified | Check against criteria |\n\n## The Verification Gate\n\nBefore making ANY completion claim:\n\n```\n1. RE-RUN    → Execute fresh, not from cache\n2. CHECK     → Compare outputs to success criteria\n3. REPRODUCE → Same inputs → same outputs\n4. ASK       → User acceptance interview\n5. CLAIM     → Only after steps 1-4\n```\n\n**Skipping any step is not verification.**\n\n## Verification Checklist\n\n### Technical Verification\n\n#### Outputs Match Expectations\n- [ ] All required outputs generated\n- [ ] Output formats correct (files, figures, tables)\n- [ ] Numbers are reasonable (sanity checks)\n- [ ] Visualizations render correctly\n\n#### Reproducibility Confirmed\n- [ ] Ran analysis twice, got same results\n- [ ] Random seeds produce consistent output\n- [ ] No dependency on execution order\n- [ ] Environment documented (packages, versions)\n\n#### Data Integrity\n- [ ] Input data unchanged\n- [ ] Row counts traceable through pipeline\n- [ ] No silent data loss or corruption\n\n### User Acceptance Interview\n\n**CRITICAL:** Before claiming completion, conduct user interview.\n\n#### Step 1: Replication Constraints\n\n```\nAskUserQuestion:\n  question: \"Were there specific methodology requirements I should have followed?\"\n  options:\n    - label: \"Yes, replicating existing analysis\"\n      description: \"Results should match a reference\"\n    - label: \"Yes, required methodology\"\n      description: \"Specific methods were mandated\"\n    - label: \"No constraints\"\n      description: \"Methodology was flexible\"\n```\n\nIf replicating:\n- Ask for reference to compare against\n- Verify results match within tolerance\n- Document any deviations and reasons\n\n#### Step 2: Results Verification\n\n```\nAskUserQuestion:\n  question: \"Do these results answer your original question?\"\n  options:\n    - label: \"Yes, fully\"\n      description: \"Analysis addresses the core question\"\n    - label: \"Partially\"\n      description: \"Some aspects addressed, others missing\"\n    - label: \"No\"\n      description: \"Does not answer the question\"\n```\n\nIf \"Partially\" or \"No\":\n1. Ask which aspects are missing\n2. Return to `/ds-implement` to address gaps\n3. Re-run verification\n\n#### Step 3: Output Format\n\n```\nAskUserQuestion:\n  question: \"Are the outputs in the format you need?\"\n  options:\n    - label: \"Yes\"\n      description: \"Format is correct\"\n    - label: \"Need adjustments\"\n      description: \"Format needs modification\"\n```\n\n#### Step 4: Confidence in Results\n\n```\nAskUserQuestion:\n  question: \"Do you have any concerns about the methodology or results?\"\n  options:\n    - label: \"No concerns\"\n      description: \"Comfortable with approach and results\"\n    - label: \"Minor concerns\"\n      description: \"Would like clarification on some points\"\n    - label: \"Major concerns\"\n      description: \"Significant issues need addressing\"\n```\n\n## Reproducibility Demonstration\n\n**MANDATORY:** Demonstrate reproducibility before completion.\n\n```python\n# Run 1\nresult1 = run_analysis(seed=42)\nhash1 = hash(str(result1))\n\n# Run 2\nresult2 = run_analysis(seed=42)\nhash2 = hash(str(result2))\n\n# Verify\nassert hash1 == hash2, \"Results not reproducible!\"\nprint(f\"Reproducibility confirmed: {hash1} == {hash2}\")\n```\n\nFor notebooks:\n```bash\n# notebook-reproduce: Clear and re-run all cells from scratch\njupyter nbconvert --execute --inplace notebook.ipynb\n\n# notebook-reproduce-with-seed: Execute notebook with fixed random seed for reproducibility\npapermill notebook.ipynb output.ipynb -p seed 42\n```\n\n## Claims Requiring Evidence\n\n| Claim | Required Evidence |\n|-------|-------------------|\n| \"Analysis complete\" | All success criteria verified |\n| \"Results reproducible\" | Same output from fresh run |\n| \"Matches reference\" | Comparison showing match |\n| \"Data quality handled\" | Documented cleaning steps |\n| \"Methodology appropriate\" | Assumptions checked |\n\n## Insufficient Evidence\n\nThese do NOT count as verification:\n\n- Previous run results (must be fresh)\n- \"Should be reproducible\" (demonstrate it)\n- Visual inspection only (quantify where possible)\n- Single run (need reproducibility check)\n- Skipped user acceptance (must ask)\n\n## Required Output Structure\n\n```markdown\n## Verification Report: [Analysis Name]\n\n### Technical Verification\n\n#### Outputs Generated\n- [ ] Output 1: [location] - verified [date/time]\n- [ ] Output 2: [location] - verified [date/time]\n\n#### Reproducibility Check\n- Run 1 hash: [value]\n- Run 2 hash: [value]\n- Match: YES/NO\n\n#### Environment\n- Python: [version]\n- Key packages: [list with versions]\n- Random seed: [value]\n\n### User Acceptance\n\n#### Replication Check\n- Constraint: [none/replicating/required methodology]\n- Reference: [if applicable]\n- Match status: [if applicable]\n\n#### User Responses\n- Results address question: [yes/partial/no]\n- Output format acceptable: [yes/needs adjustment]\n- Methodology concerns: [none/minor/major]\n\n### Verdict\n\n**COMPLETE** or **NEEDS WORK**\n\n[If COMPLETE]\n- All technical checks passed\n- User accepted results\n- Reproducibility demonstrated\n\n[If NEEDS WORK]\n- [List items requiring attention]\n- Recommended next steps\n```\n\n## Completion Criteria\n\n**Only claim COMPLETE when ALL are true:**\n\n- [ ] All success criteria from SPEC.md verified\n- [ ] Results reproducible (demonstrated, not assumed)\n- [ ] User confirmed results address their question\n- [ ] User has no major concerns\n- [ ] Outputs in acceptable format\n- [ ] If replicating: results match reference\n\n**Both technical and user acceptance must pass. No shortcuts.**\n\n## Workflow Complete\n\nWhen user confirms all criteria are met:\n\n**Announce:** \"DS workflow complete. All 5 phases passed.\"\n\nThe `/ds` workflow is now finished. Offer to:\n- Export results to final format\n- Clean up `.claude/` files\n- Start a new analysis with `/ds`"
              },
              {
                "name": "exit",
                "description": "This skill should be used when the user asks to \"exit sandbox\", \"deactivate sandbox\", \"restore full access\", or mentions they are done with workflow constraints. Deactivates the sandbox and restores access to Bash and Write/Edit tools.",
                "path": "skills/exit/SKILL.md",
                "frontmatter": {
                  "name": "exit",
                  "description": "This skill should be used when the user asks to \"exit sandbox\", \"deactivate sandbox\", \"restore full access\", or mentions they are done with workflow constraints. Deactivates the sandbox and restores access to Bash and Write/Edit tools.",
                  "version": "0.1.0"
                },
                "content": "# Exit Workflow\n\nDeactivates sandbox constraints for the current session, restoring full access to Bash and Write/Edit tools.\n\n## Purpose\n\nThe exit workflow removes temporary session constraints imposed by workflow skills (dev, ds, or other sandboxed modes). This allows unrestricted tool access when workflow constraints are no longer needed.\n\n## Deactivation Process\n\nExecute the deactivation script to disable sandbox mode. The script calls the session module to clear workflow state and restore all tools:\n\n```bash\n# Deactivate sandbox and restore full tool access\npython3 -c \"\nimport sys\nsys.path.insert(0, '${CLAUDE_PLUGIN_ROOT}/hooks/scripts/common')\nfrom session import deactivate_dev_mode\ndeactivate_dev_mode()\nprint('✓ Sandbox deactivated')\n\"\n```\n\n## Session State\n\nNote that active workflows (dev, ds) remain tracked in the session history until the session ends. The deactivation only removes the runtime constraints, not the historical record of workflow activity."
              },
              {
                "name": "gemini-batch",
                "description": "This skill should be used when the user asks to \"use Gemini Batch API\", \"process documents at scale\", \"submit a batch job\", \"upload files to Gemini\", or needs large-scale LLM processing. Includes production gotchas and best practices.",
                "path": "skills/gemini-batch/SKILL.md",
                "frontmatter": {
                  "name": "gemini-batch",
                  "version": 1,
                  "description": "This skill should be used when the user asks to \"use Gemini Batch API\", \"process documents at scale\", \"submit a batch job\", \"upload files to Gemini\", or needs large-scale LLM processing. Includes production gotchas and best practices."
                },
                "content": "# Gemini Batch API Skill\n\nLarge-scale asynchronous document processing using Google's Gemini models.\n\n## When to Use\n\n- Process thousands of documents with the same prompt\n- Cost-effective bulk extraction (50% cheaper than synchronous API)\n- Jobs that can tolerate 24-hour completion windows\n\n## IRON LAW: Use Examples First, Never Guess API\n\n**READ EXAMPLES BEFORE WRITING ANY CODE. NO EXCEPTIONS.**\n\n### The Rule\n\n```\nUser asks for batch API work\n    ↓\nMANDATORY: Read examples/batch_processor.py or examples/icon_batch_vision.py\n    ↓\nCopy the pattern exactly\n    ↓\nDO NOT guess parameter names\nDO NOT try wrapper types\nDO NOT improvise API calls\n```\n\n### Why This Matters\n\nThe Batch API has non-obvious requirements that will fail silently:\n1. **Metadata must be flat primitives** - Nested objects cause cryptic errors\n2. **Parameter is `dest=` not `destination=`** - Wrong name → TypeError\n3. **Config is plain dict** - Not a wrapper type\n4. **Examples are authoritative** - Working code beats assumptions\n\n**Rationale:** Previous agents wasted hours debugging API errors that the examples would have prevented. The patterns in `examples/` are battle-tested production code.\n\n### Rationalization Table - STOP If You Catch Yourself Thinking:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I know how APIs work\" | You're overconfident about non-obvious gotchas | Read examples first |\n| \"I can figure it out\" | You'll waste 30+ minutes on trial-and-error | Copy working patterns |\n| \"The examples might be outdated\" | They're maintained and tested | Trust the examples |\n| \"I need to customize anyway\" | Your customization comes AFTER copying base pattern | Start with examples, then adapt |\n| \"Reading examples takes too long\" | You'll save 30 minutes debugging with 2 minutes of reading | Read examples first |\n| \"My approach is simpler\" | Your simpler approach already failed | Use proven patterns |\n\n### Red Flags - STOP If You Catch Yourself Thinking:\n\n- **\"Let me try `destination=` instead of `dest=`\"** → You're about to cause a TypeError. Read examples.\n- **\"I'll create a `CreateBatchJobConfig` object\"** → You're instantiating a type instead of using a plain dict. Stop.\n- **\"I'll nest metadata like a normal API\"** → You'll trigger BigQuery type errors. Flatten your data.\n- **\"This should work like other Google APIs\"** → Your assumption is wrong; this API is different.\n- **\"I'll figure out the JSONL format\"** → You'll waste time. Copy from examples instead.\n\n### MANDATORY Checklist Before ANY Batch API Code\n\n- [ ] Read `examples/batch_processor.py` OR `examples/icon_batch_vision.py`\n- [ ] Identify which example matches the use case (Standard API vs Vertex AI)\n- [ ] Copy the example's API call pattern **exactly**\n- [ ] Copy the example's JSONL structure **exactly**\n- [ ] Copy the example's metadata structure **exactly**\n- [ ] Adapt for specific needs only after copying base pattern\n\n**Enforcement:** Writing batch API code without reading examples first violates this IRON LAW and will result in preventable errors.\n\n## Prerequisites\n\n### Install gcloud SDK\n\n```bash\n# macOS: Install Google Cloud SDK via Homebrew\nbrew install google-cloud-sdk\n\n# Linux: Install Google Cloud SDK from official sources\ncurl https://sdk.cloud.google.com | bash\n```\n\n### Authentication Setup\n\n```bash\n# Authenticate with Google Cloud Platform\ngcloud auth login\n\n# Set up Application Default Credentials for Python libraries\ngcloud auth application-default login\n\n# Enable Vertex AI API in your project\ngcloud services enable aiplatform.googleapis.com\n```\n\n**Why both auth methods?**\n- `gcloud auth login`: For gsutil and gcloud CLI commands\n- `gcloud auth application-default login`: For google-generativeai Python library\n- **CRITICAL:** Vertex AI requires ADC (step 2), not just API key\n\n### Create GCS Bucket\n\n```bash\n# Create bucket in us-central1 (required region)\ngsutil mb -l us-central1 gs://your-batch-bucket\n\n# Verify bucket location is us-central1\ngsutil ls -L -b gs://your-batch-bucket | grep \"Location\"\n```\n\nSee `references/gcs-setup.md` for complete setup guide.\n\n## Quick Start\n\n### Standard Gemini API (API Key)\n\n```python\nfrom examples.batch_processor import GeminiBatchProcessor\n\nprocessor = GeminiBatchProcessor(\n    bucket_name=\"my-batch-bucket\",  # Must be in us-central1\n    model=\"gemini-2.0-flash-lite\"\n)\n\nresults = processor.run_pipeline(\n    input_dir=\"./documents\",\n    prompt=\"Extract as JSON: {title, date, summary}\",\n    output_dir=\"./results\"\n)\n```\n\n### Vertex AI (Recommended)\n\n```python\nimport google.generativeai as genai\n\n# Use Vertex AI with ADC\nclient = genai.Client(\n    vertexai=True,\n    project=\"your-project-id\",\n    location=\"us-central1\"\n)\n\n# Submit batch job\njob = client.batches.create(\n    model=\"gemini-2.5-flash-lite\",\n    src=\"gs://bucket/requests.jsonl\",\n    dest=\"gs://bucket/outputs/\"\n)\n```\n\n## Core Workflow\n\n1. **Upload files** to GCS bucket (us-central1 region required)\n2. **Create JSONL** request file with document URIs and prompts\n3. **Submit batch job** via `genai.batches.create()`\n4. **Poll for completion** (jobs expire after 24 hours)\n5. **Download and parse** results from output URI\n6. **Handle failures** gracefully (partial failures are common)\n\n## IRON LAW: Metadata and API Call Structure\n\n**YOU MUST USE FLAT PRIMITIVES FOR METADATA. YOU MUST USE SIMPLE STRINGS FOR API PARAMETERS.**\n\n### Rule 1: Metadata Structure\n\n```\nCORRECT ✓\n\"metadata\": {\n    \"request_id\": \"icon_123\",        # String\n    \"file_name\": \"copy.svg\",         # String\n    \"file_size\": 1024                # Integer\n}\n\nWRONG ✗\n\"metadata\": {\n    \"request_id\": \"icon_123\",\n    \"file_info\": {                   # ← NESTED OBJECT FAILS!\n        \"name\": \"copy.svg\",\n        \"size\": 1024\n    }\n}\n\nWORKAROUND (if complex data needed)\n\"metadata\": {\n    \"request_id\": \"icon_123\",\n    \"file_info\": json.dumps({\"name\": \"copy.svg\", \"size\": 1024})  # JSON string OK\n}\n```\n\n**Why:** Vertex AI stores metadata in BigQuery-compatible format. BigQuery doesn't support nested types. Violation causes: `\"metadata\" in the specified input data is of unsupported type.`\n\n### Rule 2: API Call Structure\n\n```python\nCORRECT ✓\njob = client.batches.create(\n    model=\"gemini-2.5-flash-lite\",\n    src=\"gs://bucket/input.jsonl\",        # Just a string\n    dest=\"gs://bucket/output/\",           # Just a string\n    config={\"display_name\": \"my-job\"}     # Just a dict\n)\n\nWRONG ✗\njob = client.batches.create(\n    model=\"gemini-2.5-flash-lite\",\n    src=\"gs://bucket/input.jsonl\",\n    destination=\"gs://bucket/output/\",    # ← PARAMETER DOESN'T EXIST!\n)\n\nWRONG ✗\njob = client.batches.create(\n    model=\"gemini-2.5-flash-lite\",\n    src=\"gs://bucket/input.jsonl\",\n    config=types.CreateBatchJobConfig(    # ← DON'T INSTANTIATE TYPES!\n        dest=\"gs://bucket/output/\"\n    )\n)\n```\n\n**Why:** The SDK uses simple types. Parameter is `dest=` (not destination). Config is a plain dict (not a type instance). The SDK converts internally.\n\n### Rationalization Table - STOP If You Catch Yourself Thinking:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"Nested metadata is cleaner\" | Your code will fail silently with cryptic errors | Flatten or use `json.dumps()` |\n| \"I'll try `destination=` parameter\" | You'll get a TypeError; parameter doesn't exist | Use `dest=` |\n| \"I should use `CreateBatchJobConfig`\" | You're confusing internal typing with API calls | Pass plain dict to `config=` |\n| \"Other APIs accept nested objects\" | Your assumption breaks here; it's BigQuery-backed | Follow the examples |\n| \"I'll fix it if it breaks\" | Your job fails 5 minutes after submission | Get it right the first time |\n\n### Pre-Submission Validation\n\n```python\n# Add this check BEFORE submitting batch job\ndef validate_metadata(metadata: dict):\n    \"\"\"Ensure metadata contains only primitive types.\"\"\"\n    for key, value in metadata.items():\n        if isinstance(value, (dict, list)):\n            raise ValueError(\n                f\"Metadata '{key}' is {type(value).__name__}. \"\n                f\"Only primitives (str, int, float, bool) allowed. \"\n                f\"Use json.dumps() for complex data.\"\n            )\n        if not isinstance(value, (str, int, float, bool, type(None))):\n            raise ValueError(f\"Unsupported type for '{key}': {type(value)}\")\n\n# Validate all requests before submission:\nfor request in batch_requests:\n    validate_metadata(request[\"metadata\"])\n```\n\n**Enforcement:** Jobs will fail if metadata contains nested objects. There is no workaround for this requirement.\n\n## Key Gotchas\n\n| Issue | Solution |\n|-------|----------|\n| **Nested metadata fails** | **Use flat primitives or `json.dumps()` for complex data** |\n| **TypeError: unexpected keyword** | **Use `dest=` not `destination=`, pass plain dict** |\n| Auth errors with Vertex AI | Run `gcloud auth application-default login` |\n| vertexai=True requires ADC | API key is ignored with vertexai=True |\n| Missing aiplatform API | Run `gcloud services enable aiplatform.googleapis.com` |\n| Region mismatch | Use `us-central1` bucket only |\n| Wrong URI format | Use `gs://` not `https://` |\n| Invalid JSONL | Use `scripts/validate_jsonl.py` |\n| Image batch: inline data | Use `fileData.fileUri` for batch, not inline |\n| Duplicate IDs | Hash file content + prompt for unique IDs |\n| Large PDFs fail | Split at 50 pages / 50MB max |\n| JSON parsing fails | Use robust extraction (see gotchas.md) |\n| Output not found | Output URI is prefix, not file path |\n\n**Top 2 mistakes** (bolded above):\n1. Using nested objects in metadata instead of flat primitives\n2. Guessing parameter names instead of using `dest=`\n\nSee `references/gotchas.md` for detailed solutions (now with Gotchas 10 & 11).\n\n## Rate Limits\n\n| Limit | Value |\n|-------|-------|\n| Max requests per JSONL | 10,000 |\n| Max concurrent jobs | 10 |\n| Max job size | 100MB |\n| Job expiration | 24 hours |\n\n## Recommended Models\n\n| Model | Use Case | Cost |\n|-------|----------|------|\n| `gemini-2.0-flash-lite` | Most batch jobs | Lowest |\n| `gemini-2.0-flash` | Complex extraction | Medium |\n| `gemini-1.5-pro` | Highest accuracy | Highest |\n\n## Additional Resources\n\n### References\n- `references/gcs-setup.md` - **NEW:** Complete GCS and Vertex AI setup guide\n- `references/gotchas.md` - 9 critical production gotchas (updated auth section)\n- `references/best-practices.md` - Idempotent IDs, state tracking, validation\n- `references/troubleshooting.md` - Common errors and debugging\n- `references/vertex-ai.md` - Enterprise alternative with comparison\n- `references/cli-reference.md` - gsutil and gcloud commands\n\n### Examples\n- `examples/icon_batch_vision.py` - **NEW:** Batch vision analysis with Vertex AI\n- `examples/batch_processor.py` - Complete GeminiBatchProcessor class\n- `examples/pipeline_template.py` - Customizable pipeline template\n\n### Scripts\n- `scripts/validate_jsonl.py` - Validate JSONL before submission\n- `scripts/test_single.py` - Test single request before batch\n\n## External Documentation\n\n- [Gemini Batch API Guide](https://ai.google.dev/gemini-api/docs/batch)\n- [Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)\n- [Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)\n\n## Date Awareness\n\n**Pattern from oh-my-opencode:** Gemini API and documentation evolve rapidly.\n\nCurrent date: Use `datetime.now()` for:\n- API version checking\n- Model availability (\"gemini-2.5-flash-lite available as of Dec 2024\")\n- Documentation freshness validation\n\nFor API features or model names with uncertainty, verify against current date and check latest Gemini API documentation."
              },
              {
                "name": "jupytext",
                "description": "This skill should be used when the user asks to \"convert notebook to text\", \"use jupytext\", \"version control notebooks\", \"share data between kernels\", \"set up multi-kernel project\", \"pair notebooks with Python files\", \"sync ipynb and py files\", or needs multi-kernel projects (Python/R/Stata/SAS) with version-control-friendly notebooks.",
                "path": "skills/jupytext/SKILL.md",
                "frontmatter": {
                  "name": "jupytext",
                  "description": "This skill should be used when the user asks to \"convert notebook to text\", \"use jupytext\", \"version control notebooks\", \"share data between kernels\", \"set up multi-kernel project\", \"pair notebooks with Python files\", \"sync ipynb and py files\", or needs multi-kernel projects (Python/R/Stata/SAS) with version-control-friendly notebooks."
                },
                "content": "## Contents\n\n- [Execution Enforcement](#execution-enforcement)\n- [Core Concepts](#core-concepts)\n- [Multi-Kernel Data Sharing](#multi-kernel-data-sharing)\n- [Workflow Integration](#workflow-integration)\n- [Project Structure](#project-structure)\n- [Kernel Specification](#kernel-specification)\n- [Quick Troubleshooting](#quick-troubleshooting)\n- [Additional Resources](#additional-resources)\n- [Best Practices](#best-practices)\n\n# Jupytext Skill\n\nJupytext converts Jupyter notebooks to/from text formats (.py, .R, .md), enabling version control and multi-kernel workflows.\n\n## Execution Enforcement\n\n### IRON LAW: NO EXECUTION CLAIM WITHOUT OUTPUT VERIFICATION\n\nBefore claiming ANY jupytext script executed successfully, follow this sequence:\n1. **EXECUTE** using the papermill pipeline: `jupytext --to notebook --output - script.py | papermill - output.ipynb`\n2. **CHECK** for execution errors (papermill exit code and stderr)\n3. **VERIFY** output.ipynb exists and is non-empty\n4. **INSPECT** outputs using notebook-debug skill verification\n5. **CLAIM** success only after verification passes\n\nThis is non-negotiable. Claiming \"script works\" without executing through papermill is LYING to the user.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I converted to ipynb, so it works\" | Conversion ≠ execution | EXECUTE with papermill, not just convert |\n| \"The .py file looks correct\" | Syntax correctness ≠ runtime correctness | RUN and CHECK outputs |\n| \"I'll let the user execute it\" | You're passing broken code | VERIFY before claiming completion |\n| \"Just a conversion task, no execution needed\" | User expects working notebook | EXECUTE to confirm it works |\n| \"I can use `jupyter nbconvert --execute`\" | Papermill has better error handling | USE the recommended papermill pipeline |\n| \"I'll save the intermediate ipynb first\" | Creates clutter | USE the recommended pipeline (no intermediate files) |\n| \"Exit code 0 means success\" | Papermill can succeed with errors in cells | CHECK output.ipynb for tracebacks |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me just convert and return the ipynb\" → NO. EXECUTE with papermill first.\n- \"The .py file is simple, can't have errors\" → NO. Simple code fails too.\n- \"I'll execute without papermill\" → NO. Use the recommended pipeline.\n- \"Conversion completed, so job done\" → NO. Execution verification required.\n\n### Execution Verification Checklist\n\nBefore EVERY \"notebook works\" claim:\n\n**Conversion:**\n- [ ] Correct format specified (py:percent recommended)\n- [ ] Conversion command succeeded\n- [ ] No syntax errors in conversion\n\n**Execution (MANDATORY):**\n- [ ] Used recommended papermill pipeline: `jupytext --to notebook --output - script.py | papermill - output.ipynb`\n- [ ] Papermill exit code is 0\n- [ ] No errors in stderr\n- [ ] output.ipynb file created\n- [ ] output.ipynb is non-empty (>100 bytes)\n\n**Output Verification:**\n- [ ] Used notebook-debug skill's verification checklist\n- [ ] No tracebacks in any cell\n- [ ] All cells have execution_count (not null)\n- [ ] Expected outputs present (plots, dataframes, metrics)\n- [ ] No unexpected warnings or errors\n\n**Multi-Kernel Projects (if applicable):**\n- [ ] Correct kernel specified in header\n- [ ] Interchange files created (parquet/DTA)\n- [ ] Downstream notebooks can read interchange files\n\n**Only after ALL checks pass:**\n- [ ] Claim \"notebook executed successfully\"\n\n### Gate Function: Jupytext Execution\n\nFollow this sequence for EVERY jupytext task involving execution:\n\n```\n1. CONVERT  → jupytext --to notebook --output -\n2. EXECUTE  → papermill - output.ipynb (with params if needed)\n3. CHECK    → Verify exit code and stderr\n4. INSPECT  → Use notebook-debug verification\n5. VERIFY   → Outputs match expectations\n6. CLAIM    → \"Notebook works\" only after all gates passed\n```\n\n**NEVER skip execution gate.** Converting without executing proves nothing about correctness.\n\n### Honesty Framing\n\n**Claiming a jupytext script works without executing it through papermill is LYING.**\n\nThis is not just format conversion - verify that the notebook executes correctly. The user expects a working notebook, not just syntactically valid code.\n\n## Core Concepts\n\n### Percent Format (Recommended)\n\nUse percent format (`py:percent`) for all projects:\n\n```python\n# %% [markdown]\n# # Analysis Title\n\n# %%\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\n\n# %% tags=[\"parameters\"]\ninput_file = \"data.csv\"\n```\n\nCell markers: `# %%` for code, `# %% [markdown]` for markdown.\n\n**Markdown dollar signs:** Always wrap `$` in backticks to prevent LaTeX rendering - `# Cost: `$50`` not `# Cost: $50`\n\n### Project Configuration\n\nCreate `jupytext.toml` in project root:\n\n```toml\nformats = \"ipynb,py:percent\"\nnotebook_metadata_filter = \"-all\"\ncell_metadata_filter = \"-all\"\n```\n\n### Essential Commands\n\n```bash\n# Convert notebook to percent-format Python file\njupytext --to py:percent notebook.ipynb\n\n# Convert Python script to Jupyter notebook format\njupytext --to notebook script.py\n\n# Enable bidirectional pairing to keep formats synchronized\njupytext --set-formats ipynb,py:percent notebook.ipynb\n\n# Synchronize paired notebook and text file\njupytext --sync notebook.ipynb\n```\n\n### Execution (Recommended Pattern)\n\n**Always pipe to papermill for execution** - no intermediate files:\n\n```bash\n# Convert script to notebook and execute in atomic operation\njupytext --to notebook --output - script.py | papermill - output.ipynb\n\n# Convert and execute with parameter injection\njupytext --to notebook --output - script.py | papermill - output.ipynb -p start_date \"2024-01-01\" -p n_samples 1000\n\n# Convert and execute with detailed logging output\njupytext --to notebook --output - script.py | papermill - output.ipynb --log-output\n\n# Convert and execute in memory without saving intermediate files\njupytext --to notebook --output - script.py | papermill - -\n```\n\nKey flags:\n- `--output -` tells jupytext to write to stdout\n- `papermill - output.ipynb` reads from stdin, writes to file\n- `papermill - -` reads from stdin, writes to stdout (for inspection)\n\n**Why this pattern:**\n1. No intermediate `.ipynb` files cluttering the workspace\n2. Single atomic operation - convert and execute together\n3. Papermill handles parameters, logging, and error reporting\n4. Works in CI/CD pipelines without temp file cleanup\n\n### Debugging Runtime Errors\n\nAfter execution, use `notebook-debug` skill to inspect tracebacks in the output ipynb.\n\n## Multi-Kernel Data Sharing\n\nShare data between Python/R/Stata/SAS via files:\n\n| Route | Format | Write | Read |\n|-------|--------|-------|------|\n| Python -> R | Parquet | `df.to_parquet()` | `arrow::read_parquet()` |\n| Python -> Stata | DTA | `df.to_stata()` | `use \"file.dta\"` |\n| Any -> Any | CSV | Native | Native |\n| SQL queries | DuckDB | Query parquet directly | Query parquet directly |\n\n### Cross-Kernel Pipeline Pattern\n\n```\nPython (prep) -> Parquet -> R (stats) -> Parquet -> Python (report)\n                    |\n                    v\n               Stata (.dta) -> Econometrics\n```\n\n## Workflow Integration\n\n### Git Pre-commit Hook\n\nAdd the following to `.pre-commit-config.yaml`:\n\n```yaml\nrepos:\n  - repo: https://github.com/mwouts/jupytext\n    rev: v1.16.0\n    hooks:\n      - id: jupytext\n        args: [--sync]  # Synchronize paired formats before commit\n```\n\n### Version Control Strategy\n\nChoose one approach:\n\n- **Option A**: Commit only .py files (add `*.ipynb` to `.gitignore`) for minimal repository size\n- **Option B**: Commit both formats to give reviewers format choice\n\n### Editor Integration\n\nConfigure editors for automatic synchronization:\n\n- **VS Code**: Install Jupytext extension for automatic bidirectional sync\n- **JupyterLab**: Right-click notebook and select \"Pair Notebook\" for synchronization\n\n## Project Structure\n\nStandard multi-kernel project layout:\n\n```\nproject/\n├── jupytext.toml          # Project-wide settings\n├── environment.yml        # Conda env with all kernels\n├── notebooks/\n│   ├── 01_python_prep.py  # Python percent format\n│   ├── 02_r_analysis.R    # R percent format\n│   └── 03_stata_models.do # Stata script\n├── data/\n│   ├── raw/\n│   └── processed/         # Parquet/DTA interchange files\n└── results/\n```\n\n## Kernel Specification\n\nSpecify kernel in file header:\n\n```python\n# ---\n# jupyter:\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Python Analysis\n```\n\n## Quick Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Sync conflict | Delete .ipynb, regenerate from .py |\n| Wrong kernel | Add kernelspec header to .py file |\n| Metadata noise | Set `notebook_metadata_filter = \"-all\"` |\n| Cell order lost | Use percent format (preserves structure) |\n\n## Additional Resources\n\n### Reference Files\n\nDetailed patterns and configurations:\n\n- **`references/formats.md`** - All format specifications (percent, light, sphinx, myst, rmd, quarto), cell metadata, configuration options\n- **`references/kernels.md`** - Kernel setup (IRkernel, xeus-r, stata_kernel, pystata, saspy), environment configuration, troubleshooting\n- **`references/data-sharing.md`** - Cross-kernel data sharing patterns (parquet, dta, csv, duckdb), full pipeline examples, validation patterns\n\n### Example Files\n\nWorking code in `examples/`:\n\n- **`examples/python_analysis.py`** - Python percent-format template with common patterns\n- **`examples/r_analysis.R`** - R percent-format template for statistical analysis\n- **`examples/cross_kernel_pipeline.py`** - Multi-kernel data sharing example\n\n### Scripts\n\nUtility scripts in `scripts/`:\n\n- **`scripts/init_project.sh`** - Initialize jupytext project with standard structure\n- **`scripts/sync_all.sh`** - Sync all paired notebooks in project\n\n## Best Practices\n\n1. **Use percent format** - Best balance of readability and cell preservation\n2. **Strip metadata for git** - Use metadata filters for cleaner diffs\n3. **Use parquet for interchange** - Type-safe, cross-language compatible format\n4. **Document kernel requirements** - Include in README or environment.yml\n5. **Enable pre-commit hooks** - Ensure synchronization before commits"
              },
              {
                "name": "look-at",
                "description": "This skill should be used when the user asks to 'look at', 'analyze', 'describe', 'extract from', or 'what's in' media files like PDFs, images, diagrams, screenshots, or charts. Triggers include: 'what does this image show', 'extract the table from this PDF', 'describe this diagram', 'what's in this screenshot', 'analyze this chart', 'read this image', 'get text from this PDF', 'summarize this document', or requests for specific data extraction from visual or document files. Use when analyzed/interpreted content is needed rather than literal file reading (which uses Read tool).",
                "path": "skills/look-at/SKILL.md",
                "frontmatter": {
                  "name": "look-at",
                  "version": 1,
                  "description": "This skill should be used when the user asks to 'look at', 'analyze', 'describe', 'extract from', or 'what's in' media files like PDFs, images, diagrams, screenshots, or charts. Triggers include: 'what does this image show', 'extract the table from this PDF', 'describe this diagram', 'what's in this screenshot', 'analyze this chart', 'read this image', 'get text from this PDF', 'summarize this document', or requests for specific data extraction from visual or document files. Use when analyzed/interpreted content is needed rather than literal file reading (which uses Read tool)."
                },
                "content": "# Look At - Multimodal File Analysis\n\nFast, cost-effective file analysis using Google's Gemini 2.5 Flash Lite model for PDFs, images, diagrams, and other media files.\n\n## Tool Selection Enforcement\n\n### Rationalization Table - STOP When Thinking:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I can read images directly with Read\" | You'll waste thousands of context tokens showing the full image | Use look_at for analysis |\n| \"I'll use Read for this PDF\" | You'll lose table structure and visual information by extracting raw text | Use look_at for PDFs with tables/charts/diagrams |\n| \"Just a quick glance at the file\" | Your quick glances still consume full context tokens | Use look_at for targeted extraction |\n| \"I need exact text, so Read is required\" | Gemini's extraction is accurate for most use cases | Use look_at first, Read only if extraction insufficient |\n| \"look_at adds complexity\" | You gain context savings and faster processing | Use look_at for media files |\n| \"The file is small\" | Your small files still waste context if uninterpreted | Size doesn't determine tool choice, content type does |\n| \"I'll process it myself\" | You waste reasoning tokens on trivial extraction | Delegate to look_at |\n\n### Red Flags - STOP Immediately When Thinking:\n\n- If you catch yourself thinking \"Let me Read this image/PDF/screenshot\" → STOP. Use look_at for media files.\n- If you catch yourself thinking \"I can see the image directly\" → STOP. Seeing it directly still wastes context. Use look_at.\n- If you catch yourself thinking \"Just need to glance at this diagram\" → STOP. Glancing still costs context tokens. Use look_at.\n- If you catch yourself thinking \"The PDF is text-based, so Read is fine\" → STOP. If it has structure/tables/charts, use look_at.\n\n### Cost & Context Benefits\n\n| Scenario | Read Tool | look_at Tool |\n|----------|-----------|--------------|\n| **PDF with table** | Extracts raw text (~1000 tokens), loses table structure | Extracts table as structured data (~100 tokens) |\n| **Screenshot** | Loads entire image (~500 tokens), requires interpretation | Describes content (~50 tokens) |\n| **Diagram** | Shows image (~800 tokens), requires analysis | Explains architecture (~100 tokens) |\n| **Multi-page PDF** | All pages loaded (~5000 tokens) | Extracts specific sections (~200 tokens) |\n\n**look_at saves 80-95% of context tokens by extracting only relevant information.**\n\n## When to Use\n\n**Use look_at when you need:**\n- Media files the Read tool cannot interpret\n- Extracting specific information or summaries from documents\n- Describing visual content in images or diagrams\n- Analyzing charts, tables, or structured data in PDFs\n- When analyzed/extracted data is needed, not raw file contents\n\n**Never use look_at when:**\n- Source code or plain text files needing exact contents (use Read)\n- Files that need editing afterward (need literal content from Read)\n- Simple file reading where no interpretation is needed\n- Exact formatting or structure must be preserved\n\n## How It Works\n\n1. Provide a file path and a specific goal (what to extract)\n2. The helper script uploads the file to Gemini's API\n3. Gemini 2.5 Flash Lite analyzes the file and extracts requested information\n4. Only the relevant extracted information is returned (saves context tokens)\n\n## Usage Pattern\n\n**CRITICAL - Display Requirement:**\nAlways set the Bash tool `description` parameter to show a clean invocation:\n```\ndescription: \"look-at: [goal text]\"\n```\n\nNever display the full Python command to the user.\n\n```bash\n# Basic usage\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"/path/to/file.pdf\" \\\n    --goal \"Extract the title and date from this document\"\n\n# With custom model\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"/path/to/diagram.png\" \\\n    --goal \"Describe the architecture shown in this diagram\" \\\n    --model \"gemini-2.5-flash\"\n```\n\n**IMPORTANT:**\n- Always use absolute paths for files\n- Always set Bash tool `description` to `\"look-at: [goal]\"` for clean UX\n\n## Response Rules\n\nWhen using look_at, the response includes:\n- Only the extracted information matching the goal\n- Clear statement if requested information is not found\n- Concise output focused on the goal (no preamble)\n\nUse this extracted information directly in continued work without loading the full file into context.\n\n## Supported File Types\n\n| Type | Extensions | MIME Types |\n|------|-----------|------------|\n| Images | .jpg, .jpeg, .png, .webp, .heic, .heif | image/* |\n| Videos | .mp4, .mpeg, .mov, .avi, .webm | video/* |\n| Audio | .wav, .mp3, .aiff, .aac, .ogg, .flac | audio/* |\n| Documents | .pdf, .txt, .csv, .md, .html | application/pdf, text/* |\n\n## Model Options\n\n| Model | Use Case | Speed | Cost |\n|-------|----------|-------|------|\n| `gemini-2.5-flash-lite` | Default - fast, cheap analysis | Fastest | Lowest |\n| `gemini-3-flash` | More complex extraction needs | Fast | Low |\n| `gemini-3-pro-preview` | Highest accuracy required | Medium | Medium |\n\n**Default is gemini-2.5-flash-lite** for optimal speed/cost ratio.\n\n## Common Patterns\n\n**REMEMBER:** Always use `description: \"look-at: [goal]\"` in the Bash tool call.\n\n### Extract Specific Information\n```bash\n# Bash tool call with:\n# description: \"look-at: Extract the executive summary section\"\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"report.pdf\" \\\n    --goal \"Extract the executive summary section\"\n```\n\n### Describe Visual Content\n```bash\n# Bash tool call with:\n# description: \"look-at: List all UI elements and their layout\"\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"screenshot.png\" \\\n    --goal \"List all UI elements and their layout\"\n```\n\n### Analyze Diagrams\n```bash\n# Bash tool call with:\n# description: \"look-at: Explain the data flow and component relationships\"\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"architecture.png\" \\\n    --goal \"Explain the data flow and component relationships\"\n```\n\n### Extract Structured Data\n```bash\n# Bash tool call with:\n# description: \"look-at: Extract the table data as JSON\"\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"table.pdf\" \\\n    --goal \"Extract the table data as JSON with columns: name, value, date\"\n```\n\n## Environment Setup\n\n**Required environment variable:**\n```bash\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n```\n\n**Required Python package:**\n```bash\npip install google-genai\n```\n\nFor pixi-managed projects, add to `pixi.toml`:\n```toml\n[dependencies]\ngoogle-genai = \">=1.0.0\"\n```\n\n## Cost Optimization\n\n- **Gemini 2.5 Flash Lite** is the most cost-effective option\n- Only extracts requested information (saves on output tokens)\n- Avoids loading full files into main conversation context\n- Use specific goals to minimize unnecessary processing\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| API key not set | Set `GOOGLE_API_KEY` environment variable |\n| File not found | Use absolute paths, verify file exists |\n| Large file timeout | Break into smaller files or use lower-quality images |\n| Rate limit errors | Add retry logic or use batch processing |\n| Empty response | Check that goal is clear and specific |\n\n## Examples\n\nSee `examples/` directory for:\n- `analyze_pdf.sh` - PDF document extraction\n- `describe_image.sh` - Image analysis\n- `extract_table.sh` - Structured data extraction\n\n## Related Skills\n\n- `/gemini-batch` - For batch processing of many files\n- Standard `Read` tool - For text files needing exact contents"
              },
              {
                "name": "lseg-data",
                "description": "This skill should be used when the user asks to \"access LSEG data\", \"query Refinitiv\", \"get market data from Refinitiv\", \"download fundamentals from LSEG\", \"access ESG scores\", \"convert RIC to ISIN\", or needs the LSEG Data Library Python API.",
                "path": "skills/lseg-data/SKILL.md",
                "frontmatter": {
                  "name": "lseg-data",
                  "version": 1,
                  "description": "This skill should be used when the user asks to \"access LSEG data\", \"query Refinitiv\", \"get market data from Refinitiv\", \"download fundamentals from LSEG\", \"access ESG scores\", \"convert RIC to ISIN\", or needs the LSEG Data Library Python API."
                },
                "content": "## Contents\n\n- [Query Enforcement](#query-enforcement)\n- [Quick Start](#quick-start)\n- [Authentication](#authentication)\n- [Core APIs](#core-apis)\n- [Key Field Prefixes](#key-field-prefixes)\n- [RIC Symbology](#ric-symbology)\n- [Rate Limits](#rate-limits)\n- [Additional Resources](#additional-resources)\n\n# LSEG Data Library\n\nAccess financial data from LSEG (London Stock Exchange Group), formerly Refinitiv, via the `lseg.data` Python library.\n\n## Query Enforcement\n\n### IRON LAW: NO DATA CLAIM WITHOUT SAMPLE INSPECTION\n\nBefore claiming ANY LSEG query succeeded, follow these steps:\n1. **VALIDATE** field names exist (check prefixes: TR., CF_)\n2. **VALIDATE** RIC symbology is correct (.O, .N, .L, .T)\n3. **EXECUTE** the query\n4. **INSPECT** sample rows with `.head()` or `.sample()`\n5. **VERIFY** critical columns are not NULL\n6. **VERIFY** date range matches expectations\n7. **CLAIM** success only after all checks pass\n\nThis is not negotiable. Claiming data retrieval without inspecting results is LYING to the user about data quality.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"The query returned data, so it worked\" | Returned data ≠ correct data | INSPECT for NULLs, wrong dates, invalid values |\n| \"User gave me the RIC\" | Users often use wrong suffixes | VERIFY symbology against RIC Symbology section |\n| \"I'll let pandas handle missing data\" | You'll propagate bad data downstream | CHECK for NULLs BEFORE returning |\n| \"Field names look right\" | Typos are common (TR.EPS vs TR.Eps) | VALIDATE field names in documentation first |\n| \"Just a quick test\" | Test queries teach bad habits | Full validation even for tests |\n| \"I can check the data later\" | You won't | Inspection is MANDATORY before claiming success |\n| \"Rate limits don't matter for small queries\" | Small queries add up | CHECK rate limits section, use batching |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me run this and see what happens\" → NO. Validate field names and RICs FIRST.\n- \"The API will error if something is wrong\" → NO. API returns empty results, not errors.\n- \"I'll just return the dataframe to the user\" → NO. Inspect sample BEFORE returning.\n- \"Market data is always up-to-date\" → NO. Check Date Awareness section (T-1 lag).\n\n### Data Validation Checklist\n\nBefore EVERY data retrieval claim, verify the following:\n\n**For `ld.get_data()` (fundamentals/ESG):**\n- [ ] Field names use correct prefix (TR. for Refinitiv)\n- [ ] RIC symbology verified (correct exchange suffix)\n- [ ] Result inspection: `.head()` or `.sample()` executed\n- [ ] NULL check on critical fields (e.g., revenue, EPS)\n- [ ] Row count verification (is result size reasonable?)\n- [ ] Date context verified (fiscal periods, as-of dates)\n\n**For `ld.get_history()` (time series):**\n- [ ] Field names are valid (OPEN, HIGH, LOW, CLOSE, VOLUME, or CF_ prefixes)\n- [ ] Start/end dates specified explicitly\n- [ ] Date range adjusted for T-1 availability (market data lag)\n- [ ] Result inspection: check first and last rows\n- [ ] NULL check on OHLCV fields\n- [ ] Date continuity check (gaps in trading days expected, but not in date sequence)\n\n**For `symbol_conversion.Definition()` (mapping):**\n- [ ] Input identifier type specified correctly\n- [ ] Result inspection: verify mapped values exist\n- [ ] NULL check (some securities may not have all identifiers)\n\n**For ALL queries:**\n- [ ] Rate limits considered (batch if >10k data points)\n- [ ] Session management: `open_session()` at start, `close_session()` at end\n- [ ] Error handling: try/except for network failures\n- [ ] Sample inspection BEFORE claiming data is ready\n\n## Quick Start\n\nTo get started with LSEG Data Library, initialize a session and execute queries:\n\n```python\nimport lseg.data as ld\n\n# Initialize session\nld.open_session()\n\n# Get fundamentals\ndf = ld.get_data(\n    universe=['AAPL.O', 'MSFT.O'],\n    fields=['TR.CompanyName', 'TR.Revenue', 'TR.EPS']\n)\nprint(df.head())  # Inspect sample data\n\n# Get historical prices\nprices = ld.get_history(\n    universe='AAPL.O',\n    fields=['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME'],\n    start='2023-01-01',\n    end='2023-12-31'\n)\nprint(prices.head())  # Inspect sample data\n\n# Close session\nld.close_session()\n```\n\n## Authentication\n\nConfigure LSEG authentication using either a config file or environment variables.\n\n### Config File Method\n\nCreate `lseg-data.config.json`:\n```json\n{\n  \"sessions\": {\n    \"default\": \"platform.ldp\",\n    \"platform\": {\n      \"ldp\": {\n        \"app-key\": \"YOUR_APP_KEY\",\n        \"username\": \"YOUR_MACHINE_ID\",\n        \"password\": \"YOUR_PASSWORD\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables Method\n\nSet the following environment variables for LSEG authentication:\n\n```bash\n# Configure LSEG credentials via environment variables\nexport RDP_USERNAME=\"YOUR_MACHINE_ID\"\nexport RDP_PASSWORD=\"YOUR_PASSWORD\"\nexport RDP_APP_KEY=\"YOUR_APP_KEY\"\n```\n\n## Core APIs\n\n| API | Use Case | Example |\n|-----|----------|---------|\n| `ld.get_data()` | Point-in-time data | Fundamentals, ESG scores |\n| `ld.get_history()` | Time series | Historical prices, OHLCV |\n| `symbol_conversion.Definition()` | ID mapping | RIC ↔ ISIN ↔ CUSIP |\n\n## Key Field Prefixes\n\n| Prefix | Type | Example |\n|--------|------|---------|\n| `TR.` | Refinitiv fields | `TR.Revenue`, `TR.EPS` |\n| `CF_` | Composite (real-time) | `CF_LAST`, `CF_BID` |\n\n## RIC Symbology\n\n| Suffix | Exchange | Example |\n|--------|----------|---------|\n| `.O` | NASDAQ | `AAPL.O` |\n| `.N` | NYSE | `IBM.N` |\n| `.L` | London | `VOD.L` |\n| `.T` | Tokyo | `7203.T` |\n\n## Rate Limits\n\n| Endpoint | Limit |\n|----------|-------|\n| `get_data()` | 10,000 data points/request |\n| `get_history()` | 3,000 rows/request |\n| Session | 500 requests/minute |\n\n## Additional Resources\n\n### Reference Files\n\n- **`references/fundamentals.md`** - Financial statement fields, ratios, estimates\n- **`references/esg.md`** - ESG scores, pillars, controversies\n- **`references/symbology.md`** - RIC/ISIN/CUSIP conversion\n- **`references/pricing.md`** - Historical prices, real-time data\n- **`references/screening.md`** - Stock screening with Screener object\n- **`references/troubleshooting.md`** - Common issues and solutions\n- **`references/wrds-comparison.md`** - LSEG vs WRDS data mapping\n\n### Example Files\n\n- **`examples/historical_pricing.ipynb`** - Historical price retrieval\n- **`examples/fundamentals_query.py`** - Fundamental data patterns\n- **`examples/stock_screener.ipynb`** - Dynamic stock screening\n\n### Scripts\n\n- **`scripts/test_connection.py`** - Validate LSEG connectivity\n\n### Local Sample Repositories\n\nLSEG API samples at `~/resources/lseg-samples/`:\n- `Example.RDPLibrary.Python/` - Core API examples\n- `Examples.DataLibrary.Python.AdvancedUsecases/` - Advanced patterns\n- `Article.DataLibrary.Python.Screener/` - Stock screening\n\n## Date Awareness\n\nWhen querying market data, account for current date context and market data lag.\n\n### Market Data Lag\n\nMarket data typically has T-1 availability, meaning today's data becomes available tomorrow. Adjust date ranges accordingly.\n\n### Date Range Example\n\nUse current date context when querying historical prices:\n\n```python\nfrom datetime import datetime, timedelta\n\n# Get recent market data\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)\n\n# Adjust to exclude recent data (T-1 for market data availability)\nend_date = end_date - timedelta(days=1)\n\ndf = ld.get_history(\n    universe=\"AAPL.O\",\n    fields=['CLOSE'],\n    start=start_date.strftime('%Y-%m-%d'),\n    end=end_date.strftime('%Y-%m-%d')\n)\n```\n\nRemember: Always account for the T-1 lag in market data availability."
              },
              {
                "name": "marimo",
                "description": "This skill should be used when the user asks to \"use marimo\", \"create a marimo notebook\", \"debug a marimo notebook\", \"inspect cells\", \"understand reactive execution\", \"fix marimo errors\", \"convert from jupyter to marimo\", or works with marimo reactive Python notebooks.",
                "path": "skills/marimo/SKILL.md",
                "frontmatter": {
                  "name": "marimo",
                  "description": "This skill should be used when the user asks to \"use marimo\", \"create a marimo notebook\", \"debug a marimo notebook\", \"inspect cells\", \"understand reactive execution\", \"fix marimo errors\", \"convert from jupyter to marimo\", or works with marimo reactive Python notebooks."
                },
                "content": "## Contents\n\n- [Editing and Verification Enforcement](#editing-and-verification-enforcement)\n- [Key Concepts](#key-concepts)\n- [Cell Structure](#cell-structure)\n- [Editing Rules](#editing-rules)\n- [Core CLI Commands](#core-cli-commands)\n- [Export Commands](#export-commands)\n- [Data and Visualization](#data-and-visualization)\n- [Debugging Workflow](#debugging-workflow)\n- [Common Issues](#common-issues)\n- [Additional Resources](#additional-resources)\n\n# Marimo Reactive Notebooks\n\nMarimo is a reactive Python notebook where cells form a DAG and auto-execute on dependency changes. Notebooks are stored as pure `.py` files.\n\n## Editing and Verification Enforcement\n\n### IRON LAW #1: NEVER MODIFY CELL DECORATORS OR SIGNATURES\n\nOnly edit code INSIDE `@app.cell` function bodies. This is not negotiable.\n\n**NEVER modify:**\n- Cell decorators (`@app.cell`)\n- Function signatures (`def _(deps):`)\n- Return statements structure (trailing commas required)\n\n**ALWAYS verify:**\n- All used variables are in function parameters\n- All created variables are in return statement\n- Trailing comma for single returns: `return var,`\n\n### IRON LAW #2: NO EXECUTION CLAIM WITHOUT OUTPUT VERIFICATION\n\nBefore claiming ANY marimo notebook works:\n1. **VALIDATE** syntax and structure: `marimo check notebook.py`\n2. **EXECUTE** with outputs: `marimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs`\n3. **VERIFY** using notebook-debug skill's verification checklist\n4. **CLAIM** success only after verification passes\n\nThis is not negotiable. Claiming \"notebook works\" without executing and inspecting outputs is LYING to the user.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"marimo check passed, so it works\" | Syntax check ≠ runtime correctness | EXECUTE with --include-outputs and inspect |\n| \"Just a small change, can't break anything\" | Reactivity means small changes propagate everywhere | VERIFY with full execution |\n| \"I'll let marimo handle the dependency tracking\" | Verification of correct behavior is still required | CHECK outputs match expectations |\n| \"The function signature looks right\" | Wrong deps/returns break reactivity silently | VALIDATE all vars are in params AND returns |\n| \"I can modify the function signature\" | Breaks marimo's dependency detection | ONLY edit inside function bodies |\n| \"Variables can be used without returning them\" | Will cause NameError in dependent cells | RETURN all created variables |\n| \"I can skip the trailing comma for single returns\" | Python treats `return var` as returning the value, breaks unpacking | USE `return var,` for single returns |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me add this variable to the function signature\" → NO. Marimo manages signatures.\n- \"I'll just run marimo check and call it done\" → NO. Execute with outputs required.\n- \"The code looks correct\" → NO. Marimo's reactivity must be verified at runtime.\n- \"I can redefine this variable in another cell\" → NO. One variable = one cell.\n\n### Editing Checklist\n\nBefore every marimo edit:\n\n**Structure Validation:**\n- [ ] Only edit code INSIDE `@app.cell` function bodies\n- [ ] Do NOT modify decorators or signatures\n- [ ] Verify all used variables are in function parameters\n- [ ] Verify all created variables are in return statement\n- [ ] Ensure trailing comma used for single returns\n- [ ] Ensure no variable redefinitions across cells\n\n**Syntax Validation:**\n- [ ] Execute `marimo check notebook.py`\n- [ ] Verify no syntax errors reported\n- [ ] Verify no undefined variable warnings\n- [ ] Verify no redefinition warnings\n\n**Runtime Verification:**\n- [ ] Execute with `marimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs`\n- [ ] Verify export succeeded (exit code 0)\n- [ ] Verify output ipynb exists and is non-empty\n- [ ] Apply notebook-debug verification checklist\n- [ ] Verify no tracebacks in any cell\n- [ ] Verify all cells executed (execution_count not null)\n- [ ] Verify outputs match expectations\n\n**Only after ALL checks pass:**\n- [ ] Claim \"notebook works\"\n\n### Gate Function: Marimo Verification\n\nFollow this sequence for EVERY marimo task:\n\n```\n1. EDIT     → Modify code inside @app.cell function bodies only\n2. CHECK    → marimo check notebook.py\n3. EXECUTE  → marimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs\n4. INSPECT  → Use notebook-debug verification\n5. VERIFY   → Outputs match expectations\n6. CLAIM    → \"Notebook works\" only after all gates passed\n```\n\n**NEVER skip verification gates.** Marimo's reactivity means changes propagate unpredictably.\n\n### Honesty Framing\n\n**Claiming a marimo notebook works without executing it with --include-outputs and inspecting the results is LYING.**\n\nSyntax checks and code inspection prove nothing about reactive execution correctness. The user expects a working notebook where all cells execute correctly with proper dependency tracking.\n\n## Key Concepts\n\n- **Reactive execution**: Cells auto-update when dependencies change\n- **No hidden state**: Each variable defined in exactly one cell\n- **Pure Python**: `.py` files, version control friendly\n- **Cell structure**: `@app.cell` decorator pattern\n\n## Cell Structure\n\n```python\nimport marimo\n\napp = marimo.App()\n\n@app.cell\ndef _(pl):  # Dependencies as parameters\n    df = pl.read_csv(\"data.csv\")\n    return df,  # Trailing comma required for single return\n\n@app.cell\ndef _(df, pl):\n    summary = df.describe()\n    filtered = df.filter(pl.col(\"value\") > 0)\n    return summary, filtered  # Multiple returns\n```\n\n## Editing Rules\n\n- Edit code INSIDE `@app.cell` functions only\n- Never modify cell decorators or function signatures\n- Variables cannot be redefined across cells\n- All used variables must be returned from their defining cell\n- **Markdown cells: Always wrap `$` in backticks** - `mo.md(\"Cost: `$50`\")` not `mo.md(\"Cost: $50\")`\n\n## Core CLI Commands\n\n| Command | Purpose |\n|---------|---------|\n| `marimo edit notebook.py` | marimo: Open notebook in browser editor for interactive development |\n| `marimo run notebook.py` | marimo: Run notebook as executable app |\n| `marimo check notebook.py` | marimo: Validate notebook structure and syntax without execution |\n| `marimo convert notebook.ipynb` | marimo: Convert Jupyter notebook to marimo format |\n\n## Export Commands\n\n```bash\n# marimo: Export to ipynb with code only\nmarimo export ipynb notebook.py -o __marimo__/notebook.ipynb\n\n# marimo: Export to ipynb with outputs (runs notebook first)\nmarimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs\n\n# marimo: Export to HTML (runs notebook by default)\nmarimo export html notebook.py -o __marimo__/notebook.html\n\n# marimo: Export to HTML with auto-refresh on changes (live preview)\nmarimo export html notebook.py -o __marimo__/notebook.html --watch\n```\n\n**Key difference:** HTML export runs the notebook by default. ipynb export does NOT - use `--include-outputs` to run and capture outputs.\n\n**Tip:** Use `__marimo__/` folder for all exports (ipynb, html). The editor can auto-save there.\n\n## Data and Visualization\n\n- Prefer polars over pandas for performance\n- Use `mo.ui` for interactive widgets\n- SQL cells: `mo.sql(df, \"SELECT * FROM df\")`\n- Display markdown: `mo.md(\"# Heading\")`\n\n## Debugging Workflow\n\n**1. Pre-execution validation:**\n```bash\n# scripts: Validate notebook syntax and cell structure\nscripts/check_notebook.sh notebook.py\n```\nRuns syntax check, marimo validation, and cell structure overview in one command.\n\n**2. Runtime errors:** Export with outputs, then use `notebook-debug` skill:\n```bash\n# marimo: Export to ipynb with outputs for inspection\nmarimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs\n```\n\n## Common Issues\n\n| Issue | Fix |\n|-------|-----|\n| Variable redefinition | Rename one variable or merge cells |\n| Circular dependency | Break cycle by merging or restructuring |\n| Missing return | Add `return var,` with trailing comma |\n| Import not available | Ensure import cell returns the module |\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and advanced techniques, consult:\n- **`references/reactivity.md`** - DAG execution, variable rules, dependency detection patterns\n- **`references/debugging.md`** - Error patterns, runtime debugging, environment-specific issues\n- **`references/widgets.md`** - Interactive UI components and mo.ui patterns\n- **`references/sql.md`** - SQL cells and database integration techniques\n\n### Examples\n\nWorking examples available in `examples/`:\n- **`examples/basic_notebook.py`** - Minimal marimo notebook structure\n- **`examples/data_analysis.py`** - Data loading, filtering, and visualization patterns\n- **`examples/interactive_widgets.py`** - Interactive UI component usage\n\n### Scripts\n\nValidation utilities in `scripts/`:\n- **`scripts/check_notebook.sh`** - Primary validation: syntax check, marimo validation, cell structure overview\n- **`scripts/get_cell_map.py`** - Extract cell metadata (invoked by check_notebook.sh)\n\n### Related Skills\n\n- **`notebook-debug`** - Debugging executed ipynb files with tracebacks and output inspection"
              },
              {
                "name": "notebook-debug",
                "description": "This skill should be used when the user asks to \"debug notebook\", \"inspect notebook outputs\", \"find notebook error\", \"read traceback from ipynb\", \"why did notebook fail\", or needs to understand runtime errors in executed Jupyter notebooks from any source (marimo, jupytext, papermill).",
                "path": "skills/notebook-debug/SKILL.md",
                "frontmatter": {
                  "name": "notebook-debug",
                  "description": "This skill should be used when the user asks to \"debug notebook\", \"inspect notebook outputs\", \"find notebook error\", \"read traceback from ipynb\", \"why did notebook fail\", or needs to understand runtime errors in executed Jupyter notebooks from any source (marimo, jupytext, papermill)."
                },
                "content": "## Contents\n\n- [Verification Enforcement](#verification-enforcement)\n- [Why Execute to ipynb?](#why-execute-to-ipynb)\n- [Execution Commands](#execution-commands)\n- [Inspection Methods](#inspection-methods)\n- [Quick Failure Check](#quick-failure-check)\n- [Read Tool for Debugging](#read-tool-for-debugging)\n- [Common Patterns](#common-patterns)\n- [Debugging Workflow](#debugging-workflow)\n\n# Debugging Executed Notebooks\n\nThis skill covers inspecting executed `.ipynb` files to debug runtime errors, regardless of how the notebook was created (marimo, jupytext, or plain Jupyter).\n\n## Verification Enforcement\n\n### IRON LAW: NO 'NOTEBOOK WORKS' CLAIM WITHOUT TRACEBACK CHECK\n\nBefore claiming ANY notebook executed successfully, you MUST:\n1. **EXECUTE** the notebook to ipynb with outputs\n2. **CHECK** for tracebacks (Quick Failure Check section)\n3. **READ** the ipynb file with Read tool if errors found\n4. **VERIFY** all cells have execution_count (not null)\n5. **INSPECT** outputs for warnings/unexpected behavior\n6. **CLAIM** success only after all verification passes\n\nThis is not negotiable. Claiming \"notebook works\" without checking for tracebacks is LYING to the user.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"The command succeeded, so notebook works\" | Exit code 0 ≠ no errors | CHECK for tracebacks in outputs |\n| \"I'll just run the source file directly\" | You'll miss cell-level errors | EXECUTE to ipynb first, then inspect |\n| \"User will see errors when they run it\" | You're wasting their time | VERIFY before claiming completion |\n| \"I can see the code, so I know it works\" | Code that looks right can still fail | EXECUTE and READ outputs |\n| \"Quick check with grep is enough\" | Grep misses stderr and cell outputs | Use BOTH quick check AND Read tool |\n| \"Only the last cell matters\" | Middle cells can fail silently | VERIFY all cells executed (execution_count) |\n| \"I'll fix errors if user reports them\" | Proactive checking is your job | CHECK before user sees it |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me run marimo/jupytext and assume it worked\" → NO. Execute to ipynb and CHECK outputs.\n- \"The notebook ran last time, so it still works\" → NO. Fresh execution EVERY time.\n- \"I can tell from the code that it's correct\" → NO. Code inspection ≠ runtime verification.\n- \"Just a small change, can't break anything\" → NO. Small changes cause big failures.\n\n### Verification Checklist\n\nBefore claiming \"notebook works\":\n\n**Execution:**\n- [ ] Execute notebook to ipynb format\n- [ ] Use `--include-outputs` flag (for marimo)\n- [ ] Verify output file created successfully\n- [ ] Verify output file is non-empty\n\n**Traceback Check:**\n- [ ] Run quick failure check: `jq -r '.cells[].outputs[]?.text[]?' | grep \"Traceback\"`\n- [ ] Check error count: `jq '[.cells[].outputs[]? | select(.output_type == \"error\")] | length'`\n- [ ] Use Read tool to inspect full context if errors found\n\n**Cell Execution:**\n- [ ] Verify all cells have execution_count (no null values)\n- [ ] Check execution order is sequential (no out-of-order cells)\n- [ ] Verify no cells skipped due to prior failures\n\n**Output Inspection:**\n- [ ] Verify critical outputs (not just absence of errors)\n- [ ] Check expected results present (dataframes, plots, metrics)\n- [ ] Verify no warnings that indicate problems\n- [ ] Check no unexpected NaN/None/empty results\n\n**Claim success only after:**\n- [ ] All checks pass: declare \"notebook executed successfully\"\n\n### Gate Function: Notebook Verification\n\nApply this verification sequence for every notebook debugging task:\n\n```\n1. EXECUTE → Run to ipynb with outputs\n2. CHECK   → Quick traceback/error count check\n3. READ    → Full inspection with Read tool if errors\n4. VERIFY  → All cells executed, outputs as expected\n5. CLAIM   → \"Notebook works\" only after all gates passed\n```\n\n**Never skip any gate.** Each gate catches different failure modes.\n\n## Why Execute to ipynb?\n\nConverting and executing notebooks to ipynb captures:\n- Cell outputs and return values\n- Tracebacks with full context\n- Execution order and cell IDs\n\nThis makes debugging much easier than reading raw `.py` source.\n\n## Execution Commands\n\n```bash\n# Export marimo notebook to ipynb with outputs\nmarimo export ipynb notebook.py -o __marimo__/notebook.ipynb --include-outputs\n\n# Convert jupytext to ipynb and execute with outputs\njupytext --to notebook --output - script.py | papermill - output.ipynb\n\n# Execute existing ipynb notebook to capture outputs\npapermill input.ipynb output.ipynb\n```\n\n## Inspection Methods\n\n|                  | jq                            | Read tool           |\n|------------------|-------------------------------|---------------------|\n| Output           | Raw JSON with escaped strings | Clean rendered view |\n| Error visibility | Buried in outputs array       | Inline after cell   |\n| Cell context     | Need to piece together        | Cell IDs visible    |\n| Scripting        | Better for automation         | Not scriptable      |\n\n**Verdict:** Use Read for debugging/inspection, jq for scripting/CI.\n\n## Quick Failure Check\n\n```bash\n# Check for tracebacks in notebook outputs\njq -r '.cells[].outputs[]?.text[]?' notebook.ipynb | grep \"Traceback\"\n\n# Count error outputs in notebook\njq '[.cells[].outputs[]? | select(.output_type == \"error\")] | length' notebook.ipynb\n```\n\n## Read Tool for Debugging\n\nThe Read tool renders ipynb with errors inline after the failing cell:\n\n```\n<cell id=\"MJUe\">raise ValueError(\"intentional error\")</cell>\n\nTraceback (most recent call last):\n  File \"/path/to/notebook.py\", line 5, in <module>\n    raise ValueError(\"intentional error\")\nValueError: intentional error\n\n<cell id=\"vblA\">y = x + 10  # depends on x, not the error cell</cell>\n```\n\nBenefits:\n- Errors appear immediately after the cell that caused them\n- Cell IDs visible for cross-referencing\n- Full traceback with line numbers\n- No JSON parsing needed\n\n## Common Patterns\n\n### Find the Failing Cell\n\nUse the Read tool to inspect the notebook and locate tracebacks:\n```bash\n# Read notebook to find traceback location inline after failing cell\nRead __marimo__/notebook.ipynb\n```\n\n### Check Cell Execution Count\n\nIdentify cells that did not execute:\n```bash\n# Find cells with null execution_count (not executed)\njq '.cells[] | select(.execution_count == null) | .source[:50]' notebook.ipynb\n```\n\n### Extract All Errors\n\nGather all error outputs from executed cells:\n```bash\n# Extract error tracebacks from all cells\njq -r '.cells[].outputs[]? | select(.output_type == \"error\") | .traceback[]' notebook.ipynb\n```\n\n## Debugging Workflow\n\n1. **Execute notebook with outputs captured:**\n   ```bash\n   # Export marimo notebook to ipynb format with all outputs\n   marimo export ipynb nb.py -o __marimo__/nb.ipynb --include-outputs\n   ```\n\n2. **Run quick failure check:**\n   ```bash\n   # Check if execution produced tracebacks\n   jq -r '.cells[].outputs[]?.text[]?' __marimo__/nb.ipynb | grep -q \"Traceback\" && echo \"FAILED\"\n   ```\n\n3. **Inspect notebook using Read tool:**\n   ```bash\n   # Read the full notebook to identify failing cells and their errors\n   Read __marimo__/nb.ipynb\n   ```\n\n4. **Fix source code and re-run to verify**"
              },
              {
                "name": "using-skills",
                "description": "Auto-loaded at session start via SessionStart hook. Teaches skill invocation protocol, tool selection rules (look-at for media, skills for workflows), agent delegation patterns, and enforcement mechanisms. NOT user-triggered - provides foundational skill usage discipline for all sessions.",
                "path": "skills/using-skills/SKILL.md",
                "frontmatter": {
                  "name": "using-skills",
                  "version": 1,
                  "description": "Auto-loaded at session start via SessionStart hook. Teaches skill invocation protocol, tool selection rules (look-at for media, skills for workflows), agent delegation patterns, and enforcement mechanisms. NOT user-triggered - provides foundational skill usage discipline for all sessions."
                },
                "content": "# Using Skills\n\n**Invoke relevant skills BEFORE any response or action.**\n\nThis is non-negotiable. Even a 1% chance a skill applies requires checking.\n\n## CRITICAL: Skill Already Loaded - DO NOT RE-INVOKE\n\n<EXTREMELY-IMPORTANT>\n**If you see a skill name in the current conversation turn (e.g., `<command-name>/dev</command-name>`), the skill is ALREADY LOADED.**\n\n**DO NOT:**\n- ❌ Use the Skill tool to invoke it again\n- ❌ Say \"I need to invoke the skill\"\n- ❌ Call `Skill(skill=\"dev\")` or similar\n\n**DO INSTEAD:**\n- ✅ The skill instructions follow immediately in the next message\n- ✅ Just proceed to the next step\n- ✅ Follow the loaded skill's instructions directly\n\n**If you catch yourself about to invoke a skill that's already loaded, STOP. Just go to the next step.**\n</EXTREMELY-IMPORTANT>\n\n## The Rule\n\n```\nUser message arrives\n    ↓\nIs user explicitly invoking a skill (e.g., \"use /dev\")?\n    ↓\nYES → SKILL IS ALREADY LOADED\n      ↓\n      DO NOT invoke again with Skill tool\n      ↓\n      Proceed to next step (follow skill instructions)\nNO  → Check: Does this match any skill trigger?\n    ↓\nYES → Invoke skill FIRST, then follow its protocol\nNO  → Proceed normally\n```\n\n## Workflow Commands (User Must Invoke Explicitly)\n\nThese are commands, not auto-triggered skills. User must explicitly type the command:\n\n| Command | Purpose | User Types |\n|---------|---------|------------|\n| `/dev` | Feature development workflow (7 phases) | `/dev` |\n| `/ds` | Data analysis workflow (5 phases) | `/ds` |\n\n## Skill Triggers (Can Auto-Invoke)\n\n| User Intent | Skill | Trigger Words |\n|-------------|-------|---------------|\n| Bug/fix | `dev-debug` | bug, broken, fix, doesn't work, crash, error, fails |\n| Writing | `writing` | write, draft, document, essay, paper |\n| **Media analysis** | **look-at** | describe image, analyze PDF, what's in this, screenshot, diagram |\n\n## Red Flags - You're Skipping the Skill Check\n\nIf you think any of these, STOP:\n\n| Thought | Reality |\n|---------|---------|\n| **\"I need to invoke the skill properly\"** | **If user said \"use /dev\", it's ALREADY LOADED. Just proceed.** |\n| **\"Let me invoke the skill first\"** | **Check for `<command-name>` tag - it's already loaded if present** |\n| **\"I should use Skill tool for /dev\"** | **NO. User invocation = already loaded = proceed to next step** |\n| \"This is just a simple question\" | Simple questions don't involve reading code |\n| \"I'll gather information first\" | That IS investigation - use the skill |\n| \"I know exactly what to do\" | The skill provides structure you'll miss |\n| \"It's just one file\" | Scope doesn't exempt you from process |\n| \"Let me quickly check...\" | \"Quickly\" means skipping the workflow |\n| **\"I can read this image directly\"** | **Use look-at to save context tokens** |\n\n## Bug Reports - Mandatory Response\n\nWhen user mentions a bug:\n\n```\nDO NOT:\n1. Read code files\n2. Investigate independently\n3. \"Take a look\" without structure\n\nINSTEAD:\n1. Start ralph loop:\n   ralph-loop: Start Ralph Loop in current session with bug debugging\n   Skill(skill=\"ralph-loop:ralph-loop\", args=\"Debug: [symptom] --max-iterations 15 --completion-promise FIXED\")\n2. Inside loop, follow /dev-debug protocol\n```\n\n**Any code reading before starting the workflow is a violation.**\n\n## Skill Priority\n\nWhen multiple skills could apply:\n\n1. **Process skills first** - debugging, brainstorming determine approach\n2. **Then implementation** - dev, ds, writing execute the approach\n\n## How to Invoke\n\nUse the Skill tool to invoke skills:\n\n```bash\n# dev-debug: Systematic bug investigation and fixing with verification-driven methodology\nSkill(skill=\"dev-debug\")\n\n# dev: Feature development workflow with 7 phases and TDD enforcement\nSkill(skill=\"dev\")\n\n# ds: Data analysis workflow with 5 phases and output-first verification\nSkill(skill=\"ds\")\n```\n\nOr start ralph loop first for implementation/debug phases:\n\n```bash\n# ralph-loop: Per-task ralph loop pattern for implementation and debugging\nSkill(skill=\"ralph-loop:ralph-loop\", args=\"Task description --max-iterations 15\")\n```\n\n## IRON LAW: Multimodal File Analysis\n\n**NO READING IMAGES/PDFS WITH Read TOOL. USE look-at INSTEAD.**\n\n### The Rule\n\n```\nUser asks about image/PDF/media content\n    ↓\nIs it a media file requiring interpretation?\n    ↓\nYES → Use look-at skill (bash call to look_at.py)\nNO  → Use Read tool for source code/text\n```\n\n### When to Use look-at\n\n**ALWAYS use look-at for:**\n- `.jpg`, `.jpeg`, `.png`, `.webp`, `.gif`, `.heic` - Images\n- `.pdf` - PDFs requiring content extraction\n- `.mp4`, `.mov`, `.avi`, `.webm` - Videos\n- `.mp3`, `.wav`, `.aac`, `.ogg` - Audio\n- Any file where you need to UNDERSTAND content, not just see raw bytes\n\n**Pattern:**\n```bash\n# look-at: Extract information from media file with specific goal\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"/absolute/path/to/file\" \\\n    --goal \"What specific information to extract\"\n```\n\n### When NOT to Use look-at\n\n**Use Read tool instead for:**\n- Source code files (`.py`, `.js`, `.rs`, etc.) - need exact formatting for editing\n- Plain text files (`.txt`, `.md`, `.json`, etc.) - preserve exact content\n- Config files requiring exact formatting preservation\n- Any file that needs editing after reading\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I can read images directly\" | Read tool shows you the image, but wastes context tokens | Use look-at to extract ONLY what's needed |\n| \"It's just one small image\" | Still uses 1000+ tokens in conversation context | look-at returns 50-200 tokens of extracted info |\n| \"I need to see the whole thing\" | You can see it, user can't see what you see | Use look-at with specific goal |\n| \"look-at might miss details\" | You can always fall back to Read if needed | Start with look-at, escalate if insufficient |\n| \"The user didn't ask for look-at\" | look-at is FOR YOU, not the user | Use the right tool for the job |\n\n### Red Flags - STOP If You Catch Yourself:\n\n- **\"Let me read this image...\"** → NO. Use look-at.\n- **\"I'll use Read to see what's in the PDF...\"** → NO. Use look-at.\n- **\"Just quickly checking this screenshot...\"** → NO. Use look-at.\n- **Passing image path to Read tool** → STOP. Use look-at instead.\n\n### Cost & Context Benefits\n\n- **Read tool on image:** ~1,000-5,000 context tokens\n- **look-at extraction:** ~50-200 output tokens\n- **Savings:** 95%+ token reduction\n- **Speed:** Faster responses, less context bloat\n\n### Example Usage\n\n```bash\n# look-at: Extract specific information from image file\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"$HOME/Downloads/screenshot.png\" \\\n    --goal \"List all buttons and their labels\"\n\n# look-at: Analyze diagram to understand data flow\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"$HOME/Documents/architecture.png\" \\\n    --goal \"Explain the data flow between components\"\n\n# look-at: Extract information from PDF document\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"$HOME/Downloads/report.pdf\" \\\n    --goal \"Extract the executive summary section\"\n```\n\n### Enforcement\n\n**Using Read on images/PDFs when look-at should be used results in:**\n1. Wasting context tokens unnecessarily\n2. Making conversations slower\n3. Ignoring available optimization tools\n4. Violating the tool selection protocol\n\n**Validate before calling Read:** Ask \"Is this a media file?\" If yes, invoke look-at instead.\n\n## IRON LAW: Following Skill Instructions\n\n**WHEN A SKILL LOADS, YOU MUST FOLLOW ITS EXACT INSTRUCTIONS.**\n\nSkills contain specific patterns, required parameters, and enforcement rules. Skipping these requirements defeats the purpose of loading the skill.\n\n### The Rule\n\n```\nSkill loads successfully\n    ↓\nRead the skill's requirements carefully\n    ↓\nFollow ALL instructions, including:\n    - Required tool parameters (descriptions, timeouts, etc.)\n    - Specific command patterns\n    - Enforcement patterns (Iron Laws, Red Flags)\n    - Step sequences\n    ↓\nExecute using the skill's exact patterns\n```\n\n### Common Violations\n\n**Bash Description Parameter:**\n\nWhen a skill requires `description` parameter on Bash calls (like look-at), you MUST include it:\n\n```bash\n# ❌ WRONG: No description parameter\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py \\\n    --file \"/path/to/file.pdf\" \\\n    --goal \"Extract title\"\n\n# ✅ CORRECT: With description parameter as skill requires\nBash(\n    command='python3 ${CLAUDE_PLUGIN_ROOT}/skills/look-at/scripts/look_at.py --file \"/path/to/file.pdf\" --goal \"Extract title\"',\n    description=\"look-at: Extract title\"\n)\n```\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"The skill is just guidance\" | Skills contain tested, required patterns | Follow the skill's exact instructions |\n| \"I know a better way\" | Your way skips enforcement or optimization | Use the skill's pattern - it exists for a reason |\n| \"Description parameter is optional\" | When skill says REQUIRED, it's required | Add the description parameter as instructed |\n| \"I'll add it if it fails\" | You'll clutter the conversation with messy output first | Follow the pattern from the start |\n| \"It's just cosmetic\" | Clean descriptions improve UX and debugging | Professional output requires following the pattern |\n\n### Red Flags - STOP If You Catch Yourself:\n\n- **About to call Bash without description when skill requires it** → STOP. Add the description parameter.\n- **Thinking \"I'll skip this requirement\"** → STOP. Skills don't have optional requirements.\n- **\"The skill says to do X but I'll do Y\"** → STOP. Follow the skill or don't load it.\n- **Modifying the skill's pattern \"to be simpler\"** → STOP. The pattern exists for a reason.\n\n### Why This Matters\n\n**Skills encode:**\n1. **Tested patterns** - Proven to work in production\n2. **Optimization** - Context/token savings, clean output\n3. **Enforcement** - Prevent common mistakes\n4. **UX standards** - Consistent, professional output\n\n**When you skip skill instructions:**\n- ❌ You waste the effort of loading the skill\n- ❌ You create messy, unprofessional output\n- ❌ You miss optimizations (context savings, speed)\n- ❌ You violate user expectations\n- ❌ You make debugging harder\n\n**The skill loaded for a reason - follow it completely.**\n\n## Advanced Agent Harnessing Patterns\n\n**For detailed oh-my-opencode production patterns including:**\n- Background + parallel execution (3x speedup)\n- Tool restrictions for focused agents\n- Structured delegation templates\n- Failure recovery protocol\n- Environment context injection\n- Cost classification system\n- Metadata-driven prompts\n\n**See:** `references/agent-harnessing.md`\n\nQuick reference:\n- Tool restrictions: `common/helpers/tool-restrictions.md`\n- Delegation template: `common/templates/delegation-template.md`\n- Metadata infrastructure: `common/metadata/skill-metadata.py`\n- Full oh-my-opencode guide: `common/OH-MY-OPENCODE-PATTERNS.md`\n\nBased on: [obra/superpowers](https://github.com/obra/superpowers) and oh-my-opencode production patterns."
              },
              {
                "name": "wrds",
                "description": "This skill should be used when the user asks to \"query WRDS\", \"access Compustat\", \"get CRSP data\", \"pull Form 4 insider data\", \"query ISS compensation\", \"download SEC EDGAR filings\", \"get ExecuComp data\", \"access Capital IQ\", or needs WRDS PostgreSQL query patterns.",
                "path": "skills/wrds/SKILL.md",
                "frontmatter": {
                  "name": "wrds",
                  "version": 1,
                  "description": "This skill should be used when the user asks to \"query WRDS\", \"access Compustat\", \"get CRSP data\", \"pull Form 4 insider data\", \"query ISS compensation\", \"download SEC EDGAR filings\", \"get ExecuComp data\", \"access Capital IQ\", or needs WRDS PostgreSQL query patterns."
                },
                "content": "## Contents\n\n- [Query Enforcement](#query-enforcement)\n- [Quick Reference: Table Names](#quick-reference-table-names)\n- [Connection](#connection)\n- [Critical Filters](#critical-filters)\n- [Parameterized Queries](#parameterized-queries)\n- [Additional Resources](#additional-resources)\n\n# WRDS Data Access\n\nWRDS (Wharton Research Data Services) provides academic research data via PostgreSQL at `wrds-pgdata.wharton.upenn.edu:9737`.\n\n## Query Enforcement\n\n### IRON LAW: NO QUERY WITHOUT FILTER VALIDATION FIRST\n\nBefore executing ANY WRDS query, you MUST:\n1. **IDENTIFY** what filters are required for this dataset\n2. **VALIDATE** the query includes those filters\n3. **VERIFY** parameterized queries (never string formatting)\n4. **EXECUTE** the query\n5. **INSPECT** a sample of results before claiming success\n\nThis is not negotiable. Claiming query success without sample inspection is LYING to the user about data quality.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"I'll add filters later\" | You'll forget and pull bad data | Add filters NOW, before execution |\n| \"User didn't specify filters\" | Standard filters are ALWAYS required | Apply Critical Filters section defaults |\n| \"Just a quick test query\" | Test queries with bad filters teach bad patterns | Use production filters even for tests |\n| \"I'll let the user filter in pandas\" | Pulling millions of unnecessary rows wastes time/memory | Filter at database level FIRST |\n| \"The query worked, so it's correct\" | Query success ≠ data quality | INSPECT sample for invalid records |\n| \"I can use f-strings for simple queries\" | SQL injection risk + wrong type handling | ALWAYS use parameterized queries |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me run this query quickly to see what's there\" → NO. Check Critical Filters section first.\n- \"I'll just pull everything and filter later\" → NO. Database-level filtering is mandatory.\n- \"The table name is obvious from the request\" → NO. Check Quick Reference section for exact names.\n- \"I can inspect the data after the user sees it\" → NO. Sample inspection BEFORE claiming success.\n\n### Query Validation Checklist\n\nBefore EVERY query execution:\n\n**For Compustat queries (comp.funda, comp.fundq):**\n- [ ] Includes `indfmt = 'INDL'`\n- [ ] Includes `datafmt = 'STD'`\n- [ ] Includes `popsrc = 'D'`\n- [ ] Includes `consol = 'C'`\n- [ ] Uses parameterized queries for variables\n- [ ] Date range is explicitly specified\n\n**For CRSP v2 queries (crsp.dsf_v2, crsp.msf_v2):**\n- [ ] Post-query filter: `sharetype == 'NS'`\n- [ ] Post-query filter: `securitytype == 'EQTY'`\n- [ ] Post-query filter: `securitysubtype == 'COM'`\n- [ ] Post-query filter: `usincflg == 'Y'`\n- [ ] Post-query filter: `issuertype.isin(['ACOR', 'CORP'])`\n- [ ] Uses parameterized queries\n\n**For Form 4 queries (tr_insiders.table1):**\n- [ ] Transaction type filter specified (acqdisp)\n- [ ] Transaction codes specified (trancode)\n- [ ] Date range is explicitly specified\n- [ ] Uses parameterized queries\n\n**For ALL queries:**\n- [ ] Sample inspection with `.head()` or `.sample()` BEFORE claiming success\n- [ ] Row count verification (is result size reasonable?)\n- [ ] NULL value check on critical columns\n- [ ] Date range validation (does min/max match expectations?)\n\n## Quick Reference: Table Names\n\n| Dataset | Schema | Key Tables |\n|---------|--------|------------|\n| Compustat | `comp` | `company`, `funda`, `fundq`, `secd` |\n| ExecuComp | `comp_execucomp` | `anncomp` |\n| CRSP | `crsp` | `dsf`, `msf`, `stocknames`, `ccmxpf_linkhist` |\n| CRSP v2 | `crsp` | `dsf_v2`, `msf_v2`, `stocknames_v2` |\n| Form 4 Insiders | `tr_insiders` | `table1`, `header`, `company` |\n| ISS Incentive Lab | `iss_incentive_lab` | `comppeer`, `sumcomp`, `participantfy` |\n| Capital IQ | `ciq` | `wrds_compensation` |\n| IBES | `tr_ibes` | `det_epsus`, `statsum_epsus` |\n| SEC EDGAR | `wrdssec` | `wrds_forms`, `wciklink_cusip` |\n| SEC Search | `wrds_sec_search` | `filing_view`, `registrant` |\n| EDGAR | `edgar` | `filings`, `filing_docs` |\n| Fama-French | `ff` | `factors_monthly`, `factors_daily` |\n| LSEG/Datastream | `tr_ds` | `ds2constmth`, `ds2indexlist` |\n\n## Connection\n\nInitialize PostgreSQL connection to WRDS:\n\n```python\nimport psycopg2\n\nconn = psycopg2.connect(\n    host='wrds-pgdata.wharton.upenn.edu',\n    port=9737,\n    database='wrds',\n    sslmode='require'\n    # Credentials from ~/.pgpass\n)\n```\n\nConfigure authentication via `~/.pgpass` with `chmod 600`:\n```\nwrds-pgdata.wharton.upenn.edu:9737:wrds:USERNAME:PASSWORD\n```\n\nConnect via SSH tunnel:\n```bash\nssh wrds\n```\n\nThis uses `~/.ssh/wrds_rsa` for authentication.\n\n## Critical Filters\n\n### Compustat Standard Filters\nAlways include for clean fundamental data:\n```sql\nWHERE indfmt = 'INDL'\n  AND datafmt = 'STD'\n  AND popsrc = 'D'\n  AND consol = 'C'\n```\n\n### CRSP v2 Common Stock Filter\nEquivalent to legacy `shrcd IN (10, 11)`:\n```python\ndf = df.loc[\n    (df.sharetype == 'NS') &\n    (df.securitytype == 'EQTY') &\n    (df.securitysubtype == 'COM') &\n    (df.usincflg == 'Y') &\n    (df.issuertype.isin(['ACOR', 'CORP']))\n]\n```\n\n### Form 4 Transaction Types\n```sql\nWHERE acqdisp = 'D'  -- Dispositions\n  AND trancode IN ('S', 'D', 'G', 'F')  -- Sales, Dispositions, Gifts, Tax\n```\n\n## Parameterized Queries\n\nAlways use parameterized queries (never string formatting):\n\nUse scalar parameter binding for single values:\n```python\ncursor.execute(\"\"\"\n    SELECT gvkey, conm FROM comp.company WHERE gvkey = %s\n\"\"\", (gvkey,))\n```\n\nUse ANY() for list parameters:\n```python\ncursor.execute(\"\"\"\n    SELECT * FROM comp.funda WHERE gvkey = ANY(%s)\n\"\"\", (gvkey_list,))\n```\n\n## Additional Resources\n\n### Reference Files\n\nDetailed query patterns and table documentation:\n\n- **`references/compustat.md`** - Compustat tables, ExecuComp, financial variables\n- **`references/crsp.md`** - CRSP stock data, CCM linking, v2 format\n- **`references/insider-form4.md`** - Thomson Reuters Form 4, rolecodes, insider types\n- **`references/iss-compensation.md`** - ISS Incentive Lab, peer companies, compensation\n- **`references/edgar.md`** - SEC EDGAR filings, URL construction, DCN vs accession numbers\n- **`references/connection.md`** - Connection pooling, caching, error handling\n\n### Example Files\n\nWorking code from real projects:\n\n- **`examples/form4_disposals.py`** - Insider trading analysis (from SVB project)\n- **`examples/wrds_connector.py`** - Connection pooling pattern\n\n### Scripts\n\n- **`scripts/test_connection.py`** - Validate WRDS connectivity\n\n### Local Sample Notebooks\n\nWRDS-provided samples at `~/resources/wrds-code-samples/`:\n- `ResearchApps/CCM2025.ipynb` - Modern CRSP-Compustat merge\n- `ResearchApps/ff3_crspCIZ.ipynb` - Fama-French factor construction\n- `comp/sas/execcomp_ceo_screen.sas` - ExecuComp patterns\n\n## Date Awareness\n\nWhen querying historical data, leverage current date context for dynamic range calculations.\n\nCurrent date is automatically available via `datetime.now()`. Apply this to:\n- Data range validation (e.g., \"get data for last 5 years\")\n- Fiscal year calculations\n- Event study windows\n\nImplement dynamic date ranges in queries:\n```python\nfrom datetime import datetime, timedelta\n\n# Query last 5 years of data\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=5*365)\n\nquery = \"\"\"\nSELECT * FROM comp.funda\nWHERE datadate BETWEEN %s AND %s\n\"\"\"\ndf = pd.read_sql(query, conn, params=(start_date, end_date))\n```\n\nAlways incorporate current date awareness in date-dependent queries to ensure results remain fresh across time."
              },
              {
                "name": "writing-brainstorm",
                "description": "This skill should be used when the user asks to \"find something to write about\", \"brainstorm topics\", \"what should I write about\", \"find writing ideas\", \"gather sources for\", \"pull references on\", or needs help discovering topics from their reading highlights. Leverages Readwise MCP to surface patterns and gather references.",
                "path": "skills/writing-brainstorm/SKILL.md",
                "frontmatter": {
                  "name": "writing-brainstorm",
                  "description": "This skill should be used when the user asks to \"find something to write about\", \"brainstorm topics\", \"what should I write about\", \"find writing ideas\", \"gather sources for\", \"pull references on\", or needs help discovering topics from their reading highlights. Leverages Readwise MCP to surface patterns and gather references."
                },
                "content": "# Writing Brainstorm\n\nGenerate writing topics and gather references from Readwise highlights.\n\n## When to Use\n\nInvoke this skill for:\n- Discovering what to write about from reading patterns\n- Gathering sources and references for a known topic\n- Finding thematic connections across highlights\n- Building an outline with supporting quotes\n\n## Prerequisites\n\nThis skill requires the Readwise MCP server. The plugin auto-configures it, but the `READWISE_TOKEN` environment variable must be set.\n\n**Setup (if MCP not working):**\n1. Get API token from https://readwise.io/access_token\n2. Set environment variable: `export READWISE_TOKEN=your_token`\n3. Verify: `claude mcp list` should show `readwise`\n\n## Critical: Sub-Agent Pattern for Readwise Searches\n\n**NEVER call `search_readwise_highlights` directly from the main chat.** Raw search results return 50-100+ highlights, polluting context and degrading conversation quality.\n\n**ALWAYS use parallel sub-agents** (one per search theme) to:\n1. Execute the search\n2. Filter and deduplicate results\n3. Return a condensed summary\n\n### Sub-Agent Pattern\n\nFor a topic with N distinct themes, launch N parallel sub-agents using the Task tool:\n\n```\nTask(\n  subagent_type=\"general-purpose\",\n  model=\"haiku\",  # Fast and cheap for filtering\n  prompt=\"\"\"Search Readwise for highlights about **[THEME]**.\n\nUse `mcp__readwise__search_readwise_highlights` with:\n- vector_search_term: \"[semantic search terms]\"\n- full_text_queries: [{\"field_name\": \"highlight_plaintext\", \"search_term\": \"[keyword]\"}]\n\nReturn ONLY:\n- Top 3 most relevant sources (title, author)\n- Top 3 quotes worth citing (with source attribution)\n- 1-2 sentence theme summary\"\"\"\n)\n```\n\n### Example: Law Review on Private Equity Access\n\nLaunch 5 parallel agents:\n1. \"private equity retail investors democratization\"\n2. \"accredited investor definition regulation\"\n3. \"401k retirement private markets\"\n4. \"interval fund tender offer evergreen\"\n5. \"investor protection paternalism securities\"\n\nEach returns ~100 words instead of ~5000 words of raw highlights.\n\n---\n\n## Two Modes\n\n### Discovery Mode\n\nWhen user wants to find topics (\"what should I write about?\"):\n\n1. **Fetch tag landscape**\n   - Use `get_tags` to see all topic clusters\n   - Present tags grouped by frequency/recency\n\n2. **Analyze recent reading**\n   - Use `get_recent_content` to fetch recent highlights\n   - Identify recurring themes, authors, or concepts\n\n3. **Semantic pattern detection**\n   - Examine highlights for cross-cutting themes\n   - Look for: tensions, debates, unanswered questions, surprising connections\n\n4. **Present topic candidates**\n   - For each potential topic, show:\n     - Theme description\n     - Supporting highlights (2-3 examples)\n     - Relevant tags\n     - Potential angle or thesis\n\n### Gathering Mode (Progressive Workflow)\n\nWhen user has a topic (\"gather sources on X\"), follow this **human-in-the-loop** workflow:\n\n#### Phase 1: Clarify Intent\n\n**BEFORE any search**, use `AskUserQuestion` to understand:\n\n```\nAskUserQuestion(questions=[\n  {\n    \"question\": \"What's your primary angle or thesis for this piece?\",\n    \"header\": \"Angle\",\n    \"options\": [\n      {\"label\": \"Critique existing framework\", \"description\": \"Argue current approach is flawed\"},\n      {\"label\": \"Propose reform\", \"description\": \"Offer specific policy changes\"},\n      {\"label\": \"Comparative analysis\", \"description\": \"Compare approaches across jurisdictions\"},\n      {\"label\": \"Empirical analysis\", \"description\": \"Present data-driven findings\"}\n    ],\n    \"multiSelect\": false\n  },\n  {\n    \"question\": \"Who is your target audience?\",\n    \"header\": \"Audience\",\n    \"options\": [\n      {\"label\": \"Law review\", \"description\": \"Academic legal audience\"},\n      {\"label\": \"Practitioners\", \"description\": \"Lawyers, regulators, compliance\"},\n      {\"label\": \"Policy makers\", \"description\": \"Legislators, agency staff\"},\n      {\"label\": \"General educated\", \"description\": \"Informed non-specialists\"}\n    ],\n    \"multiSelect\": false\n  }\n])\n```\n\n#### Phase 2: Search Sources\n\n1. **Decompose into themes** based on clarified intent\n   - Break the topic into 3-6 distinct search themes\n   - Each theme becomes a parallel sub-agent search\n\n2. **Launch parallel sub-agents**\n   - Use the Task tool with `model=\"haiku\"` for each theme\n   - Run all searches in a single message (parallel execution)\n   - See \"Sub-Agent Pattern\" section above\n\n3. **Synthesize results**\n   - Deduplicate sources across agent responses\n   - Identify the strongest quotes from each theme\n   - Note gaps (themes with few/no highlights)\n\n#### Phase 3: Draft Outline → `OUTLINE.md`\n\nSave the outline to a file for iteration:\n\n```markdown\n# OUTLINE.md\n\n## Working Title\n[Title]\n\n## Thesis\n[One-sentence claim]\n\n## Target Audience\n[From Phase 1]\n\n## Structure\n### I. Introduction\n### II. [Section]\n### III. [Section]\n...\n\n## Key Sources\n[Deduplicated from Phase 2]\n\n## Open Questions\n[Gaps to address]\n```\n\n**Ask for feedback** on the outline before proceeding.\n\n#### Phase 4: Section Deep-Dive\n\nFor each major section, use `AskUserQuestion` to refine:\n\n```\nAskUserQuestion(questions=[\n  {\n    \"question\": \"For Section II (Background), what level of detail do you need?\",\n    \"header\": \"Depth\",\n    \"options\": [\n      {\"label\": \"Brief context\", \"description\": \"1-2 paragraphs, assume reader familiarity\"},\n      {\"label\": \"Full background\", \"description\": \"Comprehensive treatment for general reader\"},\n      {\"label\": \"Synthesis only\", \"description\": \"Synthesize precedents without detailed summaries\"}\n    ],\n    \"multiSelect\": false\n  }\n])\n```\n\nCreate `SECTION-II-OUTLINE.md` with:\n- Section thesis/purpose\n- Key arguments in order\n- Supporting sources mapped to arguments\n- Anticipated counterarguments\n\nRepeat for each section, getting human feedback before moving to prose.\n\n## Output Format\n\nProduce a markdown outline:\n\n```markdown\n# [Topic Title]\n\n## Thesis/Angle\n[One-sentence framing]\n\n## Key Sources\n- **[Source 1]** by [Author]\n  - \"[Highlight quote]\"\n  - Relevant to: [subtopic]\n\n## Outline\n### [Subtopic 1]\n- Point A (Source 1, Source 3)\n- Point B (Source 2)\n\n### [Subtopic 2]\n...\n\n## Open Questions\n- [Questions highlights don't answer]\n\n## Next Steps\n- Suggested writing skill: /writing-[domain]\n```\n\n## Domain Detection\n\nAfter gathering sources, detect the topic domain and suggest the appropriate writing skill:\n\n| Domain Indicators | Suggested Skill |\n|-------------------|-----------------|\n| Legal cases, statutes, law reviews, constitutional | `/writing-legal` (Volokh) |\n| Economics, markets, policy, data, empirical | `/writing-econ` (McCloskey) |\n| General/other | `/writing` (Strunk & White) |\n\n## Readwise MCP Tools\n\nPrimary tools for brainstorming:\n\n| Tool | Use Case | Direct Call OK? |\n|------|----------|-----------------|\n| `get_tags` | Survey topic landscape | ✅ Yes |\n| `get_recent_content` | See current reading themes | ✅ Yes |\n| `search_readwise_highlights` | Find highlights by keyword | ❌ **Sub-agent only** |\n| `get_highlights` | Retrieve with filters | ⚠️ Use caution (can be large) |\n| `get_books` | Browse source library | ✅ Yes |\n\n**Why sub-agents for search?** A single search can return 50-100 highlights (~5000+ tokens). Multiple searches compound this. Sub-agents filter to essentials before returning to main context.\n\n## File Output Convention\n\nSave brainstorming artifacts to the project's `docs/` or `scratch/` directory:\n\n```\nproject/\n├── docs/\n│   └── writing/\n│       ├── OUTLINE.md              # Main article outline\n│       ├── SECTION-I-OUTLINE.md    # Introduction details\n│       ├── SECTION-II-OUTLINE.md   # Background details\n│       └── ...\n└── scratch/\n    └── brainstorm-notes.md         # Working notes (gitignored)\n```\n\n## Workflow Examples\n\n### Discovery Mode Example\n\n**User:** \"I want to write something but don't know what\"\n\n**Process:**\n1. Fetch tags → find clusters like \"antitrust\", \"market-power\", \"regulation\"\n2. Get recent highlights → notice many from economics sources\n3. Analyze → tension between \"consumer welfare\" and \"market structure\" keeps appearing\n4. Present → \"Potential topic: The consumer welfare standard debate. You have 12 highlights across 4 sources discussing this tension. Angle: Why market structure matters beyond prices.\"\n5. Domain detection → Economics sources detected → \"Use `/writing-econ` for drafting\"\n\n### Gathering Mode Example (Progressive)\n\n**User:** \"Let's brainstorm a law review article about retail access to private equity\"\n\n**Process:**\n1. **Clarify** → AskUserQuestion: angle (critique/reform/comparative), audience (law review/practitioners)\n2. **User responds** → \"Critique existing framework, law review audience\"\n3. **Decompose** → 5 themes: PE retail access, accredited investor, 401(k) access, fund structures, investor protection\n4. **Search** → Launch 5 parallel Haiku sub-agents\n5. **Synthesize** → Dedupe sources, extract best quotes, note gaps\n6. **Save** → Write `docs/writing/OUTLINE.md`\n7. **Feedback** → \"Here's the outline. Any sections to add/remove/reorder?\"\n8. **User responds** → \"Add comparative section on EU ELTIF\"\n9. **Deep-dive** → AskUserQuestion per section, create `SECTION-II-OUTLINE.md`\n10. **Handoff** → \"Outline complete. Use `/writing-legal` to draft.\"\n\n## Integration\n\nAfter brainstorming:\n- `/writing` - General prose drafting\n- `/writing-econ` - Economics/finance articles\n- `/writing-legal` - Law review articles\n- `/ai-anti-patterns` - Check for AI writing indicators"
              },
              {
                "name": "writing-econ",
                "description": "This skill should be used when the user asks to \"write an economics paper\", \"draft a working paper\", \"edit finance writing\", \"review my econ paper\", \"write for a journal\", or needs guidance on economics and finance writing. Based on McCloskey's \"Economical Writing\" with discipline-specific word lists and examples.",
                "path": "skills/writing-econ/SKILL.md",
                "frontmatter": {
                  "name": "writing-econ",
                  "description": "This skill should be used when the user asks to \"write an economics paper\", \"draft a working paper\", \"edit finance writing\", \"review my econ paper\", \"write for a journal\", or needs guidance on economics and finance writing. Based on McCloskey's \"Economical Writing\" with discipline-specific word lists and examples."
                },
                "content": "# Economics and Finance Writing\n\nStyle guide for economics journal articles, working papers, and finance analysis based on Deirdre McCloskey's *Economical Writing*.\n\n## When to Use\n\nInvoke this skill for:\n- Economics journal articles and working papers\n- Finance analysis and market commentary\n- Policy briefs and economic reports\n- Editing economics/finance prose for clarity\n\n**For general writing**: Use `/writing` skill (Strunk & White)\n**For legal writing**: Use `/writing-legal` skill (Volokh)\n\n## Enforcement\n\n### IRON LAW #1: NO BOILERPLATE WITHOUT DELETE & RESTART\n\nIf you write ANY of these, DELETE the draft and START OVER:\n- \"This paper discusses...\"\n- Table-of-contents paragraph\n- \"As we shall see\"\n- \"It is interesting to note that...\"\n- \"The rest of this paper is organized as follows...\"\n\nThese signal you haven't found your hook. Start fresh with a compelling finding.\n\n### IRON LAW #2: NO ELEGANT VARIATION\n\nOne concept = One word. If you catch yourself varying terms (\"industrialization\" / \"development\" / \"growth\") for the same concept, you are confusing the reader. Pick ONE term and use it consistently.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"But journals use boilerplate\" | Bad journals do | HOOK reader with finding |\n| \"Elegant variation shows vocabulary\" | Shows you don't know what you mean | USE same word for same thing |\n| \"Readers need roadmap paragraph\" | They skip it | DELETE table-of-contents para |\n| \"This terminology is standard in field\" | Doesn't make it good | USE concrete Anglo-Saxon words |\n| \"Need to sound academic\" | Sounds pompous instead | WRITE like human being |\n| \"Passive voice sounds objective\" | Sounds evasive | USE active voice |\n| \"Technical writing must be formal\" | Technical ≠ turgid | BE clear AND technical |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me write a standard introduction\" → NO. Find your hook first.\n- \"I'll improve this later\" → NO. Fix boilerplate NOW or restart.\n- \"This varies the language nicely\" → NO. Consistency > variation.\n- \"Readers expect this phrase\" → NO. Expectations can be wrong.\n\n### Delete & Restart Pattern\n\n**When to delete and restart:**\n\n1. **Boilerplate detected in first paragraph** → Delete entire intro, write finding-first\n2. **Three different terms for same concept** → Delete section, pick ONE term\n3. **Table-of-contents paragraph exists** → Delete it, no replacement needed\n4. **Metric conversions every time** → Delete all but first, trust reader\n\n**How to restart:**\n\n```\nOld: \"This paper discusses the relationship between X and Y...\"\nNew: \"Trade liberalization increased wages by 15% for skilled workers.\"\n```\n\nRestart with THE FINDING, not with throat-clearing.\n\n## Core Principles\n\n### Speak to One Reader\n\nChoose an implied reader and stick with her. A skeptical but sympathetic colleague. Keep the prose at one level of difficulty. If it embarrasses you to imagine how she would read it, the stuff is embarrassing.\n\n### Avoid Boilerplate\n\n| Anti-Pattern | Why It Fails |\n|--------------|--------------|\n| \"This paper discusses...\" | Bores the reader; use a hook instead |\n| Table-of-contents paragraph | Readers skip it; they can't understand until they've read the paper |\n| Background/padding | If you discovered it was beside the point, don't include it |\n| \"As we shall see\" | Useless anticipation; the reader will see soon enough |\n| Metric conversions every time | Shows you think the reader is an ignoramus |\n\nNever repeat without apologizing (\"as I said earlier\"). If apologizing too much, you're repeating too much.\n\n### Control Tone\n\n- Avoid invective: \"This is pure nonsense\" arouses suspicion the argument is weak\n- Delete every \"very\" and \"absolutely\" - most things aren't\n- Use wit to compensate for strong opinions\n- Relax the pose of The Scientist; write like a human being\n\n### One Point Per Paragraph\n\nEnd each paragraph with a simple, street-talk encapsulation. The paragraph can be technical as long as the last sentence comes down a notch. It makes the paragraph sing.\n\n### Make Tables Self-Explanatory\n\nThe reader should understand the table without the main text. Use words in headings, not acronyms. \"Logarithm of Domestic Price\" not \"LPDOM\". Follow Tufte: no chart junk, have a point.\n\nUse meaningful labels in equations: \"Quantity of Grain = 3.56 + 5.6(Price of Grain)\" not \"Q = 3.56 + 5.6P where Q is...\"\n\n### Make Writing Cohere\n\nRepeat key words to link sentences. (AB)(BC)(CD) is easy to understand. The figure is called polyptoton. English achieves coherence by repetition, not by \"not only...but also\" which marks you as incompetent.\n\n## Word Choice\n\n### Avoid Elegant Variation\n\nUse one word to mean one thing. A paper used: \"industrialization,\" \"growing structural differentiation,\" \"economic and social development,\" \"development,\" \"economic growth,\" \"growth,\" and \"revolutionized means of production\" to mean the same thing. Don't.\n\nWhen uncertain, look back and use the same word.\n\n### Key Principles\n\n| Principle | Example |\n|-----------|---------|\n| Be concrete | \"sheep and wheat\" not \"natural resource-oriented exports\" |\n| Untie Teutonisms | \"equalization of the prices of factors\" not \"factor price equalization\" |\n| Avoid ersatz economics | Never use \"skyrocketing,\" \"fair prices,\" \"vicious cycle,\" \"exploit\" |\n| Avoid this-ism | Replace *this*, *these*, *those* with *the* |\n\nSee `references/economical-writing-full.md` for extended bad words list, Teutonism examples, and ersatz economics vocabulary.\n\n## Quick Reference\n\n| Problem | Solution |\n|---------|----------|\n| \"This paper discusses X\" | Hook the reader with the finding |\n| Table-of-contents paragraph | Delete it; readers skip it anyway |\n| \"As we shall see\" | Delete; anticipation is useless |\n| Elegant variation | Use the same word for the same thing |\n| Five-dollar words | Anglo-Saxon roots are more concrete |\n| Noun pile-ups | Untie with \"of\" |\n| This/that/these/those | Replace with \"the\" |\n| \"Not only...but also\" | Just use \"and\" |\n\n## Progressive Disclosure\n\nFor comprehensive guidance, consult:\n\n### Reference File\n\n- **`references/economical-writing-full.md`** - Complete McCloskey guide covering:\n  - 35 rules with full explanations and examples\n  - Extended bad words list with usage notes\n  - Historical and etymological context\n\n### When to Load Reference\n\nLoad the full reference when:\n- Encountering specific vocabulary questions\n- Needing detailed examples for economics jargon\n- Working on substantial manuscript revision\n- Teaching economics writing\n\n## Integration\n\nAfter completing any economics writing task, invoke `/ai-anti-patterns` to check for AI writing indicators. The `/writing` skill covers general prose principles (active voice, omit needless words) that complement this skill."
              },
              {
                "name": "writing-legal",
                "description": "This skill should be used when the user asks to \"write a law review article\", \"draft a legal paper\", \"edit legal writing\", \"review my legal article\", \"write for a journal\", \"format footnotes\", or needs guidance on academic legal writing. Based on Volokh's \"Academic Legal Writing\" with law-review-specific structure and evidence handling.",
                "path": "skills/writing-legal/SKILL.md",
                "frontmatter": {
                  "name": "writing-legal",
                  "description": "This skill should be used when the user asks to \"write a law review article\", \"draft a legal paper\", \"edit legal writing\", \"review my legal article\", \"write for a journal\", \"format footnotes\", or needs guidance on academic legal writing. Based on Volokh's \"Academic Legal Writing\" with law-review-specific structure and evidence handling."
                },
                "content": "# Academic Legal Writing\n\nStyle guide for law review articles, seminar papers, and legal scholarship based on Eugene Volokh's *Academic Legal Writing*.\n\n## When to Use\n\nInvoke this skill for:\n- Law review articles and student notes\n- Seminar papers and legal scholarship\n- Academic legal writing with footnotes\n- Editing legal prose for structure and argument\n\n**For general writing**: Use `/writing` skill (Strunk & White)\n**For economics/finance**: Use `/writing-econ` skill (McCloskey)\n\n## Enforcement\n\n### IRON LAW #1: NO CLAIM WITHOUT CONFRONTING COUNTERARGUMENTS\n\nIf your draft makes a prescriptive claim but doesn't address obvious objections, DELETE the section and START OVER. Legal scholarship requires anticipating and answering counterarguments, not ignoring them.\n\n### IRON LAW #2: NO SECONDARY SOURCE CITATIONS FOR PRIMARY SOURCES\n\nIf you cite a case/statute/historical fact via an intermediate source (law review, treatise), DELETE the citation and READ THE ORIGINAL. Even Supreme Court opinions misstate precedents.\n\n### Rationalization Table - STOP If You Think:\n\n| Excuse | Reality | Do Instead |\n|--------|---------|------------|\n| \"This article discusses...\" | Bores reader instantly | START with concrete problem or controversy |\n| \"Table-of-contents paragraph helps\" | Readers skip it | INTEGRATE roadmap into intro |\n| \"Background section comes first\" | Not before establishing relevance | SHOW problem first, background second |\n| \"Case-by-case summary is thorough\" | Tedious and unhelpful | SYNTHESIZE: \"Courts hold X except Y\" |\n| \"Counterargument would hurt my claim\" | Ignoring it hurts worse | CONFRONT and refine claim |\n| \"Treatise summary is good enough\" | Treatises have errors | READ original cases |\n| \"Arguably\" makes my point | Acknowledges controversy without arguing | MAKE the argument explicitly |\n| \"This metaphor is clear\" | Metaphors hide incomplete logic | UNPACK: what's the actual argument? |\n\n### Red Flags - STOP Immediately If You Think:\n\n- \"Let me write standard intro\" → NO. Find concrete problem first.\n- \"I'll address objections later\" → NO. Confront counterarguments NOW.\n- \"This treatise explains the case\" → NO. Read the original case.\n- \"Background section needs more\" → NO. Only include what proves claim.\n\n### Delete & Restart Pattern\n\n**When to delete and restart:**\n\n1. **Intro starts with \"This article discusses\"** → Delete, start with concrete problem\n2. **Background exceeds proof section** → Delete excessive background\n3. **Claim made without addressing objections** → Delete section, add counterargument confrontation\n4. **Citation chain to primary source** → Delete citation, read and cite original\n5. **Unpacked metaphor used as argument** → Delete, write actual logical argument\n\n**How to restart:**\n\n```\nOld: \"This article discusses privacy concerns in Fourth Amendment doctrine...\"\nNew: \"When police drones photograph backyards, does the Fourth Amendment require a warrant?\n      Courts disagree, but three features of aerial surveillance suggest yes.\"\n```\n\nStart with CONCRETE QUESTION that matters, not abstract topic description.\n\n## Law Review Article Structure\n\n### Introduction\n\nThe introduction serves three functions:\n1. Persuade readers to keep reading\n2. Summarize the article for those who won't read it\n3. Frame how readers interpret what follows\n\n**Requirements:**\n- Show the problem concretely with specific examples or hypotheticals\n- State the claim clearly—what does the article contribute?\n- Integrate the roadmap into the introduction, not as a separate paragraph\n- Hook the reader: concrete question, engaging story, controversy, or argument to rebut\n\n**Anti-patterns:**\n- Starting with \"This article discusses...\"\n- Separate table-of-contents paragraph (readers skip it)\n- Historical background before establishing relevance\n- Vague generalities about the importance of the topic\n\n### Background Section\n\nSynthesize precedents; do not summarize each case sequentially. Focus only on facts and rules necessary for the argument.\n\n| Problem | Solution |\n|---------|----------|\n| Summarizing each case | Synthesize: \"Courts generally hold X, except when Y\" |\n| Mini-treatise on the area | Only what's needed for the claim |\n| 80% background, 20% claim | Balance must favor the original contribution |\n\n### Proof of the Claim\n\nFor prescriptive claims: Show the proposal is both doctrinally sound AND good policy.\n\n**Use a test suite:** Apply the proposal to concrete scenarios (easy cases, hard cases, edge cases) to demonstrate it works.\n\n**Confront counterarguments:**\n- Turn problems to advantage: refine the claim, acknowledge uncertainty\n- Stay on offense—address objections without becoming defensive\n- Acknowledge costs honestly; readers respect candor\n\n**Connect to broader issues:**\n- How does the claim relate to parallel debates?\n- What subsidiary discoveries emerged?\n- What questions remain for future research?\n\n### Conclusion\n\nKeep conclusions brief. The real work is rewriting the introduction after the draft is complete, ensuring it accurately reflects the article's contributions.\n\n## Legal Argument Problems\n\nCommon logical problems in legal writing (see `references/volokh-distilled.md` for detailed examples):\n\n| Problem | Issue |\n|---------|-------|\n| Categorical assertions | \"Always\" and \"never\" invite counterexamples |\n| Unpacked metaphors | \"Slippery slope\" and \"chilling effect\" hide incomplete arguments |\n| Missing logical pieces | Syllogisms that skip steps (subject to scrutiny ≠ fails scrutiny) |\n| Universal criticisms | \"Chilling effect\" applies to most laws—explain why *this* one matters |\n| Undefined abstractions | \"Privacy,\" \"paternalism,\" \"democratic legitimacy\" need definitions |\n| \"Arguably\" as argument | Acknowledges controversy but doesn't make the case |\n\n## Evidence and Citation\n\n### Read Original Sources\n\nNever rely on intermediate sources for cases, statutes, or historical facts. Even Supreme Court opinions misstate precedents.\n\n| Source Type | Rule |\n|-------------|------|\n| Cases/statutes | Read the original; don't trust treatises or other cases |\n| Historical facts | Go to history books, not law review articles citing them |\n| Scientific studies | Read the study, not the article summarizing it |\n| Newspapers | Unreliable; track down underlying documents |\n| Wikipedia | Use to find sources, but cite originals |\n\n### Be Precise with Terms\n\nAvoid false synonyms: \"murder\" ≠ \"homicide\" ≠ \"killing\"; \"foreign-born\" ≠ \"noncitizen\"; \"children\" is ambiguous (0-14? 0-17? 0-24?).\n\nInclude necessary qualifiers: \"*falsely* shouting fire\" is quite different from \"shouting fire.\"\n\n### Be Explicit About Assumptions\n\nMake clear when inferring:\n- From correlation to causation\n- From one time/place to another\n- From one variable to another (arrest rate ≠ crime rate)\n\nAcknowledge the inference and defend it; don't hide it.\n\n### Handle Surveys Carefully\n\nSurveys measure only what respondents said in response to specific questions. Valid surveys require:\n- Random sampling (not self-selected, not convenience samples)\n- High response rates (70%+)\n- Sufficient sample size (1000+ for ±3% margin)\n- Unambiguous questions\n\n\"Online survey\" and \"Internet poll\" are almost sure signs of invalidity.\n\n## Rhetoric and Tone\n\n| Principle | Application |\n|-----------|-------------|\n| Understate criticism | \"Mistaken\" not \"idiotic\"—overstating raises the burden of proof |\n| Attack arguments, not people | \"This argument fails\" not \"Volokh is wrong\" |\n| Avoid caricature | Quote adherents, not critics, when explaining a position |\n\nSee `references/volokh-distilled.md` for extended discussion of rhetorical problems.\n\n## Quick Reference\n\n| Problem | Solution |\n|---------|----------|\n| \"This article discusses X\" | Hook with concrete problem |\n| Case-by-case summaries | Synthesize precedents |\n| Undefended metaphors | Unpack the concrete mechanism |\n| \"Arguably\" / \"raises concerns\" | Give the actual argument |\n| Relying on intermediate source | Read original case/study |\n| \"Many children\" | Specify: \"111 children age 0-17\" |\n| \"Correlation shows causation\" | Explain why inference is valid |\n| \"Volokh's argument is idiotic\" | \"This argument seems unsound\" |\n\n## Progressive Disclosure\n\nFor comprehensive guidance, consult:\n\n### Reference File\n\n- **`references/volokh-distilled.md`** - Extended Volokh guidance covering:\n  - Full logical problems taxonomy\n  - Word and phrase problems to avoid\n  - Extended evidence handling\n  - Survey analysis methodology\n  - Editing principles and exercises\n\n### When to Load Reference\n\nLoad the full reference when:\n- Encountering specific evidence evaluation questions\n- Needing detailed survey methodology guidance\n- Working on substantial manuscript revision\n- Checking specific word choice or usage questions\n\n## Integration\n\nAfter completing any legal writing task, invoke `/ai-anti-patterns` to check for AI writing indicators. The `/writing` skill covers general prose principles (active voice, omit needless words) that complement this skill."
              },
              {
                "name": "writing",
                "description": "This skill should be used when the user asks to \"write an article\", \"draft a blog post\", \"edit prose\", \"review my writing\", \"check style\", \"improve clarity\", or needs general writing guidance. Provides Strunk & White's Elements of Style for foundational grammar, usage, and composition principles.",
                "path": "skills/writing/SKILL.md",
                "frontmatter": {
                  "name": "writing",
                  "description": "This skill should be used when the user asks to \"write an article\", \"draft a blog post\", \"edit prose\", \"review my writing\", \"check style\", \"improve clarity\", or needs general writing guidance. Provides Strunk & White's Elements of Style for foundational grammar, usage, and composition principles."
                },
                "content": "# Writing and Editing\n\nFoundational style guide for clear, concise prose based on Strunk & White's Elements of Style.\n\n## When to Use\n\nInvoke this skill for:\n- Writing articles, blog posts, or general prose\n- Editing text for clarity, conciseness, or style\n- Reviewing grammar and usage\n- Improving sentence structure and word choice\n\n**For specialized domains:**\n- Legal writing (law review articles): Use `/writing-legal` skill (Volokh)\n- Economics/Finance: Use `/writing-econ` skill (McCloskey)\n\n## Core Principles\n\n### The Iron Law of Good Writing\n\n**Omit needless words.**\n\nEvery word must earn its place. Vigorous writing is concise. A sentence should contain no unnecessary words, a paragraph no unnecessary sentences.\n\n### Critical Rules\n\n| Rule | Explanation |\n|------|-------------|\n| Write in prose | Avoid bullet points and lists unless explicitly requested |\n| Use active voice | \"The committee approved the plan\" not \"The plan was approved\" |\n| Be concrete | Specific details over vague abstractions |\n| Put statements in positive form | Say what something is, not what it isn't |\n| Use definite language | Avoid hedging, qualifiers, and weasel words |\n\n### Red Flags - Stop If You Think\n\n| Thought | Why It's Wrong | Do Instead |\n|---------|----------------|------------|\n| \"I'll add some qualifiers to be safe\" | Weakens the writing | Make definite assertions |\n| \"Let me list these points\" | Bullet points are lazy | Write in prose paragraphs |\n| \"I should sound more formal\" | Formality often means wordiness | Write naturally, then edit |\n| \"This needs more emphasis\" | Overemphasis dilutes meaning | Let strong words speak |\n\n## How to Use This Skill\n\n### Before Writing\n\n1. Identify the main point or thesis\n2. Plan the structure: introduction, development, conclusion\n3. Gather concrete examples to support claims\n\n### During Drafting\n\n1. Write complete sentences in paragraphs\n2. Use active voice and strong verbs\n3. Be specific: \"three hours\" not \"a long time\"\n4. Avoid starting with \"There is\" or \"It is\"\n\n### During Editing\n\nApply these checks in order:\n\n**Sentence Level:**\n- Remove unnecessary words (\"in order to\" → \"to\")\n- Replace weak verbs (\"is able to\" → \"can\")\n- Convert passive to active voice\n- Eliminate redundancies (\"past history\" → \"history\")\n\n**Paragraph Level:**\n- Ensure each paragraph has one main idea\n- Check topic sentences lead clearly\n- Verify logical flow between paragraphs\n\n**Word Level:**\n- Replace abstract nouns with concrete ones\n- Use specific verbs over vague ones + adverbs\n- Cut filler words (\"very\", \"really\", \"quite\", \"rather\")\n\n## Quick Reference: Common Fixes\n\n| Weak | Strong |\n|------|--------|\n| utilize | use |\n| in order to | to |\n| due to the fact that | because |\n| at this point in time | now |\n| in the event that | if |\n| prior to | before |\n| subsequent to | after |\n| with regard to | about |\n| a large number of | many |\n| is able to | can |\n\n## Progressive Disclosure\n\nFor comprehensive guidance, consult:\n\n### Reference Files\n\n- **`references/elements-of-style.md`** - Complete Strunk & White guide covering:\n  - Elementary Rules of Usage (commas, colons, participles)\n  - Elementary Principles of Composition (paragraph unity, active voice)\n  - Words and Expressions Commonly Misused\n  - Style guidance and literary reminders\n\n### When to Load References\n\nLoad the full reference when:\n- Encountering specific grammar questions (comma usage, possessives)\n- Needing detailed guidance on composition principles\n- Checking whether specific words/expressions are commonly misused\n- Working on substantial editing tasks\n\n## Integration with AI Anti-Patterns\n\nAfter completing any writing task, invoke `/ai-anti-patterns` to check for AI writing indicators. This plugin includes PostToolUse hooks that automatically warn on common anti-patterns in Write/Edit output.\n\n## Examples\n\n**Weak original:**\n> It is important to note that there are a variety of different factors that contribute to the overall success of the project in question.\n\n**Strong revision:**\n> Several factors determine project success.\n\n**Weak original:**\n> The report was written by the team and was subsequently reviewed by management prior to being distributed to stakeholders.\n\n**Strong revision:**\n> The team wrote the report, management reviewed it, and stakeholders received it.\n\n## Related Skills\n\n- `/ai-anti-patterns` - Detect and revise AI writing patterns\n- `/writing-legal` - Academic legal writing (Volokh)\n- `/writing-econ` - Economics and finance writing (McCloskey)\n- `/docx` - Word document creation, editing, tracked changes\n- `/pdf` - PDF extraction, creation, form filling\n- `/pptx` - Presentation creation and editing\n- `/xlsx` - Spreadsheet creation and analysis"
              }
            ]
          }
        ]
      }
    }
  ]
}