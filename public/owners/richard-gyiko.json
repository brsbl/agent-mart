{
  "owner": {
    "id": "richard-gyiko",
    "display_name": "Richárd Gyikó",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/36221820?u=1261329c25adf400a7a6fe78e7977fc53e3a65c5&v=4",
    "url": "https://github.com/richard-gyiko",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 1,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "richard-gyiko/data-wrangler-plugin",
      "url": "https://github.com/richard-gyiko/data-wrangler-plugin",
      "description": "Claude Code plugin for SQL analytics over CSV, Parquet, JSON, Excel, and databases using DuckDB",
      "homepage": "",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-02T09:13:32Z",
        "created_at": "2025-12-11T21:03:23Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 646
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 661
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 221
        },
        {
          "path": "AGENTS.md",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1072
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 3422
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/claude_skills_best_practices.md",
          "type": "blob",
          "size": 41516
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-wrangler",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-wrangler/SECRETS.md",
          "type": "blob",
          "size": 3914
        },
        {
          "path": "skills/data-wrangler/SKILL.md",
          "type": "blob",
          "size": 7366
        },
        {
          "path": "skills/data-wrangler/TRANSFORMS.md",
          "type": "blob",
          "size": 7013
        },
        {
          "path": "skills/data-wrangler/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-wrangler/examples/secrets.yaml",
          "type": "blob",
          "size": 5598
        },
        {
          "path": "skills/data-wrangler/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-wrangler/scripts/query_duckdb.py",
          "type": "blob",
          "size": 38547
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/test_explore.py",
          "type": "blob",
          "size": 7206
        },
        {
          "path": "tests/test_query.py",
          "type": "blob",
          "size": 15760
        },
        {
          "path": "tests/test_secrets.py",
          "type": "blob",
          "size": 9238
        },
        {
          "path": "tests/test_write.py",
          "type": "blob",
          "size": 12472
        }
      ],
      "marketplace": {
        "name": "data-wrangler-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Richard Gyiko"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "data-wrangler",
            "description": "Transform and export data using DuckDB SQL. Read CSV/Parquet/JSON/Excel/databases, apply SQL transformations, and write results to files.",
            "source": "./",
            "category": "data-analysis",
            "version": "2.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add richard-gyiko/data-wrangler-plugin",
              "/plugin install data-wrangler@data-wrangler-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-02T09:13:32Z",
              "created_at": "2025-12-11T21:03:23Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "data-wrangler",
                "description": "Transform and export data using DuckDB SQL. Read CSV/Parquet/JSON/Excel/databases, apply SQL transformations (joins, aggregations, PIVOT/UNPIVOT, sampling), and optionally write results to files. Use when the user wants to: (1) Clean, filter, or transform data, (2) Join multiple data sources, (3) Convert between formats (CSV→Parquet, etc.), (4) Create partitioned datasets, (5) Sample large datasets, (6) Export query results. Prefer this over in-context reasoning for datasets with thousands of rows or complex transformations.\n",
                "path": "skills/data-wrangler/SKILL.md",
                "frontmatter": {
                  "name": "data-wrangler",
                  "description": "Transform and export data using DuckDB SQL. Read CSV/Parquet/JSON/Excel/databases, apply SQL transformations (joins, aggregations, PIVOT/UNPIVOT, sampling), and optionally write results to files. Use when the user wants to: (1) Clean, filter, or transform data, (2) Join multiple data sources, (3) Convert between formats (CSV→Parquet, etc.), (4) Create partitioned datasets, (5) Sample large datasets, (6) Export query results. Prefer this over in-context reasoning for datasets with thousands of rows or complex transformations.\n"
                },
                "content": "# Data Wrangler\n\nTransform and export data using DuckDB SQL.\n\n## Contents\n\n- [Usage](#usage) - Command syntax and Windows escaping\n- [Explore Mode](#explore-mode) - Quick data profiling\n- [Query Mode](#query-mode) - Return results to Claude\n- [Write Mode](#write-mode) - Export to files\n- [Request/Response Format](#requestresponse-format) - JSON structure\n- [Source Types](#source-types) - File, database, and cloud sources\n- [Transformations](#transformations) - SQL patterns reference\n- [Secrets](#secrets) - Secure credential handling\n\n## Usage\n\n**IMPORTANT - Windows Shell Escaping:**\n\n1. Always `cd` to the skill directory first\n2. Use **double quotes** for echo with escaped inner quotes (`\\\"`)\n3. Use **forward slashes** in file paths\n\n```bash\ncd \"<skill_directory>\" && echo \"{\\\"query\\\": \\\"SELECT * FROM 'D:/path/to/file.csv'\\\"}\" | uv run scripts/query_duckdb.py\n```\n\n## Explore Mode\n\n**Get schema, statistics, and sample in one call.** Use before writing queries to understand data structure.\n\n```json\n{\"mode\": \"explore\", \"path\": \"D:/data/sales.csv\"}\n```\n\n**Response:**\n```json\n{\n  \"file\": \"D:/data/sales.csv\",\n  \"format\": \"csv\",\n  \"row_count\": 15234,\n  \"columns\": [\n    {\"name\": \"order_id\", \"type\": \"BIGINT\", \"null_count\": 0, \"null_percent\": 0.0},\n    {\"name\": \"customer\", \"type\": \"VARCHAR\", \"null_count\": 45, \"null_percent\": 0.3}\n  ],\n  \"sample\": \"| order_id | customer | ... |\\\\n|----------|----------|-----|\\\\n| 1001     | Alice    | ... |\"\n}\n```\n\n**Options:**\n- `sample_rows`: Number of sample rows (default: 10, max: 100)\n- `sources`: For database tables (same as query mode)\n\n## Query Mode\n\nReturn results directly to Claude for analysis.\n\n### Direct File Queries\n\n```json\n{\"query\": \"SELECT * FROM 'data.csv' LIMIT 10\"}\n```\n\n### Multi-Source Joins\n\n```json\n{\n  \"query\": \"SELECT s.*, p.category FROM sales s JOIN products p ON s.product_id = p.id\",\n  \"sources\": [\n    {\"type\": \"file\", \"alias\": \"sales\", \"path\": \"/data/sales.parquet\"},\n    {\"type\": \"file\", \"alias\": \"products\", \"path\": \"/data/products.csv\"}\n  ]\n}\n```\n\n## Write Mode\n\nExport query results to files. Add an `output` object to write instead of returning data.\n\n### Basic Write\n\n```json\n{\n  \"query\": \"SELECT * FROM 'raw.csv' WHERE status = 'active'\",\n  \"output\": {\n    \"path\": \"D:/output/filtered.parquet\",\n    \"format\": \"parquet\"\n  }\n}\n```\n\n### Write with Options\n\n```json\n{\n  \"query\": \"SELECT *, YEAR(date) as year, MONTH(date) as month FROM 'events.csv'\",\n  \"output\": {\n    \"path\": \"D:/output/events/\",\n    \"format\": \"parquet\",\n    \"options\": {\n      \"compression\": \"zstd\",\n      \"partition_by\": [\"year\", \"month\"],\n      \"overwrite\": true\n    }\n  }\n}\n```\n\n### Output Formats\n\n| Format | Options |\n|--------|---------|\n| `parquet` | `compression` (zstd/snappy/gzip/lz4), `partition_by`, `row_group_size` |\n| `csv` | `header` (default: true), `delimiter`, `compression`, `partition_by` |\n| `json` | `array` (true=JSON array, false=newline-delimited) |\n\n### Write Response\n\nResponse includes verification info - no need for follow-up queries:\n\n```json\n{\n  \"success\": true,\n  \"output_path\": \"D:/output/events/\",\n  \"format\": \"parquet\",\n  \"rows_written\": 15234,\n  \"files_created\": [\"D:/output/events/year=2023/data_0.parquet\", \"...\"],\n  \"total_size_bytes\": 5678901,\n  \"duration_ms\": 1234\n}\n```\n\n### Overwrite Protection\n\nBy default, existing files are **not** overwritten. Set `options.overwrite: true` to allow.\n\n## Request/Response Format\n\n### Request\n\n```json\n{\n  \"query\": \"SQL statement\",\n  \"sources\": [...],\n  \"output\": {\"path\": \"...\", \"format\": \"...\"},\n  \"options\": {\"max_rows\": 200, \"format\": \"markdown\"},\n  \"secrets_file\": \"path/to/secrets.yaml\"\n}\n```\n\n### Query Mode Options\n\n- `max_rows`: Maximum rows to return (default: 200)\n- `max_bytes`: Maximum response size (default: 200000)\n- `format`: `markdown` (default), `json`, `records`, or `csv`\n\n### Query Mode Response (markdown)\n\n```\n| column1 | column2 |\n|---|---|\n| value1 | value2 |\n```\n\n### Query Mode Response (json)\n\n```json\n{\n  \"schema\": [{\"name\": \"col1\", \"type\": \"INTEGER\"}],\n  \"rows\": [[1, \"value\"]],\n  \"truncated\": false,\n  \"warnings\": [],\n  \"error\": null\n}\n```\n\n## Source Types\n\n### File (auto-detects CSV, Parquet, JSON, Excel)\n\n```json\n{\"type\": \"file\", \"alias\": \"data\", \"path\": \"/path/to/file.csv\"}\n```\n\nGlob patterns: `{\"path\": \"/logs/**/*.parquet\"}`\n\nCustom delimiter: `{\"path\": \"/data/file.csv\", \"delimiter\": \"|\"}`\n\n### PostgreSQL\n\n```json\n{\n  \"type\": \"postgres\", \"alias\": \"users\",\n  \"host\": \"host\", \"port\": 5432, \"database\": \"db\",\n  \"user\": \"user\", \"password\": \"pass\",\n  \"schema\": \"public\", \"table\": \"users\"\n}\n```\n\n### MySQL\n\n```json\n{\n  \"type\": \"mysql\", \"alias\": \"orders\",\n  \"host\": \"host\", \"port\": 3306, \"database\": \"db\",\n  \"user\": \"user\", \"password\": \"pass\", \"table\": \"orders\"\n}\n```\n\n### SQLite\n\n```json\n{\"type\": \"sqlite\", \"alias\": \"data\", \"path\": \"/path/to/db.sqlite\", \"table\": \"tablename\"}\n```\n\n### S3\n\n```json\n{\n  \"type\": \"s3\", \"alias\": \"logs\",\n  \"url\": \"s3://bucket/path/*.parquet\",\n  \"aws_region\": \"us-east-1\",\n  \"aws_access_key_id\": \"...\", \"aws_secret_access_key\": \"...\"\n}\n```\n\n## Transformations\n\nSee [TRANSFORMS.md](TRANSFORMS.md) for advanced patterns including:\n\n- **PIVOT/UNPIVOT** - Reshape data between wide and long formats\n- **Sampling** - Random subsets with `USING SAMPLE n ROWS` or `SAMPLE 10%`\n- **Dynamic columns** - `EXCLUDE`, `REPLACE`, `COLUMNS('pattern')`\n- **Window functions** - Running totals, rankings, moving averages\n- **Date/time operations** - Extraction, arithmetic, formatting\n\n### Quick Examples\n\n```sql\n-- PIVOT: Convert rows to columns\nPIVOT sales ON quarter USING SUM(revenue) GROUP BY region\n\n-- UNPIVOT: Convert columns to rows\nUNPIVOT data ON q1, q2, q3, q4 INTO NAME quarter VALUE amount\n\n-- Sampling: Random 10% with reproducible seed\nSELECT * FROM large_table USING SAMPLE 10% REPEATABLE(42)\n\n-- Dynamic columns: Exclude sensitive, transform email\nSELECT * EXCLUDE (ssn) REPLACE (LOWER(email) AS email) FROM users\n```\n\n## Workflow\n\n1. **Inspect schema**: `DESCRIBE SELECT * FROM 'file.csv'`\n2. **Preview data**: `SELECT * FROM 'file.csv' LIMIT 5`\n3. **Transform**: Apply filters, joins, aggregations\n4. **Export** (optional): Add `output` to write results\n\n## Error Handling\n\n- If `error` is non-null: Check column names, verify paths\n- If `truncated` is true: Use more aggregation or filters\n- If write fails with \"exists\": Set `options.overwrite: true`\n\n## Secrets\n\nStore credentials securely in YAML. See [SECRETS.md](SECRETS.md) for complete documentation.\n\n```json\n{\n  \"query\": \"SELECT * FROM customers LIMIT 10\",\n  \"secrets_file\": \"D:/path/to/secrets.yaml\",\n  \"sources\": [{\n    \"type\": \"postgres\", \"alias\": \"customers\",\n    \"secret\": \"my_postgres\", \"table\": \"customers\"\n  }]\n}\n```\n\nSupported: PostgreSQL, MySQL, S3, GCS, Azure, R2, HTTP, HuggingFace, Iceberg, DuckLake."
              }
            ]
          }
        ]
      }
    }
  ]
}