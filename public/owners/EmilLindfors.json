{
  "owner": {
    "id": "EmilLindfors",
    "display_name": "Emil Tomson Lindfors",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/4932954?u=c1d41076a9d9c213abaaedd192d67d63e868f8de&v=4",
    "url": "https://github.com/EmilLindfors",
    "bio": "\r\nPhD in Technological Innovation, Sr Software Engineer at @aquacloudai. Main language is Rust.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 7,
      "total_commands": 33,
      "total_skills": 19,
      "total_stars": 1,
      "total_forks": 1
    }
  },
  "repos": [
    {
      "full_name": "EmilLindfors/claude-marketplace",
      "url": "https://github.com/EmilLindfors/claude-marketplace",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 1,
        "pushed_at": "2025-11-14T17:46:35Z",
        "created_at": "2025-10-31T06:08:32Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3625
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/settings.local.json",
          "type": "blob",
          "size": 785
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 1517
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 440
        },
        {
          "path": "plugins/changelog",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/changelog/CONTEXT.md",
          "type": "blob",
          "size": 18321
        },
        {
          "path": "plugins/changelog/README.md",
          "type": "blob",
          "size": 8283
        },
        {
          "path": "plugins/changelog/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/changelog/agents/changelog-writer.md",
          "type": "blob",
          "size": 11706
        },
        {
          "path": "plugins/changelog/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/changelog/commands/changelog-add.md",
          "type": "blob",
          "size": 4811
        },
        {
          "path": "plugins/changelog/commands/changelog-init.md",
          "type": "blob",
          "size": 4589
        },
        {
          "path": "plugins/changelog/commands/changelog-view.md",
          "type": "blob",
          "size": 3133
        },
        {
          "path": "plugins/changelog/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/changelog/hooks/check-changelog-before-commit.py",
          "type": "blob",
          "size": 6756
        },
        {
          "path": "plugins/changelog/hooks/hooks.json",
          "type": "blob",
          "size": 350
        },
        {
          "path": "plugins/rust-data-engineering",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/CONTEXT.md",
          "type": "blob",
          "size": 28367
        },
        {
          "path": "plugins/rust-data-engineering/README.md",
          "type": "blob",
          "size": 10697
        },
        {
          "path": "plugins/rust-data-engineering/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/agents/data-engineering-expert.md",
          "type": "blob",
          "size": 8932
        },
        {
          "path": "plugins/rust-data-engineering/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/commands/data-datafusion-query.md",
          "type": "blob",
          "size": 14282
        },
        {
          "path": "plugins/rust-data-engineering/commands/data-iceberg-table.md",
          "type": "blob",
          "size": 15494
        },
        {
          "path": "plugins/rust-data-engineering/commands/data-object-store-setup.md",
          "type": "blob",
          "size": 4034
        },
        {
          "path": "plugins/rust-data-engineering/commands/data-parquet-read.md",
          "type": "blob",
          "size": 10458
        },
        {
          "path": "plugins/rust-data-engineering/commands/data-parquet-write.md",
          "type": "blob",
          "size": 14031
        },
        {
          "path": "plugins/rust-data-engineering/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/skills/data-lake-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/skills/data-lake-architect/SKILL.md",
          "type": "blob",
          "size": 13496
        },
        {
          "path": "plugins/rust-data-engineering/skills/datafusion-query-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/skills/datafusion-query-advisor/SKILL.md",
          "type": "blob",
          "size": 11169
        },
        {
          "path": "plugins/rust-data-engineering/skills/object-store-best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/skills/object-store-best-practices/SKILL.md",
          "type": "blob",
          "size": 13387
        },
        {
          "path": "plugins/rust-data-engineering/skills/parquet-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-data-engineering/skills/parquet-optimization/SKILL.md",
          "type": "blob",
          "size": 8656
        },
        {
          "path": "plugins/rust-error-handling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/CONTEXT.md",
          "type": "blob",
          "size": 8630
        },
        {
          "path": "plugins/rust-error-handling/README.md",
          "type": "blob",
          "size": 5857
        },
        {
          "path": "plugins/rust-error-handling/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/agents/rust-error-expert.md",
          "type": "blob",
          "size": 8597
        },
        {
          "path": "plugins/rust-error-handling/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/commands/rust-error-add-type.md",
          "type": "blob",
          "size": 8255
        },
        {
          "path": "plugins/rust-error-handling/commands/rust-error-refactor.md",
          "type": "blob",
          "size": 8602
        },
        {
          "path": "plugins/rust-error-handling/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/skills/error-conversion-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/skills/error-conversion-guide/SKILL.md",
          "type": "blob",
          "size": 13326
        },
        {
          "path": "plugins/rust-error-handling/skills/error-handler-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/skills/error-handler-advisor/SKILL.md",
          "type": "blob",
          "size": 10569
        },
        {
          "path": "plugins/rust-error-handling/skills/thiserror-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-error-handling/skills/thiserror-expert/SKILL.md",
          "type": "blob",
          "size": 11683
        },
        {
          "path": "plugins/rust-hexagonal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/CONTEXT.md",
          "type": "blob",
          "size": 9440
        },
        {
          "path": "plugins/rust-hexagonal/README.md",
          "type": "blob",
          "size": 7402
        },
        {
          "path": "plugins/rust-hexagonal/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/agents/rust-hex-architect.md",
          "type": "blob",
          "size": 8153
        },
        {
          "path": "plugins/rust-hexagonal/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/commands/rust-hex-add-adapter.md",
          "type": "blob",
          "size": 13079
        },
        {
          "path": "plugins/rust-hexagonal/commands/rust-hex-add-port.md",
          "type": "blob",
          "size": 7426
        },
        {
          "path": "plugins/rust-hexagonal/commands/rust-hex-init.md",
          "type": "blob",
          "size": 9165
        },
        {
          "path": "plugins/rust-hexagonal/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/skills/domain-layer-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/skills/domain-layer-expert/SKILL.md",
          "type": "blob",
          "size": 6514
        },
        {
          "path": "plugins/rust-hexagonal/skills/hexagonal-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/skills/hexagonal-advisor/SKILL.md",
          "type": "blob",
          "size": 13328
        },
        {
          "path": "plugins/rust-hexagonal/skills/port-adapter-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-hexagonal/skills/port-adapter-designer/SKILL.md",
          "type": "blob",
          "size": 6130
        },
        {
          "path": "plugins/rust-lambda",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/CONTEXT.md",
          "type": "blob",
          "size": 20413
        },
        {
          "path": "plugins/rust-lambda/README.md",
          "type": "blob",
          "size": 21209
        },
        {
          "path": "plugins/rust-lambda/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/agents/rust-lambda-expert.md",
          "type": "blob",
          "size": 9435
        },
        {
          "path": "plugins/rust-lambda/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-advanced.md",
          "type": "blob",
          "size": 10738
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-build.md",
          "type": "blob",
          "size": 5583
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-cost.md",
          "type": "blob",
          "size": 6925
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-deploy.md",
          "type": "blob",
          "size": 8716
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-function-urls.md",
          "type": "blob",
          "size": 13387
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-github-actions.md",
          "type": "blob",
          "size": 14853
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-iac.md",
          "type": "blob",
          "size": 15736
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-new.md",
          "type": "blob",
          "size": 5210
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-observability.md",
          "type": "blob",
          "size": 14287
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-optimize-compute.md",
          "type": "blob",
          "size": 16170
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-optimize-io.md",
          "type": "blob",
          "size": 14140
        },
        {
          "path": "plugins/rust-lambda/commands/lambda-secrets.md",
          "type": "blob",
          "size": 16289
        },
        {
          "path": "plugins/rust-lambda/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/skills/async-sync-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/skills/async-sync-advisor/SKILL.md",
          "type": "blob",
          "size": 4793
        },
        {
          "path": "plugins/rust-lambda/skills/cold-start-optimizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/skills/cold-start-optimizer/SKILL.md",
          "type": "blob",
          "size": 5284
        },
        {
          "path": "plugins/rust-lambda/skills/lambda-optimization-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-lambda/skills/lambda-optimization-advisor/SKILL.md",
          "type": "blob",
          "size": 5193
        },
        {
          "path": "plugins/rust-mcp-server",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/CONTEXT.md",
          "type": "blob",
          "size": 10854
        },
        {
          "path": "plugins/rust-mcp-server/README.md",
          "type": "blob",
          "size": 10580
        },
        {
          "path": "plugins/rust-mcp-server/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/agents/mcp-architect.md",
          "type": "blob",
          "size": 12066
        },
        {
          "path": "plugins/rust-mcp-server/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-add-prompt.md",
          "type": "blob",
          "size": 15438
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-add-resource.md",
          "type": "blob",
          "size": 13472
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-add-tool.md",
          "type": "blob",
          "size": 9140
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-deploy.md",
          "type": "blob",
          "size": 12699
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-init.md",
          "type": "blob",
          "size": 13357
        },
        {
          "path": "plugins/rust-mcp-server/commands/mcp-test.md",
          "type": "blob",
          "size": 10984
        },
        {
          "path": "plugins/rust-mcp-server/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-best-practices/SKILL.md",
          "type": "blob",
          "size": 17511
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-prompts-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-prompts-guide/SKILL.md",
          "type": "blob",
          "size": 16461
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-resources-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-resources-guide/SKILL.md",
          "type": "blob",
          "size": 14799
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-tools-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-tools-guide/SKILL.md",
          "type": "blob",
          "size": 16108
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-transport-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/mcp-transport-guide/SKILL.md",
          "type": "blob",
          "size": 14228
        },
        {
          "path": "plugins/rust-mcp-server/skills/rmcp-quickstart",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-mcp-server/skills/rmcp-quickstart/SKILL.md",
          "type": "blob",
          "size": 10976
        },
        {
          "path": "plugins/rust-modern-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/CONTEXT.md",
          "type": "blob",
          "size": 17841
        },
        {
          "path": "plugins/rust-modern-patterns/README.md",
          "type": "blob",
          "size": 13102
        },
        {
          "path": "plugins/rust-modern-patterns/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/agents/rust-modern-expert.md",
          "type": "blob",
          "size": 9680
        },
        {
          "path": "plugins/rust-modern-patterns/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-async-traits.md",
          "type": "blob",
          "size": 9321
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-modernize.md",
          "type": "blob",
          "size": 7486
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-pattern-check.md",
          "type": "blob",
          "size": 9713
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-quality-check.md",
          "type": "blob",
          "size": 8168
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-setup-tooling.md",
          "type": "blob",
          "size": 12319
        },
        {
          "path": "plugins/rust-modern-patterns/commands/rust-upgrade-edition.md",
          "type": "blob",
          "size": 7271
        },
        {
          "path": "plugins/rust-modern-patterns/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/async-patterns-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/async-patterns-guide/SKILL.md",
          "type": "blob",
          "size": 3802
        },
        {
          "path": "plugins/rust-modern-patterns/skills/let-chains-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/let-chains-advisor/SKILL.md",
          "type": "blob",
          "size": 3282
        },
        {
          "path": "plugins/rust-modern-patterns/skills/rust-2024-migration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/rust-2024-migration/SKILL.md",
          "type": "blob",
          "size": 4946
        },
        {
          "path": "plugins/rust-modern-patterns/skills/rust-tooling-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/rust-tooling-guide/SKILL.md",
          "type": "blob",
          "size": 13674
        },
        {
          "path": "plugins/rust-modern-patterns/skills/type-driven-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-modern-patterns/skills/type-driven-design/SKILL.md",
          "type": "blob",
          "size": 19017
        },
        {
          "path": "plugins/rust-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/CONTEXT.md",
          "type": "blob",
          "size": 10079
        },
        {
          "path": "plugins/rust-testing/README.md",
          "type": "blob",
          "size": 7495
        },
        {
          "path": "plugins/rust-testing/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/agents/rust-test-expert.md",
          "type": "blob",
          "size": 9072
        },
        {
          "path": "plugins/rust-testing/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/commands/rust-test-add-integration.md",
          "type": "blob",
          "size": 9825
        },
        {
          "path": "plugins/rust-testing/commands/rust-test-add-unit.md",
          "type": "blob",
          "size": 7288
        },
        {
          "path": "plugins/rust-testing/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/skills/mock-strategy-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/skills/mock-strategy-guide/SKILL.md",
          "type": "blob",
          "size": 8253
        },
        {
          "path": "plugins/rust-testing/skills/property-testing-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/skills/property-testing-guide/SKILL.md",
          "type": "blob",
          "size": 6927
        },
        {
          "path": "plugins/rust-testing/skills/test-coverage-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rust-testing/skills/test-coverage-advisor/SKILL.md",
          "type": "blob",
          "size": 6913
        },
        {
          "path": "url-shortener",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/.gitignore",
          "type": "blob",
          "size": 19
        },
        {
          "path": "url-shortener/Cargo.toml",
          "type": "blob",
          "size": 392
        },
        {
          "path": "url-shortener/README.md",
          "type": "blob",
          "size": 5979
        },
        {
          "path": "url-shortener/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/src/adapters",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/src/adapters/in_memory_repository.rs",
          "type": "blob",
          "size": 7287
        },
        {
          "path": "url-shortener/src/adapters/mod.rs",
          "type": "blob",
          "size": 319
        },
        {
          "path": "url-shortener/src/adapters/random_id_generator.rs",
          "type": "blob",
          "size": 5078
        },
        {
          "path": "url-shortener/src/domain",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/src/domain/mod.rs",
          "type": "blob",
          "size": 404
        },
        {
          "path": "url-shortener/src/domain/original_url.rs",
          "type": "blob",
          "size": 3083
        },
        {
          "path": "url-shortener/src/domain/short_code.rs",
          "type": "blob",
          "size": 3979
        },
        {
          "path": "url-shortener/src/domain/shortened_url.rs",
          "type": "blob",
          "size": 4612
        },
        {
          "path": "url-shortener/src/domain/url_id.rs",
          "type": "blob",
          "size": 1713
        },
        {
          "path": "url-shortener/src/error.rs",
          "type": "blob",
          "size": 1130
        },
        {
          "path": "url-shortener/src/lib.rs",
          "type": "blob",
          "size": 5100
        },
        {
          "path": "url-shortener/src/ports",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/src/ports/id_generator.rs",
          "type": "blob",
          "size": 662
        },
        {
          "path": "url-shortener/src/ports/mod.rs",
          "type": "blob",
          "size": 283
        },
        {
          "path": "url-shortener/src/ports/repository.rs",
          "type": "blob",
          "size": 1805
        },
        {
          "path": "url-shortener/src/service",
          "type": "tree",
          "size": null
        },
        {
          "path": "url-shortener/src/service/mod.rs",
          "type": "blob",
          "size": 222
        },
        {
          "path": "url-shortener/src/service/url_shortener_service.rs",
          "type": "blob",
          "size": 12355
        }
      ],
      "marketplace": {
        "name": "lf-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Emil Lindfors"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "changelog",
            "description": "Changelog management plugin that ensures all code commits include proper changelog entries. Provides hooks to prevent commits without changelog updates, commands for managing changelog entries, and agents for writing well-formatted changelog entries following Keep a Changelog format",
            "source": "./plugins/changelog",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install changelog@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/changelog-add",
                "description": "Add a new entry to the CHANGELOG.md file following Keep a Changelog format",
                "path": "plugins/changelog/commands/changelog-add.md",
                "frontmatter": {
                  "description": "Add a new entry to the CHANGELOG.md file following Keep a Changelog format"
                },
                "content": "Add a new changelog entry to the project's CHANGELOG.md file. This command helps you document changes following the Keep a Changelog format (https://keepachangelog.com/).\n\n## Context\nThis project maintains a CHANGELOG.md file that follows the Keep a Changelog format:\n- All notable changes are documented\n- Format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)\n- The project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html)\n- Entries are organized by version with date stamps\n- Changes are categorized as: Added, Changed, Deprecated, Removed, Fixed, Security\n\n## Task\nHelp the user add a well-formatted changelog entry:\n\n1. **Check for CHANGELOG.md**: Look for CHANGELOG.md in the project root\n2. **Identify the [Unreleased] section**: New entries go under the [Unreleased] section\n3. **Ask the user for details**:\n   - What type of change is this? (Added/Changed/Fixed/Removed/Security/Deprecated)\n   - What is the description of the change?\n   - Any additional context or details?\n4. **Format the entry** following the Keep a Changelog format:\n   - Use proper heading level (### for category)\n   - Use bullet points (-)\n   - Include relevant technical details\n   - Reference related files, endpoints, or components\n   - Be concise but descriptive\n5. **Add the entry** under the appropriate category in the [Unreleased] section\n6. **Stage the file**: Run `git add CHANGELOG.md` to stage the changes\n7. **Confirm**: Show the user the added entry and confirm it's been staged\n\n## Keep a Changelog Categories\n\n### Added\nFor new features, endpoints, functionality, or capabilities.\n\n### Changed\nFor changes in existing functionality, including updates, enhancements, or modifications.\n\n### Deprecated\nFor soon-to-be removed features or functionality.\n\n### Removed\nFor removed features, endpoints, or functionality.\n\n### Fixed\nFor bug fixes, error corrections, or issue resolutions.\n\n### Security\nFor security improvements, vulnerability fixes, or security-related changes.\n\n## Entry Format Guidelines\n\n### Good Examples\n```markdown\n### Added\n- **System Metrics Collection**: Comprehensive system monitoring using dedicated PyIceberg table\n  - Multi-Service Support: Service identification with hostname and environment\n  - Comprehensive Metrics: CPU usage, memory, disk, network, and process metrics\n  - Partitioned Storage: Efficiently partitioned by service_name, date, and hour\n\n### Fixed\n- **Loss Mortality Endpoint**: Fixed window function partitioning bug in `/v3/mortality/areas/month` endpoint\n  - Root Cause: Window functions were using incorrect partition clause\n  - Impact: Cumulative calculations now update correctly with date parameter changes\n  - Solution: Updated partition clause to `PARTITION BY fdir.aquacloud_area_name`\n\n### Changed\n- **Parameter Standardization**: Migrated all v3 endpoints from `include_self` to `exclude_self` parameter\n  - Updated 13 v3 SQL queries to use `$exclude_self` template parameter\n  - Behavioral Consistency: Both v2 and v3 APIs now use identical parameter naming\n```\n\n### Formatting Tips\n- Use **bold** for main component or feature names\n- Use sub-bullets for technical details, root causes, solutions, impacts\n- Include file paths for code changes (e.g., `services/v3/api/router.py`)\n- Reference endpoint paths when applicable (e.g., `/v3/mortality/areas/month`)\n- Be specific and technical - developers will read this\n- Group related changes under one main bullet when appropriate\n\n## Example Workflow\n\n```\nUser: I fixed a bug where the API was returning 500 errors on the feeding endpoint\n\nYou: I'll help you add that to the changelog. Let me check the CHANGELOG.md file first.\n\n[After reading the file]\n\nLet me add this under the Fixed section in the [Unreleased] area:\n\n### Fixed\n- **Feeding Endpoint Error**: Fixed 500 server error in `/v3/feeding/sfr-by-weeknumber-and-year` endpoint\n  - Root Cause: Missing AND clause in SQL WHERE statement\n  - Solution: Added proper SQL syntax to fix query parsing\n  - Impact: Endpoint now returns data successfully\n\nDoes this accurately describe your fix? Would you like me to add any additional details?\n```\n\n## Important Notes\n- Always add entries to the **[Unreleased]** section\n- Don't modify versioned sections (those are historical records)\n- If [Unreleased] section doesn't exist, create it at the top after the header\n- Maintain consistent formatting with existing entries\n- Use proper markdown syntax\n- Stage the file after adding the entry so it's ready for commit\n- Multiple related changes can be grouped under one main bullet point with sub-bullets\n\nRemember: Good changelog entries help developers understand what changed, why it changed, and what impact it has."
              },
              {
                "name": "/changelog-init",
                "description": "Initialize a new CHANGELOG.md file following Keep a Changelog format",
                "path": "plugins/changelog/commands/changelog-init.md",
                "frontmatter": {
                  "description": "Initialize a new CHANGELOG.md file following Keep a Changelog format"
                },
                "content": "Create a new CHANGELOG.md file for the project following the Keep a Changelog format standards.\n\n## Context\nA CHANGELOG.md file is essential for tracking all notable changes to a project. It should:\n- Follow the Keep a Changelog format (https://keepachangelog.com/en/1.0.0/)\n- Use Semantic Versioning (https://semver.org/spec/v2.0.0.html)\n- Be human-readable and easy to maintain\n- Have clear categories for different types of changes\n\n## Task\nInitialize a new changelog file with proper structure:\n\n1. **Check if CHANGELOG.md exists**: Don't overwrite an existing changelog\n2. **Create the file structure**:\n   - Add the standard header and introduction\n   - Include an [Unreleased] section for upcoming changes\n   - Add a template version section (if the project has an initial version)\n   - Include links to Keep a Changelog and Semantic Versioning\n3. **Ask about initial version**: Should we add an initial version entry (e.g., [1.0.0])?\n4. **Customize for the project**:\n   - Check if there's a version number in the project (package.json, pyproject.toml, etc.)\n   - Include relevant context for the specific project type\n5. **Create the file**: Write the CHANGELOG.md to the project root\n6. **Provide guidance**: Show the user how to use the new changelog\n\n## Standard Changelog Template\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [1.0.0] - YYYY-MM-DD\n\n### Added\n- Initial release\n```\n\n## Categories Explanation\n\nInclude a comment in the changelog explaining the categories:\n\n```markdown\n<!--\nCategories for changelog entries:\n\n- Added: New features, endpoints, or functionality\n- Changed: Changes in existing functionality\n- Deprecated: Soon-to-be removed features\n- Removed: Removed features or functionality\n- Fixed: Bug fixes and error corrections\n- Security: Security improvements and vulnerability fixes\n\nEach entry should be concise but descriptive, include technical details,\nand reference relevant files, endpoints, or components.\n-->\n```\n\n## Workflow\n\n```\nUser: I need to create a changelog for this project\n\nYou: I'll help you initialize a new CHANGELOG.md file. Let me first check if one already exists.\n\n[Check for existing CHANGELOG.md]\n\nGreat! I don't see an existing CHANGELOG.md. Let me create one for you.\n\n[Check for version information in package files]\n\nI found your project is using version 1.0.0 from pyproject.toml.\n\nI'll create a CHANGELOG.md with:\n- Standard Keep a Changelog header\n- [Unreleased] section for upcoming changes\n- [1.0.0] section for the initial release\n\n[Create the file]\n\nâœ… Created CHANGELOG.md in the project root!\n\nNext steps:\n1. Review the initial structure\n2. Add any existing changes to the [Unreleased] section\n3. Use /changelog-add when you make changes to the project\n4. The changelog hook will ensure you update the changelog before each commit\n\nWould you like me to add any specific entries to get you started?\n```\n\n## Version Detection\n\nTry to detect the current version from common files:\n- `package.json` (Node.js): Check the \"version\" field\n- `pyproject.toml` (Python): Check [tool.poetry.version] or [project] version\n- `Cargo.toml` (Rust): Check [package] version\n- `pom.xml` (Java/Maven): Check <version> tag\n- `build.gradle` (Gradle): Check version property\n- `setup.py` (Python): Check version parameter\n- `__version__.py`: Check __version__ variable\n\n## Customization Options\n\nAsk the user if they want to:\n- Include an initial version entry or just [Unreleased]\n- Add any specific categories they commonly use\n- Include example entries to guide future updates\n- Add project-specific notes or conventions\n\n## Important Notes\n- Never overwrite an existing CHANGELOG.md without explicit confirmation\n- Use the current date (YYYY-MM-DD format) for version entries\n- Include helpful comments for first-time users\n- Make sure the format is exactly correct (Keep a Changelog is specific about format)\n- After creating, suggest adding it to git: `git add CHANGELOG.md`\n\n## Follow-up\n\nAfter creating the changelog:\n1. Show the user the created file\n2. Explain how to use it\n3. Mention the /changelog-add command\n4. Remind them about the changelog hook that will enforce updates\n\nRemember: A good initial changelog structure sets the tone for maintaining it throughout the project's lifetime."
              },
              {
                "name": "/changelog-view",
                "description": "View recent entries from the CHANGELOG.md file",
                "path": "plugins/changelog/commands/changelog-view.md",
                "frontmatter": {
                  "description": "View recent entries from the CHANGELOG.md file"
                },
                "content": "Display recent changelog entries from the project's CHANGELOG.md file to help understand recent changes and the changelog format.\n\n## Context\nThe CHANGELOG.md file contains all notable changes to the project, organized by version and date following the Keep a Changelog format.\n\n## Task\nHelp the user view and understand recent changelog entries:\n\n1. **Locate CHANGELOG.md**: Find the CHANGELOG.md file in the project root or nearby repositories\n2. **Read the file**: Load the changelog content\n3. **Display recent entries**:\n   - Show the [Unreleased] section if it exists (this shows pending changes)\n   - Show the 3-5 most recent versioned releases\n   - Format the output in a readable way\n4. **Provide context**:\n   - Highlight the changelog format and structure\n   - Point out different change categories (Added, Changed, Fixed, etc.)\n   - Show examples of well-formatted entries\n5. **Offer next steps**:\n   - Suggest using /changelog-add if they need to add an entry\n   - Point out where new entries should be added\n\n## Display Format\n\nShow the changelog content in a structured way:\n\n```markdown\n# Recent Changelog Entries\n\n## [Unreleased]\n[If there are unreleased entries, show them here]\n\n## [Version] - Date\n[Show recent version entries]\n\n---\n\nðŸ’¡ Tips:\n- New entries should be added to the [Unreleased] section\n- Use /changelog-add to add a new entry\n- Follow the existing format and style\n```\n\n## Example Output\n\n```\n# Recent Changelog Entries from CHANGELOG.md\n\n## [Unreleased]\n\n### Fixed\n- **Feeding Endpoint Error**: Fixed 500 server error in `/v3/feeding/sfr-by-weeknumber-and-year` endpoint\n\n## [3.7.19] - 2025-09-26\n\n## [3.7.18] - 2025-09-26\n\n## [3.7.17] - 2025-09-25\n\n### Changed\n- Use sudo apt install instead of curl for just install\n\n### Fixed\n- Reintroduced the period param\n\n---\n\nðŸ’¡ The changelog follows Keep a Changelog format (https://keepachangelog.com/)\n\nCategories used:\n- Added: New features\n- Changed: Changes in existing functionality\n- Fixed: Bug fixes\n- Removed: Removed features\n- Security: Security improvements\n- Deprecated: Soon-to-be removed features\n\nTo add a new entry: /changelog-add\n```\n\n## Options\n\nIf the user specifies what they want to see, adjust the output:\n- \"latest version\" â†’ Show only the most recent release\n- \"unreleased\" â†’ Show only the [Unreleased] section\n- \"all\" â†’ Show the entire changelog\n- \"last N versions\" â†’ Show the last N versions\n\n## Additional Features\n\n- **Search**: If user asks to search for specific terms, grep through the changelog\n- **Statistics**: Can provide counts of different types of changes\n- **Format check**: Can validate that the changelog follows the expected format\n- **Compare**: Can compare what's in [Unreleased] vs what's committed\n\n## Important Notes\n- Make the output readable and well-formatted\n- Use appropriate markdown formatting\n- Highlight important sections\n- Provide helpful context about the changelog structure\n- Offer actionable next steps\n\nRemember: This command helps users understand the project's change history and learn the changelog format."
              }
            ],
            "skills": []
          },
          {
            "name": "rust-hexagonal",
            "description": "Hexagonal architecture plugin for Rust. Helps design and implement clean, maintainable Rust applications using the ports and adapters pattern. Includes commands for initializing project structure, adding ports and adapters, and an expert agent for architecture guidance",
            "source": "./plugins/rust-hexagonal",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-hexagonal@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/rust-hex-add-adapter",
                "description": "Add a new adapter implementation for an existing port",
                "path": "plugins/rust-hexagonal/commands/rust-hex-add-adapter.md",
                "frontmatter": {
                  "description": "Add a new adapter implementation for an existing port"
                },
                "content": "You are helping create a new adapter implementation for an existing port in a Rust hexagonal architecture project.\n\n## Your Task\n\nCreate a concrete adapter implementation for a port trait, following best practices for the specific technology being used.\n\n## Steps\n\n1. **List Available Ports**\n\n   First, scan the project to find existing ports:\n   - Check `src/ports/driven.rs` for driven ports\n   - Check `src/ports/driving.rs` for driving ports\n\n   Display them to the user:\n   ```\n   Available Ports:\n\n   Driven Ports (Secondary):\n   - UserRepository\n   - PaymentGateway\n   - EmailService\n\n   Driving Ports (Primary):\n   - CreateUserUseCase\n   - ProcessOrderUseCase\n   ```\n\n2. **Ask User for Details**\n\n   Ask (if not already provided):\n   - Which port to implement?\n   - What technology/adapter type? (e.g., PostgreSQL, MongoDB, HTTP, InMemory, Mock)\n   - Any configuration needed? (connection strings, API keys, etc.)\n\n3. **Create Adapter Implementation**\n\n   Based on the technology, create the appropriate adapter.\n\n   **Database Adapters (PostgreSQL example)**:\n   ```rust\n   //! PostgreSQL implementation of [PortName]\n   //!\n   //! This adapter implements [PortName] using PostgreSQL via SQLx.\n\n   use crate::domain::models::[Entity];\n   use crate::ports::driven::{[PortName], [PortName]Error};\n   use async_trait::async_trait;\n   use sqlx::PgPool;\n\n   /// PostgreSQL implementation of [PortName]\n   pub struct Postgres[PortName] {\n       pool: PgPool,\n   }\n\n   impl Postgres[PortName] {\n       pub fn new(pool: PgPool) -> Self {\n           Self { pool }\n       }\n   }\n\n   #[async_trait]\n   impl [PortName] for Postgres[PortName] {\n       async fn find_by_id(&self, id: &str) -> Result<[Entity], [PortName]Error> {\n           sqlx::query_as!(\n               [Entity],\n               \"SELECT * FROM [table_name] WHERE id = $1\",\n               id\n           )\n           .fetch_one(&self.pool)\n           .await\n           .map_err(|e| match e {\n               sqlx::Error::RowNotFound => [PortName]Error::NotFound(id.to_string()),\n               _ => [PortName]Error::Database(e.to_string()),\n           })\n       }\n\n       async fn save(&self, entity: &[Entity]) -> Result<(), [PortName]Error> {\n           sqlx::query!(\n               \"INSERT INTO [table_name] (id, [fields]) VALUES ($1, $2)\n                ON CONFLICT (id) DO UPDATE SET [fields] = $2\",\n               entity.id(),\n               // other fields\n           )\n           .execute(&self.pool)\n           .await\n           .map_err(|e| [PortName]Error::Database(e.to_string()))?;\n\n           Ok(())\n       }\n\n       async fn delete(&self, id: &str) -> Result<(), [PortName]Error> {\n           sqlx::query!(\"DELETE FROM [table_name] WHERE id = $1\", id)\n               .execute(&self.pool)\n               .await\n               .map_err(|e| [PortName]Error::Database(e.to_string()))?;\n\n           Ok(())\n       }\n   }\n\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n       // Recommend using testcontainers for integration tests\n\n       #[sqlx::test]\n       async fn test_find_by_id(pool: PgPool) {\n           let repo = Postgres[PortName]::new(pool);\n           // Test implementation\n       }\n   }\n   ```\n\n   **HTTP Client Adapters**:\n   ```rust\n   //! HTTP implementation of [PortName]\n   //!\n   //! This adapter implements [PortName] using reqwest HTTP client.\n\n   use crate::ports::driven::{[PortName], [PortName]Error};\n   use async_trait::async_trait;\n   use reqwest::Client;\n   use serde::{Deserialize, Serialize};\n\n   /// HTTP client implementation of [PortName]\n   pub struct Http[PortName] {\n       client: Client,\n       base_url: String,\n       api_key: Option<String>,\n   }\n\n   impl Http[PortName] {\n       pub fn new(base_url: String, api_key: Option<String>) -> Self {\n           Self {\n               client: Client::new(),\n               base_url,\n               api_key,\n           }\n       }\n   }\n\n   #[async_trait]\n   impl [PortName] for Http[PortName] {\n       async fn [method](&self, [params]) -> Result<[ReturnType], [PortName]Error> {\n           let url = format!(\"{}/[endpoint]\", self.base_url);\n\n           let mut request = self.client.get(&url);\n\n           if let Some(key) = &self.api_key {\n               request = request.header(\"Authorization\", format!(\"Bearer {}\", key));\n           }\n\n           let response = request\n               .send()\n               .await\n               .map_err(|e| [PortName]Error::Network(e.to_string()))?;\n\n           if !response.status().is_success() {\n               return Err([PortName]Error::HttpError(response.status().as_u16()));\n           }\n\n           response\n               .json::<[ReturnType]>()\n               .await\n               .map_err(|e| [PortName]Error::Deserialization(e.to_string()))\n       }\n   }\n\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n       use wiremock::{MockServer, Mock, ResponseTemplate};\n       use wiremock::matchers::{method, path};\n\n       #[tokio::test]\n       async fn test_[method]() {\n           let mock_server = MockServer::start().await;\n\n           Mock::given(method(\"GET\"))\n               .and(path(\"/[endpoint]\"))\n               .respond_with(ResponseTemplate::new(200).set_body_json(/* mock response */))\n               .mount(&mock_server)\n               .await;\n\n           let adapter = Http[PortName]::new(mock_server.uri(), None);\n           // Test implementation\n       }\n   }\n   ```\n\n   **In-Memory Adapters (for testing)**:\n   ```rust\n   //! In-memory implementation of [PortName]\n   //!\n   //! This adapter provides an in-memory implementation useful for testing.\n\n   use crate::domain::models::[Entity];\n   use crate::ports::driven::{[PortName], [PortName]Error};\n   use async_trait::async_trait;\n   use std::collections::HashMap;\n   use std::sync::Arc;\n   use tokio::sync::RwLock;\n\n   /// In-memory implementation of [PortName]\n   #[derive(Clone)]\n   pub struct InMemory[PortName] {\n       storage: Arc<RwLock<HashMap<String, [Entity]>>>,\n   }\n\n   impl InMemory[PortName] {\n       pub fn new() -> Self {\n           Self {\n               storage: Arc::new(RwLock::new(HashMap::new())),\n           }\n       }\n   }\n\n   impl Default for InMemory[PortName] {\n       fn default() -> Self {\n           Self::new()\n       }\n   }\n\n   #[async_trait]\n   impl [PortName] for InMemory[PortName] {\n       async fn find_by_id(&self, id: &str) -> Result<[Entity], [PortName]Error> {\n           let storage = self.storage.read().await;\n           storage\n               .get(id)\n               .cloned()\n               .ok_or_else(|| [PortName]Error::NotFound(id.to_string()))\n       }\n\n       async fn save(&self, entity: &[Entity]) -> Result<(), [PortName]Error> {\n           let mut storage = self.storage.write().await;\n           storage.insert(entity.id().to_string(), entity.clone());\n           Ok(())\n       }\n\n       async fn delete(&self, id: &str) -> Result<(), [PortName]Error> {\n           let mut storage = self.storage.write().await;\n           storage\n               .remove(id)\n               .ok_or_else(|| [PortName]Error::NotFound(id.to_string()))?;\n           Ok(())\n       }\n   }\n\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       #[tokio::test]\n       async fn test_save_and_find() {\n           let repo = InMemory[PortName]::new();\n           // Test implementation\n       }\n   }\n   ```\n\n   **Redis Cache Adapter**:\n   ```rust\n   //! Redis implementation of [CacheName]\n   //!\n   //! This adapter implements caching using Redis.\n\n   use crate::ports::driven::{[CacheName], [CacheName]Error};\n   use async_trait::async_trait;\n   use redis::{AsyncCommands, Client};\n   use serde::{de::DeserializeOwned, Serialize};\n\n   /// Redis implementation of [CacheName]\n   pub struct Redis[CacheName] {\n       client: Client,\n   }\n\n   impl Redis[CacheName] {\n       pub fn new(redis_url: &str) -> Result<Self, redis::RedisError> {\n           let client = Client::open(redis_url)?;\n           Ok(Self { client })\n       }\n   }\n\n   #[async_trait]\n   impl [CacheName] for Redis[CacheName] {\n       async fn get<T>(&self, key: &str) -> Result<Option<T>, [CacheName]Error>\n       where\n           T: DeserializeOwned,\n       {\n           let mut conn = self.client.get_async_connection()\n               .await\n               .map_err(|e| [CacheName]Error::Connection(e.to_string()))?;\n\n           let value: Option<String> = conn.get(key)\n               .await\n               .map_err(|e| [CacheName]Error::Operation(e.to_string()))?;\n\n           match value {\n               Some(v) => {\n                   let parsed = serde_json::from_str(&v)\n                       .map_err(|e| [CacheName]Error::Serialization(e.to_string()))?;\n                   Ok(Some(parsed))\n               }\n               None => Ok(None),\n           }\n       }\n\n       async fn set<T>(&self, key: &str, value: &T, ttl_seconds: Option<u64>) -> Result<(), [CacheName]Error>\n       where\n           T: Serialize,\n       {\n           let mut conn = self.client.get_async_connection()\n               .await\n               .map_err(|e| [CacheName]Error::Connection(e.to_string()))?;\n\n           let serialized = serde_json::to_string(value)\n               .map_err(|e| [CacheName]Error::Serialization(e.to_string()))?;\n\n           if let Some(ttl) = ttl_seconds {\n               conn.set_ex(key, serialized, ttl)\n                   .await\n                   .map_err(|e| [CacheName]Error::Operation(e.to_string()))?;\n           } else {\n               conn.set(key, serialized)\n                   .await\n                   .map_err(|e| [CacheName]Error::Operation(e.to_string()))?;\n           }\n\n           Ok(())\n       }\n   }\n   ```\n\n4. **Update Module Exports**\n\n   Add to `src/adapters/driven/mod.rs` (or `driving/mod.rs`):\n   ```rust\n   pub mod [adapter_name];\n   ```\n\n5. **Update Dependencies**\n\n   Check if required dependencies are in Cargo.toml and suggest additions:\n\n   For PostgreSQL:\n   ```toml\n   sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-native-tls\"] }\n   ```\n\n   For HTTP:\n   ```toml\n   reqwest = { version = \"0.11\", features = [\"json\"] }\n   ```\n\n   For Redis:\n   ```toml\n   redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }\n   ```\n\n6. **Provide Integration Example**\n\n   Show how to wire up the adapter in the application:\n\n   ```rust\n   // In main.rs or application setup\n   use crate::adapters::driven::[Adapter];\n   use crate::domain::services::[Service];\n\n   #[tokio::main]\n   async fn main() -> Result<(), Box<dyn std::error::Error>> {\n       // Setup adapter\n       let pool = PgPoolOptions::new()\n           .connect(\"postgresql://localhost/mydb\")\n           .await?;\n\n       let adapter = [Adapter]::new(pool);\n\n       // Create domain service with adapter\n       let service = [Service]::new(adapter);\n\n       // Use the service\n       service.do_work().await?;\n\n       Ok(())\n   }\n   ```\n\n7. **Suggest Testing Approach**\n\n   Based on adapter type:\n\n   **For Database Adapters**:\n   ```\n   Testing recommendations:\n   1. Use `sqlx::test` macro for integration tests\n   2. Consider testcontainers for isolated test databases\n   3. Create test fixtures for common scenarios\n   4. Test error cases (not found, connection errors)\n   ```\n\n   **For HTTP Adapters**:\n   ```\n   Testing recommendations:\n   1. Use wiremock or mockito for HTTP mocking\n   2. Test success and error responses\n   3. Test authentication/authorization\n   4. Test timeout and retry logic\n   ```\n\n   **For In-Memory Adapters**:\n   ```\n   Testing recommendations:\n   1. Test concurrent access with multiple threads\n   2. Verify data consistency\n   3. Test all CRUD operations\n   ```\n\n8. **Provide Summary**\n\n   Tell the user:\n   ```\n   âœ… Adapter '[AdapterName]' created successfully!\n\n   ## Files Created/Modified:\n   - `src/adapters/[driving|driven]/[adapter_name].rs` - Adapter implementation\n   - `src/adapters/[driving|driven]/mod.rs` - Module export\n\n   ## Dependencies to Add:\n   [List required Cargo.toml dependencies]\n\n   ## Next Steps:\n\n   1. Add dependencies to Cargo.toml\n   2. Implement the TODO items in the adapter\n   3. Write tests for the adapter\n   4. Integrate adapter in your application setup\n\n   ## Testing:\n   ```bash\n   cargo test\n   ```\n\n   ## Example Integration:\n   [Show integration example from step 6]\n   ```\n\n## Technology Templates\n\nMaintain templates for common technologies:\n- PostgreSQL, MySQL, SQLite (via sqlx)\n- MongoDB (via mongodb crate)\n- Redis (via redis crate)\n- HTTP clients (via reqwest)\n- gRPC (via tonic)\n- InMemory (HashMap/RwLock)\n- Mock (for testing)\n\n## Important Notes\n\n- Always implement `Send + Sync` for thread safety\n- Use appropriate error mapping from library errors to port errors\n- Include comprehensive tests\n- Add documentation comments\n- Follow Rust async best practices\n- Consider connection pooling for database adapters\n\n## After Completion\n\nAsk the user if they want to:\n1. Create another adapter for the same port (e.g., a test double)\n2. Add more methods to the adapter\n3. Create integration tests"
              },
              {
                "name": "/rust-hex-add-port",
                "description": "Add a new port (interface) to your hexagonal architecture",
                "path": "plugins/rust-hexagonal/commands/rust-hex-add-port.md",
                "frontmatter": {
                  "description": "Add a new port (interface) to your hexagonal architecture"
                },
                "content": "You are helping add a new port (interface/trait) to a Rust hexagonal architecture project.\n\n## Your Task\n\nGuide the user through creating a new port trait and provide the implementation scaffold.\n\n## Steps\n\n1. **Ask User for Port Details**\n\n   Ask the following questions (if not already provided):\n   - Port name (e.g., \"UserRepository\", \"PaymentGateway\", \"EmailService\")\n   - Port type: \"driving\" (primary - what domain offers) or \"driven\" (secondary - what domain needs)\n   - Brief description of what this port does\n   - Required methods (you can suggest common ones based on the name)\n\n2. **Determine Port Type**\n\n   **Driving Ports** (Primary):\n   - Use cases, application services\n   - What the application offers to external actors\n   - Usually have an \"execute\" or similar method\n   - Example: `CreateUserUseCase`, `GetOrderDetails`\n\n   **Driven Ports** (Secondary):\n   - Repositories, gateways, external services\n   - What the domain needs from infrastructure\n   - Usually CRUD or external API operations\n   - Example: `UserRepository`, `EmailGateway`, `PaymentService`\n\n3. **Create the Port Trait**\n\n   Based on the port type, create the trait in the appropriate file.\n\n   **For Driven Ports** (in `src/ports/driven.rs`):\n   ```rust\n   /// [Description of what this port does]\n   ///\n   /// This port is implemented by adapters that provide [functionality].\n   #[async_trait]\n   pub trait [PortName]: Send + Sync {\n       /// [Method description]\n       async fn [method_name](&self, [params]) -> Result<[ReturnType], [ErrorType]>;\n\n       // Add more methods as needed\n   }\n\n   /// Error type for [PortName]\n   #[derive(Debug, thiserror::Error)]\n   pub enum [PortName]Error {\n       #[error(\"Not found: {0}\")]\n       NotFound(String),\n\n       #[error(\"Operation failed: {0}\")]\n       OperationFailed(String),\n\n       #[error(\"Unknown error: {0}\")]\n       Unknown(String),\n   }\n   ```\n\n   **For Driving Ports** (in `src/ports/driving.rs`):\n   ```rust\n   /// [Description of use case]\n   ///\n   /// This use case [what it does for the user].\n   #[async_trait]\n   pub trait [UseCaseName]: Send + Sync {\n       async fn execute(&self, input: [UseCaseName]Input) -> Result<[UseCaseName]Output, [UseCaseName]Error>;\n   }\n\n   /// Input for [UseCaseName]\n   #[derive(Debug, Clone)]\n   pub struct [UseCaseName]Input {\n       // Input fields\n   }\n\n   /// Output for [UseCaseName]\n   #[derive(Debug, Clone)]\n   pub struct [UseCaseName]Output {\n       // Output fields\n   }\n\n   /// Error type for [UseCaseName]\n   #[derive(Debug, thiserror::Error)]\n   pub enum [UseCaseName]Error {\n       #[error(\"Invalid input: {0}\")]\n       InvalidInput(String),\n\n       #[error(\"Use case failed: {0}\")]\n       Failed(String),\n   }\n   ```\n\n4. **Common Port Patterns**\n\n   Suggest appropriate methods based on the port name:\n\n   **Repository Patterns**:\n   ```rust\n   async fn find_by_id(&self, id: &str) -> Result<Entity, Error>;\n   async fn find_all(&self) -> Result<Vec<Entity>, Error>;\n   async fn save(&self, entity: &Entity) -> Result<(), Error>;\n   async fn update(&self, entity: &Entity) -> Result<(), Error>;\n   async fn delete(&self, id: &str) -> Result<(), Error>;\n   ```\n\n   **Service/Gateway Patterns**:\n   ```rust\n   async fn send(&self, data: &Data) -> Result<Response, Error>;\n   async fn query(&self, params: QueryParams) -> Result<QueryResult, Error>;\n   ```\n\n   **Cache Patterns**:\n   ```rust\n   async fn get(&self, key: &str) -> Result<Option<Value>, Error>;\n   async fn set(&self, key: &str, value: Value) -> Result<(), Error>;\n   async fn delete(&self, key: &str) -> Result<(), Error>;\n   ```\n\n5. **Create Adapter Scaffold**\n\n   After creating the port, automatically create a scaffold for an adapter implementation:\n\n   **For Driven Ports** (create in `src/adapters/driven/`):\n   ```rust\n   //! [PortName] adapter implementation\n   //!\n   //! This adapter implements the [PortName] port using [technology].\n\n   use crate::ports::driven::[PortName];\n   use async_trait::async_trait;\n\n   /// [Technology] implementation of [PortName]\n   pub struct [TechnologyName][PortName] {\n       // Configuration fields\n       // e.g., connection pool, client, config\n   }\n\n   impl [TechnologyName][PortName] {\n       pub fn new(/* config params */) -> Self {\n           Self {\n               // Initialize fields\n           }\n       }\n   }\n\n   #[async_trait]\n   impl [PortName] for [TechnologyName][PortName] {\n       async fn [method_name](&self, [params]) -> Result<[ReturnType], [ErrorType]> {\n           // TODO: Implement using [technology]\n           todo!(\"Implement [method_name]\")\n       }\n   }\n\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       #[tokio::test]\n       async fn test_[method_name]() {\n           // TODO: Add tests\n       }\n   }\n   ```\n\n6. **Update Module Exports**\n\n   Add the new port to the appropriate module file:\n\n   For driven ports, add to `src/ports/driven.rs`:\n   ```rust\n   pub use self::[port_name]::*;\n   mod [port_name];\n   ```\n\n   Or add to the existing file if it's small.\n\n   For adapters, add to `src/adapters/driven/mod.rs`:\n   ```rust\n   pub mod [adapter_name];\n   ```\n\n7. **Provide Usage Example**\n\n   Show the user how to use the new port:\n\n   ```rust\n   // In a domain service\n   use crate::ports::driven::[PortName];\n\n   pub struct MyService<R>\n   where\n       R: [PortName],\n   {\n       [port_field]: R,\n   }\n\n   impl<R> MyService<R>\n   where\n       R: [PortName],\n   {\n       pub fn new([port_field]: R) -> Self {\n           Self { [port_field] }\n       }\n\n       pub async fn do_something(&self) -> Result<(), Error> {\n           self.[port_field].[method]().await?;\n           Ok(())\n       }\n   }\n   ```\n\n8. **Suggest Next Steps**\n\n   Tell the user:\n   ```\n   âœ… Port '[PortName]' created successfully!\n\n   ## Files Created/Modified:\n   - `src/ports/[driving|driven].rs` - Port trait definition\n   - `src/adapters/[driving|driven]/[adapter_name].rs` - Adapter scaffold\n\n   ## Next Steps:\n\n   1. Review the port trait and adjust methods as needed\n   2. Implement the adapter for your specific technology\n   3. Add the adapter to your application's dependency injection\n   4. Write tests for the adapter\n\n   ## Example Usage:\n   [Show usage example from step 7]\n\n   ## Implement Adapter:\n   To implement the adapter for a specific technology (e.g., PostgreSQL, HTTP):\n   - Use `/rust-hex-add-adapter` to create additional implementations\n   - Or manually edit `src/adapters/[driving|driven]/[adapter_name].rs`\n   ```\n\n## Port Naming Conventions\n\n- **Repositories**: `[Entity]Repository` (e.g., `UserRepository`, `OrderRepository`)\n- **Gateways**: `[Service]Gateway` (e.g., `PaymentGateway`, `EmailGateway`)\n- **Services**: `[Domain]Service` (e.g., `AuthenticationService`, `NotificationService`)\n- **Use Cases**: `[Action][Entity]` (e.g., `CreateUser`, `GetOrderDetails`)\n\n## Important Notes\n\n- Always use `#[async_trait]` for async trait methods\n- Include `Send + Sync` bounds for thread safety\n- Define custom error types using `thiserror`\n- Add documentation comments explaining the port's purpose\n- Follow Rust naming conventions (PascalCase for traits, snake_case for methods)\n\n## After Completion\n\nConfirm with the user that the port was created successfully and ask if they want to:\n1. Add more methods to the port\n2. Create additional adapters for this port\n3. Create another port"
              },
              {
                "name": "/rust-hex-init",
                "description": "Initialize a hexagonal architecture project structure for Rust",
                "path": "plugins/rust-hexagonal/commands/rust-hex-init.md",
                "frontmatter": {
                  "description": "Initialize a hexagonal architecture project structure for Rust"
                },
                "content": "You are helping initialize a Rust project with hexagonal architecture (ports and adapters pattern).\n\n## Your Task\n\nCreate a complete hexagonal architecture directory structure with example files to help the user get started.\n\n## Steps\n\n1. **Verify or Create Base Directory Structure**\n\n   Create the following structure:\n   ```\n   src/\n   â”œâ”€â”€ domain/\n   â”‚   â”œâ”€â”€ mod.rs\n   â”‚   â”œâ”€â”€ models.rs\n   â”‚   â””â”€â”€ services.rs\n   â”œâ”€â”€ ports/\n   â”‚   â”œâ”€â”€ mod.rs\n   â”‚   â”œâ”€â”€ driving.rs\n   â”‚   â””â”€â”€ driven.rs\n   â”œâ”€â”€ adapters/\n   â”‚   â”œâ”€â”€ mod.rs\n   â”‚   â”œâ”€â”€ driving/\n   â”‚   â”‚   â””â”€â”€ mod.rs\n   â”‚   â””â”€â”€ driven/\n   â”‚       â””â”€â”€ mod.rs\n   â””â”€â”€ lib.rs (or main.rs if it exists)\n   ```\n\n2. **Create Domain Layer Files**\n\n   **src/domain/mod.rs**:\n   ```rust\n   //! Domain layer - Core business logic\n   //!\n   //! This layer contains:\n   //! - Domain models (entities, value objects)\n   //! - Business rules and validations\n   //! - Domain services\n   //!\n   //! The domain layer has NO dependencies on ports or adapters.\n\n   pub mod models;\n   pub mod services;\n   ```\n\n   **src/domain/models.rs**:\n   ```rust\n   //! Domain models and entities\n   //!\n   //! Define your business entities here with their behaviors.\n\n   use serde::{Deserialize, Serialize};\n\n   /// Example domain entity\n   #[derive(Debug, Clone, Serialize, Deserialize)]\n   pub struct ExampleEntity {\n       id: String,\n       name: String,\n   }\n\n   impl ExampleEntity {\n       pub fn new(id: String, name: String) -> Result<Self, ValidationError> {\n           if name.is_empty() {\n               return Err(ValidationError::EmptyName);\n           }\n           Ok(Self { id, name })\n       }\n\n       pub fn id(&self) -> &str {\n           &self.id\n       }\n\n       pub fn name(&self) -> &str {\n           &self.name\n       }\n   }\n\n   #[derive(Debug, thiserror::Error)]\n   pub enum ValidationError {\n       #[error(\"Name cannot be empty\")]\n       EmptyName,\n   }\n   ```\n\n   **src/domain/services.rs**:\n   ```rust\n   //! Domain services - Business logic orchestration\n   //!\n   //! Domain services coordinate between entities and use ports\n   //! for external dependencies.\n\n   use super::models::{ExampleEntity, ValidationError};\n   use crate::ports::driven::ExampleRepository;\n\n   /// Example domain service\n   pub struct ExampleService<R>\n   where\n       R: ExampleRepository,\n   {\n       repository: R,\n   }\n\n   impl<R> ExampleService<R>\n   where\n       R: ExampleRepository,\n   {\n       pub fn new(repository: R) -> Self {\n           Self { repository }\n       }\n\n       pub async fn get_entity(&self, id: &str) -> Result<ExampleEntity, ServiceError> {\n           self.repository\n               .find_by_id(id)\n               .await\n               .map_err(ServiceError::Repository)\n       }\n\n       pub async fn create_entity(&self, name: String) -> Result<ExampleEntity, ServiceError> {\n           let entity = ExampleEntity::new(uuid::Uuid::new_v4().to_string(), name)\n               .map_err(ServiceError::Validation)?;\n\n           self.repository\n               .save(&entity)\n               .await\n               .map_err(ServiceError::Repository)?;\n\n           Ok(entity)\n       }\n   }\n\n   #[derive(Debug, thiserror::Error)]\n   pub enum ServiceError {\n       #[error(\"Validation error: {0}\")]\n       Validation(#[from] ValidationError),\n\n       #[error(\"Repository error: {0}\")]\n       Repository(#[from] crate::ports::driven::RepositoryError),\n   }\n   ```\n\n3. **Create Ports Layer Files**\n\n   **src/ports/mod.rs**:\n   ```rust\n   //! Ports layer - Interfaces for adapters\n   //!\n   //! Ports define the contracts between the domain and the outside world:\n   //! - Driving ports: What the domain offers to the outside\n   //! - Driven ports: What the domain needs from the outside\n\n   pub mod driving;\n   pub mod driven;\n   ```\n\n   **src/ports/driving.rs**:\n   ```rust\n   //! Driving ports (Primary ports)\n   //!\n   //! These are the interfaces that the domain exposes to the outside world.\n   //! Driving adapters (like REST APIs, CLI) will use these interfaces.\n\n   use crate::domain::models::ExampleEntity;\n   use async_trait::async_trait;\n\n   /// Example driving port - what the application offers\n   #[async_trait]\n   pub trait ExampleUseCase: Send + Sync {\n       async fn execute(&self, input: UseCaseInput) -> Result<ExampleEntity, UseCaseError>;\n   }\n\n   pub struct UseCaseInput {\n       pub name: String,\n   }\n\n   #[derive(Debug, thiserror::Error)]\n   pub enum UseCaseError {\n       #[error(\"Invalid input: {0}\")]\n       InvalidInput(String),\n\n       #[error(\"Service error: {0}\")]\n       Service(#[from] crate::domain::services::ServiceError),\n   }\n   ```\n\n   **src/ports/driven.rs**:\n   ```rust\n   //! Driven ports (Secondary ports)\n   //!\n   //! These are the interfaces that the domain needs from the outside world.\n   //! Driven adapters (like database repositories, HTTP clients) implement these.\n\n   use crate::domain::models::ExampleEntity;\n   use async_trait::async_trait;\n\n   /// Example repository port - what the domain needs\n   #[async_trait]\n   pub trait ExampleRepository: Send + Sync {\n       async fn find_by_id(&self, id: &str) -> Result<ExampleEntity, RepositoryError>;\n       async fn save(&self, entity: &ExampleEntity) -> Result<(), RepositoryError>;\n       async fn delete(&self, id: &str) -> Result<(), RepositoryError>;\n   }\n\n   #[derive(Debug, thiserror::Error)]\n   pub enum RepositoryError {\n       #[error(\"Not found: {0}\")]\n       NotFound(String),\n\n       #[error(\"Database error: {0}\")]\n       Database(String),\n\n       #[error(\"Unknown error: {0}\")]\n       Unknown(String),\n   }\n   ```\n\n4. **Create Adapters Layer Files**\n\n   **src/adapters/mod.rs**:\n   ```rust\n   //! Adapters layer - Implementations of ports\n   //!\n   //! Adapters connect the domain to the outside world:\n   //! - Driving adapters: REST API, CLI, gRPC\n   //! - Driven adapters: Database, HTTP clients, file systems\n\n   pub mod driving;\n   pub mod driven;\n   ```\n\n   **src/adapters/driving/mod.rs**:\n   ```rust\n   //! Driving adapters\n   //!\n   //! These adapters expose the application to the outside world.\n   //! Examples: REST API, CLI, gRPC server, GraphQL\n\n   // Example: REST API adapter would go here\n   // pub mod rest_api;\n   ```\n\n   **src/adapters/driven/mod.rs**:\n   ```rust\n   //! Driven adapters\n   //!\n   //! These adapters implement the ports needed by the domain.\n   //! Examples: PostgreSQL repository, HTTP client, Redis cache\n\n   // Example adapter implementations would go here\n   // pub mod postgres_repository;\n   // pub mod http_client;\n   ```\n\n5. **Update lib.rs or main.rs**\n\n   Add to the top of `src/lib.rs` (or `src/main.rs` if no lib.rs exists):\n   ```rust\n   //! Hexagonal Architecture Application\n   //!\n   //! This application follows the hexagonal architecture pattern:\n   //! - Domain: Core business logic\n   //! - Ports: Interfaces (traits)\n   //! - Adapters: Implementations\n\n   pub mod domain;\n   pub mod ports;\n   pub mod adapters;\n   ```\n\n6. **Update Cargo.toml**\n\n   Ensure the following dependencies are in Cargo.toml:\n   ```toml\n   [dependencies]\n   # Async runtime\n   tokio = { version = \"1\", features = [\"full\"] }\n   async-trait = \"0.1\"\n\n   # Error handling\n   thiserror = \"1.0\"\n   anyhow = \"1.0\"\n\n   # Serialization\n   serde = { version = \"1.0\", features = [\"derive\"] }\n   serde_json = \"1.0\"\n\n   # UUID generation\n   uuid = { version = \"1.0\", features = [\"v4\", \"serde\"] }\n\n   # Example: Database (uncomment if needed)\n   # sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-native-tls\"] }\n\n   # Example: HTTP (uncomment if needed)\n   # axum = \"0.7\"\n   # reqwest = { version = \"0.11\", features = [\"json\"] }\n   ```\n\n7. **Provide Usage Instructions**\n\n   After creating all files, tell the user:\n   ```\n   âœ… Hexagonal architecture structure initialized!\n\n   ## Next Steps:\n\n   1. Review the generated structure in `src/`\n   2. Define your domain models in `src/domain/models.rs`\n   3. Implement business logic in `src/domain/services.rs`\n   4. Create port traits in `src/ports/` for external dependencies\n   5. Implement adapters in `src/adapters/` for each port\n\n   ## Example Commands:\n   - Add a new port: `/rust-hex-add-port`\n   - Add an adapter: `/rust-hex-add-adapter`\n   - Get architecture help: Ask the `rust-hex-architect` agent\n\n   ## Running the Code:\n   ```bash\n   cargo build\n   cargo test\n   ```\n\n   ## Architecture Layers:\n   - **Domain**: Pure business logic (no external dependencies)\n   - **Ports**: Trait definitions (interfaces)\n   - **Adapters**: Concrete implementations\n\n   Dependencies flow: Adapters â†’ Ports â†’ Domain\n   ```\n\n## Important Notes\n\n- Create directories only if they don't exist\n- Don't overwrite existing files without asking the user first\n- If files already exist, ask if they want to merge or skip\n- Use proper Rust formatting and conventions\n- Add helpful comments explaining the architecture\n\n## After Completion\n\nTell the user about the structure created and suggest next steps for their specific use case."
              }
            ],
            "skills": [
              {
                "name": "domain-layer-expert",
                "description": "Guides users in creating rich domain models with behavior, value objects, and domain logic. Activates when users define domain entities, business rules, or validation logic.",
                "path": "plugins/rust-hexagonal/skills/domain-layer-expert/SKILL.md",
                "frontmatter": {
                  "name": "domain-layer-expert",
                  "description": "Guides users in creating rich domain models with behavior, value objects, and domain logic. Activates when users define domain entities, business rules, or validation logic.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Domain Layer Expert Skill\n\nYou are an expert at designing rich domain models in Rust. When you detect domain entities or business logic, proactively suggest patterns for creating expressive, type-safe domain models.\n\n## When to Activate\n\nActivate when you notice:\n- Entity or value object definitions\n- Business validation logic\n- Domain rules implementation\n- Anemic domain models (just data, no behavior)\n- Primitive obsession (using String/i64 for domain concepts)\n\n## Domain Model Patterns\n\n### Pattern 1: Value Objects\n\n```rust\n// âœ… Value object with validation\n#[derive(Debug, Clone, PartialEq, Eq)]\npub struct Email(String);\n\nimpl Email {\n    pub fn new(email: String) -> Result<Self, ValidationError> {\n        if !email.contains('@') {\n            return Err(ValidationError::InvalidEmail(\"Missing @ symbol\".into()));\n        }\n        if email.len() > 255 {\n            return Err(ValidationError::InvalidEmail(\"Too long\".into()));\n        }\n        Ok(Self(email))\n    }\n\n    pub fn as_str(&self) -> &str {\n        &self.0\n    }\n}\n\n// Implement TryFrom for ergonomics\nimpl TryFrom<String> for Email {\n    type Error = ValidationError;\n\n    fn try_from(s: String) -> Result<Self, Self::Error> {\n        Self::new(s)\n    }\n}\n```\n\n### Pattern 2: Entity with Identity\n\n```rust\n#[derive(Debug, Clone)]\npub struct User {\n    id: UserId,\n    email: Email,\n    name: String,\n    status: UserStatus,\n}\n\nimpl User {\n    pub fn new(email: Email, name: String) -> Self {\n        Self {\n            id: UserId::generate(),\n            email,\n            name,\n            status: UserStatus::Active,\n        }\n    }\n\n    // Domain behavior\n    pub fn deactivate(&mut self) -> Result<(), DomainError> {\n        if self.status == UserStatus::Deleted {\n            return Err(DomainError::UserAlreadyDeleted);\n        }\n        self.status = UserStatus::Inactive;\n        Ok(())\n    }\n\n    pub fn change_email(&mut self, new_email: Email) -> Result<(), DomainError> {\n        if self.status != UserStatus::Active {\n            return Err(DomainError::UserNotActive);\n        }\n        self.email = new_email;\n        Ok(())\n    }\n\n    // Getters\n    pub fn id(&self) -> &UserId { &self.id }\n    pub fn email(&self) -> &Email { &self.email }\n}\n```\n\n### Pattern 3: Domain Events\n\n```rust\n#[derive(Debug, Clone)]\npub enum UserEvent {\n    UserCreated { id: UserId, email: Email },\n    UserDeactivated { id: UserId },\n    EmailChanged { id: UserId, old_email: Email, new_email: Email },\n}\n\npub struct User {\n    id: UserId,\n    email: Email,\n    events: Vec<UserEvent>,\n}\n\nimpl User {\n    pub fn new(email: Email) -> Self {\n        let id = UserId::generate();\n        let mut user = Self {\n            id: id.clone(),\n            email: email.clone(),\n            events: vec![],\n        };\n        user.record_event(UserEvent::UserCreated { id, email });\n        user\n    }\n\n    pub fn change_email(&mut self, new_email: Email) -> Result<(), DomainError> {\n        let old_email = self.email.clone();\n        self.email = new_email.clone();\n        self.record_event(UserEvent::EmailChanged {\n            id: self.id.clone(),\n            old_email,\n            new_email,\n        });\n        Ok(())\n    }\n\n    pub fn take_events(&mut self) -> Vec<UserEvent> {\n        std::mem::take(&mut self.events)\n    }\n\n    fn record_event(&mut self, event: UserEvent) {\n        self.events.push(event);\n    }\n}\n```\n\n### Pattern 4: Business Rules\n\n```rust\npub struct Order {\n    id: OrderId,\n    items: Vec<OrderItem>,\n    status: OrderStatus,\n    total: Money,\n}\n\nimpl Order {\n    pub fn new(items: Vec<OrderItem>) -> Result<Self, DomainError> {\n        if items.is_empty() {\n            return Err(DomainError::EmptyOrder);\n        }\n\n        let total = items.iter().map(|item| item.total()).sum();\n\n        Ok(Self {\n            id: OrderId::generate(),\n            items,\n            status: OrderStatus::Pending,\n            total,\n        })\n    }\n\n    pub fn add_item(&mut self, item: OrderItem) -> Result<(), DomainError> {\n        if self.status != OrderStatus::Pending {\n            return Err(DomainError::OrderNotEditable);\n        }\n\n        self.items.push(item.clone());\n        self.total = self.total + item.total();\n        Ok(())\n    }\n\n    pub fn confirm(&mut self) -> Result<(), DomainError> {\n        if self.status != OrderStatus::Pending {\n            return Err(DomainError::OrderAlreadyConfirmed);\n        }\n\n        if self.total < Money::dollars(10) {\n            return Err(DomainError::MinimumOrderNotMet);\n        }\n\n        self.status = OrderStatus::Confirmed;\n        Ok(())\n    }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### âŒ Primitive Obsession\n\n```rust\n// BAD: Using primitives everywhere\npub struct User {\n    pub id: String,\n    pub email: String,\n    pub age: i32,\n}\n\nfn create_user(email: String, age: i32) -> User {\n    // No validation, easy to pass wrong data\n}\n\n// GOOD: Domain types\npub struct User {\n    id: UserId,\n    email: Email,\n    age: Age,\n}\n\nimpl User {\n    pub fn new(email: Email, age: Age) -> Result<Self, DomainError> {\n        // Validation already done in Email and Age types\n        Ok(Self {\n            id: UserId::generate(),\n            email,\n            age,\n        })\n    }\n}\n```\n\n### âŒ Anemic Domain Model\n\n```rust\n// BAD: Domain is just data\npub struct User {\n    pub id: String,\n    pub email: String,\n    pub status: String,\n}\n\n// Business logic in service layer\nimpl UserService {\n    pub fn deactivate_user(&self, user: &mut User) {\n        user.status = \"inactive\".to_string();\n    }\n}\n\n// GOOD: Domain has behavior\npub struct User {\n    id: UserId,\n    email: Email,\n    status: UserStatus,\n}\n\nimpl User {\n    pub fn deactivate(&mut self) -> Result<(), DomainError> {\n        if self.status == UserStatus::Deleted {\n            return Err(DomainError::UserAlreadyDeleted);\n        }\n        self.status = UserStatus::Inactive;\n        Ok(())\n    }\n}\n```\n\n## Your Approach\n\nWhen you see domain models:\n1. Check for primitive obsession\n2. Suggest value objects for domain concepts\n3. Move validation into domain types\n4. Add behavior methods to entities\n5. Ensure immutability where appropriate\n\nProactively suggest rich domain patterns when you detect anemic models or primitive obsession."
              },
              {
                "name": "hexagonal-advisor",
                "description": "Reviews code architecture for hexagonal patterns, checks dependency directions, and suggests improvements for ports and adapters separation. Activates when users work with services, repositories, or architectural patterns.",
                "path": "plugins/rust-hexagonal/skills/hexagonal-advisor/SKILL.md",
                "frontmatter": {
                  "name": "hexagonal-advisor",
                  "description": "Reviews code architecture for hexagonal patterns, checks dependency directions, and suggests improvements for ports and adapters separation. Activates when users work with services, repositories, or architectural patterns.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Hexagonal Architecture Advisor Skill\n\nYou are an expert at hexagonal architecture (ports and adapters) in Rust. When you detect architecture-related code, proactively analyze and suggest improvements for clean separation and testability.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Service or repository trait definitions\n- Domain logic mixed with infrastructure concerns\n- Direct database or HTTP client usage in business logic\n- Questions about architecture, testing, or dependency injection\n- Code that's hard to test due to tight coupling\n\n## Architecture Checklist\n\n### 1. Dependency Direction\n\n**What to Look For**:\n- Domain depending on infrastructure\n- Business logic coupled to frameworks\n- Inverted dependencies\n\n**Bad Pattern**:\n```rust\n// âŒ Domain depends on infrastructure (Postgres)\npub struct UserService {\n    db: PgPool,  // Direct dependency on PostgreSQL\n}\n\nimpl UserService {\n    pub async fn create_user(&self, email: &str) -> Result<User, Error> {\n        // Domain logic mixed with SQL\n        sqlx::query(\"INSERT INTO users...\")\n            .execute(&self.db)\n            .await?;\n        Ok(user)\n    }\n}\n```\n\n**Good Pattern**:\n```rust\n// âœ… Domain depends only on port trait\n#[async_trait]\npub trait UserRepository: Send + Sync {\n    async fn save(&self, user: &User) -> Result<(), DomainError>;\n    async fn find(&self, id: &UserId) -> Result<User, DomainError>;\n}\n\npub struct UserService<R: UserRepository> {\n    repo: R,  // Depends on abstraction\n}\n\nimpl<R: UserRepository> UserService<R> {\n    pub fn new(repo: R) -> Self {\n        Self { repo }\n    }\n\n    pub async fn create_user(&self, email: &str) -> Result<User, DomainError> {\n        let user = User::new(email)?;  // Domain validation\n        self.repo.save(&user).await?;  // Infrastructure through port\n        Ok(user)\n    }\n}\n```\n\n**Suggestion Template**:\n```\nYour domain logic directly depends on infrastructure. Create a port trait instead:\n\n#[async_trait]\npub trait UserRepository: Send + Sync {\n    async fn save(&self, user: &User) -> Result<(), DomainError>;\n}\n\npub struct UserService<R: UserRepository> {\n    repo: R,\n}\n\nThis allows you to:\n- Test with mock implementations\n- Swap implementations without changing domain\n- Keep domain pure and framework-agnostic\n```\n\n### 2. Port Definitions\n\n**What to Look For**:\n- Missing trait abstractions for external dependencies\n- Concrete types in domain services\n- Inconsistent port patterns\n\n**Good Port Patterns**:\n```rust\n// Driven Port (Secondary) - What domain needs\n#[async_trait]\npub trait UserRepository: Send + Sync {\n    async fn find(&self, id: &UserId) -> Result<User, DomainError>;\n    async fn save(&self, user: &User) -> Result<(), DomainError>;\n    async fn delete(&self, id: &UserId) -> Result<(), DomainError>;\n}\n\n// Driven Port for external services\n#[async_trait]\npub trait EmailService: Send + Sync {\n    async fn send_welcome_email(&self, user: &User) -> Result<(), DomainError>;\n}\n\n// Driving Port (Primary) - What domain exposes\n#[async_trait]\npub trait UserManagement: Send + Sync {\n    async fn register_user(&self, email: &str) -> Result<User, DomainError>;\n    async fn get_user(&self, id: &UserId) -> Result<User, DomainError>;\n}\n```\n\n**Suggestion Template**:\n```\nDefine clear port traits for your external dependencies:\n\n// What your domain needs (driven port)\n#[async_trait]\npub trait Repository: Send + Sync {\n    async fn operation(&self) -> Result<Data, Error>;\n}\n\n// What your domain exposes (driving port)\n#[async_trait]\npub trait Service: Send + Sync {\n    async fn business_operation(&self) -> Result<Output, Error>;\n}\n```\n\n### 3. Domain Purity\n\n**What to Look For**:\n- Framework types in domain models\n- SQL, HTTP, or file I/O in domain logic\n- Domain models with derive macros for serialization\n\n**Bad Pattern**:\n```rust\n// âŒ Domain model coupled to frameworks\nuse sqlx::FromRow;\nuse serde::{Serialize, Deserialize};\n\n#[derive(FromRow, Serialize, Deserialize)]  // âŒ Infrastructure concerns\npub struct User {\n    pub id: i64,  // âŒ Database type leaking\n    pub email: String,\n    pub created_at: chrono::DateTime<chrono::Utc>,  // âŒ chrono in domain\n}\n```\n\n**Good Pattern**:\n```rust\n// âœ… Pure domain model\npub struct User {\n    id: UserId,  // Domain type\n    email: Email,  // Domain value object\n}\n\nimpl User {\n    pub fn new(email: String) -> Result<Self, ValidationError> {\n        let email = Email::try_from(email)?;  // Domain validation\n        Ok(Self {\n            id: UserId::generate(),\n            email,\n        })\n    }\n\n    pub fn email(&self) -> &Email {\n        &self.email\n    }\n}\n\n// Adapter layer handles persistence\n#[derive(sqlx::FromRow)]\nstruct UserRow {\n    id: i64,\n    email: String,\n}\n\nimpl From<UserRow> for User {\n    fn from(row: UserRow) -> Self {\n        // Conversion in adapter layer\n    }\n}\n```\n\n**Suggestion Template**:\n```\nKeep your domain models pure and framework-agnostic:\n\n// Domain layer - no framework dependencies\npub struct User {\n    id: UserId,\n    email: Email,\n}\n\n// Adapter layer - handles framework concerns\n#[derive(sqlx::FromRow)]\nstruct UserRow {\n    id: i64,\n    email: String,\n}\n\nimpl From<UserRow> for User {\n    fn from(row: UserRow) -> Self {\n        // Convert database representation to domain\n    }\n}\n```\n\n### 4. Adapter Implementation\n\n**What to Look For**:\n- Adapters not implementing ports\n- Business logic in adapters\n- Missing adapter layer\n\n**Good Adapter Pattern**:\n```rust\npub struct PostgresUserRepository {\n    pool: PgPool,\n}\n\n#[async_trait]\nimpl UserRepository for PostgresUserRepository {\n    async fn save(&self, user: &User) -> Result<(), DomainError> {\n        let row = UserRow::from(user);  // Domain â†’ Infrastructure\n\n        sqlx::query!(\n            \"INSERT INTO users (id, email) VALUES ($1, $2)\",\n            row.id,\n            row.email\n        )\n        .execute(&self.pool)\n        .await\n        .map_err(|e| DomainError::RepositoryError(e.to_string()))?;\n\n        Ok(())\n    }\n\n    async fn find(&self, id: &UserId) -> Result<User, DomainError> {\n        let row = sqlx::query_as!(\n            UserRow,\n            \"SELECT id, email FROM users WHERE id = $1\",\n            id.value()\n        )\n        .fetch_one(&self.pool)\n        .await\n        .map_err(|e| match e {\n            sqlx::Error::RowNotFound => DomainError::UserNotFound(id.to_string()),\n            _ => DomainError::RepositoryError(e.to_string()),\n        })?;\n\n        Ok(User::from(row))  // Infrastructure â†’ Domain\n    }\n}\n```\n\n**Suggestion Template**:\n```\nImplement your ports in the adapter layer:\n\npub struct PostgresRepo {\n    pool: PgPool,\n}\n\n#[async_trait]\nimpl MyPort for PostgresRepo {\n    async fn operation(&self, data: &DomainType) -> Result<(), Error> {\n        // Convert domain â†’ infrastructure\n        let row = DbRow::from(data);\n\n        // Perform infrastructure operation\n        sqlx::query!(\"...\").execute(&self.pool).await?;\n\n        Ok(())\n    }\n}\n```\n\n### 5. Testing Strategy\n\n**What to Look For**:\n- Lack of test doubles\n- Tests requiring real database\n- Untestable domain logic\n\n**Good Testing Pattern**:\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n\n    // Mock repository for testing\n    struct MockUserRepository {\n        users: HashMap<UserId, User>,\n    }\n\n    impl MockUserRepository {\n        fn new() -> Self {\n            Self { users: HashMap::new() }\n        }\n\n        fn with_user(mut self, user: User) -> Self {\n            self.users.insert(user.id().clone(), user);\n            self\n        }\n    }\n\n    #[async_trait]\n    impl UserRepository for MockUserRepository {\n        async fn save(&self, user: &User) -> Result<(), DomainError> {\n            // Mock implementation\n            Ok(())\n        }\n\n        async fn find(&self, id: &UserId) -> Result<User, DomainError> {\n            self.users\n                .get(id)\n                .cloned()\n                .ok_or(DomainError::UserNotFound(id.to_string()))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_create_user() {\n        // Arrange\n        let mock_repo = MockUserRepository::new();\n        let service = UserService::new(mock_repo);\n\n        // Act\n        let result = service.create_user(\"test@example.com\").await;\n\n        // Assert\n        assert!(result.is_ok());\n    }\n}\n```\n\n**Suggestion Template**:\n```\nCreate mock implementations for testing:\n\n#[cfg(test)]\nmod tests {\n    struct MockRepository {\n        // Test state\n    }\n\n    #[async_trait]\n    impl MyPort for MockRepository {\n        async fn operation(&self) -> Result<Data, Error> {\n            // Mock behavior\n            Ok(test_data())\n        }\n    }\n\n    #[tokio::test]\n    async fn test_domain_logic() {\n        let mock = MockRepository::new();\n        let service = MyService::new(mock);\n\n        let result = service.business_operation().await;\n\n        assert!(result.is_ok());\n    }\n}\n```\n\n### 6. Composition Root\n\n**What to Look For**:\n- Dependency construction scattered throughout code\n- Missing application composition\n- Unclear wiring\n\n**Good Pattern**:\n```rust\n// Application composition root\npub struct Application {\n    user_service: Arc<UserService<PostgresUserRepository>>,\n    order_service: Arc<OrderService<PostgresOrderRepository>>,\n}\n\nimpl Application {\n    pub async fn new(config: &Config) -> Result<Self, Error> {\n        // Infrastructure setup\n        let pool = PgPoolOptions::new()\n            .max_connections(5)\n            .connect(&config.database_url)\n            .await?;\n\n        // Adapter construction\n        let user_repo = PostgresUserRepository::new(pool.clone());\n        let order_repo = PostgresOrderRepository::new(pool.clone());\n\n        // Service construction with dependencies\n        let user_service = Arc::new(UserService::new(user_repo));\n        let order_service = Arc::new(OrderService::new(order_repo));\n\n        Ok(Self {\n            user_service,\n            order_service,\n        })\n    }\n\n    pub fn user_service(&self) -> Arc<UserService<PostgresUserRepository>> {\n        self.user_service.clone()\n    }\n}\n\n// Main function\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    let config = load_config()?;\n    let app = Application::new(&config).await?;\n\n    // Wire up HTTP handlers with services\n    let router = Router::new()\n        .route(\"/users\", post(create_user_handler))\n        .with_state(app);\n\n    // Start server\n    axum::Server::bind(&\"0.0.0.0:3000\".parse()?)\n        .serve(router.into_make_service())\n        .await?;\n\n    Ok(())\n}\n```\n\n**Suggestion Template**:\n```\nCreate a composition root that wires all dependencies:\n\npub struct Application {\n    services: /* your services */\n}\n\nimpl Application {\n    pub async fn new(config: &Config) -> Result<Self, Error> {\n        // 1. Setup infrastructure\n        let pool = create_pool(&config).await?;\n\n        // 2. Create adapters\n        let repo = PostgresRepo::new(pool);\n\n        // 3. Create services with dependencies\n        let service = MyService::new(repo);\n\n        Ok(Self { service })\n    }\n}\n```\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: Anemic Domain\n\n```rust\n// âŒ BAD: Domain is just data, no behavior\npub struct User {\n    pub id: String,\n    pub email: String,\n}\n\n// Business logic in service instead of domain\nimpl UserService {\n    pub fn validate_email(&self, email: &str) -> bool {\n        email.contains('@')  // Should be in domain\n    }\n}\n\n// âœ… GOOD: Domain has behavior\npub struct User {\n    id: UserId,\n    email: Email,  // Email is a value object with validation\n}\n\nimpl Email {\n    pub fn try_from(s: String) -> Result<Self, ValidationError> {\n        if !s.contains('@') {\n            return Err(ValidationError::InvalidEmail);\n        }\n        Ok(Self(s))\n    }\n}\n```\n\n### Anti-Pattern 2: Leaky Abstractions\n\n```rust\n// âŒ BAD: Infrastructure details leak through port\n#[async_trait]\npub trait UserRepository {\n    async fn find(&self, id: i64) -> Result<UserRow, sqlx::Error>;\n    //                           ^^^        ^^^^^^^  ^^^^^^^^^^^\n    //                    Database type    DB struct  DB error\n}\n\n// âœ… GOOD: Port uses domain types only\n#[async_trait]\npub trait UserRepository {\n    async fn find(&self, id: &UserId) -> Result<User, DomainError>;\n    //                        ^^^^^^^          ^^^^  ^^^^^^^^^^^\n    //                   Domain type      Domain    Domain error\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify architecture-related code patterns\n2. **Analyze**: Check dependency direction and separation\n3. **Suggest**: Provide specific refactoring steps\n4. **Explain**: Benefits of hexagonal architecture\n\n## Communication Style\n\n- Focus on dependency inversion principle\n- Emphasize testability benefits\n- Provide complete examples with traits and implementations\n- Explain the \"why\" behind the pattern\n- Suggest incremental refactoring steps\n\nWhen you detect architectural issues, proactively suggest hexagonal patterns that will improve testability, maintainability, and flexibility."
              },
              {
                "name": "port-adapter-designer",
                "description": "Helps design port traits and adapter implementations for external dependencies. Activates when users need to abstract away databases, APIs, or other external systems.",
                "path": "plugins/rust-hexagonal/skills/port-adapter-designer/SKILL.md",
                "frontmatter": {
                  "name": "port-adapter-designer",
                  "description": "Helps design port traits and adapter implementations for external dependencies. Activates when users need to abstract away databases, APIs, or other external systems.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Port and Adapter Designer Skill\n\nYou are an expert at designing ports (trait abstractions) and adapters (implementations) for hexagonal architecture in Rust. When you detect external dependencies or integration needs, proactively suggest port/adapter patterns.\n\n## When to Activate\n\nActivate when you notice:\n- Direct usage of databases, HTTP clients, or file systems\n- Need to swap implementations for testing\n- External service integrations\n- Questions about abstraction or dependency injection\n\n## Port Design Patterns\n\n### Pattern 1: Repository Port\n\n```rust\n#[async_trait]\npub trait UserRepository: Send + Sync {\n    async fn find_by_id(&self, id: &UserId) -> Result<User, RepositoryError>;\n    async fn find_by_email(&self, email: &Email) -> Result<User, RepositoryError>;\n    async fn save(&self, user: &User) -> Result<(), RepositoryError>;\n    async fn delete(&self, id: &UserId) -> Result<(), RepositoryError>;\n    async fn list(&self, limit: usize, offset: usize) -> Result<Vec<User>, RepositoryError>;\n}\n```\n\n### Pattern 2: External Service Port\n\n```rust\n#[async_trait]\npub trait PaymentGateway: Send + Sync {\n    async fn process_payment(&self, amount: Money, card: &CardDetails) -> Result<PaymentId, PaymentError>;\n    async fn refund(&self, payment_id: &PaymentId) -> Result<RefundId, PaymentError>;\n    async fn get_status(&self, payment_id: &PaymentId) -> Result<PaymentStatus, PaymentError>;\n}\n```\n\n### Pattern 3: Notification Port\n\n```rust\n#[async_trait]\npub trait NotificationService: Send + Sync {\n    async fn send_email(&self, to: &Email, subject: &str, body: &str) -> Result<(), NotificationError>;\n    async fn send_sms(&self, phone: &PhoneNumber, message: &str) -> Result<(), NotificationError>;\n}\n```\n\n## Adapter Implementation Patterns\n\n### PostgreSQL Adapter\n\n```rust\npub struct PostgresUserRepository {\n    pool: PgPool,\n}\n\nimpl PostgresUserRepository {\n    pub fn new(pool: PgPool) -> Self {\n        Self { pool }\n    }\n}\n\n#[async_trait]\nimpl UserRepository for PostgresUserRepository {\n    async fn find_by_id(&self, id: &UserId) -> Result<User, RepositoryError> {\n        let row = sqlx::query_as!(\n            UserRow,\n            \"SELECT id, email, name FROM users WHERE id = $1\",\n            id.as_str()\n        )\n        .fetch_one(&self.pool)\n        .await\n        .map_err(|e| match e {\n            sqlx::Error::RowNotFound => RepositoryError::NotFound,\n            _ => RepositoryError::Database(e.to_string()),\n        })?;\n\n        Ok(User::try_from(row)?)\n    }\n\n    async fn save(&self, user: &User) -> Result<(), RepositoryError> {\n        sqlx::query!(\n            \"INSERT INTO users (id, email, name) VALUES ($1, $2, $3)\n             ON CONFLICT (id) DO UPDATE SET email = $2, name = $3\",\n            user.id().as_str(),\n            user.email().as_str(),\n            user.name()\n        )\n        .execute(&self.pool)\n        .await\n        .map_err(|e| RepositoryError::Database(e.to_string()))?;\n\n        Ok(())\n    }\n}\n```\n\n### HTTP Client Adapter\n\n```rust\npub struct StripePaymentGateway {\n    client: reqwest::Client,\n    api_key: String,\n}\n\n#[async_trait]\nimpl PaymentGateway for StripePaymentGateway {\n    async fn process_payment(&self, amount: Money, card: &CardDetails) -> Result<PaymentId, PaymentError> {\n        #[derive(Serialize)]\n        struct PaymentRequest {\n            amount: u64,\n            currency: String,\n            card: CardDetailsDto,\n        }\n\n        let response = self\n            .client\n            .post(\"https://api.stripe.com/v1/charges\")\n            .bearer_auth(&self.api_key)\n            .json(&PaymentRequest {\n                amount: amount.cents(),\n                currency: amount.currency().to_string(),\n                card: CardDetailsDto::from(card),\n            })\n            .send()\n            .await\n            .map_err(|e| PaymentError::Network(e.to_string()))?;\n\n        if !response.status().is_success() {\n            return Err(PaymentError::GatewayRejected(response.status().to_string()));\n        }\n\n        let data: PaymentResponse = response\n            .json()\n            .await\n            .map_err(|e| PaymentError::ParseError(e.to_string()))?;\n\n        Ok(PaymentId::from(data.id))\n    }\n}\n```\n\n### In-Memory Adapter (for testing)\n\n```rust\npub struct InMemoryUserRepository {\n    users: Arc<Mutex<HashMap<UserId, User>>>,\n}\n\nimpl InMemoryUserRepository {\n    pub fn new() -> Self {\n        Self {\n            users: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub fn with_users(users: Vec<User>) -> Self {\n        let map = users.into_iter().map(|u| (u.id().clone(), u)).collect();\n        Self {\n            users: Arc::new(Mutex::new(map)),\n        }\n    }\n}\n\n#[async_trait]\nimpl UserRepository for InMemoryUserRepository {\n    async fn find_by_id(&self, id: &UserId) -> Result<User, RepositoryError> {\n        self.users\n            .lock()\n            .await\n            .get(id)\n            .cloned()\n            .ok_or(RepositoryError::NotFound)\n    }\n\n    async fn save(&self, user: &User) -> Result<(), RepositoryError> {\n        self.users\n            .lock()\n            .await\n            .insert(user.id().clone(), user.clone());\n        Ok(())\n    }\n}\n```\n\n## Port Design Guidelines\n\n1. **Use domain types**: Parameters and return types should be domain objects\n2. **Async by default**: Most I/O is async in Rust\n3. **Return domain errors**: Convert infrastructure errors at the boundary\n4. **Send + Sync**: Required for multi-threaded async runtimes\n5. **Focused interfaces**: Each port should have a single responsibility\n\n## Your Approach\n\nWhen you see external dependencies:\n1. Identify the interface needed\n2. Design a port trait with domain types\n3. Suggest adapter implementations\n4. Show testing strategy with mocks\n\nProactively suggest port/adapter patterns when you detect tight coupling to external systems."
              }
            ]
          },
          {
            "name": "rust-error-handling",
            "description": "Error handling best practices plugin for Rust. Provides commands for creating custom error types with thiserror, refactoring panic-based code to Result-based error handling, and an expert agent for error handling guidance and code review",
            "source": "./plugins/rust-error-handling",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-error-handling@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/rust-error-add-type",
                "description": "Create a new custom error type with thiserror",
                "path": "plugins/rust-error-handling/commands/rust-error-add-type.md",
                "frontmatter": {
                  "description": "Create a new custom error type with thiserror"
                },
                "content": "You are helping create a custom error type in Rust using the `thiserror` crate for robust error handling.\n\n## Your Task\n\nGenerate a well-structured custom error type with appropriate variants and conversions.\n\n## Steps\n\n1. **Ask for Error Type Details**\n\n   Ask the user (if not provided):\n   - Error type name (e.g., \"ConfigError\", \"DatabaseError\", \"ValidationError\")\n   - What domain/module is this for?\n   - What error variants are needed? (suggest common ones based on context)\n   - Should it wrap any external error types? (std::io::Error, sqlx::Error, etc.)\n\n2. **Determine Common Patterns**\n\n   Based on the error name, suggest appropriate variants:\n\n   **For *ValidationError**:\n   - Required(String) - missing required field\n   - Invalid { field, value } - invalid field value\n   - OutOfRange { min, max } - value out of range\n\n   **For *DatabaseError / *RepositoryError**:\n   - NotFound(String)\n   - Connection (#[from] lib::Error)\n   - Query(String)\n   - Transaction\n\n   **For *NetworkError / *ApiError**:\n   - Connection (#[from] reqwest::Error)\n   - Timeout\n   - InvalidResponse(String)\n   - Unauthorized\n\n   **For *ConfigError**:\n   - MissingField(String)\n   - InvalidValue { field, value }\n   - ParseError (#[from] toml::Error or serde_json::Error)\n\n3. **Create Error Type File**\n\n   Generate the error type with thiserror:\n\n   ```rust\n   use thiserror::Error;\n\n   /// Error type for [domain/module]\n   ///\n   /// This error type represents all possible errors that can occur\n   /// when [description of what can go wrong].\n   #[derive(Error, Debug)]\n   pub enum [ErrorName] {\n       /// [Description of when this error occurs]\n       #[error(\"[User-friendly error message]\")]\n       [Variant1](String),\n\n       /// [Description]\n       #[error(\"[Message with field]: {field}\")]\n       [Variant2] {\n           field: String,\n       },\n\n       /// Wraps [external error type]\n       #[error(\"[Context message]\")]\n       [Variant3](#[from] [ExternalError]),\n\n       /// [Description]\n       #[error(\"[Message with source error]\")]\n       [Variant4](#[source] [ExternalError]),\n   }\n   ```\n\n4. **Add Result Type Alias**\n\n   Create a Result alias for convenience:\n\n   ```rust\n   /// Result type alias for [module] operations\n   pub type Result<T> = std::result::Result<T, [ErrorName]>;\n   ```\n\n5. **Create Comprehensive Example**\n\n   Provide a complete, real-world example:\n\n   ```rust\n   use thiserror::Error;\n\n   /// Errors that can occur during user operations\n   #[derive(Error, Debug)]\n   pub enum UserError {\n       /// User not found with the given identifier\n       #[error(\"User not found: {0}\")]\n       NotFound(String),\n\n       /// Invalid email format\n       #[error(\"Invalid email: {0}\")]\n       InvalidEmail(String),\n\n       /// Database operation failed\n       #[error(\"Database error\")]\n       Database(#[from] sqlx::Error),\n\n       /// User already exists\n       #[error(\"User already exists: {email}\")]\n       AlreadyExists { email: String },\n\n       /// Validation failed\n       #[error(\"Validation failed: {0}\")]\n       Validation(String),\n   }\n\n   /// Result type for user operations\n   pub type Result<T> = std::result::Result<T, UserError>;\n\n   // Example usage\n   pub async fn find_user(id: &str) -> Result<User> {\n       let user = query_user(id).await?; // Auto-converts sqlx::Error\n       validate_user(&user)?;\n       Ok(user)\n   }\n\n   fn validate_user(user: &User) -> Result<()> {\n       if user.email.is_empty() {\n           return Err(UserError::InvalidEmail(\"Email cannot be empty\".to_string()));\n       }\n       Ok(())\n   }\n   ```\n\n6. **Add to Appropriate Module**\n\n   Determine where to place the error type:\n   - If it's a domain error: `src/domain/errors.rs` or `src/domain/[module]/error.rs`\n   - If it's an infrastructure error: `src/infrastructure/errors.rs`\n   - If it's a service error: `src/services/[service]/error.rs`\n\n   Update the module's `mod.rs`:\n   ```rust\n   mod error;\n   pub use error::{[ErrorName], Result};\n   ```\n\n7. **Update Cargo.toml**\n\n   Ensure thiserror is added:\n   ```toml\n   [dependencies]\n   thiserror = \"1.0\"\n   ```\n\n8. **Add Tests**\n\n   Create tests for error handling:\n\n   ```rust\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       #[test]\n       fn test_error_display() {\n           let error = UserError::NotFound(\"user123\".to_string());\n           assert_eq!(error.to_string(), \"User not found: user123\");\n       }\n\n       #[test]\n       fn test_error_conversion() {\n           fn returns_io_error() -> std::io::Result<()> {\n               Err(std::io::Error::new(std::io::ErrorKind::NotFound, \"file not found\"))\n           }\n\n           fn wraps_error() -> Result<()> {\n               returns_io_error().map_err(|e| UserError::Validation(e.to_string()))?;\n               Ok(())\n           }\n\n           assert!(wraps_error().is_err());\n       }\n\n       #[test]\n       fn test_error_matching() {\n           let error = UserError::InvalidEmail(\"test\".to_string());\n\n           match error {\n               UserError::InvalidEmail(email) => assert_eq!(email, \"test\"),\n               _ => panic!(\"Wrong error variant\"),\n           }\n       }\n   }\n   ```\n\n9. **Provide Usage Guidance**\n\n   Show how to use the error type:\n\n   ```\n   âœ… Error type '[ErrorName]' created successfully!\n\n   ## Usage Examples:\n\n   ### Returning Errors\n   ```rust\n   fn do_work() -> Result<Output> {\n       if condition {\n           return Err([ErrorName]::SomeVariant(\"details\".to_string()));\n       }\n       Ok(output)\n   }\n   ```\n\n   ### Error Propagation\n   ```rust\n   fn process() -> Result<Data> {\n       let result = risky_operation()?; // Auto-converts with #[from]\n       Ok(result)\n   }\n   ```\n\n   ### Error Matching\n   ```rust\n   match operation() {\n       Ok(value) => println!(\"Success: {:?}\", value),\n       Err([ErrorName]::NotFound(id)) => eprintln!(\"Not found: {}\", id),\n       Err([ErrorName]::Validation(msg)) => eprintln!(\"Validation: {}\", msg),\n       Err(e) => eprintln!(\"Other error: {}\", e),\n   }\n   ```\n\n   ## Next Steps:\n\n   1. Review and adjust error variants as needed\n   2. Use this error type in your functions\n   3. Add more variants as you discover new error cases\n   4. Consider creating error conversion helpers if needed\n\n   ## Testing:\n   ```bash\n   cargo test\n   ```\n   ```\n\n## Error Message Guidelines\n\nWhen creating error messages:\n\n1. **Be Specific**: Include relevant context\n   ```rust\n   #[error(\"Failed to load config from {path}\")]\n   ConfigLoad { path: String }\n   ```\n\n2. **Be User-Friendly**: Write for humans\n   ```rust\n   #[error(\"The email address '{0}' is not valid\")]\n   InvalidEmail(String)\n   ```\n\n3. **Include Details**: Help with debugging\n   ```rust\n   #[error(\"Database query failed: {query}\")]\n   QueryFailed { query: String }\n   ```\n\n4. **Use Action Words**: Describe what went wrong\n   ```rust\n   #[error(\"Failed to connect to database at {url}\")]\n   ConnectionFailed { url: String }\n   ```\n\n## Common Patterns\n\n### Simple Error\n```rust\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Something went wrong: {0}\")]\n    Generic(String),\n}\n```\n\n### Error with Fields\n```rust\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Invalid value for {field}: expected {expected}, got {actual}\")]\n    InvalidValue {\n        field: String,\n        expected: String,\n        actual: String,\n    },\n}\n```\n\n### Wrapped External Error\n```rust\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"IO operation failed\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"Serialization failed\")]\n    Serde(#[from] serde_json::Error),\n}\n```\n\n### Error with Source\n```rust\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Operation failed\")]\n    Failed(#[source] Box<dyn std::error::Error>),\n}\n```\n\n## Important Notes\n\n- Always derive `Error` and `Debug`\n- Use `#[from]` for automatic From impl\n- Use `#[source]` to preserve error chain\n- Keep error messages concise but informative\n- Consider adding Result type alias\n- Add documentation comments\n- Include tests for error cases\n\n## After Completion\n\nAsk the user:\n1. Do you want to add more error variants?\n2. Should we create conversion helpers?\n3. Do you want to integrate this with existing error types?"
              },
              {
                "name": "/rust-error-refactor",
                "description": "Refactor code from panic-based to Result-based error handling",
                "path": "plugins/rust-error-handling/commands/rust-error-refactor.md",
                "frontmatter": {
                  "description": "Refactor code from panic-based to Result-based error handling"
                },
                "content": "You are helping refactor Rust code to use proper error handling with Result types instead of panic-based error handling.\n\n## Your Task\n\nAnalyze code for panic-prone patterns and refactor to use Result-based error handling.\n\n## Steps\n\n1. **Scan for Panic-Prone Code**\n\n   Search the codebase for:\n   - `.unwrap()` calls\n   - `.expect()` calls\n   - `panic!()` macros\n   - `.unwrap_or_default()` where errors should be handled\n   - Indexing that could panic (e.g., `vec[0]`)\n\n   Use grep to find these patterns:\n   ```\n   - unwrap()\n   - expect(\n   - panic!(\n   ```\n\n2. **Categorize Findings**\n\n   Group findings by severity:\n   - **Critical**: Production code with unwrap/panic\n   - **Warning**: expect() with poor messages\n   - **Info**: Test code (acceptable to use unwrap)\n\n   Report to user:\n   ```\n   Found panic-prone patterns:\n\n   Critical (5):\n   - src/api/handler.rs:42 - .unwrap() on database query\n   - src/config.rs:15 - .expect(\"Failed\") on file read\n   ...\n\n   Info (2):\n   - tests/integration.rs:10 - .unwrap() (OK in tests)\n   ...\n   ```\n\n3. **Ask User for Scope**\n\n   Ask which files or functions to refactor:\n   - All critical issues?\n   - Specific file or module?\n   - Specific function?\n\n4. **Refactor Each Pattern**\n\n   For each panic-prone pattern, apply appropriate refactoring:\n\n   **Pattern 1: unwrap() on Option**\n\n   Before:\n   ```rust\n   fn get_user(id: &str) -> User {\n       let user = users.get(id).unwrap();\n       user\n   }\n   ```\n\n   After:\n   ```rust\n   fn get_user(id: &str) -> Result<User, UserError> {\n       users.get(id)\n           .cloned()\n           .ok_or_else(|| UserError::NotFound(id.to_string()))\n   }\n   ```\n\n   **Pattern 2: unwrap() on Result**\n\n   Before:\n   ```rust\n   fn load_config() -> Config {\n       let content = std::fs::read_to_string(\"config.toml\").unwrap();\n       toml::from_str(&content).unwrap()\n   }\n   ```\n\n   After:\n   ```rust\n   fn load_config() -> Result<Config, ConfigError> {\n       let content = std::fs::read_to_string(\"config.toml\")\n           .map_err(|e| ConfigError::FileRead(e))?;\n\n       toml::from_str(&content)\n           .map_err(|e| ConfigError::Parse(e))\n   }\n   ```\n\n   **Pattern 3: expect() with bad message**\n\n   Before:\n   ```rust\n   let value = dangerous_op().expect(\"Failed\");\n   ```\n\n   After:\n   ```rust\n   let value = dangerous_op()\n       .map_err(|e| MyError::OperationFailed(format!(\"Dangerous operation failed: {}\", e)))?;\n   ```\n\n   **Pattern 4: panic! for validation**\n\n   Before:\n   ```rust\n   fn create_user(email: String) -> User {\n       if email.is_empty() {\n           panic!(\"Email cannot be empty\");\n       }\n       User { email }\n   }\n   ```\n\n   After:\n   ```rust\n   fn create_user(email: String) -> Result<User, ValidationError> {\n       if email.is_empty() {\n           return Err(ValidationError::Required(\"email\".to_string()));\n       }\n       Ok(User { email })\n   }\n   ```\n\n   **Pattern 5: Vec indexing**\n\n   Before:\n   ```rust\n   fn get_first(items: &Vec<Item>) -> Item {\n       items[0].clone()\n   }\n   ```\n\n   After:\n   ```rust\n   fn get_first(items: &Vec<Item>) -> Result<Item, ItemError> {\n       items.first()\n           .cloned()\n           .ok_or(ItemError::Empty)\n   }\n   ```\n\n5. **Update Function Signatures**\n\n   When refactoring a function to return Result:\n   - Change return type from `T` to `Result<T, ErrorType>`\n   - Add error type if it doesn't exist\n   - Update all return statements\n\n   Before:\n   ```rust\n   fn process_data(input: &str) -> Data {\n       // ...\n   }\n   ```\n\n   After:\n   ```rust\n   fn process_data(input: &str) -> Result<Data, ProcessError> {\n       // ...\n       Ok(data)\n   }\n   ```\n\n6. **Update Call Sites**\n\n   Find all places where the refactored function is called and update them:\n\n   **If caller already returns Result**:\n   ```rust\n   fn caller() -> Result<Output, Error> {\n       let data = process_data(input)?; // Use ? operator\n       Ok(output)\n   }\n   ```\n\n   **If caller doesn't handle errors yet**:\n   ```rust\n   // Option 1: Propagate error\n   fn caller() -> Result<Output, Error> {\n       let data = process_data(input)?;\n       Ok(output)\n   }\n\n   // Option 2: Handle locally\n   fn caller() -> Output {\n       match process_data(input) {\n           Ok(data) => process(data),\n           Err(e) => {\n               log::error!(\"Failed to process: {}\", e);\n               default_output()\n           }\n       }\n   }\n   ```\n\n7. **Add Error Types if Needed**\n\n   If refactoring requires new error types, create them:\n\n   ```rust\n   #[derive(thiserror::Error, Debug)]\n   pub enum ProcessError {\n       #[error(\"Invalid input: {0}\")]\n       InvalidInput(String),\n\n       #[error(\"IO error\")]\n       Io(#[from] std::io::Error),\n\n       #[error(\"Parse error\")]\n       Parse(#[from] serde_json::Error),\n   }\n   ```\n\n8. **Update Tests**\n\n   Refactor tests to handle Result types:\n\n   Before:\n   ```rust\n   #[test]\n   fn test_process() {\n       let result = process_data(\"input\");\n       assert_eq!(result.value, 42);\n   }\n   ```\n\n   After:\n   ```rust\n   #[test]\n   fn test_process_success() {\n       let result = process_data(\"input\").unwrap(); // OK in tests\n       assert_eq!(result.value, 42);\n   }\n\n   #[test]\n   fn test_process_error() {\n       let result = process_data(\"invalid\");\n       assert!(result.is_err());\n       match result {\n           Err(ProcessError::InvalidInput(_)) => (),\n           _ => panic!(\"Expected InvalidInput error\"),\n       }\n   }\n   ```\n\n9. **Run Tests and Fix**\n\n   After refactoring:\n   ```bash\n   cargo test\n   cargo clippy\n   ```\n\n   Fix any compilation errors or test failures.\n\n10. **Provide Refactoring Summary**\n\n    Show what was changed:\n    ```\n    âœ… Refactored error handling\n\n    ## Changes Made:\n\n    ### Files Modified:\n    - src/api/handler.rs\n    - src/config.rs\n    - src/database/query.rs\n\n    ### Functions Refactored:\n    - `load_config` - Now returns Result<Config, ConfigError>\n    - `get_user` - Now returns Result<User, UserError>\n    - `execute_query` - Now returns Result<Data, DatabaseError>\n\n    ### New Error Types Created:\n    - ConfigError in src/config.rs\n    - UserError in src/domain/user.rs\n\n    ### Patterns Replaced:\n    - 8 unwrap() calls â†’ ? operator with proper error handling\n    - 3 expect() calls â†’ descriptive error variants\n    - 2 panic!() calls â†’ Result returns\n\n    ## Next Steps:\n\n    1. Run tests: `cargo test`\n    2. Review error messages for clarity\n    3. Consider adding error context where needed\n    4. Update API documentation\n\n    ## Before/After Example:\n\n    Before:\n    ```rust\n    fn load_config() -> Config {\n        let content = std::fs::read_to_string(\"config.toml\").unwrap();\n        toml::from_str(&content).unwrap()\n    }\n    ```\n\n    After:\n    ```rust\n    fn load_config() -> Result<Config, ConfigError> {\n        let content = std::fs::read_to_string(\"config.toml\")?;\n        let config = toml::from_str(&content)?;\n        Ok(config)\n    }\n    ```\n    ```\n\n## Refactoring Guidelines\n\n1. **Don't refactor test code**: `unwrap()` is acceptable in tests\n2. **Preserve behavior**: Make sure logic stays the same\n3. **Update incrementally**: Refactor one function at a time\n4. **Test after each change**: Ensure nothing breaks\n5. **Add error context**: Make errors informative\n6. **Consider backwards compatibility**: Use deprecation if needed\n\n## When to Keep unwrap/expect\n\nKeep these patterns when:\n- In test code (`#[cfg(test)]`)\n- After explicitly checking with `if let Some` or `is_some()`\n- When panic is truly the desired behavior (e.g., invalid constants)\n- In example code or documentation\n\n## Common Refactoring Patterns\n\n### Option::unwrap â†’ ok_or\n```rust\n// Before\nlet value = map.get(key).unwrap();\n\n// After\nlet value = map.get(key)\n    .ok_or(Error::NotFound(key.to_string()))?;\n```\n\n### Result::unwrap â†’ ?\n```rust\n// Before\nlet data = parse_data(&input).unwrap();\n\n// After\nlet data = parse_data(&input)?;\n```\n\n### panic! â†’ return Err\n```rust\n// Before\nif !is_valid(&input) {\n    panic!(\"Invalid input\");\n}\n\n// After\nif !is_valid(&input) {\n    return Err(Error::InvalidInput);\n}\n```\n\n## Important Notes\n\n- Create comprehensive error types before refactoring\n- Update documentation to reflect new error returns\n- Consider API compatibility for public functions\n- Add migration guide if it's a library\n- Use `#[deprecated]` for gradual migration\n\n## After Completion\n\nAsk the user:\n1. Did all tests pass?\n2. Are there more files to refactor?\n3. Should we add more error context?\n4. Do you want to update error documentation?"
              }
            ],
            "skills": [
              {
                "name": "error-conversion-guide",
                "description": "Guides users on error conversion patterns, From trait implementations, and the ? operator. Activates when users need to convert between error types or handle multiple error types in a function.",
                "path": "plugins/rust-error-handling/skills/error-conversion-guide/SKILL.md",
                "frontmatter": {
                  "name": "error-conversion-guide",
                  "description": "Guides users on error conversion patterns, From trait implementations, and the ? operator. Activates when users need to convert between error types or handle multiple error types in a function.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Error Conversion Guide Skill\n\nYou are an expert at Rust error conversion patterns. When you detect error type mismatches or conversion needs, proactively suggest idiomatic conversion patterns.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Multiple error types in a single function\n- Manual error conversion with `map_err`\n- Type mismatch errors with the `?` operator\n- Questions about From/Into traits for errors\n- Need to combine different error types\n\n## Error Conversion Patterns\n\n### Pattern 1: Automatic Conversion with #[from]\n\n**What to Look For**:\n- Manual `map_err` calls that could be automatic\n- Repetitive error conversions\n\n**Before**:\n```rust\n#[derive(Debug)]\npub enum AppError {\n    Io(std::io::Error),\n    Parse(std::num::ParseIntError),\n}\n\nfn process() -> Result<i32, AppError> {\n    let content = std::fs::read_to_string(\"data.txt\")\n        .map_err(|e| AppError::Io(e))?;  // âŒ Manual conversion\n\n    let num = content.trim().parse::<i32>()\n        .map_err(|e| AppError::Parse(e))?;  // âŒ Manual conversion\n\n    Ok(num)\n}\n```\n\n**After**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),  // âœ… Automatic From impl\n\n    #[error(\"Parse error\")]\n    Parse(#[from] std::num::ParseIntError),  // âœ… Automatic From impl\n}\n\nfn process() -> Result<i32, AppError> {\n    let content = std::fs::read_to_string(\"data.txt\")?;  // âœ… Auto-converts\n    let num = content.trim().parse::<i32>()?;  // âœ… Auto-converts\n    Ok(num)\n}\n```\n\n**Suggestion Template**:\n```\nUse #[from] in your error enum to enable automatic conversion:\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n}\n\nThis implements From<std::io::Error> for AppError, allowing ? to automatically convert.\n```\n\n### Pattern 2: Manual From Implementation\n\n**What to Look For**:\n- Custom error types that need conversion\n- Complex conversion logic\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Database error: {message}\")]\n    Database { message: String },\n\n    #[error(\"Validation error: {0}\")]\n    Validation(String),\n}\n\n// Manual From for custom conversion logic\nimpl From<sqlx::Error> for AppError {\n    fn from(err: sqlx::Error) -> Self {\n        AppError::Database {\n            message: format!(\"Database operation failed: {}\", err),\n        }\n    }\n}\n\n// Convert with context\nimpl From<validator::ValidationErrors> for AppError {\n    fn from(err: validator::ValidationErrors) -> Self {\n        let messages: Vec<String> = err\n            .field_errors()\n            .iter()\n            .map(|(field, errors)| {\n                format!(\"{}: {:?}\", field, errors)\n            })\n            .collect();\n\n        AppError::Validation(messages.join(\", \"))\n    }\n}\n```\n\n**Suggestion Template**:\n```\nWhen you need custom conversion logic, implement From manually:\n\nimpl From<SourceError> for AppError {\n    fn from(err: SourceError) -> Self {\n        AppError::Variant {\n            field: extract_info(&err),\n        }\n    }\n}\n```\n\n### Pattern 3: Converting Between Result Types\n\n**What to Look For**:\n- Calling functions with different error types\n- Need to unify error types\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ServiceError {\n    #[error(\"Repository error\")]\n    Repository(#[from] RepositoryError),\n\n    #[error(\"External API error\")]\n    Api(#[from] ApiError),\n\n    #[error(\"Validation error\")]\n    Validation(#[from] ValidationError),\n}\n\n// All these errors convert automatically\nfn service_operation() -> Result<Data, ServiceError> {\n    // Returns Result<_, RepositoryError>\n    let data = repository.fetch()?;  // âœ… Auto-converts to ServiceError\n\n    // Returns Result<_, ValidationError>\n    validate(&data)?;  // âœ… Auto-converts to ServiceError\n\n    // Returns Result<_, ApiError>\n    let enriched = api_client.enrich(data)?;  // âœ… Auto-converts to ServiceError\n\n    Ok(enriched)\n}\n```\n\n**Suggestion Template**:\n```\nCreate a unified error type that can convert from all the errors you need:\n\n#[derive(Error, Debug)]\npub enum UnifiedError {\n    #[error(\"Database error\")]\n    Database(#[from] DbError),\n\n    #[error(\"Network error\")]\n    Network(#[from] NetworkError),\n}\n\nfn operation() -> Result<(), UnifiedError> {\n    db_operation()?;  // Auto-converts\n    network_operation()?;  // Auto-converts\n    Ok(())\n}\n```\n\n### Pattern 4: map_err for One-Off Conversions\n\n**What to Look For**:\n- Single conversion that doesn't justify From impl\n- Adding context during conversion\n\n**Pattern**:\n```rust\nuse anyhow::Context;\n\nfn process(id: &str) -> anyhow::Result<Data> {\n    // One-off conversion with context\n    let config = load_config()\n        .map_err(|e| anyhow::anyhow!(\"Failed to load config: {}\", e))?;\n\n    // Better: use context\n    let config = load_config()\n        .context(\"Failed to load config\")?;\n\n    // map_err for type conversion without From impl\n    let data = fetch_data(id)\n        .map_err(|e| format!(\"Fetch failed for {}: {}\", id, e))?;\n\n    Ok(data)\n}\n```\n\n**When to Use**:\n- One-off conversions\n- Adding context to specific call sites\n- Converting to types that don't have From impl\n\n**Suggestion Template**:\n```\nFor one-off conversions or adding context, use map_err or anyhow's context:\n\n// With map_err\noperation().map_err(|e| MyError::Custom(format!(\"Failed: {}\", e)))?;\n\n// With anyhow (preferred)\noperation().context(\"Operation failed\")?;\n```\n\n### Pattern 5: Error Type Aliases\n\n**What to Look For**:\n- Repetitive Result<T, MyError> types\n- Complex error type signatures\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"Parse error\")]\n    Parse(#[from] serde_json::Error),\n}\n\n// Type alias for cleaner signatures\npub type Result<T> = std::result::Result<T, AppError>;\n\n// Now use it everywhere\npub fn load_config() -> Result<Config> {  // âœ… Clean\n    let bytes = std::fs::read(\"config.json\")?;\n    let config = serde_json::from_slice(&bytes)?;\n    Ok(config)\n}\n\n// Instead of\npub fn load_config_verbose() -> std::result::Result<Config, AppError> {  // âŒ Verbose\n    // ...\n}\n```\n\n**Suggestion Template**:\n```\nCreate a type alias for your Result type:\n\npub type Result<T> = std::result::Result<T, AppError>;\n\nThen use it in function signatures:\n\npub fn operation() -> Result<Data> {\n    Ok(data)\n}\n```\n\n### Pattern 6: Boxing Errors\n\n**What to Look For**:\n- Need for dynamic error types\n- Functions that can return multiple error types\n\n**Pattern**:\n```rust\n// For libraries that need flexibility\ntype BoxError = Box<dyn std::error::Error + Send + Sync>;\n\nfn flexible_function() -> Result<Data, BoxError> {\n    let data1 = io_operation()?;  // std::io::Error auto-boxes\n    let data2 = parse_operation()?;  // ParseError auto-boxes\n    Ok(combine(data1, data2))\n}\n\n// Or use anyhow for applications\nuse anyhow::Result;\n\nfn application_function() -> Result<Data> {\n    let data1 = io_operation()?;\n    let data2 = parse_operation()?;\n    Ok(combine(data1, data2))\n}\n```\n\n**Trade-offs**:\n- âœ… Flexible: Can return any error type\n- âœ… Simple: No need to define custom error enum\n- âŒ Dynamic: Error type not known at compile time\n- âŒ Harder to match on specific errors\n\n**Suggestion Template**:\n```\nFor flexible error handling, use Box<dyn Error> or anyhow:\n\n// Libraries: Box<dyn Error>\ntype BoxError = Box<dyn std::error::Error + Send + Sync>;\nfn operation() -> Result<T, BoxError> { ... }\n\n// Applications: anyhow\nuse anyhow::Result;\nfn operation() -> Result<T> { ... }\n```\n\n### Pattern 7: Layered Error Conversion\n\n**What to Look For**:\n- Multi-layer architecture (domain, infra, app)\n- Need for error boundary between layers\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n// Infrastructure layer\n#[derive(Error, Debug)]\npub enum InfraError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"HTTP error\")]\n    Http(#[from] reqwest::Error),\n}\n\n// Domain layer (doesn't know about infrastructure)\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"User not found: {0}\")]\n    UserNotFound(String),\n\n    #[error(\"Invalid data: {0}\")]\n    InvalidData(String),\n}\n\n// Application layer unifies both\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Domain error: {0}\")]\n    Domain(#[from] DomainError),\n\n    #[error(\"Infrastructure error: {0}\")]\n    Infra(#[from] InfraError),\n}\n\n// Infrastructure to Domain conversion (at boundary)\nimpl From<InfraError> for DomainError {\n    fn from(err: InfraError) -> Self {\n        match err {\n            InfraError::Database(e) if e.to_string().contains(\"not found\") => {\n                DomainError::UserNotFound(\"User not found in database\".to_string())\n            }\n            _ => DomainError::InvalidData(\"Data access failed\".to_string()),\n        }\n    }\n}\n```\n\n**Suggestion Template**:\n```\nFor layered architectures, convert errors at layer boundaries:\n\n// Infrastructure â†’ Domain conversion\nimpl From<InfraError> for DomainError {\n    fn from(err: InfraError) -> Self {\n        // Convert infrastructure concepts to domain concepts\n        match err {\n            InfraError::NotFound => DomainError::EntityNotFound,\n            _ => DomainError::InfrastructureFailed,\n        }\n    }\n}\n```\n\n## Advanced Patterns\n\n### Pattern 8: Multiple Error Sources with Custom Logic\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ProcessError {\n    #[error(\"Stage 1 failed: {0}\")]\n    Stage1(String),\n\n    #[error(\"Stage 2 failed: {0}\")]\n    Stage2(String),\n}\n\n// Custom conversion with different variants\nimpl From<Stage1Error> for ProcessError {\n    fn from(err: Stage1Error) -> Self {\n        ProcessError::Stage1(err.to_string())\n    }\n}\n\nimpl From<Stage2Error> for ProcessError {\n    fn from(err: Stage2Error) -> Self {\n        ProcessError::Stage2(err.to_string())\n    }\n}\n\nfn process() -> Result<(), ProcessError> {\n    stage1()?;  // Converts to ProcessError::Stage1\n    stage2()?;  // Converts to ProcessError::Stage2\n    Ok(())\n}\n```\n\n### Pattern 9: Fallible Conversion\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ConversionError {\n    #[error(\"Incompatible error type: {0}\")]\n    Incompatible(String),\n}\n\n// TryFrom for fallible conversion\nimpl TryFrom<ExternalError> for MyError {\n    type Error = ConversionError;\n\n    fn try_from(err: ExternalError) -> Result<Self, Self::Error> {\n        match err.code() {\n            404 => Ok(MyError::NotFound),\n            500 => Ok(MyError::Internal),\n            _ => Err(ConversionError::Incompatible(\n                format!(\"Unknown error code: {}\", err.code())\n            )),\n        }\n    }\n}\n```\n\n## Common Mistakes\n\n### Mistake 1: Multiple #[from] for Same Type\n\n```rust\n// âŒ BAD: Can't have two #[from] for same type\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"First\")]\n    First(#[from] std::io::Error),\n\n    #[error(\"Second\")]\n    Second(#[from] std::io::Error),  // âŒ Conflict!\n}\n\n// âœ… GOOD: Use #[source] and manual construction\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Read failed\")]\n    ReadFailed(#[source] std::io::Error),\n\n    #[error(\"Write failed\")]\n    WriteFailed(#[source] std::io::Error),\n}\n\n// Construct manually with context\nlet err = MyError::ReadFailed(io_err);\n```\n\n### Mistake 2: Losing Error Information\n\n```rust\n// âŒ BAD: Converts to String, loses error chain\noperation().map_err(|e| MyError::Failed(e.to_string()))?;\n\n// âœ… GOOD: Preserves error chain\noperation().map_err(|e| MyError::Failed(e))?;\n// Or use #[from]\n```\n\n### Mistake 3: Not Using ? When Available\n\n```rust\n// âŒ BAD: Manual error handling\nlet result = match operation() {\n    Ok(val) => val,\n    Err(e) => return Err(e.into()),\n};\n\n// âœ… GOOD: Use ?\nlet result = operation()?;\n```\n\n## Decision Guide\n\n**Use #[from] when**:\n- Single error source type\n- No need for additional context\n- Want automatic conversion\n\n**Use #[source] when**:\n- Multiple variants for same source type\n- Need to add context (like field names)\n- Want manual construction\n\n**Use map_err when**:\n- One-off conversion\n- Adding context to specific call\n- Converting to types without From impl\n\n**Use anyhow when**:\n- Application-level code\n- Need flexibility\n- Want easy context addition\n\n**Use thiserror when**:\n- Library code\n- Want specific error types\n- Consumers need to match on errors\n\n## Your Approach\n\n1. **Detect**: Identify error conversion needs or type mismatches\n2. **Analyze**: Determine the best conversion pattern\n3. **Suggest**: Provide specific implementation\n4. **Explain**: Why this pattern is appropriate\n\n## Communication Style\n\n- Explain the trade-offs between different approaches\n- Suggest #[from] as the default, map_err as fallback\n- Point out when error information is being lost\n- Recommend type aliases for cleaner code\n\nWhen you detect error conversion issues, immediately suggest the most appropriate pattern and show how to implement it."
              },
              {
                "name": "error-handler-advisor",
                "description": "Proactively reviews error handling patterns and suggests improvements using Result types, proper error propagation, and idiomatic patterns. Activates when users write error handling code or use unwrap/expect.",
                "path": "plugins/rust-error-handling/skills/error-handler-advisor/SKILL.md",
                "frontmatter": {
                  "name": "error-handler-advisor",
                  "description": "Proactively reviews error handling patterns and suggests improvements using Result types, proper error propagation, and idiomatic patterns. Activates when users write error handling code or use unwrap/expect.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Error Handler Advisor Skill\n\nYou are an expert at Rust error handling patterns. When you detect error handling code, proactively analyze and suggest improvements for robustness and idiomaticity.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Code using `unwrap()`, `expect()`, or `panic!()`\n- Functions returning `Result` or `Option` types\n- Error propagation with `?` operator\n- Discussion about error handling or debugging errors\n- Missing error handling for fallible operations\n- Questions about thiserror, anyhow, or error patterns\n\n## Error Handling Checklist\n\n### 1. Unwrap/Expect Usage\n\n**What to Look For**:\n- `unwrap()` or `expect()` in production code\n- Potential panic points\n- Missing error handling\n\n**Bad Pattern**:\n```rust\nfn process_user(id: &str) -> User {\n    let user = db.find_user(id).unwrap();  // âŒ Will panic if not found\n    let config = load_config().expect(\"config must exist\");  // âŒ Crashes on error\n    user\n}\n```\n\n**Good Pattern**:\n```rust\nfn process_user(id: &str) -> Result<User, Error> {\n    let user = db.find_user(id)?;  // âœ… Propagates error\n    let config = load_config()\n        .context(\"Failed to load configuration\")?;  // âœ… Adds context\n    Ok(user)\n}\n```\n\n**Suggestion Template**:\n```\nI notice you're using unwrap() which will panic if the Result is Err. Consider propagating the error instead:\n\nfn process_user(id: &str) -> Result<User, Error> {\n    let user = db.find_user(id)?;\n    Ok(user)\n}\n\nThis makes errors recoverable and provides better error messages to callers.\n```\n\n### 2. Custom Error Types\n\n**What to Look For**:\n- String as error type\n- Missing custom error enums\n- Library code without specific error types\n- No error conversion implementations\n\n**Bad Pattern**:\n```rust\nfn validate_email(email: &str) -> Result<(), String> {\n    if email.is_empty() {\n        return Err(\"Email cannot be empty\".to_string());  // âŒ String errors\n    }\n    Ok(())\n}\n```\n\n**Good Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ValidationError {\n    #[error(\"Email cannot be empty\")]\n    EmptyEmail,\n\n    #[error(\"Invalid email format: {0}\")]\n    InvalidFormat(String),\n\n    #[error(\"Email too long (max {max}, got {actual})\")]\n    TooLong { max: usize, actual: usize },\n}\n\nfn validate_email(email: &str) -> Result<(), ValidationError> {\n    if email.is_empty() {\n        return Err(ValidationError::EmptyEmail);  // âœ… Typed error\n    }\n    if !email.contains('@') {\n        return Err(ValidationError::InvalidFormat(email.to_string()));\n    }\n    Ok(())\n}\n```\n\n**Suggestion Template**:\n```\nUsing String as an error type loses type information. Consider using thiserror for custom error types:\n\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ValidationError {\n    #[error(\"Email cannot be empty\")]\n    EmptyEmail,\n\n    #[error(\"Invalid email format: {0}\")]\n    InvalidFormat(String),\n}\n\nThis provides:\n- Type safety for error handling\n- Automatic Display implementation\n- Better error matching\n- Clear error semantics\n```\n\n### 3. Error Propagation\n\n**What to Look For**:\n- Manual error handling that could use `?`\n- Nested match statements for Result\n- Losing error context during propagation\n\n**Bad Pattern**:\n```rust\nfn process() -> Result<Data, Error> {\n    let config = match load_config() {\n        Ok(c) => c,\n        Err(e) => return Err(e),  // âŒ Verbose\n    };\n\n    let data = match fetch_data(&config) {\n        Ok(d) => d,\n        Err(e) => return Err(e),  // âŒ Repetitive\n    };\n\n    Ok(data)\n}\n```\n\n**Good Pattern**:\n```rust\nfn process() -> Result<Data, Error> {\n    let config = load_config()?;  // âœ… Concise\n    let data = fetch_data(&config)?;  // âœ… Clear\n    Ok(data)\n}\n```\n\n**Suggestion Template**:\n```\nYou can simplify error propagation using the ? operator:\n\nfn process() -> Result<Data, Error> {\n    let config = load_config()?;\n    let data = fetch_data(&config)?;\n    Ok(data)\n}\n\nThe ? operator automatically propagates errors up the call stack.\n```\n\n### 4. Error Context\n\n**What to Look For**:\n- Errors without context about what operation failed\n- Missing information for debugging\n- Bare error propagation\n\n**Bad Pattern**:\n```rust\nfn load_user_data(id: &str) -> Result<UserData, Error> {\n    let user = fetch_user(id)?;  // âŒ No context\n    let profile = fetch_profile(id)?;  // âŒ Which operation failed?\n    Ok(UserData { user, profile })\n}\n```\n\n**Good Pattern**:\n```rust\nuse anyhow::{Context, Result};\n\nfn load_user_data(id: &str) -> Result<UserData> {\n    let user = fetch_user(id)\n        .context(format!(\"Failed to fetch user {}\", id))?;  // âœ… Context added\n\n    let profile = fetch_profile(id)\n        .context(format!(\"Failed to fetch profile for user {}\", id))?;  // âœ… Clear context\n\n    Ok(UserData { user, profile })\n}\n```\n\n**Suggestion Template**:\n```\nAdd context to errors to make debugging easier:\n\nuse anyhow::{Context, Result};\n\nlet user = fetch_user(id)\n    .context(format!(\"Failed to fetch user {}\", id))?;\n\nThis preserves the original error while adding useful context about the operation.\n```\n\n### 5. Error Conversion\n\n**What to Look For**:\n- Missing From implementations\n- Manual error conversion\n- Incompatible error types\n\n**Bad Pattern**:\n```rust\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Database error\")]\n    Database(String),  // âŒ Loses original error\n}\n\nfn query_db() -> Result<Data, AppError> {\n    let result = sqlx::query(\"SELECT ...\").fetch_one(&pool).await\n        .map_err(|e| AppError::Database(e.to_string()))?;  // âŒ Manual conversion, loses details\n    Ok(result)\n}\n```\n\n**Good Pattern**:\n```rust\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),  // âœ… Automatic conversion, preserves error\n}\n\nfn query_db() -> Result<Data, AppError> {\n    let result = sqlx::query(\"SELECT ...\").fetch_one(&pool).await?;  // âœ… Auto-converts\n    Ok(result)\n}\n```\n\n**Suggestion Template**:\n```\nUse the #[from] attribute for automatic error conversion:\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n}\n\nThis implements From<sqlx::Error> for AppError automatically, allowing ? to convert errors.\n```\n\n### 6. Library vs Application Errors\n\n**What to Look For**:\n- Library code using anyhow\n- Application code with overly specific error types\n- Missing error type patterns\n\n**Library Code Pattern**:\n```rust\n// âœ… Libraries should use thiserror with specific error types\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ParseError {\n    #[error(\"Invalid syntax at line {line}: {msg}\")]\n    Syntax { line: usize, msg: String },\n\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n}\n\npub fn parse_file(path: &Path) -> Result<Ast, ParseError> {\n    let content = std::fs::read_to_string(path)?;\n    parse_content(&content)\n}\n```\n\n**Application Code Pattern**:\n```rust\n// âœ… Applications can use anyhow for flexibility\nuse anyhow::{Context, Result};\n\nfn main() -> Result<()> {\n    let config = load_config()\n        .context(\"Failed to load config.toml\")?;\n\n    let app = initialize_app(&config)\n        .context(\"Failed to initialize application\")?;\n\n    app.run()\n        .context(\"Application failed during execution\")?;\n\n    Ok(())\n}\n```\n\n**Suggestion Template**:\n```\nFor library code, use thiserror with specific error types:\n\n#[derive(Error, Debug)]\npub enum MyLibError {\n    #[error(\"Specific error: {0}\")]\n    Specific(String),\n}\n\nFor application code, anyhow provides flexibility:\n\nuse anyhow::{Context, Result};\n\nfn main() -> Result<()> {\n    operation().context(\"Operation failed\")?;\n    Ok(())\n}\n```\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: Ignoring Errors\n\n**Bad**:\n```rust\nlet _ = dangerous_operation();  // âŒ Silently ignores errors\n```\n\n**Good**:\n```rust\ndangerous_operation()?;  // âœ… Propagates error\n// or\nif let Err(e) = dangerous_operation() {\n    warn!(\"Operation failed: {}\", e);  // âœ… At least log it\n}\n```\n\n### Anti-Pattern 2: Too Generic Errors\n\n**Bad**:\n```rust\n#[derive(Error, Debug)]\npub enum Error {\n    #[error(\"Something went wrong: {0}\")]\n    Generic(String),  // âŒ Not specific enough\n}\n```\n\n**Good**:\n```rust\n#[derive(Error, Debug)]\npub enum Error {\n    #[error(\"User not found: {0}\")]\n    UserNotFound(String),\n\n    #[error(\"Invalid credentials\")]\n    InvalidCredentials,\n\n    #[error(\"Database connection failed\")]\n    DatabaseConnection,  // âœ… Specific variants\n}\n```\n\n### Anti-Pattern 3: Panicking in Libraries\n\n**Bad**:\n```rust\npub fn parse(input: &str) -> Value {\n    if input.is_empty() {\n        panic!(\"Input cannot be empty\");  // âŒ Library shouldn't panic\n    }\n    // ...\n}\n```\n\n**Good**:\n```rust\npub fn parse(input: &str) -> Result<Value, ParseError> {\n    if input.is_empty() {\n        return Err(ParseError::EmptyInput);  // âœ… Return error\n    }\n    // ...\n}\n```\n\n## Error Testing Patterns\n\n### Test Error Cases\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_empty_email_error() {\n        let result = validate_email(\"\");\n        assert!(result.is_err());\n        assert!(matches!(result.unwrap_err(), ValidationError::EmptyEmail));\n    }\n\n    #[test]\n    fn test_invalid_format_error() {\n        let result = validate_email(\"invalid\");\n        match result {\n            Err(ValidationError::InvalidFormat(email)) => {\n                assert_eq!(email, \"invalid\");\n            }\n            _ => panic!(\"Expected InvalidFormat error\"),\n        }\n    }\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify error handling code or potential error cases\n2. **Analyze**: Check against the checklist above\n3. **Suggest**: Provide specific improvements with code examples\n4. **Explain**: Why the suggested pattern is better\n5. **Prioritize**: Focus on potential panics and missing error handling first\n\n## Communication Style\n\n- Point out potential panics immediately\n- Suggest thiserror for libraries, anyhow for applications\n- Provide complete code examples, not just fragments\n- Explain the benefits of each pattern\n- Consider the context (library vs application, production vs prototype)\n\nWhen you detect error handling code, quickly scan for common issues and proactively suggest improvements that will make the code more robust and maintainable."
              },
              {
                "name": "thiserror-expert",
                "description": "Provides guidance on creating custom error types with thiserror, including proper derive macros, error messages, and source error chaining. Activates when users define error enums or work with thiserror.",
                "path": "plugins/rust-error-handling/skills/thiserror-expert/SKILL.md",
                "frontmatter": {
                  "name": "thiserror-expert",
                  "description": "Provides guidance on creating custom error types with thiserror, including proper derive macros, error messages, and source error chaining. Activates when users define error enums or work with thiserror.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Thiserror Expert Skill\n\nYou are an expert at using the thiserror crate to create elegant, idiomatic Rust error types. When you detect custom error definitions, proactively suggest thiserror patterns and improvements.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Custom error enum definitions\n- Manual Display or Error implementations\n- Code using `thiserror::Error` derive macro\n- Questions about error types or thiserror usage\n- Library code that needs custom error types\n\n## Thiserror Patterns\n\n### Pattern 1: Basic Error Enum\n\n**What to Look For**:\n- Manual Display implementations\n- Missing thiserror derive\n\n**Before**:\n```rust\n#[derive(Debug)]\npub enum MyError {\n    NotFound,\n    Invalid,\n}\n\nimpl std::fmt::Display for MyError {\n    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {\n        match self {\n            MyError::NotFound => write!(f, \"Not found\"),\n            MyError::Invalid => write!(f, \"Invalid\"),\n        }\n    }\n}\n\nimpl std::error::Error for MyError {}\n```\n\n**After**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Not found\")]\n    NotFound,\n\n    #[error(\"Invalid\")]\n    Invalid,\n}\n```\n\n**Suggestion Template**:\n```\nYou can simplify your error type using thiserror:\n\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Not found\")]\n    NotFound,\n\n    #[error(\"Invalid input\")]\n    Invalid,\n}\n\nThis automatically implements Display and std::error::Error.\n```\n\n### Pattern 2: Error Messages with Fields\n\n**What to Look For**:\n- Error variants with data\n- Need to include field values in error messages\n\n**Patterns**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ValidationError {\n    // Positional fields (tuple variants)\n    #[error(\"Invalid email: {0}\")]\n    InvalidEmail(String),\n\n    // Named fields with standard display\n    #[error(\"Value {value} out of range (min: {min}, max: {max})\")]\n    OutOfRange { value: i32, min: i32, max: i32 },\n\n    // Custom formatting with debug\n    #[error(\"Invalid character: {ch:?} at position {pos}\")]\n    InvalidChar { ch: char, pos: usize },\n\n    // Multiple positional args\n    #[error(\"Cannot convert {0} to {1}\")]\n    ConversionFailed(String, String),\n}\n```\n\n**Suggestion Template**:\n```\nYou can include field values in error messages:\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"User {user_id} not found\")]\n    UserNotFound { user_id: String },\n\n    #[error(\"Invalid age: {0} (must be >= 18)\")]\n    InvalidAge(u32),\n}\n\nUse {field} for named fields and {0}, {1} for positional fields.\n```\n\n### Pattern 3: Wrapping Source Errors with #[from]\n\n**What to Look For**:\n- Error variants that wrap other errors\n- Missing automatic conversions\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum AppError {\n    // Automatic From implementation\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n\n    // Multiple source error types\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"Serialization error\")]\n    Json(#[from] serde_json::Error),\n\n    // Application-specific errors (no #[from])\n    #[error(\"User not found: {0}\")]\n    UserNotFound(String),\n}\n```\n\n**Benefits**:\n- Implements `From<std::io::Error> for AppError`\n- Allows `?` operator to auto-convert\n- Preserves source error for debugging\n\n**Suggestion Template**:\n```\nUse #[from] to automatically implement From for error conversion:\n\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n}\n\nThis allows the ? operator to automatically convert these errors to AppError.\n```\n\n### Pattern 4: Source Error Chain with #[source]\n\n**What to Look For**:\n- Errors that wrap other errors but need custom messages\n- Need for error source chain\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ConfigError {\n    // #[source] preserves error chain without #[from]\n    #[error(\"Failed to load config file\")]\n    LoadFailed(#[source] std::io::Error),\n\n    // #[source] with custom error info\n    #[error(\"Invalid config format in {file}\")]\n    InvalidFormat {\n        file: String,\n        #[source]\n        source: toml::de::Error,\n    },\n\n    // Both message customization and error chain\n    #[error(\"Missing required field: {field}\")]\n    MissingField {\n        field: String,\n        #[source]\n        source: Box<dyn std::error::Error + Send + Sync>,\n    },\n}\n```\n\n**Difference from #[from]**:\n- `#[from]`: Implements `From` trait (automatic conversion)\n- `#[source]`: Only marks as source error (manual construction)\n\n**Suggestion Template**:\n```\nUse #[source] when you need custom error construction but want to preserve the error chain:\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Operation failed for user {user_id}\")]\n    OperationFailed {\n        user_id: String,\n        #[source]\n        source: DatabaseError,\n    },\n}\n\n// Construct manually with context\nreturn Err(MyError::OperationFailed {\n    user_id: id.to_string(),\n    source: db_error,\n});\n```\n\n### Pattern 5: Transparent Error Forwarding\n\n**What to Look For**:\n- Wrapper errors that should forward to inner error\n- Need for transparent error propagation\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum WrapperError {\n    // Transparent forwards all Display/source to inner error\n    #[error(transparent)]\n    Inner(#[from] InnerError),\n}\n\n// Example: Wrapper for anyhow in library\n#[derive(Error, Debug)]\npub enum LibError {\n    #[error(transparent)]\n    Other(#[from] anyhow::Error),\n}\n```\n\n**Use Cases**:\n- Wrapping errors without changing their display\n- Re-exporting errors from dependencies\n- Internal error handling that shouldn't change messages\n\n**Suggestion Template**:\n```\nUse #[error(transparent)] to forward all error information to the inner error:\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(transparent)]\n    Wrapped(#[from] InnerError),\n}\n\nThis preserves the inner error's Display and source chain completely.\n```\n\n### Pattern 6: Layered Errors\n\n**What to Look For**:\n- Applications with multiple layers (domain, infrastructure, etc.)\n- Need for error conversion between layers\n\n**Pattern**:\n```rust\nuse thiserror::Error;\n\n// Domain layer errors\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"Invalid user data: {0}\")]\n    InvalidUser(String),\n\n    #[error(\"Business rule violated: {0}\")]\n    BusinessRuleViolation(String),\n}\n\n// Infrastructure layer errors\n#[derive(Error, Debug)]\npub enum InfraError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"HTTP request failed\")]\n    Http(#[from] reqwest::Error),\n}\n\n// Application layer combines both\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Domain error: {0}\")]\n    Domain(#[from] DomainError),\n\n    #[error(\"Infrastructure error: {0}\")]\n    Infra(#[from] InfraError),\n\n    #[error(\"Application error: {0}\")]\n    Application(String),\n}\n```\n\n**Suggestion Template**:\n```\nFor layered architectures, create error types for each layer:\n\n// Domain layer\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"Invalid data: {0}\")]\n    Invalid(String),\n}\n\n// Infrastructure layer\n#[derive(Error, Debug)]\npub enum InfraError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n}\n\n// Application layer combines both\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Domain: {0}\")]\n    Domain(#[from] DomainError),\n\n    #[error(\"Infra: {0}\")]\n    Infra(#[from] InfraError),\n}\n```\n\n## Advanced Patterns\n\n### Pattern 7: Generic Error Types\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum OperationError<T>\nwhere\n    T: std::error::Error + 'static,\n{\n    #[error(\"Operation failed\")]\n    Failed(#[source] T),\n\n    #[error(\"Timeout after {0} seconds\")]\n    Timeout(u64),\n}\n```\n\n### Pattern 8: Conditional Compilation\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"IO error\")]\n    Io(#[from] std::io::Error),\n\n    #[cfg(feature = \"postgres\")]\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[cfg(feature = \"redis\")]\n    #[error(\"Cache error\")]\n    Cache(#[from] redis::RedisError),\n}\n```\n\n### Pattern 9: Enum with Unit and Complex Variants\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum ValidationError {\n    // Unit variant\n    #[error(\"Value is required\")]\n    Required,\n\n    // Tuple variant\n    #[error(\"Invalid format: {0}\")]\n    InvalidFormat(String),\n\n    // Struct variant\n    #[error(\"Out of range (expected {expected}, got {actual})\")]\n    OutOfRange { expected: String, actual: String },\n\n    // Nested error\n    #[error(\"Validation failed\")]\n    Nested(#[from] SubValidationError),\n}\n```\n\n## Best Practices\n\n### DO: Clear, Actionable Error Messages\n\n```rust\n#[derive(Error, Debug)]\npub enum ConfigError {\n    // âœ… Clear and actionable\n    #[error(\"Config file not found at '{path}'. Create one using: config init\")]\n    NotFound { path: String },\n\n    // âœ… Explains what's wrong and expected format\n    #[error(\"Invalid port number '{port}'. Expected a number between 1 and 65535\")]\n    InvalidPort { port: String },\n}\n```\n\n### DON'T: Vague Error Messages\n\n```rust\n#[derive(Error, Debug)]\npub enum BadError {\n    // âŒ Too vague\n    #[error(\"Error\")]\n    Error,\n\n    // âŒ Not helpful\n    #[error(\"Something went wrong\")]\n    Failed,\n}\n```\n\n### DO: Include Context\n\n```rust\n#[derive(Error, Debug)]\npub enum AppError {\n    // âœ… Includes what, where, and source\n    #[error(\"Failed to read file '{path}'\")]\n    ReadFailed {\n        path: String,\n        #[source]\n        source: std::io::Error,\n    },\n}\n```\n\n### DO: Type Aliases for Result\n\n```rust\npub type Result<T> = std::result::Result<T, MyError>;\n\n// Now you can use:\npub fn operation() -> Result<Value> {\n    Ok(value)\n}\n```\n\n## Common Mistakes\n\n### Mistake 1: Forgetting #[source]\n\n```rust\n// âŒ BAD: Source error not marked\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Failed\")]\n    Failed(std::io::Error),  // Missing #[source]\n}\n\n// âœ… GOOD: Properly marked\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Failed\")]\n    Failed(#[source] std::io::Error),\n}\n```\n\n### Mistake 2: Using #[from] When You Need Custom Construction\n\n```rust\n// âŒ Can't add context with #[from]\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Failed\")]\n    Failed(#[from] std::io::Error),\n}\n\n// âœ… Use #[source] for custom construction\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"Failed to read config file '{path}'\")]\n    ConfigReadFailed {\n        path: String,\n        #[source]\n        source: std::io::Error,\n    },\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify error type definitions or thiserror usage\n2. **Analyze**: Check message clarity, source chaining, and conversions\n3. **Suggest**: Provide specific improvements\n4. **Educate**: Explain when to use #[from] vs #[source] vs #[transparent]\n\n## Communication Style\n\n- Suggest thiserror for any custom error type\n- Explain the difference between #[from], #[source], and #[transparent]\n- Provide complete error type examples\n- Show how the error will be displayed\n- Point out missing error chains\n\nWhen you see custom error types, immediately suggest thiserror patterns that will make them more ergonomic and idiomatic."
              }
            ]
          },
          {
            "name": "rust-testing",
            "description": "Testing best practices plugin for Rust. Includes commands for adding unit tests, integration tests, test analysis, and an expert agent for comprehensive testing strategies, mock implementations, and property-based testing",
            "source": "./plugins/rust-testing",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-testing@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/rust-test-add-integration",
                "description": "Create integration tests in the tests/ directory",
                "path": "plugins/rust-testing/commands/rust-test-add-integration.md",
                "frontmatter": {
                  "description": "Create integration tests in the tests/ directory"
                },
                "content": "You are helping create integration tests for a Rust project in the tests/ directory.\n\n## Your Task\n\nSet up integration test infrastructure and create comprehensive integration tests that test the public API.\n\n## Steps\n\n1. **Ask for Test Details**\n\n   Ask the user:\n   - What feature/module to integration test?\n   - Do you need database/HTTP mocking?\n   - Should we use testcontainers for real databases?\n\n2. **Create Integration Test File**\n\n   Create a new file in `tests/` directory:\n\n   ```\n   tests/\n   â”œâ”€â”€ [feature]_integration.rs\n   â””â”€â”€ common/\n       â””â”€â”€ mod.rs  # Shared test utilities\n   ```\n\n3. **Set Up Common Utilities**\n\n   Create `tests/common/mod.rs`:\n\n   ```rust\n   // tests/common/mod.rs\n   #![allow(dead_code)]\n\n   use my_crate::*;\n\n   pub fn setup() {\n       // Common setup logic\n   }\n\n   pub fn teardown() {\n       // Common cleanup logic\n   }\n   ```\n\n4. **Generate Integration Test**\n\n   Create the integration test file:\n\n   ```rust\n   // tests/[feature]_integration.rs\n   use my_crate::*;\n\n   mod common;\n\n   #[test]\n   fn test_[feature]_complete_workflow() {\n       common::setup();\n\n       // Arrange\n       let app = create_test_application();\n\n       // Act\n       let result = app.execute_workflow();\n\n       // Assert\n       assert!(result.is_ok());\n\n       common::teardown();\n   }\n\n   fn create_test_application() -> Application {\n       // Setup test instance with test configuration\n       Application::new(test_config())\n   }\n\n   fn test_config() -> Config {\n       Config {\n           database_url: \"postgres://localhost/test_db\".to_string(),\n           // ... other test config\n       }\n   }\n   ```\n\n5. **Add Async Integration Tests**\n\n   For async applications:\n\n   ```rust\n   #[tokio::test]\n   async fn test_async_integration() {\n       let app = setup_test_app().await;\n\n       let result = app.process_request(test_request()).await;\n\n       assert!(result.is_ok());\n   }\n\n   async fn setup_test_app() -> Application {\n       Application::new(test_config()).await.unwrap()\n   }\n   ```\n\n6. **Add Database Integration Tests**\n\n   Using testcontainers:\n\n   ```rust\n   use testcontainers::{clients, images::postgres::Postgres};\n   use sqlx::PgPool;\n\n   #[tokio::test]\n   async fn test_with_real_database() {\n       let docker = clients::Cli::default();\n       let postgres = docker.run(Postgres::default());\n\n       let connection_string = format!(\n           \"postgres://postgres@localhost:{}/postgres\",\n           postgres.get_host_port_ipv4(5432)\n       );\n\n       let pool = PgPool::connect(&connection_string).await.unwrap();\n\n       // Run migrations\n       sqlx::migrate!(\"./migrations\")\n           .run(&pool)\n           .await\n           .unwrap();\n\n       // Now test with real database\n       let repo = PostgresRepository::new(pool);\n       let service = MyService::new(repo);\n\n       let result = service.create_item(\"test\").await;\n\n       assert!(result.is_ok());\n   }\n   ```\n\n   Or create a helper in common:\n\n   ```rust\n   // tests/common/mod.rs\n   use sqlx::PgPool;\n   use testcontainers::*;\n\n   pub async fn setup_test_database() -> PgPool {\n       let docker = clients::Cli::default();\n       let postgres = docker.run(images::postgres::Postgres::default());\n\n       let url = format!(\n           \"postgres://postgres@localhost:{}/postgres\",\n           postgres.get_host_port_ipv4(5432)\n       );\n\n       let pool = PgPool::connect(&url).await.unwrap();\n       sqlx::migrate!().run(&pool).await.unwrap();\n\n       pool\n   }\n\n   // tests/database_integration.rs\n   mod common;\n\n   #[tokio::test]\n   async fn test_repository() {\n       let pool = common::setup_test_database().await;\n       let repo = MyRepository::new(pool);\n\n       // Test repository operations\n   }\n   ```\n\n7. **Add HTTP API Integration Tests**\n\n   For testing HTTP APIs:\n\n   ```rust\n   use axum::body::Body;\n   use axum::http::{Request, StatusCode};\n   use tower::ServiceExt;\n\n   #[tokio::test]\n   async fn test_api_endpoint() {\n       let app = create_test_app();\n\n       let response = app\n           .oneshot(\n               Request::builder()\n                   .uri(\"/api/users\")\n                   .method(\"GET\")\n                   .body(Body::empty())\n                   .unwrap(),\n           )\n           .await\n           .unwrap();\n\n       assert_eq!(response.status(), StatusCode::OK);\n\n       let body = hyper::body::to_bytes(response.into_body())\n           .await\n           .unwrap();\n\n       let users: Vec<User> = serde_json::from_slice(&body).unwrap();\n       assert!(!users.is_empty());\n   }\n   ```\n\n   Or using reqwest for full HTTP testing:\n\n   ```rust\n   #[tokio::test]\n   async fn test_api_full_request() {\n       let server = spawn_test_server().await;\n\n       let client = reqwest::Client::new();\n       let response = client\n           .get(format!(\"{}/api/users\", server.url()))\n           .send()\n           .await\n           .unwrap();\n\n       assert_eq!(response.status(), 200);\n\n       let users: Vec<User> = response.json().await.unwrap();\n       assert!(!users.is_empty());\n   }\n\n   async fn spawn_test_server() -> TestServer {\n       // Start server on random port for testing\n       TestServer::spawn().await\n   }\n   ```\n\n8. **Add HTTP Mocking with wiremock**\n\n   For testing external API calls:\n\n   ```rust\n   use wiremock::{MockServer, Mock, ResponseTemplate};\n   use wiremock::matchers::{method, path};\n\n   #[tokio::test]\n   async fn test_external_api_integration() {\n       let mock_server = MockServer::start().await;\n\n       Mock::given(method(\"GET\"))\n           .and(path(\"/external/api\"))\n           .respond_with(ResponseTemplate::new(200).set_body_json(\n               serde_json::json!({\n                   \"status\": \"ok\",\n                   \"data\": \"test\"\n               })\n           ))\n           .mount(&mock_server)\n           .await;\n\n       let client = ExternalApiClient::new(&mock_server.uri());\n       let result = client.fetch_data().await.unwrap();\n\n       assert_eq!(result.status, \"ok\");\n   }\n   ```\n\n9. **Add Multi-Step Workflow Tests**\n\n   Test complete user workflows:\n\n   ```rust\n   #[tokio::test]\n   async fn test_complete_user_workflow() {\n       let app = setup_test_app().await;\n\n       // Step 1: Create user\n       let user_id = app\n           .create_user(\"test@example.com\")\n           .await\n           .unwrap();\n\n       // Step 2: Retrieve user\n       let user = app\n           .get_user(&user_id)\n           .await\n           .unwrap();\n\n       assert_eq!(user.email, \"test@example.com\");\n\n       // Step 3: Update user\n       app.update_user(&user_id, \"new@example.com\")\n           .await\n           .unwrap();\n\n       // Step 4: Verify update\n       let updated = app\n           .get_user(&user_id)\n           .await\n           .unwrap();\n\n       assert_eq!(updated.email, \"new@example.com\");\n\n       // Step 5: Delete user\n       app.delete_user(&user_id)\n           .await\n           .unwrap();\n\n       // Step 6: Verify deletion\n       let result = app.get_user(&user_id).await;\n       assert!(result.is_err());\n   }\n   ```\n\n10. **Update Dev Dependencies**\n\n    Ensure required dependencies are in Cargo.toml:\n\n    ```toml\n    [dev-dependencies]\n    tokio = { version = \"1\", features = [\"full\", \"test-util\"] }\n    testcontainers = \"0.15\"\n    wiremock = \"0.6\"\n    reqwest = { version = \"0.11\", features = [\"json\"] }\n    ```\n\n11. **Provide Summary**\n\n    ```\n    âœ… Integration tests created successfully!\n\n    ## Files Created:\n    - `tests/[feature]_integration.rs` - Main integration test file\n    - `tests/common/mod.rs` - Shared test utilities\n\n    ## Tests Added:\n    - test_[feature]_complete_workflow\n    - test_database_operations\n    - test_api_endpoints\n    - test_external_api_integration\n\n    ## Infrastructure:\n    - Database test setup with testcontainers\n    - HTTP mocking with wiremock\n    - Test configuration helpers\n\n    ## Dependencies Added:\n    [List of dev dependencies]\n\n    ## Running Integration Tests:\n\n    ```bash\n    # Run all integration tests\n    cargo test --test [feature]_integration\n\n    # Run specific test\n    cargo test test_complete_workflow\n\n    # Run with output\n    cargo test --test [feature]_integration -- --nocapture\n\n    # Run all integration tests\n    cargo test --tests\n    ```\n\n    ## Next Steps:\n    1. Review and customize test cases\n    2. Add more workflow scenarios\n    3. Run tests: `cargo test --tests`\n    4. Check if you need more test infrastructure\n    ```\n\n## Integration Test Patterns\n\n### Setup/Teardown Pattern\n```rust\nstruct TestContext {\n    pool: PgPool,\n    // Other resources\n}\n\nimpl TestContext {\n    async fn new() -> Self {\n        // Setup\n        Self {\n            pool: setup_database().await,\n        }\n    }\n}\n\nimpl Drop for TestContext {\n    fn drop(&mut self) {\n        // Cleanup\n    }\n}\n\n#[tokio::test]\nasync fn test_with_context() {\n    let ctx = TestContext::new().await;\n    // Use ctx.pool\n}\n```\n\n### Parallel Test Isolation\n```rust\n#[tokio::test]\nasync fn test_isolated_1() {\n    let db = create_unique_test_db(\"test1\").await;\n    // Each test gets its own database\n}\n\n#[tokio::test]\nasync fn test_isolated_2() {\n    let db = create_unique_test_db(\"test2\").await;\n    // Runs in parallel without interference\n}\n```\n\n## Important Notes\n\n- Integration tests are in separate crates from src/\n- Each file in tests/ is a separate binary\n- Common code goes in tests/common/ (not tests/common.rs)\n- Use real implementations when possible\n- Mock external services\n- Clean up resources after tests\n- Tests should be independent and runnable in any order\n\n## After Completion\n\nAsk the user:\n1. Did the integration tests pass?\n2. Do you need more test scenarios?\n3. Should we add performance/load tests?\n4. Do you want to set up CI/CD for these tests?"
              },
              {
                "name": "/rust-test-add-unit",
                "description": "Add comprehensive unit tests for a function or module",
                "path": "plugins/rust-testing/commands/rust-test-add-unit.md",
                "frontmatter": {
                  "description": "Add comprehensive unit tests for a function or module"
                },
                "content": "You are helping add unit tests to Rust code following best practices.\n\n## Your Task\n\nGenerate comprehensive unit tests for the specified function or module, covering success cases, error cases, and edge cases.\n\n## Steps\n\n1. **Identify Target**\n\n   Ask the user (if not specified):\n   - What function/struct/module to test?\n   - Where is it located?\n\n   Or scan the current file for testable functions.\n\n2. **Analyze Function**\n\n   Read the function to understand:\n   - What it does\n   - What inputs it takes\n   - What it returns (including error types)\n   - What edge cases exist\n\n3. **Create Test Module**\n\n   Add or update the `#[cfg(test)]` module at the bottom of the file:\n\n   ```rust\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       // Tests go here\n   }\n   ```\n\n4. **Generate Test Cases**\n\n   For each function, create tests for:\n\n   **Success Cases**:\n   ```rust\n   #[test]\n   fn test_[function]_with_valid_input() {\n       // Arrange\n       let input = create_valid_input();\n\n       // Act\n       let result = function(input);\n\n       // Assert\n       assert!(result.is_ok());\n       assert_eq!(result.unwrap(), expected_value);\n   }\n   ```\n\n   **Error Cases**:\n   ```rust\n   #[test]\n   fn test_[function]_returns_error_on_invalid_input() {\n       let result = function(invalid_input);\n\n       assert!(result.is_err());\n       assert!(matches!(result.unwrap_err(), ErrorType::Specific));\n   }\n   ```\n\n   **Edge Cases**:\n   ```rust\n   #[test]\n   fn test_[function]_with_empty_input() {\n       let result = function(\"\");\n       assert!(result.is_err());\n   }\n\n   #[test]\n   fn test_[function]_with_max_length_input() {\n       let long_input = \"x\".repeat(MAX_LENGTH);\n       let result = function(&long_input);\n       assert!(result.is_ok());\n   }\n   ```\n\n5. **Add Test Fixtures**\n\n   Create helper functions for common test data:\n\n   ```rust\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       fn create_test_user() -> User {\n           User {\n               id: \"test-id\".to_string(),\n               email: \"test@example.com\".to_string(),\n           }\n       }\n\n       fn create_test_user_with_id(id: &str) -> User {\n           User {\n               id: id.to_string(),\n               email: \"test@example.com\".to_string(),\n           }\n       }\n\n       #[test]\n       fn test_with_fixture() {\n           let user = create_test_user();\n           let result = validate_user(&user);\n           assert!(result.is_ok());\n       }\n   }\n   ```\n\n6. **Add Async Tests if Needed**\n\n   For async functions:\n\n   ```rust\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n\n       #[tokio::test]\n       async fn test_async_function() {\n           let result = async_function().await;\n           assert!(result.is_ok());\n       }\n\n       #[tokio::test]\n       async fn test_async_error_case() {\n           let result = async_function_with_error().await;\n           assert!(result.is_err());\n       }\n   }\n   ```\n\n7. **Add Mock Implementations**\n\n   For functions using traits:\n\n   ```rust\n   #[cfg(test)]\n   mod tests {\n       use super::*;\n       use std::collections::HashMap;\n\n       struct MockRepository {\n           data: HashMap<String, User>,\n       }\n\n       impl MockRepository {\n           fn new() -> Self {\n               Self { data: HashMap::new() }\n           }\n\n           fn with_user(mut self, user: User) -> Self {\n               self.data.insert(user.id.clone(), user);\n               self\n           }\n       }\n\n       #[async_trait]\n       impl UserRepository for MockRepository {\n           async fn find_by_id(&self, id: &str) -> Result<User, Error> {\n               self.data.get(id)\n                   .cloned()\n                   .ok_or(Error::NotFound(id.to_string()))\n           }\n       }\n\n       #[tokio::test]\n       async fn test_service_with_mock() {\n           let mock = MockRepository::new()\n               .with_user(create_test_user());\n\n           let service = UserService::new(mock);\n           let user = service.get_user(\"test-id\").await.unwrap();\n\n           assert_eq!(user.email, \"test@example.com\");\n       }\n   }\n   ```\n\n8. **Add Table-Driven Tests**\n\n   For multiple similar test cases:\n\n   ```rust\n   #[test]\n   fn test_validation_cases() {\n       let test_cases = vec![\n           (\"\", false, \"Empty input\"),\n           (\"a\", true, \"Single char\"),\n           (\"abc\", true, \"Valid input\"),\n           (\"x\".repeat(1000).as_str(), false, \"Too long\"),\n       ];\n\n       for (input, should_pass, description) in test_cases {\n           let result = validate(input);\n           assert_eq!(\n               result.is_ok(),\n               should_pass,\n               \"Failed for case: {}\",\n               description\n           );\n       }\n   }\n   ```\n\n9. **Provide Test Summary**\n\n   After generating tests:\n\n   ```\n   âœ… Unit tests added successfully!\n\n   ## Tests Created:\n\n   ### Success Cases (3):\n   - test_create_user_with_valid_email\n   - test_update_user_success\n   - test_delete_user_success\n\n   ### Error Cases (4):\n   - test_create_user_with_empty_email\n   - test_create_user_with_invalid_email\n   - test_update_nonexistent_user\n   - test_delete_nonexistent_user\n\n   ### Edge Cases (2):\n   - test_email_max_length\n   - test_special_characters_in_email\n\n   ## Test Fixtures:\n   - create_test_user()\n   - create_test_user_with_email(email)\n\n   ## Mock Implementations:\n   - MockUserRepository\n\n   ## Run Tests:\n   ```bash\n   cargo test\n   cargo test --package [package_name]\n   cargo test test_create_user -- --nocapture\n   ```\n\n   ## Next Steps:\n   1. Review generated tests\n   2. Add more edge cases if needed\n   3. Run tests and verify they pass\n   4. Check coverage with cargo tarpaulin\n   ```\n\n## Test Naming Conventions\n\nUse descriptive names following this pattern:\n- `test_[function]_[scenario]_[expected_result]`\n\nExamples:\n- `test_create_user_with_valid_email_succeeds`\n- `test_create_user_with_empty_email_returns_validation_error`\n- `test_parse_config_with_invalid_json_returns_parse_error`\n\n## Important Patterns\n\n### Testing Result Types\n```rust\n#[test]\nfn test_returns_ok() {\n    let result = function();\n    assert!(result.is_ok());\n    assert_eq!(result.unwrap(), expected);\n}\n\n#[test]\nfn test_returns_specific_error() {\n    let result = function();\n    assert!(result.is_err());\n\n    match result {\n        Err(MyError::Specific) => (),  // Expected\n        _ => panic!(\"Wrong error type\"),\n    }\n}\n```\n\n### Testing Option Types\n```rust\n#[test]\nfn test_returns_some() {\n    let result = function();\n    assert!(result.is_some());\n    assert_eq!(result.unwrap(), expected);\n}\n\n#[test]\nfn test_returns_none() {\n    let result = function();\n    assert!(result.is_none());\n}\n```\n\n### Testing Panics\n```rust\n#[test]\n#[should_panic(expected = \"expected panic message\")]\nfn test_panics_on_invalid_input() {\n    function_that_panics();\n}\n```\n\n### Testing with assert_matches\n```rust\nuse assert_matches::assert_matches;\n\n#[test]\nfn test_error_variant() {\n    let result = function();\n    assert_matches!(result, Err(Error::Specific { .. }));\n}\n```\n\n## After Completion\n\nAsk the user:\n1. Do tests pass? (run `cargo test`)\n2. Are there more scenarios to test?\n3. Should we add integration tests?\n4. Do you want to check test coverage?"
              }
            ],
            "skills": [
              {
                "name": "mock-strategy-guide",
                "description": "Guides users on creating mock implementations for testing with traits, providing test doubles, and avoiding tight coupling to test infrastructure. Activates when users need to test code with external dependencies.",
                "path": "plugins/rust-testing/skills/mock-strategy-guide/SKILL.md",
                "frontmatter": {
                  "name": "mock-strategy-guide",
                  "description": "Guides users on creating mock implementations for testing with traits, providing test doubles, and avoiding tight coupling to test infrastructure. Activates when users need to test code with external dependencies.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Mock Strategy Guide Skill\n\nYou are an expert at testing strategies for Rust, especially creating mock implementations for hexagonal architecture. When you detect testing needs for code with dependencies, proactively suggest mocking strategies.\n\n## When to Activate\n\nActivate when you notice:\n- Code with external dependencies (DB, HTTP, etc.)\n- Trait-based abstractions for repositories or services\n- Tests that require real infrastructure\n- Questions about mocking or test doubles\n\n## Mock Implementation Patterns\n\n### Pattern 1: Simple Mock Repository\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n\n    struct MockUserRepository {\n        users: HashMap<String, User>,\n    }\n\n    impl MockUserRepository {\n        fn new() -> Self {\n            Self {\n                users: HashMap::new(),\n            }\n        }\n\n        fn with_user(mut self, user: User) -> Self {\n            self.users.insert(user.id.clone(), user);\n            self\n        }\n    }\n\n    #[async_trait]\n    impl UserRepository for MockUserRepository {\n        async fn find(&self, id: &str) -> Result<User, Error> {\n            self.users\n                .get(id)\n                .cloned()\n                .ok_or(Error::NotFound)\n        }\n\n        async fn save(&self, user: &User) -> Result<(), Error> {\n            // Mock just succeeds\n            Ok(())\n        }\n    }\n\n    #[tokio::test]\n    async fn test_user_service() {\n        // Arrange\n        let user = User { id: \"1\".to_string(), email: \"test@example.com\".to_string() };\n        let mock_repo = MockUserRepository::new().with_user(user.clone());\n        let service = UserService::new(mock_repo);\n\n        // Act\n        let result = service.get_user(\"1\").await;\n\n        // Assert\n        assert!(result.is_ok());\n        assert_eq!(result.unwrap().id, \"1\");\n    }\n}\n```\n\n### Pattern 2: Mock with Verification\n\n```rust\n#[cfg(test)]\nmod tests {\n    use std::sync::{Arc, Mutex};\n\n    struct MockEmailService {\n        sent_emails: Arc<Mutex<Vec<Email>>>,\n    }\n\n    impl MockEmailService {\n        fn new() -> Self {\n            Self {\n                sent_emails: Arc::new(Mutex::new(Vec::new())),\n            }\n        }\n\n        fn emails_sent(&self) -> Vec<Email> {\n            self.sent_emails.lock().unwrap().clone()\n        }\n    }\n\n    #[async_trait]\n    impl EmailService for MockEmailService {\n        async fn send(&self, email: Email) -> Result<(), Error> {\n            self.sent_emails.lock().unwrap().push(email);\n            Ok(())\n        }\n    }\n\n    #[tokio::test]\n    async fn test_sends_welcome_email() {\n        let mock_email = MockEmailService::new();\n        let service = UserService::new(mock_email.clone());\n\n        service.register_user(\"test@example.com\").await.unwrap();\n\n        // Verify email was sent\n        let emails = mock_email.emails_sent();\n        assert_eq!(emails.len(), 1);\n        assert_eq!(emails[0].to, \"test@example.com\");\n        assert!(emails[0].subject.contains(\"Welcome\"));\n    }\n}\n```\n\n### Pattern 3: Mock with Controlled Failures\n\n```rust\n#[cfg(test)]\nmod tests {\n    enum MockBehavior {\n        Success,\n        NotFound,\n        DatabaseError,\n    }\n\n    struct MockRepository {\n        behavior: MockBehavior,\n    }\n\n    impl MockRepository {\n        fn with_behavior(behavior: MockBehavior) -> Self {\n            Self { behavior }\n        }\n    }\n\n    #[async_trait]\n    impl UserRepository for MockRepository {\n        async fn find(&self, id: &str) -> Result<User, Error> {\n            match self.behavior {\n                MockBehavior::Success => Ok(test_user()),\n                MockBehavior::NotFound => Err(Error::NotFound),\n                MockBehavior::DatabaseError => Err(Error::Database(\"Connection failed\".into())),\n            }\n        }\n    }\n\n    #[tokio::test]\n    async fn test_handles_not_found() {\n        let mock = MockRepository::with_behavior(MockBehavior::NotFound);\n        let service = UserService::new(mock);\n\n        let result = service.get_user(\"1\").await;\n\n        assert!(result.is_err());\n        assert!(matches!(result.unwrap_err(), Error::NotFound));\n    }\n\n    #[tokio::test]\n    async fn test_handles_database_error() {\n        let mock = MockRepository::with_behavior(MockBehavior::DatabaseError);\n        let service = UserService::new(mock);\n\n        let result = service.get_user(\"1\").await;\n\n        assert!(result.is_err());\n    }\n}\n```\n\n### Pattern 4: Builder Pattern for Mocks\n\n```rust\n#[cfg(test)]\nmod tests {\n    struct MockRepositoryBuilder {\n        users: HashMap<String, User>,\n        find_error: Option<Error>,\n        save_error: Option<Error>,\n    }\n\n    impl MockRepositoryBuilder {\n        fn new() -> Self {\n            Self {\n                users: HashMap::new(),\n                find_error: None,\n                save_error: None,\n            }\n        }\n\n        fn with_user(mut self, user: User) -> Self {\n            self.users.insert(user.id.clone(), user);\n            self\n        }\n\n        fn with_find_error(mut self, error: Error) -> Self {\n            self.find_error = Some(error);\n            self\n        }\n\n        fn build(self) -> MockRepository {\n            MockRepository {\n                users: self.users,\n                find_error: self.find_error,\n                save_error: self.save_error,\n            }\n        }\n    }\n\n    #[tokio::test]\n    async fn test_with_builder() {\n        let mock = MockRepositoryBuilder::new()\n            .with_user(test_user())\n            .with_save_error(Error::Database(\"Save failed\".into()))\n            .build();\n\n        let service = UserService::new(mock);\n\n        // Can find user\n        let user = service.get_user(\"1\").await.unwrap();\n\n        // But save fails\n        let result = service.update_user(user).await;\n        assert!(result.is_err());\n    }\n}\n```\n\n## In-Memory Test Implementations\n\nFor integration tests with real logic but no infrastructure:\n\n```rust\npub struct InMemoryUserRepository {\n    users: Arc<Mutex<HashMap<String, User>>>,\n}\n\nimpl InMemoryUserRepository {\n    pub fn new() -> Self {\n        Self {\n            users: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n}\n\n#[async_trait]\nimpl UserRepository for InMemoryUserRepository {\n    async fn find(&self, id: &str) -> Result<User, Error> {\n        self.users\n            .lock()\n            .unwrap()\n            .get(id)\n            .cloned()\n            .ok_or(Error::NotFound)\n    }\n\n    async fn save(&self, user: &User) -> Result<(), Error> {\n        self.users\n            .lock()\n            .unwrap()\n            .insert(user.id.clone(), user.clone());\n        Ok(())\n    }\n\n    async fn delete(&self, id: &str) -> Result<(), Error> {\n        self.users\n            .lock()\n            .unwrap()\n            .remove(id)\n            .ok_or(Error::NotFound)?;\n        Ok(())\n    }\n}\n```\n\n## Test Fixture Helpers\n\n```rust\n#[cfg(test)]\nmod fixtures {\n    use super::*;\n\n    pub fn test_user() -> User {\n        User {\n            id: \"test-id\".to_string(),\n            email: \"test@example.com\".to_string(),\n            name: \"Test User\".to_string(),\n        }\n    }\n\n    pub fn test_user_with_id(id: &str) -> User {\n        User {\n            id: id.to_string(),\n            email: \"test@example.com\".to_string(),\n            name: \"Test User\".to_string(),\n        }\n    }\n\n    pub fn test_users(count: usize) -> Vec<User> {\n        (0..count)\n            .map(|i| test_user_with_id(&format!(\"user-{}\", i)))\n            .collect()\n    }\n}\n```\n\n## Your Approach\n\nWhen you see code needing tests:\n1. Identify external dependencies (traits)\n2. Suggest mock implementation structure\n3. Show verification patterns\n4. Provide test fixture helpers\n\nWhen you see tests without mocks:\n1. Suggest extracting trait if tightly coupled\n2. Show how to create mock implementations\n3. Demonstrate verification patterns\n\nProactively suggest mocking strategies for testable, maintainable code."
              },
              {
                "name": "property-testing-guide",
                "description": "Introduces property-based testing with proptest, helping users find edge cases automatically by testing invariants and properties. Activates when users test algorithms or data structures.",
                "path": "plugins/rust-testing/skills/property-testing-guide/SKILL.md",
                "frontmatter": {
                  "name": "property-testing-guide",
                  "description": "Introduces property-based testing with proptest, helping users find edge cases automatically by testing invariants and properties. Activates when users test algorithms or data structures.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Property-Based Testing Guide Skill\n\nYou are an expert at property-based testing in Rust using proptest. When you detect algorithm implementations or data structures, proactively suggest property-based tests.\n\n## When to Activate\n\nActivate when you notice:\n- Algorithm implementations (sorting, parsing, encoding)\n- Data structure implementations\n- Serialization/deserialization code\n- Functions with many edge cases\n- Questions about testing complex logic\n\n## Property-Based Testing Concepts\n\n**Traditional Testing**: Test specific inputs\n**Property Testing**: Test properties that should always hold\n\n### Example: Serialization\n\n**Traditional**:\n```rust\n#[test]\nfn test_serialize_user() {\n    let user = User { id: \"123\", email: \"test@example.com\" };\n    let json = serialize(user);\n    assert_eq!(json, r#\"{\"id\":\"123\",\"email\":\"test@example.com\"}\"#);\n}\n```\n\n**Property-Based**:\n```rust\nproptest! {\n    #[test]\n    fn test_serialization_roundtrip(id in \"[a-z0-9]+\", email in \"[a-z]+@[a-z]+\\\\.com\") {\n        let user = User { id, email: email.clone() };\n        let serialized = serialize(&user)?;\n        let deserialized = deserialize(&serialized)?;\n\n        // Property: roundtrip should preserve data\n        prop_assert_eq!(user.id, deserialized.id);\n        prop_assert_eq!(user.email, deserialized.email);\n    }\n}\n```\n\n## Common Properties to Test\n\n### 1. Roundtrip Properties\n\n**Pattern**:\n```rust\nuse proptest::prelude::*;\n\nproptest! {\n    #[test]\n    fn test_encode_decode_roundtrip(data in \".*\") {\n        let encoded = encode(&data);\n        let decoded = decode(&encoded)?;\n\n        // Property: encoding then decoding gives original\n        prop_assert_eq!(data, decoded);\n    }\n}\n```\n\n### 2. Idempotence\n\n**Pattern**:\n```rust\nproptest! {\n    #[test]\n    fn test_normalize_idempotent(s in \".*\") {\n        let normalized = normalize(&s);\n        let double_normalized = normalize(&normalized);\n\n        // Property: applying twice gives same result as once\n        prop_assert_eq!(normalized, double_normalized);\n    }\n}\n```\n\n### 3. Invariants\n\n**Pattern**:\n```rust\nproptest! {\n    #[test]\n    fn test_sort_invariants(mut vec in prop::collection::vec(any::<i32>(), 0..100)) {\n        let original_len = vec.len();\n        sort(&mut vec);\n\n        // Property 1: Length unchanged\n        prop_assert_eq!(vec.len(), original_len);\n\n        // Property 2: Sorted order\n        for i in 1..vec.len() {\n            prop_assert!(vec[i-1] <= vec[i]);\n        }\n    }\n}\n```\n\n### 4. Comparison with Oracle\n\n**Pattern**:\n```rust\nproptest! {\n    #[test]\n    fn test_custom_sort_matches_stdlib(mut vec in prop::collection::vec(any::<i32>(), 0..100)) {\n        let mut expected = vec.clone();\n        expected.sort();\n\n        custom_sort(&mut vec);\n\n        // Property: matches standard library behavior\n        prop_assert_eq!(vec, expected);\n    }\n}\n```\n\n### 5. Inverse Functions\n\n**Pattern**:\n```rust\nproptest! {\n    #[test]\n    fn test_add_subtract_inverse(a in any::<i32>(), b in any::<i32>()) {\n        if let Some(sum) = a.checked_add(b) {\n            let result = sum.checked_sub(b);\n\n            // Property: subtraction is inverse of addition\n            prop_assert_eq!(result, Some(a));\n        }\n    }\n}\n```\n\n## Custom Strategies\n\n### Strategy for Domain Types\n\n```rust\nuse proptest::prelude::*;\n\nfn user_strategy() -> impl Strategy<Value = User> {\n    (\"[a-z]{5,10}\", \"[a-z]{3,8}@[a-z]{3,8}\\\\.com\", 18..100u8)\n        .prop_map(|(name, email, age)| User {\n            name,\n            email,\n            age,\n        })\n}\n\nproptest! {\n    #[test]\n    fn test_user_validation(user in user_strategy()) {\n        // Property: all generated users should be valid\n        prop_assert!(validate_user(&user).is_ok());\n    }\n}\n```\n\n### Strategy with Constraints\n\n```rust\nfn positive_money() -> impl Strategy<Value = Money> {\n    (1..1_000_000u64).prop_map(|cents| Money::from_cents(cents))\n}\n\nproptest! {\n    #[test]\n    fn test_money_operations(a in positive_money(), b in positive_money()) {\n        let sum = a + b;\n\n        // Property: sum is greater than both operands\n        prop_assert!(sum >= a);\n        prop_assert!(sum >= b);\n    }\n}\n```\n\n## Testing Patterns\n\n### Pattern 1: Parser Testing\n\n```rust\nproptest! {\n    #[test]\n    fn test_parser_never_panics(s in \".*\") {\n        // Property: parser should never panic, only return Ok or Err\n        let _ = parse(&s);  // Should not panic\n    }\n\n    #[test]\n    fn test_valid_input_parses(\n        name in \"[a-zA-Z]+\",\n        age in 0..150u8,\n    ) {\n        let input = format!(\"{},{}\", name, age);\n        let result = parse(&input);\n\n        // Property: valid input always succeeds\n        prop_assert!(result.is_ok());\n    }\n}\n```\n\n### Pattern 2: Data Structure Invariants\n\n```rust\nproptest! {\n    #[test]\n    fn test_btree_invariants(\n        operations in prop::collection::vec(\n            prop_oneof![\n                any::<i32>().prop_map(Operation::Insert),\n                any::<i32>().prop_map(Operation::Remove),\n            ],\n            0..100\n        )\n    ) {\n        let mut tree = BTree::new();\n\n        for op in operations {\n            match op {\n                Operation::Insert(val) => tree.insert(val),\n                Operation::Remove(val) => tree.remove(val),\n            }\n\n            // Property: tree maintains balance invariant\n            prop_assert!(tree.is_balanced());\n            // Property: tree maintains order invariant\n            prop_assert!(tree.is_sorted());\n        }\n    }\n}\n```\n\n### Pattern 3: Equivalence Testing\n\n```rust\nproptest! {\n    #[test]\n    fn test_optimized_version_equivalent(data in prop::collection::vec(any::<i32>(), 0..100)) {\n        let result1 = slow_but_correct(&data);\n        let result2 = fast_optimized(&data);\n\n        // Property: optimized version gives same results\n        prop_assert_eq!(result1, result2);\n    }\n}\n```\n\n## Dependencies\n\n```toml\n[dev-dependencies]\nproptest = \"1.0\"\n```\n\n## Shrinking\n\nProptest automatically finds minimal failing cases:\n\n```rust\nproptest! {\n    #[test]\n    fn test_divide(a in any::<i32>(), b in any::<i32>()) {\n        let result = divide(a, b);  // Fails when b == 0\n\n        // proptest will shrink to smallest failing case: b = 0\n        prop_assert!(result.is_ok());\n    }\n}\n```\n\n## Your Approach\n\nWhen you see:\n1. **Serialization** â†’ Suggest roundtrip property\n2. **Sorting/ordering** â†’ Suggest invariant properties\n3. **Parsers** â†’ Suggest \"never panics\" property\n4. **Algorithms** â†’ Suggest comparison with oracle\n5. **Data structures** â†’ Suggest invariant testing\n\nProactively suggest property-based tests to find edge cases automatically."
              },
              {
                "name": "test-coverage-advisor",
                "description": "Reviews test coverage and suggests missing test cases for error paths, edge cases, and business logic. Activates when users write tests or implement new features.",
                "path": "plugins/rust-testing/skills/test-coverage-advisor/SKILL.md",
                "frontmatter": {
                  "name": "test-coverage-advisor",
                  "description": "Reviews test coverage and suggests missing test cases for error paths, edge cases, and business logic. Activates when users write tests or implement new features.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Test Coverage Advisor Skill\n\nYou are an expert at comprehensive test coverage in Rust. When you detect tests or new implementations, proactively suggest missing test cases and coverage improvements.\n\n## When to Activate\n\nActivate when you notice:\n- New function implementations without tests\n- Test modules with limited coverage\n- Functions with error handling but no error tests\n- Questions about testing strategy or coverage\n\n## Test Coverage Checklist\n\n### 1. Success Path Testing\n\n**What to Look For**: Missing happy path tests\n\n**Pattern**:\n```rust\n#[test]\nfn test_create_user_success() {\n    let user = User::new(\"test@example.com\".to_string(), 25).unwrap();\n    assert_eq!(user.email(), \"test@example.com\");\n    assert_eq!(user.age(), 25);\n}\n```\n\n### 2. Error Path Testing\n\n**What to Look For**: Functions returning Result but no error tests\n\n**Missing Tests**:\n```rust\npub fn validate_email(email: &str) -> Result<(), ValidationError> {\n    if email.is_empty() {\n        return Err(ValidationError::Empty);\n    }\n    if !email.contains('@') {\n        return Err(ValidationError::InvalidFormat);\n    }\n    Ok(())\n}\n\n// âŒ NO TESTS for error cases!\n```\n\n**Suggested Tests**:\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_validate_email_success() {\n        assert!(validate_email(\"test@example.com\").is_ok());\n    }\n\n    #[test]\n    fn test_validate_email_empty() {\n        let result = validate_email(\"\");\n        assert!(matches!(result, Err(ValidationError::Empty)));\n    }\n\n    #[test]\n    fn test_validate_email_missing_at_sign() {\n        let result = validate_email(\"invalid\");\n        assert!(matches!(result, Err(ValidationError::InvalidFormat)));\n    }\n\n    #[test]\n    fn test_validate_email_no_domain() {\n        let result = validate_email(\"test@\");\n        assert!(matches!(result, Err(ValidationError::InvalidFormat)));\n    }\n}\n```\n\n**Suggestion Template**:\n```\nYour function returns Result but I don't see tests for error cases. Consider adding:\n\n#[test]\nfn test_empty_input() {\n    let result = function(\"\");\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_invalid_format() {\n    let result = function(\"invalid\");\n    assert!(matches!(result, Err(SpecificError)));\n}\n```\n\n### 3. Edge Cases\n\n**What to Look For**: Missing boundary tests\n\n**Common Edge Cases**:\n- Empty collections\n- Single item collections\n- Maximum/minimum values\n- Null/None values\n- Zero values\n- Negative numbers\n\n**Pattern**:\n```rust\n#[test]\nfn test_empty_list() {\n    let result = process_items(vec![]);\n    assert!(result.is_empty());\n}\n\n#[test]\nfn test_single_item() {\n    let result = process_items(vec![item]);\n    assert_eq!(result.len(), 1);\n}\n\n#[test]\nfn test_max_size() {\n    let items = vec![item; 1000];\n    let result = process_items(items);\n    assert!(result.len() <= 1000);\n}\n```\n\n### 4. Async Function Testing\n\n**What to Look For**: Async functions without async tests\n\n**Pattern**:\n```rust\n#[tokio::test]\nasync fn test_fetch_user_success() {\n    let repo = setup_test_repo().await;\n    let user = repo.find_user(\"123\").await.unwrap();\n    assert_eq!(user.id(), \"123\");\n}\n\n#[tokio::test]\nasync fn test_fetch_user_not_found() {\n    let repo = setup_test_repo().await;\n    let result = repo.find_user(\"nonexistent\").await;\n    assert!(result.is_err());\n}\n```\n\n### 5. Table-Driven Tests\n\n**What to Look For**: Multiple similar test cases\n\n**Before (Repetitive)**:\n```rust\n#[test]\nfn test_valid_email1() {\n    assert!(validate_email(\"test@example.com\").is_ok());\n}\n\n#[test]\nfn test_valid_email2() {\n    assert!(validate_email(\"user@domain.org\").is_ok());\n}\n\n#[test]\nfn test_invalid_email1() {\n    assert!(validate_email(\"invalid\").is_err());\n}\n```\n\n**After (Table-Driven)**:\n```rust\n#[test]\nfn test_email_validation() {\n    let test_cases = vec![\n        (\"test@example.com\", true, \"Valid email\"),\n        (\"user@domain.org\", true, \"Valid email with org TLD\"),\n        (\"invalid\", false, \"Missing @ sign\"),\n        (\"test@\", false, \"Missing domain\"),\n        (\"@example.com\", false, \"Missing local part\"),\n        (\"\", false, \"Empty string\"),\n    ];\n\n    for (email, should_pass, description) in test_cases {\n        let result = validate_email(email);\n        assert_eq!(\n            result.is_ok(),\n            should_pass,\n            \"Failed for {}: {}\",\n            email,\n            description\n        );\n    }\n}\n```\n\n## Testing Anti-Patterns\n\n### âŒ Testing Implementation Details\n\n```rust\n// BAD: Testing private fields\n#[test]\nfn test_internal_state() {\n    let obj = MyStruct::new();\n    assert_eq!(obj.internal_counter, 0);  // Testing private implementation\n}\n\n// GOOD: Testing behavior\n#[test]\nfn test_public_behavior() {\n    let obj = MyStruct::new();\n    assert_eq!(obj.get_count(), 0);  // Testing public interface\n}\n```\n\n### âŒ Tests Without Assertions\n\n```rust\n// BAD: No assertion\n#[test]\nfn test_function() {\n    function();  // What are we testing?\n}\n\n// GOOD: Clear assertion\n#[test]\nfn test_function() {\n    let result = function();\n    assert!(result.is_ok());\n}\n```\n\n### âŒ Overly Complex Tests\n\n```rust\n// BAD: Test does too much\n#[test]\nfn test_everything() {\n    // 100 lines of setup\n    // Multiple operations\n    // Many assertions\n}\n\n// GOOD: Focused tests\n#[test]\nfn test_create() { /* ... */ }\n\n#[test]\nfn test_update() { /* ... */ }\n\n#[test]\nfn test_delete() { /* ... */ }\n```\n\n## Coverage Tools\n\n```bash\n# Using tarpaulin\ncargo install cargo-tarpaulin\ncargo tarpaulin --out Html\n\n# Using llvm-cov\ncargo install cargo-llvm-cov\ncargo llvm-cov --html\ncargo llvm-cov --open  # Open in browser\n```\n\n## Test Organization\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    // Helper functions\n    fn setup() -> TestData {\n        TestData::new()\n    }\n\n    // Success cases\n    mod success {\n        use super::*;\n\n        #[test]\n        fn test_valid_input() { /* ... */ }\n    }\n\n    // Error cases\n    mod errors {\n        use super::*;\n\n        #[test]\n        fn test_invalid_input() { /* ... */ }\n\n        #[test]\n        fn test_missing_data() { /* ... */ }\n    }\n\n    // Edge cases\n    mod edge_cases {\n        use super::*;\n\n        #[test]\n        fn test_empty_input() { /* ... */ }\n\n        #[test]\n        fn test_max_size() { /* ... */ }\n    }\n}\n```\n\n## Your Approach\n\nWhen you see implementations:\n1. Check for test module\n2. Identify untested error paths\n3. Look for missing edge cases\n4. Suggest specific test cases with code\n\nWhen you see tests:\n1. Check coverage of error paths\n2. Suggest table-driven tests for similar cases\n3. Point out missing edge cases\n4. Recommend organization improvements\n\nProactively suggest missing tests to improve robustness."
              }
            ]
          },
          {
            "name": "rust-modern-patterns",
            "description": "Modern Rust patterns for Rust 2024 Edition. Includes let chains, async closures, gen blocks, match ergonomics, const improvements. Commands for modernization, edition upgrade, and pattern checking. Expert agent for Rust 2024 migration and best practices. Requires Rust 1.85.0+",
            "source": "./plugins/rust-modern-patterns",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-modern-patterns@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/rust-async-traits",
                "description": "Modernize async trait usage to native async fn in traits",
                "path": "plugins/rust-modern-patterns/commands/rust-async-traits.md",
                "frontmatter": {
                  "description": "Modernize async trait usage to native async fn in traits"
                },
                "content": "You are helping modernize async trait definitions to use native async fn in traits instead of the async-trait crate.\n\n## Your Task\n\nConvert traits using the `async-trait` crate to native async fn in traits, which has been supported since Rust 1.75.\n\n## Background\n\n**Since Rust 1.75 (December 2023):** Async functions in traits are natively supported without requiring the `async-trait` crate.\n\n**When to use async-trait:**\n- Supporting Rust versions < 1.75\n- Need for `dyn Trait` (dynamic dispatch/object safety)\n- Specific edge cases with complex bounds\n\n**When to use native async fn:**\n- Rust 1.75 or later âœ…\n- Static dispatch (generics)\n- Modern codebases\n\n## Steps\n\n1. **Check Current Usage**\n\n   Scan for async-trait usage:\n   ```rust\n   use async_trait::async_trait;\n\n   #[async_trait]\n   trait MyTrait {\n       async fn method(&self) -> Result<T, E>;\n   }\n   ```\n\n2. **Verify Rust Version**\n\n   Check Cargo.toml:\n   ```toml\n   [package]\n   rust-version = \"1.75\"  # Or higher\n   ```\n\n   If rust-version < 1.75, ask user if they can upgrade.\n\n3. **Identify Use Cases**\n\n   Categorize each async trait:\n\n   **Can Remove async-trait (most common):**\n   - Trait used with generics/static dispatch\n   - No `Box<dyn Trait>` usage\n   - Rust 1.75+\n\n   **Must Keep async-trait:**\n   - Using `dyn Trait` for dynamic dispatch\n   - Supporting older Rust versions\n   - Object safety required\n\n4. **Convert to Native Async Fn**\n\n   **Before:**\n   ```rust\n   use async_trait::async_trait;\n\n   #[async_trait]\n   pub trait UserRepository: Send + Sync {\n       async fn find_user(&self, id: &str) -> Result<User, Error>;\n       async fn save_user(&self, user: &User) -> Result<(), Error>;\n       async fn delete_user(&self, id: &str) -> Result<(), Error>;\n   }\n\n   #[async_trait]\n   impl UserRepository for PostgresRepository {\n       async fn find_user(&self, id: &str) -> Result<User, Error> {\n           sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n               .fetch_one(&self.pool)\n               .await\n       }\n\n       async fn save_user(&self, user: &User) -> Result<(), Error> {\n           sqlx::query!(\n               \"INSERT INTO users (id, email) VALUES ($1, $2)\",\n               user.id,\n               user.email\n           )\n           .execute(&self.pool)\n           .await?;\n           Ok(())\n       }\n\n       async fn delete_user(&self, id: &str) -> Result<(), Error> {\n           sqlx::query!(\"DELETE FROM users WHERE id = $1\", id)\n               .execute(&self.pool)\n               .await?;\n           Ok(())\n       }\n   }\n   ```\n\n   **After:**\n   ```rust\n   // No async_trait import needed!\n\n   pub trait UserRepository: Send + Sync {\n       async fn find_user(&self, id: &str) -> Result<User, Error>;\n       async fn save_user(&self, user: &User) -> Result<(), Error>;\n       async fn delete_user(&self, id: &str) -> Result<(), Error>;\n   }\n\n   impl UserRepository for PostgresRepository {\n       async fn find_user(&self, id: &str) -> Result<User, Error> {\n           sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n               .fetch_one(&self.pool)\n               .await\n       }\n\n       async fn save_user(&self, user: &User) -> Result<(), Error> {\n           sqlx::query!(\n               \"INSERT INTO users (id, email) VALUES ($1, $2)\",\n               user.id,\n               user.email\n           )\n           .execute(&self.pool)\n           .await?;\n           Ok(())\n       }\n\n       async fn delete_user(&self, id: &str) -> Result<(), Error> {\n           sqlx::query!(\"DELETE FROM users WHERE id = $1\", id)\n               .execute(&self.pool)\n               .await?;\n           Ok(())\n       }\n   }\n   ```\n\n5. **Handle Dynamic Dispatch Cases**\n\n   If you need `dyn Trait`, keep async-trait:\n\n   ```rust\n   // When you need this:\n   let repo: Box<dyn UserRepository> = Box::new(PostgresRepository::new(pool));\n\n   // You MUST use async-trait for object safety:\n   use async_trait::async_trait;\n\n   #[async_trait]\n   pub trait UserRepository: Send + Sync {\n       async fn find_user(&self, id: &str) -> Result<User, Error>;\n   }\n   ```\n\n   Or redesign to avoid dynamic dispatch:\n\n   ```rust\n   // Alternative: Use generics instead\n   pub async fn process_users<R: UserRepository>(repo: R) {\n       // Works with any UserRepository implementation\n       let user = repo.find_user(\"123\").await.unwrap();\n   }\n   ```\n\n6. **Remove async-trait Dependency**\n\n   Update Cargo.toml:\n\n   **Before:**\n   ```toml\n   [dependencies]\n   async-trait = \"0.1\"\n   ```\n\n   **After:**\n   ```toml\n   [dependencies]\n   # async-trait removed - using native async fn in traits\n   ```\n\n   If still needed for some traits:\n   ```toml\n   [dependencies]\n   # Only needed for dyn Trait support\n   async-trait = \"0.1\"  # Optional, only for object-safe traits\n   ```\n\n7. **Update Imports**\n\n   Remove unused async-trait imports:\n\n   ```rust\n   // Remove this if no longer needed\n   use async_trait::async_trait;\n   ```\n\n8. **Run Tests**\n\n   Verify everything compiles and works:\n\n   ```bash\n   cargo check\n   cargo test\n   cargo clippy\n   ```\n\n9. **Provide Summary**\n\n   ```\n   âœ… Modernized async trait usage!\n\n   ## Changes Made:\n\n   ### Converted to Native Async Fn (3 traits):\n   - UserRepository (src/domain/user.rs)\n   - OrderRepository (src/domain/order.rs)\n   - PaymentGateway (src/ports/payment.rs)\n\n   ### Kept async-trait (1 trait):\n   - DynamicHandler (src/handlers/mod.rs)\n     Reason: Uses Box<dyn Trait> for plugin system\n\n   ## Dependency Updates:\n   - Removed async-trait from main dependencies\n   - Added as optional for dynamic dispatch cases\n\n   ## Benefits:\n   - âœ… Zero-cost abstraction (no boxing overhead)\n   - âœ… Simpler code (no macro needed)\n   - âœ… Better error messages\n   - âœ… Native language feature\n\n   ## Before/After Example:\n\n   Before:\n   ```rust\n   use async_trait::async_trait;\n\n   #[async_trait]\n   trait Repository {\n       async fn find(&self, id: &str) -> Result<Item, Error>;\n   }\n\n   #[async_trait]\n   impl Repository for MyRepo {\n       async fn find(&self, id: &str) -> Result<Item, Error> {\n           // ...\n       }\n   }\n   ```\n\n   After:\n   ```rust\n   // No import needed!\n\n   trait Repository {\n       async fn find(&self, id: &str) -> Result<Item, Error>;\n   }\n\n   impl Repository for MyRepo {\n       async fn find(&self, id: &str) -> Result<Item, Error> {\n           // ...\n       }\n   }\n   ```\n\n   ## Next Steps:\n   1. All tests passing âœ…\n   2. Consider removing async-trait entirely if unused\n   3. Update documentation\n   ```\n\n## Key Differences\n\n### Native Async Fn (Rust 1.75+)\n\n**Pros:**\n- No external dependency\n- Zero-cost abstraction\n- Better compiler errors\n- Simpler syntax\n- Native language feature\n\n**Cons:**\n- Cannot use with `dyn Trait` directly\n- Requires Rust 1.75+\n\n**Usage:**\n```rust\ntrait MyTrait {\n    async fn method(&self) -> Result<T, E>;\n}\n\n// Use with generics\nfn process<T: MyTrait>(t: T) { }\n```\n\n### Async-Trait Crate\n\n**Pros:**\n- Works with older Rust\n- Supports `dyn Trait`\n- Object-safe traits\n\n**Cons:**\n- External dependency\n- Macro overhead\n- Slight performance cost (boxing)\n\n**Usage:**\n```rust\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait MyTrait {\n    async fn method(&self) -> Result<T, E>;\n}\n\n// Can use with dyn\nlet t: Box<dyn MyTrait> = Box::new(impl);\n```\n\n## Migration Patterns\n\n### Pattern 1: Simple Repository\n\n```rust\n// Before\n#[async_trait]\ntrait Repository {\n    async fn get(&self, id: i32) -> Option<Item>;\n}\n\n// After\ntrait Repository {\n    async fn get(&self, id: i32) -> Option<Item>;\n}\n```\n\n### Pattern 2: Generic Service\n\n```rust\n// Before\n#[async_trait]\ntrait Service<T> {\n    async fn process(&self, item: T) -> Result<(), Error>;\n}\n\n// After\ntrait Service<T> {\n    async fn process(&self, item: T) -> Result<(), Error>;\n}\n```\n\n### Pattern 3: Multiple Async Methods\n\n```rust\n// Before\n#[async_trait]\ntrait Complex {\n    async fn fetch(&self) -> Result<Data, Error>;\n    async fn save(&self, data: Data) -> Result<(), Error>;\n    async fn delete(&self, id: i32) -> Result<(), Error>;\n}\n\n// After - just remove the macro!\ntrait Complex {\n    async fn fetch(&self) -> Result<Data, Error>;\n    async fn save(&self, data: Data) -> Result<(), Error>;\n    async fn delete(&self, id: i32) -> Result<(), Error>;\n}\n```\n\n### Pattern 4: Keep for Dynamic Dispatch\n\n```rust\n// When you need this:\nstruct PluginSystem {\n    plugins: Vec<Box<dyn Plugin>>,\n}\n\n// Keep async-trait:\n#[async_trait]\ntrait Plugin: Send + Sync {\n    async fn execute(&self) -> Result<(), Error>;\n}\n```\n\n## Important Notes\n\n- Native async fn in traits requires **Rust 1.75+**\n- Check MSRV before removing async-trait\n- `dyn Trait` requires async-trait (or alternatives)\n- Static dispatch (generics) works with native async fn\n- Performance is better with native async fn (no boxing)\n\n## Version Requirements\n\n| Feature | Rust Version | Notes |\n|---------|-------------|-------|\n| Async fn in traits | 1.75.0+ | Native support |\n| async-trait crate | Any | Fallback for older versions |\n| Return-position impl Trait | 1.75.0+ | Enables async fn |\n\n## After Completion\n\nAsk the user:\n1. Did all tests pass?\n2. Can we remove async-trait entirely?\n3. Are there any dyn Trait use cases remaining?\n4. Should we update documentation?"
              },
              {
                "name": "/rust-modernize",
                "description": "Analyze and modernize Rust code to use latest features and patterns",
                "path": "plugins/rust-modern-patterns/commands/rust-modernize.md",
                "frontmatter": {
                  "description": "Analyze and modernize Rust code to use latest features and patterns"
                },
                "content": "You are helping modernize Rust code to use the latest features from Rust 2024 Edition.\n\n## Your Task\n\nAnalyze Rust code and refactor it to use modern patterns including let chains, async closures, improved match ergonomics, and other Rust 2024 features.\n\n## Steps\n\n1. **Scan for Modernization Opportunities**\n\n   Look for these patterns that can be modernized:\n   - Nested if-let statements â†’ let chains\n   - Manual async closures â†’ native async closures\n   - Old match patterns â†’ improved match ergonomics\n   - Regular functions â†’ const functions where possible\n   - Manual iterators â†’ gen blocks\n   - Complex error propagation â†’ cleaner patterns\n\n2. **Categorize Findings**\n\n   Group by modernization type:\n   ```\n   Modernization Opportunities Found:\n\n   Let Chains (5):\n   - src/user.rs:42 - Nested if-let (3 levels deep)\n   - src/api.rs:15 - Multiple Option checks\n   ...\n\n   Async Closures (3):\n   - src/tasks.rs:28 - Manual async wrapper\n   ...\n\n   Const Opportunities (2):\n   - src/config.rs:10 - Function could be const\n   ...\n   ```\n\n3. **Ask User for Scope**\n\n   Ask which modernizations to apply:\n   - All recommended changes?\n   - Specific category (let chains, async, etc.)?\n   - Specific file or function?\n\n4. **Refactor Pattern: Nested If-Let â†’ Let Chains**\n\n   **Before:**\n   ```rust\n   if let Some(user) = get_user(id) {\n       if let Some(profile) = user.profile {\n           if profile.is_active {\n               if let Some(email) = profile.email {\n                   send_email(&email);\n               }\n           }\n       }\n   }\n   ```\n\n   **After:**\n   ```rust\n   if let Some(user) = get_user(id)\n       && let Some(profile) = user.profile\n       && profile.is_active\n       && let Some(email) = profile.email\n   {\n       send_email(&email);\n   }\n   ```\n\n5. **Refactor Pattern: Manual Async â†’ Async Closures**\n\n   **Before:**\n   ```rust\n   let futures: Vec<_> = items\n       .iter()\n       .map(|item| {\n           let item = item.clone();\n           async move {\n               process_item(item).await\n           }\n       })\n       .collect();\n   ```\n\n   **After:**\n   ```rust\n   let futures: Vec<_> = items\n       .iter()\n       .map(async |item| {\n           process_item(item).await\n       })\n       .collect();\n   ```\n\n6. **Refactor Pattern: Functions â†’ Const Functions**\n\n   **Before:**\n   ```rust\n   fn calculate_buffer_size(items: usize) -> usize {\n       items * std::mem::size_of::<Item>()\n   }\n\n   static BUFFER: [u8; calculate_buffer_size(100)]; // Error!\n   ```\n\n   **After:**\n   ```rust\n   const fn calculate_buffer_size(items: usize) -> usize {\n       items * std::mem::size_of::<Item>()\n   }\n\n   const BUFFER_SIZE: usize = calculate_buffer_size(100);\n   static BUFFER: [u8; BUFFER_SIZE] = [0; BUFFER_SIZE];\n   ```\n\n7. **Refactor Pattern: Manual Iterator â†’ Gen Block**\n\n   **Before:**\n   ```rust\n   struct FibIterator {\n       a: u64,\n       b: u64,\n   }\n\n   impl Iterator for FibIterator {\n       type Item = u64;\n\n       fn next(&mut self) -> Option<Self::Item> {\n           let current = self.a;\n           self.a = self.b;\n           self.b = current + self.b;\n           Some(current)\n       }\n   }\n\n   fn fibonacci() -> FibIterator {\n       FibIterator { a: 0, b: 1 }\n   }\n   ```\n\n   **After:**\n   ```rust\n   fn fibonacci() -> impl Iterator<Item = u64> {\n       gen {\n           let (mut a, mut b) = (0, 1);\n           loop {\n               yield a;\n               (a, b) = (b, a + b);\n           }\n       }\n   }\n   ```\n\n8. **Refactor Pattern: Match Ergonomics**\n\n   **Before (Rust 2021):**\n   ```rust\n   match &option {\n       Some(x) => {\n           // x is &T, need to clone or handle reference\n           process(x.clone());\n       }\n       None => {}\n   }\n   ```\n\n   **After (Rust 2024):**\n   ```rust\n   match &option {\n       Some(mut x) => {\n           // x is &mut T (not moved), can modify in place\n           x.update();\n       }\n       None => {}\n   }\n   ```\n\n9. **Check Edition and Version**\n\n   Ensure project supports modern features:\n\n   ```toml\n   [package]\n   edition = \"2024\"\n   rust-version = \"1.85\"  # For full 2024 support\n   ```\n\n   If edition needs updating:\n   - Ask user if they want to upgrade\n   - Run `cargo fix --edition` after updating\n   - Check for breaking changes\n\n10. **Provide Modernization Summary**\n\n    ```\n    âœ… Code modernized successfully!\n\n    ## Changes Made:\n\n    ### Let Chains (5 locations):\n    - src/user.rs:42 - Flattened 3-level nesting\n    - src/api.rs:15 - Combined Option checks\n    - src/validate.rs:88 - Simplified Result handling\n\n    ### Async Closures (3 locations):\n    - src/tasks.rs:28 - Replaced manual async wrapper\n    - src/jobs.rs:45 - Simplified async map\n    - src/handlers.rs:102 - Cleaner async callback\n\n    ### Const Functions (2 locations):\n    - src/config.rs:10 - Made calculate_size const\n    - src/utils.rs:25 - Made hash_string const\n\n    ### Gen Blocks (1 location):\n    - src/iter.rs:15 - Simplified iterator with gen block\n\n    ## Edition Status:\n    - Current: edition = \"2024\" âœ…\n    - MSRV: rust-version = \"1.85\" âœ…\n\n    ## Before/After Example:\n\n    Before:\n    ```rust\n    if let Some(user) = get_user() {\n        if let Some(email) = user.email {\n            if email.contains('@') {\n                send_email(&email);\n            }\n        }\n    }\n    ```\n\n    After:\n    ```rust\n    if let Some(user) = get_user()\n        && let Some(email) = user.email\n        && email.contains('@')\n    {\n        send_email(&email);\n    }\n    ```\n\n    ## Next Steps:\n    1. Run tests: `cargo test`\n    2. Check for warnings: `cargo check`\n    3. Review changes for correctness\n    4. Consider enabling more lints for modern patterns\n\n    ## Suggested Lints:\n    Add to Cargo.toml or lib.rs:\n    ```rust\n    #![warn(rust_2024_compatibility)]\n    #![warn(let_underscore_drop)]\n    ```\n    ```\n\n## Modernization Patterns\n\n### Let Chains\n\nLook for:\n- Multiple nested if-let\n- if-let followed by if condition\n- while-let with additional conditions\n\nConvert to:\n```rust\nif let Pattern1 = expr1\n    && let Pattern2 = expr2\n    && boolean_condition\n{\n    // body\n}\n```\n\n### Async Closures\n\nLook for:\n- `.map(|x| { let x = x.clone(); async move { ... } })`\n- Manual future wrapping\n- Complex async callback patterns\n\nConvert to:\n```rust\n.map(async |x| { ... })\n```\n\n### Const Functions\n\nLook for:\n- Functions with only const-safe operations\n- Compile-time computations\n- Functions used in const contexts\n\nConvert to:\n```rust\nconst fn function_name(...) -> ReturnType {\n    // const-safe operations only\n}\n```\n\n### Gen Blocks\n\nLook for:\n- Manual Iterator implementations\n- State machines for iteration\n- Complex iteration logic\n\nConvert to:\n```rust\ngen {\n    // yield values\n}\n```\n\n## Important Notes\n\n- Only apply changes if edition = \"2024\" or offer to upgrade\n- Test thoroughly after modernization\n- Some patterns require minimum Rust versions\n- Preserve behavior - modernization should not change logic\n- Document breaking changes if any\n\n## Version Requirements\n\n| Feature | Min Rust Version | Edition |\n|---------|-----------------|---------|\n| Let chains | 1.88.0 | 2024 |\n| Async closures | 1.85.0 | 2024 |\n| Gen blocks | 1.85.0 | 2024 |\n| Match ergonomics | 1.85.0 | 2024 |\n\n## After Completion\n\nAsk the user:\n1. Did all tests pass?\n2. Are there more files to modernize?\n3. Should we enable additional lints?\n4. Do you want to update documentation?"
              },
              {
                "name": "/rust-pattern-check",
                "description": "Check code for opportunities to use modern Rust patterns",
                "path": "plugins/rust-modern-patterns/commands/rust-pattern-check.md",
                "frontmatter": {
                  "description": "Check code for opportunities to use modern Rust patterns"
                },
                "content": "You are analyzing Rust code to identify opportunities for using modern patterns from Rust 2024 Edition.\n\n## Your Task\n\nScan the codebase for patterns that could be modernized and provide a detailed report with recommendations.\n\n## Steps\n\n1. **Check Edition and Version**\n\n   First, verify the project setup:\n\n   Read Cargo.toml:\n   ```\n   Current Configuration:\n   - Edition: [edition]\n   - Rust version: [rust-version if set]\n   - Toolchain: [rustc --version]\n   ```\n\n   If not on 2024:\n   ```\n   â„¹ï¸ Project is using edition [current]. Consider upgrading to 2024 Edition\n   to use latest features. Use `/rust-upgrade-edition` to upgrade.\n   ```\n\n2. **Scan for Nested If-Let Patterns**\n\n   Search for nested if-let that could use let chains:\n\n   Pattern to find:\n   ```rust\n   if let Pattern1 = expr1 {\n       if let Pattern2 = expr2 {\n           // Could be flattened with let chains\n       }\n   }\n   ```\n\n   Use Grep to search for:\n   - `if let` followed by another `if let` in same scope\n   - Multiple levels of nesting\n\n   Report:\n   ```\n   ## Let Chain Opportunities (5):\n\n   ### High Priority (deep nesting):\n   - src/user.rs:42 - 4 levels of nested if-let\n     Current: if let -> if let -> if let -> if let\n     Suggestion: Use let chains to flatten\n\n   - src/api.rs:88 - 3 levels of nested if-let\n     Current: Multiple Option unwrapping\n     Suggestion: Combine with && in single if\n\n   ### Medium Priority:\n   - src/validate.rs:15 - 2 levels with boolean check\n   - src/handler.rs:102 - if let with additional condition\n   ```\n\n3. **Scan for Manual Async Closures**\n\n   Look for patterns like:\n   ```rust\n   .map(|x| {\n       let x = x.clone();\n       async move { ... }\n   })\n   ```\n\n   Report:\n   ```\n   ## Async Closure Opportunities (3):\n\n   - src/tasks.rs:28\n     Current: Manual async move wrapper\n     Suggestion: Use async |x| { ... } syntax\n\n   - src/jobs.rs:45\n     Current: Clone before async move\n     Suggestion: Async closure can borrow directly\n   ```\n\n4. **Scan for Const Function Opportunities**\n\n   Look for functions that:\n   - Contain only const-safe operations\n   - Are used in const contexts\n   - Could be evaluated at compile time\n\n   Patterns to check:\n   - Pure computation functions\n   - Functions with no I/O or allocations\n   - Hash functions, calculations\n\n   Report:\n   ```\n   ## Const Function Opportunities (4):\n\n   - src/config.rs:10 - calculate_buffer_size\n     Current: fn calculate_buffer_size(n: usize) -> usize\n     Suggestion: Add const keyword for compile-time eval\n     Impact: Can be used in const/static initialization\n\n   - src/hash.rs:25 - hash_string\n     Current: Regular function\n     Suggestion: Make const fn for compile-time hashing\n   ```\n\n5. **Scan for Manual Iterator Implementations**\n\n   Look for:\n   - struct implementing Iterator trait\n   - Complex next() implementations\n   - State machine patterns\n\n   Report:\n   ```\n   ## Gen Block Opportunities (2):\n\n   - src/iter.rs:15 - FibonacciIterator\n     Current: 25 lines of Iterator impl\n     Suggestion: Replace with 8-line gen block\n     Benefit: Simpler, more maintainable\n\n   - src/tree.rs:48 - TreeTraversal\n     Current: Complex state machine\n     Suggestion: Recursive gen block\n   ```\n\n6. **Scan for Match Pattern Improvements**\n\n   Look for:\n   - Match on references with moves\n   - Unclear binding modes\n   - Patterns that would benefit from 2024 ergonomics\n\n   Report:\n   ```\n   ## Match Ergonomics Opportunities (3):\n\n   - src/process.rs:65\n     Current: Match with clone to avoid move\n     Suggestion: Use 2024 ergonomics for in-place modify\n\n   - src/validate.rs:42\n     Current: Explicit ref patterns\n     Suggestion: Can be simplified with 2024 ergonomics\n   ```\n\n7. **Scan for While-Let Chains**\n\n   Look for while-let with additional conditions:\n\n   Pattern:\n   ```rust\n   while let Some(item) = iterator.next() {\n       if condition {\n           // Could use while let with &&\n       }\n   }\n   ```\n\n   Report:\n   ```\n   ## While-Let Chain Opportunities (2):\n\n   - src/parser.rs:35\n     Current: while let with nested if\n     Suggestion: Combine with && condition\n   ```\n\n8. **Scan for Never Type Opportunities**\n\n   Look for functions that never return:\n   - Functions ending with process::exit\n   - Functions that always panic\n   - Infinite loops without return\n\n   Report:\n   ```\n   ## Never Type (!) Opportunities (3):\n\n   - src/error.rs:25 - fatal_error\n     Current: fn fatal_error(msg: &str)\n     Suggestion: Change to fn fatal_error(msg: &str) -> !\n     Benefit: Compiler knows function doesn't return\n   ```\n\n9. **Check for Outdated Idioms**\n\n   Look for patterns that are outdated:\n   - try! macro instead of ?\n   - match instead of if let\n   - Unnecessary type annotations\n   - Old-style error handling\n\n   Report:\n   ```\n   ## Outdated Idioms (2):\n\n   - src/legacy.rs:15\n     Current: try!(expression)\n     Suggestion: Use ? operator\n\n   - src/utils.rs:88\n     Current: match on Result with Ok/Err\n     Suggestion: Use if let Ok(...) for simpler case\n   ```\n\n10. **Generate Comprehensive Report**\n\n    ```\n    âœ… Pattern Check Complete\n\n    ## Summary:\n\n    ðŸ“Š Total Opportunities: 19\n\n    ### By Category:\n    - Let Chains: 5 opportunities (15 nested levels total)\n    - Async Closures: 3 opportunities\n    - Const Functions: 4 opportunities\n    - Gen Blocks: 2 opportunities\n    - Match Ergonomics: 3 opportunities\n    - While-Let Chains: 2 opportunities\n\n    ### Priority:\n    - High: 7 (deep nesting, clarity improvements)\n    - Medium: 8 (performance, modern idioms)\n    - Low: 4 (minor improvements)\n\n    ## Detailed Findings:\n\n    ### 1. Let Chains (High Priority)\n\n    **src/user.rs:42** (4 levels nested)\n    ```rust\n    // Current\n    if let Some(user) = get_user(id) {\n        if let Some(profile) = user.profile {\n            if profile.is_active {\n                if let Some(email) = profile.email {\n                    send_email(&email);\n                }\n            }\n        }\n    }\n\n    // Suggested\n    if let Some(user) = get_user(id)\n        && let Some(profile) = user.profile\n        && profile.is_active\n        && let Some(email) = profile.email\n    {\n        send_email(&email);\n    }\n    ```\n\n    **Impact:** Much clearer, reduces nesting from 4 to 1 level\n\n    ### 2. Const Functions (Medium Priority)\n\n    **src/config.rs:10**\n    ```rust\n    // Current\n    fn calculate_buffer_size(items: usize) -> usize {\n        items * std::mem::size_of::<Item>()\n    }\n\n    // Suggested\n    const fn calculate_buffer_size(items: usize) -> usize {\n        items * std::mem::size_of::<Item>()\n    }\n\n    // Enables\n    const BUFFER_SIZE: usize = calculate_buffer_size(1000);\n    static BUFFER: [u8; BUFFER_SIZE] = [0; BUFFER_SIZE];\n    ```\n\n    **Impact:** Compile-time computation, better performance\n\n    ### 3. Gen Blocks (Medium Priority)\n\n    **src/iter.rs:15**\n    ```rust\n    // Current: 25 lines\n    struct FibIterator { a: u64, b: u64 }\n    impl Iterator for FibIterator {\n        type Item = u64;\n        fn next(&mut self) -> Option<u64> {\n            let current = self.a;\n            self.a = self.b;\n            self.b = current + self.b;\n            Some(current)\n        }\n    }\n\n    // Suggested: 8 lines\n    fn fibonacci() -> impl Iterator<Item = u64> {\n        gen {\n            let (mut a, mut b) = (0, 1);\n            loop {\n                yield a;\n                (a, b) = (b, a + b);\n            }\n        }\n    }\n    ```\n\n    **Impact:** 60% less code, more maintainable\n\n    ## Edition Status:\n\n    Current: edition = \"2021\"\n    âš ï¸ To use these features, upgrade to edition = \"2024\"\n\n    Use: `/rust-upgrade-edition`\n\n    ## Recommendations:\n\n    1. **Immediate:** Upgrade to Rust 2024 Edition\n    2. **High Priority:** Apply let chain modernizations (5 locations)\n    3. **Medium Priority:** Convert to const functions (4 locations)\n    4. **Medium Priority:** Simplify with gen blocks (2 locations)\n    5. **Consider:** Async closure updates when appropriate\n\n    ## Estimated Impact:\n\n    - **Readability:** +40% (reduced nesting)\n    - **Code reduction:** -15% (simpler patterns)\n    - **Performance:** +5% (compile-time computation)\n    - **Maintainability:** +30% (modern idioms)\n\n    ## Next Steps:\n\n    ```bash\n    # 1. Upgrade edition\n    /rust-upgrade-edition\n\n    # 2. Apply modernizations\n    /rust-modernize\n\n    # 3. Run tests\n    cargo test\n    ```\n\n    ## Need Help?\n\n    - Ask `rust-modern-expert` for detailed guidance\n    - Use `/rust-modernize` to apply changes automatically\n    - See examples in CONTEXT.md\n    ```\n\n## Search Patterns\n\nUse Grep to find these patterns:\n\n### Nested If-Let\n```\nPattern: if let.*\\{[^}]*if let\n```\n\n### Manual Async Closures\n```\nPattern: async move\n```\n\n### Non-const Functions\n```\nPattern: fn .+\\(.*\\) -> (usize|i32|u32|bool)\nFilter out those already const\n```\n\n### Manual Iterators\n```\nPattern: impl Iterator for\n```\n\n## Scoring System\n\nAssign priority based on:\n\n**High Priority:**\n- 3+ levels of nesting\n- Functions in hot paths\n- Widely used patterns\n\n**Medium Priority:**\n- 2 levels of nesting\n- Const-eligible functions\n- Iterator simplifications\n\n**Low Priority:**\n- Style improvements\n- Minor simplifications\n\n## Important Notes\n\n- Only suggest changes compatible with current edition\n- Note if edition upgrade is required\n- Estimate impact (readability, performance, maintainability)\n- Prioritize changes by value\n- Provide before/after examples\n\n## After Completion\n\nAsk the user:\n1. Do you want to apply these modernizations?\n2. Should we start with high priority items?\n3. Do you want to upgrade the edition first?\n4. Would you like detailed explanations for any items?"
              },
              {
                "name": "/rust-quality-check",
                "description": "Run comprehensive quality checks using modern Rust tooling (fmt, clippy, nextest, audit, deny)",
                "path": "plugins/rust-modern-patterns/commands/rust-quality-check.md",
                "frontmatter": {
                  "description": "Run comprehensive quality checks using modern Rust tooling (fmt, clippy, nextest, audit, deny)"
                },
                "content": "Run a comprehensive quality check suite on the current Rust project using modern tooling best practices.\n\n## What This Command Does\n\nThis command runs a complete quality assurance suite including:\n\n1. **Code Formatting** - Verify code follows standard formatting\n2. **Linting** - Run clippy with strict settings\n3. **Testing** - Execute tests with cargo-nextest\n4. **Security Audit** - Check for known vulnerabilities\n5. **Dependency Checks** - Validate licenses and sources (if configured)\n6. **SemVer Check** - Verify API compatibility (for libraries)\n\n## Process\n\n### 1. Check Project Structure\n\nFirst, verify this is a Rust project:\n- Look for `Cargo.toml` in current directory\n- Determine if this is a library or binary (affects checks)\n- Check for existing configurations (deny.toml, clippy.toml, etc.)\n\n### 2. Run Quality Checks\n\nExecute checks in this order:\n\n#### Format Check\n```bash\ncargo fmt --all -- --check\n```\n- Verifies code follows rustfmt standards\n- **Fails if**: Code is not formatted\n- **Fix**: Run `cargo fmt --all`\n\n#### Clippy Linting\n```bash\ncargo clippy --all-targets --all-features -- -D warnings\n```\n- Runs comprehensive linting\n- **Fails if**: Any clippy warnings exist\n- **Fix**: Address warnings or use `#[allow(...)]` with justification\n\n#### Test Suite\n```bash\n# Check if nextest is available\nif command -v cargo-nextest &> /dev/null; then\n    cargo nextest run --all-features\n    cargo test --doc  # nextest doesn't run doctests\nelse\n    cargo test --all-features\nfi\n```\n- Runs all tests\n- **Fails if**: Any test fails\n- **Fix**: Debug and fix failing tests\n\n#### Security Audit\n```bash\n# Check if cargo-audit is available\nif command -v cargo-audit &> /dev/null; then\n    cargo audit\nelse\n    echo \"âš ï¸ cargo-audit not installed. Run: cargo install cargo-audit\"\nfi\n```\n- Checks dependencies against RustSec database\n- **Fails if**: Known vulnerabilities found\n- **Fix**: Update dependencies or review advisories\n\n#### Dependency Validation (Optional)\n```bash\n# Only if deny.toml exists\nif [ -f \"deny.toml\" ]; then\n    if command -v cargo-deny &> /dev/null; then\n        cargo deny check\n    else\n        echo \"âš ï¸ deny.toml found but cargo-deny not installed\"\n        echo \"    Run: cargo install cargo-deny\"\n    fi\nfi\n```\n- Checks licenses, sources, bans, and advisories\n- **Fails if**: Policy violations found\n- **Fix**: Update dependencies or adjust policy\n\n#### SemVer Check (Libraries Only)\n```bash\n# Check if this is a library and cargo-semver-checks is available\nif grep -q \"\\\\[lib\\\\]\" Cargo.toml; then\n    if command -v cargo-semver-checks &> /dev/null; then\n        cargo semver-checks check-release\n    else\n        echo \"ðŸ“š Library detected. Consider installing cargo-semver-checks\"\n        echo \"    Run: cargo install cargo-semver-checks\"\n    fi\nfi\n```\n- Verifies API changes follow semantic versioning\n- **Fails if**: Breaking changes in non-major version\n- **Fix**: Bump version appropriately or fix API\n\n### 3. Report Results\n\nProvide a summary of all checks:\n\n```\nâœ… Rust Quality Check Results\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâœ… Format Check      - Passed\nâœ… Clippy Linting    - Passed\nâœ… Test Suite        - Passed (42 tests)\nâœ… Security Audit    - Passed (no vulnerabilities)\nâœ… Dependency Check  - Passed\nâœ… SemVer Check      - Passed\n\nAll checks passed! ðŸŽ‰\n```\n\nOr if issues found:\n\n```\nâŒ Rust Quality Check Results\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâŒ Format Check      - FAILED\n   Run: cargo fmt --all\n\nâœ… Clippy Linting    - Passed\nâŒ Test Suite        - FAILED (2 tests failed)\nâš ï¸  Security Audit    - WARNINGS (1 vulnerability)\n   Update: tokio 1.25 -> 1.26 (RUSTSEC-2023-0001)\n\nâœ… Dependency Check  - Passed\n\nFix these issues before committing.\n```\n\n## Tool Installation Guide\n\nIf tools are missing, provide installation instructions:\n\n```bash\n# Essential tools for quality checks\ncargo install cargo-nextest     # Faster test runner\ncargo install cargo-audit       # Security scanning\ncargo install cargo-deny        # Dependency validation\ncargo install cargo-semver-checks # API compatibility\n\n# Optional but recommended\ncargo install bacon             # Continuous feedback\ncargo install flamegraph        # Performance profiling\n```\n\n## Configuration Recommendations\n\n### Create clippy.toml\n\nIf `clippy.toml` doesn't exist, suggest creating one:\n\n```toml\n# clippy.toml - Clippy configuration\ncognitive-complexity-threshold = 30\nsingle-char-binding-names-threshold = 5\ntoo-many-arguments-threshold = 7\n```\n\n### Create deny.toml\n\nIf `deny.toml` doesn't exist for a project with dependencies, suggest:\n\n```bash\ncargo deny init\n```\n\nThen review and adjust the generated configuration.\n\n### Update Cargo.toml\n\nSuggest adding these to project Cargo.toml:\n\n```toml\n[package]\nedition = \"2024\"  # Use latest edition\nrust-version = \"1.85\"  # Set MSRV\n\n[profile.release]\ndebug = true  # For profiling\n\n[profile.dev]\n# Enable some optimizations for faster dev builds\nopt-level = 1\n```\n\n## CI/CD Integration\n\nProvide a GitHub Actions workflow snippet:\n\n```yaml\nname: Quality Checks\n\non: [push, pull_request]\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache dependencies\n        uses: Swatinem/rust-cache@v2\n\n      - name: Install tools\n        run: |\n          cargo install cargo-nextest\n          cargo install cargo-audit\n          cargo install cargo-deny\n\n      - name: Format check\n        run: cargo fmt --all -- --check\n\n      - name: Clippy\n        run: cargo clippy --all-targets --all-features -- -D warnings\n\n      - name: Tests\n        run: |\n          cargo nextest run --all-features\n          cargo test --doc\n\n      - name: Security audit\n        run: cargo audit\n\n      - name: Dependency check\n        run: cargo deny check\n```\n\n## Best Practices\n\nWhen running quality checks:\n\n1. **Run locally before pushing** - Catch issues early\n2. **Fix formatting first** - Easiest to resolve\n3. **Address clippy warnings** - They often catch real bugs\n4. **Don't skip tests** - Even if they're slow\n5. **Review security advisories** - Don't just update blindly\n6. **Keep tools updated** - `cargo install --force <tool>`\n7. **Configure in CI** - Enforce quality automatically\n\n## Troubleshooting\n\n### \"cargo-nextest not found\"\n```bash\ncargo install cargo-nextest\n```\n\n### \"cargo-audit not found\"\n```bash\ncargo install cargo-audit\n```\n\n### Clippy warnings overwhelming\n```bash\n# Fix incrementally\ncargo clippy --fix --allow-dirty --allow-staged\n```\n\n### Tests fail on CI but pass locally\n- Check for race conditions\n- Ensure deterministic behavior\n- Use cargo-nextest's flaky test detection\n\n### Security vulnerabilities can't be fixed\n- Check if patched versions exist\n- Review the advisory details\n- Consider alternatives if no fix available\n- Document accepted risks\n\n## Output Format\n\nProvide structured output:\n\n```\nðŸ” Running Rust Quality Checks...\n\n[1/6] Format Check...\n  âœ… Code is properly formatted\n\n[2/6] Clippy Linting...\n  âœ… No warnings found\n\n[3/6] Test Suite...\n  Running 42 tests...\n  âœ… All tests passed (42/42)\n\n[4/6] Security Audit...\n  Scanning 187 dependencies...\n  âœ… No vulnerabilities found\n\n[5/6] Dependency Check...\n  âœ… All licenses approved\n  âœ… All sources verified\n\n[6/6] SemVer Check...\n  âœ… No breaking changes detected\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâœ… All Quality Checks Passed\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nReady to commit! ðŸš€\n```\n\n## Your Task\n\nExecute the comprehensive quality check suite:\n\n1. Verify project structure\n2. Check for required tools\n3. Run all available checks\n4. Provide clear summary\n5. Suggest fixes for failures\n6. Recommend tool installations if needed\n7. Offer configuration improvements\n\nMake the output clear, actionable, and encouraging!"
              },
              {
                "name": "/rust-setup-tooling",
                "description": "Set up modern Rust development tooling (bacon, nextest, audit, deny, clippy config)",
                "path": "plugins/rust-modern-patterns/commands/rust-setup-tooling.md",
                "frontmatter": {
                  "description": "Set up modern Rust development tooling (bacon, nextest, audit, deny, clippy config)"
                },
                "content": "Set up a modern Rust development environment with best-in-class tooling for 2025.\n\n## What This Command Does\n\nThis command configures your Rust project with:\n\n1. **Essential Development Tools** - Install bacon, cargo-nextest, etc.\n2. **Quality Assurance Tools** - Set up clippy, rustfmt, cargo-audit\n3. **Security Tools** - Configure cargo-audit and cargo-deny\n4. **Configuration Files** - Create clippy.toml, deny.toml, rustfmt.toml\n5. **CI/CD Template** - Provide GitHub Actions workflow\n6. **Documentation** - Add tool usage guide to project\n\n## Process\n\n### 1. Assess Current Setup\n\nCheck what's already installed:\n\n```bash\n# Check Rust version\nrustc --version\nrustup --version\n\n# Check for existing tools\ncargo-nextest --version 2>/dev/null\ncargo-audit --version 2>/dev/null\ncargo-deny --version 2>/dev/null\nbacon --version 2>/dev/null\nflamegraph --version 2>/dev/null\ncargo-semver-checks --version 2>/dev/null\n```\n\n### 2. Install Essential Tools\n\nInstall missing tools with user confirmation:\n\n```bash\necho \"Installing modern Rust tooling...\"\necho \"\"\necho \"Essential tools:\"\necho \"  â€¢ bacon           - Background compiler\"\necho \"  â€¢ cargo-nextest   - Fast test runner\"\necho \"  â€¢ cargo-audit     - Security scanner\"\necho \"  â€¢ cargo-deny      - Dependency validator\"\necho \"\"\necho \"Optional tools:\"\necho \"  â€¢ flamegraph      - Performance profiler\"\necho \"  â€¢ cargo-semver-checks - API compatibility\"\necho \"  â€¢ cargo-machete   - Unused dependency finder\"\necho \"\"\n\n# Install essentials\ncargo install bacon\ncargo install cargo-nextest\ncargo install cargo-audit\ncargo install cargo-deny\n\n# Optionally install others\n# cargo install flamegraph\n# cargo install cargo-semver-checks\n# cargo install cargo-machete\n```\n\n### 3. Create Configuration Files\n\n#### clippy.toml\n\nCreate `clippy.toml` with sensible defaults:\n\n```toml\n# clippy.toml - Clippy linter configuration\n# See: https://doc.rust-lang.org/clippy/\n\n# Complexity thresholds\ncognitive-complexity-threshold = 30\ntoo-many-arguments-threshold = 7\ntoo-many-lines-threshold = 150\nlarge-error-threshold = 128\n\n# Naming conventions\nsingle-char-binding-names-threshold = 5\n\n# Documentation\nmissing-docs-in-private-items = false\n\n# Allow some pedantic lints that are too noisy\n# Uncomment to allow:\n# doc-markdown = \"allow\"\n# module-name-repetitions = \"allow\"\n# missing-errors-doc = \"allow\"\n```\n\n#### rustfmt.toml\n\nCreate `rustfmt.toml` for consistent formatting:\n\n```toml\n# rustfmt.toml - Rustfmt configuration\n# See: https://rust-lang.github.io/rustfmt/\n\nedition = \"2024\"\n\n# Line length\nmax_width = 100\nhard_tabs = false\ntab_spaces = 4\n\n# Imports\nimports_granularity = \"Crate\"\ngroup_imports = \"StdExternalCrate\"\nreorder_imports = true\n\n# Comments and docs\nwrap_comments = true\nformat_code_in_doc_comments = true\nnormalize_comments = true\n\n# Misc\nuse_field_init_shorthand = true\nuse_try_shorthand = true\n```\n\n#### deny.toml\n\nInitialize cargo-deny configuration:\n\n```bash\ncargo deny init\n```\n\nThen customize the generated `deny.toml`:\n\n```toml\n# deny.toml - Cargo-deny configuration\n\n[advisories]\nvulnerability = \"deny\"\nunmaintained = \"warn\"\nunsound = \"warn\"\nyanked = \"warn\"\nnotice = \"warn\"\n\n[licenses]\nunlicensed = \"deny\"\n# Adjust allowed licenses for your needs\nallow = [\n    \"MIT\",\n    \"Apache-2.0\",\n    \"BSD-3-Clause\",\n    \"BSD-2-Clause\",\n    \"ISC\",\n]\nconfidence-threshold = 0.8\n\n[bans]\nmultiple-versions = \"warn\"\nwildcards = \"warn\"\nhighlight = \"all\"\n\n# Ban known problematic crates (customize as needed)\ndeny = [\n    # Example: { name = \"openssl\", use-instead = \"rustls\" },\n]\n\n[sources]\nunknown-registry = \"deny\"\nunknown-git = \"warn\"\nallow-registry = [\"https://github.com/rust-lang/crates.io-index\"]\n```\n\n#### .cargo/config.toml\n\nCreate `.cargo/config.toml` for local development settings:\n\n```toml\n# .cargo/config.toml - Cargo configuration\n\n[alias]\n# Convenient aliases\ncheck-all = \"check --all-targets --all-features\"\ntest-all = \"nextest run --all-features\"\nlint = \"clippy --all-targets --all-features -- -D warnings\"\nquality = \"run --bin rust-quality-check\"\n\n[build]\n# Increase parallel compilation\njobs = 8  # Adjust based on CPU cores\n\n[term]\n# Better progress bars\nprogress.when = \"auto\"\nprogress.width = 80\n```\n\n### 4. Update Cargo.toml\n\nSuggest updates to the project's `Cargo.toml`:\n\n```toml\n[package]\nname = \"my-project\"\nversion = \"0.1.0\"\nedition = \"2024\"  # Use latest edition\nrust-version = \"1.85\"  # Set minimum Rust version (MSRV)\n\n# Add lints\n[lints.rust]\nunsafe_code = \"forbid\"  # Adjust as needed\nmissing_docs = \"warn\"\n\n[lints.clippy]\nall = \"warn\"\npedantic = \"warn\"\nnursery = \"warn\"\ncargo = \"warn\"\n\n# Allow some pedantic lints that are too noisy\nmodule_name_repetitions = \"allow\"\nmissing_errors_doc = \"allow\"\n\n[profile.dev]\n# Faster iterative compilation\nopt-level = 1\n\n[profile.release]\n# Enable debug symbols for profiling\ndebug = true\nlto = true\ncodegen-units = 1\n```\n\n### 5. Create Development Scripts\n\n#### scripts/quality.sh\n\nCreate a pre-commit script:\n\n```bash\n#!/bin/bash\n# scripts/quality.sh - Run quality checks\n\nset -e\n\necho \"ðŸ” Running quality checks...\"\necho \"\"\n\necho \"ðŸ“ Formatting...\"\ncargo fmt --all -- --check\n\necho \"âœ¨ Linting...\"\ncargo clippy --all-targets --all-features -- -D warnings\n\necho \"ðŸ§ª Testing...\"\nif command -v cargo-nextest &> /dev/null; then\n    cargo nextest run --all-features\n    cargo test --doc\nelse\n    cargo test --all-features\nfi\n\necho \"ðŸ”’ Security audit...\"\ncargo audit\n\necho \"ðŸ“¦ Dependency check...\"\nif [ -f \"deny.toml\" ]; then\n    cargo deny check\nfi\n\necho \"\"\necho \"âœ… All checks passed!\"\n```\n\nMake it executable:\n```bash\nchmod +x scripts/quality.sh\n```\n\n#### scripts/dev.sh\n\nCreate a development startup script:\n\n```bash\n#!/bin/bash\n# scripts/dev.sh - Start development environment\n\necho \"ðŸš€ Starting Rust development environment...\"\necho \"\"\n\n# Start bacon in background\necho \"Starting bacon clippy...\"\nbacon clippy &\nBACON_PID=$!\n\n# Trap Ctrl+C to clean up\ntrap \"echo ''; echo 'Shutting down...'; kill $BACON_PID 2>/dev/null; exit\" INT TERM\n\necho \"\"\necho \"âœ… Development environment ready!\"\necho \"\"\necho \"  ðŸ“ Bacon is running clippy in the background\"\necho \"  ðŸ”§ Make changes and see feedback automatically\"\necho \"\"\necho \"Press Ctrl+C to stop\"\necho \"\"\n\n# Keep script running\nwait $BACON_PID\n```\n\nMake it executable:\n```bash\nchmod +x scripts/dev.sh\n```\n\n### 6. Create CI/CD Workflow\n\nCreate `.github/workflows/ci.yml`:\n\n```yaml\nname: CI\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main, develop]\n\nenv:\n  CARGO_TERM_COLOR: always\n  RUSTFLAGS: -D warnings\n\njobs:\n  check:\n    name: Check\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: rustfmt, clippy\n\n      - name: Cache dependencies\n        uses: Swatinem/rust-cache@v2\n\n      - name: Check formatting\n        run: cargo fmt --all -- --check\n\n      - name: Run clippy\n        run: cargo clippy --all-targets --all-features -- -D warnings\n\n      - name: Check cargo.toml\n        run: cargo check --all-features\n\n  test:\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache dependencies\n        uses: Swatinem/rust-cache@v2\n\n      - name: Install nextest\n        uses: taiki-e/install-action@nextest\n\n      - name: Run tests\n        run: cargo nextest run --all-features\n\n      - name: Run doctests\n        run: cargo test --doc\n\n  security:\n    name: Security Audit\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache dependencies\n        uses: Swatinem/rust-cache@v2\n\n      - name: Install cargo-audit\n        run: cargo install cargo-audit\n\n      - name: Run security audit\n        run: cargo audit\n\n      - name: Install cargo-deny\n        run: cargo install cargo-deny\n\n      - name: Check dependencies\n        run: cargo deny check\n\n  coverage:\n    name: Code Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Install tarpaulin\n        run: cargo install cargo-tarpaulin\n\n      - name: Generate coverage\n        run: cargo tarpaulin --out xml --all-features\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          files: ./cobertura.xml\n```\n\n### 7. Create Documentation\n\nCreate `DEVELOPMENT.md`:\n\n```markdown\n# Development Guide\n\n## Getting Started\n\n### Prerequisites\n\n- Rust 1.85 or later\n- Modern Rust tooling (see setup below)\n\n### Setup\n\nInstall development tools:\n\n\\`\\`\\`bash\n./scripts/setup-tooling.sh  # Or manually install tools\n\\`\\`\\`\n\n### Development Workflow\n\nStart the development environment:\n\n\\`\\`\\`bash\n./scripts/dev.sh\n\\`\\`\\`\n\nThis starts bacon for continuous feedback. Make changes and see linting results automatically.\n\n### Before Committing\n\nRun quality checks:\n\n\\`\\`\\`bash\n./scripts/quality.sh\n# Or use the alias:\ncargo quality\n\\`\\`\\`\n\nThis runs:\n- Code formatting check\n- Clippy linting\n- All tests\n- Security audit\n- Dependency validation\n\n## Tools\n\n### bacon\nContinuous background compilation and linting.\n\n\\`\\`\\`bash\nbacon clippy  # Run clippy continuously\nbacon test    # Run tests continuously\n\\`\\`\\`\n\n### cargo-nextest\nFaster test runner with better output.\n\n\\`\\`\\`bash\ncargo nextest run           # Run all tests\ncargo nextest run test_name # Run specific test\n\\`\\`\\`\n\n### cargo-audit\nSecurity vulnerability scanner.\n\n\\`\\`\\`bash\ncargo audit        # Check for vulnerabilities\ncargo audit fix    # Update dependencies\n\\`\\`\\`\n\n### cargo-deny\nDependency validator for licenses, sources, and security.\n\n\\`\\`\\`bash\ncargo deny check            # Check all policies\ncargo deny check licenses   # Check licenses only\n\\`\\`\\`\n\n### flamegraph\nPerformance profiler.\n\n\\`\\`\\`bash\ncargo flamegraph --bin myapp\n\\`\\`\\`\n\n## Configuration\n\n- `clippy.toml` - Clippy linting rules\n- `rustfmt.toml` - Code formatting rules\n- `deny.toml` - Dependency policies\n- `.cargo/config.toml` - Cargo aliases and settings\n\n## CI/CD\n\nAll checks run automatically in CI:\n- Format checking\n- Clippy linting\n- Test suite\n- Security audit\n- Dependency validation\n\nSee `.github/workflows/ci.yml` for details.\n\\`\\`\\`\n\n### 8. Provide Setup Summary\n\nAfter completion, show summary:\n\n```\nâœ… Rust Development Tooling Setup Complete!\n\nInstalled Tools:\n  âœ… bacon            - Background compiler\n  âœ… cargo-nextest    - Fast test runner\n  âœ… cargo-audit      - Security scanner\n  âœ… cargo-deny       - Dependency validator\n\nCreated Configurations:\n  âœ… clippy.toml      - Linting rules\n  âœ… rustfmt.toml     - Formatting rules\n  âœ… deny.toml        - Dependency policies\n  âœ… .cargo/config.toml - Cargo settings\n\nCreated Scripts:\n  âœ… scripts/quality.sh - Pre-commit checks\n  âœ… scripts/dev.sh     - Development environment\n\nCreated Workflows:\n  âœ… .github/workflows/ci.yml - CI pipeline\n\nCreated Documentation:\n  âœ… DEVELOPMENT.md   - Developer guide\n\nNext Steps:\n  1. Review and adjust configurations to your needs\n  2. Run: ./scripts/dev.sh\n  3. Make changes and see instant feedback\n  4. Before committing: ./scripts/quality.sh\n\nHappy coding! ðŸ¦€âœ¨\n```\n\n## Tool Descriptions\n\nExplain each tool's purpose:\n\n- **bacon**: Watches files and runs cargo commands, showing minimal, actionable output\n- **cargo-nextest**: Runs tests in parallel with better reporting, 60% faster than cargo test\n- **cargo-audit**: Scans dependencies for security vulnerabilities from RustSec database\n- **cargo-deny**: Validates licenses, sources, and checks for banned/duplicated dependencies\n- **cargo-semver-checks**: Ensures API changes follow semantic versioning (for libraries)\n- **flamegraph**: Generates flamegraphs for performance profiling\n\n## Your Task\n\nSet up modern Rust development tooling:\n\n1. Check current tool installation\n2. Install missing essential tools\n3. Create configuration files\n4. Set up development scripts\n5. Create CI/CD workflow\n6. Generate documentation\n7. Provide clear next steps\n\nMake the setup smooth and explain what each tool does!"
              },
              {
                "name": "/rust-upgrade-edition",
                "description": "Upgrade Rust project to Rust 2024 Edition",
                "path": "plugins/rust-modern-patterns/commands/rust-upgrade-edition.md",
                "frontmatter": {
                  "description": "Upgrade Rust project to Rust 2024 Edition"
                },
                "content": "You are helping upgrade a Rust project to Rust 2024 Edition.\n\n## Your Task\n\nGuide the user through upgrading their project from an older edition (2015, 2018, or 2021) to Rust 2024 Edition, handling breaking changes and migration steps.\n\n## Steps\n\n1. **Check Current Status**\n\n   Read Cargo.toml to determine:\n   - Current edition\n   - Current rust-version (MSRV) if set\n   - Project structure (workspace or single crate)\n\n   ```rust\n   Current Status:\n   - Edition: 2021\n   - MSRV: Not set\n   - Type: Single crate\n   ```\n\n2. **Verify Rust Version**\n\n   Check that Rust toolchain is recent enough:\n\n   ```bash\n   rustc --version\n   ```\n\n   Required: Rust 1.85.0 or later for full Rust 2024 support.\n\n   If version is too old:\n   ```\n   âš ï¸ Rust version is too old. Rust 2024 Edition requires 1.85.0 or later.\n\n   Please update:\n   ```bash\n   rustup update stable\n   ```\n   ```\n\n3. **Create Backup**\n\n   Suggest creating a git commit or backup:\n   ```\n   ðŸ’¡ Recommendation: Commit current changes before upgrading\n\n   ```bash\n   git add .\n   git commit -m \"Pre-2024 edition upgrade snapshot\"\n   ```\n   ```\n\n4. **Update Cargo.toml**\n\n   For single crate:\n   ```toml\n   [package]\n   name = \"my-project\"\n   version = \"0.1.0\"\n   edition = \"2024\"  # Updated from 2021\n   rust-version = \"1.85\"  # Add MSRV\n   ```\n\n   For workspace:\n   ```toml\n   [workspace]\n   members = [\"crate1\", \"crate2\"]\n\n   [workspace.package]\n   edition = \"2024\"\n   rust-version = \"1.85\"\n\n   # Then each crate can inherit:\n   [package]\n   name = \"crate1\"\n   version = \"0.1.0\"\n   edition.workspace = true\n   rust-version.workspace = true\n   ```\n\n5. **Run cargo fix**\n\n   Automatically fix edition-related issues:\n\n   ```bash\n   cargo fix --edition\n   ```\n\n   This will:\n   - Fix deprecated patterns\n   - Update syntax where needed\n   - Add compatibility shims\n\n6. **Check for Warnings**\n\n   Review compiler warnings:\n\n   ```bash\n   cargo check --all-targets\n   ```\n\n   Common warnings:\n   - Match ergonomics changes\n   - Binding mode changes\n   - Reserved syntax warnings\n\n7. **Update Match Patterns (Rust 2024)**\n\n   **Breaking Change:** mut binding behavior changed\n\n   Before (2021):\n   ```rust\n   match &option {\n       Some(mut x) => {\n           // x is T (moved)\n       }\n       None => {}\n   }\n   ```\n\n   After (2024):\n   ```rust\n   match &option {\n       Some(mut x) => {\n           // x is &mut T (not moved)\n       }\n       None => {}\n   }\n   ```\n\n   If you need the old behavior:\n   ```rust\n   match option {  // Match on value, not reference\n       Some(mut x) => {\n           // x is T (moved)\n       }\n       None => {}\n   }\n   ```\n\n8. **Update Reserved Patterns**\n\n   Rust 2024 reserves some pattern combinations for future use:\n\n   ```rust\n   // âŒ Not allowed in 2024 (mixed ergonomics)\n   match value {\n       Some(ref x) => {}  // Error if not fully explicit\n       _ => {}\n   }\n\n   // âœ… Allowed (fully explicit)\n   match value {\n       &Some(ref x) => {}\n       _ => {}\n   }\n\n   // âœ… Or use ergonomics fully\n   match &value {\n       Some(x) => {}  // x is &T\n       _ => {}\n   }\n   ```\n\n9. **Run Tests**\n\n   Verify everything still works:\n\n   ```bash\n   cargo test --all-targets\n   ```\n\n   If tests fail:\n   - Check for match pattern changes\n   - Look for binding mode issues\n   - Review compiler errors carefully\n\n10. **Update Dependencies**\n\n    Check if dependencies support Rust 2024:\n\n    ```bash\n    cargo update\n    cargo check\n    ```\n\n    The MSRV-aware resolver (Rust 1.84+) will automatically select compatible versions based on your rust-version.\n\n11. **Enable Modern Features**\n\n    Now you can use Rust 2024 features:\n\n    ```rust\n    // Let chains\n    if let Some(x) = opt1\n        && let Some(y) = opt2\n    {\n        // ...\n    }\n\n    // Async closures\n    items.iter().map(async |item| {\n        process(item).await\n    })\n\n    // Gen blocks\n    let iter = gen {\n        yield 1;\n        yield 2;\n    };\n    ```\n\n12. **Update Lints**\n\n    Add modern lints to catch issues:\n\n    ```rust\n    // In lib.rs or main.rs\n    #![warn(rust_2024_compatibility)]\n    #![warn(let_underscore_drop)]\n    #![warn(unused_qualifications)]\n    ```\n\n13. **Provide Upgrade Summary**\n\n    ```\n    âœ… Successfully upgraded to Rust 2024 Edition!\n\n    ## Changes Made:\n\n    ### Cargo.toml Updates:\n    - Edition: 2021 â†’ 2024\n    - Added rust-version = \"1.85\"\n\n    ### Code Changes:\n    - Applied automatic fixes via cargo fix\n    - Updated 3 match patterns for new ergonomics\n    - Fixed 2 reserved pattern warnings\n\n    ### Tests:\n    - All tests passing âœ…\n\n    ## New Features Available:\n\n    1. **Let Chains** - Flatten nested if-let\n    2. **Async Closures** - Native async || {} syntax\n    3. **Gen Blocks** - Simplified iterators\n    4. **Improved Match Ergonomics** - Clearer semantics\n    5. **MSRV-Aware Resolver** - Automatic compatible versions\n\n    ## Next Steps:\n\n    1. Use `/rust-modernize` to apply modern patterns\n    2. Review new edition guide: https://doc.rust-lang.org/edition-guide/rust-2024/\n    3. Update CI/CD to use Rust 1.85+\n    4. Consider modernizing code patterns\n\n    ## Migration Guide:\n    See the [Rust 2024 Edition Guide](https://doc.rust-lang.org/edition-guide/rust-2024/) for details.\n    ```\n\n## Breaking Changes Checklist\n\nWhen upgrading to Rust 2024, be aware of:\n\n- [ ] **Match ergonomics** - mut bindings work differently\n- [ ] **Reserved patterns** - Some patterns reserved for future\n- [ ] **Temporary scopes in let chains** - Different drop order\n- [ ] **Impl trait captures** - More lifetime capture rules\n\n## Workspace Upgrade\n\nFor workspaces with multiple crates:\n\n1. **Update workspace root**:\n   ```toml\n   [workspace.package]\n   edition = \"2024\"\n   rust-version = \"1.85\"\n   ```\n\n2. **Update each crate**:\n   ```toml\n   [package]\n   edition.workspace = true\n   rust-version.workspace = true\n   ```\n\n3. **Run cargo fix for each crate**:\n   ```bash\n   cargo fix --edition --workspace\n   ```\n\n## Troubleshooting\n\n### cargo fix fails\n\nIf `cargo fix --edition` fails:\n1. Fix compilation errors first: `cargo check`\n2. Resolve dependency issues\n3. Try fixing one crate at a time\n4. Check for proc-macro compatibility\n\n### Tests fail after upgrade\n\nCommon issues:\n1. **Match pattern changes** - Check mut bindings\n2. **Drop order changes** - Let chains have different scoping\n3. **Lifetime changes** - Impl trait captures more lifetimes\n\n### Dependencies incompatible\n\nIf dependencies don't support Rust 2024:\n1. Check for updates: `cargo update`\n2. MSRV resolver should pick compatible versions\n3. File issues with dependency maintainers\n4. Consider alternatives if critical\n\n## Version Requirements\n\n- **Minimum Rust:** 1.85.0 for full Rust 2024 Edition\n- **Let chains:** 1.88.0\n- **MSRV resolver:** 1.84.0 (recommended before upgrade)\n\n## Edition Comparison\n\n| Feature | 2021 | 2024 |\n|---------|------|------|\n| Let chains | âŒ | âœ… |\n| Async closures | âŒ | âœ… |\n| Gen blocks | âŒ | âœ… |\n| Match ergonomics | Old | Improved |\n| MSRV resolver | âŒ | âœ… |\n\n## After Completion\n\nAsk the user:\n1. Did the upgrade complete successfully?\n2. Are all tests passing?\n3. Should we modernize the code to use new features?\n4. Do you want to update CI/CD configuration?"
              }
            ],
            "skills": [
              {
                "name": "async-patterns-guide",
                "description": "Guides users on modern async patterns including native async fn in traits, async closures, and avoiding async-trait when possible. Activates when users work with async code.",
                "path": "plugins/rust-modern-patterns/skills/async-patterns-guide/SKILL.md",
                "frontmatter": {
                  "name": "async-patterns-guide",
                  "description": "Guides users on modern async patterns including native async fn in traits, async closures, and avoiding async-trait when possible. Activates when users work with async code.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Async Patterns Guide Skill\n\nYou are an expert at modern Rust async patterns. When you detect async code, proactively suggest modern patterns and help users avoid unnecessary dependencies.\n\n## When to Activate\n\nActivate when you notice:\n- Use of async-trait crate\n- Async functions in traits\n- Async closures with manual construction\n- Questions about async patterns or performance\n\n## Key Decision: async-trait vs Native\n\n### Use Native Async Fn (Rust 1.75+)\n\n**When**:\n- Static dispatch (generics)\n- No dyn Trait needed\n- Performance-critical code\n- MSRV >= 1.75\n\n**Pattern**:\n```rust\n// âœ… Modern: No macro needed (Rust 1.75+)\ntrait UserRepository {\n    async fn find_user(&self, id: &str) -> Result<User, Error>;\n    async fn save_user(&self, user: &User) -> Result<(), Error>;\n}\n\nimpl UserRepository for PostgresRepo {\n    async fn find_user(&self, id: &str) -> Result<User, Error> {\n        self.db.query(id).await  // Native async, no macro!\n    }\n\n    async fn save_user(&self, user: &User) -> Result<(), Error> {\n        self.db.insert(user).await\n    }\n}\n\n// Use with generics (static dispatch)\nasync fn process<R: UserRepository>(repo: R) {\n    let user = repo.find_user(\"123\").await.unwrap();\n}\n```\n\n### Use async-trait Crate\n\n**When**:\n- Dynamic dispatch (dyn Trait) required\n- Need object safety\n- MSRV < 1.75\n- Plugin systems or trait objects\n\n**Pattern**:\n```rust\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait Plugin: Send + Sync {\n    async fn execute(&self) -> Result<(), Error>;\n}\n\n// Dynamic dispatch requires async-trait\nlet plugins: Vec<Box<dyn Plugin>> = vec![\n    Box::new(PluginA),\n    Box::new(PluginB),\n];\n\nfor plugin in plugins {\n    plugin.execute().await?;\n}\n```\n\n## Migration Examples\n\n### Migrating from async-trait\n\n**Before**:\n```rust\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait UserService {\n    async fn create_user(&self, email: &str) -> Result<User, Error>;\n}\n\n#[async_trait]\nimpl UserService for MyService {\n    async fn create_user(&self, email: &str) -> Result<User, Error> {\n        // implementation\n    }\n}\n```\n\n**After** (if using static dispatch):\n```rust\n// Remove async-trait dependency\ntrait UserService {\n    async fn create_user(&self, email: &str) -> Result<User, Error>;\n}\n\nimpl UserService for MyService {\n    async fn create_user(&self, email: &str) -> Result<User, Error> {\n        // implementation - no changes needed!\n    }\n}\n```\n\n## Async Closure Patterns\n\n### Modern Async Closures (Rust 1.85+)\n\n```rust\n// âœ… Native async closure\nasync fn process_all<F>(items: Vec<Item>, f: F) -> Result<(), Error>\nwhere\n    F: AsyncFn(Item) -> Result<(), Error>,\n{\n    for item in items {\n        f(item).await?;\n    }\n}\n\n// Usage\nprocess_all(items, async |item| {\n    validate(&item).await?;\n    save(&item).await\n}).await?;\n```\n\n## Performance Considerations\n\n### Static vs Dynamic Dispatch\n\n**Static (Generics)**:\n```rust\n// âœ… Zero-cost abstraction\nasync fn process<R: Repository>(repo: R) {\n    repo.save().await;\n}\n// Compiler generates specialized version for each type\n```\n\n**Dynamic (dyn Trait)**:\n```rust\n// âš ï¸ Runtime overhead (vtable indirection)\nasync fn process(repo: Box<dyn Repository>) {\n    repo.save().await;\n}\n// Requires async-trait, adds boxing overhead\n```\n\n## Your Approach\n\nWhen you see async traits:\n1. Check if dyn Trait is actually needed\n2. Suggest removing async-trait if possible\n3. Explain performance benefits of native async fn\n4. Show migration path\n\nProactively help users use modern async patterns without unnecessary dependencies."
              },
              {
                "name": "let-chains-advisor",
                "description": "Identifies deeply nested if-let expressions and suggests let chains for cleaner control flow. Activates when users write nested conditionals with pattern matching.",
                "path": "plugins/rust-modern-patterns/skills/let-chains-advisor/SKILL.md",
                "frontmatter": {
                  "name": "let-chains-advisor",
                  "description": "Identifies deeply nested if-let expressions and suggests let chains for cleaner control flow. Activates when users write nested conditionals with pattern matching.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Let Chains Advisor Skill\n\nYou are an expert at using let chains (Rust 2024) to simplify control flow. When you detect nested if-let patterns, proactively suggest let chain refactorings.\n\n## When to Activate\n\nActivate when you notice:\n- Nested if-let expressions (3+ levels)\n- Multiple pattern matches with conditions\n- Complex guard clauses\n- Difficult-to-read control flow\n\n## Let Chain Patterns\n\n### Pattern 1: Multiple Option Unwrapping\n\n**Before**:\n```rust\nfn get_user_email(id: &str) -> Option<String> {\n    if let Some(user) = database.find_user(id) {\n        if let Some(profile) = user.profile {\n            if let Some(email) = profile.email {\n                return Some(email);\n            }\n        }\n    }\n    None\n}\n```\n\n**After**:\n```rust\nfn get_user_email(id: &str) -> Option<String> {\n    if let Some(user) = database.find_user(id)\n        && let Some(profile) = user.profile\n        && let Some(email) = profile.email\n    {\n        Some(email)\n    } else {\n        None\n    }\n}\n```\n\n### Pattern 2: Pattern Matching with Conditions\n\n**Before**:\n```rust\nfn process(data: &Option<Data>) -> bool {\n    if let Some(data) = data {\n        if data.is_valid() {\n            if data.size() > 100 {\n                process_data(data);\n                return true;\n            }\n        }\n    }\n    false\n}\n```\n\n**After**:\n```rust\nfn process(data: &Option<Data>) -> bool {\n    if let Some(data) = data\n        && data.is_valid()\n        && data.size() > 100\n    {\n        process_data(data);\n        true\n    } else {\n        false\n    }\n}\n```\n\n### Pattern 3: Multiple Result Checks\n\n**Before**:\n```rust\nfn load_config() -> Result<Config, Error> {\n    if let Ok(path) = get_config_path() {\n        if let Ok(content) = std::fs::read_to_string(path) {\n            if let Ok(config) = toml::from_str(&content) {\n                return Ok(config);\n            }\n        }\n    }\n    Err(Error::ConfigNotFound)\n}\n```\n\n**After**:\n```rust\nfn load_config() -> Result<Config, Error> {\n    if let Ok(path) = get_config_path()\n        && let Ok(content) = std::fs::read_to_string(path)\n        && let Ok(config) = toml::from_str(&content)\n    {\n        Ok(config)\n    } else {\n        Err(Error::ConfigNotFound)\n    }\n}\n```\n\n### Pattern 4: While Loops\n\n**Before**:\n```rust\nwhile let Some(item) = iterator.next() {\n    if item.is_valid() {\n        if let Ok(processed) = process_item(item) {\n            results.push(processed);\n        }\n    }\n}\n```\n\n**After**:\n```rust\nwhile let Some(item) = iterator.next()\n    && item.is_valid()\n    && let Ok(processed) = process_item(item)\n{\n    results.push(processed);\n}\n```\n\n## Requirements\n\n- **Rust Version**: 1.88+\n- **Edition**: 2024\n- **Cargo.toml**:\n```toml\n[package]\nedition = \"2024\"\nrust-version = \"1.88\"\n```\n\n## Your Approach\n\nWhen you see nested patterns:\n1. Count nesting levels (3+ suggests let chains)\n2. Check if all branches return/continue\n3. Suggest let chain refactoring\n4. Verify Rust version compatibility\n\nProactively suggest let chains for cleaner, more readable code."
              },
              {
                "name": "rust-2024-migration",
                "description": "Guides users through migrating to Rust 2024 edition features including let chains, async closures, and improved match ergonomics. Activates when users work with Rust 2024 features or nested control flow.",
                "path": "plugins/rust-modern-patterns/skills/rust-2024-migration/SKILL.md",
                "frontmatter": {
                  "name": "rust-2024-migration",
                  "description": "Guides users through migrating to Rust 2024 edition features including let chains, async closures, and improved match ergonomics. Activates when users work with Rust 2024 features or nested control flow.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Rust 2024 Migration Skill\n\nYou are an expert at modern Rust patterns from the 2024 edition. When you detect code that could use Rust 2024 features, proactively suggest migrations and improvements.\n\n## When to Activate\n\nActivate when you notice:\n- Nested if-let expressions\n- Manual async closures with cloning\n- Cargo.toml with edition = \"2021\" or earlier\n- Code patterns that could benefit from Rust 2024 features\n\n## Rust 2024 Feature Patterns\n\n### 1. Let Chains (Stabilized in 1.88.0)\n\n**What to Look For**: Nested if-let or match expressions\n\n**Before (Nested)**:\n```rust\n// âŒ Deeply nested, hard to read\nfn process_user(id: &str) -> Option<String> {\n    if let Some(user) = db.find_user(id) {\n        if let Some(profile) = user.profile {\n            if profile.is_active {\n                if let Some(email) = profile.email {\n                    return Some(email);\n                }\n            }\n        }\n    }\n    None\n}\n```\n\n**After (Let Chains)**:\n```rust\n// âœ… Flat, readable chain\nfn process_user(id: &str) -> Option<String> {\n    if let Some(user) = db.find_user(id)\n        && let Some(profile) = user.profile\n        && profile.is_active\n        && let Some(email) = profile.email\n    {\n        Some(email)\n    } else {\n        None\n    }\n}\n```\n\n**Suggestion Template**:\n```\nYour nested if-let can be flattened using let chains (Rust 2024):\n\nif let Some(user) = get_user(id)\n    && let Some(profile) = user.profile\n    && profile.is_active\n{\n    // All conditions met\n}\n\nThis requires Rust 1.88+ and edition = \"2024\" in Cargo.toml.\n```\n\n### 2. Async Closures (Stabilized in 1.85.0)\n\n**Before**:\n```rust\n// âŒ Manual async closure with cloning\nlet futures: Vec<_> = items\n    .iter()\n    .map(|item| {\n        let item = item.clone();  // Need to clone for async move\n        async move {\n            fetch_data(item).await\n        }\n    })\n    .collect();\n```\n\n**After**:\n```rust\n// âœ… Native async closure\nlet futures: Vec<_> = items\n    .iter()\n    .map(async |item| {\n        fetch_data(item).await\n    })\n    .collect();\n```\n\n### 3. Async Functions in Traits (Native since 1.75)\n\n**Before**:\n```rust\n// âŒ OLD: Required async-trait crate\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait UserRepository {\n    async fn find_user(&self, id: &str) -> Result<User, Error>;\n}\n```\n\n**After**:\n```rust\n// âœ… NEW: Native async fn in traits (Rust 1.75+)\ntrait UserRepository {\n    async fn find_user(&self, id: &str) -> Result<User, Error>;\n}\n\nimpl UserRepository for PostgresRepo {\n    async fn find_user(&self, id: &str) -> Result<User, Error> {\n        self.db.query(id).await  // No macro needed!\n    }\n}\n```\n\n**When async-trait is Still Needed**:\n```rust\n// For dynamic dispatch (dyn Trait)\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait Plugin: Send + Sync {\n    async fn execute(&self) -> Result<(), Error>;\n}\n\n// This requires async-trait:\nlet plugins: Vec<Box<dyn Plugin>> = vec![\n    Box::new(PluginA),\n    Box::new(PluginB),\n];\n```\n\n### 4. Match Ergonomics 2024\n\n**Rust 2024 Changes**:\n```rust\n// Rust 2024: mut doesn't force by-value\nmatch &data {\n    Some(mut x) => {\n        // x is &mut T (not T moved)\n        x.modify();  // Modifies through reference\n    }\n    None => {}\n}\n```\n\n### 5. Gen Blocks (Stabilized in 1.85.0)\n\n**Before (Manual Iterator)**:\n```rust\nstruct RangeIter {\n    current: i32,\n    end: i32,\n}\n\nimpl Iterator for RangeIter {\n    type Item = i32;\n    fn next(&mut self) -> Option<Self::Item> {\n        if self.current < self.end {\n            let result = self.current;\n            self.current += 1;\n            Some(result)\n        } else {\n            None\n        }\n    }\n}\n```\n\n**After (Gen Block)**:\n```rust\nfn range_iter(start: i32, end: i32) -> impl Iterator<Item = i32> {\n    gen {\n        let mut current = start;\n        while current < end {\n            yield current;\n            current += 1;\n        }\n    }\n}\n```\n\n## Migration Checklist\n\nWhen migrating to Rust 2024:\n\n1. **Update Cargo.toml**:\n```toml\n[package]\nedition = \"2024\"\nrust-version = \"1.85\"  # Minimum version for Rust 2024\n```\n\n2. **Run cargo fix**:\n```bash\ncargo fix --edition\n```\n\n3. **Convert nested if-let to let chains**\n4. **Remove async-trait where not needed** (keep for dyn Trait)\n5. **Replace manual iterators with gen blocks**\n6. **Use const functions where appropriate**\n\n## Your Approach\n\nWhen you see code patterns:\n1. Identify nested control flow â†’ suggest let chains\n2. See async-trait â†’ check if native async fn works\n3. Manual iterators â†’ suggest gen blocks\n4. Async closures with cloning â†’ suggest native syntax\n\nProactively suggest Rust 2024 patterns for more elegant, idiomatic code."
              }
            ]
          },
          {
            "name": "rust-data-engineering",
            "description": "Data engineering plugin for Rust with object_store, Arrow, Parquet, DataFusion, and Iceberg. Build cloud-native data lakes, analytical query engines, and ETL pipelines. Commands for object storage, Parquet I/O, DataFusion queries, and Iceberg tables. Expert agent for data lake architecture and performance optimization",
            "source": "./plugins/rust-data-engineering",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-data-engineering@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/data-datafusion-query",
                "description": "Execute SQL queries with DataFusion against Parquet, CSV, and in-memory data",
                "path": "plugins/rust-data-engineering/commands/data-datafusion-query.md",
                "frontmatter": {
                  "description": "Execute SQL queries with DataFusion against Parquet, CSV, and in-memory data"
                },
                "content": "# DataFusion Query Execution\n\nHelp the user set up DataFusion and execute SQL queries against data stored in object storage (Parquet, CSV) or in-memory.\n\n## Steps\n\n1. **Add required dependencies**:\n   ```toml\n   [dependencies]\n   datafusion = \"39\"\n   arrow = \"52\"\n   object_store = \"0.9\"\n   tokio = { version = \"1\", features = [\"full\"] }\n   ```\n\n2. **Create a DataFusion session context**:\n   ```rust\n   use datafusion::prelude::*;\n   use datafusion::execution::context::{SessionContext, SessionConfig};\n   use datafusion::execution::runtime_env::{RuntimeEnv, RuntimeConfig};\n   use std::sync::Arc;\n\n   async fn create_context() -> Result<SessionContext> {\n       // Configure session\n       let config = SessionConfig::new()\n           .with_target_partitions(num_cpus::get()) // Match CPU count\n           .with_batch_size(8192); // Rows per batch\n\n       // Configure runtime\n       let runtime_config = RuntimeConfig::new()\n           .with_memory_limit(4 * 1024 * 1024 * 1024) // 4GB memory limit\n           .with_temp_file_path(\"/tmp/datafusion\");\n\n       let runtime = Arc::new(RuntimeEnv::new(runtime_config)?);\n\n       Ok(SessionContext::new_with_config_rt(config, runtime))\n   }\n   ```\n\n3. **Register object store** for S3/Azure/GCS:\n   ```rust\n   use object_store::aws::AmazonS3Builder;\n\n   async fn register_object_store(ctx: &SessionContext) -> Result<()> {\n       // Create S3 store\n       let s3 = AmazonS3Builder::from_env()\n           .with_bucket_name(\"my-data-lake\")\n           .build()?;\n\n       // Register with DataFusion\n       let url = \"s3://my-data-lake/\";\n       ctx.runtime_env().register_object_store(\n           &url::Url::parse(url)?,\n           Arc::new(s3),\n       );\n\n       Ok(())\n   }\n   ```\n\n4. **Register Parquet tables**:\n   ```rust\n   use datafusion::datasource::listing::{\n       ListingOptions,\n       ListingTable,\n       ListingTableConfig,\n       ListingTableUrl,\n   };\n   use datafusion::datasource::file_format::parquet::ParquetFormat;\n\n   async fn register_parquet_table(\n       ctx: &SessionContext,\n       table_name: &str,\n       path: &str,\n   ) -> Result<()> {\n       // Simple registration\n       ctx.register_parquet(\n           table_name,\n           path,\n           ParquetReadOptions::default(),\n       ).await?;\n\n       Ok(())\n   }\n\n   // Advanced registration with partitioning\n   async fn register_partitioned_table(\n       ctx: &SessionContext,\n       table_name: &str,\n       path: &str,\n   ) -> Result<()> {\n       let table_path = ListingTableUrl::parse(path)?;\n\n       let file_format = ParquetFormat::default();\n\n       let listing_options = ListingOptions::new(Arc::new(file_format))\n           .with_file_extension(\".parquet\")\n           .with_target_partitions(ctx.state().config().target_partitions())\n           .with_collect_stat(true); // Collect file statistics\n\n       let config = ListingTableConfig::new(table_path)\n           .with_listing_options(listing_options);\n\n       let table = ListingTable::try_new(config)?;\n\n       ctx.register_table(table_name, Arc::new(table))?;\n\n       Ok(())\n   }\n   ```\n\n5. **Execute SQL queries**:\n   ```rust\n   async fn execute_sql(ctx: &SessionContext, query: &str) -> Result<Vec<RecordBatch>> {\n       // Create DataFrame from SQL\n       let df = ctx.sql(query).await?;\n\n       // Collect all results\n       let batches = df.collect().await?;\n\n       Ok(batches)\n   }\n\n   // Example queries\n   async fn example_queries(ctx: &SessionContext) -> Result<()> {\n       // Simple select\n       let df = ctx.sql(\"\n           SELECT user_id, event_type, COUNT(*) as count\n           FROM events\n           WHERE date >= '2024-01-01'\n           GROUP BY user_id, event_type\n           ORDER BY count DESC\n           LIMIT 100\n       \").await?;\n\n       df.show().await?;\n\n       // Window functions\n       let df = ctx.sql(\"\n           SELECT\n               user_id,\n               timestamp,\n               amount,\n               SUM(amount) OVER (\n                   PARTITION BY user_id\n                   ORDER BY timestamp\n                   ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n               ) as running_total\n           FROM transactions\n       \").await?;\n\n       df.show().await?;\n\n       // Joins\n       let df = ctx.sql(\"\n           SELECT\n               e.user_id,\n               u.name,\n               COUNT(*) as event_count\n           FROM events e\n           JOIN users u ON e.user_id = u.id\n           GROUP BY e.user_id, u.name\n       \").await?;\n\n       df.show().await?;\n\n       Ok(())\n   }\n   ```\n\n6. **Use DataFrame API** as an alternative to SQL:\n   ```rust\n   use datafusion::prelude::*;\n\n   async fn dataframe_api_examples(ctx: &SessionContext) -> Result<()> {\n       // Get table\n       let df = ctx.table(\"events\").await?;\n\n       // Filter\n       let df = df.filter(col(\"timestamp\").gt(lit(\"2024-01-01\")))?;\n\n       // Select columns\n       let df = df.select(vec![\n           col(\"user_id\"),\n           col(\"event_type\"),\n           col(\"timestamp\"),\n       ])?;\n\n       // Aggregate\n       let df = df.aggregate(\n           vec![col(\"user_id\"), col(\"event_type\")],\n           vec![\n               count(col(\"*\")).alias(\"count\"),\n               avg(col(\"duration\")).alias(\"avg_duration\"),\n               max(col(\"timestamp\")).alias(\"max_time\"),\n           ],\n       )?;\n\n       // Sort\n       let df = df.sort(vec![\n           col(\"count\").sort(false, true), // DESC NULLS LAST\n       ])?;\n\n       // Limit\n       let df = df.limit(0, Some(100))?;\n\n       // Execute\n       let batches = df.collect().await?;\n\n       Ok(())\n   }\n   ```\n\n7. **Stream results** for large queries:\n   ```rust\n   use futures::stream::StreamExt;\n\n   async fn stream_query_results(\n       ctx: &SessionContext,\n       query: &str,\n   ) -> Result<()> {\n       let df = ctx.sql(query).await?;\n\n       // Get streaming results\n       let mut stream = df.execute_stream().await?;\n\n       // Process batches incrementally\n       let mut total_rows = 0;\n       while let Some(batch) = stream.next().await {\n           let batch = batch?;\n           total_rows += batch.num_rows();\n\n           // Process this batch\n           process_batch(&batch)?;\n\n           println!(\"Processed {} rows so far...\", total_rows);\n       }\n\n       println!(\"Total rows: {}\", total_rows);\n       Ok(())\n   }\n\n   fn process_batch(batch: &RecordBatch) -> Result<()> {\n       // Your processing logic\n       Ok(())\n   }\n   ```\n\n8. **Inspect query plans** for optimization:\n   ```rust\n   async fn explain_query(ctx: &SessionContext, query: &str) -> Result<()> {\n       // Logical plan\n       let logical_plan = ctx.sql(query).await?.into_optimized_plan()?;\n       println!(\"Logical Plan:\\n{}\", logical_plan.display_indent());\n\n       // Physical plan\n       let df = ctx.sql(query).await?;\n       let physical_plan = df.create_physical_plan().await?;\n       println!(\"Physical Plan:\\n{}\", physical_plan.display_indent());\n\n       // Or use EXPLAIN in SQL\n       let df = ctx.sql(&format!(\"EXPLAIN {}\", query)).await?;\n       df.show().await?;\n\n       Ok(())\n   }\n   ```\n\n## Advanced Features\n\n**Register CSV tables**:\n```rust\nuse datafusion::datasource::file_format::csv::CsvFormat;\n\nasync fn register_csv(ctx: &SessionContext) -> Result<()> {\n    ctx.register_csv(\n        \"users\",\n        \"s3://my-bucket/users.csv\",\n        CsvReadOptions::new()\n            .has_header(true)\n            .delimiter(b',')\n            .schema_infer_max_records(1000),\n    ).await?;\n\n    Ok(())\n}\n```\n\n**Register in-memory tables**:\n```rust\nuse datafusion::datasource::MemTable;\n\nasync fn register_memory_table(\n    ctx: &SessionContext,\n    name: &str,\n    batches: Vec<RecordBatch>,\n    schema: SchemaRef,\n) -> Result<()> {\n    let mem_table = MemTable::try_new(schema, vec![batches])?;\n    ctx.register_table(name, Arc::new(mem_table))?;\n    Ok(())\n}\n```\n\n**Create temporary views**:\n```rust\nasync fn create_view(ctx: &SessionContext) -> Result<()> {\n    // Create view from query\n    let df = ctx.sql(\"\n        SELECT user_id, COUNT(*) as count\n        FROM events\n        GROUP BY user_id\n    \").await?;\n\n    ctx.register_table(\"user_counts\", df.into_view())?;\n\n    // Now query the view\n    let results = ctx.sql(\"SELECT * FROM user_counts WHERE count > 100\").await?;\n    results.show().await?;\n\n    Ok(())\n}\n```\n\n**User-Defined Functions (UDFs)**:\n```rust\nuse datafusion::logical_expr::{create_udf, Volatility, ColumnarValue};\nuse arrow::array::StringArray;\n\nasync fn register_udfs(ctx: &SessionContext) -> Result<()> {\n    // Create scalar UDF\n    let extract_domain = create_udf(\n        \"extract_domain\",\n        vec![DataType::Utf8],\n        Arc::new(DataType::Utf8),\n        Volatility::Immutable,\n        Arc::new(|args: &[ColumnarValue]| {\n            let urls = args[0].clone().into_array(1)?;\n            let urls = urls.as_any().downcast_ref::<StringArray>().unwrap();\n\n            let domains: StringArray = urls\n                .iter()\n                .map(|url| {\n                    url.and_then(|u| url::Url::parse(u).ok())\n                        .and_then(|u| u.host_str().map(|s| s.to_string()))\n                })\n                .collect();\n\n            Ok(ColumnarValue::Array(Arc::new(domains)))\n        }),\n    );\n\n    ctx.register_udf(extract_domain);\n\n    // Use in query\n    let df = ctx.sql(\"\n        SELECT\n            extract_domain(url) as domain,\n            COUNT(*) as count\n        FROM events\n        GROUP BY domain\n    \").await?;\n\n    df.show().await?;\n\n    Ok(())\n}\n```\n\n**Write query results to Parquet**:\n```rust\nasync fn write_query_results(\n    ctx: &SessionContext,\n    query: &str,\n    output_path: &str,\n) -> Result<()> {\n    let df = ctx.sql(query).await?;\n\n    // Write to Parquet\n    df.write_parquet(\n        output_path,\n        DataFrameWriteOptions::new(),\n        Some(WriterProperties::builder()\n            .set_compression(Compression::ZSTD(ZstdLevel::try_new(3)?))\n            .build()),\n    ).await?;\n\n    Ok(())\n}\n```\n\n## Performance Optimization\n\n**Partition pruning**:\n```rust\n// DataFusion automatically prunes partitions based on WHERE clauses\nasync fn partition_pruning_example(ctx: &SessionContext) -> Result<()> {\n    // Assuming Hive-style partitioning: year=2024/month=01/...\n\n    // This query only scans year=2024/month=01 partitions\n    let df = ctx.sql(\"\n        SELECT * FROM events\n        WHERE year = 2024 AND month = 1\n    \").await?;\n\n    // Use EXPLAIN to verify partition pruning\n    let explain = ctx.sql(\"EXPLAIN SELECT * FROM events WHERE year = 2024 AND month = 1\").await?;\n    explain.show().await?;\n\n    Ok(())\n}\n```\n\n**Predicate pushdown**:\n```rust\n// DataFusion pushes predicates to Parquet readers automatically\n// This reads only relevant row groups based on statistics\n\nlet df = ctx.sql(\"\n    SELECT * FROM events\n    WHERE user_id = 'user123'\n      AND timestamp >= '2024-01-01'\n\").await?;\n```\n\n**Projection pushdown**:\n```rust\n// Only requested columns are read from Parquet\nlet df = ctx.sql(\"\n    SELECT user_id, timestamp\n    FROM events\n\").await?; // Only reads user_id and timestamp columns\n```\n\n**Parallelism tuning**:\n```rust\nlet config = SessionConfig::new()\n    .with_target_partitions(16); // Increase for better parallelism\n\nlet ctx = SessionContext::new_with_config(config);\n```\n\n## Common Patterns\n\n**Aggregating across partitions**:\n```rust\nasync fn aggregate_partitions(ctx: &SessionContext) -> Result<()> {\n    let df = ctx.sql(\"\n        SELECT\n            year,\n            month,\n            COUNT(*) as total_events,\n            COUNT(DISTINCT user_id) as unique_users,\n            AVG(duration) as avg_duration\n        FROM events\n        WHERE year = 2024\n        GROUP BY year, month\n        ORDER BY month\n    \").await?;\n\n    df.show().await?;\n    Ok(())\n}\n```\n\n**Time-series analysis**:\n```rust\nasync fn time_series_analysis(ctx: &SessionContext) -> Result<()> {\n    let df = ctx.sql(\"\n        SELECT\n            DATE_TRUNC('hour', timestamp) as hour,\n            COUNT(*) as events_per_hour,\n            AVG(value) as avg_value,\n            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY value) as p95_value\n        FROM metrics\n        WHERE timestamp >= NOW() - INTERVAL '7 days'\n        GROUP BY 1\n        ORDER BY 1\n    \").await?;\n\n    df.show().await?;\n    Ok(())\n}\n```\n\n**Complex joins**:\n```rust\nasync fn complex_join(ctx: &SessionContext) -> Result<()> {\n    let df = ctx.sql(\"\n        SELECT\n            e.event_type,\n            u.country,\n            COUNT(*) as count,\n            AVG(e.duration) as avg_duration\n        FROM events e\n        JOIN users u ON e.user_id = u.id\n        LEFT JOIN subscriptions s ON u.id = s.user_id\n        WHERE e.timestamp >= '2024-01-01'\n          AND u.active = true\n        GROUP BY e.event_type, u.country\n        HAVING count > 100\n        ORDER BY count DESC\n    \").await?;\n\n    df.show().await?;\n    Ok(())\n}\n```\n\n## Best Practices\n\n- **Use partition pruning** by filtering on partition columns (year, month, day)\n- **Select only needed columns** to leverage projection pushdown\n- **Configure appropriate parallelism** based on CPU cores and data size\n- **Use EXPLAIN** to verify query optimization\n- **Stream large results** instead of collecting all at once\n- **Register statistics** when creating tables for better query planning\n- **Create views** for commonly used queries\n- **Use UDFs** for custom business logic\n\n## Troubleshooting\n\n**Out of memory**:\n- Reduce batch size: `.with_batch_size(4096)`\n- Set memory limit: `.with_memory_limit()`\n- Stream results instead of collecting\n- Enable spilling to disk with temp_file_path\n\n**Slow queries**:\n- Use EXPLAIN to inspect query plan\n- Verify partition pruning is working\n- Check if predicates can be pushed down\n- Increase parallelism: `.with_target_partitions()`\n- Ensure object store is registered correctly\n\n**Schema errors**:\n- Verify table registration: `ctx.table(\"name\").await?.schema()`\n- Check for schema evolution in Parquet files\n- Use explicit schema for CSV files\n- Handle NULL values appropriately\n\n**Partition not found**:\n- Verify path format matches Hive partitioning\n- Check object store URL registration\n- List files to debug: `store.list(prefix).await`"
              },
              {
                "name": "/data-iceberg-table",
                "description": "Create and manage Apache Iceberg tables with ACID transactions and schema evolution",
                "path": "plugins/rust-data-engineering/commands/data-iceberg-table.md",
                "frontmatter": {
                  "description": "Create and manage Apache Iceberg tables with ACID transactions and schema evolution"
                },
                "content": "# Apache Iceberg Tables\n\nHelp the user work with Apache Iceberg tables for data lakes with ACID transactions, time travel, and schema evolution capabilities.\n\n## Steps\n\n1. **Add required dependencies**:\n   ```toml\n   [dependencies]\n   iceberg = \"0.3\"\n   iceberg-catalog-rest = \"0.3\"\n   arrow = \"52\"\n   parquet = \"52\"\n   object_store = \"0.9\"\n   tokio = { version = \"1\", features = [\"full\"] }\n   ```\n\n2. **Set up Iceberg catalog**:\n   ```rust\n   use iceberg::{Catalog, TableIdent};\n   use iceberg_catalog_rest::RestCatalog;\n\n   async fn create_catalog() -> Result<RestCatalog> {\n       // REST catalog (works with services like Polaris, Nessie, etc.)\n       let catalog = RestCatalog::new(\n           \"http://localhost:8181\",  // Catalog endpoint\n           \"warehouse\",               // Warehouse location\n       ).await?;\n\n       Ok(catalog)\n   }\n\n   // For AWS Glue catalog\n   // use iceberg_catalog_glue::GlueCatalog;\n\n   // For file-based catalog (development)\n   // use iceberg::catalog::FileCatalog;\n   ```\n\n3. **Create an Iceberg table**:\n   ```rust\n   use iceberg::{\n       spec::{Schema, NestedField, PrimitiveType, Type},\n       NamespaceIdent, TableCreation,\n   };\n\n   async fn create_table(catalog: &impl Catalog) -> Result<()> {\n       // Define schema\n       let schema = Schema::builder()\n           .with_fields(vec![\n               NestedField::required(1, \"id\", Type::Primitive(PrimitiveType::Long)),\n               NestedField::required(2, \"timestamp\", Type::Primitive(PrimitiveType::Timestamp)),\n               NestedField::required(3, \"user_id\", Type::Primitive(PrimitiveType::String)),\n               NestedField::optional(4, \"event_type\", Type::Primitive(PrimitiveType::String)),\n               NestedField::optional(5, \"properties\", Type::Primitive(PrimitiveType::String)),\n           ])\n           .build()?;\n\n       // Define partitioning\n       let partition_spec = iceberg::spec::PartitionSpec::builder()\n           .with_spec_id(0)\n           .add_partition_field(2, \"year\", iceberg::spec::Transform::Year)? // Partition by year\n           .add_partition_field(2, \"month\", iceberg::spec::Transform::Month)? // Partition by month\n           .build()?;\n\n       // Define sort order (for data clustering)\n       let sort_order = iceberg::spec::SortOrder::builder()\n           .with_order_id(0)\n           .add_sort_field(\n               iceberg::spec::SortField::builder()\n                   .source_id(2) // timestamp field\n                   .direction(iceberg::spec::SortDirection::Ascending)\n                   .null_order(iceberg::spec::NullOrder::First)\n                   .build(),\n           )\n           .build()?;\n\n       // Create table\n       let table_creation = TableCreation::builder()\n           .name(\"events\".to_string())\n           .schema(schema)\n           .partition_spec(partition_spec)\n           .sort_order(sort_order)\n           .build();\n\n       let namespace = NamespaceIdent::new(\"db\".to_string());\n       let table_ident = TableIdent::new(namespace, \"events\".to_string());\n\n       catalog.create_table(&table_ident, table_creation).await?;\n\n       println!(\"Table created: db.events\");\n       Ok(())\n   }\n   ```\n\n4. **Load an existing table**:\n   ```rust\n   async fn load_table(catalog: &impl Catalog) -> Result<iceberg::Table> {\n       let namespace = NamespaceIdent::new(\"db\".to_string());\n       let table_ident = TableIdent::new(namespace, \"events\".to_string());\n\n       let table = catalog.load_table(&table_ident).await?;\n\n       // Inspect table metadata\n       println!(\"Schema: {:?}\", table.metadata().current_schema());\n       println!(\"Location: {}\", table.metadata().location());\n       println!(\"Snapshots: {}\", table.metadata().snapshots().len());\n\n       Ok(table)\n   }\n   ```\n\n5. **Write data to Iceberg table**:\n   ```rust\n   use iceberg::writer::{IcebergWriter, RecordBatchWriter};\n   use arrow::record_batch::RecordBatch;\n\n   async fn write_data(\n       table: &iceberg::Table,\n       batches: Vec<RecordBatch>,\n   ) -> Result<()> {\n       // Create writer\n       let mut writer = table\n           .writer()\n           .partition_by(table.metadata().default_partition_spec()?)\n           .build()\n           .await?;\n\n       // Write batches\n       for batch in batches {\n           writer.write(&batch).await?;\n       }\n\n       // Commit (ACID transaction)\n       let data_files = writer.close().await?;\n\n       // Create snapshot\n       let mut append = table.new_append();\n       for file in data_files {\n           append.add_data_file(file)?;\n       }\n       append.commit().await?;\n\n       println!(\"Data written and committed\");\n       Ok(())\n   }\n   ```\n\n6. **Read data with time travel**:\n   ```rust\n   use iceberg::scan::{TableScan, TableScanBuilder};\n\n   async fn read_latest(table: &iceberg::Table) -> Result<Vec<RecordBatch>> {\n       // Read latest snapshot\n       let scan = table.scan().build().await?;\n\n       let batches = scan.to_arrow().await?;\n\n       Ok(batches)\n   }\n\n   async fn read_snapshot(\n       table: &iceberg::Table,\n       snapshot_id: i64,\n   ) -> Result<Vec<RecordBatch>> {\n       // Time travel to specific snapshot\n       let scan = table\n           .scan()\n           .snapshot_id(snapshot_id)\n           .build()\n           .await?;\n\n       let batches = scan.to_arrow().await?;\n\n       Ok(batches)\n   }\n\n   async fn read_as_of_timestamp(\n       table: &iceberg::Table,\n       timestamp_ms: i64,\n   ) -> Result<Vec<RecordBatch>> {\n       // Time travel to specific timestamp\n       let scan = table\n           .scan()\n           .as_of_timestamp(timestamp_ms)\n           .build()\n           .await?;\n\n       let batches = scan.to_arrow().await?;\n\n       Ok(batches)\n   }\n   ```\n\n7. **Perform schema evolution**:\n   ```rust\n   async fn evolve_schema(table: &mut iceberg::Table) -> Result<()> {\n       // Add new column\n       let mut update = table.update_schema();\n       update\n           .add_column(\"new_field\", Type::Primitive(PrimitiveType::String), true)?\n           .commit()\n           .await?;\n\n       println!(\"Added column: new_field\");\n\n       // Rename column\n       let mut update = table.update_schema();\n       update\n           .rename_column(\"old_name\", \"new_name\")?\n           .commit()\n           .await?;\n\n       println!(\"Renamed column: old_name -> new_name\");\n\n       // Delete column (metadata only)\n       let mut update = table.update_schema();\n       update\n           .delete_column(\"unused_field\")?\n           .commit()\n           .await?;\n\n       println!(\"Deleted column: unused_field\");\n\n       // Update column type (limited support)\n       let mut update = table.update_schema();\n       update\n           .update_column(\"numeric_field\", Type::Primitive(PrimitiveType::Double))?\n           .commit()\n           .await?;\n\n       // Reorder columns\n       let mut update = table.update_schema();\n       update\n           .move_first(\"important_field\")?\n           .move_after(\"field_a\", \"field_b\")?\n           .commit()\n           .await?;\n\n       Ok(())\n   }\n   ```\n\n8. **Query history and snapshots**:\n   ```rust\n   async fn inspect_history(table: &iceberg::Table) -> Result<()> {\n       let metadata = table.metadata();\n\n       // List all snapshots\n       println!(\"Snapshots:\");\n       for snapshot in metadata.snapshots() {\n           println!(\n               \"  ID: {}, Timestamp: {}, Summary: {:?}\",\n               snapshot.snapshot_id(),\n               snapshot.timestamp_ms(),\n               snapshot.summary()\n           );\n       }\n\n       // Get current snapshot\n       if let Some(current) = metadata.current_snapshot() {\n           println!(\"Current snapshot: {}\", current.snapshot_id());\n           println!(\"Manifest list: {}\", current.manifest_list());\n       }\n\n       // Get schema history\n       println!(\"\\nSchema versions:\");\n       for schema in metadata.schemas() {\n           println!(\"  Schema ID {}: {} fields\", schema.schema_id(), schema.fields().len());\n       }\n\n       Ok(())\n   }\n   ```\n\n## Advanced Features\n\n**Partition evolution**:\n```rust\nasync fn evolve_partitioning(table: &mut iceberg::Table) -> Result<()> {\n    // Change partition strategy without rewriting data\n    let mut update = table.update_partition_spec();\n\n    // Add day partitioning\n    update.add_field(\n        \"timestamp\",\n        \"day\",\n        iceberg::spec::Transform::Day,\n    )?;\n\n    // Remove old month partitioning\n    update.remove_field(\"month\")?;\n\n    update.commit().await?;\n\n    println!(\"Partition spec evolved\");\n    Ok(())\n}\n```\n\n**Hidden partitioning**:\n```rust\n// Iceberg supports hidden partitioning - partition on derived values\n// Users don't need to specify partition columns in queries\n\nasync fn create_table_with_hidden_partitioning(catalog: &impl Catalog) -> Result<()> {\n    let schema = Schema::builder()\n        .with_fields(vec![\n            NestedField::required(1, \"timestamp\", Type::Primitive(PrimitiveType::Timestamp)),\n            NestedField::required(2, \"data\", Type::Primitive(PrimitiveType::String)),\n        ])\n        .build()?;\n\n    // Partition by year(timestamp) and month(timestamp)\n    // But timestamp is a regular column, not a partition column\n    let partition_spec = iceberg::spec::PartitionSpec::builder()\n        .add_partition_field(1, \"year\", iceberg::spec::Transform::Year)?\n        .add_partition_field(1, \"month\", iceberg::spec::Transform::Month)?\n        .build()?;\n\n    // Now queries like:\n    // SELECT * FROM table WHERE timestamp >= '2024-01-01'\n    // Will automatically use partition pruning\n\n    Ok(())\n}\n```\n\n**Incremental reads**:\n```rust\nasync fn incremental_read(\n    table: &iceberg::Table,\n    from_snapshot_id: i64,\n    to_snapshot_id: Option<i64>,\n) -> Result<Vec<RecordBatch>> {\n    // Read only data added between snapshots\n    let scan = table\n        .scan()\n        .from_snapshot_id(from_snapshot_id)\n        .snapshot_id(to_snapshot_id.unwrap_or_else(|| {\n            table.metadata().current_snapshot().unwrap().snapshot_id()\n        }))\n        .build()\n        .await?;\n\n    let batches = scan.to_arrow().await?;\n\n    Ok(batches)\n}\n```\n\n**Filtering and projection**:\n```rust\nuse iceberg::expr::{Predicate, Reference};\n\nasync fn filtered_scan(table: &iceberg::Table) -> Result<Vec<RecordBatch>> {\n    // Build predicate\n    let predicate = Predicate::and(\n        Predicate::greater_than(\"timestamp\", 1704067200000i64), // > 2024-01-01\n        Predicate::equal(\"event_type\", \"click\"),\n    );\n\n    // Scan with predicate pushdown\n    let scan = table\n        .scan()\n        .with_filter(predicate)\n        .select(&[\"user_id\", \"timestamp\", \"event_type\"]) // Column projection\n        .build()\n        .await?;\n\n    let batches = scan.to_arrow().await?;\n\n    Ok(batches)\n}\n```\n\n**Compaction (optimize files)**:\n```rust\nasync fn compact_table(table: &iceberg::Table) -> Result<()> {\n    // Read small files\n    let scan = table.scan().build().await?;\n    let batches = scan.to_arrow().await?;\n\n    // Rewrite as larger, optimized files\n    let mut writer = table\n        .writer()\n        .partition_by(table.metadata().default_partition_spec()?)\n        .build()\n        .await?;\n\n    for batch in batches {\n        writer.write(&batch).await?;\n    }\n\n    let new_files = writer.close().await?;\n\n    // Atomic replace\n    let mut rewrite = table.new_rewrite();\n    rewrite\n        .delete_files(/* old files */)\n        .add_files(new_files)\n        .commit()\n        .await?;\n\n    Ok(())\n}\n```\n\n## Integration with DataFusion\n\n```rust\nuse datafusion::prelude::*;\nuse iceberg::datafusion::IcebergTableProvider;\n\nasync fn query_with_datafusion(table: iceberg::Table) -> Result<()> {\n    // Create DataFusion context\n    let ctx = SessionContext::new();\n\n    // Register Iceberg table\n    let provider = IcebergTableProvider::try_new(table).await?;\n    ctx.register_table(\"events\", Arc::new(provider))?;\n\n    // Query with SQL\n    let df = ctx.sql(\"\n        SELECT\n            event_type,\n            COUNT(*) as count\n        FROM events\n        WHERE timestamp >= '2024-01-01'\n        GROUP BY event_type\n    \").await?;\n\n    df.show().await?;\n\n    Ok(())\n}\n```\n\n## Common Patterns\n\n**Creating a data pipeline**:\n```rust\nasync fn data_pipeline(\n    source_store: Arc<dyn ObjectStore>,\n    table: &iceberg::Table,\n) -> Result<()> {\n    // 1. Read from source (e.g., Parquet)\n    let batches = read_parquet_files(source_store).await?;\n\n    // 2. Transform data\n    let transformed = transform_batches(batches)?;\n\n    // 3. Write to Iceberg table\n    write_data(table, transformed).await?;\n\n    println!(\"Pipeline complete\");\n    Ok(())\n}\n```\n\n**Implementing time-based retention**:\n```rust\nasync fn expire_old_snapshots(table: &mut iceberg::Table, days: i64) -> Result<()> {\n    let cutoff_ms = chrono::Utc::now().timestamp_millis() - (days * 24 * 60 * 60 * 1000);\n\n    let mut expire = table.expire_snapshots();\n    expire\n        .expire_older_than(cutoff_ms)\n        .retain_last(10) // Keep at least 10 snapshots\n        .commit()\n        .await?;\n\n    println!(\"Expired snapshots older than {} days\", days);\n    Ok(())\n}\n```\n\n**Atomic updates**:\n```rust\nasync fn atomic_update(table: &iceberg::Table) -> Result<()> {\n    // All or nothing - either entire commit succeeds or fails\n    let mut transaction = table.new_transaction();\n\n    // Multiple operations in one transaction\n    transaction.append(/* new data */);\n    transaction.update_schema(/* schema change */);\n    transaction.update_properties(/* property change */);\n\n    // Atomic commit\n    transaction.commit().await?;\n\n    Ok(())\n}\n```\n\n## Best Practices\n\n- **Use hidden partitioning** for cleaner queries and easier partition evolution\n- **Define sort order** to cluster related data together\n- **Expire old snapshots** regularly to avoid metadata bloat\n- **Use schema evolution** instead of creating new tables\n- **Leverage time travel** for debugging and auditing\n- **Compact small files** periodically for better read performance\n- **Use partition evolution** to adapt to changing data patterns\n- **Enable statistics** for query optimization\n\n## Benefits Over Raw Parquet\n\n1. **ACID Transactions**: Atomic commits prevent partial updates\n2. **Time Travel**: Query historical table states\n3. **Schema Evolution**: Add/rename/reorder columns safely\n4. **Partition Evolution**: Change partitioning without rewriting\n5. **Hidden Partitioning**: Cleaner queries, automatic partition pruning\n6. **Concurrency**: Multiple writers with optimistic concurrency\n7. **Metadata Management**: Efficient metadata operations\n8. **Data Lineage**: Track changes over time\n\n## Troubleshooting\n\n**Metadata file not found**:\n- Verify catalog configuration\n- Check object store permissions\n- Ensure table was created successfully\n\n**Schema mismatch on write**:\n- Verify writer schema matches table schema\n- Use schema evolution to add new fields\n- Check for required vs. optional fields\n\n**Slow queries**:\n- Use predicate pushdown with filters\n- Enable column projection\n- Compact small files\n- Verify partition pruning is working\n\n**Snapshot expiration issues**:\n- Ensure retain_last is set appropriately\n- Don't expire too aggressively if time travel is needed\n- Clean up orphaned files separately\n\n## Resources\n\n- [Apache Iceberg Specification](https://iceberg.apache.org/spec/)\n- [iceberg-rust Documentation](https://docs.rs/iceberg/)\n- [Iceberg Table Format](https://iceberg.apache.org/docs/latest/)"
              },
              {
                "name": "/data-object-store-setup",
                "description": "Configure object_store for cloud storage (S3, Azure, GCS, or local filesystem)",
                "path": "plugins/rust-data-engineering/commands/data-object-store-setup.md",
                "frontmatter": {
                  "description": "Configure object_store for cloud storage (S3, Azure, GCS, or local filesystem)"
                },
                "content": "# Object Store Setup\n\nHelp the user configure the `object_store` crate for their cloud provider or local filesystem.\n\n## Steps\n\n1. **Identify the storage backend** by asking the user which provider they want to use:\n   - Amazon S3\n   - Azure Blob Storage\n   - Google Cloud Storage\n   - Local filesystem (for development/testing)\n\n2. **Add the dependency** to their Cargo.toml:\n   ```toml\n   [dependencies]\n   object_store = { version = \"0.9\", features = [\"aws\", \"azure\", \"gcp\"] }\n   tokio = { version = \"1\", features = [\"full\"] }\n   ```\n\n3. **Create the appropriate builder** based on their choice:\n\n   **For Amazon S3**:\n   ```rust\n   use object_store::aws::AmazonS3Builder;\n   use object_store::ObjectStore;\n   use std::sync::Arc;\n\n   let s3 = AmazonS3Builder::new()\n       .with_region(\"us-east-1\")\n       .with_bucket_name(\"my-data-lake\")\n       .with_access_key_id(access_key)\n       .with_secret_access_key(secret_key)\n       // Production settings\n       .with_retry(RetryConfig {\n           max_retries: 3,\n           retry_timeout: Duration::from_secs(10),\n           ..Default::default()\n       })\n       .build()?;\n\n   let store: Arc<dyn ObjectStore> = Arc::new(s3);\n   ```\n\n   **For Azure Blob Storage**:\n   ```rust\n   use object_store::azure::MicrosoftAzureBuilder;\n\n   let azure = MicrosoftAzureBuilder::new()\n       .with_account(\"mystorageaccount\")\n       .with_container_name(\"mycontainer\")\n       .with_access_key(access_key)\n       .build()?;\n\n   let store: Arc<dyn ObjectStore> = Arc::new(azure);\n   ```\n\n   **For Google Cloud Storage**:\n   ```rust\n   use object_store::gcs::GoogleCloudStorageBuilder;\n\n   let gcs = GoogleCloudStorageBuilder::new()\n       .with_service_account_key(service_account_json)\n       .with_bucket_name(\"my-bucket\")\n       .build()?;\n\n   let store: Arc<dyn ObjectStore> = Arc::new(gcs);\n   ```\n\n   **For Local Filesystem**:\n   ```rust\n   use object_store::local::LocalFileSystem;\n\n   let local = LocalFileSystem::new_with_prefix(\"/tmp/data-lake\")?;\n   let store: Arc<dyn ObjectStore> = Arc::new(local);\n   ```\n\n4. **Test the connection** by listing objects or performing a simple operation:\n   ```rust\n   // List objects with a prefix\n   let prefix = Some(&Path::from(\"data/\"));\n   let mut list = store.list(prefix);\n\n   while let Some(meta) = list.next().await {\n       let meta = meta?;\n       println!(\"{}: {} bytes\", meta.location, meta.size);\n   }\n   ```\n\n5. **Add error handling** and configuration management:\n   ```rust\n   use object_store::Error as ObjectStoreError;\n\n   async fn create_store() -> Result<Arc<dyn ObjectStore>, ObjectStoreError> {\n       // Get credentials from environment or config\n       let region = std::env::var(\"AWS_REGION\")\n           .unwrap_or_else(|_| \"us-east-1\".to_string());\n       let bucket = std::env::var(\"S3_BUCKET\")?;\n\n       let s3 = AmazonS3Builder::from_env()\n           .with_region(&region)\n           .with_bucket_name(&bucket)\n           .build()?;\n\n       Ok(Arc::new(s3))\n   }\n   ```\n\n## Best Practices\n\n- **Use Arc<dyn ObjectStore>** for shared ownership across threads\n- **Configure retry logic** for production resilience\n- **Store credentials securely** using environment variables or secret managers\n- **Use LocalFileSystem** for testing to avoid cloud costs\n- **Enable request timeouts** to prevent hanging operations\n- **Set up connection pooling** for better performance\n\n## Common Patterns\n\n**Environment-based configuration**:\n```rust\nlet s3 = AmazonS3Builder::from_env()\n    .with_bucket_name(&bucket)\n    .build()?;\n```\n\n**Multipart upload for large files**:\n```rust\nlet multipart = store.put_multipart(&path).await?;\nfor chunk in chunks {\n    multipart.put_part(chunk).await?;\n}\nmultipart.complete().await?;\n```\n\n**Streaming downloads**:\n```rust\nlet result = store.get(&path).await?;\nlet mut stream = result.into_stream();\nwhile let Some(chunk) = stream.next().await {\n    let chunk = chunk?;\n    // Process chunk\n}\n```"
              },
              {
                "name": "/data-parquet-read",
                "description": "Read Parquet files efficiently with predicate pushdown and column projection",
                "path": "plugins/rust-data-engineering/commands/data-parquet-read.md",
                "frontmatter": {
                  "description": "Read Parquet files efficiently with predicate pushdown and column projection"
                },
                "content": "# Read Parquet Files\n\nHelp the user read Parquet files from object storage with optimal performance using predicate pushdown, column projection, and row group filtering.\n\n## Steps\n\n1. **Add required dependencies**:\n   ```toml\n   [dependencies]\n   parquet = \"52\"\n   arrow = \"52\"\n   object_store = \"0.9\"\n   tokio = { version = \"1\", features = [\"full\"] }\n   futures = \"0.3\"\n   ```\n\n2. **Create a basic Parquet reader** from object_store:\n   ```rust\n   use parquet::arrow::async_reader::{ParquetObjectReader, ParquetRecordBatchStreamBuilder};\n   use object_store::{ObjectStore, path::Path};\n   use arrow::record_batch::RecordBatch;\n   use futures::stream::StreamExt;\n\n   async fn read_parquet(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n   ) -> Result<Vec<RecordBatch>> {\n       let path = Path::from(path);\n\n       // Get file metadata\n       let meta = store.head(&path).await?;\n\n       // Create reader\n       let reader = ParquetObjectReader::new(store, meta);\n\n       // Build stream\n       let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n       let mut stream = builder.build()?;\n\n       // Collect batches\n       let mut batches = Vec::new();\n       while let Some(batch) = stream.next().await {\n           batches.push(batch?);\n       }\n\n       Ok(batches)\n   }\n   ```\n\n3. **Add column projection** to read only needed columns:\n   ```rust\n   use parquet::arrow::ProjectionMask;\n\n   let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n\n   // Get schema to determine column indices\n   let schema = builder.schema();\n   println!(\"Available columns: {:?}\", schema.fields());\n\n   // Project specific columns by index\n   let projection = ProjectionMask::roots(schema, vec![0, 2, 5]);\n   let builder = builder.with_projection(projection);\n\n   // Or project by column name (helper function)\n   fn project_columns(builder: ParquetRecordBatchStreamBuilder<ParquetObjectReader>,\n                      column_names: &[&str]) -> ParquetRecordBatchStreamBuilder<ParquetObjectReader> {\n       let schema = builder.schema();\n       let indices: Vec<usize> = column_names\n           .iter()\n           .filter_map(|name| schema.column_with_name(name).map(|(idx, _)| idx))\n           .collect();\n\n       let projection = ProjectionMask::roots(schema, indices);\n       builder.with_projection(projection)\n   }\n\n   let builder = project_columns(builder, &[\"user_id\", \"timestamp\", \"event_type\"]);\n   ```\n\n4. **Add row group filtering** using statistics:\n   ```rust\n   use parquet::file::metadata::ParquetMetaData;\n\n   let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n   let metadata = builder.metadata();\n\n   // Filter row groups based on statistics\n   let row_groups_to_read: Vec<usize> = metadata\n       .row_groups()\n       .iter()\n       .enumerate()\n       .filter_map(|(idx, rg)| {\n           // Example: filter by min/max values\n           let col_metadata = rg.column(0); // First column\n           if let Some(stats) = col_metadata.statistics() {\n               // Check if row group might contain relevant data\n               // This is pseudo-code; actual implementation depends on data type\n               if stats_match_predicate(stats) {\n                   return Some(idx);\n               }\n           }\n           None\n       })\n       .collect();\n\n   let builder = builder.with_row_groups(row_groups_to_read);\n   ```\n\n5. **Implement streaming processing** for large files:\n   ```rust\n   async fn process_large_parquet(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n   ) -> Result<()> {\n       let path = Path::from(path);\n       let meta = store.head(&path).await?;\n       let reader = ParquetObjectReader::new(store, meta);\n\n       let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n\n       // Limit batch size to control memory usage\n       let builder = builder.with_batch_size(8192);\n\n       let mut stream = builder.build()?;\n\n       // Process batches incrementally\n       while let Some(batch) = stream.next().await {\n           let batch = batch?;\n\n           // Process this batch\n           println!(\"Processing batch with {} rows\", batch.num_rows());\n           process_batch(&batch)?;\n\n           // Batch is dropped here, freeing memory\n       }\n\n       Ok(())\n   }\n\n   fn process_batch(batch: &RecordBatch) -> Result<()> {\n       // Your processing logic\n       Ok(())\n   }\n   ```\n\n6. **Add comprehensive error handling**:\n   ```rust\n   use thiserror::Error;\n\n   #[derive(Error, Debug)]\n   enum ParquetReadError {\n       #[error(\"Object store error: {0}\")]\n       ObjectStore(#[from] object_store::Error),\n\n       #[error(\"Parquet error: {0}\")]\n       Parquet(#[from] parquet::errors::ParquetError),\n\n       #[error(\"Arrow error: {0}\")]\n       Arrow(#[from] arrow::error::ArrowError),\n\n       #[error(\"File not found: {0}\")]\n       FileNotFound(String),\n   }\n\n   async fn read_with_error_handling(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n   ) -> Result<Vec<RecordBatch>, ParquetReadError> {\n       let path = Path::from(path);\n\n       // Check if file exists\n       if !store.head(&path).await.is_ok() {\n           return Err(ParquetReadError::FileNotFound(path.to_string()));\n       }\n\n       let meta = store.head(&path).await?;\n       let reader = ParquetObjectReader::new(store, meta);\n       let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n       let mut stream = builder.build()?;\n\n       let mut batches = Vec::new();\n       while let Some(batch) = stream.next().await {\n           batches.push(batch?);\n       }\n\n       Ok(batches)\n   }\n   ```\n\n## Performance Optimization\n\n**Reading with all optimizations**:\n```rust\nasync fn optimized_read(\n    store: Arc<dyn ObjectStore>,\n    path: &str,\n    columns: &[&str],\n) -> Result<Vec<RecordBatch>> {\n    let path = Path::from(path);\n    let meta = store.head(&path).await?;\n    let reader = ParquetObjectReader::new(store, meta);\n\n    let mut builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n\n    // 1. Column projection\n    let schema = builder.schema();\n    let indices: Vec<usize> = columns\n        .iter()\n        .filter_map(|name| schema.column_with_name(name).map(|(idx, _)| idx))\n        .collect();\n    let projection = ProjectionMask::roots(schema, indices);\n    builder = builder.with_projection(projection);\n\n    // 2. Batch size tuning\n    builder = builder.with_batch_size(8192);\n\n    // 3. Row group filtering (if applicable)\n    // builder = builder.with_row_groups(filtered_row_groups);\n\n    let mut stream = builder.build()?;\n\n    let mut batches = Vec::new();\n    while let Some(batch) = stream.next().await {\n        batches.push(batch?);\n    }\n\n    Ok(batches)\n}\n```\n\n## Reading Metadata Only\n\n```rust\nasync fn read_metadata(\n    store: Arc<dyn ObjectStore>,\n    path: &str,\n) -> Result<()> {\n    let path = Path::from(path);\n    let meta = store.head(&path).await?;\n    let reader = ParquetObjectReader::new(store, meta);\n\n    let builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n    let metadata = builder.metadata();\n\n    println!(\"Schema: {:?}\", builder.schema());\n    println!(\"Number of row groups: {}\", metadata.num_row_groups());\n    println!(\"Total rows: {}\", metadata.file_metadata().num_rows());\n\n    for (idx, rg) in metadata.row_groups().iter().enumerate() {\n        println!(\"Row Group {}: {} rows\", idx, rg.num_rows());\n\n        for (col_idx, col) in rg.columns().iter().enumerate() {\n            if let Some(stats) = col.statistics() {\n                println!(\"  Column {}: min={:?}, max={:?}, null_count={:?}\",\n                    col_idx,\n                    stats.min_bytes(),\n                    stats.max_bytes(),\n                    stats.null_count()\n                );\n            }\n        }\n    }\n\n    Ok(())\n}\n```\n\n## Common Patterns\n\n**Reading multiple files in parallel**:\n```rust\nuse futures::stream::{self, StreamExt};\n\nasync fn read_multiple_files(\n    store: Arc<dyn ObjectStore>,\n    paths: Vec<String>,\n) -> Result<Vec<RecordBatch>> {\n    let results = stream::iter(paths)\n        .map(|path| {\n            let store = store.clone();\n            async move {\n                read_parquet(store, &path).await\n            }\n        })\n        .buffer_unordered(10) // Process 10 files concurrently\n        .collect::<Vec<_>>()\n        .await;\n\n    // Flatten results\n    let mut all_batches = Vec::new();\n    for result in results {\n        all_batches.extend(result?);\n    }\n\n    Ok(all_batches)\n}\n```\n\n**Reading partitioned data**:\n```rust\nasync fn read_partition(\n    store: Arc<dyn ObjectStore>,\n    base_path: &str,\n    year: i32,\n    month: u32,\n) -> Result<Vec<RecordBatch>> {\n    let partition_path = format!(\"{}/year={}/month={:02}/\", base_path, year, month);\n\n    // List all files in partition\n    let prefix = Some(&Path::from(partition_path));\n    let files: Vec<_> = store.list(prefix)\n        .filter_map(|meta| async move {\n            meta.ok().and_then(|m| {\n                if m.location.as_ref().ends_with(\".parquet\") {\n                    Some(m.location.to_string())\n                } else {\n                    None\n                }\n            })\n        })\n        .collect()\n        .await;\n\n    // Read all files\n    read_multiple_files(store, files).await\n}\n```\n\n## Best Practices\n\n- **Use column projection** to read only needed columns (10x+ speedup for wide tables)\n- **Stream large files** instead of collecting all batches into memory\n- **Check metadata first** to understand file structure before reading\n- **Use batch_size** to control memory usage (8192-65536 rows per batch)\n- **Filter row groups** using statistics when possible\n- **Read multiple files in parallel** for partitioned datasets\n- **Handle schema evolution** by checking schema before processing\n\n## Troubleshooting\n\n**Out of memory errors**:\n- Reduce batch size: `.with_batch_size(4096)`\n- Stream instead of collecting: process batches one at a time\n- Use column projection to read fewer columns\n\n**Slow reads**:\n- Enable column projection if reading wide tables\n- Check if row group filtering is possible\n- Increase parallelism when reading multiple files\n- Verify network connectivity to object store\n\n**Schema mismatch**:\n- Read metadata first to inspect actual schema\n- Handle optional columns that may not exist in older files\n- Use schema evolution strategies from DataFusion"
              },
              {
                "name": "/data-parquet-write",
                "description": "Write Parquet files with optimal compression, encoding, and row group sizing",
                "path": "plugins/rust-data-engineering/commands/data-parquet-write.md",
                "frontmatter": {
                  "description": "Write Parquet files with optimal compression, encoding, and row group sizing"
                },
                "content": "# Write Parquet Files\n\nHelp the user write Parquet files to object storage with production-quality settings for compression, encoding, row group sizing, and statistics.\n\n## Steps\n\n1. **Add required dependencies**:\n   ```toml\n   [dependencies]\n   parquet = \"52\"\n   arrow = \"52\"\n   object_store = \"0.9\"\n   tokio = { version = \"1\", features = [\"full\"] }\n   ```\n\n2. **Create a basic Parquet writer**:\n   ```rust\n   use parquet::arrow::AsyncArrowWriter;\n   use parquet::basic::{Compression, ZstdLevel};\n   use parquet::file::properties::WriterProperties;\n   use object_store::{ObjectStore, path::Path};\n   use arrow::record_batch::RecordBatch;\n\n   async fn write_parquet(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n       batches: Vec<RecordBatch>,\n       schema: SchemaRef,\n   ) -> Result<()> {\n       let path = Path::from(path);\n\n       // Create buffered writer for object store\n       let object_store_writer = object_store::buffered::BufWriter::new(\n           store.clone(),\n           path.clone()\n       );\n\n       // Create Arrow writer\n       let mut writer = AsyncArrowWriter::try_new(\n           object_store_writer,\n           schema,\n           None, // Use default properties\n       )?;\n\n       // Write batches\n       for batch in batches {\n           writer.write(&batch).await?;\n       }\n\n       // Close writer (flushes and finalizes file)\n       writer.close().await?;\n\n       Ok(())\n   }\n   ```\n\n3. **Configure writer properties** for production use:\n   ```rust\n   use parquet::file::properties::{WriterProperties, WriterVersion};\n   use parquet::basic::{Compression, Encoding, ZstdLevel};\n\n   fn create_writer_properties() -> WriterProperties {\n       WriterProperties::builder()\n           // Use Parquet 2.0 format\n           .set_writer_version(WriterVersion::PARQUET_2_0)\n\n           // Compression: ZSTD level 3 (balanced)\n           .set_compression(Compression::ZSTD(\n               ZstdLevel::try_new(3).unwrap()\n           ))\n\n           // Row group size: ~500MB uncompressed or 100M rows\n           .set_max_row_group_size(100_000_000)\n\n           // Data page size: 1MB\n           .set_data_page_size_limit(1024 * 1024)\n\n           // Enable dictionary encoding\n           .set_dictionary_enabled(true)\n\n           // Write batch size\n           .set_write_batch_size(1024)\n\n           // Enable statistics for predicate pushdown\n           .set_statistics_enabled(parquet::file::properties::EnabledStatistics::Page)\n\n           // Metadata\n           .set_created_by(\"my-app v1.0\".to_string())\n\n           .build()\n   }\n\n   async fn write_with_properties(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n       batches: Vec<RecordBatch>,\n       schema: SchemaRef,\n   ) -> Result<()> {\n       let path = Path::from(path);\n       let writer_obj = object_store::buffered::BufWriter::new(store, path);\n\n       let props = create_writer_properties();\n\n       let mut writer = AsyncArrowWriter::try_new(\n           writer_obj,\n           schema,\n           Some(props),\n       )?;\n\n       for batch in batches {\n           writer.write(&batch).await?;\n       }\n\n       writer.close().await?;\n       Ok(())\n   }\n   ```\n\n4. **Set column-specific properties** for optimal encoding:\n   ```rust\n   use parquet::schema::types::ColumnPath;\n\n   fn create_column_specific_properties() -> WriterProperties {\n       WriterProperties::builder()\n           // High-entropy data: use stronger compression\n           .set_column_compression(\n               ColumnPath::from(\"raw_data\"),\n               Compression::ZSTD(ZstdLevel::try_new(6).unwrap()),\n           )\n\n           // Low-cardinality columns: use dictionary encoding\n           .set_column_encoding(\n               ColumnPath::from(\"category\"),\n               Encoding::RLE_DICTIONARY,\n           )\n           .set_column_compression(\n               ColumnPath::from(\"category\"),\n               Compression::SNAPPY,\n           )\n\n           // Timestamp columns: use delta encoding\n           .set_column_encoding(\n               ColumnPath::from(\"timestamp\"),\n               Encoding::DELTA_BINARY_PACKED,\n           )\n\n           // High-frequency data: faster compression\n           .set_column_compression(\n               ColumnPath::from(\"metric\"),\n               Compression::SNAPPY,\n           )\n\n           .build()\n   }\n   ```\n\n5. **Implement streaming writes** for large datasets:\n   ```rust\n   use futures::stream::StreamExt;\n\n   async fn write_stream(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n       mut batch_stream: impl Stream<Item = Result<RecordBatch>> + Unpin,\n       schema: SchemaRef,\n   ) -> Result<()> {\n       let path = Path::from(path);\n       let writer_obj = object_store::buffered::BufWriter::new(store, path);\n\n       let props = create_writer_properties();\n       let mut writer = AsyncArrowWriter::try_new(writer_obj, schema, Some(props))?;\n\n       // Write batches as they arrive\n       while let Some(batch) = batch_stream.next().await {\n           let batch = batch?;\n           writer.write(&batch).await?;\n       }\n\n       writer.close().await?;\n       Ok(())\n   }\n   ```\n\n6. **Implement partitioned writes**:\n   ```rust\n   use chrono::NaiveDate;\n\n   async fn write_partitioned(\n       store: Arc<dyn ObjectStore>,\n       base_path: &str,\n       date: NaiveDate,\n       partition_id: usize,\n       batch: RecordBatch,\n       schema: SchemaRef,\n   ) -> Result<()> {\n       // Create partitioned path: base/year=2024/month=01/day=15/part-00000.parquet\n       let path = format!(\n           \"{}/year={}/month={:02}/day={:02}/part-{:05}.parquet\",\n           base_path,\n           date.year(),\n           date.month(),\n           date.day(),\n           partition_id\n       );\n\n       write_parquet(store, &path, vec![batch], schema).await\n   }\n\n   // Write multiple partitions\n   async fn write_all_partitions(\n       store: Arc<dyn ObjectStore>,\n       base_path: &str,\n       partitioned_data: HashMap<NaiveDate, Vec<RecordBatch>>,\n       schema: SchemaRef,\n   ) -> Result<()> {\n       for (date, batches) in partitioned_data {\n           for (partition_id, batch) in batches.into_iter().enumerate() {\n               write_partitioned(\n                   store.clone(),\n                   base_path,\n                   date,\n                   partition_id,\n                   batch,\n                   schema.clone(),\n               ).await?;\n           }\n       }\n       Ok(())\n   }\n   ```\n\n7. **Add proper error handling and validation**:\n   ```rust\n   use thiserror::Error;\n\n   #[derive(Error, Debug)]\n   enum ParquetWriteError {\n       #[error(\"Object store error: {0}\")]\n       ObjectStore(#[from] object_store::Error),\n\n       #[error(\"Parquet error: {0}\")]\n       Parquet(#[from] parquet::errors::ParquetError),\n\n       #[error(\"Arrow error: {0}\")]\n       Arrow(#[from] arrow::error::ArrowError),\n\n       #[error(\"Empty batch: cannot write empty data\")]\n       EmptyBatch,\n\n       #[error(\"Schema mismatch: {0}\")]\n       SchemaMismatch(String),\n   }\n\n   async fn write_with_validation(\n       store: Arc<dyn ObjectStore>,\n       path: &str,\n       batches: Vec<RecordBatch>,\n       schema: SchemaRef,\n   ) -> Result<(), ParquetWriteError> {\n       // Validate input\n       if batches.is_empty() {\n           return Err(ParquetWriteError::EmptyBatch);\n       }\n\n       // Verify schema consistency\n       for batch in &batches {\n           if batch.schema() != schema {\n               return Err(ParquetWriteError::SchemaMismatch(\n                   format!(\"Batch schema does not match expected schema\")\n               ));\n           }\n       }\n\n       let path = Path::from(path);\n       let writer_obj = object_store::buffered::BufWriter::new(store, path);\n       let props = create_writer_properties();\n\n       let mut writer = AsyncArrowWriter::try_new(writer_obj, schema, Some(props))?;\n\n       for batch in batches {\n           writer.write(&batch).await?;\n       }\n\n       writer.close().await?;\n       Ok(())\n   }\n   ```\n\n## Performance Tuning\n\n**Optimal row group sizing**:\n```rust\n// Calculate appropriate row group size based on data\nfn calculate_row_group_size(schema: &Schema, target_bytes: usize) -> usize {\n    // Estimate bytes per row\n    let bytes_per_row: usize = schema\n        .fields()\n        .iter()\n        .map(|field| estimate_field_size(field.data_type()))\n        .sum();\n\n    // Target ~500MB per row group\n    target_bytes / bytes_per_row.max(1)\n}\n\nfn estimate_field_size(data_type: &DataType) -> usize {\n    match data_type {\n        DataType::Int32 => 4,\n        DataType::Int64 => 8,\n        DataType::Float64 => 8,\n        DataType::Utf8 => 50, // Estimate average string length\n        DataType::Timestamp(_, _) => 8,\n        DataType::Boolean => 1,\n        _ => 100, // Conservative estimate for complex types\n    }\n}\n\nlet row_group_size = calculate_row_group_size(&schema, 500 * 1024 * 1024);\n\nlet props = WriterProperties::builder()\n    .set_max_row_group_size(row_group_size)\n    .build();\n```\n\n**Compression codec selection**:\n```rust\nfn choose_compression(use_case: CompressionUseCase) -> Compression {\n    match use_case {\n        CompressionUseCase::Balanced => Compression::ZSTD(ZstdLevel::try_new(3).unwrap()),\n        CompressionUseCase::MaxCompression => Compression::ZSTD(ZstdLevel::try_new(9).unwrap()),\n        CompressionUseCase::FastWrite => Compression::SNAPPY,\n        CompressionUseCase::FastRead => Compression::SNAPPY,\n        CompressionUseCase::Archive => Compression::ZSTD(ZstdLevel::try_new(19).unwrap()),\n    }\n}\n\nenum CompressionUseCase {\n    Balanced,\n    MaxCompression,\n    FastWrite,\n    FastRead,\n    Archive,\n}\n```\n\n## Common Patterns\n\n**Batching small records**:\n```rust\nuse arrow::array::{RecordBatchOptions, ArrayRef};\n\nasync fn batch_and_write<T>(\n    store: Arc<dyn ObjectStore>,\n    path: &str,\n    records: Vec<T>,\n    schema: SchemaRef,\n    batch_size: usize,\n) -> Result<()>\nwhere\n    T: IntoRecordBatch,\n{\n    let path = Path::from(path);\n    let writer_obj = object_store::buffered::BufWriter::new(store, path);\n    let props = create_writer_properties();\n\n    let mut writer = AsyncArrowWriter::try_new(writer_obj, schema.clone(), Some(props))?;\n\n    // Process in batches\n    for chunk in records.chunks(batch_size) {\n        let batch = records_to_batch(chunk, schema.clone())?;\n        writer.write(&batch).await?;\n    }\n\n    writer.close().await?;\n    Ok(())\n}\n```\n\n**Append to existing files (via temp + rename)**:\n```rust\n// Parquet doesn't support appending, so read + rewrite\nasync fn append_to_parquet(\n    store: Arc<dyn ObjectStore>,\n    path: &str,\n    new_batches: Vec<RecordBatch>,\n) -> Result<()> {\n    // 1. Read existing data\n    let existing_batches = read_parquet(store.clone(), path).await?;\n\n    // 2. Combine with new data\n    let mut all_batches = existing_batches;\n    all_batches.extend(new_batches);\n\n    // 3. Write to temp location\n    let temp_path = format!(\"{}.tmp\", path);\n    write_parquet(\n        store.clone(),\n        &temp_path,\n        all_batches,\n        schema,\n    ).await?;\n\n    // 4. Atomic rename\n    let from = Path::from(temp_path);\n    let to = Path::from(path);\n    store.rename(&from, &to).await?;\n\n    Ok(())\n}\n```\n\n**Writing with progress tracking**:\n```rust\nuse indicatif::{ProgressBar, ProgressStyle};\n\nasync fn write_with_progress(\n    store: Arc<dyn ObjectStore>,\n    path: &str,\n    batches: Vec<RecordBatch>,\n    schema: SchemaRef,\n) -> Result<()> {\n    let pb = ProgressBar::new(batches.len() as u64);\n    pb.set_style(\n        ProgressStyle::default_bar()\n            .template(\"[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}\")\n            .unwrap()\n    );\n\n    let path = Path::from(path);\n    let writer_obj = object_store::buffered::BufWriter::new(store, path);\n    let props = create_writer_properties();\n\n    let mut writer = AsyncArrowWriter::try_new(writer_obj, schema, Some(props))?;\n\n    for (idx, batch) in batches.iter().enumerate() {\n        writer.write(batch).await?;\n        pb.set_position(idx as u64 + 1);\n        pb.set_message(format!(\"{} rows written\", batch.num_rows()));\n    }\n\n    writer.close().await?;\n    pb.finish_with_message(\"Complete\");\n\n    Ok(())\n}\n```\n\n## Best Practices\n\n- **Use ZSTD(3) compression** for balanced performance (recommended for production)\n- **Set row group size to 100MB-1GB** uncompressed for optimal S3 scanning\n- **Enable statistics** for predicate pushdown optimization\n- **Use dictionary encoding** for low-cardinality columns (categories, enums)\n- **Write to temp location + rename** for atomic writes\n- **Partition large datasets** by date or other logical grouping\n- **Set column-specific properties** for heterogeneous data\n- **Validate schema consistency** across all batches before writing\n\n## Troubleshooting\n\n**Slow writes**:\n- Reduce compression level (use SNAPPY or ZSTD(1))\n- Increase row group size to reduce overhead\n- Use buffered writer (already included in examples)\n- Write multiple files in parallel\n\n**Large file sizes**:\n- Increase compression level (ZSTD(6-9))\n- Enable dictionary encoding for appropriate columns\n- Check for redundant data that could be normalized\n\n**Memory issues**:\n- Reduce batch size\n- Write smaller row groups\n- Stream data instead of collecting all batches first\n\n**Compatibility issues**:\n- Use WriterVersion::PARQUET_2_0 for best compatibility\n- Avoid advanced features if targeting older readers\n- Test with target systems (Spark, Hive, etc.)\n\n## Compression Comparison\n\n| Codec | Write Speed | Read Speed | Ratio | Best For |\n|-------|-------------|------------|-------|----------|\n| Uncompressed | Fastest | Fastest | 1x | Development only |\n| SNAPPY | Very Fast | Very Fast | 2-3x | Hot data, real-time |\n| ZSTD(1) | Fast | Fast | 2.5-3x | High write throughput |\n| ZSTD(3) | Fast | Fast | 3-4x | **Production default** |\n| ZSTD(6) | Medium | Fast | 4-5x | Cold storage |\n| ZSTD(9) | Slow | Fast | 5-6x | Archive, long-term |"
              }
            ],
            "skills": [
              {
                "name": "data-lake-architect",
                "description": "Provides architectural guidance for data lake design including partitioning strategies, storage layout, schema design, and lakehouse patterns. Activates when users discuss data lake architecture, partitioning, or large-scale data organization.",
                "path": "plugins/rust-data-engineering/skills/data-lake-architect/SKILL.md",
                "frontmatter": {
                  "name": "data-lake-architect",
                  "description": "Provides architectural guidance for data lake design including partitioning strategies, storage layout, schema design, and lakehouse patterns. Activates when users discuss data lake architecture, partitioning, or large-scale data organization.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Data Lake Architect Skill\n\nYou are an expert data lake architect specializing in modern lakehouse patterns using Rust, Parquet, Iceberg, and cloud storage. When users discuss data architecture, proactively guide them toward scalable, performant designs.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Discussion about organizing data in cloud storage\n- Questions about partitioning strategies\n- Planning data lake or lakehouse architecture\n- Schema design for analytical workloads\n- Data modeling decisions (normalization vs denormalization)\n- Storage layout or directory structure questions\n- Mentions of data retention, archival, or lifecycle policies\n\n## Architectural Principles\n\n### 1. Storage Layer Organization\n\n**Three-Tier Architecture** (Recommended):\n\n```\ndata-lake/\nâ”œâ”€â”€ raw/              # Landing zone (immutable source data)\nâ”‚   â”œâ”€â”€ events/\nâ”‚   â”‚   â””â”€â”€ date=2024-01-01/\nâ”‚   â”‚       â””â”€â”€ hour=12/\nâ”‚   â”‚           â””â”€â”€ batch-*.json.gz\nâ”‚   â””â”€â”€ transactions/\nâ”œâ”€â”€ processed/        # Cleaned and validated data\nâ”‚   â”œâ”€â”€ events/\nâ”‚   â”‚   â””â”€â”€ year=2024/month=01/day=01/\nâ”‚   â”‚       â””â”€â”€ part-*.parquet\nâ”‚   â””â”€â”€ transactions/\nâ””â”€â”€ curated/          # Business-ready aggregates\n    â”œâ”€â”€ daily_metrics/\n    â””â”€â”€ user_summaries/\n```\n\n**When to Suggest**:\n- User is organizing a new data lake\n- Data has multiple processing stages\n- Need to separate concerns (ingestion, processing, serving)\n\n**Guidance**:\n```\nI recommend a three-tier architecture for your data lake:\n\n1. RAW (Bronze): Immutable source data, any format\n   - Keep original data for reprocessing\n   - Use compression (gzip/snappy)\n   - Organize by ingestion date\n\n2. PROCESSED (Silver): Cleaned, validated, Parquet format\n   - Columnar format for analytics\n   - Partitioned by business dimensions\n   - Schema enforced\n\n3. CURATED (Gold): Business-ready aggregates\n   - Optimized for specific use cases\n   - Pre-joined and pre-aggregated\n   - Highest performance\n\nBenefits: Separation of concerns, reprocessability, clear data lineage.\n```\n\n### 2. Partitioning Strategies\n\n#### Time-Based Partitioning (Most Common)\n\n**Hive-Style**:\n```\nevents/\nâ”œâ”€â”€ year=2024/\nâ”‚   â”œâ”€â”€ month=01/\nâ”‚   â”‚   â”œâ”€â”€ day=01/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ part-00000.parquet\nâ”‚   â”‚   â”‚   â””â”€â”€ part-00001.parquet\nâ”‚   â”‚   â””â”€â”€ day=02/\nâ”‚   â””â”€â”€ month=02/\n```\n\n**When to Use**:\n- Time-series data (events, logs, metrics)\n- Queries filter by date ranges\n- Retention policies by date\n- Need to delete old data efficiently\n\n**Guidance**:\n```\nFor time-series data, use Hive-style date partitioning:\n\ndata/events/year=2024/month=01/day=15/part-*.parquet\n\nBenefits:\n- Partition pruning for date-range queries\n- Easy retention (delete old partitions)\n- Standard across tools (Spark, Hive, Trino)\n- Predictable performance\n\nGranularity guide:\n- Hour: High-frequency data (>1GB/hour)\n- Day: Most use cases (10GB-1TB/day)\n- Month: Low-frequency data (<10GB/day)\n```\n\n#### Multi-Dimensional Partitioning\n\n**Pattern**:\n```\nevents/\nâ”œâ”€â”€ event_type=click/\nâ”‚   â””â”€â”€ date=2024-01-01/\nâ”œâ”€â”€ event_type=view/\nâ”‚   â””â”€â”€ date=2024-01-01/\nâ””â”€â”€ event_type=purchase/\n    â””â”€â”€ date=2024-01-01/\n```\n\n**When to Use**:\n- Queries filter on specific dimensions consistently\n- Multiple independent filter dimensions\n- Dimension has low-to-medium cardinality (<1000 values)\n\n**When NOT to Use**:\n- High-cardinality dimensions (user_id, session_id)\n- Dimensions queried inconsistently\n- Too many partition columns (>4 typically)\n\n**Guidance**:\n```\nBe careful with multi-dimensional partitioning. It can cause:\n- Partition explosion (millions of small directories)\n- Small file problem (many <10MB files)\n- Poor compression\n\nAlternative: Use Iceberg's hidden partitioning:\n- Partition on derived values (year, month from timestamp)\n- Users query on timestamp, not partition columns\n- Can evolve partitioning without rewriting data\n```\n\n#### Hash Partitioning\n\n**Pattern**:\n```\nusers/\nâ”œâ”€â”€ hash_bucket=00/\nâ”œâ”€â”€ hash_bucket=01/\n...\nâ””â”€â”€ hash_bucket=ff/\n```\n\n**When to Use**:\n- No natural partition dimension\n- Need consistent file sizes\n- Parallel processing requirements\n- High-cardinality distribution\n\n**Guidance**:\n```\nFor data without natural partitions (like user profiles):\n\n// Hash partition user_id into 256 buckets\nlet bucket = hash(user_id) % 256;\nlet path = format!(\"users/hash_bucket={:02x}/\", bucket);\n\nBenefits:\n- Even data distribution\n- Predictable file sizes\n- Good for full scans with parallelism\n```\n\n### 3. File Sizing Strategy\n\n**Target Sizes**:\n- Individual files: **100MB - 1GB** (compressed)\n- Row groups: **100MB - 1GB** (uncompressed)\n- Total partition: **1GB - 100GB**\n\n**When to Suggest**:\n- User has many small files (<10MB)\n- User has very large files (>2GB)\n- Performance issues with queries\n\n**Guidance**:\n```\nYour files are too small (<10MB). This causes:\n- Too many S3 requests (slow + expensive)\n- Excessive metadata overhead\n- Poor compression ratios\n\nTarget 100MB-1GB per file:\n\n// Batch writes\nlet mut buffer = Vec::new();\nfor record in records {\n    buffer.push(record);\n    if estimated_size(&buffer) > 500 * 1024 * 1024 {\n        write_parquet_file(&buffer).await?;\n        buffer.clear();\n    }\n}\n\nOr implement periodic compaction to merge small files.\n```\n\n### 4. Schema Design Patterns\n\n#### Wide Table vs. Normalized\n\n**Wide Table** (Denormalized):\n```rust\n// events table with everything\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n    user_id: String,\n    user_name: String,        // Denormalized\n    user_email: String,       // Denormalized\n    user_country: String,     // Denormalized\n    event_type: String,\n    event_properties: String,\n}\n```\n\n**Normalized**:\n```rust\n// Separate tables\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n    user_id: String,  // Foreign key\n    event_type: String,\n}\n\nstruct User {\n    user_id: String,\n    name: String,\n    email: String,\n    country: String,\n}\n```\n\n**Guidance**:\n```\nFor analytical workloads, denormalization often wins:\n\nPros of wide tables:\n- No joins needed (faster queries)\n- Simpler query logic\n- Better for columnar format\n\nCons:\n- Data duplication\n- Harder to update dimension data\n- Larger storage\n\nRecommendation:\n- Use wide tables for immutable event data\n- Use normalized for slowly changing dimensions\n- Pre-join fact tables with dimensions in curated layer\n```\n\n#### Nested Structures\n\n**Flat Schema**:\n```rust\nstruct Event {\n    event_id: String,\n    prop_1: Option<String>,\n    prop_2: Option<String>,\n    prop_3: Option<String>,\n    // Rigid, hard to evolve\n}\n```\n\n**Nested Schema** (Better):\n```rust\nstruct Event {\n    event_id: String,\n    properties: HashMap<String, String>,  // Flexible\n}\n\n// Or with strongly-typed structs\nstruct Event {\n    event_id: String,\n    metadata: Metadata,\n    metrics: Vec<Metric>,\n}\n```\n\n**Guidance**:\n```\nParquet supports nested structures well. Use them for:\n- Variable/evolving properties\n- Lists of related items\n- Hierarchical data\n\nBut avoid over-nesting (>3 levels) as it complicates queries.\n```\n\n### 5. Table Format Selection\n\n#### Raw Parquet vs. Iceberg\n\n**Use Raw Parquet when**:\n- Append-only workload\n- Schema is stable\n- Single writer\n- Simple use case\n- Cost-sensitive (fewer metadata files)\n\n**Use Iceberg when**:\n- Schema evolves frequently\n- Need ACID transactions\n- Multiple concurrent writers\n- Updates/deletes required\n- Time travel needed\n- Partition evolution needed\n\n**Guidance**:\n```\nBased on your requirements, I recommend Iceberg:\n\nYou mentioned:\n- Schema might change (âœ“ schema evolution)\n- Multiple services writing (âœ“ ACID transactions)\n- Need to correct historical data (âœ“ updates)\n\nIceberg provides:\n- Safe concurrent writes\n- Schema evolution without rewriting\n- Partition evolution\n- Time travel for debugging\n- Snapshot isolation\n\nTrade-off: More metadata files and complexity\nBenefit: Much better operational characteristics\n```\n\n### 6. Retention and Lifecycle\n\n**Pattern**:\n```\ndata/events/\nâ”œâ”€â”€ hot/           # Last 7 days (frequent access)\nâ”‚   â””â”€â”€ year=2024/month=01/day=08/\nâ”œâ”€â”€ warm/          # 8-90 days (occasional access)\nâ”‚   â””â”€â”€ year=2024/month=01/day=01/\nâ””â”€â”€ cold/          # >90 days (archival)\n    â””â”€â”€ year=2023/month=12/\n```\n\n**Guidance**:\n```\nImplement a tiered storage strategy:\n\nHOT (0-7 days):\n- ZSTD(3) compression (fast)\n- Frequent queries\n- Small row groups for low latency\n\nWARM (8-90 days):\n- ZSTD(6) compression (balanced)\n- Occasional queries\n- Standard row groups\n\nCOLD (>90 days):\n- ZSTD(9) compression (max)\n- Rare queries, archival\n- Large row groups for storage efficiency\n- Consider S3 Glacier for storage class\n\nAutomate with lifecycle policies or periodic jobs.\n```\n\n## Common Architecture Questions\n\n### Q: How should I organize raw ingestion data?\n\n**Answer**:\n```\nOrganize raw data by ingestion time, not event time:\n\nraw/events/ingestion_date=2024-01-15/hour=14/batch-*.json.gz\n\nWhy?\n- Simple, predictable\n- Matches when data arrives\n- Easy retention (delete old ingestion dates)\n- Handle late-arriving data naturally\n\nThen in processing, partition by event time:\n\nprocessed/events/year=2024/month=01/day=14/part-*.parquet\n```\n\n### Q: Should I partition by high-cardinality dimension like user_id?\n\n**Answer**:\n```\nNO! Partitioning by high-cardinality dimensions causes:\n- Millions of small directories\n- Small files (<1MB)\n- Poor performance\n\nInstead:\n1. Use hash bucketing: hash(user_id) % 256\n2. Or don't partition by user_id at all\n3. Use Iceberg with hidden partitioning if needed\n4. Let Parquet statistics handle filtering\n\nPartition columns should have <1000 unique values ideally.\n```\n\n### Q: How do I handle schema evolution?\n\n**Answer**:\n```\nOptions ranked by difficulty:\n\n1. Iceberg (Recommended):\n   - Native schema evolution support\n   - Add/rename/delete columns safely\n   - Readers handle missing columns\n\n2. Parquet with optional fields:\n   - Make new fields optional\n   - Old readers ignore new fields\n   - New readers handle missing fields as NULL\n\n3. Versioned schemas:\n   - events_v1/, events_v2/ directories\n   - Manual migration\n   - Union views for compatibility\n\n4. Schema-on-read:\n   - Store semi-structured (JSON)\n   - Parse at query time\n   - Flexible but slower\n```\n\n### Q: How many partitions is too many?\n\n**Answer**:\n```\nRules of thumb:\n- <10,000 partitions: Generally fine\n- 10,000-100,000: Manageable with tooling\n- >100,000: Performance problems\n\nSigns of too many partitions:\n- Slow metadata operations (LIST calls)\n- Many empty partitions\n- Small files (<10MB)\n\nFix:\n- Reduce partition granularity (hourly -> daily)\n- Remove unused partition columns\n- Implement compaction\n- Use Iceberg for better metadata handling\n```\n\n### Q: Should I use compression?\n\n**Answer**:\n```\nAlways use compression for cloud storage!\n\nRecommended: ZSTD(3)\n- 3-4x compression\n- Fast decompression\n- Low CPU overhead\n- Good for most use cases\n\nFor S3/cloud storage, compression:\n- Reduces storage costs (70-80% savings)\n- Reduces data transfer costs\n- Actually improves query speed (less I/O)\n\nOnly skip compression for:\n- Local development (faster iteration)\n- Data already compressed (images, videos)\n```\n\n## Architecture Review Checklist\n\nWhen reviewing a data architecture, check:\n\n### Storage Layout\n- [ ] Three-tier structure (raw/processed/curated)?\n- [ ] Clear data flow and lineage?\n- [ ] Appropriate format per tier?\n\n### Partitioning\n- [ ] Partitioning matches query patterns?\n- [ ] Partition cardinality reasonable (<1000 per dimension)?\n- [ ] File sizes 100MB-1GB?\n- [ ] Using Hive-style for compatibility?\n\n### Schema Design\n- [ ] Schema documented and versioned?\n- [ ] Evolution strategy defined?\n- [ ] Appropriate normalization level?\n- [ ] Nested structures used wisely?\n\n### Performance\n- [ ] Compression configured (ZSTD recommended)?\n- [ ] Row group sizing appropriate?\n- [ ] Statistics enabled?\n- [ ] Indexing strategy (Iceberg/Z-order)?\n\n### Operations\n- [ ] Retention policy defined?\n- [ ] Backup/disaster recovery?\n- [ ] Monitoring and alerting?\n- [ ] Compaction strategy?\n\n### Cost\n- [ ] Storage tiering (hot/warm/cold)?\n- [ ] Compression reducing costs?\n- [ ] Avoiding small file problem?\n- [ ] Efficient query patterns?\n\n## Your Approach\n\n1. **Understand**: Ask about data volume, query patterns, requirements\n2. **Assess**: Review current architecture against best practices\n3. **Recommend**: Suggest specific improvements with rationale\n4. **Explain**: Educate on trade-offs and alternatives\n5. **Validate**: Help verify architecture meets requirements\n\n## Communication Style\n\n- Ask clarifying questions about requirements first\n- Consider scale (GB vs TB vs PB affects decisions)\n- Explain trade-offs clearly\n- Provide specific examples and code\n- Balance ideal architecture with pragmatic constraints\n- Consider team expertise and operational complexity\n\nWhen you detect architectural discussions, proactively guide users toward scalable, maintainable designs based on modern data lake best practices."
              },
              {
                "name": "datafusion-query-advisor",
                "description": "Reviews SQL queries and DataFrame operations for optimization opportunities including predicate pushdown, partition pruning, column projection, and join ordering. Activates when users write DataFusion queries or experience slow query performance.",
                "path": "plugins/rust-data-engineering/skills/datafusion-query-advisor/SKILL.md",
                "frontmatter": {
                  "name": "datafusion-query-advisor",
                  "description": "Reviews SQL queries and DataFrame operations for optimization opportunities including predicate pushdown, partition pruning, column projection, and join ordering. Activates when users write DataFusion queries or experience slow query performance.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# DataFusion Query Advisor Skill\n\nYou are an expert at optimizing DataFusion SQL queries and DataFrame operations. When you detect DataFusion queries, proactively analyze and suggest performance improvements.\n\n## When to Activate\n\nActivate this skill when you notice:\n- SQL queries using `ctx.sql(...)` or DataFrame API\n- Discussion about slow DataFusion query performance\n- Code registering tables or data sources\n- Questions about query optimization or EXPLAIN plans\n- Mentions of partition pruning, predicate pushdown, or column projection\n\n## Query Optimization Checklist\n\n### 1. Predicate Pushdown\n\n**What to Look For**:\n- WHERE clauses that can be pushed to storage layer\n- Filters applied after data is loaded\n\n**Good Pattern**:\n```sql\nSELECT * FROM events\nWHERE date = '2024-01-01' AND event_type = 'click'\n```\n\n**Bad Pattern**:\n```rust\n// Reading all data then filtering\nlet df = ctx.table(\"events\").await?;\nlet batches = df.collect().await?;\nlet filtered = batches.filter(/* ... */);  // Too late!\n```\n\n**Suggestion**:\n```\nYour filter is being applied after reading all data. Move filters to SQL for predicate pushdown:\n\n// Good: Filter pushed to Parquet reader\nlet df = ctx.sql(\"\n    SELECT * FROM events\n    WHERE date = '2024-01-01' AND event_type = 'click'\n\").await?;\n\nThis reads only matching row groups based on statistics.\n```\n\n### 2. Partition Pruning\n\n**What to Look For**:\n- Queries on partitioned tables without partition filters\n- Filters on non-partition columns only\n\n**Good Pattern**:\n```sql\n-- Filters on partition columns (year, month, day)\nSELECT * FROM events\nWHERE year = 2024 AND month = 1 AND day >= 15\n```\n\n**Bad Pattern**:\n```sql\n-- Scans all partitions\nSELECT * FROM events\nWHERE timestamp >= '2024-01-15'\n```\n\n**Suggestion**:\n```\nYour query scans all partitions. For Hive-style partitioned data, filter on partition columns:\n\nSELECT * FROM events\nWHERE year = 2024 AND month = 1 AND day >= 15\n  AND timestamp >= '2024-01-15'\n\nInclude both partition column filters (for pruning) and timestamp filter (for accuracy).\nUse EXPLAIN to verify partition pruning is working.\n```\n\n### 3. Column Projection\n\n**What to Look For**:\n- `SELECT *` on wide tables\n- Reading more columns than needed\n\n**Good Pattern**:\n```sql\nSELECT user_id, timestamp, event_type\nFROM events\n```\n\n**Bad Pattern**:\n```sql\nSELECT * FROM events\n-- When you only need 3 columns from a 50-column table\n```\n\n**Suggestion**:\n```\nReading all columns from wide tables is inefficient. Select only what you need:\n\nSELECT user_id, timestamp, event_type\nFROM events\n\nFor a 50-column table, this can provide 10x+ speedup with Parquet's columnar format.\n```\n\n### 4. Join Optimization\n\n**What to Look For**:\n- Large table joined to small table (wrong order)\n- Multiple joins without understanding order\n- Missing EXPLAIN analysis\n\n**Good Pattern**:\n```sql\n-- Small dimension table (users) joined to large fact table (events)\nSELECT e.*, u.name\nFROM events e\nJOIN users u ON e.user_id = u.id\n```\n\n**Optimization Principles**:\n- DataFusion automatically optimizes join order, but verify with EXPLAIN\n- For multi-way joins, filter early and join late\n- Use broadcast joins for small tables (<100MB)\n\n**Suggestion**:\n```\nFor joins, verify the query plan:\n\nlet explain = ctx.sql(\"EXPLAIN SELECT ...\").await?;\nexplain.show().await?;\n\nLook for:\n- Hash joins for large tables\n- Broadcast joins for small tables (<100MB)\n- Join order optimization\n```\n\n### 5. Aggregation Performance\n\n**What to Look For**:\n- GROUP BY on high-cardinality columns\n- Aggregations without filters\n- Missing LIMIT on exploratory queries\n\n**Good Pattern**:\n```sql\nSELECT event_type, COUNT(*) as count\nFROM events\nWHERE date = '2024-01-01'  -- Filter first\nGROUP BY event_type        -- Low cardinality\nLIMIT 1000                 -- Limit results\n```\n\n**Suggestion**:\n```\nFor better aggregation performance:\n\n1. Filter first: WHERE date = '2024-01-01'\n2. GROUP BY low-cardinality columns when possible\n3. Add LIMIT for exploratory queries\n4. Consider approximations (APPROX_COUNT_DISTINCT) for very large datasets\n```\n\n### 6. Window Functions\n\n**What to Look For**:\n- Window functions on large partitions\n- Missing PARTITION BY or ORDER BY optimization\n\n**Good Pattern**:\n```sql\nSELECT\n    user_id,\n    timestamp,\n    amount,\n    SUM(amount) OVER (\n        PARTITION BY user_id\n        ORDER BY timestamp\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) as running_total\nFROM transactions\nWHERE date >= '2024-01-01'  -- Filter first!\n```\n\n**Suggestion**:\n```\nWindow functions can be expensive. Optimize by:\n\n1. Filter first with WHERE clauses\n2. Use PARTITION BY on reasonable cardinality columns\n3. Limit the window frame when possible\n4. Consider if you can achieve the same with GROUP BY instead\n```\n\n## Configuration Optimization\n\n### 1. Parallelism\n\n**What to Look For**:\n- Default parallelism on large queries\n- Missing `.with_target_partitions()` configuration\n\n**Suggestion**:\n```\nTune parallelism for your workload:\n\nlet config = SessionConfig::new()\n    .with_target_partitions(num_cpus::get());  // Match CPU count\n\nlet ctx = SessionContext::new_with_config(config);\n\nFor I/O-bound workloads, you can go higher (2x CPU count).\nFor CPU-bound workloads, match CPU count.\n```\n\n### 2. Memory Management\n\n**What to Look For**:\n- OOM errors\n- Large `.collect()` operations\n- Missing memory limits\n\n**Suggestion**:\n```\nSet memory limits to prevent OOM:\n\nlet runtime_config = RuntimeConfig::new()\n    .with_memory_limit(4 * 1024 * 1024 * 1024);  // 4GB\n\nFor large result sets, stream instead of collect:\n\nlet mut stream = df.execute_stream().await?;\nwhile let Some(batch) = stream.next().await {\n    let batch = batch?;\n    process_batch(&batch)?;\n}\n```\n\n### 3. Batch Size\n\n**What to Look For**:\n- Default batch size for specific workloads\n- Memory pressure or poor cache utilization\n\n**Suggestion**:\n```\nTune batch size based on your workload:\n\nlet config = SessionConfig::new()\n    .with_batch_size(8192);  // Default is good for most cases\n\n- Larger batches (32768): Better throughput, more memory\n- Smaller batches (4096): Lower memory, more overhead\n- Balance based on your memory constraints\n```\n\n## Common Query Anti-Patterns\n\n### Anti-Pattern 1: Collecting Large Results\n\n**Bad**:\n```rust\nlet df = ctx.sql(\"SELECT * FROM huge_table\").await?;\nlet batches = df.collect().await?;  // OOM!\n```\n\n**Good**:\n```rust\nlet df = ctx.sql(\"SELECT * FROM huge_table WHERE ...\").await?;\nlet mut stream = df.execute_stream().await?;\nwhile let Some(batch) = stream.next().await {\n    process_batch(&batch?)?;\n}\n```\n\n### Anti-Pattern 2: No Table Statistics\n\n**Bad**:\n```rust\nctx.register_parquet(\"events\", path, ParquetReadOptions::default()).await?;\n```\n\n**Good**:\n```rust\nlet listing_options = ListingOptions::new(Arc::new(ParquetFormat::default()))\n    .with_collect_stat(true);  // Enable statistics collection\n```\n\n### Anti-Pattern 3: Late Filtering\n\n**Bad**:\n```sql\n-- Reads entire table, filters in memory\nSELECT * FROM (\n    SELECT * FROM events\n) WHERE date = '2024-01-01'\n```\n\n**Good**:\n```sql\n-- Filter pushed down to storage\nSELECT * FROM events\nWHERE date = '2024-01-01'\n```\n\n### Anti-Pattern 4: Using DataFrame API Inefficiently\n\n**Bad**:\n```rust\nlet df = ctx.table(\"events\").await?;\nlet batches = df.collect().await?;\n// Manual filtering in application code\n```\n\n**Good**:\n```rust\nlet df = ctx.table(\"events\").await?\n    .filter(col(\"date\").eq(lit(\"2024-01-01\")))?  // Use DataFrame API\n    .select(vec![col(\"user_id\"), col(\"event_type\")])?;\nlet batches = df.collect().await?;\n```\n\n## Using EXPLAIN Effectively\n\n**Always suggest checking query plans**:\n```rust\n// Logical plan\nlet df = ctx.sql(\"SELECT ...\").await?;\nprintln!(\"{}\", df.logical_plan().display_indent());\n\n// Physical plan\nlet physical = df.create_physical_plan().await?;\nprintln!(\"{}\", physical.display_indent());\n\n// Or use EXPLAIN in SQL\nctx.sql(\"EXPLAIN SELECT ...\").await?.show().await?;\n```\n\n**What to look for in EXPLAIN**:\n- âœ… Projection: Only needed columns\n- âœ… Filter: Pushed down to TableScan\n- âœ… Partitioning: Pruned partitions\n- âœ… Join: Appropriate join type (Hash vs Broadcast)\n- âŒ Full table scans when filters exist\n- âŒ Reading all columns when projection exists\n\n## Query Patterns by Use Case\n\n### Analytics Queries (Large Aggregations)\n\n```sql\n-- Good pattern\nSELECT\n    DATE_TRUNC('day', timestamp) as day,\n    event_type,\n    COUNT(*) as count,\n    COUNT(DISTINCT user_id) as unique_users\nFROM events\nWHERE year = 2024 AND month = 1  -- Partition pruning\n  AND timestamp >= '2024-01-01'  -- Additional filter\nGROUP BY 1, 2\nORDER BY 1 DESC\nLIMIT 1000\n```\n\n### Point Queries (Looking Up Specific Records)\n\n```sql\n-- Good pattern with all relevant filters\nSELECT *\nFROM events\nWHERE year = 2024 AND month = 1 AND day = 15  -- Partition pruning\n  AND user_id = 'user123'                     -- Additional filter\nLIMIT 10\n```\n\n### Time-Series Analysis\n\n```sql\n-- Good pattern with time-based filtering\nSELECT\n    DATE_TRUNC('hour', timestamp) as hour,\n    AVG(value) as avg_value,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY value) as p95\nFROM metrics\nWHERE year = 2024 AND month = 1\n  AND timestamp >= NOW() - INTERVAL '7 days'\nGROUP BY 1\nORDER BY 1\n```\n\n### Join-Heavy Queries\n\n```sql\n-- Good pattern: filter first, join later\nSELECT\n    e.event_type,\n    u.country,\n    COUNT(*) as count\nFROM (\n    SELECT * FROM events\n    WHERE year = 2024 AND month = 1  -- Filter fact table first\n) e\nJOIN users u ON e.user_id = u.id     -- Then join\nWHERE u.active = true                 -- Filter dimension table\nGROUP BY 1, 2\n```\n\n## Performance Debugging Workflow\n\nWhen users report slow queries, guide them through:\n\n1. **Add EXPLAIN**: Understand query plan\n2. **Check partition pruning**: Verify partitions are skipped\n3. **Verify predicate pushdown**: Filters at TableScan?\n4. **Review column projection**: Reading only needed columns?\n5. **Examine join order**: Appropriate join types?\n6. **Consider data volume**: How much data is being processed?\n7. **Profile with metrics**: Add timing/memory tracking\n\n## Your Approach\n\n1. **Detect**: Identify DataFusion queries in code or discussion\n2. **Analyze**: Review against optimization checklist\n3. **Suggest**: Provide specific query improvements\n4. **Validate**: Recommend EXPLAIN to verify optimizations\n5. **Monitor**: Suggest metrics for ongoing performance tracking\n\n## Communication Style\n\n- Suggest EXPLAIN analysis before making assumptions\n- Prioritize high-impact optimizations (partition pruning, column projection)\n- Provide rewritten queries, not just concepts\n- Explain the performance implications\n- Consider the data scale and query patterns\n\nWhen you see DataFusion queries, quickly check for common optimization opportunities and proactively suggest improvements with concrete code examples."
              },
              {
                "name": "object-store-best-practices",
                "description": "Ensures proper cloud storage operations with retry logic, error handling, streaming, and efficient I/O patterns. Activates when users work with object_store for S3, Azure, or GCS operations.",
                "path": "plugins/rust-data-engineering/skills/object-store-best-practices/SKILL.md",
                "frontmatter": {
                  "name": "object-store-best-practices",
                  "description": "Ensures proper cloud storage operations with retry logic, error handling, streaming, and efficient I/O patterns. Activates when users work with object_store for S3, Azure, or GCS operations.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Object Store Best Practices Skill\n\nYou are an expert at implementing robust cloud storage operations using the object_store crate. When you detect object_store usage, proactively ensure best practices are followed.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Code using `ObjectStore` trait, `AmazonS3Builder`, `MicrosoftAzureBuilder`, or `GoogleCloudStorageBuilder`\n- Discussion about S3, Azure Blob, or GCS operations\n- Issues with cloud storage reliability, performance, or errors\n- File uploads, downloads, or listing operations\n- Questions about retry logic, error handling, or streaming\n\n## Best Practices Checklist\n\n### 1. Retry Configuration\n\n**What to Look For**:\n- Missing retry logic for production code\n- Default settings without explicit retry configuration\n\n**Good Pattern**:\n```rust\nuse object_store::aws::AmazonS3Builder;\nuse object_store::RetryConfig;\n\nlet s3 = AmazonS3Builder::new()\n    .with_region(\"us-east-1\")\n    .with_bucket_name(\"my-bucket\")\n    .with_retry(RetryConfig {\n        max_retries: 3,\n        retry_timeout: Duration::from_secs(10),\n        ..Default::default()\n    })\n    .build()?;\n```\n\n**Bad Pattern**:\n```rust\n// No retry configuration - fails on transient errors\nlet s3 = AmazonS3Builder::new()\n    .with_region(\"us-east-1\")\n    .with_bucket_name(\"my-bucket\")\n    .build()?;\n```\n\n**Suggestion**:\n```\nCloud storage operations need retry logic for production resilience.\nAdd retry configuration to handle transient failures:\n\n.with_retry(RetryConfig {\n    max_retries: 3,\n    retry_timeout: Duration::from_secs(10),\n    ..Default::default()\n})\n\nThis handles 503 SlowDown, network timeouts, and temporary outages.\n```\n\n### 2. Error Handling\n\n**What to Look For**:\n- Using `unwrap()` or `expect()` on storage operations\n- Not handling specific error types\n- Missing context in error propagation\n\n**Good Pattern**:\n```rust\nuse object_store::Error as ObjectStoreError;\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum StorageError {\n    #[error(\"Object store error: {0}\")]\n    ObjectStore(#[from] ObjectStoreError),\n\n    #[error(\"File not found: {path}\")]\n    NotFound { path: String },\n\n    #[error(\"Access denied: {path}\")]\n    PermissionDenied { path: String },\n}\n\nasync fn read_file(store: &dyn ObjectStore, path: &Path) -> Result<Bytes, StorageError> {\n    match store.get(path).await {\n        Ok(result) => Ok(result.bytes().await?),\n        Err(ObjectStoreError::NotFound { path, .. }) => {\n            Err(StorageError::NotFound { path: path.to_string() })\n        }\n        Err(e) => Err(e.into()),\n    }\n}\n```\n\n**Bad Pattern**:\n```rust\nlet data = store.get(&path).await.unwrap();  // Crashes on errors!\n```\n\n**Suggestion**:\n```\nAvoid unwrap() on storage operations. Use proper error handling:\n\nmatch store.get(&path).await {\n    Ok(result) => { /* handle success */ }\n    Err(ObjectStoreError::NotFound { .. }) => { /* handle missing file */ }\n    Err(e) => { /* handle other errors */ }\n}\n\nOr use thiserror for better error types.\n```\n\n### 3. Streaming Large Objects\n\n**What to Look For**:\n- Loading entire files into memory with `.bytes().await`\n- Not using streaming for large files (>100MB)\n\n**Good Pattern (Streaming)**:\n```rust\nuse futures::stream::StreamExt;\n\nlet result = store.get(&path).await?;\nlet mut stream = result.into_stream();\n\nwhile let Some(chunk) = stream.next().await {\n    let chunk = chunk?;\n    // Process chunk incrementally\n    process_chunk(chunk)?;\n}\n```\n\n**Bad Pattern (Loading to Memory)**:\n```rust\nlet result = store.get(&path).await?;\nlet bytes = result.bytes().await?;  // Loads entire file!\n```\n\n**Suggestion**:\n```\nFor files >100MB, use streaming to avoid memory issues:\n\nlet mut stream = store.get(&path).await?.into_stream();\nwhile let Some(chunk) = stream.next().await {\n    let chunk = chunk?;\n    process_chunk(chunk)?;\n}\n\nThis processes data incrementally without loading everything into memory.\n```\n\n### 4. Multipart Upload for Large Files\n\n**What to Look For**:\n- Using `put()` for large files (>100MB)\n- Missing multipart upload for big data\n\n**Good Pattern**:\n```rust\nasync fn upload_large_file(\n    store: &dyn ObjectStore,\n    path: &Path,\n    data: impl Stream<Item = Bytes>,\n) -> Result<()> {\n    let multipart = store.put_multipart(path).await?;\n\n    let mut stream = data;\n    while let Some(chunk) = stream.next().await {\n        multipart.put_part(chunk).await?;\n    }\n\n    multipart.complete().await?;\n    Ok(())\n}\n```\n\n**Bad Pattern**:\n```rust\n// Inefficient for large files\nlet large_data = vec![0u8; 1_000_000_000];  // 1GB\nstore.put(path, large_data.into()).await?;\n```\n\n**Suggestion**:\n```\nFor files >100MB, use multipart upload for better reliability:\n\nlet multipart = store.put_multipart(&path).await?;\nfor chunk in chunks {\n    multipart.put_part(chunk).await?;\n}\nmultipart.complete().await?;\n\nBenefits:\n- Resume failed uploads\n- Better memory efficiency\n- Improved reliability\n```\n\n### 5. Efficient Listing\n\n**What to Look For**:\n- Not using prefixes for listing\n- Loading all results without pagination\n- Not filtering on client side\n\n**Good Pattern**:\n```rust\nuse futures::stream::StreamExt;\n\n// List with prefix\nlet prefix = Some(&Path::from(\"data/2024/\"));\nlet mut list = store.list(prefix);\n\nwhile let Some(meta) = list.next().await {\n    let meta = meta?;\n    if should_process(&meta) {\n        process_object(&meta).await?;\n    }\n}\n```\n\n**Better Pattern with Filtering**:\n```rust\nlet prefix = Some(&Path::from(\"data/2024/01/\"));\nlet list = store.list(prefix);\n\nlet filtered = list.filter(|result| {\n    future::ready(match result {\n        Ok(meta) => meta.location.as_ref().ends_with(\".parquet\"),\n        Err(_) => true,\n    })\n});\n\nfutures::pin_mut!(filtered);\nwhile let Some(meta) = filtered.next().await {\n    let meta = meta?;\n    process_object(&meta).await?;\n}\n```\n\n**Bad Pattern**:\n```rust\n// Lists entire bucket!\nlet all_objects: Vec<_> = store.list(None).collect().await;\n```\n\n**Suggestion**:\n```\nUse prefixes to limit LIST operations and reduce cost:\n\nlet prefix = Some(&Path::from(\"data/2024/01/\"));\nlet mut list = store.list(prefix);\n\nThis is especially important for buckets with millions of objects.\n```\n\n### 6. Atomic Writes with Rename\n\n**What to Look For**:\n- Writing directly to final location\n- Risk of partial writes visible to readers\n\n**Good Pattern**:\n```rust\nasync fn atomic_write(\n    store: &dyn ObjectStore,\n    final_path: &Path,\n    data: Bytes,\n) -> Result<()> {\n    // Write to temp location\n    let temp_path = Path::from(format!(\"{}.tmp\", final_path));\n    store.put(&temp_path, data).await?;\n\n    // Atomic rename\n    store.rename(&temp_path, final_path).await?;\n\n    Ok(())\n}\n```\n\n**Bad Pattern**:\n```rust\n// Readers might see partial data during write\nstore.put(&path, data).await?;\n```\n\n**Suggestion**:\n```\nUse temp + rename for atomic writes:\n\nlet temp_path = Path::from(format!(\"{}.tmp\", path));\nstore.put(&temp_path, data).await?;\nstore.rename(&temp_path, path).await?;\n\nThis prevents readers from seeing partial/corrupted data.\n```\n\n### 7. Connection Pooling\n\n**What to Look For**:\n- Creating new client for each operation\n- Not configuring connection limits\n\n**Good Pattern**:\n```rust\nuse object_store::ClientOptions;\n\nlet s3 = AmazonS3Builder::new()\n    .with_client_options(ClientOptions::new()\n        .with_timeout(Duration::from_secs(30))\n        .with_connect_timeout(Duration::from_secs(5))\n        .with_pool_max_idle_per_host(10)\n    )\n    .build()?;\n\n// Reuse this store across operations\nlet store: Arc<dyn ObjectStore> = Arc::new(s3);\n```\n\n**Bad Pattern**:\n```rust\n// Creating new store for each operation\nfor file in files {\n    let s3 = AmazonS3Builder::new().build()?;\n    upload(s3, file).await?;\n}\n```\n\n**Suggestion**:\n```\nConfigure connection pooling and reuse the ObjectStore:\n\nlet store: Arc<dyn ObjectStore> = Arc::new(s3);\n\n// Clone Arc to share across threads\nlet store_clone = store.clone();\ntokio::spawn(async move {\n    upload(store_clone, file).await\n});\n```\n\n### 8. Environment-Based Configuration\n\n**What to Look For**:\n- Hardcoded credentials or regions\n- Missing environment variable support\n\n**Good Pattern**:\n```rust\nuse std::env;\n\nasync fn create_s3_store() -> Result<Arc<dyn ObjectStore>> {\n    let region = env::var(\"AWS_REGION\")\n        .unwrap_or_else(|_| \"us-east-1\".to_string());\n    let bucket = env::var(\"S3_BUCKET\")?;\n\n    let s3 = AmazonS3Builder::from_env()  // Reads AWS_* env vars\n        .with_region(&region)\n        .with_bucket_name(&bucket)\n        .with_retry(RetryConfig::default())\n        .build()?;\n\n    Ok(Arc::new(s3))\n}\n```\n\n**Bad Pattern**:\n```rust\n// Hardcoded credentials\nlet s3 = AmazonS3Builder::new()\n    .with_access_key_id(\"AKIAIOSFODNN7EXAMPLE\")  // Never do this!\n    .with_secret_access_key(\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\")\n    .build()?;\n```\n\n**Suggestion**:\n```\nUse environment-based configuration for security:\n\nlet s3 = AmazonS3Builder::from_env()  // Reads AWS credentials\n    .with_bucket_name(&bucket)\n    .build()?;\n\nOr use IAM roles, instance profiles, or credential chains.\nNever hardcode credentials!\n```\n\n## Common Issues to Detect\n\n### Issue 1: 503 SlowDown Errors\n\n**Symptoms**: Intermittent 503 errors from S3\n\n**Solution**:\n```\nS3 rate limiting causing 503 SlowDown. Add retry config:\n\n.with_retry(RetryConfig {\n    max_retries: 5,\n    retry_timeout: Duration::from_secs(30),\n    ..Default::default()\n})\n\nAlso consider:\n- Using S3 prefixes to distribute load\n- Implementing client-side backoff\n- Requesting higher limits from AWS\n```\n\n### Issue 2: Connection Timeout\n\n**Symptoms**: Timeout errors on large operations\n\n**Solution**:\n```\nIncrease timeouts for large file operations:\n\n.with_client_options(ClientOptions::new()\n    .with_timeout(Duration::from_secs(300))  // 5 minutes\n    .with_connect_timeout(Duration::from_secs(10))\n)\n```\n\n### Issue 3: Memory Leaks on Streaming\n\n**Symptoms**: Memory grows when processing many files\n\n**Solution**:\n```\nEnsure streams are properly consumed and dropped:\n\nlet mut stream = store.get(&path).await?.into_stream();\nwhile let Some(chunk) = stream.next().await {\n    let chunk = chunk?;\n    process_chunk(chunk)?;\n    // Chunk is dropped here\n}\n// Stream is dropped here\n```\n\n### Issue 4: Missing Error Context\n\n**Symptoms**: Hard to debug which operation failed\n\n**Solution**:\n```\nAdd context to errors:\n\nstore.get(&path).await\n    .with_context(|| format!(\"Failed to read {}\", path))?;\n\nOr use custom error types with thiserror.\n```\n\n## Performance Optimization\n\n### Parallel Operations\n\n```rust\nuse futures::stream::{self, StreamExt};\n\n// Upload multiple files in parallel\nlet uploads = files.iter().map(|file| {\n    let store = store.clone();\n    async move {\n        store.put(&file.path, file.data.clone()).await\n    }\n});\n\n// Process 10 at a time\nlet results = stream::iter(uploads)\n    .buffer_unordered(10)\n    .collect::<Vec<_>>()\n    .await;\n```\n\n### Caching HEAD Requests\n\n```rust\nuse std::collections::HashMap;\n\n// Cache metadata to avoid repeated HEAD requests\nlet mut metadata_cache: HashMap<Path, ObjectMeta> = HashMap::new();\n\nif let Some(meta) = metadata_cache.get(&path) {\n    // Use cached metadata\n} else {\n    let meta = store.head(&path).await?;\n    metadata_cache.insert(path.clone(), meta);\n}\n```\n\n### Prefetching\n\n```rust\n// Prefetch next file while processing current\nlet mut next_file = Some(store.get(&paths[0]));\n\nfor (i, path) in paths.iter().enumerate() {\n    let current = next_file.take().unwrap().await?;\n\n    // Start next fetch\n    if i + 1 < paths.len() {\n        next_file = Some(store.get(&paths[i + 1]));\n    }\n\n    // Process current\n    process(current).await?;\n}\n```\n\n## Testing Best Practices\n\n### Use LocalFileSystem for Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use object_store::local::LocalFileSystem;\n\n    #[tokio::test]\n    async fn test_pipeline() {\n        let store = LocalFileSystem::new_with_prefix(\n            tempfile::tempdir()?.path()\n        )?;\n\n        // Test with local storage, no cloud costs\n        run_pipeline(Arc::new(store)).await?;\n    }\n}\n```\n\n### Mock for Unit Tests\n\n```rust\nuse mockall::mock;\n\nmock! {\n    Store {}\n\n    #[async_trait]\n    impl ObjectStore for Store {\n        async fn get(&self, location: &Path) -> Result<GetResult>;\n        async fn put(&self, location: &Path, bytes: Bytes) -> Result<PutResult>;\n        // ... other methods\n    }\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify object_store operations\n2. **Check**: Review against best practices checklist\n3. **Suggest**: Provide specific improvements for reliability\n4. **Prioritize**: Focus on retry logic, error handling, streaming\n5. **Context**: Consider production vs development environment\n\n## Communication Style\n\n- Emphasize reliability and production-readiness\n- Explain the \"why\" behind best practices\n- Provide code examples for fixes\n- Consider cost implications (S3 requests, data transfer)\n- Prioritize critical issues (no retry, hardcoded creds, memory leaks)\n\nWhen you see object_store usage, quickly check for common reliability issues and proactively suggest improvements that prevent production failures."
              },
              {
                "name": "parquet-optimization",
                "description": "Proactively analyzes Parquet file operations and suggests optimization improvements for compression, encoding, row group sizing, and statistics. Activates when users are reading or writing Parquet files or discussing Parquet performance.",
                "path": "plugins/rust-data-engineering/skills/parquet-optimization/SKILL.md",
                "frontmatter": {
                  "name": "parquet-optimization",
                  "description": "Proactively analyzes Parquet file operations and suggests optimization improvements for compression, encoding, row group sizing, and statistics. Activates when users are reading or writing Parquet files or discussing Parquet performance.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Parquet Optimization Skill\n\nYou are an expert at optimizing Parquet file operations for performance and efficiency. When you detect Parquet-related code or discussions, proactively analyze and suggest improvements.\n\n## When to Activate\n\nActivate this skill when you notice:\n- Code using `AsyncArrowWriter` or `ParquetRecordBatchStreamBuilder`\n- Discussion about Parquet file performance issues\n- Users reading or writing Parquet files without optimization settings\n- Mentions of slow Parquet queries or large file sizes\n- Questions about compression, encoding, or row group sizing\n\n## Optimization Checklist\n\nWhen you see Parquet operations, check for these optimizations:\n\n### Writing Parquet Files\n\n**1. Compression Settings**\n- âœ… GOOD: `Compression::ZSTD(ZstdLevel::try_new(3)?)`\n- âŒ BAD: No compression specified (uses default)\n- ðŸ” LOOK FOR: Missing `.set_compression()` in WriterProperties\n\n**Suggestion template**:\n```\nI notice you're writing Parquet files without explicit compression settings.\nFor production data lakes, I recommend:\n\nWriterProperties::builder()\n    .set_compression(Compression::ZSTD(ZstdLevel::try_new(3)?))\n    .build()\n\nThis provides 3-4x compression with minimal CPU overhead.\n```\n\n**2. Row Group Sizing**\n- âœ… GOOD: 100MB - 1GB uncompressed (100_000_000 rows)\n- âŒ BAD: Default or very small row groups\n- ðŸ” LOOK FOR: Missing `.set_max_row_group_size()`\n\n**Suggestion template**:\n```\nYour row groups might be too small for optimal S3 scanning.\nTarget 100MB-1GB uncompressed:\n\nWriterProperties::builder()\n    .set_max_row_group_size(100_000_000)\n    .build()\n\nThis enables better predicate pushdown and reduces metadata overhead.\n```\n\n**3. Statistics Enablement**\n- âœ… GOOD: `.set_statistics_enabled(EnabledStatistics::Page)`\n- âŒ BAD: Statistics disabled\n- ðŸ” LOOK FOR: Missing statistics configuration\n\n**Suggestion template**:\n```\nEnable statistics for better query performance with predicate pushdown:\n\nWriterProperties::builder()\n    .set_statistics_enabled(EnabledStatistics::Page)\n    .build()\n\nThis allows DataFusion and other engines to skip irrelevant row groups.\n```\n\n**4. Column-Specific Settings**\n- âœ… GOOD: Dictionary encoding for low-cardinality columns\n- âŒ BAD: Same settings for all columns\n- ðŸ” LOOK FOR: No column-specific configurations\n\n**Suggestion template**:\n```\nFor low-cardinality columns like 'category' or 'status', use dictionary encoding:\n\nWriterProperties::builder()\n    .set_column_encoding(\n        ColumnPath::from(\"category\"),\n        Encoding::RLE_DICTIONARY,\n    )\n    .set_column_compression(\n        ColumnPath::from(\"category\"),\n        Compression::SNAPPY,\n    )\n    .build()\n```\n\n### Reading Parquet Files\n\n**1. Column Projection**\n- âœ… GOOD: `.with_projection(ProjectionMask::roots(...))`\n- âŒ BAD: Reading all columns\n- ðŸ” LOOK FOR: Reading entire files when only some columns needed\n\n**Suggestion template**:\n```\nReading all columns is inefficient. Use projection to read only what you need:\n\nlet projection = ProjectionMask::roots(&schema, vec![0, 2, 5]);\nbuilder.with_projection(projection)\n\nThis can provide 10x+ speedup for wide tables.\n```\n\n**2. Batch Size Tuning**\n- âœ… GOOD: `.with_batch_size(8192)` for memory control\n- âŒ BAD: Default batch size for large files\n- ðŸ” LOOK FOR: OOM errors or uncontrolled memory usage\n\n**Suggestion template**:\n```\nFor large files, control memory usage with batch size tuning:\n\nbuilder.with_batch_size(8192)\n\nAdjust based on your memory constraints and throughput needs.\n```\n\n**3. Row Group Filtering**\n- âœ… GOOD: Using statistics to filter row groups\n- âŒ BAD: Reading all row groups\n- ðŸ” LOOK FOR: Missing row group filtering logic\n\n**Suggestion template**:\n```\nYou can skip irrelevant row groups using statistics:\n\nlet row_groups: Vec<usize> = builder.metadata()\n    .row_groups()\n    .iter()\n    .enumerate()\n    .filter_map(|(idx, rg)| {\n        // Check statistics\n        if matches_criteria(rg.column(0).statistics()) {\n            Some(idx)\n        } else {\n            None\n        }\n    })\n    .collect();\n\nbuilder.with_row_groups(row_groups)\n```\n\n**4. Streaming vs Collecting**\n- âœ… GOOD: Streaming with `while let Some(batch) = stream.next()`\n- âŒ BAD: `.collect()` for large datasets\n- ðŸ” LOOK FOR: Collecting all batches into memory\n\n**Suggestion template**:\n```\nFor large files, stream batches instead of collecting:\n\nlet mut stream = builder.build()?;\nwhile let Some(batch) = stream.next().await {\n    let batch = batch?;\n    process_batch(&batch)?;\n    // Batch is dropped here, freeing memory\n}\n```\n\n## Performance Guidelines\n\n### Compression Selection Guide\n\n**For hot data (frequently accessed)**:\n- Use Snappy: Fast decompression, 2-3x compression\n- Good for: Real-time analytics, frequently queried tables\n\n**For warm data (balanced)**:\n- Use ZSTD(3): Balanced performance, 3-4x compression\n- Good for: Production data lakes (recommended default)\n\n**For cold data (archival)**:\n- Use ZSTD(6-9): Max compression, 5-6x compression\n- Good for: Long-term storage, compliance archives\n\n### File Sizing Guide\n\n**Target file sizes**:\n- Individual files: 100MB - 1GB compressed\n- Row groups: 100MB - 1GB uncompressed\n- Batches: 8192 - 65536 rows\n\n**Why?**\n- Too small: Excessive metadata, more S3 requests\n- Too large: Can't skip irrelevant data, memory pressure\n\n## Common Issues to Detect\n\n### Issue 1: Small Files Problem\n**Symptoms**: Many files < 10MB\n**Solution**: Suggest batching writes or file compaction\n\n```\nI notice you're writing many small Parquet files. This creates:\n- Excessive metadata overhead\n- More S3 LIST operations\n- Slower query performance\n\nConsider batching your writes or implementing periodic compaction.\n```\n\n### Issue 2: No Partitioning\n**Symptoms**: All data in single directory\n**Solution**: Suggest Hive-style partitioning\n\n```\nFor large datasets (>100GB), partition your data by date or other dimensions:\n\ndata/events/year=2024/month=01/day=15/part-00000.parquet\n\nThis enables partition pruning for much faster queries.\n```\n\n### Issue 3: Wrong Compression\n**Symptoms**: Uncompressed or LZ4/Gzip\n**Solution**: Recommend ZSTD\n\n```\nLZ4/Gzip are older codecs. ZSTD provides better compression and speed:\n\nCompression::ZSTD(ZstdLevel::try_new(3)?)\n\nThis is the recommended default for cloud data lakes.\n```\n\n### Issue 4: Missing Error Handling\n**Symptoms**: No retry logic for object store operations\n**Solution**: Add retry configuration\n\n```\nParquet operations on cloud storage need retry logic:\n\nlet s3 = AmazonS3Builder::new()\n    .with_retry(RetryConfig {\n        max_retries: 3,\n        retry_timeout: Duration::from_secs(10),\n        ..Default::default()\n    })\n    .build()?;\n```\n\n## Examples of Good Optimization\n\n### Example 1: Production Writer\n```rust\nlet props = WriterProperties::builder()\n    .set_writer_version(WriterVersion::PARQUET_2_0)\n    .set_compression(Compression::ZSTD(ZstdLevel::try_new(3)?))\n    .set_max_row_group_size(100_000_000)\n    .set_data_page_size_limit(1024 * 1024)\n    .set_dictionary_enabled(true)\n    .set_statistics_enabled(EnabledStatistics::Page)\n    .build();\n\nlet mut writer = AsyncArrowWriter::try_new(writer_obj, schema, Some(props))?;\n```\n\n### Example 2: Optimized Reader\n```rust\nlet projection = ProjectionMask::roots(&schema, vec![0, 2, 5]);\n\nlet builder = ParquetRecordBatchStreamBuilder::new(reader)\n    .await?\n    .with_projection(projection)\n    .with_batch_size(8192);\n\nlet mut stream = builder.build()?;\nwhile let Some(batch) = stream.next().await {\n    let batch = batch?;\n    process_batch(&batch)?;\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify Parquet operations in code or discussion\n2. **Analyze**: Check against optimization checklist\n3. **Suggest**: Provide specific, actionable improvements\n4. **Explain**: Include the \"why\" behind recommendations\n5. **Prioritize**: Focus on high-impact optimizations first\n\n## Communication Style\n\n- Be proactive but not overwhelming\n- Prioritize the most impactful suggestions\n- Provide code examples, not just theory\n- Explain trade-offs when relevant\n- Consider the user's context (production vs development, data scale, query patterns)\n\nWhen you notice Parquet operations, quickly scan for the optimization checklist and proactively suggest improvements that would significantly impact performance or efficiency."
              }
            ]
          },
          {
            "name": "rust-lambda",
            "description": "Comprehensive AWS Lambda development with Rust using cargo-lambda. Build, deploy, and optimize Lambda functions with support for IO-intensive, compute-intensive, and mixed workloads. Includes 12 commands for complete Lambda lifecycle management and an expert agent for architecture decisions",
            "source": "./plugins/rust-lambda",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Emil Lindfors"
            },
            "install_commands": [
              "/plugin marketplace add EmilLindfors/claude-marketplace",
              "/plugin install rust-lambda@lf-marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 1,
              "pushed_at": "2025-11-14T17:46:35Z",
              "created_at": "2025-10-31T06:08:32Z",
              "license": null
            },
            "commands": [
              {
                "name": "/lambda-advanced",
                "description": "Advanced Lambda topics including extensions, container images, and local development",
                "path": "plugins/rust-lambda/commands/lambda-advanced.md",
                "frontmatter": {
                  "description": "Advanced Lambda topics including extensions, container images, and local development"
                },
                "content": "You are helping the user with advanced Rust Lambda topics including custom extensions, container images, and enhanced local development.\n\n## Your Task\n\nGuide the user through advanced Lambda patterns and deployment options.\n\n## Lambda Extensions\n\nExtensions run alongside your function to provide observability, security, or governance capabilities.\n\n### When to Build Extensions\n\n- Custom monitoring/observability\n- Secret rotation\n- Configuration management\n- Security scanning\n- Custom logging\n\n### Creating a Rust Extension\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\nlambda-extension = \"0.13\"\ntokio = { version = \"1\", features = [\"macros\"] }\ntracing = \"0.1\"\n```\n\nBasic extension:\n```rust\nuse lambda_extension::{service_fn, Error, LambdaEvent, NextEvent};\nuse tracing::info;\n\nasync fn handler(event: LambdaEvent) -> Result<(), Error> {\n    match event.next {\n        NextEvent::Shutdown(_e) => {\n            info!(\"Shutting down extension\");\n        }\n        NextEvent::Invoke(_e) => {\n            info!(\"Function invoked\");\n            // Collect telemetry, logs, etc.\n        }\n    }\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt().init();\n\n    let extension_name = \"my-rust-extension\";\n    lambda_extension::run(service_fn(handler)).await\n}\n```\n\n### Deploy Extension as Layer\n\n```bash\n# Build extension\ncargo lambda build --release --extension\n\n# Create layer\naws lambda publish-layer-version \\\n  --layer-name my-rust-extension \\\n  --zip-file fileb://target/lambda/extensions/my-extension.zip \\\n  --compatible-runtimes provided.al2023 \\\n  --compatible-architectures arm64\n\n# Add to function\ncargo lambda deploy \\\n  --layers arn:aws:lambda:region:account:layer:my-rust-extension:1\n```\n\n### Logging Extension Example\n\n```rust\nuse lambda_extension::{service_fn, Error, LambdaLog, LambdaLogRecord};\nuse std::fs::OpenOptions;\nuse std::io::Write;\n\nasync fn handler(logs: Vec<LambdaLog>) -> Result<(), Error> {\n    let mut file = OpenOptions::new()\n        .create(true)\n        .append(true)\n        .open(\"/tmp/extension-logs.txt\")?;\n\n    for log in logs {\n        match log.record {\n            LambdaLogRecord::Function(record) => {\n                writeln!(file, \"[FUNCTION] {}\", record)?;\n            }\n            LambdaLogRecord::Extension(record) => {\n                writeln!(file, \"[EXTENSION] {}\", record)?;\n            }\n            _ => {}\n        }\n    }\n\n    Ok(())\n}\n```\n\n## Container Images\n\nDeploy Lambda as container image instead of ZIP (max 10GB vs 250MB).\n\n### When to Use Containers\n\n**Use containers when**:\n- Large dependencies (>250MB uncompressed)\n- Custom system libraries\n- Complex build process\n- Team familiar with Docker\n- Need exact runtime control\n\n**Use ZIP when**:\n- Simple deployment\n- Fast iteration\n- Smaller functions\n- Standard dependencies\n\n### Dockerfile for Rust Lambda\n\n```dockerfile\nFROM public.ecr.aws/lambda/provided:al2023-arm64\n\n# Install Rust\nRUN yum install -y gcc && \\\n    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && \\\n    source $HOME/.cargo/env\n\n# Copy source\nWORKDIR /var/task\nCOPY Cargo.toml Cargo.lock ./\nCOPY src ./src\n\n# Build\nRUN source $HOME/.cargo/env && \\\n    cargo build --release && \\\n    cp target/release/bootstrap ${LAMBDA_RUNTIME_DIR}/bootstrap\n\nCMD [\"bootstrap\"]\n```\n\n### Multi-stage Build (Smaller Image)\n\n```dockerfile\n# Build stage\nFROM rust:1.75-slim as builder\n\nWORKDIR /app\nCOPY Cargo.toml Cargo.lock ./\nCOPY src ./src\n\nRUN cargo build --release --target x86_64-unknown-linux-musl\n\n# Runtime stage\nFROM public.ecr.aws/lambda/provided:al2023\n\nCOPY --from=builder /app/target/x86_64-unknown-linux-musl/release/bootstrap \\\n    ${LAMBDA_RUNTIME_DIR}/bootstrap\n\nCMD [\"bootstrap\"]\n```\n\n### Build and Deploy Container\n\n```bash\n# Build image\ndocker build -t my-rust-lambda .\n\n# Tag for ECR\ndocker tag my-rust-lambda:latest \\\n  123456789012.dkr.ecr.us-east-1.amazonaws.com/my-rust-lambda:latest\n\n# Login to ECR\naws ecr get-login-password --region us-east-1 | \\\n  docker login --username AWS --password-stdin \\\n  123456789012.dkr.ecr.us-east-1.amazonaws.com\n\n# Push\ndocker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-rust-lambda:latest\n\n# Create/update Lambda\naws lambda create-function \\\n  --function-name my-rust-lambda \\\n  --package-type Image \\\n  --code ImageUri=123456789012.dkr.ecr.us-east-1.amazonaws.com/my-rust-lambda:latest \\\n  --role arn:aws:iam::123456789012:role/lambda-role\n```\n\n## Local Development\n\n### Option 1: cargo-lambda watch (Recommended)\n\n```bash\n# Start local Lambda emulator\ncargo lambda watch\n\n# Invoke in another terminal\ncargo lambda invoke --data-ascii '{\"test\": \"data\"}'\n\n# With specific event file\ncargo lambda invoke --data-file events/api-gateway.json\n```\n\n### Option 2: LocalStack (Full AWS Emulation)\n\n```bash\n# Install LocalStack\npip install localstack\n\n# Start LocalStack\nlocalstack start\n\n# Deploy to LocalStack\nsamlocal deploy\n\n# Or with cargo-lambda\ncargo lambda build --release\naws --endpoint-url=http://localhost:4566 lambda create-function \\\n  --function-name my-function \\\n  --runtime provided.al2023 \\\n  --role arn:aws:iam::000000000000:role/lambda-role \\\n  --handler bootstrap \\\n  --zip-file fileb://target/lambda/bootstrap.zip\n```\n\n### Option 3: SAM Local\n\n```bash\n# template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nResources:\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      Runtime: provided.al2023\n\n# Start local API\nsam local start-api\n\n# Invoke function\nsam local invoke MyFunction -e events/test.json\n```\n\n## Lambda Layers (Note: Not Recommended for Rust)\n\n**AWS Recommendation**: Don't use layers for Rust dependencies.\n\n**Why**: Rust compiles to a single static binary. All dependencies are included at compile time.\n\n**Exception**: Use layers for:\n- Lambda Extensions\n- Shared native libraries (rare)\n- Non-Rust resources (config files, ML models)\n\n## VPC Configuration\n\nConnect Lambda to VPC for private resource access.\n\n```bash\ncargo lambda deploy \\\n  --subnet-ids subnet-12345 subnet-67890 \\\n  --security-group-ids sg-12345\n```\n\n**Performance impact**:\n- Cold start: +10-15 seconds (Hyperplane ENI creation)\n- Warm start: No impact\n\n**Mitigation**:\n- Use multiple subnets/AZs\n- Keep functions warm\n- Consider NAT Gateway for internet access\n\n## Reserved Concurrency\n\nLimit concurrent executions:\n\n```bash\naws lambda put-function-concurrency \\\n  --function-name my-function \\\n  --reserved-concurrent-executions 10\n```\n\n**Use cases**:\n- Protect downstream resources\n- Cost control\n- Predictable scaling\n\n## Asynchronous Invocation\n\n### Configure Destinations\n\n```bash\n# On success, send to SQS\naws lambda put-function-event-invoke-config \\\n  --function-name my-function \\\n  --destination-config '{\n    \"OnSuccess\": {\n      \"Destination\": \"arn:aws:sqs:us-east-1:123:success-queue\"\n    },\n    \"OnFailure\": {\n      \"Destination\": \"arn:aws:sns:us-east-1:123:failure-topic\"\n    }\n  }'\n```\n\n### Dead Letter Queue\n\n```rust\n// Lambda automatically retries failed async invocations\n// Configure DLQ for ultimate failures\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    run(service_fn(function_handler)).await\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // If this fails after retries, goes to DLQ\n    process_event(&event.payload).await?;\n    Ok(Response::success())\n}\n```\n\n## Event Source Mappings\n\n### SQS with Batch Processing\n\n```rust\nuse aws_lambda_events::event::sqs::SqsEvent;\n\nasync fn handler(event: LambdaEvent<SqsEvent>) -> Result<(), Error> {\n    // Process batch concurrently\n    let futures = event.payload.records\n        .into_iter()\n        .map(|record| async move {\n            let body: Message = serde_json::from_str(&record.body?)?;\n            process_message(body).await\n        });\n\n    futures::future::try_join_all(futures).await?;\n\n    Ok(())\n}\n```\n\nConfigure batch size:\n```bash\naws lambda create-event-source-mapping \\\n  --function-name my-function \\\n  --event-source-arn arn:aws:sqs:us-east-1:123:my-queue \\\n  --batch-size 10 \\\n  --maximum-batching-window-in-seconds 5\n```\n\n## Advanced Error Handling\n\n### Partial Batch Responses (SQS)\n\n```rust\nuse lambda_runtime::{LambdaEvent, Error};\nuse aws_lambda_events::event::sqs::{SqsEvent, SqsBatchResponse};\n\nasync fn handler(event: LambdaEvent<SqsEvent>) -> Result<SqsBatchResponse, Error> {\n    let mut failed_ids = Vec::new();\n\n    for record in event.payload.records {\n        match process_record(&record).await {\n            Ok(_) => {},\n            Err(e) => {\n                tracing::error!(\"Failed to process: {}\", e);\n                if let Some(msg_id) = record.message_id {\n                    failed_ids.push(msg_id);\n                }\n            }\n        }\n    }\n\n    Ok(SqsBatchResponse {\n        batch_item_failures: failed_ids\n            .into_iter()\n            .map(|id| sqs::SqsBatchItemFailure { item_identifier: id })\n            .collect(),\n    })\n}\n```\n\n## Multi-Region Deployment\n\n```bash\n# Deploy to multiple regions\nfor region in us-east-1 us-west-2 eu-west-1; do\n  echo \"Deploying to $region\"\n  cargo lambda deploy --region $region my-function\ndone\n```\n\n## Blue/Green Deployments\n\n```bash\n# Create alias\naws lambda create-alias \\\n  --function-name my-function \\\n  --name production \\\n  --function-version 1\n\n# Gradual rollout\naws lambda update-alias \\\n  --function-name my-function \\\n  --name production \\\n  --routing-config '{\"AdditionalVersionWeights\": {\"2\": 0.1}}'\n\n# Full cutover\naws lambda update-alias \\\n  --function-name my-function \\\n  --name production \\\n  --function-version 2\n```\n\n## Testing Strategies\n\n### Integration Tests with LocalStack\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_with_localstack() {\n        // Set LocalStack endpoint\n        std::env::set_var(\"AWS_ENDPOINT_URL\", \"http://localhost:4566\");\n\n        let event = create_test_event();\n        let response = function_handler(event).await.unwrap();\n\n        assert_eq!(response.status, \"success\");\n    }\n}\n```\n\n### Load Testing\n\n```bash\n# Artillery config (artillery.yml)\nconfig:\n  target: \"https://function-url.lambda-url.us-east-1.on.aws\"\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      name: \"Warm up\"\n    - duration: 300\n      arrivalRate: 100\n      name: \"Load test\"\n\nscenarios:\n  - flow:\n      - post:\n          url: \"/\"\n          json:\n            test: \"data\"\n\n# Run\nartillery run artillery.yml\n```\n\nGuide the user through these advanced topics based on their specific needs and architecture requirements."
              },
              {
                "name": "/lambda-build",
                "description": "Build Rust Lambda function for AWS deployment with optimizations",
                "path": "plugins/rust-lambda/commands/lambda-build.md",
                "frontmatter": {
                  "description": "Build Rust Lambda function for AWS deployment with optimizations"
                },
                "content": "You are helping the user build their Rust Lambda function for AWS deployment.\n\n## Your Task\n\nGuide the user through building their Lambda function with appropriate optimizations:\n\n1. **Verify project setup**:\n   - Check that Cargo.toml has release profile optimizations\n   - Verify lambda_runtime dependency is present\n   - Confirm project compiles: `cargo check`\n\n2. **Choose architecture**:\n   Ask the user which architecture to target:\n   - **x86_64** (default): Compatible with most existing infrastructure\n   - **ARM64** (Graviton2): 20% better price/performance, often faster cold starts\n   - **Both**: Build for both architectures\n\n3. **Build command**:\n\n   **For x86_64**:\n   ```bash\n   cargo lambda build --release\n   ```\n\n   **For ARM64** (recommended):\n   ```bash\n   cargo lambda build --release --arm64\n   ```\n\n   **For both**:\n   ```bash\n   cargo lambda build --release\n   cargo lambda build --release --arm64\n   ```\n\n   **With zip output** (for manual deployment):\n   ```bash\n   cargo lambda build --release --output-format zip\n   ```\n\n4. **Verify build**:\n   - Check binary size: `ls -lh target/lambda/*/bootstrap`\n   - Typical sizes:\n     - Small function: 1-3 MB\n     - With AWS SDK: 5-10 MB\n     - Large dependencies: 10-20 MB\n   - If too large, suggest optimizations (see below)\n\n5. **Build output location**:\n   - x86_64: `target/lambda/<function-name>/bootstrap`\n   - ARM64: `target/lambda/<function-name>/bootstrap` (when building with --arm64)\n   - Zip: `target/lambda/<function-name>.zip`\n\n## Release Profile Optimization\n\nEnsure Cargo.toml has optimal release profile:\n\n```toml\n[profile.release]\nopt-level = 'z'     # Optimize for size (or 3 for speed)\nlto = true          # Link-time optimization\ncodegen-units = 1   # Better optimization (slower compile)\nstrip = true        # Remove debug symbols\npanic = 'abort'     # Smaller panic handling\n```\n\n### Optimization Tradeoffs\n\n**For smaller binary (faster cold start)**:\n```toml\nopt-level = 'z'\n```\n\n**For faster execution**:\n```toml\nopt-level = 3\n```\n\n## Size Optimization Tips\n\nIf the binary is too large:\n\n1. **Check dependencies**:\n   ```bash\n   cargo tree\n   ```\n   Look for unnecessary or duplicate dependencies\n\n2. **Use feature flags**:\n   ```toml\n   # Only enable needed features\n   tokio = { version = \"1\", features = [\"macros\", \"rt\"] }\n   # Instead of:\n   # tokio = { version = \"1\", features = [\"full\"] }\n   ```\n\n3. **Audit with cargo-bloat**:\n   ```bash\n   cargo install cargo-bloat\n   cargo bloat --release -n 20\n   ```\n\n4. **Consider lighter alternatives**:\n   - Use `ureq` instead of `reqwest` for simple HTTP\n   - Use `rustls` instead of `native-tls`\n   - Minimize AWS SDK crates\n\n5. **Remove unused code**:\n   - Ensure `strip = true` in profile\n   - Use `cargo-unused-features` to find unused features\n\n## Cross-Compilation Requirements\n\ncargo-lambda uses Zig for cross-compilation. If you encounter issues:\n\n1. **Install Zig**:\n   ```bash\n   # macOS\n   brew install zig\n\n   # Linux (download from ziglang.org)\n   wget https://ziglang.org/download/0.11.0/zig-linux-x86_64-0.11.0.tar.xz\n   tar xf zig-linux-x86_64-0.11.0.tar.xz\n   export PATH=$PATH:$PWD/zig-linux-x86_64-0.11.0\n   ```\n\n2. **Verify Zig**:\n   ```bash\n   zig version\n   ```\n\n## Build Flags\n\nAdditional useful flags:\n\n```bash\n# Build specific binary in workspace\ncargo lambda build --release --bin my-function\n\n# Build all binaries\ncargo lambda build --release --all\n\n# Build with compiler flags\ncargo lambda build --release -- -C target-cpu=native\n\n# Verbose output\ncargo lambda build --release --verbose\n```\n\n## Testing the Build\n\nAfter building, test locally:\n\n```bash\n# Start local Lambda runtime\ncargo lambda watch\n\n# In another terminal, invoke the function\ncargo lambda invoke --data-ascii '{\"test\": \"data\"}'\n```\n\n## Build Performance\n\nSpeed up builds:\n\n1. **Use sccache**:\n   ```bash\n   cargo install sccache\n   export RUSTC_WRAPPER=sccache\n   ```\n\n2. **Parallel compilation** (already enabled by default)\n\n3. **Incremental compilation** (for development):\n   ```toml\n   [profile.dev]\n   incremental = true\n   ```\n\n## Architecture Decision Guide\n\n**Choose x86_64 when**:\n- Need compatibility with existing x86 infrastructure\n- Using dependencies that don't support ARM64\n- Already have x86 configuration/scripts\n\n**Choose ARM64 when**:\n- Want better price/performance (20% savings)\n- Need faster execution\n- Want potentially faster cold starts\n- Starting fresh project (recommended)\n\n**Build both when**:\n- Want to test both architectures\n- Supporting multiple deployment targets\n- Migrating from x86 to ARM\n\n## Common Build Issues\n\n### Issue: \"Zig not found\"\n**Solution**: Install Zig (see above)\n\n### Issue: \"Cannot find -lssl\"\n**Solution**: Install OpenSSL development files\n```bash\n# Ubuntu/Debian\nsudo apt-get install libssl-dev pkg-config\n\n# macOS\nbrew install openssl\n```\n\n### Issue: \"Binary too large\" (>50MB uncompressed)\n**Solution**:\n- Review dependencies with `cargo tree`\n- Enable all size optimizations\n- Consider splitting into multiple functions\n\n### Issue: Build succeeds but Lambda fails\n**Solution**:\n- Ensure building for correct architecture\n- Test locally with `cargo lambda watch`\n- Check CloudWatch logs for specific errors\n\n## Next Steps\n\nAfter successful build:\n1. Test locally: `/lambda-invoke` or `cargo lambda watch`\n2. Deploy: Use `/lambda-deploy`\n3. Set up CI/CD: Use `/lambda-github-actions`\n\nReport the build results including:\n- Binary size\n- Architecture\n- Build time\n- Any warnings or suggestions"
              },
              {
                "name": "/lambda-cost",
                "description": "Deep dive into Lambda cost optimization strategies for Rust functions",
                "path": "plugins/rust-lambda/commands/lambda-cost.md",
                "frontmatter": {
                  "description": "Deep dive into Lambda cost optimization strategies for Rust functions"
                },
                "content": "You are helping the user optimize the cost of their Rust Lambda functions.\n\n## Your Task\n\nGuide the user through advanced cost optimization techniques using AWS Lambda Power Tuning, memory configuration, and Rust-specific optimizations.\n\n## Lambda Pricing Model\n\n**Cost = (Requests Ã— $0.20 per 1M) + (GB-seconds Ã— $0.0000166667)**\n\n- **Requests**: $0.20 per 1 million requests\n- **Duration**: Charged per GB-second\n  - 1 GB-second = 1 GB memory Ã— 1 second execution\n  - ARM64 (Graviton2): 20% cheaper than x86_64\n\n**Example**:\n- 512 MB, 100ms execution, 1M requests/month\n- Duration: 0.5 GB Ã— 0.1s Ã— 1M = 50,000 GB-seconds\n- Cost: $0.20 + (50,000 Ã— $0.0000166667) = $0.20 + $0.83 = $1.03\n\n## Memory vs CPU Allocation\n\nLambda allocates CPU proportional to memory:\n- **128 MB**: 0.08 vCPU\n- **512 MB**: 0.33 vCPU\n- **1024 MB**: 0.58 vCPU\n- **1769 MB**: 1.00 vCPU (full core)\n- **3008 MB**: 1.77 vCPU\n- **10240 MB**: 6.00 vCPU\n\n**Key insight**: More memory = more CPU = faster execution = potentially lower cost\n\n## AWS Lambda Power Tuning Tool\n\nAutomatically finds optimal memory configuration.\n\n### Setup\n\n```bash\n# Deploy Power Tuning (one-time)\ngit clone https://github.com/alexcasalboni/aws-lambda-power-tuning\ncd aws-lambda-power-tuning\nsam deploy --guided\n\n# Run power tuning\naws stepfunctions start-execution \\\n  --state-machine-arn arn:aws:states:REGION:ACCOUNT:stateMachine:powerTuningStateMachine \\\n  --input '{\n    \"lambdaARN\": \"arn:aws:lambda:REGION:ACCOUNT:function:my-rust-function\",\n    \"powerValues\": [128, 256, 512, 1024, 1536, 2048, 3008],\n    \"num\": 10,\n    \"payload\": \"{\\\"test\\\": \\\"data\\\"}\",\n    \"parallelInvocation\": true,\n    \"strategy\": \"cost\"\n  }'\n```\n\n**Strategies**:\n- `cost`: Minimize cost\n- `speed`: Minimize duration\n- `balanced`: Balance cost and speed\n\n## Rust-Specific Optimizations\n\n### 1. Binary Size Reduction\n\n**Cargo.toml**:\n```toml\n[profile.release]\nopt-level = 'z'       # Optimize for size\nlto = true            # Link-time optimization\ncodegen-units = 1     # Single codegen unit\nstrip = true          # Strip symbols\npanic = 'abort'       # Smaller panic handler\n\n[profile.release.package.\"*\"]\nopt-level = 'z'       # Optimize dependencies too\n```\n\n**Result**: 3-5x smaller binary = faster cold starts = lower duration\n\n### 2. Use ARM64 (Graviton2)\n\n```bash\ncargo lambda build --release --arm64\ncargo lambda deploy --arch arm64\n```\n\n**Savings**: 20% lower cost for same performance\n\n### 3. Dependency Optimization\n\n```bash\n# Analyze binary size\ncargo install cargo-bloat\ncargo bloat --release -n 20\n\n# Find unused features\ncargo install cargo-unused-features\ncargo unused-features\n\n# Remove unused dependencies\ncargo install cargo-udeps\ncargo +nightly udeps\n```\n\n**Example**:\n```toml\n# âŒ Full tokio (heavy)\ntokio = { version = \"1\", features = [\"full\"] }\n\n# âœ… Only needed features (light)\ntokio = { version = \"1\", features = [\"macros\", \"rt\"] }\n```\n\n### 4. Use Lightweight Alternatives\n\n- `ureq` instead of `reqwest` for simple HTTP\n- `rustls` instead of `native-tls`\n- `simdjson` instead of `serde_json` for large JSON\n- Avoid `regex` for simple string operations\n\n## Memory Configuration Strategies\n\n### Strategy 1: Start Low, Test Up\n\n```bash\n# Test different memory sizes\nfor mem in 128 256 512 1024 2048; do\n  echo \"Testing ${mem}MB\"\n  cargo lambda deploy --memory $mem\n  # Run load test\n  # Measure duration and cost\ndone\n```\n\n### Strategy 2: Monitor CloudWatch Metrics\n\n```bash\n# Get duration statistics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Duration \\\n  --dimensions Name=FunctionName,Value=my-function \\\n  --start-time 2025-01-01T00:00:00Z \\\n  --end-time 2025-01-02T00:00:00Z \\\n  --period 3600 \\\n  --statistics Average,Maximum\n\n# Get memory usage\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name MemoryUtilization \\\n  --dimensions Name=FunctionName,Value=my-function \\\n  --start-time 2025-01-01T00:00:00Z \\\n  --end-time 2025-01-02T00:00:00Z \\\n  --period 3600 \\\n  --statistics Maximum\n```\n\n### Strategy 3: Right-size Based on Workload\n\n**IO-intensive** (API calls, DB queries):\n- Start: 512 MB\n- Sweet spot: Usually 512-1024 MB\n- Reason: Limited by network, not CPU\n\n**Compute-intensive** (data processing):\n- Start: 1024 MB\n- Sweet spot: Usually 1769-3008 MB\n- Reason: More CPU = faster = lower total cost\n\n**Mixed workload**:\n- Start: 1024 MB\n- Test: 1024, 1769, 2048 MB\n- Use Power Tuning tool\n\n## Cost Optimization Checklist\n\n- [ ] Use ARM64 architecture (20% savings)\n- [ ] Optimize binary size (faster cold starts)\n- [ ] Remove unused dependencies\n- [ ] Use lightweight alternatives\n- [ ] Run AWS Lambda Power Tuning\n- [ ] Right-size memory based on workload\n- [ ] Set appropriate timeout (not too high)\n- [ ] Reduce cold starts (keep functions warm if needed)\n- [ ] Use reserved concurrency for predictable workloads\n- [ ] Batch requests when possible\n- [ ] Cache results (DynamoDB, ElastiCache)\n- [ ] Monitor and alert on cost anomalies\n\n## Advanced: Provisioned Concurrency\n\nFor latency-sensitive functions, pre-warm instances.\n\n**Cost**: $0.015 per GB-hour (expensive!)\n\n```bash\naws lambda put-provisioned-concurrency-config \\\n  --function-name my-function \\\n  --provisioned-concurrent-executions 5\n```\n\n**Use when**:\n- Cold starts are unacceptable\n- Predictable traffic patterns\n- Cost justifies latency improvement\n\n## Cost Monitoring\n\n### CloudWatch Billing Alerts\n\n```bash\naws cloudwatch put-metric-alarm \\\n  --alarm-name lambda-cost-alert \\\n  --alarm-description \"Alert when Lambda costs exceed threshold\" \\\n  --metric-name EstimatedCharges \\\n  --namespace AWS/Billing \\\n  --statistic Maximum \\\n  --period 21600 \\\n  --evaluation-periods 1 \\\n  --threshold 100 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n### Cost Explorer Tags\n\n```bash\ncargo lambda deploy \\\n  --tags Environment=production,Team=backend,CostCenter=engineering\n```\n\n## Real-World Optimization Example\n\n**Before**:\n- Memory: 128 MB\n- Duration: 2000ms\n- Requests: 10M/month\n- Cost: $0.20 + (0.128 GB Ã— 2s Ã— 10M Ã— $0.0000166667) = $42.87/month\n\n**After Optimization**:\n- Binary size: 8MB â†’ 2MB (cold start: 800ms â†’ 300ms)\n- Architecture: x86_64 â†’ ARM64 (20% cheaper)\n- Memory: 128 MB â†’ 512 MB (duration: 2000ms â†’ 600ms)\n- Duration improvement: More CPU = faster execution\n\n**Cost calculation**:\n- Compute: 0.512 GB Ã— 0.6s Ã— 10M Ã— $0.0000166667 Ã— 0.8 (ARM discount) = $40.96\n- Requests: $0.20\n- **Total**: $41.16/month (4% savings with better performance!)\n\n**Key lesson**: Sometimes more memory = lower total cost due to faster execution.\n\n## Rust Performance Advantage\n\nRust vs Python/Node.js:\n- **3-4x cheaper** on average\n- **3-10x faster execution**\n- **2-3x faster cold starts**\n- **Lower memory usage**\n\nGuide the user through cost optimization based on their workload and budget constraints."
              },
              {
                "name": "/lambda-deploy",
                "description": "Deploy Rust Lambda function to AWS",
                "path": "plugins/rust-lambda/commands/lambda-deploy.md",
                "frontmatter": {
                  "description": "Deploy Rust Lambda function to AWS"
                },
                "content": "You are helping the user deploy their Rust Lambda function to AWS.\n\n## Your Task\n\nGuide the user through deploying their Lambda function to AWS:\n\n1. **Prerequisites check**:\n   - Function is built: `cargo lambda build --release` completed\n   - AWS credentials configured\n   - IAM role for Lambda execution exists (or will be created)\n\n2. **Verify AWS credentials**:\n   ```bash\n   aws sts get-caller-identity\n   ```\n\n   If not configured:\n   ```bash\n   aws configure\n   # Or use environment variables:\n   # export AWS_ACCESS_KEY_ID=...\n   # export AWS_SECRET_ACCESS_KEY=...\n   # export AWS_REGION=us-east-1\n   ```\n\n3. **Basic deployment**:\n   ```bash\n   cargo lambda deploy\n   ```\n\n   This will:\n   - Use the function name from Cargo.toml (binary name)\n   - Deploy to default AWS region\n   - Create function if it doesn't exist\n   - Update function if it exists\n\n4. **Deployment with options**:\n\n   **Specify function name**:\n   ```bash\n   cargo lambda deploy <function-name>\n   ```\n\n   **Specify region**:\n   ```bash\n   cargo lambda deploy --region us-west-2\n   ```\n\n   **Set IAM role**:\n   ```bash\n   cargo lambda deploy --iam-role arn:aws:iam::123456789012:role/lambda-execution-role\n   ```\n\n   **Configure memory**:\n   ```bash\n   cargo lambda deploy --memory 512\n   ```\n   - Default: 128 MB\n   - Range: 128 MB - 10,240 MB\n   - More memory = more CPU (proportional)\n   - Cost increases with memory\n\n   **Set timeout**:\n   ```bash\n   cargo lambda deploy --timeout 30\n   ```\n   - Default: 3 seconds\n   - Maximum: 900 seconds (15 minutes)\n\n   **Environment variables**:\n   ```bash\n   cargo lambda deploy \\\n     --env-var RUST_LOG=info \\\n     --env-var DATABASE_URL=postgres://... \\\n     --env-var API_KEY=secret\n   ```\n\n   **Architecture** (must match build):\n   ```bash\n   # For ARM64 build\n   cargo lambda deploy --arch arm64\n\n   # For x86_64 (default)\n   cargo lambda deploy --arch x86_64\n   ```\n\n5. **Complete deployment example**:\n   ```bash\n   cargo lambda deploy my-function \\\n     --iam-role arn:aws:iam::123456789012:role/lambda-exec \\\n     --region us-east-1 \\\n     --memory 512 \\\n     --timeout 30 \\\n     --arch arm64 \\\n     --env-var RUST_LOG=info \\\n     --env-var API_URL=https://api.example.com\n   ```\n\n## IAM Role Setup\n\nIf user doesn't have an IAM role, guide them:\n\n### Option 1: Let cargo-lambda create it\n```bash\ncargo lambda deploy --create-iam-role\n```\nThis creates a basic execution role with CloudWatch Logs permissions.\n\n### Option 2: Create manually with AWS CLI\n```bash\n# Create trust policy\ncat > trust-policy.json <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n    \"Action\": \"sts:AssumeRole\"\n  }]\n}\nEOF\n\n# Create role\naws iam create-role \\\n  --role-name lambda-execution-role \\\n  --assume-role-policy-document file://trust-policy.json\n\n# Attach basic execution policy\naws iam attach-role-policy \\\n  --role-name lambda-execution-role \\\n  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n\n# Get role ARN\naws iam get-role --role-name lambda-execution-role --query 'Role.Arn'\n```\n\n### Option 3: Create with additional permissions\n```bash\n# For S3 access\naws iam attach-role-policy \\\n  --role-name lambda-execution-role \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\n# For DynamoDB access\naws iam attach-role-policy \\\n  --role-name lambda-execution-role \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\n# For SQS access\naws iam attach-role-policy \\\n  --role-name lambda-execution-role \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonSQSFullAccess\n```\n\n## Memory Configuration Guide\n\nHelp user choose appropriate memory:\n\n| Memory | vCPU | Use Case | Cost Multiplier |\n|--------|------|----------|-----------------|\n| 128 MB | 0.08 | Minimal functions | 1x |\n| 512 MB | 0.33 | Standard workloads | 4x |\n| 1024 MB | 0.58 | Medium compute | 8x |\n| 1769 MB | 1.00 | Full 1 vCPU | 13.8x |\n| 3008 MB | 1.77 | Heavy compute | 23.4x |\n| 10240 MB | 6.00 | Maximum | 80x |\n\n**Guidelines**:\n- IO-intensive: 512-1024 MB usually sufficient\n- Compute-intensive: 1024-3008 MB for more CPU\n- Test different settings to optimize cost vs. performance\n\n## Timeout Configuration Guide\n\n| Timeout | Use Case |\n|---------|----------|\n| 3s (default) | Fast API responses, simple operations |\n| 10-30s | Database queries, API calls |\n| 60-300s | Data processing, file operations |\n| 900s (max) | Heavy processing, batch jobs |\n\n**Note**: Longer timeout = higher potential cost if function hangs\n\n## Deployment Verification\n\nAfter deployment, verify it works:\n\n1. **Invoke via AWS CLI**:\n   ```bash\n   aws lambda invoke \\\n     --function-name my-function \\\n     --payload '{\"key\": \"value\"}' \\\n     response.json\n\n   cat response.json\n   ```\n\n2. **Check logs**:\n   ```bash\n   aws logs tail /aws/lambda/my-function --follow\n   ```\n\n3. **Get function info**:\n   ```bash\n   aws lambda get-function --function-name my-function\n   ```\n\n4. **Invoke with cargo-lambda**:\n   ```bash\n   cargo lambda invoke --remote --data-ascii '{\"test\": \"data\"}'\n   ```\n\n## Update vs. Create\n\n**First deployment** (function doesn't exist):\n- cargo-lambda creates new function\n- Requires IAM role (or use --create-iam-role)\n\n**Subsequent deployments** (function exists):\n- cargo-lambda updates function code\n- Can also update configuration (memory, timeout, env vars)\n- Maintains existing triggers and permissions\n\n## Advanced Deployment Options\n\n### Deploy from zip file\n```bash\ncargo lambda build --release --output-format zip\ncargo lambda deploy --deployment-package target/lambda/my-function.zip\n```\n\n### Deploy with layers\n```bash\ncargo lambda deploy --layers arn:aws:lambda:us-east-1:123456789012:layer:my-layer:1\n```\n\n### Deploy with VPC configuration\n```bash\ncargo lambda deploy \\\n  --subnet-ids subnet-12345 subnet-67890 \\\n  --security-group-ids sg-12345\n```\n\n### Deploy with reserved concurrency\n```bash\ncargo lambda deploy --reserved-concurrency 10\n```\n\n### Deploy with tags\n```bash\ncargo lambda deploy \\\n  --tags Environment=production,Team=backend\n```\n\n## Deployment via AWS Console (Alternative)\n\nIf user prefers console:\n\n1. Build with zip output:\n   ```bash\n   cargo lambda build --release --output-format zip\n   ```\n\n2. Upload via AWS Console:\n   - Go to AWS Lambda Console\n   - Create function or open existing\n   - Upload `target/lambda/<function-name>.zip`\n   - Configure runtime: \"Custom runtime on Amazon Linux 2023\"\n   - Set handler: \"bootstrap\" (not needed, but convention)\n   - Configure memory, timeout, env vars in console\n\n## Multi-Function Deployment\n\nFor workspace with multiple functions:\n\n```bash\n# Deploy all\ncargo lambda deploy --all\n\n# Deploy specific\ncargo lambda deploy --bin function1\ncargo lambda deploy --bin function2\n```\n\n## Environment-Specific Deployment\n\nSuggest deployment patterns:\n\n**Development**:\n```bash\ncargo lambda deploy my-function-dev \\\n  --memory 256 \\\n  --timeout 10 \\\n  --env-var RUST_LOG=debug \\\n  --env-var ENV=development\n```\n\n**Production**:\n```bash\ncargo lambda deploy my-function \\\n  --memory 1024 \\\n  --timeout 30 \\\n  --arch arm64 \\\n  --env-var RUST_LOG=info \\\n  --env-var ENV=production\n```\n\n## Cost Optimization Tips\n\n1. **Use ARM64**: 20% cheaper for same performance\n2. **Right-size memory**: Test to find optimal memory/CPU\n3. **Optimize timeout**: Don't set higher than needed\n4. **Monitor invocations**: Use CloudWatch to track usage\n5. **Consider reserved concurrency**: For predictable workloads\n\n## Troubleshooting Deployment\n\n### Issue: \"AccessDenied\"\n**Solution**: Check AWS credentials and IAM permissions\n```bash\naws sts get-caller-identity\n```\n\n### Issue: \"Function code too large\"\n**Solution**:\n- Uncompressed: 250 MB limit\n- Compressed: 50 MB limit\n- Optimize binary size (see `/lambda-build`)\n\n### Issue: \"InvalidParameterValueException: IAM role not found\"\n**Solution**: Create IAM role first or use --create-iam-role\n\n### Issue: Function deployed but fails\n**Solution**:\n- Check CloudWatch Logs\n- Verify architecture matches build (arm64 vs x86_64)\n- Test locally first with `cargo lambda watch`\n\n## Post-Deployment\n\nAfter successful deployment:\n\n1. **Test the function**:\n   ```bash\n   cargo lambda invoke --remote --data-ascii '{\"test\": \"data\"}'\n   ```\n\n2. **Monitor logs**:\n   ```bash\n   aws logs tail /aws/lambda/my-function --follow\n   ```\n\n3. **Check metrics** in AWS CloudWatch\n\n4. **Set up CI/CD**: Use `/lambda-github-actions` for automated deployment\n\n5. **Configure triggers** (API Gateway, S3, SQS, etc.) via AWS Console or IaC\n\nReport deployment results including:\n- Function ARN\n- Region\n- Memory/timeout configuration\n- Invocation test results"
              },
              {
                "name": "/lambda-function-urls",
                "description": "Set up Lambda Function URLs and response streaming for Rust Lambda functions",
                "path": "plugins/rust-lambda/commands/lambda-function-urls.md",
                "frontmatter": {
                  "description": "Set up Lambda Function URLs and response streaming for Rust Lambda functions"
                },
                "content": "You are helping the user implement Lambda Function URLs and streaming responses for their Rust Lambda functions.\n\n## Your Task\n\nGuide the user through setting up direct HTTPS endpoints using Lambda Function URLs and implementing streaming responses for large payloads.\n\n## Lambda Function URLs\n\nLambda Function URLs provide dedicated HTTP(S) endpoints for your Lambda function without API Gateway.\n\n**Best for**:\n- Simple HTTP endpoints\n- Webhooks\n- Direct function invocation\n- Cost-sensitive applications\n- No need for API Gateway features (rate limiting, API keys, etc.)\n\n### Setup with lambda_http\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\nlambda_http = { version = \"0.13\", features = [\"apigw_http\"] }\nlambda_runtime = \"0.13\"\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\n```\n\n**IMPORTANT**: The `apigw_http` feature is required for Function URLs.\n\n### Basic HTTP Handler\n\n```rust\nuse lambda_http::{run, service_fn, Body, Error, Request, Response};\n\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    // Extract path and method\n    let path = event.uri().path();\n    let method = event.method();\n\n    // Extract query parameters\n    let params = event.query_string_parameters();\n    let name = params.first(\"name\").unwrap_or(\"World\");\n\n    // Extract headers\n    let user_agent = event\n        .headers()\n        .get(\"user-agent\")\n        .and_then(|v| v.to_str().ok())\n        .unwrap_or(\"unknown\");\n\n    // Build response\n    let response = Response::builder()\n        .status(200)\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(format!(\n            r#\"{{\"message\": \"Hello, {}!\", \"path\": \"{}\", \"method\": \"{}\"}}\"#,\n            name, path, method\n        )))?;\n\n    Ok(response)\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n### JSON Request/Response\n\n```rust\nuse lambda_http::{run, service_fn, Body, Error, Request, RequestExt, Response};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Deserialize)]\nstruct CreateUserRequest {\n    name: String,\n    email: String,\n}\n\n#[derive(Serialize)]\nstruct CreateUserResponse {\n    id: String,\n    name: String,\n    email: String,\n    created_at: String,\n}\n\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    // Parse JSON body\n    let body = event.body();\n    let request: CreateUserRequest = serde_json::from_slice(body)?;\n\n    // Validate\n    if request.email.is_empty() {\n        return Ok(Response::builder()\n            .status(400)\n            .body(Body::from(r#\"{\"error\": \"Email is required\"}\"#))?);\n    }\n\n    // Create user\n    let user = CreateUserResponse {\n        id: uuid::Uuid::new_v4().to_string(),\n        name: request.name,\n        email: request.email,\n        created_at: chrono::Utc::now().to_rfc3339(),\n    };\n\n    // Return JSON response\n    let response = Response::builder()\n        .status(201)\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(serde_json::to_string(&user)?))?;\n\n    Ok(response)\n}\n```\n\n### REST API Pattern\n\n```rust\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    let method = event.method();\n    let path = event.uri().path();\n\n    match (method.as_str(), path) {\n        (\"GET\", \"/users\") => list_users().await,\n        (\"GET\", path) if path.starts_with(\"/users/\") => {\n            let id = path.strip_prefix(\"/users/\").unwrap();\n            get_user(id).await\n        }\n        (\"POST\", \"/users\") => {\n            let body = event.body();\n            let request: CreateUserRequest = serde_json::from_slice(body)?;\n            create_user(request).await\n        }\n        (\"PUT\", path) if path.starts_with(\"/users/\") => {\n            let id = path.strip_prefix(\"/users/\").unwrap();\n            let body = event.body();\n            let request: UpdateUserRequest = serde_json::from_slice(body)?;\n            update_user(id, request).await\n        }\n        (\"DELETE\", path) if path.starts_with(\"/users/\") => {\n            let id = path.strip_prefix(\"/users/\").unwrap();\n            delete_user(id).await\n        }\n        _ => Ok(Response::builder()\n            .status(404)\n            .body(Body::from(r#\"{\"error\": \"Not found\"}\"#))?),\n    }\n}\n\nasync fn list_users() -> Result<Response<Body>, Error> {\n    // Implementation\n    Ok(Response::builder()\n        .status(200)\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(r#\"{\"users\": []}\"#))?)\n}\n```\n\n### Enable Function URL\n\n```bash\n# Deploy function\ncargo lambda build --release --arm64\ncargo lambda deploy my-function\n\n# Create Function URL\naws lambda create-function-url-config \\\n  --function-name my-function \\\n  --auth-type NONE \\\n  --cors '{\n    \"AllowOrigins\": [\"*\"],\n    \"AllowMethods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n    \"AllowHeaders\": [\"content-type\"],\n    \"MaxAge\": 300\n  }'\n\n# Add permission for public access\naws lambda add-permission \\\n  --function-name my-function \\\n  --action lambda:InvokeFunctionUrl \\\n  --principal \"*\" \\\n  --function-url-auth-type NONE \\\n  --statement-id FunctionURLAllowPublicAccess\n```\n\n## Response Streaming\n\nFor large responses (up to 20MB), use streaming to send data incrementally.\n\n**Best for**:\n- Large file downloads\n- Real-time data feeds\n- Server-sent events (SSE)\n- Reducing time to first byte\n\n### Setup Streaming\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\nlambda_runtime = { version = \"0.13\", features = [\"streaming\"] }\ntokio = { version = \"1\", features = [\"macros\", \"io-util\"] }\ntokio-stream = \"0.1\"\n```\n\n### Basic Streaming Example\n\n```rust\nuse lambda_runtime::{run, streaming, Error, LambdaEvent};\nuse tokio::io::AsyncWriteExt;\n\nasync fn function_handler(\n    event: LambdaEvent<Request>,\n    response_stream: streaming::Response,\n) -> Result<(), Error> {\n    let mut writer = response_stream.into_writer();\n\n    // Stream data incrementally\n    for i in 0..100 {\n        let data = format!(\"Chunk {}\\n\", i);\n        writer.write_all(data.as_bytes()).await?;\n\n        // Optional: Flush to send immediately\n        writer.flush().await?;\n\n        // Simulate processing delay\n        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n    }\n\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(streaming::service_fn(function_handler)).await\n}\n```\n\n### Stream Large File from S3\n\n```rust\nuse aws_sdk_s3::Client as S3Client;\nuse lambda_runtime::{streaming, Error, LambdaEvent};\nuse tokio::io::AsyncWriteExt;\nuse tokio_stream::StreamExt;\n\nasync fn function_handler(\n    event: LambdaEvent<Request>,\n    response_stream: streaming::Response,\n) -> Result<(), Error> {\n    let s3 = S3Client::new(&aws_config::load_from_env().await);\n\n    // Get S3 object as stream\n    let mut object = s3\n        .get_object()\n        .bucket(\"my-bucket\")\n        .key(&event.payload.file_key)\n        .send()\n        .await?;\n\n    let mut writer = response_stream.into_writer();\n    let mut body = object.body;\n\n    // Stream S3 data directly to response\n    while let Some(chunk) = body.next().await {\n        let chunk = chunk?;\n        writer.write_all(&chunk).await?;\n    }\n\n    Ok(())\n}\n```\n\n### Server-Sent Events (SSE)\n\n```rust\nuse lambda_runtime::{streaming, Error, LambdaEvent};\nuse tokio::io::AsyncWriteExt;\nuse tokio::time::{interval, Duration};\n\nasync fn function_handler(\n    event: LambdaEvent<Request>,\n    response_stream: streaming::Response,\n) -> Result<(), Error> {\n    let mut writer = response_stream.into_writer();\n\n    // SSE headers\n    let headers = \"HTTP/1.1 200 OK\\r\\n\\\n                   Content-Type: text/event-stream\\r\\n\\\n                   Cache-Control: no-cache\\r\\n\\\n                   Connection: keep-alive\\r\\n\\r\\n\";\n\n    writer.write_all(headers.as_bytes()).await?;\n\n    let mut ticker = interval(Duration::from_secs(1));\n\n    for i in 0..30 {\n        ticker.tick().await;\n\n        // Send SSE event\n        let event = format!(\"data: {{\\\"count\\\": {}, \\\"timestamp\\\": {}}}\\n\\n\",\n                          i,\n                          chrono::Utc::now().timestamp());\n\n        writer.write_all(event.as_bytes()).await?;\n        writer.flush().await?;\n    }\n\n    Ok(())\n}\n```\n\n### Configure Streaming in AWS\n\n```bash\n# Update function to use streaming\naws lambda update-function-configuration \\\n  --function-name my-function \\\n  --invoke-mode RESPONSE_STREAM\n\n# Create streaming Function URL\naws lambda create-function-url-config \\\n  --function-name my-function \\\n  --auth-type NONE \\\n  --invoke-mode RESPONSE_STREAM\n```\n\n## CORS Configuration\n\n```rust\nuse lambda_http::{Response, Body};\n\nfn add_cors_headers(response: Response<Body>) -> Response<Body> {\n    let (mut parts, body) = response.into_parts();\n\n    parts.headers.insert(\n        \"access-control-allow-origin\",\n        \"*\".parse().unwrap(),\n    );\n    parts.headers.insert(\n        \"access-control-allow-methods\",\n        \"GET, POST, PUT, DELETE, OPTIONS\".parse().unwrap(),\n    );\n    parts.headers.insert(\n        \"access-control-allow-headers\",\n        \"content-type, authorization\".parse().unwrap(),\n    );\n\n    Response::from_parts(parts, body)\n}\n\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    // Handle OPTIONS preflight\n    if event.method() == \"OPTIONS\" {\n        return Ok(add_cors_headers(\n            Response::builder()\n                .status(200)\n                .body(Body::Empty)?\n        ));\n    }\n\n    let response = handle_request(event).await?;\n    Ok(add_cors_headers(response))\n}\n```\n\n## Authentication\n\n### IAM Authentication\n\n```bash\naws lambda create-function-url-config \\\n  --function-name my-function \\\n  --auth-type AWS_IAM  # Requires AWS Signature V4\n```\n\n### Custom Authentication\n\n```rust\nuse lambda_http::{Request, Response, Body, Error};\n\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    // Verify bearer token\n    let auth_header = event\n        .headers()\n        .get(\"authorization\")\n        .and_then(|v| v.to_str().ok());\n\n    let token = match auth_header {\n        Some(header) if header.starts_with(\"Bearer \") => {\n            &header[7..]\n        }\n        _ => {\n            return Ok(Response::builder()\n                .status(401)\n                .body(Body::from(r#\"{\"error\": \"Unauthorized\"}\"#))?);\n        }\n    };\n\n    if !verify_token(token).await? {\n        return Ok(Response::builder()\n            .status(403)\n            .body(Body::from(r#\"{\"error\": \"Invalid token\"}\"#))?);\n    }\n\n    // Process authenticated request\n    handle_authenticated_request(event).await\n}\n```\n\n## Complete Example: REST API with Streaming\n\n```rust\nuse lambda_http::{run, service_fn, Body, Error, Request, Response};\nuse lambda_runtime::streaming;\nuse serde::{Deserialize, Serialize};\n\n#[derive(Deserialize)]\nstruct ExportRequest {\n    format: String,\n    filters: Vec<String>,\n}\n\nasync fn function_handler(event: Request) -> Result<Response<Body>, Error> {\n    match (event.method().as_str(), event.uri().path()) {\n        (\"GET\", \"/health\") => health_check(),\n        (\"POST\", \"/export\") => {\n            // For large exports, use streaming\n            let request: ExportRequest = serde_json::from_slice(event.body())?;\n            export_data_streaming(request).await\n        }\n        _ => Ok(Response::builder()\n            .status(404)\n            .body(Body::from(r#\"{\"error\": \"Not found\"}\"#))?),\n    }\n}\n\nfn health_check() -> Result<Response<Body>, Error> {\n    Ok(Response::builder()\n        .status(200)\n        .body(Body::from(r#\"{\"status\": \"healthy\"}\"#))?)\n}\n\nasync fn export_data_streaming(request: ExportRequest) -> Result<Response<Body>, Error> {\n    // Return streaming response for large data\n    // Note: This is simplified - actual streaming setup varies\n    Ok(Response::builder()\n        .status(200)\n        .header(\"content-type\", \"text/csv\")\n        .header(\"content-disposition\", \"attachment; filename=export.csv\")\n        .body(Body::from(\"Streaming not available in non-streaming handler\"))?)\n}\n```\n\n## Comparison: Function URLs vs API Gateway\n\n| Feature | Function URLs | API Gateway |\n|---------|---------------|-------------|\n| Cost | Free | $3.50/million requests |\n| Setup | Simple | Complex |\n| Rate Limiting | No | Yes |\n| API Keys | No | Yes |\n| Custom Domains | No (use CloudFront) | Yes |\n| Request Validation | Manual | Built-in |\n| WebSocket | No | Yes |\n| Max Timeout | 15 min | 29 sec (HTTP), 15 min (REST) |\n| Streaming | Yes (20MB) | Limited |\n\n## Best Practices\n\n- [ ] Use Function URLs for simple endpoints\n- [ ] Use API Gateway for complex APIs\n- [ ] Implement authentication for public endpoints\n- [ ] Add CORS headers for web clients\n- [ ] Use streaming for large responses (>1MB)\n- [ ] Implement proper error handling\n- [ ] Add request validation\n- [ ] Monitor with CloudWatch Logs\n- [ ] Set appropriate timeout and memory\n- [ ] Use compression for large responses\n- [ ] Cache responses when possible\n- [ ] Document your API endpoints\n\nGuide the user through implementing Function URLs or streaming based on their needs."
              },
              {
                "name": "/lambda-github-actions",
                "description": "Set up GitHub Actions CI/CD pipeline for Rust Lambda deployment",
                "path": "plugins/rust-lambda/commands/lambda-github-actions.md",
                "frontmatter": {
                  "description": "Set up GitHub Actions CI/CD pipeline for Rust Lambda deployment"
                },
                "content": "You are helping the user set up a GitHub Actions workflow for automated Lambda deployment.\n\n## Your Task\n\nCreate a complete GitHub Actions workflow for building, testing, and deploying Rust Lambda functions.\n\n1. **Ask about deployment preferences**:\n   - Which AWS region(s)?\n   - Which architecture (x86_64, arm64, or both)?\n   - Deploy on every push to main, or only on tags/releases?\n   - AWS authentication method (OIDC or access keys)?\n   - Single function or multiple functions?\n\n2. **Create workflow file**:\n   Create `.github/workflows/deploy-lambda.yml` with appropriate configuration\n\n3. **Set up AWS authentication**:\n   - OIDC (recommended, more secure)\n   - Access keys (simpler setup)\n\n4. **Configure required secrets** in GitHub repo settings\n\n## Complete Workflow Examples\n\n### Option 1: OIDC Authentication (Recommended)\n\n```yaml\nname: Deploy Lambda\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  CARGO_TERM_COLOR: always\n  AWS_REGION: us-east-1\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache cargo dependencies\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cargo/bin/\n            ~/.cargo/registry/index/\n            ~/.cargo/registry/cache/\n            ~/.cargo/git/db/\n            target/\n          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Run tests\n        run: cargo test --verbose\n\n      - name: Run clippy\n        run: cargo clippy -- -D warnings\n\n      - name: Check formatting\n        run: cargo fmt -- --check\n\n  build-and-deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    permissions:\n      id-token: write   # Required for OIDC\n      contents: read\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache cargo dependencies\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cargo/bin/\n            ~/.cargo/registry/index/\n            ~/.cargo/registry/cache/\n            ~/.cargo/git/db/\n            target/\n          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Install Zig\n        uses: goto-bus-stop/setup-zig@v2\n        with:\n          version: 0.11.0\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Build Lambda (ARM64)\n        run: cargo lambda build --release --arm64\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n\n      - name: Deploy to AWS Lambda\n        run: |\n          cargo lambda deploy \\\n            --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE_ARN }} \\\n            --region ${{ env.AWS_REGION }} \\\n            --arch arm64\n\n      - name: Test deployed function\n        run: |\n          aws lambda invoke \\\n            --function-name ${{ secrets.LAMBDA_FUNCTION_NAME }} \\\n            --payload '{\"test\": true}' \\\n            response.json\n          cat response.json\n```\n\n### Option 2: Access Keys Authentication\n\n```yaml\nname: Deploy Lambda\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  CARGO_TERM_COLOR: always\n  AWS_REGION: us-east-1\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache cargo dependencies\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cargo/bin/\n            ~/.cargo/registry/index/\n            ~/.cargo/registry/cache/\n            ~/.cargo/git/db/\n            target/\n          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Run tests\n        run: cargo test --verbose\n\n  build-and-deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache cargo dependencies\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cargo/bin/\n            ~/.cargo/registry/index/\n            ~/.cargo/registry/cache/\n            ~/.cargo/git/db/\n            target/\n          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Install Zig\n        uses: goto-bus-stop/setup-zig@v2\n        with:\n          version: 0.11.0\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Build Lambda (ARM64)\n        run: cargo lambda build --release --arm64\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ env.AWS_REGION }}\n\n      - name: Deploy to AWS Lambda\n        run: |\n          cargo lambda deploy \\\n            --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE_ARN }} \\\n            --region ${{ env.AWS_REGION }} \\\n            --arch arm64\n```\n\n### Option 3: Multi-Architecture Build\n\n```yaml\nname: Deploy Lambda\n\non:\n  push:\n    branches: [ main ]\n  release:\n    types: [ published ]\n\nenv:\n  CARGO_TERM_COLOR: always\n\njobs:\n  build-matrix:\n    strategy:\n      matrix:\n        include:\n          - arch: x86_64\n            aws_arch: x86_64\n            build_flags: \"\"\n          - arch: arm64\n            aws_arch: arm64\n            build_flags: \"--arm64\"\n\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Install Zig\n        uses: goto-bus-stop/setup-zig@v2\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Build for ${{ matrix.arch }}\n        run: cargo lambda build --release ${{ matrix.build_flags }} --output-format zip\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: lambda-${{ matrix.arch }}\n          path: target/lambda/**/*.zip\n\n  deploy:\n    needs: build-matrix\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    permissions:\n      id-token: write\n      contents: read\n\n    strategy:\n      matrix:\n        arch: [arm64, x86_64]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Download artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: lambda-${{ matrix.arch }}\n          path: target/lambda\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: us-east-1\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Deploy ${{ matrix.arch }}\n        run: |\n          cargo lambda deploy my-function-${{ matrix.arch }} \\\n            --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE_ARN }} \\\n            --arch ${{ matrix.arch }}\n```\n\n### Option 4: Multiple Functions\n\n```yaml\nname: Deploy Lambda Functions\n\non:\n  push:\n    branches: [ main ]\n\nenv:\n  CARGO_TERM_COLOR: always\n  AWS_REGION: us-east-1\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dtolnay/rust-toolchain@stable\n      - run: cargo test --all\n\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    permissions:\n      id-token: write\n      contents: read\n\n    strategy:\n      matrix:\n        function:\n          - name: api-handler\n            memory: 512\n            timeout: 30\n          - name: data-processor\n            memory: 2048\n            timeout: 300\n          - name: event-consumer\n            memory: 1024\n            timeout: 60\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Install Zig\n        uses: goto-bus-stop/setup-zig@v2\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Build ${{ matrix.function.name }}\n        run: cargo lambda build --release --arm64 --bin ${{ matrix.function.name }}\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n\n      - name: Deploy ${{ matrix.function.name }}\n        run: |\n          cargo lambda deploy ${{ matrix.function.name }} \\\n            --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE_ARN }} \\\n            --region ${{ env.AWS_REGION }} \\\n            --memory ${{ matrix.function.memory }} \\\n            --timeout ${{ matrix.function.timeout }} \\\n            --arch arm64 \\\n            --env-var RUST_LOG=info\n```\n\n## AWS OIDC Setup\n\nFor OIDC authentication (recommended), set up in AWS:\n\n### 1. Create OIDC Provider in AWS IAM\n\n```bash\naws iam create-open-id-connect-provider \\\n  --url https://token.actions.githubusercontent.com \\\n  --client-id-list sts.amazonaws.com \\\n  --thumbprint-list 6938fd4d98bab03faadb97b34396831e3780aea1\n```\n\n### 2. Create IAM Role for GitHub Actions\n\n```bash\n# Create trust policy\ncat > github-actions-trust-policy.json <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": \"repo:YOUR_GITHUB_ORG/YOUR_REPO:*\"\n        }\n      }\n    }\n  ]\n}\nEOF\n\n# Create role\naws iam create-role \\\n  --role-name GitHubActionsLambdaDeployRole \\\n  --assume-role-policy-document file://github-actions-trust-policy.json\n\n# Attach policies\naws iam attach-role-policy \\\n  --role-name GitHubActionsLambdaDeployRole \\\n  --policy-arn arn:aws:iam::aws:policy/AWSLambda_FullAccess\n\n# Get role ARN (save this for GitHub secrets)\naws iam get-role --role-name GitHubActionsLambdaDeployRole --query 'Role.Arn'\n```\n\n## GitHub Secrets Configuration\n\nConfigure these secrets in GitHub repository settings (Settings â†’ Secrets and variables â†’ Actions):\n\n### For OIDC:\n- `AWS_ROLE_ARN`: ARN of the GitHub Actions IAM role\n- `LAMBDA_EXECUTION_ROLE_ARN`: ARN of the Lambda execution role\n- `LAMBDA_FUNCTION_NAME` (optional): Function name if different from repo\n\n### For Access Keys:\n- `AWS_ACCESS_KEY_ID`: AWS access key\n- `AWS_SECRET_ACCESS_KEY`: AWS secret key\n- `LAMBDA_EXECUTION_ROLE_ARN`: ARN of the Lambda execution role\n\n### Optional secrets:\n- `AWS_REGION`: Override default region\n- Environment-specific variables as needed\n\n## Advanced Patterns\n\n### Deploy on Git Tags\n\n```yaml\non:\n  push:\n    tags:\n      - 'v*'\n\n# In deploy step:\n- name: Get tag version\n  id: tag\n  run: echo \"version=${GITHUB_REF#refs/tags/}\" >> $GITHUB_OUTPUT\n\n- name: Deploy with version tag\n  run: |\n    cargo lambda deploy \\\n      --tags Version=${{ steps.tag.outputs.version }}\n```\n\n### Environment-Specific Deployment\n\n```yaml\non:\n  push:\n    branches:\n      - main\n      - develop\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Set environment\n        id: env\n        run: |\n          if [ \"${{ github.ref }}\" = \"refs/heads/main\" ]; then\n            echo \"name=production\" >> $GITHUB_OUTPUT\n            echo \"suffix=\" >> $GITHUB_OUTPUT\n          else\n            echo \"name=development\" >> $GITHUB_OUTPUT\n            echo \"suffix=-dev\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: Deploy\n        run: |\n          cargo lambda deploy my-function${{ steps.env.outputs.suffix }} \\\n            --env-var ENVIRONMENT=${{ steps.env.outputs.name }}\n```\n\n### Conditional Deployment\n\n```yaml\n      - name: Check if Lambda code changed\n        id: lambda-changed\n        uses: dorny/paths-filter@v2\n        with:\n          filters: |\n            lambda:\n              - 'src/**'\n              - 'Cargo.toml'\n              - 'Cargo.lock'\n\n      - name: Deploy Lambda\n        if: steps.lambda-changed.outputs.lambda == 'true'\n        run: cargo lambda deploy\n```\n\n### Slack Notifications\n\n```yaml\n      - name: Notify Slack on success\n        if: success()\n        uses: slackapi/slack-github-action@v1\n        with:\n          payload: |\n            {\n              \"text\": \"Lambda deployed successfully: ${{ github.repository }}\"\n            }\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n```\n\n## Performance Optimizations\n\n### Faster Builds with Caching\n\n```yaml\n      - name: Cache Rust\n        uses: Swatinem/rust-cache@v2\n        with:\n          cache-on-failure: true\n```\n\n### Parallel Jobs\n\n```yaml\njobs:\n  test:\n    # Testing job\n\n  build:\n    # Build job (independent of test for speed)\n\n  deploy:\n    needs: [test, build]  # Only deploy if both succeed\n```\n\n## Troubleshooting CI/CD\n\n### Issue: \"cargo-lambda: command not found\"\n**Solution**: Ensure `pip install cargo-lambda` runs before use\n\n### Issue: \"Zig not found\"\n**Solution**: Add `goto-bus-stop/setup-zig@v2` step\n\n### Issue: \"AWS credentials not configured\"\n**Solution**: Verify secrets are set and aws-actions step is included\n\n### Issue: Build caching not working\n**Solution**: Use `Swatinem/rust-cache@v2` for better Rust caching\n\n### Issue: Deployment fails intermittently\n**Solution**: Add retry logic or use `aws lambda wait function-updated`\n\n## Testing the Workflow\n\n1. **Create workflow file** in `.github/workflows/deploy-lambda.yml`\n\n2. **Configure secrets** in GitHub settings\n\n3. **Push to trigger**:\n   ```bash\n   git add .github/workflows/deploy-lambda.yml\n   git commit -m \"Add Lambda deployment workflow\"\n   git push\n   ```\n\n4. **Monitor** in GitHub Actions tab\n\n5. **Check logs** for any issues\n\n## Best Practices\n\n1. **Always run tests** before deployment\n2. **Use OIDC** instead of long-lived credentials\n3. **Cache dependencies** for faster builds\n4. **Deploy on main branch** only, test on PRs\n5. **Use matrix builds** for multiple architectures\n6. **Tag deployments** with version info\n7. **Add notifications** for deployment status\n8. **Set up monitoring** and alerts in AWS\n9. **Use environments** for production deployments\n10. **Document secrets** needed in README\n\nAfter creating the workflow, guide the user through:\n1. Setting up required secrets\n2. Testing the workflow\n3. Monitoring deployments\n4. Handling failures"
              },
              {
                "name": "/lambda-iac",
                "description": "Set up Infrastructure as Code for Rust Lambda functions using SAM, Terraform, or CDK",
                "path": "plugins/rust-lambda/commands/lambda-iac.md",
                "frontmatter": {
                  "description": "Set up Infrastructure as Code for Rust Lambda functions using SAM, Terraform, or CDK"
                },
                "content": "You are helping the user set up Infrastructure as Code (IaC) for their Rust Lambda functions.\n\n## Your Task\n\nGuide the user through deploying and managing Lambda infrastructure using their preferred IaC tool.\n\n## Infrastructure as Code Options\n\n### Option 1: AWS SAM (Serverless Application Model)\n\n**Best for**:\n- Serverless-focused projects\n- Quick prototyping\n- Built-in local testing\n- Teams familiar with CloudFormation\n\n**Advantages**:\n- Official AWS support for Rust with cargo-lambda\n- Built-in local testing with `sam local`\n- Simpler for pure serverless applications\n- Good integration with Lambda features\n\n#### Basic SAM Template\n\nCreate `template.yaml`:\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Rust Lambda Function\n\nGlobals:\n  Function:\n    Timeout: 30\n    MemorySize: 512\n    Runtime: provided.al2023\n    Architectures:\n      - arm64\n    Environment:\n      Variables:\n        RUST_LOG: info\n\nResources:\n  MyRustFunction:\n    Type: AWS::Serverless::Function\n    Metadata:\n      BuildMethod: rust-cargolambda\n      BuildProperties:\n        Binary: my-function\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /hello\n            Method: get\n      Policies:\n        - AWSLambdaBasicExecutionRole\n\n  ComputeFunction:\n    Type: AWS::Serverless::Function\n    Metadata:\n      BuildMethod: rust-cargolambda\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      MemorySize: 2048\n      Timeout: 300\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref ProcessingBucket\n            Events: s3:ObjectCreated:*\n\n  ProcessingBucket:\n    Type: AWS::S3::Bucket\n\nOutputs:\n  ApiUrl:\n    Description: \"API Gateway endpoint URL\"\n    Value: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n```\n\n#### SAM Commands\n\n```bash\n# Build\nsam build\n\n# Test locally\nsam local invoke MyRustFunction -e events/test.json\n\n# Start local API\nsam local start-api\n\n# Deploy\nsam deploy --guided\n\n# Deploy with parameters\nsam deploy \\\n  --stack-name my-rust-lambda \\\n  --capabilities CAPABILITY_IAM \\\n  --region us-east-1\n```\n\n#### Multi-Function SAM Template\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nGlobals:\n  Function:\n    Runtime: provided.al2023\n    Architectures:\n      - arm64\n    Environment:\n      Variables:\n        RUST_LOG: info\n\nResources:\n  # API Handler - IO-optimized\n  ApiHandler:\n    Type: AWS::Serverless::Function\n    Metadata:\n      BuildMethod: rust-cargolambda\n      BuildProperties:\n        Binary: api-handler\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      MemorySize: 512\n      Timeout: 30\n      Events:\n        GetUsers:\n          Type: Api\n          Properties:\n            Path: /users\n            Method: get\n      Environment:\n        Variables:\n          DATABASE_URL: !Sub \"{{resolve:secretsmanager:${DBSecret}:SecretString:connection_string}}\"\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - AWSSecretsManagerGetSecretValuePolicy:\n            SecretArn: !Ref DBSecret\n\n  # Data Processor - Compute-optimized\n  DataProcessor:\n    Type: AWS::Serverless::Function\n    Metadata:\n      BuildMethod: rust-cargolambda\n      BuildProperties:\n        Binary: data-processor\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      MemorySize: 3008\n      Timeout: 300\n      Events:\n        S3Upload:\n          Type: S3\n          Properties:\n            Bucket: !Ref DataBucket\n            Events: s3:ObjectCreated:*\n            Filter:\n              S3Key:\n                Rules:\n                  - Name: prefix\n                    Value: raw/\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - S3ReadPolicy:\n            BucketName: !Ref DataBucket\n        - S3WritePolicy:\n            BucketName: !Ref DataBucket\n\n  # Event Consumer - SQS triggered\n  EventConsumer:\n    Type: AWS::Serverless::Function\n    Metadata:\n      BuildMethod: rust-cargolambda\n      BuildProperties:\n        Binary: event-consumer\n    Properties:\n      CodeUri: .\n      Handler: bootstrap\n      MemorySize: 1024\n      Timeout: 60\n      Events:\n        SQSEvent:\n          Type: SQS\n          Properties:\n            Queue: !GetAtt EventQueue.Arn\n            BatchSize: 10\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - SQSPollerPolicy:\n            QueueName: !GetAtt EventQueue.QueueName\n\n  DataBucket:\n    Type: AWS::S3::Bucket\n\n  EventQueue:\n    Type: AWS::SQS::Queue\n    Properties:\n      VisibilityTimeout: 360\n\n  DBSecret:\n    Type: AWS::SecretsManager::Secret\n    Properties:\n      Description: Database connection string\n      GenerateSecretString:\n        SecretStringTemplate: '{\"username\": \"admin\"}'\n        GenerateStringKey: \"password\"\n        PasswordLength: 32\n\nOutputs:\n  ApiEndpoint:\n    Value: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/\"\n  DataBucket:\n    Value: !Ref DataBucket\n  QueueUrl:\n    Value: !Ref EventQueue\n```\n\n### Option 2: Terraform\n\n**Best for**:\n- Multi-cloud or hybrid infrastructure\n- Complex infrastructure requirements\n- Teams already using Terraform\n- More control over AWS resources\n\n**Advantages**:\n- Broader ecosystem (300+ providers)\n- State management\n- Module reusability\n- Better for mixed workloads (Lambda + EC2 + RDS, etc.)\n\n#### Basic Terraform Configuration\n\nCreate `main.tf`:\n\n```hcl\nterraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n}\n\n# IAM Role for Lambda\nresource \"aws_iam_role\" \"lambda_role\" {\n  name = \"${var.function_name}-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"lambda.amazonaws.com\"\n      }\n    }]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"lambda_basic\" {\n  role       = aws_iam_role.lambda_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n}\n\n# Lambda Function\nresource \"aws_lambda_function\" \"rust_function\" {\n  filename         = \"target/lambda/${var.function_name}/bootstrap.zip\"\n  function_name    = var.function_name\n  role            = aws_iam_role.lambda_role.arn\n  handler         = \"bootstrap\"\n  source_code_hash = filebase64sha256(\"target/lambda/${var.function_name}/bootstrap.zip\")\n  runtime         = \"provided.al2023\"\n  architectures   = [\"arm64\"]\n  memory_size     = var.memory_size\n  timeout         = var.timeout\n\n  environment {\n    variables = {\n      RUST_LOG = var.log_level\n    }\n  }\n\n  tracing_config {\n    mode = \"Active\"\n  }\n}\n\n# CloudWatch Log Group\nresource \"aws_cloudwatch_log_group\" \"lambda_logs\" {\n  name              = \"/aws/lambda/${var.function_name}\"\n  retention_in_days = 14\n}\n\n# API Gateway (Optional)\nresource \"aws_apigatewayv2_api\" \"lambda_api\" {\n  name          = \"${var.function_name}-api\"\n  protocol_type = \"HTTP\"\n}\n\nresource \"aws_apigatewayv2_stage\" \"lambda_stage\" {\n  api_id      = aws_apigatewayv2_api.lambda_api.id\n  name        = \"prod\"\n  auto_deploy = true\n}\n\nresource \"aws_apigatewayv2_integration\" \"lambda_integration\" {\n  api_id           = aws_apigatewayv2_api.lambda_api.id\n  integration_type = \"AWS_PROXY\"\n  integration_uri  = aws_lambda_function.rust_function.invoke_arn\n}\n\nresource \"aws_apigatewayv2_route\" \"lambda_route\" {\n  api_id    = aws_apigatewayv2_api.lambda_api.id\n  route_key = \"GET /hello\"\n  target    = \"integrations/${aws_apigatewayv2_integration.lambda_integration.id}\"\n}\n\nresource \"aws_lambda_permission\" \"api_gateway\" {\n  statement_id  = \"AllowAPIGatewayInvoke\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.rust_function.function_name\n  principal     = \"apigateway.amazonaws.com\"\n  source_arn    = \"${aws_apigatewayv2_api.lambda_api.execution_arn}/*/*\"\n}\n\n# Outputs\noutput \"function_arn\" {\n  value = aws_lambda_function.rust_function.arn\n}\n\noutput \"api_endpoint\" {\n  value = aws_apigatewayv2_stage.lambda_stage.invoke_url\n}\n```\n\nCreate `variables.tf`:\n\n```hcl\nvariable \"aws_region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"function_name\" {\n  description = \"Lambda function name\"\n  type        = string\n}\n\nvariable \"memory_size\" {\n  description = \"Lambda memory size in MB\"\n  type        = number\n  default     = 512\n}\n\nvariable \"timeout\" {\n  description = \"Lambda timeout in seconds\"\n  type        = number\n  default     = 30\n}\n\nvariable \"log_level\" {\n  description = \"Rust log level\"\n  type        = string\n  default     = \"info\"\n}\n```\n\n#### Terraform Module for Rust Lambda\n\nCreate `modules/rust-lambda/main.tf`:\n\n```hcl\nresource \"aws_iam_role\" \"lambda_role\" {\n  name = \"${var.function_name}-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"lambda.amazonaws.com\"\n      }\n    }]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"lambda_basic\" {\n  role       = aws_iam_role.lambda_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n}\n\nresource \"aws_lambda_function\" \"function\" {\n  filename         = var.zip_file\n  function_name    = var.function_name\n  role            = aws_iam_role.lambda_role.arn\n  handler         = \"bootstrap\"\n  source_code_hash = filebase64sha256(var.zip_file)\n  runtime         = \"provided.al2023\"\n  architectures   = [var.architecture]\n  memory_size     = var.memory_size\n  timeout         = var.timeout\n\n  environment {\n    variables = var.environment_variables\n  }\n\n  dynamic \"vpc_config\" {\n    for_each = var.vpc_config != null ? [var.vpc_config] : []\n    content {\n      subnet_ids         = vpc_config.value.subnet_ids\n      security_group_ids = vpc_config.value.security_group_ids\n    }\n  }\n\n  tracing_config {\n    mode = var.enable_xray ? \"Active\" : \"PassThrough\"\n  }\n}\n\nresource \"aws_cloudwatch_log_group\" \"lambda_logs\" {\n  name              = \"/aws/lambda/${var.function_name}\"\n  retention_in_days = var.log_retention_days\n}\n```\n\nUsage:\n\n```hcl\nmodule \"api_handler\" {\n  source = \"./modules/rust-lambda\"\n\n  function_name          = \"api-handler\"\n  zip_file              = \"target/lambda/api-handler/bootstrap.zip\"\n  memory_size           = 512\n  timeout               = 30\n  architecture          = \"arm64\"\n  enable_xray           = true\n  log_retention_days    = 7\n\n  environment_variables = {\n    RUST_LOG     = \"info\"\n    DATABASE_URL = data.aws_secretsmanager_secret_version.db.secret_string\n  }\n}\n\nmodule \"data_processor\" {\n  source = \"./modules/rust-lambda\"\n\n  function_name          = \"data-processor\"\n  zip_file              = \"target/lambda/data-processor/bootstrap.zip\"\n  memory_size           = 3008\n  timeout               = 300\n  architecture          = \"arm64\"\n  enable_xray           = true\n  log_retention_days    = 7\n\n  environment_variables = {\n    RUST_LOG = \"info\"\n  }\n}\n```\n\n#### Terraform Commands\n\n```bash\n# Initialize\nterraform init\n\n# Plan\nterraform plan -var=\"function_name=my-rust-lambda\"\n\n# Apply\nterraform apply -var=\"function_name=my-rust-lambda\" -auto-approve\n\n# Destroy\nterraform destroy -var=\"function_name=my-rust-lambda\"\n```\n\n### Option 3: AWS CDK (TypeScript/Python)\n\n**Best for**:\n- Type-safe infrastructure definitions\n- Complex constructs and patterns\n- Teams comfortable with programming languages\n- Reusable infrastructure components\n\n#### CDK Example (TypeScript)\n\n```typescript\nimport * as cdk from 'aws-cdk-lib';\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\nimport * as apigateway from 'aws-cdk-lib/aws-apigatewayv2';\nimport * as integrations from 'aws-cdk-lib/aws-apigatewayv2-integrations';\n\nexport class RustLambdaStack extends cdk.Stack {\n  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n\n    const rustFunction = new lambda.Function(this, 'RustFunction', {\n      runtime: lambda.Runtime.PROVIDED_AL2023,\n      handler: 'bootstrap',\n      code: lambda.Code.fromAsset('target/lambda/my-function/bootstrap.zip'),\n      architecture: lambda.Architecture.ARM_64,\n      memorySize: 512,\n      timeout: cdk.Duration.seconds(30),\n      environment: {\n        RUST_LOG: 'info',\n      },\n      tracing: lambda.Tracing.ACTIVE,\n    });\n\n    const api = new apigateway.HttpApi(this, 'RustApi', {\n      defaultIntegration: new integrations.HttpLambdaIntegration(\n        'RustIntegration',\n        rustFunction\n      ),\n    });\n\n    new cdk.CfnOutput(this, 'ApiUrl', {\n      value: api.url!,\n    });\n  }\n}\n```\n\n## Comparison Table\n\n| Feature | SAM | Terraform | CDK |\n|---------|-----|-----------|-----|\n| Learning Curve | Low | Medium | Medium-High |\n| Rust Support | Excellent | Good | Good |\n| Local Testing | Built-in | Limited | Limited |\n| Multi-Cloud | No | Yes | No |\n| Type Safety | No | HCL | Yes |\n| Community | AWS-focused | Large | Growing |\n| State Management | CloudFormation | Terraform State | CloudFormation |\n\n## Integration with cargo-lambda\n\nAll IaC tools work well with cargo-lambda:\n\n```bash\n# Build for deployment\ncargo lambda build --release --arm64 --output-format zip\n\n# Then deploy with your IaC tool\nsam deploy\n# or\nterraform apply\n# or\ncdk deploy\n```\n\n## Best Practices\n\n1. **Version Control**: Store IaC templates in Git\n2. **Separate Environments**: Use workspaces/stages for dev/staging/prod\n3. **Secrets Management**: Use AWS Secrets Manager, never hardcode\n4. **Outputs**: Export important values (ARNs, URLs)\n5. **Modules**: Create reusable components\n6. **Testing**: Validate templates before deployment\n7. **CI/CD**: Automate IaC deployment\n8. **State Management**: Secure Terraform state (S3 + DynamoDB)\n9. **Documentation**: Comment complex configurations\n10. **Tagging**: Tag resources for cost tracking\n\n## Local Testing with SAM\n\n```bash\n# Test function locally\nsam local invoke MyRustFunction -e events/test.json\n\n# Start local API Gateway\nsam local start-api\n\n# Start local Lambda endpoint\nsam local start-lambda\n\n# Generate sample events\nsam local generate-event apigateway aws-proxy > event.json\nsam local generate-event s3 put > s3-event.json\n```\n\n## Using with LocalStack\n\nFor full local AWS emulation:\n\n```bash\n# Install LocalStack\npip install localstack\n\n# Start LocalStack\nlocalstack start\n\n# Deploy to LocalStack with SAM\nsamlocal deploy\n\n# Or with Terraform\nterraform apply \\\n  -var=\"aws_region=us-east-1\" \\\n  -var=\"endpoint=http://localhost:4566\"\n```\n\n## Migration Path\n\n**Starting fresh**:\n- Choose SAM for pure serverless, simple projects\n- Choose Terraform for complex, multi-service infrastructure\n- Choose CDK for type-safe, programmatic definitions\n\n**Existing infrastructure**:\n- Import existing resources into Terraform/CDK\n- Use CloudFormation template generation from SAM\n- Gradual migration with hybrid approach\n\n## Recommended Structure\n\n```\nmy-rust-lambda/\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ main.rs\nâ”œâ”€â”€ Cargo.toml\nâ”œâ”€â”€ template.yaml         # SAM\nâ”œâ”€â”€ terraform/           # Terraform\nâ”‚   â”œâ”€â”€ main.tf\nâ”‚   â”œâ”€â”€ variables.tf\nâ”‚   â””â”€â”€ outputs.tf\nâ”œâ”€â”€ cdk/                 # CDK\nâ”‚   â”œâ”€â”€ lib/\nâ”‚   â”‚   â””â”€â”€ stack.ts\nâ”‚   â””â”€â”€ bin/\nâ”‚       â””â”€â”€ app.ts\nâ””â”€â”€ events/             # Test events\n    â”œâ”€â”€ api-event.json\n    â””â”€â”€ s3-event.json\n```\n\nHelp the user choose the right IaC tool based on their needs and guide them through setup and deployment."
              },
              {
                "name": "/lambda-new",
                "description": "Create a new Rust Lambda function project with cargo-lambda",
                "path": "plugins/rust-lambda/commands/lambda-new.md",
                "frontmatter": {
                  "description": "Create a new Rust Lambda function project with cargo-lambda"
                },
                "content": "You are helping the user create a new Rust Lambda function project using cargo-lambda.\n\n## Your Task\n\nGuide the user through creating a new Lambda function project with the following steps:\n\n1. **Check if cargo-lambda is installed**:\n   - Run `cargo lambda --version` to verify installation\n   - If not installed, provide installation instructions:\n     ```bash\n     # Via Homebrew (macOS/Linux)\n     brew tap cargo-lambda/cargo-lambda\n     brew install cargo-lambda\n\n     # Via pip\n     pip install cargo-lambda\n\n     # From source\n     cargo install cargo-lambda\n     ```\n\n2. **Ask for project details** (if not provided):\n   - Function name\n   - Event type (API Gateway, S3, SQS, EventBridge, custom, or basic)\n   - Workload type (IO-intensive, compute-intensive, or mixed)\n\n3. **Create the project**:\n   ```bash\n   cargo lambda new <function-name>\n   ```\n   Or with event type:\n   ```bash\n   cargo lambda new <function-name> --event-type <type>\n   ```\n\n4. **Set up the basic structure** based on workload type:\n\n   **For IO-intensive** (default):\n   - Use full async/await\n   - Add dependencies: tokio, reqwest, aws-sdk crates as needed\n   - Example handler with concurrent operations\n\n   **For compute-intensive**:\n   - Add rayon to Cargo.toml\n   - Example using spawn_blocking + rayon\n   - Async only at boundaries\n\n   **For mixed**:\n   - Both patterns combined\n   - Async for IO, sync for compute\n\n5. **Add essential dependencies** to Cargo.toml:\n   ```toml\n   [dependencies]\n   lambda_runtime = \"0.13\"\n   tokio = { version = \"1\", features = [\"macros\"] }\n   serde = { version = \"1\", features = [\"derive\"] }\n   serde_json = \"1\"\n   anyhow = \"1\"\n   thiserror = \"1\"\n   tracing = { version = \"0.1\", features = [\"log\"] }\n   tracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n\n   # Add based on workload:\n   # For compute: rayon = \"1.10\"\n   # For HTTP: reqwest = { version = \"0.12\", features = [\"json\"] }\n   # For AWS services: aws-sdk-* crates\n   ```\n\n6. **Configure release profile** for optimization:\n   ```toml\n   [profile.release]\n   opt-level = 'z'     # Optimize for size\n   lto = true          # Link-time optimization\n   codegen-units = 1   # Better optimization\n   strip = true        # Remove debug symbols\n   panic = 'abort'     # Smaller panic handler\n   ```\n\n7. **Create example handler** matching the selected pattern\n\n8. **Test locally**:\n   ```bash\n   cd <function-name>\n   cargo lambda watch\n\n   # In another terminal:\n   cargo lambda invoke --data-ascii '{\"key\": \"value\"}'\n   ```\n\n## Event Type Templates\n\nProvide appropriate code based on event type:\n\n- **basic**: Simple JSON request/response\n- **apigw**: API Gateway proxy request/response\n- **s3**: S3 event processing\n- **sqs**: SQS message processing\n- **eventbridge**: EventBridge/CloudWatch Events\n\n## Example Handlers\n\nShow the user a complete working example for their chosen pattern:\n\n### IO-Intensive Example\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Deserialize)]\nstruct Request {\n    user_ids: Vec<String>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    count: usize,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Concurrent async operations\n    let futures = event.payload.user_ids\n        .into_iter()\n        .map(|id| fetch_user_data(&id));\n\n    let results = futures::future::try_join_all(futures).await?;\n\n    Ok(Response { count: results.len() })\n}\n\nasync fn fetch_user_data(id: &str) -> Result<(), Error> {\n    // Async IO operation\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n### Compute-Intensive Example\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse tokio::task;\n\n#[derive(Deserialize)]\nstruct Request {\n    numbers: Vec<i64>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    results: Vec<i64>,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let numbers = event.payload.numbers;\n\n    // CPU work in spawn_blocking with Rayon\n    let results = task::spawn_blocking(move || {\n        numbers\n            .par_iter()\n            .map(|&n| expensive_computation(n))\n            .collect::<Vec<_>>()\n    })\n    .await?;\n\n    Ok(Response { results })\n}\n\nfn expensive_computation(n: i64) -> i64 {\n    // CPU-intensive work\n    (0..1000).fold(n, |acc, _| acc.wrapping_mul(31))\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n## Next Steps\n\nAfter creating the project, suggest:\n1. Review and customize the handler\n2. Add tests\n3. Test locally with `cargo lambda watch`\n4. Build with `/lambda-build`\n5. Deploy with `/lambda-deploy`\n\nBe helpful and guide the user through any questions or issues they encounter."
              },
              {
                "name": "/lambda-observability",
                "description": "Set up advanced observability for Rust Lambda with OpenTelemetry, X-Ray, and structured logging",
                "path": "plugins/rust-lambda/commands/lambda-observability.md",
                "frontmatter": {
                  "description": "Set up advanced observability for Rust Lambda with OpenTelemetry, X-Ray, and structured logging"
                },
                "content": "You are helping the user implement comprehensive observability for their Rust Lambda functions.\n\n## Your Task\n\nGuide the user through setting up production-grade observability including distributed tracing, metrics, and structured logging.\n\n## Observability Stack Options\n\n### Option 1: AWS X-Ray (Native AWS Solution)\n\n**Best for**:\n- AWS-native monitoring\n- Quick setup\n- CloudWatch integration\n- Basic distributed tracing needs\n\n#### Enable X-Ray in Lambda\n\n**Via cargo-lambda:**\n```bash\ncargo lambda deploy --enable-tracing\n```\n\n**Via SAM template:**\n```yaml\nResources:\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Tracing: Active  # Enable X-Ray\n```\n\n**Via Terraform:**\n```hcl\nresource \"aws_lambda_function\" \"function\" {\n  # ... other config ...\n\n  tracing_config {\n    mode = \"Active\"\n  }\n}\n```\n\n#### X-Ray with xray-lite\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\nxray-lite = \"0.1\"\naws-config = \"1\"\naws-sdk-dynamodb = \"1\"  # or other AWS services\n```\n\nBasic usage:\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse xray_lite::SubsegmentContext;\nuse xray_lite_aws_sdk::XRayAwsSdkExtension;\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // X-Ray automatically creates parent segment for Lambda\n\n    // Create subsegment for custom operation\n    let subsegment = SubsegmentContext::from_lambda_ctx(&event.context);\n\n    // Trace AWS SDK calls\n    let config = aws_config::load_from_env().await\n        .xray_extension(subsegment.clone());\n\n    let dynamodb = aws_sdk_dynamodb::Client::new(&config);\n\n    // This DynamoDB call will be traced automatically\n    let result = dynamodb\n        .get_item()\n        .table_name(\"MyTable\")\n        .key(\"id\", AttributeValue::S(\"123\".to_string()))\n        .send()\n        .await?;\n\n    Ok(Response { data: result })\n}\n```\n\n### Option 2: OpenTelemetry (Vendor-Neutral)\n\n**Best for**:\n- Multi-vendor monitoring\n- Portability across platforms\n- Advanced telemetry needs\n- Custom metrics and traces\n\n#### Setup OpenTelemetry\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\nlambda_runtime = \"0.13\"\nlambda-otel-lite = \"0.1\"  # Lightweight OpenTelemetry for Lambda\nopentelemetry = \"0.22\"\nopentelemetry-otlp = \"0.15\"\nopentelemetry_sdk = \"0.22\"\ntracing = \"0.1\"\ntracing-opentelemetry = \"0.23\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"json\"] }\n```\n\n#### Basic OpenTelemetry Setup\n\n```rust\nuse lambda_otel_lite::{init_telemetry, HttpTracerProviderBuilder};\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse opentelemetry::trace::TracerProvider;\nuse tracing::{info, instrument};\nuse tracing_subscriber::layer::SubscriberExt;\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // Initialize OpenTelemetry\n    let tracer_provider = HttpTracerProviderBuilder::default()\n        .with_default_text_map_propagator()\n        .with_stdout_client()  // For testing, use OTLP for production\n        .build()?;\n\n    let tracer = tracer_provider.tracer(\"my-rust-lambda\");\n\n    // Setup tracing subscriber\n    let telemetry_layer = tracing_opentelemetry::layer()\n        .with_tracer(tracer);\n\n    let subscriber = tracing_subscriber::registry()\n        .with(tracing_subscriber::EnvFilter::from_default_env())\n        .with(tracing_subscriber::fmt::layer())\n        .with(telemetry_layer);\n\n    tracing::subscriber::set_global_default(subscriber)?;\n\n    run(service_fn(function_handler)).await\n}\n\n#[instrument(skip(event))]\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(request_id = %event.context.request_id, \"Processing request\");\n\n    let result = process_data(&event.payload).await?;\n\n    Ok(Response { result })\n}\n\n#[instrument]\nasync fn process_data(request: &Request) -> Result<Data, Error> {\n    info!(\"Processing data\");\n\n    // Your processing logic\n    // All operations within this function will be traced\n\n    Ok(Data::new())\n}\n```\n\n#### OpenTelemetry with OTLP Exporter\n\nFor production, export to observability backend:\n\n```rust\nuse lambda_otel_lite::HttpTracerProviderBuilder;\nuse opentelemetry_otlp::WithExportConfig;\n\nlet tracer_provider = HttpTracerProviderBuilder::default()\n    .with_stdout_client()\n    .enable_otlp(\n        opentelemetry_otlp::new_exporter()\n            .http()\n            .with_endpoint(\"https://your-collector:4318\")\n            .with_headers([(\"api-key\", \"your-key\")])\n    )?\n    .build()?;\n```\n\n### Option 3: Datadog Integration\n\n**Best for**:\n- Datadog users\n- Comprehensive APM\n- Log aggregation\n- Custom metrics\n\nAdd Datadog Lambda Extension layer and configure:\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse tracing::{info, instrument};\nuse tracing_subscriber::{fmt, EnvFilter};\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // JSON format for Datadog log parsing\n    tracing_subscriber::fmt()\n        .json()\n        .with_env_filter(EnvFilter::from_default_env())\n        .with_target(false)\n        .with_current_span(false)\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n\n#[instrument(\n    skip(event),\n    fields(\n        request_id = %event.context.request_id,\n        user_id = %event.payload.user_id,\n    )\n)]\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(\"Processing user request\");\n\n    // Datadog automatically traces this\n    let result = fetch_user_data(&event.payload.user_id).await?;\n\n    Ok(Response { result })\n}\n```\n\nDeploy with Datadog extension layer:\n```bash\ncargo lambda deploy \\\n  --layers arn:aws:lambda:us-east-1:464622532012:layer:Datadog-Extension-ARM:latest \\\n  --env-var DD_API_KEY=your-api-key \\\n  --env-var DD_SITE=datadoghq.com \\\n  --env-var DD_SERVICE=my-rust-service \\\n  --env-var DD_ENV=production\n```\n\n## Structured Logging Best Practices\n\n### Using tracing with Spans\n\n```rust\nuse tracing::{info, warn, error, debug, span, Level};\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let span = span!(\n        Level::INFO,\n        \"process_request\",\n        request_id = %event.context.request_id,\n        user_id = %event.payload.user_id,\n    );\n\n    let _enter = span.enter();\n\n    info!(\"Starting request processing\");\n\n    match process_user(&event.payload.user_id).await {\n        Ok(user) => {\n            info!(user_name = %user.name, \"User processed successfully\");\n            Ok(Response { user })\n        }\n        Err(e) => {\n            error!(error = %e, \"Failed to process user\");\n            Err(e)\n        }\n    }\n}\n\n#[instrument(skip(db), fields(user_id = %user_id))]\nasync fn process_user(user_id: &str) -> Result<User, Error> {\n    debug!(\"Fetching user from database\");\n\n    let user = fetch_from_db(user_id).await?;\n\n    info!(email = %user.email, \"User fetched\");\n\n    Ok(user)\n}\n```\n\n### JSON Structured Logging\n\n```rust\nuse tracing_subscriber::{fmt, EnvFilter, layer::SubscriberExt};\nuse serde_json::json;\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // JSON output for CloudWatch Insights\n    tracing_subscriber::fmt()\n        .json()\n        .with_env_filter(EnvFilter::from_default_env())\n        .with_current_span(true)\n        .with_span_list(true)\n        .with_target(false)\n        .without_time()  // CloudWatch adds timestamp\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n\n// Logs will be structured JSON:\n// {\"level\":\"info\",\"message\":\"Processing request\",\"request_id\":\"abc123\",\"user_id\":\"user456\"}\n```\n\n### Custom Metrics with OpenTelemetry\n\n```rust\nuse opentelemetry::metrics::{Counter, Histogram};\nuse opentelemetry::KeyValue;\n\nstruct Metrics {\n    request_counter: Counter<u64>,\n    duration_histogram: Histogram<f64>,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let start = std::time::Instant::now();\n\n    // Increment counter\n    metrics.request_counter.add(\n        1,\n        &[\n            KeyValue::new(\"function\", \"my-lambda\"),\n            KeyValue::new(\"region\", \"us-east-1\"),\n        ],\n    );\n\n    let result = process_request(&event.payload).await?;\n\n    // Record duration\n    let duration = start.elapsed().as_secs_f64();\n    metrics.duration_histogram.record(\n        duration,\n        &[KeyValue::new(\"status\", \"success\")],\n    );\n\n    Ok(result)\n}\n```\n\n## CloudWatch Logs Insights Queries\n\nWith structured logging, you can query efficiently:\n\n```\n# Find errors for specific user\nfields @timestamp, message, error\n| filter user_id = \"user456\"\n| filter level = \"error\"\n| sort @timestamp desc\n\n# Calculate p95 latency\nfields duration_ms\n| stats percentile(duration_ms, 95) as p95_latency by bin(5m)\n\n# Count requests by status\nfields @timestamp\n| filter message = \"Request completed\"\n| stats count() by status\n```\n\n## Distributed Tracing Pattern\n\nFor microservices calling each other:\n\n```rust\nuse opentelemetry::global;\nuse opentelemetry::trace::{Tracer, TracerProvider, SpanKind};\nuse opentelemetry_http::HeaderExtractor;\n\nasync fn function_handler(event: LambdaEvent<ApiGatewayRequest>) -> Result<Response, Error> {\n    let tracer = global::tracer(\"my-service\");\n\n    // Extract trace context from incoming request\n    let parent_cx = global::get_text_map_propagator(|propagator| {\n        let headers = HeaderExtractor::new(&event.payload.headers);\n        propagator.extract(&headers)\n    });\n\n    // Create span with parent context\n    let span = tracer\n        .span_builder(\"handle_request\")\n        .with_kind(SpanKind::Server)\n        .start_with_context(&tracer, &parent_cx);\n\n    let cx = opentelemetry::Context::current_with_span(span);\n\n    // Call downstream service with trace context\n    let client = reqwest::Client::new();\n    let response = client\n        .get(\"https://downstream-service.com/api\")\n        .header(\"traceparent\", extract_traceparent(&cx))\n        .send()\n        .await?;\n\n    Ok(Response { data: response.text().await? })\n}\n```\n\n## AWS ADOT Lambda Layer\n\nFor automatic instrumentation (limited Rust support):\n\n```bash\n# Add ADOT layer (note: Rust needs manual instrumentation)\ncargo lambda deploy \\\n  --layers arn:aws:lambda:us-east-1:901920570463:layer:aws-otel-collector-arm64-ver-0-90-1:1 \\\n  --env-var AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-instrument \\\n  --env-var OPENTELEMETRY_COLLECTOR_CONFIG_FILE=/var/task/collector.yaml\n```\n\n## Cold Start Monitoring\n\nTrack cold start vs warm start:\n\n```rust\nuse std::sync::atomic::{AtomicBool, Ordering};\n\nstatic COLD_START: AtomicBool = AtomicBool::new(true);\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let is_cold_start = COLD_START.swap(false, Ordering::Relaxed);\n\n    info!(\n        cold_start = is_cold_start,\n        \"Lambda invocation\"\n    );\n\n    // Process request...\n\n    Ok(Response {})\n}\n```\n\n## Error Tracking\n\n### Capturing Error Context\n\n```rust\nuse tracing::error;\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum LambdaError {\n    #[error(\"Database error: {0}\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"External API error: {status}, {message}\")]\n    ExternalApi { status: u16, message: String },\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    match process_request(&event.payload).await {\n        Ok(result) => {\n            info!(\"Request processed successfully\");\n            Ok(Response { result })\n        }\n        Err(e) => {\n            error!(\n                error = %e,\n                error_type = std::any::type_name_of_val(&e),\n                request_id = %event.context.request_id,\n                \"Request failed\"\n            );\n\n            // Optionally send to error tracking service\n            send_to_sentry(&e, &event.context).await;\n\n            Err(e.into())\n        }\n    }\n}\n```\n\n## Performance Monitoring\n\n### Measure Operation Duration\n\n```rust\nuse std::time::Instant;\nuse tracing::info;\n\n#[instrument]\nasync fn expensive_operation() -> Result<Data, Error> {\n    let start = Instant::now();\n\n    let result = do_work().await?;\n\n    let duration = start.elapsed();\n    info!(duration_ms = duration.as_millis(), \"Operation completed\");\n\n    Ok(result)\n}\n```\n\n### Automatic Instrumentation\n\n```rust\nuse tracing::instrument;\n\n// Automatically creates span and logs entry/exit\n#[instrument(\n    skip(db),  // Don't log entire db object\n    fields(\n        user_id = %user_id,\n        operation = \"fetch_user\"\n    ),\n    err  // Log errors automatically\n)]\nasync fn fetch_user(db: &Database, user_id: &str) -> Result<User, Error> {\n    db.get_user(user_id).await\n}\n```\n\n## Observability Checklist\n\n- [ ] Enable X-Ray or OpenTelemetry tracing\n- [ ] Use structured logging (JSON format)\n- [ ] Add span instrumentation to key functions\n- [ ] Track cold vs warm starts\n- [ ] Monitor error rates and types\n- [ ] Measure operation durations\n- [ ] Set up CloudWatch Logs Insights queries\n- [ ] Configure alerts for errors and latency\n- [ ] Track custom business metrics\n- [ ] Propagate trace context across services\n- [ ] Set appropriate log retention\n- [ ] Use log levels correctly (debug, info, warn, error)\n\n## Recommended Stack\n\n**For AWS-only**:\n- X-Ray for tracing\n- CloudWatch Logs with structured JSON\n- CloudWatch Insights for queries\n- xray-lite for Rust integration\n\n**For multi-cloud/vendor-neutral**:\n- OpenTelemetry for tracing\n- OTLP exporter to your backend\n- lambda-otel-lite for Lambda optimization\n- tracing crate for structured logging\n\n**For Datadog users**:\n- Datadog Lambda Extension\n- DD_TRACE_ENABLED for automatic tracing\n- JSON structured logging\n- Custom metrics via DogStatsD\n\n## Dependencies\n\n```toml\n[dependencies]\n# Basic tracing\ntracing = { version = \"0.1\", features = [\"log\"] }\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"json\"] }\n\n# X-Ray\nxray-lite = \"0.1\"\nxray-lite-aws-sdk = \"0.1\"\n\n# OpenTelemetry\nlambda-otel-lite = \"0.1\"\nopentelemetry = \"0.22\"\nopentelemetry-otlp = \"0.15\"\nopentelemetry_sdk = \"0.22\"\ntracing-opentelemetry = \"0.23\"\n\n# AWS SDK (for tracing AWS calls)\naws-config = \"1\"\naws-sdk-dynamodb = \"1\"  # or other services\n```\n\nGuide the user through setting up observability appropriate for their needs and monitoring backend."
              },
              {
                "name": "/lambda-optimize-compute",
                "description": "Optimize Rust Lambda function for compute-intensive workloads using Rayon and spawn_blocking",
                "path": "plugins/rust-lambda/commands/lambda-optimize-compute.md",
                "frontmatter": {
                  "description": "Optimize Rust Lambda function for compute-intensive workloads using Rayon and spawn_blocking"
                },
                "content": "You are helping the user optimize their Lambda function for compute-intensive workloads.\n\n## Your Task\n\nGuide the user to optimize their Lambda for CPU-intensive operations using synchronous parallel processing with Rayon and spawn_blocking.\n\n## Compute-Intensive Characteristics\n\nFunctions that:\n- Process large datasets\n- Perform mathematical computations\n- Transform/parse data\n- Image/video processing\n- Compression/decompression\n- Encryption/hashing\n- Machine learning inference\n\n**Goal**: Maximize CPU utilization without blocking async runtime\n\n## Key Principle: Async at Boundaries, Sync in Middle\n\n```\nInput (async) â†’ CPU Work (sync) â†’ Output (async)\n```\n\n```rust\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Phase 1: Async input (if needed)\n    let data = fetch_input_data().await?;\n\n    // Phase 2: Sync compute with spawn_blocking + Rayon\n    let results = tokio::task::spawn_blocking(move || {\n        data.par_iter()\n            .map(|item| expensive_computation(item))\n            .collect::<Vec<_>>()\n    })\n    .await?;\n\n    // Phase 3: Async output (if needed)\n    upload_results(&results).await?;\n\n    Ok(Response { results })\n}\n```\n\n## Core Pattern: spawn_blocking + Rayon\n\n### Why This Pattern?\n\n1. **spawn_blocking**: Moves work off async runtime to blocking thread pool\n2. **Rayon**: Efficiently parallelizes CPU work across available cores\n3. **Together**: Best CPU utilization without blocking async operations\n\n### Basic Pattern\n\n```rust\nuse tokio::task;\nuse rayon::prelude::*;\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let numbers = event.payload.numbers;\n\n    // Move CPU work to blocking thread pool\n    let results = task::spawn_blocking(move || {\n        // Use Rayon for parallel computation\n        numbers\n            .par_iter()\n            .map(|&n| cpu_intensive_work(n))\n            .collect::<Vec<_>>()\n    })\n    .await?;\n\n    Ok(Response { results })\n}\n\n// Pure CPU work - synchronous, no async\nfn cpu_intensive_work(n: i64) -> i64 {\n    // Heavy computation here\n    (0..10000).fold(n, |acc, _| {\n        acc.wrapping_mul(31).wrapping_add(17)\n    })\n}\n```\n\n## Complete Compute-Optimized Example\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse tokio::task;\nuse tracing::{info, instrument};\n\n#[derive(Deserialize)]\nstruct Request {\n    data: Vec<DataPoint>,\n    algorithm: String,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    processed: Vec<ProcessedData>,\n    stats: Statistics,\n}\n\n#[derive(Debug, Clone, Deserialize)]\nstruct DataPoint {\n    values: Vec<f64>,\n    metadata: String,\n}\n\n#[derive(Debug, Serialize)]\nstruct ProcessedData {\n    result: f64,\n    classification: String,\n}\n\n#[derive(Debug, Serialize)]\nstruct Statistics {\n    count: usize,\n    mean: f64,\n    std_dev: f64,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let data = event.payload.data;\n    let algorithm = event.payload.algorithm;\n\n    info!(\"Processing {} data points with {}\", data.len(), algorithm);\n\n    // CPU-intensive work in spawn_blocking\n    let results = task::spawn_blocking(move || {\n        process_data_parallel(data, &algorithm)\n    })\n    .await??;\n\n    Ok(results)\n}\n\n// All CPU work happens here - synchronous and parallel\nfn process_data_parallel(data: Vec<DataPoint>, algorithm: &str) -> Result<Response, Error> {\n    // Parallel processing with Rayon\n    let processed: Vec<ProcessedData> = data\n        .par_iter()\n        .map(|point| {\n            let result = match algorithm {\n                \"standard\" => compute_standard(&point.values),\n                \"advanced\" => compute_advanced(&point.values),\n                _ => compute_standard(&point.values),\n            };\n\n            let classification = classify_result(result);\n\n            ProcessedData { result, classification }\n        })\n        .collect();\n\n    // Compute statistics\n    let stats = compute_statistics(&processed);\n\n    Ok(Response { processed, stats })\n}\n\n// Pure computation - no IO, no async\nfn compute_standard(values: &[f64]) -> f64 {\n    // CPU-intensive mathematical computation\n    let sum: f64 = values.iter().sum();\n    let mean = sum / values.len() as f64;\n\n    values.iter()\n        .map(|&x| (x - mean).powi(2))\n        .sum::<f64>()\n        .sqrt()\n}\n\nfn compute_advanced(values: &[f64]) -> f64 {\n    // Even more intensive computation\n    let mut result = 0.0;\n    for &v in values {\n        for i in 0..1000 {\n            result += v * (i as f64).sin();\n        }\n    }\n    result\n}\n\nfn classify_result(value: f64) -> String {\n    match value {\n        x if x < 0.0 => \"low\".to_string(),\n        x if x < 10.0 => \"medium\".to_string(),\n        _ => \"high\".to_string(),\n    }\n}\n\nfn compute_statistics(processed: &[ProcessedData]) -> Statistics {\n    let count = processed.len();\n    let mean = processed.iter().map(|p| p.result).sum::<f64>() / count as f64;\n\n    let variance = processed\n        .iter()\n        .map(|p| (p.result - mean).powi(2))\n        .sum::<f64>() / count as f64;\n\n    let std_dev = variance.sqrt();\n\n    Statistics { count, mean, std_dev }\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n## Advanced Rayon Patterns\n\n### Custom Thread Pool\n\n```rust\nuse rayon::ThreadPoolBuilder;\nuse std::sync::OnceLock;\n\nstatic THREAD_POOL: OnceLock<rayon::ThreadPool> = OnceLock::new();\n\nfn get_thread_pool() -> &'static rayon::ThreadPool {\n    THREAD_POOL.get_or_init(|| {\n        ThreadPoolBuilder::new()\n            .num_threads(num_cpus::get())\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let data = event.payload.data;\n\n    let results = task::spawn_blocking(move || {\n        let pool = get_thread_pool();\n\n        pool.install(|| {\n            data.par_iter()\n                .map(|item| process(item))\n                .collect::<Vec<_>>()\n        })\n    })\n    .await?;\n\n    Ok(Response { results })\n}\n```\n\n### Parallel Fold (Reduce)\n\n```rust\nfn parallel_sum(numbers: Vec<i64>) -> i64 {\n    numbers\n        .par_iter()\n        .fold(|| 0i64, |acc, &x| acc + expensive_transform(x))\n        .reduce(|| 0, |a, b| a + b)\n}\n\nfn expensive_transform(n: i64) -> i64 {\n    // CPU work\n    (0..1000).fold(n, |acc, _| acc.wrapping_mul(31))\n}\n```\n\n### Parallel Chunks\n\n```rust\nuse rayon::prelude::*;\n\nfn process_in_chunks(data: Vec<u8>) -> Vec<Vec<u8>> {\n    data.par_chunks(1024)  // Process in 1KB chunks\n        .map(|chunk| {\n            // Expensive processing per chunk\n            compress_chunk(chunk)\n        })\n        .collect()\n}\n\nfn compress_chunk(chunk: &[u8]) -> Vec<u8> {\n    // CPU-intensive compression\n    chunk.to_vec()  // Placeholder\n}\n```\n\n### Parallel Chain\n\n```rust\nfn multi_stage_processing(data: Vec<DataPoint>) -> Vec<Output> {\n    data.par_iter()\n        .filter(|point| point.is_valid())\n        .map(|point| normalize(point))\n        .map(|normalized| transform(normalized))\n        .filter(|result| result.score > 0.5)\n        .collect()\n}\n```\n\n## Mixed IO + Compute Pattern\n\nFor functions that do both IO and compute:\n\n```rust\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Phase 1: Async IO - Download data\n    let raw_data = download_from_s3(&event.payload.bucket, &event.payload.key).await?;\n\n    // Phase 2: Sync compute - Process data\n    let processed = task::spawn_blocking(move || {\n        process_data_parallel(raw_data)\n    })\n    .await??;\n\n    // Phase 3: Async IO - Upload results\n    upload_to_s3(&event.payload.output_bucket, &processed).await?;\n\n    Ok(Response { success: true })\n}\n\nfn process_data_parallel(data: Vec<u8>) -> Result<Vec<ProcessedChunk>, Error> {\n    // Parse and process in parallel\n    let chunks: Vec<Vec<u8>> = data\n        .chunks(1024)\n        .map(|c| c.to_vec())\n        .collect();\n\n    let results = chunks\n        .par_iter()\n        .map(|chunk| {\n            // CPU-intensive per chunk\n            parse_and_transform(chunk)\n        })\n        .collect::<Result<Vec<_>, _>>()?;\n\n    Ok(results)\n}\n```\n\n## Image Processing Example\n\n```rust\nasync fn function_handler(event: LambdaEvent<S3Event>) -> Result<(), Error> {\n    for record in event.payload.records {\n        let bucket = record.s3.bucket.name.unwrap();\n        let key = record.s3.object.key.unwrap();\n\n        // Async: Download image\n        let image_data = download_from_s3(&bucket, &key).await?;\n\n        // Sync: Process image with Rayon\n        let processed = task::spawn_blocking(move || {\n            process_image_parallel(image_data)\n        })\n        .await??;\n\n        // Async: Upload result\n        let output_key = format!(\"processed/{}\", key);\n        upload_to_s3(&bucket, &output_key, &processed).await?;\n    }\n\n    Ok(())\n}\n\nfn process_image_parallel(image_data: Vec<u8>) -> Result<Vec<u8>, Error> {\n    // Parse image\n    let img = parse_image(&image_data)?;\n    let (width, height) = img.dimensions();\n\n    // Process rows in parallel\n    let rows: Vec<Vec<Pixel>> = (0..height)\n        .into_par_iter()\n        .map(|y| {\n            (0..width)\n                .map(|x| {\n                    let pixel = img.get_pixel(x, y);\n                    apply_filter(pixel)  // CPU-intensive\n                })\n                .collect()\n        })\n        .collect();\n\n    // Flatten and encode\n    encode_image(rows)\n}\n```\n\n## Data Transformation Example\n\n```rust\n#[derive(Deserialize)]\nstruct CsvRow {\n    id: String,\n    values: Vec<f64>,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Async: Download CSV from S3\n    let csv_data = download_csv(&event.payload.s3_key).await?;\n\n    // Sync: Parse and transform with Rayon\n    let transformed = task::spawn_blocking(move || {\n        parse_and_transform_csv(csv_data)\n    })\n    .await??;\n\n    // Async: Write to database\n    write_to_database(&transformed).await?;\n\n    Ok(Response { rows_processed: transformed.len() })\n}\n\nfn parse_and_transform_csv(csv_data: String) -> Result<Vec<TransformedRow>, Error> {\n    let rows: Vec<CsvRow> = csv_data\n        .lines()\n        .skip(1)  // Skip header\n        .map(|line| parse_csv_line(line))\n        .collect::<Result<Vec<_>, _>>()?;\n\n    // Parallel transformation\n    let transformed = rows\n        .par_iter()\n        .map(|row| {\n            // CPU-intensive transformation\n            TransformedRow {\n                id: row.id.clone(),\n                mean: calculate_mean(&row.values),\n                median: calculate_median(&row.values),\n                std_dev: calculate_std_dev(&row.values),\n                outliers: detect_outliers(&row.values),\n            }\n        })\n        .collect();\n\n    Ok(transformed)\n}\n```\n\n## Error Handling\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum ComputeError {\n    #[error(\"Invalid input: {0}\")]\n    InvalidInput(String),\n\n    #[error(\"Computation failed: {0}\")]\n    ComputationFailed(String),\n\n    #[error(\"Task join error: {0}\")]\n    JoinError(#[from] tokio::task::JoinError),\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let data = event.payload.data;\n\n    if data.is_empty() {\n        return Err(ComputeError::InvalidInput(\"Empty data\".to_string()).into());\n    }\n\n    let results = task::spawn_blocking(move || {\n        process_with_validation(data)\n    })\n    .await??;  // Handle both JoinError and computation errors\n\n    Ok(Response { results })\n}\n\nfn process_with_validation(data: Vec<DataPoint>) -> Result<Vec<Output>, ComputeError> {\n    let results: Result<Vec<_>, _> = data\n        .par_iter()\n        .map(|point| {\n            if !point.is_valid() {\n                return Err(ComputeError::InvalidInput(\n                    format!(\"Invalid point: {:?}\", point)\n                ));\n            }\n\n            process_point(point)\n                .map_err(|e| ComputeError::ComputationFailed(e.to_string()))\n        })\n        .collect();\n\n    results\n}\n```\n\n## Memory Configuration\n\nFor compute-intensive Lambda, more memory = more CPU:\n\n```bash\n# More memory for more CPU power\ncargo lambda deploy my-function --memory 3008\n\n# Lambda vCPU allocation:\n# 1769 MB = 1 full vCPU\n# 3008 MB = ~1.77 vCPU\n# 10240 MB = 6 vCPU\n```\n\n**Recommendation**: Test different memory settings to find optimal cost/performance\n\n## Performance Optimization Checklist\n\n- [ ] Use `tokio::task::spawn_blocking` for CPU work\n- [ ] Use Rayon `.par_iter()` for parallel processing\n- [ ] Keep async only at IO boundaries\n- [ ] Avoid async/await inside CPU-intensive functions\n- [ ] Use appropriate Lambda memory (more memory = more CPU)\n- [ ] Minimize data copying (use references where possible)\n- [ ] Profile to find hot paths\n- [ ] Consider chunking for very large datasets\n- [ ] Use `par_chunks` for better cache locality\n- [ ] Test with realistic data sizes\n- [ ] Monitor CPU utilization in CloudWatch\n\n## Dependencies\n\nAdd to `Cargo.toml`:\n\n```toml\n[dependencies]\nlambda_runtime = \"0.13\"\ntokio = { version = \"1\", features = [\"macros\", \"rt-multi-thread\"] }\nrayon = \"1.10\"\n\n# Serialization\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\n\n# Error handling\nanyhow = \"1\"\nthiserror = \"1\"\n\n# Tracing\ntracing = \"0.1\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n\n# Optional: CPU count\nnum_cpus = \"1\"\n\n# For image processing (example)\n# image = \"0.24\"\n\n# For CSV processing (example)\n# csv = \"1\"\n```\n\n## Testing Performance\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_parallel_performance() {\n        let data = vec![DataPoint::new(); 1000];\n\n        let start = std::time::Instant::now();\n\n        let results = task::spawn_blocking(move || {\n            process_data_parallel(data)\n        })\n        .await\n        .unwrap();\n\n        let duration = start.elapsed();\n\n        println!(\"Processed {} items in {:?}\", results.len(), duration);\n\n        // Verify parallelism is effective\n        assert!(duration.as_millis() < 5000);\n    }\n\n    #[test]\n    fn test_computation() {\n        let input = DataPoint::example();\n        let result = cpu_intensive_work(&input);\n        assert!(result.is_valid());\n    }\n}\n```\n\n## Benchmarking\n\nUse Criterion for benchmarking:\n\n```rust\n// benches/compute_bench.rs\nuse criterion::{black_box, criterion_group, criterion_main, Criterion};\n\nfn benchmark_computation(c: &mut Criterion) {\n    let data = vec![1i64; 10000];\n\n    c.bench_function(\"sequential\", |b| {\n        b.iter(|| {\n            data.iter()\n                .map(|&n| black_box(expensive_computation(n)))\n                .collect::<Vec<_>>()\n        })\n    });\n\n    c.bench_function(\"parallel\", |b| {\n        b.iter(|| {\n            data.par_iter()\n                .map(|&n| black_box(expensive_computation(n)))\n                .collect::<Vec<_>>()\n        })\n    });\n}\n\ncriterion_group!(benches, benchmark_computation);\ncriterion_main!(benches);\n```\n\nRun with:\n```bash\ncargo bench\n```\n\n## Monitoring\n\nAdd instrumentation:\n\n```rust\nuse tracing::{info, instrument};\n\n#[instrument(skip(data))]\nfn process_data_parallel(data: Vec<DataPoint>) -> Result<Vec<Output>, Error> {\n    let start = std::time::Instant::now();\n    let count = data.len();\n\n    let results = data\n        .par_iter()\n        .map(|point| process_point(point))\n        .collect::<Result<Vec<_>, _>>()?;\n\n    let duration = start.elapsed();\n\n    info!(\n        count,\n        duration_ms = duration.as_millis(),\n        throughput = count as f64 / duration.as_secs_f64(),\n        \"Processing complete\"\n    );\n\n    Ok(results)\n}\n```\n\nAfter optimization, verify:\n- CPU utilization is high (check CloudWatch)\n- Execution time scales with data size\n- Memory usage is within limits\n- Cost is optimized for workload"
              },
              {
                "name": "/lambda-optimize-io",
                "description": "Optimize Rust Lambda function for IO-intensive workloads with async patterns",
                "path": "plugins/rust-lambda/commands/lambda-optimize-io.md",
                "frontmatter": {
                  "description": "Optimize Rust Lambda function for IO-intensive workloads with async patterns"
                },
                "content": "You are helping the user optimize their Lambda function for IO-intensive workloads.\n\n## Your Task\n\nGuide the user to optimize their Lambda for maximum IO performance using async/await patterns.\n\n## IO-Intensive Characteristics\n\nFunctions that:\n- Make multiple HTTP/API requests\n- Query databases\n- Read/write from S3, DynamoDB\n- Call external services\n- Process message queues\n- Send notifications\n\n**Goal**: Maximize concurrency to reduce wall-clock time and cost\n\n## Key Optimization Strategies\n\n### 1. Concurrent Operations\n\nReplace sequential operations with concurrent ones:\n\n**âŒ Sequential (Slow)**:\n```rust\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Each operation waits for previous one - slow!\n    let user = fetch_user().await?;\n    let posts = fetch_posts().await?;\n    let comments = fetch_comments().await?;\n\n    Ok(Response { user, posts, comments })\n}\n```\n\n**âœ… Concurrent (Fast)**:\n```rust\nuse futures::future::try_join_all;\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // All operations run concurrently - fast!\n    let (user, posts, comments) = tokio::try_join!(\n        fetch_user(),\n        fetch_posts(),\n        fetch_comments(),\n    )?;\n\n    Ok(Response { user, posts, comments })\n}\n```\n\n**Performance impact**: 3 sequential 100ms calls = 300ms. Concurrent = ~100ms.\n\n### 2. Parallel Collection Processing\n\n**âŒ Sequential iteration**:\n```rust\nlet mut results = Vec::new();\nfor id in user_ids {\n    let data = fetch_data(&id).await?;\n    results.push(data);\n}\n```\n\n**âœ… Concurrent iteration**:\n```rust\nuse futures::future::try_join_all;\n\nlet futures = user_ids\n    .iter()\n    .map(|id| fetch_data(id));\n\nlet results = try_join_all(futures).await?;\n```\n\n**Alternative with buffer (limits concurrency)**:\n```rust\nuse futures::stream::{self, StreamExt};\n\nlet results = stream::iter(user_ids)\n    .map(|id| fetch_data(&id))\n    .buffer_unordered(10)  // Max 10 concurrent requests\n    .collect::<Vec<_>>()\n    .await;\n```\n\n### 3. Reuse Connections\n\n**âŒ Creating new client each time**:\n```rust\nasync fn fetch_data(url: &str) -> Result<Data, Error> {\n    let client = reqwest::Client::new();  // New connection every call!\n    client.get(url).send().await?.json().await\n}\n```\n\n**âœ… Shared client with connection pooling**:\n```rust\nuse std::sync::OnceLock;\nuse reqwest::Client;\n\n// Initialized once per container\nstatic HTTP_CLIENT: OnceLock<Client> = OnceLock::new();\n\nfn get_client() -> &'static Client {\n    HTTP_CLIENT.get_or_init(|| {\n        Client::builder()\n            .timeout(Duration::from_secs(30))\n            .pool_max_idle_per_host(10)\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn fetch_data(url: &str) -> Result<Data, Error> {\n    let client = get_client();  // Reuses connections!\n    client.get(url).send().await?.json().await\n}\n```\n\n### 4. Database Connection Pooling\n\n**For PostgreSQL with sqlx**:\n```rust\nuse std::sync::OnceLock;\nuse sqlx::{PgPool, postgres::PgPoolOptions};\n\nstatic DB_POOL: OnceLock<PgPool> = OnceLock::new();\n\nasync fn get_pool() -> &'static PgPool {\n    DB_POOL.get_or_init(|| async {\n        PgPoolOptions::new()\n            .max_connections(5)  // Limit connections\n            .connect(&std::env::var(\"DATABASE_URL\").unwrap())\n            .await\n            .unwrap()\n    }).await\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let pool = get_pool().await;\n\n    // Use connection pool for queries\n    let user = sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", user_id)\n        .fetch_one(pool)\n        .await?;\n\n    Ok(Response { user })\n}\n```\n\n**For DynamoDB**:\n```rust\nuse std::sync::OnceLock;\nuse aws_sdk_dynamodb::Client;\n\nstatic DYNAMODB_CLIENT: OnceLock<Client> = OnceLock::new();\n\nasync fn get_dynamodb() -> &'static Client {\n    DYNAMODB_CLIENT.get_or_init(|| async {\n        let config = aws_config::load_from_env().await;\n        Client::new(&config)\n    }).await\n}\n```\n\n### 5. Batch Operations\n\nWhen possible, use batch APIs:\n\n**âŒ Individual requests**:\n```rust\nfor key in keys {\n    let item = dynamodb.get_item()\n        .table_name(\"MyTable\")\n        .key(\"id\", AttributeValue::S(key))\n        .send()\n        .await?;\n}\n```\n\n**âœ… Batch request**:\n```rust\nlet batch_keys = keys\n    .iter()\n    .map(|key| {\n        [(\n            \"id\".to_string(),\n            AttributeValue::S(key.clone())\n        )].into()\n    })\n    .collect();\n\nlet response = dynamodb.batch_get_item()\n    .request_items(\"MyTable\", KeysAndAttributes::builder()\n        .set_keys(Some(batch_keys))\n        .build()?)\n    .send()\n    .await?;\n```\n\n## Complete IO-Optimized Example\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse serde::{Deserialize, Serialize};\nuse std::sync::OnceLock;\nuse reqwest::Client;\nuse futures::future::try_join_all;\nuse tracing::info;\n\n#[derive(Deserialize)]\nstruct Request {\n    user_ids: Vec<String>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    users: Vec<UserData>,\n}\n\n#[derive(Serialize)]\nstruct UserData {\n    user: User,\n    posts: Vec<Post>,\n    followers: usize,\n}\n\n// Shared HTTP client with connection pooling\nstatic HTTP_CLIENT: OnceLock<Client> = OnceLock::new();\n\nfn get_client() -> &'static Client {\n    HTTP_CLIENT.get_or_init(|| {\n        Client::builder()\n            .timeout(Duration::from_secs(30))\n            .pool_max_idle_per_host(10)\n            .tcp_keepalive(Duration::from_secs(60))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(\"Processing {} users\", event.payload.user_ids.len());\n\n    // Process all users concurrently\n    let user_futures = event.payload.user_ids\n        .into_iter()\n        .map(|user_id| fetch_user_data(user_id));\n\n    let users = try_join_all(user_futures).await?;\n\n    Ok(Response { users })\n}\n\nasync fn fetch_user_data(user_id: String) -> Result<UserData, Error> {\n    let client = get_client();\n\n    // Fetch all user data concurrently\n    let (user, posts, followers) = tokio::try_join!(\n        fetch_user(client, &user_id),\n        fetch_posts(client, &user_id),\n        fetch_follower_count(client, &user_id),\n    )?;\n\n    Ok(UserData { user, posts, followers })\n}\n\nasync fn fetch_user(client: &Client, user_id: &str) -> Result<User, Error> {\n    let response = client\n        .get(format!(\"https://api.example.com/users/{}\", user_id))\n        .send()\n        .await?\n        .json()\n        .await?;\n    Ok(response)\n}\n\nasync fn fetch_posts(client: &Client, user_id: &str) -> Result<Vec<Post>, Error> {\n    let response = client\n        .get(format!(\"https://api.example.com/users/{}/posts\", user_id))\n        .send()\n        .await?\n        .json()\n        .await?;\n    Ok(response)\n}\n\nasync fn fetch_follower_count(client: &Client, user_id: &str) -> Result<usize, Error> {\n    let response: FollowerResponse = client\n        .get(format!(\"https://api.example.com/users/{}/followers\", user_id))\n        .send()\n        .await?\n        .json()\n        .await?;\n    Ok(response.count)\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n## AWS SDK Optimization\n\n### S3 Concurrent Operations\n\n```rust\nuse aws_sdk_s3::Client;\nuse futures::future::try_join_all;\n\nasync fn download_multiple_files(\n    s3: &Client,\n    bucket: &str,\n    keys: Vec<String>,\n) -> Result<Vec<Bytes>, Error> {\n    let futures = keys.iter().map(|key| async move {\n        s3.get_object()\n            .bucket(bucket)\n            .key(key)\n            .send()\n            .await?\n            .body\n            .collect()\n            .await\n            .map(|data| data.into_bytes())\n    });\n\n    try_join_all(futures).await\n}\n```\n\n### DynamoDB Parallel Queries\n\n```rust\nasync fn query_multiple_partitions(\n    dynamodb: &Client,\n    partition_keys: Vec<String>,\n) -> Result<Vec<Item>, Error> {\n    let futures = partition_keys.iter().map(|pk| {\n        dynamodb\n            .query()\n            .table_name(\"MyTable\")\n            .key_condition_expression(\"pk = :pk\")\n            .expression_attribute_values(\":pk\", AttributeValue::S(pk.clone()))\n            .send()\n    });\n\n    let results = try_join_all(futures).await?;\n\n    let items = results\n        .into_iter()\n        .flat_map(|r| r.items.unwrap_or_default())\n        .collect();\n\n    Ok(items)\n}\n```\n\n## Timeout and Retry Configuration\n\n```rust\nuse reqwest::Client;\nuse std::time::Duration;\n\nfn get_client() -> &'static Client {\n    HTTP_CLIENT.get_or_init(|| {\n        Client::builder()\n            .timeout(Duration::from_secs(30))      // Total request timeout\n            .connect_timeout(Duration::from_secs(10)) // Connection timeout\n            .pool_max_idle_per_host(10)            // Connection pool size\n            .tcp_keepalive(Duration::from_secs(60)) // Keep connections alive\n            .build()\n            .unwrap()\n    })\n}\n```\n\nWith retries:\n```rust\nuse tower::{ServiceBuilder, ServiceExt};\nuse tower::retry::RetryLayer;\n\n// Add to dependencies: tower = { version = \"0.4\", features = [\"retry\"] }\n\nasync fn fetch_with_retry(url: &str) -> Result<Response, Error> {\n    let client = get_client();\n\n    for attempt in 1..=3 {\n        match client.get(url).send().await {\n            Ok(response) => return Ok(response),\n            Err(e) if attempt < 3 => {\n                tokio::time::sleep(Duration::from_millis(100 * attempt)).await;\n                continue;\n            }\n            Err(e) => return Err(e.into()),\n        }\n    }\n\n    unreachable!()\n}\n```\n\n## Streaming Large Responses\n\nFor large files or responses:\n\n```rust\nuse tokio::io::AsyncWriteExt;\nuse futures::StreamExt;\n\nasync fn download_large_file(s3: &Client, bucket: &str, key: &str) -> Result<(), Error> {\n    let mut object = s3\n        .get_object()\n        .bucket(bucket)\n        .key(key)\n        .send()\n        .await?;\n\n    // Stream to avoid loading entire file in memory\n    let mut body = object.body;\n\n    while let Some(chunk) = body.next().await {\n        let chunk = chunk?;\n        // Process chunk\n        process_chunk(&chunk).await?;\n    }\n\n    Ok(())\n}\n```\n\n## Concurrency Limits\n\nControl concurrency to avoid overwhelming external services:\n\n```rust\nuse futures::stream::{self, StreamExt};\n\nasync fn process_with_limit(\n    items: Vec<Item>,\n    max_concurrent: usize,\n) -> Result<Vec<Output>, Error> {\n    let results = stream::iter(items)\n        .map(|item| async move {\n            process_item(item).await\n        })\n        .buffer_unordered(max_concurrent)  // Limit concurrent operations\n        .collect::<Vec<_>>()\n        .await;\n\n    results.into_iter().collect()\n}\n```\n\n## Error Handling for Async Operations\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum LambdaError {\n    #[error(\"HTTP error: {0}\")]\n    Http(#[from] reqwest::Error),\n\n    #[error(\"Database error: {0}\")]\n    Database(String),\n\n    #[error(\"Timeout: operation took too long\")]\n    Timeout,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Use timeout for async operations\n    let result = tokio::time::timeout(\n        Duration::from_secs(25),  // Lambda timeout - buffer\n        process_request(event.payload)\n    )\n    .await\n    .map_err(|_| LambdaError::Timeout)??;\n\n    Ok(result)\n}\n```\n\n## Performance Checklist\n\nApply these optimizations:\n\n- [ ] Use `tokio::try_join!` for fixed concurrent operations\n- [ ] Use `futures::future::try_join_all` for dynamic collections\n- [ ] Initialize clients/pools once with `OnceLock`\n- [ ] Configure connection pooling for HTTP clients\n- [ ] Use batch APIs when available\n- [ ] Set appropriate timeouts\n- [ ] Add retries for transient failures\n- [ ] Stream large responses\n- [ ] Limit concurrency to avoid overwhelming services\n- [ ] Use `buffer_unordered` for controlled parallelism\n- [ ] Avoid blocking operations in async context\n- [ ] Monitor cold start times\n- [ ] Test with realistic event sizes\n\n## Dependencies for IO Optimization\n\nAdd to `Cargo.toml`:\n\n```toml\n[dependencies]\nlambda_runtime = \"0.13\"\ntokio = { version = \"1\", features = [\"macros\", \"rt-multi-thread\", \"time\"] }\nfutures = \"0.3\"\n\n# HTTP client\nreqwest = { version = \"0.12\", features = [\"json\"] }\n\n# AWS SDKs\naws-config = \"1\"\naws-sdk-s3 = \"1\"\naws-sdk-dynamodb = \"1\"\n\n# Database (if needed)\nsqlx = { version = \"0.7\", features = [\"runtime-tokio-rustls\", \"postgres\"] }\n\n# Error handling\nanyhow = \"1\"\nthiserror = \"1\"\n\n# Tracing\ntracing = \"0.1\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n\n# Optional: retries\ntower = { version = \"0.4\", features = [\"retry\", \"timeout\"] }\n```\n\n## Testing IO Performance\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_concurrent_performance() {\n        let start = std::time::Instant::now();\n\n        let results = fetch_multiple_users(vec![\"1\", \"2\", \"3\"]).await.unwrap();\n\n        let duration = start.elapsed();\n\n        // Should be ~100ms (concurrent), not ~300ms (sequential)\n        assert!(duration.as_millis() < 150);\n        assert_eq!(results.len(), 3);\n    }\n}\n```\n\n## Monitoring\n\nAdd instrumentation to track IO performance:\n\n```rust\nuse tracing::{info, instrument};\n\n#[instrument(skip(client))]\nasync fn fetch_user(client: &Client, user_id: &str) -> Result<User, Error> {\n    let start = std::time::Instant::now();\n\n    let result = client\n        .get(format!(\"https://api.example.com/users/{}\", user_id))\n        .send()\n        .await?\n        .json()\n        .await?;\n\n    info!(user_id, duration_ms = start.elapsed().as_millis(), \"User fetched\");\n\n    Ok(result)\n}\n```\n\nAfter optimization, verify:\n- Cold start time (should be minimal)\n- Warm execution time (should be low due to concurrency)\n- Memory usage (should be moderate)\n- Error rates (should be low with retries)\n- CloudWatch metrics show improved performance"
              },
              {
                "name": "/lambda-secrets",
                "description": "Manage secrets and configuration for Rust Lambda functions using AWS Secrets Manager and Parameter Store",
                "path": "plugins/rust-lambda/commands/lambda-secrets.md",
                "frontmatter": {
                  "description": "Manage secrets and configuration for Rust Lambda functions using AWS Secrets Manager and Parameter Store"
                },
                "content": "You are helping the user securely manage secrets and configuration for their Rust Lambda functions.\n\n## Your Task\n\nGuide the user through implementing secure secrets management using AWS Secrets Manager, Systems Manager Parameter Store, and the Parameters and Secrets Lambda Extension.\n\n## Secrets Management Options\n\n### Option 1: AWS Parameters and Secrets Lambda Extension (Recommended)\n\n**Best for**:\n- Production workloads\n- Cost-conscious applications\n- Low-latency requirements\n- Local caching needs\n\n**Advantages**:\n- Cached locally (reduces latency and cost)\n- No SDK calls needed\n- Automatic refresh\n- Works with both Secrets Manager and Parameter Store\n\n#### Setup\n\n1. **Add the extension layer** to your Lambda:\n\n```bash\ncargo lambda deploy \\\n  --layers arn:aws:lambda:us-east-1:177933569100:layer:AWS-Parameters-and-Secrets-Lambda-Extension-Arm64:11\n```\n\nFor x86_64:\n```bash\ncargo lambda deploy \\\n  --layers arn:aws:lambda:us-east-1:177933569100:layer:AWS-Parameters-and-Secrets-Lambda-Extension:11\n```\n\n2. **Add IAM permissions**:\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\",\n        \"ssm:GetParameter\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:us-east-1:123456789012:secret:my-secret-*\",\n        \"arn:aws:ssm:us-east-1:123456789012:parameter/myapp/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"kms:Decrypt\",\n      \"Resource\": \"arn:aws:kms:us-east-1:123456789012:key/key-id\"\n    }\n  ]\n}\n```\n\n3. **Use the Rust client**:\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\naws-parameters-and-secrets-lambda = \"0.1\"\nserde_json = \"1\"\n```\n\nBasic usage:\n```rust\nuse aws_parameters_and_secrets_lambda::{Manager, ParameterError};\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse std::env;\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Get secret from Secrets Manager\n    let manager = Manager::new();\n    let secret_value = manager\n        .get_secret(\"my-database-password\")\n        .await?;\n\n    // Parse as JSON if needed\n    let db_config: DatabaseConfig = serde_json::from_str(&secret_value)?;\n\n    // Use the secret\n    let connection = connect_to_db(&db_config).await?;\n\n    Ok(Response { success: true })\n}\n\n#[derive(Deserialize)]\nstruct DatabaseConfig {\n    host: String,\n    port: u16,\n    username: String,\n    password: String,\n    database: String,\n}\n```\n\n#### Get Parameter Store Values\n\n```rust\nuse aws_parameters_and_secrets_lambda::Manager;\n\nasync fn get_config() -> Result<AppConfig, Error> {\n    let manager = Manager::new();\n\n    // Get simple parameter\n    let api_url = manager\n        .get_parameter(\"/myapp/api-url\")\n        .await?;\n\n    // Get SecureString parameter (automatically decrypted)\n    let api_key = manager\n        .get_parameter(\"/myapp/api-key\")\n        .await?;\n\n    Ok(AppConfig {\n        api_url,\n        api_key,\n    })\n}\n```\n\n#### Caching and TTL\n\nThe extension caches secrets/parameters automatically. Configure TTL:\n\n```bash\ncargo lambda deploy \\\n  --layers arn:aws:lambda:...:layer:AWS-Parameters-and-Secrets-Lambda-Extension-Arm64:11 \\\n  --env-var PARAMETERS_SECRETS_EXTENSION_CACHE_ENABLED=true \\\n  --env-var PARAMETERS_SECRETS_EXTENSION_CACHE_SIZE=1000 \\\n  --env-var PARAMETERS_SECRETS_EXTENSION_MAX_CONNECTIONS=3\n```\n\n### Option 2: AWS SDK Direct Calls\n\n**Best for**:\n- Simple use cases\n- One-time secret retrieval\n- When extension layer isn't available\n\n#### Using AWS SDK for Secrets Manager\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\naws-config = \"1\"\naws-sdk-secretsmanager = \"1\"\n```\n\nUsage:\n```rust\nuse aws_config::BehaviorVersion;\nuse aws_sdk_secretsmanager::Client as SecretsManagerClient;\nuse std::sync::OnceLock;\n\nstatic SECRETS_CLIENT: OnceLock<SecretsManagerClient> = OnceLock::new();\n\nasync fn get_secrets_client() -> &'static SecretsManagerClient {\n    SECRETS_CLIENT.get_or_init(|| async {\n        let config = aws_config::load_defaults(BehaviorVersion::latest()).await;\n        SecretsManagerClient::new(&config)\n    }).await\n}\n\nasync fn get_database_password() -> Result<String, Error> {\n    let client = get_secrets_client().await;\n\n    let response = client\n        .get_secret_value()\n        .secret_id(\"prod/database/password\")\n        .send()\n        .await?;\n\n    Ok(response.secret_string().unwrap().to_string())\n}\n\n// For JSON secrets\nasync fn get_database_config() -> Result<DatabaseConfig, Error> {\n    let client = get_secrets_client().await;\n\n    let response = client\n        .get_secret_value()\n        .secret_id(\"prod/database/config\")\n        .send()\n        .await?;\n\n    let secret_string = response.secret_string().unwrap();\n    let config: DatabaseConfig = serde_json::from_str(secret_string)?;\n\n    Ok(config)\n}\n```\n\n#### Using AWS SDK for Parameter Store\n\nAdd to `Cargo.toml`:\n```toml\n[dependencies]\naws-config = \"1\"\naws-sdk-ssm = \"1\"\n```\n\nUsage:\n```rust\nuse aws_sdk_ssm::Client as SsmClient;\nuse std::sync::OnceLock;\n\nstatic SSM_CLIENT: OnceLock<SsmClient> = OnceLock::new();\n\nasync fn get_ssm_client() -> &'static SsmClient {\n    SSM_CLIENT.get_or_init(|| async {\n        let config = aws_config::load_defaults(BehaviorVersion::latest()).await;\n        SsmClient::new(&config)\n    }).await\n}\n\nasync fn get_parameter(name: &str) -> Result<String, Error> {\n    let client = get_ssm_client().await;\n\n    let response = client\n        .get_parameter()\n        .name(name)\n        .with_decryption(true)  // Decrypt SecureString\n        .send()\n        .await?;\n\n    Ok(response.parameter().unwrap().value().unwrap().to_string())\n}\n\n// Get multiple parameters\nasync fn get_parameters_by_path(path: &str) -> Result<HashMap<String, String>, Error> {\n    let client = get_ssm_client().await;\n\n    let mut parameters = HashMap::new();\n    let mut next_token = None;\n\n    loop {\n        let mut request = client\n            .get_parameters_by_path()\n            .path(path)\n            .with_decryption(true)\n            .recursive(true);\n\n        if let Some(token) = next_token {\n            request = request.next_token(token);\n        }\n\n        let response = request.send().await?;\n\n        for param in response.parameters() {\n            parameters.insert(\n                param.name().unwrap().to_string(),\n                param.value().unwrap().to_string(),\n            );\n        }\n\n        next_token = response.next_token().map(|s| s.to_string());\n        if next_token.is_none() {\n            break;\n        }\n    }\n\n    Ok(parameters)\n}\n```\n\n### Option 3: Environment Variables (For Non-Sensitive Config)\n\n**Best for**:\n- Non-sensitive configuration\n- Simple deployments\n- Configuration that changes per environment\n\n```rust\nuse std::env;\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let api_url = env::var(\"API_URL\")\n        .expect(\"API_URL must be set\");\n\n    let timeout_secs: u64 = env::var(\"TIMEOUT_SECONDS\")\n        .unwrap_or_else(|_| \"30\".to_string())\n        .parse()\n        .expect(\"TIMEOUT_SECONDS must be a number\");\n\n    // Use configuration\n    let client = build_client(&api_url, timeout_secs);\n\n    Ok(Response { })\n}\n```\n\nDeploy with environment variables:\n```bash\ncargo lambda deploy \\\n  --env-var API_URL=https://api.example.com \\\n  --env-var TIMEOUT_SECONDS=30 \\\n  --env-var ENVIRONMENT=production\n```\n\n## Best Practices\n\n### 1. Initialize Secrets at Startup\n\n```rust\nuse std::sync::OnceLock;\n\nstruct AppSecrets {\n    database_password: String,\n    api_key: String,\n    encryption_key: String,\n}\n\nstatic SECRETS: OnceLock<AppSecrets> = OnceLock::new();\n\nasync fn init_secrets() -> Result<&'static AppSecrets, Error> {\n    SECRETS.get_or_try_init(|| async {\n        let manager = Manager::new();\n\n        Ok(AppSecrets {\n            database_password: manager.get_secret(\"db-password\").await?,\n            api_key: manager.get_parameter(\"/myapp/api-key\").await?,\n            encryption_key: manager.get_secret(\"encryption-key\").await?,\n        })\n    }).await\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // Load secrets once at startup\n    init_secrets().await?;\n\n    run(service_fn(function_handler)).await\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Access pre-loaded secrets\n    let secrets = SECRETS.get().unwrap();\n\n    let connection = connect_with_password(&secrets.database_password).await?;\n\n    Ok(Response {})\n}\n```\n\n### 2. Separate Secrets by Environment\n\n```\n# Development\n/dev/myapp/database/password\n/dev/myapp/api-key\n\n# Staging\n/staging/myapp/database/password\n/staging/myapp/api-key\n\n# Production\n/prod/myapp/database/password\n/prod/myapp/api-key\n```\n\nUsage:\n```rust\nlet env = std::env::var(\"ENVIRONMENT\").unwrap_or_else(|_| \"dev\".to_string());\nlet param_name = format!(\"/{}/myapp/database/password\", env);\n\nlet password = manager.get_parameter(&param_name).await?;\n```\n\n### 3. Handle Secret Rotation\n\n```rust\nuse std::sync::RwLock;\nuse std::time::{Duration, Instant};\n\nstruct CachedSecret {\n    value: String,\n    last_updated: Instant,\n    ttl: Duration,\n}\n\nstatic SECRET_CACHE: OnceLock<RwLock<HashMap<String, CachedSecret>>> = OnceLock::new();\n\nasync fn get_secret_with_ttl(name: &str, ttl: Duration) -> Result<String, Error> {\n    let cache = SECRET_CACHE.get_or_init(|| RwLock::new(HashMap::new()));\n\n    // Check cache\n    {\n        let cache = cache.read().unwrap();\n        if let Some(cached) = cache.get(name) {\n            if cached.last_updated.elapsed() < cached.ttl {\n                return Ok(cached.value.clone());\n            }\n        }\n    }\n\n    // Fetch new value\n    let manager = Manager::new();\n    let value = manager.get_secret(name).await?;\n\n    // Update cache\n    {\n        let mut cache = cache.write().unwrap();\n        cache.insert(name.to_string(), CachedSecret {\n            value: value.clone(),\n            last_updated: Instant::now(),\n            ttl,\n        });\n    }\n\n    Ok(value)\n}\n```\n\n### 4. Validate Secrets Format\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum SecretError {\n    #[error(\"Invalid secret format: {0}\")]\n    InvalidFormat(String),\n\n    #[error(\"Missing required field: {0}\")]\n    MissingField(String),\n}\n\nfn validate_database_config(config: &DatabaseConfig) -> Result<(), SecretError> {\n    if config.host.is_empty() {\n        return Err(SecretError::MissingField(\"host\".to_string()));\n    }\n\n    if config.port == 0 {\n        return Err(SecretError::InvalidFormat(\"Port must be non-zero\".to_string()));\n    }\n\n    if config.password.len() < 12 {\n        return Err(SecretError::InvalidFormat(\n            \"Password must be at least 12 characters\".to_string()\n        ));\n    }\n\n    Ok(())\n}\n```\n\n## Creating Secrets\n\n### Via AWS CLI\n\n**Secrets Manager**:\n```bash\n# Simple string secret\naws secretsmanager create-secret \\\n  --name prod/database/password \\\n  --secret-string \"MySuperSecretPassword123!\"\n\n# JSON secret\naws secretsmanager create-secret \\\n  --name prod/database/config \\\n  --secret-string '{\n    \"host\": \"db.example.com\",\n    \"port\": 5432,\n    \"username\": \"app_user\",\n    \"password\": \"MySuperSecretPassword123!\",\n    \"database\": \"myapp\"\n  }'\n```\n\n**Parameter Store**:\n```bash\n# String parameter\naws ssm put-parameter \\\n  --name /myapp/api-url \\\n  --value \"https://api.example.com\" \\\n  --type String\n\n# SecureString parameter (encrypted)\naws ssm put-parameter \\\n  --name /myapp/api-key \\\n  --value \"sk_live_abc123\" \\\n  --type SecureString\n\n# With KMS key\naws ssm put-parameter \\\n  --name /myapp/encryption-key \\\n  --value \"my-encryption-key\" \\\n  --type SecureString \\\n  --key-id alias/myapp-key\n```\n\n### Via Terraform\n\n**Secrets Manager**:\n```hcl\nresource \"aws_secretsmanager_secret\" \"database_password\" {\n  name        = \"prod/database/password\"\n  description = \"Database password for production\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"database_password\" {\n  secret_id     = aws_secretsmanager_secret.database_password.id\n  secret_string = var.database_password  # From Terraform variables\n}\n\n# JSON secret\nresource \"aws_secretsmanager_secret\" \"database_config\" {\n  name = \"prod/database/config\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"database_config\" {\n  secret_id = aws_secretsmanager_secret.database_config.id\n  secret_string = jsonencode({\n    host     = \"db.example.com\"\n    port     = 5432\n    username = \"app_user\"\n    password = var.database_password\n    database = \"myapp\"\n  })\n}\n```\n\n**Parameter Store**:\n```hcl\nresource \"aws_ssm_parameter\" \"api_url\" {\n  name  = \"/myapp/api-url\"\n  type  = \"String\"\n  value = \"https://api.example.com\"\n}\n\nresource \"aws_ssm_parameter\" \"api_key\" {\n  name  = \"/myapp/api-key\"\n  type  = \"SecureString\"\n  value = var.api_key\n}\n```\n\n## Secrets Manager vs Parameter Store\n\n| Feature | Secrets Manager | Parameter Store |\n|---------|----------------|-----------------|\n| Cost | $0.40/secret/month + API calls | Free (Standard), $0.05/param/month (Advanced) |\n| Max size | 65 KB | 4 KB (Standard), 8 KB (Advanced) |\n| Rotation | Built-in | Manual |\n| Versioning | Yes | Yes |\n| Cross-account | Yes | Yes (Advanced) |\n| Best for | Passwords, API keys | Configuration, non-rotated secrets |\n\n## Complete Example\n\n```rust\nuse aws_parameters_and_secrets_lambda::Manager;\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse serde::Deserialize;\nuse std::sync::OnceLock;\nuse tracing::info;\n\n#[derive(Deserialize, Clone)]\nstruct AppConfig {\n    database: DatabaseConfig,\n    api_key: String,\n    feature_flags: FeatureFlags,\n}\n\n#[derive(Deserialize, Clone)]\nstruct DatabaseConfig {\n    host: String,\n    port: u16,\n    username: String,\n    password: String,\n    database: String,\n}\n\n#[derive(Deserialize, Clone)]\nstruct FeatureFlags {\n    new_feature_enabled: bool,\n    max_batch_size: usize,\n}\n\nstatic CONFIG: OnceLock<AppConfig> = OnceLock::new();\n\nasync fn load_config() -> Result<&'static AppConfig, Error> {\n    CONFIG.get_or_try_init(|| async {\n        let manager = Manager::new();\n        let env = std::env::var(\"ENVIRONMENT\")?;\n\n        // Get database config from Secrets Manager\n        let db_secret = manager\n            .get_secret(&format!(\"{}/database/config\", env))\n            .await?;\n        let database: DatabaseConfig = serde_json::from_str(&db_secret)?;\n\n        // Get API key from Parameter Store\n        let api_key = manager\n            .get_parameter(&format!(\"/{}/api-key\", env))\n            .await?;\n\n        // Get feature flags from Parameter Store\n        let flags_json = manager\n            .get_parameter(&format!(\"/{}/feature-flags\", env))\n            .await?;\n        let feature_flags: FeatureFlags = serde_json::from_str(&flags_json)?;\n\n        Ok(AppConfig {\n            database,\n            api_key,\n            feature_flags,\n        })\n    }).await\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt::init();\n\n    // Load configuration at startup\n    info!(\"Loading configuration...\");\n    load_config().await?;\n    info!(\"Configuration loaded successfully\");\n\n    run(service_fn(function_handler)).await\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let config = CONFIG.get().unwrap();\n\n    info!(\"Processing request with feature flags: {:?}\", config.feature_flags);\n\n    // Use configuration\n    let db = connect_to_database(&config.database).await?;\n    let api_client = ApiClient::new(&config.api_key);\n\n    // Your business logic here\n\n    Ok(Response { success: true })\n}\n```\n\n## Security Checklist\n\n- [ ] Use Secrets Manager for sensitive data (passwords, keys)\n- [ ] Use Parameter Store for configuration\n- [ ] Never log secret values\n- [ ] Use IAM policies to restrict access\n- [ ] Enable encryption at rest (KMS)\n- [ ] Use separate secrets per environment\n- [ ] Implement secret rotation\n- [ ] Validate secret format at startup\n- [ ] Cache secrets to reduce API calls\n- [ ] Use extension layer for production\n- [ ] Set appropriate TTL for cached secrets\n- [ ] Monitor secret access in CloudTrail\n- [ ] Use least privilege IAM permissions\n\nGuide the user through implementing secure secrets management appropriate for their needs."
              }
            ],
            "skills": [
              {
                "name": "async-sync-advisor",
                "description": "Guides users on choosing between async and sync patterns for Lambda functions, including when to use tokio, rayon, and spawn_blocking. Activates when users write Lambda handlers with mixed workloads.",
                "path": "plugins/rust-lambda/skills/async-sync-advisor/SKILL.md",
                "frontmatter": {
                  "name": "async-sync-advisor",
                  "description": "Guides users on choosing between async and sync patterns for Lambda functions, including when to use tokio, rayon, and spawn_blocking. Activates when users write Lambda handlers with mixed workloads.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Async/Sync Advisor Skill\n\nYou are an expert at choosing the right concurrency pattern for AWS Lambda in Rust. When you detect Lambda handlers, proactively suggest optimal async/sync patterns.\n\n## When to Activate\n\nActivate when you notice:\n- Lambda handlers with CPU-intensive operations\n- Mixed I/O and compute workloads\n- Use of `tokio::task::spawn_blocking` or `rayon`\n- Questions about async vs sync or performance\n\n## Decision Guide\n\n### Use Async For: I/O-Intensive Operations\n\n**When**:\n- HTTP/API calls\n- Database queries\n- S3/DynamoDB operations\n- Multiple independent I/O operations\n\n**Pattern**:\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // âœ… All I/O is async - perfect use case\n    let (user, profile, settings) = tokio::try_join!(\n        fetch_user(id),\n        fetch_profile(id),\n        fetch_settings(id),\n    )?;\n\n    Ok(Response { user, profile, settings })\n}\n```\n\n### Use Sync + spawn_blocking For: CPU-Intensive Operations\n\n**When**:\n- Data processing\n- Image/video manipulation\n- Encryption/hashing\n- Parsing large files\n\n**Pattern**:\n```rust\nuse tokio::task;\n\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let data = event.payload.data;\n\n    // âœ… Move CPU work to blocking thread pool\n    let result = task::spawn_blocking(move || {\n        // Synchronous CPU-intensive work\n        expensive_computation(&data)\n    })\n    .await??;\n\n    Ok(Response { result })\n}\n```\n\n### Use Rayon For: Parallel CPU Work\n\n**When**:\n- Processing large collections\n- Parallel data transformation\n- CPU-bound operations that can be parallelized\n\n**Pattern**:\n```rust\nuse rayon::prelude::*;\nuse tokio::task;\n\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let items = event.payload.items;\n\n    // âœ… Combine spawn_blocking with Rayon for parallel CPU work\n    let results = task::spawn_blocking(move || {\n        items\n            .par_iter()\n            .map(|item| cpu_intensive_work(item))\n            .collect::<Vec<_>>()\n    })\n    .await?;\n\n    Ok(Response { results })\n}\n```\n\n## Mixed Workload Pattern\n\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Phase 1: Async I/O - Download data\n    let download_futures = event.payload.urls\n        .into_iter()\n        .map(|url| async move {\n            reqwest::get(&url).await?.bytes().await\n        });\n    let raw_data = futures::future::try_join_all(download_futures).await?;\n\n    // Phase 2: Sync compute - Process with Rayon\n    let processed = task::spawn_blocking(move || {\n        raw_data\n            .par_iter()\n            .map(|bytes| process_data(bytes))\n            .collect::<Result<Vec<_>, _>>()\n    })\n    .await??;\n\n    // Phase 3: Async I/O - Upload results\n    let upload_futures = processed\n        .into_iter()\n        .enumerate()\n        .map(|(i, data)| async move {\n            upload_to_s3(&format!(\"result-{}.dat\", i), &data).await\n        });\n    futures::future::try_join_all(upload_futures).await?;\n\n    Ok(Response { success: true })\n}\n```\n\n## Common Mistakes\n\n### âŒ Using async for CPU work\n\n```rust\n// BAD: Async adds overhead for CPU-bound work\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let result = expensive_cpu_computation(&event.payload.data);  // Blocks async runtime\n    Ok(Response { result })\n}\n\n// GOOD: Use spawn_blocking\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let data = event.payload.data.clone();\n    let result = tokio::task::spawn_blocking(move || {\n        expensive_cpu_computation(&data)\n    })\n    .await?;\n    Ok(Response { result })\n}\n```\n\n### âŒ Not using concurrency for I/O\n\n```rust\n// BAD: Sequential I/O\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let user = fetch_user(id).await?;\n    let posts = fetch_posts(id).await?;  // Waits for user first\n    Ok(Response { user, posts })\n}\n\n// GOOD: Concurrent I/O\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let (user, posts) = tokio::try_join!(\n        fetch_user(id),\n        fetch_posts(id),\n    )?;\n    Ok(Response { user, posts })\n}\n```\n\n## Your Approach\n\nWhen you see Lambda handlers:\n1. Identify workload type (I/O vs CPU)\n2. Suggest appropriate pattern (async vs sync)\n3. Show how to combine patterns for mixed workloads\n4. Explain performance implications\n\nProactively suggest the optimal concurrency pattern for the workload."
              },
              {
                "name": "cold-start-optimizer",
                "description": "Provides guidance on reducing Lambda cold start times through binary optimization, lazy initialization, and deployment strategies. Activates when users discuss cold starts or deployment configuration.",
                "path": "plugins/rust-lambda/skills/cold-start-optimizer/SKILL.md",
                "frontmatter": {
                  "name": "cold-start-optimizer",
                  "description": "Provides guidance on reducing Lambda cold start times through binary optimization, lazy initialization, and deployment strategies. Activates when users discuss cold starts or deployment configuration.",
                  "allowed-tools": "Read, Grep",
                  "version": "1.0.0"
                },
                "content": "# Cold Start Optimizer Skill\n\nYou are an expert at optimizing AWS Lambda cold starts for Rust functions. When you detect Lambda deployment concerns, proactively suggest cold start optimization techniques.\n\n## When to Activate\n\nActivate when you notice:\n- Lambda deployment configurations\n- Questions about cold starts or initialization\n- Missing cargo.toml optimizations\n- Global state initialization patterns\n\n## Optimization Strategies\n\n### 1. Binary Size Reduction\n\n**Cargo.toml Configuration**:\n```toml\n[profile.release]\nopt-level = 'z'     # Optimize for size (vs 's' or 3)\nlto = true          # Link-time optimization\ncodegen-units = 1   # Single codegen unit for better optimization\nstrip = true        # Strip symbols from binary\npanic = 'abort'     # Smaller panic handler\n```\n\n**Impact**: Can reduce binary size by 50-70%, significantly improving cold start times.\n\n### 2. Lazy Initialization\n\n**Bad Pattern**:\n```rust\n// âŒ Initializes everything on cold start\nstatic HTTP_CLIENT: reqwest::Client = reqwest::Client::new();\nstatic DB_POOL: PgPool = create_pool().await;  // Won't even compile\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // Heavy initialization before handler is ready\n    tracing_subscriber::fmt().init();\n    init_aws_sdk().await;\n    warm_cache().await;\n\n    run(service_fn(handler)).await\n}\n```\n\n**Good Pattern**:\n```rust\nuse std::sync::OnceLock;\n\n// âœ… Lazy initialization - only creates when first used\nstatic HTTP_CLIENT: OnceLock<reqwest::Client> = OnceLock::new();\n\nfn get_client() -> &'static reqwest::Client {\n    HTTP_CLIENT.get_or_init(|| {\n        reqwest::Client::builder()\n            .timeout(Duration::from_secs(10))\n            .build()\n            .unwrap()\n    })\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // Minimal initialization\n    tracing_subscriber::fmt()\n        .without_time()\n        .init();\n\n    run(service_fn(handler)).await\n}\n```\n\n### 3. Dependency Optimization\n\n**Audit Dependencies**:\n```bash\ncargo tree\ncargo bloat --release\n```\n\n**Reduce Features**:\n```toml\n[dependencies]\n# âŒ BAD: Pulls in everything\ntokio = \"1\"\n\n# âœ… GOOD: Only what you need\ntokio = { version = \"1\", features = [\"rt-multi-thread\", \"macros\"] }\n\n# âœ… Disable default features when possible\nserde = { version = \"1\", default-features = false, features = [\"derive\"] }\n```\n\n### 4. ARM64 (Graviton2)\n\n**Build for ARM64**:\n```bash\ncargo lambda build --release --arm64\n```\n\n**Deploy with ARM64**:\n```bash\ncargo lambda deploy --memory 512 --arch arm64\n```\n\n**Benefits**:\n- 20% better price/performance\n- Often faster cold starts\n- Lower memory footprint\n\n### 5. Provisioned Concurrency\n\nFor critical functions with strict latency requirements:\n\n```bash\n# CloudFormation/SAM\nProvisionedConcurrencyConfig:\n  ProvisionedConcurrentExecutions: 2\n\n# Or via AWS CLI\naws lambda put-provisioned-concurrency-config \\\n  --function-name my-function \\\n  --provisioned-concurrent-executions 2\n```\n\n**Trade-off**: Costs more but eliminates cold starts.\n\n## Initialization Patterns\n\n### Pattern 1: OnceLock for Expensive Resources\n\n```rust\nuse std::sync::OnceLock;\n\nstatic AWS_CONFIG: OnceLock<aws_config::SdkConfig> = OnceLock::new();\nstatic S3_CLIENT: OnceLock<aws_sdk_s3::Client> = OnceLock::new();\n\nasync fn get_s3_client() -> &'static aws_sdk_s3::Client {\n    S3_CLIENT.get_or_init(|| {\n        let config = AWS_CONFIG.get_or_init(|| {\n            tokio::runtime::Handle::current()\n                .block_on(aws_config::load_from_env())\n        });\n        aws_sdk_s3::Client::new(config)\n    })\n}\n```\n\n### Pattern 2: Conditional Initialization\n\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // Only initialize if needed\n    let client = if event.payload.needs_api_call {\n        Some(get_http_client())\n    } else {\n        None\n    };\n\n    // Process without client if not needed\n    process(event.payload, client).await\n}\n```\n\n## Measurement and Monitoring\n\n### CloudWatch Insights Query\n\n```\nfilter @type = \"REPORT\"\n| stats avg(@initDuration), max(@initDuration), count(*) by bin(5m)\n```\n\n### Local Testing\n\n```bash\n# Measure binary size\nls -lh target/lambda/bootstrap/bootstrap.zip\n\n# Test cold start locally\ncargo lambda watch\ncargo lambda invoke --data-ascii '{\"test\": \"data\"}'\n```\n\n## Best Practices Checklist\n\n- [ ] Configure release profile for size optimization\n- [ ] Use lazy initialization with OnceLock\n- [ ] Minimize dependencies and features\n- [ ] Build for ARM64 (Graviton2)\n- [ ] Audit binary size with cargo bloat\n- [ ] Measure cold starts in CloudWatch\n- [ ] Use provisioned concurrency for critical paths\n- [ ] Keep initialization in main() minimal\n\n## Your Approach\n\nWhen you see Lambda deployment code:\n1. Check Cargo.toml for optimization settings\n2. Look for eager initialization that could be lazy\n3. Suggest ARM64 deployment\n4. Provide measurement strategies\n\nProactively suggest cold start optimizations when you detect Lambda configuration or initialization patterns."
              },
              {
                "name": "lambda-optimization-advisor",
                "description": "Reviews AWS Lambda functions for performance, memory configuration, and cost optimization. Activates when users write Lambda handlers or discuss Lambda performance.",
                "path": "plugins/rust-lambda/skills/lambda-optimization-advisor/SKILL.md",
                "frontmatter": {
                  "name": "lambda-optimization-advisor",
                  "description": "Reviews AWS Lambda functions for performance, memory configuration, and cost optimization. Activates when users write Lambda handlers or discuss Lambda performance.",
                  "allowed-tools": "Read, Grep, Glob",
                  "version": "1.0.0"
                },
                "content": "# Lambda Optimization Advisor Skill\n\nYou are an expert at optimizing AWS Lambda functions written in Rust. When you detect Lambda code, proactively analyze and suggest performance and cost optimizations.\n\n## When to Activate\n\nActivate when you notice:\n- Lambda handler functions using `lambda_runtime`\n- Sequential async operations that could be concurrent\n- Missing resource initialization patterns\n- Questions about Lambda performance or cold starts\n- Cargo.toml configurations for Lambda deployments\n\n## Optimization Checklist\n\n### 1. Concurrent Operations\n\n**What to Look For**: Sequential async operations\n\n**Bad Pattern**:\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // âŒ Sequential: takes 3+ seconds total\n    let user = fetch_user(&event.payload.user_id).await?;\n    let posts = fetch_posts(&event.payload.user_id).await?;\n    let comments = fetch_comments(&event.payload.user_id).await?;\n\n    Ok(Response { user, posts, comments })\n}\n```\n\n**Good Pattern**:\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // âœ… Concurrent: all three requests happen simultaneously\n    let (user, posts, comments) = tokio::try_join!(\n        fetch_user(&event.payload.user_id),\n        fetch_posts(&event.payload.user_id),\n        fetch_comments(&event.payload.user_id),\n    )?;\n\n    Ok(Response { user, posts, comments })\n}\n```\n\n**Suggestion**: Use `tokio::join!` or `tokio::try_join!` for concurrent operations. This can reduce execution time by 3-5x for I/O-bound workloads.\n\n### 2. Resource Initialization\n\n**What to Look For**: Creating clients inside the handler\n\n**Bad Pattern**:\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // âŒ Creates new client for every invocation\n    let client = reqwest::Client::new();\n    let data = client.get(\"https://api.example.com\").await?;\n    Ok(Response { data })\n}\n```\n\n**Good Pattern**:\n```rust\nuse std::sync::OnceLock;\n\n// âœ… Initialized once per container (reused across invocations)\nstatic HTTP_CLIENT: OnceLock<reqwest::Client> = OnceLock::new();\n\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    let client = HTTP_CLIENT.get_or_init(|| {\n        reqwest::Client::builder()\n            .timeout(Duration::from_secs(10))\n            .build()\n            .unwrap()\n    });\n\n    let data = client.get(\"https://api.example.com\").await?;\n    Ok(Response { data })\n}\n```\n\n**Suggestion**: Use `OnceLock` for expensive resources (HTTP clients, database pools, AWS SDK clients) that should be initialized once and reused.\n\n### 3. Binary Size Optimization\n\n**What to Look For**: Missing release profile optimizations\n\n**Check Cargo.toml**:\n```toml\n[profile.release]\nopt-level = 'z'     # âœ… Optimize for size\nlto = true          # âœ… Link-time optimization\ncodegen-units = 1   # âœ… Better optimization\nstrip = true        # âœ… Strip symbols\npanic = 'abort'     # âœ… Smaller panic handler\n```\n\n**Suggestion**: Configure release profile for smaller binaries. Smaller binaries = faster cold starts and lower storage costs.\n\n### 4. ARM64 (Graviton2) Usage\n\n**What to Look For**: Building for x86_64 only\n\n**Build Command**:\n```bash\n# âœ… Build for ARM64 (20% better price/performance)\ncargo lambda build --release --arm64\n```\n\n**Suggestion**: Use ARM64 for 20% better price/performance and often faster cold starts.\n\n### 5. Memory Configuration\n\n**What to Look For**: Default memory settings\n\n**Guidelines**:\n```bash\n# Test different memory configs\ncargo lambda deploy --memory 512   # For simple functions\ncargo lambda deploy --memory 1024  # For standard workloads\ncargo lambda deploy --memory 2048  # For CPU-intensive tasks\n```\n\n**Suggestion**: Lambda allocates CPU proportionally to memory. For CPU-bound tasks, increasing memory can reduce execution time and total cost.\n\n## Cost Optimization Patterns\n\n### Pattern 1: Batch Processing\n\n```rust\nasync fn handler(event: LambdaEvent<Vec<Item>>) -> Result<(), Error> {\n    // Process multiple items in one invocation\n    let futures = event.payload.iter().map(|item| process_item(item));\n    futures::future::try_join_all(futures).await?;\n    Ok(())\n}\n```\n\n### Pattern 2: Early Return\n\n```rust\nasync fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    // âœ… Validate early, fail fast\n    if event.payload.user_id.is_empty() {\n        return Err(Error::from(\"user_id required\"));\n    }\n\n    // Expensive operations only if validation passes\n    let user = fetch_user(&event.payload.user_id).await?;\n    Ok(Response { user })\n}\n```\n\n## Your Approach\n\n1. **Detect**: Identify Lambda handler code\n2. **Analyze**: Check for concurrent operations, resource init, config\n3. **Suggest**: Provide specific optimizations with code examples\n4. **Explain**: Impact on performance and cost\n\nProactively suggest optimizations that will reduce Lambda execution time and costs."
              }
            ]
          }
        ]
      }
    }
  ]
}