{
  "owner": {
    "id": "muratcankoylan",
    "display_name": "Muratcan Koylan",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/132029956?u=378061079a14997eb05a2d0769faa4884e3bc7a0&v=4",
    "url": "https://github.com/muratcankoylan",
    "bio": "AI Agent Systems Manager @ 99Ravens | Prompt design & context engineering, persona embodiment and multi-agent architectures.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 65,
      "total_stars": 6603,
      "total_forks": 526
    }
  },
  "repos": [
    {
      "full_name": "muratcankoylan/Agent-Skills-for-Context-Engineering",
      "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
      "description": "A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.",
      "homepage": null,
      "signals": {
        "stars": 6603,
        "forks": 526,
        "pushed_at": "2026-01-12T17:02:21Z",
        "created_at": "2025-12-21T02:43:42Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2180
        },
        {
          "path": ".cursorindexingignore",
          "type": "blob",
          "size": 110
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 534
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2444
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1103
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 12687
        },
        {
          "path": "SKILL.md",
          "type": "blob",
          "size": 7754
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/agentskills.md",
          "type": "blob",
          "size": 51608
        },
        {
          "path": "docs/blogs.md",
          "type": "blob",
          "size": 132777
        },
        {
          "path": "docs/claude_research.md",
          "type": "blob",
          "size": 15452
        },
        {
          "path": "docs/compression.md",
          "type": "blob",
          "size": 20447
        },
        {
          "path": "docs/gemini_research.md",
          "type": "blob",
          "size": 32317
        },
        {
          "path": "docs/hncapsule.md",
          "type": "blob",
          "size": 7853
        },
        {
          "path": "docs/netflix_context.md",
          "type": "blob",
          "size": 20579
        },
        {
          "path": "docs/vercel_tool.md",
          "type": "blob",
          "size": 7676
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/README.md",
          "type": "blob",
          "size": 2421
        },
        {
          "path": "examples/book-sft-pipeline/SKILL.md",
          "type": "blob",
          "size": 14252
        },
        {
          "path": "examples/book-sft-pipeline/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/README.md",
          "type": "blob",
          "size": 5460
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/dataset_sample.jsonl",
          "type": "blob",
          "size": 3968
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/pangram",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/pangram/Screenshot 2025-12-27 at 3.05.04 AM.png",
          "type": "blob",
          "size": 206831
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/pangram/Screenshot 2025-12-27 at 3.05.36 AM.png",
          "type": "blob",
          "size": 137345
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/pangram/Screenshot 2025-12-27 at 3.07.18 AM.png",
          "type": "blob",
          "size": 132416
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/sample_outputs.md",
          "type": "blob",
          "size": 3564
        },
        {
          "path": "examples/book-sft-pipeline/examples/gertrude-stein/training_config.json",
          "type": "blob",
          "size": 2814
        },
        {
          "path": "examples/book-sft-pipeline/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/references/segmentation-strategies.md",
          "type": "blob",
          "size": 10817
        },
        {
          "path": "examples/book-sft-pipeline/references/tinker-format.md",
          "type": "blob",
          "size": 6068
        },
        {
          "path": "examples/book-sft-pipeline/references/tinker.txt",
          "type": "blob",
          "size": 146019
        },
        {
          "path": "examples/book-sft-pipeline/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/book-sft-pipeline/scripts/pipeline_example.py",
          "type": "blob",
          "size": 6695
        },
        {
          "path": "examples/digital-brain-skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/.gitignore",
          "type": "blob",
          "size": 442
        },
        {
          "path": "examples/digital-brain-skill/AGENT.md",
          "type": "blob",
          "size": 1567
        },
        {
          "path": "examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md",
          "type": "blob",
          "size": 12982
        },
        {
          "path": "examples/digital-brain-skill/README.md",
          "type": "blob",
          "size": 7416
        },
        {
          "path": "examples/digital-brain-skill/SKILL.md",
          "type": "blob",
          "size": 7043
        },
        {
          "path": "examples/digital-brain-skill/SKILLS-MAPPING.md",
          "type": "blob",
          "size": 8848
        },
        {
          "path": "examples/digital-brain-skill/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/agents/AGENTS.md",
          "type": "blob",
          "size": 2599
        },
        {
          "path": "examples/digital-brain-skill/agents/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/agents/scripts/content_ideas.py",
          "type": "blob",
          "size": 4325
        },
        {
          "path": "examples/digital-brain-skill/agents/scripts/idea_to_draft.py",
          "type": "blob",
          "size": 4164
        },
        {
          "path": "examples/digital-brain-skill/agents/scripts/stale_contacts.py",
          "type": "blob",
          "size": 3942
        },
        {
          "path": "examples/digital-brain-skill/agents/scripts/weekly_review.py",
          "type": "blob",
          "size": 3224
        },
        {
          "path": "examples/digital-brain-skill/content",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/content/CONTENT.md",
          "type": "blob",
          "size": 2250
        },
        {
          "path": "examples/digital-brain-skill/content/calendar.md",
          "type": "blob",
          "size": 2096
        },
        {
          "path": "examples/digital-brain-skill/content/engagement.jsonl",
          "type": "blob",
          "size": 663
        },
        {
          "path": "examples/digital-brain-skill/content/ideas.jsonl",
          "type": "blob",
          "size": 486
        },
        {
          "path": "examples/digital-brain-skill/content/posts.jsonl",
          "type": "blob",
          "size": 627
        },
        {
          "path": "examples/digital-brain-skill/content/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/content/templates/linkedin-post.md",
          "type": "blob",
          "size": 1605
        },
        {
          "path": "examples/digital-brain-skill/content/templates/newsletter.md",
          "type": "blob",
          "size": 1580
        },
        {
          "path": "examples/digital-brain-skill/content/templates/thread.md",
          "type": "blob",
          "size": 1170
        },
        {
          "path": "examples/digital-brain-skill/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/examples/content-workflow.md",
          "type": "blob",
          "size": 4540
        },
        {
          "path": "examples/digital-brain-skill/examples/meeting-prep.md",
          "type": "blob",
          "size": 5763
        },
        {
          "path": "examples/digital-brain-skill/identity",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/identity/IDENTITY.md",
          "type": "blob",
          "size": 1457
        },
        {
          "path": "examples/digital-brain-skill/identity/bio-variants.md",
          "type": "blob",
          "size": 1696
        },
        {
          "path": "examples/digital-brain-skill/identity/brand.md",
          "type": "blob",
          "size": 3730
        },
        {
          "path": "examples/digital-brain-skill/identity/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/identity/prompts/content-generation.xml",
          "type": "blob",
          "size": 1439
        },
        {
          "path": "examples/digital-brain-skill/identity/prompts/reply-generator.xml",
          "type": "blob",
          "size": 1318
        },
        {
          "path": "examples/digital-brain-skill/identity/values.yaml",
          "type": "blob",
          "size": 1960
        },
        {
          "path": "examples/digital-brain-skill/identity/voice.md",
          "type": "blob",
          "size": 3397
        },
        {
          "path": "examples/digital-brain-skill/knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/knowledge/KNOWLEDGE.md",
          "type": "blob",
          "size": 2447
        },
        {
          "path": "examples/digital-brain-skill/knowledge/bookmarks.jsonl",
          "type": "blob",
          "size": 559
        },
        {
          "path": "examples/digital-brain-skill/knowledge/competitors.md",
          "type": "blob",
          "size": 2285
        },
        {
          "path": "examples/digital-brain-skill/knowledge/learning.yaml",
          "type": "blob",
          "size": 2120
        },
        {
          "path": "examples/digital-brain-skill/knowledge/research",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/knowledge/research/_template.md",
          "type": "blob",
          "size": 1285
        },
        {
          "path": "examples/digital-brain-skill/network",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/network/NETWORK.md",
          "type": "blob",
          "size": 3096
        },
        {
          "path": "examples/digital-brain-skill/network/circles.yaml",
          "type": "blob",
          "size": 2020
        },
        {
          "path": "examples/digital-brain-skill/network/contacts.jsonl",
          "type": "blob",
          "size": 897
        },
        {
          "path": "examples/digital-brain-skill/network/interactions.jsonl",
          "type": "blob",
          "size": 454
        },
        {
          "path": "examples/digital-brain-skill/network/intros.md",
          "type": "blob",
          "size": 2177
        },
        {
          "path": "examples/digital-brain-skill/operations",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/operations/OPERATIONS.md",
          "type": "blob",
          "size": 2058
        },
        {
          "path": "examples/digital-brain-skill/operations/goals.yaml",
          "type": "blob",
          "size": 2132
        },
        {
          "path": "examples/digital-brain-skill/operations/meetings.jsonl",
          "type": "blob",
          "size": 635
        },
        {
          "path": "examples/digital-brain-skill/operations/metrics.jsonl",
          "type": "blob",
          "size": 707
        },
        {
          "path": "examples/digital-brain-skill/operations/reviews",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/operations/reviews/_weekly_template.md",
          "type": "blob",
          "size": 1720
        },
        {
          "path": "examples/digital-brain-skill/operations/todos.md",
          "type": "blob",
          "size": 1293
        },
        {
          "path": "examples/digital-brain-skill/package.json",
          "type": "blob",
          "size": 1287
        },
        {
          "path": "examples/digital-brain-skill/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/references/file-formats.md",
          "type": "blob",
          "size": 7478
        },
        {
          "path": "examples/digital-brain-skill/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/digital-brain-skill/scripts/install.sh",
          "type": "blob",
          "size": 1993
        },
        {
          "path": "examples/interleaved_thinking",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/README.md",
          "type": "blob",
          "size": 20169
        },
        {
          "path": "examples/interleaved_thinking/SKILL.md",
          "type": "blob",
          "size": 6430
        },
        {
          "path": "examples/interleaved_thinking/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/docs/agentthinking.md",
          "type": "blob",
          "size": 6518
        },
        {
          "path": "examples/interleaved_thinking/docs/interleavedthinking.md",
          "type": "blob",
          "size": 30733
        },
        {
          "path": "examples/interleaved_thinking/docs/m2-1.md",
          "type": "blob",
          "size": 9539
        },
        {
          "path": "examples/interleaved_thinking/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/examples/01_basic_capture.py",
          "type": "blob",
          "size": 2293
        },
        {
          "path": "examples/interleaved_thinking/examples/02_tool_usage.py",
          "type": "blob",
          "size": 5879
        },
        {
          "path": "examples/interleaved_thinking/examples/03_full_optimization.py",
          "type": "blob",
          "size": 39972
        },
        {
          "path": "examples/interleaved_thinking/generated_skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/SKILL.md",
          "type": "blob",
          "size": 8515
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/references/optimization_summary.json",
          "type": "blob",
          "size": 899
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/references/optimized_prompt.txt",
          "type": "blob",
          "size": 81
        },
        {
          "path": "examples/interleaved_thinking/generated_skills/comprehensive-research-agent/references/patterns_found.json",
          "type": "blob",
          "size": 12732
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/final_prompt.txt",
          "type": "blob",
          "size": 3256
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_1",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_1/analysis.txt",
          "type": "blob",
          "size": 2829
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_1/optimization.txt",
          "type": "blob",
          "size": 485
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_1/optimized_prompt.txt",
          "type": "blob",
          "size": 81
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_1/trace.txt",
          "type": "blob",
          "size": 28468
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_10",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_10/analysis.txt",
          "type": "blob",
          "size": 3764
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_10/trace.txt",
          "type": "blob",
          "size": 28925
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_2",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_2/analysis.txt",
          "type": "blob",
          "size": 3455
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_2/optimization.txt",
          "type": "blob",
          "size": 6968
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_2/optimized_prompt.txt",
          "type": "blob",
          "size": 3529
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_2/trace.txt",
          "type": "blob",
          "size": 22300
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_3",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_3/analysis.txt",
          "type": "blob",
          "size": 3221
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_3/optimization.txt",
          "type": "blob",
          "size": 7888
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_3/optimized_prompt.txt",
          "type": "blob",
          "size": 4408
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_3/trace.txt",
          "type": "blob",
          "size": 28981
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_4",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_4/analysis.txt",
          "type": "blob",
          "size": 3062
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_4/optimization.txt",
          "type": "blob",
          "size": 6559
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_4/optimized_prompt.txt",
          "type": "blob",
          "size": 3256
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_4/trace.txt",
          "type": "blob",
          "size": 23901
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_5",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_5/analysis.txt",
          "type": "blob",
          "size": 3646
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_5/optimization.txt",
          "type": "blob",
          "size": 6426
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_5/optimized_prompt.txt",
          "type": "blob",
          "size": 3061
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_5/trace.txt",
          "type": "blob",
          "size": 31339
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_6",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_6/analysis.txt",
          "type": "blob",
          "size": 460
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_6/optimization.txt",
          "type": "blob",
          "size": 485
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_6/optimized_prompt.txt",
          "type": "blob",
          "size": 81
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_6/trace.txt",
          "type": "blob",
          "size": 19327
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_7",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_7/analysis.txt",
          "type": "blob",
          "size": 2659
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_7/optimization.txt",
          "type": "blob",
          "size": 5490
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_7/optimized_prompt.txt",
          "type": "blob",
          "size": 2613
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_7/trace.txt",
          "type": "blob",
          "size": 21242
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_8",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_8/analysis.txt",
          "type": "blob",
          "size": 3188
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_8/optimization.txt",
          "type": "blob",
          "size": 6468
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_8/optimized_prompt.txt",
          "type": "blob",
          "size": 3385
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_8/trace.txt",
          "type": "blob",
          "size": 22082
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_9",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_9/analysis.txt",
          "type": "blob",
          "size": 2768
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_9/optimization.txt",
          "type": "blob",
          "size": 5954
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_9/optimized_prompt.txt",
          "type": "blob",
          "size": 2673
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/iteration_9/trace.txt",
          "type": "blob",
          "size": 26158
        },
        {
          "path": "examples/interleaved_thinking/optimization_artifacts/summary.json",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "examples/interleaved_thinking/pyproject.toml",
          "type": "blob",
          "size": 1931
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/__init__.py",
          "type": "blob",
          "size": 1188
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/analyzer.py",
          "type": "blob",
          "size": 16141
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/capture.py",
          "type": "blob",
          "size": 14610
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/cli.py",
          "type": "blob",
          "size": 8612
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/loop.py",
          "type": "blob",
          "size": 18048
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/models.py",
          "type": "blob",
          "size": 4955
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/optimizer.py",
          "type": "blob",
          "size": 14558
        },
        {
          "path": "examples/interleaved_thinking/reasoning_trace_optimizer/skill_generator.py",
          "type": "blob",
          "size": 16396
        },
        {
          "path": "examples/interleaved_thinking/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/interleaved_thinking/tests/__init__.py",
          "type": "blob",
          "size": 43
        },
        {
          "path": "examples/interleaved_thinking/tests/test_models.py",
          "type": "blob",
          "size": 3936
        },
        {
          "path": "examples/llm-as-judge-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/.gitignore",
          "type": "blob",
          "size": 239
        },
        {
          "path": "examples/llm-as-judge-skills/.prettierrc",
          "type": "blob",
          "size": 107
        },
        {
          "path": "examples/llm-as-judge-skills/CONTRIBUTING.md",
          "type": "blob",
          "size": 1926
        },
        {
          "path": "examples/llm-as-judge-skills/LICENSE",
          "type": "blob",
          "size": 1072
        },
        {
          "path": "examples/llm-as-judge-skills/README.md",
          "type": "blob",
          "size": 23890
        },
        {
          "path": "examples/llm-as-judge-skills/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/evaluator-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/evaluator-agent/evaluator-agent.md",
          "type": "blob",
          "size": 4360
        },
        {
          "path": "examples/llm-as-judge-skills/agents/index.md",
          "type": "blob",
          "size": 2557
        },
        {
          "path": "examples/llm-as-judge-skills/agents/orchestrator-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/orchestrator-agent/orchestrator-agent.md",
          "type": "blob",
          "size": 4856
        },
        {
          "path": "examples/llm-as-judge-skills/agents/research-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/agents/research-agent/research-agent.md",
          "type": "blob",
          "size": 4442
        },
        {
          "path": "examples/llm-as-judge-skills/env.example",
          "type": "blob",
          "size": 177
        },
        {
          "path": "examples/llm-as-judge-skills/eslint.config.js",
          "type": "blob",
          "size": 465
        },
        {
          "path": "examples/llm-as-judge-skills/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/examples/basic-evaluation.ts",
          "type": "blob",
          "size": 2752
        },
        {
          "path": "examples/llm-as-judge-skills/examples/full-evaluation-workflow.ts",
          "type": "blob",
          "size": 4616
        },
        {
          "path": "examples/llm-as-judge-skills/examples/generate-rubric.ts",
          "type": "blob",
          "size": 2041
        },
        {
          "path": "examples/llm-as-judge-skills/examples/pairwise-comparison.ts",
          "type": "blob",
          "size": 3103
        },
        {
          "path": "examples/llm-as-judge-skills/package.json",
          "type": "blob",
          "size": 2105
        },
        {
          "path": "examples/llm-as-judge-skills/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/agent-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/agent-system/orchestrator-prompt.md",
          "type": "blob",
          "size": 5600
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/evaluation/direct-scoring-prompt.md",
          "type": "blob",
          "size": 3812
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/evaluation/pairwise-comparison-prompt.md",
          "type": "blob",
          "size": 4858
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/index.md",
          "type": "blob",
          "size": 3064
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/research",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/prompts/research/research-synthesis-prompt.md",
          "type": "blob",
          "size": 4169
        },
        {
          "path": "examples/llm-as-judge-skills/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/context-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/context-fundamentals/context-fundamentals.md",
          "type": "blob",
          "size": 3012
        },
        {
          "path": "examples/llm-as-judge-skills/skills/index.md",
          "type": "blob",
          "size": 2305
        },
        {
          "path": "examples/llm-as-judge-skills/skills/llm-evaluator",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/llm-evaluator/llm-evaluator.md",
          "type": "blob",
          "size": 2537
        },
        {
          "path": "examples/llm-as-judge-skills/skills/tool-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/skills/tool-design/tool-design.md",
          "type": "blob",
          "size": 4999
        },
        {
          "path": "examples/llm-as-judge-skills/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/src/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/src/agents/evaluator.ts",
          "type": "blob",
          "size": 2805
        },
        {
          "path": "examples/llm-as-judge-skills/src/agents/index.ts",
          "type": "blob",
          "size": 126
        },
        {
          "path": "examples/llm-as-judge-skills/src/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/src/config/index.ts",
          "type": "blob",
          "size": 409
        },
        {
          "path": "examples/llm-as-judge-skills/src/index.ts",
          "type": "blob",
          "size": 405
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools/evaluation/direct-score.ts",
          "type": "blob",
          "size": 5311
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools/evaluation/generate-rubric.ts",
          "type": "blob",
          "size": 4694
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools/evaluation/index.ts",
          "type": "blob",
          "size": 659
        },
        {
          "path": "examples/llm-as-judge-skills/src/tools/evaluation/pairwise-compare.ts",
          "type": "blob",
          "size": 7623
        },
        {
          "path": "examples/llm-as-judge-skills/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/tests/evaluation.test.ts",
          "type": "blob",
          "size": 7548
        },
        {
          "path": "examples/llm-as-judge-skills/tests/setup.ts",
          "type": "blob",
          "size": 560
        },
        {
          "path": "examples/llm-as-judge-skills/tests/skills.test.ts",
          "type": "blob",
          "size": 7605
        },
        {
          "path": "examples/llm-as-judge-skills/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/tools/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/tools/evaluation/direct-score.md",
          "type": "blob",
          "size": 4227
        },
        {
          "path": "examples/llm-as-judge-skills/tools/evaluation/generate-rubric.md",
          "type": "blob",
          "size": 5276
        },
        {
          "path": "examples/llm-as-judge-skills/tools/evaluation/pairwise-compare.md",
          "type": "blob",
          "size": 4677
        },
        {
          "path": "examples/llm-as-judge-skills/tools/index.md",
          "type": "blob",
          "size": 3575
        },
        {
          "path": "examples/llm-as-judge-skills/tools/orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/tools/orchestration/delegate-to-agent.md",
          "type": "blob",
          "size": 4430
        },
        {
          "path": "examples/llm-as-judge-skills/tools/research",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/llm-as-judge-skills/tools/research/read-url.md",
          "type": "blob",
          "size": 4060
        },
        {
          "path": "examples/llm-as-judge-skills/tools/research/web-search.md",
          "type": "blob",
          "size": 3275
        },
        {
          "path": "examples/llm-as-judge-skills/tsconfig.json",
          "type": "blob",
          "size": 655
        },
        {
          "path": "examples/llm-as-judge-skills/vitest.config.ts",
          "type": "blob",
          "size": 541
        },
        {
          "path": "examples/x-to-book-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/x-to-book-system/PRD.md",
          "type": "blob",
          "size": 24151
        },
        {
          "path": "examples/x-to-book-system/README.md",
          "type": "blob",
          "size": 10108
        },
        {
          "path": "examples/x-to-book-system/SKILLS-MAPPING.md",
          "type": "blob",
          "size": 10133
        },
        {
          "path": "researcher",
          "type": "tree",
          "size": null
        },
        {
          "path": "researcher/example_output.md",
          "type": "blob",
          "size": 8437
        },
        {
          "path": "researcher/llm-as-a-judge.md",
          "type": "blob",
          "size": 20867
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation/SKILL.md",
          "type": "blob",
          "size": 17586
        },
        {
          "path": "skills/advanced-evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation/references/bias-mitigation.md",
          "type": "blob",
          "size": 9078
        },
        {
          "path": "skills/advanced-evaluation/references/implementation-patterns.md",
          "type": "blob",
          "size": 9061
        },
        {
          "path": "skills/advanced-evaluation/references/metrics-guide.md",
          "type": "blob",
          "size": 9333
        },
        {
          "path": "skills/advanced-evaluation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/advanced-evaluation/scripts/evaluation_example.py",
          "type": "blob",
          "size": 11550
        },
        {
          "path": "skills/bdi-mental-states",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bdi-mental-states/SKILL.md",
          "type": "blob",
          "size": 10090
        },
        {
          "path": "skills/bdi-mental-states/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bdi-mental-states/references/bdi-ontology-core.md",
          "type": "blob",
          "size": 6482
        },
        {
          "path": "skills/bdi-mental-states/references/framework-integration.md",
          "type": "blob",
          "size": 20983
        },
        {
          "path": "skills/bdi-mental-states/references/rdf-examples.md",
          "type": "blob",
          "size": 11126
        },
        {
          "path": "skills/bdi-mental-states/references/sparql-competency.md",
          "type": "blob",
          "size": 9382
        },
        {
          "path": "skills/context-compression",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-compression/SKILL.md",
          "type": "blob",
          "size": 12344
        },
        {
          "path": "skills/context-compression/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-compression/references/evaluation-framework.md",
          "type": "blob",
          "size": 8449
        },
        {
          "path": "skills/context-compression/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-compression/scripts/compression_evaluator.py",
          "type": "blob",
          "size": 21759
        },
        {
          "path": "skills/context-degradation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-degradation/SKILL.md",
          "type": "blob",
          "size": 15582
        },
        {
          "path": "skills/context-degradation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-degradation/references/patterns.md",
          "type": "blob",
          "size": 9963
        },
        {
          "path": "skills/context-degradation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-degradation/scripts/degradation_detector.py",
          "type": "blob",
          "size": 14596
        },
        {
          "path": "skills/context-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-fundamentals/SKILL.md",
          "type": "blob",
          "size": 12137
        },
        {
          "path": "skills/context-fundamentals/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-fundamentals/references/context-components.md",
          "type": "blob",
          "size": 7970
        },
        {
          "path": "skills/context-fundamentals/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-fundamentals/scripts/context_manager.py",
          "type": "blob",
          "size": 11408
        },
        {
          "path": "skills/context-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-optimization/SKILL.md",
          "type": "blob",
          "size": 8461
        },
        {
          "path": "skills/context-optimization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-optimization/references/optimization_techniques.md",
          "type": "blob",
          "size": 9347
        },
        {
          "path": "skills/context-optimization/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-optimization/scripts/compaction.py",
          "type": "blob",
          "size": 12283
        },
        {
          "path": "skills/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/evaluation/SKILL.md",
          "type": "blob",
          "size": 10542
        },
        {
          "path": "skills/evaluation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/evaluation/references/metrics.md",
          "type": "blob",
          "size": 10139
        },
        {
          "path": "skills/evaluation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/evaluation/scripts/evaluator.py",
          "type": "blob",
          "size": 16060
        },
        {
          "path": "skills/filesystem-context",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/filesystem-context/SKILL.md",
          "type": "blob",
          "size": 13613
        },
        {
          "path": "skills/filesystem-context/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/filesystem-context/references/implementation-patterns.md",
          "type": "blob",
          "size": 18228
        },
        {
          "path": "skills/filesystem-context/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/filesystem-context/scripts/filesystem_context.py",
          "type": "blob",
          "size": 12203
        },
        {
          "path": "skills/hosted-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hosted-agents/SKILL.md",
          "type": "blob",
          "size": 11771
        },
        {
          "path": "skills/hosted-agents/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hosted-agents/references/infrastructure-patterns.md",
          "type": "blob",
          "size": 20449
        },
        {
          "path": "skills/hosted-agents/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hosted-agents/scripts/sandbox_manager.py",
          "type": "blob",
          "size": 15764
        },
        {
          "path": "skills/memory-systems",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/memory-systems/SKILL.md",
          "type": "blob",
          "size": 12872
        },
        {
          "path": "skills/memory-systems/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/memory-systems/references/implementation.md",
          "type": "blob",
          "size": 15419
        },
        {
          "path": "skills/memory-systems/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/memory-systems/scripts/memory_store.py",
          "type": "blob",
          "size": 13866
        },
        {
          "path": "skills/multi-agent-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-patterns/SKILL.md",
          "type": "blob",
          "size": 14650
        },
        {
          "path": "skills/multi-agent-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-patterns/references/frameworks.md",
          "type": "blob",
          "size": 12482
        },
        {
          "path": "skills/multi-agent-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-patterns/scripts/coordination.py",
          "type": "blob",
          "size": 14926
        },
        {
          "path": "skills/project-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/project-development/SKILL.md",
          "type": "blob",
          "size": 14841
        },
        {
          "path": "skills/project-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/project-development/references/case-studies.md",
          "type": "blob",
          "size": 14830
        },
        {
          "path": "skills/project-development/references/pipeline-patterns.md",
          "type": "blob",
          "size": 16921
        },
        {
          "path": "skills/project-development/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/project-development/scripts/pipeline_template.py",
          "type": "blob",
          "size": 20257
        },
        {
          "path": "skills/tool-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tool-design/SKILL.md",
          "type": "blob",
          "size": 15496
        },
        {
          "path": "skills/tool-design/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tool-design/references/architectural_reduction.md",
          "type": "blob",
          "size": 8138
        },
        {
          "path": "skills/tool-design/references/best_practices.md",
          "type": "blob",
          "size": 10510
        },
        {
          "path": "skills/tool-design/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tool-design/scripts/description_generator.py",
          "type": "blob",
          "size": 7080
        },
        {
          "path": "template",
          "type": "tree",
          "size": null
        },
        {
          "path": "template/SKILL.md",
          "type": "blob",
          "size": 3651
        }
      ],
      "marketplace": {
        "name": "context-engineering-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Muratcan Koylan",
          "email": "muratcan.koylan@outlook.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "context-engineering-fundamentals",
            "description": "Core context engineering skills covering fundamentals, degradation patterns, compression strategies, and optimization techniques for AI agent systems",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
              "/plugin install context-engineering-fundamentals@context-engineering-marketplace"
            ],
            "signals": {
              "stars": 6603,
              "forks": 526,
              "pushed_at": "2026-01-12T17:02:21Z",
              "created_at": "2025-12-21T02:43:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-evaluation",
                "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
                "path": "skills/advanced-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "advanced-evaluation",
                  "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment."
                },
                "content": "# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0"
              },
              {
                "name": "bdi-mental-states",
                "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.",
                "path": "skills/bdi-mental-states/SKILL.md",
                "frontmatter": {
                  "name": "bdi-mental-states",
                  "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration."
                },
                "content": "# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)"
              },
              {
                "name": "context-compression",
                "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
                "path": "skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "context-degradation",
                "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
                "path": "skills/context-degradation/SKILL.md",
                "frontmatter": {
                  "name": "context-degradation",
                  "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures."
                },
                "content": "# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-fundamentals",
                "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.",
                "path": "skills/context-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "context-fundamentals",
                  "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems."
                },
                "content": "# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-optimization",
                "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.",
                "path": "skills/context-optimization/SKILL.md",
                "frontmatter": {
                  "name": "context-optimization",
                  "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity."
                },
                "content": "# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "evaluation",
                "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
                "path": "skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "filesystem-context",
                "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.",
                "path": "skills/filesystem-context/SKILL.md",
                "frontmatter": {
                  "name": "filesystem-context",
                  "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading."
                },
                "content": "# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "hosted-agents",
                "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.",
                "path": "skills/hosted-agents/SKILL.md",
                "frontmatter": {
                  "name": "hosted-agents",
                  "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments."
                },
                "content": "# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "memory-systems",
                "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.",
                "path": "skills/memory-systems/SKILL.md",
                "frontmatter": {
                  "name": "memory-systems",
                  "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence."
                },
                "content": "# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "multi-agent-patterns",
                "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.",
                "path": "skills/multi-agent-patterns/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-patterns",
                  "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution."
                },
                "content": "# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
                "path": "skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.",
                "path": "skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          },
          {
            "name": "agent-architecture",
            "description": "Multi-agent patterns, memory systems, tool design, filesystem-based context, and hosted agent infrastructure for building production AI agent architectures",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
              "/plugin install agent-architecture@context-engineering-marketplace"
            ],
            "signals": {
              "stars": 6603,
              "forks": 526,
              "pushed_at": "2026-01-12T17:02:21Z",
              "created_at": "2025-12-21T02:43:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-evaluation",
                "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
                "path": "skills/advanced-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "advanced-evaluation",
                  "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment."
                },
                "content": "# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0"
              },
              {
                "name": "bdi-mental-states",
                "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.",
                "path": "skills/bdi-mental-states/SKILL.md",
                "frontmatter": {
                  "name": "bdi-mental-states",
                  "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration."
                },
                "content": "# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)"
              },
              {
                "name": "context-compression",
                "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
                "path": "skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "context-degradation",
                "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
                "path": "skills/context-degradation/SKILL.md",
                "frontmatter": {
                  "name": "context-degradation",
                  "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures."
                },
                "content": "# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-fundamentals",
                "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.",
                "path": "skills/context-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "context-fundamentals",
                  "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems."
                },
                "content": "# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-optimization",
                "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.",
                "path": "skills/context-optimization/SKILL.md",
                "frontmatter": {
                  "name": "context-optimization",
                  "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity."
                },
                "content": "# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "evaluation",
                "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
                "path": "skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "filesystem-context",
                "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.",
                "path": "skills/filesystem-context/SKILL.md",
                "frontmatter": {
                  "name": "filesystem-context",
                  "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading."
                },
                "content": "# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "hosted-agents",
                "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.",
                "path": "skills/hosted-agents/SKILL.md",
                "frontmatter": {
                  "name": "hosted-agents",
                  "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments."
                },
                "content": "# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "memory-systems",
                "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.",
                "path": "skills/memory-systems/SKILL.md",
                "frontmatter": {
                  "name": "memory-systems",
                  "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence."
                },
                "content": "# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "multi-agent-patterns",
                "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.",
                "path": "skills/multi-agent-patterns/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-patterns",
                  "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution."
                },
                "content": "# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
                "path": "skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.",
                "path": "skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          },
          {
            "name": "agent-evaluation",
            "description": "Evaluation frameworks and LLM-as-judge techniques for testing and validating AI agent systems",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
              "/plugin install agent-evaluation@context-engineering-marketplace"
            ],
            "signals": {
              "stars": 6603,
              "forks": 526,
              "pushed_at": "2026-01-12T17:02:21Z",
              "created_at": "2025-12-21T02:43:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-evaluation",
                "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
                "path": "skills/advanced-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "advanced-evaluation",
                  "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment."
                },
                "content": "# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0"
              },
              {
                "name": "bdi-mental-states",
                "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.",
                "path": "skills/bdi-mental-states/SKILL.md",
                "frontmatter": {
                  "name": "bdi-mental-states",
                  "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration."
                },
                "content": "# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)"
              },
              {
                "name": "context-compression",
                "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
                "path": "skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "context-degradation",
                "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
                "path": "skills/context-degradation/SKILL.md",
                "frontmatter": {
                  "name": "context-degradation",
                  "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures."
                },
                "content": "# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-fundamentals",
                "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.",
                "path": "skills/context-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "context-fundamentals",
                  "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems."
                },
                "content": "# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-optimization",
                "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.",
                "path": "skills/context-optimization/SKILL.md",
                "frontmatter": {
                  "name": "context-optimization",
                  "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity."
                },
                "content": "# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "evaluation",
                "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
                "path": "skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "filesystem-context",
                "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.",
                "path": "skills/filesystem-context/SKILL.md",
                "frontmatter": {
                  "name": "filesystem-context",
                  "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading."
                },
                "content": "# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "hosted-agents",
                "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.",
                "path": "skills/hosted-agents/SKILL.md",
                "frontmatter": {
                  "name": "hosted-agents",
                  "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments."
                },
                "content": "# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "memory-systems",
                "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.",
                "path": "skills/memory-systems/SKILL.md",
                "frontmatter": {
                  "name": "memory-systems",
                  "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence."
                },
                "content": "# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "multi-agent-patterns",
                "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.",
                "path": "skills/multi-agent-patterns/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-patterns",
                  "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution."
                },
                "content": "# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
                "path": "skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.",
                "path": "skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          },
          {
            "name": "agent-development",
            "description": "Project development methodology for LLM-powered applications including pipeline architecture and batch processing",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
              "/plugin install agent-development@context-engineering-marketplace"
            ],
            "signals": {
              "stars": 6603,
              "forks": 526,
              "pushed_at": "2026-01-12T17:02:21Z",
              "created_at": "2025-12-21T02:43:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-evaluation",
                "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
                "path": "skills/advanced-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "advanced-evaluation",
                  "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment."
                },
                "content": "# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0"
              },
              {
                "name": "bdi-mental-states",
                "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.",
                "path": "skills/bdi-mental-states/SKILL.md",
                "frontmatter": {
                  "name": "bdi-mental-states",
                  "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration."
                },
                "content": "# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)"
              },
              {
                "name": "context-compression",
                "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
                "path": "skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "context-degradation",
                "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
                "path": "skills/context-degradation/SKILL.md",
                "frontmatter": {
                  "name": "context-degradation",
                  "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures."
                },
                "content": "# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-fundamentals",
                "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.",
                "path": "skills/context-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "context-fundamentals",
                  "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems."
                },
                "content": "# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-optimization",
                "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.",
                "path": "skills/context-optimization/SKILL.md",
                "frontmatter": {
                  "name": "context-optimization",
                  "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity."
                },
                "content": "# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "evaluation",
                "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
                "path": "skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "filesystem-context",
                "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.",
                "path": "skills/filesystem-context/SKILL.md",
                "frontmatter": {
                  "name": "filesystem-context",
                  "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading."
                },
                "content": "# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "hosted-agents",
                "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.",
                "path": "skills/hosted-agents/SKILL.md",
                "frontmatter": {
                  "name": "hosted-agents",
                  "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments."
                },
                "content": "# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "memory-systems",
                "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.",
                "path": "skills/memory-systems/SKILL.md",
                "frontmatter": {
                  "name": "memory-systems",
                  "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence."
                },
                "content": "# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "multi-agent-patterns",
                "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.",
                "path": "skills/multi-agent-patterns/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-patterns",
                  "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution."
                },
                "content": "# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
                "path": "skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.",
                "path": "skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          },
          {
            "name": "cognitive-architecture",
            "description": "BDI mental state modeling and cognitive architecture patterns for building rational agents with formal belief-desire-intention representations",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering",
              "/plugin install cognitive-architecture@context-engineering-marketplace"
            ],
            "signals": {
              "stars": 6603,
              "forks": 526,
              "pushed_at": "2026-01-12T17:02:21Z",
              "created_at": "2025-12-21T02:43:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "advanced-evaluation",
                "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
                "path": "skills/advanced-evaluation/SKILL.md",
                "frontmatter": {
                  "name": "advanced-evaluation",
                  "description": "This skill should be used when the user asks to \"implement LLM-as-judge\", \"compare model outputs\", \"create evaluation rubrics\", \"mitigate evaluation bias\", or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment."
                },
                "content": "# Advanced Evaluation\n\nThis skill covers production-grade techniques for evaluating LLM outputs using LLMs as judges. It synthesizes research from academic papers, industry practices, and practical implementation experience into actionable patterns for building reliable evaluation systems.\n\n**Key insight**: LLM-as-a-Judge is not a single technique but a family of approaches, each suited to different evaluation contexts. Choosing the right approach and mitigating known biases is the core competency this skill develops.\n\n## When to Activate\n\nActivate this skill when:\n\n- Building automated evaluation pipelines for LLM outputs\n- Comparing multiple model responses to select the best one\n- Establishing consistent quality standards across evaluation teams\n- Debugging evaluation systems that show inconsistent results\n- Designing A/B tests for prompt or model changes\n- Creating rubrics for human or automated evaluation\n- Analyzing correlation between automated and human judgments\n\n## Core Concepts\n\n### The Evaluation Taxonomy\n\nEvaluation approaches fall into two primary categories with distinct reliability profiles:\n\n**Direct Scoring**: A single LLM rates one response on a defined scale.\n- Best for: Objective criteria (factual accuracy, instruction following, toxicity)\n- Reliability: Moderate to high for well-defined criteria\n- Failure mode: Score calibration drift, inconsistent scale interpretation\n\n**Pairwise Comparison**: An LLM compares two responses and selects the better one.\n- Best for: Subjective preferences (tone, style, persuasiveness)\n- Reliability: Higher than direct scoring for preferences\n- Failure mode: Position bias, length bias\n\nResearch from the MT-Bench paper (Zheng et al., 2023) establishes that pairwise comparison achieves higher agreement with human judges than direct scoring for preference-based evaluation, while direct scoring remains appropriate for objective criteria with clear ground truth.\n\n### The Bias Landscape\n\nLLM judges exhibit systematic biases that must be actively mitigated:\n\n**Position Bias**: First-position responses receive preferential treatment in pairwise comparison. Mitigation: Evaluate twice with swapped positions, use majority vote or consistency check.\n\n**Length Bias**: Longer responses are rated higher regardless of quality. Mitigation: Explicit prompting to ignore length, length-normalized scoring.\n\n**Self-Enhancement Bias**: Models rate their own outputs higher. Mitigation: Use different models for generation and evaluation, or acknowledge limitation.\n\n**Verbosity Bias**: Detailed explanations receive higher scores even when unnecessary. Mitigation: Criteria-specific rubrics that penalize irrelevant detail.\n\n**Authority Bias**: Confident, authoritative tone rated higher regardless of accuracy. Mitigation: Require evidence citation, fact-checking layer.\n\n### Metric Selection Framework\n\nChoose metrics based on the evaluation task structure:\n\n| Task Type | Primary Metrics | Secondary Metrics |\n|-----------|-----------------|-------------------|\n| Binary classification (pass/fail) | Recall, Precision, F1 | Cohen's κ |\n| Ordinal scale (1-5 rating) | Spearman's ρ, Kendall's τ | Cohen's κ (weighted) |\n| Pairwise preference | Agreement rate, Position consistency | Confidence calibration |\n| Multi-label | Macro-F1, Micro-F1 | Per-label precision/recall |\n\nThe critical insight: High absolute agreement matters less than systematic disagreement patterns. A judge that consistently disagrees with humans on specific criteria is more problematic than one with random noise.\n\n## Evaluation Approaches\n\n### Direct Scoring Implementation\n\nDirect scoring requires three components: clear criteria, a calibrated scale, and structured output format.\n\n**Criteria Definition Pattern**:\n```\nCriterion: [Name]\nDescription: [What this criterion measures]\nWeight: [Relative importance, 0-1]\n```\n\n**Scale Calibration**:\n- 1-3 scales: Binary with neutral option, lowest cognitive load\n- 1-5 scales: Standard Likert, good balance of granularity and reliability\n- 1-10 scales: High granularity but harder to calibrate, use only with detailed rubrics\n\n**Prompt Structure for Direct Scoring**:\n```\nYou are an expert evaluator assessing response quality.\n\n## Task\nEvaluate the following response against each criterion.\n\n## Original Prompt\n{prompt}\n\n## Response to Evaluate\n{response}\n\n## Criteria\n{for each criterion: name, description, weight}\n\n## Instructions\nFor each criterion:\n1. Find specific evidence in the response\n2. Score according to the rubric (1-{max} scale)\n3. Justify your score with evidence\n4. Suggest one specific improvement\n\n## Output Format\nRespond with structured JSON containing scores, justifications, and summary.\n```\n\n**Chain-of-Thought Requirement**: All scoring prompts must require justification before the score. Research shows this improves reliability by 15-25% compared to score-first approaches.\n\n### Pairwise Comparison Implementation\n\nPairwise comparison is inherently more reliable for preference-based evaluation but requires bias mitigation.\n\n**Position Bias Mitigation Protocol**:\n1. First pass: Response A in first position, Response B in second\n2. Second pass: Response B in first position, Response A in second\n3. Consistency check: If passes disagree, return TIE with reduced confidence\n4. Final verdict: Consistent winner with averaged confidence\n\n**Prompt Structure for Pairwise Comparison**:\n```\nYou are an expert evaluator comparing two AI responses.\n\n## Critical Instructions\n- Do NOT prefer responses because they are longer\n- Do NOT prefer responses based on position (first vs second)\n- Focus ONLY on quality according to the specified criteria\n- Ties are acceptable when responses are genuinely equivalent\n\n## Original Prompt\n{prompt}\n\n## Response A\n{response_a}\n\n## Response B\n{response_b}\n\n## Comparison Criteria\n{criteria list}\n\n## Instructions\n1. Analyze each response independently first\n2. Compare them on each criterion\n3. Determine overall winner with confidence level\n\n## Output Format\nJSON with per-criterion comparison, overall winner, confidence (0-1), and reasoning.\n```\n\n**Confidence Calibration**: Confidence scores should reflect position consistency:\n- Both passes agree: confidence = average of individual confidences\n- Passes disagree: confidence = 0.5, verdict = TIE\n\n### Rubric Generation\n\nWell-defined rubrics reduce evaluation variance by 40-60% compared to open-ended scoring.\n\n**Rubric Components**:\n1. **Level descriptions**: Clear boundaries for each score level\n2. **Characteristics**: Observable features that define each level\n3. **Examples**: Representative text for each level (optional but valuable)\n4. **Edge cases**: Guidance for ambiguous situations\n5. **Scoring guidelines**: General principles for consistent application\n\n**Strictness Calibration**:\n- **Lenient**: Lower bar for passing scores, appropriate for encouraging iteration\n- **Balanced**: Fair, typical expectations for production use\n- **Strict**: High standards, appropriate for safety-critical or high-stakes evaluation\n\n**Domain Adaptation**: Rubrics should use domain-specific terminology. A \"code readability\" rubric mentions variables, functions, and comments. A \"medical accuracy\" rubric references clinical terminology and evidence standards.\n\n## Practical Guidance\n\n### Evaluation Pipeline Design\n\nProduction evaluation systems require multiple layers:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 Evaluation Pipeline              │\n├─────────────────────────────────────────────────┤\n│                                                   │\n│  Input: Response + Prompt + Context               │\n│           │                                       │\n│           ▼                                       │\n│  ┌─────────────────────┐                         │\n│  │   Criteria Loader   │ ◄── Rubrics, weights    │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Primary Scorer    │ ◄── Direct or Pairwise  │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │   Bias Mitigation   │ ◄── Position swap, etc. │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  ┌─────────────────────┐                         │\n│  │ Confidence Scoring  │ ◄── Calibration         │\n│  └──────────┬──────────┘                         │\n│             │                                     │\n│             ▼                                     │\n│  Output: Scores + Justifications + Confidence     │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Common Anti-Patterns\n\n**Anti-pattern: Scoring without justification**\n- Problem: Scores lack grounding, difficult to debug or improve\n- Solution: Always require evidence-based justification before score\n\n**Anti-pattern: Single-pass pairwise comparison**\n- Problem: Position bias corrupts results\n- Solution: Always swap positions and check consistency\n\n**Anti-pattern: Overloaded criteria**\n- Problem: Criteria measuring multiple things are unreliable\n- Solution: One criterion = one measurable aspect\n\n**Anti-pattern: Missing edge case guidance**\n- Problem: Evaluators handle ambiguous cases inconsistently\n- Solution: Include edge cases in rubrics with explicit guidance\n\n**Anti-pattern: Ignoring confidence calibration**\n- Problem: High-confidence wrong judgments are worse than low-confidence\n- Solution: Calibrate confidence to position consistency and evidence strength\n\n### Decision Framework: Direct vs. Pairwise\n\nUse this decision tree:\n\n```\nIs there an objective ground truth?\n├── Yes → Direct Scoring\n│   └── Examples: factual accuracy, instruction following, format compliance\n│\n└── No → Is it a preference or quality judgment?\n    ├── Yes → Pairwise Comparison\n    │   └── Examples: tone, style, persuasiveness, creativity\n    │\n    └── No → Consider reference-based evaluation\n        └── Examples: summarization (compare to source), translation (compare to reference)\n```\n\n### Scaling Evaluation\n\nFor high-volume evaluation:\n\n1. **Panel of LLMs (PoLL)**: Use multiple models as judges, aggregate votes\n   - Reduces individual model bias\n   - More expensive but more reliable for high-stakes decisions\n\n2. **Hierarchical evaluation**: Fast cheap model for screening, expensive model for edge cases\n   - Cost-effective for large volumes\n   - Requires calibration of screening threshold\n\n3. **Human-in-the-loop**: Automated evaluation for clear cases, human review for low-confidence\n   - Best reliability for critical applications\n   - Design feedback loop to improve automated evaluation\n\n## Examples\n\n### Example 1: Direct Scoring for Accuracy\n\n**Input**:\n```\nPrompt: \"What causes seasons on Earth?\"\nResponse: \"Seasons are caused by Earth's tilted axis. As Earth orbits the Sun, \ndifferent hemispheres receive more direct sunlight at different times of year.\"\nCriterion: Factual Accuracy (weight: 1.0)\nScale: 1-5\n```\n\n**Output**:\n```json\n{\n  \"criterion\": \"Factual Accuracy\",\n  \"score\": 5,\n  \"evidence\": [\n    \"Correctly identifies axial tilt as primary cause\",\n    \"Correctly explains differential sunlight by hemisphere\",\n    \"No factual errors present\"\n  ],\n  \"justification\": \"Response accurately explains the cause of seasons with correct \nscientific reasoning. Both the axial tilt and its effect on sunlight distribution \nare correctly described.\",\n  \"improvement\": \"Could add the specific tilt angle (23.5°) for completeness.\"\n}\n```\n\n### Example 2: Pairwise Comparison with Position Swap\n\n**Input**:\n```\nPrompt: \"Explain machine learning to a beginner\"\nResponse A: [Technical explanation with jargon]\nResponse B: [Simple analogy-based explanation]\nCriteria: [\"clarity\", \"accessibility\"]\n```\n\n**First Pass (A first)**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.8 }\n```\n\n**Second Pass (B first)**:\n```json\n{ \"winner\": \"A\", \"confidence\": 0.6 }\n```\n(Note: Winner is A because B was in first position)\n\n**Mapped Second Pass**:\n```json\n{ \"winner\": \"B\", \"confidence\": 0.6 }\n```\n\n**Final Result**:\n```json\n{\n  \"winner\": \"B\",\n  \"confidence\": 0.7,\n  \"positionConsistency\": {\n    \"consistent\": true,\n    \"firstPassWinner\": \"B\",\n    \"secondPassWinner\": \"B\"\n  }\n}\n```\n\n### Example 3: Rubric Generation\n\n**Input**:\n```\ncriterionName: \"Code Readability\"\ncriterionDescription: \"How easy the code is to understand and maintain\"\ndomain: \"software engineering\"\nscale: \"1-5\"\nstrictness: \"balanced\"\n```\n\n**Output** (abbreviated):\n```json\n{\n  \"levels\": [\n    {\n      \"score\": 1,\n      \"label\": \"Poor\",\n      \"description\": \"Code is difficult to understand without significant effort\",\n      \"characteristics\": [\n        \"No meaningful variable or function names\",\n        \"No comments or documentation\",\n        \"Deeply nested or convoluted logic\"\n      ]\n    },\n    {\n      \"score\": 3,\n      \"label\": \"Adequate\",\n      \"description\": \"Code is understandable with some effort\",\n      \"characteristics\": [\n        \"Most variables have meaningful names\",\n        \"Basic comments present for complex sections\",\n        \"Logic is followable but could be cleaner\"\n      ]\n    },\n    {\n      \"score\": 5,\n      \"label\": \"Excellent\",\n      \"description\": \"Code is immediately clear and maintainable\",\n      \"characteristics\": [\n        \"All names are descriptive and consistent\",\n        \"Comprehensive documentation\",\n        \"Clean, modular structure\"\n      ]\n    }\n  ],\n  \"edgeCases\": [\n    {\n      \"situation\": \"Code is well-structured but uses domain-specific abbreviations\",\n      \"guidance\": \"Score based on readability for domain experts, not general audience\"\n    }\n  ]\n}\n```\n\n## Guidelines\n\n1. **Always require justification before scores** - Chain-of-thought prompting improves reliability by 15-25%\n\n2. **Always swap positions in pairwise comparison** - Single-pass comparison is corrupted by position bias\n\n3. **Match scale granularity to rubric specificity** - Don't use 1-10 without detailed level descriptions\n\n4. **Separate objective and subjective criteria** - Use direct scoring for objective, pairwise for subjective\n\n5. **Include confidence scores** - Calibrate to position consistency and evidence strength\n\n6. **Define edge cases explicitly** - Ambiguous situations cause the most evaluation variance\n\n7. **Use domain-specific rubrics** - Generic rubrics produce generic (less useful) evaluations\n\n8. **Validate against human judgments** - Automated evaluation is only valuable if it correlates with human assessment\n\n9. **Monitor for systematic bias** - Track disagreement patterns by criterion, response type, model\n\n10. **Design for iteration** - Evaluation systems improve with feedback loops\n\n## Integration\n\nThis skill integrates with:\n\n- **context-fundamentals** - Evaluation prompts require effective context structure\n- **tool-design** - Evaluation tools need proper schemas and error handling\n- **context-optimization** - Evaluation prompts can be optimized for token efficiency\n- **evaluation** (foundational) - This skill extends the foundational evaluation concepts\n\n## References\n\nInternal reference:\n- [LLM-as-Judge Implementation Patterns](./references/implementation-patterns.md)\n- [Bias Mitigation Techniques](./references/bias-mitigation.md)\n- [Metric Selection Guide](./references/metrics-guide.md)\n\nExternal research:\n- [Eugene Yan: Evaluating the Effectiveness of LLM-Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n- [Judging LLM-as-a-Judge (Zheng et al., 2023)](https://arxiv.org/abs/2306.05685)\n- [G-Eval: NLG Evaluation using GPT-4 (Liu et al., 2023)](https://arxiv.org/abs/2303.16634)\n- [Large Language Models are not Fair Evaluators (Wang et al., 2023)](https://arxiv.org/abs/2305.17926)\n\nRelated skills in this collection:\n- evaluation - Foundational evaluation concepts\n- context-fundamentals - Context structure for evaluation prompts\n- tool-design - Building evaluation tools\n\n---\n\n## Skill Metadata\n\n**Created**: 2024-12-24\n**Last Updated**: 2024-12-24\n**Author**: Muratcan Koylan\n**Version**: 1.0.0"
              },
              {
                "name": "bdi-mental-states",
                "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration.",
                "path": "skills/bdi-mental-states/SKILL.md",
                "frontmatter": {
                  "name": "bdi-mental-states",
                  "description": "This skill should be used when the user asks to \"model agent mental states\", \"implement BDI architecture\", \"create belief-desire-intention models\", \"transform RDF to beliefs\", \"build cognitive agent\", or mentions BDI ontology, mental state modeling, rational agency, or neuro-symbolic AI integration."
                },
                "content": "# BDI Mental State Modeling\n\nTransform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns. This skill enables agents to reason about context through cognitive architecture, supporting deliberative reasoning, explainability, and semantic interoperability within multi-agent systems.\n\n## When to Activate\n\nActivate this skill when:\n- Processing external RDF context into agent beliefs about world states\n- Modeling rational agency with perception, deliberation, and action cycles\n- Enabling explainability through traceable reasoning chains\n- Implementing BDI frameworks (SEMAS, JADE, JADEX)\n- Augmenting LLMs with formal cognitive structures (Logic Augmented Generation)\n- Coordinating mental states across multi-agent platforms\n- Tracking temporal evolution of beliefs, desires, and intentions\n- Linking motivational states to action plans\n\n## Core Concepts\n\n### Mental Reality Architecture\n\n**Mental States (Endurants)**: Persistent cognitive attributes\n- `Belief`: What the agent believes to be true about the world\n- `Desire`: What the agent wishes to bring about\n- `Intention`: What the agent commits to achieving\n\n**Mental Processes (Perdurants)**: Events that modify mental states\n- `BeliefProcess`: Forming/updating beliefs from perception\n- `DesireProcess`: Generating desires from beliefs\n- `IntentionProcess`: Committing to desires as actionable intentions\n\n### Cognitive Chain Pattern\n\n```turtle\n:Belief_store_open a bdi:Belief ;\n    rdfs:comment \"Store is open\" ;\n    bdi:motivates :Desire_buy_groceries .\n\n:Desire_buy_groceries a bdi:Desire ;\n    rdfs:comment \"I desire to buy groceries\" ;\n    bdi:isMotivatedBy :Belief_store_open .\n\n:Intention_go_shopping a bdi:Intention ;\n    rdfs:comment \"I will buy groceries\" ;\n    bdi:fulfils :Desire_buy_groceries ;\n    bdi:isSupportedBy :Belief_store_open ;\n    bdi:specifies :Plan_shopping .\n```\n\n### World State Grounding\n\nMental states reference structured configurations of the environment:\n\n```turtle\n:Agent_A a bdi:Agent ;\n    bdi:perceives :WorldState_WS1 ;\n    bdi:hasMentalState :Belief_B1 .\n\n:WorldState_WS1 a bdi:WorldState ;\n    rdfs:comment \"Meeting scheduled at 10am in Room 5\" ;\n    bdi:atTime :TimeInstant_10am .\n\n:Belief_B1 a bdi:Belief ;\n    bdi:refersTo :WorldState_WS1 .\n```\n\n### Goal-Directed Planning\n\nIntentions specify plans that address goals through task sequences:\n\n```turtle\n:Intention_I1 bdi:specifies :Plan_P1 .\n\n:Plan_P1 a bdi:Plan ;\n    bdi:addresses :Goal_G1 ;\n    bdi:beginsWith :Task_T1 ;\n    bdi:endsWith :Task_T3 .\n\n:Task_T1 bdi:precedes :Task_T2 .\n:Task_T2 bdi:precedes :Task_T3 .\n```\n\n## T2B2T Paradigm\n\nTriples-to-Beliefs-to-Triples implements bidirectional flow between RDF knowledge graphs and internal mental states:\n\n**Phase 1: Triples-to-Beliefs**\n```turtle\n# External RDF context triggers belief formation\n:WorldState_notification a bdi:WorldState ;\n    rdfs:comment \"Push notification: Payment request $250\" ;\n    bdi:triggers :BeliefProcess_BP1 .\n\n:BeliefProcess_BP1 a bdi:BeliefProcess ;\n    bdi:generates :Belief_payment_request .\n```\n\n**Phase 2: Beliefs-to-Triples**\n```turtle\n# Mental deliberation produces new RDF output\n:Intention_pay a bdi:Intention ;\n    bdi:specifies :Plan_payment .\n\n:PlanExecution_PE1 a bdi:PlanExecution ;\n    bdi:satisfies :Plan_payment ;\n    bdi:bringsAbout :WorldState_payment_complete .\n```\n\n## Notation Selection by Level\n\n| C4 Level | Notation | Mental State Representation |\n|----------|----------|----------------------------|\n| L1 Context | ArchiMate | Agent boundaries, external perception sources |\n| L2 Container | ArchiMate | BDI reasoning engine, belief store, plan executor |\n| L3 Component | UML | Mental state managers, process handlers |\n| L4 Code | UML/RDF | Belief/Desire/Intention classes, ontology instances |\n\n## Justification and Explainability\n\nMental entities link to supporting evidence for traceable reasoning:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:isJustifiedBy :Justification_J1 .\n\n:Justification_J1 a bdi:Justification ;\n    rdfs:comment \"Official announcement received via email\" .\n\n:Intention_I1 a bdi:Intention ;\n    bdi:isJustifiedBy :Justification_J2 .\n\n:Justification_J2 a bdi:Justification ;\n    rdfs:comment \"Location precondition satisfied\" .\n```\n\n## Temporal Dimensions\n\nMental states persist over bounded time periods:\n\n```turtle\n:Belief_B1 a bdi:Belief ;\n    bdi:hasValidity :TimeInterval_TI1 .\n\n:TimeInterval_TI1 a bdi:TimeInterval ;\n    bdi:hasStartTime :TimeInstant_9am ;\n    bdi:hasEndTime :TimeInstant_11am .\n```\n\nQuery mental states active at specific moments:\n\n```sparql\nSELECT ?mentalState WHERE {\n    ?mentalState bdi:hasValidity ?interval .\n    ?interval bdi:hasStartTime ?start ;\n              bdi:hasEndTime ?end .\n    FILTER(?start <= \"2025-01-04T10:00:00\"^^xsd:dateTime && \n           ?end >= \"2025-01-04T10:00:00\"^^xsd:dateTime)\n}\n```\n\n## Compositional Mental Entities\n\nComplex mental entities decompose into constituent parts for selective updates:\n\n```turtle\n:Belief_meeting a bdi:Belief ;\n    rdfs:comment \"Meeting at 10am in Room 5\" ;\n    bdi:hasPart :Belief_meeting_time , :Belief_meeting_location .\n\n# Update only location component\n:BeliefProcess_update a bdi:BeliefProcess ;\n    bdi:modifies :Belief_meeting_location .\n```\n\n## Integration Patterns\n\n### Logic Augmented Generation (LAG)\n\nAugment LLM outputs with ontological constraints:\n\n```python\ndef augment_llm_with_bdi_ontology(prompt, ontology_graph):\n    ontology_context = serialize_ontology(ontology_graph, format='turtle')\n    augmented_prompt = f\"{ontology_context}\\n\\n{prompt}\"\n    \n    response = llm.generate(augmented_prompt)\n    triples = extract_rdf_triples(response)\n    \n    is_consistent = validate_triples(triples, ontology_graph)\n    return triples if is_consistent else retry_with_feedback()\n```\n\n### SEMAS Rule Translation\n\nMap BDI ontology to executable production rules:\n\n```prolog\n% Belief triggers desire formation\n[HEAD: belief(agent_a, store_open)] / \n[CONDITIONALS: time(weekday_afternoon)] » \n[TAIL: generate_desire(agent_a, buy_groceries)].\n\n% Desire triggers intention commitment\n[HEAD: desire(agent_a, buy_groceries)] / \n[CONDITIONALS: belief(agent_a, has_shopping_list)] » \n[TAIL: commit_intention(agent_a, buy_groceries)].\n```\n\n## Guidelines\n\n1. Model world states as configurations independent of agent perspectives, providing referential substrate for mental states.\n\n2. Distinguish endurants (persistent mental states) from perdurants (temporal mental processes), aligning with DOLCE ontology.\n\n3. Treat goals as descriptions rather than mental states, maintaining separation between cognitive and planning layers.\n\n4. Use `hasPart` relations for meronymic structures enabling selective belief updates.\n\n5. Associate every mental entity with temporal constructs via `atTime` or `hasValidity`.\n\n6. Use bidirectional property pairs (`motivates`/`isMotivatedBy`, `generates`/`isGeneratedBy`) for flexible querying.\n\n7. Link mental entities to `Justification` instances for explainability and trust.\n\n8. Implement T2B2T through: (1) translate RDF to beliefs, (2) execute BDI reasoning, (3) project mental states back to RDF.\n\n9. Define existential restrictions on mental processes (e.g., `BeliefProcess ⊑ ∃generates.Belief`).\n\n10. Reuse established ODPs (EventCore, Situation, TimeIndexedSituation, BasicPlan, Provenance) for interoperability.\n\n## Competency Questions\n\nValidate implementation against these SPARQL queries:\n\n```sparql\n# CQ1: What beliefs motivated formation of a given desire?\nSELECT ?belief WHERE {\n    :Desire_D1 bdi:isMotivatedBy ?belief .\n}\n\n# CQ2: Which desire does a particular intention fulfill?\nSELECT ?desire WHERE {\n    :Intention_I1 bdi:fulfils ?desire .\n}\n\n# CQ3: Which mental process generated a belief?\nSELECT ?process WHERE {\n    ?process bdi:generates :Belief_B1 .\n}\n\n# CQ4: What is the ordered sequence of tasks in a plan?\nSELECT ?task ?nextTask WHERE {\n    :Plan_P1 bdi:hasComponent ?task .\n    OPTIONAL { ?task bdi:precedes ?nextTask }\n} ORDER BY ?task\n```\n\n## Anti-Patterns\n\n1. **Conflating mental states with world states**: Mental states reference world states, they are not world states themselves.\n\n2. **Missing temporal bounds**: Every mental state should have validity intervals for diachronic reasoning.\n\n3. **Flat belief structures**: Use compositional modeling with `hasPart` for complex beliefs.\n\n4. **Implicit justifications**: Always link mental entities to explicit justification instances.\n\n5. **Direct intention-to-action mapping**: Intentions specify plans which contain tasks; actions execute tasks.\n\n## Integration\n\n- **RDF Processing**: Apply after parsing external RDF context to construct cognitive representations\n- **Semantic Reasoning**: Combine with ontology reasoning to infer implicit mental state relationships\n- **Multi-Agent Communication**: Integrate with FIPA ACL for cross-platform belief sharing\n- **Temporal Context**: Coordinate with temporal reasoning for mental state evolution\n- **Explainable AI**: Feed into explanation systems tracing perception through deliberation to action\n- **Neuro-Symbolic AI**: Apply in LAG pipelines to constrain LLM outputs with cognitive structures\n\n## References\n\nSee `references/` folder for detailed documentation:\n- `bdi-ontology-core.md` - Core ontology patterns and class definitions\n- `rdf-examples.md` - Complete RDF/Turtle examples\n- `sparql-competency.md` - Full competency question SPARQL queries\n- `framework-integration.md` - SEMAS, JADE, LAG integration patterns\n\nPrimary sources:\n- Zuppiroli et al. \"The Belief-Desire-Intention Ontology\" (2025)\n- Rao & Georgeff \"BDI agents: From theory to practice\" (1995)\n- Bratman \"Intention, plans, and practical reason\" (1987)"
              },
              {
                "name": "context-compression",
                "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
                "path": "skills/context-compression/SKILL.md",
                "frontmatter": {
                  "name": "context-compression",
                  "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits."
                },
                "content": "# Context Compression Strategies\n\nWhen agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.\n\n## When to Activate\n\nActivate this skill when:\n- Agent sessions exceed context window limits\n- Codebases exceed context windows (5M+ token systems)\n- Designing conversation summarization strategies\n- Debugging cases where agents \"forget\" what files they modified\n- Building evaluation frameworks for compression quality\n\n## Core Concepts\n\nContext compression trades token savings against information loss. Three production-ready approaches exist:\n\n1. **Anchored Iterative Summarization**: Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary. Structure forces preservation by dedicating sections to specific information types.\n\n2. **Opaque Compression**: Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Cannot verify what was preserved.\n\n3. **Regenerative Full Summary**: Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles due to full regeneration rather than incremental merging.\n\nThe critical insight: structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.\n\n## Detailed Topics\n\n### Why Tokens-Per-Task Matters\n\nTraditional compression metrics target tokens-per-request. This is the wrong optimization. When compression loses critical details like file paths or error messages, the agent must re-fetch information, re-explore approaches, and waste tokens recovering context.\n\nThe right metric is tokens-per-task: total tokens consumed from task start to completion. A compression strategy saving 0.5% more tokens but causing 20% more re-fetching costs more overall.\n\n### The Artifact Trail Problem\n\nArtifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0 in evaluations. Even structured summarization with explicit file sections struggles to maintain complete file tracking across long sessions.\n\nCoding agents need to know:\n- Which files were created\n- Which files were modified and what changed\n- Which files were read but not changed\n- Function names, variable names, error messages\n\nThis problem likely requires specialized handling beyond general summarization: a separate artifact index or explicit file-state tracking in agent scaffolding.\n\n### Structured Summary Sections\n\nEffective structured summaries include explicit sections:\n\n```markdown\n## Session Intent\n[What the user is trying to accomplish]\n\n## Files Modified\n- auth.controller.ts: Fixed JWT token generation\n- config/redis.ts: Updated connection pooling\n- tests/auth.test.ts: Added mock setup for new config\n\n## Decisions Made\n- Using Redis connection pool instead of per-request connections\n- Retry logic with exponential backoff for transient failures\n\n## Current State\n- 14 tests passing, 2 failing\n- Remaining: mock setup for session service tests\n\n## Next Steps\n1. Fix remaining test failures\n2. Run full test suite\n3. Update documentation\n```\n\nThis structure prevents silent loss of file paths or decisions because each section must be explicitly addressed.\n\n### Compression Trigger Strategies\n\nWhen to trigger compression matters as much as how to compress:\n\n| Strategy | Trigger Point | Trade-off |\n|----------|---------------|-----------|\n| Fixed threshold | 70-80% context utilization | Simple but may compress too early |\n| Sliding window | Keep last N turns + summary | Predictable context size |\n| Importance-based | Compress low-relevance sections first | Complex but preserves signal |\n| Task-boundary | Compress at logical task completions | Clean summaries but unpredictable timing |\n\nThe sliding window approach with structured summaries provides the best balance of predictability and quality for most coding agent use cases.\n\n### Probe-Based Evaluation\n\nTraditional metrics like ROUGE or embedding similarity fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.\n\nProbe-based evaluation directly measures functional quality by asking questions after compression:\n\n| Probe Type | What It Tests | Example Question |\n|------------|---------------|------------------|\n| Recall | Factual retention | \"What was the original error message?\" |\n| Artifact | File tracking | \"Which files have we modified?\" |\n| Continuation | Task planning | \"What should we do next?\" |\n| Decision | Reasoning chain | \"What did we decide about the Redis issue?\" |\n\nIf compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.\n\n### Evaluation Dimensions\n\nSix dimensions capture compression quality for coding agents:\n\n1. **Accuracy**: Are technical details correct? File paths, function names, error codes.\n2. **Context Awareness**: Does the response reflect current conversation state?\n3. **Artifact Trail**: Does the agent know which files were read or modified?\n4. **Completeness**: Does the response address all parts of the question?\n5. **Continuity**: Can work continue without re-fetching information?\n6. **Instruction Following**: Does the response respect stated constraints?\n\nAccuracy shows the largest variation between compression methods (0.6 point gap). Artifact trail is universally weak (2.2-2.5 range).\n\n## Practical Guidance\n\n### Three-Phase Compression Workflow\n\nFor large codebases or agent systems exceeding context windows, apply compression through three phases:\n\n1. **Research Phase**: Produce a research document from architecture diagrams, documentation, and key interfaces. Compress exploration into a structured analysis of components and dependencies. Output: single research document.\n\n2. **Planning Phase**: Convert research into implementation specification with function signatures, type definitions, and data flow. A 5M token codebase compresses to approximately 2,000 words of specification.\n\n3. **Implementation Phase**: Execute against the specification. Context remains focused on the spec rather than raw codebase exploration.\n\n### Using Example Artifacts as Seeds\n\nWhen provided with a manual migration example or reference PR, use it as a template to understand the target pattern. The example reveals constraints that static analysis cannot surface: which invariants must hold, which services break on changes, and what a clean migration looks like.\n\nThis is particularly important when the agent cannot distinguish essential complexity (business requirements) from accidental complexity (legacy workarounds). The example artifact encodes that distinction.\n\n### Implementing Anchored Iterative Summarization\n\n1. Define explicit summary sections matching your agent's needs\n2. On first compression trigger, summarize truncated history into sections\n3. On subsequent compressions, summarize only new truncated content\n4. Merge new summary into existing sections rather than regenerating\n5. Track which information came from which compression cycle for debugging\n\n### When to Use Each Approach\n\n**Use anchored iterative summarization when:**\n- Sessions are long-running (100+ messages)\n- File tracking matters (coding, debugging)\n- You need to verify what was preserved\n\n**Use opaque compression when:**\n- Maximum token savings required\n- Sessions are relatively short\n- Re-fetching costs are low\n\n**Use regenerative summaries when:**\n- Summary interpretability is critical\n- Sessions have clear phase boundaries\n- Full context review is acceptable on each compression\n\n### Compression Ratio Considerations\n\n| Method | Compression Ratio | Quality Score | Trade-off |\n|--------|-------------------|---------------|-----------|\n| Anchored Iterative | 98.6% | 3.70 | Best quality, slightly less compression |\n| Regenerative | 98.7% | 3.44 | Good quality, moderate compression |\n| Opaque | 99.3% | 3.35 | Best compression, quality loss |\n\nThe 0.7% additional tokens retained by structured summarization buys 0.35 quality points. For any task where re-fetching costs matter, this trade-off favors structured approaches.\n\n## Examples\n\n**Example 1: Debugging Session Compression**\n\nOriginal context (89,000 tokens, 178 messages):\n- 401 error on /api/auth/login endpoint\n- Traced through auth controller, middleware, session store\n- Found stale Redis connection\n- Fixed connection pooling, added retry logic\n- 14 tests passing, 2 failing\n\nStructured summary after compression:\n\n```markdown\n## Session Intent\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n- auth.controller.ts: No changes (read only)\n- middleware/cors.ts: No changes (examined)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n```\n\n**Example 2: Probe Response Quality**\n\nAfter compression, asking \"What was the original error?\":\n\nGood response (structured summarization):\n> \"The original error was a 401 Unauthorized response from the /api/auth/login endpoint. Users received this error with valid credentials. Root cause was stale Redis connection in session store.\"\n\nPoor response (aggressive compression):\n> \"We were debugging an authentication issue. The login was failing. We fixed some configuration problems.\"\n\nThe structured response preserves endpoint, error code, and root cause. The aggressive response loses all technical detail.\n\n## Guidelines\n\n1. Optimize for tokens-per-task, not tokens-per-request\n2. Use structured summaries with explicit sections for file tracking\n3. Trigger compression at 70-80% context utilization\n4. Implement incremental merging rather than full regeneration\n5. Test compression quality with probe-based evaluation\n6. Track artifact trail separately if file tracking is critical\n7. Accept slightly lower compression ratios for better quality retention\n8. Monitor re-fetching frequency as a compression quality signal\n\n## Integration\n\nThis skill connects to several others in the collection:\n\n- context-degradation - Compression is a mitigation strategy for degradation\n- context-optimization - Compression is one optimization technique among many\n- evaluation - Probe-based evaluation applies to compression testing\n- memory-systems - Compression relates to scratchpad and summary memory patterns\n\n## References\n\nInternal reference:\n- [Evaluation Framework Reference](./references/evaluation-framework.md) - Detailed probe types and scoring rubrics\n\nRelated skills in this collection:\n- context-degradation - Understanding what compression prevents\n- context-optimization - Broader optimization strategies\n- evaluation - Building evaluation frameworks\n\nExternal resources:\n- Factory Research: Evaluating Context Compression for AI Agents (December 2025)\n- Research on LLM-as-judge evaluation methodology (Zheng et al., 2023)\n- Netflix Engineering: \"The Infinite Software Crisis\" - Three-phase workflow and context compression at scale (AI Summit 2025)\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-22\n**Last Updated**: 2025-12-26\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              },
              {
                "name": "context-degradation",
                "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
                "path": "skills/context-degradation/SKILL.md",
                "frontmatter": {
                  "name": "context-degradation",
                  "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures."
                },
                "content": "# Context Degradation Patterns\n\nLanguage models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems. Context degradation is not a binary state but a continuum of performance degradation that manifests in several distinct ways.\n\n## When to Activate\n\nActivate this skill when:\n- Agent performance degrades unexpectedly during long conversations\n- Debugging cases where agents produce incorrect or irrelevant outputs\n- Designing systems that must handle large contexts reliably\n- Evaluating context engineering choices for production systems\n- Investigating \"lost in middle\" phenomena in agent outputs\n- Analyzing context-related failures in agent behavior\n\n## Core Concepts\n\nContext degradation manifests through several distinct patterns. The lost-in-middle phenomenon causes information in the center of context to receive less attention. Context poisoning occurs when errors compound through repeated reference. Context distraction happens when irrelevant information overwhelms relevant content. Context confusion arises when the model cannot determine which context applies. Context clash develops when accumulated information directly conflicts.\n\nThese patterns are predictable and can be mitigated through architectural patterns like compaction, masking, partitioning, and isolation.\n\n## Detailed Topics\n\n### The Lost-in-Middle Phenomenon\n\nThe most well-documented degradation pattern is the \"lost-in-middle\" effect, where models demonstrate U-shaped attention curves. Information at the beginning and end of context receives reliable attention, while information buried in the middle suffers from dramatically reduced recall accuracy.\n\n**Empirical Evidence**\nResearch demonstrates that relevant information placed in the middle of context experiences 10-40% lower recall accuracy compared to the same information at the beginning or end. This is not a failure of the model but a consequence of attention mechanics and training data distributions.\n\nModels allocate massive attention to the first token (often the BOS token) to stabilize internal states. This creates an \"attention sink\" that soaks up attention budget. As context grows, the limited budget is stretched thinner, and middle tokens fail to garner sufficient attention weight for reliable retrieval.\n\n**Practical Implications**\nDesign context placement with attention patterns in mind. Place critical information at the beginning or end of context. Consider whether information will be queried directly or needs to support reasoning—if the latter, placement matters less but overall signal quality matters more.\n\nFor long documents or conversations, use summary structures that surface key information at attention-favored positions. Use explicit section headers and transitions to help models navigate structure.\n\n### Context Poisoning\n\nContext poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.\n\n**How Poisoning Occurs**\nPoisoning typically enters through three pathways. First, tool outputs may contain errors or unexpected formats that models accept as ground truth. Second, retrieved documents may contain incorrect or outdated information that models incorporate into reasoning. Third, model-generated summaries or intermediate outputs may introduce hallucinations that persist in context.\n\nThe compounding effect is severe. If an agent's goals section becomes poisoned, it develops strategies that take substantial effort to undo. Each subsequent decision references the poisoned content, reinforcing incorrect assumptions.\n\n**Detection and Recovery**\nWatch for symptoms including degraded output quality on tasks that previously succeeded, tool misalignment where agents call wrong tools or parameters, and hallucinations that persist despite correction attempts. When these symptoms appear, consider context poisoning.\n\nRecovery requires removing or replacing poisoned content. This may involve truncating context to before the poisoning point, explicitly noting the poisoning in context and asking for re-evaluation, or restarting with clean context and preserving only verified information.\n\n### Context Distraction\n\nContext distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge. The model attends to everything in context regardless of relevance, and this creates pressure to use provided information even when internal knowledge is more accurate.\n\n**The Distractor Effect**\nResearch shows that even a single irrelevant document in context reduces performance on tasks involving relevant documents. Multiple distractors compound degradation. The effect is not about noise in absolute terms but about attention allocation—irrelevant information competes with relevant information for limited attention budget.\n\nModels do not have a mechanism to \"skip\" irrelevant context. They must attend to everything provided, and this obligation creates distraction even when the irrelevant information is clearly not useful.\n\n**Mitigation Strategies**\nMitigate distraction through careful curation of what enters context. Apply relevance filtering before loading retrieved documents. Use namespacing and organization to make irrelevant sections easy to ignore structurally. Consider whether information truly needs to be in context or can be accessed through tool calls instead.\n\n### Context Confusion\n\nContext confusion arises when irrelevant information influences responses in ways that degrade quality. This is related to distraction but distinct—confusion concerns the influence of context on model behavior rather than attention allocation.\n\nIf you put something in context, the model has to pay attention to it. The model may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints that came from different contexts. Confusion is especially problematic when context contains multiple task types or when switching between tasks within a single session.\n\n**Signs of Confusion**\nWatch for responses that address the wrong aspect of a query, tool calls that seem appropriate for a different task, or outputs that mix requirements from multiple sources. These indicate confusion about what context applies to the current situation.\n\n**Architectural Solutions**\nArchitectural solutions include explicit task segmentation where different tasks get different context windows, clear transitions between task contexts, and state management that isolates context for different objectives.\n\n### Context Clash\n\nContext clash develops when accumulated information directly conflicts, creating contradictory guidance that derails reasoning. This differs from poisoning where one piece of information is incorrect—in clash, multiple correct pieces of information contradict each other.\n\n**Sources of Clash**\nClash commonly arises from multi-source retrieval where different sources have contradictory information, version conflicts where outdated and current information both appear in context, and perspective conflicts where different viewpoints are valid but incompatible.\n\n**Resolution Approaches**\nResolution approaches include explicit conflict marking that identifies contradictions and requests clarification, priority rules that establish which source takes precedence, and version filtering that excludes outdated information from context.\n\n### Empirical Benchmarks and Thresholds\n\nResearch provides concrete data on degradation patterns that inform design decisions.\n\n**RULER Benchmark Findings**\nThe RULER benchmark delivers sobering findings: only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. GPT-5.2 shows the least degradation among current models, while many still drop 30+ points at extended contexts. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding.\n\n**Model-Specific Degradation Thresholds**\n| Model | Degradation Onset | Severe Degradation | Notes |\n|-------|-------------------|-------------------|-------|\n| GPT-5.2 | ~64K tokens | ~200K tokens | Best overall degradation resistance with thinking mode |\n| Claude Opus 4.5 | ~100K tokens | ~180K tokens | 200K context window, strong attention management |\n| Claude Sonnet 4.5 | ~80K tokens | ~150K tokens | Optimized for agents and coding tasks |\n| Gemini 3 Pro | ~500K tokens | ~800K tokens | 1M context window, native multimodality |\n| Gemini 3 Flash | ~300K tokens | ~600K tokens | 3x speed of Gemini 2.5, 81.2% MMMU-Pro |\n\n**Model-Specific Behavior Patterns**\nDifferent models exhibit distinct failure modes under context pressure:\n\n- **Claude 4.5 series**: Lowest hallucination rates with calibrated uncertainty. Claude Opus 4.5 achieves 80.9% on SWE-bench Verified. Tends to refuse or ask clarification rather than fabricate.\n- **GPT-5.2**: Two modes available - instant (fast) and thinking (reasoning). Thinking mode reduces hallucination through step-by-step verification but increases latency.\n- **Gemini 3 Pro/Flash**: Native multimodality with 1M context window. Gemini 3 Flash offers 3x speed improvement over previous generation. Strong at multi-modal reasoning across text, code, images, audio, and video.\n\nThese patterns inform model selection for different use cases. High-stakes tasks benefit from Claude 4.5's conservative approach or GPT-5.2's thinking mode; speed-critical tasks may use instant modes.\n\n### Counterintuitive Findings\n\nResearch reveals several counterintuitive patterns that challenge assumptions about context management.\n\n**Shuffled Haystacks Outperform Coherent Ones**\nStudies found that shuffled (incoherent) haystacks produce better performance than logically coherent ones. This suggests that coherent context may create false associations that confuse retrieval, while incoherent context forces models to rely on exact matching.\n\n**Single Distractors Have Outsized Impact**\nEven a single irrelevant document reduces performance significantly. The effect is not proportional to the amount of noise but follows a step function where the presence of any distractor triggers degradation.\n\n**Needle-Question Similarity Correlation**\nLower similarity between needle and question pairs shows faster degradation with context length. Tasks requiring inference across dissimilar content are particularly vulnerable.\n\n### When Larger Contexts Hurt\n\nLarger context windows do not uniformly improve performance. In many cases, larger contexts create new problems that outweigh benefits.\n\n**Performance Degradation Curves**\nModels exhibit non-linear degradation with context length. Performance remains stable up to a threshold, then degrades rapidly. The threshold varies by model and task complexity. For many models, meaningful degradation begins around 8,000-16,000 tokens even when context windows support much larger sizes.\n\n**Cost Implications**\nProcessing cost grows disproportionately with context length. The cost to process a 400K token context is not double the cost of 200K—it increases exponentially in both time and computing resources. For many applications, this makes large-context processing economically impractical.\n\n**Cognitive Load Metaphor**\nEven with an infinite context, asking a single model to maintain consistent quality across dozens of independent tasks creates a cognitive bottleneck. The model must constantly switch context between items, maintain a comparative framework, and ensure stylistic consistency. This is not a problem that more context solves.\n\n## Practical Guidance\n\n### The Four-Bucket Approach\n\nFour strategies address different aspects of context degradation:\n\n**Write**: Save context outside the window using scratchpads, file systems, or external storage. This keeps active context lean while preserving information access.\n\n**Select**: Pull relevant context into the window through retrieval, filtering, and prioritization. This addresses distraction by excluding irrelevant information.\n\n**Compress**: Reduce tokens while preserving information through summarization, abstraction, and observation masking. This extends effective context capacity.\n\n**Isolate**: Split context across sub-agents or sessions to prevent any single context from growing large enough to degrade. This is the most aggressive strategy but often the most effective.\n\n### Architectural Patterns\n\nImplement these strategies through specific architectural patterns. Use just-in-time context loading to retrieve information only when needed. Use observation masking to replace verbose tool outputs with compact references. Use sub-agent architectures to isolate context for different tasks. Use compaction to summarize growing context before it exceeds limits.\n\n## Examples\n\n**Example 1: Detecting Degradation**\n```yaml\n# Context grows during long conversation\nturn_1: 1000 tokens\nturn_5: 8000 tokens\nturn_10: 25000 tokens\nturn_20: 60000 tokens (degradation begins)\nturn_30: 90000 tokens (significant degradation)\n```\n\n**Example 2: Mitigating Lost-in-Middle**\n```markdown\n# Organize context with critical info at edges\n\n[CURRENT TASK]                      # At start\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT]                  # Middle (less attention)\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS]                     # At end\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n```\n\n## Guidelines\n\n1. Monitor context length and performance correlation during development\n2. Place critical information at beginning or end of context\n3. Implement compaction triggers before degradation becomes severe\n4. Validate retrieved documents for accuracy before adding to context\n5. Use versioning to prevent outdated information from causing clash\n6. Segment tasks to prevent context confusion across different objectives\n7. Design for graceful degradation rather than assuming perfect conditions\n8. Test with progressively larger contexts to find degradation thresholds\n\n## Integration\n\nThis skill builds on context-fundamentals and should be studied after understanding basic context concepts. It connects to:\n\n- context-optimization - Techniques for mitigating degradation\n- multi-agent-patterns - Using isolation to prevent degradation\n- evaluation - Measuring and detecting degradation in production\n\n## References\n\nInternal reference:\n- [Degradation Patterns Reference](./references/patterns.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-optimization - Mitigation techniques\n- evaluation - Detection and measurement\n\nExternal resources:\n- Research on attention mechanisms and context window limitations\n- Studies on the \"lost-in-middle\" phenomenon\n- Production engineering guides from AI labs\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-fundamentals",
                "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems.",
                "path": "skills/context-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "context-fundamentals",
                  "description": "This skill should be used when the user asks to \"understand context\", \"explain context windows\", \"design agent architecture\", \"debug context issues\", \"optimize context usage\", or discusses context components, attention mechanics, progressive disclosure, or context budgeting. Provides foundational understanding of context engineering for AI agent systems."
                },
                "content": "# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates n² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationships gets stretched thin.\n\nModels develop attention patterns from training data distributions where shorter sequences predominate. This means models have less experience with and fewer specialized parameters for context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.\n\n**Position Encoding and Context Extension**\nPosition encoding interpolation allows models to handle longer sequences by adapting them to originally trained smaller contexts. However, this adaptation introduces degradation in token position understanding. Models remain highly capable at longer contexts but show reduced precision for information retrieval and long-range reasoning compared to performance on shorter contexts.\n\n**The Progressive Disclosure Principle**\nProgressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when a skill is activated for specific tasks.\n\nThis approach keeps agents fast while giving them access to more context on demand. The principle applies at multiple levels: skill selection, document loading, and even tool result retrieval.\n\n### Context Quality Versus Context Quantity\n\nThe assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.\n\nSeveral factors create pressure for context efficiency. Processing cost grows disproportionately with context length—not just double the cost for double the tokens, but exponentially more in time and computing resources. Model performance degrades beyond certain context lengths even when the window technically supports more tokens. Long inputs remain expensive even with prefix caching.\n\nThe guiding principle is informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not, and design systems that can access additional information on demand.\n\n### Context as Finite Resource\n\nContext must be treated as a finite resource with diminishing marginal returns. Like humans with limited working memory, language models have an attention budget drawn on when parsing large volumes of context.\n\nEvery new token introduced depletes this budget by some amount. This creates the need for careful curation of available tokens. The engineering problem is optimizing utility against inherent constraints.\n\nContext engineering is iterative and the curation phase happens each time you decide what to pass to the model. It is not a one-time prompt writing exercise but an ongoing discipline of context management.\n\n## Practical Guidance\n\n### File-System-Based Access\n\nAgents with filesystem access can use progressive disclosure naturally. Store reference materials, documentation, and data externally. Load files only when needed using standard filesystem operations. This pattern avoids stuffing context with information that may not be relevant.\n\nThe file system itself provides structure that agents can navigate. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance. Metadata of file references provides a mechanism to efficiently refine behavior.\n\n### Hybrid Strategies\n\nThe most effective agents employ hybrid strategies. Pre-load some context for speed (like CLAUDE.md files or project rules), but enable autonomous exploration for additional context as needed. The decision boundary depends on task characteristics and context dynamics.\n\nFor contexts with less dynamic content, pre-loading more upfront makes sense. For rapidly changing or highly specific information, just-in-time loading avoids stale context.\n\n### Context Budgeting\n\nDesign with explicit context budgets in mind. Know the effective context limit for your model and task. Monitor context usage during development. Implement compaction triggers at appropriate thresholds. Design systems assuming context will degrade rather than hoping it will not.\n\nEffective context budgeting requires understanding not just raw token counts but also attention distribution patterns. The middle of context receives less attention than the beginning and end. Place critical information at attention-favored positions.\n\n## Examples\n\n**Example 1: Organizing System Prompts**\n```markdown\n<BACKGROUND_INFORMATION>\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n</BACKGROUND_INFORMATION>\n\n<INSTRUCTIONS>\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n- Follow PEP 8 style guidelines\n</INSTRUCTIONS>\n\n<TOOL_GUIDANCE>\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n</TOOL_GUIDANCE>\n\n<OUTPUT_DESCRIPTION>\nProvide code blocks with syntax highlighting.\nExplain non-obvious decisions in comments.\n</OUTPUT_DESCRIPTION>\n```\n\n**Example 2: Progressive Document Loading**\n```markdown\n# Instead of loading all documentation at once:\n\n# Step 1: Load summary\ndocs/api_summary.md          # Lightweight overview\n\n# Step 2: Load specific section as needed\ndocs/api/endpoints.md        # Only when API calls needed\ndocs/api/authentication.md   # Only when auth context needed\n```\n\n## Guidelines\n\n1. Treat context as a finite resource with diminishing returns\n2. Place critical information at attention-favored positions (beginning and end)\n3. Use progressive disclosure to defer loading until needed\n4. Organize system prompts with clear section boundaries\n5. Monitor context usage during development\n6. Implement compaction triggers at 70-80% utilization\n7. Design for context degradation rather than hoping to avoid it\n8. Prefer smaller high-signal context over larger low-signal context\n\n## Integration\n\nThis skill provides foundational context that all other skills build upon. It should be studied first before exploring:\n\n- context-degradation - Understanding how context fails\n- context-optimization - Techniques for extending context capacity\n- multi-agent-patterns - How context isolation enables multi-agent systems\n- tool-design - How tool definitions interact with context\n\n## References\n\nInternal reference:\n- [Context Components Reference](./references/context-components.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-degradation - Understanding context failure patterns\n- context-optimization - Techniques for efficient context use\n\nExternal resources:\n- Research on transformer attention mechanisms\n- Production engineering guides from leading AI labs\n- Framework documentation on context window management\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "context-optimization",
                "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity.",
                "path": "skills/context-optimization/SKILL.md",
                "frontmatter": {
                  "name": "context-optimization",
                  "description": "This skill should be used when the user asks to \"optimize context\", \"reduce token costs\", \"improve context efficiency\", \"implement KV-cache optimization\", \"partition context\", or mentions context limits, observation masking, context budgeting, or extending effective context capacity."
                },
                "content": "# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concerns—the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation**\nAggregate results from partitioned subtasks by validating all partitions completed, merging compatible results, and summarizing if still too large.\n\n### Budget Management\n\n**Context Budget Allocation**\nDesign explicit context budgets. Allocate tokens to categories: system prompt, tool definitions, retrieved docs, message history, and reserved buffer. Monitor usage against budget and trigger optimization when approaching limits.\n\n**Trigger-Based Optimization**\nMonitor signals for optimization triggers: token utilization above 80%, degradation indicators, and performance drops. Apply appropriate optimization techniques based on context composition.\n\n## Practical Guidance\n\n### Optimization Decision Framework\n\nWhen to optimize:\n- Context utilization exceeds 70%\n- Response quality degrades as conversations extend\n- Costs increase due to long contexts\n- Latency increases with conversation length\n\nWhat to apply:\n- Tool outputs dominate: observation masking\n- Retrieved documents dominate: summarization or partitioning\n- Message history dominates: compaction with summarization\n- Multiple components: combine strategies\n\n### Performance Considerations\n\nCompaction should achieve 50-70% token reduction with less than 5% quality degradation. Masking should achieve 60-80% reduction in masked observations. Cache optimization should achieve 70%+ hit rate for stable workloads.\n\nMonitor and iterate on optimization strategies based on measured effectiveness.\n\n## Examples\n\n**Example 1: Compaction Trigger**\n```python\nif context_tokens / context_limit > 0.8:\n    context = compact_context(context)\n```\n\n**Example 2: Observation Masking**\n```python\nif len(observation) > max_length:\n    ref_id = store_observation(observation)\n    return f\"[Obs:{ref_id} elided. Key: {extract_key(observation)}]\"\n```\n\n**Example 3: Cache-Friendly Ordering**\n```python\n# Stable content first\ncontext = [system_prompt, tool_definitions]  # Cacheable\ncontext += [reused_templates]  # Reusable\ncontext += [unique_content]  # Unique\n```\n\n## Guidelines\n\n1. Measure before optimizing—know your current state\n2. Apply compaction before masking when possible\n3. Design for cache stability with consistent prompts\n4. Partition before context becomes problematic\n5. Monitor optimization effectiveness over time\n6. Balance token savings against quality preservation\n7. Test optimization at production scale\n8. Implement graceful degradation for edge cases\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- multi-agent-patterns - Partitioning as isolation\n- evaluation - Measuring optimization effectiveness\n- memory-systems - Offloading context to memory\n\n## References\n\nInternal reference:\n- [Optimization Techniques Reference](./references/optimization_techniques.md) - Detailed technical reference\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- context-degradation - Understanding when to optimize\n- evaluation - Measuring optimization\n\nExternal resources:\n- Research on context window limitations\n- KV-cache optimization techniques\n- Production engineering guides\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "evaluation",
                "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
                "path": "skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines."
                },
                "content": "# Evaluation Methods for Agent Systems\n\nEvaluation of agent systems requires different approaches than traditional software or even standard language model applications. Agents make dynamic decisions, are non-deterministic between runs, and often lack single correct answers. Effective evaluation must account for these characteristics while providing actionable feedback. A robust evaluation framework enables continuous improvement, catches regressions, and validates that context engineering choices achieve intended effects.\n\n## When to Activate\n\nActivate this skill when:\n- Testing agent performance systematically\n- Validating context engineering choices\n- Measuring improvements over time\n- Catching regressions before deployment\n- Building quality gates for agent pipelines\n- Comparing different agent configurations\n- Evaluating production systems continuously\n\n## Core Concepts\n\nAgent evaluation requires outcome-focused approaches that account for non-determinism and multiple valid paths. Multi-dimensional rubrics capture various quality aspects: factual accuracy, completeness, citation accuracy, source quality, and tool efficiency. LLM-as-judge provides scalable evaluation while human evaluation catches edge cases.\n\nThe key insight is that agents may find alternative paths to goals—the evaluation should judge whether they achieve right outcomes while following reasonable processes.\n\n**Performance Drivers: The 95% Finding**\nResearch on the BrowseComp evaluation (which tests browsing agents' ability to locate hard-to-find information) found that three factors explain 95% of performance variance:\n\n| Factor | Variance Explained | Implication |\n|--------|-------------------|-------------|\n| Token usage | 80% | More tokens = better performance |\n| Number of tool calls | ~10% | More exploration helps |\n| Model choice | ~5% | Better models multiply efficiency |\n\nThis finding has significant implications for evaluation design:\n- **Token budgets matter**: Evaluate agents with realistic token budgets, not unlimited resources\n- **Model upgrades beat token increases**: Upgrading to Claude Sonnet 4.5 or GPT-5.2 provides larger gains than doubling token budgets on previous versions\n- **Multi-agent validation**: The finding validates architectures that distribute work across agents with separate context windows\n\n## Detailed Topics\n\n### Evaluation Challenges\n\n**Non-Determinism and Multiple Valid Paths**\nAgents may take completely different valid paths to reach goals. One agent might search three sources while another searches ten. They might use different tools to find the same answer. Traditional evaluations that check for specific steps fail in this context.\n\nThe solution is outcome-focused evaluation that judges whether agents achieve right outcomes while following reasonable processes.\n\n**Context-Dependent Failures**\nAgent failures often depend on context in subtle ways. An agent might succeed on simple queries but fail on complex ones. It might work well with one tool set but fail with another. Failures may emerge only after extended interaction when context accumulates.\n\nEvaluation must cover a range of complexity levels and test extended interactions, not just isolated queries.\n\n**Composite Quality Dimensions**\nAgent quality is not a single dimension. It includes factual accuracy, completeness, coherence, tool efficiency, and process quality. An agent might score high on accuracy but low in efficiency, or vice versa.\n\nEvaluation rubrics must capture multiple dimensions with appropriate weighting for the use case.\n\n### Evaluation Rubric Design\n\n**Multi-Dimensional Rubric**\nEffective rubrics cover key dimensions with descriptive levels:\n\nFactual accuracy: Claims match ground truth (excellent to failed)\n\nCompleteness: Output covers requested aspects (excellent to failed)\n\nCitation accuracy: Citations match claimed sources (excellent to failed)\n\nSource quality: Uses appropriate primary sources (excellent to failed)\n\nTool efficiency: Uses right tools reasonable number of times (excellent to failed)\n\n**Rubric Scoring**\nConvert dimension assessments to numeric scores (0.0 to 1.0) with appropriate weighting. Calculate weighted overall scores. Determine passing threshold based on use case requirements.\n\n### Evaluation Methodologies\n\n**LLM-as-Judge**\nLLM-based evaluation scales to large test sets and provides consistent judgments. The key is designing effective evaluation prompts that capture the dimensions of interest.\n\nProvide clear task description, agent output, ground truth (if available), evaluation scale with level descriptions, and request structured judgment.\n\n**Human Evaluation**\nHuman evaluation catches what automation misses. Humans notice hallucinated answers on unusual queries, system failures, and subtle biases that automated evaluation misses.\n\nEffective human evaluation covers edge cases, samples systematically, tracks patterns, and provides contextual understanding.\n\n**End-State Evaluation**\nFor agents that mutate persistent state, end-state evaluation focuses on whether the final state matches expectations rather than how the agent got there.\n\n### Test Set Design\n\n**Sample Selection**\nStart with small samples during development. Early in agent development, changes have dramatic impacts because there is abundant low-hanging fruit. Small test sets reveal large effects.\n\nSample from real usage patterns. Add known edge cases. Ensure coverage across complexity levels.\n\n**Complexity Stratification**\nTest sets should span complexity levels: simple (single tool call), medium (multiple tool calls), complex (many tool calls, significant ambiguity), and very complex (extended interaction, deep reasoning).\n\n### Context Engineering Evaluation\n\n**Testing Context Strategies**\nContext engineering choices should be validated through systematic evaluation. Run agents with different context strategies on the same test set. Compare quality scores, token usage, and efficiency metrics.\n\n**Degradation Testing**\nTest how context degradation affects performance by running agents at different context sizes. Identify performance cliffs where context becomes problematic. Establish safe operating limits.\n\n### Continuous Evaluation\n\n**Evaluation Pipeline**\nBuild evaluation pipelines that run automatically on agent changes. Track results over time. Compare versions to identify improvements or regressions.\n\n**Monitoring Production**\nTrack evaluation metrics in production by sampling interactions and evaluating randomly. Set alerts for quality drops. Maintain dashboards for trend analysis.\n\n## Practical Guidance\n\n### Building Evaluation Frameworks\n\n1. Define quality dimensions relevant to your use case\n2. Create rubrics with clear, actionable level descriptions\n3. Build test sets from real usage patterns and edge cases\n4. Implement automated evaluation pipelines\n5. Establish baseline metrics before making changes\n6. Run evaluations on all significant changes\n7. Track metrics over time for trend analysis\n8. Supplement automated evaluation with human review\n\n### Avoiding Evaluation Pitfalls\n\nOverfitting to specific paths: Evaluate outcomes, not specific steps.\nIgnoring edge cases: Include diverse test scenarios.\nSingle-metric obsession: Use multi-dimensional rubrics.\nNeglecting context effects: Test with realistic context sizes.\nSkipping human evaluation: Automated evaluation misses subtle issues.\n\n## Examples\n\n**Example 1: Simple Evaluation**\n```python\ndef evaluate_agent_response(response, expected):\n    rubric = load_rubric()\n    scores = {}\n    for dimension, config in rubric.items():\n        scores[dimension] = assess_dimension(response, expected, dimension)\n    overall = weighted_average(scores, config[\"weights\"])\n    return {\"passed\": overall >= 0.7, \"scores\": scores}\n```\n\n**Example 2: Test Set Structure**\n\nTest sets should span multiple complexity levels to ensure comprehensive evaluation:\n\n```python\ntest_set = [\n    {\n        \"name\": \"simple_lookup\",\n        \"input\": \"What is the capital of France?\",\n        \"expected\": {\"type\": \"fact\", \"answer\": \"Paris\"},\n        \"complexity\": \"simple\",\n        \"description\": \"Single tool call, factual lookup\"\n    },\n    {\n        \"name\": \"medium_query\",\n        \"input\": \"Compare the revenue of Apple and Microsoft last quarter\",\n        \"complexity\": \"medium\",\n        \"description\": \"Multiple tool calls, comparison logic\"\n    },\n    {\n        \"name\": \"multi_step_reasoning\",\n        \"input\": \"Analyze sales data from Q1-Q4 and create a summary report with trends\",\n        \"complexity\": \"complex\",\n        \"description\": \"Many tool calls, aggregation, analysis\"\n    },\n    {\n        \"name\": \"research_synthesis\",\n        \"input\": \"Research emerging AI technologies, evaluate their potential impact, and recommend adoption strategy\",\n        \"complexity\": \"very_complex\",\n        \"description\": \"Extended interaction, deep reasoning, synthesis\"\n    }\n]\n```\n\n## Guidelines\n\n1. Use multi-dimensional rubrics, not single metrics\n2. Evaluate outcomes, not specific execution paths\n3. Cover complexity levels from simple to complex\n4. Test with realistic context sizes and histories\n5. Run evaluations continuously, not just before release\n6. Supplement LLM evaluation with human review\n7. Track metrics over time for trend detection\n8. Set clear pass/fail thresholds based on use case\n\n## Integration\n\nThis skill connects to all other skills as a cross-cutting concern:\n\n- context-fundamentals - Evaluating context usage\n- context-degradation - Detecting degradation\n- context-optimization - Measuring optimization effectiveness\n- multi-agent-patterns - Evaluating coordination\n- tool-design - Evaluating tool effectiveness\n- memory-systems - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Metrics Reference](./references/metrics.md) - Detailed evaluation metrics and implementation\n\n## References\n\nInternal skills:\n- All other skills connect to evaluation for quality measurement\n\nExternal resources:\n- LLM evaluation benchmarks\n- Agent evaluation research papers\n- Production monitoring practices\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "filesystem-context",
                "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading.",
                "path": "skills/filesystem-context/SKILL.md",
                "frontmatter": {
                  "name": "filesystem-context",
                  "description": "This skill should be used when the user asks to \"offload context to files\", \"implement dynamic context discovery\", \"use filesystem for agent memory\", \"reduce context window bloat\", or mentions file-based context management, tool output persistence, agent scratch pads, or just-in-time context loading."
                },
                "content": "# Filesystem-Based Context Engineering\n\nThe filesystem provides a single interface through which agents can flexibly store, retrieve, and update an effectively unlimited amount of context. This pattern addresses the fundamental constraint that context windows are limited while tasks often require more information than fits in a single window.\n\nThe core insight is that files enable dynamic context discovery: agents pull relevant context on demand rather than carrying everything in the context window. This contrasts with static context, which is always included regardless of relevance.\n\n## When to Activate\n\nActivate this skill when:\n- Tool outputs are bloating the context window\n- Agents need to persist state across long trajectories\n- Sub-agents must share information without direct message passing\n- Tasks require more context than fits in the window\n- Building agents that learn and update their own instructions\n- Implementing scratch pads for intermediate results\n- Terminal outputs or logs need to be accessible to agents\n\n## Core Concepts\n\nContext engineering can fail in four predictable ways. First, when the context an agent needs is not in the total available context. Second, when retrieved context fails to encapsulate needed context. Third, when retrieved context far exceeds needed context, wasting tokens and degrading performance. Fourth, when agents cannot discover niche information buried in many files.\n\nThe filesystem addresses these failures by providing a persistent layer where agents write once and read selectively, offloading bulk content while preserving the ability to retrieve specific information through search tools.\n\n## Detailed Topics\n\n### The Static vs Dynamic Context Trade-off\n\n**Static Context**\nStatic context is always included in the prompt: system instructions, tool definitions, and critical rules. Static context consumes tokens regardless of task relevance. As agents accumulate more capabilities (tools, skills, instructions), static context grows and crowds out space for dynamic information.\n\n**Dynamic Context Discovery**\nDynamic context is loaded on-demand when relevant to the current task. The agent receives minimal static pointers (names, descriptions, file paths) and uses search tools to load full content when needed.\n\nDynamic discovery is more token-efficient because only necessary data enters the context window. It can also improve response quality by reducing potentially confusing or contradictory information.\n\nThe trade-off: dynamic discovery requires the model to correctly identify when to load additional context. This works well with current frontier models but may fail with less capable models that do not recognize when they need more information.\n\n### Pattern 1: Filesystem as Scratch Pad\n\n**The Problem**\nTool calls can return massive outputs. A web search may return 10k tokens of raw content. A database query may return hundreds of rows. If this content enters the message history, it remains for the entire conversation, inflating token costs and potentially degrading attention to more relevant information.\n\n**The Solution**\nWrite large tool outputs to files instead of returning them directly to the context. The agent then uses targeted retrieval (grep, line-specific reads) to extract only the relevant portions.\n\n**Implementation**\n```python\ndef handle_tool_output(output: str, threshold: int = 2000) -> str:\n    if len(output) < threshold:\n        return output\n    \n    # Write to scratch pad\n    file_path = f\"scratch/{tool_name}_{timestamp}.txt\"\n    write_file(file_path, output)\n    \n    # Return reference instead of content\n    key_summary = extract_summary(output, max_tokens=200)\n    return f\"[Output written to {file_path}. Summary: {key_summary}]\"\n```\n\nThe agent can then use `grep` to search for specific patterns or `read_file` with line ranges to retrieve targeted sections.\n\n**Benefits**\n- Reduces token accumulation over long conversations\n- Preserves full output for later reference\n- Enables targeted retrieval instead of carrying everything\n\n### Pattern 2: Plan Persistence\n\n**The Problem**\nLong-horizon tasks require agents to make plans and follow them. But as conversations extend, plans can fall out of attention or be lost to summarization. The agent loses track of what it was supposed to do.\n\n**The Solution**\nWrite plans to the filesystem. The agent can re-read its plan at any point, reminding itself of the current objective and progress. This is sometimes called \"manipulating attention through recitation.\"\n\n**Implementation**\nStore plans in structured format:\n```yaml\n# scratch/current_plan.yaml\nobjective: \"Refactor authentication module\"\nstatus: in_progress\nsteps:\n  - id: 1\n    description: \"Audit current auth endpoints\"\n    status: completed\n  - id: 2\n    description: \"Design new token validation flow\"\n    status: in_progress\n  - id: 3\n    description: \"Implement and test changes\"\n    status: pending\n```\n\nThe agent reads this file at the start of each turn or when it needs to re-orient.\n\n### Pattern 3: Sub-Agent Communication via Filesystem\n\n**The Problem**\nIn multi-agent systems, sub-agents typically report findings to a coordinator agent through message passing. This creates a \"game of telephone\" where information degrades through summarization at each hop.\n\n**The Solution**\nSub-agents write their findings directly to the filesystem. The coordinator reads these files directly, bypassing intermediate message passing. This preserves fidelity and reduces context accumulation in the coordinator.\n\n**Implementation**\n```\nworkspace/\n  agents/\n    research_agent/\n      findings.md        # Research agent writes here\n      sources.jsonl      # Source tracking\n    code_agent/\n      changes.md         # Code agent writes here\n      test_results.txt   # Test output\n  coordinator/\n    synthesis.md         # Coordinator reads agent outputs, writes synthesis\n```\n\nEach agent operates in relative isolation but shares state through the filesystem.\n\n### Pattern 4: Dynamic Skill Loading\n\n**The Problem**\nAgents may have many skills or instruction sets, but most are irrelevant to any given task. Stuffing all instructions into the system prompt wastes tokens and can confuse the model with contradictory or irrelevant guidance.\n\n**The Solution**\nStore skills as files. Include only skill names and brief descriptions in static context. The agent uses search tools to load relevant skill content when the task requires it.\n\n**Implementation**\nStatic context includes:\n```\nAvailable skills (load with read_file when relevant):\n- database-optimization: Query tuning and indexing strategies\n- api-design: REST/GraphQL best practices\n- testing-strategies: Unit, integration, and e2e testing patterns\n```\n\nAgent loads `skills/database-optimization/SKILL.md` only when working on database tasks.\n\n### Pattern 5: Terminal and Log Persistence\n\n**The Problem**\nTerminal output from long-running processes accumulates rapidly. Copying and pasting output into agent input is manual and inefficient.\n\n**The Solution**\nSync terminal output to files automatically. The agent can then grep for relevant sections (error messages, specific commands) without loading entire terminal histories.\n\n**Implementation**\nTerminal sessions are persisted as files:\n```\nterminals/\n  1.txt    # Terminal session 1 output\n  2.txt    # Terminal session 2 output\n```\n\nAgents query with targeted grep:\n```bash\ngrep -A 5 \"error\" terminals/1.txt\n```\n\n### Pattern 6: Learning Through Self-Modification\n\n**The Problem**\nAgents often lack context that users provide implicitly or explicitly during interactions. Traditionally, this requires manual system prompt updates between sessions.\n\n**The Solution**\nAgents write learned information to their own instruction files. Subsequent sessions load these files, incorporating learned context automatically.\n\n**Implementation**\nAfter user provides preference:\n```python\ndef remember_preference(key: str, value: str):\n    preferences_file = \"agent/user_preferences.yaml\"\n    prefs = load_yaml(preferences_file)\n    prefs[key] = value\n    write_yaml(preferences_file, prefs)\n```\n\nSubsequent sessions include a step to load user preferences if the file exists.\n\n**Caution**\nThis pattern is still emerging. Self-modification requires careful guardrails to prevent agents from accumulating incorrect or contradictory instructions over time.\n\n### Filesystem Search Techniques\n\nModels are specifically trained to understand filesystem traversal. The combination of `ls`, `glob`, `grep`, and `read_file` with line ranges provides powerful context discovery:\n\n- `ls` / `list_dir`: Discover directory structure\n- `glob`: Find files matching patterns (e.g., `**/*.py`)\n- `grep`: Search file contents for patterns, returns matching lines\n- `read_file` with ranges: Read specific line ranges without loading entire files\n\nThis combination often outperforms semantic search for technical content (code, API docs) where semantic meaning is sparse but structural patterns are clear.\n\nSemantic search and filesystem search work well together: semantic search for conceptual queries, filesystem search for structural and exact-match queries.\n\n## Practical Guidance\n\n### When to Use Filesystem Context\n\n**Use filesystem patterns when:**\n- Tool outputs exceed 2000 tokens\n- Tasks span multiple conversation turns\n- Multiple agents need to share state\n- Skills or instructions exceed what fits comfortably in system prompt\n- Logs or terminal output need selective querying\n\n**Avoid filesystem patterns when:**\n- Tasks complete in single turns\n- Context fits comfortably in window\n- Latency is critical (file I/O adds overhead)\n- Simple model incapable of filesystem tool use\n\n### File Organization\n\nStructure files for discoverability:\n```\nproject/\n  scratch/           # Temporary working files\n    tool_outputs/    # Large tool results\n    plans/           # Active plans and checklists\n  memory/            # Persistent learned information\n    preferences.yaml # User preferences\n    patterns.md      # Learned patterns\n  skills/            # Loadable skill definitions\n  agents/            # Sub-agent workspaces\n```\n\nUse consistent naming conventions. Include timestamps or IDs in scratch files for disambiguation.\n\n### Token Accounting\n\nTrack where tokens originate:\n- Measure static vs dynamic context ratio\n- Monitor tool output sizes before and after offloading\n- Track how often dynamic context is actually loaded\n\nOptimize based on measurements, not assumptions.\n\n## Examples\n\n**Example 1: Tool Output Offloading**\n```\nInput: Web search returns 8000 tokens\nBefore: 8000 tokens added to message history\nAfter: \n  - Write to scratch/search_results_001.txt\n  - Return: \"[Results in scratch/search_results_001.txt. Key finding: API rate limit is 1000 req/min]\"\n  - Agent greps file when needing specific details\nResult: ~100 tokens in context, 8000 tokens accessible on demand\n```\n\n**Example 2: Dynamic Skill Loading**\n```\nInput: User asks about database indexing\nStatic context: \"database-optimization: Query tuning and indexing\"\nAgent action: read_file(\"skills/database-optimization/SKILL.md\")\nResult: Full skill loaded only when relevant\n```\n\n**Example 3: Chat History as File Reference**\n```\nTrigger: Context window limit reached, summarization required\nAction: \n  1. Write full history to history/session_001.txt\n  2. Generate summary for new context window\n  3. Include reference: \"Full history in history/session_001.txt\"\nResult: Agent can search history file to recover details lost in summarization\n```\n\n## Guidelines\n\n1. Write large outputs to files; return summaries and references to context\n2. Store plans and state in structured files for re-reading\n3. Use sub-agent file workspaces instead of message chains\n4. Load skills dynamically rather than stuffing all into system prompt\n5. Persist terminal and log output as searchable files\n6. Combine grep/glob with semantic search for comprehensive discovery\n7. Organize files for agent discoverability with clear naming\n8. Measure token savings to validate filesystem patterns are effective\n9. Implement cleanup for scratch files to prevent unbounded growth\n10. Guard self-modification patterns with validation\n\n## Integration\n\nThis skill connects to:\n\n- context-optimization - Filesystem offloading is a form of observation masking\n- memory-systems - Filesystem-as-memory is a simple memory layer\n- multi-agent-patterns - Sub-agent file workspaces enable isolation\n- context-compression - File references enable lossless \"compression\"\n- tool-design - Tools should return file references for large outputs\n\n## References\n\nInternal reference:\n- [Implementation Patterns](./references/implementation-patterns.md) - Detailed pattern implementations\n\nRelated skills in this collection:\n- context-optimization - Token reduction techniques\n- memory-systems - Persistent storage patterns\n- multi-agent-patterns - Agent coordination\n\nExternal resources:\n- LangChain Deep Agents: How agents can use filesystems for context engineering\n- Cursor: Dynamic context discovery patterns\n- Anthropic: Agent Skills specification\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-07\n**Last Updated**: 2026-01-07\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "hosted-agents",
                "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments.",
                "path": "skills/hosted-agents/SKILL.md",
                "frontmatter": {
                  "name": "hosted-agents",
                  "description": "This skill should be used when the user asks to \"build background agent\", \"create hosted coding agent\", \"set up sandboxed execution\", \"implement multiplayer agent\", or mentions background agents, sandboxed VMs, agent infrastructure, Modal sandboxes, self-spawning agents, or remote coding environments."
                },
                "content": "# Hosted Agent Infrastructure\n\nHosted agents run in remote sandboxed environments rather than on local machines. When designed well, they provide unlimited concurrency, consistent execution environments, and multiplayer collaboration. The critical insight is that session speed should be limited only by model provider time-to-first-token, with all infrastructure setup completed before the user starts their session.\n\n## When to Activate\n\nActivate this skill when:\n- Building background coding agents that run independently of user devices\n- Designing sandboxed execution environments for agent workloads\n- Implementing multiplayer agent sessions with shared state\n- Creating multi-client agent interfaces (Slack, Web, Chrome extensions)\n- Scaling agent infrastructure beyond local machine constraints\n- Building systems where agents spawn sub-agents for parallel work\n\n## Core Concepts\n\nHosted agents address the fundamental limitation of local agent execution: resource contention, environment inconsistency, and single-user constraints. By moving agent execution to remote sandboxed environments, teams gain unlimited concurrency, reproducible environments, and collaborative workflows.\n\nThe architecture consists of three layers: sandbox infrastructure for isolated execution, API layer for state management and client coordination, and client interfaces for user interaction across platforms. Each layer has specific design requirements that enable the system to scale.\n\n## Detailed Topics\n\n### Sandbox Infrastructure\n\n**The Core Challenge**\nSpinning up full development environments quickly is the primary technical challenge. Users expect near-instant session starts, but development environments require cloning repositories, installing dependencies, and running build steps.\n\n**Image Registry Pattern**\nPre-build environment images on a regular cadence (every 30 minutes works well). Each image contains:\n- Cloned repository at a known commit\n- All runtime dependencies installed\n- Initial setup and build commands completed\n- Cached files from running app and test suite once\n\nWhen starting a session, spin up a sandbox from the most recent image. The repository is at most 30 minutes out of date, making synchronization with the latest code much faster.\n\n**Snapshot and Restore**\nTake filesystem snapshots at key points:\n- After initial image build (base snapshot)\n- When agent finishes making changes (session snapshot)\n- Before sandbox exit for potential follow-up\n\nThis enables instant restoration for follow-up prompts without re-running setup.\n\n**Git Configuration for Background Agents**\nSince git operations are not tied to a specific user during image builds:\n- Generate GitHub app installation tokens for repository access during clone\n- Update git config's `user.name` and `user.email` when committing and pushing changes\n- Use the prompting user's identity for commits, not the app identity\n\n**Warm Pool Strategy**\nMaintain a pool of pre-warmed sandboxes for high-volume repositories:\n- Sandboxes are ready before users start sessions\n- Expire and recreate pool entries as new image builds complete\n- Start warming sandbox as soon as user begins typing (predictive warm-up)\n\n### Agent Framework Selection\n\n**Server-First Architecture**\nChoose an agent framework structured as a server first, with TUI and desktop apps as clients. This enables:\n- Multiple custom clients without duplicating agent logic\n- Consistent behavior across all interaction surfaces\n- Plugin systems for extending functionality\n- Event-driven architectures for real-time updates\n\n**Code as Source of Truth**\nSelect frameworks where the agent can read its own source code to understand behavior. This is underrated in AI development: having the code as source of truth prevents hallucination about the agent's own capabilities.\n\n**Plugin System Requirements**\nThe framework should support plugins that:\n- Listen to tool execution events (e.g., `tool.execute.before`)\n- Block or modify tool calls conditionally\n- Inject context or state at runtime\n\n### Speed Optimizations\n\n**Predictive Warm-Up**\nStart warming the sandbox as soon as a user begins typing their prompt:\n- Clone latest changes in parallel with user typing\n- Run initial setup before user hits enter\n- For fast spin-up, sandbox can be ready before user finishes typing\n\n**Parallel File Reading**\nAllow the agent to start reading files immediately, even if sync from latest base branch is not complete:\n- In large repositories, incoming prompts rarely modify recently-changed files\n- Agent can research immediately without waiting for git sync\n- Block file edits (not reads) until synchronization completes\n\n**Maximize Build-Time Work**\nMove everything possible to the image build step:\n- Full dependency installation\n- Database schema setup\n- Initial app and test suite runs (populates caches)\n- Build-time duration is invisible to users\n\n### Self-Spawning Agents\n\n**Agent-Spawned Sessions**\nCreate tools that allow agents to spawn new sessions:\n- Research tasks across different repositories\n- Parallel subtask execution for large changes\n- Multiple smaller PRs from one major task\n\nFrontier models are capable of containing themselves. The tools should:\n- Start a new session with specified parameters\n- Read status of any session (check-in capability)\n- Continue main work while sub-sessions run in parallel\n\n**Prompt Engineering for Self-Spawning**\nEngineer prompts to guide when agents spawn sub-sessions:\n- Research tasks that require cross-repository exploration\n- Breaking monolithic changes into smaller PRs\n- Parallel exploration of different approaches\n\n### API Layer\n\n**Per-Session State Isolation**\nEach session requires its own isolated state storage:\n- Dedicated database per session (SQLite per session works well)\n- No session can impact another's performance\n- Handles hundreds of concurrent sessions\n\n**Real-Time Streaming**\nAgent work involves high-frequency updates:\n- Token streaming from model providers\n- Tool execution status updates\n- File change notifications\n\nWebSocket connections with hibernation APIs reduce compute costs during idle periods while maintaining open connections.\n\n**Synchronization Across Clients**\nBuild a single state system that synchronizes across:\n- Chat interfaces\n- Slack bots\n- Chrome extensions\n- Web interfaces\n- VS Code instances\n\nAll changes sync to the session state, enabling seamless client switching.\n\n### Multiplayer Support\n\n**Why Multiplayer Matters**\nMultiplayer enables:\n- Teaching non-engineers to use AI effectively\n- Live QA sessions with multiple team members\n- Real-time PR review with immediate changes\n- Collaborative debugging sessions\n\n**Implementation Requirements**\n- Data model must not tie sessions to single authors\n- Pass authorship info to each prompt\n- Attribute code changes to the prompting user\n- Share session links for instant collaboration\n\nWith proper synchronization architecture, multiplayer support is nearly free to add.\n\n### Authentication and Authorization\n\n**User-Based Commits**\nUse GitHub authentication to:\n- Obtain user tokens for PR creation\n- Open PRs on behalf of the user (not the app)\n- Prevent users from approving their own changes\n\n**Sandbox-to-API Flow**\n1. Sandbox pushes changes (updating git user config)\n2. Sandbox sends event to API with branch name and session ID\n3. API uses user's GitHub token to create PR\n4. GitHub webhooks notify API of PR events\n\n### Client Implementations\n\n**Slack Integration**\nThe most effective distribution channel for internal adoption:\n- Creates virality loop as team members see others using it\n- No syntax required, natural chat interface\n- Classify repository from message, thread context, and channel name\n\nBuild a classifier to determine which repository to work in:\n- Fast model with descriptions of available repositories\n- Include hints for common repositories\n- Allow \"unknown\" option for ambiguous cases\n\n**Web Interface**\nCore features:\n- Works on desktop and mobile\n- Real-time streaming of agent work\n- Hosted VS Code instance running inside sandbox\n- Streamed desktop view for visual verification\n- Before/after screenshots for PRs\n\nStatistics page showing:\n- Sessions resulting in merged PRs (primary metric)\n- Usage over time\n- Live \"humans prompting\" count (prompts in last 5 minutes)\n\n**Chrome Extension**\nFor non-engineering users:\n- Sidebar chat interface with screenshot tool\n- DOM and React internals extraction instead of raw images\n- Reduces token usage while maintaining precision\n- Distribute via managed device policy (bypasses Chrome Web Store)\n\n## Practical Guidance\n\n### Follow-Up Message Handling\n\nDecide how to handle messages sent during execution:\n- **Queue approach**: Messages wait until current prompt completes\n- **Insert approach**: Messages are processed immediately\n\nQueueing is simpler to manage and lets users send thoughts on next steps while agent works. Build mechanism to stop agent mid-execution when needed.\n\n### Metrics That Matter\n\nTrack metrics that indicate real value:\n- Sessions resulting in merged PRs (primary success metric)\n- Time from session start to first model response\n- PR approval rate and revision count\n- Agent-written code percentage across repositories\n\n### Adoption Strategy\n\nInternal adoption patterns that work:\n- Work in public spaces (Slack channels) for visibility\n- Let the product create virality loops\n- Don't force usage over existing tools\n- Build to people's needs, not hypothetical requirements\n\n## Guidelines\n\n1. Pre-build environment images on regular cadence (30 minutes is a good default)\n2. Start warming sandboxes when users begin typing, not when they submit\n3. Allow file reads before git sync completes; block only writes\n4. Structure agent framework as server-first with clients as thin wrappers\n5. Isolate state per session to prevent cross-session interference\n6. Attribute commits to the user who prompted, not the app\n7. Track merged PRs as primary success metric\n8. Build for multiplayer from the start; it is nearly free with proper sync architecture\n\n## Integration\n\nThis skill builds on multi-agent-patterns for agent coordination and tool-design for agent-tool interfaces. It connects to:\n\n- multi-agent-patterns - Self-spawning agents follow supervisor patterns\n- tool-design - Building tools for agent spawning and status checking\n- context-optimization - Managing context across distributed sessions\n- filesystem-context - Using filesystem for session state and artifacts\n\n## References\n\nInternal reference:\n- [Infrastructure Patterns](./references/infrastructure-patterns.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- multi-agent-patterns - Coordination patterns for self-spawning agents\n- tool-design - Designing tools for hosted environments\n- context-optimization - Managing context in distributed systems\n\nExternal resources:\n- [Ramp](https://builders.ramp.com/post/why-we-built-our-background-agent) - Why We Built Our Own Background Agent\n- [Modal Sandboxes](https://modal.com/docs/guide/sandbox) - Cloud sandbox infrastructure\n- [Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) - Per-session state management\n- [OpenCode](https://github.com/sst/opencode) - Server-first agent framework\n\n---\n\n## Skill Metadata\n\n**Created**: 2026-01-12\n**Last Updated**: 2026-01-12\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "memory-systems",
                "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence.",
                "path": "skills/memory-systems/SKILL.md",
                "frontmatter": {
                  "name": "memory-systems",
                  "description": "This skill should be used when the user asks to \"implement agent memory\", \"persist state across sessions\", \"build knowledge graph\", \"track entities\", or mentions memory architecture, temporal knowledge graphs, vector stores, entity memory, or cross-session persistence."
                },
                "content": "# Memory System Design\n\nMemory provides the persistence layer that allows agents to maintain continuity across sessions and reason over accumulated knowledge. Simple agents rely entirely on context for memory, losing all state when sessions end. Sophisticated agents implement layered memory architectures that balance immediate context needs with long-term knowledge retention. The evolution from vector stores to knowledge graphs to temporal knowledge graphs represents increasing investment in structured memory for improved retrieval and reasoning.\n\n## When to Activate\n\nActivate this skill when:\n- Building agents that must persist across sessions\n- Needing to maintain entity consistency across conversations\n- Implementing reasoning over accumulated knowledge\n- Designing systems that learn from past interactions\n- Creating knowledge bases that grow over time\n- Building temporal-aware systems that track state changes\n\n## Core Concepts\n\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context.\n\nSimple vector stores lack relationship and temporal structure. Knowledge graphs preserve relationships for reasoning. Temporal knowledge graphs add validity periods for time-aware queries. Implementation choices depend on query complexity, infrastructure constraints, and accuracy requirements.\n\n## Detailed Topics\n\n### Memory Architecture Fundamentals\n\n**The Context-Memory Spectrum**\nMemory exists on a spectrum from immediate context to permanent storage. At one extreme, working memory in the context window provides zero-latency access but vanishes when sessions end. At the other extreme, permanent storage persists indefinitely but requires retrieval to enter context. Effective architectures use multiple layers along this spectrum.\n\nThe spectrum includes working memory (context window, zero latency, volatile), short-term memory (session-persistent, searchable, volatile), long-term memory (cross-session persistent, structured, semi-permanent), and permanent memory (archival, queryable, permanent). Each layer has different latency, capacity, and persistence characteristics.\n\n**Why Simple Vector Stores Fall Short**\nVector RAG provides semantic retrieval by embedding queries and documents in a shared embedding space. Similarity search retrieves the most semantically similar documents. This works well for document retrieval but lacks structure for agent memory.\n\nVector stores lose relationship information. If an agent learns that \"Customer X purchased Product Y on Date Z,\" a vector store can retrieve this fact if asked directly. But it cannot answer \"What products did customers who purchased Product Y also buy?\" because relationship structure is not preserved.\n\nVector stores also struggle with temporal validity. Facts change over time, but vector stores provide no mechanism to distinguish \"current fact\" from \"outdated fact\" except through explicit metadata and filtering.\n\n**The Move to Graph-Based Memory**\nKnowledge graphs preserve relationships between entities. Instead of isolated document chunks, graphs encode that Entity A has Relationship R to Entity B. This enables queries that traverse relationships rather than just similarity.\n\nTemporal knowledge graphs add validity periods to facts. Each fact has a \"valid from\" and optionally \"valid until\" timestamp. This enables time-travel queries that reconstruct knowledge at specific points in time.\n\n**Benchmark Performance Comparison**\nThe Deep Memory Retrieval (DMR) benchmark provides concrete performance data across memory architectures:\n\n| Memory System | DMR Accuracy | Retrieval Latency | Notes |\n|---------------|--------------|-------------------|-------|\n| Zep (Temporal KG) | 94.8% | 2.58s | Best accuracy, fast retrieval |\n| MemGPT | 93.4% | Variable | Good general performance |\n| GraphRAG | ~75-85% | Variable | 20-35% gains over baseline RAG |\n| Vector RAG | ~60-70% | Fast | Loses relationship structure |\n| Recursive Summarization | 35.3% | Low | Severe information loss |\n\nZep demonstrated 90% reduction in retrieval latency compared to full-context baselines (2.58s vs 28.9s for GPT-5.2). This efficiency comes from retrieving only relevant subgraphs rather than entire context history.\n\nGraphRAG achieves approximately 20-35% accuracy gains over baseline RAG in complex reasoning tasks and reduces hallucination by up to 30% through community-based summarization.\n\n### Memory Layer Architecture\n\n**Layer 1: Working Memory**\nWorking memory is the context window itself. It provides immediate access to information currently being processed but has limited capacity and vanishes when sessions end.\n\nWorking memory usage patterns include scratchpad calculations where agents track intermediate results, conversation history that preserves dialogue for current task, current task state that tracks progress on active objectives, and active retrieved documents that hold information currently being used.\n\nOptimize working memory by keeping only active information, summarizing completed work before it falls out of attention, and using attention-favored positions for critical information.\n\n**Layer 2: Short-Term Memory**\nShort-term memory persists across the current session but not across sessions. It provides search and retrieval capabilities without the latency of permanent storage.\n\nCommon implementations include session-scoped databases that persist until session end, file-system storage in designated session directories, and in-memory caches keyed by session ID.\n\nShort-term memory use cases include tracking conversation state across turns without stuffing context, storing intermediate results from tool calls that may be needed later, maintaining task checklists and progress tracking, and caching retrieved information within sessions.\n\n**Layer 3: Long-Term Memory**\nLong-term memory persists across sessions indefinitely. It enables agents to learn from past interactions and build knowledge over time.\n\nLong-term memory implementations range from simple key-value stores to sophisticated graph databases. The choice depends on complexity of relationships to model, query patterns required, and acceptable infrastructure complexity.\n\nLong-term memory use cases include learning user preferences across sessions, building domain knowledge bases that grow over time, maintaining entity registries with relationship history, and storing successful patterns that can be reused.\n\n**Layer 4: Entity Memory**\nEntity memory specifically tracks information about entities (people, places, concepts, objects) to maintain consistency. This creates a rudimentary knowledge graph where entities are recognized across multiple interactions.\n\nEntity memory maintains entity identity by tracking that \"John Doe\" mentioned in one conversation is the same person in another. It maintains entity properties by storing facts discovered about entities over time. It maintains entity relationships by tracking relationships between entities as they are discovered.\n\n**Layer 5: Temporal Knowledge Graphs**\nTemporal knowledge graphs extend entity memory with explicit validity periods. Facts are not just true or false but true during specific time ranges.\n\nThis enables queries like \"What was the user's address on Date X?\" by retrieving facts valid during that date range. It prevents context clash when outdated information contradicts new data. It enables temporal reasoning about how entities changed over time.\n\n### Memory Implementation Patterns\n\n**Pattern 1: File-System-as-Memory**\nThe file system itself can serve as a memory layer. This pattern is simple, requires no additional infrastructure, and enables the same just-in-time loading that makes file-system-based context effective.\n\nImplementation uses the file system hierarchy for organization. Use naming conventions that convey meaning. Store facts in structured formats (JSON, YAML). Use timestamps in filenames or metadata for temporal tracking.\n\nAdvantages: Simplicity, transparency, portability.\nDisadvantages: No semantic search, no relationship tracking, manual organization required.\n\n**Pattern 2: Vector RAG with Metadata**\nVector stores enhanced with rich metadata provide semantic search with filtering capabilities.\n\nImplementation embeds facts or documents and stores with metadata including entity tags, temporal validity, source attribution, and confidence scores. Query includes metadata filters alongside semantic search.\n\n**Pattern 3: Knowledge Graph**\nKnowledge graphs explicitly model entities and relationships. Implementation defines entity types and relationship types, uses graph database or property graph storage, and maintains indexes for common query patterns.\n\n**Pattern 4: Temporal Knowledge Graph**\nTemporal knowledge graphs add validity periods to facts, enabling time-travel queries and preventing context clash from outdated information.\n\n### Memory Retrieval Patterns\n\n**Semantic Retrieval**\nRetrieve memories semantically similar to current query using embedding similarity search.\n\n**Entity-Based Retrieval**\nRetrieve all memories related to specific entities by traversing graph relationships.\n\n**Temporal Retrieval**\nRetrieve memories valid at specific time or within time range using validity period filters.\n\n### Memory Consolidation\n\nMemories accumulate over time and require consolidation to prevent unbounded growth and remove outdated information.\n\n**Consolidation Triggers**\nTrigger consolidation after significant memory accumulation, when retrieval returns too many outdated results, periodically on a schedule, or when explicit consolidation is requested.\n\n**Consolidation Process**\nIdentify outdated facts, merge related facts, update validity periods, archive or delete obsolete facts, and rebuild indexes.\n\n## Practical Guidance\n\n### Integration with Context\n\nMemories must integrate with context systems to be useful. Use just-in-time memory loading to retrieve relevant memories when needed. Use strategic injection to place memories in attention-favored positions.\n\n### Memory System Selection\n\nChoose memory architecture based on requirements:\n- Simple persistence needs: File-system memory\n- Semantic search needs: Vector RAG with metadata\n- Relationship reasoning needs: Knowledge graph\n- Temporal validity needs: Temporal knowledge graph\n\n## Examples\n\n**Example 1: Entity Tracking**\n```python\n# Track entity across conversations\ndef remember_entity(entity_id, properties):\n    memory.store({\n        \"type\": \"entity\",\n        \"id\": entity_id,\n        \"properties\": properties,\n        \"last_updated\": now()\n    })\n\ndef get_entity(entity_id):\n    return memory.retrieve_entity(entity_id)\n```\n\n**Example 2: Temporal Query**\n```python\n# What was the user's address on January 15, 2024?\ndef query_address_at_time(user_id, query_time):\n    return temporal_graph.query(\"\"\"\n        MATCH (user)-[r:LIVES_AT]->(address)\n        WHERE user.id = $user_id\n        AND r.valid_from <= $query_time\n        AND (r.valid_until IS NULL OR r.valid_until > $query_time)\n        RETURN address\n    \"\"\", {\"user_id\": user_id, \"query_time\": query_time})\n```\n\n## Guidelines\n\n1. Match memory architecture to query requirements\n2. Implement progressive disclosure for memory access\n3. Use temporal validity to prevent outdated information conflicts\n4. Consolidate memories periodically to prevent unbounded growth\n5. Design for memory retrieval failures gracefully\n6. Consider privacy implications of persistent memory\n7. Implement backup and recovery for critical memories\n8. Monitor memory growth and performance over time\n\n## Integration\n\nThis skill builds on context-fundamentals. It connects to:\n\n- multi-agent-patterns - Shared memory across agents\n- context-optimization - Memory-based context loading\n- evaluation - Evaluating memory quality\n\n## References\n\nInternal reference:\n- [Implementation Reference](./references/implementation.md) - Detailed implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- multi-agent-patterns - Cross-agent memory\n\nExternal resources:\n- Graph database documentation (Neo4j, etc.)\n- Vector store documentation (Pinecone, Weaviate, etc.)\n- Research on knowledge graphs and reasoning\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "multi-agent-patterns",
                "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution.",
                "path": "skills/multi-agent-patterns/SKILL.md",
                "frontmatter": {
                  "name": "multi-agent-patterns",
                  "description": "This skill should be used when the user asks to \"design multi-agent system\", \"implement supervisor pattern\", \"create swarm architecture\", \"coordinate multiple agents\", or mentions multi-agent patterns, context isolation, agent handoffs, sub-agents, or parallel agent execution."
                },
                "content": "# Multi-Agent Architecture Patterns\n\nMulti-agent architectures distribute work across multiple language model instances, each with its own context window. When designed well, this distribution enables capabilities beyond single-agent limits. When designed poorly, it introduces coordination overhead that negates benefits. The critical insight is that sub-agents exist primarily to isolate context, not to anthropomorphize role division.\n\n## When to Activate\n\nActivate this skill when:\n- Single-agent context limits constrain task complexity\n- Tasks decompose naturally into parallel subtasks\n- Different subtasks require different tool sets or system prompts\n- Building systems that must handle multiple domains simultaneously\n- Scaling agent capabilities beyond single-context limits\n- Designing production agent systems with multiple specialized components\n\n## Core Concepts\n\nMulti-agent systems address single-agent context limitations through distribution. Three dominant patterns exist: supervisor/orchestrator for centralized control, peer-to-peer/swarm for flexible handoffs, and hierarchical for layered abstraction. The critical design principle is context isolation—sub-agents exist primarily to partition context rather than to simulate organizational roles.\n\nEffective multi-agent systems require explicit coordination protocols, consensus mechanisms that avoid sycophancy, and careful attention to failure modes including bottlenecks, divergence, and error propagation.\n\n## Detailed Topics\n\n### Why Multi-Agent Architectures\n\n**The Context Bottleneck**\nSingle agents face inherent ceilings in reasoning capability, context management, and tool coordination. As tasks grow more complex, context windows fill with accumulated history, retrieved documents, and tool outputs. Performance degrades according to predictable patterns: the lost-in-middle effect, attention scarcity, and context poisoning.\n\nMulti-agent architectures address these limitations by partitioning work across multiple context windows. Each agent operates in a clean context focused on its subtask. Results aggregate at a coordination layer without any single context bearing the full burden.\n\n**The Token Economics Reality**\nMulti-agent systems consume significantly more tokens than single-agent approaches. Production data shows:\n\n| Architecture | Token Multiplier | Use Case |\n|--------------|------------------|----------|\n| Single agent chat | 1× baseline | Simple queries |\n| Single agent with tools | ~4× baseline | Tool-using tasks |\n| Multi-agent system | ~15× baseline | Complex research/coordination |\n\nResearch on the BrowseComp evaluation found that three factors explain 95% of performance variance: token usage (80% of variance), number of tool calls, and model choice. This validates the multi-agent approach of distributing work across agents with separate context windows to add capacity for parallel reasoning.\n\nCritically, upgrading to better models often provides larger performance gains than doubling token budgets. Claude Sonnet 4.5 showed larger gains than doubling tokens on earlier Sonnet versions. GPT-5.2's thinking mode similarly outperforms raw token increases. This suggests model selection and multi-agent architecture are complementary strategies.\n\n**The Parallelization Argument**\nMany tasks contain parallelizable subtasks that a single agent must execute sequentially. A research task might require searching multiple independent sources, analyzing different documents, or comparing competing approaches. A single agent processes these sequentially, accumulating context with each step.\n\nMulti-agent architectures assign each subtask to a dedicated agent with a fresh context. All agents work simultaneously, then return results to a coordinator. The total real-world time approaches the duration of the longest subtask rather than the sum of all subtasks.\n\n**The Specialization Argument**\nDifferent tasks benefit from different agent configurations: different system prompts, different tool sets, different context structures. A general-purpose agent must carry all possible configurations in context. Specialized agents carry only what they need.\n\nMulti-agent architectures enable specialization without combinatorial explosion. The coordinator routes to specialized agents; each agent operates with lean context optimized for its domain.\n\n### Architectural Patterns\n\n**Pattern 1: Supervisor/Orchestrator**\nThe supervisor pattern places a central agent in control, delegating to specialists and synthesizing results. The supervisor maintains global state and trajectory, decomposes user objectives into subtasks, and routes to appropriate workers.\n\n```\nUser Query -> Supervisor -> [Specialist, Specialist, Specialist] -> Aggregation -> Final Output\n```\n\nWhen to use: Complex tasks with clear decomposition, tasks requiring coordination across domains, tasks where human oversight is important.\n\nAdvantages: Strict control over workflow, easier to implement human-in-the-loop interventions, ensures adherence to predefined plans.\n\nDisadvantages: Supervisor context becomes bottleneck, supervisor failures cascade to all workers, \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly.\n\n**The Telephone Game Problem and Solution**\nLangGraph benchmarks found supervisor architectures initially performed 50% worse than optimized versions due to the \"telephone game\" problem where supervisors paraphrase sub-agent responses incorrectly, losing fidelity.\n\nThe fix: implement a `forward_message` tool allowing sub-agents to pass responses directly to users:\n\n```python\ndef forward_message(message: str, to_user: bool = True):\n    \"\"\"\n    Forward sub-agent response directly to user without supervisor synthesis.\n    \n    Use when:\n    - Sub-agent response is final and complete\n    - Supervisor synthesis would lose important details\n    - Response format must be preserved exactly\n    \"\"\"\n    if to_user:\n        return {\"type\": \"direct_response\", \"content\": message}\n    return {\"type\": \"supervisor_input\", \"content\": message}\n```\n\nWith this pattern, swarm architectures slightly outperform supervisors because sub-agents respond directly to users, eliminating translation errors.\n\nImplementation note: Implement direct pass-through mechanisms allowing sub-agents to pass responses directly to users rather than through supervisor synthesis when appropriate.\n\n**Pattern 2: Peer-to-Peer/Swarm**\nThe peer-to-peer pattern removes central control, allowing agents to communicate directly based on predefined protocols. Any agent can transfer control to any other through explicit handoff mechanisms.\n\n```python\ndef transfer_to_agent_b():\n    return agent_b  # Handoff via function return\n\nagent_a = Agent(\n    name=\"Agent A\",\n    functions=[transfer_to_agent_b]\n)\n```\n\nWhen to use: Tasks requiring flexible exploration, tasks where rigid planning is counterproductive, tasks with emergent requirements that defy upfront decomposition.\n\nAdvantages: No single point of failure, scales effectively for breadth-first exploration, enables emergent problem-solving behaviors.\n\nDisadvantages: Coordination complexity increases with agent count, risk of divergence without central state keeper, requires robust convergence constraints.\n\nImplementation note: Define explicit handoff protocols with state passing. Ensure agents can communicate their context needs to receiving agents.\n\n**Pattern 3: Hierarchical**\nHierarchical structures organize agents into layers of abstraction: strategic, planning, and execution layers. Strategy layer agents define goals and constraints; planning layer agents break goals into actionable plans; execution layer agents perform atomic tasks.\n\n```\nStrategy Layer (Goal Definition) -> Planning Layer (Task Decomposition) -> Execution Layer (Atomic Tasks)\n```\n\nWhen to use: Large-scale projects with clear hierarchical structure, enterprise workflows with management layers, tasks requiring both high-level planning and detailed execution.\n\nAdvantages: Mirrors organizational structures, clear separation of concerns, enables different context structures at different levels.\n\nDisadvantages: Coordination overhead between layers, potential for misalignment between strategy and execution, complex error propagation.\n\n### Context Isolation as Design Principle\n\nThe primary purpose of multi-agent architectures is context isolation. Each sub-agent operates in a clean context window focused on its subtask without carrying accumulated context from other subtasks.\n\n**Isolation Mechanisms**\nFull context delegation: For complex tasks where the sub-agent needs complete understanding, the planner shares its entire context. The sub-agent has its own tools and instructions but receives full context for its decisions.\n\nInstruction passing: For simple, well-defined subtasks, the planner creates instructions via function call. The sub-agent receives only the instructions needed for its specific task.\n\nFile system memory: For complex tasks requiring shared state, agents read and write to persistent storage. The file system serves as the coordination mechanism, avoiding context bloat from shared state passing.\n\n**Isolation Trade-offs**\nFull context delegation provides maximum capability but defeats the purpose of sub-agents. Instruction passing maintains isolation but limits sub-agent flexibility. File system memory enables shared state without context passing but introduces latency and consistency challenges.\n\nThe right choice depends on task complexity, coordination needs, and acceptable latency.\n\n### Consensus and Coordination\n\n**The Voting Problem**\nSimple majority voting treats hallucinations from weak models as equal to reasoning from strong models. Without intervention, multi-agent discussions devolve into consensus on false premises due to inherent bias toward agreement.\n\n**Weighted Voting**\nWeight agent votes by confidence or expertise. Agents with higher confidence or domain expertise carry more weight in final decisions.\n\n**Debate Protocols**\nDebate protocols require agents to critique each other's outputs over multiple rounds. Adversarial critique often yields higher accuracy on complex reasoning than collaborative consensus.\n\n**Trigger-Based Intervention**\nMonitor multi-agent interactions for specific behavioral markers. Stall triggers activate when discussions make no progress. Sycophancy triggers detect when agents mimic each other's answers without unique reasoning.\n\n### Framework Considerations\n\nDifferent frameworks implement these patterns with different philosophies. LangGraph uses graph-based state machines with explicit nodes and edges. AutoGen uses conversational/event-driven patterns with GroupChat. CrewAI uses role-based process flows with hierarchical crew structures.\n\n## Practical Guidance\n\n### Failure Modes and Mitigations\n\n**Failure: Supervisor Bottleneck**\nThe supervisor accumulates context from all workers, becoming susceptible to saturation and degradation.\n\nMitigation: Implement output schema constraints so workers return only distilled summaries. Use checkpointing to persist supervisor state without carrying full history.\n\n**Failure: Coordination Overhead**\nAgent communication consumes tokens and introduces latency. Complex coordination can negate parallelization benefits.\n\nMitigation: Minimize communication through clear handoff protocols. Batch results where possible. Use asynchronous communication patterns.\n\n**Failure: Divergence**\nAgents pursuing different goals without central coordination can drift from intended objectives.\n\nMitigation: Define clear objective boundaries for each agent. Implement convergence checks that verify progress toward shared goals. Use time-to-live limits on agent execution.\n\n**Failure: Error Propagation**\nErrors in one agent's output propagate to downstream agents that consume that output.\n\nMitigation: Validate agent outputs before passing to consumers. Implement retry logic with circuit breakers. Use idempotent operations where possible.\n\n## Examples\n\n**Example 1: Research Team Architecture**\n```text\nSupervisor\n├── Researcher (web search, document retrieval)\n├── Analyzer (data analysis, statistics)\n├── Fact-checker (verification, validation)\n└── Writer (report generation, formatting)\n```\n\n**Example 2: Handoff Protocol**\n```python\ndef handle_customer_request(request):\n    if request.type == \"billing\":\n        return transfer_to(billing_agent)\n    elif request.type == \"technical\":\n        return transfer_to(technical_agent)\n    elif request.type == \"sales\":\n        return transfer_to(sales_agent)\n    else:\n        return handle_general(request)\n```\n\n## Guidelines\n\n1. Design for context isolation as the primary benefit of multi-agent systems\n2. Choose architecture pattern based on coordination needs, not organizational metaphor\n3. Implement explicit handoff protocols with state passing\n4. Use weighted voting or debate protocols for consensus\n5. Monitor for supervisor bottlenecks and implement checkpointing\n6. Validate outputs before passing between agents\n7. Set time-to-live limits to prevent infinite loops\n8. Test failure scenarios explicitly\n\n## Integration\n\nThis skill builds on context-fundamentals and context-degradation. It connects to:\n\n- memory-systems - Shared state management across agents\n- tool-design - Tool specialization per agent\n- context-optimization - Context partitioning strategies\n\n## References\n\nInternal reference:\n- [Frameworks Reference](./references/frameworks.md) - Detailed framework implementation patterns\n\nRelated skills in this collection:\n- context-fundamentals - Context basics\n- memory-systems - Cross-agent memory\n- context-optimization - Partitioning strategies\n\nExternal resources:\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) - Multi-agent patterns and state management\n- [AutoGen Framework](https://microsoft.github.io/autogen/) - GroupChat and conversational patterns\n- [CrewAI Documentation](https://docs.crewai.com/) - Hierarchical agent processes\n- [Research on Multi-Agent Coordination](https://arxiv.org/abs/2308.00352) - Survey of multi-agent systems\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-20\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "project-development",
                "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
                "path": "skills/project-development/SKILL.md",
                "frontmatter": {
                  "name": "project-development",
                  "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches."
                },
                "content": "# Project Development Methodology\n\nThis skill covers the principles for identifying tasks suited to LLM processing, designing effective project architectures, and iterating rapidly using agent-assisted development. The methodology applies whether building a batch processing pipeline, a multi-agent research system, or an interactive agent application.\n\n## When to Activate\n\nActivate this skill when:\n- Starting a new project that might benefit from LLM processing\n- Evaluating whether a task is well-suited for agents versus traditional code\n- Designing the architecture for an LLM-powered application\n- Planning a batch processing pipeline with structured outputs\n- Choosing between single-agent and multi-agent approaches\n- Estimating costs and timelines for LLM-heavy projects\n\n## Core Concepts\n\n### Task-Model Fit Recognition\n\nNot every problem benefits from LLM processing. The first step in any project is evaluating whether the task characteristics align with LLM strengths. This evaluation should happen before writing any code.\n\n**LLM-suited tasks share these characteristics:**\n\n| Characteristic | Why It Fits |\n|----------------|-------------|\n| Synthesis across sources | LLMs excel at combining information from multiple inputs |\n| Subjective judgment with rubrics | LLMs handle grading, evaluation, and classification with criteria |\n| Natural language output | When the goal is human-readable text, not structured data |\n| Error tolerance | Individual failures do not break the overall system |\n| Batch processing | No conversational state required between items |\n| Domain knowledge in training | The model already has relevant context |\n\n**LLM-unsuited tasks share these characteristics:**\n\n| Characteristic | Why It Fails |\n|----------------|--------------|\n| Precise computation | Math, counting, and exact algorithms are unreliable |\n| Real-time requirements | LLM latency is too high for sub-second responses |\n| Perfect accuracy requirements | Hallucination risk makes 100% accuracy impossible |\n| Proprietary data dependence | The model lacks necessary context |\n| Sequential dependencies | Each step depends heavily on the previous result |\n| Deterministic output requirements | Same input must produce identical output |\n\nThe evaluation should happen through manual prototyping: take one representative example and test it directly with the target model before building any automation.\n\n### The Manual Prototype Step\n\nBefore investing in automation, validate task-model fit with a manual test. Copy one representative input into the model interface. Evaluate the output quality. This takes minutes and prevents hours of wasted development.\n\nThis validation answers critical questions:\n- Does the model have the knowledge required for this task?\n- Can the model produce output in the format you need?\n- What level of quality should you expect at scale?\n- Are there obvious failure modes to address?\n\nIf the manual prototype fails, the automated system will fail. If it succeeds, you have a baseline for comparison and a template for prompt design.\n\n### Pipeline Architecture\n\nLLM projects benefit from staged pipeline architectures where each stage is:\n- **Discrete**: Clear boundaries between stages\n- **Idempotent**: Re-running produces the same result\n- **Cacheable**: Intermediate results persist to disk\n- **Independent**: Each stage can run separately\n\n**The canonical pipeline structure:**\n\n```\nacquire → prepare → process → parse → render\n```\n\n1. **Acquire**: Fetch raw data from sources (APIs, files, databases)\n2. **Prepare**: Transform data into prompt format\n3. **Process**: Execute LLM calls (the expensive, non-deterministic step)\n4. **Parse**: Extract structured data from LLM outputs\n5. **Render**: Generate final outputs (reports, files, visualizations)\n\nStages 1, 2, 4, and 5 are deterministic. Stage 3 is non-deterministic and expensive. This separation allows re-running the expensive LLM stage only when necessary, while iterating quickly on parsing and rendering.\n\n### File System as State Machine\n\nUse the file system to track pipeline state rather than databases or in-memory structures. Each processing unit gets a directory. Each stage completion is marked by file existence.\n\n```\ndata/{id}/\n├── raw.json         # acquire stage complete\n├── prompt.md        # prepare stage complete\n├── response.md      # process stage complete\n├── parsed.json      # parse stage complete\n```\n\nTo check if an item needs processing: check if the output file exists. To re-run a stage: delete its output file and downstream files. To debug: read the intermediate files directly.\n\nThis pattern provides:\n- Natural idempotency (file existence gates execution)\n- Easy debugging (all state is human-readable)\n- Simple parallelization (each directory is independent)\n- Trivial caching (files persist across runs)\n\n### Structured Output Design\n\nWhen LLM outputs must be parsed programmatically, prompt design directly determines parsing reliability. The prompt must specify exact format requirements with examples.\n\n**Effective structure specification includes:**\n\n1. **Section markers**: Explicit headers or prefixes for parsing\n2. **Format examples**: Show exactly what output should look like\n3. **Rationale disclosure**: \"I will be parsing this programmatically\"\n4. **Constrained values**: Enumerated options, score ranges, formats\n\n**Example prompt structure:**\n```\nAnalyze the following and provide your response in exactly this format:\n\n## Summary\n[Your summary here]\n\n## Score\nRating: [1-10]\n\n## Details\n- Key point 1\n- Key point 2\n\nFollow this format exactly because I will be parsing it programmatically.\n```\n\nThe parsing code must handle variations gracefully. LLMs do not follow instructions perfectly. Build parsers that:\n- Use regex patterns flexible enough to handle minor formatting variations\n- Provide sensible defaults when sections are missing\n- Log parsing failures for later review rather than crashing\n\n### Agent-Assisted Development\n\nModern agent-capable models can accelerate development significantly. The pattern is:\n\n1. Describe the project goal and constraints\n2. Let the agent generate initial implementation\n3. Test and iterate on specific failures\n4. Refine prompts and architecture based on results\n\nThis is about rapid iteration: generate, test, fix, repeat. The agent handles boilerplate and initial structure while you focus on domain-specific requirements and edge cases.\n\nKey practices for effective agent-assisted development:\n- Provide clear, specific requirements upfront\n- Break large projects into discrete components\n- Test each component before moving to the next\n- Keep the agent focused on one task at a time\n\n### Cost and Scale Estimation\n\nLLM processing has predictable costs that should be estimated before starting. The formula:\n\n```\nTotal cost = (items × tokens_per_item × price_per_token) + API overhead\n```\n\nFor batch processing:\n- Estimate input tokens per item (prompt + context)\n- Estimate output tokens per item (typical response length)\n- Multiply by item count\n- Add 20-30% buffer for retries and failures\n\nTrack actual costs during development. If costs exceed estimates significantly, re-evaluate the approach. Consider:\n- Reducing context length through truncation\n- Using smaller models for simpler items\n- Caching and reusing partial results\n- Parallel processing to reduce wall-clock time (not token cost)\n\n## Detailed Topics\n\n### Choosing Single vs Multi-Agent Architecture\n\nSingle-agent pipelines work for:\n- Batch processing with independent items\n- Tasks where items do not interact\n- Simpler cost and complexity management\n\nMulti-agent architectures work for:\n- Parallel exploration of different aspects\n- Tasks exceeding single context window capacity\n- When specialized sub-agents improve quality\n\nThe primary reason for multi-agent is context isolation, not role anthropomorphization. Sub-agents get fresh context windows for focused subtasks. This prevents context degradation on long-running tasks.\n\nSee `multi-agent-patterns` skill for detailed architecture guidance.\n\n### Architectural Reduction\n\nStart with minimal architecture. Add complexity only when proven necessary. Production evidence shows that removing specialized tools often improves performance.\n\nVercel's d0 agent achieved 100% success rate (up from 80%) by reducing from 17 specialized tools to 2 primitives: bash command execution and SQL. The file system agent pattern uses standard Unix utilities (grep, cat, find, ls) instead of custom exploration tools.\n\n**When reduction outperforms complexity:**\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability\n- Your specialized tools were constraining rather than enabling\n- You are spending more time maintaining scaffolding than improving outcomes\n\n**When complexity is necessary:**\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting agent capabilities\n- Operations are truly complex and benefit from structured workflows\n\nSee `tool-design` skill for detailed tool architecture guidance.\n\n### Iteration and Refactoring\n\nExpect to refactor. Production agent systems at scale require multiple architectural iterations. Manus refactored their agent framework five times since launch. The Bitter Lesson suggests that structures added for current model limitations become constraints as models improve.\n\nBuild for change:\n- Keep architecture simple and unopinionated\n- Test across model strengths to verify your harness is not limiting performance\n- Design systems that benefit from model improvements rather than locking in limitations\n\n## Practical Guidance\n\n### Project Planning Template\n\n1. **Task Analysis**\n   - What is the input? What is the desired output?\n   - Is this synthesis, generation, classification, or analysis?\n   - What error rate is acceptable?\n   - What is the value per successful completion?\n\n2. **Manual Validation**\n   - Test one example with target model\n   - Evaluate output quality and format\n   - Identify failure modes\n   - Estimate tokens per item\n\n3. **Architecture Selection**\n   - Single pipeline vs multi-agent\n   - Required tools and data sources\n   - Storage and caching strategy\n   - Parallelization approach\n\n4. **Cost Estimation**\n   - Items × tokens × price\n   - Development time\n   - Infrastructure requirements\n   - Ongoing operational costs\n\n5. **Development Plan**\n   - Stage-by-stage implementation\n   - Testing strategy per stage\n   - Iteration milestones\n   - Deployment approach\n\n### Anti-Patterns to Avoid\n\n**Skipping manual validation**: Building automation before verifying the model can do the task wastes significant time when the approach is fundamentally flawed.\n\n**Monolithic pipelines**: Combining all stages into one script makes debugging and iteration difficult. Separate stages with persistent intermediate outputs.\n\n**Over-constraining the model**: Adding guardrails, pre-filtering, and validation logic that the model could handle on its own. Test whether your scaffolding helps or hurts.\n\n**Ignoring costs until production**: Token costs compound quickly at scale. Estimate and track from the beginning.\n\n**Perfect parsing requirements**: Expecting LLMs to follow format instructions perfectly. Build robust parsers that handle variations.\n\n**Premature optimization**: Adding caching, parallelization, and optimization before the basic pipeline works correctly.\n\n## Examples\n\n**Example 1: Batch Analysis Pipeline (Karpathy's HN Time Capsule)**\n\nTask: Analyze 930 HN discussions from 10 years ago with hindsight grading.\n\nArchitecture:\n- 5-stage pipeline: fetch → prompt → analyze → parse → render\n- File system state: data/{date}/{item_id}/ with stage output files\n- Structured output: 6 sections with explicit format requirements\n- Parallel execution: 15 workers for LLM calls\n\nResults: $58 total cost, ~1 hour execution, static HTML output.\n\n**Example 2: Architectural Reduction (Vercel d0)**\n\nTask: Text-to-SQL agent for internal analytics.\n\nBefore: 17 specialized tools, 80% success rate, 274s average execution.\n\nAfter: 2 tools (bash + SQL), 100% success rate, 77s average execution.\n\nKey insight: The semantic layer was already good documentation. Claude just needed access to read files directly.\n\nSee [Case Studies](./references/case-studies.md) for detailed analysis.\n\n## Guidelines\n\n1. Validate task-model fit with manual prototyping before building automation\n2. Structure pipelines as discrete, idempotent, cacheable stages\n3. Use the file system for state management and debugging\n4. Design prompts for structured, parseable outputs with explicit format examples\n5. Start with minimal architecture; add complexity only when proven necessary\n6. Estimate costs early and track throughout development\n7. Build robust parsers that handle LLM output variations\n8. Expect and plan for multiple architectural iterations\n9. Test whether scaffolding helps or constrains model performance\n10. Use agent-assisted development for rapid iteration on implementation\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - Understanding context constraints for prompt design\n- tool-design - Designing tools for agent systems within pipelines\n- multi-agent-patterns - When to use multi-agent versus single pipelines\n- evaluation - Evaluating pipeline outputs and agent performance\n- context-compression - Managing context when pipelines exceed limits\n\n## References\n\nInternal references:\n- [Case Studies](./references/case-studies.md) - Karpathy HN Capsule, Vercel d0, Manus patterns\n- [Pipeline Patterns](./references/pipeline-patterns.md) - Detailed pipeline architecture guidance\n\nRelated skills in this collection:\n- tool-design - Tool architecture and reduction patterns\n- multi-agent-patterns - When to use multi-agent architectures\n- evaluation - Output evaluation frameworks\n\nExternal resources:\n- Karpathy's HN Time Capsule project: https://github.com/karpathy/hn-time-capsule\n- Vercel d0 architectural reduction: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools\n- Manus context engineering: Peak Ji's blog on context engineering lessons\n- Anthropic multi-agent research: How we built our multi-agent research system\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-25\n**Last Updated**: 2025-12-25\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.0.0"
              },
              {
                "name": "tool-design",
                "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces.",
                "path": "skills/tool-design/SKILL.md",
                "frontmatter": {
                  "name": "tool-design",
                  "description": "This skill should be used when the user asks to \"design agent tools\", \"create tool descriptions\", \"reduce tool complexity\", \"implement MCP tools\", or mentions tool consolidation, architectural reduction, tool naming conventions, or agent-tool interfaces."
                },
                "content": "# Tool Design for Agents\n\nTools are the primary mechanism through which agents interact with the world. They define the contract between deterministic systems and non-deterministic agents. Unlike traditional software APIs designed for developers, tool APIs must be designed for language models that reason about intent, infer parameter values, and generate calls from natural language requests. Poor tool design creates failure modes that no amount of prompt engineering can fix. Effective tool design follows specific principles that account for how agents perceive and use tools.\n\n## When to Activate\n\nActivate this skill when:\n- Creating new tools for agent systems\n- Debugging tool-related failures or misuse\n- Optimizing existing tool sets for better agent performance\n- Designing tool APIs from scratch\n- Evaluating third-party tools for agent integration\n- Standardizing tool conventions across a codebase\n\n## Core Concepts\n\nTools are contracts between deterministic systems and non-deterministic agents. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. Effective tool descriptions are prompt engineering that shapes agent behavior.\n\nKey principles include: clear descriptions that answer what, when, and what returns; response formats that balance completeness and token efficiency; error messages that enable recovery; and consistent conventions that reduce cognitive load.\n\n## Detailed Topics\n\n### The Tool-Agent Interface\n\n**Tools as Contracts**\nTools are contracts between deterministic systems and non-deterministic agents. When humans call APIs, they understand the contract and make appropriate requests. Agents must infer the contract from descriptions and generate calls that match expected formats.\n\nThis fundamental difference requires rethinking API design. The contract must be unambiguous, examples must illustrate expected patterns, and error messages must guide correction. Every ambiguity in tool definitions becomes a potential failure mode.\n\n**Tool Description as Prompt**\nTool descriptions are loaded into agent context and collectively steer behavior. The descriptions are not just documentation—they are prompt engineering that shapes how agents reason about tool use.\n\nPoor descriptions like \"Search the database\" with cryptic parameter names force agents to guess. Optimized descriptions include usage context, examples, and defaults. The description answers: what the tool does, when to use it, and what it produces.\n\n**Namespacing and Organization**\nAs tool collections grow, organization becomes critical. Namespacing groups related tools under common prefixes, helping agents select appropriate tools at the right time.\n\nNamespacing creates clear boundaries between functionality. When an agent needs database information, it routes to the database namespace. When it needs web search, it routes to web namespace.\n\n### The Consolidation Principle\n\n**Single Comprehensive Tools**\nThe consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better. This leads to a preference for single comprehensive tools over multiple narrow tools.\n\nInstead of implementing list_users, list_events, and create_event, implement schedule_event that finds availability and schedules. The comprehensive tool handles the full workflow internally rather than requiring agents to chain multiple calls.\n\n**Why Consolidation Works**\nAgents have limited context and attention. Each tool in the collection competes for attention in the tool selection phase. Each tool adds description tokens that consume context budget. Overlapping functionality creates ambiguity about which tool to use.\n\nConsolidation reduces token consumption by eliminating redundant descriptions. It eliminates ambiguity by having one tool cover each workflow. It reduces tool selection complexity by shrinking the effective tool set.\n\n**When Not to Consolidate**\nConsolidation is not universally correct. Tools with fundamentally different behaviors should remain separate. Tools used in different contexts benefit from separation. Tools that might be called independently should not be artificially bundled.\n\n### Architectural Reduction\n\nThe consolidation principle, taken to its logical extreme, leads to architectural reduction: removing most specialized tools in favor of primitive, general-purpose capabilities. Production evidence shows this approach can outperform sophisticated multi-tool architectures.\n\n**The File System Agent Pattern**\nInstead of building custom tools for data exploration, schema lookup, and query validation, provide direct file system access through a single command execution tool. The agent uses standard Unix utilities (grep, cat, find, ls) to explore, understand, and operate on your system.\n\nThis works because:\n1. File systems are a proven abstraction that models understand deeply\n2. Standard tools have predictable, well-documented behavior\n3. The agent can chain primitives flexibly rather than being constrained to predefined workflows\n4. Good documentation in files replaces the need for summarization tools\n\n**When Reduction Outperforms Complexity**\nReduction works when:\n- Your data layer is well-documented and consistently structured\n- The model has sufficient reasoning capability to navigate complexity\n- Your specialized tools were constraining rather than enabling the model\n- You're spending more time maintaining scaffolding than improving outcomes\n\nReduction fails when:\n- Your underlying data is messy, inconsistent, or poorly documented\n- The domain requires specialized knowledge the model lacks\n- Safety constraints require limiting what the agent can do\n- Operations are truly complex and benefit from structured workflows\n\n**Stop Constraining Reasoning**\nA common anti-pattern is building tools to \"protect\" the model from complexity. Pre-filtering context, constraining options, wrapping interactions in validation logic. These guardrails often become liabilities as models improve.\n\nThe question to ask: are your tools enabling new capabilities, or are they constraining reasoning the model could handle on its own?\n\n**Build for Future Models**\nModels improve faster than tooling can keep up. An architecture optimized for today's model may be over-constrained for tomorrow's. Build minimal architectures that can benefit from model improvements rather than sophisticated architectures that lock in current limitations.\n\nSee [Architectural Reduction Case Study](./references/architectural_reduction.md) for production evidence.\n\n### Tool Description Engineering\n\n**Description Structure**\nEffective tool descriptions answer four questions:\n\nWhat does the tool do? Clear, specific description of functionality. Avoid vague language like \"helps with\" or \"can be used for.\" State exactly what the tool accomplishes.\n\nWhen should it be used? Specific triggers and contexts. Include both direct triggers (\"User asks about pricing\") and indirect signals (\"Need current market rates\").\n\nWhat inputs does it accept? Parameter descriptions with types, constraints, and defaults. Explain what each parameter controls.\n\nWhat does it return? Output format and structure. Include examples of successful responses and error conditions.\n\n**Default Parameter Selection**\nDefaults should reflect common use cases. They reduce agent burden by eliminating unnecessary parameter specification. They prevent errors from omitted parameters.\n\n### Response Format Optimization\n\nTool response size significantly impacts context usage. Implementing response format options gives agents control over verbosity.\n\nConcise format returns essential fields only, appropriate for confirmation or basic information. Detailed format returns complete objects with all fields, appropriate when full context is needed for decisions.\n\nInclude guidance in tool descriptions about when to use each format. Agents learn to select appropriate formats based on task requirements.\n\n### Error Message Design\n\nError messages serve two audiences: developers debugging issues and agents recovering from failures. For agents, error messages must be actionable. They must tell the agent what went wrong and how to correct it.\n\nDesign error messages that enable recovery. For retryable errors, include retry guidance. For input errors, include corrected format. For missing data, include what's needed.\n\n### Tool Definition Schema\n\nUse a consistent schema across all tools. Establish naming conventions: verb-noun pattern for tool names, consistent parameter names across tools, consistent return field names.\n\n### Tool Collection Design\n\nResearch shows tool description overlap causes model confusion. More tools do not always lead to better outcomes. A reasonable guideline is 10-20 tools for most applications. If more are needed, use namespacing to create logical groupings.\n\nImplement mechanisms to help agents select the right tool: tool grouping, example-based selection, and hierarchy with umbrella tools that route to specialized sub-tools.\n\n### MCP Tool Naming Requirements\n\nWhen using MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\nFormat: `ServerName:tool_name`\n\n```python\n# Correct: Fully qualified names\n\"Use the BigQuery:bigquery_schema tool to retrieve table schemas.\"\n\"Use the GitHub:create_issue tool to create issues.\"\n\n# Incorrect: Unqualified names\n\"Use the bigquery_schema tool...\"  # May fail with multiple servers\n```\n\nWithout the server prefix, agents may fail to locate tools, especially when multiple MCP servers are available. Establish naming conventions that include server context in all tool references.\n\n### Using Agents to Optimize Tools\n\nClaude can optimize its own tools. When given a tool and observed failure modes, it diagnoses issues and suggests improvements. Production testing shows this approach achieves 40% reduction in task completion time by helping future agents avoid mistakes.\n\n**The Tool-Testing Agent Pattern**:\n\n```python\ndef optimize_tool_description(tool_spec, failure_examples):\n    \"\"\"\n    Use an agent to analyze tool failures and improve descriptions.\n    \n    Process:\n    1. Agent attempts to use tool across diverse tasks\n    2. Collect failure modes and friction points\n    3. Agent analyzes failures and proposes improvements\n    4. Test improved descriptions against same tasks\n    \"\"\"\n    prompt = f\"\"\"\n    Analyze this tool specification and the observed failures.\n    \n    Tool: {tool_spec}\n    \n    Failures observed:\n    {failure_examples}\n    \n    Identify:\n    1. Why agents are failing with this tool\n    2. What information is missing from the description\n    3. What ambiguities cause incorrect usage\n    \n    Propose an improved tool description that addresses these issues.\n    \"\"\"\n    \n    return get_agent_response(prompt)\n```\n\nThis creates a feedback loop: agents using tools generate failure data, which agents then use to improve tool descriptions, which reduces future failures.\n\n### Testing Tool Design\n\nEvaluate tool designs against criteria: unambiguity, completeness, recoverability, efficiency, and consistency. Test tools by presenting representative agent requests and evaluating the resulting tool calls.\n\n## Practical Guidance\n\n### Anti-Patterns to Avoid\n\nVague descriptions: \"Search the database for customer information\" leaves too many questions unanswered.\n\nCryptic parameter names: Parameters named x, val, or param1 force agents to guess meaning.\n\nMissing error handling: Tools that fail with generic errors provide no recovery guidance.\n\nInconsistent naming: Using id in some tools, identifier in others, and customer_id in some creates confusion.\n\n### Tool Selection Framework\n\nWhen designing tool collections:\n1. Identify distinct workflows agents must accomplish\n2. Group related actions into comprehensive tools\n3. Ensure each tool has a clear, unambiguous purpose\n4. Document error cases and recovery paths\n5. Test with actual agent interactions\n\n## Examples\n\n**Example 1: Well-Designed Tool**\n```python\ndef get_customer(customer_id: str, format: str = \"concise\"):\n    \"\"\"\n    Retrieve customer information by ID.\n    \n    Use when:\n    - User asks about specific customer details\n    - Need customer context for decision-making\n    - Verifying customer identity\n    \n    Args:\n        customer_id: Format \"CUST-######\" (e.g., \"CUST-000001\")\n        format: \"concise\" for key fields, \"detailed\" for complete record\n    \n    Returns:\n        Customer object with requested fields\n    \n    Errors:\n        NOT_FOUND: Customer ID not found\n        INVALID_FORMAT: ID must match CUST-###### pattern\n    \"\"\"\n```\n\n**Example 2: Poor Tool Design**\n\nThis example demonstrates several tool design anti-patterns:\n\n```python\ndef search(query):\n    \"\"\"Search the database.\"\"\"\n    pass\n```\n\n**Problems with this design:**\n\n1. **Vague name**: \"search\" is ambiguous - search what, for what purpose?\n2. **Missing parameters**: What database? What format should query take?\n3. **No return description**: What does this function return? A list? A string? Error handling?\n4. **No usage context**: When should an agent use this versus other tools?\n5. **No error handling**: What happens if the database is unavailable?\n\n**Failure modes:**\n- Agents may call this tool when they should use a more specific tool\n- Agents cannot determine correct query format\n- Agents cannot interpret results\n- Agents cannot recover from failures\n\n## Guidelines\n\n1. Write descriptions that answer what, when, and what returns\n2. Use consolidation to reduce ambiguity\n3. Implement response format options for token efficiency\n4. Design error messages for agent recovery\n5. Establish and follow consistent naming conventions\n6. Limit tool count and use namespacing for organization\n7. Test tool designs with actual agent interactions\n8. Iterate based on observed failure modes\n9. Question whether each tool enables or constrains the model\n10. Prefer primitive, general-purpose tools over specialized wrappers\n11. Invest in documentation quality over tooling sophistication\n12. Build minimal architectures that benefit from model improvements\n\n## Integration\n\nThis skill connects to:\n- context-fundamentals - How tools interact with context\n- multi-agent-patterns - Specialized tools per agent\n- evaluation - Evaluating tool effectiveness\n\n## References\n\nInternal references:\n- [Best Practices Reference](./references/best_practices.md) - Detailed tool design guidelines\n- [Architectural Reduction Case Study](./references/architectural_reduction.md) - Production evidence for tool minimalism\n\nRelated skills in this collection:\n- context-fundamentals - Tool context interactions\n- evaluation - Tool testing patterns\n\nExternal resources:\n- MCP (Model Context Protocol) documentation\n- Framework tool conventions\n- API design best practices for agents\n- Vercel d0 agent architecture case study\n\n---\n\n## Skill Metadata\n\n**Created**: 2025-12-20\n**Last Updated**: 2025-12-23\n**Author**: Agent Skills for Context Engineering Contributors\n**Version**: 1.1.0"
              }
            ]
          }
        ]
      }
    }
  ]
}