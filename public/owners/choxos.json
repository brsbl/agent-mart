{
  "owner": {
    "id": "choxos",
    "display_name": "Ahmad Sofi-Mahmudi",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/6255852?u=b0b2b02f9ca5bcfb4f4dc2c1b28019f5df3e3f2a&v=4",
    "url": "https://github.com/choxos",
    "bio": "GNU/Linux Enthusiast, Open Science Advocate",
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 9,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "choxos/BayesianAgent",
      "url": "https://github.com/choxos/BayesianAgent",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-11T23:05:27Z",
        "created_at": "2025-12-11T17:19:35Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1485
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/test-all.yml",
          "type": "blob",
          "size": 2395
        },
        {
          "path": ".github/workflows/test-jags.yml",
          "type": "blob",
          "size": 1935
        },
        {
          "path": ".github/workflows/test-pymc.yml",
          "type": "blob",
          "size": 1474
        },
        {
          "path": ".github/workflows/test-stan.yml",
          "type": "blob",
          "size": 2121
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 601
        },
        {
          "path": "BayesianAgent_prompt.md",
          "type": "blob",
          "size": 551
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 6227
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/agents/bugs-specialist.md",
          "type": "blob",
          "size": 10475
        },
        {
          "path": "plugins/bayesian-modeling/agents/model-architect.md",
          "type": "blob",
          "size": 6051
        },
        {
          "path": "plugins/bayesian-modeling/agents/model-reviewer.md",
          "type": "blob",
          "size": 8782
        },
        {
          "path": "plugins/bayesian-modeling/agents/pymc-specialist.md",
          "type": "blob",
          "size": 16078
        },
        {
          "path": "plugins/bayesian-modeling/agents/stan-specialist.md",
          "type": "blob",
          "size": 14124
        },
        {
          "path": "plugins/bayesian-modeling/agents/test-runner.md",
          "type": "blob",
          "size": 15381
        },
        {
          "path": "plugins/bayesian-modeling/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/commands/create-model.md",
          "type": "blob",
          "size": 3309
        },
        {
          "path": "plugins/bayesian-modeling/commands/review-model.md",
          "type": "blob",
          "size": 3719
        },
        {
          "path": "plugins/bayesian-modeling/commands/run-diagnostics.md",
          "type": "blob",
          "size": 5088
        },
        {
          "path": "plugins/bayesian-modeling/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/bugs-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/bugs-fundamentals/SKILL.md",
          "type": "blob",
          "size": 4284
        },
        {
          "path": "plugins/bayesian-modeling/skills/hierarchical-models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/hierarchical-models/SKILL.md",
          "type": "blob",
          "size": 2575
        },
        {
          "path": "plugins/bayesian-modeling/skills/meta-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/meta-analysis/SKILL.md",
          "type": "blob",
          "size": 4351
        },
        {
          "path": "plugins/bayesian-modeling/skills/model-diagnostics",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/model-diagnostics/SKILL.md",
          "type": "blob",
          "size": 5193
        },
        {
          "path": "plugins/bayesian-modeling/skills/pymc-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/pymc-fundamentals/SKILL.md",
          "type": "blob",
          "size": 6833
        },
        {
          "path": "plugins/bayesian-modeling/skills/regression-models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/regression-models/SKILL.md",
          "type": "blob",
          "size": 2892
        },
        {
          "path": "plugins/bayesian-modeling/skills/stan-fundamentals",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/stan-fundamentals/SKILL.md",
          "type": "blob",
          "size": 5626
        },
        {
          "path": "plugins/bayesian-modeling/skills/survival-models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/survival-models/SKILL.md",
          "type": "blob",
          "size": 4227
        },
        {
          "path": "plugins/bayesian-modeling/skills/time-series-models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bayesian-modeling/skills/time-series-models/SKILL.md",
          "type": "blob",
          "size": 3563
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/conftest.R",
          "type": "blob",
          "size": 7746
        },
        {
          "path": "tests/integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/integration/jags",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/integration/jags/test_hierarchical_jags.R",
          "type": "blob",
          "size": 2660
        },
        {
          "path": "tests/integration/jags/test_meta_analysis_jags.R",
          "type": "blob",
          "size": 2709
        },
        {
          "path": "tests/integration/jags/test_regression_jags.R",
          "type": "blob",
          "size": 3093
        },
        {
          "path": "tests/integration/pymc",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/integration/pymc/conftest.py",
          "type": "blob",
          "size": 3794
        },
        {
          "path": "tests/integration/pymc/test_hierarchical_pymc.py",
          "type": "blob",
          "size": 4378
        },
        {
          "path": "tests/integration/pymc/test_meta_analysis_pymc.py",
          "type": "blob",
          "size": 5212
        },
        {
          "path": "tests/integration/pymc/test_regression_pymc.py",
          "type": "blob",
          "size": 4109
        },
        {
          "path": "tests/integration/stan",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/integration/stan/test_hierarchical_stan.R",
          "type": "blob",
          "size": 2949
        },
        {
          "path": "tests/integration/stan/test_meta_analysis_stan.R",
          "type": "blob",
          "size": 3109
        },
        {
          "path": "tests/integration/stan/test_regression_stan.R",
          "type": "blob",
          "size": 2647
        },
        {
          "path": "tests/integration/stan/test_timeseries_stan.R",
          "type": "blob",
          "size": 2874
        },
        {
          "path": "tests/models",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/models/jags",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/models/jags/hierarchical.txt",
          "type": "blob",
          "size": 290
        },
        {
          "path": "tests/models/jags/meta_analysis.txt",
          "type": "blob",
          "size": 292
        },
        {
          "path": "tests/models/jags/regression.txt",
          "type": "blob",
          "size": 285
        },
        {
          "path": "tests/models/stan",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/models/stan/ar1.stan",
          "type": "blob",
          "size": 942
        },
        {
          "path": "tests/models/stan/hierarchical.stan",
          "type": "blob",
          "size": 965
        },
        {
          "path": "tests/models/stan/meta_analysis.stan",
          "type": "blob",
          "size": 938
        },
        {
          "path": "tests/models/stan/regression.stan",
          "type": "blob",
          "size": 793
        },
        {
          "path": "tests/run_tests.R",
          "type": "blob",
          "size": 2089
        }
      ],
      "marketplace": {
        "name": "bayesian-modeling-agent",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Ahmad Sofi-Mahmudi",
          "email": "choxos@users.noreply.github.com",
          "url": "https://github.com/choxos"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "bayesian-modeling",
            "description": "Create, review, and validate Bayesian models in Stan, PyMC, JAGS, and WinBUGS",
            "source": "./plugins/bayesian-modeling",
            "category": "statistics",
            "version": "1.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add choxos/BayesianAgent",
              "/plugin install bayesian-modeling@bayesian-modeling-agent"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-11T23:05:27Z",
              "created_at": "2025-12-11T17:19:35Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/create-model",
                "description": "Interactive workflow for creating Bayesian models in Stan, JAGS, WinBUGS, or PyMC",
                "path": "plugins/bayesian-modeling/commands/create-model.md",
                "frontmatter": {
                  "name": "create-model",
                  "description": "Interactive workflow for creating Bayesian models in Stan, JAGS, WinBUGS, or PyMC"
                },
                "content": "# Bayesian Model Creation Workflow\n\nYou are helping the user create a Bayesian model. Follow this structured workflow:\n\n## Step 1: Gather Requirements\n\nAsk the user to specify:\n\n1. **Model Type** (select one):\n   - Hierarchical/Multilevel model\n   - Regression model (linear, logistic, Poisson, etc.)\n   - Time series model (AR, state-space, etc.)\n   - Survival analysis model\n   - Meta-analysis model\n\n2. **Target Language**:\n   - Stan with cmdstanr (DEFAULT for R - recommended)\n   - PyMC 5 with ArviZ (DEFAULT for Python)\n   - JAGS with R2jags\n   - WinBUGS with R2WinBUGS (Windows only)\n\n3. **Experience Level**:\n   - Beginner (extensive comments, educational explanations)\n   - Intermediate (standard documentation)\n   - Advanced (minimal comments, efficiency-focused)\n\n4. **Data Description**:\n   - Outcome variable type (continuous, binary, count, time-to-event)\n   - Predictor variables\n   - Grouping structure (if hierarchical)\n   - Sample sizes\n\n5. **Prior Preferences** (optional):\n   - Specific prior distributions\n   - Informative vs weakly informative\n   - Domain-specific constraints\n\n## Step 2: Route to Specialist\n\nBased on the target language:\n\n- **Stan**: Use @stan-specialist with skills:\n  - `stan-fundamentals` for syntax\n  - Appropriate model type skill (hierarchical-models, regression-models, etc.)\n\n- **PyMC**: Use @pymc-specialist with skills:\n  - `pymc-fundamentals` for syntax\n  - Appropriate model type skill\n\n- **JAGS/WinBUGS**: Use @bugs-specialist with skills:\n  - `bugs-fundamentals` for syntax\n  - Appropriate model type skill\n\n## Step 3: Generate Model\n\nThe specialist will provide:\n\n1. **Complete model code** with appropriate comments based on experience level\n\n2. **Integration code** (R or Python):\n   - Data preparation\n   - Model compilation/fitting\n   - Basic diagnostics (posterior/ArviZ)\n\n3. **Generated quantities** for:\n   - Posterior predictive checks\n   - Derived quantities of interest\n\n## Step 4: Validate Output\n\nBefore presenting to user, verify:\n\n- [ ] Model syntax is correct for target language\n- [ ] All parameters have priors\n- [ ] Parameterization is correct (SD for Stan/PyMC, precision for BUGS)\n- [ ] Integration code is complete and runnable (R or Python)\n- [ ] Comments match experience level\n\n## Example Interaction\n\n**User**: I need a model for patient outcomes nested within hospitals.\n\n**Assistant**: I'll help you create a hierarchical model. Let me gather some details:\n\n1. **Outcome type**: Is your outcome continuous (e.g., recovery time), binary (e.g., survived/died), or count (e.g., readmissions)?\n\n2. **Predictors**: What patient-level and hospital-level variables do you want to include?\n\n3. **Language preference**: Would you like Stan (R), PyMC (Python), or JAGS?\n\n4. **Experience level**: How much detail would you like in the comments?\n\n---\n\n**Critical Reminders**:\n\n- Default to Stan for R users, PyMC for Python users\n- Always include complete integration code (R or Python)\n- Warn about parameterization when relevant (SD for Stan/PyMC, precision for BUGS)\n- Suggest non-centered parameterization for hierarchical models if appropriate\n- For PyMC, remind users to use `pm.math` operations inside models"
              },
              {
                "name": "/review-model",
                "description": "Review and improve existing Bayesian models for correctness, efficiency, and best practices",
                "path": "plugins/bayesian-modeling/commands/review-model.md",
                "frontmatter": {
                  "name": "review-model",
                  "description": "Review and improve existing Bayesian models for correctness, efficiency, and best practices"
                },
                "content": "# Bayesian Model Review Workflow\n\nYou are reviewing a user's existing Bayesian model. Follow this structured approach:\n\n## Step 1: Receive Model Code\n\nAsk the user to paste their model code. Automatically detect:\n- Language (Stan / JAGS / WinBUGS)\n- Model type\n- Complexity level\n\n## Step 2: Syntax Check\n\n### For Stan:\n- Valid block order (functions → data → transformed data → parameters → transformed parameters → model → generated quantities)\n- Correct array syntax (`array[N] real`, not `real[N]`)\n- Proper constraint syntax (`<lower=0>`, `simplex`, etc.)\n- Semicolons on all statements\n\n### For JAGS/WinBUGS:\n- Single `model { }` block\n- Proper distribution prefix (`d` for distributions)\n- Correct indexing syntax\n- Valid truncation syntax (`T(lower, upper)`)\n\n## Step 3: Statistical Review\n\nCheck the following using @model-reviewer:\n\n### Priors\n- [ ] All parameters have explicit priors\n- [ ] Priors are appropriate for the scale of data\n- [ ] No improper priors that could cause issues\n- [ ] Informative priors are justified\n\n### Parameterization\n- [ ] **Stan**: Using SD (sigma), not precision\n- [ ] **BUGS/JAGS**: Using precision (tau = 1/sigma²) correctly\n- [ ] Covariance vs precision matrices are correct\n- [ ] Hierarchical models: centered vs non-centered appropriateness\n\n### Efficiency\n- [ ] Vectorization used where possible (Stan)\n- [ ] No unnecessary loops\n- [ ] Appropriate transformed parameter placement\n- [ ] Cholesky factors for covariance matrices\n\n### Common Errors\n- [ ] Integer division issues\n- [ ] Missing constraints on parameters\n- [ ] Potential numerical overflow/underflow\n- [ ] Invalid parameter combinations\n\n## Step 4: Generate Report\n\nProvide a structured review:\n\n```markdown\n## Model Review Report\n\n### Language Detected\n[Stan / JAGS / WinBUGS]\n\n### Model Type\n[Hierarchical / Regression / Time Series / etc.]\n\n### Syntax Issues\n- [List any syntax errors or warnings]\n\n### Statistical Concerns\n- [List concerns about priors, parameterization, etc.]\n\n### Efficiency Improvements\n- [Suggestions for better performance]\n\n### Recommended Changes\n1. [Specific change with code example]\n2. [Another change...]\n\n### Corrected Model (if needed)\n[Full corrected model code]\n```\n\n## Step 5: Offer Improvements\n\nBased on review, offer to:\n1. Fix identified issues\n2. Add missing components (generated quantities, diagnostics)\n3. Convert to different language (Stan ↔ JAGS)\n4. Add posterior predictive checks\n\n## Example Review Output\n\n```\n## Model Review Report\n\n### Language Detected\nJAGS\n\n### Syntax Issues\n✓ No syntax errors detected\n\n### Statistical Concerns\n⚠️ **Prior on tau is very vague**: `tau ~ dgamma(0.001, 0.001)` can\n   cause sampling issues. Consider `sigma ~ dunif(0, 100)` with\n   `tau <- pow(sigma, -2)` instead.\n\n⚠️ **Missing prior on alpha**: The intercept `alpha` has no prior,\n   defaulting to improper uniform.\n\n### Efficiency Improvements\n- Consider combining the two loops on lines 5-8 and 10-13\n\n### Recommended Changes\n\n1. Add prior for alpha:\n   ```\n   alpha ~ dnorm(0, 0.0001)\n   ```\n\n2. Use half-uniform prior on SD:\n   ```\n   sigma ~ dunif(0, 100)\n   tau <- pow(sigma, -2)\n   ```\n```\n\n## Critical Checklist\n\nWhen reviewing any model:\n\n1. **Language Detection**: Look for `~` (both), `target +=` (Stan only), `dnorm`/`dgamma` (BUGS)\n2. **Parameterization Warning**: If converting or comparing, ALWAYS note SD vs precision\n3. **Prior Completeness**: Every stochastic node in parameters needs a prior\n4. **Computational Issues**: Divergences, low ESS, and Rhat problems often trace to parameterization"
              },
              {
                "name": "/run-diagnostics",
                "description": "Execute Bayesian models with test data and report convergence diagnostics",
                "path": "plugins/bayesian-modeling/commands/run-diagnostics.md",
                "frontmatter": {
                  "name": "run-diagnostics",
                  "description": "Execute Bayesian models with test data and report convergence diagnostics"
                },
                "content": "# Model Execution and Diagnostics Workflow\n\nYou are helping the user run and diagnose their Bayesian model.\n\n## Step 1: Identify Model and Data\n\nDetermine:\n1. **Model language**: Stan / JAGS / WinBUGS / PyMC\n2. **Data source**:\n   - User-provided data\n   - Generate synthetic test data\n\n## Step 2: Generate Test Data (if needed)\n\nUse @test-runner to create appropriate synthetic data:\n\n### For Regression Models\n```r\nset.seed(42)\nN <- 100\nK <- 3\nX <- matrix(rnorm(N * K), N, K)\ntrue_beta <- c(0.5, -0.3, 0.8)\ntrue_sigma <- 1\ny <- X %*% true_beta + rnorm(N, 0, true_sigma)\n\nstan_data <- list(N = N, K = K, X = X, y = as.vector(y))\n```\n\n### For Hierarchical Models\n```r\nset.seed(42)\nJ <- 8\nN_per_group <- 20\ntrue_mu <- 5\ntrue_tau <- 3\ntrue_theta <- rnorm(J, true_mu, true_tau)\n\ny <- unlist(lapply(1:J, function(j) rnorm(N_per_group, true_theta[j], 2)))\ngroup <- rep(1:J, each = N_per_group)\n\nstan_data <- list(N = J * N_per_group, J = J, group = group, y = y)\n```\n\n## Step 3: Execute Model\n\n### Stan (cmdstanr)\n```r\nlibrary(cmdstanr)\n\n# Compile\nmod <- cmdstan_model(\"model.stan\")\n\n# Short test run\nfit <- mod$sample(\n  data = stan_data,\n  seed = 12345,\n  chains = 2,\n  parallel_chains = 2,\n  iter_warmup = 500,\n  iter_sampling = 500,\n  refresh = 100\n)\n```\n\n### JAGS (R2jags)\n```r\nlibrary(R2jags)\n\nfit <- jags(\n  data = jags_data,\n  parameters.to.save = c(\"mu\", \"sigma\", \"theta\"),\n  model.file = \"model.txt\",\n  n.chains = 2,\n  n.iter = 2000,\n  n.burnin = 1000,\n  progress.bar = \"text\"\n)\n```\n\n### PyMC (Python)\n```python\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\n# Define model\nwith pm.Model() as model:\n    # Priors\n    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # Likelihood\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_data)\n\n    # Sample\n    trace = pm.sample(\n        draws=1000,\n        tune=1000,\n        chains=2,\n        cores=2,\n        random_seed=12345,\n        return_inferencedata=True\n    )\n\n# Diagnostics\nprint(az.summary(trace))\naz.plot_trace(trace)\n```\n\n## Step 4: Report Diagnostics\n\nGenerate a diagnostic report:\n\n```markdown\n## Execution Report\n\n### Model Information\n- **Language**: [Stan/JAGS/PyMC]\n- **File**: model.stan\n- **Test Data**: Synthetic (N=100)\n\n### Sampling Summary\n- **Chains**: 2\n- **Warmup**: 500\n- **Sampling**: 500\n- **Total time**: X.X seconds\n\n### Convergence Diagnostics\n\n| Metric | Value | Threshold | Status |\n|--------|-------|-----------|--------|\n| Max Rhat | X.XX | < 1.01 | ✓/✗ |\n| Min ESS (bulk) | XXX | > 400 | ✓/✗ |\n| Min ESS (tail) | XXX | > 400 | ✓/✗ |\n| Divergences | X | = 0 | ✓/✗ |\n| Max treedepth | X | = 0 | ✓/✗ |\n\n### Parameter Summary\n\n| Parameter | Mean | SD | 5% | 95% | Rhat | ESS |\n|-----------|------|----|----|-----|------|-----|\n| mu | X.XX | X.XX | X.XX | X.XX | X.XX | XXX |\n| sigma | X.XX | X.XX | X.XX | X.XX | X.XX | XXX |\n\n### Parameter Recovery (Synthetic Data)\n\n| Parameter | True | Estimate | In 90% CI |\n|-----------|------|----------|-----------|\n| mu | 5.00 | 5.12 | ✓ |\n| sigma | 1.00 | 0.98 | ✓ |\n\n### Warnings\n[List any warnings from sampling]\n\n### Recommendations\n[Suggestions based on diagnostics]\n```\n\n## Step 5: Troubleshooting\n\nIf issues detected, provide specific guidance:\n\n### Divergences\n```\nISSUE: X divergent transitions detected\n\nSOLUTIONS:\n1. Increase adapt_delta:\n   fit <- mod$sample(..., adapt_delta = 0.95)\n\n2. Use non-centered parameterization for hierarchical parameters\n\n3. Check for multimodality in posterior\n```\n\n### Low ESS\n```\nISSUE: ESS for parameter X is only YY\n\nSOLUTIONS:\n1. Run longer chains (increase iter_sampling)\n2. Check for high autocorrelation\n3. Consider reparameterization\n```\n\n### High Rhat\n```\nISSUE: Rhat for parameter X is Y.YY\n\nSOLUTIONS:\n1. Run longer warmup period\n2. Check for label switching (mixture models)\n3. Verify model is identified\n```\n\n## Quick Test Commands\n\n### Test Stan Model\n```r\n# One-liner syntax check\ncmdstanr::cmdstan_model(\"model.stan\", compile = FALSE)$check_syntax()\n\n# Quick test run\nsource(\"test_model.R\")  # Generated test script\n```\n\n### Test JAGS Model\n```r\n# Quick test\nlibrary(R2jags)\ntest_data <- list(N = 10, y = rnorm(10))\njags(data = test_data, model.file = \"model.txt\",\n     parameters.to.save = \"mu\", n.chains = 1, n.iter = 100)\n```\n\n### Test PyMC Model\n```python\nimport pymc as pm\nimport numpy as np\nimport arviz as az\n\n# Quick test\ny_test = np.random.randn(20)\nwith pm.Model() as test_model:\n    mu = pm.Normal(\"mu\", 0, 10)\n    y = pm.Normal(\"y\", mu=mu, sigma=1, observed=y_test)\n    trace = pm.sample(200, tune=200, chains=1, progressbar=False)\n\nprint(az.summary(trace))\n```\n\n## Convergence Checklist\n\nBefore reporting success:\n- [ ] All Rhat values < 1.01 (Stan) or < 1.1 (JAGS)\n- [ ] All ESS values > 100 (minimum) or > 400 (ideal)\n- [ ] Zero divergent transitions (Stan)\n- [ ] Not hitting max treedepth (Stan)\n- [ ] Parameters recovered (if synthetic data)\n- [ ] No obvious pathologies in trace plots"
              }
            ],
            "skills": [
              {
                "name": "bugs-fundamentals",
                "description": "Foundational knowledge for writing BUGS/JAGS models including precision parameterization, declarative syntax, distributions, and R integration. Use when creating or reviewing BUGS/JAGS models.",
                "path": "plugins/bayesian-modeling/skills/bugs-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "bugs-fundamentals",
                  "description": "Foundational knowledge for writing BUGS/JAGS models including precision parameterization, declarative syntax, distributions, and R integration. Use when creating or reviewing BUGS/JAGS models."
                },
                "content": "# BUGS/JAGS Fundamentals\n\n## When to Use This Skill\n\n- Writing new WinBUGS or JAGS models\n- Understanding BUGS declarative syntax\n- Converting between BUGS and Stan\n- Integrating with R via R2jags or R2WinBUGS\n\n## Model Structure\n\nBUGS uses a **single declarative block** where order doesn't matter:\n\n```\nmodel {\n  # Likelihood (order doesn't matter)\n  for (i in 1:N) {\n    y[i] ~ dnorm(mu[i], tau)\n    mu[i] <- alpha + beta * x[i]\n  }\n\n  # Priors\n  alpha ~ dnorm(0, 0.001)\n  beta ~ dnorm(0, 0.001)\n  tau ~ dgamma(0.001, 0.001)\n\n  # Derived quantities\n  sigma <- 1 / sqrt(tau)\n}\n```\n\n## CRITICAL: Precision Parameterization\n\n**BUGS uses PRECISION (tau = 1/variance), NOT standard deviation:**\n\n| Distribution | BUGS Syntax | Meaning |\n|-------------|-------------|---------|\n| Normal | `dnorm(mu, tau)` | tau = 1/sigma² |\n| MVN | `dmnorm(mu[], Omega[,])` | Omega = inverse(Sigma) |\n\n### Converting SD ↔ Precision\n```\n# Precision from SD\ntau <- pow(sigma, -2)\n\n# SD from precision\nsigma <- 1 / sqrt(tau)\n```\n\n## Distribution Reference\n\n### Continuous (All use precision!)\n```\ny ~ dnorm(mu, tau)        # Normal: tau = 1/sigma²\ny ~ dlnorm(mu, tau)       # Log-normal (log-scale)\ny ~ dt(mu, tau, df)       # Student-t\ny ~ dunif(lower, upper)   # Uniform\ny ~ dgamma(shape, rate)   # Gamma\ny ~ dbeta(a, b)           # Beta\ny ~ dexp(lambda)          # Exponential (rate)\ny ~ dweib(shape, lambda)  # Weibull\ny ~ ddexp(mu, tau)        # Double exponential\n```\n\n### Discrete\n```\ny ~ dbern(p)              # Bernoulli\ny ~ dbin(p, n)            # Binomial (p first!)\ny ~ dpois(lambda)         # Poisson\ny ~ dnegbin(p, r)         # Negative binomial\ny ~ dcat(p[])             # Categorical\ny ~ dmulti(p[], n)        # Multinomial\n```\n\n### Multivariate\n```\ny[1:K] ~ dmnorm(mu[], Omega[,])    # MVN (precision matrix!)\nOmega[1:K,1:K] ~ dwish(R[,], df)   # Wishart (for precision)\np[1:K] ~ ddirch(alpha[])           # Dirichlet\n```\n\n## Syntax Essentials\n\n### Stochastic vs Deterministic\n```\n# Stochastic (random variable)\ny ~ dnorm(mu, tau)\n\n# Deterministic (function)\nmu <- alpha + beta * x\n```\n\n### Loops\n```\nfor (i in 1:N) {\n  y[i] ~ dnorm(mu[i], tau)\n}\n```\n\n### Truncation (JAGS)\n```\ny ~ dnorm(mu, tau) T(lower, upper)\ny ~ dnorm(mu, tau) T(0, )     # Lower only\n```\n\n### Logical Functions (JAGS)\n```\nind <- step(y - threshold)   # 1 if y >= threshold\neq <- equals(y, 0)           # 1 if y == 0\n```\n\n## Common Priors\n\n```\n# Vague normal (variance = 1000)\nalpha ~ dnorm(0, 0.001)\n\n# Half-Cauchy on SD (via uniform)\nsigma ~ dunif(0, 100)\ntau <- pow(sigma, -2)\n\n# Vague gamma on precision\ntau ~ dgamma(0.001, 0.001)\n\n# Correlation matrix\nOmega ~ dwish(I[,], K + 1)\n```\n\n## R Integration\n\n### R2jags (Recommended)\n```r\nlibrary(R2jags)\n\njags.data <- list(N = 100, y = y, x = x)\njags.params <- c(\"alpha\", \"beta\", \"sigma\")\njags.inits <- function() {\n  list(alpha = 0, beta = 0, tau = 1)\n}\n\nfit <- jags(\n  data = jags.data,\n  inits = jags.inits,\n  parameters.to.save = jags.params,\n  model.file = \"model.txt\",\n  n.chains = 4,\n  n.iter = 10000,\n  n.burnin = 5000\n)\n\nprint(fit)\nfit$BUGSoutput$summary\n```\n\n### R2WinBUGS (Windows)\n```r\nlibrary(R2WinBUGS)\n\nfit <- bugs(\n  data = bugs.data,\n  inits = bugs.inits,\n  parameters.to.save = bugs.params,\n  model.file = \"model.txt\",\n  n.chains = 3,\n  n.iter = 10000,\n  bugs.directory = \"C:/WinBUGS14/\"\n)\n```\n\n## Key Differences from Stan\n\n| Feature | BUGS/JAGS | Stan |\n|---------|-----------|------|\n| Normal | `dnorm(mu, tau)` precision | `normal(mu, sigma)` SD |\n| MVN | `dmnorm(mu, Omega)` precision | `multi_normal(mu, Sigma)` cov |\n| Syntax | Declarative (DAG) | Imperative (sequential) |\n| Blocks | Single model{} | 7 optional blocks |\n| Sampling | Gibbs + Metropolis | HMC/NUTS |\n| Discrete | Direct sampling | Marginalization required |\n\n## Common Errors\n\n1. **Using SD instead of precision**: `dnorm(0, 1)` means variance=1, NOT SD=1\n2. **Wrong binomial order**: `dbin(p, n)` not `dbin(n, p)`\n3. **Missing initial values**: Provide inits for complex models\n4. **Invalid parent values**: Check for NA/NaN in data"
              },
              {
                "name": "hierarchical-models",
                "description": "Patterns for hierarchical/multilevel Bayesian models including random effects, partial pooling, and centered vs non-centered parameterizations.",
                "path": "plugins/bayesian-modeling/skills/hierarchical-models/SKILL.md",
                "frontmatter": {
                  "name": "hierarchical-models",
                  "description": "Patterns for hierarchical/multilevel Bayesian models including random effects, partial pooling, and centered vs non-centered parameterizations."
                },
                "content": "# Hierarchical Models\n\n## When to Use\n\n- Nested/grouped data (students in schools, patients in hospitals)\n- Repeated measurements on subjects\n- Meta-analysis with study-level variation\n- Partial pooling between complete pooling and no pooling\n\n## Core Concept: Partial Pooling\n\n```\nGroup means shrink toward overall mean based on:\n- Within-group sample size\n- Within-group variance\n- Between-group variance\n```\n\n## Stan Implementation\n\n### Centered Parameterization (Default)\n```stan\ndata {\n  int<lower=0> N;           // Total observations\n  int<lower=0> J;           // Number of groups\n  array[N] int<lower=1,upper=J> group;\n  vector[N] y;\n}\nparameters {\n  real mu;                  // Population mean\n  real<lower=0> tau;        // Between-group SD\n  real<lower=0> sigma;      // Within-group SD\n  vector[J] theta;          // Group means\n}\nmodel {\n  // Hyperpriors\n  mu ~ normal(0, 10);\n  tau ~ cauchy(0, 2.5);\n  sigma ~ exponential(1);\n\n  // Group effects\n  theta ~ normal(mu, tau);\n\n  // Likelihood\n  y ~ normal(theta[group], sigma);\n}\n```\n\n### Non-Centered Parameterization (Better for weak data/small tau)\n```stan\nparameters {\n  real mu;\n  real<lower=0> tau;\n  real<lower=0> sigma;\n  vector[J] theta_raw;      // Standard normal\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * theta_raw;\n}\nmodel {\n  theta_raw ~ std_normal();\n  // ... rest same\n}\n```\n\n**When to use non-centered**: Divergences, small tau, few observations per group.\n\n## JAGS Implementation\n\n```\nmodel {\n  for (i in 1:N) {\n    y[i] ~ dnorm(theta[group[i]], tau.y)\n  }\n\n  for (j in 1:J) {\n    theta[j] ~ dnorm(mu, tau.theta)\n  }\n\n  # Hyperpriors\n  mu ~ dnorm(0, 0.0001)\n  tau.theta <- pow(sigma.theta, -2)\n  sigma.theta ~ dunif(0, 100)\n  tau.y <- pow(sigma.y, -2)\n  sigma.y ~ dunif(0, 100)\n}\n```\n\n## Classic Example: Eight Schools\n\n```stan\ndata {\n  int<lower=0> J;\n  array[J] real y;          // Observed effects\n  array[J] real<lower=0> sigma;  // Known SEs\n}\nparameters {\n  real mu;\n  real<lower=0> tau;\n  vector[J] theta_raw;\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * theta_raw;\n}\nmodel {\n  mu ~ normal(0, 5);\n  tau ~ cauchy(0, 5);\n  theta_raw ~ std_normal();\n  y ~ normal(theta, sigma);\n}\n```\n\n## Diagnostics\n\n- Check tau posterior (very small → use non-centered)\n- Divergences often indicate centered/non-centered mismatch\n- Compare to no-pooling and complete-pooling models"
              },
              {
                "name": "meta-analysis",
                "description": "Bayesian meta-analysis models including fixed effects, random effects, and network meta-analysis with Stan and JAGS implementations.",
                "path": "plugins/bayesian-modeling/skills/meta-analysis/SKILL.md",
                "frontmatter": {
                  "name": "meta-analysis",
                  "description": "Bayesian meta-analysis models including fixed effects, random effects, and network meta-analysis with Stan and JAGS implementations."
                },
                "content": "# Meta-Analysis Models\n\n## Fixed Effects Meta-Analysis\n\n### Stan\n```stan\ndata {\n  int<lower=0> K;           // Number of studies\n  vector[K] y;              // Effect estimates\n  vector<lower=0>[K] se;    // Standard errors\n}\nparameters {\n  real theta;               // Common effect\n}\nmodel {\n  theta ~ normal(0, 10);\n  y ~ normal(theta, se);\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:K) {\n    y[i] ~ dnorm(theta, prec[i])\n    prec[i] <- pow(se[i], -2)\n  }\n  theta ~ dnorm(0, 0.0001)\n}\n```\n\n## Random Effects Meta-Analysis\n\n### Stan (Non-centered, recommended)\n```stan\ndata {\n  int<lower=0> K;\n  vector[K] y;\n  vector<lower=0>[K] se;\n}\nparameters {\n  real mu;                  // Overall mean\n  real<lower=0> tau;        // Between-study SD\n  vector[K] eta;            // Study effects (standardized)\n}\ntransformed parameters {\n  vector[K] theta = mu + tau * eta;\n}\nmodel {\n  // Priors\n  mu ~ normal(0, 10);\n  tau ~ cauchy(0, 0.5);     // Half-Cauchy\n  eta ~ std_normal();\n\n  // Likelihood\n  y ~ normal(theta, se);\n}\ngenerated quantities {\n  real theta_new = normal_rng(mu, tau);  // Predictive\n  real I2 = square(tau) / (square(tau) + mean(square(se)));\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:K) {\n    y[i] ~ dnorm(theta[i], prec[i])\n    prec[i] <- pow(se[i], -2)\n    theta[i] ~ dnorm(mu, tau.theta)\n  }\n\n  mu ~ dnorm(0, 0.0001)\n  tau.theta <- pow(sigma.theta, -2)\n  sigma.theta ~ dunif(0, 10)\n\n  # Heterogeneity\n  tau2 <- pow(sigma.theta, 2)\n}\n```\n\n## Binary Outcomes\n\n### Stan (Log-Odds)\n```stan\ndata {\n  int<lower=0> K;\n  array[K] int<lower=0> r1;   // Events in treatment\n  array[K] int<lower=0> n1;   // Total in treatment\n  array[K] int<lower=0> r2;   // Events in control\n  array[K] int<lower=0> n2;   // Total in control\n}\nparameters {\n  real d;                     // Overall log-OR\n  real<lower=0> tau;\n  vector[K] delta;            // Study-specific log-OR\n  vector[K] mu;               // Baseline log-odds\n}\nmodel {\n  d ~ normal(0, 10);\n  tau ~ cauchy(0, 0.5);\n  delta ~ normal(d, tau);\n  mu ~ normal(0, 10);\n\n  r2 ~ binomial_logit(n2, mu);\n  r1 ~ binomial_logit(n1, mu + delta);\n}\ngenerated quantities {\n  real OR = exp(d);\n}\n```\n\n## Network Meta-Analysis (NMA)\n\n### Stan (Consistency Model)\n```stan\ndata {\n  int<lower=0> K;             // Number of studies\n  int<lower=0> T;             // Number of treatments\n  array[K] int<lower=1> t1;   // Treatment 1 index\n  array[K] int<lower=1> t2;   // Treatment 2 index\n  vector[K] y;                // Effect estimate\n  vector<lower=0>[K] se;\n}\nparameters {\n  vector[T-1] d_raw;          // Basic parameters (vs reference)\n  real<lower=0> tau;\n  vector[K] delta;\n}\ntransformed parameters {\n  vector[T] d;\n  d[1] = 0;                   // Reference treatment\n  d[2:T] = d_raw;\n}\nmodel {\n  d_raw ~ normal(0, 10);\n  tau ~ cauchy(0, 0.5);\n\n  for (k in 1:K) {\n    delta[k] ~ normal(d[t2[k]] - d[t1[k]], tau);\n    y[k] ~ normal(delta[k], se[k]);\n  }\n}\ngenerated quantities {\n  // Treatment rankings\n  array[T] int rank;\n  {\n    array[T] int order = sort_indices_desc(d);\n    for (t in 1:T) rank[order[t]] = t;\n  }\n}\n```\n\n## Publication Bias\n\n### Selection Model (Stan)\n```stan\ndata {\n  int<lower=0> K;\n  vector[K] y;\n  vector<lower=0>[K] se;\n  vector<lower=0,upper=1>[K] published;  // Publication indicator\n}\nparameters {\n  real mu;\n  real<lower=0> tau;\n  vector[K] theta;\n  real<lower=0> alpha;        // Selection severity\n}\nmodel {\n  theta ~ normal(mu, tau);\n  y ~ normal(theta, se);\n\n  // Selection model: higher z-scores more likely published\n  for (k in 1:K) {\n    real z = y[k] / se[k];\n    published[k] ~ bernoulli(Phi(alpha * z));\n  }\n}\n```\n\n## Key Statistics\n\n```stan\ngenerated quantities {\n  // Heterogeneity\n  real tau2 = square(tau);\n  real I2 = tau2 / (tau2 + mean(square(se)));\n\n  // Prediction interval\n  real pred_lower = mu - 1.96 * tau;\n  real pred_upper = mu + 1.96 * tau;\n\n  // Probability effect > 0\n  real prob_positive = 1 - normal_cdf(0 | mu, tau);\n}\n```\n\n## Priors for Heterogeneity\n\n| Context | tau prior |\n|---------|-----------|\n| Pharmacological | `half_normal(0, 0.5)` |\n| Medical devices | `half_normal(0, 1)` |\n| Behavioral | `half_cauchy(0, 1)` |\n| Default | `half_cauchy(0, 0.5)` |"
              },
              {
                "name": "model-diagnostics",
                "description": "MCMC diagnostics for Bayesian models including convergence assessment, effective sample size, divergences, and posterior predictive checks.",
                "path": "plugins/bayesian-modeling/skills/model-diagnostics/SKILL.md",
                "frontmatter": {
                  "name": "model-diagnostics",
                  "description": "MCMC diagnostics for Bayesian models including convergence assessment, effective sample size, divergences, and posterior predictive checks."
                },
                "content": "# Model Diagnostics\n\n## Key Convergence Metrics\n\n| Metric | Good Value | Concern |\n|--------|------------|---------|\n| Rhat | < 1.01 | > 1.1 indicates non-convergence |\n| ESS bulk | > 400 | < 100 unreliable estimates |\n| ESS tail | > 400 | < 100 unreliable intervals |\n| Divergences | 0 | Any indicates geometry issues |\n| Max treedepth | 0 hits | Hitting limit = slow exploration |\n\n## Stan Diagnostics (cmdstanr)\n\n```r\nlibrary(cmdstanr)\n\nfit <- mod$sample(data = stan_data, ...)\n\n# Quick check\nfit$cmdstan_diagnose()\n\n# Summary with diagnostics\nfit$summary()\n\n# Detailed diagnostics\nfit$diagnostic_summary()\n\n# Extract specific metrics\ndraws <- fit$draws()\nrhat <- posterior::rhat(draws)\ness_bulk <- posterior::ess_bulk(draws)\ness_tail <- posterior::ess_tail(draws)\n\n# Divergences\nnp <- fit$sampler_diagnostics()\nsum(np[,,\"divergent__\"])\n\n# Treedepth\nsum(np[,,\"treedepth__\"] == 10)  # Default max\n```\n\n## JAGS Diagnostics (R2jags)\n\n```r\nlibrary(R2jags)\nlibrary(coda)\n\nfit <- jags(...)\n\n# Summary (includes Rhat, n.eff)\nprint(fit)\nfit$BUGSoutput$summary\n\n# Rhat\nmax(fit$BUGSoutput$summary[,\"Rhat\"])\n\n# Effective sample size\nmin(fit$BUGSoutput$summary[,\"n.eff\"])\n\n# Convert to coda\nmcmc_obj <- as.mcmc(fit)\n\n# Gelman-Rubin\ngelman.diag(mcmc_obj)\n\n# Autocorrelation\nautocorr.diag(mcmc_obj)\nautocorr.plot(mcmc_obj)\n\n# Geweke diagnostic\ngeweke.diag(mcmc_obj)\n```\n\n## Visual Diagnostics\n\n### Trace Plots\n```r\n# Stan (bayesplot)\nlibrary(bayesplot)\nmcmc_trace(fit$draws(), pars = c(\"mu\", \"sigma\"))\n\n# JAGS\ntraceplot(fit)\n```\n\n### Rank Histograms\n```r\n# Should be uniform if chains mixed well\nmcmc_rank_hist(fit$draws(), pars = \"mu\")\n```\n\n### Pairs Plot (Detect Correlations)\n```r\nmcmc_pairs(fit$draws(), pars = c(\"mu\", \"sigma\", \"tau\"))\n```\n\n## Divergence Diagnosis (Stan)\n\n```r\n# Identify divergent transitions\nnp <- nuts_params(fit)\ndivergent <- np[np$Parameter == \"divergent__\" & np$Value == 1, ]\n\n# Pairs plot highlighting divergences\nmcmc_pairs(fit$draws(), np = np,\n           pars = c(\"mu\", \"tau\"),\n           off_diag_args = list(size = 0.5))\n\n# Common fixes:\n# 1. Increase adapt_delta\nfit <- mod$sample(data = stan_data, adapt_delta = 0.95)\n\n# 2. Use non-centered parameterization\n# 3. Reparameterize (use Cholesky for covariances)\n```\n\n## Effective Sample Size\n\n```r\n# Rule of thumb: ESS > 10 * num_chains for reliable Rhat\n# ESS > 100 for reasonable posterior estimates\n# ESS > 400 for reliable tail quantiles\n\n# If low ESS:\n# 1. Run longer chains\n# 2. Thin the samples (last resort)\n# 3. Improve parameterization\n# 4. Use more informative priors\n```\n\n## Posterior Predictive Checks\n\n### Stan (generated quantities)\n```stan\ngenerated quantities {\n  array[N] real y_rep;\n  for (n in 1:N)\n    y_rep[n] = normal_rng(mu[n], sigma);\n}\n```\n\n### R Visualization\n```r\nlibrary(bayesplot)\n\n# Density overlay\ny_rep <- fit$draws(\"y_rep\", format = \"matrix\")\nppc_dens_overlay(y, y_rep[1:50, ])\n\n# Intervals\nppc_intervals(y, y_rep)\n\n# Statistics\nppc_stat(y, y_rep, stat = \"mean\")\nppc_stat(y, y_rep, stat = \"sd\")\nppc_stat(y, y_rep, stat = function(x) max(x) - min(x))\n```\n\n## Model Comparison\n\n### LOO-CV (Stan)\n```r\nlibrary(loo)\n\n# Add log_lik to generated quantities\nloo1 <- fit1$loo()\nloo2 <- fit2$loo()\n\n# Compare\nloo_compare(loo1, loo2)\n\n# Check Pareto k diagnostics\nplot(loo1)\n```\n\n### WAIC\n```r\nwaic1 <- waic(log_lik1)\nwaic2 <- waic(log_lik2)\nloo_compare(waic1, waic2)\n```\n\n### DIC (JAGS)\n```r\nfit$BUGSoutput$DIC\nfit$BUGSoutput$pD  # Effective number of parameters\n```\n\n## Troubleshooting Guide\n\n| Problem | Symptoms | Solutions |\n|---------|----------|-----------|\n| Non-convergence | Rhat > 1.1 | Longer warmup, better inits |\n| Divergences | divergent__ > 0 | Non-centered param, higher adapt_delta |\n| Low ESS | ESS < 100 | Longer chains, better param |\n| Slow mixing | High autocorrelation | Reparameterize, QR decomposition |\n| Hitting max_treedepth | treedepth == max | Increase max_treedepth |\n\n## Quick Diagnostic Checklist\n\n```r\ncheck_diagnostics <- function(fit) {\n  cat(\"=== MCMC Diagnostics ===\\n\")\n\n  # For Stan\n  if (inherits(fit, \"CmdStanMCMC\")) {\n    summ <- fit$summary()\n    diag <- fit$diagnostic_summary()\n\n    cat(\"Max Rhat:\", max(summ$rhat, na.rm=TRUE),\n        ifelse(max(summ$rhat, na.rm=TRUE) < 1.01, \"✓\", \"✗\"), \"\\n\")\n    cat(\"Min ESS bulk:\", min(summ$ess_bulk, na.rm=TRUE),\n        ifelse(min(summ$ess_bulk, na.rm=TRUE) > 400, \"✓\", \"✗\"), \"\\n\")\n    cat(\"Divergences:\", sum(diag$num_divergent),\n        ifelse(sum(diag$num_divergent) == 0, \"✓\", \"✗\"), \"\\n\")\n    cat(\"Max treedepth:\", sum(diag$num_max_treedepth),\n        ifelse(sum(diag$num_max_treedepth) == 0, \"✓\", \"✗\"), \"\\n\")\n  }\n\n  # For JAGS\n  if (inherits(fit, \"rjags\")) {\n    summ <- fit$BUGSoutput$summary\n    cat(\"Max Rhat:\", max(summ[,\"Rhat\"], na.rm=TRUE),\n        ifelse(max(summ[,\"Rhat\"], na.rm=TRUE) < 1.1, \"✓\", \"✗\"), \"\\n\")\n    cat(\"Min n.eff:\", min(summ[,\"n.eff\"], na.rm=TRUE),\n        ifelse(min(summ[,\"n.eff\"], na.rm=TRUE) > 100, \"✓\", \"✗\"), \"\\n\")\n    cat(\"DIC:\", fit$BUGSoutput$DIC, \"\\n\")\n  }\n}\n```"
              },
              {
                "name": "pymc-fundamentals",
                "description": "Foundational knowledge for writing PyMC 5 models including syntax, distributions, sampling, and ArviZ diagnostics. Use when creating or reviewing PyMC models.",
                "path": "plugins/bayesian-modeling/skills/pymc-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "pymc-fundamentals",
                  "description": "Foundational knowledge for writing PyMC 5 models including syntax, distributions, sampling, and ArviZ diagnostics. Use when creating or reviewing PyMC models."
                },
                "content": "# PyMC 5 Fundamentals\n\n## When to Use This Skill\n\n- Writing new PyMC models in Python\n- Understanding PyMC syntax and API\n- Converting models from Stan/JAGS to PyMC\n- Diagnosing sampling issues with ArviZ\n\n## Model Structure\n\n```python\nimport pymc as pm\nimport numpy as np\nimport arviz as az\n\nwith pm.Model() as model:\n    # 1. Priors\n    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # 2. Likelihood\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_data)\n\n    # 3. Sample\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# 4. Diagnostics\naz.summary(trace)\n```\n\n## CRITICAL: SD Parameterization\n\n**PyMC uses SD (like Stan), NOT precision (like BUGS):**\n\n```python\n# PyMC (SD)\npm.Normal(\"x\", mu=0, sigma=1)      # sigma is SD\n\n# BUGS equivalent would be tau = 1/sigma² = 1\n```\n\n## Distribution Quick Reference\n\n### Continuous\n```python\npm.Normal(\"x\", mu=0, sigma=1)           # Normal\npm.HalfNormal(\"x\", sigma=1)             # Half-normal (>0)\npm.HalfCauchy(\"x\", beta=2.5)            # Half-Cauchy (>0)\npm.Exponential(\"x\", lam=1)              # Exponential\npm.Uniform(\"x\", lower=0, upper=1)       # Uniform\npm.Beta(\"x\", alpha=1, beta=1)           # Beta\npm.Gamma(\"x\", alpha=2, beta=1)          # Gamma\npm.StudentT(\"x\", nu=3, mu=0, sigma=1)   # Student-t\npm.LogNormal(\"x\", mu=0, sigma=1)        # Log-normal\npm.TruncatedNormal(\"x\", mu=0, sigma=1, lower=0)  # Truncated\n```\n\n### Discrete\n```python\npm.Bernoulli(\"x\", p=0.5)                # Bernoulli\npm.Binomial(\"x\", n=10, p=0.5)           # Binomial\npm.Poisson(\"x\", mu=5)                   # Poisson\npm.NegativeBinomial(\"x\", mu=5, alpha=1) # Negative binomial\npm.Categorical(\"x\", p=[0.3, 0.5, 0.2])  # Categorical\n```\n\n### Multivariate\n```python\npm.MvNormal(\"x\", mu=np.zeros(K), cov=np.eye(K))\npm.Dirichlet(\"x\", a=np.ones(K))\npm.LKJCholeskyCov(\"chol\", n=K, eta=2, sd_dist=pm.Exponential.dist(1))\n```\n\n## Sampling\n\n```python\n# Standard NUTS\ntrace = pm.sample(\n    draws=1000,          # Samples per chain\n    tune=1000,           # Warmup\n    chains=4,\n    cores=4,\n    target_accept=0.8,   # Increase for divergences\n    random_seed=42,\n    return_inferencedata=True\n)\n\n# Variational inference (fast)\napprox = pm.fit(n=30000, method=\"advi\")\ntrace = approx.sample(1000)\n\n# Predictive sampling\nprior_pred = pm.sample_prior_predictive(500)\npost_pred = pm.sample_posterior_predictive(trace)\n```\n\n## Bayesian Workflow (Statistical Rethinking)\n\n### 1. Prior Predictive Check\n```python\nwith model:\n    prior_pred = pm.sample_prior_predictive(500, random_seed=42)\naz.plot_ppc(prior_pred, group=\"prior\")\n```\n\n### 2. Fit Model\n```python\nwith model:\n    trace = pm.sample(1000, tune=1000, target_accept=0.9,\n                      return_inferencedata=True)\n```\n\n### 3. Diagnostics\n```python\naz.summary(trace, hdi_prob=0.89)\naz.plot_trace(trace)\naz.plot_rank_hist(trace)  # Ranked histograms (preferred)\n```\n\n### 4. Posterior Predictive Check\n```python\nwith model:\n    post_pred = pm.sample_posterior_predictive(trace)\naz.plot_ppc(post_pred, num_pp_samples=100)\n```\n\n### 5. Model Comparison\n```python\nloo1 = az.loo(trace1)\nloo2 = az.loo(trace2)\naz.compare({\"m1\": trace1, \"m2\": trace2})\naz.plot_khat(loo1)  # k > 0.7 is problematic\n```\n\n## pm.Deterministic for Tracking\n\n**Always track mu for plotting:**\n\n```python\n# Inside model\nmu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta))\n\n# Access later\ntrace.posterior[\"mu\"]  # All samples of mu\n```\n\n## Data Extraction Patterns\n\n```python\n# Extract to DataFrame\ntrace_df = az.extract_dataset(trace).to_dataframe()\n\n# Access specific parameters\npost = az.extract_dataset(trace[\"posterior\"])\nmu_samples = post[\"mu\"].values\n\n# Get numpy arrays\nalpha_values = trace.posterior[\"alpha\"].values  # (chains, draws)\n```\n\n## HDI Visualization\n\n```python\n# Compute mu at new x values\nx_seq = np.linspace(x.min(), x.max(), 100)\nmu_pred = post[\"alpha\"] + post[\"beta\"] * x_seq[:, None]\n\n# Plot HDI bands\naz.plot_hdi(x_seq, mu_pred.T, hdi_prob=0.89)\nplt.scatter(x, y)\n```\n\n## ArviZ Diagnostics\n\n```python\nimport arviz as az\n\n# Configure defaults\naz.rcParams[\"stats.hdi_prob\"] = 0.89\n\n# Summary table\nsummary = az.summary(trace, hdi_prob=0.89)\n\n# Key metrics\nmax_rhat = summary[\"r_hat\"].max()       # Should be < 1.01\nmin_ess = summary[\"ess_bulk\"].min()     # Should be > 400\n\n# Plots\naz.plot_trace(trace)                    # Trace plots\naz.plot_rank_hist(trace)                # Ranked histograms (preferred!)\naz.plot_posterior(trace)                # Posteriors\naz.plot_forest(trace)                   # Forest plot\naz.plot_pair(trace)                     # Pairs plot\n\n# Model comparison\naz.loo(trace)                           # LOO-CV\naz.waic(trace)                          # WAIC\naz.compare({\"m1\": trace1, \"m2\": trace2})\n```\n\n## Diagnostic Checklist\n\n- [ ] Rhat < 1.01 for all parameters\n- [ ] ESS_bulk > 400\n- [ ] ESS_tail > 400\n- [ ] Prior predictive produces sensible values\n- [ ] Posterior predictive matches data pattern\n- [ ] Pareto k < 0.7 for LOO\n\n## Non-Centered Parameterization\n\nFor hierarchical models:\n\n```python\n# Centered (may have divergences)\ntheta = pm.Normal(\"theta\", mu=mu, sigma=tau, shape=J)\n\n# Non-centered (recommended)\ntheta_raw = pm.Normal(\"theta_raw\", mu=0, sigma=1, shape=J)\ntheta = pm.Deterministic(\"theta\", mu + tau * theta_raw)\n```\n\n## PyTensor Math Operations\n\nInside `with pm.Model()`, use `pm.math` not `np`:\n\n```python\n# Correct\nmu = pm.math.dot(X, beta)\np = pm.math.sigmoid(eta)\nlog_x = pm.math.log(x)\n\n# Wrong (will fail)\nmu = np.dot(X, beta)  # Don't use numpy inside model\n```\n\n## Common Priors\n\n```python\n# Intercept\nalpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n\n# Coefficients\nbeta = pm.Normal(\"beta\", mu=0, sigma=2.5, shape=K)\n\n# Scale (SD)\nsigma = pm.HalfNormal(\"sigma\", sigma=1)\nsigma = pm.HalfCauchy(\"sigma\", beta=2.5)\nsigma = pm.Exponential(\"sigma\", lam=1)\n\n# Hierarchical SD\ntau = pm.HalfCauchy(\"tau\", beta=2.5)\n\n# Correlation matrix\nchol, corr, stds = pm.LKJCholeskyCov(\"chol\", n=K, eta=2,\n                                      sd_dist=pm.Exponential.dist(1))\n```\n\n## Key Differences from Stan\n\n| Feature | PyMC | Stan |\n|---------|------|------|\n| Syntax | Python | DSL |\n| Arrays | `shape=K` | `array[K]` |\n| Math | `pm.math.dot()` | `*` operator |\n| Blocks | Single context | 7 blocks |\n| Output | InferenceData | CmdStanMCMC |\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Divergences | Increase `target_accept` to 0.9-0.99 |\n| Low ESS | Run longer chains, reparameterize |\n| Shape errors | Check `shape=` parameter |\n| Slow | Use ADVI for quick approximation |\n| Memory | Reduce chains or use mini-batch |"
              },
              {
                "name": "regression-models",
                "description": "Bayesian regression models including linear, logistic, Poisson, negative binomial, and robust regression with Stan and JAGS implementations.",
                "path": "plugins/bayesian-modeling/skills/regression-models/SKILL.md",
                "frontmatter": {
                  "name": "regression-models",
                  "description": "Bayesian regression models including linear, logistic, Poisson, negative binomial, and robust regression with Stan and JAGS implementations."
                },
                "content": "# Regression Models\n\n## Linear Regression\n\n### Stan\n```stan\ndata {\n  int<lower=0> N;\n  int<lower=0> K;\n  matrix[N, K] X;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  alpha ~ normal(0, 10);\n  beta ~ normal(0, 5);\n  sigma ~ exponential(1);\n  y ~ normal(alpha + X * beta, sigma);\n}\ngenerated quantities {\n  array[N] real y_rep;\n  for (n in 1:N)\n    y_rep[n] = normal_rng(alpha + X[n] * beta, sigma);\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:N) {\n    y[i] ~ dnorm(mu[i], tau)\n    mu[i] <- alpha + inprod(X[i,], beta[])\n  }\n  alpha ~ dnorm(0, 0.001)\n  for (k in 1:K) { beta[k] ~ dnorm(0, 0.001) }\n  tau ~ dgamma(0.001, 0.001)\n  sigma <- 1/sqrt(tau)\n}\n```\n\n## Logistic Regression\n\n### Stan\n```stan\ndata {\n  int<lower=0> N;\n  int<lower=0> K;\n  matrix[N, K] X;\n  array[N] int<lower=0,upper=1> y;\n}\nparameters {\n  real alpha;\n  vector[K] beta;\n}\nmodel {\n  alpha ~ normal(0, 2.5);\n  beta ~ normal(0, 2.5);\n  y ~ bernoulli_logit(alpha + X * beta);\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:N) {\n    y[i] ~ dbern(p[i])\n    logit(p[i]) <- alpha + inprod(X[i,], beta[])\n  }\n  alpha ~ dnorm(0, 0.4)    # SD ≈ 1.58\n  for (k in 1:K) { beta[k] ~ dnorm(0, 0.4) }\n}\n```\n\n## Poisson Regression\n\n### Stan\n```stan\nmodel {\n  alpha ~ normal(0, 5);\n  beta ~ normal(0, 2.5);\n  y ~ poisson_log(alpha + X * beta);\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:N) {\n    y[i] ~ dpois(lambda[i])\n    log(lambda[i]) <- alpha + inprod(X[i,], beta[])\n  }\n}\n```\n\n## Negative Binomial (Overdispersed Counts)\n\n### Stan\n```stan\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> phi;  // Overdispersion\n}\nmodel {\n  phi ~ exponential(1);\n  y ~ neg_binomial_2_log(alpha + X * beta, phi);\n}\n```\n\n## Robust Regression (Student-t Errors)\n\n### Stan\n```stan\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> sigma;\n  real<lower=1> nu;  // Degrees of freedom\n}\nmodel {\n  nu ~ gamma(2, 0.1);  // Prior on df\n  y ~ student_t(nu, alpha + X * beta, sigma);\n}\n```\n\n## QR Decomposition (For Correlated Predictors)\n\n```stan\ntransformed data {\n  matrix[N, K] Q = qr_thin_Q(X) * sqrt(N - 1.0);\n  matrix[K, K] R = qr_thin_R(X) / sqrt(N - 1.0);\n  matrix[K, K] R_inv = inverse(R);\n}\nparameters {\n  vector[K] theta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(Q * theta, sigma);\n}\ngenerated quantities {\n  vector[K] beta = R_inv * theta;\n}\n```\n\n## Prior Recommendations\n\n| Parameter | Weakly Informative | Reference |\n|-----------|-------------------|-----------|\n| Intercept | `normal(0, 10)` | Scale of outcome |\n| Coefficients | `normal(0, 2.5)` | Gelman et al. |\n| SD (sigma) | `exponential(1)` | Half-normal alternative |\n| Logistic coef | `normal(0, 2.5)` | ~4 logit units = extreme |"
              },
              {
                "name": "stan-fundamentals",
                "description": "Foundational knowledge for writing Stan 2.37 models including program structure, type system, distributions, and best practices. Use when creating or reviewing Stan models.",
                "path": "plugins/bayesian-modeling/skills/stan-fundamentals/SKILL.md",
                "frontmatter": {
                  "name": "stan-fundamentals",
                  "description": "Foundational knowledge for writing Stan 2.37 models including program structure, type system, distributions, and best practices. Use when creating or reviewing Stan models."
                },
                "content": "# Stan Fundamentals\n\n## When to Use This Skill\n\n- Writing new Stan models from scratch\n- Understanding Stan program structure\n- Learning Stan syntax and conventions\n- Translating models from other languages to Stan\n- Optimizing existing Stan code\n\n## Program Structure\n\nStan models have up to 7 blocks in this exact order:\n\n```stan\nfunctions { }           // User-defined functions\ndata { }                // Input data declarations\ntransformed data { }    // Data preprocessing\nparameters { }          // Model parameters\ntransformed parameters { } // Derived parameters\nmodel { }               // Log probability\ngenerated quantities { }  // Posterior predictions\n```\n\nAll blocks are optional. Empty string is valid (but useless) Stan program.\n\n## Type System Quick Reference\n\n### Scalars\n```stan\nint n;                    // Integer\nreal x;                   // Real number\ncomplex z;                // Complex number\n```\n\n### Vectors and Matrices\n```stan\nvector[N] v;              // Column vector\nrow_vector[N] r;          // Row vector\nmatrix[M, N] A;           // Matrix\n```\n\n### Arrays (Modern Syntax)\n```stan\narray[N] real x;          // 1D array of reals\narray[M, N] int y;        // 2D array of integers\narray[J] vector[K] theta; // Array of vectors\n```\n\n### Constrained Types\n```stan\nreal<lower=0> sigma;              // Non-negative\nreal<lower=0, upper=1> p;         // Probability\nsimplex[K] theta;                 // Sums to 1\nordered[K] c;                     // Ascending\ncorr_matrix[K] Omega;             // Correlation\ncov_matrix[K] Sigma;              // Covariance\ncholesky_factor_corr[K] L_Omega;  // Cholesky correlation\n```\n\n## Key Distributions\n\n### Continuous (SD parameterization!)\n```stan\ny ~ normal(mu, sigma);      // sigma is SD\ny ~ student_t(nu, mu, sigma);\ny ~ cauchy(mu, sigma);\ny ~ exponential(lambda);\ny ~ gamma(alpha, beta);\ny ~ beta(a, b);\ny ~ lognormal(mu, sigma);\n```\n\n### Discrete\n```stan\ny ~ bernoulli(theta);\ny ~ binomial(n, theta);\ny ~ poisson(lambda);\ny ~ neg_binomial_2(mu, phi);\ny ~ categorical(theta);\n```\n\n### Multivariate\n```stan\ny ~ multi_normal(mu, Sigma);        // Sigma is COVARIANCE\ny ~ multi_normal_cholesky(mu, L);\ny ~ lkj_corr(eta);\n```\n\n## Essential Patterns\n\n### Vectorization\n```stan\n// GOOD - Efficient\ny ~ normal(mu, sigma);\n\n// BAD - Slow\nfor (n in 1:N) y[n] ~ normal(mu[n], sigma);\n```\n\n### Non-Centered Parameterization\n```stan\nparameters {\n  vector[J] theta_raw;\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * theta_raw;\n}\nmodel {\n  theta_raw ~ std_normal();\n}\n```\n\n### Target Syntax\n```stan\n// These are equivalent:\ny ~ normal(mu, sigma);\ntarget += normal_lpdf(y | mu, sigma);\n```\n\n## Common Priors\n\n```stan\n// Location parameters\nmu ~ normal(0, 10);\n\n// Scale parameters\nsigma ~ exponential(1);\nsigma ~ cauchy(0, 2.5);  // half-Cauchy when sigma > 0\n\n// Probabilities\ntheta ~ beta(1, 1);  // Uniform on (0,1)\n\n// Regression coefficients\nbeta ~ normal(0, 2.5);\n\n// Correlation matrices\nOmega ~ lkj_corr(2);  // eta=2 favors identity\n```\n\n## R Integration (cmdstanr)\n\n```r\nlibrary(cmdstanr)\nmod <- cmdstan_model(\"model.stan\")\nfit <- mod$sample(data = stan_data, chains = 4)\nfit$summary()\nfit$cmdstan_diagnose()\n```\n\n## Bayesian Workflow (Statistical Rethinking)\n\n### 1. Prior Predictive Check\n```r\n# Simulate from priors before fitting\nn_sim <- 1000\nprior_alpha <- rnorm(n_sim, 0, 10)\nprior_sigma <- rexp(n_sim, 1)\n# Plot: do these produce sensible y values?\n```\n\n### 2. Fit Model\n```r\nfit <- mod$sample(data = stan_data, chains = 4, adapt_delta = 0.95)\n```\n\n### 3. Diagnostics\n```r\nfit$summary()              # Rhat, ESS\nfit$cmdstan_diagnose()     # Divergences, treedepth\nlibrary(bayesplot)\nmcmc_rank_hist(fit$draws()) # Ranked traceplots (preferred)\n```\n\n### 4. Posterior Predictive Check\n```r\ny_rep <- fit$draws(\"y_rep\", format = \"matrix\")\nlibrary(bayesplot)\nppc_dens_overlay(y, y_rep[1:100, ])\n```\n\n### 5. Model Comparison\n```r\nlibrary(loo)\nloo1 <- loo(fit1$draws(\"log_lik\"))\nloo2 <- loo(fit2$draws(\"log_lik\"))\nloo_compare(loo1, loo2)\n```\n\n## link vs sim Pattern\n\n### link(): Uncertainty in mu (epistemic)\n```r\n# Posterior of expected value\npost <- fit$draws(format = \"df\")\nmu <- post$alpha + post$beta * x_new  # Matrix of mu samples\nmu_PI <- apply(mu, 2, quantile, c(0.055, 0.945))\n```\n\n### sim(): Prediction interval (epistemic + aleatoric)\n```r\n# Includes observation noise\ny_sim <- rnorm(n_samples, mu, post$sigma)\ny_PI <- apply(y_sim, 2, quantile, c(0.055, 0.945))\n```\n\n## Generated Quantities Template\n\nAlways include for diagnostics and model comparison:\n\n```stan\ngenerated quantities {\n  vector[N] log_lik;  // For LOO/WAIC\n  array[N] real y_rep;  // For posterior predictive checks\n\n  for (n in 1:N) {\n    log_lik[n] = normal_lpdf(y[n] | mu[n], sigma);\n    y_rep[n] = normal_rng(mu[n], sigma);\n  }\n}\n```\n\n## Diagnostic Checklist\n\n- [ ] Rhat < 1.01 for all parameters\n- [ ] ESS_bulk > 400\n- [ ] ESS_tail > 400\n- [ ] Zero divergences\n- [ ] Not hitting max_treedepth\n- [ ] Prior predictive produces sensible values\n- [ ] Posterior predictive matches data pattern\n\n## Key Differences from BUGS\n\n| Feature | Stan | BUGS/JAGS |\n|---------|------|-----------|\n| Normal | `normal(mu, sigma)` SD | `dnorm(mu, tau)` precision |\n| MVN | `multi_normal(mu, Sigma)` cov | `dmnorm(mu, Omega)` precision |\n| Execution | Sequential (order matters) | Declarative (order doesn't matter) |\n| Sampling | HMC/NUTS | Gibbs/Metropolis |"
              },
              {
                "name": "survival-models",
                "description": "Bayesian survival analysis models including exponential, Weibull, log-normal, and piecewise exponential hazard models with censoring support.",
                "path": "plugins/bayesian-modeling/skills/survival-models/SKILL.md",
                "frontmatter": {
                  "name": "survival-models",
                  "description": "Bayesian survival analysis models including exponential, Weibull, log-normal, and piecewise exponential hazard models with censoring support."
                },
                "content": "# Survival Models\n\n## Data Structure\n\n```stan\ndata {\n  int<lower=0> N;\n  vector<lower=0>[N] time;      // Observed/censored time\n  array[N] int<lower=0,upper=1> event;  // 1=event, 0=censored\n  matrix[N, K] X;               // Covariates\n}\n```\n\n## Exponential Model\n\n### Stan\n```stan\nparameters {\n  real alpha;           // Log baseline hazard\n  vector[K] beta;\n}\nmodel {\n  alpha ~ normal(0, 2);\n  beta ~ normal(0, 1);\n\n  for (n in 1:N) {\n    real lambda = exp(alpha + X[n] * beta);\n    if (event[n] == 1)\n      target += exponential_lpdf(time[n] | lambda);\n    else\n      target += exponential_lccdf(time[n] | lambda);  // Survival\n  }\n}\n```\n\n### JAGS (with censoring)\n```\nmodel {\n  for (i in 1:N) {\n    is.censored[i] ~ dinterval(t[i], t.cen[i])\n    t[i] ~ dexp(lambda[i])\n    log(lambda[i]) <- alpha + inprod(X[i,], beta[])\n  }\n  alpha ~ dnorm(0, 0.25)\n  for (k in 1:K) { beta[k] ~ dnorm(0, 1) }\n}\n```\n\n## Weibull Model\n\n### Stan (AFT Parameterization)\n```stan\nparameters {\n  real alpha;                    // Intercept (log scale)\n  vector[K] beta;\n  real<lower=0> shape;           // Weibull shape\n}\nmodel {\n  alpha ~ normal(0, 5);\n  beta ~ normal(0, 2);\n  shape ~ exponential(1);\n\n  for (n in 1:N) {\n    real mu = alpha + X[n] * beta;\n    if (event[n] == 1)\n      target += weibull_lpdf(time[n] | shape, exp(mu));\n    else\n      target += weibull_lccdf(time[n] | shape, exp(mu));\n  }\n}\n```\n\n### JAGS\n```\nmodel {\n  for (i in 1:N) {\n    is.censored[i] ~ dinterval(t[i], t.cen[i])\n    t[i] ~ dweib(shape, lambda[i])\n    log(lambda[i]) <- alpha + inprod(X[i,], beta[])\n  }\n  shape ~ dgamma(1, 0.001)\n  alpha ~ dnorm(0, 0.01)\n  for (k in 1:K) { beta[k] ~ dnorm(0, 0.01) }\n}\n```\n\n## Log-Normal Model\n\n### Stan\n```stan\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  for (n in 1:N) {\n    real mu = alpha + X[n] * beta;\n    if (event[n] == 1)\n      target += lognormal_lpdf(time[n] | mu, sigma);\n    else\n      target += lognormal_lccdf(time[n] | mu, sigma);\n  }\n}\n```\n\n## Piecewise Exponential (Cox-like)\n\n### Stan\n```stan\ndata {\n  int<lower=0> N;\n  int<lower=0> J;               // Number of intervals\n  vector[J] cuts;               // Cut points\n  matrix[N, J] d;               // Time in each interval\n  array[N] int<lower=0,upper=1> event;\n  array[N] int<lower=1,upper=J> interval;  // Event interval\n  matrix[N, K] X;\n}\nparameters {\n  vector[J] log_baseline;       // Log baseline hazard per interval\n  vector[K] beta;\n}\nmodel {\n  log_baseline ~ normal(0, 2);\n  beta ~ normal(0, 1);\n\n  for (n in 1:N) {\n    real log_hazard = log_baseline[interval[n]] + X[n] * beta;\n\n    // Contribution from all intervals\n    for (j in 1:J)\n      target += -d[n,j] * exp(log_baseline[j] + X[n] * beta);\n\n    // Event contribution\n    if (event[n] == 1)\n      target += log_hazard;\n  }\n}\n```\n\n## Frailty Model (Random Effects)\n\n### Stan\n```stan\ndata {\n  int<lower=0> N;\n  int<lower=0> G;               // Number of groups\n  array[N] int<lower=1,upper=G> group;\n  // ... rest of survival data\n}\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> shape;\n  vector[G] frailty_raw;        // Non-centered\n  real<lower=0> sigma_frailty;\n}\ntransformed parameters {\n  vector[G] frailty = sigma_frailty * frailty_raw;\n}\nmodel {\n  sigma_frailty ~ exponential(1);\n  frailty_raw ~ std_normal();\n\n  for (n in 1:N) {\n    real mu = alpha + X[n] * beta + frailty[group[n]];\n    // ... Weibull likelihood with censoring\n  }\n}\n```\n\n## Generated Quantities\n\n```stan\ngenerated quantities {\n  // Hazard ratio for 1-unit increase in X[,1]\n  real HR = exp(beta[1]);\n\n  // Median survival at X=0\n  real median_survival = exp(alpha) * pow(log(2), 1/shape);\n\n  // Survival function at time t=1\n  array[N] real S_1;\n  for (n in 1:N)\n    S_1[n] = exp(-pow(1 / exp(alpha + X[n] * beta), shape));\n}\n```\n\n## Interpretation\n\n- **Weibull shape**: <1 decreasing hazard, =1 constant (exponential), >1 increasing\n- **HR**: Hazard ratio (multiplicative effect on hazard)\n- **AFT**: Accelerated failure time (multiplicative effect on survival time)"
              },
              {
                "name": "time-series-models",
                "description": "Bayesian time series models including AR, MA, ARMA, state-space models, and dynamic linear models in Stan and JAGS.",
                "path": "plugins/bayesian-modeling/skills/time-series-models/SKILL.md",
                "frontmatter": {
                  "name": "time-series-models",
                  "description": "Bayesian time series models including AR, MA, ARMA, state-space models, and dynamic linear models in Stan and JAGS."
                },
                "content": "# Time Series Models\n\n## AR(1) Model\n\n### Stan\n```stan\ndata {\n  int<lower=0> T;\n  vector[T] y;\n}\nparameters {\n  real mu;\n  real<lower=-1, upper=1> phi;  // Stationarity\n  real<lower=0> sigma;\n}\nmodel {\n  mu ~ normal(0, 10);\n  phi ~ uniform(-1, 1);\n  sigma ~ exponential(1);\n\n  // Stationary initial distribution\n  y[1] ~ normal(mu, sigma / sqrt(1 - phi^2));\n\n  // AR(1) likelihood\n  for (t in 2:T)\n    y[t] ~ normal(mu + phi * (y[t-1] - mu), sigma);\n}\n```\n\n### Vectorized Stan (Efficient)\n```stan\nmodel {\n  y[1] ~ normal(mu, sigma / sqrt(1 - square(phi)));\n  y[2:T] ~ normal(mu + phi * (y[1:(T-1)] - mu), sigma);\n}\n```\n\n### JAGS\n```\nmodel {\n  y[1] ~ dnorm(mu, tau / (1 - phi * phi))\n  for (t in 2:T) {\n    y[t] ~ dnorm(mu + phi * (y[t-1] - mu), tau)\n  }\n  mu ~ dnorm(0, 0.001)\n  phi ~ dunif(-1, 1)\n  tau ~ dgamma(0.001, 0.001)\n  sigma <- 1/sqrt(tau)\n}\n```\n\n## AR(p) Model\n\n### Stan\n```stan\ndata {\n  int<lower=0> T;\n  int<lower=1> P;  // AR order\n  vector[T] y;\n}\nparameters {\n  real mu;\n  vector[P] phi;\n  real<lower=0> sigma;\n}\nmodel {\n  mu ~ normal(0, 10);\n  phi ~ normal(0, 0.5);\n  sigma ~ exponential(1);\n\n  for (t in (P+1):T) {\n    real pred = mu;\n    for (p in 1:P)\n      pred += phi[p] * (y[t-p] - mu);\n    y[t] ~ normal(pred, sigma);\n  }\n}\n```\n\n## Local Level (Random Walk + Noise)\n\n### Stan\n```stan\ndata {\n  int<lower=0> T;\n  vector[T] y;\n}\nparameters {\n  vector[T] mu;           // Latent state\n  real<lower=0> sigma_y;  // Observation noise\n  real<lower=0> sigma_mu; // State noise\n}\nmodel {\n  sigma_y ~ exponential(1);\n  sigma_mu ~ exponential(1);\n\n  // State evolution (random walk)\n  mu[1] ~ normal(y[1], sigma_y);\n  mu[2:T] ~ normal(mu[1:(T-1)], sigma_mu);\n\n  // Observations\n  y ~ normal(mu, sigma_y);\n}\n```\n\n## Local Linear Trend\n\n### Stan\n```stan\nparameters {\n  vector[T] mu;           // Level\n  vector[T] delta;        // Trend\n  real<lower=0> sigma_y;\n  real<lower=0> sigma_mu;\n  real<lower=0> sigma_delta;\n}\nmodel {\n  // Level evolution\n  mu[2:T] ~ normal(mu[1:(T-1)] + delta[1:(T-1)], sigma_mu);\n\n  // Trend evolution\n  delta[2:T] ~ normal(delta[1:(T-1)], sigma_delta);\n\n  // Observations\n  y ~ normal(mu, sigma_y);\n}\n```\n\n## Seasonal Model\n\n### Stan (Additive Seasonality)\n```stan\ndata {\n  int<lower=0> T;\n  int<lower=2> S;  // Season length (e.g., 12 for monthly)\n  vector[T] y;\n}\nparameters {\n  vector[T] mu;\n  vector[S-1] gamma_init;  // Initial seasonal effects\n  real<lower=0> sigma_y;\n  real<lower=0> sigma_mu;\n  real<lower=0> sigma_gamma;\n}\ntransformed parameters {\n  vector[T] gamma;\n  // Sum-to-zero constraint\n  for (t in 1:(S-1))\n    gamma[t] = gamma_init[t];\n  gamma[S] = -sum(gamma_init);\n  for (t in (S+1):T)\n    gamma[t] = -sum(gamma[(t-S+1):(t-1)]) + normal_rng(0, sigma_gamma);\n}\nmodel {\n  y ~ normal(mu + gamma, sigma_y);\n}\n```\n\n## GARCH(1,1) (Volatility Clustering)\n\n### Stan\n```stan\nparameters {\n  real mu;\n  real<lower=0> alpha0;\n  real<lower=0, upper=1> alpha1;\n  real<lower=0, upper=1-alpha1> beta1;\n}\ntransformed parameters {\n  vector<lower=0>[T] sigma2;\n  sigma2[1] = alpha0 / (1 - alpha1 - beta1);\n  for (t in 2:T)\n    sigma2[t] = alpha0 + alpha1 * square(y[t-1] - mu) + beta1 * sigma2[t-1];\n}\nmodel {\n  y ~ normal(mu, sqrt(sigma2));\n}\n```\n\n## Diagnostics\n\n- Check stationarity constraints (|phi| < 1 for AR)\n- Examine residual autocorrelation (ACF/PACF)\n- One-step-ahead predictions for model comparison\n- Use `generated quantities` for forecasting"
              }
            ]
          }
        ]
      }
    }
  ]
}