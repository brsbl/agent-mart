{
  "owner": {
    "id": "MarkEnverus",
    "display_name": "Jark",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/89552395?u=80e698ec96e115f62c02cc49888b57003a550628&v=4",
    "url": "https://github.com/MarkEnverus",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 4,
      "total_skills": 0,
      "total_stars": 0,
      "total_forks": 1
    }
  },
  "repos": [
    {
      "full_name": "MarkEnverus/claude-scraper-agent",
      "url": "https://github.com/MarkEnverus/claude-scraper-agent",
      "description": "Automated scraper generation for PRT sourcing pipeline using Claude Code agents",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 1,
        "pushed_at": "2025-12-18T23:18:17Z",
        "created_at": "2025-11-23T16:36:21Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/README.md",
          "type": "blob",
          "size": 845
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1033
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 419
        },
        {
          "path": ".scraper-dev.example.md",
          "type": "blob",
          "size": 3425
        },
        {
          "path": "AGENT_CORE.md",
          "type": "blob",
          "size": 19108
        },
        {
          "path": "AGENT_VALIDATION_FIX.md",
          "type": "blob",
          "size": 12664
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 5754
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4391
        },
        {
          "path": "INSTALLATION.md",
          "type": "blob",
          "size": 5500
        },
        {
          "path": "INSTALLATION_GUIDE.md",
          "type": "blob",
          "size": 7879
        },
        {
          "path": "LANCEDB_PLUGIN_INTEGRATION.md",
          "type": "blob",
          "size": 15390
        },
        {
          "path": "PROJECT_SUMMARY.md",
          "type": "blob",
          "size": 10125
        },
        {
          "path": "QUICK_REFERENCE.md",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 13612
        },
        {
          "path": "infrastructure",
          "type": "tree",
          "size": null
        },
        {
          "path": "infrastructure/README.template.md",
          "type": "blob",
          "size": 8571
        },
        {
          "path": "infrastructure/collection_framework.py",
          "type": "blob",
          "size": 18552
        },
        {
          "path": "infrastructure/hash_registry.py",
          "type": "blob",
          "size": 10381
        },
        {
          "path": "infrastructure/kafka_utils.py",
          "type": "blob",
          "size": 18302
        },
        {
          "path": "infrastructure/logging_json.py",
          "type": "blob",
          "size": 5579
        },
        {
          "path": "infrastructure/pyproject.toml.template",
          "type": "blob",
          "size": 2308
        },
        {
          "path": "infrastructure/s3_utils.py",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "install-infrastructure.sh",
          "type": "blob",
          "size": 3901
        },
        {
          "path": "mypy.ini",
          "type": "blob",
          "size": 446
        },
        {
          "path": "plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/agents/ba-collator.md",
          "type": "blob",
          "size": 16187
        },
        {
          "path": "plugin/agents/ba-enhanced.md",
          "type": "blob",
          "size": 77479
        },
        {
          "path": "plugin/agents/ba-validator.md",
          "type": "blob",
          "size": 11194
        },
        {
          "path": "plugin/agents/code-quality-checker.md",
          "type": "blob",
          "size": 21698
        },
        {
          "path": "plugin/agents/email-collector-generator.md",
          "type": "blob",
          "size": 20331
        },
        {
          "path": "plugin/agents/ftp-collector-generator.md",
          "type": "blob",
          "size": 19841
        },
        {
          "path": "plugin/agents/http-collector-generator.md",
          "type": "blob",
          "size": 17840
        },
        {
          "path": "plugin/agents/scraper-fixer.md",
          "type": "blob",
          "size": 6023
        },
        {
          "path": "plugin/agents/scraper-generator.md",
          "type": "blob",
          "size": 44928
        },
        {
          "path": "plugin/agents/scraper-updater.md",
          "type": "blob",
          "size": 28371
        },
        {
          "path": "plugin/agents/website-parser-generator.md",
          "type": "blob",
          "size": 13715
        },
        {
          "path": "plugin/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/commands/analyze.md",
          "type": "blob",
          "size": 704
        },
        {
          "path": "plugin/commands/create-scraper.md",
          "type": "blob",
          "size": 441
        },
        {
          "path": "plugin/commands/fix-scraper.md",
          "type": "blob",
          "size": 6675
        },
        {
          "path": "plugin/commands/update-scraper.md",
          "type": "blob",
          "size": 8437
        },
        {
          "path": "plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/scraper-creation.md",
          "type": "blob",
          "size": 4263
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/test_hash_registry.py",
          "type": "blob",
          "size": 5500
        },
        {
          "path": "tests/test_kafka_utils.py",
          "type": "blob",
          "size": 18759
        },
        {
          "path": "tests/test_s3_utils.py",
          "type": "blob",
          "size": 13896
        },
        {
          "path": "uninstall.sh",
          "type": "blob",
          "size": 2448
        }
      ],
      "marketplace": {
        "name": "scraper-agent-marketplace",
        "version": "1.0.0",
        "description": "Automated scraper generation system for data collection pipelines",
        "owner_info": {
          "name": "Mark Johnson",
          "email": "mark.johnson@example.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "scraper-dev",
            "description": "Automated scraper generation for data collection pipelines with support for HTTP/REST APIs, website parsing, FTP/SFTP, and email attachments. Generates type-safe, mypy and ruff compliant code with automatic quality checking. Features tabbed question interface for better UX. Includes version tracking, scraper maintenance tools (/fix-scraper, /update-scraper), self-contained Kafka notification support, and optional config file pre-configuration.",
            "source": "./plugin",
            "category": "development",
            "version": "1.11.0",
            "author": {
              "name": "Mark Johnson",
              "email": "mark.johnson@example.com"
            },
            "install_commands": [
              "/plugin marketplace add MarkEnverus/claude-scraper-agent",
              "/plugin install scraper-dev@scraper-agent-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 1,
              "pushed_at": "2025-12-18T23:18:17Z",
              "created_at": "2025-11-23T16:36:21Z",
              "license": null
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Analyze any data source (API, FTP, website, email) using enhanced BA agent",
                "path": "plugin/commands/analyze.md",
                "frontmatter": {
                  "description": "Analyze any data source (API, FTP, website, email) using enhanced BA agent"
                },
                "content": "Launch the enhanced business analyst agent to analyze any data source from a URL.\n\n**Supported source types:**\n- REST/SOAP APIs\n- FTP/SFTP servers\n- Email data sources\n- Website portals with downloadable data\n- Database connection documentation\n\n**Usage:**\n```\n/scraper-dev:analyze {url}\n/scraper-dev:analyze {url} --type website\n```\n\nUse the Task tool with subagent_type='scraper-dev:ba-enhanced' to start the analysis.\n\n**CRITICAL**: Do NOT attempt analysis yourself. You MUST invoke the agent using the Task tool. The agent has specialized capabilities for browser automation and multi-protocol testing."
              },
              {
                "name": "/create-scraper",
                "description": "Interactive scraper generation workflow",
                "path": "plugin/commands/create-scraper.md",
                "frontmatter": {
                  "description": "Interactive scraper generation workflow"
                },
                "content": "Launch the scraper generation workflow by invoking the scraper-generator agent.\n\nUse the Task tool with subagent_type='scraper-dev:scraper-generator' to start the interactive scraper creation process.\n\nCRITICAL: Do NOT attempt to generate scrapers yourself. You MUST invoke the agent using the Task tool. The agent will ask the user all required questions using AskUserQuestion."
              },
              {
                "name": "/fix-scraper",
                "description": "Debug and fix issues in existing scrapers",
                "path": "plugin/commands/fix-scraper.md",
                "frontmatter": {
                  "description": "Debug and fix issues in existing scrapers"
                },
                "content": "# Fix Scraper Command\n\nYou are the Scraper Fix Agent. Your role is to diagnose and fix issues in existing scrapers.\n\n## ‚ö†Ô∏è CRITICAL ANTI-HALLUCINATION RULES ‚ö†Ô∏è\n\n**NEVER simulate or fabricate bash output. ALWAYS use actual tool results.**\n\nWhen scanning for scrapers:\n- ALWAYS run the actual bash command: `find sourcing/scraping -name \"scraper_*.py\" -type f 2>/dev/null`\n- ONLY report scrapers that appear in ACTUAL bash output\n- If bash returns empty/no results ‚Üí Report \"No scrapers found\"\n- NEVER use example data (NYISO, PJM, CAISO) unless it appears in actual bash output\n\nWhen reading files:\n- ALWAYS use Read tool to read the actual file\n- NEVER assume file contents based on patterns\n- If file doesn't exist ‚Üí Report \"File not found\"\n- Show actual file contents, not simulated content\n\nIf uncertain ‚Üí Re-run the bash command to verify.\nWhen showing results to user ‚Üí Include actual bash output for transparency.\n\n## Your Process\n\n### Step 1: Scan for Scrapers\n\nAsk user for their sourcing project path, then run:\n\n```bash\nfind sourcing/scraping -name \"scraper_*.py\" -type f 2>/dev/null\n```\n\nParse paths to extract {dataSource}/{dataSet} from results.\n\n**If NO scrapers found:**\n- Report: \"No scrapers found in sourcing/scraping/\"\n- Ask user if they want to create a new scraper instead\n- STOP - do not proceed\n\n**If scrapers found:**\n- Present list to user with AskUserQuestion\n- Options: Show actual paths from bash output\n- Let user select which scraper to fix\n\n### Step 2: Read Scraper Code\n\nFor the selected scraper, read these files (if they exist):\n1. Main scraper file: `sourcing/scraping/{source}/scraper_{source}_{type}_{method}.py`\n2. Test file: `sourcing/scraping/{source}/tests/test_scraper_{source}_{type}_{method}.py`\n3. README: `sourcing/scraping/{source}/README.md`\n\nUse Read tool for each file. If file doesn't exist, note that.\n\n### Step 3: Diagnose Issue\n\nAsk user: \"What's the problem with this scraper?\"\n\nUse AskUserQuestion with options:\n- \"API endpoint changed\" - The data source changed their API\n- \"Data format changed\" - Response format is different now\n- \"Authentication errors\" - Auth method changed or credentials invalid\n- \"Import errors\" - Missing dependencies or infrastructure updates\n- \"Scraper throwing errors\" - Runtime errors during execution\n- \"Other (describe)\" - Custom issue\n\nIf user provides error logs or stack traces, analyze them.\n\n### Step 4: Analyze and Propose Fix\n\nBased on the problem type:\n\n**For API endpoint changes:**\n- Compare current endpoint in code vs what user says it should be\n- Check if query parameters need updates\n- Verify authentication headers\n\n**For data format changes:**\n- Check how response is parsed\n- Look at validation logic\n- Check what fields are expected vs what's received\n\n**For authentication errors:**\n- Review auth method in code\n- Check environment variable usage\n- Verify header format\n\n**For import errors:**\n- Check infrastructure imports (collection_framework, hash_registry, kafka_utils, logging_json)\n- Verify infrastructure files exist\n- Check version compatibility if INFRASTRUCTURE_VERSION is present\n\n**For runtime errors:**\n- Analyze stack trace\n- Check for common issues (null pointers, type mismatches, missing error handling)\n- Look at try/catch blocks\n\nPresent findings and proposed fix to user:\n- Explain the root cause\n- Show specific code changes needed\n- Use Edit tool to demonstrate the diff\n\n### Step 5: Apply Fix\n\nAfter user approval:\n1. Use Edit tool to update scraper code\n2. Update tests if needed\n3. Update README if needed\n4. Update LAST_UPDATED timestamp if version tracking exists:\n   ```python\n   # LAST_UPDATED: {current_date}\n   ```\n\n### Step 6: Run Code Quality Checks\n\nAfter applying fixes, run quality checks to ensure no new issues were introduced:\n\nUse Task tool with subagent_type='scraper-dev:code-quality-checker':\n```python\nTask(\n    subagent_type='scraper-dev:code-quality-checker',\n    description='Check quality after fix',\n    prompt=f\"\"\"\n    Run mypy and ruff checks on the fixed scraper:\n\n    File: {scraper_file_path}\n\n    Process:\n    1. Check if mypy and ruff are installed\n    2. Run mypy type checking\n    3. Run ruff style checking\n    4. Report results\n    5. Offer to auto-fix any issues found\n    6. Re-run checks after fixes\n\n    Only complete when checks pass or user approves remaining issues.\n    \"\"\"\n)\n```\n\n### Step 7: Report Final Status\n\nAfter quality checker completes, report what was changed:\n\n```\n‚úÖ Scraper fixed successfully!\n\n**Changes Made:**\n- Updated API endpoint from X to Y\n- Fixed response parsing logic\n- Updated tests to match new format\n\nüîç Code Quality Results:\n- mypy: ‚úÖ 0 errors\n- ruff: ‚úÖ 0 issues\n\n**Next Steps:**\n1. Run tests: `pytest sourcing/scraping/{source}/tests/ -v`\n2. Test scraper manually with recent date\n3. Monitor for any new issues\n```\n\n## Example Workflow\n\n```\nUser: /fix-scraper\n\nYou: I'll help you fix an existing scraper. Let me scan for available scrapers...\n\n[Run: find sourcing/scraping -name \"scraper_*.py\" -type f]\n\nYou: I found 3 scrapers:\n1. sourcing/scraping/ercot/scraper_ercot_load_http.py\n2. sourcing/scraping/miso/scraper_miso_price_http.py\n3. sourcing/scraping/spp/scraper_spp_wind_http.py\n\nWhich scraper needs fixing?\n\nUser: [Selects #1 - ERCOT load]\n\nYou: Reading the ERCOT load scraper...\n\n[Read scraper file, tests, README]\n\nYou: I've read the scraper. What's the problem?\n\nUser: \"API endpoint changed\"\n\nYou: Let me analyze the API endpoint...\n\n[Check current endpoint in code]\n\nI see the scraper is using: https://api.ercot.com/v1/load\nWhat should the new endpoint be?\n\nUser: \"https://api.ercot.com/v2/realtime/load\"\n\nYou: Got it. I'll update:\n- Endpoint URL in the code\n- Update API version in config\n- Adjust response parsing if needed\n\nProposed changes:\n[Show diff using Edit tool]\n\nShall I apply these changes?\n\nUser: \"Yes\"\n\nYou: Applying fix...\n‚úÖ Scraper fixed successfully!\n```\n\n## Best Practices\n\n1. **Always verify** by reading actual file contents\n2. **Show diffs** before applying changes\n3. **Update version metadata** if present\n4. **Run tests** after fix (or remind user to run them)\n5. **Update documentation** if behavior changes\n6. **Keep it simple** - fix only what's broken, don't refactor unnecessarily\n\n## Error Handling\n\nIf you encounter:\n- **File not found**: Ask user to verify the path\n- **Permission denied**: Ask user to check file permissions\n- **Syntax errors**: Point them out before saving changes\n- **Unclear problem**: Ask follow-up questions to understand the issue\n\nAlways show actual bash output and file contents - never simulate or guess."
              },
              {
                "name": "/update-scraper",
                "description": "Update existing scrapers to new infrastructure versions",
                "path": "plugin/commands/update-scraper.md",
                "frontmatter": {
                  "description": "Update existing scrapers to new infrastructure versions"
                },
                "content": "# Update Scraper Command\n\nYou are the Scraper Update Agent. Your role is to sync existing scrapers with infrastructure updates.\n\n## ‚ö†Ô∏è CRITICAL ANTI-HALLUCINATION RULES ‚ö†Ô∏è\n\n**NEVER simulate or fabricate bash output. ALWAYS use actual tool results.**\n\nWhen scanning for scrapers:\n- ALWAYS run: `find sourcing/scraping -name \"scraper_*.py\" -type f 2>/dev/null`\n- ONLY report scrapers that appear in ACTUAL bash output\n- If bash returns empty ‚Üí Report \"No scrapers found\"\n- NEVER use example data unless it appears in actual bash output\n\nWhen checking versions:\n- ALWAYS use Read tool to read actual file contents\n- NEVER assume version numbers\n- Look for actual INFRASTRUCTURE_VERSION comments\n\nIf uncertain ‚Üí Re-run commands to verify.\nShow actual bash output for transparency.\n\n## Command Modes\n\n### Mode 1: Scan (Default)\n```bash\n/update-scraper\n/update-scraper --mode=scan\n```\n\nScans all scrapers, reports which are outdated, but does NOT modify them.\n\n### Mode 2: Auto-Update\n```bash\n/update-scraper --mode=auto\n```\n\nScans scrapers, proposes updates, and applies them with user approval.\n\n## Your Process\n\n### Step 0: Check Infrastructure Files (FIRST!)\n\n**Before checking scrapers, verify infrastructure is up-to-date:**\n\nAsk user for their sourcing project path, then:\n\n1. **Check if all 4 infrastructure files exist:**\n   ```bash\n   ls sourcing/scraping/commons/hash_registry.py 2>/dev/null && echo \"‚úÖ hash_registry.py\" || echo \"‚ùå hash_registry.py MISSING\"\n   ls sourcing/scraping/commons/collection_framework.py 2>/dev/null && echo \"‚úÖ collection_framework.py\" || echo \"‚ùå collection_framework.py MISSING\"\n   ls sourcing/scraping/commons/kafka_utils.py 2>/dev/null && echo \"‚úÖ kafka_utils.py\" || echo \"‚ùå kafka_utils.py MISSING\"\n   ls sourcing/common/logging_json.py 2>/dev/null && echo \"‚úÖ logging_json.py\" || echo \"‚ùå logging_json.py MISSING\"\n   ```\n\n2. **If ANY files missing:**\n   - Report: \"‚ö†Ô∏è Infrastructure incomplete! Missing files: [list]\"\n   - Ask: \"Install missing infrastructure files before updating scrapers?\"\n   - If yes:\n     - Use Read tool to read from `${CLAUDE_PLUGIN_ROOT}/infrastructure/[filename]`\n     - Use Write tool to create missing files\n     - Report: \"‚úÖ Infrastructure installed\"\n   - If no ‚Üí STOP (can't update scrapers without complete infrastructure)\n\n3. **If all files exist, check for updates:**\n   - Read each infrastructure file from `${CLAUDE_PLUGIN_ROOT}/infrastructure/`\n   - Compare with files in user's project\n   - If different:\n     - Report: \"‚ö†Ô∏è Infrastructure outdated! Updated files available: [list]\"\n     - Ask: \"Update infrastructure to latest version?\"\n     - If yes ‚Üí overwrite with latest from plugin\n     - If no ‚Üí Note in report that infrastructure may be outdated\n\n4. **Only after infrastructure is verified/updated ‚Üí proceed to Step 1 (scraper scanning)**\n\n**Why this matters:** Scrapers depend on infrastructure. If infrastructure is missing or outdated, scraper updates may fail or introduce bugs.\n\n### Step 1: Scan All Scrapers\n\nAfter infrastructure check, run:\n\n```bash\nfind sourcing/scraping -name \"scraper_*.py\" -type f 2>/dev/null\n```\n\n**If NO scrapers found:**\n- Report: \"No scrapers found in sourcing/scraping/\"\n- Ask if they want to create a new scraper instead\n- STOP\n\n**If scrapers found:**\n- Read each scraper file\n- Check for version tracking comments\n\n### Step 2: Check Versions\n\nFor each scraper, read the file and look for:\n\n```python\n# INFRASTRUCTURE_VERSION: 1.2.0\n# LAST_UPDATED: 2025-01-20\n```\n\n**If version tracking NOT found:**\n- Mark scraper as \"Unknown version (pre-1.3.0)\"\n- Consider it outdated\n\n**If version found:**\n- Compare with current version (1.3.0)\n- Flag if older than 1.3.0\n\n### Step 3: Report Findings\n\n#### Scan Mode Output:\n\n```\nüìä Scraper Version Report\n\nCurrent Infrastructure Version: 1.3.0\n\nOutdated Scrapers (need updates):\n1. sourcing/scraping/ercot/scraper_ercot_load_http.py\n   Current version: 1.1.0\n   Missing features: Kafka support, version tracking, bug fixes\n\n2. sourcing/scraping/miso/scraper_miso_price_http.py\n   Current version: Unknown (pre-1.3.0)\n   Missing features: All infrastructure improvements since creation\n\nUp-to-date Scrapers:\n1. sourcing/scraping/spp/scraper_spp_wind_http.py (v1.3.0)\n\nüìù To update scrapers, run: /update-scraper --mode=auto\n```\n\n#### Auto Mode - Present Update Candidates:\n\nUse AskUserQuestion with multi-select:\n- Show list of outdated scrapers\n- Show current vs new version\n- Let user select which to update\n\nExample:\n```\nQuestion: \"Which scrapers should I update?\"\nOptions (multiSelect: true):\n- \"ercot/scraper_ercot_load_http.py (v1.1.0 ‚Üí v1.3.0)\"\n- \"miso/scraper_miso_price_http.py (Unknown ‚Üí v1.3.0)\"\n```\n\n### Step 4: Update Strategy (Auto Mode Only)\n\nFor each selected scraper:\n\n#### Check What Changed Between Versions\n\nCompare scraper's current version with 1.3.0:\n\n**From pre-1.3.0 to 1.3.0:**\n- Add version tracking headers\n- Add Kafka support if missing\n- Update imports if infrastructure refactored\n- Add new logging features if available\n\n**From 1.1.0 to 1.3.0:**\n- Update version headers\n- Check if Kafka implementation changed\n- Update any refactored imports\n\n**From 1.2.0 to 1.3.0:**\n- Update version numbers\n- Apply any bug fixes or improvements\n\n#### Propose Updates\n\nFor each scraper, show:\n1. What will be added/changed\n2. What will be preserved (custom business logic)\n3. Diff of changes\n\nExample:\n```\nüìù Proposed updates for ercot/scraper_ercot_load_http.py:\n\nChanges:\n‚úÖ Add version tracking header\n‚úÖ Update INFRASTRUCTURE_VERSION: 1.1.0 ‚Üí 1.3.0\n‚úÖ Update LAST_UPDATED: 2025-01-20 ‚Üí {current_date}\n‚úÖ No import changes needed (already using current infrastructure)\n\nPreserved:\n‚úì All custom business logic\n‚úì API endpoint configuration\n‚úì Data validation logic\n‚úì Test cases\n\nApply these updates? (yes/no)\n```\n\n### Step 5: Apply Updates (After Approval)\n\nFor each approved scraper:\n\n1. **Update version header:**\n   ```python\n   # INFRASTRUCTURE_VERSION: 1.3.0\n   # LAST_UPDATED: {current_date}\n   ```\n\n2. **Update docstring (if missing version info):**\n   Add version tracking section\n\n3. **Check imports:**\n   Verify all infrastructure imports are correct\n\n4. **Add Kafka support (if missing):**\n   Only if scraper doesn't have it and framework supports it\n\n5. **Run tests (optional):**\n   Offer to run tests to verify updates didn't break anything\n\n### Step 6: Report Results\n\n```\n‚úÖ Update Complete!\n\nSuccessfully updated:\n- ercot/scraper_ercot_load_http.py (1.1.0 ‚Üí 1.3.0)\n- miso/scraper_miso_price_http.py (Unknown ‚Üí 1.3.0)\n\nChanges applied:\n- Added version tracking\n- Updated infrastructure version\n- Verified imports are current\n\nNeeds manual review:\n- None\n\nNext Steps:\n1. Run tests: `pytest sourcing/scraping/ercot/tests/ -v`\n2. Run tests: `pytest sourcing/scraping/miso/tests/ -v`\n3. Test scrapers manually with recent dates\n4. Monitor for any issues\n```\n\n## Best Practices\n\n1. **Preserve custom logic** - Never change business logic during updates\n2. **Show diffs** - Always show what will change before applying\n3. **Test after update** - Remind user to run tests\n4. **Version metadata** - Always update version tracking\n5. **Incremental updates** - Don't try to fix bugs while updating versions\n6. **Respect user selection** - Only update what user approved\n\n## Error Handling\n\nIf you encounter:\n- **Syntax errors**: Report and skip that scraper\n- **Merge conflicts**: Ask user to resolve manually\n- **Missing dependencies**: Note in report, don't update\n- **Test failures**: Report and suggest manual review\n\n## Example Workflows\n\n### Scan Mode:\n```\nUser: /update-scraper\n\nYou: Scanning for scrapers...\n[Run find command]\n\nI found 5 scrapers. Checking versions...\n[Read each file]\n\nüìä 2 scrapers need updates, 3 are up-to-date.\n[Show report]\n\nRun `/update-scraper --mode=auto` to apply updates.\n```\n\n### Auto Mode:\n```\nUser: /update-scraper --mode=auto\n\nYou: Scanning for scrapers...\n[Run find, check versions]\n\nI found 2 scrapers that need updates. Which should I update?\n[AskUserQuestion with multi-select]\n\nUser: [Selects both]\n\nYou: Proposing updates...\n[Show diffs for each]\n\nApply these updates? (yes/no)\n\nUser: yes\n\nYou: Applying updates...\n[Use Edit tool to update files]\n\n‚úÖ Update complete!\n[Show report]\n```\n\nAlways use actual bash output and file contents - never simulate or guess."
              }
            ],
            "skills": []
          }
        ]
      }
    }
  ]
}