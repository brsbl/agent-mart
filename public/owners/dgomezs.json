{
  "owner": {
    "id": "dgomezs",
    "display_name": "dgomezs",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/6229674?u=86891dc8c1f8f01490a8c7d0c6845a255d6e5d2a&v=4",
    "url": "https://github.com/dgomezs",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 2,
      "total_commands": 5,
      "total_skills": 0,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "dgomezs/claude-code",
      "url": "https://github.com/dgomezs/claude-code",
      "description": "Marketplace for my Claude Code plugins",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-10-26T06:00:47Z",
        "created_at": "2025-10-25T04:16:09Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1790
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/skill-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": ".claude/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 11547
        },
        {
          "path": ".claude/skills/skill-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/skill-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10863
        },
        {
          "path": ".claude/skills/skill-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3247
        },
        {
          "path": ".claude/skills/skill-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 2165
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 296
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4904
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "MARKETPLACE.md",
          "type": "blob",
          "size": 6058
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 14604
        },
        {
          "path": "plugin.json",
          "type": "blob",
          "size": 3791
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/README.md",
          "type": "blob",
          "size": 3712
        },
        {
          "path": "plugins/dgomezs-toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dgomezs-toolkit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dgomezs-toolkit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 521
        },
        {
          "path": "plugins/dgomezs-toolkit/README.md",
          "type": "blob",
          "size": 1061
        },
        {
          "path": "plugins/tdd-specflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tdd-specflow/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tdd-specflow/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1225
        },
        {
          "path": "plugins/tdd-specflow/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tdd-specflow/agents/codebase-analyzer.md",
          "type": "blob",
          "size": 8907
        },
        {
          "path": "plugins/tdd-specflow/agents/codebase-locator.md",
          "type": "blob",
          "size": 5506
        },
        {
          "path": "plugins/tdd-specflow/agents/codebase-pattern-finder.md",
          "type": "blob",
          "size": 8188
        },
        {
          "path": "plugins/tdd-specflow/agents/qa-engineer.md",
          "type": "blob",
          "size": 16058
        },
        {
          "path": "plugins/tdd-specflow/agents/requirements-analyzer.md",
          "type": "blob",
          "size": 15080
        },
        {
          "path": "plugins/tdd-specflow/agents/software-architect.md",
          "type": "blob",
          "size": 9829
        },
        {
          "path": "plugins/tdd-specflow/agents/tdd-green.md",
          "type": "blob",
          "size": 8055
        },
        {
          "path": "plugins/tdd-specflow/agents/tdd-red.md",
          "type": "blob",
          "size": 5419
        },
        {
          "path": "plugins/tdd-specflow/agents/tdd-refactor.md",
          "type": "blob",
          "size": 6648
        },
        {
          "path": "plugins/tdd-specflow/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tdd-specflow/commands/create-research.md",
          "type": "blob",
          "size": 13672
        },
        {
          "path": "plugins/tdd-specflow/commands/create-spec.md",
          "type": "blob",
          "size": 1781
        },
        {
          "path": "plugins/tdd-specflow/commands/create-tech-design.md",
          "type": "blob",
          "size": 2010
        },
        {
          "path": "plugins/tdd-specflow/commands/start-tdd.md",
          "type": "blob",
          "size": 2649
        },
        {
          "path": "plugins/tdd-specflow/commands/test-scenarios.md",
          "type": "blob",
          "size": 15842
        }
      ],
      "marketplace": {
        "name": "tdd-specflow-marketplace",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "dgomezs",
          "url": "https://github.com/dgomezs/claude-code"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "tdd-specflow",
            "description": "Complete TDD workflow automation with BDD-style test scenarios, specialized agents for each TDD phase, and intelligent orchestration",
            "source": "./plugins/tdd-specflow",
            "category": "development-workflow",
            "version": "0.0.3",
            "author": {
              "name": "dgomezs",
              "url": "https://github.com/dgomezs"
            },
            "install_commands": [
              "/plugin marketplace add dgomezs/claude-code",
              "/plugin install tdd-specflow@tdd-specflow-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-10-26T06:00:47Z",
              "created_at": "2025-10-25T04:16:09Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/create-research",
                "description": "Research codebase to understand existing implementation",
                "path": "plugins/tdd-specflow/commands/create-research.md",
                "frontmatter": {
                  "allowed-tools": "Task(*), Read(*), Glob(*), Grep(*), TodoWrite(*)",
                  "argument-hint": "<question or topic>",
                  "description": "Research codebase to understand existing implementation"
                },
                "content": "# Research Codebase\n\nYou are tasked with conducting comprehensive research across the codebase to understand existing implementation and create documentation that can feed into the TDD workflow.\n\n## CRITICAL: YOUR ONLY JOB IS TO DOCUMENT AND EXPLAIN THE CODEBASE AS IT EXISTS TODAY\n- DO NOT suggest improvements or changes\n- DO NOT critique the implementation or identify problems\n- DO NOT recommend refactoring, optimization, or architectural changes\n- ONLY describe what exists, where it exists, how it works, and how components interact\n- You are creating technical documentation of the existing system\n\n## Workflow\n\n### Step 1: Understand the Research Question\n\nWhen invoked with: `/create-research \"How does user authentication work?\"`\n\nParse the question and identify:\n- What components/features to investigate\n- What architectural layers/modules are involved\n- What patterns to look for\n\n### Step 2: Create Research Plan\n\nUse TodoWrite to track your research tasks:\n- Find relevant files and components\n- Analyze implementation details\n- Discover patterns and conventions\n- Document findings\n\n### Step 3: Launch Parallel Research Agents with Rich Context\n\nSpawn multiple specialized agents concurrently using Task tool.\n\n**IMPORTANT**: Each agent must receive:\n1. The original research question\n2. The task/ticket context (if available)\n3. Specific deliverable requirements\n4. Cross-referencing instructions\n\n**Agent 1 - codebase-locator:**\n\nProvide detailed instructions:\n```\nCONTEXT:\n- Research question: [original question]\n- Ticket/requirement: [if available, include ticket path or summary]\n- Related components: [any components mentioned in question]\n\nTASK:\nFind all files related to:\n1. [Main component/feature]\n2. Dependencies and imports/requires\n3. Core business logic and data structures\n4. External integrations and data access\n5. Test files for all above\n\nDELIVERABLES:\n1. Complete file paths with descriptions\n2. File roles (entry point/implementation/utility/test)\n3. Import/dependency graph (which files depend on which)\n4. Files grouped by modules/layers (adapt to discovered structure)\n5. Test coverage mapping (which test tests which implementation)\n\nOUTPUT FORMAT:\n## Component Locations\n\n### [Module/Layer Name] - [Component Type]\n- `path/to/file.ext` - [description] [ROLE: entry point/implementation/utility/test]\n  - Dependencies: [list imports/requires]\n  - Tests: [test file name]\n\n### Dependencies\n- `file-a.ext` imports/requires from `file-b.ext` at line X\n- [map all import/dependency relationships]\n\n### Test Coverage\n- `test_file.ext` or `file.test.ext` - tests for [component]\n```\n\n**Agent 2 - codebase-analyzer:**\n\nProvide detailed instructions:\n```\nCONTEXT:\n- Research question: [original question]\n- Ticket/requirement: [if available]\n- Core files to analyze: [specify key files or reference Agent 1 results if sequential]\n\nTASK:\nAnalyze implementation details:\n1. Complete execution flow from entry to exit\n2. Data transformations at each step\n3. Validation and error handling points\n4. Dependencies on other components\n5. Edge cases and special handling\n\nDELIVERABLES:\n1. Function/method signatures with file:line references\n2. Step-by-step execution flow\n3. Data transformation pipeline\n4. Error handling and validation logic\n5. Dependencies referenced by file path\n\nOUTPUT FORMAT:\n## Implementation Analysis\n\n### Execution Flow\n1. Entry point: `file.ext:line` - [what happens, parameters received]\n2. Validation: `file.ext:line` - [what's validated, how, error cases]\n3. Processing: `file.ext:line` - [transformations, business logic]\n4. Exit/return: `file.ext:line` - [what's returned, side effects]\n\n### Key Functions/Methods\n- `functionName()` at `file.ext:line`\n  - Parameters: [types and descriptions]\n  - Returns: [type and description]\n  - Dependencies: [list files/components used with lines]\n  - Error cases: [what throws/returns, when, with examples]\n  - Edge cases: [special handling]\n\n### Data Transformations\n- Input: [type/structure] → Output: [type/structure]\n- Transformation at `file.ext:line`: [description]\n```\n\n**Agent 3 - codebase-pattern-finder:**\n\nProvide detailed instructions:\n```\nCONTEXT:\n- Research question: [original question]\n- Components analyzed: [reference from Agent 1/2]\n- Ticket/requirement: [if available]\n\nTASK:\nFind examples of patterns in the codebase:\n1. Naming conventions (files, classes, methods)\n2. Repeated architectural patterns\n3. Common validation approaches\n4. Error handling strategies\n5. Layer separation rules\n\nDELIVERABLES:\n1. Pattern name and description\n2. At least 2-3 concrete examples with file:line references\n3. Consistency analysis (always/mostly/sometimes used)\n4. Code snippets showing pattern in action\n5. Exceptions or variations noted\n\nOUTPUT FORMAT:\n## Patterns Discovered\n\n### Pattern: [Name]\n- **Description**: [how it works]\n- **Found in**: `file1.ext:line`, `file2.ext:line`, `file3.ext:line`\n- **Consistency**: [always/mostly/sometimes used]\n- **Example**:\n  ```\n  [code snippet showing pattern]\n  ```\n- **Variations**: [note any deviations from standard pattern]\n\n### Pattern: [Name]\n[repeat for each pattern found]\n```\n\n**EXECUTION**: Run these agents in PARALLEL (single message with multiple Task tool calls).\n\n### Step 4: Synthesize Agent Findings (CRITICAL QUALITY STEP)\n\nAfter ALL agents complete, perform comprehensive synthesis:\n\n**1. Cross-Reference Findings**\n- Map Agent 1's file locations to Agent 2's implementation details\n- Link Agent 3's patterns to specific files from Agent 1\n- Identify gaps: Did Agent 2 analyze all files from Agent 1?\n- Verify: Do patterns from Agent 3 appear in files from Agent 1?\n\n**2. Validate Completeness**\n- Does Agent 2's execution flow reference all core components from Agent 1?\n- Do Agent 3's patterns explain Agent 2's implementation choices?\n- Are there files in Agent 1 not covered by Agent 2?\n- Are test files from Agent 1 explained (what they test)?\n\n**3. Resolve Conflicts**\n- If agents report different information, investigate yourself\n- Read files directly to confirm ambiguous findings\n- Use Grep to verify pattern claims from Agent 3\n- Check import statements to validate Agent 1's dependency graph\n\n**4. Fill Gaps**\n- If Agent 2 missed error handling, search for it yourself using Grep\n- If Agent 3 found only 1 example of a pattern, search for more\n- If Agent 1 missed test files, glob for common test patterns (`*test*`, `*spec*`, `test_*`)\n- If execution flow is incomplete, read the source files directly\n\n**5. Enrich with Context**\n- Read the original ticket/requirement again\n- Check if research fully answers the original question\n- Add any missing details from your own investigation\n- Note any architectural insights discovered during synthesis\n\n**6. Document Synthesis Process**\n- Track which findings came from agents vs your investigation\n- Note any conflicts resolved\n- Record gaps that were filled\n- Document manual verification performed\n\n### Step 5: Quality Verification Checklist\n\nBefore writing research.md, verify:\n\n- [ ] All files from Agent 1 are explained in Agent 2's analysis\n- [ ] Agent 2's execution flow has file:line references for every step\n- [ ] Agent 3's patterns have at least 2-3 concrete examples each\n- [ ] Error handling is documented (not just happy path)\n- [ ] Test files are identified and their purpose explained\n- [ ] Architectural layers are clearly separated in documentation\n- [ ] Cross-references between agents are resolved\n- [ ] Original research question is fully answered\n- [ ] Import dependencies are verified (not just assumed)\n- [ ] Edge cases and special handling are documented\n\n**If any item is unchecked**, investigate yourself using:\n- Read tool for specific files\n- Grep for patterns across codebase\n- Glob for missing files\n- Manual code tracing\n\n### Step 6: Generate Research Document\n\nCreate `research.md` in the task directory (or root if no task context) with explicit synthesis markers:\n\n```markdown\n# Research: [Question/Topic]\n\n**Date**: [Current date]\n**Question**: [Original research question]\n\n## Summary\n[Synthesized high-level overview from all agents + your analysis]\n\n## Detailed Findings\n\n### Component Structure\n[From Agent 1 + your verification]\n\n**Files Discovered** (Agent 1):\n- `path/to/file.ext` - [description] [ROLE]\n  - Dependencies: [imports/requires]\n  - Tests: [test files]\n\n**Verified Dependencies** (Your investigation):\n- Cross-referenced import/require statements at `file.ext:line`\n- Confirmed dependency chain: A → B → C\n- [any corrections or additions]\n\n### Implementation Details\n[From Agent 2 + your enrichment]\n\n**Core Flow** (Agent 2):\n1. Entry: `file.ext:45` - [description with parameters]\n2. Validation: `file.ext:67` - [what's validated, how]\n3. Processing: `file.ext:89` - [transformations]\n4. Exit: `file.ext:102` - [what's returned]\n\n**Additional Details** (Your research):\n- Error case at `file.ext:110` - throws/returns error when [condition]\n- Edge case handling at `file.ext:125` - [special behavior]\n- Missing from agent analysis: [any gaps you filled]\n\n### Patterns Discovered\n[From Agent 3 + your validation]\n\n**Pattern: [Name]** (Agent 3):\n- **Description**: [how it works]\n- **Found in**: `file1.ext:line`, `file2.ext:line`\n- **Example**:\n  ```\n  [code snippet]\n  ```\n\n**Pattern Validation** (Your verification):\n- Confirmed with grep: [X] occurrences across [module/directory]\n- Additional examples found: `file3.ext:line`, `file4.ext:line`\n- Exceptions noted: `legacy-file.ext` uses different pattern because [reason]\n\n### Cross-Agent Synthesis\n[Your analysis connecting all findings]\n\nAgent 1 identified [X] core files. Agent 2 analyzed [Y] of them. The remaining files are:\n- `helper.ext` - Utility used by all components at: `file-a.ext:12`, `file-b.ext:34`\n- Purpose: [description from your reading]\n\nAgent 3 found pattern X in [locations]. This explains Agent 2's implementation choice at `file.ext:45` because [architectural reasoning].\n\nThe execution flow from Agent 2 aligns with the file structure from Agent 1, confirming [architectural principle].\n\n### Test Coverage\n- Unit tests: `test_file.ext` or `file.test.ext` - tests [what] from `file.ext`\n- Integration tests: `integration_test.ext` - tests [what flow]\n- Coverage gaps: [any areas without tests]\n\n## Architecture Insights\n[Synthesized patterns with evidence]\n\n- **Pattern observed**: [description]\n  - Evidence: [file references from agents + your verification]\n- **Module separation**: [how it's maintained]\n  - [Module type] never imports from [other module type] (verified in [X] files)\n- **Error handling strategy**: [approach used]\n  - Examples: [file:line references]\n\n## File References\n[Complete, verified list grouped by module/layer/directory]\n\n### [Module/Layer Name 1]\n- `path/to/file.ext` - [description]\n\n### [Module/Layer Name 2]\n- `path/to/file.ext` - [description]\n\n### [Module/Layer Name 3]\n- `path/to/file.ext` - [description]\n\n(Adapt structure to discovered codebase organization)\n\n## Research Quality Notes\n[Transparency about synthesis process]\n\n**Agent Coverage**:\n- Agent 1 (codebase-locator): Found [X] files across [Y] modules/layers\n- Agent 2 (codebase-analyzer): Analyzed [X] core files, [Y] execution flows\n- Agent 3 (codebase-pattern-finder): Found [X] patterns with [Y] total examples\n\n**Manual Enrichment**:\n- Added error handling details not covered by Agent 2\n- Verified Agent 3's pattern claims with [X] additional examples via Grep\n- Cross-referenced ticket requirements - all [X] requirements addressed\n- Filled [X] gaps identified during synthesis\n\n**Conflicts Resolved**:\n- Agent [X] reported [Y], but file reading confirmed [Z]\n- [any other discrepancies and resolutions]\n\n## Next Steps\nThis research can be used to:\n1. Inform requirements and specifications for modifications\n2. Understand impact of proposed changes\n3. Identify test scenarios that need coverage\n4. Guide refactoring and architectural decisions\n```\n\n### Step 7: Report Completion\n\nAfter creating research.md, provide this summary:\n\n```\n✅ Research Complete: [Topic]\n\nResearch document created at: ./research.md\n\nKey findings:\n- [Major finding 1 with file reference]\n- [Major finding 2 with file reference]\n- [Major finding 3 with file reference]\n\nResearch quality:\n- Agent 1 found [X] files across [Y] modules/layers\n- Agent 2 analyzed [X] execution flows\n- Agent 3 identified [X] patterns\n- Manual enrichment: [X] gaps filled, [Y] verifications performed\n\nThis research can now be used to:\n- Inform specification creation (if using TDD workflow)\n- Guide architectural decisions\n- Understand impact of proposed changes\n```\n\n## Integration with Development Workflow\n\nThis research can be used in various workflows:\n```\n/create-research → research.md → [use as input for specifications, design docs, or implementation]\n```\n\n## Important Notes\n\n**Research Quality Strategy**:\n- All agents work in parallel for efficiency (speed benefit)\n- Rich context provided to each agent (quality benefit)\n- Mandatory synthesis phase (quality assurance)\n- Quality checklist before documentation (completeness verification)\n- Transparency about agent vs manual findings (traceability)\n\n**Research Purpose**:\n- Research is purely documentary - no suggestions or improvements\n- Focus on understanding what exists, not what should exist\n- Output is structured to inform specifications, designs, and implementations\n- Document actual implementation, not ideal implementation\n- All findings must include specific file:line references"
              },
              {
                "name": "/create-spec",
                "description": "Create requirements.md with testable acceptance criteria from a ticket or prompt",
                "path": "plugins/tdd-specflow/commands/create-spec.md",
                "frontmatter": {
                  "allowed-tools": "Task(*), Read(*), Glob(*)",
                  "argument-hint": "<ticket-path | prompt> [research.md]",
                  "description": "Create requirements.md with testable acceptance criteria from a ticket or prompt"
                },
                "content": "Execute requirements analysis for: $ARGUMENTS\n\n**Step 0: Parse Arguments and Determine Context**\n1. Parse $ARGUMENTS to extract:\n   - Primary argument: Either a ticket file path (e.g., ticket.md) OR a direct text prompt\n   - Optional second argument: research.md path\n2. Determine if first argument is a file path or prompt:\n   - If it ends with .md or contains path separators, treat as file path\n   - Otherwise, treat as direct prompt text\n3. If file path: Get the directory path where the ticket file is located\n4. If prompt: Use current working directory as output location\n5. This directory will be used as the output location for requirements.md\n6. Check if research.md was provided as second argument\n\n**Step 1: Requirements Analysis**\nLaunch the requirements-analyzer agent:\n\nSource: [ticket path from $ARGUMENTS OR direct prompt text]\nResearch file: [research.md path if provided as second argument]\nOutput directory: [Same directory as ticket file OR current working directory]\nOutput filename: requirements.md (not spec.md)\nTask: Analyze requirements and create requirements.md with testable acceptance criteria. If research.md is provided, use it to inform requirements based on existing implementation patterns. If source is a direct prompt, use it as the requirements input.\n\n**Expected Output:**\n✅ requirements.md with acceptance criteria in the target directory\n\n**Next Steps:**\nAfter requirements.md is created, you can:\n1. Run `/test-scenarios <directory>` to generate detailed test scenarios\n2. Or continue with the full workflow using `/create-design <directory>`"
              },
              {
                "name": "/create-tech-design",
                "description": "Create technical design (tech-design.md) from requirements.md",
                "path": "plugins/tdd-specflow/commands/create-tech-design.md",
                "frontmatter": {
                  "allowed-tools": "Task(*), Read(*), Glob(*)",
                  "argument-hint": "<directory>",
                  "description": "Create technical design (tech-design.md) from requirements.md"
                },
                "content": "Execute technical design creation for the directory at: $ARGUMENTS\n\n**Prerequisites:**\n- requirements.md must exist in the directory (run `/create-spec` first if needed)\n\n**Step 0: Parse Arguments and Validate**\n1. Extract directory path from $ARGUMENTS\n2. Verify that requirements.md exists in this directory (MANDATORY)\n3. Check if research.md exists in the same directory (OPTIONAL - for context on existing patterns)\n4. Check if scenarios.md exists in the same directory (OPTIONAL - for detailed test context)\n\n**Step 1: Launch software-architect Agent**\n\nLaunch software-architect agent to create technical design:\n\n- Read requirements.md from: [directory]/requirements.md (MANDATORY - contains acceptance criteria)\n- Read research.md from: [directory]/research.md (OPTIONAL - if exists, use existing patterns and components)\n- Read scenarios.md from: [directory]/scenarios.md (OPTIONAL - if exists, provides detailed test scenarios for design context)\n- Output directory: [directory]\n- Output filename: tech-design.md (not design.md)\n- Task: Create tech-design.md with technical design covering ALL acceptance criteria in requirements.md. If research.md is provided, use existing patterns and components from the research. If scenarios.md is provided, use test scenarios to inform design decisions.\n\n**Step 2: If Design Options Presented**\n\nIf the software-architect agent pauses to present multiple design alternatives:\n1. Review the design options with the user\n2. Present pros/cons and recommendation clearly\n3. Wait for user to select an option\n4. Once user selects, relaunch software-architect agent to complete tech-design.md with chosen option\n\n**Expected Output:**\n✅ tech-design.md with technical design in the directory\n\n**Next Steps:**\nAfter tech-design.md is created, you can begin implementation using your preferred development approach."
              },
              {
                "name": "/start-tdd",
                "description": "Continue TDD implementation using orchestrator agent",
                "path": "plugins/tdd-specflow/commands/start-tdd.md",
                "frontmatter": {
                  "allowed-tools": "Task(*)",
                  "argument-hint": "<task-directory>",
                  "description": "Continue TDD implementation using orchestrator agent"
                },
                "content": "Execute TDD orchestration for: $ARGUMENTS\n\n**STEP 0: Validate Prerequisites**\n\nBefore starting TDD, verify task directory contains required files:\n\n1. **Parse task directory** from $ARGUMENTS\n2. **Check required files exist** using Read tool:\n   - `scenarios.md` - REQUIRED (contains scenarios with TDD tracking checkboxes)\n   - `test-scenarios/` directory - REQUIRED (contains happy-path.md, error-cases.md, edge-cases.md)\n   - `tech-design.md` - REQUIRED (provides architectural guidance and implementation strategy)\n\n3. **If any required file missing:**\n```\n❌ Prerequisites validation failed\n\nMissing required files:\n- [list missing files]\n\nRun the workflow commands to generate required files:\n/test-scenarios [task-directory]\n/create-tech-design [task-directory]\n```\nSTOP execution - do not proceed.\n\n4. **If all required files exist:**\n```\n✅ Prerequisites validated\n- scenarios.md ✓\n- test-scenarios/ ✓\n- tech-design.md ✓\n```\nProceed to STEP 1.\n\n**STEP 1: Read scenarios.md** in the task directory and find the first scenario that needs work.\n\n**STEP 2: Detect phase** by checking the scenario's checkboxes:\n- `[ ] [ ] [ ]` = RED phase → Launch tdd-red agent\n- `[x] [ ] [ ]` = GREEN phase → Launch tdd-green agent\n- `[x] [x] [ ]` = REFACTOR phase → Launch tdd-refactor agent\n- `[x] [x] [x]` = Complete → Move to next scenario\n\nIf all scenarios are complete, report task completion.\n\n**STEP 3: Launch the appropriate agent** with Task tool:\n- RED: `subagent_type: \"tdd-red\"` - Agent will write failing test\n- GREEN: `subagent_type: \"tdd-green\"` - Agent will implement minimal code\n- REFACTOR: `subagent_type: \"tdd-refactor\"` - Agent will improve code quality\n\nPass the task directory path to the agent. Agents will use tech-design.md for architectural guidance and implementation strategy, scenarios.md for tracking progress, and test-scenarios/ for detailed scenario specifications.\n\n**STEP 4: After agent completes**, show:\n- What phase completed\n- Current progress (all scenarios with checkbox states)\n- Suggested commit message\n\n**STEP 5: Ask user** what to do next:\n1. Commit and continue to next phase/scenario\n2. Stop here to review\n3. Continue without committing\n4. Skip refactoring (only when in REFACTOR phase) - move directly to next scenario\n\n**Notes:**\n- Option 4 is only available when in REFACTOR phase (scenario shows `[x] [x] [ ]`)\n- Skipping refactoring is acceptable since scenarios are functionally complete after GREEN phase\n- Do NOT automatically continue to the next phase."
              },
              {
                "name": "/test-scenarios",
                "description": "Create or manage test scenarios from requirements.md using the qa-engineer agent. Supports creating, adding, modifying, or discovering scenarios.",
                "path": "plugins/tdd-specflow/commands/test-scenarios.md",
                "frontmatter": {
                  "allowed-tools": "Task(*), Read(*), Glob(*), Write(*), Edit(*), Bash(mkdir:*)",
                  "argument-hint": "<task-directory> [operation-prompt]",
                  "description": "Create or manage test scenarios from requirements.md using the qa-engineer agent. Supports creating, adding, modifying, or discovering scenarios."
                },
                "content": "# Test Scenarios Command\n\nExecute test scenario creation or management for: $ARGUMENTS\n\n## Overview\n\nThis command creates and manages test scenarios from a requirements.md file containing high-level acceptance criteria.\n\nIt orchestrates scenario generation by:\n1. Determining what operation to perform (create all, add one, modify one, discover gaps)\n2. Reading requirements.md and preparing context\n3. Calling qa-engineer agent to generate scenario content\n4. Handling all file operations (writing, numbering, organizing, linking)\n\nThe qa-engineer agent ONLY generates scenario content. This command handles everything else.\n\n## Step 1: Parse Arguments and Determine Operation\n\n### Operation Detection\n\nAnalyze `$ARGUMENTS` to determine the operation:\n\n**EXPAND Mode** - Create comprehensive scenarios from requirements.md:\n- Single argument: directory path containing `requirements.md`\n- Example: `/test-scenarios apps/feature/task-001/`\n\n**ADD Mode** - Add single scenario to existing set:\n- First argument: directory path\n- Second argument contains \"add\"\n- Example: `/test-scenarios apps/feature/task-001/ \"add scenario for null input to AC-1\"`\n\n**MODIFY Mode** - Edit existing scenario:\n- First argument: directory path\n- Second argument contains \"modify\" or \"edit\"\n- Example: `/test-scenarios apps/feature/task-001/ \"modify scenario 1.2 to test empty string\"`\n\n**DISCOVER Mode** - Find gaps in existing scenarios:\n- First argument: directory path\n- Second argument contains \"discover\" or \"gaps\"\n- Optional: Tag additional context files (e.g., @research.md, @tech-design.md) for deeper analysis\n- Example: `/test-scenarios apps/feature/task-001/ \"discover gaps\"`\n- Example with context: `/test-scenarios apps/feature/task-001/ \"discover gaps\" @research.md @tech-design.md`\n\n**DELETE Mode** - Remove a scenario:\n- First argument: directory path\n- Second argument contains \"delete\" or \"remove\"\n- Example: `/test-scenarios apps/feature/task-001/ \"delete scenario 1.3\"`\n\n## Step 2: Gather Context\n\nAll modes use `requirements.md` as the base input containing high-level acceptance criteria.\n\nRead relevant files from the task directory:\n\n### For EXPAND Mode:\n```\nRequired:\n- <directory>/requirements.md (high-level acceptance criteria)\n```\n\n### For ADD/MODIFY/DISCOVER/DELETE Modes:\n```\nRequired:\n- <directory>/requirements.md (high-level acceptance criteria - for context)\n- <directory>/scenarios.md (existing scenarios with implementation tracking)\n- <directory>/test-scenarios/*.md (existing scenario details)\n\nOptional (if tagged in user prompt):\n- Any additional context files (research.md, tech-design.md, code files, etc.)\n- These provide deeper context for scenario discovery and analysis\n```\n\n## Step 3: Prepare Agent Request\n\nBased on operation mode, formulate a simple request for qa-engineer:\n\n### EXPAND Mode Request:\n```\nGenerate comprehensive test scenarios for these acceptance criteria:\n\n[paste requirements.md content - high-level acceptance criteria]\n\nGenerate multiple scenarios per acceptance criterion, ordered by implementation priority.\n```\n\n### ADD Mode Request:\n```\nGenerate ONE scenario for AC-[N] to test: [specific behavior]\n\nAcceptance Criterion:\n[AC-[N] from requirements.md]\n\nExisting scenarios for AC-[N]:\n[list existing scenarios with names only]\n\nCheck for duplicates against existing scenarios.\n```\n\n### MODIFY Mode Request:\n```\nModify scenario [N.M]:\n\nCurrent scenario:\n[paste current scenario]\n\nCurrent implementation progress: [checkboxes state]\n\nRequested change: [what user specified]\n\nPreserve structure and warn if existing tests/implementation may need updates.\n```\n\n### DISCOVER Mode Request:\n```\nAnalyze existing scenarios for gaps and generate NEW scenarios to fill those gaps.\n\nAcceptance Criteria:\n[paste all ACs from requirements.md]\n\nExisting Scenarios:\n[paste organized list of scenarios by AC with their types]\n\n[IF additional context files were tagged, include them here:]\nAdditional Context:\n\n[For each tagged file, include a section:]\nFile: [filename]\n[paste file content]\n\n[Repeat for all tagged files]\n\nGenerate new scenarios to fill any gaps. If no gaps found, return: \"No gaps found - coverage is complete\"\n```\n\n## Step 4: Invoke qa-engineer Agent\n\nUse Task tool to launch qa-engineer with the prepared request:\n\n```\nTask: qa-engineer\nDescription: Generate test scenario content\n\n[Paste the request from Step 3]\n```\n\nThe agent is the QA domain expert. It will:\n- Ask clarification questions if needed (wait for answers)\n- Apply its QA heuristics automatically\n- Assign scenario types and priorities based on QA expertise\n- Generate scenario content in Given-When-Then format\n- Return structured JSON with scenarios, warnings, and optional context requests\n\n**Expected JSON Response:**\n```json\n{\n  \"scenarios\": [...],\n  \"warnings\": {\n    \"duplicates\": [...],\n    \"gaps\": [...],\n    \"implementation_impact\": [...]\n  },\n  \"context_requests\": [...]  // Optional, for DISCOVER mode\n}\n```\n\n**Handle Context Requests (DISCOVER mode only):**\nIf agent returns `context_requests` array:\n1. Read each requested file\n2. Append to the original request with section: \"Additional Context:\\n[file content]\"\n3. Re-invoke agent with updated request\n4. Repeat until agent returns scenarios or \"No gaps found\"\n\n## Step 5: Common File Operations\n\nAll modes (except DELETE) use these common operations after receiving agent JSON output.\n\n### 5.1: Parse Agent JSON Response\n\n```javascript\nconst response = JSON.parse(agent_output);\nconst scenarios = response.scenarios || [];\nconst warnings = response.warnings || {};\nconst contextRequests = response.context_requests || [];\n```\n\n### 5.2: Display Warnings\n\n```\nif (warnings.duplicates?.length > 0) {\n  Display: ⚠️  Duplicate warnings: [list warnings.duplicates]\n}\nif (warnings.gaps?.length > 0) {\n  Display: ℹ️  Identified gaps: [list warnings.gaps]\n}\nif (warnings.implementation_impact?.length > 0) {\n  Display: ⚠️  Implementation impact: [list warnings.implementation_impact]\n  Display: \"Review existing tests/code for needed updates\"\n}\n```\n\n### 5.3: Write Scenario Content to Type File\n\nFor any scenario that needs to be written or updated:\n\n```bash\n# Determine target file from scenario.type:\ntarget_file = test-scenarios/{scenario.type}.md\n# Where scenario.type is one of: \"happy-path\", \"error-case\", \"edge-case\"\n\n# Write scenario.content exactly as received from agent:\n[Paste scenario.content]\n\n# Agent's content includes:\n# - ## Scenario N.M: [Name] heading\n# - Proper blank lines (MD022/MD032 compliance)\n# - Trailing --- separator\n```\n\n### 5.4: Update scenarios.md Tracking\n\nFor any scenario that needs tracking entry:\n\n```markdown\n### Scenario N.M: [scenario.name]\n- **Type**: [scenario.type]\n- **Details**: [test-scenarios/{scenario.type}.md#scenario-nm](test-scenarios/{scenario.type}.md#scenario-nm)\n- **Implementation Progress**: [ ] Test Written [ ] Implementation [ ] Refactoring\n```\n\n**Important**: When modifying existing entry, PRESERVE checkbox states.\n\n### 5.5: Determine Next Scenario Number\n\nWhen adding new scenarios, find the next available number for an AC:\n\n```bash\n# Read scenarios.md\n# Find all scenarios for the target AC (e.g., \"AC-2\")\n# Find highest number (e.g., if 2.1, 2.2, 2.3 exist → next is 2.4)\n# Return next_number\n```\n\n## Step 6: Mode-Specific Workflows\n\nAfter receiving JSON output from qa-engineer, perform mode-specific operations:\n\n### For EXPAND Mode:\n\nCreates all scenario files from scratch.\n\n**Your tasks:**\n\n1. **Use Step 5.1** to parse JSON response\n\n2. **Use Step 5.2** to display warnings\n\n3. **Group scenarios by type**:\n   - Filter `scenarios` where `type === \"happy-path\"` → happy-path.md\n   - Filter `scenarios` where `type === \"error-case\"` → error-cases.md\n   - Filter `scenarios` where `type === \"edge-case\"` → edge-cases.md\n\n4. **Create directory**:\n   ```bash\n   mkdir -p <directory>/test-scenarios\n   ```\n\n5. **Assign scenario numbers**:\n   - Group scenarios by `acceptance_criterion` field (e.g., \"AC-1\")\n   - Sort within each AC by `priority` field (1, 2, 3...)\n   - Assign numbers: AC-1 → 1.1, 1.2, 1.3...; AC-2 → 2.1, 2.2...\n   - Agent determines priority (happy-path first, then error-case, then edge-case)\n\n6. **Write type files** with headers:\n\n   **happy-path.md**:\n   ```markdown\n   # Happy Path Scenarios\n\n   Valid inputs and successful outcomes that represent typical user workflows.\n\n   ---\n\n   [For each happy-path scenario, use Step 5.3 to write scenario.content]\n   ```\n\n   **error-cases.md**:\n   ```markdown\n   # Error Case Scenarios\n\n   Invalid inputs and failure conditions.\n\n   ---\n\n   [For each error-case scenario, use Step 5.3 to write scenario.content]\n   ```\n\n   **edge-cases.md**:\n   ```markdown\n   # Edge Case Scenarios\n\n   Boundary values, limits, and unusual but valid conditions.\n\n   ---\n\n   [For each edge-case scenario, use Step 5.3 to write scenario.content]\n   ```\n\n7. **Create scenarios.md** with tracking:\n   ```markdown\n   # Test Scenarios\n\n   [For each AC in requirements.md:]\n   ## AC-N: [Title from requirements.md]\n\n   **Source**: [requirements.md#ac-n](requirements.md#ac-n)\n\n   [For each scenario for this AC, use Step 5.4 to create tracking entry]\n   ```\n\n### For ADD Mode:\n\nAdds one new scenario to existing set.\n\n**Your tasks:**\n\n1. **Use Step 5.1** to parse JSON (get `scenarios[0]` as the single scenario)\n\n2. **Use Step 5.2** to display warnings\n\n3. **Check for duplicates** - if `warnings.duplicates` is not empty:\n   ```\n   Ask user: \"Proceed anyway? (yes/no)\"\n   If no, abort\n   ```\n\n4. **Use Step 5.5** to determine next scenario number for the AC\n\n5. **Use Step 5.3** to append scenario.content to appropriate type file\n\n6. **Use Step 5.4** to add tracking entry to scenarios.md under the AC section\n\n7. **Display confirmation**:\n   ```\n   ✅ Added Scenario N.M: [scenario.name]\n   - Type: [scenario.type]\n   - AC: [scenario.acceptance_criterion]\n   ```\n\n### For MODIFY Mode:\n\nUpdates an existing scenario.\n\n**Your tasks:**\n\n1. **Use Step 5.1** to parse JSON (get `scenarios[0]` as the updated scenario)\n\n2. **Use Step 5.2** to display warnings\n\n3. **Update type file**:\n   - Find scenario in `test-scenarios/{scenario.type}.md`\n   - Locate section starting with `## Scenario N.M:`\n   - Replace entire section (from heading to `---`) with **Step 5.3** content\n\n4. **Update scenarios.md** (if name or type changed):\n   - Find scenario entry `### Scenario N.M`\n   - Update name and type using **Step 5.4** format\n   - **PRESERVE existing checkbox states**\n\n5. **Display confirmation**:\n   ```\n   ✅ Modified Scenario N.M: [scenario.name]\n\n   If tests/implementation exist, review them for needed updates.\n   ```\n\n### For DISCOVER Mode:\n\nAnalyzes existing scenarios for gaps and adds missing scenarios.\n\n**Your tasks:**\n\n1. **Use Step 5.1** to parse JSON\n\n2. **Handle context requests** (if any):\n   ```\n   if (response.context_requests?.length > 0) {\n     For each requested file:\n       - Read the file\n       - Append to original request: \"Additional Context:\\n\\nFile: {filename}\\n{content}\\n\"\n     Re-invoke qa-engineer agent with updated request\n     Return to step 1 when agent responds\n   }\n   ```\n\n3. **If scenarios found** (`response.scenarios.length > 0`):\n\n   a. **Use Step 5.2** to display warnings (especially gaps identified)\n\n   b. **For each new scenario**:\n      - **Use Step 5.5** to determine next scenario number for the AC\n      - **Use Step 5.3** to append to appropriate type file\n      - **Use Step 5.4** to add tracking entry to scenarios.md\n\n   c. **Display summary**:\n      ```\n      ✅ Discovered and added {count} new scenarios:\n      - Scenario X.Y: [scenario.name] ({scenario.type})\n      [list all new scenarios]\n      ```\n\n4. **If no gaps found** (`response.message === \"No gaps found - coverage is complete\"`):\n   ```\n   Display:\n     ✅ No gaps found - scenario coverage is complete\n   ```\n\n### For DELETE Mode:\n\n**No agent needed** - this is a file operation only\n\n**Your tasks:**\n\n1. **Parse scenario number** from user request (e.g., \"1.3\")\n\n2. **Remove from scenarios.md**:\n   ```bash\n   # Find and remove the scenario entry:\n   ### Scenario N.M: [Name]\n   - **Type**: [type]\n   - **Details**: [link]\n   - **Implementation Progress**: [checkboxes]\n   ```\n\n3. **Remove from scenario detail file**:\n   ```bash\n   # Find and remove from <directory>/test-scenarios/{type}.md\n   ## Scenario N.M: [Name]\n   [entire scenario content]\n   ---\n   ```\n\n4. **Display confirmation**:\n   ```\n   ✅ Deleted Scenario N.M: [Name]\n   - Removed from scenarios.md\n   - Removed from test-scenarios/{type}.md\n\n   ⚠️ Note: If this scenario had implementation (tests/code), you may need to remove those manually.\n   ```\n\n## Expected File Structure\n\nAfter EXPAND mode, the directory structure will be:\n\n```\n<task-directory>/\n├── requirements.md            (INPUT - read-only, contains acceptance criteria)\n├── scenarios.md               (OUTPUT - all scenarios with Implementation tracking)\n└── test-scenarios/\n    ├── happy-path.md         (OUTPUT - success cases)\n    ├── error-cases.md        (OUTPUT - failure cases)\n    └── edge-cases.md         (OUTPUT - boundary cases)\n```\n\n## Usage Examples\n\n### Example 1: Create comprehensive scenarios from requirements.md\n```bash\n/test-scenarios apps/snyk-cmd/docs/features/bulk-ignore/tasks/task-001/\n```\n\nCreates all scenarios from requirements.md in that directory.\n\n### Example 2: Add single scenario\n```bash\n/test-scenarios apps/feature/task-001/ \"add scenario for null organization name to AC-3\"\n```\n\nAdds one scenario to existing set for AC-3.\n\n### Example 3: Modify existing scenario\n```bash\n/test-scenarios apps/feature/task-001/ \"modify scenario 2.1 to test empty string instead of null\"\n```\n\nUpdates scenario 2.1 with new behavior.\n\n### Example 4: Discover and add missing scenarios\n```bash\n/test-scenarios apps/feature/task-001/ \"discover gaps in existing scenarios\"\n```\n\nAnalyzes scenarios for gaps and automatically generates and adds missing test scenarios.\n\n### Example 4a: Discover gaps with additional context\n```bash\n/test-scenarios apps/feature/task-001/ \"discover gaps\" @research.md @tech-design.md\n```\n\nUses additional context files to discover edge cases, technical constraints, and integration scenarios that may not be obvious from requirements alone. The qa-engineer agent may ask clarifying questions, then generates and adds the missing scenarios automatically.\n\n### Example 5: Delete scenario\n```bash\n/test-scenarios apps/feature/task-001/ \"delete scenario 1.3\"\n```\n\nRemoves scenario 1.3 from scenarios.md and test-scenarios files.\n\n## Key Principles\n\nThis command is the **orchestrator**:\n- ✅ Determines what to do (mode selection)\n- ✅ Prepares context for agent\n- ✅ Calls agent with simple request\n- ✅ Parses structured JSON output\n- ✅ Handles all file operations\n- ✅ Manages scenario numbering based on agent's priority\n- ✅ Creates links and tracking\n- ✅ Displays warnings to user\n- ✅ Handles context requests from agent (DISCOVER mode)\n\nThe qa-engineer agent is the **QA expert**:\n- ✅ Applies QA heuristics\n- ✅ Classifies scenarios by type (happy-path/error-case/edge-case)\n- ✅ Assigns priority based on QA expertise\n- ✅ Links scenarios to acceptance criteria\n- ✅ Generates Given-When-Then scenarios\n- ✅ Uses business-friendly language\n- ✅ Warns about duplicates and implementation impact\n- ✅ Requests additional context if needed (DISCOVER mode)\n- ✅ Returns structured JSON for easy parsing\n- ❌ Does NOT handle files, numbering, or organization"
              }
            ],
            "skills": []
          },
          {
            "name": "dgomezs-toolkit",
            "description": "A personal toolkit of development skills and best practices for Java, Quarkus, testing, and general software engineering",
            "source": "./plugins/dgomezs-toolkit",
            "category": "skills-library",
            "version": "0.0.1",
            "author": {
              "name": "dgomezs",
              "url": "https://github.com/dgomezs"
            },
            "install_commands": [
              "/plugin marketplace add dgomezs/claude-code",
              "/plugin install dgomezs-toolkit@tdd-specflow-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-10-26T06:00:47Z",
              "created_at": "2025-10-25T04:16:09Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}